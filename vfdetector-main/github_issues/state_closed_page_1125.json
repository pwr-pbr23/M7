[{"number": 19481, "title": "PowerSignOptimizer not available on TPUv2", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: TPUv2-8\r\n- **TensorFlow installed from (source or binary)**: VM disk image as configured by ctpu\r\n- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072\r\n- **Python version**: 2.7.13\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: Attempt to execute ```tf.contrib.opt.PowerSignOptimizer()``` on a TPUv2\r\n\r\n### Describe the problem\r\nThe Powersign optimizer ```tf.contrib.opt.PowerSignOptimizer()``` does not have a OpKernel for the XLA_TPU_JIT device.\r\n\r\n### Source code / logs\r\n```\r\nNotFoundError (see above for traceback): No registered 'ResourceApplyPowerSign' OpKernel for XLA_TPU_JIT devices compatible with node PowerSignOptimizer/update_dense/bias/ResourceApplyPowerSign = ResourceApplyPowerSign[T=DT_FLOAT, _class=[\"loc:@PowerSignOptimizer/update_dense/bias/Read/ReadVariableOp\"], use_locking=false, _device=\"/device:TPU_REPLICATED_CORE\"](_arg106, _arg10, PowerSignOptimizer/learning_rate, PowerSignOptimizer/logbase, PowerSignOptimizer/sign_decay, PowerSignOptimizer/beta, CrossReplicaSum_95)\r\n        .  Registered:  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_BFLOAT16]\r\n  device='CPU'; T in [DT_HALF]\r\n```\r\n", "comments": ["Nagging Assignee @tatatodd: It has been 123 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Apologies for the long delay, the op was implemented a while ago (but after you filed this issue), and is here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2xla/kernels/training_ops.cc#L904"]}, {"number": 19480, "title": "Manually placing operations in eager execution raises FailedPreconditionError.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: tensorflow 1.6\r\n- **Python version**: python3.6\r\n- **Bazel version (if compiling from source)**: \r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: V9.0.176\r\n- **GPU model and memory**:  NVIDIA Corporation GV100GL [Tesla V100 SXM2 16GB] (rev a1)\r\n- **Exact command to reproduce**: \r\n\r\n\r\n\r\n### Describe the problem\r\nI have a model that I believe was not automatically being placed onto an available GPU.  \r\n\r\nI then placed this part of the computation inside a with_device() block.  This schematically looks like:\r\n\r\n ```\r\n# define some variables\r\nwith tf.device(\"/device:GPU:0\"):\r\n     with tfe.GradientTape(persistent=True) as tape:\r\n        # calculate loss based on variables\r\n\r\n```\r\n\r\nThe error is thrown during the loss calculation step.  \r\n\r\n### Source code / logs\r\n\r\n\r\n2018-05-22 13:57:11.963021: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-05-22 13:57:12.324280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: \r\nname: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\r\npciBusID: 0000:04:00.0\r\ntotalMemory: 15.78GiB freeMemory: 15.36GiB\r\n2018-05-22 13:57:12.324584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\r\n2018-05-22 13:57:12.623548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14878 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:04:00.0, compute capability: 7.0)\r\nTraceback (most recent call last):\r\n  File \"tf_registration_continuous.py\", line 619, in <module>\r\n    cProfile.run('main()', file)\r\n  File \"/share/software/user/open/python/3.6.1/lib/python3.6/cProfile.py\", line 16, in run\r\n    return _pyprofile._Utils(Profile).run(statement, filename, sort)\r\n  File \"/share/software/user/open/python/3.6.1/lib/python3.6/profile.py\", line 55, in run\r\n    prof.run(statement)\r\n  File \"/share/software/user/open/python/3.6.1/lib/python3.6/cProfile.py\", line 95, in run\r\n    return self.runctx(cmd, dict, dict)\r\n  File \"/share/software/user/open/python/3.6.1/lib/python3.6/cProfile.py\", line 100, in runctx\r\n    exec(cmd, globals, locals)\r\n  File \"<string>\", line 1, in <module>\r\n  File \"tf_registration_continuous.py\", line 615, in main\r\n    accuracy ,runtime, final_loss = run_registration(directory='output/' + hparams_run.name, hparams=hparams_run, dataset_path = dataset_path, save_figs=False)\r\n  File \"tf_registration_continuous.py\", line 525, in run_registration\r\n    rm.register(save_summaries = save_figs, make_animation=save_figs)\r\n  File \"tf_registration_continuous.py\", line 190, in register\r\n    num_images_to_optimize_params= self.hparams.num_images_to_optimize_params\r\n  File \"tf_registration_continuous.py\", line 105, in single_registration_step\r\n    _ = self.eif.warp(scale, num_images_for_loss_calculation)\r\n  File \"/home/groups/bmacint/Ultrasound/timing/elastic_image_field.py\", line 118, in warp\r\n    field_image.initialize_translation()\r\n  File \"/home/groups/bmacint/Ultrasound/timing/field_image.py\", line 87, in initialize_translation\r\n    self.translation_warp_points = tf.tile(self.translation[tf.newaxis, tf.newaxis, :],\r\n  File \"/share/software/user/open/py-tensorflow/1.6.0_py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 828, in _SliceHelperVar\r\n    return _slice_helper(var._AsTensor(), slice_spec, var)\r\n  File \"/share/software/user/open/py-tensorflow/1.6.0_py36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 741, in _AsTensor\r\n    return self.value()\r\n  File \"/share/software/user/open/py-tensorflow/1.6.0_py36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 572, in value\r\n    return self._read_variable_op()\r\n  File \"/share/software/user/open/py-tensorflow/1.6.0_py36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 655, in _read_variable_op\r\n    self._dtype)\r\n  File \"/share/software/user/open/py-tensorflow/1.6.0_py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\", line 304, in read_variable_op\r\n    attrs=_attrs, ctx=_ctx, name=name)\r\n  File \"/share/software/user/open/py-tensorflow/1.6.0_py36/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\", line 66, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 2, in raise_from\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Error while reading resource variable 152 from Container: eager-execution-0/. This could mean that the variable was uninitialized. Invalid argument: Trying to access resource located in device /job:localhost/replica:0/task:0/device:CPU:0 from device /job:localhost/replica:0/task:0/device:GPU:0 [Op:ReadVariableOp]\r\n\r\n", "comments": ["I have been able to reproduce this issue with a simple minimal example:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ntfe = tf.contrib.eager\r\ntfe.enable_eager_execution(config=tf.ConfigProto(allow_soft_placement=True,\r\n                                        log_device_placement=True), device_policy=tfe.DEVICE_PLACEMENT_WARN)\r\n\r\na = tfe.Variable([2.,3.])\r\n\r\nwith tf.device(\"/device:CPU:0\"):\r\n    print(a)\r\n    print(a.device)\r\nwith tf.device(\"/device:GPU:0\"):\r\n    print(a)\r\n    print(a.device)\r\n```\r\n\r\nThe error is similar to the one quoted above:\r\n\r\n2018-05-22 15:31:14.700544: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-05-22 15:31:14.995858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: \r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:84:00.0\r\ntotalMemory: 11.92GiB freeMemory: 11.85GiB\r\n2018-05-22 15:31:14.996414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\r\n2018-05-22 15:31:15.471576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11492 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:84:00.0, compute capability: 3.7)\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla K80, pci bus id: 0000:84:00.0, compute capability: 3.7\r\n2018-05-22 15:31:15.717080: I tensorflow/core/common_runtime/direct_session.cc:297] Device mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla K80, pci bus id: 0000:84:00.0, compute capability: 3.7\r\n\r\n<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([ 2.,  3.], dtype=float32)>\r\n\r\nTraceback (most recent call last):\r\n  File \"minimal_failing_case.py\", line 16, in <module>\r\n    print(a)\r\n  File \"/share/software/user/open/py-tensorflow/1.6.0_py36/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 239, in __repr__\r\n    ops.numpy_text(self.read_value(), is_repr=True))\r\n  File \"/share/software/user/open/py-tensorflow/1.6.0_py36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 677, in read_value\r\n    value = self._read_variable_op()\r\n  File \"/share/software/user/open/py-tensorflow/1.6.0_py36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 655, in _read_variable_op\r\n    self._dtype)\r\n  File \"/share/software/user/open/py-tensorflow/1.6.0_py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\", line 304, in read_variable_op\r\n    attrs=_attrs, ctx=_ctx, name=name)\r\n  File \"/share/software/user/open/py-tensorflow/1.6.0_py36/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\", line 66, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 2, in raise_from\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Error while reading resource variable 2 from Container: eager-execution-0/. This could mean that the variable was uninitialized. Invalid argument: Trying to access resource located in device /job:localhost/replica:0/task:0/device:CPU:0 from device /job:localhost/replica:0/task:0/device:GPU:0 [Op:ReadVariableOp]", "@alextp, PTAL.\r\n\r\nThanks!", "@akshaym can you take a look at this? Something which is supposed to force-colocate the read variable op with the handle is not force-colocating it. The colocation code is in https://github.com/tensorflow/tensorflow/blob/09b562c8f21a8f9fcae71cbb5b97434c06ba1ca9/tensorflow/core/common_runtime/eager/execute.cc#L432 but some condition must be missed.", "Hi @Noahyt, \r\n\r\nThanks for the report. I'm able to reproduce this on 1.6.0 but unable to reproduce on the latest version of tensorflow (1.8), so its been fixed already. It'd be great if you could upgrade your installation of tensorflow!\r\n\r\nThanks!", "I'm noticing a simiilar problem with TF 2.0 nightly (`2.0.0-dev20190228`) which enforces Eager mode. \r\n\r\nThe following minimal code:\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.datasets import cifar10\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Flatten, Dense\r\nfrom tensorflow.keras.layers import Conv2D\r\n\r\n\r\n(X_train, y_train), _ = cifar10.load_data()\r\nX_train = X_train.astype('float32') / 255.0\r\n\r\n\r\nwith tf.device('cpu:0'):\r\n    model = Sequential([\r\n        Conv2D(32, (3, 3), input_shape=(32, 32, 3)),\r\n        Flatten(),\r\n        Dense(10, activation='softmax')\r\n    ])\r\n    \r\n    model.compile(loss='sparse_categorical_crossentropy',\r\n                  optimizer='rmsprop',\r\n                  metrics=['accuracy'])\r\n\r\n    \r\nmodel.fit(X_train, y_train,\r\n          batch_size=1024,\r\n          epochs=2)\r\n```\r\n\r\nproduces error:\r\n```python\r\n---------------------------------------------------------------------------\r\nFailedPreconditionError                   Traceback (most recent call last)\r\n<ipython-input-1-d00bb2a707a4> in <module>\r\n     24 model.fit(X_train, y_train,\r\n     25           batch_size=1024,\r\n---> 26           epochs=2)\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    871           validation_steps=validation_steps,\r\n    872           validation_freq=validation_freq,\r\n--> 873           steps_name='steps_per_epoch')\r\n    874 \r\n    875   def evaluate(self,\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\r\n    349 \r\n    350         # Get outputs.\r\n--> 351         batch_outs = f(ins_batch)\r\n    352         if not isinstance(batch_outs, list):\r\n    353           batch_outs = [batch_outs]\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in __call__(self, inputs)\r\n   3215         value = math_ops.cast(value, tensor.dtype)\r\n   3216       converted_inputs.append(value)\r\n-> 3217     outputs = self._graph_fn(*converted_inputs)\r\n   3218     return nest.pack_sequence_as(self._outputs_structure,\r\n   3219                                  [x.numpy() for x in outputs])\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n    523       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\r\n    524           list(kwargs.keys()), list(self._arg_keywords)))\r\n--> 525     return self._call_flat(args)\r\n    526 \r\n    527   def _filtered_call(self, args, kwargs):\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args)\r\n    592     # Only need to override the gradient in graph mode and when we have outputs.\r\n    593     if context.executing_eagerly() or not self.outputs:\r\n--> 594       outputs = self._inference_function.call(ctx, args)\r\n    595     else:\r\n    596       self._register_gradient()\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args)\r\n    380             attrs=(\"executor_type\", executor_type,\r\n    381                    \"config_proto\", config),\r\n--> 382             ctx=ctx)\r\n    383       # Replace empty list with None\r\n    384       outputs = outputs or None\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     64     else:\r\n     65       message = e.message\r\n---> 66     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     67   except TypeError as e:\r\n     68     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/six.py in raise_from(value, from_value)\r\n\r\nFailedPreconditionError: Error while reading resource variable _AnonymousVar15 from Container: localhost. This could mean that the variable was uninitialized. Invalid argument: Trying to access resource _AnonymousVar15 located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\r\n\t [[{{node training/RMSprop/RMSprop/update_3/mul/ReadVariableOp}}]] [Op:__inference_keras_scratch_graph_646]\r\n```\r\n\r\nI've been able to fix this in 3 ways:\r\n1) use `tensorflow.compat.v1.disable_eager_execution()`\r\n2) remove the `Conv2D` layer\r\n3) remove the `batch_size` and `epochs` arguments from the `.fit` call\r\n\r\nthe behavior seems weird, can anyone explain what's going on?", "I've also opened a separate issue #26244 "]}, {"number": 19479, "title": "INTEL-MKL: Fix build issue with old GCC version - MklConcat related", "body": "This fix addresses a build issue related to MklConcat with old GCC version. \r\nFor GCC 5.0/4.8 or older version, key of unordered_map can only be primitive type and cannot be \"enum\". \r\nThis issue does not occur with GCC 6.*. ", "comments": ["@gzmkl  Please fix formatting errors identified by pylint. ", "Hi Tatiana, \r\nI did code style check on mkl_concat_ops.cc, with the following output\r\n  $ /mnt/aipg_tensorflow_shared/cpplint.py mkl_concat_op.cc\r\n     mkl_concat_op.cc:30:  Include the directory when naming .h files  [build/include] [4]\r\n     mkl_concat_op.cc:31:  Include the directory when naming .h files  [build/include] [4]\r\n     Done processing mkl_concat_op.cc\r\n    Total errors found: 2\r\nThe \"header file\" errors are excusable (as we did before), right?", "Let's ask people who excused these before.\r\n@rmlarsen @andydavis1 What do you think?", "I don't recall why these were excused in the past. Could they be fixed easily?", "Well, such style error appears in multiple source (.cc/.cpp) files.\r\nIf we want to fix, it is better to be done in a separate PR.\r\n\r\nAs you may have noticed, this error is not introduced in code change of this PR. \r\n\r\nThanks!", "By the way, the errors are related to including MKL DNN header files\r\n\r\n     30 #include \"mkl_dnn.h\"\r\n     31 #include \"mkl_dnn_types.h\"\r\n\r\nWhere their \"include\" folder is located<build output folder>/external/mkl_dnn/include/, not closely tied to the tensorflow root folder. \r\n\r\n", "Hi Tatiana, \r\nI am sorry for miss-reading your original comments. It was about pylint instead of cpplint. \r\nMy PR does not touch any python code so it should not cause any pylint error.\r\n\r\nFuture investigation shows that the \"pylint\" issue has been fixed by \"yongtang\" in last 24 hours.\r\nYou can check the history of this file: tensorflow/contrib/cmake/tools/create_def_file.py.\r\n\r\nRegards,\r\nGZ\r\n", "Hi Rasmus, please let me know when you start to review this. \r\nI have budgeted my time to take actions on your suggestions for code change.\r\nBest,\r\nGZ", "Sorry, comment on a wrong PR"]}, {"number": 19478, "title": "Does eager execution allow dynamic batching like Tensorflow fold?", "body": "Tensorflow fold has no updates since months and as far as I know it only works with Tensorflow v1.0.\r\nDoes anybody know if eager execution in newer Tensorflow versions supports dynamic batching like Tensorflow fold? If not, is it likely that future versions would support it?\r\nThanks for any answer!\r\n \r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: N/A\r\n- **TensorFlow version (use command below)**: from 1.4 to the latest\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "yes, @tensorflowbutler this is still an issue.", "@ixime This is a stale issue. Is this still an issue for you. Can you please check with recent TF versions and share a simple standalone code to reproduce the issue. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/19478\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/19478\">No</a>\n"]}, {"number": 19477, "title": "tf.contrib.tensorrt.create_inference_graph fails for ssd_mobilenet_v2_coco network with error", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0 / 7.1.3\r\n- **GPU model and memory**: GeForce 940MX / 2GB \r\n- **TensorRT version**: 3.0.4\r\n- **Exact command to reproduce**:\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.tensorrt as trt\r\nfrom tensorflow.python.platform import gfile\r\n\r\n\r\ndef get_graph_def_from_pb(file):\r\n    with gfile.FastGFile(file, 'rb') as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n        return graph_def\r\n\r\n\r\ntrt_graph = trt.create_inference_graph(\r\n                input_graph_def=get_graph_def_from_pb(\"~/Downloads/ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb\"), # from http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz\r\n                outputs=[\"detection_boxes\"],\r\n                max_batch_size=1,\r\n                max_workspace_size_bytes=500000000,\r\n                precision_mode=\"FP32\")\r\n\r\n\r\nwith gfile.FastGFile(\"trt_frozen_graph.pb\", 'wb') as f:\r\n    f.write(trt_graph.SerializeToString())\r\n```\r\n\r\n### Describe the problem\r\nWhen I try to convert ssd_mobilenet_v2_coco for the use with TensorRT in tensorflow, the convert step fails with below's Traceback. This seems to be a problem within tf.contrib.tensorrt.create_inference_graph [Line 115](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensorrt/python/trt_convert.py#L115) which itself is an exception risen if the c++ binary [trt_convert](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensorrt/python/trt_convert.py#L102) returns an error. Unfortunately here my bread-crumbs end, as I've not found the line in [convert_graph.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensorrt/convert/convert_graph.cc), which might be responsible for this behaviour.\r\n\r\nUsing a much simpler network (just a few CNN layers and FCN layers) works - so it does not seem to be my TensorRT installation.\r\n\r\n\r\n\r\n\r\n### Source code / logs\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/q0we9saweq/projects/tf_helpers/tensorrt_prep.py\", line 20, in <module>\r\n    precision_mode=\"FP32\")\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tensorrt/python/trt_convert.py\", line 115, in create_inference_graph\r\n    int(msg[0]))\r\ntensorflow.python.framework.errors_impl.NotFoundError: No attr named 'index_type' in NodeDef:\r\n\t [[Node: MultipleGridAnchorGenerator/Meshgrid/ExpandedShape/ones = Fill[T=DT_INT32](MultipleGridAnchorGenerator/Meshgrid/ExpandedShape/Reshape, MultipleGridAnchorGenerator/Meshgrid/ExpandedShape/ones/Const)]]\r\n\t [[Node: MultipleGridAnchorGenerator/Meshgrid/ExpandedShape/ones = Fill[T=DT_INT32](MultipleGridAnchorGenerator/Meshgrid/ExpandedShape/Reshape, MultipleGridAnchorGenerator/Meshgrid/ExpandedShape/ones/Const)]]\r\n```\r\n", "comments": ["Duplicate to #18744. Closing."]}, {"number": 19476, "title": "Error while importing tensorflow (DLL Load Failed+No module named '_pywrap_tensorflow_internal')", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\nHello, this is my first time on github so forgive me if I do something wrong.\r\nI've been interested in machine learning and i've tried to install tensorflow (to use keras).\r\nWhen I finally thought everything was installed and ready, I got this issue when importing tensorflow on python. Can someone help me?\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: nope\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows10 64 bits\r\n- **TensorFlow installed from (source or binary)**: pip3 install --upgrade tensorflow\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: don't know what that is\r\n- **GCC/Compiler version (if compiling from source)**: neither that\r\n- **CUDA/cuDNN version**: neither that\r\n- **GPU model and memory**: i have an intel CPU\r\n- **Exact command to reproduce**: import tensorflow\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nTraceback (most recent call last):\r\n  File \"<console>\", line 1, in <module>\r\n  File \"c:\\users\\antoine rollet\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"c:\\users\\antoine rollet\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"c:\\users\\antoine rollet\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"c:\\users\\antoine rollet\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"c:\\users\\antoine rollet\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: Une routine d\u2019initialisation d\u2019une biblioth\u00e8que de liens dynamiques (DLL) a \u00e9chou\u00e9.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\antoine rollet\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"c:\\users\\antoine rollet\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"c:\\users\\antoine rollet\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"c:\\users\\antoine rollet\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["I'm facing the same issue :(", "Ok I managed to avoid the problem by installing tensorflow 1.5 instead of 1.8. It does not solve the issue but might help some people", "In my case it still doesn't solve the issue. 1.5 works well but doesn't have the 'eager execution' feature.", "Did you look at https://stackoverflow.com/questions/42011070/on-windows-running-import-tensorflow-generates-no-module-named-pywrap-tenso which might have some pointers?", "yeah it is still an issue\r\ni am using python 3.6.5\r\nwindows 7\r\nthe issue is still same if i try to install tensorflow 1.6 or above", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@Kundan999  - Hi, have you tried with tensorflow v 1.11 ? ", "Closing this issue due to staleness.\r\nPlease use the latest version of TensorFlow and build again. Feel free to open a [new issue](https://github.com/tensorflow/tensorflow/issues/new/choose) if the problem still persists. Thanks!"]}, {"number": 19475, "title": "transform_graph obfuscate_names error in Windows 10", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: source \r\n- **TensorFlow version (use command below)**: v1.7.1\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: Not used\r\n- **GCC/Compiler version (if compiling from source)**: Visual Studio 2015 (MSBuild.exe)\r\n- **CUDA/cuDNN version**: CUDA 9.1, cuDNN 7.0.5\r\n- **GPU model and memory**: NVIDIA GeForce GTX 1070\r\n- **Exact command to reproduce**:\r\n1. Open cmd\r\n2. Go to the directory where a saved model is stored.\r\n2. Run the following command\r\nPATH\\TO\\tensorflow-1.7.1\\tensorflow\\contrib\\cmake\\build_cuda_v9.1\\Release\\transform_graph --in_graph=saved_model.pb --out_graph=saved_model_2.pb --inputs=\"image:0\" --outputs=\"probability:0\" --transforms=\"obfuscate_names\"\r\n\r\n### Describe the problem\r\nI had the following error after running the command.\r\n\r\n[libprotobuf ERROR D:\\workspace_tensorflow_source\\tensorflow-1.7.1\\tensorflow\\contrib\\cmake\\build_cuda_v9.1\\protobuf\\src\\protobuf\\src\\google\\protobuf\\text_format.cc:288] Error parsing text-format tensorflow.GraphDef: 1:1: Invalid control characters encountered in text.\r\n[libprotobuf ERROR D:\\workspace_tensorflow_source\\tensorflow-1.7.1\\tensorflow\\contrib\\cmake\\build_cuda_v9.1\\protobuf\\src\\protobuf\\src\\google\\protobuf\\text_format.cc:288] Error parsing text-format tensorflow.GraphDef: 1:4: Interpreting non ascii codepoint 192.\r\n[libprotobuf ERROR D:\\workspace_tensorflow_source\\tensorflow-1.7.1\\tensorflow\\contrib\\cmake\\build_cuda_v9.1\\protobuf\\src\\protobuf\\src\\google\\protobuf\\text_format.cc:288] Error parsing text-format tensorflow.GraphDef: 1:4: Expected identifier, got: \u2514\r\n2018-05-22 13:39:08.760764: E D:\\workspace_tensorflow_source\\tensorflow-1.7.1\\tensorflow\\tools\\graph_transforms\\transform_graph.cc:200] Loading graph 'saved_model.pb' failed with Can't parse saved_model.pb as binary proto\r\n         (both text and binary parsing failed for file saved_model.pb)\r\n2018-05-22 13:39:08.767541: E D:\\workspace_tensorflow_source\\tensorflow-1.7.1\\tensorflow\\tools\\graph_transforms\\transform_graph.cc:202] usage: D:\\workspace_tensorflow_source\\tensorflow-1.7.1\\tensorflow\\contrib\\cmake\\build_cuda_v9.1\\Release\\transform_graph\r\nFlags:\r\n        --in_graph=\"\"                           string  input graph file name\r\n        --out_graph=\"\"                          string  output graph file name\r\n        --inputs=\"\"                             string  inputs\r\n        --outputs=\"\"                            string  outputs\r\n        --transforms=\"\"                         string  list of transforms\r\n        --output_as_text=false                  bool    whether to write the graph in text protobuf format\r\n\r\nTransforms are:\r\nadd_default_attributes\r\nbackport_concatv2\r\nbackport_tensor_array_v3\r\nflatten_atrous_conv\r\nfold_batch_norms\r\nfold_constants\r\nfold_old_batch_norms\r\nfreeze_requantization_ranges\r\nfuse_pad_and_conv\r\nfuse_resize_and_conv\r\nfuse_resize_pad_and_conv\r\ninsert_logging\r\nmerge_duplicate_nodes\r\nobfuscate_names\r\nquantize_nodes\r\nquantize_weights\r\nremove_attribute\r\nremove_control_dependencies\r\nremove_device\r\nremove_nodes\r\nrename_attribute\r\nrename_op\r\nround_weights\r\nset_device\r\nsort_by_execution_order\r\nsparsify_gather\r\nstrip_unused_nodes\r\n\r\n### Source code / logs\r\nI created a saved model using Estimator. The following shows how I create a saved model.\r\n\r\n            # Get warm start settings\r\n            wss = self._get_warm_start_settings()\r\n\r\n            # Create a estimator w/ or w/o warm start\r\n            estimator = tf.estimator.Estimator(\r\n                model_fn=NetTrainer.model_fn,\r\n                model_dir=self.model_path,\r\n                params={\r\n                    'net_id': str(self.net_id)\r\n                },\r\n                config=tf.estimator.RunConfig(\r\n                    save_checkpoints_steps=training_config.CHECKPOINTS_STEPS,\r\n                    save_summary_steps=training_config.SUMMARY_STEPS,\r\n                    keep_checkpoint_max=training_config.KEEP_CHENCKPOINT_MAX\r\n                ),\r\n                warm_start_from=wss)\r\n \r\n            for lp in list(range(0, training_config.NUM_TRAINING_EVALUATION_CYCLES)):\r\n                                  \r\n                # Train the model\r\n                estimator.train(\r\n                    input_fn=lambda:self.train_input_fn(training_db_reader_state),\r\n                    steps=training_config.TRAINING_STEPS,\r\n                    max_steps=None)\r\n  \r\n                # Evaluate the model\r\n                eval_result = estimator.evaluate(\r\n                    input_fn=lambda:self.eval_input_fn(evaluation_db_reader_state))\r\n                  \r\n                tf.logging.info('training-evaluation cycle: {:d}'.format(lp))\r\n                tf.logging.info('global_step: {global_step:d}'.format(**eval_result))\r\n                tf.logging.info('Test set accuracy: {accuracy:0.3f}'.format(**eval_result))\r\n                \r\n            # Export the model\r\n            image_shape = self._get_image_shape(include_batch=True, batch_size=1);\r\n            feature_spec = {config.IMAGE_KEY: tf.placeholder(tf.float32, shape=image_shape, name=config.IMAGE_KEY)}\r\n            serving_input_receiver_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(feature_spec, default_batch_size=1)\r\n\r\n            estimator.export_savedmodel(\r\n                export_dir_base=self.saved_model_path,\r\n                serving_input_receiver_fn=serving_input_receiver_fn,\r\n                strip_default_attrs=True)\r\n\r\n\r\n", "comments": ["Nagging Assignee @bignamehyp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Is it still an issue with latest TF?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 19474, "title": "Update Dockerfile to be based on 9.2-base-ubuntu18.04", "body": "", "comments": ["While this has a great potential to work, we are not continuously testing TF on cuda 9.2\r\nI would like to see what @zheng-xq and @tfboyd think about this change before I merge it.", "referencing this ticket: https://github.com/tensorflow/tensorflow/issues/18906", "@tfboyd ?", "I feel the device driver is too new for most people as it is not easy to get via the official debian repository or even the ubuntu repoistory (I just checked again) which is updated faster.  NVIDIA does not support officially support the driver on their DGX-1.  It is not hard to install but I find it odd how any people cannot install these tools.  I posted some basic [performance numbers ](https://github.com/tensorflow/tensorflow/issues/18906) and you get most of the CUDA 9.2 gains with CUDA 9.0/cuDNN 7.1.4.  I would like us to support two versions of CUDA like the other ML tools but that is not something the infrastructure is supporting right now.  That will have to change with the CUDA 10 launch.  There is some mid and long-term hope.  NVIDIA has suggested they will make CUDA 10 through 10.x backwards compatible with the initial device driver, and as soon as CUDA is early access I want to publish builds.    \r\n\r\nPer the issue below, I am open to suggestions and other ways to look at it.  One perspective is people that cannot update their device driver should get an older version of TensorFlow.  I kind of dig that idea, but I wonder how painful that is for people to then find out they need a much newer device driver.  On the plus side NVIDIA is publishing better CUDA install packages.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/18906\r\n\r\nLet me know what you think and if you have time what you are using that is specifically in CUDA 9.2/7.1.4 that is not in CUDA 9.0/7.1.4.  I am curious and that might help tip the scales in the short-term.", "I believe that in order to use Ubuntu 18.04 you need to be using CUDA 9.2 (see https://github.com/NVIDIA/nvidia-docker/issues/666#issuecomment-389118126). I would like to upgrade to 18.04 for a number of reasons, the primary being native support for Python 3.6.", "@cancan101  Ahh so CUDA 9.2 is not the goal the goal is Ubuntu 18.04.  CUDA 9.2 is a means to an end.  I did not consider that.", "18.04 seems fresh and as an indication are not offering an upgrade from 16.04 LTS until maybe later July.  I do not know how popular the Docker is but moving it to Python 3.6 as the default seems like a big deal if people were expecting it to be 2.x.  NVIDIA dropped 14.04 and we should move to 18.04 I do not think it should happen right now, that is an even bigger move in my opinion than CUDA 9.2.  \r\n\r\nThank you for the info.  I really had not considered the Ubuntu angle.\r\n\r\n**If we moved to building tensorflow with CUDA 9.2 would that reduce your pain?**  You could have a custom docker file with a pip install tf-nightly or whatever and get Ubuntu 18.04.  Just asking.", "no no no. I am not suggesting moving from python 2 to python 3.6. There is already a python 3 image which I am suggesting moving to python 3.6 (from 3.5). ", "see: https://github.com/tensorflow/tensorflow/blob/444d3a7b382cd2a0f3759eb9f7791054f6319843/tensorflow/tools/docker/parameterized_docker_build.sh#L282-L316", "\r\nLet me try and get some base truth (to keep it straight for me, I can be slow and I do not know if what I was reading is correct.)\r\n\r\n- Ubuntu 18.04 is default Python 3.6, meaning if we move to that as the base we would need to switch the default as part of the DockerFile to Python 2.x for the python 2.x build\r\n- Ubuntu 18.04 needs CUDA 9.2\r\n- CUDA 9.2 needs device driver 396.26\r\n\r\nMy current concerns (and I want to upgrade):\r\n\r\n1. Device driver is really new.\r\n2. Ubuntu 18.04 was just released.\r\n\r\nIf I made up a random rule or idea.  I would jump to Ubuntu 18.04 with CUDA 10.  That way people just have to upgrade once and off we go.  I still would like to have a second CUDA build but that is not up to me.\r\n", ">Ubuntu 18.04 is default Python 3.6, meaning if we move to that as the base we would need to switch the default as part of the DockerFile to Python 2.x for the python 2.x build\r\n\r\nWhat I should have said above is that the default python3 version in 18.04 went from 3.5 to 3.6.", "I think what you are suggesting is the following, just to get the ask 100% correct.\r\n\r\n- Upgrade both Python 2 and 3 Dockers to Ubuntu 18.04 (which then means CUDA 9.2)\r\n- For the Python 2.x Docker we would include command to swap in Python 2.x as the default.\r\n\r\nI know this does not help you now, but I am inclined to push for 18.04 for the CUDA 10 release.  Thoughts? dirty looks?  Maybe donate a pile of GPUs and we do a second CUDA build.  :-)  Send Gunan and others gifts (under the value that would make the gifts of legal concern).  \r\n", "Yes that sounds about right. \r\n\r\nWhat is the timeline for Cuda 10 + support in tf + docker images for Cuda 10? If it's soon, than might be no problem. If could be a while, I'd love to get onto 18.04 without having to wait for that. ", "My guess is CUDA 10 will be later Q3 or early Q4.  I can see moving to CUDA 9.2 near the end of the cycle in early Q3.  That makes the move to Ubuntu 18.04 not really relevant, and I still think it may be a good time to move to 18.04 as that keeps us in the middle of the cycles  16.04 end of life's April 2021.  \r\n\r\nHow many systems to you manage and are you upgrading them all to Ubuntu 18.04?  That seems aggressive for a production system.  I am going to mark this closed as we will not be accepting the change in the near-term.", "We use Docker so upgrading the \"system\" involves bumping the base Docker image. All we need to do is update the Nvidia driver version on the host. That involves pre-baking an AMI.", "Last question, if I install tf from pypi, should it work with Cuda 9.2?", "No, the pypi versions are  built against CUDA 9.0.", "> No, the pypi versions are built against CUDA 9.0.\r\n\r\nA few versions later and it is no longer true. TF 1.13.1 uses CUDA 10.0. What if our servers don't have NVIDIA drivers new enough for CUDA 10? What is the newest TF wheel I can still use under CUDA 9.0? \r\n\r\n(And a suggestion: why not include CUDA / cuDNN versions in wheel names?)", "https://www.tensorflow.org/install/source#linux   \r\n\r\ntensorflow_gpu-1.12.0\r\n\r\nYou have no idea how long it took me to find this closed issue and add that link .\r\n\r\nI try to keep us on the same CUDA until a major release happens do to driver issues.  NVIDIA keeps getting closer to backward compatibility but it is not there just yet.  Really close and likely kind of working, but I also like to keep it simple even when I get flamed by a minority of people or maybe majority secretly I have no idea.  :-)", "@mirekphd \r\nhttps://www.tensorflow.org/install/source#linux\r\n1.12 was the last on CUDA 9.0\r\n\r\nI try to keep us on the same CUDA until a major release comes out.  This results in me getting a decent amount of flames from the interwebs.  NVIDIA is close and kind of has some backwards compatibility working and keepings making it better every quarter.  I have hopes the situation gets better.\r\n\r\nI cannot believe I am commenting on an issue closed almost 1 year ago. It took me 30 minutes just to find this issue after seeing it on my phone and then finding the link to the versions on the website.  Time to go home.", "@tfboyd : thank you, much appreciated. I ended up installing [1.12.2 from PyPi](https://pypi.org/project/tensorflow-gpu/1.12.2/) and it works in the GPU under CUDA 9.0 (9.0.176.3-1). \r\n\r\nWell, Google is the best search engine for closed Github issues, as the closed issues tend to contain more solutions than the open ones, so the closed ones should rank higher:)", "I need to get some internet points for commenting on old issues.  I really\nam glad you found what you needed.  I read more closed bugs when looking\nfor answers as well.\n\n*From: *mirekphd <notifications@github.com>\n*Date: *Tue, May 14, 2019, 4:42 AM\n*To: *tensorflow/tensorflow\n*Cc: *Toby Boyd, Mention\n\n@tfboyd <https://github.com/tfboyd> : thank you, much appreciated. I ended\n> up installing 1.12.2 from PyPi\n> <https://pypi.org/project/tensorflow-gpu/1.12.2/> and it works in the GPU\n> under CUDA 9.0 (9.0.176.3-1).\n>\n> Well, Google is the best search engine for closed Github issues, as the\n> closed issues tend to contain more solutions than the open ones, so the\n> closed ones should rank higher:)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/19474?email_source=notifications&email_token=AFTF5MWGAFEPMTLZ6LM2GBDPVKQTZA5CNFSM4FBE4QIKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLGHHQ#issuecomment-492200862>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AFTF5MTUIJZWOQ3DGFALPM3PVKQTZANCNFSM4FBE4QIA>\n> .\n>\n"]}, {"number": 19473, "title": "Random initialization of a GPU variable with more than INT32_MAX elements crashes with CUDA_ERROR_ILLEGAL_ADDRESS", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.5.2\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: V100\r\n- **Bazel version**: N/A\r\n- **Exact command to reproduce**: see below\r\n\r\n### Describe the problem\r\nRandom initialization of a GPU variable with more than INT32_MAX elements crashes with CUDA_ERROR_ILLEGAL_ADDRESS.\r\n\r\n### Source code / logs\r\nThe following code runs with no problem (ran into this with large embedding tables):\r\n```python\r\nimport tensorflow as tf\r\nn = 13417676\r\nh = 160\r\nx = tf.get_variable('x', [n, h])\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n```\r\nThe following code crashes with CUDA_ERROR_ILLEGAL_ACCESS:\r\n```python\r\nimport tensorflow as tf\r\nn = 13417677 # increased by 1\r\nh = 160\r\nx = tf.get_variable('x', [n, h])\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n```\r\n\r\nRunning with `cuda-memcheck` reports the following:\r\n```\r\n========= Invalid __global__ write of size 16\r\n=========     at 0x00000850 in void tensorflow::functor::FillPhiloxRandomKernelLaunch<tensorflow::random::UniformDistribution<tensorflow::random::PhiloxRandom, float>>(tensorflow::random::PhiloxRandom, tensorflow::random::PhiloxRandomResultElementType*, __int64, tensorflow::functor::FillPhiloxRandomKernelLaunch<tensorflow::random::UniformDistribution<tensorflow::random::PhiloxRandom, float>>)\r\n=========     by thread (1023,0,0) in block (127,0,0)\r\n=========     Address 0x7f7e3c6806f0 is out of bounds\r\n```\r\n\r\nChecking the source code, the following looks suspicious to me: https://github.com/tensorflow/tensorflow/blob/982549ea3423df4270ff154e5c764beb43d472da/tensorflow/core/kernels/random_op_gpu.cu.cc#L136\r\n\r\nThe `int32 offset` variable is later used to index into the output array -- and I believe it can overflow and generate an illegal negative index.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version", "Updated, Bazel version is N/A (not relevant here)", "@zheng-xq could you PTAL?", "Ping on this bug -- it's still broken. I believe the relevant int32 value that should be int64 is now here: https://github.com/tensorflow/tensorflow/blob/2199d3064c3f7778d51b9d62f6e81d1640d980f4/tensorflow/core/kernels/random_op_gpu.h#L137", "I see this is fixed here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/random_op_gpu.h#L148 in latest version, closing this request. Thank you."]}, {"number": 19472, "title": "Corrected compute_gradient docstring", "body": "After testing the tf.test.compute_gradient function with complex inputs,\r\nnoticed that the docstring specifies that the real/imag partial derivatives\r\nare in the four corners of a larger matrix, when the result given indicates\r\nthey are instead interleaved.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 19471, "title": "T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Nagging Assignee @michaelisard: It has been 18 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "You can close it.\n\nOn 11 June 2018 at 03:32, Alfred Sorten Wolf <notifications@github.com>\nwrote:\n\n> Nagging Assignee @michaelisard <https://github.com/michaelisard>: It has\n> been 18 days with no activity and this issue has an assignee. Please update\n> the label and/or status accordingly.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19471#issuecomment-396085210>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AcHEoltgk1OgvYZXIK3rsyMVu3u3aNkqks5t7ZeAgaJpZM4UJG7x>\n> .\n>\n", "train_dataset = tf.data.TextLineDataset(train_dataset_fp)\r\n2018-06-23 12:47:01.260933: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n>>>this message received while run first tensorflow example"]}, {"number": 19470, "title": "Branch 197583446", "body": "", "comments": ["Why does cmake build still say \"pending\". Tests were triggered two days ago it appears.", "I just triggered it manually."]}, {"number": 19469, "title": "C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I'm not sure there's much we can do. `h5py` is used by keras. Could you try to file a bug upstream?", "what is that?\nhow can I try it?\n\nOn Mon, 28 May 2018 03:53 drpngx, <notifications@github.com> wrote:\n\n> I'm not sure there's much we can do. h5py is used by keras. Could you try\n> to file a bug upstream?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19469#issuecomment-392381321>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AcHEoryy5trsvhXTxrzLgMNol3Qw7l37ks5t2ydYgaJpZM4UJG5U>\n> .\n>\n", "Nagging Assignee @drpngx: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "You can close it.\n\nOn 12 June 2018 at 00:26, Alfred Sorten Wolf <notifications@github.com>\nwrote:\n\n> Nagging Assignee @drpngx <https://github.com/drpngx>: It has been 14 days\n> with no activity and this issue has an assignee. Please update the label\n> and/or status accordingly.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19469#issuecomment-396347264>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AcHEoh9UxUdxowi6xp2IaZchgQirHv5Cks5t7r1ugaJpZM4UJG5U>\n> .\n>\n"]}, {"number": 19468, "title": "  The scripts freeze_graph.exe, saved_model_cli.exe, tensorboard.exe, toco.exe and toco_from_protos.exe are installed in 'c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\Scripts' which is not on PATH.   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Closing an empty issue."]}, {"number": 19467, "title": "get_variables constraint doesn't work", "body": "```\r\nimport tensorflow as tf\r\na = tf.get_variable('a', shape=[], constraint=lambda t: tf.clip_by_value(t, -2, 2))\r\nsess = tf.Session()\r\nsess.run(a.assign(3.))\r\nsess.run(a)\r\n```\r\n\r\nI tried on `tf 1.8.0` and `tf 1.5.0`, both outputs `3`", "comments": ["If you read the API:\r\n```\r\nconstraint: An optional projection function to be applied to the variable\r\n        after being updated by an `Optimizer` (e.g. used to implement norm\r\n        constraints or value constraints for layer weights). The function must\r\n        take as input the unprojected Tensor representing the value of the\r\n        variable and return the Tensor for the projected value\r\n        (which must have the same shape). Constraints are not safe to\r\n        use when doing asynchronous distributed training.\r\n```", "Thanks, I got it. The constraint is only applied after being updated by a `Optimizer`"]}, {"number": 19466, "title": "libpng compiled without VSX option (for IBM Power8)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: RHEL75\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.7.1\r\n- **Python version**:  Python 3.5.4+ (default, Jan  9 2018, 16:01:00)\r\n- **Bazel version (if compiling from source)**: 0.13.0\r\n- **GCC/Compiler version (if compiling from source)**: GCC 6.4.1\r\n- **CUDA/cuDNN version**: 9.2/7.1.3\r\n- **GPU model and memory**: P100 Nvidia\r\n- **Exact command to reproduce**: import tensorflow\r\n\r\n### Describe the problem\r\nCompiling from source TF on IBM Power8 (ppc64le) leads to a problem with libpng. \r\nBy default, TF download and get it's own libpng. This however lacks VSX symbols inside. To add those symbols libpng should be compiled with the following flags:\r\n\r\n```\r\n./configure --enable-powerpc-vsx --enable-hardware-optimizations\r\n```\r\n\r\notherwise it leads to the following error when importing tensorflow:\r\n\r\n```ImportError: /hpc_modules/at10.0-based/deeplearning/tensorflow/tensorflow-v1.7.1_openmpi-3.0.1/lib64/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: png_init_filter_functions_vsx```\r\n\r\nI think the `third_party/png.BUILD` file needs to be updated accordingly, but I do not have experience with bazel to do that myself.\r\n\r\nCould you please solve this?\r\n\r\n### Source code / logs\r\n```> python3\r\nPython 3.5.4+ (default, Jan  9 2018, 16:01:00)\r\n[GCC 6.4.1 20170720 (Advance-Toolchain-at10.0) IBM AT 10 branch, based on subve on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"/hpc_modules/at10.0-based/deeplearning/tensorflow/tensorflow-v1.7.1_openmpi-3.0.1/lib64/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/hpc_modules/at10.0-based/deeplearning/tensorflow/tensorflow-v1.7.1_openmpi-3.0.1/lib64/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/hpc_modules/at10.0-based/deeplearning/tensorflow/tensorflow-v1.7.1_openmpi-3.0.1/lib64/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/opt/at10.0/lib64/python3.5/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/opt/at10.0/lib64/python3.5/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /hpc_modules/at10.0-based/deeplearning/tensorflow/tensorflow-v1.7.1_openmpi-3.0.1/lib64/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: png_init_filter_functions_vsx\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/hpc_modules/at10.0-based/deeplearning/tensorflow/tensorflow-v1.7.1_openmpi-3.0.1/lib64/python3.5/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *  # pylint: disable=redefined-builtin\r\n  File \"/hpc_modules/at10.0-based/deeplearning/tensorflow/tensorflow-v1.7.1_openmpi-3.0.1/lib64/python3.5/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/hpc_modules/at10.0-based/deeplearning/tensorflow/tensorflow-v1.7.1_openmpi-3.0.1/lib64/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/hpc_modules/at10.0-based/deeplearning/tensorflow/tensorflow-v1.7.1_openmpi-3.0.1/lib64/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/hpc_modules/at10.0-based/deeplearning/tensorflow/tensorflow-v1.7.1_openmpi-3.0.1/lib64/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/hpc_modules/at10.0-based/deeplearning/tensorflow/tensorflow-v1.7.1_openmpi-3.0.1/lib64/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/opt/at10.0/lib64/python3.5/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/opt/at10.0/lib64/python3.5/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /hpc_modules/at10.0-based/deeplearning/tensorflow/tensorflow-v1.7.1_openmpi-3.0.1/lib64/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: png_init_filter_functions_vsx\r\n\r\n\r\nFailed to load the native TensorFlow runtime.```", "comments": ["My preliminary comment is that we need to ask for https://github.com/glennrp/libpng/blob/libpng16/scripts/pnglibconf.h.prebuilt to be updated with\r\n`+ #define PNG_POWERPC_VSX_OPT 2`\r\nor\r\n```\r\n< /*#undef PNG_POWERPC_VSX_CHECK_SUPPORTED*/\r\n> #define PNG_POWERPC_VSX_CHECK_SUPPORTED\r\n\r\n```\r\nBut I am not sure about the propagation of that git repo into https://mirror.bazel.build/github.com/glennrp/libpng/archive/v1.6.34.tar.gz\r\n\r\nIn the mean time, I am trying out updating tensorflow/workspace.bzl to swap out the mirror.bazel... link to a local one with updated scripts/pnglibconf.h.prebuilt to see if that resolves it.   I will get back to you with the results.   (Just to be clear, I am as much a tourist to the TF and Bazel projects as you, but have almost the same environment.  So if there any TF project/Bazel natives that want to chime in, you will be much more competent at commenting on the unintended consequences of this proposed change on the x64 world or style points on the modifications to the Bazel workspace.)   Anyway I will get back to you with the results of my experiment, but in the mean time if I ultimately fail, this might give you another guidepost to follow on your setup.\r\n\r\nBy the way, I am getting stalled during the compilation rather than at the python import, but it is on the same symbol so I think we are on the same road and any solutions one of us has is likely to the helpful to the other.\r\n", "Thank you for the help! I really look forward for the results of your experiments.\r\n\r\nWith respect to my tests one clarification. I also had the issue during compilation and I solved it with a trick: I implemented a short script that runs in the background during the installation, and swap the libpng.a file generated by tensorflow with mine. \r\n\r\n```\r\n# TRY TO FIX LIBPNG\r\nMONITORDIR=$install_path/tensorflow-git/bazel-out/ppc-py3-opt/bin/external/png_archive/\r\n\r\n# REMOVE OLD TF LIBPNG\r\nrm -rf $MONITORDIR/libpng.a\r\n\r\n# WAIT UNTIL TF CREATES IT AGAIN\r\nwhile [ ! -f $MONITORDIR/libpng.a ]\r\ndo\r\n    sleep 1\r\ndone\r\n\r\n# REPLACE TF LIBPNG WITH MINE\r\ncd $MONITORDIR\r\nrm -rf libpng.a\r\ncp -f $LIBPNG_PATH/lib/lib* .\r\ncd -\r\n``` \r\n\r\nThis helps during compilation, however during execution I end up with the issue above. So I believe that somewhere else the wrong libpng is still linked... same with 1.8.0\r\n\r\n", "Ok, so here are two quick-and-dirty patches that should make it go.   (It appeared to get TF 1.8 going, but I am still not sure because we wanted it for Magenta, and we do not yet have that going, so can't be sure the stuff underneath is not messed.) \r\n\r\nVersion 1 replaces the glennrp git repo with my git repo having applied the minor patching.   However, after all my fiddling, I applied that patch via the existing patch anyway, so version 2 (\"gentler\") just pulls the libpng from glennrp and does the patch on the fly.   Version 1 is the current state of my build repo, but Version 2 should be sufficient, I have just not wiped my repo and built again.\r\n\r\n\r\n[tf-ppc64le.patch.txt](https://github.com/tensorflow/tensorflow/files/2037085/tf-ppc64le.patch.txt)\r\n[tf-ppc64le-gentler.patch.txt](https://github.com/tensorflow/tensorflow/files/2037088/tf-ppc64le-gentler.patch.txt)\r\n\r\nI am still trying to figure out if there is a way of getting Bazel to do what configure and CMake can do where they decided what defines and files to include into the project depending on what platform we are on.   Until I can get that, I suspect we can not apply this to the main tree without breaking x64 compiles.", "Perhaps a dup of:\r\nhttps://github.com/tensorflow/tensorflow/pull/19291", "Can we check whether #19291 has fixed this? \r\n\r\n@maxwellc-dal, once libpng is updated upstream and a new release is available, we can mirror a new version. ", "Confirming #19291 resolves the problem in my build environment also.   ", "Ok, I will close this then. Thank you!"]}, {"number": 19465, "title": "Mixing Keras layers and tf.layers  in Estimator  ( or otherwise)", "body": "I am trying to figure out a way to mix Kears   and tf layers. Would be great  to be able to do something like the following. \r\n\r\n  x = tf.keras.layers.Dense(64, activation='relu', name ='d1')(inputs)\r\n  x = tf.keras.layers.Dense(64, activation='relu', name ='d2')(x)\r\n  predictions = tf.layers.Dense (units=10, activation=tf.nn.softmax, name='d3')(x)\r\n\r\nHave I written custom code : NO\r\nOS Platform and Distribution: N/A\r\nTensorFlow installed from : N/A\r\nTensorFlow version: 1.8\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 19 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Yes,   as far as I can tell, it is still an issue. ", "I think you'd better use tf.keras.layer only if possible, because tf.layers will be depreciated in the future.", "Tfdb and tensorboard debugger, histogram, distribution etc support are not supported  yet for keras.layers.   Can someone throw any light  on  the plan for debugging features for keras.layers ? \r\n\r\n", "Could you explain in more details (say, give an example)? I'm not familiar with debugger, however it sounds unreasonable for me that tensorboard doesn't support tf.keras.layers.", "Thanks for the help, @facaiy!\r\n\r\nThis sounds like a question for [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow), or a feature request for https://github.com/tensorflow/tensorboard, so I'm closing this issue."]}, {"number": 19464, "title": "TF failing to build on Bazel CI", "body": "See logs at https://buildkite.com/bazel/bazel-with-downstream-projects-bazel/builds/246#a2654e2b-dbaf-4c4b-909f-1dfd16b7fe7a\r\n\r\nThe underlying error is:\r\n```\r\nERROR: /usr/local/google/home/jcater/.cache/bazel/_bazel_jcater/30de07a0c21303385c48fed63725bc15/external/grpc/BUILD:1300:1: C++ compilation of rule '@grpc//:grpc_resolver_dns_ares' failed (Exit 1)\r\nexternal/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:30:10: fatal error: ares.h: No such file or directory\r\n #include <ares.h>\r\n          ^~~~~~~~\r\ncompilation terminated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n\r\nI can't determine why ares.h cannot be found. The generated .tf_configure.bazelrc contains the line:\r\n```\r\nbuild --define grpc_no_ares=true\r\n```\r\n", "comments": ["Apparently this was a bug in the Bazel CI system. My apologies.", "How did you solved it?", "The problem I had was a flake in our CI system. If you are seeing something similar I am afraid I don't have any advice.", "I had same issue, solved it by:\n\napt-get install libc-ares-dev\n\n", "same problem. does \"apt-get install libc-ares-dev\" work for everybody? but it did not work for me, how can i slove it? ", "@NorwayLobster whats the error message thats being thrown ?\r\n", "sorry, it has been a too long time, honestly, I forgot. @psn9 ", "I had this problem here, fixed it by installing `libc-ares-dev`, then ran into #23402 which was fixable with https://github.com/tensorflow/tensorflow/issues/23402#issuecomment-436932197. Thanks to everybody involved for fixing things when they break!", "i use mac,when i use commad brew install libc-ares-dev,the error message is Error: No available formula with the name \"libc-ares-dev\",how to solve it ?3q", "> i use mac,when i use commad brew install libc-ares-dev,the error message is Error: No available formula with the name \"libc-ares-dev\",how to solve it ?3q\r\n\r\nbrew install c-ares\r\n", "@jiakai0419 \r\n> brew install c-ares\r\n\r\nThis had no effect.  Error persists on Mac. \r\n\r\n\"Warning: c-ares 1.15.0 is already installed and up-to-date\"\r\n\r\n$ brew install c-ares\r\nUpdating Homebrew...\r\nWarning: c-ares 1.15.0 is already installed and up-to-date\r\nTo reinstall 1.15.0, run `brew reinstall c-ares`\r\n\r\n$ brew install libc-ares-dev\r\nError: No available formula with the name \"libc-ares-dev\" \r\n==> Searching for a previously deleted formula (in the last month)...\r\nError: No previously deleted formula found.\r\n==> Searching for similarly named formulae...\r\nError: No similarly named formulae found.\r\n==> Searching taps...\r\n==> Searching taps on GitHub...\r\nError: No formulae found in taps.\r\n\r\nEDIT: What did resolve the error was to include the flag `--define=grpc_no_ares=true` in the bazel command line. "]}, {"number": 19463, "title": "AttributeError: 'NoneType' object has no attribute 'rfind'", "body": " **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n-**TensorFlow installed from (source)**:\r\n**tensorflow v1.6.0-0-gd2e24b6039 1.6.0**:\r\n**Python 3.5**: \r\n\r\n**Reproduce**\r\ngit clone tensorflow \r\npython3 tensorflow/examples/speech_commands/train.py\r\npython3 tensorflow/examples/speech_commands/freeze.py \\\r\n--start_checkpoint=/tmp/speech_commands_train/conv.ckpt-18000 \\\r\n--output_file=/tmp/my_frozen_graph.pb\r\n\r\n**Error appears:**\r\n/home/lukas/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n2018-05-22 21:59:00.562103: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\r\nConverted 6 variables to const ops.\r\nTraceback (most recent call last):\r\n  File \"/home/lukas/Desktop/tensorflow-master/tensorflow/examples/speech_commands/freeze.py\", line 180, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/lukas/.local/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/home/lukas/Desktop/tensorflow-master/tensorflow/examples/speech_commands/freeze.py\", line 124, in main\r\n    os.path.dirname(FLAGS.output_file),\r\n  File \"/usr/lib/python3.5/posixpath.py\", line 148, in dirname\r\n    i = p.rfind(sep) + 1\r\nAttributeError: 'NoneType' object has no attribute 'rfind'\r\n\r\n**Thanks** ", "comments": []}, {"number": 19462, "title": "Fix discrepancies between doc and implementation for math_ops", "body": "In the docstring of math_ops, there are some discrepancies between the supported data types and the actual implementation.\r\n\r\nSpecifically, `abs` `pow`, and `round` supports `float16` now but was not mentioned in the docstring.\r\n\r\nThis fix updates the docstring to address the discrepancies.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 19461, "title": "Add Apache Parquet support for TensorFlow Dataset", "body": "Apache Parquet is a widely used columnar storage format available in the Hadoop ecosystem. \r\n\r\nThis PR is a preliminary attempt to add Apache Parquet support for TensorFlow's Dataset API. It should help many to working on existing parquet-formatted big data with TesnorFlow.\r\n\r\nThe PR may not cover all the use cases though it could be served as a starting point for further improvement in the future.\r\n\r\nThe ParquetDataset depends on parquet-cpp (Apache) project as well as other dependencies (e.g, Thrift, etc.). The ParquetDataset only builds on Linux at the moment. This PR also adds the option in `./configure` so that those dependencies could be skipped.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Seeing compilation errors...\r\n\r\nIn file included from external/arrow/cpp/src/arrow/buffer.h:28:\r\nexternal/arrow/cpp/src/arrow/memory_pool.h:24:10: fatal error: 'arrow/util/visibility.h' file not found\r\n#include \"arrow/util/visibility.h\"", "@case540 The PR has been updated and build failures have been fixed. Please take a look.", "I'm terribly sorry for the delay on reviewing this PR. @skye has more experience than I do with the Parquet APIs kindly offered to take over the review. I'll be happy to answer any questions that arise about the `tf.data` integration.", "Thanks @skye for the review. The comment has been addressed. The current test is failing but that is due to a Bazel bug. I opened the issue in bazel:\r\nhttps://github.com/bazelbuild/bazel/issues/5932\r\nThe bug has been fixed so it will be in the next bazel release.\r\n\r\nCan you take a look and see if the PR is OK? Once bazel is updated then all tests should pass I believe.", "Bazel 0.17.1 has been released (https://github.com/bazelbuild/bazel/issues/5059#issuecomment-421332163) and it fixed the issue we are facing here. The tests in TF still use Bazel 0.15.0.\r\n\r\nA separate PR #22281 has been created to update Bazel to 0.17.1.\r\n\r\n/cc @gunan ", "This PR still depends on #22449 (to update bazel to 0.17.1). Will update the PR once #22449 is fixed.", "Nagging Assignee @case540: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'm removing myself as reviewer because I assume that we'll integrate this via SIG IO (where hopefully the same build issues won't be a problem!)...", "This PR has been migrated to  tensorflow/io#21 with all builds pass. I will close this PR once  tensorflow/io#21 is merged.", "Since tensorflow/io#21 has been merged. I will close this PR. Thanks everyone for the help!"]}, {"number": 19460, "title": "Mistake in tensorflow API doc", "body": "As you can see in the [documentation](https://www.tensorflow.org/api_docs/python/tf/nn/softmax), the 'axis' attribute is not mandatory.\r\nBut, its default value is \"None\" in the function call example, but is '-1' in the description of the 'axis' attribute.\r\n\r\n\r\n", "comments": ["If None is passed, the function will replace it by -1. So I think the document is corrrct. ", "Yup, as @facaiy mentioned, [`None` is treated as -1](https://github.com/tensorflow/tensorflow/blob/3b85959a57ef2656cdcf2cfac62d68713948ba54/tensorflow/python/ops/nn_ops.py#L1736)"]}, {"number": 19459, "title": "Update get_started_for_beginners.md", "body": "Older versions used \"_feature\" and \"_label\" instead of \"_x\" and \"_y\" as in the newest versions of the get_started model in the code base.  This documentation was inconsistent, using the former for loading the training and test data, but the latter when calling eval().  This commit makes the notation consistent.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I have now signed the CLA! :)", "CLAs look good, thanks!\n\n<!-- ok -->", "Closing due to above comment. "]}, {"number": 19458, "title": "I am trying to deploy my tensorflow model(CNN) to android, when I am trying to use my .pb file( generated after training the model )in android studio it shows! ", "body": "05-22 16:23:02.608 27021-27021/com.example.bibhu.smscnn E/AndroidRuntime: FATAL EXCEPTION: main\r\nProcess: com.example.bibhu.smscnn, PID: 27021\r\njava.lang.RuntimeException: Unable to start activity ComponentInfo{com.example.bibhu.smscnn/com.example.bibhu.smscnn.MainActivity}: java.lang.RuntimeException: Failed to load model from 'file:///android_asset/graph11.pb'\r\nat android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2820)\r\nat android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2895)\r\nat android.app.ActivityThread.-wrap11(Unknown Source:0)\r\nat android.app.ActivityThread$H.handleMessage(ActivityThread.java:1596)\r\nat android.os.Handler.dispatchMessage(Handler.java:105)\r\nat android.os.Looper.loop(Looper.java:164)\r\nat android.app.ActivityThread.main(ActivityThread.java:6565)\r\nat java.lang.reflect.Method.invoke(Native Method)\r\nat com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240)\r\nat com.android.internal.os.ZygoteInit.main(ZygoteInit.java:767)\r\nCaused by: java.lang.RuntimeException: Failed to load model from 'file:///android_asset/graph11.pb'\r\nat org.tensorflow.contrib.android.TensorFlowInferenceInterface.(TensorFlowInferenceInterface.java:113)\r\nat com.example.bibhu.smscnn.TensorFlowClassifier.create(TensorFlowClassifier.java:70)\r\nat com.example.bibhu.smscnn.TensorflowIntegrationExample.loadModel(TensorflowIntegrationExample.java:35)\r\nat com.example.bibhu.smscnn.TensorflowIntegrationExample.(TensorflowIntegrationExample.java:27)\r\nat com.example.bibhu.smscnn.MainActivity.onCreate(MainActivity.java:21)\r\nat android.app.Activity.performCreate(Activity.java:6975)\r\nat android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1214)\r\nat android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2773)\r\nat android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2895) \r\nat android.app.ActivityThread.-wrap11(Unknown Source:0) \r\nat android.app.ActivityThread$H.handleMessage(ActivityThread.java:1596) \r\nat android.os.Handler.dispatchMessage(Handler.java:105) \r\nat android.os.Looper.loop(Looper.java:164) \r\nat android.app.ActivityThread.main(ActivityThread.java:6565) \r\nat java.lang.reflect.Method.invoke(Native Method) \r\nat com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240) \r\nat com.android.internal.os.ZygoteInit.main(ZygoteInit.java:767) \r\nCaused by: java.io.IOException: Not a valid TensorFlow Graph serialization: Input 1 of node embedding/embedding_lookup was passed float from inputTensor:0 incompatible with expected int32.\r\nat org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph(TensorFlowInferenceInterface.java:561)\r\nat org.tensorflow.contrib.android.TensorFlowInferenceInterface.(TensorFlowInferenceInterface.java:105)\r\nat com.example.bibhu.smscnn.TensorFlowClassifier.create(TensorFlowClassifier.java:70) \r\nat com.example.bibhu.smscnn.TensorflowIntegrationExample.loadModel(TensorflowIntegrationExample.java:35) \r\nat com.example.bibhu.smscnn.TensorflowIntegrationExample.(TensorflowIntegrationExample.java:27) \r\nat com.example.bibhu.smscnn.MainActivity.onCreate(MainActivity.java:21) \r\nat android.app.Activity.performCreate(Activity.java:6975) \r\nat android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1214) \r\nat android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2773) \r\nat android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2895) \r\nat android.app.ActivityThread.-wrap11(Unknown Source:0) \r\nat android.app.ActivityThread$H.handleMessage(ActivityThread.java:1596) \r\nat android.os.Handler.dispatchMessage(Handler.java:105) \r\nat android.os.Looper.loop(Looper.java:164) \r\nat android.app.ActivityThread.main(ActivityThread.java:6565) \r\nat java.lang.reflect.Method.invoke(Native Method) \r\nat com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240) \r\nat com.android.internal.os.ZygoteInit.main(ZygoteInit.java:767) ", "comments": ["How did you solve this issue?\r\nI am facing the same issue. Please help!", "Hi,  @Akshathar8, Read this line   Caused by: java.io.IOException: Not a valid TensorFlow Graph serialization: Input 1 of node embedding/embedding_lookup was passed float from inputTensor:0 incompatible with expected int32."]}, {"number": 19456, "title": "Why won't there be higher level APIs on TensorFlow that support AGI?", "body": "Needed higher level APIs that support AGI on top of TensorFlow...", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "I suspect you meant to file an issue/ feature request. Closing the PR."]}, {"number": 19455, "title": "CuDNN error while fitting CNN ", "body": "OS: Windows 10 64bit\r\nTensorFlow GPU: 1.5.0 (installed with pip)\r\nPython Version: 3.6.4\r\nIDE : Spyder 3.2.6\r\nCUDA: v9.0\r\nCuDNN: v7.0.5 for CUDA 9.0\r\nGPU: GeForce GTX 960M 4GB\r\nNVIDIA drivers: 397.64\r\n\r\n\r\nSo, I was trying to run some CNN (and also CapsNet where the first layer is just a convolution layer). And it keeps crushing with the error below. From what I found on the internet I decided to reinstall cuda and cudnn. And the code with CNN worked. Though capsnet still didn't. I googled more and found out that cuda reset your drivers after installation, so, I needed to update my drivers. I had something like 380 or 384. Don't remember exactly. I installed 397. And now they both don't work again. I'm desperate. I don't know what to do anymore.\r\nThe CNN code works fine on CPU though.\r\n\r\nError given by CNN code:\r\n\r\n` 2018\udae1\udea9\udae1\udeba 15:50:40.136802: I C:\\tf_jenkins\\workspace\\rel\u2011win\\M\\windows\u2011gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2018\udae1\udea9\udae1\udeba 15:50:40.925004: I C:\\tf_jenkins\\workspace\\rel\u2011win\\M\\windows\u2011gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1105] Found device 0 with properties: \r\nname: GeForce GTX 960M major: 5 minor: 0 memoryClockRate(GHz): 1.176\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 4.00GiB freeMemory: 3.34GiB\r\n2018\udae1\udea9\udae1\udeba 15:50:40.930269: I C:\\tf_jenkins\\workspace\\rel\u2011win\\M\\windows\u2011gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) \u2011> (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n2018\udae1\udea9\udae1\udeba 15:50:47.834229: E C:\\tf_jenkins\\workspace\\rel\u2011win\\M\\windows\u2011gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:444] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2018\udae1\udea9\udae1\udeba 15:50:47.866263: E C:\\tf_jenkins\\workspace\\rel\u2011win\\M\\windows\u2011gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:444] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2018\udae1\udea9\udae1\udeba 15:50:47.905931: E C:\\tf_jenkins\\workspace\\rel\u2011win\\M\\windows\u2011gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:444] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2018\udae1\udea9\udae1\udeba 15:50:47.907669: E C:\\tf_jenkins\\workspace\\rel\u2011win\\M\\windows\u2011gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:444] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2018\udae1\udea9\udae1\udeba 15:50:47.909351: E C:\\tf_jenkins\\workspace\\rel\u2011win\\M\\windows\u2011gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:444] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2018\udae1\udea9\udae1\udeba 15:50:47.911059: E C:\\tf_jenkins\\workspace\\rel\u2011win\\M\\windows\u2011gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:444] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2018\udae1\udea9\udae1\udeba 15:50:48.585303: E C:\\tf_jenkins\\workspace\\rel\u2011win\\M\\windows\u2011gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2018\udae1\udea9\udae1\udeba 15:50:48.586924: E C:\\tf_jenkins\\workspace\\rel\u2011win\\M\\windows\u2011gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n2018\udae1\udea9\udae1\udeba 15:50:48.588290: F C:\\tf_jenkins\\workspace\\rel\u2011win\\M\\windows\u2011gpu\\PY\\36\\tensorflow\\core\\kernels\\conv_ops.cc:717] Check failed: stream\u2011>parent()\u2011>GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo(), &algorithms) \r\n2018\udae1\udea9\udae1\udeba 15:53:45.749902: I C:\\tf_jenkins\\workspace\\rel\u2011win\\M\\windows\u2011gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2018\udae1\udea9\udae1\udeba 15:53:46.246273: I C:\\tf_jenkins\\workspace\\rel\u2011win\\M\\windows\u2011gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1105] Found device 0 with properties: \r\nname: GeForce GTX 960M major: 5 minor: 0 memoryClockRate(GHz): 1.176\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 4.00GiB freeMemory: 3.34GiB\r\n2018\udae1\udea9\udae1\udeba 15:53:46.248497: I C:\\tf_jenkins\\workspace\\rel\u2011win\\M\\windows\u2011gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) \u2011> (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n2018\udae1\udea9\udae1\udeba 15:53:49.372376: E C:\\tf_jenkins\\workspace\\rel\u2011win\\M\\windows\u2011gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:444] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2018\udae1\udea9\udae1\udeba 15:53:49.373772: E C:\\tf_jenkins\\workspace\\rel\u2011win\\M\\windows\u2011gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:444] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2018\udae1\udea9\udae1\udeba 15:53:49.377767: E C:\\tf_jenkins\\workspace\\rel\u2011win\\M\\windows\u2011gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:444] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2018\udae1\udea9\udae1\udeba 15:53:49.379147: E C:\\tf_jenkins\\workspace\\rel\u2011win\\M\\windows\u2011gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:444] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2018\udae1\udea9\udae1\udeba 15:53:49.380568: E C:\\tf_jenkins\\workspace\\rel\u2011win\\M\\windows\u2011gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:444] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2018\udae1\udea9\udae1\udeba 15:53:49.382145: E C:\\tf_jenkins\\workspace\\rel\u2011win\\M\\windows\u2011gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:444] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2018\udae1\udea9\udae1\udeba 15:53:49.646183: E C:\\tf_jenkins\\workspace\\rel\u2011win\\M\\windows\u2011gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2018\udae1\udea9\udae1\udeba 15:53:49.647436: E C:\\tf_jenkins\\workspace\\rel\u2011win\\M\\windows\u2011gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n2018\udae1\udea9\udae1\udeba 15:53:49.648615: F C:\\tf_jenkins\\workspace\\rel\u2011win\\M\\windows\u2011gpu\\PY\\36\\tensorflow\\core\\kernels\\conv_ops.cc:717] Check failed: stream\u2011>parent()\u2011>GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo(), &algorithms) `\r\n\r\n\r\nPart of the error given by capsnet:\r\n\r\n` 2018\udae1\udea9\udae1\udeba 15:49:02.623151: W C:\\tf_jenkins\\workspace\\rel\u2011win\\M\\windows\u2011gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:277] *****************************************___________________________________________________________\r\n2018\udae1\udea9\udae1\udeba 15:49:02.624700: W C:\\tf_jenkins\\workspace\\rel\u2011win\\M\\windows\u2011gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1198] Resource exhausted: OOM when allocating tensor with shape[24,6,6,3,3,32,32,4,4,4] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n2018\udae1\udea9\udae1\udeba 16:22:12.617066: I C:\\tf_jenkins\\workspace\\rel\u2011win\\M\\windows\u2011gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2018\udae1\udea9\udae1\udeba 16:22:13.091828: I C:\\tf_jenkins\\workspace\\rel\u2011win\\M\\windows\u2011gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1105] Found device 0 with properties: \r\nname: GeForce GTX 960M major: 5 minor: 0 memoryClockRate(GHz): 1.176\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 4.00GiB freeMemory: 3.34GiB\r\n2018\udae1\udea9\udae1\udeba 16:22:13.094135: I C:\\tf_jenkins\\workspace\\rel\u2011win\\M\\windows\u2011gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) \u2011> (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n2018\udae1\udea9\udae1\udeba 16:23:36.501624: E C:\\tf_jenkins\\workspace\\rel\u2011win\\M\\windows\u2011gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2018\udae1\udea9\udae1\udeba 16:23:36.504510: E C:\\tf_jenkins\\workspace\\rel\u2011win\\M\\windows\u2011gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n2018\udae1\udea9\udae1\udeba 16:23:36.507233: F C:\\tf_jenkins\\workspace\\rel\u2011win\\M\\windows\u2011gpu\\PY\\36\\tensorflow\\core\\kernels\\conv_ops.cc:717] Check failed: stream\u2011>parent()\u2011>GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo(), &algorithms) `\r\n\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hi. Can you add the following to the start of your script and see if that has any effect?\r\n`os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'`", "Adding `os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'` didn't help anyhow.\r\n\r\nOS Platform and Distribution: Windows 10 64bit\r\nTensorFlow installed binary from https://pypi.org/project/tensorflow-gpu/#files\r\nTensorFlow version: 1.8.0  (I updated it and tried again. Still having a problem)\r\nPython Version: 3.6.5\r\nBazel version N/A\r\nIDE : Spyder 3.2.8\r\nCUDA version: v9.0\r\nCuDNN version: v7.0.5 for CUDA 9.0\r\nGPU model and memory: NVIDIA GeForce GTX 960M 4GB\r\nRAM: 16GB\r\nNVIDIA drivers: 398.11\r\n\r\nBasically the error appears when I am trying to fit the CNN into the dataset (sometimes it fits fine in different codes with CNN. Idk why. But in 95% cases it doesn't.).\r\nThere are many different codes I am trying to run which are failing. This one is the shortest:\r\n`\r\n\"\"\"Importing libriaries\"\"\"\r\nfrom future import print_function\r\nimport keras\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout, Flatten\r\nfrom keras.layers import Conv2D, MaxPooling2D\r\nimport matplotlib.pyplot as plt\r\nimport os\r\n\r\nfrom sklearn.datasets import fetch_olivetti_faces\r\nfrom sklearn.cross_validation import train_test_split\r\n\r\nos.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'\r\n\r\nbatch_size = 128\r\nnum_classes = 40\r\nepochs = 250\r\n\r\n\"\"\" input image dimensions\"\"\"\r\nimg_rows, img_cols = 64, 64\r\n\r\n\"\"\"Importing Olivetti dataset and splitting it into train and test sets.\"\"\"\r\ndata = fetch_olivetti_faces(shuffle=True)\r\nX = data.images\r\ny = data.target\r\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\r\n\r\nx_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\nx_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\ninput_shape = (img_rows, img_cols, 1)\r\n\r\n\"\"\"Reshaping our data, so it would be possible to fit a CNN to it\"\"\"\r\nprint('x_train shape:', x_train.shape)\r\nprint(x_train.shape[0], 'train samples')\r\nprint(x_test.shape[0], 'test samples')\r\n\r\n\"\"\"Converting class vectors to binary class matrices\"\"\"\r\ny_train = keras.utils.to_categorical(y_train, num_classes)\r\ny_test = keras.utils.to_categorical(y_test, num_classes)\r\n\r\n\"\"\"Network\"\"\"\r\nmodel = Sequential()\r\nmodel.add(Conv2D(32, kernel_size=(3, 3),\r\nactivation='relu',\r\ninput_shape=input_shape))\r\nmodel.add(Conv2D(32, (3, 3), activation='relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Dropout(0.25))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(128, activation='relu'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(num_classes, activation='softmax'))\r\n\r\nmodel.compile(loss=keras.losses.categorical_crossentropy,\r\noptimizer=keras.optimizers.Adadelta(),\r\nmetrics=['accuracy'])\r\n\r\n\"\"\"Fitting CNN to our data and getting results\"\"\"\r\nhistory = model.fit(x_train, y_train,\r\nbatch_size=batch_size,\r\nepochs=epochs,\r\nverbose=1,\r\nvalidation_data=(x_test, y_test))\r\nscore = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\r\n`\r\n\r\n", "This is a stale issue. I think this was resolved in recent TF versions.\r\n\r\nAlso, note that support for `TF1.x` is not there anymore. \r\n\r\nI am closing this issue as this was resolved in recent TF versions. Please feel free to test with TF2.x and open a new issue If the issue persists with recent TF versions. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/19455\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/19455\">No</a>\n"]}, {"number": 19454, "title": "R1.8", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 19453, "title": "Update Release.md to remove duplicate release", "body": "The beginning of the release note for 1.4.0 seems to have been duplicated,\r\nremove the duplication.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Nagging Assignee @case540: It has been 19 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @case540: It has been 34 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This looks like it was fixed already? Reopen if that is not the case. Thanks for the PR and sorry for taking so long to look at it."]}, {"number": 19452, "title": "tensorflow-lite batch_to_space_nd when will support crops attribute?", "body": "commit id : 52e2698 \r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8.0 gpu\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:0.12.0\r\n- **GCC/Compiler version (if compiling from source)**:N/A\r\n- **CUDA/cuDNN version**: 7.5.18\r\n- **GPU model and memory**:TITAN,12GB\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nI saw the \"tf.batch_to_space_nd - as long as the input tensor is 4D (1 batch + 2 spatial + 1 other) and the crops attribute is not used\" in TensorFlow Lite & TensorFlow Compatibility Guide page.\r\nI want to convert customed model to tensorflow-lite format which has crops attribute in batch_to_space_nd op. I wandor when will batch_to_space_nd support crops attribute?\r\nThank you very much.\r\n\r\n\r\n\r\n", "comments": ["It looks like support was very recently added -- https://github.com/tensorflow/tensorflow/commit/874cf8e1d332175c8a90d7512f8385e98e2a7377 . Have you tried with your model yet?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "Yes, this should be supported try in the latest nighty, and if it doens't work please reopen this issue!\r\n"]}, {"number": 19451, "title": "Feature Request: evaluate both train_data and test_data using tf.contrib.learn.Experiment?", "body": "### Describe the problem\r\nfeature request\uff1a\r\ni'm using the api \"tf.contrib.learn.Experiment\" for distributed training, but i don't know the method for evaluating the train_data and test_data at the same step, i want to check the model is overfit or not, so i want to print both train_auc and test_auc after saving model each time.\r\n\r\nnow, i can only see the performance on the test data for every saved model. is there any methods to print both train and test evaluation? Thanks a lot!\r\n\r\n### Source code / logs\r\nhere is  code:\r\n`def train(self):\r\n        if self.job_name == \"worker\":\r\n            # get data\r\n            self.init_worker_input_split()\r\n            self.init_worker_test_input_split()\r\n\r\n        # data input function\r\n        train_input = lambda: dataset_v1.create_input_fun(self.file_names,\r\n                                                          FLAGS.batch_size,\r\n                                                          repeat_count=FLAGS.train_epochs)\r\n        test_input = lambda: dataset_v1.create_input_fun(self.test_file_names,\r\n                                                         FLAGS.batch_size,\r\n                                                         perform_shuffle=False)\r\n        # estimator define\r\n        model = self.build_estimator(FLAGS.log_dir, FLAGS.model_type)\r\n\r\n        distribute_exp = tf.contrib.learn.Experiment(estimator=model,\r\n                                                     train_input_fn=train_input,\r\n                                                     eval_input_fn=test_input,\r\n                                                     min_eval_frequency=100\r\n                                                     )\r\n\r\n        if self.job_name == \"ps\":\r\n            # Starts a TensorFlow server and joins the serving thread\r\n            distribute_exp.run_std_server()\r\n        elif self.job_name == \"worker\":\r\n            distribute_exp.train_and_evaluate()\r\n\r\n        if self.is_chief:\r\n            # Evaluate on the evaluation data.\r\n            distribute_exp.evaluate(name='test_performance')\r\n\r\n        tf.logging.info(\"Optimization Finished!\")`\r\n\r\nhere is the logs:\r\nINFO:tensorflow:Validation (step 7014908): loss = 15.8598, accuracy_baseline = 0.925938, global_step = 7014348, auc = 0.683994, prediction/mean = 0.0714387, label/mean = 0.0740625, average_loss = 0.24781, auc_precision_recall = 0.167725, accuracy = 0.927031\r\nINFO:tensorflow:global_step/sec: 98.1175\r\nINFO:tensorflow:global_step/sec: 124.708\r\nINFO:tensorflow:global_step/sec: 121.122\r\nINFO:tensorflow:loss = 11.3313, step = 7016362 (21.941 sec)\r\nINFO:tensorflow:global_step/sec: 122.56\r\nINFO:tensorflow:global_step/sec: 114.473\r\nINFO:tensorflow:global_step/sec: 122.207\r\nINFO:tensorflow:global_step/sec: 123.767\r\nINFO:tensorflow:global_step/sec: 125.336\r\nINFO:tensorflow:loss = 14.5477, step = 7016886 (4.307 sec)\r\nINFO:tensorflow:global_step/sec: 120.105\r\nINFO:tensorflow:global_step/sec: 110.698\r\nINFO:tensorflow:global_step/sec: 124.799\r\nINFO:tensorflow:global_step/sec: 121.776\r\nINFO:tensorflow:global_step/sec: 114.484\r\nINFO:tensorflow:loss = 16.5542, step = 7017438 (4.837 sec)\r\nINFO:tensorflow:global_step/sec: 125.595\r\nINFO:tensorflow:global_step/sec: 122.375\r\nINFO:tensorflow:global_step/sec: 122.392\r\nINFO:tensorflow:global_step/sec: 124.201\r\nINFO:tensorflow:global_step/sec: 122.038\r\nINFO:tensorflow:loss = 17.7121, step = 7017930 (3.817 sec)\r\nINFO:tensorflow:global_step/sec: 102.7\r\nINFO:tensorflow:global_step/sec: 121.585\r\nINFO:tensorflow:global_step/sec: 119.404\r\nINFO:tensorflow:global_step/sec: 121.501\r\nINFO:tensorflow:global_step/sec: 119.815\r\nINFO:tensorflow:loss = 16.0022, step = 7018577 (5.630 sec)\r\nINFO:tensorflow:global_step/sec: 118.615\r\nINFO:tensorflow:global_step/sec: 122.272\r\nINFO:tensorflow:global_step/sec: 121.206\r\nINFO:tensorflow:global_step/sec: 123.087\r\nINFO:tensorflow:global_step/sec: 120.079\r\nINFO:tensorflow:loss = 20.2255, step = 7019070 (4.049 sec)\r\nINFO:tensorflow:global_step/sec: 118.552\r\nINFO:tensorflow:global_step/sec: 121.404\r\nINFO:tensorflow:Saving checkpoints for 7019347 into viewfs://hadoop-meituan/user/hadoop-generalshop/caiqi.sun/spark/dl/demo/output/wide_deep_all_82/model.ckpt.\r\nINFO:tensorflow:global_step/sec: 102.859\r\nINFO:tensorflow:Starting evaluation at 2018-05-22-03:56:24\r\nINFO:tensorflow:Restoring parameters from viewfs://hadoop-meituan/user/hadoop-generalshop/caiqi.sun/spark/dl/demo/output/wide_deep_all_82/model.ckpt-7019347\r\nINFO:tensorflow:Evaluation [10/100]\r\nINFO:tensorflow:Evaluation [20/100]\r\nINFO:tensorflow:Evaluation [30/100]\r\nINFO:tensorflow:Evaluation [40/100]\r\nINFO:tensorflow:Evaluation [50/100]\r\nINFO:tensorflow:Evaluation [60/100]\r\nINFO:tensorflow:Evaluation [70/100]\r\nINFO:tensorflow:Evaluation [80/100]\r\nINFO:tensorflow:Evaluation [90/100]\r\nINFO:tensorflow:Evaluation [100/100]\r\nINFO:tensorflow:Finished evaluation at 2018-05-22-03:56:31\r\nINFO:tensorflow:Saving dict for global step 7019354: accuracy = 0.927031, accuracy_baseline = 0.925938, auc = 0.683203, auc_precision_recall = 0.170481, average_loss = 0.247913, global_step = 7019354, label/mean = 0.0740625, loss = 15.8664, prediction/mean = 0.0705214\r\nINFO:tensorflow:Validation (step 7019922): loss = 15.8664, accuracy_baseline = 0.925938, global_step = 7019354, auc = 0.683203, prediction/mean = 0.0705214, label/mean = 0.0740625, average_loss = 0.247913, auc_precision_recall = 0.170481, accuracy = 0.927031\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 19 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 34 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}]