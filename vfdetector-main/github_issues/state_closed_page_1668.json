[{"number": 2865, "title": "Docker GPU CUDA failure fails to be caught by CI", "body": "Two issues:\n1. It looks like the recent docker images, including `r0.9rc0-devel-gpu` and `nightly-devel-gpu` (as of June 14) are failing to load libcuda. The `latest-devel-gpu` seems to work fine.\n2. The CI tests seem to be missing these failures and reporting success.\n### Environment info\n\n**Host:**\n  Ubuntu 15.10\n  GTX 980, driver version 352.63\n  docker 1.11.2 build b9f10c9\n  using nvidia-docker 1.0.0.rc.2-1_amd64\n\n**Container images** (a la https://hub.docker.com/r/tensorflow/tensorflow/tags/):\n  `r0.9rc0-devel-gpu` (bff7093a7715, built ~June 6)\n  `nightly-devel-gpu` (d285481a3e65, built ~June 14)\n  `latest-devel-gpu` (9e12b89c50bb, built ~two months ago)\n### Steps to reproduce\n\n(and installed CUDA versions)\n\nHere's where the fun starts. I'll do this via the docker commands I ran, per container.\n\nFor container `r0.9rc0-devel-gpu`:\n\n```\n$ nvidia-docker run -it tensorflow/tensorflow:r0.9rc0-devel-gpu bash -c echo LD_LIBRARY_PATH: $LD_LIBRARY_PATH\nLD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:\n\n$ nvidia-docker run -it tensorflow/tensorflow:r0.9rc0-devel-gpu bash -c ls -l /usr/local/nvidia/lib64/libcuda*\nlrwxrwxrwx 1  999  999       41 Jun 13 21:38 /usr/local/nvidia/lib64/libcuda.so.1 -> /usr/local/nvidia/lib64/libcuda.so.352.63\n-rw-r--r-- 2 root root 14283432 Nov  8  2015 /usr/local/nvidia/lib64/libcuda.so.352.63\n\n$ nvidia-docker run -it tensorflow/tensorflow:r0.9rc0-devel-gpu bash -c python -c \"import tensorflow; print tensorflow.__version__\"\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:102] Couldn't open CUDA library libcuda.so. LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:160] hostname: 03e202e5d433\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:185] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:347] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  352.63  Sat Nov  7 21:25:42 PST 2015\nGCC version:  gcc version 4.9.3 (Ubuntu 4.9.3-5ubuntu1) \n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] kernel reported version is: 352.63.0\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1076] LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1077] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so; dlerror: libcuda.so: cannot open shared object file: No such file or directory\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n0.9.0rc0\n```\n\nFor container `nightly-devel-gpu`:\n\n```\n$ nvidia-docker run -it tensorflow/tensorflow:nightly-devel-gpu bash -c echo LD_LIBRARY_PATH: $LD_LIBRARY_PATH\nLD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:\n\n$ nvidia-docker run -it tensorflow/tensorflow:nightly-devel-gpu bash -c ls -l /usr/local/nvidia/lib64/libcuda*\nlrwxrwxrwx 1  999  999       41 Jun 13 21:38 /usr/local/nvidia/lib64/libcuda.so.1 -> /usr/local/nvidia/lib64/libcuda.so.352.63\n-rw-r--r-- 2 root root 14283432 Nov  8  2015 /usr/local/nvidia/lib64/libcuda.so.352.63\n\n$ nvidia-docker run -it tensorflow/tensorflow:nightly-devel-gpu bash -c python -c \"import tensorflow; print tensorflow.__version__\"\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:102] Couldn't open CUDA library libcuda.so. LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:160] hostname: 1a40427098ed\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:185] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:347] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  352.63  Sat Nov  7 21:25:42 PST 2015\nGCC version:  gcc version 4.9.3 (Ubuntu 4.9.3-5ubuntu1) \n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] kernel reported version is: 352.63.0\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1077] LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1078] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so; dlerror: libcuda.so: cannot open shared object file: No such file or directory\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n0.8.0\n```\n\nFor container `latest-devel-gpu`:\n\n```\n$ nvidia-docker run -it tensorflow/tensorflow:latest-devel-gpu bash -c echo LD_LIBRARY_PATH: $LD_LIBRARY_PATH\nLD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:\n\n$ nvidia-docker run -it tensorflow/tensorflow:latest-devel-gpu bash -c ls -l /usr/local/nvidia/lib64/libcuda*\nlrwxrwxrwx 1  999  999       41 Jun 13 21:38 /usr/local/nvidia/lib64/libcuda.so.1 -> /usr/local/nvidia/lib64/libcuda.so.352.63\n-rw-r--r-- 2 root root 14283432 Nov  8  2015 /usr/local/nvidia/lib64/libcuda.so.352.63\n\n$ nvidia-docker run -it tensorflow/tensorflow:latest-devel-gpu bash -c python -c \"import tensorflow; print tensorflow.__version__\"\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n0.8.0\n```\n\nSo it looks like the GPU libraries are failing on the first two docker images. What's interesting to note here is also that all three python statements still execute without error. I.E., running `print tensorflow.__version__` completes (and prints the correct version) and python exits with return code 0, even though the GPU failed to load properly. This is not a bug, since the fallback behavior of running on the CPU should still work. However, it also seems this is causing problems for the CI tests, since it isn't detecting a failure:\n\nIt seemed odd to me that these docker images weren't working, since you use them for CI. So I checked the CI logs, and _they're the same_. For instance, for the docker CI build for the nightly GPU devel image, this is a snippet from the log showing the output from the seventh test:\n\n(full source is http://ci.tensorflow.org/view/Nightly/job/nightly-docker-gpu/TF_DOCKER_BUILD_IS_DEVEL=YES,TF_DOCKER_BUILD_TYPE=GPU,label=gpu-linux/lastBuild/consoleFull)\n\n```\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:102] Couldn't open CUDA library libcuda.so. LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:160] hostname: 883e6f728cb9\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:185] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:347] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  352.79  Wed Jan 13 16:17:53 PST 2016\nGCC version:  gcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04.1) \n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] kernel reported version is: 352.79.0\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1077] LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1078] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so; dlerror: libcuda.so: cannot open shared object file: No such file or directory\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nE tensorflow/stream_executor/cuda/cuda_driver.cc:491] failed call to cuInit: CUDA_ERROR_NO_DEVICE\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:147] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:81] No GPU devices available on machine.\n(7 / 7) tutorial test-on-install PASSED: translate_test  (Elapsed time: 8997 ms)\ntutorial test-on-install test(s):  7 passed; 0 failed; 0 skipped\n```\n\nI'm still learning Docker, so it's entirely possible that I've made some blunder here, but it does seem like there's something going on here between the docker images.\n", "comments": ["See https://github.com/tensorflow/tensorflow/issues/2525#issuecomment-223172883\n", "Ahh, it seems I didn't read that issue carefully enough. I had seen the resolution on cudnn, but the libcuda conclusion was different.\n\n> Using the constant is a good idea, hopefully CUDNN_MAJOR is akin to the soname and will only change if the ABI breaks (I will warn the cuDNN team about that). For CUDA, I think it is safe to just hardcode \"1\" because libcuda has always provided backward compatibilty.\n\nI'll submit a pull request to do just that.\n\n#808 also had this issue, but the conclusion was to use nvidia-docker (which is not the solution here, since I'm already using it). I should note that adding a `RUN ln -s /usr/local/nvidia/lib64/libcuda.so.1 /usr/lib/x86_64-linux-gnu/libcuda.so` to the Dockerfile as suggested in that thread _does_ work and is another solution, but specifying the correct soname version in `dso_loader.cc` solves both and seems to be cleaner. If NVidia ever does break ABI compatibility on libcuda, then we should have a `GetLibcudaVesionNumber()` function in `dso_loader.cc` anyways. (And if someone wants to add that during the process of solving #2873, then all the better.)\n\nThe CI tests should really be detecting this, though, and I'm unsure where to start on fixing that.\n", "@caisq @jendap @gunan Any idea why we are missing this in CI? \n\nI'm reopening this and changing title appropriately. (I merged the fixing PR, thanks @rdadolf!)\n", "Thanks, @martinwicke.\n\nAlso, I alluded to this in the original post, but I think the CI issue boils down to the tests checking for correctness on the output of the test, whereas in this case, we actually care that the output was correct _and_ that the output was generated by code on the GPU. Since TF fails gracefully back to the CPU when the cuda libraries aren't found, the tests run along happily (and no non-zero return code is generated).\n", "We have a way to enforce the GPU being used (force_gpu arg for test\nsession), but we have no test for it. We should make such a test.\nOn Thu, Jun 16, 2016 at 04:27 Bob Adolf notifications@github.com wrote:\n\n> Thanks, @martinwicke https://github.com/martinwicke.\n> \n> Also, I alluded to this in the original post, but I think the CI issue\n> boils down to the tests checking for correctness on the output of the test,\n> whereas in this case, we actually care that the output was correct _and_\n> that the output was generated by code on the GPU. Since TF fails gracefully\n> back to the CPU when the cuda libraries aren't found, the tests run along\n> happily (and no non-zero return code is generated).\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2865#issuecomment-226458745,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAjO_fy7njvqpSMDp588QYjyBhF1FjCKks5qMTMEgaJpZM4I1wXq\n> .\n", "Testception\n", "I will close this issue. We're re-working a number of things in the test setup which hopefully as a side effect addresses this.\n"]}, {"number": 2864, "title": "Branch 124869789", "body": "", "comments": []}, {"number": 2863, "title": "fix dim mismatch bug in resnet example.", "body": "- There was a bug which confused residual blocks with residual groups,\n  which is just  a list of blocks. The output of each block should be of\n  the same dimension as the input so that a residual connection can be\n  made. Upscaling only occurs between groups.\n- Changed some of the variable names to clarify the distinction between\n  groups, blocks and layers.\n", "comments": ["Can one of the admins verify this patch?\n", "Interesting, I do not get the `NotImplementedError` when I run the code on tensorflow v0.8.0. The code to load the saved model is definitely hit when there is a model at specified path.\n", "I think this error has to do with commit 150005ec3f995473ee11913d055fb6fe7e93479f which changed the behavior of `_restore` in `learn/estimators/base.py` with v0.9.0 RC0, but I am not familiar enough with TensorFlow to know if this is an issue or a misuse of `TensorFlowEstimator` (I have encountered this error on two machines with different configurations so I guess this issue is not machine-specific).\nThat is why I have caught the exception `NotImplementedError` on #2861.\n", "Ref #2950\n", "@gideonite @ilai-deutel I pretty much disabled restore code, because it's not really working well with current way of handling the graph. This example should remove usage of restore and just point into the same directory. If you want to change it or I can create a PR with your commits and the change in saving/restoring.\n", "@ilblackdragon Cool. Just pushed a small change. Does this address your comments?\n\nThis is probably not the right place to open this conversation but I'm thinking that the examples should somehow get plugged into the automatic testing system. Even if it's just to make sure that the syntax is valid. Should I open a new issue?\n", "@tensorflow-jenkins test this please\n"]}, {"number": 2862, "title": "Tensorboard: non-square images are overlapped", "body": "### Environment info\n\nOperating System: Ubuntu 14.04\n\nIf installed from sources, provide the commit hash:\n592675b2b8d1cabbf923638942ea6f200abe353a\n### Steps to reproduce\n1. Log some non-square (rectangle, 72x105 in my case) images (at least two) with SummaryWriter.\n2. Load the log file with tensorboard.\n3. Now, you will see the images are overlapped on images tab.\n### What have you tried?\n1. I tried to fix the CSS with Chrome debugger.<br />\n   I changed `height` property in `.tf-image-grid-0 .tag-name-cell.tf-image-grid` and `.tf-image-grid-0 .image-cell.tf-image-grid` to `auto`, and it seems to be good.<br />\n   However, I don't know what files need to be patched to apply this change.\n### Logs or other output that would be helpful\n\nHere is a screenshot of overlapped images.\n`input/image/1`, `input/image/2` is not being shown properly.\nLast image isn't overlapped.\n\n<img width=\"620\" alt=\"2016-06-15 4 20 07\" src=\"https://cloud.githubusercontent.com/assets/5977817/16056438/7631b366-32b0-11e6-940e-1dfb7f532990.png\">\n", "comments": ["Thanks for reporting this issue @qbx2. I'm doing a bunch of work on polishing the images dashboard right now. I'll make sure this gets fixed in the near future.\n"]}, {"number": 2861, "title": "Fix bugs in Residual Network implementation", "body": "- Fix `res_net` implementation by moving  upscaling outside of the `layer_i` loop (bug was introduced with commit 5c145f0e3cabc7e61440ddf32c1ac28f5b9d499e)\n- `learn.TensorFlowEstimator.restore()` raises `NotImplementedError`; disable restoration until it is implemented\n", "comments": ["Can one of the admins verify this patch?\n", "I think that I've duplicated this change in https://github.com/tensorflow/tensorflow/pull/2863 without noticing that it had just been fixed by this PR. I do think that my change does a better job at clarifying the ResNet implementation though =)\n"]}, {"number": 2860, "title": "[Installation problem] ImportError: No module named google.protobuf", "body": "Installation problem\n### Environment info\n\nOperating System: Ubuntu 14.04\n\nInstalled version of CUDA and cuDNN: None\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n### Which pip package you installed.\n- adium-theme-ubuntu (0.3.4)\n- apt-xapian-index (0.45)\n- argparse (1.2.1)\n- auxlib (0.0.39)\n- chardet (2.0.1)\n- colorama (0.2.5)\n- command-not-found (0.3)\n- conda (4.0.8)\n- configobj (4.7.2)\n- Cython (0.20.1post0)\n- debtagshw (0.1)\n- decorator (3.4.0)\n- defer (1.0.6)\n- deluge (1.3.6)\n- dirspec (13.10)\n- dnspython (1.11.1)\n- duplicity (0.6.23)\n- enum34 (1.1.6)\n- gyp (0.1)\n- h5py (2.2.1)\n- html5lib (0.999)\n- httplib2 (0.8)\n- ipython (1.2.1)\n- Jinja2 (2.7.2)\n- Landscape-Client (14.12)\n- lockfile (0.8)\n- lxml (3.3.3)\n- Mako (0.9.1)\n- MarkupSafe (0.18)\n- matplotlib (1.3.1)\n- mercurial (2.8.2)\n- nose (1.3.1)\n- numexpr (2.2.2)\n- numpy (1.11.0)\n- oauthlib (0.6.1)\n- oneconf (0.3.7.14.04.1)\n- PAM (0.4.2)\n- pexpect (3.1)\n- Pillow (2.3.0)\n- pip (1.5.4)\n- piston-mini-client (0.7.5)\n- pomodoro-indicator (0.1.0)\n- powerline-status (2.2.dev9999-git.841c25f6b61e1632d7f2343289c52698bcb9b805)\n- protobuf (3.0.0b2)\n- psutil (3.2.2)\n- pycosat (0.6.1)\n- pycrypto (2.6.1)\n- pycups (1.9.66)\n- pycurl (7.19.3)\n- pygame (1.9.1release)\n- pygobject (3.12.0)\n- pyne (0.5.0-rc1)\n- pyOpenSSL (0.13)\n- pyparsing (2.0.1)\n- pyrtlsdr (0.2.0)\n- pyserial (2.6)\n- pysmbc (1.0.14.1)\n- PyTAPS (1.4)\n- python-apt (0.9.3.5ubuntu2)\n- python-dateutil (1.5)\n- python-debian (0.1.21-nmu2ubuntu2)\n- python-libtorrent (0.16.13)\n- pytz (2012c)\n- pyxdg (0.25)\n- PyYAML (3.11)\n- reportlab (3.0)\n- requests (2.2.1)\n- scipy (0.13.3)\n- sessioninstaller (0.0.0)\n- setuptools (23.0.0)\n- simplegeneric (0.8.1)\n- six (1.10.0)\n- software-center-aptd-plugins (0.0.0)\n- sympy (0.7.4.1)\n- system-service (0.1.6)\n- tables (3.1.1)\n- tensorflow (0.9.0rc0)\n- Twisted-Core (13.2.0)\n- Twisted-Web (13.2.0)\n- unity-launcher-folders (14.09.4)\n- unity-lens-photos (1.0)\n- urllib3 (1.7.1)\n- vboxapi (1.0)\n- wheel (0.29.0)\n- wsgiref (0.1.2)\n- xdiagnose (3.6.3build2)\n- zope.interface (4.0.5)\n### The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/**init**.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/**init**.py\", line 58, in <module>\n    raise ImportError(msg)\nImportError: Traceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/**init**.py\", line 52, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\n    from google.protobuf import descriptor as _descriptor\nImportError: No module named google.protobuf\n\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there.\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n\nI followed instructed steps to install tensorflow, however, my python seems to miss the googlebuf package. Weirdly, it does recognize tensorflow package under the same dist-packages folder.\n### What have you tried?\n\nI tried to modify the PYTHONPATH and reinstall tensorflow and pip several times.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n\ue0b0 ~ \ue0b0 python\nPython 2.7.11 |Anaconda 4.0.0 (64-bit)| (default, Dec  6 2015, 18:08:32) \n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\nAnaconda is brought to you by Continuum Analytics.\nPlease check out: http://continuum.io/thanks and https://anaconda.org\n\n> > > import tensorflow\n> > > Traceback (most recent call last):\n> > >   File \"<stdin>\", line 1, in <module>\n> > >   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/**init**.py\", line 23, in <module>\n> > >     from tensorflow.python import *\n> > >   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/**init**.py\", line 58, in <module>\n> > >     raise ImportError(msg)\n> > > ImportError: Traceback (most recent call last):\n> > >   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/**init**.py\", line 52, in <module>\n> > >     from tensorflow.core.framework.graph_pb2 import *\n> > >   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\n> > >     from google.protobuf import descriptor as _descriptor\n> > > ImportError: No module named google.protobuf\n\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there.\n\n", "comments": ["Could you try running:\n\nsudo pip install protobuf\n\nand retry your command?\n", "Hi, I tried and here is the response.\n\n mapur@mapur-ThinkPad-T540p \ue0b0 ~ \ue0b0 sudo pip install protobuf\nRequirement already satisfied (use --upgrade to upgrade): protobuf in /usr/local/lib/python2.7/dist-packages\nCleaning up...\n mapur@mapur-ThinkPad-T540p \ue0b0 ~ \ue0b0 python \nPython 2.7.11 |Anaconda 4.0.0 (64-bit)| (default, Dec  6 2015, 18:08:32) \n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\nAnaconda is brought to you by Continuum Analytics.\nPlease check out: http://continuum.io/thanks and https://anaconda.org\n\n> > > import tensorflow\n> > > Traceback (most recent call last):\n> > >   File \"<stdin>\", line 1, in <module>\n> > >   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/**init**.py\", line 23, in <module>\n> > >     from tensorflow.python import *\n> > >   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/**init**.py\", line 58, in <module>\n> > >     raise ImportError(msg)\n> > > ImportError: Traceback (most recent call last):\n> > >   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/**init**.py\", line 52, in <module>\n> > >     from tensorflow.core.framework.graph_pb2 import *\n> > >   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\n> > >     from google.protobuf import descriptor as _descriptor\n> > > ImportError: No module named google.protobuf\n\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there.\n\n", "I believe this is caused by a conda issue that it wont work if there's a library outside conda but conda neither can use it nor can uninstall it.\n", "Closing since this seems to have been identified and/or resolved.\n"]}, {"number": 2859, "title": "tf.train.exponential_decay examples use 32 bit number for batch", "body": "As far as I can tell, `tf.train.exponential_decay` examples seem to use a 32-bit signed number for global_step, because the batch number is also a `tf.int32`.  This means that long runs can result in [unpleasant surprises](http://nyus.joshuawise.com/fml-tensorflow.png) with learning rates, which result in a frustrating experience for new users. [1]  Examples should be updated to initialize `batch` as a `dtype=tf.int64`.\n\nI originally believed this to be a `tf.train.exponential_decay` bug, and wrote some code to minimize the issue.  I now understand that this comes from the variable, but you can have the below repro case anyway, because I think it makes it a little more obvious as to the kind of thing that can happen.  When you run it, you'll note a discontinuity between epoch 85 and 86 in learning rate that comes from `batch * batch_size` overflowing (and results in losing an evening's training...).\n\n``` python\n\nimport tensorflow as tf\n\ndef main(argv=None):\n  epoch_size = 25000000\n  epoches = 100\n  batch_size = 16384\n\n  batch = tf.Variable(0) # <-- this line, in examples, should be changed to have dtype=tf.int64, with explanation of why\n  learning_rate = tf.train.exponential_decay(\n      0.01,\n      batch * batch_size,\n      epoch_size,\n      0.95,\n      staircase = True)\n  loss = tf.Variable(0.0)\n  eval_frequency = int(epoch_size / batch_size / 2)\n\n  optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9) \\\n     .minimize(loss, global_step = batch)\n\n  with tf.Session() as sess:\n    tf.initialize_all_variables().run()\n\n    for step in xrange(int(epoches * epoch_size) // batch_size):\n      _, b, lr = sess.run([optimizer, batch, learning_rate])\n      if step % eval_frequency == 0:\n        print(\"Step %d (epoch %.2f): batch %d, learning rate %.6f\" %\n            (step, float(step) * batch_size / epoch_size, b, lr))\n\nif __name__ == '__main__':\n  tf.app.run()\n```\n\n_[1] No, I did not checkpoint midway through.  Yes, I have learned my lesson._\n", "comments": ["Examples are supposed to cover common use cases. If you do expect your experiments to overflow a 32-bit integer, then yes, please explicitly set it to int64.\n", "Hmm, this might be an indication that what I'm doing is unreasonable :-) But my understanding is that \"more data is better\", and that 12 hour training runs are not super uncommon in the world of DNNs.  (My inexperience may show through here!)  I had actually (obviously, incorrectly) assumed that the type of that Variable would have been some kind of floating point datatype, given that it wasn't specified in the documentation.\n\nGiven how much detail the rest of the (wonderful!) documentation goes into about gotchas and issues that one might have, I think I respectfully disagree: I tripped on this relatively readily (after only a week or so of using the tool), and it cost me 10 hours or so of training time and a day's worth of results, and I think that highlighting this could help new users to think through what exactly is going on in the examples.\n\nBut, it's your project!  Either way, thanks for the consideration.\n"]}, {"number": 2858, "title": "[install problem] Tensorflow with Anaconda on Ubuntu", "body": "On my Ubuntu 14.04, I have installed tensorflow from source, as specified in the Tensorflow Installation instructions and it works well.\n\nThen I install anaconda but it changes the $PATH environment variable, so I cannot import tensorflow.\nFollowing the instruction in http://stackoverflow.com/questions/33646541/tensorflow-and-anaconda-on-ubuntu, I created an environment and installed tensorflow in it, as specified in https://www.tensorflow.org/versions/r0.9/get_started/os_setup.html#anaconda-installation. I use this command `sudo pip install --upgrade $TF_BINARY_URL` to install tensorflow. But the tensorflow is not installed in this environment. It seems that it just updates the tensorflow I have installed previously. The terminal window is shown in the image:\n\nI have solved this problem. Use `pip install --upgrade $TF_BINARY_URL` instead. Using sudo will install the package globally. \n\n![terminal](https://cloud.githubusercontent.com/assets/19935904/16053977/1d0deb32-3239-11e6-9875-f14eb6cc22b3.png)\n", "comments": []}, {"number": 2857, "title": "Merge iOS and Pi changes into 0.9 branch", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "Confirming this is covered under the CLA.\n", "Jenkins, test this please.\n"]}, {"number": 2856, "title": "Branch 124848019", "body": "", "comments": []}, {"number": 2855, "title": "Jenkins test take dependencies from pip package instead of .runfiles directory", "body": "TLDR; Jenkins looks for dependencies in `python2.7/site-packages/tensorflow/..` but it should look for them in `bazel-bin/.../test.runfiles/...`. Possibly related to https://github.com/tensorflow/tensorflow/issues/2844\n\nI'm looking at a test failure in https://github.com/tensorflow/tensorflow/pull/2747, and it can't find a data dependency while running on Jenkins, meanwhile this test passes under `bazel test` in a fresh git clone.\n\nMore specifically, `bazel test tensorflow/contrib/immediate/mnist_inference_test` copies all Python files and data dependencies to `bazel-bin/tensorflow/contrib/immediate/mnist_inference_test.runfiles/`\n\nHowever, from the path of the error, it seems it's looking for dependencies in `python2.7/site-packages/tensorflow/` instead of `.runfiles` directory. The path to convolutional.py should look like this `bazel-bin/tensorflow/contrib/immediate/mnist_inference_test.runfiles/org_tensorflow/tensorflow/models/image/mnist/convolutional.py`, instead of `site-packages`. When dependencies are resolved to `site-packages`,  this case failure is be expected, because while Python files are included in the PIP package, but  data dependencies of tests will not be.\n\n```\nTraceback (most recent call last):\n  File \"/workspace/pip_test/tests/mnist_inference_test.py\", line 23, in testMnistInference\n    test_data = convolutional.extract_data(test_data_filename, 10000)\n  File \"/workspace/pip_test/venv/local/lib/python2.7/site-packages/tensorflow/models/image/mnist/convolutional.py\", line 83, in extract_data\n    with gzip.open(filename) as bytestream:\n  File \"/usr/lib/python2.7/gzip.py\", line 34, in open\n    return GzipFile(filename, mode, compresslevel)\n  File \"/usr/lib/python2.7/gzip.py\", line 94, in __init__\n    fileobj = self.myfileobj = __builtin__.open(filename, mode or 'rb')\nIOError: [Errno 2] No such file or directory: 'tensorflow/contrib/immediate/python/immediate/testdata/t10k-images-idx3-ubyte.gz'\n\n```\n", "comments": ["@caisq, is this a pip install issue?\n", "Definitely sounds related to #2844. \n", "This is now resolved?\r\nAt least on my own machines I have TF setup locally, and bazel seems to capture my changes in my workspace during bazel test. @yaroslavvb, could you verify?", "I don't have Mac development env setup since releasable Mac GPU builds obviated the need to build myself. I think this could be closed and re-opened if someone else runs into this"]}, {"number": 2854, "title": "change weights during training", "body": "Hi,\nI train a model and save it to ckpt file.\nThe next think i want do to is to load the model and change some of his weights. I read the TF API and didn't find any way to do that. Am i missing something? Is there a way to change the weights during training?\n\nthank a lot!\n", "comments": ["How do you want to change the weights?\n", "Every weight < threshold \nAssign weight = const\n", "http://stackoverflow.com/questions/34220532/how-to-assign-value-to-a-tensorflow-variable\n", "You can do something like:\n\nv = tf.Variable(1.0)\nc = tf.constant(2.0)\nc2 =  tf.cond(v < c, lambda: tf.constant(3.0), lambda: tf.constant(4.0))\nv2 = tf.assign(v, c2)\nwith tf.Session() as sess:\n  sess.run(tf.initialize_all_variables())\n  print sess.run(v2)\n", "thanks\n", "Is there a way to change some variable into \"untrainable\" while training?\n"]}, {"number": 2853, "title": "Enable tf.neg() for SparseTensor", "body": "Enabled `tf.neg()` for SparseTensor. Added tests and verified locally. This partially addresses #1828.\n", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, test this please.\n"]}, {"number": 2852, "title": "I think 'class EmbeddingWrapper' has problem", "body": "I found that there are some differences between r0.7 and r0.9 which is that : class EmbeddingWrapper in rnn_cell.py in r0.9 doens't has some @property values, just like `def input_size` , `def output_size` . and when i run rnn.rnn with embeded cell, it causes an NotImplementedError which is from RNNCell 's function\n\n``` javascript\ndef __call__(self, inputs, state, scope=None):\n    \"\"\"Run this RNN cell on inputs, starting from the given state.\n\n    Args:\n      inputs: `2-D` tensor with shape `[batch_size x input_size]`.\n      state: if `self.state_size` is an integer, this should be a `2-D Tensor`\n        with shape `[batch_size x self.state_size]`.  Otherwise, if\n        `self.state_size` is a tuple of integers, this should be a tuple\n        with shapes `[batch_size x s] for s in self.state_size`.\n      scope: VariableScope for the created subgraph; defaults to class name.\n\n    Returns:\n      A pair containing:\n      - Output: A `2-D` tensor with shape `[batch_size x self.output_size]`.\n      - New state: Either a single `2-D` tensor, or a tuple of tensors matching\n        the arity and shapes of `state`.\n    \"\"\"\n    raise NotImplementedError(\"Abstract method\")\n```\n\nDid i do something wrong?\n", "comments": ["Lukasz, ptal?\n", "I made a code change correcting it (adding output_state to EmbeddingWrapper), so that should be working now on master. Closing, please re-open if it's still not working.\n"]}, {"number": 2851, "title": "How to reduce_max variable length sentences?", "body": "When I use word-embedding to train model, variable-sentence-length is a problem that could hurt performance.\nThe task is same as [#2849](https://github.com/tensorflow/tensorflow/issues/2849) . I open this new issue because of variable-sentence-length problem.\nThe task is to classify each sentence into 10 classes. Each sentence is length of 1~30 words. Firstly I use embedding_lookup to get all 1~30 word-vectors, then I use tf.map_fn(lambda x:tf.reduce_max(x,0), \u2026) to reduce each sentence\u2019s 1~30 word-vectors into a single \u201csentence vector\u201d. Then It put this \u201csentence vector\u201d as DNN's input. The code is same as #2849:\n[WE_example.py.txt](https://github.com/tensorflow/tensorflow/files/313877/WE_example.py.txt)\n\nIn the above code, I have to use 30 as sentence length. Because the \u201ctf.map_fn(tf.reduce_max(...))\" could only merge sub-matrixes of the same size, for example, the minibatch is [100, 30, 200], then all sub-matrixes are [30, 200], using \"tf.map_fn(tf.reduce_max(...))\" could merge each sub-matrix into [200]. \n\nBut in the real world, sentences are of variable lengths. If I add padding data to the tail of sub-matrixes, I will not only give accuracy problem, but also bring more data to transfer between CPU&GPU, and also give more unneccesary computation cost.\nThe ideal solution is providing a powerful_reduce function to support reduce on sub-sections. For example, Five sentences' length are {3, 10, 7, 25, 2}. I gave powerful_reduce a matrix of size [3+10+7+25+2, 200], and a 1D array of content { 3, 10, 7, 25, 2}, it could return the reduce results of the five sub-matrixes, the result shape is [5, 200]. \nAnother way is to provide a \"mask\" array as input parameter. The reduce_max operation will only operater on the items with mask 1, and skip the items with mask 0.\nPreviously I used CUDA Thrust Library, it could handle it by using \"for_each\". \n\nSo, could tensorflow provide a powerful reduce_max function to support variable-size sub matrixes?\nAnother question is , I found my GPU usage is only about 20%, in order to improve the GPU usage, I prefer to run multiple processes on the same GPU card using distributed tensorflow. But I don't know how much GPU memory my process will occupy(the upper-bound). In another word, how could I safely decide the minimum \"x\" in \"tf.GPUOptions(per_process_gpu_memory_fraction = x)\"?\n\nThanks a lot in advance~\n", "comments": ["I feel that your first and second questions are probably better suited for StackOverflow.  Users there may have faced similar problems, and may be able to help. Please ask it there and tag it with the `tensorflow`.\n"]}, {"number": 2850, "title": "Saver() not saving as .ckpt file", "body": "I'm trying to save and restore a model. From all the support I have found [here](https://www.tensorflow.org/versions/r0.9/how_tos/variables/index.html) & [here](http://stackoverflow.com/questions/33759623/tensorflow-how-to-restore-a-previously-saved-model-python) `Saver.save()` is supposed to save a model file of type `.ckpt` & then `Saver.restore()` is supposed to used that file name coupled with the directory it is in. However, my code does not save the files as .ckpt files, it just saves them as binary files. I could be wrong on that. My reasoning is that when I call `Saver.restore()` it returns None.\n\nThe files are being saved as `model.ckpt-10` and then `model.ckpt-20` and so on. I have the checkpoint text file. I've tried using that as well, but no luck.\n\n```\nsaver = tf.train.Saver()\nTRAIN_FLAG = 1\n\n# Launch the graph\nwith tf.Session() as sess:\n    sess.run(init)\n    # writer = tf.train.SummaryWriter(\"/tmp/logs\", sess.graph_def)\n    if TRAIN_FLAG:\n        from tensorflow.contrib.learn.python.learn.datasets.scroll import scroll_data\n        data = scroll_data.read_data('/home/kendall/Desktop/')\n        step = 1\n        flag = 0\n        while flag == 0:\n            batch_y, batch_x = data.train.next_batch(batch_size)\n            batch_x = batch_x.reshape((batch_size, n_input))\n            batch_y = batch_y.reshape((batch_size, n_classes))\n            # Run optimization op (backprop)\n            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n            if step % display_step == 0:\n                flag = 1\n                save_path = \"model.ckpt\"\n                saver.save(sess, save_path, global_step=step)\n                # Calculate batch accuracy\n                acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n                # Calculate batch loss\n                loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n                print \"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n                      \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n                      \"{:.5f}\".format(acc)\n            step += 1\n        print \"Optimization Finished!\"\nelse:\n    pdb.set_trace()\n    ckpt = tf.train.get_checkpoint_state(\"/home/kendall/Academic/Summer\\ 2016/Programs/\", \"my-model-7001\")\n    saver.restore(sess, ckpt.model_checkpoint_path)\n    im = Image.open('/home/kendall/Desktop/HA900_frames/frame0635.tif')\n    batch_x = np.array(im)\n    prediction = sess.run(optimizer, feed_dict={x: batch_x})\n    print prediction\n```\n", "comments": ["I've met the same problem as yours. Can you tell me how you deal with it. thx\n"]}, {"number": 2849, "title": "Lack of support for word-embedding on GPU", "body": "Dear experts,\n\nI faced some problems when training model with word-embedding. For simplify the case, I make a helloworld example in this file:\n[WE_example.py.txt](https://github.com/tensorflow/tensorflow/files/313712/WE_example.py.txt)\n\nThe task is to classify each sentence into 10 classes. Each sentence is length of 30 words. Firstly I use embedding_lookup to get all 30 word-vectors, then I use tf.map_fn(lambda x:tf.reduce_max(x,0), \u2026) to reduce each sentence\u2019s 30 word-vectors into a single \u201csentence vector\u201d. Then It put this \u201csentence vector\u201d as DNN's input.\n\nThe following are problems & questions:\na.    If I use \u201ctf.train.RMSPropOptimizer\u201d as optimizer, it reports \u201cNotImplementedError\u201d, the details is in the following stderr file:\n[RMSPropOptimizer_NotImplemented.stderr.txt](https://github.com/tensorflow/tensorflow/files/313717/RMSPropOptimizer_NotImplemented.stderr.txt)\nSo, is there a plan to implement RMSPropOptimizer\u2019s support for word-embedding in next release?\n\nb.    Then I replace \u201ctf.train.RMSPropOptimizer\u201d by \u201ctf.train.MomentumOptimizer\u201d.  It works! But the GPU usage is only around 22%. I think putting embedding matrix on host memory (\u201c/cpu:0\u201d) causes data-transfer delay, which may lower down the GPU usage. Then I remove the \u201cwith tf.device(\u2018/cpu:0\u2019)\u201d in line 21(just before definition of word-embedding variable), to put the word-embedding matrix into GPU memory. And run it again. It shows \u201ctensorflow.python.framework.errors.InvalidArgumentError: AttrValue must not have reference type value of float_ref\u201d. \nI put the stdout & stderr as follows:\n[MomentumOptimizer_on_GPU.stdout.txt](https://github.com/tensorflow/tensorflow/files/313725/MomentumOptimizer_on_GPU.stdout.txt)\n[MomentumOptimizer_on_GPU.stderr.txt](https://github.com/tensorflow/tensorflow/files/313727/MomentumOptimizer_on_GPU.stderr.txt)\n\nIt seems that putting word-embedding into GPU memory is not supported by all operators. I also found the same issue in [word2vec_basic.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py) , the line 144 \u201c# Ops and variables pinned to the CPU because of missing GPU implementation\u201d .\nAnd also [word2vec.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/embedding/word2vec.py) Ln508. It also put word-embedding definition onto /cpu:0.\n\nWord-embedding is very popular in NLP domain. It is usually very large(more than 1GB in internet-level data). So putting it from host memory side to GPU global memory side may save a lot of data-transfer time cost. Is there any plan to fully support it in ALL operators in future release?\n\nc.    Then I replace \u201ctf.train.MomentumOptimizer\u201d by \u201ctf.train.GradientDescentOptimizer\u201d, still removing \u201c/cpu:0\u201d. Then it works! The GPU usage is around 25%~30%. If adding \u201c/cpu:0\u201d, the GPU usage is around 20%~25%.\n\nd.    Because the GPU usage is always below 30%(also in my real code running real data). Is there any way to profiling it to find where is the speed bottleneck?\n\nSorry for so many questions... But it's really a blocking issue for NLP guys...\n", "comments": ["For d, you can follow instructions here to get profiling information:\n\nhttp://stackoverflow.com/questions/37751739/tensorflow-code-optimization-strategy/37774430#37774430\n", "There are a number of operators (especially those working on integer types) which are not registered for GPU, and for some fairly complex reasons this is not likely to change in the near future.\n\nMany of the general questions above are better suited for StackOverflow. Please could you ask them there and tag them with the `tensorflow` tag.\n", "Might be helpful:\r\nhttps://github.com/ravyg/tensorflow/commit/2a83490455f0f02ea05fc447551cacea1c1b8cb6\r\nSupports:\r\n1) Multi-threaded Incremental word2vec\r\n2) Mini-batched skip-gram model GPU support \r\n3) Works with Tensorflow r0.12 with Checkpoints (New saver in TFr0.12)\r\n"]}, {"number": 2848, "title": "GPU Host To Device copies don't appear to respect GPU operation dependencies", "body": "I've been trying out the fancy new [tracing](http://stackoverflow.com/a/37774430/1611416) functionality in the Tensorflow nightlies. It's really great, thanks for providing it!\n\nOne thing I noticed while [enqueueing](https://github.com/ska-sa/montblanc/blob/29b264ee114a6ef63159808125b1a4505cc5ffa2/montblanc/tensorflow/rime_ops/test_all.py#L66-L127) multiple GPU operations with `tf.control_dependencies` is that Host to Device memory copies for the inputs of consecutive GPU ops can be scheduled in an interleaved pattern, instead of consecutively. In my case I have something like this:\n1. op EBeam with inputs: ebeam\n2. op SumCoherencies with inputs: gterm, ant2, model_vis, flag, ant1, ant2\n\nbut in the [trace](https://github.com/tensorflow/tensorflow/files/313540/timeline.json.zip) I see the memory copies scheduled as:\n- ant1, flag, **ebeam**, gterm, ant2, model_vis\n\nwhen I would expect, due to the the consecutive scheduling of EBeam and SumCoherencies ops, that the memory copies would be scheduled as:\n- **ebeam**, ant1, flag,  gterm, ant2, model_vis\n\nI also notice that the EBeam op only starts executing on the GPU after all the SumCoherencies inputs have been copied to the GPU (rather than just the ebeam input) so the GPU is idle. There is no dependency on SumCoherencies by EBeam (Its the other way round).\n\n_There are several other ops and inputs that I haven't mentioned for the sake of brevity_\n### Environment info\n\nOperating System: `Ubuntu 14.04.4`\n\nInstalled version of CUDA and cuDNN: `CUDA 7.5 and cuDNN 4.0.7`\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n``` bash\n~$ ls -l /usr/local/cuda/lib64/libcud*\n-rw-r--r-- 1 root root    322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root        16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root        19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root    383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root    720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a\nlrwxrwxrwx 1 3319 users       13 Feb  9 19:48 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.4\nlrwxrwxrwx 1 3319 users       17 Feb  9 19:48 /usr/local/cuda/lib64/libcudnn.so.4 -> libcudnn.so.4.0.7\n-rwxrwxr-x 1 3319 users 61453024 Feb  9 00:12 /usr/local/cuda/lib64/libcudnn.so.4.0.7\n-rw-rw-r-- 1 3319 users 62025862 Feb  9 00:12 /usr/local/cuda/lib64/libcudnn_static.a\n```\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed. `python 2 GPU nightly`\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`. `0.8.0`\n\nIf installed from sources, provide the commit hash: **N/A**\n### Steps to reproduce\n1. Ran [this](https://github.com/ska-sa/montblanc/blob/29b264ee114a6ef63159808125b1a4505cc5ffa2/montblanc/tensorflow/rime_ops/test_all.py#L66-L127) script. (If necessary, source with Makefile is [here](https://github.com/ska-sa/montblanc/tree/29b264ee114a6ef63159808125b1a4505cc5ffa2/montblanc/tensorflow/rime_ops) and relevent commit is [29b264ee](https://github.com/ska-sa/montblanc/tree/29b264ee114a6ef63159808125b1a4505cc5ffa2))\n2. Inspected the timeline ([timeline.json.zip](https://github.com/tensorflow/tensorflow/files/313540/timeline.json.zip)) in `chrome://tracing/`\n### What have you tried?\n1. Using `tf.control_dependencies` to modify op execution order\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["This is indeed the expected behavior.   Control dependencies only constrain the order of evaluation of the compute ops in your program.  The device to host memcpy operations are executed as _Send/_Recv ops which are automatically inserted when the graph is partitioned between cpu and gpu devices.  \n\nThe _Recv ops  have zero inputs and so are schedulable immediately.  The _Send ops (and hence the transfers) are schedulable as soon as the input data has been produced (in this case by Identity ops with names like 'variable/read' which take a single \"snapshot\" of each variable).  \n\nThe order that the transfers happen _should_ have no influence on the result of the computation - but, in this case, do appear to have a significant impact on latency.  (You appear to have some _huge_ transfers!)\n\nIn the distributed setting, GraphOptions.enable_recv_scheduling can be used to constrain the order of _Recv nodes.  However, this doesn't currently influence intra-machine transfers.\n\nIf you really care about hand-constraining the order of the GPU transfers, you could _probably_ write something using a bunch of _additional_ identity ops on the cpu and gpu device and explicitly put control dependencies on these.   \n\n@yuanbyu  might have more to say on this issue?\n", "@prb12 Thanks for the detailed feedback, its very helpful for understanding Tensorflow's transfer handling.\n\n> The order that the transfers happen should have no influence on the result of the computation - but, in this case, do appear to have a significant impact on latency. (You appear to have some huge transfers!)\n\nI think it's highly dependent on an algorithm's compute to I/O ratio -- Transfers don't matter much if one is performing a lot of GPU compute for a small amount of input data. As this ratio decreases one wants to ensure that one's data transfers overlap compute in order to maximise device usage. For instance, Gregg and Hazelwood have devised a [taxonomy](http://www.cs.virginia.edu/kim/docs/ispass11.pdf) for classifying GPU algorithms according to these properties.\n\nBut I suspect I'm preaching to the converted here ;-). Similar principles probably apply  to Google's bread and butter -- scheduling RPC network transfers -- and its encouraging to hear about the **enable_recv_scheduling** option. I'll have a look at `tf.identity` -- there is a nice  [stackoverflow](http://stackoverflow.com/a/34877802/1611416) answer for other's looking at this issue.\n\nHaving said that,  would there be scope for improving the scheduling of intra-machine send/recv ops?\n", "> Having said that, would there be scope for improving the scheduling of intra-machine send/recv ops?\n\nIn the general case, doing a good job of this requires having an accurate cost model including the computation cost of ops and the shapes of all Tensors.  In the presence of custom ops and dynamically fed tensors (whose shape we often don't know until runtime), there is a lot of missing information.  Even in the presence of full information, computing a good schedule is an expensive algorithm which needs to trade off a lot of variables (e.g. peak GPU memory usage, critical path latency, potential concurrency)  \n\nHaving said that, there are probably a bunch of fairly simple heuristics which may work well in simple cases like this... \n", "Closing this since it has been a while and the code might have changed. Feel free to open a new issue if the problem persists with new code."]}, {"number": 2847, "title": "[WIP] Gradient reversal op", "body": "\"[Unsupervised Domain Adaptation by Backpropagation](http://sites.skoltech.ru/compvision/projects/grl/files/paper.pdf)\" introduces a very simple but effective \"gradient reversal layer\" that is the identity for the forward computation and flips the gradient during backpropagation. This PR implements this as a user op called `FlipGradientOp`. The gradient registration is [here](https://github.com/pumpikano/tf-dann/blob/master/flip_gradient.py) (with a small MNIST experiment in that repo that uses the op).\n\nI created the PR to gauge if there is any interest in getting something like this into core or at least contrib. As you can see, the operation itself is dead simple and very general. The same effect can be achieved by processing gradients before application, but the op implementation means that everything works naturally as a result of autodiff. I may have missed a way to achieve this functionality with existing operations - if so, let me know!\n\nThe example implementation here of `FlipGradientOp` effectively multiplies the gradient by -1. It may be worth generalizing this as a `ScaleGradientOp` that scales the incoming gradient by an arbitrary scalar tensor.\n\nI'm happy to get this in mergeable shape if the tensorflow team approves.\n", "comments": ["Can one of the admins verify this patch?\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Do you know about `tf.Graph.gradient_override_map`?  It's a bit ugly, but it lets you do this with unmodified TensorFlow.  Can you check if it suffices for your purposes?\n", "Thanks @girving, I wasn't aware of `tf.Graph.gradient_override_map`. For my use case, I need to provide a scalar tensor to scale the gradients. To do this, I need this tensor to exist already and use it in the gradient function. I came up with this approach, which I think is acceptable, if a little hacky:\n\n``` python\nclass FlipGradientsBuilder(object):\n    def __init__(self):\n        self.num_calls = 0\n\n    def __call__(self, x, l):\n        grad_name = \"FlipGradient%d\" % self.num_calls\n        @ops.RegisterGradient(grad_name)\n        def _flip_gradients(op, grad):\n            return [tf.neg(grad) * l]\n\n        g = tf.get_default_graph()\n        with g.gradient_override_map({\"Identity\": grad_name}):\n            y = tf.identity(x)\n\n        self.num_calls += 1\n        return y\n\nflip_gradients = FlipGradientsBuilder()\n```\n\nThis allows me to call `y = flip_gradients(x, l)` any number of times when building the graph with minimal overhead as far as I can tell. I wanted to share this solution in case others find it useful.\n", "@pumpikano Yep, that's how you'd have to use it.  I agree that `gradient_override_map` is pretty clunky, but given that it works I think it's better to use it than to introduce a special `flip_gradient` op.\n", "Cool, thanks. I'm closing this PR.\n"]}, {"number": 2846, "title": "Increase speed of getting pool3 features on CPU machine", "body": "Currently on my CPU setup, Tensorflow is taking almost 0.5 second for one image, to get the pool3 features. I'm using the retrained Inception model, after training it on my own images.\n\nAny way to increase this speed?\n", "comments": ["This is a question better suited for StackOverflow. Please ask it there and tag it with the `tensorflow` tag.\n"]}, {"number": 2845, "title": "Reuse add_loss function and removed skflow folder", "body": "I don't think people are still importing from skflow, instead of importing from learn. \n\nBTW, @martinwicke @ilblackdragon Are you in the process of removing `learn.ops` to avoid the duplication? What's the best way to handle (`is_training` in `contrib.layers`) and (`IS_TRAINING` in `learn.ops.dropout_ops` and `learn.ops.batch_norm_ops`)?\n", "comments": ["May be separate PRs for remove vs adding losses? feel a bit ... dirty ... to have it one :)\n", "About is_training - if you look at layers.batch_norm or dropout they take is_training bool or tensor. so it should be straightforward to convert.\n", "We don't have anything in flight for removal just yet - so if you want to start, feel free.\n", "@ilblackdragon Sorry for this combined PR ;-) Will do next time. Sure, I'll take a look when I get another chance. It's good for me to get some exposure to modules other than tf.learn. \n", "@ilblackdragon Green!\n"]}, {"number": 2844, "title": "Setting up TensorFlow for development affects subsequent `bazel test` runs", "body": "After following instructions in [Setting up TensorFlow for development](https://www.tensorflow.org/versions/r0.9/get_started/os_setup.html#setting-up-tensorflow-for-development) , `bazel test` will look in `_python_build` before looking in `.runfiles` directory.\n\nThe result is that some tests may fail which would otherwise pass in a hermetic requirement. Also it could mean that some tests would pass which would otherwise fail in a hermetic environment. A work-around is to remove `_python_build` directory before running `bazel test`\n\nBreaking into debugger while running test, I see following in `sys.path`, which confirms that `_python_build` is placed before `.runfiles`\n\n```\n['/Users/yaroslavvb/tfimmediate_hood/tensorflow/tensorflow/contrib/immediate/python/immediate',\n '/Library/Python/2.7/site-packages/six-1.10.0-py2.7.egg',\n '/Library/Python/2.7/site-packages/numpy-1.11.0-py2.7-macosx-10.10-intel.egg',\n '/Library/Python/2.7/site-packages/wheel-0.29.0-py2.7.egg',\n '/Library/Python/2.7/site-packages/pip-8.1.1-py2.7.egg',\n '/Library/Python/2.7/site-packages/ipython-4.2.0-py2.7.egg',\n '/Library/Python/2.7/site-packages/gnureadline-6.3.3-py2.7-macosx-10.8-intel.egg',\n '/Library/Python/2.7/site-packages/appnope-0.1.0-py2.7.egg',\n '/Library/Python/2.7/site-packages/pexpect-4.0.1-py2.7.egg',\n '/Library/Python/2.7/site-packages/backports.shutil_get_terminal_size-1.0.0-py2.7.egg',\n '/Library/Python/2.7/site-packages/traitlets-4.2.1-py2.7.egg',\n '/Library/Python/2.7/site-packages/simplegeneric-0.8.1-py2.7.egg',\n '/Library/Python/2.7/site-packages/pickleshare-0.7.2-py2.7.egg',\n '/Library/Python/2.7/site-packages/decorator-4.0.9-py2.7.egg',\n '/Library/Python/2.7/site-packages/ptyprocess-0.5.1-py2.7.egg',\n '/Library/Python/2.7/site-packages/ipython_genutils-0.1.0-py2.7.egg',\n '/Library/Python/2.7/site-packages/pathlib2-2.1.0-py2.7.egg',\n '/Library/Python/2.7/site-packages/protobuf-3.0.0b2-py2.7.egg',\n '/Library/Python/2.7/site-packages/Python_contrib_nbextensions-alpha-py2.7.egg',\n '/Library/Python/2.7/site-packages/PyYAML-3.11-py2.7-macosx-10.11-intel.egg',\n '/Library/Python/2.7/site-packages',\n '/Users/yaroslavvb/tfimmediate_hood/tensorflow/_python_build',\n '/Users/yaroslavvb/tfimmediate_hood/tensorflow/bazel-bin/tensorflow/contrib/immediate/itensor_test.runfiles',\n '/Users/yaroslavvb/tfimmediate_hood/tensorflow/bazel-bin/tensorflow/contrib/immediate/itensor_test.runfiles/protobuf/python',\n '/Users/yaroslavvb/tfimmediate_hood/tensorflow/bazel-bin/tensorflow/contrib/immediate/itensor_test.runfiles/org_tensorflow',\n '/Users/yaroslavvb/tfimmediate_hood/tensorflow/bazel-bin/tensorflow/contrib/immediate/itensor_test.runfiles/protobuf',\n '/Users/yaroslavvb/tfimmediate_hood/tensorflow/bazel-bin/tensorflow/contrib/immediate/itensor_test.runfiles/six_archive',\n\n\n```\n", "comments": ["That is very sad. The dev setup is in effect a hack, I guess we've been caught. It is probably enough to add a warning about the test issue to the dev environment setup instructions?\n"]}, {"number": 2843, "title": "Branch 124789706", "body": "", "comments": ["Jenkins, test this please.\n"]}, {"number": 2842, "title": "Update version number in pip package setup.py", "body": "Updated version number. \nMinor change. But I was confused for a short time about the old version number in the newly generated .whl file...\n", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, test this please.\n", "this is incomplete -- there are a bunch of other places to make this change.\n\n@martinwicke @caisq should we merge the version change from the r0.9 back into master?  Should this have been done first before branching?\n", "In the past we have merged back after the actual release. There's no reason\nnot to merge back more often. I'd like the actual version number change to\nbe a commit on the release branch but that's really only an aesthetic\npreference.\nOn Tue, Jun 14, 2016 at 11:57 Vijay Vasudevan notifications@github.com\nwrote:\n\n> this is incomplete -- there are a bunch of other places to make this\n> change.\n> \n> @martinwicke https://github.com/martinwicke @caisq\n> https://github.com/caisq should we merge the version change from the\n> r0.9 back into master? Should this have been done first before branching?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2842#issuecomment-225981568,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAjO_Wp9ETWIwRI44jjshRY56JsrzsaYks5qLvmAgaJpZM4I01TI\n> .\n", "We'll revert this, it breaks a bunch of stuff.\n", "I did not realize that the version number should also get updated in all these other places. I should have checked that. I was also not sure about where to base the pull request. \n\nI am sorry for your inconvenience.\n", "Don't worry. We shouldn't have merged it.\nOn Fri, Jun 17, 2016 at 11:56 Robin-des-Bois notifications@github.com\nwrote:\n\n> I did not realize that the version number should also get updated in all\n> these other places. I should have checked that. I was also not sure about\n> where to base the pull request.\n> \n> I am sorry for your inconvenience.\n> \n> \u2014\n> You are receiving this because you commented.\n> \n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2842#issuecomment-226852705,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAjO_eWJpjHUdN-BeGfDeRXrMU0k4uvvks5qMu3igaJpZM4I01TI\n> .\n"]}, {"number": 2841, "title": "Updated Eigen include paths for iOS examples", "body": "", "comments": []}, {"number": 2840, "title": "segfault in perftools::gputools::StreamExecutor::DeviceMemoryUsage - on a busy gpu", "body": "I'm using tensorflow 0.9.0rc0 with cuda 7.5 on Tesla K40c .\n\nThe GPU I'm specifying via `device_id` and\n\n```\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7, allow_growth=True)\n    sess_cfg = tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options)\n```\n\nis running under a heavy load now (multiple instances of tf are working with it in parallel, so it has 70-80% gpu voltage), but still has ~20-30% free memory, so I wanted to run a one more tiny script.\n\nIf I run the script under `gdb`, I get:\n\n```\nProgram received signal SIGSEGV, Segmentation fault.\n0x00007fffd51237a1 in perftools::gputools::StreamExecutor::DeviceMemoryUsage(long long*, long long*) const ()\n   from /data2/users/usman/anaconda2/envs/tfpy3/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so\n```\n\nis it indeed \"out of resources\" or there's something wrong with the installation? Shouldn't it somehow signalize about those problems in user code, rather than just segfaulting?\n", "comments": ["@MInner, the error comes from stream-executor. I will try to loop in some developers to help there. \n\nSomething seems wrong here. If you don't run this script under \"gdb\", what is the output? Could you post the entire log somewhere, and post the link here? \n", "Hi, @zheng-xq . If I run it without `gdb`, nothing interesting happens (no error messages or whatever, just the ordinary debug output regarding cuda .so files and etc.) until I try to execute the graph, and then it just segfaults silently (before any \"allocation attempt\" messages pop up). The very same script works just fine, if the GPU is not under heavy load; but when it was (under load), it failed with 100% chance. What kind of log should I share? The `dgb` output is just \"[Thread XXXX]\" and then the segfault message I posted above. I also can't see anything in `/var/crash/` (that is server Ubuntu 14.04.02, so apport should have posted something there, but it didn't, hmm)\n\nThere is a _very_ little change that this is a gpu driver problem (and it is the thing that causes segfault under heavy load), because I remember that we used to experience some issues with gpu drives at some point in the past.\n", "Swapping my action with Jason, who owns stream-executor. Feel free to assign back to me when we feel TensorFlow is at fault here. \n\nThe segfault is suspicious. Well behaving TensorFlow or stream-executor code should check for error conditions and quit with a clear error message. \n", "Really adding Jason. +@henline\n", "Hi @MInner. Thanks for your report.\n\nI think it would be useful for me to see the full stdout and stderr for the run that causes the segfault. That should include all the ordinary debug output you mentioned (loading cuda .so files, etc.).\n\nI'd also like to see the full backtrace when you run it under gdb. You should be able to get that by using the gdb 'backtrace' command.\n", "Sorry for not answering that long, I needed to wait for a heavy cluster load to reproduce this error\n\nhere's the `gdb` output and the backtrace:\nhttps://gist.github.com/MInner/dff6d209948d95cf4da9245dbbab18e8 .\n\nAnd yes, I totally forgot about the user-level warning regarding allocating a huge dense matrix - it comes from the modified code snippet `last_relevant()` from #206 that extracts \"last relevant output\" of dynamically unrolled rnn with different input lengths .\n", "Hi there,\n\nI think I get the similar error as @MInner\nAlso, this happens on only when I run on a 6G GPU. The program works normally on a 12G GPU card. \nIt would be great if there's any followup. \nHere's my debug output and backtrace\n\n```\nStarting program: /data/sls/scratch/wnhsu/pyenv_tf/r08/bin/python dnn_train_galebc2_debug.py\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n[New Thread 0x7fffd82e6700 (LWP 9747)]\n[New Thread 0x7fffd7ae5700 (LWP 9748)]\n[New Thread 0x7fffd52e4700 (LWP 9749)]\n[New Thread 0x7fffd2ae3700 (LWP 9750)]\n[New Thread 0x7fffd02e2700 (LWP 9751)]\n[New Thread 0x7fffcdae1700 (LWP 9752)]\n[New Thread 0x7fffcb2e0700 (LWP 9753)]\n[Thread 0x7fffd7ae5700 (LWP 9748) exited]\n[Thread 0x7fffd52e4700 (LWP 9749) exited]\n[Thread 0x7fffd82e6700 (LWP 9747) exited]\n[Thread 0x7fffcdae1700 (LWP 9752) exited]\n[Thread 0x7fffd02e2700 (LWP 9751) exited]\n[Thread 0x7fffd2ae3700 (LWP 9750) exited]\n[Thread 0x7fffcb2e0700 (LWP 9753) exited]\nbuilding DNN model with 5 hidden layers of 1024 hidden units\nbuilding train_op with constant learning rate 0.001\nLoading data into memory...\nali-to-post ark:- ark:- \nali-to-pdf /data/sls/scratch/wnhsu/gale_mandarin_bc_phase2/exp/tri3b_dnn_2048x5_ali/final.mdl 'ark:gunzip -c /data/sls/scratch/wnhsu/gale_mandarin_bc_phase2/exp/tri3b_dnn_2048x5_ali/ali.*.gz |' ark:- \nLOG (ali-to-pdf:main():ali-to-pdf.cc:68) Converted 76184 alignments to pdf sequences.\nLOG (ali-to-post:main():ali-to-post.cc:65) Converted 76184 alignments.\n   7804 utterances loaded...\n   avg-sequence-len = 608\n[New Thread 0x7fffcb2e0700 (LWP 9764)]\n[New Thread 0x7fffcdae1700 (LWP 9765)]\n[New Thread 0x7fffd02e2700 (LWP 9766)]\n[New Thread 0x7fffd2ae3700 (LWP 9767)]\n[New Thread 0x7fffa9dda700 (LWP 9768)]\n[New Thread 0x7fffa95d9700 (LWP 9769)]\n[New Thread 0x7fffa8dd8700 (LWP 9770)]\n[New Thread 0x7fffa5484700 (LWP 9771)]\n[New Thread 0x7fffa4c83700 (LWP 9772)]\n[Thread 0x7fffa4c83700 (LWP 9772) exited]\n\nProgram received signal SIGSEGV, Segmentation fault.\n0x00007fffed6730b8 in perftools::gputools::StreamExecutor::DeviceMemoryUsage(long long*, long long*) const ()\n   from /data/sls/scratch/wnhsu/pyenv_tf/r08/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n(gdb) backtrace\n#0  0x00007fffed6730b8 in perftools::gputools::StreamExecutor::DeviceMemoryUsage(long long*, long long*) const ()\n   from /data/sls/scratch/wnhsu/pyenv_tf/r08/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#1  0x00007fffed40e258 in tensorflow::GPUMachineManager() () from /data/sls/scratch/wnhsu/pyenv_tf/r08/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#2  0x00007fffed40c4d0 in tensorflow::BaseGPUDeviceFactory::GetValidDeviceIds(std::vector<int, std::allocator<int> >*) ()\n   from /data/sls/scratch/wnhsu/pyenv_tf/r08/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#3  0x00007fffed40cd50 in tensorflow::BaseGPUDeviceFactory::CreateDevices(tensorflow::SessionOptions const&, std::string const&, std::vector<tensorflow::Device*, std::allocator<tensorflow::Device*> >*) ()\n   from /data/sls/scratch/wnhsu/pyenv_tf/r08/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#4  0x00007fffed5e50d6 in tensorflow::DeviceFactory::AddDevices(tensorflow::SessionOptions const&, std::string const&, std::vector<tensorflow::Device*, std::allocator<tensorflow::Device*> >*) ()\n   from /data/sls/scratch/wnhsu/pyenv_tf/r08/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#5  0x00007fffed3d0b71 in tensorflow::DirectSessionFactory::NewSession(tensorflow::SessionOptions const&) ()\n   from /data/sls/scratch/wnhsu/pyenv_tf/r08/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#6  0x00007fffed6076d7 in tensorflow::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) ()\n   from /data/sls/scratch/wnhsu/pyenv_tf/r08/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#7  0x00007fffed5d60e1 in TF_NewSession () from /data/sls/scratch/wnhsu/pyenv_tf/r08/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#8  0x00007fffec839c3b in _wrap_TF_NewSession () from /data/sls/scratch/wnhsu/pyenv_tf/r08/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#9  0x000000000049968d in PyEval_EvalFrameEx ()\n#10 0x00000000004a090c in PyEval_EvalCodeEx ()\n#11 0x0000000000499a52 in PyEval_EvalFrameEx ()\n#12 0x00000000004a1c9a in ?? ()\n#13 0x00000000004dfe94 in ?? ()\n#14 0x0000000000505f96 in PyObject_Call ()\n#15 0x00000000004de41a in ?? ()\n#16 0x00000000005039eb in ?? ()\n#17 0x0000000000499be5 in PyEval_EvalFrameEx ()\n#18 0x00000000004a090c in PyEval_EvalCodeEx ()\n#19 0x000000000049ab45 in PyEval_EvalFrameEx ()\n#20 0x00000000004a1634 in ?? ()\n#21 0x000000000044e4a5 in PyRun_FileExFlags ()\n#22 0x000000000044ec9f in PyRun_SimpleFileExFlags ()\n#23 0x000000000044f904 in Py_Main ()\n#24 0x00007ffff7818f45 in __libc_start_main (main=0x44f9c2 <main>, argc=2, argv=0x7fffffffd3b8, init=<optimized out>, fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7fffffffd3a8)\n    at libc-start.c:287\n#25 0x0000000000578c4e in _start ()\n```\n", "Looking through the code, I didn't see any obvious culprit for this. I'm having a hard time reproducing it on my own.\n\n@MInner or @mhng1580 do you happen to have a simple reproducer for this which includes all the data and python commands needed to trigger the segfault?\n", "I have the similar segfault when I run cifar10_eval.py. My env is python3.5, CUDA7.5 with cuDNN 5, tensorflow 0.9, ubuntu 14.04. When I ran the code with tf 0.8, it was fine. \n\nWhen I run gdb with the code. I've got the segfault as below:\nProgram received signal SIGSEGV, Segmentation fault.\n0x00007fffebbc0011 in perftools::gputools::StreamExecutor::DeviceMemoryUsage(long long_, long long_) const ()\n   from /opt/conda/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so\n\nAnyone knows how to fix this?\n", "This definitely looks like its related to the amount of free memory available.\n\nThis is tested on: \n\n```\nOS Version:            Mac OS X 10.11.6 (15G31)\nKernel Version: Darwin 15.6.0\nGraphics: NVIDIA GeForce GTX 960, NVIDIA GeForce GTX 960, PCIe 2GB\ntensorflow version: 0.9.0 (8a4f6abb395b3f1bca732797068021c786c1ec76)\n```\n\nI'm using a monitor thats 4096x2160 with Billions of Colors and 60 Hz. This is relevant because it directly impacts the amount of free memory available on the GPU. Also just as important is how many windows are displayed on the current desktop.\n\n1) With many windows, Chrome tabs and viewing a Youtube video:\n\n```\n$ python test.py\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.7.5.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.5.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.7.5.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.7.5.dylib locally\n[1]    54037 segmentation fault  python test.py\n```\n\n2) Same desktop, closing the youtube video and some chrome windows:\n\n```\n$ python test.py                                                                                                                            139 \u21b5\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.7.5.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.5.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.7.5.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.1.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.7.5.dylib locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] OS X does not support NUMA - returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: GeForce GTX 960\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.367\npciBusID 0000:04:00.0\nTotal memory: 2.00GiB\nFree memory: 80.17MiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960, pci bus id: 0000:04:00.0)\nDevice mapping:\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX 960, pci bus id: 0000:04:00.0\nI tensorflow/core/common_runtime/direct_session.cc:175] Device mapping:\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX 960, pci bus id: 0000:04:00.0\n\nMatMul: /job:localhost/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] MatMul: /job:localhost/replica:0/task:0/gpu:0\nb: /job:localhost/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] b: /job:localhost/replica:0/task:0/gpu:0\na: /job:localhost/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] a: /job:localhost/replica:0/task:0/gpu:0\nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 80.17M (84066304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\n[[ 22.  28.]\n [ 49.  64.]]\n```\n\n3) Switching to a desktop with no windows open:\n\n```\n$ python test.py\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.7.5.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.5.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.7.5.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.1.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.7.5.dylib locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] OS X does not support NUMA - returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: GeForce GTX 960\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.367\npciBusID 0000:04:00.0\nTotal memory: 2.00GiB\nFree memory: 1.05GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960, pci bus id: 0000:04:00.0)\nDevice mapping:\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX 960, pci bus id: 0000:04:00.0\nI tensorflow/core/common_runtime/direct_session.cc:175] Device mapping:\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX 960, pci bus id: 0000:04:00.0\n\nMatMul: /job:localhost/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] MatMul: /job:localhost/replica:0/task:0/gpu:0\nb: /job:localhost/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] b: /job:localhost/replica:0/task:0/gpu:0\na: /job:localhost/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] a: /job:localhost/replica:0/task:0/gpu:0\n[[ 22.  28.]\n [ 49.  64.]]\n```\n", "Closing since this is based on old code that changed substantially in the last six months. Feel free to open a new issue if the problem persists with new code."]}, {"number": 2839, "title": "Restore a working CMake build", "body": "This includes changes from #2705, which fixed some of the breakage.\n", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "@martinwicke: This is the PR I mentioned on Friday. Do you know how to invoke @googlebot to tell it that the CLAs are valid?\n", "This is the PR you're thinking about splitting again?\n", "@martinwicke: Yep, I'm going to merge #2705 and rebase this on top of it so it contains just my changes.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@martinwicke: OK, this now contains my change only. Good to merge?\n", "Merged. Note this doesn't actually restore the cmake build.\n"]}, {"number": 2838, "title": "Get placeholder for initial state of nested RNN", "body": "For RNN cells, we get the initial state using `cell.zero_state()` and the last state after processing a sequence using `rnn.dynamic_rnn()`. However, to use the last state as the initial state for the next run, one must create a `tf.placeholder()`. As far as I know, currently there is no way to create and fill such  a placeholder (or nested tuple of placeholders) automatically. Such a feature would be very useful so that we don't have to adjust the placeholder manually when changing the RNN cell.\n", "comments": ["Is this request specifically for truncated BPTT?  or something more general?\n", "It's for both truncated BPTT and architectures using LSTM decoders. In the second case, the cells are initialized with some encoded activation. For an example see: Skip-Thought Vectors (Kiros et al. 2015).\n", "I am also interested in having a good way to remember the LSTM states for the next batch. This question was also asked by me on StackOverflow: http://stackoverflow.com/questions/38241410/tensorflow-remember-lstm-state-for-next-batch-stateful-lstm\n", "We are working on a system that makes this easy. Something should already\nin the github repo within a coupe of weeks.\nOn Jul 10, 2016 3:34 AM, \"Tom Runia\" notifications@github.com wrote:\n\n> I am also interested in having a good way to remember the LSTM states for\n> the next batch. This question was also asked by me on StackOverflow:\n> http://stackoverflow.com/questions/38241410/tensorflow-remember-lstm-state-for-next-batch-stateful-lstm\n> \n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2838#issuecomment-231582001,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/ABtimw3vtaf2mdKwtZrF-DDmMN1jgNwjks5qUMq6gaJpZM4I0pMP\n> .\n", "This might be of interest to @danijar : https://github.com/tensorflow/tensorflow/issues/2695\nGreat that you are working on things to make this easier, I will wait for an update :-)\n", "In the example [ptb_word_lm.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py), line 257\n\n```\n  state = m.initial_state.eval()\n  for step, (x, y) in enumerate(reader.ptb_iterator(data, m.batch_size,\n                                                    m.num_steps)):\n    cost, state, _ = session.run([m.cost, m.final_state, eval_op],\n                                 {m.input_data: x,\n                                  m.targets: y,\n                                  m.initial_state: state})\n```\n\nHowever, there is no placeholder for `m.ititial_state`. Why this could work?\n", "@yangzw You can feed in values for any tensor. Placeholders are just special in that they throw an error if you don't feed them while a variable would silently use its last value.\n", "We now have a comprehensive solution for truncated BPTT; introduced in 955efc90026ee08205ad84551dd4a308439fd741.  See `tf.contrib.training.batch_sequences_with_states`.  Unfortunately for now the only examples are in the unit tests.\n\nAutomatically creating nested placeholders would be useful.  I'll look into adding this.\n", "Is there example code for the `my_parser` function in the example in the [documentation](https://github.com/tensorflow/tensorflow/blob/4d579354034497d7fb58d01263dea72fc0014edd/tensorflow/g3doc/api_docs/python/functions_and_classes/shard4/tf.contrib.training.batch_sequences_with_states.md) for `batch_sequences_with_states`?\n\nI'm trying to figure it out from the documentation but am still having [questions](http://stackoverflow.com/questions/39112622/how-do-i-set-tensorflow-rnn-state-when-state-is-tuple-true).\n", "@ebrevdo : In my opinion there should be something more general. For cases in which one trains a RNN with the whole sequence being fed at once but during inference requires fetching and feeding the states on a per time-step basis, the solution right now is not very neat. Is there some hope of fetching and feeding list of state tuples for MultiRNNCell in some way.\n", "I'm current solving this by holding the state in a non-trainable variable that I initialize from the default state. The variable name is prefixed by `state/` and I have helper functions to return a dictionary from name to tensor containing all variables matching this prefix. Similarly, I have a helper function to assign variable values from this dictionary.\n\nThis is a general way to handle context, but it's not straight forward using the existing TensorFlow features. Moreover, it doesn't work with the new decision to represent LSTM states as tuples.\n\nI can contribute code to TensorFlow for a feature like this, but we should think this through first, and see that it matches TensorFlow's preferred way to handle states.\n", "@danijar: I recommend against using a non-trainable variable because this is not thread-safe (you can't run multiple inference threads against the same graph).  However, it's not too hard to create some placeholder tensors and wrap them in the necessary tuple type.  Similarly when calling a session run, one can pull out the \"next state\" tuple and store it, feeding it as an input to the next session.run.  This is decidedly more thread-safe than the variable solution (and in fact is zero-copy if you're doing this in a C++ client; though sadly not zero-copy in python since TF runtime must copy feed_dict inputs from python since python does its own memory management)\n\n@nitishgupta you can now fetch an arbitrary tuple type in python.  don't think you can feed one though (but that may have changed recently?)  since usually per-step RNN inference is meant done in a C++ client, i don't have any plans to add python sugar for this.\n\n@wpm  an example of \"my_parser\" is something that reads a serialized `SequenceExample` via a reader and deserializes it using `parse_single_sequence_example`.  The `parse_single_sequence_example` call returns `context` and `sequences` dictionaries that exactly match some of the inputs of `batch_sequences_with_states`.\n", "@danijar since session.run only supports feed_dict of the format {key: value} where key is either a Tensor or a string (the tensor's name), i'm unable to create this kind of placeholder.  However you can always use:\n\n```\nc_state = tf.placeholder(...)\nh_state = tf.placeholder(...)\ninitial_state = tf.nn.rnn_cell.LSTMStateTuple(c_state, h_state)\n\nsess.run(..., feed_dict={c_state: ..., h_state: ...})\n```\n", "@ebrevdo Thanks for the example. It would be nice to be able to interchange the cell without changing the placeholder/feed logic. That way, people can write operations that take a cell as parameter, and supports any inplementation, including use defined ones. Do you know how if that's possible?\n", "> It would be nice to be able to interchange the cell without changing the placeholder/feed logic.\r\n\r\n:+1: ", "Why is it be interesting to use the output of a run as the input of the next run? By the different runs do you mean two different batches? Do you mean two series within a batch? What if the zero_state was just random noise?", "@kovek When input sequences are too long to fit into memory, we need to chunk them. Even though we usually don't backpropagate across chunk boundaries, we want to at least carry over the hidden state of the RNN.", "I now do something like this:\r\n\r\n```python\r\nfrom tensorflow.contrib.framework import nest\r\n\r\n\r\ndef rnn_placeholders(state):\r\n    \"\"\"Convert RNN state tensors to placeholders with the zero state as default.\"\"\"\r\n    if isinstance(state, tf.contrib.rnn.LSTMStateTuple):\r\n        c, h = state\r\n        c = tf.placeholder_with_default(c, c.shape, c.op.name)\r\n        h = tf.placeholder_with_default(h, h.shape, h.op.name)\r\n        return tf.contrib.rnn.LSTMStateTuple(c, h)\r\n    elif isinstance(state, tf.Tensor):\r\n        h = state\r\n        h = tf.placeholder_with_default(h, h.shape, h.op.name)\r\n        return h\r\n    else:\r\n        structure = [rnn_placeholders(x) for x in state]\r\n        return tuple(structure)\r\n\r\n\r\nstate = rnn_placeholders(cell.zero_state(batch_size, tf.float32))\r\n\r\nfor tensor in nest.flatten(state):\r\n    tf.add_to_collection('rnn_state_input', tensor)\r\n\r\nx, new_state = tf.nn.dynamic_rnn(...)\r\n\r\nfor tensor in nest.flatten(new_state):\r\n    tf.add_to_collection('rnn_state_output', tensor)\r\n```\r\nwhich works with arbitrarily nested RNN structures and works well enough (although fetching seems wasteful) until the state managing stuff @ebrevdo seems to be working on in the seq2seq stuff (it's hinted at in around the RNN documentation, anyway) becomes available.\r\n\r\nI could do a pull request to contrib if an utility function like this would be of interest, @ebrevdo.", "@ebrevdo , does `tf.contrib.training.batch_sequences_with_states` work with sequence to sequence models, and if so, how?\r\n\r\nMy guess is that I need to create two `batch_sequences_with_state`, one for the input sequence and the other for the output sequence, and then feed one into my encoder and the other into my decoder. Is that correct?\r\n", "@danijar , maybe you know the answer?", "@RylanSchaeffer I don't know but this doesn't seem related to creating a placeholder for the RNN state. Please ask the question on StackOverflow instead.", "Here's a cleaner alternative relying more on @ebrevdo's `nest`:\r\n\r\n```python\r\nfrom tensorflow.contrib.framework import nest\r\n\r\nstate = nest.map_structure(\r\n    lambda x: tf.placeholder_with_default(x, x.shape, x.op.name),\r\n    cell.zero_state(batch_size, tf.float32))\r\n\r\nfor tensor in nest.flatten(state):\r\n    tf.add_to_collection('rnn_state_input', tensor)\r\n\r\nx, new_state = tf.nn.dynamic_rnn(...)\r\n\r\nfor tensor in nest.flatten(new_state):\r\n    tf.add_to_collection('rnn_state_output', tensor)\r\n```", "@carlthome How do you use this implementation while running the session, we still dont specify an inital state to the dynamic RNN right? Could you please ellaborate?", "The way used in PTB language modeling solves the problem. But it does not look elegant. I wonder if there is any follow-up regarding feeding the previous output of RNN to the current input. I used PyTorch for similar tasks and it seems that their ways of solving it is more elegant.", "> @yangzw You can feed in values for any tensor. Placeholders are just special in that they throw an error if you don't feed them while a variable would silently use its last value.\r\n\r\nYes. testing code:\r\n`\r\nx = tf.Variable(2)\r\ny = tf.constant(3)\r\n\r\nmul = tf.math.multiply(x, y)\r\n\r\nwith tf.Session() as s:\r\n    tf.global_variables_initializer().run()\r\n    print(s.run(mul, feed_dict={x: 6}))\r\n    print(s.run(x))\r\n`\r\n\r\nResult: \r\n18\r\n2"]}, {"number": 2837, "title": "Enable tf.abs() for SparseTensor", "body": "Enabled `tf.abs()` for `SparseTensor`. Added tests and verified locally. This partially addresses #1828.\n", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, please test this\n", "Jenkins, test this please.\n"]}, {"number": 2835, "title": "Discard Pull Request (PR)", "body": "Greetings @googlebot! Send me your Contributor License Agreement\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}]