[{"number": 32768, "title": "RHEL pip install 2.0-rc issues, No module named tensorflow_core.keras", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos 7.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.0-rc2, 2.0-rc1\r\n- Python version: 3.6.5\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nRunning pip installs the package without issues but fails to import keras when using.\r\nI have tried both gpu and cpu packages with same result.\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\npython3 -m pip install tensorflow-gpu==2.0.0-rc2\r\nfrom tensorflow import keras\r\ninputs = keras.Input(shape=(784,))\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/pyinstalls/python3.6/tensorflow/__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"/pyinstalls/python3.6/tensorflow/__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"/pyinstalls/python3.6/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\r\nModuleNotFoundError: No module named tensorflow_core.keras\r\n\r\nimport tensorflow.python.keras\r\nModuleNotFoundError: No module named tensorflow_core.python\r\n```\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["By any chance, do you have `keras_applications` and `keras_preprocessing` installed?\r\n\r\nI cannot reproduce but knowing the relevant import area these two seem like might be related.\r\n\r\n```\r\n(nodash) mihaimaruseac@ankh:/tmp/32751/nodash$ python\r\nPython 3.6.8 (default, Jan  3 2019, 03:42:36) \r\n[GCC 8.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from tensorflow import keras\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'2.0.0-rc2'\r\n```", "I do have those packages yes:\r\n```\r\nPython 3.6.5 (default, Jan 12 2018, 18:10:22) \r\n[GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux\r\nimport tensorflow as tf\r\n>>> tf.__version__\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute '__version__'\r\n>>> import keras_applications\r\n>>> keras_applications.__version__\r\n'1.0.8'\r\n>>> import keras_preprocessing\r\n>>> keras_preprocessing.__version__\r\n'1.1.0'\r\n```", "It's strange that not even `tf.__version__` works for you.\r\n\r\nCan you create a fresh environment and install tensorflow in it? Please attach all logs.\r\n\r\n`(mkdir new; virtualenv new -ppython3.6; cd new; source bin/activate; pip install tensorflow; python -c \"import tensorflow as tf; print(tf.__version__)\") > logs`", "I was missing CUDA/cudnn on my PATH, adding those then rerunning pip install fixed the issue.\r\n\r\n(pip install without CUDA on path does not install: libtensorflow_framework.so.2)", "@fjallraven,\r\nGlad its fixed. Are you happy to close this issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32768\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32768\">No</a>\n"]}, {"number": 32767, "title": "Build Error: failed (Exit 1): crosstool_wrapper_driver_is_not_gcc", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): https://github.com/tensorflow/tensorflow\r\n- TensorFlow version: Both 1.14 & 1.13\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: No\r\n- Bazel version (if compiling from source): 0.24.1 for 1.14 & 0.19.2 for 1.13\r\n- GCC/Compiler version (if compiling from source): Output of gcc --version is 7.4.0\r\n- CUDA/cuDNN version: CUDA 10.0 / cuDNN 7.5\r\n- GPU model and memory: 16 GB Volta Arch GPU\r\n\r\n\r\nWhen I try to build Tensorflow from source, I get annoying errors with both versions of 1.13 and 1.14. When I try to build 1.13 with the below command;\r\n\r\n`bazel build --config=opt --config=nonccl --verbose_failures     //tensorflow/tools/pip_package:build_pip_package     //tensorflow:libtensorflow_cc.so     //tensorflow:libtensorflow_framework.so     --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0`\r\n\r\nI get the following error after maybe 3-4 hours:\r\n\r\n```\r\nERROR: /root/tensorflow/tensorflow/contrib/tensorrt/BUILD:268:1: C++ compilation of rule '//tensorflow/contrib/tensorrt:trt_conversion' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda-10.0 \\\r\n    CUDNN_INSTALL_PATH=/usr/lib/aarch64-linux-gnu \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc-5 \\\r\n    LD_LIBRARY_PATH=/usr/local/cuda-10.0/targets/aarch64-linux/lib: \\\r\n    PATH=/root/bazel/output:/usr/local/cuda-10.0/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3.6/dist-packages \\\r\n    TENSORRT_INSTALL_PATH=/usr/lib/aarch64-linux-gnu \\\r\n    TF_CUDA_CLANG=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=7.2 \\\r\n    TF_CUDA_VERSION=10.0 \\\r\n    TF_CUDNN_VERSION=7 \\\r\n    TF_NCCL_VERSION='' \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n    TF_NEED_ROCM=0 \\\r\n    TF_TENSORRT_VERSION=5.1.6 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/aarch64-opt/bin/tensorflow/contrib/tensorrt/_objs/trt_conversion/convert_nodes.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/tensorflow/contrib/tensorrt/_objs/trt_conversion/convert_nodes.pic.o' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -DTF_USE_SNAPPY -iquote . -iquote bazel-out/aarch64-opt/genfiles -iquote bazel-out/aarch64-opt/bin -iquote external/com_google_absl -iquote bazel-out/aarch64-opt/genfiles/external/com_google_absl -iquote bazel-out/aarch64-opt/bin/external/com_google_absl -iquote external/bazel_tools -iquote bazel-out/aarch64-opt/genfiles/external/bazel_tools -iquote bazel-out/aarch64-opt/bin/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/aarch64-opt/genfiles/external/eigen_archive -iquote bazel-out/aarch64-opt/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/aarch64-opt/genfiles/external/local_config_sycl -iquote bazel-out/aarch64-opt/bin/external/local_config_sycl -iquote external/nsync -iquote bazel-out/aarch64-opt/genfiles/external/nsync -iquote bazel-out/aarch64-opt/bin/external/nsync -iquote external/gif_archive -iquote bazel-out/aarch64-opt/genfiles/external/gif_archive -iquote bazel-out/aarch64-opt/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/aarch64-opt/genfiles/external/jpeg -iquote bazel-out/aarch64-opt/bin/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/aarch64-opt/genfiles/external/protobuf_archive -iquote bazel-out/aarch64-opt/bin/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/aarch64-opt/genfiles/external/com_googlesource_code_re2 -iquote bazel-out/aarch64-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/aarch64-opt/genfiles/external/farmhash_archive -iquote bazel-out/aarch64-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/aarch64-opt/genfiles/external/fft2d -iquote bazel-out/aarch64-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/aarch64-opt/genfiles/external/highwayhash -iquote bazel-out/aarch64-opt/bin/external/highwayhash -iquote external/zlib_archive -iquote bazel-out/aarch64-opt/genfiles/external/zlib_archive -iquote bazel-out/aarch64-opt/bin/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/aarch64-opt/genfiles/external/local_config_cuda -iquote bazel-out/aarch64-opt/bin/external/local_config_cuda -iquote external/double_conversion -iquote bazel-out/aarch64-opt/genfiles/external/double_conversion -iquote bazel-out/aarch64-opt/bin/external/double_conversion -iquote external/local_config_tensorrt -iquote bazel-out/aarch64-opt/genfiles/external/local_config_tensorrt -iquote bazel-out/aarch64-opt/bin/external/local_config_tensorrt -isystem external/eigen_archive -isystem bazel-out/aarch64-opt/genfiles/external/eigen_archive -isystem bazel-out/aarch64-opt/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/aarch64-opt/genfiles/external/nsync/public -isystem bazel-out/aarch64-opt/bin/external/nsync/public -isystem external/gif_archive/lib -isystem bazel-out/aarch64-opt/genfiles/external/gif_archive/lib -isystem bazel-out/aarch64-opt/bin/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/aarch64-opt/genfiles/external/protobuf_archive/src -isystem bazel-out/aarch64-opt/bin/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/aarch64-opt/genfiles/external/farmhash_archive/src -isystem bazel-out/aarch64-opt/bin/external/farmhash_archive/src -isystem external/zlib_archive -isystem bazel-out/aarch64-opt/genfiles/external/zlib_archive -isystem bazel-out/aarch64-opt/bin/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/aarch64-opt/genfiles/external/local_config_cuda/cuda -isystem bazel-out/aarch64-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/aarch64-opt/genfiles/external/local_config_cuda/cuda/cuda/include -isystem bazel-out/aarch64-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/aarch64-opt/genfiles/external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/aarch64-opt/bin/external/local_config_cuda/cuda/cuda/include/crt -isystem external/double_conversion -isystem bazel-out/aarch64-opt/genfiles/external/double_conversion -isystem bazel-out/aarch64-opt/bin/external/double_conversion -isystem external/local_config_tensorrt/include -isystem bazel-out/aarch64-opt/genfiles/external/local_config_tensorrt/include -isystem bazel-out/aarch64-opt/bin/external/local_config_tensorrt/include '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections '-march=native' -Wno-sign-compare '-D_GLIBCXX_USE_CXX11_ABI=0' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-DGOOGLE_CUDA=1' '-DGOOGLE_TENSORRT=1' -pthread '-DGOOGLE_CUDA=1' '-DGOOGLE_TENSORRT=1' -c tensorflow/contrib/tensorrt/convert/convert_nodes.cc -o bazel-out/aarch64-opt/bin/tensorflow/contrib/tensorrt/_objs/trt_conversion/convert_nodes.pic.o)\r\ntensorflow/contrib/tensorrt/convert/convert_nodes.cc: In constructor 'tensorflow::tensorrt::convert::TRT_TensorOrWeights::TRT_TensorOrWeights(nvinfer1::DataType, const nvinfer1::Dims&, int)':\r\ntensorflow/contrib/tensorrt/convert/convert_nodes.cc:516:60: error: invalid new-expression of abstract class type 'tensorflow::tensorrt::convert::TRT_TensorOrWeights::SimpleITensor'\r\n     : simple_itensor_(new SimpleITensor(trt_dtype, trt_dims)),\r\n                                                            ^\r\ntensorflow/contrib/tensorrt/convert/convert_nodes.cc:459:28: note:   because the following virtual functions are pure within 'tensorflow::tensorrt::convert::TRT_TensorOrWeights::SimpleITensor':\r\n class TRT_TensorOrWeights::SimpleITensor : public nvinfer1::ITensor {\r\n                            ^\r\nIn file included from ./tensorflow/contrib/tensorrt/log/trt_logger.h:23:0,\r\n                 from ./tensorflow/contrib/tensorrt/convert/convert_nodes.h:26,\r\n                 from tensorflow/contrib/tensorrt/convert/convert_nodes.cc:16:\r\nbazel-out/aarch64-opt/genfiles/external/local_config_tensorrt/tensorrt/include/NvInfer.h:774:18: note: \tvirtual bool nvinfer1::ITensor::dynamicRangeIsSet() const\r\n     virtual bool dynamicRangeIsSet() const = 0;\r\n                  ^\r\nbazel-out/aarch64-opt/genfiles/external/local_config_tensorrt/tensorrt/include/NvInfer.h:779:18: note: \tvirtual void nvinfer1::ITensor::resetDynamicRange()\r\n     virtual void resetDynamicRange() = 0;\r\n                  ^\r\nbazel-out/aarch64-opt/genfiles/external/local_config_tensorrt/tensorrt/include/NvInfer.h:786:19: note: \tvirtual float nvinfer1::ITensor::getDynamicRangeMin() const\r\n     virtual float getDynamicRangeMin() const = 0;\r\n                   ^\r\nbazel-out/aarch64-opt/genfiles/external/local_config_tensorrt/tensorrt/include/NvInfer.h:793:19: note: \tvirtual float nvinfer1::ITensor::getDynamicRangeMax() const\r\n     virtual float getDynamicRangeMax() const = 0;\r\n\r\n```\r\n\r\nAnd if I try to build 1.14 with the command below;\r\n\r\n`bazel build --config=opt --config=nonccl //tensorflow/tools/pip_package:build_pip_package --verbose_failures --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"`\r\n\r\nI get the following error:\r\n\r\n```\r\nERROR: /opt/tf13/tensorflow/tensorflow/lite/kernels/BUILD:286:1: C++ compilation of rule '//tensorflow/lite/kernels:builtin_op_kernels' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/8c266c5a221eef177229796f3ca6ace6/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/usr/local/cuda-10.0/targets/aarch64-linux/lib: \\\r\n    PATH=/root/bazel/output:/usr/local/cuda-10.0/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/lite/kernels/_objs/builtin_op_kernels/depthwise_conv.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/lite/kernels/_objs/builtin_op_kernels/depthwise_conv.pic.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -iquote . -iquote bazel-out/host/genfiles -iquote bazel-out/host/bin -iquote external/gemmlowp -iquote bazel-out/host/genfiles/external/gemmlowp -iquote bazel-out/host/bin/external/gemmlowp -iquote external/com_google_absl -iquote bazel-out/host/genfiles/external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/arm_neon_2_x86_sse -iquote bazel-out/host/genfiles/external/arm_neon_2_x86_sse -iquote bazel-out/host/bin/external/arm_neon_2_x86_sse -iquote external/eigen_archive -iquote bazel-out/host/genfiles/external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/genfiles/external/local_config_sycl -iquote bazel-out/host/bin/external/local_config_sycl -iquote external/flatbuffers -iquote bazel-out/host/genfiles/external/flatbuffers -iquote bazel-out/host/bin/external/flatbuffers -iquote external/fft2d -iquote bazel-out/host/genfiles/external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/farmhash_archive -iquote bazel-out/host/genfiles/external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -isystem external/eigen_archive -isystem bazel-out/host/genfiles/external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem tensorflow/lite/schema -isystem bazel-out/host/genfiles/tensorflow/lite/schema -isystem bazel-out/host/bin/tensorflow/lite/schema -isystem external/flatbuffers/include -isystem bazel-out/host/genfiles/external/flatbuffers/include -isystem bazel-out/host/bin/external/flatbuffers/include -isystem external/farmhash_archive/src -isystem bazel-out/host/genfiles/external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 '-march=native' -g0 -DFARMHASH_NO_CXX_STRING -Wno-sign-compare '-Wno-error=reorder' -c tensorflow/lite/kernels/depthwise_conv.cc -o bazel-out/host/bin/tensorflow/lite/kernels/_objs/builtin_op_kernels/depthwise_conv.pic.o)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h: In static member function 'static void tflite::optimized_ops::depthwise_conv::PackMacroBlock<(tflite::DepthwiseConvImplementation)3, (tflite::DepthwiseConvDepthMultiplication)0, 0>::PackMacroBlockNeon(const uint8*, int8*, const tflite::optimized_ops::depthwise_conv::DepthwiseConvDotProdParams*)':\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:5981:24: note: use -flax-vector-conversions to permit conversions between vectors with differing element types or numbers of subparts\r\n           input_data_a = vld1q_u8(input_data_0);\r\n                        ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:5981:24: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:5982:24: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n           input_data_b = vld1q_u8(input_data_0 + 1 * input_depth);\r\n                        ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:5983:24: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n           input_data_c = vld1q_u8(input_data_0 + 2 * input_depth);\r\n                        ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:5984:24: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n           input_data_d = vld1q_u8(input_data_0 + 3 * input_depth);\r\n                        ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:5993:55: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n             work_reg_a = veorq_s8(work_reg_a, sign_bit);\r\n                                                       ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:5994:55: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n             work_reg_b = veorq_s8(work_reg_b, sign_bit);\r\n                                                       ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6000:26: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n             input_data_a = vld1q_u8(input_data_0);\r\n                          ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6001:26: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n             input_data_b = vld1q_u8(input_data_0 + 1 * input_depth);\r\n                          ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6009:61: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n             work_reg_a_sp = veorq_s8(work_reg_a_sp, sign_bit);\r\n                                                             ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6010:61: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n             work_reg_b_sp = veorq_s8(work_reg_b_sp, sign_bit);\r\n                                                             ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6012:26: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n             input_data_c = vld1q_u8(input_data_0 + 2 * input_depth);\r\n                          ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6013:26: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n             input_data_d = vld1q_u8(input_data_0 + 3 * input_depth);\r\n                          ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6029:53: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n           work_reg_a = veorq_s8(work_reg_a, sign_bit);\r\n                                                     ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6030:53: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n           work_reg_b = veorq_s8(work_reg_b, sign_bit);\r\n                                                     ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6042:59: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n           work_reg_a_sp = veorq_s8(work_reg_a_sp, sign_bit);\r\n                                                           ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6043:59: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n           work_reg_b_sp = veorq_s8(work_reg_b_sp, sign_bit);\r\n                                                           ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'\r\n   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)\r\n                                                                       ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6053:26: note: in expansion of macro 'vld1q_lane_s8x8'\r\n           input_data_a = vld1q_lane_s8x8(input_data_0, input_data_a, 0);\r\n                          ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'\r\n   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)\r\n                                                                       ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6055:15: note: in expansion of macro 'vld1q_lane_s8x8'\r\n               vld1q_lane_s8x8(input_data_0 + 1 * input_depth, input_data_b, 0);\r\n               ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'\r\n   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)\r\n                                                                       ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6057:15: note: in expansion of macro 'vld1q_lane_s8x8'\r\n               vld1q_lane_s8x8(input_data_0 + 2 * input_depth, input_data_c, 0);\r\n               ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'\r\n   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)\r\n                                                                       ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6059:15: note: in expansion of macro 'vld1q_lane_s8x8'\r\n               vld1q_lane_s8x8(input_data_0 + 3 * input_depth, input_data_d, 0);\r\n               ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6066:53: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n           work_reg_a = veorq_s8(work_reg_a, sign_bit);\r\n                                                     ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6067:53: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n           work_reg_b = veorq_s8(work_reg_b, sign_bit);\r\n                                                     ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6085:24: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n           input_data_c = vdupq_n_u8(kSignBit);\r\n                        ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'\r\n   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)\r\n                                                                       ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6086:26: note: in expansion of macro 'vld1q_lane_s8x8'\r\n           input_data_a = vld1q_lane_s8x8(input_data_0, input_data_a, 0);\r\n                          ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6087:24: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n           input_data_d = vdupq_n_u8(kSignBit);\r\n                        ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'\r\n   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)\r\n                                                                       ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6090:17: note: in expansion of macro 'vld1q_lane_s8x8'\r\n                 vld1q_lane_s8x8(input_data_0 + input_depth, input_data_b, 0);\r\n                 ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'\r\n   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)\r\n                                                                       ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6092:30: note: in expansion of macro 'vld1q_lane_s8x8'\r\n               input_data_c = vld1q_lane_s8x8(input_data_0 + 2 * input_depth,\r\n                              ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6099:53: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n           work_reg_a = veorq_s8(work_reg_a, sign_bit);\r\n                                                     ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6100:53: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n           work_reg_b = veorq_s8(work_reg_b, sign_bit);\r\n                                                     ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h: In static member function 'static void tflite::optimized_ops::depthwise_conv::PackMacroBlock<(tflite::DepthwiseConvImplementation)3, (tflite::DepthwiseConvDepthMultiplication)0, 1>::PackMacroBlockNeon(int32, int32, const uint8*, int8*, const tflite::optimized_ops::depthwise_conv::DepthwiseConvDotProdParams*)':\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6244:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n               input_data_a = vld1q_u8(input_data_0);\r\n                            ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6245:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n               input_data_b = vld1q_u8(input_data_0 + 1 * input_depth);\r\n                            ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6246:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n               input_data_c = vld1q_u8(input_data_0 + 2 * input_depth);\r\n                            ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6247:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n               input_data_d = vld1q_u8(input_data_0 + 3 * input_depth);\r\n                            ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6256:59: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n                 work_reg_a = veorq_s8(work_reg_a, sign_bit);\r\n                                                           ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6257:59: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n                 work_reg_b = veorq_s8(work_reg_b, sign_bit);\r\n                                                           ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6263:30: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n                 input_data_a = vld1q_u8(input_data_0);\r\n                              ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6264:30: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n                 input_data_b = vld1q_u8(input_data_0 + 1 * input_depth);\r\n                              ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6272:65: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n                 work_reg_a_sp = veorq_s8(work_reg_a_sp, sign_bit);\r\n                                                                 ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6273:65: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n                 work_reg_b_sp = veorq_s8(work_reg_b_sp, sign_bit);\r\n                                                                 ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6275:30: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n                 input_data_c = vld1q_u8(input_data_0 + 2 * input_depth);\r\n                              ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6276:30: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n                 input_data_d = vld1q_u8(input_data_0 + 3 * input_depth);\r\n                              ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6292:57: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n               work_reg_a = veorq_s8(work_reg_a, sign_bit);\r\n                                                         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6293:57: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n               work_reg_b = veorq_s8(work_reg_b, sign_bit);\r\n                                                         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6305:63: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n               work_reg_a_sp = veorq_s8(work_reg_a_sp, sign_bit);\r\n                                                               ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6306:63: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n               work_reg_b_sp = veorq_s8(work_reg_b_sp, sign_bit);\r\n                                                               ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'\r\n   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)\r\n                                                                       ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6316:30: note: in expansion of macro 'vld1q_lane_s8x8'\r\n               input_data_a = vld1q_lane_s8x8(input_data_0, input_data_a, 0);\r\n                              ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'\r\n   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)\r\n                                                                       ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6317:30: note: in expansion of macro 'vld1q_lane_s8x8'\r\n               input_data_b = vld1q_lane_s8x8(input_data_0 + 1 * input_depth,\r\n                              ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'\r\n   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)\r\n                                                                       ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6319:30: note: in expansion of macro 'vld1q_lane_s8x8'\r\n               input_data_c = vld1q_lane_s8x8(input_data_0 + 2 * input_depth,\r\n                              ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'\r\n   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)\r\n                                                                       ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6321:30: note: in expansion of macro 'vld1q_lane_s8x8'\r\n               input_data_d = vld1q_lane_s8x8(input_data_0 + 3 * input_depth,\r\n                              ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6329:57: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n               work_reg_a = veorq_s8(work_reg_a, sign_bit);\r\n                                                         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6330:57: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n               work_reg_b = veorq_s8(work_reg_b, sign_bit);\r\n                                                         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6344:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n               input_data_a = vdupq_n_u8(-input_offset);\r\n                            ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6345:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n               input_data_b = vdupq_n_u8(-input_offset);\r\n                            ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6346:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n               input_data_c = vdupq_n_u8(-input_offset);\r\n                            ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6347:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n               input_data_d = vdupq_n_u8(-input_offset);\r\n                            ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'\r\n   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)\r\n                                                                       ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6349:32: note: in expansion of macro 'vld1q_lane_s8x8'\r\n                 input_data_a = vld1q_lane_s8x8(input_data_0, input_data_a, 0);\r\n                                ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'\r\n   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)\r\n                                                                       ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6351:34: note: in expansion of macro 'vld1q_lane_s8x8'\r\n                   input_data_b = vld1q_lane_s8x8(input_data_0 + input_depth,\r\n                                  ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'\r\n   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)\r\n                                                                       ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6354:36: note: in expansion of macro 'vld1q_lane_s8x8'\r\n                     input_data_c = vld1q_lane_s8x8(\r\n                                    ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6362:57: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n               work_reg_a = veorq_s8(work_reg_a, sign_bit);\r\n                                                         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6363:57: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n               work_reg_b = veorq_s8(work_reg_b, sign_bit);\r\n                                                         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6389:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n               input_data_a = vdupq_n_u8(-input_offset);\r\n                            ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6390:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n               input_data_b = vld1q_u8(input_data_0 + 1 * input_depth);\r\n                            ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6391:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n               input_data_c = vld1q_u8(input_data_0 + 2 * input_depth);\r\n                            ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6392:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n               input_data_d = vld1q_u8(input_data_0 + 3 * input_depth);\r\n                            ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6401:59: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n                 work_reg_a = veorq_s8(work_reg_a, sign_bit);\r\n                                                           ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6402:59: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n                 work_reg_b = veorq_s8(work_reg_b, sign_bit);\r\n                                                           ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6408:30: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n                 input_data_a = vdupq_n_u8(-input_offset);\r\n                              ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6409:30: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n                 input_data_b = vld1q_u8(input_data_0 + 1 * input_depth);\r\n                              ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6417:65: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n                 work_reg_a_sp = veorq_s8(work_reg_a_sp, sign_bit);\r\n                                                                 ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6418:65: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n                 work_reg_b_sp = veorq_s8(work_reg_b_sp, sign_bit);\r\n                                                                 ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6420:30: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n                 input_data_c = vld1q_u8(input_data_0 + 2 * input_depth);\r\n                              ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6421:30: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n                 input_data_d = vld1q_u8(input_data_0 + 3 * input_depth);\r\n                              ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6437:57: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n               work_reg_a = veorq_s8(work_reg_a, sign_bit);\r\n                                                         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6438:57: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n               work_reg_b = veorq_s8(work_reg_b, sign_bit);\r\n                                                         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6450:63: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n               work_reg_a_sp = veorq_s8(work_reg_a_sp, sign_bit);\r\n                                                               ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6451:63: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n               work_reg_b_sp = veorq_s8(work_reg_b_sp, sign_bit);\r\n                                                               ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6461:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n               input_data_a = vdupq_n_u8(-input_offset);\r\n                            ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'\r\n   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)\r\n                                                                       ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6462:30: note: in expansion of macro 'vld1q_lane_s8x8'\r\n               input_data_b = vld1q_lane_s8x8(input_data_0 + 1 * input_depth,\r\n                              ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'\r\n   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)\r\n                                                                       ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6464:30: note: in expansion of macro 'vld1q_lane_s8x8'\r\n               input_data_c = vld1q_lane_s8x8(input_data_0 + 2 * input_depth,\r\n                              ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'\r\n   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)\r\n                                                                       ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6466:30: note: in expansion of macro 'vld1q_lane_s8x8'\r\n               input_data_d = vld1q_lane_s8x8(input_data_0 + 3 * input_depth,\r\n                              ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6474:57: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n               work_reg_a = veorq_s8(work_reg_a, sign_bit);\r\n                                                         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6475:57: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n               work_reg_b = veorq_s8(work_reg_b, sign_bit);\r\n                                                         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6490:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n               input_data_a = vdupq_n_u8(-input_offset);\r\n                            ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6491:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n               input_data_b = vdupq_n_u8(-input_offset);\r\n                            ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6492:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n               input_data_c = vdupq_n_u8(-input_offset);\r\n                            ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6493:28: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n               input_data_d = vdupq_n_u8(-input_offset);\r\n                            ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'\r\n   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)\r\n                                                                       ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6496:32: note: in expansion of macro 'vld1q_lane_s8x8'\r\n                 input_data_b = vld1q_lane_s8x8(input_data_0 + input_depth,\r\n                                ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:46:71: error: cannot convert 'int8x16_t {aka __vector(16) signed char}' to 'uint64x2_t {aka __vector(2) long unsigned int}' for argument '2' to 'uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)'\r\n   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)\r\n                                                                       ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6499:34: note: in expansion of macro 'vld1q_lane_s8x8'\r\n                   input_data_c = vld1q_lane_s8x8(input_data_0 + 2 * input_depth,\r\n                                  ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6506:57: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n               work_reg_a = veorq_s8(work_reg_a, sign_bit);\r\n                                                         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6507:57: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n               work_reg_b = veorq_s8(work_reg_b, sign_bit);\r\n                                                         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h: In static member function 'static void tflite::optimized_ops::depthwise_conv::PackMacroBlock<(tflite::DepthwiseConvImplementation)3, (tflite::DepthwiseConvDepthMultiplication)1, 1>::PackMacroBlockNeon(int32, int32, const uint8*, int8*, const tflite::optimized_ops::depthwise_conv::DepthwiseConvDotProdParams*)':\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6647:75: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint64x1_t {aka __vector(1) long unsigned int}' for argument '1' to 'uint64x1_t vshl_u64(uint64x1_t, int64x1_t)'\r\n       padding_mask = vshl_u64(padding_mask, vdup_n_s64(8 * copy_remaining));\r\n                                                                           ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6659:20: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n           work_reg = vld1q_u8(input_block_data + input_block_offset);\r\n                    ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6660:56: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x16_t vextq_s8(int8x16_t, int8x16_t, int)'\r\n           work_reg = vextq_s8(padding_reg, work_reg, 15);\r\n                                                        ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6661:49: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n           work_reg = veorq_s8(work_reg, sign_bit);\r\n                                                 ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6669:20: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n           work_reg =\r\n                    ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6671:49: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n           work_reg = veorq_s8(work_reg, sign_bit);\r\n                                                 ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6679:25: error: cannot convert 'uint8x8_t {aka __vector(8) unsigned char}' to 'int8x8_t {aka __vector(8) signed char}' in assignment\r\n           half_work_reg =\r\n                         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6681:70: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x8_t vget_low_s8(int8x16_t)'\r\n           half_work_reg = veor_s8(half_work_reg, vget_low_s8(sign_bit));\r\n                                                                      ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6701:25: error: cannot convert 'uint8x8_t {aka __vector(8) unsigned char}' to 'int8x8_t {aka __vector(8) signed char}' in assignment\r\n           half_work_reg =\r\n                         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6705:76: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint64x1_t {aka __vector(1) long unsigned int}' for argument '1' to 'uint64x1_t vshl_u64(uint64x1_t, int64x1_t)'\r\n               vshl_u64(half_work_reg, vdup_n_s64(-8 * (8 - copy_remaining)));\r\n                                                                            ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6707:60: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x8_t vget_low_s8(int8x16_t)'\r\n               vbsl_s8(padding_mask, vget_low_s8(padding_reg), half_work_reg);\r\n                                                            ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6709:70: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x8_t vget_low_s8(int8x16_t)'\r\n           half_work_reg = veor_s8(half_work_reg, vget_low_s8(sign_bit));\r\n                                                                      ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6729:75: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint64x1_t {aka __vector(1) long unsigned int}' for argument '1' to 'uint64x1_t vshl_u64(uint64x1_t, int64x1_t)'\r\n       padding_mask = vshl_u64(padding_mask, vdup_n_s64(8 * copy_remaining));\r\n                                                                           ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:48:67: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'int32x2_t {aka __vector(2) int}' for argument '2' to 'int32x2_t vld1_lane_s32(const int32_t*, int32x2_t, int)'\r\n   vld1_lane_s32(reinterpret_cast<const int32*>(src), reg, lane_num)\r\n                                                                   ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6741:27: note: in expansion of macro 'vld1_lane_8x4'\r\n           half_work_reg = vld1_lane_8x4(input_block_data + input_block_offset,\r\n                           ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6743:58: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x8_t vget_low_s8(int8x16_t)'\r\n           half_work_reg = vext_s8(vget_low_s8(padding_reg), half_work_reg, 7);\r\n                                                          ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6744:70: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x8_t vget_low_s8(int8x16_t)'\r\n           half_work_reg = veor_s8(half_work_reg, vget_low_s8(sign_bit));\r\n                                                                      ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'\r\n   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)\r\n                                                                ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6746:11: note: in expansion of macro 'vst1_lane_8x4'\r\n           vst1_lane_8x4(scratch_data, half_work_reg, 0);\r\n           ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:48:67: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'int32x2_t {aka __vector(2) int}' for argument '2' to 'int32x2_t vld1_lane_s32(const int32_t*, int32x2_t, int)'\r\n   vld1_lane_s32(reinterpret_cast<const int32*>(src), reg, lane_num)\r\n                                                                   ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6753:15: note: in expansion of macro 'vld1_lane_8x4'\r\n               vld1_lane_8x4(input_block_data + input_block_offset + copy_done,\r\n               ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6755:70: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x8_t vget_low_s8(int8x16_t)'\r\n           half_work_reg = veor_s8(half_work_reg, vget_low_s8(sign_bit));\r\n                                                                      ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'\r\n   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)\r\n                                                                ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6759:11: note: in expansion of macro 'vst1_lane_8x4'\r\n           vst1_lane_8x4(scratch_data + start_width + copy_done, half_work_reg,\r\n           ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:48:67: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'int32x2_t {aka __vector(2) int}' for argument '2' to 'int32x2_t vld1_lane_s32(const int32_t*, int32x2_t, int)'\r\n   vld1_lane_s32(reinterpret_cast<const int32*>(src), reg, lane_num)\r\n                                                                   ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6776:27: note: in expansion of macro 'vld1_lane_8x4'\r\n           half_work_reg = vld1_lane_8x4(\r\n                           ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6781:76: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint64x1_t {aka __vector(1) long unsigned int}' for argument '1' to 'uint64x1_t vshl_u64(uint64x1_t, int64x1_t)'\r\n               vshl_u64(half_work_reg, vdup_n_s64(-8 * (4 - copy_remaining)));\r\n                                                                            ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6783:60: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x8_t vget_low_s8(int8x16_t)'\r\n               vbsl_s8(padding_mask, vget_low_s8(padding_reg), half_work_reg);\r\n                                                            ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6785:70: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x8_t vget_low_s8(int8x16_t)'\r\n           half_work_reg = veor_s8(half_work_reg, vget_low_s8(sign_bit));\r\n                                                                      ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'\r\n   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)\r\n                                                                ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6789:11: note: in expansion of macro 'vst1_lane_8x4'\r\n           vst1_lane_8x4(scratch_data + start_width + copy_done, half_work_reg,\r\n           ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'\r\n   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)\r\n                                                                ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6798:9: note: in expansion of macro 'vst1_lane_8x4'\r\n         vst1_lane_8x4(scratch_data + start_width + copy_done, half_work_reg, 0);\r\n         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'\r\n   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)\r\n                                                                ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6799:9: note: in expansion of macro 'vst1_lane_8x4'\r\n         vst1_lane_8x4(scratch_data + start_width + copy_done + 4, half_work_reg,\r\n         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'\r\n   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)\r\n                                                                ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6801:9: note: in expansion of macro 'vst1_lane_8x4'\r\n         vst1_lane_8x4(scratch_data + start_width + copy_done + 8, half_work_reg,\r\n         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'\r\n   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)\r\n                                                                ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6803:9: note: in expansion of macro 'vst1_lane_8x4'\r\n         vst1_lane_8x4(scratch_data + start_width + copy_done + 12,\r\n         ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6818:23: error: cannot convert 'uint8x8_t {aka __vector(8) unsigned char}' to 'int8x8_t {aka __vector(8) signed char}' in assignment\r\n         half_work_reg = vdup_n_u8(-input_offset);\r\n                       ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6831:68: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x8_t vget_low_s8(int8x16_t)'\r\n         half_work_reg = veor_s8(half_work_reg, vget_low_s8(sign_bit));\r\n                                                                    ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'\r\n   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)\r\n                                                                ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6842:9: note: in expansion of macro 'vst1_lane_8x4'\r\n         vst1_lane_8x4(scratch_data_base + scratch_data_offset + 4,\r\n         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'\r\n   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)\r\n                                                                ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6844:9: note: in expansion of macro 'vst1_lane_8x4'\r\n         vst1_lane_8x4(scratch_data_base + scratch_data_offset + 8,\r\n         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'\r\n   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)\r\n                                                                ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6846:9: note: in expansion of macro 'vst1_lane_8x4'\r\n         vst1_lane_8x4(scratch_data_base + scratch_data_offset + 12,\r\n         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'\r\n   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)\r\n                                                                ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6848:9: note: in expansion of macro 'vst1_lane_8x4'\r\n         vst1_lane_8x4(scratch_data_base + scratch_data_offset + 16,\r\n         ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6857:75: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint64x1_t {aka __vector(1) long unsigned int}' for argument '1' to 'uint64x1_t vshl_u64(uint64x1_t, int64x1_t)'\r\n       padding_mask = vshl_u64(padding_mask, vdup_n_s64(8 * copy_remaining));\r\n                                                                           ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6859:57: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint8x8_t {aka __vector(8) unsigned char}' for argument '2' to 'uint8x8_t vset_lane_u8(uint8_t, uint8x8_t, int)'\r\n         padding_mask = vset_lane_u8(255, padding_mask, 0);\r\n                                                         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6864:54: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint64x1_t {aka __vector(1) long unsigned int}' for argument '1' to 'uint64x1_t vshl_n_u64(uint64x1_t, int)'\r\n           half_work_reg = vshl_n_u64(half_work_reg, 8);\r\n                                                      ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6871:54: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'int64x1_t {aka __vector(1) long int}' for argument '1' to 'int64x1_t vshl_n_s64(int64x1_t, int)'\r\n           half_work_reg = vshl_n_s64(half_work_reg, 8);\r\n                                                      ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6874:58: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x8_t vget_low_s8(int8x16_t)'\r\n             vbsl_s8(padding_mask, vget_low_s8(padding_reg), half_work_reg);\r\n                                                          ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6876:68: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x8_t vget_low_s8(int8x16_t)'\r\n         half_work_reg = veor_s8(half_work_reg, vget_low_s8(sign_bit));\r\n                                                                    ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'\r\n   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)\r\n                                                                ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6880:9: note: in expansion of macro 'vst1_lane_8x4'\r\n         vst1_lane_8x4(scratch_data_base + scratch_data_offset, half_work_reg,\r\n         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'\r\n   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)\r\n                                                                ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6888:9: note: in expansion of macro 'vst1_lane_8x4'\r\n         vst1_lane_8x4(scratch_data_base + scratch_data_offset + 4,\r\n         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'\r\n   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)\r\n                                                                ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6890:9: note: in expansion of macro 'vst1_lane_8x4'\r\n         vst1_lane_8x4(scratch_data_base + scratch_data_offset + 8,\r\n         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'\r\n   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)\r\n                                                                ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6892:9: note: in expansion of macro 'vst1_lane_8x4'\r\n         vst1_lane_8x4(scratch_data_base + scratch_data_offset + 12,\r\n         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'\r\n   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)\r\n                                                                ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6894:9: note: in expansion of macro 'vst1_lane_8x4'\r\n         vst1_lane_8x4(scratch_data_base + scratch_data_offset + 16,\r\n         ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h: In static member function 'static void tflite::optimized_ops::depthwise_conv::PackMacroBlock<(tflite::DepthwiseConvImplementation)3, (tflite::DepthwiseConvDepthMultiplication)1, 0>::PackMacroBlockNeon(int32, int32, const uint8*, int8*, const tflite::optimized_ops::depthwise_conv::DepthwiseConvDotProdParams*)':\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6993:20: error: cannot convert 'uint8x16_t {aka __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' in assignment\r\n           work_reg =\r\n                    ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:6995:49: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '2' to 'int8x16_t veorq_s8(int8x16_t, int8x16_t)'\r\n           work_reg = veorq_s8(work_reg, sign_bit);\r\n                                                 ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7002:25: error: cannot convert 'uint8x8_t {aka __vector(8) unsigned char}' to 'int8x8_t {aka __vector(8) signed char}' in assignment\r\n           half_work_reg =\r\n                         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7004:70: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x8_t vget_low_s8(int8x16_t)'\r\n           half_work_reg = veor_s8(half_work_reg, vget_low_s8(sign_bit));\r\n                                                                      ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7023:25: error: cannot convert 'uint8x8_t {aka __vector(8) unsigned char}' to 'int8x8_t {aka __vector(8) signed char}' in assignment\r\n           half_work_reg =\r\n                         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7027:76: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint64x1_t {aka __vector(1) long unsigned int}' for argument '1' to 'uint64x1_t vshl_u64(uint64x1_t, int64x1_t)'\r\n               vshl_u64(half_work_reg, vdup_n_s64(-8 * (8 - copy_remaining)));\r\n                                                                            ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7029:70: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x8_t vget_low_s8(int8x16_t)'\r\n           half_work_reg = veor_s8(half_work_reg, vget_low_s8(sign_bit));\r\n                                                                      ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:48:67: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'int32x2_t {aka __vector(2) int}' for argument '2' to 'int32x2_t vld1_lane_s32(const int32_t*, int32x2_t, int)'\r\n   vld1_lane_s32(reinterpret_cast<const int32*>(src), reg, lane_num)\r\n                                                                   ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7057:15: note: in expansion of macro 'vld1_lane_8x4'\r\n               vld1_lane_8x4(input_block_data + input_block_offset + copy_done,\r\n               ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7059:70: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x8_t vget_low_s8(int8x16_t)'\r\n           half_work_reg = veor_s8(half_work_reg, vget_low_s8(sign_bit));\r\n                                                                      ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'\r\n   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)\r\n                                                                ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7062:11: note: in expansion of macro 'vst1_lane_8x4'\r\n           vst1_lane_8x4(scratch_data + copy_done, half_work_reg, 0);\r\n           ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:48:67: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'int32x2_t {aka __vector(2) int}' for argument '2' to 'int32x2_t vld1_lane_s32(const int32_t*, int32x2_t, int)'\r\n   vld1_lane_s32(reinterpret_cast<const int32*>(src), reg, lane_num)\r\n                                                                   ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7078:27: note: in expansion of macro 'vld1_lane_8x4'\r\n           half_work_reg = vld1_lane_8x4(\r\n                           ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7083:76: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint64x1_t {aka __vector(1) long unsigned int}' for argument '1' to 'uint64x1_t vshl_u64(uint64x1_t, int64x1_t)'\r\n               vshl_u64(half_work_reg, vdup_n_s64(-8 * (4 - copy_remaining)));\r\n                                                                            ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7085:70: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x8_t vget_low_s8(int8x16_t)'\r\n           half_work_reg = veor_s8(half_work_reg, vget_low_s8(sign_bit));\r\n                                                                      ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'\r\n   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)\r\n                                                                ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7088:11: note: in expansion of macro 'vst1_lane_8x4'\r\n           vst1_lane_8x4(scratch_data + copy_done, half_work_reg, 0);\r\n           ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'\r\n   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)\r\n                                                                ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7094:9: note: in expansion of macro 'vst1_lane_8x4'\r\n         vst1_lane_8x4(scratch_data + copy_done, half_work_reg, 0);\r\n         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'\r\n   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)\r\n                                                                ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7095:9: note: in expansion of macro 'vst1_lane_8x4'\r\n         vst1_lane_8x4(scratch_data + copy_done + 4, half_work_reg, 0);\r\n         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'\r\n   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)\r\n                                                                ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7096:9: note: in expansion of macro 'vst1_lane_8x4'\r\n         vst1_lane_8x4(scratch_data + copy_done + 8, half_work_reg, 0);\r\n         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'\r\n   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)\r\n                                                                ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7097:9: note: in expansion of macro 'vst1_lane_8x4'\r\n         vst1_lane_8x4(scratch_data + copy_done + 12, half_work_reg, 0);\r\n         ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7107:54: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint64x1_t {aka __vector(1) long unsigned int}' for argument '1' to 'uint64x1_t vshl_n_u64(uint64x1_t, int)'\r\n           half_work_reg = vshl_n_u64(half_work_reg, 8);\r\n                                                      ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7114:68: error: cannot convert 'const uint8x16_t {aka const __vector(16) unsigned char}' to 'int8x16_t {aka __vector(16) signed char}' for argument '1' to 'int8x8_t vget_low_s8(int8x16_t)'\r\n         half_work_reg = veor_s8(half_work_reg, vget_low_s8(sign_bit));\r\n                                                                    ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'\r\n   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)\r\n                                                                ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7118:9: note: in expansion of macro 'vst1_lane_8x4'\r\n         vst1_lane_8x4(scratch_data_base + scratch_data_offset, half_work_reg,\r\n         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'\r\n   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)\r\n                                                                ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7124:9: note: in expansion of macro 'vst1_lane_8x4'\r\n         vst1_lane_8x4(scratch_data_base + scratch_data_offset + 4,\r\n         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'\r\n   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)\r\n                                                                ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7126:9: note: in expansion of macro 'vst1_lane_8x4'\r\n         vst1_lane_8x4(scratch_data_base + scratch_data_offset + 8,\r\n         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'\r\n   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)\r\n                                                                ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7128:9: note: in expansion of macro 'vst1_lane_8x4'\r\n         vst1_lane_8x4(scratch_data_base + scratch_data_offset + 12,\r\n         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:37:64: error: cannot convert 'int8x8_t {aka __vector(8) signed char}' to 'uint32x2_t {aka __vector(2) unsigned int}' for argument '2' to 'void vst1_lane_u32(uint32_t*, uint32x2_t, int)'\r\n   vst1_lane_u32(reinterpret_cast<uint32_t*>(dst), reg, lane_num)\r\n                                                                ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:7130:9: note: in expansion of macro 'vst1_lane_8x4'\r\n         vst1_lane_8x4(scratch_data_base + scratch_data_offset + 16,\r\n         ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n\r\nWhat is the problem here? It is so frustrating to wait 4 hours and see it explodes at the exact same point. I am also using `-flax-vector-conversions` `-fomit-frame-pointer` flags within the tensorflow/lite/tools/make/targets/aarch64_makefile.inc file but this doesn't resolve the issue. What am I missing?\r\n", "comments": ["Same problem here", "From [this issue](https://github.com/tensorflow/serving/issues/1190) I tried not compile the TensorRT (running again ./configure), and then worked fine. ", "@doruksonmez, Can you try @daviduarte's workaround and let us know if the issue still persists. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32767\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32767\">No</a>\n"]}, {"number": 32766, "title": "model_pruning: why does strip_pruning_vars always show 50% zeros?", "body": " I'm trying tensorflow/contrib/model_pruning/examples/cifar10/cifar10_train. Tensorboard shows conv1 sparsity increased to 70%\uff0cconv2 sparsity increased to 90%\uff0cbut  strip_pruning_vars alway shows 50% zeros\u3002\r\n   \r\n![image](https://user-images.githubusercontent.com/11002654/65488056-cd4b1f00-deda-11e9-92fa-9d77d009103c.png)\r\n\r\nUse `tf.compat.v1.graph_util.extract_sub_graph`\r\nI0924 14:51:55.225000 140698707568448 graph_util_impl.py:311] Froze 15 variables.\r\nI0924 14:51:55.267543 140698707568448 graph_util_impl.py:364] Converted 15 variables to const ops.\r\nI0924 14:51:55.336094 140698707568448 strip_pruning_vars_lib.py:69] conv1/weights/masked_weight has 4800 values, **50.00% zeros** \r\nI0924 14:51:55.345871 140698707568448 strip_pruning_vars_lib.py:69] conv2/weights/masked_weight has 102400 values, **50.00% zeros** \r\nI0924 14:51:55.447343 140698707568448 strip_pruning_vars_lib.py:69] local3/weights/masked_weight has 884736 values, **50.00% zeros** \r\nI0924 14:51:55.470997 140698707568448 strip_pruning_vars_lib.py:69] local4/weights/masked_weight has 73728 values, **50.00% zeros** \r\nI0924 14:51:55.495334 140698707568448 strip_pruning_vars_lib.py:69] softmax_linear/weights/masked_weight has 1920 values, **50.00% zeros** \r\nI0924 14:51:55.537974 140698707568448 strip_pruning_vars.py:73] \r\nFinal graph written to /home/terse/code/programming/tensorflow/model_pruning/cifar_pruning_stripped.pb", "comments": []}, {"number": 32765, "title": "A question about tensorflow lite", "body": " I am a machine learning beginner. And I want to know the scale of neural networks and and the number of parameters that tensorflow lite can support  so far, because I have not found a similar answer elsewhere. I look forward to your answer.", "comments": ["@GGGWB \r\n\r\nPlease, go through [TensorFlow Lite guide](https://www.tensorflow.org/lite/guide) and see if it helps you.Thanks!", "Thank you very much for your answer, but I have read the TensorFlow Lite guide carefully and still have not found the relevant answer. In your experience, taking the perceptron as an example, how many layers and how many nodes per layer can be executed on the mobile phone. Additionally, I would like to know, when will the training be implemented on mobile devices? Thank you once again for your answer.", "@GGGWB This is a stale issue.\r\n\r\nSince the issue opened, there were lot of improvements in TFlite and micro. Difficult to say how many neurons, hidden layers without knowing the target device. But, Some of the big models were converted to TFLite format and used for `object_detection`, `pose_estimation`, `speech recognition`, `segmentation` and many more applications. Please feel free to check those pre-trained models and examples https://www.tensorflow.org/lite/examples\r\n\r\nI am closing this issue. Please feel free to reopen if there are any more questions. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32765\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32765\">No</a>\n"]}, {"number": 32764, "title": "ValueError when passing tensors to keras subclass model calls", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 / Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not tested\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.0.0-rc0 / rc1 / rc2\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 10.0 / 7.6.3\r\n- GPU model and memory: Quadro K620\r\n\r\n**Describe the current behavior**\r\nUsing conditional statements on tensors passed to keras subclassed models without `@tf.function` decorator leads to ValueError Exception\r\n` ValueError: Incompatible shapes between op input and calculated input gradient.  Forward operation: Identity.  Input index: 0. Original input shape: (64, 1024).  Calculated input gradient shape: (32, 1024)`\r\nNeither `tf.cond` or python's if statement work without decorator.\r\n\r\nThe Model works fine if the call function is decorated with `@tf.function` decorator.\r\nPassing Python Boolean values instead of `tf.constant(True)` works fine, with and without `@tf.function` decorator.\r\n  \r\n**EDIT**: Eager mode works fine. (without any `@tf.function`)\r\n\r\n**Describe the expected behavior**\r\nThe model should work without mandating `@tf.function` decorator. See the code for details.\r\n\r\n**Code to reproduce the issue**\r\nCode that reproduces the problem. [Colab Link](https://colab.research.google.com/drive/1zNGK5sc3JVVveg-5Fe3JE0MVqxgd6h6d)\r\n\r\n**Other info / logs**\r\n\r\n    ValueError: in converted code:\r\n\r\n    <ipython-input-26-76bba83ea439>:12 train_step  *\r\n        gradients_mdan = tape.gradient(model_loss, model.trainable_variables)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/backprop.py:1014 gradient\r\n        unconnected_gradients=unconnected_gradients)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/imperative_grad.py:76 imperative_grad\r\n        compat.as_str(unconnected_gradients.value))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/backprop.py:138 _gradient_function\r\n        return grad_fn(mock_op, *out_grads)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/cond_v2.py:120 _IfGrad\r\n        true_graph, grads, util.unique_grad_fn_name(true_graph.name))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/cond_v2.py:395 _create_grad_func\r\n        func_graph=_CondGradFuncGraph(name, func_graph))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py:915 func_graph_from_py_func\r\n        func_outputs = python_func(*func_args, **func_kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/cond_v2.py:394 <lambda>\r\n        lambda: _grad_fn(func_graph, grads), [], {},\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/cond_v2.py:373 _grad_fn\r\n        src_graph=func_graph)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gradients_util.py:714 _GradientsHelper\r\n        (op.name, i, t_in.shape, in_grad.shape))\r\n\r\n    ValueError: Incompatible shapes between op input and calculated input gradient.  Forward operation: Identity.  Input index: 0. Original input shape: (64, 1024).  Calculated input gradient shape: (32, 1024)", "comments": ["Adding @mdanatg since this might related to auto graph.", "@sghoshjr, since you have some confusion about tf.function, let me provide some more details.\r\n\r\n1. TF function will turn a python into a tf graph, which will speed up the training performance. This also means for any value created in the body of a tf function, they will be traced and lift out from the graph. In your case, the python boolean will be convert to tensor automatically if they are created within tf.function.\r\n\r\n2. Checking against the boolean value in call(). Most likely, your model call() method is executed in the graph context, which mean the if check of the boolean tensor will probably not work as expected (eg it will just do a None check), unless the auto graph is enabled(default behavior). In the call body of your example, the \"if tf.reduce_all(....)\" is likely to be the issue here. The correct way to do it is using tf.cond(), and make sure the result from True and False branch should have the same shape (in your case they don't).\r\n\r\nI think its highly likely that auto graph converted your code, created a wrong shape gradient by relying on the True branch shape, while the execution path actually goes to False branch. The error message maybe complaining the wrong place, and we probably should update it (eg, the True/False branch should have the same shape), which should point you to fix your model.\r\n\r\nAfter I fix the shape of True/False branch, the model trains correctly.\r\n\r\nMore general comments for your code:\r\n1. For any static value that don't change, probably create them as python types in the __init__ method in model, rather than passing them around in the call().\r\n2. Annotation the outmost training function with tf.function is enough. The keras model call() method already have some handling of graph context, and it doesn't need any tf.function usually.", "I think in this case autograph is used in both cases, so the `if` statement is fine - I checked that it's converted to `tf.cond` in all cases. BTW, I get the shape error even when `source_train` is a Python boolean - can you double-check?\r\n\r\nThe gradient calculation should work even if the `tf.cond` returns different shapes, and indeed it does when `tf.function` is added. So the fact that it works with `tf.function` but not without definitely looks like a bug.\r\n\r\n@saxenasaurabh @alextp any idea where the error might be coming from?", "Code generated by Autograph is the same whenever `@tf.function` is used. As I understand, it recursively converts the child function calls as well.\r\n\r\nWhen using Python Boolean along with `if source_train is True:` py cond, it works as expected. [[this works](https://colab.research.google.com/drive/1Pte92bg40-cqdypvDhzlCfmwZTR8FSyR)]\r\n\r\nBut using tf tensors, along with `tf.equal` check does not work. It works only in eager mode, not in graph mode.\r\n\r\nAlso, in the autograph converted code, all tensors past the conditional statement had a batch size of `None`. To get the batch size, the tensors need to be returned from the function decorated with `@tf.function`.\r\n\r\nSo in my case, `l_logits` had a shape of `[None, 10]`. It should be `[32, 10]` (with source_train to False).\r\nChanging the loss function to `loss_func(l_logits[0:32], labels)` also works, even when passing tf tensors. [[this also works](https://colab.research.google.com/drive/1AY_0RcaL_ob6eiJ5ma6WNBegr19R__eg)]", "Just to be clear, the original code does work in eager mode. i.e. removing both the `@tf.function` decorator.", "The shape of `feature_slice` is different in both branches of `if`. It seems that only one branch is expected to be used during training, however the graph building code does not know this and tries to build gradient functions for both branches and it sees an inconsistency in the shape of the output gradient.\r\n\r\nI didn't dig too deep but the reason this works with adding `tf.function` is that somehow the input gradient wrt `feature_slice` has an undefined size for the batch dim in that case. So now the output gradients wrt `feature` from both branches has undefined size for the batch dim which are compatible. The incomplete shape here is probably a bug.\r\n\r\nI think you should be using a python bool here.\r\n\r\nOne thing I am unclear about is why we allow building a `tf.cond` with incompatible output shapes in the first place. I will look into that.", "@kkimdev FYI\r\n\r\nI concur with @saxenasaurabh, although there are a few separate issues. I ran multiple combinations to try to see what happens. It looks like the `else` branch of the `if` statement is incorrect, and the error that you get when that's actually executed is a bit more helpful: \"logits and labels must be broadcastable: logits_size=[64,10] labels_size=[32,10]\".\r\n\r\nBy the way, you can safely write `if source_train == False:` and it will work correctly with tensors too (you no longer need to write tf.equal). I'll use that below.\r\n\r\nHere is what I observed:\r\n  * both `tf.function` decorators off + `output = model(images, True)`:\r\n    * raises \"logits and labels must be broadcastable: logits_size=[64,10] labels_size=[32,10]\"\r\n  * both `tf.function` decorators off + `output = model(images, False)`: works\r\n  * both `tf.function` decorators off + `output = model(images, tf.constant(True))`:\r\n    * raises \"logits and labels must be broadcastable: logits_size=[64,10] labels_size=[32,10]\"\r\n  * both `tf.function` decorators off + `output = model(images, tf.constant(False))`: works\r\n  * `train_step` decorator on, `call` decorator off  + `output = model(images, True)`:\r\n    * raises \"Dimensions must be equal, but are 64 and 32 for 'softmax_cross_entropy_with_logits\"\r\n  * `train_step` decorator on, `call` decorator off  + `output = model(images, False)`: works\r\n  * `train_step` decorator on, `call` decorator off  + `output = model(images, tf.constant(True))`:\r\n    * raises \"Incompatible shapes between op input and calculated input gradient\"\r\n  * `train_step` decorator on, `call` decorator off  + `output = model(images, tf.constant(False))`:\r\n    * raises \"Incompatible shapes between op input and calculated input gradient\"\r\n  * both `tf.function` decorators on + `output = model(images, True)`:\r\n    * raises \"Dimensions must be equal, but are 64 and 32 for 'softmax_cross_entropy_with_logits\"\r\n  * both `tf.function` decorators on + `output = model(images, False)`: works\r\n  * both `tf.function` decorators on + `output = model(images, tf.constant(True))`:\r\n    * raises \"logits and labels must be broadcastable: logits_size=[64,10] labels_size=[32,10]\"\r\n  * both `tf.function` decorators on + `output = model(images, tf.constant(False))`: works\r\n\r\nSo, the conclusion is that based on whether `tf.function` is added, you get a different error message, and in one case there is an error even when the good branch would otherwise be executed. This is something that we'll need to remedy.\r\n", "The `source_train` was supposed to be used to be used to with a dataset of batch size of 64. (so, in the snippet, it should not be True). Likewise, \r\n\r\n> \"logits and labels must be broadcastable: logits_size=[64,10] labels_size=[32,10]\"\r\n\r\nits supposed to use a different dataset which has 64 batch sized labels.\r\nSee the use case here: [DANN Implementation](https://github.com/sghoshjr/Domain-Adversarial-Neural-Network/blob/master/DANN.py)\r\n\r\nAlso, slicing along the batch axis should be supported. `tf.cond` does allow partial shapes.\r\n\r\n1. Why does python boolean or `==` work, but not `tf.equal` check.\r\n2. Why does decorating both calls remove the error? Both functions are converted by autograph in each case.\r\n3. Python conditionals allows to build even models with different no. of outputs. But why does `tf.cond` require same shape of outputs.\r\n", "A few answers:\r\n 1. Ultimately it's all about the type of the expression in the `if` condition. If the condition evaluates to a Python boolean, then the `if` statement remains a Python `if`. If it evaluates to a `Tensor`, then the statement is a `tf.cond`. Check out the [autograph reference](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#if-statements) for more info. When you use `tf.equal`, its result is always a Tensor, but when you use `==`, the result is the type of `source_train`. To convince yourself, of that, add this print: `print(type(source_train == True))` and see how the type of `source_train` matches the result (and how the buggy case happens when it prints Tensor).\r\n\r\n 2. By the looks of it, that is a bug.\r\n\r\n 3. tf.cond is fine with different output shapes. But as you can see from the above, in one of the cases the gradient calculation doesn't like it - that's probably the same bug as (2).\r\n\r\nHaving `source_train` be connected to the dataset size explains things. In that case, it's probably easiest to use a Python boolean because you don't need to support both batch sizes in the same run and it avoids the bug.\r\n\r\nBTW, you can actually write that `if` statement as simply `if source_train:`. It will work in just the same way as `if source == True:`.", "Was able to reproduce the issue with TF v2.2 and TF-nightly i.e. v2.4.0-dev20200722. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/02a7b325e8148baa5e3e4b2e461abe04/32764.ipynb). Thanks!", "Unfortunately I didn't have time to work on this, un-asigning myself. For reference, the issue amounts to inconsistent error messages.", "I can able to reproduce the issue using TF version 2.5 and 2.6 nightly as well. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/8de352e91567f8fe9fd8ef459743ef73/32764.ipynb).Thanks!", "@sghoshjr Was able to reproduce the issue in TF v2.7.0,Please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/a174d38fd3ac0e53288b5f8e77052f19/32764.ipynb#scrollTo=mRConvYwwvCd).Thanks!", "Hi There,\r\n\r\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \r\n\r\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32764\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32764\">No</a>\n"]}, {"number": 32763, "title": "tf.sparse.reduce_sum slower/less memory efficient than (unsorted_)segment_sum", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes (below)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **ubuntu 16.04**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **v1.12.1-9365-gff401a6 1.15.0-dev20190821**\r\n- Python version: **3.6.9**\r\n- CUDA/cuDNN version: **10.0** / ??\r\n- GPU model and memory: **GTX-1070**\r\n\r\n**Current behavior**\r\n`tf.sparse.reduce_sum` (result of a custom kernel) **50-200x slower** and 2-5x less memory efficient compared to `tf.math.unsorted_segment_sum` implementation.\r\n\r\n**Describe the expected behavior**\r\nCustom kernel performance should be no worse than implementations in terms of other operations.\r\n\r\n**Code to reproduce the issue**\r\nA very basic implementation (without support for rank > 2 or multiple summation axes) is as follows.\r\n```python\r\ndef seg_sum(sp, axis=1, ordered=False):\r\n    \"\"\"\r\n    math.(unsorted_)segment_sum.\r\n\r\n    Args:\r\n        sp: rank 2 sparse tensor\r\n        axis: int, axis along which to sum\r\n        ordered: if True, other axis indices are assumed to be ascending.\r\n\r\n    Returns:\r\n        rank 1 dense tensor equivalent to tf.sparse.reduce_sum(sp, axis=axis)\r\n    \"\"\"\r\n    if sp.shape.ndims != 2:\r\n        raise NotImplementedError\r\n    other_axis = 0 if axis in (1, -1) else 1\r\n    if ordered:\r\n        return tf.math.segment_sum(sp.values, sp.indices[:, other_axis])\r\n    else:\r\n        return tf.math.unsorted_segment_sum(sp.values,\r\n                                            sp.indices[:, other_axis],\r\n                                            sp.dense_shape[other_axis])\r\n```\r\nExtending this to support full functionality should not be difficult.\r\n\r\nBasic testing/benchmarking script:\r\n```python\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef get_data(dense_shape, mean_edges=100, seed=123):\r\n    \"\"\"Generate random sparse matrix data.\"\"\"\r\n    N, M = dense_shape\r\n    r = np.random.RandomState(seed)\r\n\r\n    num_edges = int(mean_edges * N)\r\n\r\n    flat_index = (r.uniform(size=num_edges) * N * M).astype(np.int64)\r\n    # flat_index = np.concatenate([flat_index, np.arange(0, N * M, M)], axis=0)\r\n    flat_index = np.array(sorted(set(flat_index)), dtype=np.int64)\r\n    i, j = np.unravel_index(flat_index, (N, M))  # pylint: disable=unbalanced-tuple-unpacking\r\n    sparse_indices = np.stack((i, j), axis=-1)\r\n    weights = np.random.uniform(size=(i.shape[0],)).astype(np.float32)\r\n    return sparse_indices, weights\r\n\r\n\r\ndef sparse_sum(sp, axis=1):\r\n    \"\"\"sparse.reduce_sum.\"\"\"\r\n    return tf.sparse.reduce_sum(sp, axis=axis)\r\n\r\n\r\ndef sparse_sum2(sp, axis=1):\r\n    \"\"\"sparse.reduce_sum_sparse -> to_dense.\"\"\"\r\n    return tf.sparse.to_dense(tf.sparse.reduce_sum_sparse(sp, axis=axis))\r\n\r\n\r\ndef seg_sum(sp, axis=1, ordered=False):\r\n    \"\"\"\r\n    math.(unsorted_)segment_sum.\r\n\r\n    Args:\r\n        sp: rank 2 sparse tensor\r\n        axis: int, axis along which to sum\r\n        ordered: if True, other axis indices are assumed to be ascending.\r\n\r\n    Returns:\r\n        rank 1 dense tensor equivalent to tf.sparse.reduce_sum(sp, axis=axis)\r\n    \"\"\"\r\n    if sp.shape.ndims != 2:\r\n        raise NotImplementedError\r\n    other_axis = 0 if axis in (1, -1) else 1\r\n    if ordered:\r\n        return tf.math.segment_sum(sp.values, sp.indices[:, other_axis])\r\n    else:\r\n        return tf.math.unsorted_segment_sum(sp.values,\r\n                                            sp.indices[:, other_axis],\r\n                                            sp.dense_shape[other_axis])\r\n\r\n\r\ndef compare(dense_shape, axis=1, ordered=False, **kwargs):\r\n    sparse_indices, weights = get_data(dense_shape, **kwargs)\r\n    sparse_indices = tf.constant(sparse_indices, dtype=tf.int64)\r\n    weights = tf.constant(weights, dtype=tf.float32)\r\n    sp = tf.SparseTensor(sparse_indices, weights, dense_shape)\r\n    sparse = sparse_sum(sp, axis=axis)\r\n    sparse2 = sparse_sum2(sp, axis=axis)\r\n    seg = seg_sum(sp, axis=axis, ordered=ordered)\r\n    sparse_grad, = tf.gradients(sparse, weights)\r\n    # sparse2_grad, = tf.gradients(sparse2, weights)\r\n    seg_grad, = tf.gradients(seg, weights)\r\n    err = tf.reduce_max(tf.abs(seg - sparse))\r\n    shape_err = tf.reduce_max(tf.abs(tf.shape(sparse) - tf.shape(seg)))\r\n    err2 = tf.reduce_max(tf.abs(seg - sparse2))\r\n    shape_err2 = tf.reduce_max(tf.abs(tf.shape(sparse2) - tf.shape(seg)))\r\n    grad_err = tf.reduce_max(tf.abs(sparse_grad - seg_grad))\r\n    # grad_err2 = tf.reduce_max(tf.abs(sparse2_grad - seg_grad))\r\n\r\n    with tf.Session() as sess:\r\n        err, shape_err, err2, shape_err2, grad_err = sess.run(\r\n            (err, shape_err, err2, shape_err2, grad_err))\r\n    assert (err < 1e-4)\r\n    assert (shape_err == 0)\r\n    assert (shape_err2 == 0)\r\n    assert (grad_err < 1e-4)\r\n    return err\r\n\r\n\r\ndef run_benchmarks(dense_shape, axis=1, ordered=False, **kwargs):\r\n    sparse_indices, weights = get_data(dense_shape, **kwargs)\r\n    sparse_indices = tf.constant(sparse_indices, dtype=tf.int64)\r\n    weights = tf.constant(weights, dtype=tf.float32)\r\n    sp = tf.SparseTensor(sparse_indices, weights, dense_shape)\r\n    sparse = sparse_sum(sp, axis=axis)\r\n    sparse_grad, = tf.gradients(sparse, weights)\r\n\r\n    seg = seg_sum(sp, axis=axis, ordered=ordered)\r\n    seg_grad = tf.gradients(seg, weights)\r\n\r\n    sparse2 = sparse_sum2(sp, axis=axis)\r\n    # sparse2_grad, = tf.gradients(sparse2, weights)\r\n    # print(sparse2_grad)\r\n    names = []\r\n    time = []\r\n    mem = []\r\n\r\n    def update(name, result):\r\n        time.append(result['wall_time'])\r\n        mem.append(result['extras']['allocator_maximum_num_bytes_GPU_0_bfc'])\r\n        names.append(name)\r\n\r\n    with tf.Session() as sess:\r\n        print('------------------')\r\n        print('----- SPARSE -----')\r\n        bm = tf.test.Benchmark()\r\n        result = bm.run_op_benchmark(sess, (sparse, sparse_grad))\r\n        update('sparse', result)\r\n        # # no gradients to sparse2 - unfair comparison\r\n        print('------------------')\r\n        print('----- SPARSE2 -----')\r\n        bm = tf.test.Benchmark()\r\n        result = bm.run_op_benchmark(sess, (sparse2,))\r\n        update('sparse2', result)\r\n        print('------------------')\r\n        print('----- SEG ----')\r\n        bm = tf.test.Benchmark()\r\n        result = bm.run_op_benchmark(sess, (seg, seg_grad))\r\n        update('seg', result)\r\n\r\n    time = np.array(time)\r\n    i = np.argmin(time)\r\n    best_time = time[i]\r\n    print('Fastest:  {}, {}'.format(names[i], best_time))\r\n\r\n    mem = np.array(mem)\r\n    j = np.argmin(mem)\r\n    best_mem = mem[j]\r\n    print('Smallest: {}, {:.2f}mb'.format(names[j], best_mem / (1024**2)))\r\n\r\n    print('rel time, rel mem, name')\r\n    for name, t, m in zip(names, time, mem):\r\n        print('{:.3f}, {:.3f}, {}'.format(t / best_time, m / best_mem, name))\r\n    return time, mem, names\r\n\r\n\r\naxis = 0\r\nordered = False\r\ndense_shape = (int(1e4), int(1e6))\r\ncompare(axis=axis, dense_shape=dense_shape, ordered=ordered)\r\nrun_benchmarks(axis=axis, dense_shape=dense_shape, ordered=ordered)\r\n\r\n```\r\n\r\n**Other info / logs**\r\n```\r\n------------------\r\n----- SPARSE -----\r\n2019-09-24 16:03:05.305709: I tensorflow/core/profiler/lib/profiler_session.cc:205] Profiler session started.\r\nentry {\r\n  name: \"TensorFlowBenchmark.run_op_benchmark\"\r\n  iters: 10\r\n  wall_time: 0.2983849048614502\r\n  extras {\r\n    key: \"allocator_maximum_num_bytes_GPU_0_bfc\"\r\n    value {\r\n      double_value: 23998744.0\r\n    }\r\n  }\r\n  extras {\r\n    key: \"allocator_maximum_num_bytes_cpu\"\r\n    value {\r\n      double_value: 4000000.0\r\n    }\r\n  }\r\n  extras {\r\n    key: \"allocator_maximum_num_bytes_gpu_host_bfc\"\r\n    value {\r\n      double_value: 4.0\r\n    }\r\n  }\r\n}\r\n\r\n------------------\r\n----- SPARSE2 -----\r\nentry {\r\n  name: \"TensorFlowBenchmark.run_op_benchmark\"\r\n  iters: 10\r\n  wall_time: 0.32063305377960205\r\n  extras {\r\n    key: \"allocator_maximum_num_bytes_GPU_0_bfc\"\r\n    value {\r\n      double_value: 19998956.0\r\n    }\r\n  }\r\n  extras {\r\n    key: \"allocator_maximum_num_bytes_cpu\"\r\n    value {\r\n      double_value: 11582884.0\r\n    }\r\n  }\r\n}\r\n\r\n------------------\r\n----- SEG ----\r\nentry {\r\n  name: \"TensorFlowBenchmark.run_op_benchmark\"\r\n  iters: 10\r\n  wall_time: 0.0016645193099975586\r\n  extras {\r\n    key: \"allocator_maximum_num_bytes_GPU_0_bfc\"\r\n    value {\r\n      double_value: 7999788.0\r\n    }\r\n  }\r\n}\r\n\r\nFastest:  seg, 0.0016645193099975586\r\nSmallest: seg, 7.63mb\r\nrel time, rel mem, name\r\n179.262, 3.000, sparse\r\n192.628, 2.500, sparse2\r\n1.000, 1.000, seg\r\n```\r\n", "comments": ["Please find the gist of colab  for[ TF-15.0rc1](https://colab.sandbox.google.com/gist/oanush/1d1de9cd803a5d388d4c44f2034a6697/32763.ipynb) and TF-15.0 when tried executing the code.\r\nThanks!", "Any updates on this? Testing in TF 2.3 and sparse reduce_sum is excruciatingly slow, 50ms and I only have about 50k elements. @jackd Can your workaround be extended to higher dimensions? I'm using 6D tensors", "@atyshka what axis are you reducing over? If it's the first or last it might be easiest just to reshape to rank-2, use the above, then reshape back?", "Actually not one but the last two axes, which complicates things a little", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 32762, "title": "ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none) ERROR: No matching distribution found for tensorflow", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["this is the error i'm getting for installation of tensorflow library on windows 64 bit. ", "@nolanding ,\r\nPlease refer this [link](https://stackoverflow.com/questions/42317075/tensorflow-r1-0-could-not-a-find-a-version-that-satisfies-the-requirement-tens) and check if it helps resolving the problem, meanwhile also fill the standard template data.\r\nThanks!", "@nolanding ,\r\nAny update on the issue ?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32762\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32762\">No</a>\n", "I was using python 2 and found that in Windows on python 2 tensor flow does not supports."]}, {"number": 32761, "title": " keras.metrics.Accuracy != keras.metrics.accuracy", "body": "The following test \r\n```\r\n@test_util.run_all_in_graph_and_eager_modes\r\nclass KerasAccuracyTest(test.TestCase):\r\n\r\n def test_accuracy_vs_accuracy(self):\r\n    y_true = constant_op.constant([1, 0, 1])\r\n    y_pred = constant_op.constant([0.8, 0.1, 0.9])\r\n    ret_a_tensor = metrics.accuracy(y_true, y_pred)\r\n    ret_a = np.mean(self.evaluate(ret_a_tensor))\r\n    \r\n    acc_obj = metrics.Accuracy(name='my_acc')\r\n    self.evaluate(variables.variables_initializer(acc_obj.variables))\r\n    update_op = acc_obj.update_state(y_true, y_pred)\r\n    self.evaluate(update_op)\r\n    ret_b = self.evaluate(acc_obj.result())\r\n    self.assertEqual(ret_a, ret_b)\r\n```\r\n\r\ndoes not pass because \r\nhttps://github.com/tensorflow/tensorflow/blob/3d5e79e08ae299812e0eaf6183f4886591e932bd/tensorflow/python/keras/metrics.py#L576-L577\r\n\r\nseems to be doing two casts they should not.\r\n\r\nAlso it is unclear to me what is the expected value for `ret_a` (which I would have understood to be 1, but currently returns 1/3).\r\n\r\n\r\n\r\n\r\n", "comments": ["yes right\r\n", "no wrong\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32761\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32761\">No</a>\n", "Hi There,\r\n\r\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \r\n\r\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32761\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32761\">No</a>\n"]}, {"number": 32760, "title": "TensorFlow Lite save  from_session error", "body": "xs = tf.placeholder(tf.float32, [10, datalen], name='input')\r\npredic_val = tf.argmax(graph, 1, name='output')\r\nconverter = tf.lite.TFLiteConverter.from_session(sess, [xs], [predic_val])\r\ntflite_model = converter.convert()\r\nopen('%s/twave_%d.lite' % (path,i), \"wb\").write(tflite_model)\r\n\r\n\r\n\r\n2019-09-24 11:30:42.542541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-09-24 11:30:42.542549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-09-24 11:30:42.542557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-09-24 11:30:42.546249: E tensorflow/core/grappler/grappler_item_builder.cc:656] Init node is_training/Assign doesn't exist in graph\r\nSee console for info.\r\n2019-09-24 11:30:46.310416: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 153 operators, 209 arrays (0 quantized)\r\n2019-09-24 11:30:46.311793: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 153 operators, 209 arrays (0 quantized)\r\n2019-09-24 11:30:46.311861: F ./tensorflow/lite/toco/model.h:352] Check failed: dims_.size() > i (0 vs. 0)\r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x00007fb218c41740 (most recent call first):\r\n  File \"/opt/python366/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 52 in execute\r\n  File \"/opt/python366/lib/python3.6/site-packages/absl/app.py\", line 250 in _run_main\r\n  File \"/opt/python366/lib/python3.6/site-packages/absl/app.py\", line 299 in run\r\n  File \"/opt/python366/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py\", line 40 in run\r\n  File \"/opt/python366/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 89 in main\r\n  File \"/opt/python366/bin/toco_from_protos\", line 10 in <module>\r\n\r\n\r\n\r\n2019-09-24 11:31:03.164355: W tensorflow/core/kernels/queue_base.cc:277] _4_batch/fifo_queue: Skipping cancelled enqueue attempt with queue not closed\r\n2019-09-24 11:31:03.164499: W tensorflow/core/kernels/queue_base.cc:277] _0_input_producer: Skipping cancelled enqueue attempt with queue not closed\r\n2019-09-24 11:31:03.164595: W tensorflow/core/kernels/queue_base.cc:277] _3_input_producer_1: Skipping cancelled enqueue attempt with queue not closed\r\n2019-09-24 11:31:03.164863: W tensorflow/core/kernels/queue_base.cc:277] _1_shuffle_batch/random_shuffle_queue: Skipping cancelled enqueue attempt with queue not closed\r\n2019-09-24 11:31:03.164932: W tensorflow/core/kernels/queue_base.cc:277] _1_shuffle_batch/random_shuffle_queue: Skipping cancelled enqueue attempt with queue not closed\r\n2019-09-24 11:31:03.164955: W tensorflow/core/kernels/queue_base.cc:277] _1_shuffle_batch/random_shuffle_queue: Skipping cancelled enqueue attempt with queue not closed\r\n2019-09-24 11:31:03.164971: W tensorflow/core/kernels/queue_base.cc:277] _1_shuffle_batch/random_shuffle_queue: Skipping cancelled enqueue attempt with queue not closed\r\n2019-09-24 11:31:03.164986: W tensorflow/core/kernels/queue_base.cc:277] _1_shuffle_batch/random_shuffle_queue: Skipping cancelled enqueue attempt with queue not closed\r\nINFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Enqueue operation was cancelled", "comments": ["the TOCO converter will be deprecated. Have you tried the new MLIR-based converter?\r\n\r\nplease refer to https://www.tensorflow.org/lite/convert/python_api  for more info.", "@linyia01,\r\nIs this still an issue?\r\n\r\nPlease take a look at the above comment and let us know if you have any further queries. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32760\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32760\">No</a>\n"]}, {"number": 32759, "title": "call custom op using c++", "body": "I built a very simple custom op `zero_out` and try to run it using c++, the question is how to call this op, there is no doc about it.\r\n\r\ncode of custom op `zero_out`\r\n```\r\n#include \"tensorflow/core/framework/op.h\"\r\n#include \"tensorflow/core/framework/shape_inference.h\"\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\nusing namespace tensorflow;\r\n\r\nREGISTER_OP(\"ZeroOut\")\r\n.Input(\"to_zero: float\")\r\n.Output(\"zeroed: float\")\r\n.SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\r\nc->set_output(0, c->input(0));\r\nreturn Status::OK();\r\n});\r\n\r\nclass ZeroOutOp : public OpKernel {\r\npublic:\r\n    explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}\r\n\r\n    void Compute(OpKernelContext* context) override {\r\n        const Tensor& input_tensor = context->input(0);\r\n        auto input = input_tensor.flat<float>();\r\n        Tensor* output_tensor = NULL;\r\n        OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),                                                     &output_tensor));\r\n        auto output_flat = output_tensor->flat<float>();\r\n        const int N = input.size();\r\n        for (int i = 1; i < N; i++)\r\n            output_flat(i) = 0;\r\n        if (N > 0) output_flat(0) = input(0);\r\n    }\r\n};\r\n\r\nREGISTER_KERNEL_BUILDER(Name(\"ZeroOut\").Device(DEVICE_CPU), ZeroOutOp);\r\n```\r\nAfter building, I got `zero_out.so`, then I load the lib using c++\r\n```\r\n    TF_Status* status_load = TF_NewStatus();\r\n    TF_Library* lib_handle = TF_LoadLibrary(\"libzero_out.so\", status_load);\r\n    TF_Code code = TF_GetCode(status_load);\r\n    cout << \"code: \" << code << endl;  // output: 0\r\n\r\n    TF_Buffer op_list_buf = TF_GetOpList(lib_handle);\r\n    tensorflow::OpList op_list;\r\n    op_list.ParseFromArray(op_list_buf.data, op_list_buf.length);\r\n    cout << \"oplist size = \" << op_list.op_size() << endl;    // output: 1\r\n    cout << \"oplist name = \" << op_list.op(0).name() << endl;    // output: ZeroOut\r\n```\r\nI think the op should have been registered, then how to invoke the op function, Could you give some examples or code snippet?\r\n", "comments": ["To use the op in Python, See https://www.tensorflow.org/guide/extend/op#use_the_op_in_python\r\nFor invoking the op in C++, you may refer this [thread](https://stackoverflow.com/questions/50799510/how-to-run-custom-gpu-tensorflowop-from-c-code)", "@ymodak yes, it is worked when using python, but how to invoke the op in C++ is not solved in your link. In short, when you add and register an op, there is no header file to include, how to call the op function, maybe parsed in `TF_GetOpList`, but no clear doc or example for that", "core dump when using SetShapeFn under tf2.0.", "I don't think you are supposed to directly call any of the c++ ops, custom ops or not. But you can call it through the `Session` API, following https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/public/README.md#c-api-example\r\n\r\nYou might also be able to use eager's API `TFE_Execute`, but you'll still need to create context manually.\r\n", "Closing this issue. Feel free to re-open if you have further questions!"]}, {"number": 32758, "title": "Use static python_version dependency configuration in setup.py", "body": "It's related to #31983\r\n\r\nUsing static python version dependency description is better at least for poetry,\r\nlike `backports.weakref` or `enum34` has one.", "comments": ["This should be opened against master, merged there and later cherry-picked into the release branch.\r\n\r\nOn the other hand, the cherry-pick window for 2.0 is mostly closed, don't think this will get in there. But if it is on master it will be picked up in 2.1", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32758) for more info**.\n\n<!-- need_author_consent -->", "What should i do with \"CLAs are signed, but unable to verify author consent\"?", "@aflc I think the commit was authored/committed with different user email/account. You can change the email from the one lined to @roy-freee to the email linked to @aflc and push again, that will work I believe.\r\n", "Most likely, your commit uses a different email address than your\nGitHub account (on mobile, haven't checked). Amending the commit might be\neasiest.\n\nOn Tue, Sep 24, 2019, 17:56 Hiroyuki Tanaka <notifications@github.com>\nwrote:\n\n> What should i do with \"CLAs are signed, but unable to verify author\n> consent\"?\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/32758?email_source=notifications&email_token=AAEM57JE2SRJ2USFOGAJYE3QLKZKJA5CNFSM4IZYRCZ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD7QHG6I#issuecomment-534803321>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAEM57O6PV2S7URLSZOGGS3QLKZKJANCNFSM4IZYRCZQ>\n> .\n>\n", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32758) for more info**.\n\n<!-- ok -->", "Thanks @martinwicke!", "> This should be opened against master, merged there and later cherry-picked into the release branch.\r\n> \r\n> On the other hand, the cherry-pick window for 2.0 is mostly closed, don't think this will get in there. But if it is on master it will be picked up in 2.1\r\n\r\nThe issue is fixed starting from [2.1.0rc](https://github.com/tensorflow/tensorflow/releases/tag/v2.1.0-rc2). Is there any plan to cherry-pick commit to make tensorflow ```1.15``` work with poetry ?", "Please make a PR and assign to me. We will merge it only if we do a patch release, but it's better to have it there."]}, {"number": 32757, "title": "Explicit python version dependency", "body": "It's related to https://github.com/tensorflow/tensorflow/issues/31983\r\n\r\nUsing static  python version dependency description is better at least for poetry,\r\nlike `backports.weakref` or `enum34` has one.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32757) for more info**.\n\n<!-- need_sender_cla -->", "Sorry, wrong user and i will recreate another one.", "I opened #32758"]}, {"number": 32756, "title": "[r1.15-CherryPick]:Fix presubmits for 1.15", "body": "", "comments": []}, {"number": 32755, "title": "Bincount Op test \"test_negative\" fails with TF 2.0", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 16.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**\r\n- TensorFlow installed from (source or binary): **Binary**\r\n- TensorFlow version (use command below): **2.0.0-rc1**\r\n- Python version: **3.6**\r\n- Bazel version (if compiling from source): **N/A**\r\n- GCC/Compiler version (if compiling from source): **N/A**\r\n- CUDA/cuDNN version: **10.0**\r\n- GPU model and memory: **RTX 2080 Ti**, **11G**\r\n\r\n**Describe the current behavior**\r\nThe test_negative test in `tensorflow/python/kernel_tests/bincount_op_test.py` fails, as the bincount call with negative values does not throw an InvalidArgumentError. \r\n\r\nThis behavior might be the result of the op being called on the GPU, as only the CPU call is expected to throw the error, as per the comment here: https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/kernel_tests/bincount_op_test.py#L107.\r\n\r\nSetting CUDA_VISIBLE_DEVICES to be empty forces the op to run on CPU and the test passes (the invalid argument error is successfully thrown), but passing `use_gpu=False` as an option to the session wrapper does not have this effect.\r\n\r\n**Describe the expected behavior**\r\nThe test_negative test should pass, as the call to bincount with a negative input value is expected to throw an InvalidArgumentError. \r\n\r\n**Code to reproduce the issue**\r\nRun the python test `tensorflow/python/kernel_tests/bincount_op_test.py`.\r\n", "comments": ["@MattConley, Will it be possible to provide the sample standalone code to replicate the issue. Thanks!", "@gadagashwini Unfortunately the minimal repro I have at the moment is the bincount_op_test itself, specifically `test_negative` (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/bincount_op_test.py#L106-L110).\r\nThe issue might be with the test framework itself, as internal session wrappers are used even with TF 2.0.\r\n\r\nAlso, setting CUDA_VISIBLE_DEVICES to be empty forces the op to run on CPU and the test passes (the invalid argument error is successfully thrown), but passing `use_gpu=False` as an option to the session wrapper does not have this effect.", "I was able to reproduce the error reported in TF 2.0 nightly version ```'2.0.0-dev20190924'```\r\n```python\r\nOP_REQUIRES failed at bincount_op.cc:111 : Invalid argument: Input arr must be non-negative!\r\n```", "@reedwm can you take a look at this?\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32755\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32755\">No</a>\n"]}, {"number": 32754, "title": "[r2.0-CherryPick]:Fix presubmits for 2.0", "body": "", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32754) for more info**.\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32754) for more info**.\n\n<!-- cla_yes -->"]}, {"number": 32753, "title": "Fix TensorListResize being incorrectly marked as stateful", "body": "cc @reedwm ", "comments": []}, {"number": 32752, "title": "enter", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n# Copy and paste here\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 32751, "title": "tensorflow-gpu 2.0 pip package outdated/missing/name wrong", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/install/pip\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe current pip package that exists on PyPi is `tensorflow-gpu==2.0.0rc2`. The documentation says `tensorflow-gpu==2.0.0-rc1`, which is 2 typos.\r\n\r\nThe command to install it is `pip install tensorflow-gpu==2.0.0rc2`.\r\n\r\nThe documents say `pip install tensorflow-gpu==2.0.0-rc1`, which is two typos.\r\n\r\nThis bug has existed for all release candidates.\r\n\r\n```python\r\nERROR: Could not find a version that satisfies the requirement tensorflow-gpu==dont_exist (from versions: 0.12.1, 1.0.0, 1.0.1, 1.1.0rc1, 1.1.0rc2, 1.1.0, 1.2.0rc0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.3.0rc0, 1.3.0rc1, 1.3.0rc2, 1.3.0, 1.4.0rc0, 1.4.0rc1, 1.4.0, 1.4.1, 1.5.0rc0, 1.5.0rc1, 1.5.0, 1.5.1, 1.6.0rc0, 1.6.0rc1, 1.6.0, 1.7.0rc0, 1.7.0rc1, 1.7.0, 1.7.1, 1.8.0rc0, 1.8.0rc1, 1.8.0, 1.9.0rc0, 1.9.0rc1, 1.9.0rc2, 1.9.0, 1.10.0rc0, 1.10.0rc1, 1.10.0, 1.10.1, 1.11.0rc0, 1.11.0rc1, 1.11.0rc2, 1.11.0, 1.12.0rc0, 1.12.0rc1, 1.12.0rc2, 1.12.0, 1.12.2, 1.12.3, 1.13.0rc0, 1.13.0rc1, 1.13.0rc2, 1.13.1, 1.13.2, 1.14.0rc0, 1.14.0rc1, 1.14.0, 1.15.0rc0, 1.15.0rc1, 2.0.0a0, 2.0.0b0, 2.0.0b1, 2.0.0rc0, 2.0.0rc1, 2.0.0rc2)\r\n```\r\n", "comments": ["Btw, both `tensorflow-gpu==2.0.0-rc2` and `tensorflow-gpu==2.0.0rc2` works.\r\n\r\nFor that matter, even `pip install tensorflow-gpu==2.0.0----rc2` works.", "In my tests `-rc2` did not work. Nor did previous release candidates. With the dash has never worked, there hasn\u2019t been an asset with a dash on pypi. I listed the available pypi packages above, there is no dash.\r\n\r\nI\u2019m using pip with late model Anaconda Python 3/a conda environment.", "This is most likely an issue with the pip version.\r\n\r\nOn my system I am able to install both with dash and without dash\r\n\r\n```console\r\n(dash) mihaimaruseac@ankh:/tmp/32751/dash$ pip install -q tensorflow-gpu==2.0.0-rc2\r\n(dash) mihaimaruseac@ankh:/tmp/32751/dash$ echo $?\r\n0\r\n(dash) mihaimaruseac@ankh:/tmp/32751/dash$ python -c \"import tensorflow as tf; print(tf.__path__)\"\r\n['/tmp/32751/dash/lib/python3.6/site-packages/tensorflow']\r\n\r\n# change environment\r\n\r\n(nodash) mihaimaruseac@ankh:/tmp/32751/nodash$ pip install -q tensorflow-gpu==2.0.0rc2\r\n(nodash) mihaimaruseac@ankh:/tmp/32751/nodash$ echo $?\r\n0\r\n(nodash) mihaimaruseac@ankh:/tmp/32751/nodash$ python -c \"import tensorflow as tf; print(tf.__path__)\"\r\n['/tmp/32751/nodash/lib/python3.6/site-packages/tensorflow']\r\n```\r\n\r\nI am using pip 19.2.3 which seems to be the latest (`pip install --upgrade pip` doesn't bring anything new).\r\n\r\nAs such, closing the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32751\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32751\">No</a>\n", "@mihaimaruseac I have the same pip version as you. That you can\u2019t reproduce it doesn\u2019t mean it doesn\u2019t exist - look at the versions available on the pypi page for tensorflow-gpu and you\u2019ll see the one with the dashes is missing. This seems to be proof of the problem?\r\n\r\nI pasted the versions available in the ticket, check them out.", "See https://github.com/pypa/pip/issues/5021", "@mihaimaruseac I have the latest version of pip and this is happening. Given that you know this is a bug, why have you closed this ticket? Shouldn't you leave it open until the bug in the parent library is resolved? You could also change the package name to remove.\r\n\r\nI don't get how the above bug explains this having no dash:\r\n\r\nSee: https://pypi.org/project/tensorflow-gpu/2.0.0rc2/", "I closed the bug because this is not an issue that tensorflow devs need to handle. Leaving it open will just drown out other issues that we should be working on. Afaik, it is standard to deduplicate issues and close those that are caused from upstream, favoring the upstream issue.", "@mihaimaruseac Just so I understand you correctly, the reason pypi lists no package with a dash is because of this bug? And pip removes the dash automatically?\r\n\r\nI just want to verify this is why https://pypi.org/project/tensorflow-gpu/2.0.0rc2/ works and https://pypi.org/project/tensorflow-gpu/2.0.0-rc2/ does not, it redirects.", "See [the corresponding section in PEP 440](https://www.python.org/dev/peps/pep-0440/#public-version-identifiers) and [examples in same PEP](https://www.python.org/dev/peps/pep-0440/#pre-releases): there is no dash in the canonical name. \r\n\r\nRegarding the PyPi website: the redirect that is being done is legit, rather than display the same content at two URLs one returns a 301 and redirects to the canonical URL.\r\n\r\nRegarding the `pip` expected behavior: see [normalization section in same PEP](https://www.python.org/dev/peps/pep-0440/#normalization). It explains why `pip install TensorFlow==2.0.0-----c00002` should do the same as `pip install tensorflow==2.0.0rc2`. If it doesn't, see the bug I linked above on PyPa.", "Ok, thank you. I get it."]}, {"number": 32750, "title": "Simplify the signature of DatasetParams::GetInputs()", "body": "This PR simplies the signature of `DatasetParams::GetInputs(..)` by changing it to `GetInputTensors()`. \r\n\r\nThanks for @aaudiber refactoring `dataset_test_base`(https://github.com/tensorflow/tensorflow/commit/ee8e382cfbb0ec568e1799a14e0628d1fc97ca23). This change comes from his suggestion: \r\n\r\n- `DatasetParams` is immutable and only returns the input parameters, so does not own any reference; \r\n\r\n- `DatasetTestBaseV2` 1) creates the input datasets based on the dataset parameters; 2) manages the lifecycle of the created datasets and input tensors.\r\n\r\nBased on the above rules, `DatasetParams::GetInputs(...)` is changed to `GetInputTensors()` which only returns the input tensors except the input datasets. `DatasetTestBaseV2` will create the input datasets and store them internally.\r\n\r\ncc:@jsimsa", "comments": ["@feihugis Thanks for these improvements! Great to see so many lines reduced.\r\n\r\nThe build is failing because of `MapAndBatchDatasetParams`:\r\n`\r\ntensorflow/core/kernels/data/experimental/map_and_batch_dataset_op_test.cc:49:10: error: 'tensorflow::Status tensorflow::data::experimental::{anonymous}::MapAndBatchDatasetParams::GetInputs(const std::vector<tensorflow::Tensor*>&, std::vector<std::unique_ptr<tensorflow::Tensor> >*, absl::InlinedVector<tensorflow::TensorValue, 4>*) const' marked 'override', but does not override\r\n   Status GetInputs(const std::vector<Tensor*>& input_datasets,\r\n`", "@aaudiber Thanks for the prompt review! The comments are addressed here(https://github.com/tensorflow/tensorflow/pull/32750/commits/c84916b036b98f33d338e64e68996d29b1bcce75). Could you please take a look when you get a chance?", "@aaudiber One more commit (https://github.com/tensorflow/tensorflow/pull/32750/commits/4f0b9cb55f97a1f25f502bf5997997e1bfa5d0b2) is added in this PR. It is inspired by the new APIs [MakeDataset(...)](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/dataset_test_base.h#L806) you added. \r\n\r\nBefore, [MakeDatasetTensor(...)](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/dataset_test_base.cc#L814) only supports a few kinds of DatasetParams because it requires a pre-defined function (e.g. [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/function_testlib.h#L147)); With this commit, `MakeDatasetTensor(...)` creates a dataset tensor using a similar way with `MakeDataset(...)` so that different dataset params can be supported with no need of the pre-defined functions.", "> Awesome, does this mean we can remove `CreateFactory` now?\r\n\r\nYeah, we can remove `CreateFactory`. I will do it in another PR, which will also update the function name `op_name()` to `dataset_type()` as discussed [here](https://github.com/tensorflow/tensorflow/pull/32676#discussion_r326422186).", "@aaudiber Could you please re-approve this PR as my last commit released your approval?", "@aaudiber The internal checks failed. Could you please help check the logs and paste them here? Thanks!", "@feihugis there was an internal test that needed to be updated to the new API. I did the update and this is merged now.", "> @feihugis there was an internal test that needed to be updated to the new API. I did the update and this is merged now.\r\n\r\n@aaudiber Thanks so much for your help!!"]}, {"number": 32749, "title": "Improve API doc for ZipDataset", "body": "This PR fills out the API documentation for the `ZipDataset` op. I extended the arguments lists to match the operator's definition, and I added a few paragraphs of description, including information about the different code paths by which a `ZipDataset` op might show up in a graph.", "comments": []}, {"number": 32748, "title": "AttributeError when passing tf.Variable into tf.function ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v2.0.0-rc2\r\n- Python version: 3.7.2\r\n- Bazel version (if compiling from source): 0.25.2\r\n- GCC/Compiler version (if compiling from source): 5.4.0 20160609\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n**Describe the current behavior**\r\n```python\r\nimport tensorflow as tf\r\n@tf.function\r\ndef foo(v):\r\n    pass\r\n\r\nfoo(tf.Variable(1))\r\nfoo(tf.Variable(1))\r\n```\r\n\r\nUsing this code piece, tensorflow throws an error: `AttributeError: 'NoneType' object has no attribute 'shape'`\r\n\r\n**Describe the expected behavior**\r\nNo error occured.\r\n\r\n**Code to reproduce the issue**\r\nProvided above\r\n\r\n**Other info / logs**\r\nNone", "comments": ["@dovahcrow,\r\nWorkaround can be change any one of `tf.Variable` to `tf.constant` or both to `tf.constant`.\r\n```\r\nimport tensorflow as tf\r\n@tf.function\r\ndef foo(v):\r\n    pass\r\n\r\nfoo(tf.constant(1))\r\nfoo(tf.constant(1))\r\n```\r\nLet us know this helps. Thanks!", "@gadagashwini Thanks! I currently solve this by using `tf.identity` to convert the `Variable` to a `Tensor`. But still, I wonder, is this a bug or feature?", "@dovahcrow,\r\nI could reproduce the issue with TF 2.0.0.rc2. Please see the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/4960b9907cbf83daae6a643136653125/untitled164.ipynb). Thanks!", "@dovahcrow @tf.function on any defined function allows it to retrace the function when necessary to generate the correct graphs. \r\n\r\nI think your intention is not to control and trace the graph. In that case, you could use the following code\r\n\r\n```\r\nimport tensorflow as tf\r\n@tf.function\r\ndef foo(v):\r\n    pass\r\na = tf.Variable(1)\r\nb = tf.Variable(1)\r\nfoo(a)\r\nfoo(b)\r\n```\r\n\r\nPlease check the resource [here](https://www.tensorflow.org/beta/tutorials/eager/tf_function#when_to_retrace) for more details on the retrace.\r\n\r\nPlease close the issue if this was resolved by the modification. Thanks!", "@jvishnuvardhan Hi, I don't think this is because of the misuse of the API.\r\nFirst, by changing the code piece to \r\n```python\r\nimport tensorflow as tf\r\n@tf.function\r\ndef foo(v):\r\n    pass\r\na = tf.Variable(1)\r\nfoo(a)\r\na = tf.Variable(1)\r\nfoo(a)\r\n```\r\nTensorflow will raise the error I reported. In this case Tensorflow is disallowing my redefining the variable which is totally a legal operation in Python. This is very unexpected and super hard to debug when happened in a large codebase (in my case I spent 5 hours to reduce thousands lines of code to such a simple case.)\r\nSecond, as per the document said:\r\n> For all other Python types, the keys are based on the object id() so that methods are traced independently for each instance of a class. In the future, TensorFlow may add more sophisticated caching for Python objects that can be safely converted to tensors.\r\n```python\r\nimport tensorflow as tf\r\n@tf.function\r\ndef foo(v):\r\n    pass\r\na = tf.Variable(1)\r\nprint(id(a))\r\nfoo(a)\r\na = tf.Variable(1)\r\nprint(id(a))\r\nfoo(a)\r\n```\r\nI get this as the output\r\n```\r\n140522833653432\r\n140522833649960\r\n```\r\n, as well as the error\r\n```python\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n~/.pyenv/versions/3.7.2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _hash_fix(self, elem)\r\n    108     try:\r\n--> 109       hash(elem)\r\n    110     except TypeError:\r\n\r\nTypeError: weak object has gone away\r\n```\r\nThis indicates `@tf.function` somehow didn't retrace the graph even if the `id`s of the objects are different.", "Thank you guys for the notes. I spend two days to realize that when using @tf.function causes the error and when removing @tf.function everything works fine. Then I came up with search phrase of \"AttributeError: 'NoneType' object has no attribute 'shape' + when using tf.function\" and I found this post. I fixed the issue by using tf.constant when feeding training set to the function. ", "Sorry for the late update. This was fixed in 4a5b3dbee4e2e5b60f58fe61530b73664e36d87e and should land in 2.1.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32748\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32748\">No</a>\n"]}, {"number": 32747, "title": "add missing reverse_sequence docstring", "body": "", "comments": ["You can also refer this guide: https://github.com/tensorflow/docs/blob/master/site/en/community/contribute/docs_ref.md", "There is way to locally test the changes now: https://github.com/tensorflow/docs/blob/master/site/en/community/contribute/docs_ref.md#test-on-your-local-machine", "Thanks @yashk2810 and @alextp for the help. "]}, {"number": 32746, "title": "[r1.15-CherryPick]: Cherrypicks 8 rk24", "body": "", "comments": ["Replaced by: https://github.com/tensorflow/tensorflow/pull/32794\r\n"]}, {"number": 32745, "title": "Cherrypicks 3 y932", "body": "", "comments": ["It seems that the last commit depends on changes to the TF core BUILD files. ", "I will defer the conflicting change to a later release.", "Replaced by: https://github.com/tensorflow/tensorflow/pull/32746"]}, {"number": 32744, "title": "[Grappler] Fix bug in layout optimizer introduced by cl/247704284. Re\u2026", "body": "\u2026store the original graph if Tune() fails.\r\n\r\nPiperOrigin-RevId: 270737190", "comments": []}, {"number": 32743, "title": "Failed to load delegate from libedgetpu.so.1.0 with tflite_runtime 1.14", "body": "**System information**\r\n- Have I written code (based on the docs):\r\n```\r\nfrom tflite_runtime.interpreter import Interpreter\r\nfrom tflite_runtime.interpreter import load_delegate\r\nmodel_path='my_compiled_model.tflite'\r\ninterpreter = Interpreter(model_path,\r\n  experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\r\n```\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n```\r\n  Operating System: Ubuntu 18.04.3 LTS\r\n            Kernel: Linux 4.15.0-60-generic\r\n      Architecture: x86-64\r\n```\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: laptop\r\n- TensorFlow installed from (source or binary):\r\n`pip3 install tflite_runtime-1.14.0-cp36-cp36m-linux_x86_64.whl`\r\n- TensorFlow version (use command below): tflite_runtime 1.14\r\n- Python version: `Python 3.6.5 :: Anaconda, Inc.`\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\nThis is the code that I ran:\r\n```\r\nfrom tflite_runtime.interpreter import Interpreter\r\nfrom tflite_runtime.interpreter import load_delegate\r\nmodel_path='my_compiled_model.tflite'\r\ninterpreter = Interpreter(model_path,\r\n  experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\r\n```\r\nfollowing this tutorial:\r\nhttps://www.tensorflow.org/lite/guide/python\r\n\r\nThis was working before, but somehow broken with this error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/nam/anaconda3/lib/python3.6/site-packages/tflite_runtime/interpreter.py\", line 165, in load_delegate\r\n    delegate = Delegate(library, options)\r\n  File \"/home/nam/anaconda3/lib/python3.6/site-packages/tflite_runtime/interpreter.py\", line 119, in __init__\r\n    raise ValueError(capture.message)\r\nValueError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"evaluate_edgetpu_cifar10.py\", line 51, in <module>\r\n    interpreter = Interpreter(file_name,experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\r\n  File \"/home/nam/anaconda3/lib/python3.6/site-packages/tflite_runtime/interpreter.py\", line 168, in load_delegate\r\n    library, str(e)))\r\nValueError: Failed to load delegate from libedgetpu.so.1.0\r\n```\r\nI have been messing around a lot with my machine since by installing different versions of tf. But for the purpose of using the tflite_runtime.interpreter's load_delegate function, shouldn't just the pip install works? \r\nVery weird behavior :/\r\nalso I do have `libedgetpu.so.1.0` installed here:\r\n```\r\n% ls /usr/lib/x86_64-linux-gnu/libedgetpu.so.1.0\r\n/usr/lib/x86_64-linux-gnu/libedgetpu.so.1.0\r\n```\r\n\r\nThanks in advance for the help!\r\n\r\n[EDIT]\r\nI guess I'll update the issue here with a solution so that any body else can reference:\r\n`ValueError: Failed to load delegate from libedgetpu.so.1.0` really is just due to the delegate library not being able to communicate with the edgetpu. This is a very standard linux problem and has nothing to do with the tensorflow library or libedgetpu. The failures most likely stems from some type of errno from the kernel which returns as failure to the user side.\r\n\r\nSo the easiest fix is to run with sudo:\r\n```\r\n$ sudo python your_script.py\r\n```\r\n\r\nBut the most permanent fix is to add your linux user to the `plugdev` group which will allows you to access devices without sudo (this will requires a reboot after):\r\n```\r\n$ sudo usermod -aG plugdev $USER\r\n```", "comments": ["Hi @Namburger , I had the same issue!\r\n\r\nMake sure your Coral USB Accelerator is plugged in when you run your code. If the USB Accelerator isn't plugged in when you call the 'load_delegate' function, it will result in that error. If it IS plugged in, that error won't occur.", "I am facing the same issue , even when the USB accelerator is plugged in and the LED in it is shining bright ........ ", "@EdjeElectronics wow... you were correct... thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32743\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32743\">No</a>\n", "> I am facing the same issue , even when the USB accelerator is plugged in and the LED in it is shining bright ........\r\n\r\n@programmer290399 you might need to add your linux user to plugdev group:\r\n```\r\n$ sudo usermod -aG plugdev [your username]\r\n```\r\n", "> > I am facing the same issue , even when the USB accelerator is plugged in and the LED in it is shining bright ........\r\n> \r\n> @programmer290399 you might need to add your linux user to plugdev group:\r\n> \r\n> ```\r\n> $ sudo usermod -aG plugdev [your username]\r\n> ```\r\n\r\nI plugged in the device, and I did this too, it still is showing the same error. I am using ubuntu installed on a virtualbox in Mac. The USB device is getting attached, but load_delegate is giving error", "@krishna-nag ahh, I see, most likely the usb device isn't detected in your VM. Possibly this will help: https://dev.to/kojikanao/coral-edgetpu-usb-with-virtualbox-57e1", "I got the same problem in native Ubuntu.  Reboot helps.", "There were two issues that needed resolving on my setup, coming from a fresh installed Pi 4 + fresh installed edge TPU.\r\n\r\n1. A udev rules needs to be added for the TPU, which according to the \"[Getting Started](https://coral.ai/docs/accelerator/get-started/#3-run-a-model-using-the-tensorflow-lite-api)\" page should have happened when installing the libedgetpu1-* package.\r\n```\r\n/etc/udev/rules.d/99-edgetpu-accelerator.rules\r\nSUBSYSTEM==\"usb\",ATTRS{idVendor}==\"1a6e\",GROUP=\"plugdev\"\r\nSUBSYSTEM==\"usb\",ATTRS{idVendor}==\"18d1\",GROUP=\"plugdev\"\r\n```\r\n\r\n2. The user needs to be part of the plugdev group as mentioned by @Namburger .\r\n`  sudo usermod -aG plugdev [your username]`\r\n\r\n3. Reboot\r\nThis seems to be a bug with the libedgetpu1-* packages. I tried both -std and -max versions (v12-1). ", "Thanks @cruzzer for the suggestions!\r\n\r\nDoes anyone have tried to use the TPU with a program in a Docker container?\r\nI am getting the same `Failed to load delegate from libedgetpu.so.1.0` error when running my docker container.", "@adr-arroyo you can check this out :)\r\nhttps://github.com/google-coral/tflite/issues/3#issuecomment-547942348\r\ntl;dr: most likely just have to throw it a `--privileged` flag ", "OS env:\r\n```\r\n# lsb_release -a\r\nNo LSB modules are available.\r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 14.04.6 LTS\r\nRelease:\t14.04\r\nCodename:\ttrusty\r\n\r\n# python3 -V\r\nPython 3.5.3\r\n```\r\nruntime error message:\r\n```\r\n# python3 coral/tflite/python/examples/classification/classify_image.py --model edgetpu/classify/models/output_tflite_graph_edgetpu.tflite --labels edgetpu/classify/models/labels.txt --input edgetpu/classify/flower.jpg \r\nTraceback (most recent call last):\r\n  File \"e-AI/coral/tflite/python/examples/classification/classify_image.py\", line 118, in <module>\r\n    main()\r\n  File \"e-AI/coral/tflite/python/examples/classification/classify_image.py\", line 95, in main\r\n    interpreter = make_interpreter(args.model)\r\n  File \"e-AI/coral/tflite/python/examples/classification/classify_image.py\", line 69, in make_interpreter\r\n    {'device': device[0]} if device else {})\r\n  File \"/usr/local/lib/python3.5/dist-packages/tflite_runtime/interpreter.py\", line 165, in load_delegate\r\n    delegate = Delegate(library, options)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tflite_runtime/interpreter.py\", line 89, in __init__\r\n    self._library = ctypes.pydll.LoadLibrary(library)\r\n  File \"/usr/lib/python3.5/ctypes/__init__.py\", line 425, in LoadLibrary\r\n    return self._dlltype(name)\r\n  File \"/usr/lib/python3.5/ctypes/__init__.py\", line 347, in __init__\r\n    self._handle = _dlopen(self._name, mode)\r\nOSError: /usr/lib/x86_64-linux-gnu/libedgetpu.so.1: symbol _ZTTNSt7__cxx1119basic_ostringstreamIcSt11char_traitsIcESaIcEEE, version GLIBCXX_3.4.21 not defined in file libstdc++.so.6 with link time reference\r\nException ignored in: <bound method Delegate.__del__ of <tflite_runtime.interpreter.Delegate object at 0x7f7a5ff57438>>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tflite_runtime/interpreter.py\", line 124, in __del__\r\n    if self._library is not None:\r\nAttributeError: 'Delegate' object has no attribute '_library'\r\n```\r\nchecked as below:\r\n```\r\n# strings /usr/lib/x86_64-linux-gnu/libedgetpu.so.1.0 |grep GLIBCXX\r\nGLIBCXX_3.4\r\nGLIBCXX_3.4.11\r\nGLIBCXX_3.4.21\r\nGLIBCXX_3.4.9\r\nGLIBCXX_3.4.14\r\nGLIBCXX_3.4.15\r\nGLIBCXX_3.4.18\r\nGLIBCXX_3.4.17\r\nGLIBCXX_3.4.5\r\nGLIBCXX_3.4.20\r\nGLIBCXX_3.4.19\r\n\r\n# strings /usr/lib/x86_64-linux-gnu/libstdc++.so.6 |grep GLIBCXX\r\nGLIBCXX_3.4\r\nGLIBCXX_3.4.1\r\nGLIBCXX_3.4.2\r\nGLIBCXX_3.4.3\r\nGLIBCXX_3.4.4\r\nGLIBCXX_3.4.5\r\nGLIBCXX_3.4.6\r\nGLIBCXX_3.4.7\r\nGLIBCXX_3.4.8\r\nGLIBCXX_3.4.9\r\nGLIBCXX_3.4.10\r\nGLIBCXX_3.4.11\r\nGLIBCXX_3.4.12\r\nGLIBCXX_3.4.13\r\nGLIBCXX_3.4.14\r\nGLIBCXX_3.4.15\r\nGLIBCXX_3.4.16\r\nGLIBCXX_3.4.17\r\nGLIBCXX_3.4.18\r\nGLIBCXX_3.4.19\r\nGLIBCXX_3.4.20\r\nGLIBCXX_3.4.21\r\nGLIBCXX_3.4.22\r\nGLIBCXX_3.4.23\r\nGLIBCXX_3.4.24\r\nGLIBCXX_3.4.25\r\nGLIBCXX_3.4.26\r\nGLIBCXX_3.4.27\r\nGLIBCXX_3.4.28\r\nGLIBCXX_DEBUG_MESSAGE_LENGTH\r\n```\r\n", "Thanks @Namburger !\r\n\r\nNow my program is able to use the TPU from docker, I had to add `-v /dev/bus/usb:/dev/bus/usb` along with the `--priviledge` flag to the docker run command so that it works.", "I have another question for you guys,\r\nRegarding the output of the TPU, it is returned as type uint8 (mandatory and set in the post-training quantization, as well as the input). How do you cast it back to your original type? \r\n\r\nIn my case I cast float32 values between (0 and 1) to uint8, then I need this cast back to a proper float32 values to use my scaler and descale the results.\r\n", "same issue on a raspberry pi \r\n\r\n```\r\npi@raspberrypi:~/coral/google-coral/examples-camera/raspicam \\> lsb_release -a\r\nNo LSB modules are available.\r\nDistributor ID:\tRaspbian\r\nDescription:\tRaspbian GNU/Linux 10 (buster)\r\nRelease:\t10\r\nCodename:\tbuster\r\n\r\npi@raspberrypi:~/coral \\> ls\r\ntflite  tflite_runtime-1.14.0-cp37-cp37m-linux_armv7l.whl\r\npi@raspberrypi:~/coral \\> pip3 install tflite_runtime-1.14.0-cp37-cp37m-linux_armv7l.whl\r\nLooking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple\r\nRequirement already satisfied: tflite-runtime==1.14.0 from file:///home/pi/coral/tflite_runtime-1.14.0-cp37-cp37m-linux_armv7l.whl in /usr/local/lib/python3.7/dist-packages (1.14.0)\r\npi@raspberrypi:~/coral \\> ls\r\ntflite  tflite_runtime-1.14.0-cp37-cp37m-linux_armv7l.whl\r\npi@raspberrypi:~/coral \\> git clone https://github.com/tensorflow/examples --depth 1\r\nCloning into 'examples'...\r\nremote: Enumerating objects: 1293, done.\r\nremote: Counting objects: 100% (1293/1293), done.\r\nremote: Compressing objects: 100% (836/836), done.\r\nremote: Total 1293 (delta 369), reused 994 (delta 250), pack-reused 0\r\nReceiving objects: 100% (1293/1293), 8.28 MiB | 2.15 MiB/s, done.\r\nResolving deltas: 100% (369/369), done.\r\npi@raspberrypi:~/coral \\> cd examples/lite/examples/image_classification/raspberry_pi\r\npi@raspberrypi:~/coral/examples/lite/examples/image_classification/raspberry_pi \\> bash download.sh /tmp\r\nLooking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple\r\nRequirement already satisfied: numpy in /usr/lib/python3/dist-packages (from -r requirements.txt (line 2)) (1.16.2)\r\nRequirement already satisfied: picamera in /usr/lib/python3/dist-packages (from -r requirements.txt (line 3)) (1.13)\r\nRequirement already satisfied: Pillow in /usr/lib/python3/dist-packages (from -r requirements.txt (line 4)) (5.4.1)\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100 2997k  100 2997k    0     0  1761k      0  0:00:01  0:00:01 --:--:-- 1762k\r\nArchive:  mobilenet_v1_1.0_224_quant_and_labels.zip\r\n  inflating: /tmp/labels_mobilenet_quant_v1_224.txt  \r\n   creating: /tmp/__MACOSX/\r\n  inflating: /tmp/__MACOSX/._labels_mobilenet_quant_v1_224.txt  \r\n  inflating: /tmp/mobilenet_v1_1.0_224_quant.tflite  \r\n  inflating: /tmp/__MACOSX/._mobilenet_v1_1.0_224_quant.tflite  \r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100 4428k  100 4428k    0     0  1917k      0  0:00:02  0:00:02 --:--:-- 1916k\r\nDownloaded files are in /tmp\r\npi@raspberrypi:~/coral/examples/lite/examples/image_classification/raspberry_pi \\> python3 classify_picamera.py \\\r\n>   --model /tmp/mobilenet_v1_1.0_224_quant.tflite \\\r\n>   --labels /tmp/labels_mobilenet_quant_v1_224.txt\r\nINFO: Initialized TensorFlow Lite runtime.\r\n^CTraceback (most recent call last):\r\n  File \"classify_picamera.py\", line 96, in <module>\r\n    main()\r\n  File \"classify_picamera.py\", line 82, in main\r\n    Image.ANTIALIAS)\r\n  File \"/usr/lib/python3/dist-packages/PIL/Image.py\", line 1806, in resize\r\n    return self._new(self.im.resize(size, resample, box))\r\nKeyboardInterrupt\r\npi@raspberrypi:~/coral/examples/lite/examples/image_classification/raspberry_pi \\> cd\r\npi@raspberrypi:~ \\> cd coral/\r\npi@raspberrypi:~/coral \\> ls\r\nexamples  tflite  tflite_runtime-1.14.0-cp37-cp37m-linux_armv7l.whl\r\npi@raspberrypi:~/coral \\> mv examples/ obj-example\r\npi@raspberrypi:~/coral \\> mkdir google-coral && cd google-coral\r\npi@raspberrypi:~/coral/google-coral \\> git clone https://github.com/google-coral/examples-camera.git --depth 1\r\nCloning into 'examples-camera'...\r\nremote: Enumerating objects: 37, done.\r\nremote: Counting objects: 100% (37/37), done.\r\nremote: Compressing objects: 100% (35/35), done.\r\nremote: Total 37 (delta 12), reused 14 (delta 1), pack-reused 0\r\nUnpacking objects: 100% (37/37), done.\r\npi@raspberrypi:~/coral/google-coral \\> cd examples-camera\r\npi@raspberrypi:~/coral/google-coral/examples-camera \\> sh download_models.sh\r\n--2020-01-17 20:23:10--  https://dl.google.com/coral/canned_models/all_models.tar.gz\r\nResolving dl.google.com (dl.google.com)... 172.217.7.206, 2607:f8b0:4004:802::200e\r\nConnecting to dl.google.com (dl.google.com)|172.217.7.206|:443... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: 184961153 (176M) [application/octet-stream]\r\nSaving to: \u2018all_models.tar.gz\u2019\r\n\r\nall_models.tar.gz               100%[=======================================================>] 176.39M  3.01MB/s    in 77s     \r\n\r\n2020-01-17 20:24:27 (2.30 MB/s) - \u2018all_models.tar.gz\u2019 saved [184961153/184961153]\r\n\r\n./\r\n./mobilenet_v2_1.0_224_inat_bird_quant_edgetpu.tflite\r\n./mobilenet_v2_1.0_224_inat_bird_quant.tflite\r\n./inception_v3_299_quant_edgetpu.tflite\r\n./mobilenet_v2_1.0_224_quant_edgetpu.tflite\r\n./inat_insect_labels.txt\r\n./inception_v2_224_quant_edgetpu.tflite\r\n./mobilenet_v1_1.0_224_quant.tflite\r\n./mobilenet_v2_1.0_224_inat_insect_quant_edgetpu.tflite\r\n./mobilenet_v1_1.0_224_quant_embedding_extractor_edgetpu.tflite\r\n./mobilenet_v2_1.0_224_quant.tflite\r\n./inception_v2_224_quant.tflite\r\n./imagenet_labels.txt\r\n./mobilenet_ssd_v2_face_quant_postprocess_edgetpu.tflite\r\n./coco_labels.txt\r\n./inception_v1_224_quant_edgetpu.tflite\r\n./mobilenet_v2_1.0_224_inat_plant_quant.tflite\r\n./inat_bird_labels.txt\r\n./mobilenet_v1_1.0_224_quant_edgetpu.tflite\r\n./mobilenet_ssd_v1_coco_quant_postprocess.tflite\r\n./inception_v4_299_quant.tflite\r\n./pet_labels.txt\r\n./inception_v3_299_quant.tflite\r\n./mobilenet_ssd_v2_coco_quant_postprocess_edgetpu.tflite\r\n./mobilenet_ssd_v2_coco_quant_postprocess.tflite\r\n./mobilenet_ssd_v1_coco_quant_postprocess_edgetpu.tflite\r\n./inat_plant_labels.txt\r\n./mobilenet_v2_1.0_224_inat_insect_quant.tflite\r\n./inception_v1_224_quant.tflite\r\n./inception_v4_299_quant_edgetpu.tflite\r\n./mobilenet_ssd_v2_face_quant_postprocess.tflite\r\n./mobilenet_v1_1.0_224_quant_embedding_extractor.tflite\r\n./mobilenet_v2_1.0_224_inat_plant_quant_edgetpu.tflite\r\npi@raspberrypi:~/coral/google-coral/examples-camera \\> cd raspicam\r\npi@raspberrypi:~/coral/google-coral/examples-camera/raspicam \\> bash install_requirements.sh\r\nLooking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple\r\nRequirement already satisfied: picamera in /usr/lib/python3/dist-packages (1.13)\r\npi@raspberrypi:~/coral/google-coral/examples-camera/raspicam \\> python3 classify_capture.py\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/dist-packages/tflite_runtime/interpreter.py\", line 165, in load_delegate\r\n    delegate = Delegate(library, options)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tflite_runtime/interpreter.py\", line 119, in __init__\r\n    raise ValueError(capture.message)\r\nValueError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"classify_capture.py\", line 91, in <module>\r\n    main()\r\n  File \"classify_capture.py\", line 55, in main\r\n    interpreter = common.make_interpreter(args.model)\r\n  File \"/home/pi/coral/google-coral/examples-camera/raspicam/common.py\", line 27, in make_interpreter\r\n    {'device': device[0]} if device else {})\r\n  File \"/usr/local/lib/python3.7/dist-packages/tflite_runtime/interpreter.py\", line 168, in load_delegate\r\n    library, str(e)))\r\nValueError: Failed to load delegate from libedgetpu.so.1\r\n\r\npi@raspberrypi:~/coral/google-coral/examples-camera/raspicam \\> sudo usermod -aG plugdev pi\r\npi@raspberrypi:~/coral/google-coral/examples-camera/raspicam \\> python3 classify_capture.py\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/dist-packages/tflite_runtime/interpreter.py\", line 165, in load_delegate\r\n    delegate = Delegate(library, options)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tflite_runtime/interpreter.py\", line 119, in __init__\r\n    raise ValueError(capture.message)\r\nValueError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"classify_capture.py\", line 91, in <module>\r\n    main()\r\n  File \"classify_capture.py\", line 55, in main\r\n    interpreter = common.make_interpreter(args.model)\r\n  File \"/home/pi/coral/google-coral/examples-camera/raspicam/common.py\", line 27, in make_interpreter\r\n    {'device': device[0]} if device else {})\r\n  File \"/usr/local/lib/python3.7/dist-packages/tflite_runtime/interpreter.py\", line 168, in load_delegate\r\n    library, str(e)))\r\nValueError: Failed to load delegate from libedgetpu.so.1\r\n\r\n```", "rebooting my pi - fixed the issue for me", "In my case, Coral USB accelerator works on USB3.0 so, I've changed USB compatibility option to USB 3.0 and it solved.", "by the way guys, please also upgrading the [tflite_runtime](https://www.tensorflow.org/lite/guide/python) package.\r\n", "> USB compatibility option\r\n\r\nCan you explain how you did that?  \r\nHow you set (assuming raspberry pi 4) to USB3.0 compatibility mode/option? ", "@Syirrus are you still having the same issue?\r\nthis all fixed now if you upgrade to tflite_runtime-2.1.0.post1 instead of 1.14.\r\nI don't think USB2.0 or USB3.0 matter because it works consistently on my RPI3 b+ (2.0).\r\nAnyways, to answer your question, the pi4 has 2 USB2.0 ports and 2 USB3.0 ports,  just switch it to the correct port.", "> tflite_runtime-2.1.0.post1\r\n\r\nI installed the tflite_runtime-2.1.0 (tflite_runtime-2.1.0.post1-cp37-cp37m-linux_armv7l.whl).  I still get the same behavior on a RPI4.  However, if I plug the coral USB stick in to a USB 2.0 port, instead of the USB3.0 port it works.  When I switch to the native 3.0 USB port on the RPI4 I get the same error as above.\r\n\r\nIn fact, after I run the classification script and they run a lsusb, the Coral USB stick is no longer attached logically to the system, though physically it is still plugged in. \r\n\r\n", "> @Syirrus are you still having the same issue?\r\n> this all fixed now if you upgrade to tflite_runtime-2.1.0.post1 instead of 1.14.\r\n> I don't think USB2.0 or USB3.0 matter because it works consistently on my RPI3 b+ (2.0).\r\n> Anyways, to answer your question, the pi4 has 2 USB2.0 ports and 2 USB3.0 ports, just switch it to the correct port.\r\n\r\nDoes that make sense?", "@Syirrus Ahh, I see.\r\nSo it's working in USB 2.0 but not 3.0? That's annoying, because with 2.0, data transferring speed is going to be a bottle neck. But this looks like a rpi4 issue, check [here](https://www.raspberrypi.org/documentation/hardware/raspberrypi/usb/README.md).\r\nFor my pi3 b+, this isn't even an option since it only has 2.0 :/, good luck with yours", "> @Syirrus Ahh, I see.\r\n> So it's working in USB 2.0 but not 3.0? That's annoying, because with 2.0, data transferring speed is going to be a bottle neck. But this looks like a rpi4 issue, check [here](https://www.raspberrypi.org/documentation/hardware/raspberrypi/usb/README.md).\r\n> For my pi3 b+, this isn't even an option since it only has 2.0 :/, good luck with yours\r\n\r\nExactly, it is working on USB 2.0, but NOT USB 3.0 which is a bottleneck :(. I will check out the link to RPI4 issues.  Thank you so much!", "No problems!", "> No problems!\r\n\r\nAfter spending many hours on and off thinking about this problem and combing the net, I finally solved this problem with this \"Failed to load delegate from libedgetpu.so.1.0 error\". Essentially, the cable that Google provided with my Coral USB TPU stinks.  I broke down and purchased a USB 3.1 (10Gbps) (NOT 5Gbps) cable and everything worked perfectly on the USB 3.0 port for the Raspberry Pi 4b (4GB). I hope this helps someone else in the same situation I was in.", "Weird as it may seem, I can confirm what @Syirrus reports. After many hours of fighting this error, it finally worked when I changed the USB cable. Thanks!", " I recently moved from a USB 2.0 system to USB 3.0 and had the same issue. It is intermittent, so it's not a plugdev issue.\r\n\r\nAs suggested above it does seem to be an issue with Google\u2019s cable.  I bought [this cable](https://www.amazon.com/Anker-Powerline-Certified-Samsung-MacBook/dp/B07213D35X/ref=sr_1_4?dchild=1&keywords=usb+3.1+anker+usb+c&qid=1611410490&sr=8-4) and replaced the google provided cable. That seems to have completely eliminated the issue and I\u2019m getting good inference speed (averaging 19ms for an 800px image using mobiledet and pycoral wrappers)", "I am getting the same error, running a docker in win10...Using Google coral edge tpu pci version", "_(last edit: May 28,2021, to handle situation when lsusb works, but coral doesn't load the library)_\r\n\r\nI'm going to leave another finding here (Only applies to intermittent libedgetpu delegate loading issues, unrelated to plugdev/permissions)\r\n\r\n1. Replacing the cable eliminates _most_ problems related to  intermittent loading libedgetpu issues\r\n2. However, there are times this happens, and especially if I interrupt an ongoing TPU operation \r\n3. When this happens, a reboot usually fixes it, but I'd prefer not to have to reboot. There are two approaches I found. \r\n     - (Works for me) You reset the full USB ecosystem - [this script](http://billauer.co.il/blog/2013/02/usb-reset-ehci-uhci-linux/) does that - tested on ubuntu 20. Depending on your OS it may require different paths - see comments in that link. I use this approach.\r\n    \r\n     - (Does not work for me) If you prefer not to reset the entire USB ecosystem, [this is a more focussed way](https://askubuntu.com/a/661) (You can get the coral device details by doing `lsusb | grep -i google`. However, in my case, when the TPU actually fails, this method does not work - it mulls around for a while and then errors out\r\n\r\nSo the rest of the post is how I go about detecting failure and then I reset the full USB system.\r\nHere is what I observed:\r\n- Sometimes, when coral fails, lsusb also fails showing the google device\r\n- I realized later, that there are times that coral fails to load, but lsusb continues to show the device \r\n\r\nSo to make sure the device always works, I do the following:\r\n- Try lsusb - if it fails, obviously coral is not working, restart usb\r\n- If lsusb shows google, try to load the delegate using pycoral - if that fails restart usb \r\n- If that passes, nothing to do\r\n\r\nSpecifically, I have set up a cron file that checks that the coral device is detected every hour:\r\n```\r\npp@homeserver:~$ cat /etc/cron.d/coral-usb-checker \r\nSHELL=/bin/sh\r\nPATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin\r\n\r\n# every hour\r\n0 * * * * root usb_reset_all.sh\r\n```\r\n\r\nThe modified script (credit above in my notes):\r\n\r\n```bash\r\npp@homeserver:~$ cat /usr/local/bin/usb_reset_all.sh \r\n#!/bin/bash\r\n\r\nfilename='/tmp/coral_status.txt'\r\nnow=$(date)\r\nif [[ $EUID != 0 ]] ; then\r\n  echo \"${now}:$0  must be run as root!\" \r\n  echo \"${now}:$0  must be run as root!\" > ${filename}\r\n  exit 1\r\nfi\r\n\r\n# keep log file to 100 lines\r\nif [ -f \"${filename}\" ] ;then\r\n        echo \"$(tail -100 ${filename})\" > ${filename}\r\nfi\r\n\r\n [[ \"${1}\" == \"--force\" ]] &&  IS_FORCED=true || IS_FORCED=false\r\n\r\nis_coral_working() {\r\n        # Note if you have multiple results from this lsusb command\r\n        # You can specifically check with lsusb -d <deviceid>\r\n        # Example: lsusb -d 18d1:9302 in my case which is what lsusb prints as the \"ID\" for my device\r\n\r\n        if [[ -z `lsusb | grep -i google` ]] ; then\r\n                echo \"${now}:lsusb check failed\" >> ${filename}\r\n\r\n                return 0\r\n        fi\r\n        echo \"${now}:lsusb check passed, checking load_edgetpu_delegate()\" >> ${filename}\r\n        res=`python << HEREDOC\r\nfrom pycoral.utils.edgetpu import load_edgetpu_delegate\r\ntry:\r\n        load_edgetpu_delegate()\r\n        print ('success')\r\nexcept Exception as e:\r\n        print('error')\r\nHEREDOC\r\n`\r\n        if [[ \"${res}\" = \"success\" ]]; then\r\n                return 0\r\n        else\r\n                return 1\r\n        fi\r\n}\r\nrestart_usb() {\r\n        # credit: http://billauer.co.il/blog/2013/02/usb-reset-ehci-uhci-linux/\r\n        echo \"${now}:Restarting USB\" >> ${filename}\r\n        for xhci in /sys/bus/pci/drivers/?hci_hcd ; do\r\n          if ! cd $xhci ; then\r\n            echo \"${now}:Weird error. Failed to change directory to $xhci\" >> ${filename}\r\n            exit 1\r\n          fi\r\n\r\n          echo \"${now}:Resetting devices from $xhci...\" >> ${filename}\r\n\r\n          for i in ????:??:??.? ; do\r\n            echo -n \"$i\" > unbind 2>/dev/null\r\n            echo -n \"$i\" > bind 2>/dev/null\r\n          done\r\n        done\r\n        echo \"${now}:Completed operation\" >> ${filename}\r\n}\r\n\r\n\r\necho \"${now}--------------------------------------------------------\" >> ${filename}\r\n\r\nif [[ ${IS_FORCED} == true ]] ; then\r\n        echo \"${now}:Forcing restart as user specified --force\" >> ${filename}\r\n        restart_usb\r\n\r\nelif is_coral_working ; then \r\n        echo \"${now}:Coral working fine\" >> ${filename}\r\n\r\nelse\r\n        echo \"${now}:ERROR: Coral detection failed\" >> ${filename}\r\n        restart_usb\r\nfi\r\n\r\n```\r\n\r\nThe nice part is my resident services that depend on coral automatically get restarted (not sure how, but it does). So I don't need to restart it manually.\r\n\r\n", "**USB Accelerator connected to a Proxmox VM failure scenario (FIXED)**\r\n\r\nI experienced a similar but different problem when connecting the USB Accelerator to a VM hosted on proxmox:\r\n\r\nWhen I plugged the USB Accelerator into the physical machine (proxmox physical host), it showed up like this:\r\n\r\n```\r\nroot@proxmox:~# lsusb\r\n...\r\nBus 001 Device 008: ID 1a6e:089a Global Unichip Corp. \r\n...\r\n```\r\nSo added device to the VM where I want to work with Coral:\r\n\r\n```\r\nroot@proxmox:~# qm set 202 -usb0 host=1a6e:089a\r\nupdate VM 202: -usb0 host=1a6e:089a\r\n```\r\nAnd rebooted the VM. The `1a6e:089a` device showed up in the `lsusb` command inside the VM. Yay! Now the fun begins! I try to run the sample, and I get loooooong pause then the error at the top of this bug:\r\n\r\n```\r\njfernandez@docker:~/dev/coral/pycoral$ python3 examples/classify_image.py --model test_data/mobilenet_v2_1.0_224_inat_bird_quant_edgetpu.tflite --labels test_data/inat_bird_labels.txt --input test_data/parrot.jpg\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3/dist-packages/tflite_runtime/interpreter.py\", line 152, in load_delegate\r\n    delegate = Delegate(library, options)\r\n  File \"/usr/lib/python3/dist-packages/tflite_runtime/interpreter.py\", line 111, in __init__\r\n    raise ValueError(capture.message)\r\nValueError\r\n \r\nDuring handling of the above exception, another exception occurred:\r\n \r\nTraceback (most recent call last):\r\n  File \"examples/classify_image.py\", line 84, in <module>\r\n    main()\r\n  File \"examples/classify_image.py\", line 61, in main\r\n    interpreter = make_interpreter(*args.model.split('@'))\r\n  File \"/usr/lib/python3/dist-packages/pycoral/utils/edgetpu.py\", line 66, in make_interpreter\r\n    delegates = [load_edgetpu_delegate({'device': device} if device else {})]\r\n  File \"/usr/lib/python3/dist-packages/pycoral/utils/edgetpu.py\", line 42, in load_edgetpu_delegate\r\n    return tflite.load_delegate(_EDGETPU_SHARED_LIB, options or {})\r\n  File \"/usr/lib/python3/dist-packages/tflite_runtime/interpreter.py\", line 154, in load_delegate\r\n    raise ValueError('Failed to load delegate from {}\\n{}'.format(\r\nValueError: Failed to load delegate from libedgetpu.so.1\r\n```\r\n\r\nAfter trying a bunch of fixes from this thread, I noticed the USB Accelerator device **_had changed_**:\r\n\r\n```\r\nroot@proxmox:~# lsusb\r\n...\r\nBus 001 Device 012: ID 18d1:9302 Google Inc.\r\n...\r\n```\r\nWhat?! Looks like running the example changed the device host ID of the USB Accelerator (was: `1a6e:089a`,  is now:`18d1:9302`). A side effect of loading a USB driver for it? Only the USB driver daemons know... Point is, now the VM can't talk to the USB device anymore, so I have to re-connect the USB device to the VM with the new device ID:\r\n\r\n```\r\nroot@proxmox:~# qm set 202 -usb0 host=18d1:9302\r\nupdate VM 202: -usb0 host=18d1:9302\r\n```\r\nReboot my VM again. And the sample runs!\r\n\r\nAlso, at one point, I ran this to add myself to the plugdev group, but I'm not sure it this had any effect:\r\n```\r\nsudo usermod -aG plugdev [your username]\r\n```\r\n\r\nHope this saves someone a bit of time.\r\n\r\n\\\\\\\\Joe", "> **USB Accelerator connected to a Proxmox VM failure scenario (FIXED)**\r\n> \r\n> I experienced a similar but different problem when connecting the USB Accelerator to a VM hosted on proxmox:\r\n> \r\n> When I plugged the USB Accelerator into the physical machine (proxmox physical host), it showed up like this:\r\n> \r\n> ```\r\n> root@proxmox:~# lsusb\r\n> ...\r\n> Bus 001 Device 008: ID 1a6e:089a Global Unichip Corp. \r\n> ...\r\n> ```\r\n> \r\n> So added device to the VM where I want to work with Coral:\r\n> \r\n> ```\r\n> root@proxmox:~# qm set 202 -usb0 host=1a6e:089a\r\n> update VM 202: -usb0 host=1a6e:089a\r\n> ```\r\n> \r\n> And rebooted the VM. The `1a6e:089a` device showed up in the `lsusb` command inside the VM. Yay! Now the fun begins! I try to run the sample, and I get loooooong pause then the error at the top of this bug:\r\n> \r\n> ```\r\n> jfernandez@docker:~/dev/coral/pycoral$ python3 examples/classify_image.py --model test_data/mobilenet_v2_1.0_224_inat_bird_quant_edgetpu.tflite --labels test_data/inat_bird_labels.txt --input test_data/parrot.jpg\r\n> Traceback (most recent call last):\r\n>   File \"/usr/lib/python3/dist-packages/tflite_runtime/interpreter.py\", line 152, in load_delegate\r\n>     delegate = Delegate(library, options)\r\n>   File \"/usr/lib/python3/dist-packages/tflite_runtime/interpreter.py\", line 111, in __init__\r\n>     raise ValueError(capture.message)\r\n> ValueError\r\n>  \r\n> During handling of the above exception, another exception occurred:\r\n>  \r\n> Traceback (most recent call last):\r\n>   File \"examples/classify_image.py\", line 84, in <module>\r\n>     main()\r\n>   File \"examples/classify_image.py\", line 61, in main\r\n>     interpreter = make_interpreter(*args.model.split('@'))\r\n>   File \"/usr/lib/python3/dist-packages/pycoral/utils/edgetpu.py\", line 66, in make_interpreter\r\n>     delegates = [load_edgetpu_delegate({'device': device} if device else {})]\r\n>   File \"/usr/lib/python3/dist-packages/pycoral/utils/edgetpu.py\", line 42, in load_edgetpu_delegate\r\n>     return tflite.load_delegate(_EDGETPU_SHARED_LIB, options or {})\r\n>   File \"/usr/lib/python3/dist-packages/tflite_runtime/interpreter.py\", line 154, in load_delegate\r\n>     raise ValueError('Failed to load delegate from {}\\n{}'.format(\r\n> ValueError: Failed to load delegate from libedgetpu.so.1\r\n> ```\r\n> \r\n> After trying a bunch of fixes from this thread, I noticed the USB Accelerator device **_had changed_**:\r\n> \r\n> ```\r\n> root@proxmox:~# lsusb\r\n> ...\r\n> Bus 001 Device 012: ID 18d1:9302 Google Inc.\r\n> ...\r\n> ```\r\n> \r\n> What?! Looks like running the example changed the device host ID of the USB Accelerator (was: `1a6e:089a`, is now:`18d1:9302`). A side effect of loading a USB driver for it? Only the USB driver daemons know... Point is, now the VM can't talk to the USB device anymore, so I have to re-connect the USB device to the VM with the new device ID:\r\n> \r\n> ```\r\n> root@proxmox:~# qm set 202 -usb0 host=18d1:9302\r\n> update VM 202: -usb0 host=18d1:9302\r\n> ```\r\n> \r\n> Reboot my VM again. And the sample runs!\r\n> \r\n> Also, at one point, I ran this to add myself to the plugdev group, but I'm not sure it this had any effect:\r\n> \r\n> ```\r\n> sudo usermod -aG plugdev [your username]\r\n> ```\r\n> \r\n> Hope this saves someone a bit of time.\r\n> \r\n> \\\\Joe\r\n\r\nI have the same setup using the tpu on a proxmox VM , added the as a pci device with an adapter. \r\nIn my case the resetting device works. \r\nwith lspci get device id:\r\n**02:00.0 System peripheral: Device 1ac1:089a**\r\nfind the corresponding  the device folder:\r\n**sys/bus/pci/devices/0000:02:00.0/uevent:PCI_ID=1AC1:089A**\r\nand reset the device:\r\n**echo 1 >/sys/bus/pci/devices/0000\\:02\\:00.0/remove\r\necho 1 >/sys/bus/pci/rescan**\r\n \r\n\r\nI believe resetting the USB will also work as mentioned above.\r\n\r\n\r\n\r\n\r\n"]}, {"number": 32742, "title": "[r2.0-CherryPick]: [Grappler] Fix bug in layout optimizer introduced by cl/247704284. Re\u2026", "body": "\u2026store the original graph if Tune() fails.\r\n\r\nPiperOrigin-RevId: 270737190", "comments": []}, {"number": 32741, "title": "Cherrypicks bx1 wk", "body": "", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32741) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 32740, "title": "TFLite: Converting 2 Operation Network[Slice, Transpose] results in converter error despite both ops being supported", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Sierra 10.12.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-rc1\r\n- Python version: 3.7.1\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: CPU\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nModel conversion from script errors unexpectedly with:\r\nF tensorflow/lite/toco/tooling_util.cc:661] Check failed: dim >= 1 (0 vs. 1)\r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x00007fffbc0853c0 (most recent call first):\r\n  File \"/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 52 in execute\r\n  File \"/Users/t.capes/miniconda3/lib/python3.7/site-packages/absl/app.py\", line 251 in _run_main\r\n  File \"/Users/t.capes/miniconda3/lib/python3.7/site-packages/absl/app.py\", line 300 in run\r\n  File \"/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py\", line 40 in run\r\n  File \"/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 89 in main\r\n  File \"/Users/t.capes/miniconda3/bin/toco_from_protos\", line 10 in <module>\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nimport tensorflow.compat.v1 as tf\r\nimport numpy as np\r\n\r\ntf.disable_v2_behavior()\r\ninitial_input = tf.placeholder(dtype=tf.float32, shape=(None,5,1024))\r\ncap_i = tf.slice(initial_input, [0,0,0], [0,5,1023])\r\ncap_iT = tf.transpose(cap_i, perm=[0,2,1])\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\ntf.io.write_graph(sess.graph_def, '', 'train.pbtxt')\r\nconverter = tf.lite.TFLiteConverter.from_session(sess, [initial_input], [cap_iT])\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\nopen('converted_model.tflite', \"wb\").write(tflite_model)\r\nsess.close()\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0923 16:48:55.162434 140736348050368 deprecation.py:323] From /Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nnon-resource variables are not supported in the long term\r\n2019-09-23 16:48:55.168686: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-09-23 16:48:55.186235: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f8f68dba420 executing computations on platform Host. Devices:\r\n2019-09-23 16:48:55.186260: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-09-23 16:48:55.192597: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-09-23 16:48:55.192676: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-09-23 16:48:55.194251: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize\r\n2019-09-23 16:48:55.194270: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2019-09-23 16:48:55.194277: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2019-09-23 16:48:55.196041: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-09-23 16:48:55.196100: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-09-23 16:48:55.198204: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize\r\n2019-09-23 16:48:55.198218: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 6 nodes (-1), 5 edges (0), time = 0.619ms.\r\n2019-09-23 16:48:55.198224: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 6 nodes (0), 5 edges (0), time = 0.175ms.\r\nTraceback (most recent call last):\r\n  File \"tf_test_1.py\", line 16, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py\", line 983, in convert\r\n    **converter_kwargs)\r\n  File \"/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py\", line 449, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py\", line 200, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2019-09-23 16:48:57.055966: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 2 operators, 6 arrays (0 quantized)\r\n2019-09-23 16:48:57.056188: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 2 operators, 6 arrays (0 quantized)\r\n2019-09-23 16:48:57.056303: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 2 operators, 6 arrays (0 quantized)\r\n2019-09-23 16:48:57.056347: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 2 operators, 6 arrays (0 quantized)\r\n2019-09-23 16:48:57.056388: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 2 operators, 6 arrays (0 quantized)\r\n2019-09-23 16:48:57.056432: F tensorflow/lite/toco/tooling_util.cc:661] Check failed: dim >= 1 (0 vs. 1)\r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x00007fffbc0853c0 (most recent call first):\r\n  File \"/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 52 in execute\r\n  File \"/Users/t.capes/miniconda3/lib/python3.7/site-packages/absl/app.py\", line 251 in _run_main\r\n  File \"/Users/t.capes/miniconda3/lib/python3.7/site-packages/absl/app.py\", line 300 in run\r\n  File \"/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py\", line 40 in run\r\n  File \"/Users/t.capes/miniconda3/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 89 in main\r\n  File \"/Users/t.capes/miniconda3/bin/toco_from_protos\", line 10 in <module>", "comments": ["Found issue, thought slice was supported but only strided_slice is.\r\n```\r\nimport tensorflow.compat.v1 as tf\r\nimport numpy as np\r\n\r\ntf.disable_v2_behavior()\r\ninitial_input = tf.placeholder(dtype=tf.float32, shape=(None,5,1024))\r\ncap_i = tf.strided_slice(initial_input, [0,0,0], [0,5,1024], [1,1,1], shrink_axis_mask=1)\r\ncap_i_reshaped =tf.reshape(cap_i,[1,5,1024])\r\ncap_iT = tf.transpose(cap_i_reshaped, perm=[0,2,1])\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\ntf.io.write_graph(sess.graph_def, '', 'train.pbtxt')\r\nconverter = tf.lite.TFLiteConverter.from_session(sess, [initial_input], [cap_iT])\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\nopen('converted_model.tflite', \"wb\").write(tflite_model)\r\nsess.close()\r\n```\r\nWorks properly. Closing issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32740\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32740\">No</a>\n"]}, {"number": 32739, "title": "typo fix", "body": "Fixing typo in docstrings: tensorflow.python.keras.engine.training.Model.predict() and evaluate().\r\nIn the description of `batch_size`, it should be 'if' instead of 'is'.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32739) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32739) for more info**.\n\n<!-- ok -->"]}]