[{"number": 26691, "title": "Collision with built in python \"logging\" module", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac OS Mojave\r\n- TensorFlow installed from (source or binary): pip \r\n- TensorFlow version (use command below): 2.0.0 alpha-0\r\n- Python version: 3.7.2\r\n\r\n**Describe the current behavior**\r\nWhen using Python internal \"logging\" module with Tensorflow, a logging module emits below warning message.\r\n\r\n> WARNING: Logging before flag parsing goes to stderr.\r\n\r\nAlso, it prints out duplicate logging message with my own formatter and something internally defined format(I guess) as below.\r\n\r\n> $ python3 test.py \r\n[INFO|test.py:29] 2019-03-14 17:30:14,402 >> Hello World.\r\nWARNING: Logging before flag parsing goes to stderr.\r\nI0314 17:30:14.402168 4528047552 test.py:29] Hello World.\r\n[INFO|test.py:30] 2019-03-14 17:30:14,402 >> This is message 1\r\nI0314 17:30:14.402369 4528047552 test.py:30] This is message 1\r\n\r\nWhen I try without importing Tensorflow, then it works properly.\r\n\r\n> $ python3 test.py \r\n[INFO|test.py:29] 2019-03-14 17:37:40,172 >> Hello World.\r\n[INFO|test.py:30] 2019-03-14 17:37:40,172 >> This is message 1\r\n\r\n**Describe the expected behavior**\r\nMaybe stream handler is colliding each other.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport logging\r\nimport tensorflow as tf\r\n\r\nformatter = logging.Formatter('[%(levelname)s|%(filename)s:%(lineno)s] %(asctime)s >> %(message)s')\r\n\r\nstream_logger = logging.getLogger('logger_stream')\r\nstream_logger.setLevel(logging.INFO)\r\n\r\n# Set handler\r\nstreamHandler = logging.StreamHandler()\r\nstreamHandler.setFormatter(formatter)\r\nstream_logger.addHandler(streamHandler)\r\n\r\nstream_logger.info(\"Hello World\")\r\nstream_logger.info(\"This is message 1\")\r\n```", "comments": ["Adding @caisq who I think might have more familiarity with this code (I haven't really touched the logger code myself).", "I ran into the same issue with 1.14rc1. I think this is caused by abseil instead of tensorflow: abseil/abseil-py#99\r\n\r\nWorkaround: https://github.com/dhalperi/pybatfish/blob/f8ddd3938148f9a5d9c14c371a099802c564fac3/pybatfish/client/capirca.py#L33-L50", "@pesser is right, this is due to absl logging upstream.\r\n\r\nIf possible, I'd recommend using the absl logger instead: https://github.com/abseil/abseil-py/blob/master/absl/logging/__init__.py\r\nThe absl logger installs itself in place of the default logger, so for the most part you can continue to use the regular logger.info(), etc. functions. Some configuration can be done directly in the module.\r\n\r\nExample of setting a custom format using absl logger for @skku-dhkim 's case:\r\n```\r\nPython 2.7.16 (default, Apr  6 2019, 01:42:57) \r\n[GCC 7.3.0] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import logging\r\n>>> import tensorflow as tf\r\n>>> import absl.logging\r\n>>> formatter = logging.Formatter('[%(levelname)s|%(filename)s:%(lineno)s] %(asctime)s >> %(message)s')\r\n>>> absl.logging.get_absl_handler().setFormatter(formatter)\r\n>>> stream_logger = logging.getLogger('logger_stream')\r\n>>> stream_logger.setLevel(logging.INFO)\r\n>>> stream_logger.info(\"Hello World\")\r\nWARNING: Logging before flag parsing goes to stderr.\r\n[INFO|<stdin>:1] 2019-07-01 13:37:51,808 >> Hello World\r\n```\r\n\r\nIf you're really set on using the default logger, you can use the mentioned workaround to disable absl logging.\r\n\r\nExample (@eldar 's case from #29842):\r\n```\r\nPython 2.7.16 (default, Apr  6 2019, 01:42:57) \r\n[GCC 7.3.0] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import logging\r\n>>> import tensorflow as tf\r\n>>> import absl.logging\r\n>>> logging.root.removeHandler(absl.logging._absl_handler)\r\n>>> absl.logging._warn_preinit_stderr = False\r\n>>> logging.basicConfig(filename='log.txt', filemode='w', datefmt='%Y-%m-%d %H:%M:%S', level=logging.INFO, format='%(asctime)-15s %(message)s')\r\n>>> logging.info('logs to file, as expected')\r\n```\r\n\r\n```\r\n$  cat log.txt\r\n2019-07-01 13:24:40 logs to file, as expected\r\n```\r\n\r\nClosing this issue as it's a (mostly intentional) change in behavior -- please follow up on the abseil-py repo if you have concerns about upstream.", "Breaking native logging was intended? Let me see if I understand this correctly. If I want my codebase to be compatible with tensorflow, my choice to either change all my loggers or to change all my loggers?\r\n\r\nIf I end up using absl, I'm not really solving the problem, I would just pass it on downstream.\r\n\r\nI'm a bit surprised by this outcome, to be honest. Are you sure this is the direction you want to go?\r\n\r\nI was under the impression that Tensorflow was more than an experimentation tool, it's supposed to be robust enough to embed into production code. To have to do a refactor (like changing all my loggers) each time I update doesn't exactly fit into that picture.", "Hi Kristian,\r\nPerhaps I misspoke: the intentional change here was the decision to adopt absl, in place of the forked versions of several core libraries that TensorFlow had been maintaining. The move to absl.app pulled in absl.logging, which seems to have some unfortunately complicated interactions with existing user logging.\r\nIdeally, this behavior would be fixed upstream in the abseil-py repo (as you can see by abseil/abseil-py#99, we're not the only ones bothered).\r\nI suppose we could give a shot at bundling the removeHandler() workaround directly into TensorFlow, though -- reopening while I play with this.", "Ah great thanks so much @revan", "@revan I really appreciate your understanding in this matter. The default logging library is in common use in the python eco system, and certain best practices are associated with it. One of them is that libraries shouldn't make decisions on how how to handle log items on behalf of developers. Quoting [Logging HOWTO](https://docs.python.org/3/howto/logging.html#configuring-logging-for-a-library):\r\n\r\n> **Note:** It is strongly advised that you _do not add any handlers other than `NullHandler` to your library\u2019s loggers_. This is because the configuration of handlers is the prerogative of the application developer who uses your library. The application developer knows their target audience and what handlers are most appropriate for their application: if you add handlers \u2018under the hood\u2019, you might well interfere with their ability to carry out unit tests and deliver logs which suit their requirements.\r\n\r\nI also use the logging library and was surprised and confused upon upgrading to TF 1.14.", "Update: I've got *some* sort of patch that appears to fix this, but it involves risky looking logic running in the `__init__.py` so I'm really hoping we can fix this upstream instead.\r\nMy concern with including the removeHandler workaround in our own import is that now we break anyone who intends to use absl.logging: since absl.logging installs itself at import time, subsequent imports won't reenable it. We can hack around this by deleting absl.logging from `sys.modules` to force a reimport, but this brings us solidly into Python's \"don't do this\" territory.\r\n\r\nAnyway, if we fix it here it wouldn't be out until the next TensorFlow release, so I'm afraid the workarounds noted earlier are the way to go for now.", "Thanks for looking into it @revan. Yeah, it's the tampering of global state by absl combined with the fact that things will start to rely on this tampering that makes it so tricky.\r\n\r\nI really hope that https://github.com/abseil/abseil-py/issues/99 will be properly addressed.", "abseil/abseil-py#99 has been fixed upstream, and the new v0.8.0 on pypi seems to work just as it should.\r\n\r\nI have a change in-flight to bump our dependency to 0.8.0, but since our current dep is \"> 0.7.1\" new pip installs of TensorFlow 1.14 should already be pulling the new absl so we're effectively fixed. Go ahead and `pip install --upgrade absl-py` on your existing install to get the fix.", "Thanks for the follow-up @revan ", "It works properly now. Thanks \ud83d\udc4d ", "One possible workaround, so long as you don't need your logger to propagate, is to set `propagate` on your logger to `False`:\r\n\r\n`my_logger.propagate = False`"]}, {"number": 26690, "title": "the elapse time shown in timeline not match with count by hand", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 2.7.5\r\n- CUDA/cuDNN version: 10/7\r\n- GPU model and memory: 16G\r\n\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI have ran a program and use time.time()-start_time to calculate the elapse time (I have run 1000 times to get the average time), which is 5ms. But when use timeline in one run, I got 10ms+. So I doubt that, does `FULL_TRACE` will cost more time?\r\n", "comments": ["@cklsoft I think you can write time for each iteration into a file and check mean, standard deviation, etc. I think it is possible that FULL TRACE will cost couple of ms. \r\n\r\nPlease post this kind of support questions in Stackoverflow. There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 26689, "title": "Installation issue with tensorflow : pywrap_tensorflow_internal ", "body": "**System information**\r\n- Windows 10 64 bit \r\n- TensorFlow installed from (source or binary): pip install tensorflow\r\n- TensorFlow version (or github SHA if from source): tensorflow 1.12.0\r\n\r\n\r\n\r\n```\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Osama\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"C:\\Program Files (x86)\\Python37-32\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Osama\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Osama\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Osama\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow_internal\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"FirstVideoDetection.py\", line 1, in <module>\r\n    from imageai.Detection import VideoObjectDetection\r\n  File \"C:\\Users\\Osama\\AppData\\Roaming\\Python\\Python37\\site-packages\\imageai\\Detection\\__init__.py\", line 3, in <module>\r\n    from imageai.Detection.keras_retinanet.models.resnet import resnet50_retinanet\r\n  File \"C:\\Users\\Osama\\AppData\\Roaming\\Python\\Python37\\site-packages\\imageai\\Detection\\keras_retinanet\\models\\resnet.py\", line 19, in <module>\r\n    import keras\r\n  File \"C:\\Users\\Osama\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"C:\\Users\\Osama\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n  File \"C:\\Users\\Osama\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n  File \"C:\\Users\\Osama\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\__init__.py\", line 89, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"C:\\Users\\Osama\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Osama\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Osama\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Osama\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Osama\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"C:\\Program Files (x86)\\Python37-32\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\nDuring handling of the above exception, another exception occurred:\r\n```\r\n\r\n\r\n\r\nI'm working on object detection in video, i've already installed the tensorflow packages, when i run the code it gives me an error stating\r\n\r\n **ModuleNotFoundError: No module named 'tensorflow.python._pywrap_tensorflow_internal'**\r\n\r\n**_### How do i solve this error ? ? ?_** \r\n\r\n", "comments": ["Are you trying to install TF-GPU version? Also what is your cpu model/make in your system?", "No I simply installed the basic version. I.e pip install tensorflow. Cpu\nversion is\nLenovo G40 - 80E400GTU S  5th Generation Intel Core i3-5005U 8 gb ram\nn Fri, Mar 15, 2019, 3:30 AM ymodak <notifications@github.com> wrote:\n\n> Are you trying to install TF-GPU version? Also what is your cpu model/make\n> in your system?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26689#issuecomment-473089727>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AuRma2NkyVPu7Mb2_YuvRD0-y7moenTjks5vWs2QgaJpZM4bzj7_>\n> .\n>\n", "Can you please describe the steps you followed to install TF? Also did you take a look at [Install TensorFlow with pip](https://www.tensorflow.org/install/pip)?", "Yes I did. I just did 'pip install tensorflow ' nothing else\n\nOn Tue, Mar 26, 2019, 7:35 AM ymodak <notifications@github.com> wrote:\n\n> Can you please describe the steps you followed to install TF? Also did you\n> take a look at Install TensorFlow with pip\n> <https://www.tensorflow.org/install/pip>?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26689#issuecomment-476450542>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AuRma9GgX-26SgVJOfzHM_nsWZ2qoEH4ks5vaYdvgaJpZM4bzj7_>\n> .\n>\n", "What version of python are you using? Note that TF 1.12 is not compatible with Python 3.7 you may have to switch to Python 3.6 or lower.\r\nAlso TF 1.13 supports Python 3.7", "Ok thanks. I'll check\n\nOn Thu, Mar 28, 2019, 2:18 AM ymodak <notifications@github.com> wrote:\n\n> What version of python are you using? Note that TF 1.12 is not compatible\n> with 3.7 you may have to switch to Python 3.6 or lower.\n> Also TF 1.13 supports Python 3.7\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26689#issuecomment-477350294>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AuRma3bvlO5tEXRV6MhQXh_E7lyewJYdks5va-AWgaJpZM4bzj7_>\n> .\n>\n", "@OsamaNaeem1995  Were you able to install TF successfully? ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26689\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26689\">No</a>\n", "Yes. I just installed an older version and it works fine\n\nOn Tue, Apr 9, 2019, 10:51 PM ymodak <notifications@github.com> wrote:\n\n> @OsamaNaeem1995 <https://github.com/OsamaNaeem1995> Were you able to\n> install TF successfully?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26689#issuecomment-481360132>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AuRmazMpb9NMQ6LPdxbXD8QPI1gBwsF3ks5vfNM_gaJpZM4bzj7_>\n> .\n>\n"]}, {"number": 26688, "title": "[TF2.0] Run sample of experts is not work on local", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):win10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:no\r\n- TensorFlow installed from (source or binary): pip install tensorflow-gpu==2.0.0-alpha0\r\n- TensorFlow version (use command below):conda list 2.0.0a0\r\n- Python version:Python 3.6.8 :: Anaconda custom (64-bit)\r\n- CUDA/cuDNN version:V10.0.130/7.5.0\r\n- GPU model and memory:GTX 1050 TI\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI follow this page:\r\nhttps://www.tensorflow.org/alpha#for-experts and like to \r\nhttps://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/quickstart/advanced.ipynb\r\nand run cell-1 is work fine on colab\r\nbut i copy the cell-1 code and run it from jupyter notebook in local and got error message:\r\n```\r\n---------------------------------------------------------------------------\r\nModuleNotFoundError                       Traceback (most recent call last)\r\n<ipython-input-24-30fd6eda57f1> in <module>\r\n----> 1 import tensorflow_datasets as tfds\r\n      2 from tensorflow.keras.layers import Dense, Flatten, Conv2D\r\n      3 from tensorflow.keras import Model\r\n\r\nModuleNotFoundError: No module named 'tensorflow_datasets'\r\n```\r\n\r\n**Describe the expected behavior**\r\nit'll be work\r\n\r\n**Other info / logs**\r\n\r\n", "comments": ["You need to install tensorflow datasets separately. Did you try this before importing?\r\n```pip install tensorflow-datasets```", "Hi, ymodak\r\nthis is work, thanks a lot\r\nBut i run this code to watch log by tensorboard, tf1.13 is work, but tf2.0 alpha throw the traceback:\r\n```\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-4-34b42149d051> in <module>\r\n----> 1 model.fit(x_train, y_train, epochs=50, callbacks=[tensor_board])\r\n      2 \r\n      3 model.evaluate(x_test, y_test)\r\n\r\n~\\Anaconda3\\envs\\lab\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    871           validation_steps=validation_steps,\r\n    872           validation_freq=validation_freq,\r\n--> 873           steps_name='steps_per_epoch')\r\n    874 \r\n    875   def evaluate(self,\r\n\r\n~\\Anaconda3\\envs\\lab\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\r\n    361         # Callbacks batch end.\r\n    362         batch_logs = cbks.make_logs(model, batch_logs, batch_outs, mode)\r\n--> 363         callbacks._call_batch_hook(mode, 'end', batch_index, batch_logs)\r\n    364         progbar.on_batch_end(batch_index, batch_logs)\r\n    365 \r\n\r\n~\\Anaconda3\\envs\\lab\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py in _call_batch_hook(self, mode, hook, batch, logs)\r\n    225     for callback in self.callbacks:\r\n    226       batch_hook = getattr(callback, hook_name)\r\n--> 227       batch_hook(batch, logs)\r\n    228     self._delta_ts[hook_name].append(time.time() - t_before_callbacks)\r\n    229 \r\n\r\n~\\Anaconda3\\envs\\lab\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py in on_train_batch_end(self, batch, logs)\r\n    507     \"\"\"\r\n    508     # For backwards compatibility.\r\n--> 509     self.on_batch_end(batch, logs=logs)\r\n    510 \r\n    511   def on_test_batch_begin(self, batch, logs=None):\r\n\r\n~\\Anaconda3\\envs\\lab\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py in on_batch_end(self, batch, logs)\r\n   1275     self._total_batches_seen += 1\r\n   1276     if self._is_tracing:\r\n-> 1277       self._log_trace()\r\n   1278     elif (not self._is_tracing and\r\n   1279           self._total_batches_seen == self._profile_batch - 1):\r\n\r\n~\\Anaconda3\\envs\\lab\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py in _log_trace(self)\r\n   1306             name='batch_%d' % self._total_batches_seen,\r\n   1307             step=self._total_batches_seen,\r\n-> 1308             profiler_outdir=self.log_dir)\r\n   1309       self._is_tracing = False\r\n   1310 \r\n\r\n~\\Anaconda3\\envs\\lab\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py in trace_export(name, step, profiler_outdir)\r\n   1142 \r\n   1143   if profiler:\r\n-> 1144     _profiler.save(profiler_outdir, _profiler.stop())\r\n   1145 \r\n   1146   trace_off()\r\n\r\n~\\Anaconda3\\envs\\lab\\lib\\site-packages\\tensorflow\\python\\eager\\profiler.py in save(logdir, result)\r\n    120       logdir, 'plugins', 'profile',\r\n    121       datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\r\n--> 122   gfile.MakeDirs(plugin_dir)\r\n    123   with gfile.Open(os.path.join(plugin_dir, 'local.trace'), 'wb') as f:\r\n    124     f.write(result)\r\n\r\n~\\Anaconda3\\envs\\lab\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py in recursive_create_dir(dirname)\r\n    446     errors.OpError: If the operation fails.\r\n    447   \"\"\"\r\n--> 448   recursive_create_dir_v2(dirname)\r\n    449 \r\n    450 \r\n\r\n~\\Anaconda3\\envs\\lab\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py in recursive_create_dir_v2(path)\r\n    462   \"\"\"\r\n    463   with errors.raise_exception_on_not_ok_status() as status:\r\n--> 464     pywrap_tensorflow.RecursivelyCreateDir(compat.as_bytes(path), status)\r\n    465 \r\n    466 \r\n\r\n~\\Anaconda3\\envs\\lab\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)\r\n    546             None, None,\r\n    547             compat.as_text(c_api.TF_Message(self.status.status)),\r\n--> 548             c_api.TF_GetCode(self.status.status))\r\n    549     # Delete the underlying status object from memory otherwise it stays alive\r\n    550     # as there is a reference to status from this from the traceback due to\r\n\r\nNotFoundError: Failed to create a directory: ./logs/test\\plugins\\profile\\2019-03-15_11-19-41; No such file or directory\r\n```\r\n\r\ncode:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.callbacks import TensorBoard\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10, activation='softmax')\r\n])\r\ntensor_board = TensorBoard('./logs/test')\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\nmodel.fit(x_train, y_train, epochs=50, callbacks=[tensor_board])\r\nmodel.evaluate(x_test, y_test)\r\n```", "Glad it worked. The new code snippet you provided looks incomplete. Also since this is a different issue can you please post it separately and we track it there. Make sure you provide all the information asked by the template in addition to your complete code snippet. Thanks!"]}, {"number": 26687, "title": "Iterate on dataset/iterator when using @tf.function", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Archlinux\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-dev20190312\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: 1080 Ti\r\n\r\n**Describe the current behavior**\r\n\r\nCan't convert the dataset/iterator to its graph representation: I'm forced to loop in eager mode, but I want to have a graph representation of the loop too (since I need to export a SavedModel that contains the loop itself).\r\n\r\n**Describe the expected behavior**\r\n\r\nThe iterator / the dataset should be converted in its graph representation.\r\n\r\n**Code to reproduce the issue**\r\n\r\nA code that loops using Tensorflow primitives (like `for i in tf.range(10)`) can be converted into its graph representation without any problem, while a code that creates a python iterator from a dataset object (using `iter(dataset)`) can't.\r\n\r\nMoreover, maybe because we are in the early stage of the development, I'm unable to convert a loop that loops over a dataset to its graph representation.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef real_gen():\r\n    for i in range(10):\r\n        yield i\r\n\r\ndataset = tf.data.Dataset.from_generator(real_gen, (tf.float32))\r\n\r\n@tf.function\r\ndef itertest():\r\n    tf.print(next(iter(dataset)))\r\nitertest()\r\n\r\n@tf.function\r\ndef loopiter():\r\n    for real in dataset:\r\n        tf.print(real)\r\nloopiter()\r\n```\r\n\r\nThe `itertest()` call fails because `RuntimeError: dataset.__iter__() is only supported when eager execution is enabled.`\r\n\r\nWhile the `loopiter()` call fails because `tensorflow.python.framework.errors_impl.NotFoundError: Function __inference_Dataset_flat_map_flat_map_fn_22 is not defined.\r\n         [[{{node ReduceDataset}}]] [Op:__inference_loopiter_35]`", "comments": ["I am closing this issue as this should now be supported as for TF 1.14 and TF 2.0-beta. Please verify and if you encounter a problem re-open this issue with up-to-date information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26687\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26687\">No</a>\n"]}, {"number": 26686, "title": "When using tf.gradients in the forward pass, my codes stop at `compute_gradients`", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary (conda install)\r\n- TensorFlow version (use command below): 1.9.0\r\n- Python version: 3.5\r\n- CUDA/cuDNN version: 390.25\r\n- GPU model and memory: GTX1080, 11GB\r\n\r\n\r\n### 1. What I did:\r\nMy codes like this example: [tensorpack faster rcnn example](https://github.com/tensorpack/tensorpack/blob/f417c49fe45759fb2c69cafcabe6613ea85ec469/examples/FasterRCNN/train.py#L98)\r\n\r\nI used DetectModel as the base class to implement the detection model of the SSD architecture. The training, optimizer, and get_inference_tensor_names are unchanged, the build_graph is reimplemented in the subclass, and the preprocess is modified. Other modules (such as anchor_generator, target_assign), I used the code in tensorflow offical models] (https://github.com/tensorflow/models/tree/master/research/object_detection).\r\n\r\nmy codes as follow\r\n```\r\n\r\nclass MyMetaArch(ModelDesc):\r\n\r\n    def inputs(self):\r\n        ret = [\r\n            tf.placeholder(tf.float32, (None, None, None, 1), 'image'),\r\n            tf.placeholder(tf.float32, (8, None, 4), 'gt_boxes'),\r\n            tf.placeholder(tf.int64, (8, None,), 'gt_labels')\r\n        ]\r\n        return ret\r\n\r\n    def build_graph(self, *inputs):\r\n        inputs = dict(zip(self.input_names, inputs))\r\n        images = inputs[\"image\"]\r\n\r\n        x = Conv2D('conv1', images, filters=16, kernel_size=3, strides=(2, 2))\r\n        x = Conv2D('conv2', x, filters=32, kernel_size=3, strides=(2, 2))\r\n        x = Conv2D('conv3', x, filters=64, kernel_size=3, strides=(2, 2))\r\n        x = Conv2D('conv4', x, filters=64, kernel_size=3, strides=(2, 2))\r\n        ...\r\n        class_prediction_with_background = tf.concat(...) # concat multi branches \r\n        class_prediction_with_background = class_prediction_with_background [:,:,1] # face detector, only care about the foreground\r\n\r\n        gradients = tf.gradients(class_prediction_with_background , [images])[0]\r\n        regular_vals = tf.reduce_mean(gradients)\r\n        ...\r\n        class_losses = class_losses + regular_vals\r\n```\r\n### 2. What I observed:\r\n(1) **Include the ENTIRE logs here:**\r\nBut there are not any errors. The runtime log as follows\uff0c and and I added some extra logs.\r\n\r\n```\r\n\u001b[32m[0312 17:31:05 @interface.py:86]\u001b[0m setup_graph\r\n\u001b[32m[0312 17:31:05 @tower.py:217]\u001b[0m _setup_input\r\n\u001b[32m[0312 17:31:05 @input_source.py:220]\u001b[0m Setting up the queue 'QueueInput/input_queue' with size Tensor(\"QueueInput/input_queue_Size:0\", shape=(), dtype=int32) for CPU prefetching ...\r\n\u001b[32m[0312 17:31:05 @tower.py:219]\u001b[0m _setup_graph\r\n\u001b[32m[0312 17:31:05 @trainers.py:186]\u001b[0m SyncMultiGPUTrainerReplicated _setup_graph\r\n\u001b[32m[0312 17:31:05 @trainers.py:189]\u001b[0m _make_get_grad_fn\r\n\u001b[32m[0312 17:31:05 @trainers.py:191]\u001b[0m call_for_each_tower\r\n\u001b[32m[0312 17:31:05 @training.py:109]\u001b[0m Building graph for training tower 0 on device /gpu:0 ...\r\n\u001b[32m[0312 17:31:05 @training.py:116]\u001b[0m override_to_local_variable /gpu:0\r\n\u001b[32m[0312 17:31:05 @tower.py:248]\u001b[0m get_grad_fn: start\r\n\u001b[32m[0312 17:31:05 @tower.py:280]\u001b[0m NOT XLA_COMPILE: compute_grad_from_inputs\r\n\u001b[32m[0312 17:31:05 @tower.py:253]\u001b[0m compute_grad_from_inputs start\r\n\u001b[32m[0312 17:31:05 @model_desc.py:262]\u001b[0m start build_graph\r\n\u001b[32m[0312 17:31:06 @registry.py:125]\u001b[0m conv1/conv1 input: [8, 644, 644, 1]\r\n\u001b[32m[0312 17:31:06 @registry.py:133]\u001b[0m conv1/conv1 output: [8, 160, 160, 12]\r\n...\r\n\u001b[32m[0312 17:31:07 @registry.py:125]\u001b[0m conv3_1 input: [8, 20, 20, 64]\r\n\u001b[32m[0312 17:31:08 @registry.py:133]\u001b[0m conv3_1 output: [8, 20, 20, 128]\r\n\u001b[32m[0312 17:31:08 @registry.py:125]\u001b[0m conv3_2 input: [8, 22, 22, 128]\r\n\u001b[32m[0312 17:31:08 @registry.py:133]\u001b[0m conv3_2 output: [8, 10, 10, 256]\r\n\u001b[32m[0312 17:31:08 @registry.py:125]\u001b[0m conv4_1 input: [8, 10, 10, 256]\r\n\u001b[32m[0312 17:31:08 @registry.py:133]\u001b[0m conv4_1 output: [8, 10, 10, 128]\r\n\u001b[32m[0312 17:31:08 @registry.py:125]\u001b[0m conv4_2 input: [8, 12, 12, 128]\r\n\u001b[32m[0312 17:31:08 @registry.py:133]\u001b[0m conv4_2 output: [8, 5, 5, 256]\r\n\u001b[32m[0312 17:31:08 @registry.py:125]\u001b[0m prediction_layers/box_encoding_predictor_0 input: [8, 20, 20, 64]\r\n\u001b[32m[0312 17:31:08 @registry.py:133]\u001b[0m prediction_layers/box_encoding_predictor_0 output: [8, 20, 20, 84]\r\n\u001b[32m[0312 17:31:08 @registry.py:125]\u001b[0m prediction_layers/class_predictor_0 input: [8, 20, 20, 64]\r\n\u001b[32m[0312 17:31:08 @registry.py:133]\u001b[0m prediction_layers/class_predictor_0 output: [8, 20, 20, 42]\r\n\u001b[32m[0312 17:31:08 @registry.py:125]\u001b[0m prediction_layers/box_encoding_predictor_1 input: [8, 10, 10, 256]\r\n\u001b[32m[0312 17:31:08 @registry.py:133]\u001b[0m prediction_layers/box_encoding_predictor_1 output: [8, 10, 10, 4]\r\n\u001b[32m[0312 17:31:08 @registry.py:125]\u001b[0m prediction_layers/class_predictor_1 input: [8, 10, 10, 256]\r\n\u001b[32m[0312 17:31:08 @registry.py:133]\u001b[0m prediction_layers/class_predictor_1 output: [8, 10, 10, 2]\r\n\u001b[32m[0312 17:31:08 @registry.py:125]\u001b[0m prediction_layers/box_encoding_predictor_2 input: [8, 5, 5, 256]\r\n\u001b[32m[0312 17:31:08 @registry.py:133]\u001b[0m prediction_layers/box_encoding_predictor_2 output: [8, 5, 5, 4]\r\n\u001b[32m[0312 17:31:08 @registry.py:125]\u001b[0m prediction_layers/class_predictor_2 input: [8, 5, 5, 256]\r\n\u001b[32m[0312 17:31:08 @registry.py:133]\u001b[0m prediction_layers/class_predictor_2 output: [8, 5, 5, 2]\r\n\u001b[32m[0312 17:31:11 @FaceboxesModel.py:325]\u001b[0m cls_losses: Tensor(\"tower0/Loss/Loss_1/mul:0\", shape=(8, 8525), dtype=float32, device=/device:GPU:0)\r\n\u001b[32m[0312 17:31:13 @FaceboxesModel.py:236]\u001b[0m start regularize_cost\r\n\u001b[32m[0312 17:31:13 @regularize.py:95]\u001b[0m regularize_cost() found 35 variables to regularize.\r\n\u001b[32m[0312 17:31:13 @regularize.py:20]\u001b[0m The following tensors will be regularized: conv1/conv1/W:0,...,conv3_1/W:0, conv3_2/W:0, conv4_1/W:0, conv4_2/W:0,...\r\n\u001b[32m[0312 17:31:13 @FaceboxesModel.py:239]\u001b[0m end regularize_cost\r\n\u001b[32m[0312 17:31:13 @FaceboxesModel.py:242]\u001b[0m start add_n\r\n\u001b[32m[0312 17:31:13 @FaceboxesModel.py:244]\u001b[0m end add_n\r\n\u001b[32m[0312 17:31:13 @model_desc.py:264]\u001b[0m end build_graph\r\n\u001b[32m[0312 17:31:13 @model_desc.py:268]\u001b[0m _build_graph_get_cost return Tensor(\"tower0/total_cost:0\", shape=(), dtype=float32, device=/device:GPU:0)\r\n\u001b[32m[0312 17:31:13 @tower.py:255]\u001b[0m get_cost_fn Tensor(\"tower0/total_cost:0\", shape=(), dtype=float32, device=/device:GPU:0)\r\n\u001b[32m[0312 17:31:13 @tower.py:268]\u001b[0m get_opt_fn\r\n```\r\nThe code seems to be stuck at this line: https://github.com/tensorpack/tensorpack/blob/master/tensorpack/train/tower.py#L261\r\nThe function is `compute_gradients`", "comments": ["As I have commented in the [original issue](https://github.com/tensorpack/tensorpack/issues/1103#issuecomment-472099735), please try to provide enough information (code, instructions) so that others can __reproduce__ your issues. \r\nOtherwise it's unlikely your issues can be timely diagnosed. See also https://stackoverflow.com/help/mcve", "@ppwwyyxx I think maybe caused by tf.concat. I try use the convolution before concat as a parameter of tf.gradients , the graph can be created.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n"]}, {"number": 26685, "title": "build from src for //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed", "body": "commit e7f508844eab13927f32abdb5d7f0eb4e91caee0\r\n**System information**\r\n\r\n== cat /etc/issue ===============================================\r\nLinux luban-350 3.10.0-514.16.1.el7.x86_64 #1 SMP Wed Apr 12 15:04:24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"7 (Core)\"\r\nVERSION_ID=\"7\"\r\nCENTOS_MANTISBT_PROJECT_VERSION=\"7\"\r\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7\"\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (GCC) 8.3.0\r\nCopyright (C) 2018 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux luban-350 3.10.0-514.16.1.el7.x86_64 #1 SMP Wed Apr 12 15:04:24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy                            1.14.1   \r\nprotobuf                         3.5.2    \r\ntensorflow                       1.5.0rc1 \r\ntensorflow-tensorboard           1.5.1    \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.5.0-rc1\r\ntf.GIT_VERSION = v1.5.0-rc1-4-g04b3f6c\r\ntf.COMPILER_VERSION = v1.5.0-rc1-4-g04b3f6c\r\nSanity check: array([1], dtype=int32)\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"tensorflow/python/pywrap_tensorflow.py\", line 25, in <module>\r\n    from tensorflow.python.platform import self_check\r\n  File \"tensorflow/python/platform/self_check.py\", line 27, in <module>\r\n    raise ImportError(\"Could not import tensorflow. Do not import tensorflow \"\r\nImportError: Could not import tensorflow. Do not import tensorflow from its source directory; change directory to outside the TensorFlow source tree, and relaunch your Python interpreter from there.\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nThu Mar 14 15:31:33 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 410.72       Driver Version: 410.72       CUDA Version: 10.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla P40           Off  | 00000000:02:00.0 Off |                  N/A |\r\n| N/A   25C    P0    52W / 250W |     10MiB / 22919MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla P40           Off  | 00000000:03:00.0 Off |                  N/A |\r\n| N/A   22C    P0    53W / 250W |     10MiB / 22919MiB |     64%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  Tesla P40           Off  | 00000000:83:00.0 Off |                  N/A |\r\n| N/A   41C    P0    70W / 250W |  16779MiB / 22919MiB |     93%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  Tesla P40           Off  | 00000000:84:00.0 Off |                  N/A |\r\n| N/A   23C    P0    53W / 250W |     10MiB / 22919MiB |     99%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-10.0/lib64/libcudart_static.a\r\n/usr/local/cuda-10.0/lib64/libcudart.so.10.0.130\r\n/usr/local/cuda-10.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-10.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-9.0/lib64/libcudart.so.9.0.176\r\n/usr/local/cuda-9.0/lib64/libcudart_static.a\r\n[luban@luban-350 tensorflow]$ \r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nbazel  build -c opt -- //tensorflow/python/tools/...\r\n\r\n\r\n**Any other info / logs**\r\n```\r\ncc1plus: warning: unrecognized command line option '-Wno-writable-strings'\r\nERROR: /home/luban/zhanghui/tensorflow/tensorflow/python/keras/api/BUILD:28:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/home/luban/.cache/bazel/_bazel_luban/64ebbf8410c89b3c0f2d97b092db8898/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"/home/luban/.cache/bazel/_bazel_luban/64ebbf8410c89b3c0f2d97b092db8898/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 83, in <module>\r\n    from tensorflow.python import keras\r\n  File \"/home/luban/.cache/bazel/_bazel_luban/64ebbf8410c89b3c0f2d97b092db8898/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/keras/__init__.py\", line 30, in <module>\r\n    from tensorflow.python.keras import datasets\r\n  File \"/home/luban/.cache/bazel/_bazel_luban/64ebbf8410c89b3c0f2d97b092db8898/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/keras/datasets/__init__.py\", line 25, in <module>\r\n    from tensorflow.python.keras.datasets import imdb\r\n  File \"/home/luban/.cache/bazel/_bazel_luban/64ebbf8410c89b3c0f2d97b092db8898/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/keras/datasets/imdb.py\", line 25, in <module>\r\n    from tensorflow.python.keras.preprocessing.sequence import _remove_long_seq\r\n  File \"/home/luban/.cache/bazel/_bazel_luban/64ebbf8410c89b3c0f2d97b092db8898/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/keras/preprocessing/__init__.py\", line 21, in <module>\r\n    import keras_preprocessing\r\nImportError: No module named keras_preprocessing\r\nINFO: Elapsed time: 2623.895s, Critical Path: 178.05s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 11634 processes: 11634 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "comments": ["I  have the same problem, can you tell me how to solve it ?", "Me too.\r\n\r\n> commit [e7f5088](https://github.com/tensorflow/tensorflow/commit/e7f508844eab13927f32abdb5d7f0eb4e91caee0)\r\n> **System information**\r\n> \r\n> == cat /etc/issue ===============================================\r\n> Linux luban-350 3.10.0-514.16.1.el7.x86_64 #1 SMP Wed Apr 12 15:04:24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n> VERSION=\"7 (Core)\"\r\n> VERSION_ID=\"7\"\r\n> CENTOS_MANTISBT_PROJECT_VERSION=\"7\"\r\n> REDHAT_SUPPORT_PRODUCT_VERSION=\"7\"\r\n> \r\n> == are we in docker =============================================\r\n> Yes\r\n> \r\n> == compiler =====================================================\r\n> c++ (GCC) 8.3.0\r\n> Copyright (C) 2018 Free Software Foundation, Inc.\r\n> This is free software; see the source for copying conditions. There is NO\r\n> warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n> \r\n> == uname -a =====================================================\r\n> Linux luban-350 3.10.0-514.16.1.el7.x86_64 #1 SMP Wed Apr 12 15:04:24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n> \r\n> == check pips ===================================================\r\n> numpy 1.14.1\r\n> protobuf 3.5.2\r\n> tensorflow 1.5.0rc1\r\n> tensorflow-tensorboard 1.5.1\r\n> \r\n> == check for virtualenv =========================================\r\n> False\r\n> \r\n> == tensorflow import ============================================\r\n> tf.VERSION = 1.5.0-rc1\r\n> tf.GIT_VERSION = v1.5.0-rc1-4-g04b3f6c\r\n> tf.COMPILER_VERSION = v1.5.0-rc1-4-g04b3f6c\r\n> Sanity check: array([1], dtype=int32)\r\n> Traceback (most recent call last):\r\n> File \"\", line 1, in\r\n> File \"tensorflow/**init**.py\", line 24, in\r\n> from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import\r\n> File \"tensorflow/python/**init**.py\", line 49, in\r\n> from tensorflow.python import pywrap_tensorflow\r\n> File \"tensorflow/python/pywrap_tensorflow.py\", line 25, in\r\n> from tensorflow.python.platform import self_check\r\n> File \"tensorflow/python/platform/self_check.py\", line 27, in\r\n> raise ImportError(\"Could not import tensorflow. Do not import tensorflow \"\r\n> ImportError: Could not import tensorflow. Do not import tensorflow from its source directory; change directory to outside the TensorFlow source tree, and relaunch your Python interpreter from there.\r\n> \r\n> == env ==========================================================\r\n> LD_LIBRARY_PATH /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:\r\n> DYLD_LIBRARY_PATH is unset\r\n> \r\n> == nvidia-smi ===================================================\r\n> Thu Mar 14 15:31:33 2019\r\n> +-----------------------------------------------------------------------------+\r\n> | NVIDIA-SMI 410.72 Driver Version: 410.72 CUDA Version: 10.0 |\r\n> |-------------------------------+----------------------+----------------------+\r\n> | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |\r\n> | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |\r\n> |===============================+======================+======================|\r\n> | 0 Tesla P40 Off | 00000000:02:00.0 Off | N/A |\r\n> | N/A 25C P0 52W / 250W | 10MiB / 22919MiB | 100% Default |\r\n> +-------------------------------+----------------------+----------------------+\r\n> | 1 Tesla P40 Off | 00000000:03:00.0 Off | N/A |\r\n> | N/A 22C P0 53W / 250W | 10MiB / 22919MiB | 64% Default |\r\n> +-------------------------------+----------------------+----------------------+\r\n> | 2 Tesla P40 Off | 00000000:83:00.0 Off | N/A |\r\n> | N/A 41C P0 70W / 250W | 16779MiB / 22919MiB | 93% Default |\r\n> +-------------------------------+----------------------+----------------------+\r\n> | 3 Tesla P40 Off | 00000000:84:00.0 Off | N/A |\r\n> | N/A 23C P0 53W / 250W | 10MiB / 22919MiB | 99% Default |\r\n> +-------------------------------+----------------------+----------------------+\r\n> \r\n> +-----------------------------------------------------------------------------+\r\n> | Processes: GPU Memory |\r\n> | GPU PID Type Process name Usage |\r\n> |=============================================================================|\r\n> +-----------------------------------------------------------------------------+\r\n> \r\n> == cuda libs ===================================================\r\n> /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61\r\n> /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a\r\n> /usr/local/cuda-10.0/lib64/libcudart_static.a\r\n> /usr/local/cuda-10.0/lib64/libcudart.so.10.0.130\r\n> /usr/local/cuda-10.0/doc/man/man7/libcudart.7\r\n> /usr/local/cuda-10.0/doc/man/man7/libcudart.so.7\r\n> /usr/local/cuda-9.0/doc/man/man7/libcudart.7\r\n> /usr/local/cuda-9.0/doc/man/man7/libcudart.so.7\r\n> /usr/local/cuda-9.0/lib64/libcudart.so.9.0.176\r\n> /usr/local/cuda-9.0/lib64/libcudart_static.a\r\n> [luban@luban-350 tensorflow]$\r\n> \r\n> **Describe the problem**\r\n> \r\n> bazel build -c opt -- //tensorflow/python/tools/...\r\n> \r\n> **Any other info / logs**\r\n> \r\n> ```\r\n> cc1plus: warning: unrecognized command line option '-Wno-writable-strings'\r\n> ERROR: /home/luban/zhanghui/tensorflow/tensorflow/python/keras/api/BUILD:28:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1)\r\n> Traceback (most recent call last):\r\n>   File \"/home/luban/.cache/bazel/_bazel_luban/64ebbf8410c89b3c0f2d97b092db8898/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n>     from tensorflow.python.tools.api.generator import doc_srcs\r\n>   File \"/home/luban/.cache/bazel/_bazel_luban/64ebbf8410c89b3c0f2d97b092db8898/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 83, in <module>\r\n>     from tensorflow.python import keras\r\n>   File \"/home/luban/.cache/bazel/_bazel_luban/64ebbf8410c89b3c0f2d97b092db8898/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/keras/__init__.py\", line 30, in <module>\r\n>     from tensorflow.python.keras import datasets\r\n>   File \"/home/luban/.cache/bazel/_bazel_luban/64ebbf8410c89b3c0f2d97b092db8898/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/keras/datasets/__init__.py\", line 25, in <module>\r\n>     from tensorflow.python.keras.datasets import imdb\r\n>   File \"/home/luban/.cache/bazel/_bazel_luban/64ebbf8410c89b3c0f2d97b092db8898/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/keras/datasets/imdb.py\", line 25, in <module>\r\n>     from tensorflow.python.keras.preprocessing.sequence import _remove_long_seq\r\n>   File \"/home/luban/.cache/bazel/_bazel_luban/64ebbf8410c89b3c0f2d97b092db8898/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/keras/preprocessing/__init__.py\", line 21, in <module>\r\n>     import keras_preprocessing\r\n> ImportError: No module named keras_preprocessing\r\n> INFO: Elapsed time: 2623.895s, Critical Path: 178.05s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\n> INFO: 11634 processes: 11634 local.\r\n> FAILED: Build did NOT complete successfully\r\n> ```\r\n\r\nMe too have same issue. How to fix it?", "See https://github.com/tensorflow/tensorflow/issues/36903 which worked for me. Basically:\r\n\r\npip3 install Keras-Preprocessing\r\n"]}, {"number": 26684, "title": "Repeatedly allocating a graph and summary writer leaks memory", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I have\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): `v1.12.0-10061-gf3954bf900 1.13.1`\r\n- Python version: 3.7.2\r\n- Bazel version (if compiling from source): 0.23.1\r\n- GCC/Compiler version (if compiling from source): `Apple LLVM version 10.0.0 (clang-1000.11.45.5)`\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nRepeatedly allocating a graph and making a summary writer leaks memory.\r\n\r\n**Describe the expected behavior**\r\n\r\nMemory should be freed when the graph leaves scope.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\n#!/usr/bin/env python3\r\n\r\nimport resource\r\nimport tensorflow as tf\r\n\r\nprev = 0\r\nwhile True:\r\n    peak = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\r\n    print(f'peak memory = {peak:,} (+{peak-prev:,})')\r\n    prev = peak\r\n\r\n    with tf.Graph().as_default(), tf.init_scope():\r\n        tf.contrib.summary.create_file_writer('/tmp/tb')\r\n```\r\n\r\n**Other info / logs**\r\n\r\nHere's what the output looks like:\r\n\r\n```\r\npeak memory = 174,493,696 (+174,493,696)\r\n  \r\nWARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\nIf you depend on functionality not listed there, please file an issue.\r\n\r\npeak memory = 202,215,424 (+27,721,728)\r\npeak memory = 202,264,576 (+49,152)\r\npeak memory = 202,309,632 (+45,056)\r\npeak memory = 202,358,784 (+49,152)\r\npeak memory = 202,432,512 (+73,728)\r\npeak memory = 202,473,472 (+40,960)\r\npeak memory = 202,522,624 (+49,152)\r\npeak memory = 202,567,680 (+45,056)\r\npeak memory = 202,604,544 (+36,864)\r\npeak memory = 202,641,408 (+36,864)\r\npeak memory = 202,694,656 (+53,248)\r\npeak memory = 202,739,712 (+45,056)\r\npeak memory = 202,784,768 (+45,056)\r\npeak memory = 202,829,824 (+45,056)\r\npeak memory = 202,878,976 (+49,152)\r\npeak memory = 202,919,936 (+40,960)\r\npeak memory = 202,981,376 (+61,440)\r\npeak memory = 203,026,432 (+45,056)\r\npeak memory = 203,067,392 (+40,960)\r\n...\r\npeak memory = 999,665,664 (+49,152)\r\npeak memory = 999,718,912 (+53,248)\r\npeak memory = 999,768,064 (+49,152)\r\npeak memory = 999,817,216 (+49,152)\r\npeak memory = 999,866,368 (+49,152)\r\npeak memory = 999,915,520 (+49,152)\r\npeak memory = 999,964,672 (+49,152)\r\npeak memory = 1,000,009,728 (+45,056)\r\npeak memory = 1,000,058,880 (+49,152)\r\npeak memory = 1,000,108,032 (+49,152)\r\npeak memory = 1,000,161,280 (+53,248)\r\npeak memory = 1,000,202,240 (+40,960)\r\npeak memory = 1,000,255,488 (+53,248)\r\n...\r\n```", "comments": ["Forgot that I was using a slightly messy TensorFlow tree.  I've reconfirmed that the bug persists at https://github.com/tensorflow/tensorflow/pull/26705, which is 5b24fba0857394dab67359963726b3bcce071575 plus a one line header include addition to make it build on my machine.", "No more TF bugs, @skye? :)", "@nfelt are you familiar with summary writers?", "@skye To clarify what I wrote wasn't intended to ask you to do anything, was just expressing sympathy as a fellow ex-tensorflow person who occasionally gets added to bugs.", "Haha no worries @girving, I just switched teams this week, so still figuring out what to do with all my github issues :)", "Thanks for the report.  I can reproduce this against last night's `tf-nightly` on both macOS and Linux (for anyone else reproducing, maxrss is in bytes on macOS but kb on linux, so the raw numbers are about 1000x smaller).\r\n\r\nI also could still reproduce this even with `tf.init_scope()` removed.\r\n\r\ncc @alextp if you have any intuition about what might be causing this.\r\n\r\n", "@rohan100jain I think the issue is that SUMMARY_WRITER_INIT_OP keeps a strong reference to the graph instead of a weak reference, so the graph can never be GC'd.\r\n", "It looks like there are two problems, one large and one small:\r\n\r\n**Large:** Every `tf.Operation` has a strong reference to the graph it lives inside (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L1921).  This should probably be a weak reference.  This is presumably the cause of the leak, since we store a long lived reference to the op in `_SUMMARY_WRITER_INIT_OP ` at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/summary_ops_v2.py#L221.\r\n\r\n**Small:** `_SUMMARY_WRITER_INIT_OP` should be something like a https://docs.python.org/3/library/weakref.html#weakref.WeakKeyDictionary with keys being the graphs, not keys being strings that live forever.  Even if we fix the large issue the small issue would remain, and thus pretty much any use of `_graph_key` is a bug.", "Update: I made a brief attempt at removing that one reference, but it didn't work, so I think there are others.  Probably a more concerted push is required.", "@alextp @rohan100jain Did you get a chance to look at this?  I want to make sure it isn't dropped, since it's blocking my upgrade of TensorFlow.", "@nfelt can you look?", "What's the likely ETA here?  I'm still blocked from upgrading due to this, so if no one is planning to fix I'd like to know for my own planning purposes.  In particular, if it's going to be weeks more I will have to fix it myself, but then you have to be happy with my weakref design decisions.", "I will try to take a closer look today, but if _SUMMARY_WRITER_INIT_OP isn't the main issue then I don't have a good guess at what the leak might be so it may take some time to figure this out.\r\n\r\nJust so I understand, which upgrade path exactly are you blocked on?  Is this upgrading from tf.summary to tf.contrib.summary?  Or did you notice that this leak occurs across a TF version update?", "@nfelt Thanks!  The leak appears going from TensorFlow 1.12 to 1.13, so it's blocking the 1.13 upgrade (and therefore also my Python 3.7 upgrade).", "Note that it's quite possibly I simply failed to fix the `_SUMMARY_WRITER_INIT_OP` reference, but as I mentioned it does seem like every use of `_graph_key` is a bug, and therefore it feels likely that there are other strong references.", "Can you check whether using `gc` frees the graph?\r\n\r\nI.e., is this a simple reference cycle problem, or is a reference to the graph kept hidden someplace? ", "Ah... Of course as long as the summary init op reference exists, that won't help. ", "Nope, `gc.collect` has no effect.", "Per discussion with @alextp I think what's easiest here is to revert the part of https://github.com/tensorflow/tensorflow/commit/aa8f428a9310b3fd8371bddf612e480b27618b2e that changed this from a graph collection to a python dict.  That seems very likely to be the root cause, and it was changed due to deprecation of global collections, but this is a 1.x-only usage anyway, and that seems like the easiest way to fix the regression.", "That sounds good.  Arguably global variables with references to graphs should be even more deprecated. :)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26684\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26684\">No</a>\n", "I've confirmed that this fixes both my minimized test case and my unminimized original code.  Thank you @nfelt!", "Glad to hear that :)"]}, {"number": 26683, "title": "TFRecordWriter.flush() do not seem to flush", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.5 LTS\r\nTensorFlow installed from (source or binary): pip\r\nTensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0\r\nPython version: 3.5.2:\r\nCUDA/cuDNN version: cuda-9.0, cudnn-7.3.1\r\nGPU model and memory: Titan Xp, Titan V\r\n\r\n\r\n\r\n**Describe the current behavior**\r\n```\r\nimport tensorflow as tf\r\n\r\nmystring = 'start blahblahblah end\\n' * 10000\r\n\r\n# https://www.zlib.net/manual.html\r\nZ_NO_FLUSH = 0\r\nZ_PARTIAL_FLUSH = 1\r\nZ_SYNC_FLUSH = 2\r\nZ_FULL_FLUSH = 3\r\n\r\nwriter = tf.python_io.TFRecordWriter('data', tf.io.TFRecordOptions(flush_mode=Z_NO_FLUSH))\r\nwriter.write(mystring.encode())\r\n# << - at this point, mystring is not fully written on disk as expected.\r\nwriter.flush()\r\n# << - still not fully written\r\nwriter.close()\r\n# << - now it is fully written\r\n```\r\n\r\nI found `flush_mode` option in `tf.io.TFRecordOptions`.\r\nAlthough it is only effect `compression_type` is None but anyway I tried from `0` (Z_NO_FLUSH) to `3` (Z_FULL_FLUSH) as stated in https://www.zlib.net/manual.html, none of them work.\r\nDo I miss something or is it a bug?\r\n", "comments": ["@jsimsa Can you take a look or reassign to someone who knows about record writing?", "@cih9088 the `flush_mode` option is only meaningful when the compression type is ZLIB. For other compression types (e.g. `None` used by your example) it will have no effect.", "Is this a bug or intended? https://github.com/tensorflow/tensorflow/pull/27219 seems to try to fix the issue.", "@cih9088,\r\nCan you please let us know if [this PR](https://github.com/tensorflow/tensorflow/pull/27219) has resolved this issue? Thanks!", "@rmothukuru \r\nIt was resolved by #27219 right after the merge but I'm not sure if this is still a problem after all these years because I haven't used it for a long time.", "Closing this issue as it has been resolved with #27219. Please feel free to open it if the issue still persists. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26683\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26683\">No</a>\n"]}, {"number": 26682, "title": "Add Go function to create serialized ConfigOptions protos", "body": "As noted in #22926, the only way to specify session options when creating a session from the Go API involves running a Python program to generate the serialized byte representation of a ConfigOptions protobuf, then pasting those bytes into a `[]byte` literal in the Go code.\r\n\r\nThis PR adds a Go function, `NewConfigOptions`, that generates a serialized `ConfigOptions` protocol buffer message based on parameters provided by the caller. To minimize impact on the rest of the TensorFlow code, the implementation of `NewConfigOptions` relies on the experimental C API call `TF_CreateConfig`. As a result, only the options exposed in `TF_CreateConfig` are available. \r\n\r\nUsers can use `NewConfigOptions` to initialize the `Config` field of a `SessionOptions` struct, then pass the struct to `NewSession()`:\r\n```go\r\nconfig := NewConfigOptions(true, true, 1)\r\nopts := SessionOptions{Config: config}\r\ns, err := NewSession(graph, &opts)\r\n```\r\n\r\nI did not change the any existing Go code that passes ConfigOptions as serialized byte strings directly will still work; for example:\r\n```go\r\nopts := SessionOptions{Config: []byte(\"(\\x01\")}\r\ns, err := NewSession(graph, &opts)\r\n```", "comments": ["> Thanks for doing this!\r\n> \r\n> I think it might be better to define a `struct` instead of a `func`, which will make it easier to add/remove/update options in the future and also make it easier to read the code (`true, true, 0` isn't that easy to follow ;)). Something like:\r\n> \r\n> ```go\r\n> type Config struct {\r\n>   XLACompile bool\r\n>   AllowGPUMemoryGrowth bool\r\n> }\r\n> \r\n> func (c *Config) Bytes() []byte {\r\n>   // Mostly what you have in NewConfigOptions right now\r\n> }\r\n> ```\r\n> \r\n> Which would cause the test to look like:\r\n> \r\n> ```go\r\n> config := SessionConfig{XLACompile: true, GPUAllowMemoryGrowth: true, NumCPUs: 1}\r\n> opts := SessionOptions{Config: config.Bytes()}\r\n> ```\r\n> \r\n> instead of:\r\n> \r\n> ```go\r\n> config := NewConfigOptions(true, true, 1)\r\n> ```\r\n> \r\n> Thoughts?\r\n\r\nGreat idea! Let me update this PR to use a struct. It'll make it easier to adjust to changes in `TF_CreateConfig`. Also I can add some sensible default values too.", "Update: I've added a new struct `Config` to encapsulate the session configuration parameters. I've also added logic to ensure that the zero values for all the fields in the `Config` struct correspond to the TensorFlow defaults of the associated parameters.\r\nI also removed some outdated comments from `session_test.go` that I should have removed earlier.", "Please address @asimshankar's comments and I'll approve / merge this.", "I've pushed an additional commit that should address the review comments. I also updated the API docs for `SessionOptions.Config`.\r\n\r\nPlease let me know if there is anything else I need to do on this PR.", "Why this is reverted?\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/6e9cb400d17f60e50cefe59fc099fc5b6e4074ce#diff-dcd28ad951bd17e9c512d3b564640bab"]}, {"number": 26681, "title": "Delete dupe LEAKY_RELU", "body": "(see remaining version above the deleted lines)", "comments": []}, {"number": 26680, "title": "boolean_mask with all-zero mask produces allocator error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):archlinux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):b'v1.13.1-0-g6612da8' 1.13.1\r\n- Python version:3.7\r\n- Bazel version (if compiling from source):n/a\r\n- GCC/Compiler version (if compiling from source):n/a\r\n- CUDA/cuDNN version:10.0/7.5.0\r\n- GPU model and memory:GTX1080ti\r\n\r\nThis code:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nshp = [100, 100]\r\nx = tf.placeholder(tf.bool, shape=shp, name='x')\r\ny = tf.random.uniform(shape=shp)\r\nz = tf.boolean_mask(y, x)\r\nsess = tf.InteractiveSession()\r\nfalse = np.zeros(shp).astype(np.bool)\r\nsess.run(z, feed_dict={x: false})\r\n```\r\nruns on GPU, and prints:\r\n```\r\n2019-03-13 12:13:35.552607: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\r\n2019-03-13 12:13:35.577673: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3501330000 Hz\r\n2019-03-13 12:13:35.579052: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55d524877dc0 executing computations on platform Host. Devices:\r\n2019-03-13 12:13:35.579096: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-03-13 12:13:35.691632: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55d52431d150 executing computations on platform CUDA. Devices:\r\n2019-03-13 12:13:35.691689: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2019-03-13 12:13:35.692485: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:65:00.0\r\ntotalMemory: 10.91GiB freeMemory: 8.92GiB\r\n2019-03-13 12:13:35.692524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2019-03-13 12:13:36.081417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-03-13 12:13:36.081451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2019-03-13 12:13:36.081457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \r\n2019-03-13 12:13:36.081720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8642 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\n2019-03-13 12:13:36.082699: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2019-03-13 12:13:36.123279: E tensorflow/core/common_runtime/bfc_allocator.cc:373] tried to deallocate nullptr\r\n```\r\nThere is an error in the end. It is not fatal, the code returns correct results. But I assume it is still a bug that is worth fixing.\r\nIf `x` is not full of zeros, it does not produce such errors.\r\n\r\nUPDATE: this issue does not exist in 1.12", "comments": ["@azaks2 do you know who should debug this allocator issue?", "I found the issue disappeared in nightly tf-nightly-gpu-1.14.1.dev20190412", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26680\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26680\">No</a>\n"]}, {"number": 26679, "title": "Error in bazel-out/x64_windows-opt/genfiles\\tensorflow/compiler/xla/xla_data.pb.h", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64 bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: checkout from `master` branch\r\n- Python version: Python 3.6.8 :: Anaconda, Inc.\r\n- Installed using virtualenv? pip? conda?: conda 4.6.8\r\n- Bazel version (if compiling from source): 0.22.0\r\n- GCC/Compiler version (if compiling from source): cl.exe C/C++ Optimizing Compiler Version 19.16.27027.1 for x64, Visual Studio Build Tools 2017\r\n- CUDA/cuDNN version: CUDA Toolkit V10.0.130, cuDNN 7.5.0\r\n- GPU model and memory: GTX 1060 6GB\r\n\r\n**Describe the problem**\r\n\r\nCompile error.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\n(neural) Stepii@STEPII D:\\Neural\r\n$ git clone https://github.com/tensorflow/tensorflow.git\r\nCloning into 'tensorflow'...\r\nremote: Enumerating objects: 7251, done.\r\nremote: Counting objects: 100% (7251/7251), done.\r\nremote: Compressing objects: 100% (2280/2280), done.\r\nremote: Total 557207 (delta 4925), reused 7233 (delta 4916), pack-reused 549956\r\nReceiving objects: 100% (557207/557207), 330.05 MiB | 378.00 KiB/s, done.\r\nResolving deltas: 100% (448922/448922), done.\r\nChecking out files: 100% (16777/16777), done.\r\n\r\n(neural) Stepii@STEPII D:\\Neural\r\n$ cd tensorflow\r\n\r\n(neural) Stepii@STEPII D:\\Neural\\tensorflow\r\n$ python configure.py\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nINFO: Invocation ID: 1f6b8c12-47e3-4c5e-80f4-5d3d74fb17b1\r\nYou have bazel 0.22.0- (@non-git) installed.\r\nPlease specify the location of python. [Default is D:\\Programs\\Anaconda3\\envs\\neural\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  D:\\Programs\\Anaconda3\\envs\\neural\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [D:\\Programs\\Anaconda3\\envs\\neural\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: y\r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10.0]:\r\n\r\n\r\nPlease specify the location where CUDA 10.0 toolkit is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]:\r\n\r\n\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 6.1\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]: /arch:AVX2\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\r\nEigen strong inline overridden.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=gdr            # Build with GDR support.\r\n        --config=verbs          # Build with libverbs support.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=noignite       # Disable Apache Ignite support.\r\n        --config=nokafka        # Disable Apache Kafka support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n\r\n(neural) Stepii@STEPII D:\\Neural\\tensorflow\r\n$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n[...]\r\n\r\nERROR: D:/neural/tensorflow/tensorflow/compiler/xla/service/cpu/BUILD:572:1: C++ compilation of rule '//tensorflow/compiler/xla/service/cpu:runtime_fft' failed (Exit 2): python.exe failed: error executing command\r\n  cd C:/users/stepii/_bazel_stepii/5mniti2w/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\ATLMFC\\include;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\include;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\cppwinrt\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\ATLMFC\\lib\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\MSBuild\\15.0\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\\\MSBuild\\15.0\\bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\Tools\\;;C:\\Windows\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=D:/Programs/Anaconda3/envs/neural/python.exe\r\n    SET PYTHON_LIB_PATH=D:/Programs/Anaconda3/envs/neural/lib/site-packages\r\n    SET TEMP=C:\\Users\\Stepii\\AppData\\Local\\Temp\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_CUDA_VERSION=10.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TMP=C:\\Users\\Stepii\\AppData\\Local\\Temp\r\n  D:/Programs/Anaconda3/envs/neural/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /showIncludes /MD /O2 /DNDEBUG -w -DWIN32_LEAN_AND_MEAN /arch:AVX2 -DEIGEN_AVOID_STL_ARRAY /Fobazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/runtime_fft/runtime_fft.o /c tensorflow/compiler/xla/service/cpu/runtime_fft.cc\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nbazel-out/x64_windows-opt/genfiles\\tensorflow/compiler/xla/xla_data.pb.h(290): error C2059: syntax error: 'constant'\r\nbazel-out/x64_windows-opt/genfiles\\tensorflow/compiler/xla/xla_data.pb.h(290): error C3805: 'constant': unexpected token, expected either '}' or a ','\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 1458.697s, Critical Path: 391.20s\r\nINFO: 3277 processes: 3277 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nFull log [here](https://github.com/tensorflow/tensorflow/files/3044240/FullLog.txt)\r\n", "comments": ["@ZunDubCore Did you install python through Anaconda? Did you check whether the CUDA and cuDNN are referenced correctly. Could you check the [instructions](https://github.com/jvishnuvardhan/Installing-TensorFlow-GPU-on-Windows-10-TF1.12) to verify whether the CUDA paths are referenced correctly. Thanks!", "@jvishnuvardhan Python installed through Anaconda\r\n```\r\n(tf_gpu) D:\\Neural\\tensorflow>python\r\nPython 3.6.8 |Anaconda, Inc.| (default, Feb 21 2019, 18:30:04) [MSC v.1916 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>>\r\n```\r\nDouble checked, all CUDA paths are referenced correctly and are in the PATH environment variable.", "Similar issue #25773", "Not getting this error anymore. Instead see #26958. Closing the issue", "> Similar issue #25773\r\n\r\nHi ZunDubCore, How did you pass the issue you mentioned?", "@Opbrave Actually, I don't know what I've done. The error just disappeared by itself. But instead, a different error appeared.", "OK, the error returned.", "@jvishnuvardhan anything I can do to fix this?", "@Stepiii Sorry for the delay in my response.\r\nCould you check with latest TF1.13.1 and please let us know whether the bug persists? Thanks!", "@jvishnuvardhan Which branch do I have to checkout? There's no r.1.13.1.  ", "@Stepiii https://pypi.org/project/tensorflow/ Thanks!", "@jvishnuvardhan That site has no downloads for source code, only .whl's. I want to compile tensorflow from sources.", "@Stepiii Current master is [TF1.13.1](https://github.com/tensorflow/tensorflow). Thanks! ", "@jvishnuvardhan So I need to checkout the `master` branch (not `r1.13`) and compile tensorflow from there? ", "@Stepiii Correct. Thanks!", "@jvishnuvardhan Thanks! I have updated the post with the new logs. Now I have tried to build from master branch, but similar error appeared.", "Hi @Stepiii, sorry for the delay getting back to you, I was OOO when this bug was assigned to me.\r\n\r\n```\r\nbazel-out/x64_windows-opt/genfiles\\tensorflow/compiler/xla/xla_data.pb.h(290): error C2059: syntax error: 'constant'\r\nbazel-out/x64_windows-opt/genfiles\\tensorflow/compiler/xla/xla_data.pb.h(290): error C3805: 'constant': unexpected token, expected either '}' or a ','\r\n```\r\n\r\nThis is failing when building a protocol buffer, which is generated code.  IOW the problem here is either with protobuf, or with nvcc/msvc.  In fact I'd say that if you answered \"n\" to \"use XLA?\" there's a good chance you'd get a build error in one of TensorFlow's protocol buffer files.  (If you don't, that would also be kind of interesting...)\r\n\r\nI don't have a Windows machine to debug this on.  If you're willing to attach the contents of `bazel-out/x64_windows-opt/genfiles\\tensorflow/compiler/xla/xla_data.pb.h` I might be able to help some, but no promises.  The Windows build is kind of unloved, as I guess you're discovering, so even if we fix this one problem, how long will it stay green?  I have no good answers for you, I'm afraid.  :-/", "Dear @jlebar, I assume the root cause here is the same one as in the equivalent issue for TF2.0 #25773, which is not an inherent problem of protobuf but a name clash of a message field name with a windows header macro (see my comment on the referenced issue), leading to a weird error in the generated header as the compiler sees a \"2 = 14\" entry.", "Ah, wonderful, thank you!\r\n\r\nLet's dupe this to #25773.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26679\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26679\">No</a>\n"]}, {"number": 26678, "title": "[ROCm] Changing from Cuda* to Gpu*, the contents for tensforflow/core/kerns/cuda_device_array* files", "body": "This PR is a follow-up on PR #24295 .\r\n\r\nThat PR renames the files\r\n```\r\ntensorflow/core/kernels/cuda_device_array.h\r\ntensorflow/core/kernels/cuda_device_array_gpu.h\r\n```  \r\nto \r\n```\r\ntensorflow/core/kernels/gpu_device_array.h\r\ntensorflow/core/kernels/gpu_device_array_gpu.h\r\n```\r\n\r\nThis PR does the `Cuda*` to `Gpu*` renaming for the contents within those two files.\r\n\r\nIf need be we can merge this PR, into PR #24295 \r\n\r\n------------------------------\r\n\r\n@tatianashp , @whchung : just FYI", "comments": ["rebased PR to remove merge conflicts", "@shahzadlone would you mind check this PR again? thanks."]}, {"number": 26677, "title": "tf.argmax axis argument error returns \"Use the `axis` argument instead\"", "body": "i also try  tf.math.argmax instance of  tf.argmax which is also produce same error.\r\n\r\ny_pred_cls = tf.argmax(y_pred, dimension=1, axis=None)\r\n'''y_pred_cls = tf.math.argmax(\r\n    y_pred,\r\n    axis=None,\r\n    name=None,\r\n    dimension=1\r\n)'''", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26677\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26677\">No</a>\n"]}, {"number": 26676, "title": "Remove the exposure of tf.io.TFRecordCompressionType from v2 API", "body": "This fix tries to address the issue raised in #26643 where in v2 the `tf.io.TFRecordCompressionType` is exposed and used in `tf.io.TFRecordWriter`, but the same could not be applied to `tf.data.TFRecordDataset`, thus inconsistency.\r\n\r\nAt the moment since both `tf.io.TFRecordWriter` and `tf.data.TFRecordDataset` support a compression type string `\"GZIP\"`, `\"ZLIB\"` already, it makes sense to make it consistent and remove the `tf.io.TFRecordCompressionType` altogether from v2 (user could just simply use `\"GZIP\"` or `\"ZLIB\"`).\r\n\r\nThis fix remove the v2 endpoint of `tf.io.TFRecordCompressionType`, and update the docs.\r\n\r\nThis fix fixes #26643.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang can you please check failed tests", "@rthadur Thanks. It was a pylint line issue (> 80 chars long). Just fixed the issue and updated the PR."]}, {"number": 26675, "title": "Remove a few warnings on macOS", "body": "1. Remove unused captures in lambdas.\r\n2. Reorder a field initializer list.\r\n3. Remove some unused typedefs.\r\n4. Remove a few unused variables.\r\n\r\nMany warnings remain, including many thread safety warnings which are\r\nsomewhat alarming.", "comments": ["@jsimsa Am I correct that the Windows failure is unrelated?  Not sure about the copybara one.", "Correct, the Windows failure seems unrelated. The copybara one tracks the status of the internal review that should get kicked off by @rthadur shortly.", "@jsimsa What's the next step here?", "Thank you!"]}, {"number": 26674, "title": "tests inside tflite/kernels/internal fail to build", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n-  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: commit 6583b83dc9393118c60a9c11f15453c650fab89f\r\n- Python version: 2.7.15rc1\r\n- Installed using virtualenv? pip? conda?: sources\r\n- Bazel version (if compiling from source): 0.20.0\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n\r\n\r\n**Describe the problem**\r\nSome tests inside tflite/kernels/internal (6 out of 11) fail to build. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel test --verbose_failures --config=opt //tensorflow/lite/kernels/internal/... -k\r\n\r\n**Any other info / logs**\r\n```\r\nERROR: /local/people/kamil_herba/tensorflow-x86/tensorflow/lite/kernels/internal/BUILD:713:1: Couldn't build file tensorflow/lite/kernels/internal/_objs/batch_to_space_nd_test/batch_to_space_nd_test.o: undeclare\r\nd inclusion(s) in rule '//tensorflow/lite/kernels/internal:batch_to_space_nd_test':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/lite/kernels/internal/batch_to_space_nd_test.cc':\r\n  'tensorflow/lite/kernels/internal/tensor_ctypes.h'\r\nIn file included from ./tensorflow/lite/kernels/internal/common.h:42:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:37,\r\n                 from tensorflow/lite/kernels/internal/batch_to_space_nd_test.cc:15:\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h: In function 'void vst1q_lane_f32(float32_t*, float32x4_t, int)':\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h:9725:31: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n     *(ptr) =  *((float*)&ilane);\r\n                               ^\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h: In function 'float32_t vgetq_lane_f32(float32x4_t, int)':\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h:11964:22: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n     return *(float*)&ilane;\r\n                      ^\r\nERROR: /local/people/kamil_herba/tensorflow-x86/tensorflow/lite/kernels/internal/BUILD:619:1: Couldn't build file tensorflow/lite/kernels/internal/_objs/resize_bilinear_test/resize_bilinear_test.o: undeclared in\r\nclusion(s) in rule '//tensorflow/lite/kernels/internal:resize_bilinear_test':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/lite/kernels/internal/resize_bilinear_test.cc':\r\n  'tensorflow/lite/kernels/internal/tensor_ctypes.h'\r\nIn file included from ./tensorflow/lite/kernels/internal/common.h:42:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:37,\r\n                 from tensorflow/lite/kernels/internal/resize_bilinear_test.cc:20:\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h: In function 'void vst1q_lane_f32(float32_t*, float32x4_t, int)':\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h:9725:31: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n     *(ptr) =  *((float*)&ilane);\r\n                               ^\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h: In function 'float32_t vgetq_lane_f32(float32x4_t, int)':\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h:11964:22: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n     return *(float*)&ilane;\r\n                      ^\r\nERROR: /local/people/kamil_herba/tensorflow-x86/tensorflow/lite/kernels/internal/BUILD:631:1: Couldn't build file tensorflow/lite/kernels/internal/_objs/resize_nearest_neighbor_test/resize_nearest_neighbor_test.\r\no: undeclared inclusion(s) in rule '//tensorflow/lite/kernels/internal:resize_nearest_neighbor_test':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/lite/kernels/internal/resize_nearest_neighbor_test.cc':\r\n  'tensorflow/lite/kernels/internal/tensor_ctypes.h'\r\nIn file included from ./tensorflow/lite/kernels/internal/common.h:42:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:37,\r\n                 from tensorflow/lite/kernels/internal/resize_nearest_neighbor_test.cc:20:\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h: In function 'void vst1q_lane_f32(float32_t*, float32x4_t, int)':\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h:9725:31: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n     *(ptr) =  *((float*)&ilane);\r\n                               ^\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h: In function 'float32_t vgetq_lane_f32(float32x4_t, int)':\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h:11964:22: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n     return *(float*)&ilane;\r\n\r\nERROR: /local/people/kamil_herba/tensorflow-x86/tensorflow/lite/kernels/internal/BUILD:660:1: Couldn't build file tensorflow/lite/kernels/internal/_objs/logsoftmax_quantized_test/logsoftmax_quantized_test.o: und\r\neclared inclusion(s) in rule '//tensorflow/lite/kernels/internal:logsoftmax_quantized_test':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/lite/kernels/internal/logsoftmax_quantized_test.cc':\r\n  'tensorflow/lite/kernels/internal/tensor_ctypes.h'\r\nIn file included from ./tensorflow/lite/kernels/internal/common.h:42:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:37,\r\n                 from tensorflow/lite/kernels/internal/logsoftmax_quantized_test.cc:26:\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h: In function 'void vst1q_lane_f32(float32_t*, float32x4_t, int)':\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h:9725:31: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n     *(ptr) =  *((float*)&ilane);\r\n                               ^\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h: In function 'float32_t vgetq_lane_f32(float32x4_t, int)':\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h:11964:22: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n     return *(float*)&ilane;\r\n                      ^\r\nERROR: /local/people/kamil_herba/tensorflow-x86/tensorflow/lite/kernels/internal/BUILD:643:1: Couldn't build file tensorflow/lite/kernels/internal/_objs/softmax_quantized_test/softmax_quantized_test.o: undeclare\r\nd inclusion(s) in rule '//tensorflow/lite/kernels/internal:softmax_quantized_test':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/lite/kernels/internal/softmax_quantized_test.cc':\r\n  'tensorflow/lite/kernels/internal/tensor_ctypes.h'\r\nIn file included from ./tensorflow/lite/kernels/internal/common.h:42:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:37,\r\n                 from tensorflow/lite/kernels/internal/softmax_quantized_test.cc:26:\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h: In function 'void vst1q_lane_f32(float32_t*, float32x4_t, int)':\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h:9725:31: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n     *(ptr) =  *((float*)&ilane);\r\n                               ^\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h: In function 'float32_t vgetq_lane_f32(float32x4_t, int)':\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h:11964:22: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n     return *(float*)&ilane;\r\n\r\nERROR: /local/people/kamil_herba/tensorflow-x86/tensorflow/lite/kernels/internal/BUILD:682:1: Couldn't build file tensorflow/lite/kernels/internal/_objs/log_quantized_test/log_quantized_test.o: undeclared inclusion(s) in rule '//tensorflow/lite/kernels/internal:log_quantized_test':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/lite/kernels/internal/log_quantized_test.cc':\r\n  'tensorflow/lite/kernels/internal/tensor_ctypes.h'\r\nIn file included from ./tensorflow/lite/kernels/internal/common.h:42:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:37,\r\n                 from tensorflow/lite/kernels/internal/log_quantized_test.cc:30:\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h: In function 'void vst1q_lane_f32(float32_t*, float32x4_t, int)':\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h:9725:31: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n     *(ptr) =  *((float*)&ilane);\r\n                               ^\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h: In function 'float32_t vgetq_lane_f32(float32x4_t, int)':\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h:11964:22: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n     return *(float*)&ilane;\r\n```", "comments": ["I've tested this and couldn't reproduce the issue. @herbakamil , it seems that the issue is fixed now. Please feel free to reopen it if you still have the issue."]}, {"number": 26673, "title": "[Document] no module name resize_image_with_pad", "body": "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/image/resize_image_with_pad\r\n\r\nthere no function \"resize_image_with_pad\" in module \"image\". Its name is: \"resize_with_pad\"\r\n", "comments": ["duplicate with https://github.com/tensorflow/tensorflow/issues/26651\r\n\r\nI close it"]}, {"number": 26672, "title": "Not able to convert Keras model to tflite - TF 2.0 alpha", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not yet to that phase\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 2.7.12\r\n- Bazel version (if compiling from source): 0.23.0\r\n- GCC/Compiler version (if compiling from source): 5.4.0 20160609\r\n- CUDA/cuDNN version: CPU\r\n- GPU model and memory: CPU\r\n\r\n**Current Behavior**\r\nOn conversion to tflite from keras model, fused batch norm creates a resource node issue (before fine tuning: dog_cat_classification_initial), and conv2d creates a resource node issue(after fine tuning: dog_cat_classification_final).\r\nOn conversion to tflite from a keras model using TF2.0, extra resource nodes are getting generated for each and every op that is available. So, upon conversion to tflite, all the resource nodes sum up to form the input, which does not allow the tflite to work as expected. \r\n\r\n**expected behavior**\r\nBatch normalization should be supported by TFLITE. \r\nTFLite produced as an output of the tflite conversion process, should produce no resource nodes as it was the case in TF1.12\r\n\r\n**Code to reproduce the issue**\r\nThis is a code to convert the mobilenet based classification model into tflite. This is developed using TF2.0 following the official dogs and cats classification published in the tensorflow page.\r\nI have attached with this issue, the h5 file and hdf5 file extracted from the same model. I hope we can try invoking the same model and try creating the tflite file. The same code creates different issues with the same model architecture in two different situations(initial and fine tuned).\r\n\r\n`import tensorflow as tf`\r\n`model = tf.keras.models.load_model('dog_cat_classification_initial.hdf5')`\r\n`@tf.function`\r\n`def to_save(x):      return model(x)`\r\n\r\n`from tensorflow.lite.python import lite`\r\n`from tensorflow.lite.python.interpreter import Interpreter`\r\n`from tensorflow.python import keras`\r\n`from tensorflow.python.eager import def_function`\r\n`from tensorflow.python.framework import constant_op`\r\n`from tensorflow.python.framework import dtypes`\r\n`from tensorflow.python.framework import tensor_spec`\r\n`from tensorflow.python.framework import test_util`\r\n`from tensorflow.python.ops import variables`\r\n`from tensorflow.python.platform import test`\r\n`from tensorflow.python.saved_model.load import load`\r\n`from tensorflow.python.saved_model.save import save`\r\n`from tensorflow.python.training.tracking import tracking`\r\n`tf.keras.backend.set_learning_phase(False)`\r\n`concrete_func = to_save.get_concrete_function(tensor_spec.TensorSpec([None, 160, 160, 3], dtypes.float32))`\r\n`converter = lite.TFLiteConverterV2.from_concrete_function(concrete_func)`\r\n`tflite_model = converter.convert()`\r\n`open(\"new_classificaton.tflite\",\"wb\").write(tflite_model)`\r\n\r\nModel file:\r\n\r\n\r\n[tf2_tflite_issues.zip](https://github.com/tensorflow/tensorflow/files/2961884/tf2_tflite_issues.zip)\r\n\r\n**Other info / logs**\r\n\r\n**dog_cat_classification_initial.hdf5**\r\n\r\n2019-03-13 19:52:41.281172: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2594290000 Hz\r\n2019-03-13 19:52:41.281379: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x5aee1c0 executing computations on platform Host. Devices:\r\n2019-03-13 19:52:41.281401: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0313 19:53:20.606553 140373983291136 hdf5_format.py:261] Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\r\n2019-03-13 19:53:21.310751: I tensorflow/core/grappler/devices.cc:53] Number of eligible GPUs (core count >= 8): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-03-13 19:53:21.310873: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2019-03-13 19:53:21.344653: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:666] Optimization results for grappler item: graph_to_optimize\r\n2019-03-13 19:53:21.344712: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:668]   function_optimizer: Graph size after: 775 nodes (0), 960 edges (0), time = 1.54ms.\r\n2019-03-13 19:53:21.344726: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:668]   function_optimizer: Graph size after: 775 nodes (0), 960 edges (0), time = 1.95ms.\r\nTraceback (most recent call last):\r\n  File \"conversion.py\", line 26, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/home/santhosh/venv_tf2/local/lib/python2.7/site-packages/tensorflow/lite/python/lite.py\", line 246, in convert\r\n    self._func)\r\n  File \"/home/santhosh/venv_tf2/local/lib/python2.7/site-packages/tensorflow/python/framework/convert_to_constants.py\", line 173, in convert_variables_to_constants_v2\r\n    \"data\": tensor_data[input_name],\r\nKeyError: u'sequential/mobilenetv2_1.00_160/bn_Conv1/FusedBatchNorm/ReadVariableOp/resource'\r\n\r\n**dog_cat_classification_final.hdf5**\r\n2019-03-13 19:52:08.602599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2594290000 Hz\r\n2019-03-13 19:52:08.602794: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x545d360 executing computations on platform Host. Devices:\r\n2019-03-13 19:52:08.602817: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-03-13 19:52:25.827825: I tensorflow/core/grappler/devices.cc:53] Number of eligible GPUs (core count >= 8): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-03-13 19:52:25.827926: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2019-03-13 19:52:25.844869: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:666] Optimization results for grappler item: graph_to_optimize\r\n2019-03-13 19:52:25.844920: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:668]   function_optimizer: Graph size after: 775 nodes (0), 960 edges (0), time = 0.703ms.\r\n2019-03-13 19:52:25.844930: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:668]   function_optimizer: Graph size after: 775 nodes (0), 960 edges (0), time = 1.121ms.\r\nTraceback (most recent call last):\r\n  File \"conversion.py\", line 26, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/home/santhosh/venv_tf2/local/lib/python2.7/site-packages/tensorflow/lite/python/lite.py\", line 246, in convert\r\n    self._func)\r\n  File \"/home/santhosh/venv_tf2/local/lib/python2.7/site-packages/tensorflow/python/framework/convert_to_constants.py\", line 173, in convert_variables_to_constants_v2\r\n    \"data\": tensor_data[input_name],\r\nKeyError: u'sequential/mobilenetv2_1.00_160/Conv1/Conv2D/ReadVariableOp/resource'\r\n", "comments": ["This error is coming from a known issue with `convert_variables_to_constants_v2` that should be fixed in the coming weeks. The work around (in the interim while the issue is being fixed) is to save the model instead of using `@tf.function` to generate the concrete function. Here is an example of how that might look:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.models.load_model('dog_cat_classification_initial.hdf5')\r\n\r\n# Save the model.\r\nexport_dir = \"/tmp/test_saved_model\"\r\ntf.saved_model.save(model, export_dir)\r\n\r\n# Load model and get the concrete function.\r\nmodel = tf.saved_model.load(export_dir)\r\nconcrete_func = model.signatures[\r\n  tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\r\n\r\n# Convert the model.\r\nconverter = tf.lite.TFLiteConverter.from_concrete_function(concrete_func)\r\ntflite_model = converter.convert()\r\nopen(\"new_classificaton.tflite\",\"wb\").write(tflite_model)\r\n```\r\n\r\nHowever, saving and loading seems to result in the following error (from the `tf.saved_model.load(export_dir)` call):\r\n```\r\nAttributeError: '_UserObject' object has no attribute '_create_or_restore_slot_variable'\r\n```\r\n\r\n@allenlavoie Any thoughts? I was running from today's `tf-nightly-2.0-preview`.", "Yeah we need to support Optimizers in SavedModels. Right now the imported model isn't checkpoint compatible with the exported model. There's a bug for it internally. The SavedModel will work fine for things other than `tf.saved_model.load` (saved_model_cli/tflite/etc.) in the meantime.", "Given the issues, it seems like this model unfortunately won't work with the 2.0 alpha. The bug in `convert_variables_to_constants_v2` will hopefully be resolved by the end of the month so the model should work in an upcoming nightly. Leaving this bug open as we work on issues preventing this model from converting.", "If you need a workaround for the optimizer issue, you should be able to `del model.optimizer` before saving. But it sounds like that's not the main issue.", "https://github.com/tensorflow/tensorflow/commit/147e6777068ea569cbecbd270b8b07c648bae09b should fix the optimizer exception.", "The work around in https://github.com/tensorflow/tensorflow/issues/26672#issuecomment-472519970 works in the latest nightly.", "tf-nightly-2.0-preview fixed KeyError: '../FusedBatchNorm/ReadVariableOp/resource'. But it don't fold  BatchNorm therefore it generate error tflife not support BatchNormalization.\r\n\"Here is a list of operators for which you will need custom implementations: BatchNormalization.\"", "Adding here for documentation for future users. This functionality has been changed in https://github.com/tensorflow/tensorflow/commit/312222d2a0657d4e0086ec0add4331063d0b2264. `from_concrete_function` is now `from_concrete_functions`. Additionally, `from_keras_model` and `from_saved_model` have been added.\r\n\r\n@anhtu812 Please file a separate bug with additional information including how to reproduce the error."]}, {"number": 26670, "title": "kTfLiteNullBufferHandle must be macro", "body": "tensorflow/lite/c/c_api_internal.h have constant variable kTfLiteNullBufferHandle (not macro or static const). So we can't include this header file in several codes.", "comments": ["I think there are current many .cc files including the c_api_internal.h. What's the specific error your encounter?", "a.c\r\n```\r\n#include <tensorflow/lite/experimental/c/c_api.h>\r\n\r\nextern void foo();\r\n\r\nint\r\nmain() {\r\n  foo();\r\n}\r\n```\r\nb.c\r\n```\r\n#include <tensorflow/lite/experimental/c/c_api.h>\r\n\r\nvoid\r\nfoo() {\r\n}\r\n```\r\n```\r\n$ gcc -I/path/to/tensorflow/lite/experimental/c -c a.c\r\n$ gcc -I/path/to/tensorflow/lite/experimental/c -c b.c\r\n$ gcc -o tmp.exe a.o b.o \r\nb.o:b.c:(.rdata+0x0): multiple definition of `kTfLiteNullBufferHandle'\r\na.o:a.c:(.rdata+0x0): first defined here\r\n```\r\n", "@haozha111  Thanks your review. I hope to add one more fix.\r\n\r\n```\r\ntensorflow/tensorflow/tensorflow/lite/c/c_api_internal.h:60:34: warning: 'struct TfLiteContext' declared inside parameter lis\r\nt will not be visible outside of this definition or declaration\r\n   TfLiteStatus (*Refresh)(struct TfLiteContext* context);\r\n```\r\n   \r\n```\r\ndiff --git a/tensorflow/lite/c/c_api_internal.h b/tensorflow/lite/c/c_api_internal.h\r\nindex 80d99d92e9..5939607d3c 100644\r\n--- a/tensorflow/lite/c/c_api_internal.h\r\n+++ b/tensorflow/lite/c/c_api_internal.h\r\n@@ -50,6 +50,8 @@ typedef enum {\r\n   kTfLiteMaxExternalContexts = 3\r\n } TfLiteExternalContextType;\r\n \r\n+struct TfLiteContext;\r\n+\r\n // An external context is a collection of information unrelated to the TF Lite\r\n // framework, but useful to a subset of the ops. TF Lite knows very little\r\n // about about the actual contexts, but it keeps a list of them, and is able to\r\n```\r\n\r\nShould be better to separate PR? or Can i include the patch into this PR?", "> @haosdent Thanks your review. I hope to add one more fix.\r\n> \r\n> ```\r\n> tensorflow/tensorflow/tensorflow/lite/c/c_api_internal.h:60:34: warning: 'struct TfLiteContext' declared inside parameter lis\r\n> t will not be visible outside of this definition or declaration\r\n>    TfLiteStatus (*Refresh)(struct TfLiteContext* context);\r\n> ```\r\n> ```\r\n> diff --git a/tensorflow/lite/c/c_api_internal.h b/tensorflow/lite/c/c_api_internal.h\r\n> index 80d99d92e9..5939607d3c 100644\r\n> --- a/tensorflow/lite/c/c_api_internal.h\r\n> +++ b/tensorflow/lite/c/c_api_internal.h\r\n> @@ -50,6 +50,8 @@ typedef enum {\r\n>    kTfLiteMaxExternalContexts = 3\r\n>  } TfLiteExternalContextType;\r\n>  \r\n> +struct TfLiteContext;\r\n> +\r\n>  // An external context is a collection of information unrelated to the TF Lite\r\n>  // framework, but useful to a subset of the ops. TF Lite knows very little\r\n>  // about about the actual contexts, but it keeps a list of them, and is able to\r\n> ```\r\n> Should be better to separate PR? or Can i include the patch into this PR?\r\n\r\nThanks for catching this! You can just make that change in this PR I think.", "Just  an update:\r\n\r\nWe decide to use enum to define the kTfLiteNullBufferHandle, not Macro. I will make this change internally and hold back this PR. Thanks!", "Issue of `struct TfLiteContext` still not fixed?", "> Issue of `struct TfLiteContext` still not fixed?\r\n\r\nI have fixed it yesterday. Only thing different from your change is that I used enum to declare the kTfLiteNullBufferHandle rather than Macro. Please see it here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/c_api_internal.h", "Thank you."]}, {"number": 26669, "title": "[ROCm] Enable ROCm support for the \"softmax_cross_entropy_with_logits\" op", "body": "This PR enables ROCm support for the \"softmax_cross_entropy_with_logits\" op.\r\n\r\nPR #26457 is a pre-req for this PR, and hence this PR includes commits from that PR.\r\nOnly the last commit in this PR should be reviewed here \r\n(as all others will be reviewed as part of PR #26457 )\r\n\r\n-----------------------\r\n\r\n@tatianashp @whchung : just FYI.", "comments": ["I'm planning to submit softplus_op (#26457) first, then this PR should become lighter.", "@chsigg, rebased this PR", "Removing myself - I'll leave this to @chsigg "]}, {"number": 26668, "title": "Benchmarker of tensorflow/lite initialize tensorflow", "body": "Current implementation of tensorflow/lite/testing have initialization of TensorFlow core.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/069c503b4ac5bf4c89aeb38332685e427e0d3e03/tensorflow/lite/testing/init_tensorflow.cc#L23-L30\r\n\r\nWhy this is needed? Benchmarker always include binary of tensorflow. I think this is enough.\r\n\r\n```diff\r\ndiff --git a/tensorflow/lite/testing/init_tensorflow.cc b/tensorflow/lite/testing/init_tensorflow.cc\r\nindex ed4d123744..d8d639f410 100644\r\n--- a/tensorflow/lite/testing/init_tensorflow.cc\r\n+++ b/tensorflow/lite/testing/init_tensorflow.cc\r\n@@ -20,12 +20,5 @@ limitations under the License.\r\n #include \"tensorflow/core/platform/init_main.h\"\r\n \r\n namespace tflite {\r\n-void InitTensorFlow() {\r\n-  static const char* kFakeName = \"fake program name\";\r\n-  int argc = 1;\r\n-  char* fake_name_copy = strdup(kFakeName);\r\n-  char** argv = &fake_name_copy;\r\n-  ::tensorflow::port::InitMain(kFakeName, &argc, &argv);\r\n-  free(fake_name_copy);\r\n-}\r\n+void InitTensorFlow() {}\r\n }  // namespace tflite\r\n```\r\n", "comments": ["I believe this is needed at least in this test:\r\nhttps://github.com/tensorflow/tensorflow/blob/5b24fba0857394dab67359963726b3bcce071575/tensorflow/lite/tools/benchmark/benchmark_plus_flex_main.cc#L16\r\n\r\nBasically, in flex mode, TF Lite will fall back to use TF ops, and that's the reason we need to include tensorflow core dependencies. Note that not all TF Lite benchmark tests need import core TF."]}, {"number": 26667, "title": "Multipose outputs", "body": "**System information**\r\n- TensorFlow version: tf-lite\r\n- Doc Link: https://www.tensorflow.org/lite/models/pose_estimation/overview\r\nhttps://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5\r\n\r\n\r\n**By using the getOutputTensors function in tflite interpreter, I can get that there are 4 output arrays with dimension of  1*23*17*17, 1*23*17*34, 1*23*17*64 and 1*23*17*1. However in the tflite medium article or website, the usage of full output is not explained. How can I get the pose coordinates in the image? In the heatmap array, I am getting very low confidence scores close to 0.00 for every pixel. Here is what I have tried : https://stackoverflow.com/questions/55136861/how-to-use-outputs-of-posenet-model-in-tflite**\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue? - No**\r\n", "comments": ["@jvishnuvardhan You may have been thinking of someone else. I don't work on tflite. ", "Any progress?", "Hey! I have the same problem (Model used was multi_person_mobilenet_v1_075_float.tflite). Additionally tried to run it in python using the interpreter without success ", "I wrote a post about PoseNet on iOS and Android. Hope it helps. \r\nhttps://medium.com/flutter-community/posenet-for-ios-android-and-flutter-using-tensorflow-lite-836788a110c7", "Same issue here. Downloaded the starter model from the [tflite page](https://www.tensorflow.org/lite/models/pose_estimation/overview) as referenced as well in [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/pose_estimation/overview.md) markdown file and its output formats and names don't match anything mentioned in any docs.\r\n\r\nIf you load the tflite model into [Netron](https://lutzroeder.github.io/netron/), you will see that the outputs are also named differently (heatmaps, short offset, mid offset, segment) instead of (heatmaps, offsets, displacement forward, displacement backwards).\r\n\r\nCould it be that this was a prematurely uploaded version of PoseNet v2?\r\n\r\nCheers ", "@death14stroke \r\nCould you please confirm if this is still an issue.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26667\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26667\">No</a>\n"]}, {"number": 26666, "title": "Added hyperbolic sin and cos to keras backend", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26666) for more info**.\n\n<!-- need_sender_cla -->", "Sorry, no CLAs. The changes are trivial. Feel free to recreate from your acc. We also need `real`, `imag` and `einsum`.", "Thanks, I'm on it. Can we close this? @KOLANICH ", "@kyscg , are you going to create a new PR with the stuff?", "Yes, I will do it soon. Is there anything else you want me to keep in mind???", "`linalg.einsum` though I am not sure that it is avilable in other libs (it is unavailable in CNTK) and  `K.eye`'s `batch_shape` parameter."]}, {"number": 26665, "title": "tensorflow 2.0, variable_scope(), TypeError: __call__() got an unexpected keyword argument 'partition_info'", "body": "I have convert a CNN model from tf1.x to tf2.0 by using ` tf_upgrade_v2 `, but when i used this converted model, i got an error:\r\n\r\n`File \"/home/hsw/virtual_env/tf2.0/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 2492, in default_variable_creator\r\n    import_scope=import_scope, distribute_strategy=distribute_strategy)\r\n  File \"/home/hsw/virtual_env/tf2.0/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 216, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"/home/hsw/virtual_env/tf2.0/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 422, in __init__\r\n    constraint=constraint)\r\n  File \"/home/hsw/virtual_env/tf2.0/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 545, in _init_from_args\r\n    initial_value() if init_from_fn else initial_value,\r\n  File \"/home/hsw/virtual_env/tf2.0/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 886, in <lambda>\r\n    shape.as_list(), dtype=dtype, partition_info=partition_info)\r\nTypeError: __call__() got an unexpected keyword argument 'partition_info'`\r\n\r\nit seems like something wrong in `variables.py`, and the converted model such as like this :\r\n\r\n        with tf.compat.v1.variable_scope('backbone', reuse=tf.compat.v1.AUTO_REUSE):\r\n            net = tf.compat.v1.layers.separable_conv2d(inputs, 16, 3, 1, 'same',\r\n                                         activation=tf.nn.elu,\r\n                                         depthwise_initializer=tf.keras.initializers.glorot_normal(),\r\n                                         pointwise_initializer=tf.keras.initializers.glorot_normal(),\r\n                                         name='conv1')\r\n            net = tf.compat.v1.layers.max_pooling2d(net, 2, 2, padding='same')\r\n            net = tf.compat.v1.layers.separable_conv2d(net, 32, 3, 1, 'same',\r\n                                         activation=tf.nn.elu,\r\n                                         depthwise_initializer=tf.keras.initializers.glorot_normal(),\r\n                                         pointwise_initializer=tf.keras.initializers.glorot_normal(),\r\n                                         name='conv2')\r\n\r\nhow should do to solve this problem?", "comments": ["Can I see the full stack trace?", "\r\n![Selection_012](https://user-images.githubusercontent.com/18358653/54325147-ac586600-463b-11e9-9f22-7060a5acb656.png)\r\nThis is the full stack trace.\r\n\r\n", "Thanks! This seems to be an issue with the line \r\n\r\n```\r\n          init_val = lambda: initializer(  # pylint: disable=g-long-lambda\r\n              shape.as_list(), dtype=dtype, partition_info=partition_info)\r\n```\r\n\r\nwhich calls the initializer. For some reason this is calling the v2 tf.keras initializers instead of the compat.v1 initializers. I think this is a bug in the rename script.\r\n\r\n@pavithrasv can you revert the change in the rename script to rename tf v1 initializers to tf v2 initializers since they're not compatible?\r\n\r\nA workaround here is to replace the calls to tf.keras.initializers in the translated code with tf.compat.v1 initializers (take what was there originally and replace tf with tf.compat.v1).", "Thank you Alex. We do not rename v1 initializers to v2. The issue is because we do not rename v1 initializers to compat.V1, because of this users starts seeing the V2 version.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/75eca85ab68d984ee905978483fa4d871fd92660/tensorflow/tools/compatibility/tf_upgrade_v2.py#L781\r\n\r\nI will add this renaming for all the initializers.\r\n ", "Thanks all, fixed this by using `tf.compat.v1.keras` replace` tf.keras.`", "@murdockhou , Have you resolved this problem, how to do it?  I \uff48\uff41ve the same problem.", "@alextp @ \r\nI have similar issue. I attached my problem below.\r\n**I replace tf.compat.v1.keras with tf.keras, but can't solve my problem.** \r\n\r\n\r\n________________________________________________________________\r\nIn tf2_util.py\r\ndef conv2d( . . .  ~):\r\n   . . . . . . . ~ skip\r\n with tf.compat.v1.variable_scope(scope) as sc:\r\n    . . . . . . .  ~\r\n      kernel = **_variable_with_weight_decay**('weights',\r\n                                           shape=kernel_shape,\r\n                                           use_xavier=use_xavier,\r\n                                           stddev=stddev,\r\n                                           wd=weight_decay)\r\n\r\ndef _variable_with_weight_decay(name, shape, stddev, wd, use_xavier=True):\r\n  . . . . . . .  .~ skip\r\n  if use_xavier:\r\n    initializer = **tf.keras.initializers.VarianceScaling**(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\")\r\n  else:\r\n    initializer = tf.compat.v1.truncated_normal_initializer(stddev=stddev)\r\n\r\n\r\ndef _variable_on_cpu(name, shape, initializer, use_fp16=False):\r\n  . . . . . . .  .~  skip\r\n  with tf.device(\"/cpu:0\"):\r\n    dtype = tf.float16 if use_fp16 else tf.float32\r\n    _var = **tf.compat.v1.get_variable**(name, shape, initializer=initializer, dtype=dtype)_ \r\n  return var\r\n_______________________________________________________________________\r\n\r\nI guess have error above code 'tf.compat.v1.get_variable' on 'def _variable_on_cpu'\r\nDetail error explaination is below \r\n\r\n##############################################################\r\n  Downloads/3D/pointnet2/tf2_train.py:94 train_one_epoch  *\r\n        train_one_step(train_data, train_label, model, optimizer)\r\n    anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:416 __call__\r\n        self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n    Downloads/3D/pointnet2/tf2_train.py:67 train_one_step  *\r\n        pred = model.get_model(data, True, None)\r\n    Downloads/3D/pointnet2/models_from_pointnet/pointnet_cls.py:27 get_model  *\r\n        transform = input_transform_net(point_cloud, is_training, bn_decay, K=3)\r\n    Downloads/3D/pointnet2/models_from_pointnet/transform_nets.py:19 input_transform_net  *\r\n        net = tf_util.conv2d(input_image, 64, [1,3],\r\n    Downloads/3D/pointnet2/utils_from_pointnet/tf2_util.py:164 conv2d  *\r\n        kernel = _variable_with_weight_decay('weights',\r\n    Downloads/3D/pointnet2/utils_from_pointnet/tf2_util.py:45 _variable_with_weight_decay  *\r\n        var = _variable_on_cpu(name, shape, initializer)\r\n    Downloads/3D/pointnet2/utils_from_pointnet/tf2_util.py:21 _variable_on_cpu  *\r\n        var = tf.compat.v1.get_variable(name, shape, initializer=initializer, dtype=dtype)\r\n    anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:1503 get_variable\r\n        aggregation=aggregation)\r\n    anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:1246 get_variable\r\n        aggregation=aggregation)\r\n    anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:569 get_variable\r\n        aggregation=aggregation)\r\n    anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:521 _true_getter\r\n        aggregation=aggregation)\r\n    anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:936 _get_single_variable\r\n        aggregation=aggregation)\r\n    anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:260 __call__\r\n        return cls._variable_v1_call(*args, **kwargs)\r\n    anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:221 _variable_v1_call\r\n        shape=shape)\r\n    anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:60 getter\r\n        return captured_getter(captured_previous, **kwargs)\r\n    anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:347 variable_capturing_scope\r\n        lifted_initializer_graph=lifted_initializer_graph, **kwds)\r\n    anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:264 __call__\r\n        return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n    anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:139 __init__\r\n        initial_value() if init_from_fn else initial_value,\r\n    anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:908 <lambda>\r\n        partition_info=partition_info)\r\n\r\n    TypeError: __call__() got an unexpected keyword argument 'partition_info'\r\n##############################################################\r\n\r\nHOW CAN I SOLVE IT?\r\nTHANKS, IN ADVANCE.\r\n", "Can you file a separate issue?", "Downloads/3D/pointnet2/tf2_train.py:94 train_one_epoch *\r\ntrain_one_step(train_data, train_label, model, optimizer)\r\nanaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:416 call\r\nself._initialize(args, kwds, add_initializers_to=initializer_map)\r\nDownloads/3D/pointnet2/tf2_train.py:67 train_one_step *\r\npred = model.get_model(data, True, None)\r\nDownloads/3D/pointnet2/models_from_pointnet/pointnet_cls.py:27 get_model *\r\ntransform = input_transform_net(point_cloud, is_training, bn_decay, K=3)\r\nDownloads/3D/pointnet2/models_from_pointnet/transform_nets.py:19 input_transform_net *\r\nnet = tf_util.conv2d(input_image, 64, [1,3],\r\nDownloads/3D/pointnet2/utils_from_pointnet/tf2_util.py:164 conv2d *\r\nkernel = _variable_with_weight_decay('weights',\r\nDownloads/3D/pointnet2/utils_from_pointnet/tf2_util.py:45 _variable_with_weight_decay *\r\nvar = _variable_on_cpu(name, shape, initializer)\r\nDownloads/3D/pointnet2/utils_from_pointnet/tf2_util.py:21 _variable_on_cpu *\r\nvar = tf.compat.v1.get_variable(name, shape, initializer=initializer, dtype=dtype)\r\nanaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:1503 get_variable\r\naggregation=aggregation)\r\nanaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:1246 get_variable\r\naggregation=aggregation)\r\nanaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:569 get_variable\r\naggregation=aggregation)\r\nanaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:521 _true_getter\r\naggregation=aggregation)\r\nanaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:936 _get_single_variable\r\naggregation=aggregation)\r\nanaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:260 call\r\nreturn cls._variable_v1_call(*args, **kwargs)\r\nanaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:221 _variable_v1_call\r\nshape=shape)\r\nanaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:60 getter\r\nreturn captured_getter(captured_previous, **kwargs)\r\nanaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:347 variable_capturing_scope\r\nlifted_initializer_graph=lifted_initializer_graph, **kwds)\r\nanaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:264 call\r\nreturn super(VariableMetaclass, cls).call(*args, **kwargs)\r\nanaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:139 init\r\ninitial_value() if init_from_fn else initial_value,\r\nanaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:908\r\npartition_info=partition_info)\r\n\r\nTypeError: __call__() got an unexpected keyword argument 'partition_info'\r\n\r\n\r\nAbove Error is the main issue. \r\nI Guess it came from below this lines.\r\n\r\ndef _variable_on_cpu(name, shape, initializer, use_fp16=False):\r\n. . . . . . . .~ skip\r\nwith tf.device(\"/cpu:0\"):\r\ndtype = tf.float16 if use_fp16 else tf.float32\r\nvar = **tf.compat.v1.get_variable**(name, shape, initializer=initializer, dtype=dtype)\r\nreturn var\r\n\r\n\r\nThanks you @alextp ", "I meant a separate issue with full instructions to reproduce. This looks\nlike it's coming from mixing tf v1 layers with tf v2 initializers.\n\nOn Thu, Jun 27, 2019 at 5:23 PM tolry418 <notifications@github.com> wrote:\n\n> Downloads/3D/pointnet2/tf2_train.py:94 train_one_epoch *\n> train_one_step(train_data, train_label, model, optimizer)\n> anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:416\n> call\n> self._initialize(args, kwds, add_initializers_to=initializer_map)\n> Downloads/3D/pointnet2/tf2_train.py:67 train_one_step *\n> pred = model.get_model(data, True, None)\n> Downloads/3D/pointnet2/models_from_pointnet/pointnet_cls.py:27 get_model *\n> transform = input_transform_net(point_cloud, is_training, bn_decay, K=3)\n> Downloads/3D/pointnet2/models_from_pointnet/transform_nets.py:19\n> input_transform_net *\n> net = tf_util.conv2d(input_image, 64, [1,3],\n> Downloads/3D/pointnet2/utils_from_pointnet/tf2_util.py:164 conv2d *\n> kernel = _variable_with_weight_decay('weights',\n> Downloads/3D/pointnet2/utils_from_pointnet/tf2_util.py:45\n> _variable_with_weight_decay *\n> var = _variable_on_cpu(name, shape, initializer)\n> Downloads/3D/pointnet2/utils_from_pointnet/tf2_util.py:21 _variable_on_cpu\n> *\n> var = tf.compat.v1.get_variable(name, shape, initializer=initializer,\n> dtype=dtype)\n> anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:1503\n> get_variable\n> aggregation=aggregation)\n> anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:1246\n> get_variable\n> aggregation=aggregation)\n> anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:569\n> get_variable\n> aggregation=aggregation)\n> anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:521\n> _true_getter\n> aggregation=aggregation)\n> anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:936\n> _get_single_variable\n> aggregation=aggregation)\n> anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:260\n> call\n> return cls._variable_v1_call(*args, **kwargs)\n> anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:221\n> _variable_v1_call\n> shape=shape)\n> anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:60\n> getter\n> return captured_getter(captured_previous, **kwargs)\n> anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:347\n> variable_capturing_scope\n> lifted_initializer_graph=lifted_initializer_graph, **kwds)\n> anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:264\n> call\n> return super(VariableMetaclass, cls).call(*args, **kwargs)\n> anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:139\n> init\n> initial_value() if init_from_fn else initial_value,\n>\n> anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:908\n> partition_info=partition_info)\n>\n> TypeError: *call*() got an unexpected keyword argument 'partition_info'\n>\n> Above Error is the main issue.\n> I Guess it came from below this lines.\n>\n> def _variable_on_cpu(name, shape, initializer, use_fp16=False):\n> . . . . . . . .~ skip\n> with tf.device(\"/cpu:0\"):\n> dtype = tf.float16 if use_fp16 else tf.float32\n> var = *tf.compat.v1.get_variable*(name, shape, initializer=initializer,\n> dtype=dtype)\n> return var\n>\n> Thanks you @alextp <https://github.com/alextp>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26665?email_source=notifications&email_token=AAABHRJXDK7Y6SOKZWLZCTDP4VKWRA5CNFSM4G5UOJCKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODYYWV5I#issuecomment-506555125>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRPL2UGAC5CO6RKBJLLP4VKWRANCNFSM4G5UOJCA>\n> .\n>\n\n\n-- \n - Alex\n", "Now tf 2.0.0beta0 has solved this problem... Thank @pavithrasv !"]}, {"number": 26664, "title": "Using create_training_graph with act_quant not found error ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary):\r\nBinary\r\n- TensorFlow version (use command below):\r\n1.9.0\r\n- Python version:\r\n3.6\r\n- CUDA/cuDNN version:\r\n9.0\r\n- GPU model and memory:\r\n\r\nAfter the addition of the quantization training code like:\r\n\r\n    ......\r\n    full_precision_graph = tf.get_default_graph()\r\n    with tf.variable_scope(params['model_scope']):\r\n    build_the_network()\r\n    tensorflow.contrib.quantize.create_training_graph(input_graph=full_precision_graph, quant_delay=0)\r\n    ......\r\n\r\n\r\nI intend to train a int8 model from a pre-trained f32 ckpt model. However, the error raised as \r\n\r\n    ......\r\n    netname/act_quant/max not found in checkpoint\r\n    ......\r\n\r\n\r\nI am wondering if something wrong with my usage of the quantization creating. I only defined the train quantization net and the screen did print out something like the following information before the error occurred\r\n\r\n    ......\r\n    INFO:tensorflow:Skipping netname/conv3_eltwise4/add, because its followed by an activation.\r\n    ....", "comments": ["Okay I know the problem. No such error if I train from the scratch...So what's the correct way to train from a pre-trained f32 model that contains no elements like /act_quant/max?", "I kinda solved it...In case of fine-tuning from full-precision model without quantization parameters like .../act_quant/min, one just restores the weights and biases from the .ckpt file and let the graph fill the default quantization parameters. Passing the tf.saver APIs using the collected weights and biases from .ckpt file will do this. And I think the current tf APIs are enough to realize this, so no need of extra support for this. Anyway, thanks for the attention from @ymodak. ", "Thanks for the update, closing since it seems to be worked around now.", "@asjmasjm what is the workaround? I am using the export_inference_graph.py to export a quantized model, but I am getting the error FeatureExtractor/MobilenetV3/Conv/conv_quant/max not found in checkpoint. I have trained the model myself (starting from f32 but trained a quantized model).  ", "Problem still unsolved.\r\n@petewarden don't you want to write what did you mean?", "I'm also experiencing this issue as well, any insights or solutions would be great"]}, {"number": 26663, "title": "Lite: Split Op Axis Validation Added", "body": "1:> Axis value check added.\r\n2:> Contradicting code removed (optimized_ops)\r\n3:> Error message improvised.\r\n4:> Variable name corrected", "comments": ["@haozha111 , @jdduke : Would please conclude on this PR, TIA!"]}, {"number": 26662, "title": "tf.logging does not exist in tf 2.0, yet is used in example", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: 2.0.0-alpha0\r\n- Doc Link: \r\nhttps://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l02c01_celsius_to_fahrenheit.ipynb\r\nand \r\nhttps://classroom.udacity.com/courses/ud187/lessons/e0c70c77-5584-4f83-a47b-a67a6172ae75/concepts/fe91023e-9699-418a-8f4e-58c6acad1169\r\n\r\n\r\n**Describe the documentation issue**\r\nIf you try to execute this code in a python REPL, it fails at\r\n\r\n    tf.logging.set_verbosity(tf.logging.ERROR)\r\n\r\nwith \r\n\r\n    AttributeError: 'module' object has no attribute 'logging'\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n\r\nDepends on how you want it. As of https://www.tensorflow.org/versions/r2.0/api_docs/python/tf, it seems the logging API does not exist. Of the methods in https://stackoverflow.com/questions/35911252/disable-tensorflow-debugging-information, only \r\n\r\n    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \r\n\r\nworked.\r\n", "comments": ["@serv-inc Those examples are not related to TF2.0. Those examples uses Tensorflow master(TF1.13.1).\r\nIn the example, import tensorflow as tf was used. For 2.0 we need to use, tensorflow==2.0.0-alpha0. Thanks!", "@jvishnuvardhan : thank you for the explanation. This is a course about Tensorflow 2.0. I had only installed the 2.0 version on my system, which is why `import tensorflow as tf` used 2.0. Anyways, it seems to be a non-issue if this is simply not supposed to work in 2.0.", "I am closing this issue as tf.logging is not there in TF2.0. Thanks!", "I fixed this in all the examples I could find:\r\n\r\n```\r\nimport logging\r\nlogger = tf.get_logger()\r\nlogger.setLevel(logging.ERROR)\r\n```", "@MarkDaoust is this documented somewhere? "]}, {"number": 26661, "title": "what does USE_EIGEN_TENSOR used?", "body": "There are some  USE_EIGEN_TENSOR macros, What does it used for?\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}]