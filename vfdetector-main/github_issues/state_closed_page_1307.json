[{"number": 13898, "title": "R1.3", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->"]}, {"number": 13897, "title": "No OpKernel was registered to support Op 'SegmentSum'", "body": "### ENV\r\n* OSX 10.12.6\r\n* No GPU\r\n* Tensorflow master. Built from source by `tensorflow/contrib/makefile/build_all_linux.sh`\r\n\r\n### Error info\r\n```\r\nInvalid argument: No OpKernel was registered to support Op 'SegmentSum' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[Node: emb_179/embedding_lookup_sparse = SegmentSum[T=DT_FLOAT, Tindices=DT_INT32](emb_179/embedding_lookup_sparse/mul, emb_179/embedding_lookup_sparse/Cast)]]\r\n``` \r\n\r\n### My usage\r\nI use python to train model and freeze the graph with checkpoint. Then I use c++ to load freeze graph protobuf and to predict for some inputs.\r\n#### python core part\r\n```\r\nwith tf.variable_scope('input/sparse_field'):\r\n  with tf.variable_scope('index'):\r\n    sparse_index = tf.placeholder(tf.int64)\r\n  with tf.variable_scope('id'):\r\n   sparse_id = tf.placeholder(tf.int64)\r\n   with tf.variable_scope('value'):\r\n    sparse_val = tf.placeholder(tf.float32)\r\n  with tf.variable_scope('shape'):\r\n    sparse_shape = tf.placeholder(tf.int64)\r\nwith tf.variable_scope('label'):\r\n  label = tf.placeholder(tf.float32)\r\nsparse_ids = tf.SparseTensor(sparse_index, sparse_id, sparse_shape)\r\nsparse_vals = tf.SparseTensor(sparse_index, sparse_val, sparse_shape)\r\n\r\ninput_size = 100\r\nembedding_size = 50\r\nwith tf.variable_scope(\"emb_179\"):\r\n  embedding_variable = tf.Variable(tf.truncated_normal([input_size, embedding_size], stddev=0.05), name='emb' + str(field_id))\r\n  embedding = tf.nn.embedding_lookup_sparse(embedding_variable, sparse_ids, sparse_vals, \"mod\", combiner=\"sum\")\r\n...\r\n``` \r\n#### C++ prediction core part\r\n```\r\nauto id_indice_tensor =\r\ntest::AsTensor<int64>(indice, {static_cast<int64>(indice.size()/2), 2});\r\ninputs.push_back(std::pair<std::string, Tensor>(\"input/sparse_field/index/Placeholder\", id_indice_tensor));\r\nauto id_list_tensor = test::AsTensor<int64>(fid_list);\r\ninputs.push_back(std::pair<std::string, Tensor>(\"input/sparse_field/id/Placeholder\", id_list_tensor));\r\nauto val_list_tensor = test::AsTensor<float>(fval_list);\r\ninputs.push_back(std::pair<std::string, Tensor>(\"input/sparse_field/value/Placeholder\", val_list_tensor));\r\n\r\nstd::vector<tensorflow::Tensor> outputs;\r\nStatus status = session->Run(inputs, {\"predict/add\"}, {}, &outputs);\r\n```\r\n\r\n### Have checked\r\nI have checked it has registered all real type and complex type in cpu mode. \r\nhttps://github.com/tensorflow/tensorflow/blob/d1183ca6a245cd0b498c46fd1079909ebc4abc3a/tensorflow/core/kernels/segment_reduction_ops.cc#L333\r\n```\r\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_REAL_CPU_KERNELS_ALL);\r\nREGISTER_COMPLEX_CPU_KERNELS_ALL(complex64);\r\nREGISTER_COMPLEX_CPU_KERNELS_ALL(complex128);\r\n```", "comments": ["Hi,formath:\r\nTry to add `tensorflow/core/kernels/segment_reduction_ops.cc` at the end of `/xx/tensorflow/tensorflow/contrib/makefile/tf_op_files.txt`. \r\n\r\nYou can refer to https://github.com/tensorflow/tensorflow/issues/3543.\r\n\r\nAfter doing this, the following error missed, but a new error emerges as listed in the appendix.\r\n`Invalid argument: No OpKernel was registered to support Op 'SegmentSum' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>`\r\n\r\nHope that is useful to you\r\n  \r\n#### Appendix \r\n```\r\nAdd graph to session successfully\r\nInvalid argument: indices[0] = 384 is not in [0, 11)\r\n\t [[Node: emb_6/embedding_lookup_sparse/embedding_lookup = Gather[Tindices=DT_INT64, Tparams=DT_FLOAT, _class=[\"loc:@emb_6/emb6\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](emb_6/emb6, _arg_input/sparse_6/id/Placeholder_0_9)]]\r\n```\r\n    \r\n    ", "@kiminh Thanks. I will open another issue for the new problem. Welcome your contribution."]}, {"number": 13896, "title": "Build Error - Unable to clone jsoncpp repo, all others seem to be working", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**:  Source from master branch\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: 3.6.2\r\n- **Bazel version (if compiling from source)**:  cmake 3.9.4\r\n- **CUDA/cuDNN version**: CUDA 9.0/cuDNN 7\r\n- **GPU model and memory**:  GTX 1080\r\n- **Exact command to reproduce**:  `MSBuild /filelogger /m:4 /p:Configuration=Release tf_python_build_pip_package.vcxproj`\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI have been trying to build the TensorFlow pip package.  For some reason, while I have verified that the build is able to clone the other git repos, the `jsoncpp` one seems to be killing my build.\r\n\r\n### Source code / logs\r\n`    42>CustomBuild:\r\n         Creating directories for 'jsoncpp'`\r\n`    42>CustomBuild:\r\n         Performing download step (git clone) for 'jsoncpp'`\r\n`    42>CustomBuild:\r\n         fatal: could not create work tree dir 'jsoncpp': Permission denied`\r\n`    42>CustomBuild:\r\n         fatal: could not create work tree dir 'jsoncpp': Permission denied`\r\n`    42>CustomBuild:\r\n         fatal: could not create work tree dir 'jsoncpp': Permission denied`\r\n`    42>CustomBuild:\r\n         -- Had to git clone more than once:\r\n                   3 times.`\r\n`    42>CustomBuild:\r\n         CMake Error at C:/Users/Bryce/Documents/Programming/os_clones/tensorflow/tensorflow/contrib/cmake/build/jsoncpp/tmp/jsoncpp-gitclone.cmake:66 (message):\r\n           Failed to clone repository:\r\n           'https://github.com/open-source-parsers/jsoncpp.git'`\r\n\r\nAnd later on,\r\n\r\n`C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\VC\\VCTargets\\Microsoft.CppCommon.targets(171,5): error MSB6006: \"cmd.exe\" exited with code 1. [C:\\Users\\Bryce\\Documents\\Programming\\os_clones\\tensorflow\\tensorflow\\contrib\\cmake\\build\\jsoncpp.vcxproj]`\r\n\r\n[msbuild.log](https://github.com/tensorflow/tensorflow/files/1404600/msbuild.log)", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it seems like a problem with your environment. There is also a larger community that reads questions there. Thanks!"]}, {"number": 13895, "title": "In the estimator of Tensorflow, how does it work when model_fn is called multiple times?", "body": "    def model_fn(features, labels, mode, params):\r\n      \"\"\"Model function for Estimator.\"\"\"\r\n    \r\n      # Connect the first hidden layer to input layer\r\n      # (features[\"x\"]) with relu activation\r\n      first_hidden_layer = tf.layers.dense(features[\"x\"], 10, activation=tf.nn.relu)\r\n    \r\n      # Connect the second hidden layer to first hidden layer with relu\r\n      second_hidden_layer = tf.layers.dense(\r\n          first_hidden_layer, 10, activation=tf.nn.relu)\r\n    \r\n      # Connect the output layer to second hidden layer (no activation fn)\r\n      output_layer = tf.layers.dense(second_hidden_layer, 1)\r\n    \r\n      # Reshape output layer to 1-dim Tensor to return predictions\r\n      predictions = tf.reshape(output_layer, [-1])\r\n    \r\n      # Provide an estimator spec for `ModeKeys.PREDICT`.\r\n      if mode == tf.estimator.ModeKeys.PREDICT:\r\n        return tf.estimator.EstimatorSpec(\r\n            mode=mode,\r\n            predictions={\"ages\": predictions})\r\n    \r\n      # Calculate loss using mean squared error\r\n      loss = tf.losses.mean_squared_error(labels, predictions)\r\n    \r\n      # Calculate root mean squared error as additional eval metric\r\n      eval_metric_ops = {\r\n          \"rmse\": tf.metrics.root_mean_squared_error(\r\n              tf.cast(labels, tf.float64), predictions)\r\n      }\r\n    \r\n      optimizer = tf.train.GradientDescentOptimizer(\r\n          learning_rate=params[\"learning_rate\"])\r\n      train_op = optimizer.minimize(\r\n          loss=loss, global_step=tf.train.get_global_step())\r\n    \r\n      # Provide an estimator spec for `ModeKeys.EVAL` and `ModeKeys.TRAIN` modes.\r\n      return tf.estimator.EstimatorSpec(\r\n          mode=mode,\r\n          loss=loss,\r\n          train_op=train_op,\r\n          eval_metric_ops=eval_metric_ops)\r\n\r\nAbove is an example of the model_fn used by Tensorflow's [Estimator][1].\r\n\r\nAs mentioned in the tutorial, this model_fn could be called in different context (train, predict, evaluate). However, I'm a bit confused, because each time the model_fn is called, **instead of reusing existing graph, it seems to create a new graph.(or create new node in the graph)**\r\n\r\nFor example, firstly I called model_fn under TRAIN mode, then I called model_fn with PREDICT mode. How can I make sure the PREDICT one is reusing the weight of the trained values?\r\n\r\n  [1]: https://www.tensorflow.org/extend/estimators", "comments": ["Is it true that the code in the tutorial forgets to add \"reuse=True\" as the parameter of layer functions?", "Right, that might be a problem.\r\n\r\n\r\n@MarkDaoust WDYT?", "The code and doc is correct, it just doesn't emphasize enough that it rebuilds the graph from scratch for each method, and for evaluate/predict is loads variables from the most recent checkpoint.\r\n\r\nGenerally we're moving away from the whole `name_scope` system for reuse, the future of variable sharing is layer objects, if you want to reuse a variable reuse the layer object.\r\n\r\nWe're rewriting this doc right now, I'll make sure it's more clear about this.", "Closing since the doc will be obsolete soon. Please check the new doc once it's out. Thank you both!", "Any idea when the new doc will be out? I still have this problem, the estimator is virtually useless if it creates the same graphs every time I need to evaluate something. \r\n\r\nCheers,\r\n\r\nFrancesco Saverio\r\n  ", "@FrancescoSaverioZuppichini: The updated docs will be available in 1.5, but drafts are already in the repo, [this doc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/get_started/checkpoints.md\r\n) describes how checkpointing works with estimators.\r\n\r\nIn the mean time, let me summarize:\r\n\r\n@hanfeisun: Yes, each (`train`, `evaluate`, `predict`) method rebuilds the `graph`. But the estimator **always** loads the checkpoints from the `model_dir` into the graph before doing anything. In fact `evaluate` and `predict` can't be used until after you've called `train` at least once, so that there is a checkpoint to load. \r\n\r\n\r\nThe `reuse` layer argument does something different. It's an outdated way to use the same layer on two different inputs. A much better way to have this effect is to use the layer classes, and just reuse the layer object. I believe the following two code blocks are equivalent:\r\n\r\n```\r\nout0 = tf.layers.dense(x, 10)\r\nout1= tf.layers.dense(y,10, reuse=True)\r\n```\r\n\r\n```\r\nmy_layer = tf.layers.Dense(10)\r\nout0 = my_layer(x)\r\nout1 = my_layer(y)\r\n```\r\n\r\nI hope this helps, but in the future this sort of question is probably better suited for stack overflow.\r\n", "Thank you @MarkDaoust for the answer. So I am forced to use the layer library? What if I want to use my own implementation? Is there any way to set a flag inside the model_fn? Or to have access to the estimator graph and directly used a scope do build in the same graph? Also, I am wondering how to run validation, assuming I want to train for one epoch and then run the model on the validation set I have to first call `.train` and then `.evaluate` that will re-build the graph slowing everything. Any clever way to do so? I think is better to discuss here since It is issue lots of people have and the first link that pop up on google is this one.\r\n\r\nCheers,\r\n\r\nFrancesco Saverio\r\n  ", "> So I am forced to use the layer library?\r\n\r\nNo, we encourage you to use `layers` to wrap trainable state, but whether or not you use `layers` has no effect on the checkpoint reloading logic.\r\n\r\n> I have to first call `.train` and then `.evaluate` that will re-build the graph slowing everything.\r\n\r\nIf you want to run repeated evaluations while training, consider the [train_and_evaluate](https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate) function. \r\n\r\nIf you're working outside of estimators, [`Datasets`](https://www.tensorflow.org/programmers_guide/datasets) do have advanced iterator types that allow you to swap input pipelines without rebuilding the graph. But often the changes you want to make to the graph between `train`, `evaluate`, and `predict` are more extensive than just the input stream.", "Thank you, really. Definitely, the API could have been better implemented. I am used to the Dataset, I could wrap the `Dataset` inside the input function but I am not sure how to swap between two datasets using the `make_initializable_iterator` since `train_and_evaluate` have not any hooks. \r\n\r\nOne last question, it is not clear to me how to avoid the re-creation of the graph using a generic implementation (assuming no `layers` library). Should I always set `reuse=True`, is there a faster way? Maybe with a scope? \r\n\r\nI am sorry to bother you again, but really, the doc is so confusing that I could not find any answers to my questions.\r\n\r\nThank you again :)", "Hello again,\r\n\r\nIs there any news? I saw there is a pre-release of the 1.5 version but I was not able to find how to avoid the model re-creation at every call of train/evaluate.\r\n\r\nCheers", "Well that's kind of my point. If you're using estimators you have to rebuild it on each call to `train`, `evaluate` or `predict`.\r\n\r\nThe only way around that is to use a different model abstraction, or roll your own.\r\n\r\nFor example [the Eager examples](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples) may provide the flexibility you're looking for.", "As far as I understood they said they will fix it in 1.5. Do you have any idea why they do that? I mean, adding a flag something like `rebuild=True` was impossible?", "Dear @MarkDaoust the eager execution cannot be used with the Estimator. Also ` train_and_evaluate` will run evaluation AFTER training. I want to have a validation test that must be run at every train step but this seems impossible to do with the Estimator, maybe a better design could have solved the problem. Any ideas? I want to run one train step and one evaluation step WITHOUT loading the graph in order to also plot the validation score", "I have spent a week on my project based on Estimator until I stopped here. I am an early developer and had full faith in it. After having encountered this countless times, my heart is broken. If we can't evaluate during training, then **why is Estimator useful**? ", "I also stop to use it. It is a total crap. I will write my own", ">  If we can't evaluate during training, then why is Estimator useful?\r\n\r\nOther possible approaches:\r\n\r\n * Alternate `train` and `eval` calls\r\n * In a separate process, monitor the `model_dir` for checkpoints and run `eval` on each time a new one appears.\r\n\r\n", "That is more a hack than a solution", "CC @martinwicke ", "@ispirmustafa FYI also.\r\n\r\nSorry you all are frustrated. It also appears the documentation is adding to your confusion. Let me clarify a few points:\r\n\r\n* Estimator will evaluate during training. See the train_and_evaluate documentation. It sounds like you're doing single machine execution.\r\n* You can simply pass a function returning a dataset to Estimator. There shouldn't be a problem swapping out inputs.\r\n* Estimator will run the `model_fn` every time you call `train`, `predict`, or `evaluate`. \r\n* It will automatically reload the last checkpoint (or you can specify one)\r\n* You don't have to worry about reuse between train and eval. As long as your variables are called the same (most likely, simply by using the same or similar enough code to create the layers or raw variables), they will be automatically shared between training and evaluation.\r\n* Personally, I would stay away from `reuse`, and I would never use `variable_scope`. If you must (or have an existing model using this), that's fine. You can use either of these features to reuse variables within your model. Note that has absolutely no effect on whether variables are shared between training and evaluation. Unless you take steps to prevent it, variables are always shared between training and evaluation.", "an addition to @martinwicke, `Estimator` creates a new graph for every call of `model_fn`. That makes variable names kept same for each `mode`. ", "I am kind of confuse again. If `Estimator` creates a new graph for every call of `model_fn`, then each of `train` and `evaluate` has a graph. A loop iterates `train` and `evaluate` will create two new graphs every time? @ispirmustafa @martinwicke ", "@tengerye  yes! That's the problem", "Yes, if you call train and evaluate in a loop, you will create two new graphs every time. You will also read and write a checkpoint in each iteration of your loop. If you epoch is long, you probably want a checkpoint every time anyway so that's not a big deal, but if you train on less than an epoch in each iteration, such loops are a bad idea.\r\n\r\n@FrancescoSaverioZuppichini I'm afraid I lost track of what the actual problem is.", "Thank you sincerely for your kind answer @martinwicke. According to your reply, `train_and_evaluate` may suit my requirement. I believe @FrancescoSaverioZuppichini and me are expecting more flexible implementation that we could write some snippets of codes to replace a component instead of re-writing everything. \r\nThank you again.", "@tengerye which component would you like to replace?\r\n\r\nMany of the restrictions of Estimator are in place so you can scale up without having to change your code. That makes local training less convenient, but it avoids a lot of pitfalls later.", "@martinwicke I see your point now. I thought it would be better if training and evaluation could progress interactively under single machine execution. I think maybe it it better to provide an independent implementation especially for single machine? Since a lot of people work on single machine at the beginning.", "For single machine training you can start with tf.keras.Model. Once your ready to scale, use that Model in a model_fn, or use model_to_estimator. \r\n\r\nMaybe there's a way to do interactive training in a way that still allows you to scale without forcing people to rethink their models, but I have no solution to that (the workflow above using Keras is the best I've seen).", "@martinwicke May you elaborate the argument that _\"... but if you train on less than an epoch in each iteration, **such loops are a bad idea.\"**_ .  In many machine learning methods, shouldn't we need to evaluate the model after every epoch? \r\n\r\nAFAIK, Keras has the idea to evaluate model every epoch  in its `fit_generator` and `fit`. Particularly, one use case (that been showed in TensorFlow official code [here](https://github.com/tensorflow/models/blob/ce4459762727264f35a00a7ff1f8151380ea681d/official/resnet/resnet.py#L605) . Here is my example using K-fold cross validation for `Estimator`. : \r\n\r\n```python\r\nfor _ in range(training_epochs // epochs_per_eval):\r\n  # #####################\r\n  # K-fold cross validation\r\n  # #####################\r\n  train_features, train_labels, eval_features, eval_labels = data_provider.split_training_data(\r\n      test_size=0.2,\r\n      shuffle=True)\r\n\r\n  # The pipeline below is similar to `fit_generator` in Keras\r\n  # ##############\r\n  # Training cycle\r\n  # ##############\r\n  estimator.train(\r\n      input_fn=lambda: data_provider.get_input_fn(\r\n          mode=tf.estimator.ModeKeys.TRAIN,\r\n          features=train_features,\r\n          labels=train_labels,\r\n          batch_size=batch_size,\r\n          parse_fn=parse_sample_fn,\r\n          preprocess_fn=preprocess,\r\n          steps_per_epoch=steps_per_epoch * epochs_per_eval,\r\n          shuffle_buffer=shuffle_buffer,\r\n          num_parallel_calls=cpu_cores),\r\n      steps=steps_per_epoch * epochs_per_eval,)\r\n\r\n  # ################\r\n  # Evaluation cycle\r\n  # ################\r\n  eval_result = estimator.evaluate(\r\n      input_fn=lambda: data_provider.get_input_fn(\r\n          mode=tf.estimator.ModeKeys.EVAL,\r\n          features=eval_features,\r\n          labels=eval_labels,\r\n          batch_size=batch_size,\r\n          parse_fn=parse_sample_fn,\r\n          preprocess_fn=preprocess,\r\n          steps_per_epoch=200,\r\n          shuffle_buffer=None,\r\n          num_parallel_calls=cpu_cores),\r\n      steps=200)\r\n  print(eval_result)\r\nprint(\"---- Training Completed ----\")\r\n```\r\n\r\nOne of the issues ( I think) with `Estimator` in this approach is : \r\n* If `epochs_per_eval` and `steps_per_epoch` is relative small, the overhead to create the Graph for training and evaluation in each iteration in a loop is huge.\r\n\r\nMy question is:\r\n* Is it possible to share the Graph in [`_train_model`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L817) and [`_evaluate_model`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L910), instead of creating them everytime ? Thanks for reading.", "> My question is:\r\n>\r\n> - Is it possible to share the Graph in _train_model and _evaluate_model, instead of creating them \r\n>   everytime ? Thanks for reading.\r\n\r\nThe reason for this strict separation is that we want to guarantee that the model can be scaled up and trained in a distributed fashion. Therefore we make sure that the behavior of an `Estimator` is consistent, whether you train it in local mode or on a cluster of machines. If we allowed you to share the graph between training and eval, it would be very easy to introduce subtle mistakes that would break the training once you run it distributed, so we do not allow it.\r\n\r\nI understand that this makes life harder in \"local\" mode. If you are not interested in distributed training, I recommend using `tf.keras.Model`. You can always go back to an Estimator using `model_to_estimator` if you change your mind.", "@martinwicke Thank you for your answer but I think you just did not think about that. I cannot believe it is too hard to add a parameter, something like `reset=False` to the Estimator. ", "It would be hard to do in `Estimator`, but you could probably do it in `train_and_evaluate`, changing how it evaluates in local mode. We had actually discussed this, and it would be feasible for `train_and_evaluate` to build both train and eval graphs against the same set of variables, not load checkpoints, and switch between training and evaluation in memory. This would still not share the graphs (which wouldn't be a good idea given the distribution concerns), but it would avoid reloading. \r\n\r\nWe never prioritized this since it's unclear to me how much you gain from this. For small models, checkpoint loading isn't a huge cost, and for large models, checkpoint loading again isn't a huge cost compared to the cost of training. ", "I can assure you that it is impossible to do cross-validation since loading the model takes too long. Probably the question is still unclear, all these people are asking to train the model for one epoch, run the validation set, and restart until the train loop is finished. \r\n\r\nSo assuming we have 100 epochs, we need to run 100 times the estimator in train mode and 100 times the estimator in evaluate mode. This adds up to 200 times the model is restored. Come on guy! You are Google, are you? `train_and_evaluate` will just evaluate after the train is done. Meaning, that it will first train for, following our example, 100 epochs and then run the evaluation one once! \r\n\r\nI find really hard to believe that, after all these people explain the same problem, is still so unclean. Without any way to properly do cross-validation, the Estimator is almost a useless piece of software since we can not even see if our model is overfitting or not using a validation set.\r\n\r\n@martinwicke Thank you to quickly reply to us, I hope now the problem is cleaner.", "`train_and_evaluate` *will* evaluate after each epoch (or more precisely in intervals you can set yourself). As you say it will also load the checkpoint after each epoch. So I do indeed not understand the problem. From your description I think it would solve your problem just fine, unless there is a performance problem, and I have tried to explain why I don't think performance is an issue here. \r\n\r\nAs I explained above, Estimator does make some tradeoffs where it enforces things that are unnecessary in order to ensure that the transition to distributed training will work smoothly. If Estimator is not suitable for your use case (and especially if you do not need distributed training), you should look into tf.keras.Model.\r\n\r\nI do suggest you keep your comments civil, otherwise it will be hard to have a constructive discussion.", "@martinwicke I am sorry if something I wrote seems uncivil to you. The doc about `train_and_evaluate` does not says anything about a parameter to set the intervals https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate \r\n\r\nI was trying to use Estimator with a model to generate text, even with just one layer of LSTM with 256 cells, it takes forever to load the graph after each epoch.  So for now, I just give up. Paradoxically the problem is even highlighted when using small models to test/play with. \r\n\r\nI agree that the model must be loaded to ensure consistency in a distributed training, but still, I can not see why such an easy and important feature was not implemented. ", "You are correct, the information about the local run behavior for evaluation is missing from the `train_and_evaluate` docstring (and also from the EvalSpec docstring; @xiejw FYI, we should add this). \r\n\r\n`train_and_evaluate` will evaluate every `eval_spec.throttle_sec` seconds. It will stop after `train_spec.max_steps`. \r\n\r\nI am still surprised a small model takes long to load (how long is \"forever\"?). Could it be your Python code? I.e. does it also take long before it starts the first iteration, before there is a checkpoint? It will create the graph using its Python code, not load it from the checkpoint, so that can be a source of slowness. \r\n", "I agree with @FrancescoSaverioZuppichini . I read the docstring and the implementation itself, `train_and_evaluate` is just simply a for loop iterating between estimator.train and estimator.evaluate. \r\n\r\nI never try distributed training cos I can't afford it. By doing train and evaluate with estimators is painful even though it does offer many other good features. \r\n\r\nEach restoring from checkpoint and kick start the session takes ridiculously long time. Unless you are training ImageNet with millions images on models of tens of layers which consumes your whole weekend just for one epoch, re-build the graph every time is so time consuming. In my case, it takes about 5-10% training time cos my dataset is small.\r\n\r\nWhat many peoples here want is (I try to summarize again):\r\nfor e in range(epochs):\r\n      train for whole training set.\r\n      evaluate on train set, save loss, accuracy, metrics... to tensorboard at ./summary_train/\r\n      evaluate on val set, save loss, accuracy, metrics... to tensorboard at ./summary_val/\r\n\r\nAt the end of the day, we will have a trained model, a chart indicates the loss and accuracy of both train and val metrics to measures overfitting and so on.\r\n\r\nEstimator does this with 2 or 3 times re-building the graph and loading checkpoints.\r\n\r\nAbout graph consistency between training and evaluation, one can simply build a 2-in-1 graph with a `is_training` placeholder indicating when should the graph execute in which mode. `tf.layers.barch_norm` does allow passing `training` with a bool tensor. Or we can use `tf.cond` to branch between each mode. But estimators do not allow this anyway.\r\n\r\nIt is OK to keep \"training things\" like gradients, optimizer parameters in validation mode instead of release them and re-build the graph again.\r\n\r\nI like estimators cos it helps aggregation of metrics and predictions. You can't just tell people to do it their own if they are not satisfied, what's the purpose of estimators except from distributed training anyway?", "A little bit orthogonal, but I thought it may address one of @FrancescoSaverioZuppichini 's issue of reloading the model for evaluation at each epoch (or whatever frequency you configure):\r\n\r\nI often run things locally (if data is not huge), but most of the time I run different processes (both locally) for training and evaluating.\r\n\r\nSo one would run with \"--train\" for the 100 (?) epochs, saving checkpoints at every X time/epochs.\r\n\r\nAnd the evaluation process would monitor for new checkpoints, load and evaluate them in parallel, without stopping the training. This can be done with [tf.contrib.Experiment](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Experiment) -- see [continuous_eval()](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Experiment#continuous_eval) -- and [tf.contrib.learn_runner](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/learn_runner/run) -- with `schedule=\"continuous_eval\"` -- with not many extra lines of code.\r\n\r\nI hope it helps.\r\n\r\nBtw, how big are your checkpoints ? When you say it takes a long time to load, how much is it roughly ? (10s, 1min, 10min, 30min, 1hour?)\r\n\r\nOh, also, how are you doing cross-validation ? Sorry I didn't figure from the code/description ... \r\n\r\ncheers \r\n\r\n", "Our model takes up nearly all of our GPU memory so we cannot instantiate two of the graphs on one computer. However we want to periodically evaluate sharing the same parameters on the same computer. I guess we cannot use Estimators for this?", "@jperl: you can, however, expect the tf.Graph will be reloaded from GPU every time you call Estimator.evaluate. \r\n\r\nIt can be slow sometime, especially when your steps_per_eval is small. ", "I'd like to get a bit more information on this problem but from the predict() point of view.\r\n\r\nI posted on stackoverflow but really there is not much support there for now. I can see why people would try here.\r\n \r\nhttps://stackoverflow.com/questions/49966447/keep-the-tensorflow-estimator-in-memory-while-waiting-for-live-prediction-inputs\r\n\r\nIt look like when running the estimator live and doing one prediction at a time (as data comes in) most of the prediction time is wasted on reloading the model.\r\n\r\nIn my use case I'd like to do the [prediction, action, prediction, action, ...] as fast as possible.\r\n\r\nIs there a way around having the model reloaded in between predictions?", "Hi @fgervais ,\r\nfor predict could you please check out [predictor.from_estimator](https://www.tensorflow.org/api_docs/python/tf/contrib/predictor/from_estimator)", "I'll give it a try thank you", "To clarify:\r\n\r\n> train_and_evaluate will just evaluate after the train is done. Meaning, that it will first train for, following our example, 100 epochs and then run the evaluation one once!\r\n\r\nThat is incorrect. `train_and_evaluate` trains for a configurable amount of iterations, then runs an evaluation until the specified `input_fn` finishes. Then it starts training again by restoring the weights that were used for the eval, and consuming the input stream from where it left off. There might be some minor technicalities in my summarized explanation, but it is conceptually accurate. \r\n\r\nI've been training reasonably large models on a single laptop using `train_and_evaluate` since version 1.2, and I think it works extremely well. The time it takes to rebuild the graph and restore parameters is negligible when you take into account all the time that's spent actually training.", "@ispirmustafa @fgervais I'm also giving this a try. It feels as if I'm almost there, but I'm still struggling with a couple of issues. I've just posted a question with a minimalistic example on stackoverflow:  \r\nhttps://stackoverflow.com/questions/50111636/unable-to-use-core-estimator-with-contrib-predictor\r\n", "@ispirmustafa the predictor worked, thank you.\r\n\r\nIt it helps, I added a bit of information on how I did it [here](https://stackoverflow.com/questions/49966447/keep-the-tensorflow-estimator-in-memory-while-waiting-for-live-prediction-inputs/).", "I made a public repo which provides a complete example of using the predictor with a custom core estimator, where it compares prediction results and performance between estimator.predict() and predictor():  \r\n\r\nhttps://github.com/dage/tensorflow-estimator-predictor-example", "Hi @dage @fgervais \r\nThank you for update. \r\nIf you want to update pydoc of estimator.predict to give your examples I'll be happy to review.", "Hi, I need to see the plots for validation convergence as I train using estimators, and cannot see how to pass validation set to the train function, or how to compute validation losses, I also tried to write my own, without estimators, and always gets the error that graph cannot be larger than 2Gig, could you please help. thanks.", "@rabeehkarimi use tf.estimator.train_and_evaluate. ", "@formigone I'm afraid that your understanding of how input_fn works is not accurate. \r\nAccording to the instruction:\r\n> It is also recommended to train the model a little longer, say multiple epochs, before\r\n  performing evaluation, as the input pipeline starts from scratch for each\r\n  training.\r\n\r\nSo i guess with estimator api, if I want to evaluate within one epoch without resetting the input flow, I have to somehow store the iterator status inside the input_fn? \r\n", "If you're using Estimator in a distributed system, evaluation happens in parallel to training, and not strictly in between epochs. \r\n\r\nIn local mode, to evaluate, say, twice per epoch, you can either use a variable in the input_fn, and store state there, or you can store that state in a hook. Either way, this is somewhat complicated. \r\n\r\nAnother option (and the one used by most of our production pipelines) is to shuffle the inputs. That way, you will get a different set of inputs every time you run, and you can simply evaluate after a given set of steps and then evaluate, without taking special care of the input state. If you do this, be careful that you don't only shuffle the beginning of the data.", "What about the following solution, where we pass in the same estimator to a `SessionRunHook` and evaluate it every certain number of steps (which could be calculated to be roughly one epoch).  As far as I can tell, since `train` is only called once, it doesn't recreate its graph [though the calls to `evaluate` will].  Does this break things?\r\n\r\n```python\r\nclass EvalEarlyStopHook(tf.train.SessionRunHook)\r\n\r\n    def __init__(self, estimator, eval_input_fn, num_steps):\r\n\r\n        self._estimator = estimator\r\n        self._input_fn = eval_input\r\n        self._num_steps = num_steps\r\n\r\n    ...\r\n\r\n    def after_run(self, run_context, run_values):\r\n\r\n        global_step = run_values.results['global_step']\r\n        if (global_step-1) % self._num_steps == 0:\r\n            ev_results = self._estimator.evaluate(input_fn=self._input_fn)\r\n\r\n            if do_stuff_with_results:\r\n                 run_context.request_stop()\r\n```\r\n\r\nWith the main call being something like:\r\n\r\n```python\r\nestimator.train(input_fn=train_input_fn,\r\n                        hooks=[EvalEarlyStopHook(estimator, eval_input_fn, num_steps))\r\n```", "Just came across [InMemoryEvaluatorHook](https://www.tensorflow.org/versions/r1.9/api_docs/python/tf/contrib/estimator/InMemoryEvaluatorHook) API in TF 1.9. Thank you @ispirmustafa and TF team!", "Hi @jperl ,\r\nPlease let us know your feedback based on your usage. ", "@ispirmustafa I just upgraded our project to TF 1.9 and the `InMemoryEvaluatorHook`. It works fantastically :ok_hand:. It has spec up our train/eval loop significantly -- and now evaluation happens at a deterministic step interval (unlike before when there was some weirdness with throttle secs).\r\n\r\nThere is just one minor bug that I made a PR for https://github.com/tensorflow/tensorflow/pull/20822/files (w/ test).", "Hi guys, any suggestions about different post-process input size with estimator? Here is my case:\r\nI use different batch size\uff0832 and 1\uff09 in train and evaluation with estimator and use a subclass of tf.train.SessionRunHook to achieve the purpose for validating during training.\r\n\r\nIn input_fn i use tf.cond to judge the mode(EVAL or TRAIN) and apply different codes respectively. But it seems that when execute estimator.train \uff0ctraining data with batch size 32 will flow through the codes for validation set\uff08with batch size 1) (i think it is caused by tf.cond)  \r\n\r\nSo i wonder if there are ways to allow data with different size flow through different codes under different mode(TRAIN and EVAL) in input_fn\r\n\r\nThank you ~", "@jperl Correct me if I'm wrong, the `InMemoryEvaluatorHook` solves the issue of validating the *network* while training it, but the `tf.estimator.Estimator` still creates multiple graphs (one for train + one for evaluation) ? ", "That is correct.\n", "Hi @martinwicke ,\r\n`InMemoryEvaluatorHook` looks like a great way to evaluate while training but I am running into issue where all eval_metrics are evaluated to 0.0 after the very first time the evaluation hook is triggered. \r\nHere is a snippet of my output when `every_n_iter=50`:\r\n\r\nINFO:tensorflow:Saving dict for global step 0: accuracy = 0.158545, global_step = 0, loss = 3.36605\r\nINFO:tensorflow:Saving dict for global step 50: accuracy = 0.0, global_step = 50, loss = 0.0\r\nINFO:tensorflow:Saving dict for global step 100: accuracy = 0.0, global_step = 100, loss = 0.0\r\nINFO:tensorflow:Saving dict for global step 150: accuracy = 0.0, global_step = 150, loss = 0.0\r\n\r\nMy suspicion is that it does not work because my  `input_fn` is defined as `tf.estimator.inputs.numpy_input_fn` with `num_epochs=1`.\r\n\r\nThe exact definition of the input function that I use to create the `InMemoryEvaluatorHook` is:\r\n```\r\n    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={\"x\": eval_data},\r\n        y=eval_labels,\r\n        num_epochs=1,\r\n        batch_size=25,\r\n        shuffle=False)\r\n```\r\nHow can I confirm if my input_fn is really to blame here? Any ideas how to get around this? Thanks", "Yes, I think your hunch is correct. You probably have to use `num_epochs=None` and set the appropriate amount of eval steps to stop evaluation.", "Hi @tavramov ,\r\nMost probably it's not related to num_epoch.\r\nCould you please create a simple script to reproduce this issue? Also do you mind to open a new issue?", "@martinwicke - Your recommendation works,.The eval metrics get evaluated properly when I set `num_epochs=None` in the `tf.estimator.inputs.numpy_input_fn()` and use the `steps` argument in `InMemoryEvaluatorHook` to stop the evaluation.  But this is a dangerous hack, hopefully there is a better solution to this.\r\n\r\n@ispirmustafa - Issue can be easily reproduced. I just inserted `InMemoryEvaluatorHook` to [cnn_mnist.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py)\r\nSubmitted Issue #[21590](https://github.com/tensorflow/tensorflow/issues/21590) to track this defect\r\nThanks\r\n", "@shanest does your proposal work? it seemed more readable than InMemoryEvaluatorHook, it would be nice if works. It might will create another \"evaluate\" session inside \"train\" session, I am not sure how it will behave, could you please report its behavior?\r\n", "Although `InMemoryEvaluatorHook ` is not loading checkpoint from disk, [it fetches all variable from \"train\" graph and feed it to the \"evaluation\" graph](https://github.com/tensorflow/tensorflow/blob/25c197e02393bd44f50079945409009dd4d434f8/tensorflow/contrib/estimator/python/estimator/hooks.py#L170). It still recreates an evaluation graph and all the variables with it . So the problem of graph recreation remains and the slow loading ckpt from disk is simply replaced with a memory copy. Are there any follow ups on graph reusing in `tf.estimator` ?\r\n", "@JerrikEph If you can I recommend switching to a Keras model / training loop. Then you can create a callback to evaluate using the same model. After switching to Keras models I have not looked back to estimators.", "while do evaluation in the runhook, the eval_metrics calculate each batch and the average the result or calculate after all evaluation dataset? like auc in eval_metrics. Anybody knows this?Please help"]}, {"number": 13894, "title": "Cauchy Distribution", "body": "Added the cauchy distribution to `contrib.distributions`.\r\n\r\nThe cauchy distribution is an example of a distribution with undefined moments. I couldn't find an existing implementation to reference against, so I'm not sure if my approach to this:\r\n```python\r\nif self.allow_nan_stats:\r\n  return constant_op.constant(float(\"nan\"), shape=self.batch_shape)\r\nelse:\r\n  raise ValueError(\"`mean` is undefined for Cauchy distribution.\")\r\n```\r\nis correct, please advise.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it.", "CLAs look good, thanks!\n\n<!-- ok -->", "Thanks for the review. I've made those changes you requested. As an aside, I'm having some issues running unit tests locally. Docker container builds fine, collects the tests, then skips them all. Is it alright if I rely on the CI servers to do the heavy lifting?", "Yes; that's fine.  But I think you may have to ask us to trigger it.\n\nOn Fri, Oct 27, 2017 at 4:12 PM, Charles Shenton <notifications@github.com>\nwrote:\n\n> Thanks for the review. I've made those changes you requested. As an aside,\n> I'm having some issues running unit tests locally. Docker container builds\n> fine, collects the tests, then skips them all. Is it alright if I rely on\n> the CI servers to do the heavy lifting?\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/13894#issuecomment-340118668>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim4ESNd2ov74agb9LxLM-4e2ven43ks5swmNxgaJpZM4QBye4>\n> .\n>\n", "Okay, I'll investigate getting the local tests working a bit further then.", "Okay, now using `np.nan` (instead of `float(\"nan\")` with correct dtype, let me know if there's anything else.", "Okay, fixed some fluff ups in `BUILD` which had the test and dependency paths wrong. Can confirm that the tests on this commit are all passing locally. Could someone please trigger the TF Test Suite again?", "Jenkins, test this please.", "@ebrevdo maybe kickoff another test run?"]}, {"number": 13893, "title": "Update protobuf.cmake to b04e5cba356212e4e8c66c61bbe0c3a20537c5b9", "body": "This fix tries to address the issue raised in #8187 where protobuf.cmake used different version as bazel.\r\n\r\nThe reason for discrepancy was due to the fact that a customerized protobuf was needed with Windows patch. Since the patch has been merged in (https://github.com/google/protobuf/pull/2203), it makes sense to update protobuf.cmake so that the same version of cmake is used.\r\n\r\nThis fix fixes #8187.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 13892, "title": "Inconsistent Result of SyncReplicaOptimizer", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac Sierra 10.12.6\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0\r\n- **Python version**: Python 3.5.2 |Anaconda custom (x86_64)\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: Not used/\r\n- **GPU model and memory**: Not used/\r\n- **Exact command to reproduce**: python synchronous_sgd.py (see below)\r\n\r\n### Describe the problem\r\nTraining a trivial model of 2-layer fully connected MNIST, with one parameter server thread and one worker thread to reproduce this issue.\r\n\r\nThe file is linked here. We run `python synchronized_sgd.py` and `python async_sgd.py` one after one **in the same terminal so that they receive same random results** to recreate the bug.\r\n\r\nThe only difference in the two files below is: async comment out 10 trivial lines from sync. (Please diff)\r\n\r\nhttps://github.com/heyucongtom/PGRD/blob/master/synchronized_sgd.py\r\nhttps://github.com/heyucongtom/PGRD/blob/master/async_sgd.py\r\n\r\nI make sure both trainer receive the exactly same data for each batch, and I also fixed the random seed. As a results, both model shall get exactly the same output. However, they don't.\r\n\r\nThe problem is, after the first step, the two models are in sync. At exactly the **second run of the train_op**, this train_op of the sync replica doesn't update the model, nor does it update the global step, resulting in output:\r\n\r\n```\r\nWorker 0: training step 0 done (global step: 0)\r\nOn trainer 0, iteration 0 ps it reaches 0.078900 accuracy\r\nWorker 0: training step 1 done (global step: 1)\r\nOn trainer 0, iteration 1 ps it reaches 0.319800 accuracy\r\nWorker 0: training step 2 done (global step: 1)\r\nOn trainer 0, iteration 1 ps it reaches 0.319800 accuracy\r\nWorker 0: training step 3 done (global step: 2)\r\nOn trainer 0, iteration 2 ps it reaches 0.455800 accuracy\r\nWorker 0: training step 4 done (global step: 3)\r\nOn trainer 0, iteration 3 ps it reaches 0.477400 accuracy\r\nWorker 0: training step 5 done (global step: 4)\r\nOn trainer 0, iteration 4 ps it reaches 0.478100 accuracy\r\n```\r\n\r\n\r\n**As a comparison, let's take the output of the simple async version. With exactly**\r\n\r\n```\r\nWorker 0: training step 0 done (global step: 0)\r\nOn trainer 0, iteration 0 ps it reaches 0.078900 accuracy\r\nWorker 0: training step 1 done (global step: 1)\r\nOn trainer 0, iteration 1 ps it reaches 0.319800 accuracy <After the first training step, the accuracy is the same, which is expected.>\r\nWorker 0: training step 2 done (global step: 2)\r\nOn trainer 0, iteration 2 ps it reaches 0.279100 accuracy <Something different happening.>\r\nWorker 0: training step 3 done (global step: 3)\r\nOn trainer 0, iteration 3 ps it reaches 0.427200 accuracy\r\nWorker 0: training step 4 done (global step: 4)\r\nOn trainer 0, iteration 4 ps it reaches 0.567100 accuracy\r\nWorker 0: training step 5 done (global step: 5)\r\nOn trainer 0, iteration 5 ps it reaches 0.617500 accuracy\r\nWorker 0: training step 6 done (global step: 6)\r\nOn trainer 0, iteration 6 ps it reaches 0.561400 accuracy\r\n```\r\n\r\nI read through the source code of SyncReplicaOptimizer and find out that the train_op returned by that optimizer is a Assign operation, which could be only executed after the grads were applied and global steps enqueued. So sync and async with only one process should be exactly the same.\r\n\r\nThis behavior is mysterious to me now. Not sure if I got anything wrong.\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Please correct me if I were wrong, but I think this problem belongs to the bug category, since this behavior is unexpected."]}, {"number": 13891, "title": "Add int64 axis support for reduction ops.", "body": "This fix is a follow up to PR #13863. In PR #13863 the program crash is fixed if int64 axis is passed to reduction ops,e.g. reduce_sum, reduce_max, etc. However, it does not process the case of int64 axis support, it merely fixes the crash.\r\n\r\nThis fix adds the support for int64 axis of reduction ops: reduce_sum, reduce_prod, reduce_mean, reduce_max, reduce_min, reduce_all reduce_any.\r\n\r\nTest cases have been added as well.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "Unrelated failures (that are fixed at HEAD, I'm not sure why they are showing up here again)."]}, {"number": 13890, "title": "tf.image.crop_and_resize() return 0 values when assigned to GPU on  the Jetson TX2 ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04.LTS\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**:  2.7.12\r\n- **Bazel version (if compiling from source)**: 0.5.2\r\n- **CUDA/cuDNN version**: 8.0/6.0.21\r\n- **GPU model and memory**: Nvidia Tegra X2\r\n- **Exact command to reproduce**:`tf.image.crop_and_resize(raw_sample, boxes, box_ind)`\r\n\r\n### Describe the problem\r\nI'm getting completly different results from tensorflow's function `tf.image.crop_and_resize(...)` when assigned it to gpu and cpu.\r\nIn other words:\r\n  -when I run this ops on CPU, I get correct results( I mean, the  right crops)\r\n  -when I put it on the GPU device I get crops fulled with 0 values.\r\n\r\n### Source code / logs\r\nHere, you can see a simple use case:\r\n```\r\nimport tensorflow as tf \r\nimport numpy as np\r\nimport cv2 #Just importing cv2 to read  image, you use PIL or anything else to load it\r\n\r\ndevice='gpu' \r\n\r\ndef img2batch_crops(input_image):\r\n    raw_sample_tensor_4d=tf.expand_dims(input_image, 0)\r\n    \r\n    #Setting the size to crop and the final size of cropped images\r\n    patches_top=[0,0.5]\r\n    patches_bottom =[0.5,0.5]\r\n    crop_size = [100,100]\r\n    boxes=tf.stack([patches_top, patches_top, patches_bottom, patches_bottom], axis=1)\r\n    \r\n    ##Here is the bug:\r\n        #When device == 'cpu', I got  results \r\n        #When device == 'gpu', I got  black cropped images( 0 values)\r\n    with tf.device('/'+device+':0'):  \r\n        crops=tf.image.crop_and_resize(raw_sample_tensor_4d, boxes, box_ind=tf.zeros_like(patches_top, dtype=tf.int32), crop_size=crop_size, name=\"croper\")\r\n\r\n    return crops\r\n\r\n\r\ndef main():\r\n\r\n\timg_data = cv2.imread('image.jpg') #Just loading the image,\r\n\r\n\tprint(\"Shape and type of image input \",img_data.shape, img_data.dtype) #Print the shape and the type of the image, supposed to be a numpy array\r\n\r\n\traw_image = tf.placeholder(dtype=tf.float32, shape=img_data.shape, name='input_image')\r\n     \r\n       crops = img2batch_crops(raw_image) # Adding ops to the graph\r\n\r\n\twith tf.Session() as sess:\r\n\t    myBatchedImages = sess.run(crops, feed_dict={raw_image:img_data})\r\n\t    cv2.imwrite('result_'+device+'.jpg',myBatchedImages[0])   ## Savej just one cropped image to see how it looks like\r\n\r\nmain()\r\n```\r\n", "comments": ["@gpapan can you take a look or redirect? Thanks!", "@mingxingtan can you please take a look?", "@mingxingtan can you please take a look?\r\n\r\n", "I am not able to reproduce the issue on my local Volta GPU.", "@0fficer1 can you please sync to the latest version of tensorflow to see the issue still exists?\r\nThanks", "I will try and keep you in touch.\r\n\r\nOn Apr 21, 2018, at 7:16 PM, Yanping Huang <notifications@github.com<mailto:notifications@github.com>> wrote:\r\n\r\n\r\n@0fficer1<https://github.com/0fficer1> can you please sync to the latest version of tensorflow to see the issue still exists?\r\nThanks\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/13890#issuecomment-383335855>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AIRlysWltNnc65zS5YNsPUlaTDMELVJEks5tq6-9gaJpZM4QBuSt>.\r\n\r\nL'information contenue dans ce courriel (y compris les pi\u00e8ces jointes) est confidentielle et vise uniquement son destinataire ou ses destinataires. Toute autre distribution, copie ou divulgation est interdite. Si vous avez re\u00e7u ce courriel par erreur, veuillez nous en aviser et \u00e9liminer ce courriel, ainsi que les pi\u00e8ces jointes, de votre syst\u00e8me informatique et de vos dossiers.\r\n\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@jhseu Do you know who to ask about this? ", "angersson@, I was not able to reproduce this issue on my local Titan X or Titan V. The given example ran well on my local GPUs.\r\n\r\nIf anyone can reproduce the error, I would be happy to investigate the issue.  Otherwise, I believe you can safely close the issue.", "@mingxingtan I assume you mean you weren't able to reproduce the issue? Closing.", "I can reproduce this issue on TX2,with tensorflow 1.6.0, \r\n\r\n@mingxingtan This op will return *correct* value on desktop/tesla GPU, but it will return wrong value on *TX2* 's GPU. \r\n\r\n", "I also got the same issues with tensorflow 1.8.0 on jetson TX2 when using GPU, however, the op will work correct when using `with tf.device('/cpu:0'):`"]}, {"number": 13889, "title": "tf.image_crop_and_resize return 0 values when using GPU on Jetson TX2", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["same here", "Did  you get the same error ?\r\n\r\nGet Outlook for iOS<https://aka.ms/o0ukef>\r\n________________________________\r\nFrom: Lzane \u674e\u6fa4\u5e06 <notifications@github.com>\r\nSent: Tuesday, March 20, 2018 5:42:28 AM\r\nTo: tensorflow/tensorflow\r\nCc: Axel-Christian Gue\u00ef; State change\r\nSubject: Re: [tensorflow/tensorflow] tf.image_crop_and_resize return 0 values when using GPU on Jetson TX2 (#13889)\r\n\r\n\r\nsame here\r\n\r\n\u2014\r\nYou are receiving this because you modified the open/close state.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/13889#issuecomment-374516423>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AIRlyuBKgU0iZzwUTJnuK2z6_6my_tApks5tgMDzgaJpZM4QBtRg>.\r\n\r\nL'information contenue dans ce courriel (y compris les pi\u00e8ces jointes) est confidentielle et vise uniquement son destinataire ou ses destinataires. Toute autre distribution, copie ou divulgation est interdite. Si vous avez re\u00e7u ce courriel par erreur, veuillez nous en aviser et \u00e9liminer ce courriel, ainsi que les pi\u00e8ces jointes, de votre syst\u00e8me informatique et de vos dossiers.\r\n\r\n", "@0fficer1 Yes\r\nWhen I run `tf.image_crop_and_resize` on TX2, it return 0 or something wired like 3.9e-4. \r\nI have tried version 1.3, 1.4, 1.6 on my TX2 and all failed.\r\n\r\nI finally use `tf.image.crop_to_bounding_box` and `tf.image.resize_images` as an alternative. But now I have to handle the padding by my self.\r\n\r\nDo you have better solutions?", "Definitely not. In my case, I had to make a for loop to iterate over my boxes to do the same thing as you did.\r\n\r\nGet Outlook for iOS<https://aka.ms/o0ukef>\r\n________________________________\r\nFrom: Lzane \u674e\u6fa4\u5e06 <notifications@github.com>\r\nSent: Wednesday, March 21, 2018 12:07:51 AM\r\nTo: tensorflow/tensorflow\r\nCc: Axel-Christian Gue\u00ef; Mention\r\nSubject: Re: [tensorflow/tensorflow] tf.image_crop_and_resize return 0 values when using GPU on Jetson TX2 (#13889)\r\n\r\n\r\n@0fficer1<https://github.com/0fficer1> Yes\r\nWhen I run tf.image_crop_and_resize on TX2, it return 0 or something wired like 3.9e-4.\r\nI have tried version 1.3, 1.4, 1.6 on my TX2 and all failed.\r\n\r\nI finally use tf.image.crop_to_bounding_box and tf.image.resize_images as an alternative. But now I have to handle the padding by my self.\r\n\r\nDo you have better solutions?\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/13889#issuecomment-374822338>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AIRlyhDEJ56H6sDUdw8vbIl14HC9OE77ks5tgcQHgaJpZM4QBtRg>.\r\n\r\nL'information contenue dans ce courriel (y compris les pi\u00e8ces jointes) est confidentielle et vise uniquement son destinataire ou ses destinataires. Toute autre distribution, copie ou divulgation est interdite. Si vous avez re\u00e7u ce courriel par erreur, veuillez nous en aviser et \u00e9liminer ce courriel, ainsi que les pi\u00e8ces jointes, de votre syst\u00e8me informatique et de vos dossiers.\r\n\r\n", "I wonder whether this will do harm to the performance, or just more complicated code?", "Have you tried to convert to tensorflow model into tensorrt?  ", "I didn\u2019t benchmarked it either test it on TensorRt.\r\n\r\nGet Outlook for iOS<https://aka.ms/o0ukef>\r\n________________________________\r\nFrom: Lzane \u674e\u6fa4\u5e06 <notifications@github.com>\r\nSent: Wednesday, March 21, 2018 2:23:33 AM\r\nTo: tensorflow/tensorflow\r\nCc: Axel-Christian Gue\u00ef; Mention\r\nSubject: Re: [tensorflow/tensorflow] tf.image_crop_and_resize return 0 values when using GPU on Jetson TX2 (#13889)\r\n\r\n\r\nHave you tried to convert to tensorflow model into tensorrt?\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/13889#issuecomment-374836448>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AIRlyngI5DHXxJwknmnMhSIgQqJl5i5Hks5tgePVgaJpZM4QBtRg>.\r\n\r\nL'information contenue dans ce courriel (y compris les pi\u00e8ces jointes) est confidentielle et vise uniquement son destinataire ou ses destinataires. Toute autre distribution, copie ou divulgation est interdite. Si vous avez re\u00e7u ce courriel par erreur, veuillez nous en aviser et \u00e9liminer ce courriel, ainsi que les pi\u00e8ces jointes, de votre syst\u00e8me informatique et de vos dossiers.\r\n\r\n"]}, {"number": 13888, "title": "InvalidArgumentError: No OpKernel was registered to support Op 'Resampler' with these attrs. ", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: Python 3.6.2 \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: CUDA 8/ CUDNN 6\r\n- **GPU model and memory**: GeForce 940MX\r\n- **Exact command to reproduce**: tf.contrib.resampler.resampler(inp,warp)\r\n\r\n### Describe the problem\r\nNo registered kernels for the resampler operation. The code is as follows\r\n`import tensorflow as tf`\r\n`inp = tf.ones([1,4,4,3],dtype=tf.float32)`\r\n`warp = tf.zeros([1,4,4,2],dtype=tf.float32)`\r\n`out = tf.contrib.resampler.resampler(inp,warp)`\r\n`print(out)`\r\n`sess = tf.Session()`\r\n`print(sess.run(out))`\r\n\r\n### Source code / logs\r\nI get the following output and error, traceback\r\n\r\nTensor(\"resampler_1/Resampler:0\", shape=(1, 4, 4, 3), dtype=float32)\r\n\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\nF:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_call(self, fn, *args)\r\n   1326     try:\r\n-> 1327       return fn(*args)\r\n   1328     except errors.OpError as e:\r\n\r\nF:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1296       # Ensure any changes to the graph are reflected in the runtime.\r\n-> 1297       self._extend_graph()\r\n   1298       with errors.raise_exception_on_not_ok_status() as status:\r\n\r\nF:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _extend_graph(self)\r\n   1357           tf_session.TF_ExtendGraph(\r\n-> 1358               self._session, graph_def.SerializeToString(), status)\r\n   1359         self._opened = True\r\n\r\nF:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\contextlib.py in __exit__(self, type, value, traceback)\r\n     87             try:\r\n---> 88                 next(self.gen)\r\n     89             except StopIteration:\r\n\r\nF:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py in raise_exception_on_not_ok_status()\r\n    465           compat.as_text(pywrap_tensorflow.TF_Message(status)),\r\n--> 466           pywrap_tensorflow.TF_GetCode(status))\r\n    467   finally:\r\n\r\nInvalidArgumentError: No OpKernel was registered to support Op 'Resampler' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:\r\n &lt;no registered kernels&gt;\r\n\r\n\t [[Node: resampler_1/Resampler = Resampler[T=DT_FLOAT](ones_2, zeros_2)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-3-0ff9594126bf> in <module>()\r\n      5 print(out)\r\n      6 sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n----> 7 print(sess.run(out))\r\n\r\nF:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    893     try:\r\n    894       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 895                          run_metadata_ptr)\r\n    896       if run_metadata:\r\n    897         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\nF:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1122     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1123       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1124                              feed_dict_tensor, options, run_metadata)\r\n   1125     else:\r\n   1126       results = []\r\n\r\nF:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1319     if handle is None:\r\n   1320       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\r\n-> 1321                            options, run_metadata)\r\n   1322     else:\r\n   1323       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)\r\n\r\nF:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_call(self, fn, *args)\r\n   1338         except KeyError:\r\n   1339           pass\r\n-> 1340       raise type(e)(node_def, op, message)\r\n   1341 \r\n   1342   def _extend_graph(self):\r\n\r\nInvalidArgumentError: No OpKernel was registered to support Op 'Resampler' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:\r\n &lt;no registered kernels&gt;\r\n\r\n\t [[Node: resampler_1/Resampler = Resampler[T=DT_FLOAT](ones_2, zeros_2)]]\r\n\r\nCaused by op 'resampler_1/Resampler', defined at:\r\n  File \"F:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"F:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"F:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"F:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"F:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\r\n    ioloop.IOLoop.instance().start()\r\n  File \"F:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\r\n    super(ZMQIOLoop, self).start()\r\n  File \"F:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\r\n    handler_func(fd_obj, events)\r\n  File \"F:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"F:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\r\n    self._handle_recv()\r\n  File \"F:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"F:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"F:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"F:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"F:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"F:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"F:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"F:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"F:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"F:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"F:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-3-0ff9594126bf>\", line 4, in <module>\r\n    out = tf.contrib.resampler.resampler(inp,warp)\r\n  File \"F:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\resampler\\python\\ops\\resampler_ops.py\", line 59, in resampler\r\n    return gen_resampler_ops.resampler(data_tensor, warp_tensor)\r\n  File \"F:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\resampler\\ops\\gen_resampler_ops.py\", line 28, in resampler\r\n    result = _op_def_lib.apply_op(\"Resampler\", data=data, warp=warp, name=name)\r\n  File \"F:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"F:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2630, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"F:\\Sharath\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'Resampler' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:\r\n  &lt;no registered kernels&gt;\r\n\r\n\t [[Node: resampler_1/Resampler = Resampler[T=DT_FLOAT](ones_2, zeros_2)]]\r\n", "comments": ["I cannot reproduce this error. Maybe this is related to the specific platform and setup of the user. ", "@Sharath-girish do you see this error if you disable GPU? (You can disable your GPU by setting the env variable CUDA_VISIBLE_DEVICES='' and rerunning your script)", "@liyzjj I've also tried this on python 3.5 but got the same error.\r\n@skye  I disabled gpu by setting the variable to -1\r\n`os.environ[\"CUDA_VISIBLE_DEVICES\"]='-1'`\r\n\r\nAlternatively, I also tried setting tf device to '/cpu:0'\r\nI still get the error as\r\nInvalidArgumentError: No OpKernel was registered to support Op 'Resampler' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  &lt;no registered kernels&gt;\r\n\r\n\t [[Node: resampler/Resampler = Resampler[T=DT_FLOAT](ones, zeros)]]", "Hm, I'm not able to repro either. I suspect the problem has something to do with the Windows binary... @gunan do you have a way to try to repro this? @Sharath-girish you could try building from source: https://github.com/tensorflow/tensorflow/tree/r0.12/tensorflow/contrib/cmake", "It looks like resampler op kernels are excluded from the windows cmake build:\r\nhttps://github.com/tensorflow/tensorflow/commit/a80c8b583fa3d619b358a91aefd05069227b8967\r\nhttps://github.com/tensorflow/tensorflow/blame/r1.4/tensorflow/contrib/cmake/tf_core_kernels.cmake#L160\r\n\r\nAlso, resampler ops cuda kernels are not included in GPU kernels that need to be built:\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/contrib/cmake/tf_core_kernels.cmake#L181\r\n\r\nYou can try editing the lines I linked above and rebuilding from sources, but the build may fail. It is contrib, so my knowledge about why the build failed in June and this was excluded ends here.", "I got a similar error when I run c++ Tensorflow using GPU (using VS2015). I have successfully build Tensorflow for CPU and GPU using cmake. I can successfully load and run the graph previously saved from my python model in Tensorflow 1.4 using CPU c++ with no problem; but when I tried to load the same graph using Tensorflow 1.4 using GPU c++ and I get the following error:\r\n`- status {state_=unique_ptr {code=INVALID_ARGUMENT (3) msg=\"No OpKernel was registered to support Op 'OneHot' with these attrs. Registered devices: [CPU,GPU], Registered kernels:\\n <no registered kernels>\\n\\n\\t [[Node: one_hot = OneHot[T=DT_FLOAT, TI=DT_INT32, _... } } tensorflow::Status`\r\n\r\nAny advice on this? \r\n\r\nMy system:\r\nOS Platform and Distribution = Windows 10 Pro 64-bit (build 15063)\r\nTensorFlow installed from = https://github.com/tensorflow/tensorflow/archive/v1.4.0.zip\r\nTensorFlow version = 1.4.0\r\nBazel version = N/A I am using cmake cmake-3.9.4-win64-x64\r\nCUDA/cuDNN version = Cuda 8.0/Cudnn 6\r\nGPU model and memory = NVIDIA Quadro M4000, VRAM 8152MB\r\nExact command to reproduce = N/A running it in visual studio 2015\r\n\r\n", "I got a similar error when I run Tensorflow using python3\r\n\r\nMy system:\r\nOS Platform and Distribution = Linux 16.04 64-bit\r\nTensorFlow installed from = ?\r\nTensorFlow version = 1.6.0\r\nBazel version = ?\r\nCUDA/cuDNN version = ??\r\nGPU model and memory = ?\r\nExact command to reproduce = can be found here https://github.com/antoinecomp/Davidshah", "Hi @Sharath-girish  @liyzjj  @skye  @gunan @cuevas1208  \r\nTensorflow=1.10.0\r\nOS platform =Windows 10\r\nCUDA/cuDNN version=9.0\r\n\r\nI get this error while testing DISN code.\r\nInvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'Resampler' with these attrs. Registered devices: [CPU], Registered kernels:\r\n\r\n[[Node: resampler/Resampler = Resampler[T=DT_FLOAT](ResizeBilinear_1, Minimum)]]\r\nPlease guide. Thanks!", "Looks like this is about a very very old version of TF, which is not supported any more.\r\nPlease use 1.15 or 2.x"]}, {"number": 13887, "title": "ValueError raised when using AdamOptimizer, but not for GradientDescentOptimizer", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS Sierra 10.12.1\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.2.0-rc2-21-g12f033d 1.2.0\r\n- **Python version**: 3.5.0\r\n\r\n### Source code / logs\r\n\r\nTaken from this issue: https://github.com/tensorflow/tensorflow/issues/6220#issuecomment-314688444, the below code reproduces the error.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nclass SimpleModel:\r\n    def __init__(self):\r\n        self.loss = self.calc_loss()\r\n        self.train = self.train_model(self.loss)\r\n    def calc_loss(self):\r\n        W = tf.get_variable(\"w\", [1])\r\n        b = tf.Variable(tf.zeros([1]))\r\n        y = W * x_data + b\r\n        return tf.reduce_mean(tf.square(y - y_data))\r\n    def train_model(self, loss):\r\n        return tf.train.AdamOptimizer(0.5).minimize(loss)\r\n        #return tf.train.GradientDescentOptimizer(0.5)\r\nx_data = np.random.rand(100).astype(np.float32)\r\ny_data = x_data * 0.1 + 0.3\r\ns1 = SimpleModel()\r\ntf.get_variable_scope().reuse_variables()\r\ns2 = SimpleModel()\r\n```\r\n\r\nRunning this gives the error `ValueError: Variable w/Adam_2/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?`, but when `train_model` is changed to return the `GradientDescentOptimizer`, the code compiles correctly.\r\n\r\nI've tried following all the advice in the issue that relates to this (https://github.com/tensorflow/tensorflow/issues/6220), but haven't had any luck. ", "comments": ["I am having the same issue. I use tensorflow 1.3 and my operating system is ubuntu 16.4.", "Did you look at this?\r\nhttps://stackoverflow.com/questions/44440900/cant-access-tensorflow-adam-optimizer-namespace", "Hi again!\r\nYour comment helped a lot. Also the follow up here:\r\nhttps://github.com/adeshpande3/Generative-Adversarial-Networks/issues/1\r\nsolves the problem.\r\nThank you.", "Woohoo! Thank you for following up and the link!\n\nOn Sat, Oct 21, 2017, 10:24 PM kazemSafari <notifications@github.com> wrote:\n\n> Hi again!\n> Your comment helped a lot. Also the follow up here:\n> adeshpande3/Generative-Adversarial-Networks#1\n> <https://github.com/adeshpande3/Generative-Adversarial-Networks/issues/1>\n> solves the problem.\n> Thank you.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13887#issuecomment-338452546>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbYycMQGrEqy8q_D5EJp5zI4Hq3-0ks5sutGigaJpZM4QBqub>\n> .\n>\n", "So from what I understand, this issue seems to come from the fact that we can't actually set reuse = False for a variable scope? I was able to fix the problem by introducing a sub-scope like the above comments mentioned, but overall I'm still a bit confused about the underlying issue. The traceback given here: \r\n`ValueError: Variable w/Adam_2/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?`\r\n\r\nSeems to imply that we can set the reuse flag to false/none, but the documentation [here](https://www.tensorflow.org/versions/r0.12/how_tos/variable_scope/#variable_scope_example) says \"note that you cannot set the reuse flag to false\". I may be interpreting this incorrectly, but could anything be done about maybe a better log message in this case? Thanks!", "I believe the documentation is saying you can only change the reuse flag from false to true inside a scope (by calling `scope.reuse_variables()`); but you can't change the reuse flag from true to false inside a scope. The log message I think means 'did you mean be in a non-reusing VarScope', and I agree that it's not super clear.\r\n\r\n@martinwicke do you think it's worth changing the message?", "@lukaszkaiser @ebrevdo WDYT? \r\n\r\nI'd favor changing the message, but want to be sure I'm interpreting the situation correctly. ", "It should certainly say \"reuse=False\" not \"reuse=None\" (that's been changed a long time ago). Nowadays (starting with TF 1.4) you can actually set \"reuse=tf.AUTO_REUSE\" and the error will go away in all circumstances (changing True/False can be tricky). Maybe it's worth mentioning that in the message and suggesting AUTO_REUSE?", "(As for setting reuse=False, you can do it but it'll be overridden if your parent scope has it set to True.)", "@lukaszkaiser @martinwicke I'd be happy to send in a PR to change the `reuse=None` message if we've decided to do that as my first contribution to Tensorflow!", "Please do, thanks!", "@martinwicke https://github.com/tensorflow/tensorflow/pull/14015 is merged, so I'm closing this for now. Thanks for your help!"]}, {"number": 13886, "title": "Give each variable a unique name in accumulate_n_v2_eager_test.", "body": "", "comments": ["@tensorflow-jenkins test this please"]}, {"number": 13885, "title": "tf.reduce_mean is not compatible with np.mean", "body": "[tf.reduce_mean](https://www.tensorflow.org/api_docs/python/tf/reduce_mean) emphasized that this function is compatible with numpy:\r\n\r\n> Equivalent to np.mean\r\n\r\nBut it doesn't in the output type. Consider the following code for example:\r\n\r\n```\r\nimport tensorflow as tf\r\nx = tf.Variable([1, 0, 1, 0])\r\ninit = tf.global_variables_initializer()\r\nsess = tf.Session()\r\nsess.run(init)\r\nprint(sess.run(tf.reduce_mean(x)))\r\n\r\n```\r\n\r\nThe output is zero. It seems that tf.reduce_mean infer the output type from the input tensor because casting the input tensor to float values, solve the problem. This attribute is not compatible to np.mean:\r\n\r\n```\r\nimport numpy as np\r\nprint(np.mean([1,0,0,1]))\r\n```\r\n\r\n\r\n### System information\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.3\r\n- **Python version**: 3.6\r\n", "comments": ["In numpy, `numpy.mean` has a `dtype` that could be used to specify the output type:\r\nhttps://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.mean.html\r\n\r\n By default this is `dtype=float64` for integer types and same type as input for non-integer types.\r\n\r\nIt might be possible to add a dtype in tf.reduce_mean though a type cast is always needed I assume.", "@MarkDaoust should we update the numpy compat docstring? Or encourage a contribution?\r\n\r\n/CC @aselle ", "@drpngx @MarkDaoust  _dtype_ is an optional parameter in both TF and Numpy. I think it should be clear in the document or adapt to Numpy. It's my honer to contribute in both cases.", "Sure, if you want to update the `@compatability(numpy)` note we'd welcome the PR.\r\n\r\nBut I don't think this is enough of a bug that we can break backwards compatibility to fix it. ", "@drpngx I'd like to send a PR to add the `dtype` argument for `reduce_mean` funciton.", "@DjangoPeng sounds good, it'll have to go to api review. Thank you!", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Fixed by #13941"]}, {"number": 13884, "title": "Add `int64` type `multiples` support for `tf.tile`", "body": "In the doc of `tf.tile` (tf.tile.__doc__) both `int32` and `int64` are supported for `multiples`. However, the kernel for `int64` is not registered yet.\r\n\r\nThis fix adds the support of `int64` `multiples` so that the behavior matches the description of the docs.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Thanks @vrv for the review. The PR has been updated. Please take a look.", "@tensorflow-jenkins test this please"]}, {"number": 13883, "title": "Why do custom read op only works on test_session", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: \r\nNO\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\npip install tensorflow\r\n\r\n- **TensorFlow version (use command below)**:\r\n('v1.3.0-rc2-20-g0787eee', '1.3.0')\r\n\r\n- **Python version**: \r\nPython 2.7.12 from anaconda\r\n\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\nI'm using a cpu only tensorflow.\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI wrote an custom kernel op in tensorflow for reading csv format data.\r\n\r\nIt works just fine in the TestCase with the\u00a0sess\u00a0object return by\u00a0test_session()\u00a0function.\r\n\r\nWhen I turn to normal codes, the reader op returns the same result every time. Then I put some debug printing at the beginning of the\u00a0MyOp:Compute\u00a0function. It seems like after the first run, the\u00a0sess.run(myop)\u00a0never calls the\u00a0MyOp:Compute\u00a0function at all.\r\n\r\nThen I return to my test cases, if I replace the session object with an\u00a0tf.Session()\u00a0instead of\u00a0self.test_session(), it failed the same way.\r\n\r\n### Source code / logs\r\nto share more details, here's my mini demo codes: https://github.com/littleDing/mini_csv_reader\r\n\r\nmajor codes in test cases\r\n```\r\ndef testSimple(self):\r\n  input_data_schema, feas, batch_size = self.get_simple_format()\r\n  iter_op = ops.csv_iter('./sample_data.txt', input_data_schema, feas, batch_size=batch_size, label='label2')\r\n  with self.test_session() as sess:\r\n    label,sign = sess.run(iter_op)\r\n    print label\r\n\r\n    self.assertAllEqual(label.shape, [batch_size])\r\n    self.assertAllEqual(sign.shape, [batch_size, len(feas)])\r\n    self.assertAllEqual(sum(label), 2)\r\n    self.assertAllEqual(sign[0,:], [7,0,4,1,1,1,5,9,8])\r\n\r\n    label,sign = sess.run(iter_op)\r\n    self.assertAllEqual(label.shape, [batch_size])\r\n    self.assertAllEqual(sign.shape, [batch_size, len(feas)])\r\n    self.assertAllEqual(sum(label), 1)\r\n    self.assertAllEqual(sign[0,:], [9,9,3,1,1,1,5,4,8])\r\n```\r\nmajor codes in normal session call:\r\n```\r\ndef testing_tf():\r\n    path = './sample_data.txt'\r\n    input_data_schema, feas, batch_size = get_simple_format()\r\n    with tf.device('/cpu:0'):\r\n        n_data_op = tf.placeholder(dtype=tf.float32)\r\n        iter_op = ops.csv_iter(path, input_data_schema, feas, batch_size=batch_size, label='label2') \r\n        init_op = [tf.global_variables_initializer(), tf.local_variables_initializer() ]\r\n\r\n    with tf.Session() as sess:\r\n      sess.run(init_op)\r\n      n_data = 0\r\n      for batch_idx in range(3):\r\n        print '>>>>>>>>>>>>>> before run batch', batch_idx\r\n        ## it should be some debug printing here, but nothing come out when batch_idx>0\r\n        label,sign = sess.run(iter_op)\r\n        print '>>>>>>>>>>>>>> after run batch', batch_idx\r\n        ## the content of sign remain the same every time\r\n        print sign\r\n        if len(label) == 0:\r\n          break\r\n```\r\n\r\n", "comments": ["Can I encourage you to use the `tf.data` interface and the [built-in](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/decode_csv_op.cc#L29) CSV op?", "The build-in interfaces(both tf.data and csv op) are too slow for large scale data. \r\nEarly tests (at about 2016-11 with tensorflow 1.0 or earlier version) shows that use cython + pandas + feeddict to load csv data would be 2~5x speed up than the build-in ops.\r\n\r\nA problem using pandas + feeddict is that it blocks with the python GIL and python codes is still slow, thus the speed up would NOT scale when number of threads is over 5.\r\n\r\nSo I'm wondering if it is possible to translate the codes into pure cpp so that it could benefit from the tf.queue ops.", "Cc @mrry\n\nI'm surprised that the data interface would be slow. Can you publish how\nyou treated that?\n\nOn Sat, Oct 21, 2017, 7:15 PM littleDing <notifications@github.com> wrote:\n\n> The build-in interfaces(both tf.data and csv op) are to slow for large\n> scale data.\n> Early tests (at about 2016-11 with tensorflow 1.0 or earlier version)\n> shows that use cython + pandas + feeddict to load csv data would be 2~5x\n> speed up than the build-in ops.\n>\n> A problem using pandas + feeddict is that it blocks with the python GIL\n> and python codes is still slow, thus the speed up would scale when number\n> of threads is over 5.\n>\n> So I'm wondering if it is possible to translate the codes into pure cpp so\n> that it could benefit from the tf.queue ops.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13883#issuecomment-338445604>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbQEklezj6kjSp60LRQ7WTEDwWywjks5suqVDgaJpZM4QBpDI>\n> .\n>\n", "Are you feeding example by example?\n\nOn Sat, Oct 21, 2017, 7:17 PM Patrick Nguyen <drpng@google.com> wrote:\n\n> Cc @mrry\n>\n> I'm surprised that the data interface would be slow. Can you publish how\n> you treated that?\n>\n> On Sat, Oct 21, 2017, 7:15 PM littleDing <notifications@github.com> wrote:\n>\n>> The build-in interfaces(both tf.data and csv op) are to slow for large\n>> scale data.\n>> Early tests (at about 2016-11 with tensorflow 1.0 or earlier version)\n>> shows that use cython + pandas + feeddict to load csv data would be 2~5x\n>> speed up than the build-in ops.\n>>\n>> A problem using pandas + feeddict is that it blocks with the python GIL\n>> and python codes is still slow, thus the speed up would scale when number\n>> of threads is over 5.\n>>\n>> So I'm wondering if it is possible to translate the codes into pure cpp\n>> so that it could benefit from the tf.queue ops.\n>>\n>> \u2014\n>> You are receiving this because you were assigned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/13883#issuecomment-338445604>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/AT_SbQEklezj6kjSp60LRQ7WTEDwWywjks5suqVDgaJpZM4QBpDI>\n>> .\n>>\n>\n", "Wow~ Thanks for replying so fast :-)\r\n\r\nWell, I am not sure if I am using the build-in ops in the right way. I just follow the documents on the official website. Example by example, or using a multi-thread queue with batch. The pb interfaces was also tested then, but it seems like converting raw data to pb format is also a great cost.\r\n\r\nIs there any official speed benchmarks for this data reading ops? I could reproduce my own again and check if the speed up still exists.", "I answered the question on Stack Overflow: https://stackoverflow.com/a/46892223/3574081\r\n\r\nPlease open a new issue if you see performance problems with the `tf.data` ops (note that they were first added in May 2017 in TensorFlow 1.2, so you might want to update your experiments). "]}, {"number": 13882, "title": "Branch 172965466", "body": "", "comments": ["@tensorflow-jenkins test this please", "@gunan any ideas what's going on with some of these failures?", "@tensorflow-jenkins test this please", "the jenkins failures mostly seem to be due to some issues with APT during the setup phase of our dockerfiles, maybe something in ubuntu repositories changed?  I am puzzled.\r\nIt looks like ubuntu 14.10 is still not in EOL status, either.\r\nSo I am not sure what is going on with the breakages.", "@tensorflow-jenkins test this please", "I just tested https://github.com/tensorflow/tensorflow/pull/13893 and it came back clean, so if this fails once more, it's something in this push that is causing the failures, which means i probably can't push until we figure it out"]}, {"number": 13881, "title": "Fix doc in TF_CALL_ when invoked in mobile platform", "body": "This is a small doc fix that includes bool as part of the types that is supported in mobile, as bool is clearly invoked in the following define (See Ln 105 and Ln 135) in mobile platform:\r\n```cpp\r\n#define TF_CALL_bool(m) m(bool)\r\n```\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?"]}, {"number": 13880, "title": "RMSProp fails with InteractiveSession() and embed_sequence on GPU", "body": "Note:\r\nThis works fine if any of the following\r\n- without GPU\r\n- if use \"normal\" Session() instead of the interactive one.\r\n- if don't do `embed_sequence`\r\n- user other optimiser, not RMSProp\r\n\r\nThis is the snippet to get the error:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import layers, framework\r\nimport numpy as np\r\n\r\nsess = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=True))\r\ninput = tf.placeholder(tf.int64, shape=[None])\r\noptimiser = tf.train.RMSPropOptimizer(0.1)\r\nx = layers.embed_sequence(input, vocab_size=20, embed_dim=5)\r\nloss = tf.reduce_sum(x)\r\ntrain_op = optimiser.minimize(loss)\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(train_op, feed_dict={input: np.random.randint(10, size=5)})\r\n```\r\n\r\nGives\r\n\r\n```\r\n2017-10-21 15:12:38.503957: E tensorflow/core/framework/op_segment.cc:53] Create kernel failed: Invalid argument: AttrValue must not have reference type value of float_ref\r\n   for attr 'tensor_type'\r\n  ; NodeDef: EmbedSequence/embeddings/RMSProp_1/_19 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_36_EmbedSequence/embeddings/RMSProp_1\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](^RMSProp/learning_rate, ^RMSProp/decay, ^RMSProp/momentum, ^RMSProp/epsilon, ^RMSProp/update_EmbedSequence/embeddings/UnsortedSegmentSum, ^RMSProp/update_EmbedSequence/embeddings/Unique); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n2017-10-21 15:12:38.504081: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: AttrValue must not have reference type value of float_ref\r\n   for attr 'tensor_type'\r\n  ; NodeDef: EmbedSequence/embeddings/RMSProp_1/_19 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_36_EmbedSequence/embeddings/RMSProp_1\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](^RMSProp/learning_rate, ^RMSProp/decay, ^RMSProp/momentum, ^RMSProp/epsilon, ^RMSProp/update_EmbedSequence/embeddings/UnsortedSegmentSum, ^RMSProp/update_EmbedSequence/embeddings/Unique); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n   [[Node: EmbedSequence/embeddings/RMSProp_1/_19 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_36_EmbedSequence/embeddings/RMSProp_1\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](^RMSProp/learning_rate, ^RMSProp/decay, ^RMSProp/momentum, ^RMSProp/epsilon, ^RMSProp/update_EmbedSequence/embeddings/UnsortedSegmentSum, ^RMSProp/update_EmbedSequence/embeddings/Unique)]]\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1327, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1306, in _run_fn\r\n    status, run_metadata)\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: AttrValue must not have reference type value of float_ref\r\n   for attr 'tensor_type'\r\n  ; NodeDef: EmbedSequence/embeddings/RMSProp_1/_19 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_36_EmbedSequence/embeddings/RMSProp_1\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](^RMSProp/learning_rate, ^RMSProp/decay, ^RMSProp/momentum, ^RMSProp/epsilon, ^RMSProp/update_EmbedSequence/embeddings/UnsortedSegmentSum, ^RMSProp/update_EmbedSequence/embeddings/Unique); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n   [[Node: EmbedSequence/embeddings/RMSProp_1/_19 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_36_EmbedSequence/embeddings/RMSProp_1\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](^RMSProp/learning_rate, ^RMSProp/decay, ^RMSProp/momentum, ^RMSProp/epsilon, ^RMSProp/update_EmbedSequence/embeddings/UnsortedSegmentSum, ^RMSProp/update_EmbedSequence/embeddings/Unique)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"rmsprop_fail.py\", line 19, in <module>\r\n    sess.run(train_op, feed_dict={input: np.random.randint(10, size=5)})\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: AttrValue must not have reference type value of float_ref\r\n   for attr 'tensor_type'\r\n  ; NodeDef: EmbedSequence/embeddings/RMSProp_1/_19 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_36_EmbedSequence/embeddings/RMSProp_1\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](^RMSProp/learning_rate, ^RMSProp/decay, ^RMSProp/momentum, ^RMSProp/epsilon, ^RMSProp/update_EmbedSequence/embeddings/UnsortedSegmentSum, ^RMSProp/update_EmbedSequence/embeddings/Unique); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n   [[Node: EmbedSequence/embeddings/RMSProp_1/_19 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_36_EmbedSequence/embeddings/RMSProp_1\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](^RMSProp/learning_rate, ^RMSProp/decay, ^RMSProp/momentum, ^RMSProp/epsilon, ^RMSProp/update_EmbedSequence/embeddings/UnsortedSegmentSum, ^RMSProp/update_EmbedSequence/embeddings/Unique)]]\r\n```\r\n\r\nIf remove the `allow_soft_placement` then get:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1327, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1306, in _run_fn\r\n    status, run_metadata)\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'gradients/EmbedSequence/embedding_lookup_grad/ToInt32': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'\r\nColocation Debug Info:\r\nColocation group had the following types and devices:\r\nSparseApplyRMSProp: CPU\r\nUnsortedSegmentSum: GPU CPU\r\nGather: GPU CPU\r\nStridedSlice: GPU CPU\r\nUnique: GPU CPU\r\nShape: GPU CPU\r\nCast: GPU CPU\r\nIdentity: GPU CPU\r\nVariableV2: GPU CPU\r\nConst: GPU CPU\r\n\t [[Node: gradients/EmbedSequence/embedding_lookup_grad/ToInt32 = Cast[DstT=DT_INT32, SrcT=DT_INT64, _class=[\"loc:@EmbedSequence/embeddings\"]](gradients/EmbedSequence/embedding_lookup_grad/Shape)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"rmsprop_fail.py\", line 19, in <module>\r\n    sess.run(train_op, feed_dict={input: np.random.randint(10, size=5)})\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'gradients/EmbedSequence/embedding_lookup_grad/ToInt32': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'\r\nColocation Debug Info:\r\nColocation group had the following types and devices:\r\nSparseApplyRMSProp: CPU\r\nUnsortedSegmentSum: GPU CPU\r\nGather: GPU CPU\r\nStridedSlice: GPU CPU\r\nUnique: GPU CPU\r\nShape: GPU CPU\r\nCast: GPU CPU\r\nIdentity: GPU CPU\r\nVariableV2: GPU CPU\r\nConst: GPU CPU\r\n\t [[Node: gradients/EmbedSequence/embedding_lookup_grad/ToInt32 = Cast[DstT=DT_INT32, SrcT=DT_INT64, _class=[\"loc:@EmbedSequence/embeddings\"]](gradients/EmbedSequence/embedding_lookup_grad/Shape)]]\r\n\r\nCaused by op 'gradients/EmbedSequence/embedding_lookup_grad/ToInt32', defined at:\r\n  File \"rmsprop_fail.py\", line 15, in <module>\r\n    train_op = optimiser.minimize(loss)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 315, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 386, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 542, in gradients\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 348, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 542, in <lambda>\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_grad.py\", line 365, in _GatherGrad\r\n    params_shape = math_ops.to_int32(params_shape)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\", line 797, in to_int32\r\n    return cast(x, dtypes.int32, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\", line 716, in cast\r\n    return gen_math_ops.cast(x, base_type, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 450, in cast\r\n    result = _op_def_lib.apply_op(\"Cast\", x=x, DstT=DstT, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\n...which was originally created as op 'EmbedSequence/embedding_lookup', defined at:\r\n  File \"rmsprop_fail.py\", line 11, in <module>\r\n    x = layers.embed_sequence(input, vocab_size=20, embed_dim=5)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/encoders.py\", line 142, in embed_sequence\r\n    return embedding_ops.embedding_lookup(embeddings, ids)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/embedding_ops.py\", line 294, in embedding_lookup\r\n    transform_fn=None)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/embedding_ops.py\", line 123, in _embedding_lookup_and_transform\r\n    result = _gather_and_clip(params[0], ids, max_norm, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/embedding_ops.py\", line 57, in _gather_and_clip\r\n    embs = array_ops.gather(params, ids, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 2409, in gather\r\n    validate_indices=validate_indices, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 1219, in gather\r\n    validate_indices=validate_indices, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'gradients/EmbedSequence/embedding_lookup_grad/ToInt32': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'\r\nColocation Debug Info:\r\nColocation group had the following types and devices:\r\nSparseApplyRMSProp: CPU\r\nUnsortedSegmentSum: GPU CPU\r\nGather: GPU CPU\r\nStridedSlice: GPU CPU\r\nUnique: GPU CPU\r\nShape: GPU CPU\r\nCast: GPU CPU\r\nIdentity: GPU CPU\r\nVariableV2: GPU CPU\r\nConst: GPU CPU\r\n\t [[Node: gradients/EmbedSequence/embedding_lookup_grad/ToInt32 = Cast[DstT=DT_INT32, SrcT=DT_INT64, _class=[\"loc:@EmbedSequence/embeddings\"]](gradients/EmbedSequence/embedding_lookup_grad/Shape)]]\r\n```\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.3 LTS \r\n- **TensorFlow installed from (source or binary)**: docker\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 3.5\r\n- **CUDA/cuDNN version**:\r\nCuda compilation tools, release 8.0, V8.0.61\r\n- **GPU model and memory**:\r\nTesla K80\r\n\r\n", "comments": ["I was able to reproduce this, and am trying to debug it. As a workaround, your code should work if you use a Session instead of an InteractiveSession. Specifically, the session config place_pruned_graph = True causes the problem, but I'm not sure why yet.", "Ah, I see you already figured this out. Didn't read carefully enough :) I'll keep digging.", "Hey, Any update on this? I can't use RMSProp, AdaGrad or AdaDelta with an InteractiveSession. Only AdamOptimizer and GradientDescentOptimizer work. Tried both TF 1.3 and 1.4", "No further updates at this time, sorry. I had to work on other things, but I'll try to take another look at this sometime this week.", "I think I figured it out... under the hood, this code is creating colocated graph operations, including the variables. The problem is that when `place_pruned_graph = True` (which InteractiveSession sets), the variable initialization run() call places a Variable op on the GPU (since it doesn't see any of the pruned colocated ops). Then when you try to run the train_op, it \"realizes\" all the colocated ops must be run on CPU due to a missing GPU kernel, but it's too late since the Variable op was already placed on the GPU.\r\n\r\nUnfortunately there isn't a great fix for this... for now, I suggest either using a regular Session, or explicitly setting `place_pruned_graph=False` in your ConfigProto. Alternatively, you could try out Eager instead of using a session at all :)\r\n\r\nI'll keep this issue open, but unfortunately I don't think there will be an actual fix anytime soon.", "Is there any progress about this issue? ", "No, as I stated above, there aren't any plans to address this. Please see the rest of the thread for workarounds.", "Actually, I saw the placement code, and I got some idea that if node A placed other device compare to node B which is in the colocate attribute value of node A. When I fixed the code to locate in same place with colocate node, it seems work. However It would be not a good solution. ", "If I run the example code I get the following:\r\n\r\n```\r\nInvalidArgumentError: AttrValue must not have reference type value of float_ref\r\n\t for attr 'tensor_type'\r\n\t; NodeDef: EmbedSequence/embeddings/RMSProp_1/_19 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_36_EmbedSequence/embeddings/RMSProp_1\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^RMSProp/learning_rate, ^RMSProp/decay, ^RMSProp/momentum, ^RMSProp/epsilon, ^RMSProp/update_EmbedSequence/embeddings/UnsortedSegmentSum, ^RMSProp/update_EmbedSequence/embeddings/Unique); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n\t [[Node: EmbedSequence/embeddings/RMSProp_1/_19 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_36_EmbedSequence/embeddings/RMSProp_1\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^RMSProp/learning_rate, ^RMSProp/decay, ^RMSProp/momentum, ^RMSProp/epsilon, ^RMSProp/update_EmbedSequence/embeddings/UnsortedSegmentSum, ^RMSProp/update_EmbedSequence/embeddings/Unique)]]\r\n```\r\n\r\nThis is with CUDA 8.0, TF 1.4 on windows with a Titan X card.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "This issue is very much still present, any reason it was closed?", "I closed it since we now get nagged every two weeks if there's no activity, and there are no current plans to fix this. Please feel free to continue the discussion or comment if you also experience this problem, and we can reopen if/when we have a plan for actually working on this."]}, {"number": 13879, "title": "Variable rnn/multi_rnn_cell/cell_0/gru_cell/gates/kernel already exists, disallowed.", "body": "Hello,\r\nI wrote this code:\r\n\r\ndef rnn_inputs(FLAGS, input_data):\r\n\r\n    with tf.variable_scope('rnn_inputs', reuse=True):\r\n        W_input = tf.get_variable(\"W_input\",\r\n            [FLAGS.en_vocab_size, FLAGS.num_hidden_units])\r\n\r\n    # <num_examples, seq_len, num_hidden_units>\r\n    embeddings = tf.nn.embedding_lookup(W_input, input_data)\r\n\r\n    return embeddings\r\n\r\ndef rnn_softmax(FLAGS, outputs):\r\n    with tf.variable_scope('rnn_softmax', reuse=True):\r\n        W_softmax = tf.get_variable(\"W_softmax\",\r\n            [FLAGS.num_hidden_units, FLAGS.num_classes])\r\n        b_softmax = tf.get_variable(\"b_softmax\", [FLAGS.num_classes])\r\n\r\n    logits = tf.matmul(outputs, W_softmax) + b_softmax\r\n\r\n    return logits\r\n\r\nclass model(object):\r\n\r\n    def __init__(self, FLAGS):\r\n\r\n        # Placeholders\r\n        self.inputs_X = tf.placeholder(tf.int32,\r\n            shape=[None, None], name='inputs_X')\r\n        self.targets_y = tf.placeholder(tf.float32,\r\n            shape=[None, None], name='targets_y')\r\n        self.seq_lens = tf.placeholder(tf.int32,\r\n            shape=[None, ], name='seq_lens')\r\n        self.dropout = tf.placeholder(tf.float32)\r\n\r\n        # RNN cell\r\n        stacked_cell = rnn_cell(FLAGS, self.dropout)\r\n\r\n        # Inputs to RNN\r\n        with tf.variable_scope('rnn_inputs',reuse=True):\r\n            W_input = tf.get_variable(\"W_input\",\r\n                [FLAGS.en_vocab_size, FLAGS.num_hidden_units])\r\n\r\n        inputs = rnn_inputs(FLAGS, self.inputs_X)\r\n        #initial_state = stacked_cell.zero_state(FLAGS.batch_size, tf.float32)\r\n\r\n        # Outputs from RNN\r\n        all_outputs, state = tf.nn.dynamic_rnn(cell=stacked_cell, inputs=inputs,\r\n            sequence_length=self.seq_lens, dtype=tf.float32)\r\n\r\n        # state has the last RELEVANT output automatically since we fed in seq_len\r\n        # [0] because state is a tuple with a tensor inside it\r\n        outputs = state[0]\r\n\r\n        # Process RNN outputs\r\n        with tf.variable_scope('rnn_softmax',reuse=True):\r\n            W_softmax = tf.get_variable(\"W_softmax\",\r\n                [FLAGS.num_hidden_units, FLAGS.num_classes])\r\n            b_softmax = tf.get_variable(\"b_softmax\", [FLAGS.num_classes])\r\n\r\n        # Logits\r\n        logits = rnn_softmax(FLAGS, outputs)\r\n        probabilities = tf.nn.softmax(logits)\r\n        self.accuracy = tf.equal(tf.argmax(\r\n            self.targets_y,1), tf.argmax(logits,1))\r\n\r\n        # Loss\r\n        self.loss = tf.reduce_mean(\r\n            tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=self.targets_y))\r\n\r\n        # Optimization\r\n        self.lr = tf.Variable(0.0, trainable=False)\r\n        trainable_vars = tf.trainable_variables()\r\n        # clip the gradient to avoid vanishing or blowing up gradients\r\n        grads, _ = tf.clip_by_global_norm(\r\n            tf.gradients(self.loss, trainable_vars), FLAGS.max_gradient_norm)\r\n        optimizer = tf.train.AdamOptimizer(self.lr)\r\n        self.train_optimizer = optimizer.apply_gradients(\r\n            zip(grads, trainable_vars))\r\n\r\n        # Below are values we will use for sampling (generating the sentiment\r\n        # after each word.)\r\n\r\n        # this is taking all the ouputs for the first input sequence\r\n        # (only 1 input sequence since we are sampling)\r\n        sampling_outputs = all_outputs[0]\r\n\r\n        # Logits\r\n        sampling_logits = rnn_softmax(FLAGS, sampling_outputs)\r\n        self.sampling_probabilities = tf.nn.softmax(sampling_logits)\r\n\r\n        # Components for model saving\r\n        self.global_step = tf.Variable(0, trainable=False)\r\n        self.saver = tf.train.Saver(tf.all_variables())\r\n\r\n    def step(self, sess, batch_X, batch_seq_lens, batch_y=None, dropout=0.0,\r\n        forward_only=True, sampling=False):\r\n\r\n        input_feed = {self.inputs_X: batch_X,\r\n                      self.targets_y: batch_y,\r\n                      self.seq_lens: batch_seq_lens,\r\n                      self.dropout: dropout}\r\n\r\n        if forward_only:\r\n            if not sampling:\r\n                output_feed = [self.loss,\r\n                               self.accuracy]\r\n            elif sampling:\r\n                input_feed = {self.inputs_X: batch_X,\r\n                              self.seq_lens: batch_seq_lens,\r\n                              self.dropout: dropout}\r\n                output_feed = [self.sampling_probabilities]\r\n        else: # training\r\n            output_feed = [self.train_optimizer,\r\n                           self.loss,\r\n                           self.accuracy]\r\n\r\n\r\n        outputs = sess.run(output_feed, input_feed)\r\n\r\n        if forward_only:\r\n            if not sampling:\r\n                return outputs[0], outputs[1]\r\n            elif sampling:\r\n                return outputs[0]\r\n        else: # training\r\n            return outputs[0], outputs[1], outputs[2]\r\n\r\n**But I faced this error while training:**\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-19-93fd337a0d5c> in <module>()\r\n----> 1 train()\r\n\r\n<ipython-input-18-62be6fa1e73e> in train()\r\n      9 \r\n     10         # Load old model or create new one\r\n---> 11         model = create_model(sess, FLAGS)\r\n     12 \r\n     13         # Train results\r\n\r\n<ipython-input-17-0c9c27ad52d3> in create_model(sess, FLAGS)\r\n      1 def create_model(sess, FLAGS):\r\n      2 \r\n----> 3     text_model = model(FLAGS)\r\n      4 \r\n      5     ckpt = tf.train.get_checkpoint_state(FLAGS.ckpt_dir)\r\n\r\n<ipython-input-15-bd33cb4f9d34> in __init__(self, FLAGS)\r\n     18         with tf.variable_scope('rnn_inputs',reuse=True):\r\n     19             W_input = tf.get_variable(\"W_input\",\r\n---> 20                 [FLAGS.en_vocab_size, FLAGS.num_hidden_units])\r\n     21 \r\n     22         inputs = rnn_inputs(FLAGS, self.inputs_X)\r\n\r\nC:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\r\n   1063       collections=collections, caching_device=caching_device,\r\n   1064       partitioner=partitioner, validate_shape=validate_shape,\r\n-> 1065       use_resource=use_resource, custom_getter=custom_getter)\r\n   1066 get_variable_or_local_docstring = (\r\n   1067     \"\"\"%s\r\n\r\nC:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\r\n    960           collections=collections, caching_device=caching_device,\r\n    961           partitioner=partitioner, validate_shape=validate_shape,\r\n--> 962           use_resource=use_resource, custom_getter=custom_getter)\r\n    963 \r\n    964   def _get_partitioned_variable(self,\r\n\r\nC:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in get_variable(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\r\n    365           reuse=reuse, trainable=trainable, collections=collections,\r\n    366           caching_device=caching_device, partitioner=partitioner,\r\n--> 367           validate_shape=validate_shape, use_resource=use_resource)\r\n    368 \r\n    369   def _get_partitioned_variable(\r\n\r\nC:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in _true_getter(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\r\n    350           trainable=trainable, collections=collections,\r\n    351           caching_device=caching_device, validate_shape=validate_shape,\r\n--> 352           use_resource=use_resource)\r\n    353 \r\n    354     if custom_getter is not None:\r\n\r\nC:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in _get_single_variable(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\r\n    680       raise ValueError(\"Variable %s does not exist, or was not created with \"\r\n    681                        \"tf.get_variable(). Did you mean to set reuse=None in \"\r\n--> 682                        \"VarScope?\" % name)\r\n    683     if not shape.is_fully_defined() and not initializing_from_value:\r\n    684       raise ValueError(\"Shape of a new variable (%s) must be fully defined, \"\r\n\r\n**ValueError:** Variable rnn_inputs/W_input does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n\r\n**So,I set reuse=None, but it showed another error:**\r\n\r\n**ValueError**: Variable rnn_inputs/W_input already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\r\n\r\n  File \"C:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\r\n    self._traceback = _extract_stack()\r\n  File \"C:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"C:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n\r\n**I again set back reuse = True, which should be the case,but this is error this time:**\r\n\r\nVariable rnn/multi_rnn_cell/cell_0/gru_cell/gates/kernel already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\r\n\r\n  File \"C:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\r\n    self._traceback = _extract_stack()\r\n  File \"C:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"C:\\Users\\Prof subhasis\\Anaconda31\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n\r\nCan anybody help me with this?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 13878, "title": "protobuf lib path bug fix for benckmark on osx", "body": "### bug info\r\nIf not fixed, `/usr/local/lib` will before `tensorflow/contrib/makefile/gen/protobuf-host/lib/` in the library search path. It will cause `undefined symbol` problem of protobuf when building `benchmark` because of incompatibility of system installed protobuf.\r\n\r\n### fix\r\nHave tested on maxOS 10.12.6\r\n\r\n@vrv ", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Thanks, do you know if this is specific to Mac OS X or does it apply more generally?", "Linux has already set this so has no problem. I have also tested IOS and it works right. Others, e.g, Android and PI, I don't have those environment so have no test.", "@tensorflow-jenkins test this please", "Unrelated failures.", "view"]}, {"number": 13877, "title": "Update layers_test.py", "body": "Hello!\r\nI have merged the two pull requests i have made earlier (https://github.com/tensorflow/tensorflow/pull/13829 and https://github.com/tensorflow/tensorflow/pull/13864) with reference to the issue https://github.com/tensorflow/tensorflow/issues/11673 .\r\nPlease suggest if any more changes are required.", "comments": ["Can one of the admins verify this patch?", "Okay we'll look at that other PR that has both the test change and the feature change in one PR, thanks", "sir this that PR which has both the merged pull requests.", "I only see one file in this commit, see: https://github.com/tensorflow/tensorflow/pull/13877/files", "Can you just use the branch in https://github.com/tensorflow/tensorflow/pull/13829 and fix everything up there?  Thanks"]}, {"number": 13876, "title": "Should we provide parameters for 'data_dir' or 'untar' to cifar10.load_data?", "body": "https://github.com/tensorflow/tensorflow/blob/d7409d32bba5ffa89141ec5427780f68a3b6942d/tensorflow/python/keras/_impl/keras/datasets/cifar10.py#L30\r\n\r\nThis is maybe really a trivial issue for such a brilliant framework,\r\nbut it would be great more friendly to add this flexibility to load data from somewhere that I already have it and unpacked. Or it will fixed me to dig into the source to find where and in which format should this data be stored.\r\n\r\nHope that make sense.\r\n\r\nThanks a lot for you time.", "comments": ["Sounds fine by me. This will need to go through an API review.\r\n\r\nCC @martinwicke ", "@drpngx @uZeroJ I think it's reasonable. I'd like to make a PR to add the flexibility.", "@DjangoPeng Thanks!", "I think @fchollet  has given a reasonable explaination in the PR. I agree on his point and suggest to close the issue, cause there is already a function for using local data files.  \r\n\r\n@uZeroJ @drpngx  What do you think?", "@DjangoPeng , Got it. Anyway thanks again."]}, {"number": 13875, "title": "Missing MPI collectives op symbols in TF build", "body": "Running into a build issue when trying to use MPI collectives. We are able to build a recent version of TF using NVIDIA's changes for CUDA 9/cuDNN 7. However, it appears that the build strips the MPI collectives library ops:\r\n\r\n```\r\n% python -c 'import tensorflow.contrib.mpi_collectives as mpi'\r\nTraceback (most recent call last):\r\n File \"<string>\", line 1, in <module>\r\n File \"/mnt/home/lib/python3.6/site-packages/tensorflow/contrib/mpi_collectives/__init__.py\", line 126, in <module>\r\n   from tensorflow.contrib.mpi_collectives.mpi_ops import size\r\n File \"/mnt/home/lib/python3.6/site-packages/tensorflow/contrib/mpi_collectives/mpi_ops.py\", line 59, in <module>\r\n   'MPIAllreduce'])\r\n File \"/mnt/home/lib/python3.6/site-packages/tensorflow/contrib/mpi_collectives/mpi_ops.py\", line 51, in _load_library\r\n   (expected_op, name))\r\nNameError: Could not find operator MPISize in dynamic library mpi_collectives.so\r\n```\r\n\r\nIt seems like the issue might be caused by commit 5c7f9e3, which changes linking behavior, but we're unable to bisect due to commit order dependencies.\r\n\r\n\r\nAnyone have an idea how to fix the issue? We're willing to update the MPI collectives code and submit a PR fix.\r\n\r\n\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: We've applied NVIDIA's CUDA 9/cuDNN 7 patches for mixed-precision per this page: http://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html#training_tensorflow\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 14.04\r\n- **TensorFlow version (use command below)**: ea94bbe9fa9f9b3d01fb057c02ef7873d76bf09c\r\n- **Python version**: 3.6\r\n- **Bazel version**: 0.5.4\r\n- **CUDA/cuDNN version**: CUDA 9.0.103_rc/cuDNN 7.0-rc\r\n- **Exact command to reproduce**: `python -c 'import tensorflow.contrib.mpi_collectives as mpi'`\r\n\r\n\r\n\r\n\r\n", "comments": ["One test/workaround is to build with `--config=monolithic`. That will give you the old op loading behavior.\r\n\r\nSeveral questions come to mind, like why we're not running this code in tests regularly (are we?), and what the actual issue is with the op loading. I can take a look next week.", "We've tested a few more things: we tried `--config=monolithic`, we built the current TF mainline head ea94bbe without the CUDA 9/cuDNN 7 changes from NVIDIA, and built using prior versions of CUDA (8) and cuDNN (6). The error exists after each of these builds. IOW, the NVIDIA patches were not part of the problem.\r\n\r\nWe've started looking into Bazel's process for static linking against the MPI library. It seems like we might not be pulling MPI symbols into the TF binary, so during linking, gcc might be stripping out the MPI ops thinking they don't connect to anything (i.e., dead code elimination).\r\n\r\nThe reason we don't have any Jenkins tests for tf.contrib.mpi_collectives is that we don't currently have a way to build TF with MPI enabled. It requires an MPI library to run the simple ops, and to test the collectives ops, we would need Bazel to be able to run multiple threads or processes that would communicate.", "Does it work if you add alwayslink=1 to https://github.com/tensorflow/tensorflow/blob/53e7541cf7efa61ba22c9f042e07031d87c8f145/tensorflow/tensorflow.bzl#L1138 ?", "Just wanted to give an update since others have asked about this as well: We've tried a number of different combinations of `alwayslink`, `linkstatic`, and `linkshared`, but nothing has fixed this issue. I think we will need to restructure the Bazel targets to get finer-grained control of the binary or library options during build. I won't be able to look into this soon, but if anyone wants to use the MPI collectives in the meantime, we recommend building a TF mainline commit before 5c7f9e3", "So it does work right before that commit? I thought you couldn't bisect.\r\n\r\nCan you try copying the \"defines=\" from //tensorflow/core:lib_internal to //tensorflow/core:lib_internal_impl (or just wait a bit because I have a commit which will sync and do that)? Defines were not applied to .cc files in that commit. That's the only difference I know of when building with --config=monolithic.", "After removing the dependency on CUDA 9/cuDNN 7 changes (by establishing they weren't the problem), we were able to bisect. We built with the commit just before 5c7f9e3 (i.e., 054b882), and the MPI collectives import works there.\r\n\r\nThanks for the recommendations! I'll give them a try sometime next week.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "I submitted a fix for this in PR #15534. Stay tuned there for updates.", "The fix #15534 is merged. `mpi_collectives` in mainline now works for us. Closing this issue."]}, {"number": 13874, "title": "Update node_def.proto comments", "body": "The device field had outdated comments.\r\n\r\nNote: We could consider adding tpu as an example here, e.g. \"gpu\" | \"cpu\" | \"tpu\".  Thoughts?", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "Feel free to send an update :)", "Hi. Was this change announced in the release notes anywhere?  ", "Hi,\n\nI did not see this mentioned in relnotes for 1.4.\n\nOn Mon, Nov 20, 2017 at 2:46 PM, Thomas Deegan <notifications@github.com>\nwrote:\n\n> Hi. Was this change announced in the release notes anywhere?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/13874#issuecomment-345856458>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AKa_OZ_NFEt6LSP0Lhy1NxzZVG3UQ1Iaks5s4gFLgaJpZM4QBXNL>\n> .\n>\n", "This just updated a stale comment, /device:FOO has been supported in the core codebase well before 1.0, though not all libraries have handled this form of the device name until more recently.\r\n\r\n/gpu:0 and /cpu:0 will remain for the foreseeable future due to backwards compatibility, but new devices must use /device:$XPU:$id."]}, {"number": 13873, "title": "Variable name for the eager test", "body": "", "comments": ["Test failure seems like infra failure, since one of the previously failing suites passed, I'm assuming it's all good."]}, {"number": 13872, "title": "OS X: tensorflow java image not found", "body": "i am running with Mac OS\r\n```\r\n<dependencies>\r\n\t\t<dependency>\r\n\t\t\t<groupId>org.tensorflow</groupId>\r\n\t\t\t<artifactId>tensorflow</artifactId>\r\n\t\t\t<version>1.4.0-rc0</version>\r\n\t\t</dependency>\r\n\t</dependencies>\r\n```\r\n\r\nplease note the issue is seen in 1.4.0-rc0 and 1.3.0, however the example works fine with 1.3.0-rc0\r\n\r\n```\r\norg.tensorflow.NativeLibrary: tryLoadLibraryFailed: no tensorflow_jni in java.library.path\r\norg.tensorflow.NativeLibrary: jniResourceName: org/tensorflow/native/darwin-x86_64/libtensorflow_jni.dylib\r\norg.tensorflow.NativeLibrary: frameworkResourceName: org/tensorflow/native/darwin-x86_64/libtensorflow_framework.dylib\r\norg.tensorflow.NativeLibrary: org/tensorflow/native/darwin-x86_64/libtensorflow_framework.dylib not found. This is fine assuming org/tensorflow/native/darwin-x86_64/libtensorflow_jni.dylib is not built to depend on it.\r\norg.tensorflow.NativeLibrary: extracting native library to: /var/folders/bj/v1l790113yn16zvhljd6yl8h0000gn/T/tensorflow_native_libraries-1508538938374-0/libtensorflow_jni.dylib\r\norg.tensorflow.NativeLibrary: copied 90774332 bytes to /var/folders/bj/v1l790113yn16zvhljd6yl8h0000gn/T/tensorflow_native_libraries-1508538938374-0/libtensorflow_jni.dylib\r\nException in thread \"main\" java.lang.UnsatisfiedLinkError: /private/var/folders/bj/v1l790113yn16zvhljd6yl8h0000gn/T/tensorflow_native_libraries-1508538938374-0/libtensorflow_jni.dylib: dlopen(/private/var/folders/bj/v1l790113yn16zvhljd6yl8h0000gn/T/tensorflow_native_libraries-1508538938374-0/libtensorflow_jni.dylib, 1): Library not loaded: @rpath/libtensorflow_framework.so\r\n  Referenced from: /private/var/folders/bj/v1l790113yn16zvhljd6yl8h0000gn/T/tensorflow_native_libraries-1508538938374-0/libtensorflow_jni.dylib\r\n  Reason: image not found\r\n\tat java.lang.ClassLoader$NativeLibrary.load(Native Method)\r\n\tat java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1937)\r\n\tat java.lang.ClassLoader.loadLibrary(ClassLoader.java:1822)\r\n\tat java.lang.Runtime.load0(Runtime.java:809)\r\n\tat java.lang.System.load(System.java:1086)\r\n\tat org.tensorflow.NativeLibrary.load(NativeLibrary.java:96)\r\n\tat org.tensorflow.TensorFlow.init(TensorFlow.java:66)\r\n\tat org.tensorflow.TensorFlow.<clinit>(TensorFlow.java:70)\r\n\tat org.tensorflow.Graph.<clinit>(Graph.java:258)\r\n\tat xTensorflow.HelloTF.main(HelloTF.java:13)\r\n```", "comments": ["Thanks for a report, there seems to be an issue with 1.4.0-rc0 and Mac OS X.\r\nHowever the point release 1.3.0 should be fine.\r\n\r\nThe  snippet you provided seems to be on the usage of 1.4.0-rc0. Could you provide the same log for when you're using 1.3.0?", "I ran into the same problem in 1.4.0-rc0 and 1.4.0-rc1. But it works well in 1.3.0\r\nIt's import to me for supporting string tensor with Java in version 1.4.0\r\n", "Yup, we will have this fixed in 1.4.0-rc1 (if there is one) and surely in the 1.4.0 release.\r\n(For the record, things should be fine for Linux and Windows. If the problem appears in anything other than 1.4.0-rc0 on OS X, please let us know)", "@asimshankar yes 1.3.0 works fine", "1.4.0-rc1 was just published and should have the problem fixed. Could you confirm that it works @sujitbiswas @mclodaa ?", "@asimshankar it works well in 1.4.0-rc1. Thx!\r\nI wonder what causes this problem. I tried to solve this problem, but failed.", "@asimshankar  i am using the maven [repo](https://mvnrepository.com/artifact/org.tensorflow/libtensorflow) it is not yet updated with 1.4.0-rc1", "This should be resolved with 1.4.0 and 1.4.0-rc1. Please feel free to reopen if that is not the case.\r\n\r\n@sujitbiswas : I've noticed that mvnrepository.com takes a while to update sometimes. [Maven Central](https://search.maven.org/#search%7Cga%7C1%7Cg%3A%22org.tensorflow%22) would be more accurate.", "<dependencies>\r\n\t\t<dependency>\r\n\t\t\t<groupId>org.tensorflow</groupId>\r\n\t\t\t<artifactId>tensorflow</artifactId>\r\n\t\t\t<version>1.13-1</version>\r\n\t\t</dependency>\r\n\t</dependencies>\r\n\r\nI use the version1.13.1, then this problem appeared too."]}, {"number": 13871, "title": "Remove deprecated 32bit IOS builds", "body": "Apple has stopped supporting 32bit iOS builds staring with iOS11.\r\nBuilding these binaries are pretty much useless on iOS, but saves\r\na lot of time by just building arm64 and x86_64.\r\n\r\nTEST: build ios libraries with ./build_all_ios.sh", "comments": ["Can one of the admins verify this patch?", "Pete perhaps you might know more here :)", "Thanks for the PR! Unfortunately we still have a lot of internal and external apps that are shipping for iOS 10, and want to support old devices. I'd love to see if there was a clean way to make these architectures optional though, since you're right that for many developers building them is a waste of time.", "@petewarden I have another PR #13920  which will allow building just the architecture you want. I will update it so the default builds all the current libs. ", "New PR #13959 is up which builds all the architectures by default or you can pass -a \"arm64\" for just one arch. "]}, {"number": 13870, "title": "Add known Dataset issue to RELEASE.md.", "body": "Adding info about issue using Unicode strings with Datasets to 1.4 release notes.", "comments": []}, {"number": 13869, "title": "conv2d_transpose crashing, \"NotFoundError: No algorithm worked!\", only with batch size >=2^16", "body": "I'm getting a crash running a simple autoencoder network, stack trace below. Interestingly if I trim the batch size of the input (validate_vecs_normed) to 65535, everything is fine. E.g.\r\n`validate_vecs_normed.shape (65536, 75)` crashes, `validate_vecs_normed.shape (65536, 75)` does not. The size of input is less than 20 MB so should be plenty of room with a 12GB card.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  [...snip...] \r\n    recon, batch_cost = sess.run([decoded, cost], feed_dict={x_in_unrav: validate_vecs_normed})\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: No algorithm worked!\r\n\t [[Node: conv2d_transpose = Conv2DBackpropInput[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](stack, W_0/read, enc_output_0)]]\r\n\t [[Node: cost/_25 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_47_cost\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\nCaused by op u'conv2d_transpose', defined at:\r\n  [...snip...]\r\n    saver = tf.train.import_meta_graph(os.path.join(model_dir, model_meta_format.format(fold_ind)))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1698, in import_meta_graph\r\n    **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/meta_graph.py\", line 656, in import_scoped_meta_graph\r\n    producer_op_list=producer_op_list)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 313, in import_graph_def\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nNotFoundError (see above for traceback): No algorithm worked!\r\n\t [[Node: conv2d_transpose = Conv2DBackpropInput[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](stack, W_0/read, enc_output_0)]]\r\n\t [[Node: cost/_25 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_47_cost\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n```\r\nPossibly related to #11327 or #9576 but not sure. One other thing, when running same code on tensorflow 1.0.1, there was no crash, but the return value \"decoded\" was all zeros when running a large batch size, and normal with smaller. I'm not sure if it's the same 65535/65536 threshold, I hadn't found it at that point.\r\n\r\nJust for fun I ran with `TF_USE_CUDNN=0` but that crashes with `UnimplementedError (see above for traceback): Conv2D for GPU is not currently supported without cudnn`\r\n\r\nOS: Ubuntu 16.04. \r\nRunning docker image based on tensorflow/tensorflow:1.3.0-devel-gpu (so python 2.7.12, CUDA v8.0, cuDNN v6.0), with nvidia-docker 17.05.0-ce. \r\nDocker image tensorflow/tensorflow:1.0.1-gpu returns all zeros instead of crashing.\r\nGPUs: 2x Titan X (Pascal).\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@rmlarsen do you know about issues there perhaps?", "From cudnn 7.0.1 release notes:\r\n```\r\nFixed bug that produced a silent computation error for when a batch size was larger\r\nthan 65536 for CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM .\r\n```\r\nAlthough the release notes didn't say whether it works or fails loudly in 7.0.1. But it certainly doesn't work with cudnn 6.", "Currently cudnn 7 works with tf, but only if built from source, right?", "@sullivak yup\r\n- https://github.com/tensorflow/tensorflow/issues/12052#issuecomment-335357372\r\n- https://github.com/tensorflow/tensorflow/issues/12052#issuecomment-337555746\r\n\r\nDoes anyone know about a publicly available (docker hub) image for this? I found [this](https://github.com/chi-hung/Dockerbuilds-Keras) by @chu-hung, which looks promising. I also asked at https://github.com/tensorflow/tensorflow/pull/14102#issuecomment-341579981\r\n\r\n", "Hi all,\r\n\r\nI've run into this issue, also doing an autoencoder type task. I've reduced it to a simple case, below.\r\n\r\nThis does not seem *just* about the batch count. In this example filter_width < 4 and filter_stride < 1 will still work.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ninput_width = 1000\r\nin_channels = 1\r\nout_channels = 1\r\nfilter_width = 5\r\nfilter_stride = 2\r\nn_samples = 65536\r\n\r\nx = tf.placeholder(tf.float32, [None, input_width, in_channels])\r\n\r\nx_2d = tf.reshape(x, [tf.shape(x)[0], 1, input_width, in_channels])\r\n\r\nl1 = tf.layers.conv2d(\r\n    x_2d,\r\n    out_channels,\r\n    [1, filter_width],\r\n    [1, filter_stride],\r\n    padding='same'\r\n)\r\n\r\nl2 = tf.layers.conv2d_transpose(\r\n    l1,\r\n    in_channels,\r\n    [1, filter_width],\r\n    [1, filter_stride],\r\n    padding='same'\r\n)\r\n\r\ny = tf.reshape(l2, [tf.shape(l2)[0], -1, in_channels])\r\n\r\nx_samples = np.random.randn(n_samples, input_width, 1)\r\n\r\nminimise = tf.train.AdamOptimizer().minimize(tf.reduce_sum(tf.square(x - y)))\r\n\r\nwith tf.Session() as session:\r\n    session.run(tf.global_variables_initializer())\r\n    session.run(minimise, feed_dict={x: x_samples})\r\n\r\n```", "@tahsmith Have you solved this problem? I met the same errors when the size of the first dim is larger than 65535.", "I met the same problem for cudnn 7.3.1, cuda 10.0. When using batch size > 65535, the outputs after 65535 is incorrect. Moreover, there is no problem for fully-connected neural network, but this problems occur for some neural network structure.", "Hi @sullivak! \r\nWe are checking to see if you still need help in this issue . Please create a new issue if the issue is replicating in Latest version TF 2.6 . Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/13869\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/13869\">No</a>\n"]}]