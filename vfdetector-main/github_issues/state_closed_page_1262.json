[{"number": 15280, "title": "Missing dlcose()/FreeLibrary() after dlopen()/LoadLibrary()", "body": "Have I written custom code: N/A\r\n OS Platform and Distribution: N/A\r\n TensorFlow installed from: N/A\r\n TensorFlow version: N/A\r\n Bazel version: N/A\r\n CUDA/cuDNN version: N/A\r\n GPU model and memory: N/A\r\n Exact command to reproduce: N/A\r\n\r\nProblem description:\r\nA. Looking at code below, there are couple of issues:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/nnapi/NeuralNetworksShim.h#L34-L59\r\n   1.There is no dlcose() call after dlopen() and dysym() in the code above.\r\n   2.There can be two successful getLibraryHandle() calls without dlclose() in loadFunction().\r\n\r\nB. More generally, the code below shows there is no interface to unload DLL either.\r\nhttps://github.com/tensorflow/tensorflow/blob/359d6f9716c0bb9bd8201ce600da98b0481a8049/tensorflow/core/platform/env.h#L254-L280", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler, template updated. Thanks.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "@tensorflowbutler, I did answer for the updated #issue template 23 days ago, please double check the description and let me know if you need more information.\r\nI think the issue is still there. I have no access to change the label \"awaiting response\" to other status.\r\n  "]}, {"number": 15279, "title": "Minor optimization of conv_ops.", "body": "If data_format is NCHW, and the computation is done on the GPU, the temporary tensor `transformed_output` is not needed. One can use directly the `output` tensor. This\r\nleaves more memory for the cudnn scratch allocator, possibly improving performance\r\nof the convolution algorithm.\r\n\r\nI also fixed several style issues in `conv_ops.cc` reported by clang-format.\r\n\r\nI did not add any tests because the change is covered by `python/kernel_tests/conv_ops_test.py`.", "comments": ["Can one of the admins verify this patch?", "@zheng-xq you're far more familiar with these kernels, can you take a look?", "@zheng-xq ping XQ.", "Nagging Reviewer @zheng-xq: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 243 days with no activity and the `awaiting review` label has been applied.", "hi @codrut3, we are working through PR backlogs, is this something you're still interested in getting committed?  If so, you'll have to rebase to deal with the conflicts.  Sorry for the delay for this very useful improvement.", "Hi @vrv,\r\n\r\nYes, if you are interested, I want to get it committed. This change followed my investigation into a bug, where I discovered that memory usage is not optimal for data format NCHW. From what I see, this optimization still applies to the current conv_ops.cc.\r\n\r\nAnyway, in the meantime I changed countries, and I lost the GPU that I used for dev. So I need a few days to find a GPU card and set up a new dev environment.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "Hi @vrv,\r\nI resolved the conflicts, but the checks are not being run. Please, what should I do?", "I made a small syntax / comment edit, but LG.  Running tests (again).  We have to trigger the tests, unfortunately.", "Hi @vrv, @wt-huang,\r\n\r\nI think the tests passed. Is there anything else I should do?", "I clicked some other buttons that apparently were necessary but I'm awaiting next steps too..."]}, {"number": 15278, "title": "No gradient for argmax", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary (pip)\r\n- **TensorFlow version (use command below)**: 1.5.0-dev20171210 (nightly from today)\r\n- **Python version**: 3.6.2\r\n\r\n`ArgMax` crashes with `no gradient` error. I had a working version before I made slight adjustments to my code. I am very confused how this error can happen.\r\n\r\nStacktrace:\r\n```\r\n  File \"C:\\Development\\Tools\\miniconda\\envs\\mpgan-nightly\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\learn_runner.py\", line 46, in _execute_schedule\r\n    return task()\r\n  File \"C:\\Development\\Tools\\miniconda\\envs\\mpgan-nightly\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\experiment.py\", line 377, in train\r\n    saving_listeners=self._saving_listeners)\r\n  File \"C:\\Development\\Tools\\miniconda\\envs\\mpgan-nightly\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\experiment.py\", line 824, in _call_train\r\n    saving_listeners=saving_listeners)\r\n  File \"C:\\Development\\Tools\\miniconda\\envs\\mpgan-nightly\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 314, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"C:\\Development\\Tools\\miniconda\\envs\\mpgan-nightly\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 743, in _train_model\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"C:\\Development\\Tools\\miniconda\\envs\\mpgan-nightly\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 725, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"C:\\Development\\Tools\\miniconda\\envs\\mpgan-nightly\\lib\\site-packages\\tensorflow\\contrib\\gan\\python\\estimator\\python\\gan_estimator_impl.py\", line 162, in _model_fn\r\n    add_summaries)\r\n  File \"C:\\Development\\Tools\\miniconda\\envs\\mpgan-nightly\\lib\\site-packages\\tensorflow\\contrib\\gan\\python\\estimator\\python\\gan_estimator_impl.py\", line 235, in _gan_model_fn\r\n    labels=None)\r\n  File \"C:\\Development\\Tools\\miniconda\\envs\\mpgan-nightly\\lib\\site-packages\\tensorflow\\contrib\\gan\\python\\estimator\\python\\head_impl.py\", line 189, in create_estimator_spec\r\n    self._discriminator_optimizer)\r\n  File \"C:\\Development\\Tools\\miniconda\\envs\\mpgan-nightly\\lib\\site-packages\\tensorflow\\contrib\\gan\\python\\train.py\", line 540, in gan_train_ops\r\n    **kwargs)\r\n  File \"C:\\Development\\Tools\\miniconda\\envs\\mpgan-nightly\\lib\\site-packages\\tensorflow\\contrib\\training\\python\\training\\training.py\", line 447, in create_train_op\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"C:\\Development\\Tools\\miniconda\\envs\\mpgan-nightly\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 456, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"C:\\Development\\Tools\\miniconda\\envs\\mpgan-nightly\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 581, in gradients\r\n    (op.name, op.type))\r\nLookupError: No gradient defined for operation 'Generator/cond/generator/generator/while/BasicDecoderStep/TrainingHelperSample/ArgMax' (op type: ArgMax)\r\n```", "comments": ["It seems duplicate of https://github.com/tensorflow/tensorflow/issues/14931#issuecomment-347751803. CF: #14947.\r\n\r\n>Ah sorry, brain freeze. Actually, we could create a practical implementation of gradient for argmax, but it just wouldn't be very useful for learning.", "What makes me wonder is that my generator worked in a previous configuration and the error originates from `tf.contrib.seq2seq.TrainingHelper` which is a common class for seq2seq training. The only thing I believe I have changed is switching from tf1.4 to tf1.5 nightly", "So I tested my code with tensorflow 1.4 instead of tf-nightly from yesterday and I do not get the `ArgMax no gradient` bug. This seems to be an issue in 1.5 (nightly)", "It seems likely unrelated to your particular code, but just in case (and for completion) can you include the code snippet that is throwing this error?", "I cannot share the whole code, it is rather large.\r\nBut it comes from the `TrainingHelper`\r\n\r\n```python\r\nembedded_input = tf.nn.embedding_lookup(\r\n            params=embedding,\r\n            ids=input_data)\r\n\r\n# Internally it has a `sample` method that uses argmax\r\nhelper = tf.contrib.seq2seq.TrainingHelper(\r\n              inputs=embedded_input,\r\n              sequence_length=sequence_length,\r\n              time_major=self.time_major)\r\n\r\nrnn_initial_state = tf.zeros([batch_size, self.cell_\r\nrnn_initial_state = tf.contrib.rnn.LSTMStateTuple(\r\n          rnn_initial_state, rnn_initial_state)\r\n\r\nrnn_initial_state = tf.zeros([batch_size, self.cell_size])\r\nrnn_initial_state = tf.contrib.rnn.LSTMStateTuple(\r\n    rnn_initial_state, rnn_initial_state)\r\n\r\ndecoder = tf.contrib.seq2seq.BasicDecoder(\r\n    cell=rnn_cell,\r\n    helper=helper,\r\n    initial_state=rnn_initial_state,\r\n    output_layer=output_projection)\r\nfinal_outputs, state, _ = tf.contrib.seq2seq.dynamic_decode(\r\n    decoder=decoder,\r\n    maximum_iterations=maximum_iterations,\r\n    output_time_major=self.time_major,\r\n    swap_memory=True,\r\n    scope=scope)\r\n```", "Perhaps it is related to 69c32459 , which removed the `tf.cast` about 5 days ago.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\na = tf.constant([1.0])\r\nb = a * 2\r\n\r\ndef use_cast(b):\r\n    c = tf.argmax(b, axis=-1)\r\n    c = tf.cast(c, tf.int32)\r\n    return c\r\n\r\ndef not_use_cast(b):\r\n    return tf.argmax(b, axis=-1, output_type=tf.int32)\r\n\r\nprint(\"use cast:\")\r\nprint(tf.gradients(use_cast(b), a))\r\n\r\nprint(\"===========\")\r\nprint(\"not use cast\")\r\nprint(tf.gradients(not_use_cast(b), a))\r\n```\r\nOutput:\r\n```python\r\n~/Downloads \u276f\u276f\u276f python test.py                                                               \u23ce\r\nuse cast:\r\n[None]\r\n===========\r\nnot use cast\r\nTraceback (most recent call last):\r\n  File \"/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 551, in gradients\r\n    grad_fn = ops.get_gradient_function(op)\r\n  File \"/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2134, in get_gradient_function\r\n    return _gradient_registry.lookup(op_type)\r\n  File \"/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/registry.py\", line 93, in lookup\r\n    \"%s registry has no entry for: %s\" % (self._name, name))\r\nLookupError: gradient registry has no entry for: ArgMax\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 19, in <module>\r\n    print(tf.gradients(not_use_case(b), a))\r\n  File \"/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 555, in gradients\r\n    (op.name, op.type))\r\nLookupError: No gradient defined for operation 'ArgMax_1' (op type: ArgMax)\r\n```", "Must be. The helper is of no use for me with that bug being present", "@sleighsoft could you revert the change of 69c3245 on your tensorflow (I mean, the python file,  which is installed on your computer) and test whether it works? ", "I tested it with the reverted code and then it works.\r\nAnother interesting thing is that with tfnightly I get the\r\n```\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\n```\r\ntwice. And also 2x\r\n```\r\nINFO:tensorflow:Saving checkpoints for 1 into tmp/adversarial\\model.ckpt.\r\nINFO:tensorflow:Saving checkpoints for 1 into tmp/adversarial\\model.ckpt.\r\n```", "Hi, @martinwicke @joel-shor , could you take a look ? I find we don't register argmax (and argmin) as `ops.NotDifferentiable` or `REGISTER_OP_NO_GRADIENT`, is it intentional? Thanks.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "This seems to be fixed by the above PR.", "I have the same error with seq2seq Model using Tensorflow 1.5rc1. In TensorFlow 1.4, it works without errors.", "Can you upload some console logs?", "This is the error code i receive when i run my code with tensorflow 1.5rc1:\r\n\r\n~~~~ Traceback (most recent call last):\r\n  File \"C:\\Users\\Tobi\\AppData\\Local\\conda\\conda\\envs\\tensorflow1.5\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 577, in gradients\r\n    grad_fn = ops.get_gradient_function(op)\r\n  File \"C:\\Users\\Tobi\\AppData\\Local\\conda\\conda\\envs\\tensorflow1.5\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2305, in get_gradient_function\r\n    return _gradient_registry.lookup(op_type)\r\n  File \"C:\\Users\\Tobi\\AppData\\Local\\conda\\conda\\envs\\tensorflow1.5\\lib\\site-packages\\tensorflow\\python\\framework\\registry.py\", line 93, in lookup\r\n    \"%s registry has no entry for: %s\" % (self._name, name))\r\nLookupError: gradient registry has no entry for: ArgMax\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:/Pycharm Projects/YahooSemEval/yahoo_main.py\", line 48, in <module>\r\n    main()\r\n  File \"D:/Pycharm Projects/YahooSemEval/yahoo_main.py\", line 44, in main\r\n    mode=args.mode, attention=args.attention)\r\n  File \"D:\\Pycharm Projects\\YahooSemEval\\model\\pre_train_model.py\", line 80, in __init__\r\n    self.build_mle_modul(target_output, self.final_outputs.rnn_output)\r\n  File \"D:\\Pycharm Projects\\YahooSemEval\\model\\pre_train_model.py\", line 284, in build_mle_modul\r\n    self.mle_train_op = tf.train.AdamOptimizer().minimize(self.loss)\r\n  File \"C:\\Users\\Tobi\\AppData\\Local\\conda\\conda\\envs\\tensorflow1.5\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 355, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"C:\\Users\\Tobi\\AppData\\Local\\conda\\conda\\envs\\tensorflow1.5\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 456, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"C:\\Users\\Tobi\\AppData\\Local\\conda\\conda\\envs\\tensorflow1.5\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 581, in gradients\r\n    (op.name, op.type))\r\nLookupError: No gradient defined for operation 'train_decoder/decoder/while/BasicDecoderStep/TrainingHelperSample/ArgMax' (op type: ArgMax)\r\n~~~~ \r\n\r\nWith tensorflow 1.4, it runs correctly. My code has a similar structur as the code postet from sleighsoft.", "@av8ramit, It seems the innocent-looking 69c3245 has broken gradients for argmax in 1.5. Did this make it into final? If this is easy to trigger, we probably have to patch it. Not sure though given the timeline for 1.6.", "@martinwicke 69c324591ba4dfeafb403ee59de56ffe063c1e94 is present in 1.5.0 (Github shows it being a part of 1.5.0, as does `git co google/r1.5 && git log --grep 'Use argmax output_type'`)", "That probably means argmax gradients would fail in 1.5. Bummer. \r\n\r\nWe need to fix this though. I'd love to understand why it fails, it looks like a bug in argmax itself, since using the dtype shouldn't break gradients. And we have to fix this soon, otherwise we'll have to patch 1.6.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @martinwicke: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Ok what happened here is that ArgMax never had a gradient, but really should have a gradient of `None` registered (telling TF that it's not differentiable). \r\n\r\nPreviously, the cast would swallow that error though, since it isn't differentiable, therefore TF never even looked for the inner (argmax) gradient. \r\n\r\nThis should have been fixed in 18bab99ac33f31192d400aebcfb7670a121655bd. \r\n\r\n\r\n", "@martinwicke Thanks for fix. I think argmin is not differentiable as well.", "So the reverted change could be applied again?", "I believe so. At least we can try.\n"]}, {"number": 15277, "title": "[XLA] Make the client_test able to be disabled using a manifest file", "body": "This change allows these 3 tests in the XLA ClientTest to be selectively disabled using the manifest mechanism.\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 15276, "title": "[XLA] Fix OS/X compile error with std::transform and  std::addressof", "body": "I don't know if this has been flagged up elsewhere...\r\n\r\non my platform (OS/X, XLA), I receive the following error when trying to compile tensorflow/compiler/xla/service/llvm_ir/kernel_support_library.cc:\r\n\r\n```\r\ntensorflow/compiler/xla/service/llvm_ir/kernel_support_library.cc:101:5: error: no matching function for call to 'transform'\r\n    std::transform(function->arg_begin(), function->arg_end(),\r\n    ^~~~~~~~~~~~~~\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/algorithm:1922:1: note: candidate template ignored: couldn't infer template argument '_UnaryOperation'\r\ntransform(_InputIterator __first, _InputIterator __last, _OutputIterator __result, _UnaryOperation __op)\r\n^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/algorithm:1932:1: note: candidate function template not viable: requires 5 arguments, but 4 were provided\r\ntransform(_InputIterator1 __first1, _InputIterator1 __last1, _InputIterator2 __first2,\r\n^\r\n```\r\n\r\nIt is possible that std::addressof cannot be matched with the unary function template.\r\n\r\nThis code change replaces addressof with an function, which does match std::function<> ok.\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "Thanks @DavidNorman. Do you mind resolving the conflicts?", "no problem.\r\n", "done\r\n", "I thought this was already resolved by #15229?  I'm not familiar with github's workflow so I may be missing something.", "It's probable, I have not pulled the master for a week due to vacation.....", "yes - it has been fixed.  although I prefer my anonymous inline function to the multi-line expansion :)\r\n"]}, {"number": 15275, "title": "[XLA] Allow components in the plugins directory to create devices", "body": "Due to a change in the visibility of sub-components of the XLA JIT, the Graphcore device was unable to use the target needed for creating devices.\r\n\r\nThis change adds the plugin directory to the set of directories allowed to use the JIT.\r\n\r\n\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "hi - would it be possible to have a look at this.  it is one of the only outstanding changes i have between our TF repo and the public one.", "I believe that commit cfaaace259a60a605e2abef535b0109f0d67fcd5 has sorted this independently.\r\n"]}, {"number": 15274, "title": "breaking change to distributed training moving from TF v1.3 to v1.4: \u201cUnavailableError: Trying to connect an http1.x server\u201d", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nn/a\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nRed Hat Enterprise Linux Server release 7.3 (Maipo)\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n1.4 (moving from 1,3)\r\nb'v1.4.0-4-g9283868' 1.4.0 in specific\r\n- **Python version**: \r\nPython 3.6.2 :: Anaconda custom (64-bit)\r\n- **Bazel version (if compiling from source)**:\r\n0.5.4\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)\r\n- **CUDA/cuDNN version**:\r\nN/a\r\n- **GPU model and memory**:\r\nN/a\r\n- **Exact command to reproduce**:\r\nsee below. Appears to be any distributed training use case with the cluster spec defined as below.\r\n\r\n### Describe the problem\r\n\r\nI believe this could be fairly characterized as a BUG.\r\n\r\nI raised (and solved) this issue on stackoverflow [here](https://stackoverflow.com/questions/47143043/breaking-change-to-distributed-training-moving-from-tf-v1-3-to-v1-4-unavailabl/47191081#47191081)\r\n\r\nIn short, upgrading from tf v1.3 to 1.4 distributed training broke with the error\r\n\"tensorflow.python.framework.errors_impl.UnavailableError: Trying to connect an http1.x server\".\r\nI was training across multiple CPU cores all on the same localhost.\r\n\r\nThe fix was to change the ps to the  string \"localhost\" explicitly in the cluster spec instead of the IP \"127.0.0.1\". It stops trying to connect to the localhost via my proxy server (which indeed, was HTTP1.x only i think). I would call this a bug since that IP should be the equivalent, and I'm not sure what other kind of behavior might have inadvertently changed between versions (since this always worked in TF <= 1.3). \r\n\r\nTo be fair, I'm also not sure if this is a tensorflow or more gRPC specific issue. In either case, the above is a work around for the problem.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 15273, "title": "Dataset Iterator is not an iterator", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: confidential\r\n- **TensorFlow installed from (source or binary)**: [Pypi](https://pypi.python.org/pypi/tensorflow-gpu/1.4.1)\r\n- **TensorFlow version (use command below)**: `v1.4.0-19-ga52c8d9 1.4.1`\r\n- **Python version**: 3.5.3\r\n- **CUDA/cuDNN version**: (sensitive information replaced by `xxx`)\r\n```\r\n$ apt search cud | grep installed\r\nlibcublas8.0/xxx,now 8.0.44-4 amd64 [installed]\r\nlibcuda1/xxx,now 375.66-1 amd64 [installed,automatic]\r\nlibcuda1-i386/xxx,now 375.66-1 i386 [installed,automatic]\r\nlibcudart8.0/xxx,now 8.0.44-4 amd64 [installed]\r\nlibcudnn6/now 6.0.21-1+cuda8.0 amd64 [installed,local]\r\nlibcufft8.0/xxx,now 8.0.44-4 amd64 [installed]\r\nlibcurand8.0/xxx,now 8.0.44-4 amd64 [installed]\r\nlibnvidia-fatbinaryloader/xxx,now 375.66-1 amd64 [installed,automatic]\r\nlibnvidia-ptxjitcompiler/xxx,now 375.66-1 amd64 [installed,automatic]\r\n```\r\n- **GPU model and memory**: Quadro K1200, 4019 MiB\r\n- **Exact command to reproduce**:\r\nRun [convert_to_records.py](https://github.com/tensorflow/models/tree/5a5d330539dff11eef79ca2e716fb477baf13cf9/official/mnist) from the official MNIST example, then:\r\n```python\r\n>>> import tensorflow as tf\r\n>>> ds = tf.data.TFRecordDataset(['/tmp/mnist_data'])\r\n>>> i  = ds.make_one_shot_iterator()\r\n>>> next(i)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: 'Iterator' object is not an iterator\r\n```\r\n\r\n### Describe the problem\r\n\r\nThe returned \"iterator\" is not an iterator, because it does not provide a `__next__` or `next` method. It does provide a `get_next` method, but that is not what Python expects.\r\n", "comments": ["I think it is expected. You can't write code like:\r\n```python\r\nfor item in ds.make_one_shot_iterator()\r\n   print(item)\r\n```", "Is there a technical reason that prevents `Iterator` from being an iterator? It looks like the iterator does something unexpected, [and there is already a warning for that](https://github.com/tensorflow/tensorflow/blob/4be5afc8b656cf6b540b547a1faafd2a0a82f5f2/tensorflow/python/data/ops/iterator_ops.py#L30-L39). But as far as I can see, nothing prevents a `next()` and `__next__()` method that simply call `get_next()`.", "CC @mrry, who is the author of the note.", "> Is there a technical reason that prevents Iterator from being an iterator?\r\n\r\nYes. In TensorFlow (except Eager mode), we use the `tf.data.Iterator` to get symbolic `tf.Tensor` objects that can be chained together with other operations to build a dataflow graph. We typically build the graph once, and use it many times. Wrapping the graph construction in a `for` loop (by using the `tf.data.Iterator` as a Python iterator) would not be efficient. Furthermore, `tf.data.Iterator` is implemented using TensorFlow operations, so you would need to provide a `tf.Session` to run these operations, and the Python iterator protocol doesn't provide a way to do that.\r\n\r\nIn eager mode, you can wrap a `tf.data.Dataset` in a [`tf.contrib.eager.Iterator`](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/eager/Iterator), which *is* usable as a Python iterator. This definitely seems to be a more natural use of the `tf.data` API, and we'd encourage you to try it out!"]}, {"number": 15272, "title": "Wrongly used dropout bug", "body": "Hi, \r\n\r\n    I guess it should use **tf.layers.dropout** instead of **tf.nn.dropout** here? Because in the inference stage, all the nodes should be used instead of dropout. I guess this is a bug. \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/examples/tutorials/mnist/mnist_deep.py#L92\r\n\r\n    Many thanks! \r\n\r\nBest wishes, \r\n\r\nQiuqiang\r\n    ", "comments": ["For `minst_deep.py`, the `keep_prob` argument of `tf.nn.dropout` is set to 1.0 to keep all nodes at test stage. It is not necessary to change to `tf.layers.dropout`.", "Many thanks! I see! I will close this issue! "]}, {"number": 15271, "title": "TFGAN ideas for pretraining/training", "body": "With the PR #14723 enabling `get_hooks_fn` to be set manually instead of the default `1` step generator and `1` step discriminator there comes a set of new \"problems\" / things to consider.\r\n\r\n1. The `Estimator` saves configured tensor summaries in the background. This does not work when doing something like a `2/2` split for generator/discriminator or a `2/3`. Then `Estimator` will only save one step instead of the actual amounts of steps taken (right?). This means that we have to consider an option to configure the `FileWriter`. I believe that is possible for the vanilla `Estimator` with `scaffolds` but not for the `GANEstimator` currently. This `FileWriter` has to be accessible by the `RunTrainOpsHook`. We would also need a generator + discriminator specific \"global_step\" that will be used in the `RunTrainOpsHook`. A quick idea would be to use the `overall_global_step * train_steps` of a `RunTrainOpsHook` or to use `dummy_global_step_generator` and `dummy_global_step_discriminator`.\r\n\r\n2. When \"pre-training\" (with a normal call to `.train()` and a modified `sequential_train_hook(10,10)`) the generator for e.g. `10` steps and then the discriminator for `10` steps. Does the discriminator `loss_fn` receive 10 times new data from the generator through `gan_model.discriminator_real_outputs` or will it always be the same data? For pre-training I would assume I can feed in the same output data from the generator in batches multiple epochs. I believe that is not possible in the current setup, but correct me if I am wrong.\r\n\r\n3. I have different loss functions for both pre-training and training. There is not `ModeKeys.PRETRAIN` to switch between them.\r\n\r\nIf I find more things I'll add them here.", "comments": ["1) The GANEstimator is purposefully designed to record 1 step for 2/2 if that is what's specified. If want it to be saved as 2 steps, you should specify 1/1, no?\r\n\r\n2) The generator would get 10 different batches of data. That is the normal behavior; AFAIK it is not standard to train multiple times on the same minibatch.\r\n\r\n3) That behavior isn't standard in any estimator. An easy fix would be to have your loss function inside a `tf.cond` based on the global step.", "1. It makes it difficult to compare the performance of runs with different splits. If there is an easy way to configure it I would use it but if that is too much hassle to implement I can also live without it.\r\n2. You are so correct, I completely misinterpreted that ....\r\n3. The idea with `tf.cond` is great.", "Is it ok if I open up a pull request that allows to disable `_use_check_shapes` in `GANEstimator`?", "I'm waiting on a review to make this change internally, then it will be\npushed to github within 24 hours. Sorry for the delay.\n\nOn Mon, Dec 11, 2017 at 2:09 PM, Julian Niedermeier <\nnotifications@github.com> wrote:\n\n> Is it ok if I open up a pull request that allows to disable\n> _use_check_shapes in GANEstimator?\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15271#issuecomment-350734034>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AFvffP8LeSHNin9WcOlJqUH2_-DaNNp2ks5s_Te0gaJpZM4Q9Q5p>\n> .\n>\n", "Great :)", "An idea regarding pre-training\r\n```python\r\nfrom tensorflow.contrib.gan.python import namedtuples\r\nfrom tensorflow.python.training import session_run_hook\r\nfrom tensorflow.python.training import training_util\r\nimport tensorflow as tf\r\n\r\n\r\nclass RunTrainOpsHook(session_run_hook.SessionRunHook):\r\n  \"\"\"A hook to run train ops a fixed number of times.\"\"\"\r\n\r\n  def __init__(self, train_ops, pretrain_steps, train_steps):\r\n    \"\"\"Run train ops a certain number of times.\r\n\r\n    Args:\r\n      train_ops: A train op or iterable of train ops to run.\r\n      train_steps: The number of times to run the op(s).\r\n    \"\"\"\r\n    if not isinstance(train_ops, (list, tuple)):\r\n      train_ops = [train_ops]\r\n    self._pretrain_steps = pretrain_steps\r\n    self._train_ops = train_ops\r\n    self._train_steps = train_steps\r\n\r\n  def before_run(self, run_context):\r\n    global_step = training_util.get_or_create_global_step()\r\n    global_step_value = tf.train.global_step(run_context.session, global_step)\r\n    steps = self._pretrain_steps\r\n    if global_step_value > 0:\r\n      steps = self._train_steps\r\n    for _ in range(steps):\r\n      run_context.session.run(self._train_ops)\r\n\r\n\r\ndef get_sequential_train_hooks(pretrain_steps=namedtuples.GANTrainSteps(10, 1),\r\n                               train_steps=namedtuples.GANTrainSteps(1, 1)):\r\n  \"\"\"Returns a hooks function for sequential GAN training.\r\n\r\n  Args:\r\n    train_steps: A `GANTrainSteps` tuple that determines how many generator\r\n      and discriminator training steps to take.\r\n\r\n  Returns:\r\n    A function that takes a GANTrainOps tuple and returns a list of hooks.\r\n  \"\"\"\r\n  def get_hooks(train_ops):\r\n\r\n    generator_hook = RunTrainOpsHook(train_ops.generator_train_op,\r\n                                     pretrain_steps.generator_train_steps,\r\n                                     train_steps.generator_train_steps)\r\n    discriminator_hook = RunTrainOpsHook(\r\n        train_ops.discriminator_train_op,\r\n        pretrain_steps.discriminator_train_steps,\r\n        train_steps.discriminator_train_steps)\r\n    return [generator_hook, discriminator_hook]\r\n  return get_hooks\r\n```\r\nThe `RunTrainOpsHook` allows to set the number of pretraining steps which will be used only for the first step. After that you can configure the actual adverserial training approach e.g. 1/1", "Alternatively a new `GANTrainSteps` class can be created or the current one could be extended with pretrain steps to make it a 4 tuple `((pretrain_gen, pretrain_dis), (train_gen, train_dis))`.", "I looked at the graph of `GANEstimator` in Tensorboard and I have an additional `Discriminator_1`.\r\nIs that to be expected?\r\n\r\n![image](https://user-images.githubusercontent.com/9438971/33876870-026f8cda-df27-11e7-86d1-2766f21574f1.png)\r\n", "1) One discriminator is for the real data, the other is for the fake data. The weights are shared.\r\n2) To support pretraining, how would you feel about making a function in `train.py` called `get_pretraining_sequential_train_steps(pretraining_steps, train_steps)` that runs `pretraining.discriminator_train_steps` pretraining for D, `pretraining.generator_train_steps` pretraining for G, then `train_steps` alternating?", "1. Is there a way to name them more accurately?\r\n2. Where is `get_sequential_train_steps` used right now?", "1) That is the normal variable scope naming mechanism. You could add an extra encapsulating name scope\r\n2) A few places. You should use code search to find them all", "When searching in `tensorflow\\contrib\\gan\\python` I only find it to be defined in `contrib\\gan\\python\\train.py` and never used.\r\n\r\nhttps://github.com/tensorflow/tensorflow/search?utf8=%E2%9C%93&q=get_sequential_train_steps&type=\r\n\r\nAlso returns it only being used in a test", "Regarding 1. in your comment https://github.com/tensorflow/tensorflow/issues/15271#issuecomment-350702852\r\n\r\nWhen altering the train_steps or get_hooks_fn, how would I log the loss for the pretraining steps?", "Is there a way to do gradient clipping with `GANEstimator` already?\r\n\r\nAs far as I can tell `gan_train_ops` in `train.py` takes `kwargs` and passes them to `training.create_train_op` which accepts a `transform_grads_fn` that can perform e.g. gradient clipping.\r\n\r\nIf it is ok for you I add another parameter to `GANEstimator` to allow passing that through to `create_train_op`.", "https://github.com/tensorflow/models/blob/master/research/gan/pix2pix/train.py#L123", "Regarding 3. in your https://github.com/tensorflow/tensorflow/issues/15271#issuecomment-350702852\r\n\r\nThe `global_step` is currently updated non-deterministically so it can happen that while it still runs training the global step already gets updated (not tested for loss calculation but happens for `tf.cond(global_step,...)` in `generator_fn`). Maybe `global_step_inc` should depend on the loss with ` tf.control_dependencies `?", "Is transform_grads_fn already exposed in ganestimator? I can't seem to find it.\r\n\r\nI can also still create a pretraining hook or something similar but I do not know how to test it as your suggested method is not used.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Sorry for the delay.\r\n\r\nI'm open to threading `transform_grads_fn` through the GANEstimator. Would you like to send a PR with that?\r\n\r\nYou should be able to use normal debugging techniques to check that your pretraining `get_hooks_fn` is correct", "@joel-shor has threading for `transform_grads_fn` been added to the GANEstimator yet?"]}, {"number": 15270, "title": "Fix issue #15269", "body": "A possible solution to fix issue #15269.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "@adamcavendish could you sign the CLA?", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@drpngx Thx for reminding me of that", "Jenkins, test this please.", "I'm afraid of this, especially given how unpredictable the variable_scope reuse behavior is. @fchollet  @lukaszkaiser do you have a good enough understanding of this to be confident this does not break things in some obscure way? See more discussion on the issue #15269.", "Thanks for the PR!\r\n\r\n@martinwicke I don't know, but let's find out.\r\n\r\n@adamcavendish without new unit tests, it is impossible to assess the effect of the fix or whether it does fix the issue. Please add a series of unit tests that assert that the new behavior is the one we expect (see discussion). These should be tests that would have been previously failing and would now be passing.", "I second @fchollet : let's first add tests to make clear what's happening.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "@adamcavendish any chances to write the tests?", "@drpngx Sure, but it does change things, especially `variable_scope` is a stable API. Like what I've said in the issue #15269 , it'd change `batch_norm` layer's incremental naming. Therefore, what kind of UT would you expect? Simply adding my issue's example or some further?", "@fchollet and @lukaszkaiser can chime in. You need to show exercise expected behavior.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "My comment is still valid and has not been acted on:\r\n\r\n\"without new unit tests, it is impossible to assess the effect of the fix or whether it does fix the issue. Please add a series of unit tests that assert that the new behavior is the one we expect (see discussion). These should be tests that would have been previously failing and would now be passing.\"\r\n\r\nSince it has been a while, I consider the PR abandoned and I will close the PR. Feel free to reopen later, with unit tests."]}, {"number": 15269, "title": "tf.layers.Layer.set_scope() problem might cause unexpected duplicate name ValueError", "body": "- **OS Platform and Distribution**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version**: 1.4.1\r\n- **Python version**: 3.5.2\r\n- **CUDA/cuDNN version**: CUDA 8.0, CUDNN 7.0.5\r\n- **GPU model and memory**: GTX1080 (8G)\r\n- **Exact command to reproduce**: python3 test.py\r\n- **Have I written custom code**: True\r\n- **Bazel version**: N/A\r\n- **GCC version**: N/A\r\n\r\n\r\nRepoduce code:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n_BATCH_NORM_DECAY = 0.997\r\n_BATCH_NORM_EPSILON = 1e-5\r\n\r\ndef two_batchnorm(inputs):\r\n    with tf.variable_scope('two_batchnorm'):\r\n        inputs = tf.layers.batch_normalization(\r\n            inputs=inputs,\r\n            axis=3,\r\n            momentum=_BATCH_NORM_DECAY,\r\n            epsilon=_BATCH_NORM_EPSILON,\r\n            center=True,\r\n            scale=True,\r\n            training=True,\r\n            fused=True)\r\n        inputs = tf.layers.batch_normalization(\r\n            inputs=inputs,\r\n            axis=3,\r\n            momentum=_BATCH_NORM_DECAY,\r\n            epsilon=_BATCH_NORM_EPSILON,\r\n            center=True,\r\n            scale=True,\r\n            training=True,\r\n            fused=True)\r\n    return inputs\r\n\r\ninputs = tf.placeholder(tf.float32, [1, 5, 5, 3])\r\nx = inputs\r\nx = two_batchnorm(x)\r\nx = two_batchnorm(x)\r\n```\r\n\r\nIt'll trigger an unexpected ValueError as following:\r\n\r\n```\r\nValueError: Variable two_batchnorm/batch_normalization/gamma already exists, disallowed.\r\n```\r\n\r\nRemoving the variable scope `with tf.variable_scope('two_batchnorm')` in `two_batchnorm` will work as expected.\r\n\r\nAll variables defined in the graph should be (in creation sequense):\r\n\r\n```\r\ntwo_batchnorm/batch_normalization/gamma:0\r\ntwo_batchnorm/batch_normalization/beta:0\r\ntwo_batchnorm/batch_normalization/moving_mean:0\r\ntwo_batchnorm/batch_normalization/moving_variance:0\r\ntwo_batchnorm/batch_normalization_1/gamma:0\r\ntwo_batchnorm/batch_normalization_1/beta:0\r\ntwo_batchnorm/batch_normalization_1/moving_mean:0\r\ntwo_batchnorm/batch_normalization_1/moving_variance:0\r\ntwo_batchnorm/batch_normalization_2/gamma:0\r\ntwo_batchnorm/batch_normalization_2/beta:0\r\ntwo_batchnorm/batch_normalization_2/moving_mean:0\r\ntwo_batchnorm/batch_normalization_2/moving_variance:0\r\ntwo_batchnorm/batch_normalization_3/gamma:0\r\ntwo_batchnorm/batch_normalization_3/beta:0\r\ntwo_batchnorm/batch_normalization_3/moving_mean:0\r\ntwo_batchnorm/batch_normalization_3/moving_variance:0\r\n```\r\n\r\nHowever, with `tf.layers.Layer`'s `add_variable` logics, it'll result in an unexpected value name as following (in creation sequence):\r\n\r\n```\r\ntwo_batchnorm/batch_normalization/gamma:0\r\ntwo_batchnorm/batch_normalization/beta:0\r\ntwo_batchnorm/batch_normalization/moving_mean:0\r\ntwo_batchnorm/batch_normalization/moving_variance:0\r\ntwo_batchnorm/batch_normalization_1/gamma:0\r\ntwo_batchnorm/batch_normalization_1/beta:0\r\ntwo_batchnorm/batch_normalization_1/moving_mean:0\r\ntwo_batchnorm/batch_normalization_1/moving_variance:0\r\ntwo_batchnorm/batch_normalization/gamma:0                        <-- ValueError raised here.\r\ntwo_batchnorm/batch_normalization/beta:0\r\ntwo_batchnorm/batch_normalization/moving_mean:0\r\ntwo_batchnorm/batch_normalization/moving_variance:0\r\ntwo_batchnorm/batch_normalization_1/gamma:0\r\ntwo_batchnorm/batch_normalization_1/beta:0\r\ntwo_batchnorm/batch_normalization_1/moving_mean:0\r\ntwo_batchnorm/batch_normalization_1/moving_variance:0\r\n```\r\n\r\nOne solution might be using `self._name` to setup Layer's scope, not `self._base_name`.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nBazel version", "Looks like the butler jumped the gun here.\r\n\r\nThanks for prepping a PR for this. Since it seems like a QOL improvement, I'll mark it as a FR rather than a bug, and wait for input on the PR rather than pinging someone here directly.", "@tensorflowbutler Fields added.\r\n@angersson Since it might cause someone's wrapping functions, i.e. `block` in `resnet` or `densenet` fail to have a function variable scope name, it's not simply a naive issue in my example or so.", "@adamcavendish I think you recreate the variable_scope, which introduces the name conflict:\r\n```python\r\nimport tensorflow as tf\r\n\r\nwith tf.variable_scope(\"a\"):\r\n    v = tf.get_variable(\"v\", [1])\r\n\r\nwith tf.variable_scope(\"a\"):\r\n    v1 = tf.get_variable(\"v\", [1])\r\n```\r\n\r\nPerhaps a better way is to always create new variable_scope when invoked repeatedly,  I mean, use `with tf.variable_scope(None, default_name='two_batchnorm'):` instead.\r\n\r\nBy the way, if I remember correctly, `self._name` starts from one by default, eg: dense_1, dense_2, which might break the existing behavior.", "@facaiy Yep, you're right. It does break the existing behavior. However, we have to break the existing behaviors if we'd like solve the problem.\r\n\r\nI just wonder why it should be an expected behavior, I mean if deliberately, in my example, `two_bn/bn, two_bn/bn_1, two_bn/bn, two_bn/bn_1` should be accepted rather than `two_bn/bn, two_bn/bn_1, two_bn/bn_2, two_bn/bn_3` or `two_bn/bn_1, two_bn/bn_2, two_bn/bn_3, two_bn/bn_4`.", "Do you try \r\n```python\r\nwith tf.variable_scope(None, default_name='two_batchnorm'):\r\n```\r\n\r\nIt might resolve your problem. ", "@facaiy Yep, however, it'll make incrementations on `two_bn`, but not `bn_x`", "@adamcavendish Yes, increment on variable_scope is intentional: it sees function `two_batchnorm` as an unit, and we create two ones here: `two_batchnorm`, and `two_batchnorm_1`.\r\n\r\nI'm afraid that #15270 might also break the `reuse` behavior of `tf.layers`:\r\n[ tf.layers.batch_normalization ](https://www.tensorflow.org/versions/master/api_docs/python/tf/layers/batch_normalization)\r\n> reuse: Boolean, whether to reuse the weights of a previous layer by the same name.\r\n\r\ncc @fchollet who I know might care the issue.", "I think very probably you're right, but I'd leave this issue and PR open for further discussions.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Ping @fchollet , the authority on this issue.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @fchollet: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I've been very busy on my work these days, but I'll figure out how to do it afterwards."]}, {"number": 15268, "title": "correct the misspell of Quantize", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "@103yiran could you sign the CLA?"]}, {"number": 15267, "title": "2017-12-11 17:53:55.834374: F tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: Slice", "body": "System information\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n### System information\r\n- **OS Platform and Distribution **:Linux Ubuntu 16.04\r\n- **TensorFlow installed from **:source\r\n- **TensorFlow version ,installed from source**:1.4\r\n- **Python version**:2.7 \r\n- **Bazel version :1.5\r\n- **GCC/Compiler version**: 5.4.0   20160609\r\n- **cpu version(**:Intel\u00ae Core\u2122 i5-7500 CPU @ 3.40GHz \u00d7 4\r\nCUDA/cuDNN version\r\nN/A\r\nGPU model and memory\r\nN/A\r\n- **Exact command to reproduce**:\r\n\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nuse this command:\r\n\r\nbazel-bin/tensorflow/contrib/lite/toco/toco  \\\r\n--input_file=/home/liu/az/caffe-tensorflow-master/MobileNet/frozen_graph.pb \\\r\n--input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE \\\r\n--output_file=/home/liu/az/caffe-tensorflow-master/MobileNet/mobilenet.lite --inference_type=FLOAT \\\r\n--inference_input_type=FLOAT --input_arrays=img \\\r\n--output_arrays=prob --input_shapes=1,20,20,3\r\n\r\n**I am trying to convert a graph from frozen .pb to .lite format using toco, but I get this error:**\r\n\r\n2017-12-11 17:53:55.833614: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1046] Converting unsupported operation: Pack\r\n2017-12-11 17:53:55.833891: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 40 operators, 58 arrays (0 quantized)\r\n2017-12-11 17:53:55.834169: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 14 operators, 30 arrays (0 quantized)\r\n2017-12-11 17:53:55.834226: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 14 operators, 30 arrays (0 quantized)\r\n2017-12-11 17:53:55.834281: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:312] Total transient array allocated size: 128000 bytes, theoretical optimal value: 128000 bytes.\r\n2017-12-11 17:53:55.834374: F tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: Slice\r\nAborted (core dumped)\r\n\r\nwho can tell me how to get rid of this error?\r\n\r\n### Source code / logs\r\n\r\n# -*- coding: utf-8 -*-\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.lite.python.lite\r\n#\u521b\u5efa\u4e00\u4e2a\u4ea4\u4e92\u5f0fSession\r\nsess = tf.InteractiveSession()\r\n#\u521b\u5efa\u4e24\u4e2a\u5360\u4f4d\u7b26\uff0cx\u4e3a\u8f93\u5165\u7f51\u7edc\u7684\u56fe\u50cf\uff0cy_\u4e3a\u8f93\u5165\u7f51\u7edc\u7684\u56fe\u50cf\u7c7b\u522b\r\nx = tf.placeholder(name=\"img\",dtype=tf.float32, shape=[1,20,20,3])\r\ny_ = tf.placeholder(name=\"output\",dtype=tf.float32, shape=[None, 2])\r\n#\u6743\u91cd\u521d\u59cb\u5316\u51fd\u6570\r\ndef weight_variable(shape):\r\n    #\u8f93\u51fa\u670d\u4ece\u622a\u5c3e\u6b63\u6001\u5206\u5e03\u7684\u968f\u673a\u503c\r\n    initial = tf.truncated_normal(shape, stddev=0.1)\r\n    return tf.Variable(initial)\r\n#\u504f\u7f6e\u521d\u59cb\u5316\u51fd\u6570\r\ndef bias_variable(shape):\r\n    initial = tf.constant(0.1, shape=shape)\r\n    return tf.Variable(initial)\r\n#\u521b\u5efa\u5377\u79efop\r\n#x \u662f\u4e00\u4e2a4\u7ef4\u5f20\u91cf\uff0cshape\u4e3a[batch,height,width,channels]\r\n#\u5377\u79ef\u6838\u79fb\u52a8\u6b65\u957f\u4e3a1\u3002\u586b\u5145\u7c7b\u578b\u4e3aSAME,\u53ef\u4ee5\u4e0d\u4e22\u5f03\u4efb\u4f55\u50cf\u7d20\u70b9\r\ndef conv2d(x, W, type):\r\n    if type == \"SAME\":\r\n        return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding=\"SAME\")\r\n    else:\r\n        return tf.nn.conv2d(x,W, strides=[1,1,1,1],padding=\"VALID\")\r\n\r\n#\u521b\u5efa\u6c60\u5316op\r\n#\u91c7\u7528\u6700\u5927\u6c60\u5316\uff0c\u4e5f\u5c31\u662f\u53d6\u7a97\u53e3\u4e2d\u7684\u6700\u5927\u503c\u4f5c\u4e3a\u7ed3\u679c\r\n#x \u662f\u4e00\u4e2a4\u7ef4\u5f20\u91cf\uff0cshape\u4e3a[batch,height,width,channels]\r\n#ksize\u8868\u793apool\u7a97\u53e3\u5927\u5c0f\u4e3a2x2,\u4e5f\u5c31\u662f\u9ad82\uff0c\u5bbd2\r\n#strides\uff0c\u8868\u793a\u5728height\u548cwidth\u7ef4\u5ea6\u4e0a\u7684\u6b65\u957f\u90fd\u4e3a2\r\ndef max_pool_2x2(x):\r\n    return tf.nn.max_pool(x, ksize=[1,2,2,1],\r\n                          strides=[1,2,2,1], padding=\"SAME\")\r\n#\u7b2c1\u5c42\uff0c\u5377\u79ef\u5c42\r\nW_conv1 = weight_variable([3,3,3,16])\r\nb_conv1 = bias_variable([16])\r\n\r\n#x_image = tf.reshape(x, [-1,20,20,3])\r\nx_image = x;\r\n\r\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1,\"SAME\") + b_conv1)\r\nW_conv2 = weight_variable([3,3,16,64])\r\nb_conv2 = bias_variable([64])\r\nh_conv2 = tf.nn.relu(conv2d(h_conv1, W_conv2,\"SAME\") + b_conv2)\r\nh_pool1 = max_pool_2x2(h_conv2)\r\n\r\n\r\nW_conv3 = weight_variable([3,3,64,64])\r\nb_conv3 = weight_variable([64])\r\nh_conv3 = tf.nn.relu(conv2d(h_pool1, W_conv3,\"SAME\") + b_conv3)\r\n\r\nh_pool2 = max_pool_2x2(h_conv3)\r\n\r\nW_conv4 = weight_variable([2,2,64,16])\r\nb_conv4 = weight_variable([16])\r\nh_conv4 = tf.nn.relu(conv2d(h_pool2, W_conv4,\"VALID\") + b_conv4)\r\n\r\nW_conv5 = weight_variable([3,3,16,2])\r\nb_conv5 = weight_variable([2])\r\nh_conv5 = tf.nn.relu(conv2d(h_conv4, W_conv5,\"VALID\") + b_conv5)\r\n\r\nh_pool3 = max_pool_2x2(h_conv5)\r\n\r\ny_conv = tf.nn.softmax(h_pool3,name=\"prob\")\r\nout = tf.identity(y_conv, name=\"prob\")\r\n#\u9884\u6d4b\u503c\u548c\u771f\u5b9e\u503c\u4e4b\u95f4\u7684\u4ea4\u53c9\u5892\r\ncross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv))\r\n\r\n#train op, \u4f7f\u7528ADAM\u4f18\u5316\u5668\u6765\u505a\u68af\u5ea6\u4e0b\u964d\u3002\u5b66\u4e60\u7387\u4e3a0.0001\r\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\r\n\r\n#\u8bc4\u4f30\u6a21\u578b\uff0ctf.argmax\u80fd\u7ed9\u51fa\u67d0\u4e2atensor\u5bf9\u8c61\u5728\u67d0\u4e00\u7ef4\u4e0a\u6570\u636e\u6700\u5927\u503c\u7684\u7d22\u5f15\u3002\r\n#\u56e0\u4e3a\u6807\u7b7e\u662f\u75310,1\u7ec4\u6210\u4e86one-hot vector\uff0c\u8fd4\u56de\u7684\u7d22\u5f15\u5c31\u662f\u6570\u503c\u4e3a1\u7684\u4f4d\u7f6e\r\ncorrect_predict = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\r\n\r\n#\u8ba1\u7b97\u6b63\u786e\u9884\u6d4b\u9879\u7684\u6bd4\u4f8b\uff0c\u56e0\u4e3atf.equal\u8fd4\u56de\u7684\u662f\u5e03\u5c14\u503c\uff0c\r\n#\u4f7f\u7528tf.cast\u628a\u5e03\u5c14\u503c\u8f6c\u6362\u6210\u6d6e\u70b9\u6570\uff0c\u7136\u540e\u7528tf.reduce_mean\u6c42\u5e73\u5747\u503c\r\naccuracy = tf.reduce_mean(tf.cast(correct_predict, \"float\"))\r\nsaver=tf.train.Saver()\r\n#\u521d\u59cb\u5316\u53d8\u91cf\r\nwith tf.Session() as sess:\r\n    sess.run(tf.initialize_all_variables())\r\n    constant_graph = tf.get_default_graph().as_graph_def()\r\n    with tf.gfile.FastGFile('../MobileNet/' + 'mobile.pb', mode='wb') as f:\r\n        f.write(constant_graph.SerializeToString())\r\n        saver.save(sess, \"../MobileNet/mobile.ckpt\")\r\n    tflite_model = tf.contrib.lite.toco_convert(sess.graph_def, [x], [out])\r\n    open(\"converteds_model.tflite\", \"wb\").write(tflite_model)\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nCUDA/cuDNN version\nGPU model and memory", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 15266, "title": " //tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu_parallel  test fails on ppc64le", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n     Ubuntu 16.04 (ppc64le)\r\n- **TensorFlow installed from (source or binary)**:\r\n      Installed from source (v1.3.1)\r\n- **TensorFlow version (use command below)**:\r\n      TF1.3.1\r\n- **Python version**: \r\n     Python 2.7.5\r\n- **Bazel version (if compiling from source)**:\r\n       0.5.4\r\n- **CUDA/cuDNN version**:\r\n     NA\r\n- **GPU model and memory**:\r\n      NA\r\n- **Exact command to reproduce**:\r\n      bazel test -c opt //tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu_parallel       \r\n\r\n\r\n**Describe the problem**\r\n\r\nHere 2 sub tests are failing on ppc64le i.e.` IsFiniteScalarF32`  and `IsFiniteR1F32s` in file array_elementwise_ops_test.cc\r\n\r\nFor IsFiniteScalarF32 sub-test , error at line 100 : \r\nhttps://github.com/tensorflow/tensorflow/blob/v1.3.1/tensorflow/compiler/xla/tests/array_elementwise_ops_test.cc#L100\r\n`ComputeAndCompareR0<bool>(&builder, false, {});`\r\n The failure due to expected: false  vs actual: true\r\n\r\nFor IsFiniteR1F32s sub-test , error at line 126 :\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.3.1/tensorflow/compiler/xla/tests/array_elementwise_ops_test.cc#L126\r\n`ComputeAndCompareR1<bool>(&builder, {false, true, false, true, false, false}, {});`\r\nThe failure due to expected: {010100}  vs actual: {111100}\r\n\r\nCurrently trying to find the root cause , started debugging further on this. Any inputs/help appreciated.Thanks!\r\n\r\n**Source code / logs**\r\n\r\n1 .**IsFiniteScalarF32 sub-test log :**\r\n```\r\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\r\n-----------------------------------------------------------------------------\r\nNote: This is test shard 5 of 25.\r\n[==========] Running 6 tests from 2 test cases.\r\n[----------] Global test environment set-up.\r\n[----------] 5 tests from ArrayElementwiseOpTest\r\n[ RUN      ] ArrayElementwiseOpTest.IsFiniteScalarF32\r\n2017-12-11 09:36:20.653075: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n2017-12-11 09:36:20.654000: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n2017-12-11 09:36:20.654482: I tensorflow/compiler/xla/service/service.cc:187] XLA service 0x100062a6ea0 executing computations on platform Host. Devices:\r\n2017-12-11 09:36:20.654493: I tensorflow/compiler/xla/service/service.cc:195]   StreamExecutor device (0): <undefined>, <undefined>\r\ntensorflow/compiler/xla/tests/literal_test_util.cc:157: Failure\r\nValue of: Equal(expected, actual)\r\n  Actual: false (expected: false\r\nactual:   true)\r\nExpected: true\r\nexpected:\r\nfalse\r\n        vs actual:\r\ntrue\r\ntensorflow/compiler/xla/tests/literal_test_util.cc:157: Failure\r\nValue of: Equal(expected, actual)\r\n  Actual: false (expected: false\r\nactual:   true)\r\nExpected: true\r\nexpected:\r\nfalse\r\n        vs actual:\r\ntrue\r\n[  FAILED  ] ArrayElementwiseOpTest.IsFiniteScalarF32 (60 ms)\r\n[ RUN      ] ArrayElementwiseOpTest.LogicalNotZeroElement\r\n2017-12-11 09:36:20.712501: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n[       OK ] ArrayElementwiseOpTest.LogicalNotZeroElement (6 ms)\r\n[ RUN      ] ArrayElementwiseOpTest.LogOfPowerF32\r\n2017-12-11 09:36:20.719016: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n[       OK ] ArrayElementwiseOpTest.LogOfPowerF32 (12 ms)\r\n[ RUN      ] ArrayElementwiseOpTest.Max3DAndScalarZeroElementS32s\r\n2017-12-11 09:36:20.731087: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n[       OK ] ArrayElementwiseOpTest.Max3DAndScalarZeroElementS32s (7 ms)\r\n[ RUN      ] ArrayElementwiseOpTest.Compare1DTo2DS32Ne\r\n2017-12-11 09:36:20.738675: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n[       OK ] ArrayElementwiseOpTest.Compare1DTo2DS32Ne (13 ms)\r\n[----------] 5 tests from ArrayElementwiseOpTest (99 ms total)\r\n\r\n[----------] 1 test from ArrayElementwiseOpTestParamCount/ArrayElementwiseOpTestParamCount\r\n[ RUN      ] ArrayElementwiseOpTestParamCount/ArrayElementwiseOpTestParamCount.SquareManyValues/0\r\n2017-12-11 09:36:20.751273: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n[       OK ] ArrayElementwiseOpTestParamCount/ArrayElementwiseOpTestParamCount.SquareManyValues/0 (34 ms)\r\n[----------] 1 test from ArrayElementwiseOpTestParamCount/ArrayElementwiseOpTestParamCount (34 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 6 tests from 2 test cases ran. (133 ms total)\r\n[  PASSED  ] 5 tests.\r\n[  FAILED  ] 1 test, listed below:\r\n[  FAILED  ] ArrayElementwiseOpTest.IsFiniteScalarF32\r\n\r\n 1 FAILED TEST\r\n\r\n```\r\n2. **IsFiniteR1F32s sub-test log:**\r\n\r\n```\r\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\r\n-----------------------------------------------------------------------------\r\nNote: This is test shard 6 of 25.\r\n[==========] Running 6 tests from 2 test cases.\r\n[----------] Global test environment set-up.\r\n[----------] 5 tests from ArrayElementwiseOpTest\r\n[ RUN      ] ArrayElementwiseOpTest.IsFiniteR1F32s\r\n2017-12-11 09:36:21.201704: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n2017-12-11 09:36:21.202563: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n2017-12-11 09:36:21.203145: I tensorflow/compiler/xla/service/service.cc:187] XLA service 0x10015cc6ea0 executing computations on platform Host. Devices:\r\n2017-12-11 09:36:21.203154: I tensorflow/compiler/xla/service/service.cc:195]   StreamExecutor device (0): <undefined>, <undefined>\r\ntensorflow/compiler/xla/tests/literal_test_util.cc:157: Failure\r\nValue of: Equal(expected, actual)\r\n  Actual: false (expected: {010100}\r\nactual:   {111100})\r\nExpected: true\r\nexpected:\r\n{010100}\r\n        vs actual:\r\n{111100}\r\n[  FAILED  ] ArrayElementwiseOpTest.IsFiniteR1F32s (17 ms)\r\n[ RUN      ] ArrayElementwiseOpTest.CompareEqF32s\r\n2017-12-11 09:36:21.218227: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n[       OK ] ArrayElementwiseOpTest.CompareEqF32s (10 ms)\r\n[ RUN      ] ArrayElementwiseOpTest.MulOfExpF32\r\n2017-12-11 09:36:21.228744: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n[       OK ] ArrayElementwiseOpTest.MulOfExpF32 (12 ms)\r\n[ RUN      ] ArrayElementwiseOpTest.Min2DTo1DF32s\r\n2017-12-11 09:36:21.240780: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n[       OK ] ArrayElementwiseOpTest.Min2DTo1DF32s (15 ms)\r\n[ RUN      ] ArrayElementwiseOpTest.Compare1DTo2DS32Ge\r\n2017-12-11 09:36:21.255836: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n[       OK ] ArrayElementwiseOpTest.Compare1DTo2DS32Ge (19 ms)\r\n[----------] 5 tests from ArrayElementwiseOpTest (73 ms total)\r\n\r\n[----------] 1 test from ArrayElementwiseOpTestParamCount/ArrayElementwiseOpTestParamCount\r\n[ RUN      ] ArrayElementwiseOpTestParamCount/ArrayElementwiseOpTestParamCount.SquareManyValues/1\r\n2017-12-11 09:36:21.274589: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n[       OK ] ArrayElementwiseOpTestParamCount/ArrayElementwiseOpTestParamCount.SquareManyValues/1 (73 ms)\r\n[----------] 1 test from ArrayElementwiseOpTestParamCount/ArrayElementwiseOpTestParamCount (73 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 6 tests from 2 test cases ran. (146 ms total)\r\n[  PASSED  ] 5 tests.\r\n[  FAILED  ] 1 test, listed below:\r\n[  FAILED  ] ArrayElementwiseOpTest.IsFiniteR1F32s\r\n\r\n 1 FAILED TEST\r\n\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code", "Hi @tensorflowbutler, I have updated the relevant field :  `Have I written custom code :N/A`", "Continued with the debugging, found following things:\r\n\r\n**For IsFiniteScalarF32 sub-test we are getting incorrect value at line**: \r\nhttps://github.com/tensorflow/tensorflow/blob/v1.3.1/tensorflow/compiler/xla/tests/array_elementwise_ops_test.cc#L99\r\n`auto result = builder.IsFinite(builder.ConstantR0<float>(NAN));`\r\nand https://github.com/tensorflow/tensorflow/blob/v1.3.1/tensorflow/compiler/xla/tests/array_elementwise_ops_test.cc#L104\r\n`auto result_non_canonical = builder.IsFinite(builder.ConstantR0<float>(kNonCanonicalNaN));`\r\n\r\n`builder.ConstantR0<float>(...)` on ppc64le incorrectly returning True instead of False when passed a NaN/kNonCanonicalNaN.\r\n\r\n**For IsFiniteR1F32s sub-test we are getting incorrect value at line** https://github.com/tensorflow/tensorflow/blob/v1.3.1/tensorflow/compiler/xla/tests/array_elementwise_ops_test.cc#L122-L123\r\n```\r\n auto a = builder.ConstantR1<float>(\r\n      {{NAN, 7.0f, kNonCanonicalNaN, -1.0f, inf, -inf}});\r\n```\r\n\r\n`builder.ConstantR1<float>(...) `on ppc64le incorrectly returning True instead of False when passed a NaN/kNonCanonicalNaN.\r\n\r\nLooked at `builder.ConstantR0<float>(...)` implementation, ConstantR0 defined in https://github.com/tensorflow/tensorflow/blob/v1.3.1/tensorflow/compiler/xla/client/computation_builder.h#L829-L831\r\n\r\n```\r\ntemplate <typename NativeT>\r\nComputationDataHandle ComputationBuilder::ConstantR0(NativeT value) {\r\n  return ConstantOp([value](Literal* literal) { literal->PopulateR0(value); });\r\n```\r\n\r\n, going further, goes to ConstantOp([value](Literal* literal) { literal->PopulateR0(value); }); in https://github.com/tensorflow/tensorflow/blob/v1.3.1/tensorflow/compiler/xla/client/computation_builder.cc#L161-L164\r\n\r\n```\r\nComputationDataHandle ComputationBuilder::ConstantOp(\r\n    const PopulateLiteral& populate) {\r\n  if (!first_error_.ok() || !PrepareComputation().ok()) {\r\n    return ComputationDataHandle();\r\n```\r\n\r\nCurrently Putting prints and comparing the results with x86 to find exact difference.\r\n\r\n", "Looks like this returns incorrect value on ppc64le i.e. `builder.ConstantR0<float>(NAN)`\r\n\r\nThe code flow for `builder.ConstantR0<float>(NAN)` :\r\n` builder.ConstantR0<float>(NAN) --> ConstantOp([value](Literal* literal) { literal->PopulateR0(value); });   -->   ParseOpResponse(s, &response)`\r\n\r\nI was looking into the ConstantOp implementation in file  https://github.com/tensorflow/tensorflow/blob/v1.3.1/tensorflow/compiler/xla/client/computation_builder.cc#L161-L180 , this function returns `ParseOpResponse(s, &response)`\r\n\r\nI have added these 2 lines before `ParseOpResponse(s, &response)` (i.e. line 180 & 181)\r\n```\r\nstd::cout << \"\\n Status :  \" << s  << \"\\n\";\r\nstd::cout << \"\\n Response :  \"<< response.output().ShortDebugString() << \" \\n\";\r\n```\r\nOn power and x86 getting same output ;\r\n```\r\nStatus :  OK\r\nResponse : handle: 1\r\n```\r\n\r\nAlso understood that , 'ParseOpResponse' function returns `response->output()`\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.3.1/tensorflow/compiler/xla/client/computation_builder.cc#L50\r\n\r\nI tried to print the value of `response->output()`, but I was getting errors.\r\nUsing below line getting partial output , and that is mismatching with X86:  \r\n```\r\nstd::cout << \"\\n response->mutable_output :  \" << response.mutable_output() << \"\\n\";\r\n```\r\nOutput on ppc64le  : ` response->mutable_output :  0x1002d4d7f70  `\r\nmismatching with x86 :  `response->mutable_output :  0x1c0a730`\r\n\r\nNot sure but looks like this may be the reason of failure. Any comment on this ?\r\nHow to print the entire value of `response->output()` ?\r\n", "I was debugging further and got to know that -\r\nIf we build the TF in debug mode and execute this test, then the test passes on power platform.\r\n\r\n```\r\n$ bazel build  --compilation_mode=dbg //tensorflow/tools/pip_package:build_pip_package\r\n$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n$ pip install /tmp/tensorflow_pkg/tensorflow-1.3.1*\r\n$ bazel test -c opt  //tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu_parallel\r\n\r\nTarget //tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu_parallel up-to-date:\r\n  bazel-bin/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel\r\nINFO: Elapsed time: 0.556s, Critical Path: 0.01s\r\n//tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu_parallel  PASSED in 3.2s\r\n  Stats over 25 runs: max = 3.2s, min = 1.4s, avg = 2.4s, dev = 0.5s\r\n\r\nExecuted 0 out of 1 test: 1 test passes.\r\n\r\n```\r\nShould we debug further on this? Or is it some compiler related issue on power and not the TF code issue? \r\n@gunan and @drpngx Can you please provide your thoughts on this. Thanks!  ", "If there's a discrepancy between debug and opt builds, then a good way is to try asan and msan builds. If that doesn't work, maybe valgrind.", "Thanks for the reply @drpngx , now I 'm trying asan and msan builds.\r\n\r\nAlso I observed that , for opt build as well this test passes using below command :  \r\n`./bazel-bin/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel`", "Could you try --runs_per_test=100 as well?\n\nOn Fri, Dec 29, 2017, 2:02 AM sandipmgiri <notifications@github.com> wrote:\n\n> Thanks for the reply @drpngx <https://github.com/drpngx> , now I 'm\n> trying asan and msan builds.\n>\n> Also I observed that , for opt build as well this test passes using below\n> command :\n>\n> ./bazel-bin/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15266#issuecomment-354424911>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbUMkovHUqrX5-xBZRFN6Z-lu8MSBks5tFLjDgaJpZM4Q9I6N>\n> .\n>\n", "over.\r\n\r\nEom Seong Soo, HonestyandTrust CEO, Outlook<http://aka.ms/weboutlook>.\r\n________________________________\r\n\ubcf4\ub0b8 \uc0ac\ub78c: drpngx <notifications@github.com>\r\n\ubcf4\ub0b8 \ub0a0\uc9dc: 2017\ub144 12\uc6d4 29\uc77c \uae08\uc694\uc77c \uc624\ud6c4 12:20\r\n\ubc1b\ub294 \uc0ac\ub78c: tensorflow/tensorflow\r\n\ucc38\uc870: Subscribed\r\n\uc81c\ubaa9: Re: [tensorflow/tensorflow] //tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu_parallel test fails on ppc64le (#15266)\r\n\r\nCould you try --runs_per_test=100 as well?\r\n\r\nOn Fri, Dec 29, 2017, 2:02 AM sandipmgiri <notifications@github.com> wrote:\r\n\r\n> Thanks for the reply @drpngx <https://github.com/drpngx> , now I 'm\r\n> trying asan and msan builds.\r\n>\r\n> Also I observed that , for opt build as well this test passes using below\r\n> command :\r\n>\r\n> ./bazel-bin/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel\r\n>\r\n> \u2014\r\n> You are receiving this because you were mentioned.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/15266#issuecomment-354424911>,\r\n> or mute the thread\r\n> <https://github.com/notifications/unsubscribe-auth/AT_SbUMkovHUqrX5-xBZRFN6Z-lu8MSBks5tFLjDgaJpZM4Q9I6N>\r\n> .\r\n>\r\n\r\n\u2014\r\nYou are receiving this because you are subscribed to this thread.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/15266#issuecomment-354495692>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AYBxg0ZJnPsdrVhYZW3UZK-vdQlKmAcDks5tFUmcgaJpZM4Q9I6N>.\r\n", "Sorry, I'm not sure what you mean. Did you try?", "Hi @drpngx ,\r\n$   `bazel test -c opt   --runs_per_test=100 //tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu_parallel `\r\n\r\n...\r\nExecuted 1 out of 1 test: 1 fails locally.\r\nThere were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are.\r\n", "OK, great, now that you have a repro case, please try to fix.", "Hi @drpngx , debugging further on this.\r\n\r\nI could build TF using asan option - `bazel build -c opt --copt -fsanitize=address  //tensorflow/tools/pip_package:build_pip_package`\r\nHowever getting some errors while running the test - `bazel test -c opt --copt -fsanitize=address  //tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu_parallel `\r\n\r\n```\r\nINFO: From Compiling external/llvm/lib/Support/BinaryStreamReader.cpp:\r\nexternal/llvm/lib/Support/BinaryStreamReader.cpp: In member function 'llvm::Error llvm::BinaryStreamReader::readCString(llvm::StringRef&)':\r\nexternal/llvm/lib/Support/BinaryStreamReader.cpp:46:12: warning: 'C' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if (*C == '\\0')\r\n            ^\r\nERROR: /tensorflow/tensorflow/compiler/xla/tests/BUILD:451:1: Linking of rule '//tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu_parallel' failed: gcc failed: error executing command /usr/bin/gcc -o bazel-out/local-opt/bin/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel '-Wl,-rpath,$ORIGIN/../../../../_solib_ppc/' -Lbazel-out/local-opt/bin/_solib_ppc -pthread ... (remaining 10 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nbazel-out/local-opt/bin/_solib_ppc/libexternal_Scurl_Slibcurl.so: error: undefined reference to '__asan_report_load2'\r\nbazel-out/local-opt/bin/_solib_ppc/libexternal_Scurl_Slibcurl.so: error: undefined reference to '__asan_report_store2'\r\nbazel-out/local-opt/bin/_solib_ppc/libexternal_Scurl_Slibcurl.so: error: undefined reference to '__asan_stack_malloc_8'\r\nbazel-out/local-opt/bin/_solib_ppc/libexternal_Scurl_Slibcurl.so: error: undefined reference to '__asan_stack_free_8'\r\nbazel-out/local-opt/bin/_solib_ppc/libexternal_Sllvm_Slibipo.so: error: undefined reference to '__asan_poison_memory_region'\r\nbazel-out/local-opt/bin/_solib_ppc/libexternal_Sllvm_Slibipo.so: error: undefined reference to '__asan_unpoison_memory_region'\r\nbazel-out/local-opt/bin/_solib_ppc/libexternal_Sllvm_Slibaarch64_Uasm_Uprinter.so: error: undefined reference to '__asan_load4'\r\nbazel-out/local-opt/bin/_solib_ppc/libexternal_Sllvm_Slibaarch64_Uasm_Uprinter.so: error: undefined reference to '__asan_load8'\r\nbazel-out/local-opt/bin/_solib_ppc/libexternal_Sllvm_Slibaarch64_Uasm_Uprinter.so: error: undefined reference to '__asan_load1'\r\nbazel-out/local-opt/bin/_solib_ppc/libexternal_Sllvm_Slibaarch64_Uasm_Uprinter.so: error: undefined reference to '__asan_load2'\r\nbazel-out/local-opt/bin/_solib_ppc/libexternal_Sllvm_Slibaarch64_Uasm_Uprinter.so: error: undefined reference to '__asan_store1'\r\nbazel-out/local-opt/bin/_solib_ppc/libexternal_Sllvm_Slibscalar.so: error: undefined reference to '__asan_stack_malloc_10'\r\nbazel-out/local-opt/bin/_solib_ppc/libexternal_Sllvm_Slibscalar.so: error: undefined reference to '__asan_stack_free_10'\r\nbazel-out/local-opt/bin/_solib_ppc/libexternal_Sllvm_Slibsupport.so: error: undefined reference to '__asan_stack_malloc_9'\r\nbazel-out/local-opt/bin/_solib_ppc/libexternal_Sllvm_Slibsupport.so: error: undefined reference to '__asan_stack_free_9'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/function_ops_op_lib/tensorflow/core/ops/function_ops.o(.toc+0x0): error: undefined reference to '__asan_option_detect_stack_use_after_return'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/function_ops_op_lib/tensorflow/core/ops/function_ops.o:function_ops.cc:function tensorflow::{lambda(tensorflow::shape_inference::InferenceContext*)#2}::_FUN(tensorflow::shape_inference::InferenceContext*): error: undefined reference to '__asan_report_store8'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/function_ops_op_lib/tensorflow/core/ops/function_ops.o:function_ops.cc:function tensorflow::{lambda(tensorflow::shape_inference::InferenceContext*)#1}::_FUN(tensorflow::shape_inference::InferenceContext*): error: undefined reference to '__asan_report_load8'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/function_ops_op_lib/tensorflow/core/ops/function_ops.o:function_ops.cc:function tensorflow::{lambda(tensorflow::shape_inference::InferenceContext*)#1}::_FUN(tensorflow::shape_inference::InferenceContext*): error: undefined reference to '__asan_report_store8'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/function_ops_op_lib/tensorflow/core/ops/function_ops.o:function_ops.cc:function tensorflow::{lambda(tensorflow::shape_inference::InferenceContext*)#1}::_FUN(tensorflow::shape_inference::InferenceContext*): error: undefined reference to '__asan_report_store8'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/function_ops_op_lib/tensorflow/core/ops/function_ops.o:function_ops.cc:function tensorflow::OpDefBuilder::~OpDefBuilder(): error: undefined reference to '__asan_report_load8'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/function_ops_op_lib/tensorflow/core/ops/function_ops.o:function_ops.cc:function tensorflow::OpDefBuilder::~OpDefBuilder(): error: undefined reference to '__asan_report_load8'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/function_ops_op_lib/tensorflow/core/ops/function_ops.o:function_ops.cc:function tensorflow::OpDefBuilder::~OpDefBuilder(): error: undefined reference to '__asan_report_load8'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/function_ops_op_lib/tensorflow/core/ops/function_ops.o:function_ops.cc:function __static_initialization_and_destruction_0(int, int) [clone .constprop.15]: error: undefined reference to '__asan_before_dynamic_init'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/function_ops_op_lib/tensorflow/core/ops/function_ops.o:function_ops.cc:function __static_initialization_and_destruction_0(int, int) [clone .constprop.15]: error: undefined reference to '__asan_after_dynamic_init'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/function_ops_op_lib/tensorflow/core/ops/function_ops.o:function_ops.cc:function __static_initialization_and_destruction_0(int, int) [clone .constprop.15]: error: undefined reference to '__asan_stack_malloc_3'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/function_ops_op_lib/tensorflow/core/ops/function_ops.o:function_ops.cc:function __static_initialization_and_destruction_0(int, int) [clone .constprop.15]: error: undefined reference to '__asan_stack_free_3'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/function_ops_op_lib/tensorflow/core/ops/function_ops.o:function_ops.cc:function _GLOBAL__sub_D_00099_0_function_ops.cc: error: undefined reference to '__asan_unregister_globals'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/function_ops_op_lib/tensorflow/core/ops/function_ops.o:function_ops.cc:function _GLOBAL__sub_I_00099_1_function_ops.cc: error: undefined reference to '__asan_init_v4'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/function_ops_op_lib/tensorflow/core/ops/function_ops.o:function_ops.cc:function _GLOBAL__sub_I_00099_1_function_ops.cc: error: undefined reference to '__asan_register_globals'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/functional_ops_op_lib/tensorflow/core/ops/functional_ops.o(.toc+0x0): error: undefined reference to '__asan_option_detect_stack_use_after_return'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/functional_ops_op_lib/tensorflow/core/ops/functional_ops.o:functional_ops.cc:function tensorflow::{lambda(tensorflow::shape_inference::InferenceContext*)#1}::_FUN(tensorflow::shape_inference::InferenceContext*): error: undefined reference to '__asan_stack_malloc_2'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/functional_ops_op_lib/tensorflow/core/ops/functional_ops.o:functional_ops.cc:function tensorflow::{lambda(tensorflow::shape_inference::InferenceContext*)#1}::_FUN(tensorflow::shape_inference::InferenceContext*): error: undefined reference to '__asan_stack_free_2'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/functional_ops_op_lib/tensorflow/core/ops/functional_ops.o:functional_ops.cc:function tensorflow::{lambda(tensorflow::shape_inference::InferenceContext*)#1}::_FUN(tensorflow::shape_inference::InferenceContext*): error: undefined reference to '__asan_report_store8'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/functional_ops_op_lib/tensorflow/core/ops/functional_ops.o:functional_ops.cc:function __static_initialization_and_destruction_0(int, int) [clone .constprop.16]: error: undefined reference to '__asan_before_dynamic_init'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/functional_ops_op_lib/tensorflow/core/ops/functional_ops.o:functional_ops.cc:function __static_initialization_and_destruction_0(int, int) [clone .constprop.16]: error: undefined reference to '__asan_after_dynamic_init'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/functional_ops_op_lib/tensorflow/core/ops/functional_ops.o:functional_ops.cc:function __static_initialization_and_destruction_0(int, int) [clone .constprop.16]: error: undefined reference to '__asan_stack_malloc_3'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/functional_ops_op_lib/tensorflow/core/ops/functional_ops.o:functional_ops.cc:function __static_initialization_and_destruction_0(int, int) [clone .constprop.16]: error: undefined reference to '__asan_stack_free_3'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/functional_ops_op_lib/tensorflow/core/ops/functional_ops.o:functional_ops.cc:function _GLOBAL__sub_D_00099_0_functional_ops.cc: error: undefined reference to '__asan_unregister_globals'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/functional_ops_op_lib/tensorflow/core/ops/functional_ops.o:functional_ops.cc:function _GLOBAL__sub_I_00099_1_functional_ops.cc: error: undefined reference to '__asan_init_v4'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/functional_ops_op_lib/tensorflow/core/ops/functional_ops.o:functional_ops.cc:function _GLOBAL__sub_I_00099_1_functional_ops.cc: error: undefined reference to '__asan_register_globals'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/gpu_init/tensorflow/core/common_runtime/gpu/gpu_init.o(.toc+0x0): error: undefined reference to '__asan_option_detect_stack_use_after_return'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/gpu_init/tensorflow/core/common_runtime/gpu/gpu_init.o:gpu_init.cc:function void std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_construct<char const*>(char const*, char const*, std::forward_iterator_tag) [clone .isra.23]: error: undefined reference to '__asan_handle_no_return'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/gpu_init/tensorflow/core/common_runtime/gpu/gpu_init.o:gpu_init.cc:function void std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_construct<char const*>(char const*, char const*, std::forward_iterator_tag) [clone .isra.23]: error: undefined reference to '__asan_report_store1'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/gpu_init/tensorflow/core/common_runtime/gpu/gpu_init.o:gpu_init.cc:function void std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_construct<char const*>(char const*, char const*, std::forward_iterator_tag) [clone .isra.23]: error: undefined reference to '__asan_stack_free_1'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/gpu_init/tensorflow/core/common_runtime/gpu/gpu_init.o:gpu_init.cc:function void std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_construct<char const*>(char const*, char const*, std::forward_iterator_tag) [clone .isra.23]: error: undefined reference to '__asan_stack_malloc_1'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/gpu_init/tensorflow/core/common_runtime/gpu/gpu_init.o:gpu_init.cc:function void std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_construct<char const*>(char const*, char const*, std::forward_iterator_tag) [clone .isra.23]: error: undefined reference to '__asan_report_load1'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/gpu_init/tensorflow/core/common_runtime/gpu/gpu_init.o:gpu_init.cc:function void std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_construct<char const*>(char const*, char const*, std::forward_iterator_tag) [clone .isra.23]: error: undefined reference to '__asan_report_store1'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/gpu_init/tensorflow/core/common_runtime/gpu/gpu_init.o:gpu_init.cc:function _GLOBAL__sub_I__ZN10tensorflow25ValidateGPUMachineManagerEv: error: undefined reference to '__asan_before_dynamic_init'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/gpu_init/tensorflow/core/common_runtime/gpu/gpu_init.o:gpu_init.cc:function _GLOBAL__sub_I__ZN10tensorflow25ValidateGPUMachineManagerEv: error: undefined reference to '__asan_after_dynamic_init'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/gpu_init/tensorflow/core/common_runtime/gpu/gpu_init.o:gpu_init.cc:function perftools::gputools::port::StatusOr<perftools::gputools::Platform*>::ValueOrDie() const: error: undefined reference to '__asan_report_load4'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/gpu_init/tensorflow/core/common_runtime/gpu/gpu_init.o:gpu_init.cc:function perftools::gputools::port::StatusOr<perftools::gputools::Platform*>::ValueOrDie() const: error: undefined reference to '__asan_report_store4'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/gpu_init/tensorflow/core/common_runtime/gpu/gpu_init.o:gpu_init.cc:function perftools::gputools::port::StatusOr<perftools::gputools::Platform*>::ValueOrDie() const: error: undefined reference to '__asan_handle_no_return'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/gpu_init/tensorflow/core/common_runtime/gpu/gpu_init.o:gpu_init.cc:function perftools::gputools::port::StatusOr<perftools::gputools::Platform*>::ValueOrDie() const: error: undefined reference to '__asan_stack_free_3'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/gpu_init/tensorflow/core/common_runtime/gpu/gpu_init.o:gpu_init.cc:function perftools::gputools::port::StatusOr<perftools::gputools::Platform*>::ValueOrDie() const: error: undefined reference to '__asan_stack_malloc_3'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/gpu_init/tensorflow/core/common_runtime/gpu/gpu_init.o:gpu_init.cc:function _GLOBAL__sub_D_00099_0__ZN10tensorflow25ValidateGPUMachineManagerEv: error: undefined reference to '__asan_unregister_globals'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/gpu_init/tensorflow/core/common_runtime/gpu/gpu_init.o:gpu_init.cc:function _GLOBAL__sub_I_00099_1__ZN10tensorflow25ValidateGPUMachineManagerEv: error: undefined reference to '__asan_init_v4'\r\nbazel-out/local-opt/bin/tensorflow/core/_objs/gpu_init/tensorflow/core/common_runtime/gpu/gpu_init.o:gpu_init.cc:function _GLOBAL__sub_I_00099_1__ZN10tensorflow25ValidateGPUMachineManagerEv: error: undefined reference to '__asan_register_globals'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/no_op/tensorflow/core/kernels/no_op.o:no_op.cc:function _GLOBAL__sub_I_no_op.cc: error: undefined reference to '__asan_before_dynamic_init'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/no_op/tensorflow/core/kernels/no_op.o:no_op.cc:function _GLOBAL__sub_I_no_op.cc: error: undefined reference to '__asan_after_dynamic_init'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/no_op/tensorflow/core/kernels/no_op.o:no_op.cc:function _GLOBAL__sub_D_00099_0_no_op.cc: error: undefined reference to '__asan_unregister_globals'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/no_op/tensorflow/core/kernels/no_op.o:no_op.cc:function _GLOBAL__sub_I_00099_1_no_op.cc: error: undefined reference to '__asan_init_v4'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/no_op/tensorflow/core/kernels/no_op.o:no_op.cc:function _GLOBAL__sub_I_00099_1_no_op.cc: error: undefined reference to '__asan_register_globals'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o(.toc+0x0): error: undefined reference to '__asan_option_detect_stack_use_after_return'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o:sendrecv_ops.cc:function tensorflow::OpKernel::IsExpensive(): error: undefined reference to '__asan_report_load1'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o:sendrecv_ops.cc:function tensorflow::GetRendezvousKeyPrefix(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned long long, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&): error: undefined reference to '__asan_stack_malloc_1'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o:sendrecv_ops.cc:function tensorflow::GetRendezvousKeyPrefix(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned long long, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&): error: undefined reference to '__asan_stack_free_1'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o:sendrecv_ops.cc:function tensorflow::GetRendezvousKey(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tensorflow::FrameAndIter const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*): error: undefined reference to '__asan_report_store1'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o:sendrecv_ops.cc:function tensorflow::GetRendezvousKey(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tensorflow::FrameAndIter const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*): error: undefined reference to '__asan_stack_malloc_2'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o:sendrecv_ops.cc:function tensorflow::GetRendezvousKey(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tensorflow::FrameAndIter const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*): error: undefined reference to '__asan_stack_free_2'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o:sendrecv_ops.cc:function std::_Function_handler<void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool), std::_Bind<tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda(std::function<void ()>, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)#1} (std::function<void ()>, std::_Placeholder<1>, tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda(std::function<void ()>, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)#1}<2>, tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda(std::function<void ()>, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)#1}<3>, tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda(std::function<void ()>, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)#1}<4>, tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda(std::function<void ()>, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)#1}<5>)> >::_M_invoke(std::_Any_data const&, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool&&): error: undefined reference to '__asan_report_load1'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o:sendrecv_ops.cc:function std::_Function_handler<void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool), std::_Bind<tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda(std::function<void ()>, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)#1} (std::function<void ()>, std::_Placeholder<1>, tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda(std::function<void ()>, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)#1}<2>, tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda(std::function<void ()>, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)#1}<3>, tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda(std::function<void ()>, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)#1}<4>, tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda(std::function<void ()>, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)#1}<5>)> >::_M_invoke(std::_Any_data const&, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool&&): error: undefined reference to '__asan_report_store1'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o:sendrecv_ops.cc:function std::_Function_handler<void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool), std::_Bind<tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda(std::function<void ()>, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)#1} (std::function<void ()>, std::_Placeholder<1>, tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda(std::function<void ()>, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)#1}<2>, tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda(std::function<void ()>, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)#1}<3>, tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda(std::function<void ()>, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)#1}<4>, tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda(std::function<void ()>, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)#1}<5>)> >::_M_invoke(std::_Any_data const&, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool&&): error: undefined reference to '__asan_handle_no_return'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o:sendrecv_ops.cc:function std::_Function_handler<void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool), std::_Bind<tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda(std::function<void ()>, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)#1} (std::function<void ()>, std::_Placeholder<1>, tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda(std::function<void ()>, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)#1}<2>, tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda(std::function<void ()>, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)#1}<3>, tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda(std::function<void ()>, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)#1}<4>, tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda(std::function<void ()>, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)#1}<5>)> >::_M_invoke(std::_Any_data const&, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool&&): error: undefined reference to '__asan_stack_free_1'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o:sendrecv_ops.cc:function std::_Function_handler<void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool), std::_Bind<tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda(std::function<void ()>, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)#1} (std::function<void ()>, std::_Placeholder<1>, tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda(std::function<void ()>, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)#1}<2>, tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda(std::function<void ()>, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)#1}<3>, tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda(std::function<void ()>, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)#1}<4>, tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda(std::function<void ()>, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)#1}<5>)> >::_M_invoke(std::_Any_data const&, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool&&): error: undefined reference to '__asan_stack_malloc_1'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o:sendrecv_ops.cc:function void std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_construct<char*>(char*, char*, std::forward_iterator_tag) [clone .isra.49]: error: undefined reference to '__asan_handle_no_return'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o:sendrecv_ops.cc:function void std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_construct<char*>(char*, char*, std::forward_iterator_tag) [clone .isra.49]: error: undefined reference to '__asan_stack_free_1'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o:sendrecv_ops.cc:function void std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_construct<char*>(char*, char*, std::forward_iterator_tag) [clone .isra.49]: error: undefined reference to '__asan_stack_malloc_1'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o:sendrecv_ops.cc:function void std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_construct<char*>(char*, char*, std::forward_iterator_tag) [clone .isra.49]: error: undefined reference to '__asan_report_load1'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o:sendrecv_ops.cc:function tensorflow::SendOp::SendOp(tensorflow::OpKernelConstruction*): error: undefined reference to '__asan_report_store4'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o:sendrecv_ops.cc:function tensorflow::SendOp::SendOp(tensorflow::OpKernelConstruction*): error: undefined reference to '__asan_report_load4'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o:sendrecv_ops.cc:function tensorflow::SendOp::SendOp(tensorflow::OpKernelConstruction*): error: undefined reference to '__asan_stack_malloc_3'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o:sendrecv_ops.cc:function tensorflow::SendOp::SendOp(tensorflow::OpKernelConstruction*): error: undefined reference to '__asan_stack_free_3'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o:sendrecv_ops.cc:function tensorflow::SendOp::SendOp(tensorflow::OpKernelConstruction*): error: undefined reference to '__asan_report_store4'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o:sendrecv_ops.cc:function tensorflow::SendOp::SendOp(tensorflow::OpKernelConstruction*): error: undefined reference to '__asan_report_load4'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o:sendrecv_ops.cc:function tensorflow::SendOp::SendOp(tensorflow::OpKernelConstruction*): error: undefined reference to '__asan_report_store4'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o:sendrecv_ops.cc:function tensorflow::SendOp::SendOp(tensorflow::OpKernelConstruction*): error: undefined reference to '__asan_report_load4'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o:sendrecv_ops.cc:function tensorflow::Status tensorflow::errors::Internal<char const*>(char const*): error: undefined reference to '__asan_stack_malloc_2'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o:sendrecv_ops.cc:function tensorflow::Status tensorflow::errors::Internal<char const*>(char const*): error: undefined reference to '__asan_stack_free_2'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o:sendrecv_ops.cc:function tensorflow::SendOp::Compute(tensorflow::OpKernelContext*): error: undefined reference to '__asan_stack_free_4'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o:sendrecv_ops.cc:function tensorflow::SendOp::Compute(tensorflow::OpKernelContext*): error: undefined reference to '__asan_stack_malloc_4'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o:sendrecv_ops.cc:function tensorflow::SendOp::Compute(tensorflow::OpKernelContext*): error: undefined reference to '__asan_report_load_n'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o:sendrecv_ops.cc:function tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>): error: undefined reference to '__asan_stack_free_5'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o:sendrecv_ops.cc:function tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>): error: undefined reference to '__asan_stack_malloc_5'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o:sendrecv_ops.cc:function tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>): error: undefined reference to '__asan_report_store16'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/sendrecv_ops/tensorflow/core/kernels/sendrecv_ops.o:sendrecv_ops.cc:function tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>): error: undefined reference to '__asan_report_load16'\r\nbazel-out/local-opt/bin/tensorflow/compiler/xla/tests/_objs/array_elementwise_ops_test_cpu_parallel/tensorflow/compiler/xla/tests/array_elementwise_ops_test.o:array_elementwise_ops_test.cc:function bool testing::internal::MatchPrintAndExplain<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&>(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, testing::Matcher<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&> const&, testing::MatchResultListener*): error: undefined reference to '__asan_stack_malloc_4'\r\nbazel-out/local-opt/bin/tensorflow/compiler/xla/tests/_objs/array_elementwise_ops_test_cpu_parallel/tensorflow/compiler/xla/tests/array_elementwise_ops_test.o:array_elementwise_ops_test.cc:function bool testing::internal::MatchPrintAndExplain<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&>(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, testing::Matcher<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&> const&, testing::MatchResultListener*): error: undefined reference to '__asan_stack_free_4'\r\nbazel-out/local-opt/bin/tensorflow/compiler/xla/tests/_objs/array_elementwise_ops_test_cpu_parallel/tensorflow/compiler/xla/tests/array_elementwise_ops_test.o:array_elementwise_ops_test.cc:function std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > testing::PrintToString<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&): error: undefined reference to '__asan_stack_malloc_4'\r\nbazel-out/local-opt/bin/tensorflow/compiler/xla/tests/_objs/array_elementwise_ops_test_cpu_parallel/tensorflow/compiler/xla/tests/array_elementwise_ops_test.o:array_elementwise_ops_test.cc:function std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > testing::PrintToString<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&): error: undefined reference to '__asan_stack_free_4'\r\nbazel-out/local-opt/bin/tensorflow/compiler/xla/tests/_objs/array_elementwise_ops_test_cpu_parallel/tensorflow/compiler/xla/tests/array_elementwise_ops_test.o:array_elementwise_ops_test.cc:function testing::AssertionResult testing::internal::CmpHelperEQ<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >(char const*, char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&): error: undefined reference to '__asan_stack_free_2'\r\nbazel-out/local-opt/bin/tensorflow/compiler/xla/tests/_objs/array_elementwise_ops_test_cpu_parallel/tensorflow/compiler/xla/tests/array_elementwise_ops_test.o:array_elementwise_ops_test.cc:function testing::AssertionResult testing::internal::CmpHelperEQ<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >(char const*, char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&): error: undefined reference to '__asan_stack_malloc_2'\r\nbazel-out/local-opt/bin/tensorflow/compiler/xla/tests/_objs/array_elementwise_ops_test_cpu_parallel/tensorflow/compiler/xla/tests/array_elementwise_ops_test.o:array_elementwise_ops_test.cc:function testing::internal::ParameterizedTestCaseInfo<xla::(anonymous namespace)::ArrayElementwiseOpTestParamCount>::RegisterTests(): error: undefined reference to '__asan_stack_free_5'\r\nbazel-out/local-opt/bin/tensorflow/compiler/xla/tests/_objs/array_elementwise_ops_test_cpu_parallel/tensorflow/compiler/xla/tests/array_elementwise_ops_test.o:array_elementwise_ops_test.cc:function testing::internal::ParameterizedTestCaseInfo<xla::(anonymous namespace)::ArrayElementwiseOpTestParamCount>::RegisterTests(): error: undefined reference to '__asan_stack_malloc_5'\r\nbazel-out/local-opt/bin/tensorflow/compiler/xla/tests/_objs/array_elementwise_ops_test_cpu_parallel/tensorflow/compiler/xla/tests/array_elementwise_ops_test.o:array_elementwise_ops_test.cc:function main: error: undefined reference to '__asan_stack_free_5'\r\nbazel-out/local-opt/bin/tensorflow/compiler/xla/tests/_objs/array_elementwise_ops_test_cpu_parallel/tensorflow/compiler/xla/tests/array_elementwise_ops_test.o:array_elementwise_ops_test.cc:function main: error: undefined reference to '__asan_stack_malloc_5'\r\nbazel-out/local-opt/bin/tensorflow/compiler/xla/tests/_objs/array_elementwise_ops_test_cpu_parallel/tensorflow/compiler/xla/tests/array_elementwise_ops_test.o:array_elementwise_ops_test.cc:function xla::(anonymous namespace)::ArrayElementwiseOpTest_Compare1DTo2DS32Ne_Test::TestBody(): error: undefined reference to '__asan_stack_malloc_4'\r\nbazel-out/local-opt/bin/tensorflow/compiler/xla/tests/_objs/array_elementwise_ops_test_cpu_parallel/tensorflow/compiler/xla/tests/array_elementwise_ops_test.o:array_elementwise_ops_test.cc:function xla::(anonymous namespace)::ArrayElementwiseOpTest_Compare1DTo2DS32Ne_Test::TestBody(): error: undefined reference to '__asan_stack_free_4'\r\nbazel-out/local-opt/bin/tensorflow/compiler/xla/tests/_objs/array_elementwise_ops_test_cpu_parallel/tensorflow/compiler/xla/tests/array_elementwise_ops_test.o:array_elementwise_ops_test.cc:function xla::(anonymous namespace)::ArrayElementwiseOpTest_ClampF32ScalarVector_Test::TestBody(): error: undefined reference to '__asan_stack_malloc_5'\r\nbazel-out/local-opt/bin/tensorflow/compiler/xla/tests/_objs/array_elementwise_ops_test_cpu_parallel/tensorflow/compiler/xla/tests/array_elementwise_ops_test.o:array_elementwise_ops_test.cc:function xla::(anonymous namespace)::ArrayElementwiseOpTest_ClampF32ScalarVector_Test::TestBody(): error: undefined reference to '__asan_stack_free_5'\r\nbazel-out/local-opt/bin/tensorflow/compiler/xla/tests/_objs/array_elementwise_ops_test_cpu_parallel/tensorflow/compiler/xla/tests/array_elementwise_ops_test.o:array_elementwise_ops_test.cc:function xla::(anonymous namespace)::ArrayElementwiseOpTest_CannotAddOpaques_Test::TestBody(): error: undefined reference to '__asan_stack_malloc_6'\r\nbazel-out/local-opt/bin/tensorflow/compiler/xla/tests/_objs/array_elementwise_ops_test_cpu_parallel/tensorflow/compiler/xla/tests/array_elementwise_ops_test.o:array_elementwise_ops_test.cc:function xla::(anonymous namespace)::ArrayElementwiseOpTest_CannotAddOpaques_Test::TestBody(): error: undefined reference to '__asan_stack_free_6'\r\nbazel-out/local-opt/bin/tensorflow/compiler/xla/tests/_objs/array_elementwise_ops_test_cpu_parallel/tensorflow/compiler/xla/tests/array_elementwise_ops_test.o:array_elementwise_ops_test.cc:function xla::(anonymous namespace)::ArrayElementwiseOpTest_Min2DTo4DF32s_Test::TestBody(): error: undefined reference to '__asan_stack_free_7'\r\nbazel-out/local-opt/bin/tensorflow/compiler/xla/tests/_objs/array_elementwise_ops_test_cpu_parallel/tensorflow/compiler/xla/tests/array_elementwise_ops_test.o:array_elementwise_ops_test.cc:function xla::(anonymous namespace)::ArrayElementwiseOpTest_Min2DTo4DF32s_Test::TestBody(): error: undefined reference to '__asan_stack_malloc_7'\r\nbazel-out/local-opt/bin/tensorflow/compiler/xla/tests/_objs/array_elementwise_ops_test_cpu_parallel/tensorflow/compiler/xla/tests/array_elementwise_ops_test.o:array_elementwise_ops_test.cc:function xla::(anonymous namespace)::ArrayElementwiseOpTest_4DBinaryOpF32s_Test::TestBody(): error: undefined reference to '__asan_stack_malloc_7'\r\nbazel-out/local-opt/bin/tensorflow/compiler/xla/tests/_objs/array_elementwise_ops_test_cpu_parallel/tensorflow/compiler/xla/tests/array_elementwise_ops_test.o:array_elementwise_ops_test.cc:function xla::(anonymous namespace)::ArrayElementwiseOpTest_4DBinaryOpF32s_Test::TestBody(): error: undefined reference to '__asan_stack_free_7'\r\nbazel-out/local-opt/bin/tensorflow/compiler/xla/tests/_objs/array_elementwise_ops_test_cpu_parallel/tensorflow/compiler/xla/tests/array_elementwise_ops_test.o:array_elementwise_ops_test.cc:function xla::(anonymous namespace)::ArrayElementwiseOpTest_R4PlusR1InDim1_Test::TestBody(): error: undefined reference to '__asan_stack_malloc_7'\r\nbazel-out/local-opt/bin/tensorflow/compiler/xla/tests/_objs/array_elementwise_ops_test_cpu_parallel/tensorflow/compiler/xla/tests/array_elementwise_ops_test.o:array_elementwise_ops_test.cc:function xla::(anonymous namespace)::ArrayElementwiseOpTest_R4PlusR1InDim1_Test::TestBody(): error: undefined reference to '__asan_stack_free_7'\r\nbazel-out/local-opt/bin/tensorflow/compiler/xla/tests/_objs/array_elementwise_ops_test_cpu_parallel/tensorflow/compiler/xla/tests/array_elementwise_ops_test.o:array_elementwise_ops_test.cc:function std::_Function_handler<void (xla::Literal*), xla::ComputationDataHandle xla::ComputationBuilder::ConstantR2<float>(std::initializer_list<std::initializer_list<float> >)::{lambda(xla::Literal*)#1}>::_M_invoke(std::_Any_data const&, xla::Literal*&&): error: undefined reference to '__asan_report_load16'\r\nbazel-out/local-opt/bin/tensorflow/compiler/xla/tests/_objs/array_elementwise_ops_test_cpu_parallel/tensorflow/compiler/xla/tests/array_elementwise_ops_test.o:array_elementwise_ops_test.cc:function xla::(anonymous namespace)::ArrayElementwiseOpTest_Max3DAndScalarS32s_Test::TestBody(): error: undefined reference to '__asan_stack_malloc_6'\r\nbazel-out/local-opt/bin/tensorflow/compiler/xla/tests/_objs/array_elementwise_ops_test_cpu_parallel/tensorflow/compiler/xla/tests/array_elementwise_ops_test.o:array_elementwise_ops_test.cc:function xla::(anonymous namespace)::ArrayElementwiseOpTest_Max3DAndScalarS32s_Test::TestBody(): error: undefined reference to '__asan_stack_free_6'\r\nbazel-out/local-opt/bin/tensorflow/compiler/xla/tests/_objs/array_elementwise_ops_test_cpu_parallel/tensorflow/compiler/xla/tests/array_elementwise_ops_test.o:array_elementwise_ops_test.cc:function xla::(anonymous namespace)::ArrayElementwiseOpTest_Add1DTo3DTwoWaysOver2_Test::TestBody(): error: undefined reference to '__asan_stack_malloc_6'\r\nbazel-out/local-opt/bin/tensorflow/compiler/xla/tests/_objs/array_elementwise_ops_test_cpu_parallel/tensorflow/compiler/xla/tests/array_elementwise_ops_test.o:array_elementwise_ops_test.cc:function xla::(anonymous namespace)::ArrayElementwiseOpTest_Add1DTo3DTwoWaysOver2_Test::TestBody(): error: undefined reference to '__asan_stack_free_6'\r\nbazel-out/local-opt/bin/tensorflow/compiler/xla/tests/_objs/array_elementwise_ops_test_cpu_parallel/tensorflow/compiler/xla/tests/array_elementwise_ops_test.o:array_elementwise_ops_test.cc:function xla::(anonymous namespace)::ArrayElementwiseOpTest_Add1DTo3DTwoWaysOver0_Test::TestBody(): error: undefined reference to '__asan_stack_malloc_6'\r\nbazel-out/local-opt/bin/tensorflow/compiler/xla/tests/_objs/array_elementwise_ops_test_cpu_parallel/tensorflow/compiler/xla/tests/array_elementwise_ops_test.o:array_elementwise_ops_test.cc:function xla::(anonymous namespace)::ArrayElementwiseOpTest_Add1DTo3DTwoWaysOver0_Test::TestBody(): error: undefined reference to '__asan_stack_free_6'\r\nbazel-out/local-opt/bin/tensorflow/compiler/xla/tests/_objs/array_elementwise_ops_test_cpu_parallel/tensorflow/compiler/xla/tests/array_elementwise_ops_test.o:array_elementwise_ops_test.cc:function std::_Function_handler<void (xla::Literal*), xla::ComputationDataHandle xla::ComputationBuilder::ConstantR2<int>(std::initializer_list<std::initializer_list<int> >)::{lambda(xla::Literal*)#1}>::_M_invoke(std::_Any_data const&, xla::Literal*&&): error: undefined reference to '__asan_report_load16'\r\nbazel-out/local-opt/bin/tensorflow/core/platform/cloud/_objs/gcs_file_system/tensorflow/core/platform/cloud/gcs_file_system.o:gcs_file_system.cc:function std::_Function_base::_Base_manager<tensorflow::(anonymous namespace)::GcsWritableFile::SyncImpl()::{lambda()#1}>::_M_manager(std::_Any_data&, std::_Function_base::_Base_manager<tensorflow::(anonymous namespace)::GcsWritableFile::SyncImpl()::{lambda()#1}> const&, std::_Manager_operation): error: undefined reference to '__asan_report_store_n'\r\nbazel-out/local-opt/bin/tensorflow/core/platform/cloud/_objs/gcs_file_system/tensorflow/core/platform/cloud/gcs_file_system.o:gcs_file_system.cc:function std::_Function_base::_Base_manager<tensorflow::(anonymous namespace)::GcsWritableFile::SyncImpl()::{lambda()#1}>::_M_manager(std::_Any_data&, std::_Function_base::_Base_manager<tensorflow::(anonymous namespace)::GcsWritableFile::SyncImpl()::{lambda()#1}> const&, std::_Manager_operation): error: undefined reference to '__asan_report_load_n'\r\nbazel-out/local-opt/bin/tensorflow/core/platform/cloud/_objs/gcs_file_system/tensorflow/core/platform/cloud/gcs_file_system.o:gcs_file_system.cc:function std::_Function_base::_Base_manager<std::_Bind<std::_Mem_fn<tensorflow::Status (tensorflow::GcsFileSystem::*)(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)> (tensorflow::GcsFileSystem*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)> >::_M_manager(std::_Any_data&, std::_Any_data const&, std::_Manager_operation): error: undefined reference to '__asan_report_store16'\r\nbazel-out/local-opt/bin/tensorflow/core/platform/cloud/_objs/gcs_file_system/tensorflow/core/platform/cloud/gcs_file_system.o:gcs_file_system.cc:function std::_Function_base::_Base_manager<std::_Bind<std::_Mem_fn<tensorflow::Status (tensorflow::GcsFileSystem::*)(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)> (tensorflow::GcsFileSystem*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)> >::_M_manager(std::_Any_data&, std::_Any_data const&, std::_Manager_operation): error: undefined reference to '__asan_report_load16'\r\nbazel-out/local-opt/bin/tensorflow/core/platform/cloud/_objs/gcs_file_system/tensorflow/core/platform/cloud/gcs_file_system.o:gcs_file_system.cc:function tensorflow::GcsFileSystem::NewRandomAccessFile(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unique_ptr<tensorflow::RandomAccessFile, std::default_delete<tensorflow::RandomAccessFile> >*): error: undefined reference to '__asan_report_store_n'\r\nbazel-out/local-opt/bin/tensorflow/core/platform/cloud/_objs/gcs_file_system/tensorflow/core/platform/cloud/gcs_file_system.o:gcs_file_system.cc:function std::function<tensorflow::Status ()>::function<std::_Bind<std::_Mem_fn<tensorflow::Status (tensorflow::GcsFileSystem::*)(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)> (tensorflow::GcsFileSystem*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)>, void, void>(std::_Bind<std::_Mem_fn<tensorflow::Status (tensorflow::GcsFileSystem::*)(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)> (tensorflow::GcsFileSystem*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)>): error: undefined reference to '__asan_report_store16'\r\nbazel-out/local-opt/bin/tensorflow/core/platform/cloud/_objs/gcs_file_system/tensorflow/core/platform/cloud/gcs_file_system.o:gcs_file_system.cc:function tensorflow::GcsFileSystem::Stat(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tensorflow::FileStatistics*): error: undefined reference to '__asan_report_load_n'\r\nbazel-out/local-opt/bin/tensorflow/core/platform/cloud/_objs/gcs_file_system/tensorflow/core/platform/cloud/gcs_file_system.o:gcs_file_system.cc:function tensorflow::GcsFileSystem::Stat(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tensorflow::FileStatistics*): error: undefined reference to '__asan_report_store_n'\r\nbazel-out/local-opt/bin/tensorflow/core/platform/cloud/_objs/gcs_file_system/tensorflow/core/platform/cloud/gcs_file_system.o:gcs_file_system.cc:function tensorflow::GcsFileSystem::Stat(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tensorflow::FileStatistics*): error: undefined reference to '__asan_report_load_n'\r\nbazel-out/local-opt/bin/tensorflow/core/platform/cloud/_objs/gcs_file_system/tensorflow/core/platform/cloud/gcs_file_system.o:gcs_file_system.cc:function tensorflow::GcsFileSystem::Stat(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tensorflow::FileStatistics*): error: undefined reference to '__asan_report_store_n'\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu_parallel failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 51.493s, Critical Path: 46.34s\r\n\r\nExecuted 0 out of 1 test: 1 fails to build.\r\n\r\n```\r\n\r\n@drpngx , I have never used asan: can you please give me some indication on how to debug with asan?\r\n\r\n ```\r\n$ gcc --version\r\ngcc (Ubuntu/IBM 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n```\r\n", "You also need it as linkopt, see an example [here](https://github.com/bazelment/trunk/blob/master/tools/bazel.rc).", "I could build and run the test using Address Sanitizer , however we are not getting any memory related information , getting the same output as we got without this option.\r\n\r\nCommands which I used : \r\n```\r\n$ bazel build -c opt --copt -fsanitize=address --linkopt -fsanitize=address //tensorflow/tools/pip_package:build_pip_package\r\n$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n$ pip install /tmp/tensorflow_pkg/tensorflow-1.*\r\n$ bazel test -c opt --copt -fsanitize=address --linkopt -fsanitize=address  //tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu_parallel\r\n```\r\n  ", "Also tried with valgrind , see below \r\n```\r\n $ bazel test -c opt --test_output=errors --run_under=valgrind   //tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu_parallel\r\n\r\nINFO: Found 1 test target...\r\nFAIL: //tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu_parallel (shard 1 of 25) (see /root/.cache/bazel/_bazel_root/5f017a062de46af0cfc4acf7d455b107/execroot/org_tensorflow/bazel-out/local-opt/testlogs/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel/shard_1_of_25/test.log).\r\nINFO: From Testing //tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu_parallel (shard 1 of 25):\r\n==================== Test output for //tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu_parallel (shard 1 of 25):\r\n==15591== Memcheck, a memory error detector\r\n==15591== Copyright (C) 2002-2015, and GNU GPL'd, by Julian Seward et al.\r\n==15591== Using Valgrind-3.12.0 and LibVEX; rerun with -h for copyright info\r\n==15591== Command: tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel --xla_backend_extra_options=xla_cpu_parallel\r\n==15591==\r\nNote: This is test shard 1 of 25.\r\n[==========] Running 1 test from 1 test case.\r\n[----------] Global test environment set-up.\r\n[----------] 1 test from ArrayElementwiseOpTest\r\n[ RUN      ] ArrayElementwiseOpTest.IsFiniteScalarF32\r\n2018-01-03 04:39:48.886316: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n2018-01-03 04:39:49.051276: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n2018-01-03 04:39:49.119867: I tensorflow/compiler/xla/service/service.cc:187] XLA service 0xb8b9e90 executing computations on platform Host. Devices:\r\n2018-01-03 04:39:49.122534: I tensorflow/compiler/xla/service/service.cc:195]   StreamExecutor device (0): <undefined>, <undefined>\r\n \r\ntensorflow/compiler/xla/tests/literal_test_util.cc:159: Failure\r\nValue of: Equal(expected, actual)\r\n  Actual: false (expected: false\r\nactual:   true)\r\nExpected: true\r\n**expected:false vs actual:true**\r\n[  FAILED  ] ArrayElementwiseOpTest.IsFiniteScalarF32 (6582 ms)\r\n[----------] 1 test from ArrayElementwiseOpTest (6593 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 1 test from 1 test case ran. (6632 ms total)\r\n[  PASSED  ] 0 tests.\r\n[  FAILED  ] 1 test, listed below:\r\n[  FAILED  ] ArrayElementwiseOpTest.IsFiniteScalarF32\r\n\r\n 1 FAILED TEST\r\n==15591==\r\n==15591== HEAP SUMMARY:\r\n==15591==     in use at exit: 1,214,571 bytes in 9,223 blocks\r\n==15591==   total heap usage: 28,854 allocs, 19,631 frees, 3,839,455 bytes allocated\r\n==15591==\r\n==15591== LEAK SUMMARY:\r\n==15591==    definitely lost: 0 bytes in 0 blocks\r\n==15591==    indirectly lost: 0 bytes in 0 blocks\r\n==15591==      possibly lost: 21,760 bytes in 34 blocks\r\n==15591==    still reachable: 1,192,811 bytes in 9,189 blocks\r\n==15591==                       of which reachable via heuristic:\r\n==15591==                         stdstring          : 106,077 bytes in 2,191 blocks\r\n==15591==                         newarray           : 3,576 bytes in 20 blocks\r\n==15591==         suppressed: 0 bytes in 0 blocks\r\n==15591== Rerun with --leak-check=full to see details of leaked memory\r\n==15591==\r\n==15591== For counts of detected and suppressed errors, rerun with: -v\r\n==15591== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)\r\n================================================================================\r\nTarget //tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu_parallel up-to-date:\r\n  bazel-bin/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel\r\nINFO: Elapsed time: 23.308s, Critical Path: 22.98s\r\nFAILED: //tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu_parallel (see /root/.cache/bazel/_bazel_root/5f017a062de46af0cfc4acf7d455b107/execroot/org_tensorflow/bazel-out/local-opt/testlogs/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel/shard_1_of_25/test.log /root/.cache/bazel/_bazel_root/5f017a062de46af0cfc4acf7d455b107/execroot/org_tensorflow/bazel-out/local-opt/testlogs/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel/shard_6_of_25/test.log /root/.cache/bazel/_bazel_root/5f017a062de46af0cfc4acf7d455b107/execroot/org_tensorflow/bazel-out/local-opt/testlogs/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel/shard_20_of_25/test.log /root/.cache/bazel/_bazel_root/5f017a062de46af0cfc4acf7d455b107/execroot/org_tensorflow/bazel-out/local-opt/testlogs/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel/shard_2_of_25/test.log /root/.cache/bazel/_bazel_root/5f017a062de46af0cfc4acf7d455b107/execroot/org_tensorflow/bazel-out/local-opt/testlogs/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel/shard_3_of_25/test.log /root/.cache/bazel/_bazel_root/5f017a062de46af0cfc4acf7d455b107/execroot/org_tensorflow/bazel-out/local-opt/testlogs/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel/shard_10_of_25/test.log /root/.cache/bazel/_bazel_root/5f017a062de46af0cfc4acf7d455b107/execroot/org_tensorflow/bazel-out/local-opt/testlogs/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel/shard_4_of_25/test.log /root/.cache/bazel/_bazel_root/5f017a062de46af0cfc4acf7d455b107/execroot/org_tensorflow/bazel-out/local-opt/testlogs/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel/shard_16_of_25/test.log /root/.cache/bazel/_bazel_root/5f017a062de46af0cfc4acf7d455b107/execroot/org_tensorflow/bazel-out/local-opt/testlogs/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel/shard_8_of_25/test.log /root/.cache/bazel/_bazel_root/5f017a062de46af0cfc4acf7d455b107/execroot/org_tensorflow/bazel-out/local-opt/testlogs/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel/shard_9_of_25/test.log /root/.cache/bazel/_bazel_root/5f017a062de46af0cfc4acf7d455b107/execroot/org_tensorflow/bazel-out/local-opt/testlogs/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel/shard_7_of_25/test.log /root/.cache/bazel/_bazel_root/5f017a062de46af0cfc4acf7d455b107/execroot/org_tensorflow/bazel-out/local-opt/testlogs/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel/shard_14_of_25/test.log /root/.cache/bazel/_bazel_root/5f017a062de46af0cfc4acf7d455b107/execroot/org_tensorflow/bazel-out/local-opt/testlogs/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel/shard_5_of_25/test.log /root/.cache/bazel/_bazel_root/5f017a062de46af0cfc4acf7d455b107/execroot/org_tensorflow/bazel-out/local-opt/testlogs/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel/shard_25_of_25/test.log /root/.cache/bazel/_bazel_root/5f017a062de46af0cfc4acf7d455b107/execroot/org_tensorflow/bazel-out/local-opt/testlogs/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel/shard_23_of_25/test.log /root/.cache/bazel/_bazel_root/5f017a062de46af0cfc4acf7d455b107/execroot/org_tensorflow/bazel-out/local-opt/testlogs/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel/shard_21_of_25/test.log /root/.cache/bazel/_bazel_root/5f017a062de46af0cfc4acf7d455b107/execroot/org_tensorflow/bazel-out/local-opt/testlogs/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel/shard_22_of_25/test.log /root/.cache/bazel/_bazel_root/5f017a062de46af0cfc4acf7d455b107/execroot/org_tensorflow/bazel-out/local-opt/testlogs/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel/shard_13_of_25/test.log /root/.cache/bazel/_bazel_root/5f017a062de46af0cfc4acf7d455b107/execroot/org_tensorflow/bazel-out/local-opt/testlogs/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel/shard_11_of_25/test.log /root/.cache/bazel/_bazel_root/5f017a062de46af0cfc4acf7d455b107/execroot/org_tensorflow/bazel-out/local-opt/testlogs/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel/shard_19_of_25/test.log /root/.cache/bazel/_bazel_root/5f017a062de46af0cfc4acf7d455b107/execroot/org_tensorflow/bazel-out/local-opt/testlogs/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel/shard_18_of_25/test.log /root/.cache/bazel/_bazel_root/5f017a062de46af0cfc4acf7d455b107/execroot/org_tensorflow/bazel-out/local-opt/testlogs/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel/shard_12_of_25/test.log /root/.cache/bazel/_bazel_root/5f017a062de46af0cfc4acf7d455b107/execroot/org_tensorflow/bazel-out/local-opt/testlogs/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel/shard_17_of_25/test.log /root/.cache/bazel/_bazel_root/5f017a062de46af0cfc4acf7d455b107/execroot/org_tensorflow/bazel-out/local-opt/testlogs/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel/shard_24_of_25/test.log /root/.cache/bazel/_bazel_root/5f017a062de46af0cfc4acf7d455b107/execroot/org_tensorflow/bazel-out/local-opt/testlogs/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel/shard_15_of_25/test.log)\r\nINFO: From Testing //tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu_parallel\r\n==================== Test output for //tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu_parallel:\r\n==15591== Memcheck, a memory error detector\r\n==15591== Copyright (C) 2002-2015, and GNU GPL'd, by Julian Seward et al.\r\n==15591== Using Valgrind-3.12.0 and LibVEX; rerun with -h for copyright info\r\n==15591== Command: tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel --xla_backend_extra_options=xla_cpu_parallel\r\n==15591==\r\nNote: This is test shard 1 of 25.\r\n[==========] Running 1 test from 1 test case.\r\n[----------] Global test environment set-up.\r\n[----------] 1 test from ArrayElementwiseOpTest\r\n[ RUN      ] ArrayElementwiseOpTest.IsFiniteScalarF32\r\n2018-01-03 04:39:48.886316: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n2018-01-03 04:39:49.051276: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n2018-01-03 04:39:49.119867: I tensorflow/compiler/xla/service/service.cc:187] XLA service 0xb8b9e90 executing computations on platform Host. Devices:\r\n2018-01-03 04:39:49.122534: I tensorflow/compiler/xla/service/service.cc:195]   StreamExecutor device (0): <undefined>, <undefined>\r\n\r\ntensorflow/compiler/xla/tests/literal_test_util.cc:159: Failure\r\nValue of: Equal(expected, actual)\r\n  Actual: false (expected: false\r\nactual:   true)\r\nExpected: true\r\n**expected: false    vs actual:true**\r\n[  FAILED  ] ArrayElementwiseOpTest.IsFiniteScalarF32 (6582 ms)\r\n[----------] 1 test from ArrayElementwiseOpTest (6593 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 1 test from 1 test case ran. (6632 ms total)\r\n[  PASSED  ] 0 tests.\r\n[  FAILED  ] 1 test, listed below:\r\n[  FAILED  ] ArrayElementwiseOpTest.IsFiniteScalarF32\r\n\r\n 1 FAILED TEST\r\n==15591==\r\n==15591== HEAP SUMMARY:\r\n==15591==     in use at exit: 1,214,571 bytes in 9,223 blocks\r\n==15591==   total heap usage: 28,854 allocs, 19,631 frees, 3,839,455 bytes allocated\r\n==15591==\r\n==15591== LEAK SUMMARY:\r\n==15591==    definitely lost: 0 bytes in 0 blocks\r\n==15591==    indirectly lost: 0 bytes in 0 blocks\r\n==15591==      possibly lost: 21,760 bytes in 34 blocks\r\n==15591==    still reachable: 1,192,811 bytes in 9,189 blocks\r\n==15591==                       of which reachable via heuristic:\r\n==15591==                         stdstring          : 106,077 bytes in 2,191 blocks\r\n==15591==                         newarray           : 3,576 bytes in 20 blocks\r\n==15591==         suppressed: 0 bytes in 0 blocks\r\n==15591== Rerun with --leak-check=full to see details of leaked memory\r\n==15591==\r\n==15591== For counts of detected and suppressed errors, rerun with: -v\r\n==15591== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)\r\n================================================================================\r\n//tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu_parallel (24/25 cached) FAILED in 1 out of 25 in 23.0s\r\n  Stats over 25 runs: max = 23.0s, min = 12.3s, avg = 14.5s, dev = 1.9s\r\n  /root/.cache/bazel/_bazel_root/5f017a062de46af0cfc4acf7d455b107/execroot/org_tensorflow/bazel-out/local-opt/testlogs/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel/shard_1_of_25/test.log\r\n\r\nExecuted 1 out of 1 test: 1 fails locally.\r\nThere were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are.\r\n\r\n```", "Also observed that even if we build TF in opt mode , then also test passes on ppc64le using command : `./bazel-out/local-opt/bin/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel `\r\n\r\nBut it fails only when we run with \"bazel test\" i.e. `bazel test -c opt //tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu_parallel`\r\n\r\nI tried but couldn't find the reason. \r\n@gunan and @drpngx  - can you please share your comments on this. What would be the reason? \r\n\r\nThanks!\r\n  ", "Investigated further on this and found the reason , see below details :\r\n\r\nWhile running the test using : `./bazel-out/local-opt/bin/tensorflow/compiler/xla/tests/array_elementwise_ops_test_cpu_parallel` , no `--xla_backend_extra_options` is being set.  Thats why the test is passing.\r\n\r\nHowever, while running the test using \"`bazel test`\" command , the `'--xla_backend_extra_options=xla_cpu_parallel'` option is being set automatically, and test is failing.\r\n\r\nWhich is found in file https://github.com/tensorflow/tensorflow/blob/v1.3.1/tensorflow/compiler/xla/tests/build_defs.bzl#L120\r\n `this_backend_args += [\"--xla_backend_extra_options=\\\"xla_cpu_parallel\\\"\"]`\r\n\r\nI tried removing this line , and test passes with \"`bazel test`\" as well (as no `--xla_backend_extra_options` is being set).\r\n\r\n\r\nNow here, as per my opinion below two options to fix this test :\r\n\r\n1) Add `ppc64le` specific condition in file https://github.com/tensorflow/tensorflow/blob/v1.3.1/tensorflow/compiler/xla/tests/build_defs.bzl#L120, something like this :\r\n\r\n  ```\r\n if !(ppc64le):\r\n     this_backend_args += [\"--xla_backend_extra_options=\\\"xla_cpu_parallel\\\"\"]     \r\n   endif\r\n```\r\n\r\n\r\n\r\n2) While running the test using \"`bazel test`\" command, disable `'--xla_backend_extra_options=xla_cpu_parallel'` option.\r\n\r\n\r\n@gunan and @drpngx - Which one according to you is the better option? Also please provide if you have any other suggestions/comments.Thanks!\r\n", "It sounds like option 1 is better, but, it basically means that the `xla_cpu_parallel` doesn't work on ppc64le. That's the root cause that should be fixed, ideally. I don't see why (in theory) this shouldn't work on ppc64le.", "Ran below XLA tests to check `xla_cpu_parallel` works on ppc64le or not -\r\nLooks like in compiler/xla module , executed each test with and without `xla_cpu_parallel `:\r\n\r\n```\r\n//tensorflow/compiler/xla/tests:axpy_simple_test_cpu   //  Executed without xla_cpu_parallel and PASSED\r\n//tensorflow/compiler/xla/tests:axpy_simple_test_cpu_parallel     //  Executed with xla_cpu_parallel and PASSED\r\n\r\n//tensorflow/compiler/xla/tests:bad_rng_shape_validation_test_cpu    //  Executed without xla_cpu_parallel and PASSED\r\n//tensorflow/compiler/xla/tests:bad_rng_shape_validation_test_cpu_parallel  //  Executed with xla_cpu_parallel and PASSED\r\n\r\n//tensorflow/compiler/xla/tests:batch_normalization_test_cpu       //  Executed without xla_cpu_parallel and PASSED\r\n//tensorflow/compiler/xla/tests:batch_normalization_test_cpu_parallel  //  Executed with xla_cpu_parallel and PASSED\r\n\r\n//tensorflow/compiler/xla/tests:binop_scaling_test_cpu            //  Executed without xla_cpu_parallel and PASSED\r\n//tensorflow/compiler/xla/tests:binop_scaling_test_cpu_parallel   //  Executed with xla_cpu_parallel and PASSED\r\n\r\n//tensorflow/compiler/xla/tests:broadcast_simple_test_cpu        //  Executed without xla_cpu_parallel and PASSED\r\n//tensorflow/compiler/xla/tests:broadcast_simple_test_cpu_parallel  //  Executed with xla_cpu_parallel and PASSED\r\n\r\n//tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu    //  Executed without xla_cpu_parallel and PASSED\r\n//tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu_parallel  //  Executed with xla_cpu_parallel and FAILED\r\n```\r\n\r\nOnly `//tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu_parallel `test fails when we run with `xla_cpu_parallel`.\r\n  \r\n  \r\n  \r\n  \r\n  ", "Do you happen to know which test fails in particular? It should print it out.", "Here 2 sub tests are failing on ppc64le i.e. `IsFiniteScalarF32` and` IsFiniteR1F32s` in file array_elementwise_ops_test.cc\r\n\r\nFor` IsFiniteScalarF32` sub-test , error at line 100 :\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.3.1/tensorflow/compiler/xla/tests/array_elementwise_ops_test.cc#L100\r\n`ComputeAndCompareR0<bool>(&builder, false, {});` // The failure due to expected: false vs actual: true\r\n\r\nFor `IsFiniteR1F32s `sub-test , error at line 126 :\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.3.1/tensorflow/compiler/xla/tests/array_elementwise_ops_test.cc#L126\r\n`ComputeAndCompareR1<bool>(&builder, {false, true, false, true, false, false}, {});` // The failure due to expected: {010100} vs actual: {111100}\r\n\r\nSource code / logs\r\n\r\n**1 .IsFiniteScalarF32 sub-test log :**\r\n\r\n```\r\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\r\n-----------------------------------------------------------------------------\r\nNote: This is test shard 5 of 25.\r\n[==========] Running 6 tests from 2 test cases.\r\n[----------] Global test environment set-up.\r\n[----------] 5 tests from ArrayElementwiseOpTest\r\n[ RUN      ] ArrayElementwiseOpTest.IsFiniteScalarF32\r\n2017-12-11 09:36:20.653075: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n2017-12-11 09:36:20.654000: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n2017-12-11 09:36:20.654482: I tensorflow/compiler/xla/service/service.cc:187] XLA service 0x100062a6ea0 executing computations on platform Host. Devices:\r\n2017-12-11 09:36:20.654493: I tensorflow/compiler/xla/service/service.cc:195]   StreamExecutor device (0): <undefined>, <undefined>\r\ntensorflow/compiler/xla/tests/literal_test_util.cc:157: Failure\r\nValue of: Equal(expected, actual)\r\n  Actual: false (expected: false actual:   true)\r\nExpected: true\r\n\r\n**`expected: false vs actual: true`**\r\n\r\n[  FAILED  ] ArrayElementwiseOpTest.IsFiniteScalarF32 (60 ms)\r\n[ RUN      ] ArrayElementwiseOpTest.LogicalNotZeroElement\r\n2017-12-11 09:36:20.712501: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n[       OK ] ArrayElementwiseOpTest.LogicalNotZeroElement (6 ms)\r\n[ RUN      ] ArrayElementwiseOpTest.LogOfPowerF32\r\n2017-12-11 09:36:20.719016: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n[       OK ] ArrayElementwiseOpTest.LogOfPowerF32 (12 ms)\r\n[ RUN      ] ArrayElementwiseOpTest.Max3DAndScalarZeroElementS32s\r\n2017-12-11 09:36:20.731087: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n[       OK ] ArrayElementwiseOpTest.Max3DAndScalarZeroElementS32s (7 ms)\r\n[ RUN      ] ArrayElementwiseOpTest.Compare1DTo2DS32Ne\r\n2017-12-11 09:36:20.738675: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n[       OK ] ArrayElementwiseOpTest.Compare1DTo2DS32Ne (13 ms)\r\n[----------] 5 tests from ArrayElementwiseOpTest (99 ms total)\r\n\r\n[----------] 1 test from ArrayElementwiseOpTestParamCount/ArrayElementwiseOpTestParamCount\r\n[ RUN      ] ArrayElementwiseOpTestParamCount/ArrayElementwiseOpTestParamCount.SquareManyValues/0\r\n2017-12-11 09:36:20.751273: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n[       OK ] ArrayElementwiseOpTestParamCount/ArrayElementwiseOpTestParamCount.SquareManyValues/0 (34 ms)\r\n[----------] 1 test from ArrayElementwiseOpTestParamCount/ArrayElementwiseOpTestParamCount (34 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 6 tests from 2 test cases ran. (133 ms total)\r\n[  PASSED  ] 5 tests.\r\n[  FAILED  ] 1 test, listed below:\r\n[  FAILED  ] ArrayElementwiseOpTest.IsFiniteScalarF32\r\n\r\n 1 FAILED TEST\r\n```\r\n\r\n**2.IsFiniteR1F32s sub-test log:**\r\n```\r\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\r\n-----------------------------------------------------------------------------\r\nNote: This is test shard 6 of 25.\r\n[==========] Running 6 tests from 2 test cases.\r\n[----------] Global test environment set-up.\r\n[----------] 5 tests from ArrayElementwiseOpTest\r\n[ RUN      ] ArrayElementwiseOpTest.IsFiniteR1F32s\r\n2017-12-11 09:36:21.201704: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n2017-12-11 09:36:21.202563: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n2017-12-11 09:36:21.203145: I tensorflow/compiler/xla/service/service.cc:187] XLA service 0x10015cc6ea0 executing computations on platform Host. Devices:\r\n2017-12-11 09:36:21.203154: I tensorflow/compiler/xla/service/service.cc:195]   StreamExecutor device (0): <undefined>, <undefined>\r\ntensorflow/compiler/xla/tests/literal_test_util.cc:157: Failure\r\nValue of: Equal(expected, actual)\r\n  Actual: false (expected: {010100}\r\nactual:   {111100})\r\nExpected: true\r\n\r\n**expected: {010100} vs actual: {111100}**\r\n\r\n[  FAILED  ] ArrayElementwiseOpTest.IsFiniteR1F32s (17 ms)\r\n[ RUN      ] ArrayElementwiseOpTest.CompareEqF32s\r\n2017-12-11 09:36:21.218227: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n[       OK ] ArrayElementwiseOpTest.CompareEqF32s (10 ms)\r\n[ RUN      ] ArrayElementwiseOpTest.MulOfExpF32\r\n2017-12-11 09:36:21.228744: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n[       OK ] ArrayElementwiseOpTest.MulOfExpF32 (12 ms)\r\n[ RUN      ] ArrayElementwiseOpTest.Min2DTo1DF32s\r\n2017-12-11 09:36:21.240780: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n[       OK ] ArrayElementwiseOpTest.Min2DTo1DF32s (15 ms)\r\n[ RUN      ] ArrayElementwiseOpTest.Compare1DTo2DS32Ge\r\n2017-12-11 09:36:21.255836: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n[       OK ] ArrayElementwiseOpTest.Compare1DTo2DS32Ge (19 ms)\r\n[----------] 5 tests from ArrayElementwiseOpTest (73 ms total)\r\n\r\n[----------] 1 test from ArrayElementwiseOpTestParamCount/ArrayElementwiseOpTestParamCount\r\n[ RUN      ] ArrayElementwiseOpTestParamCount/ArrayElementwiseOpTestParamCount.SquareManyValues/1\r\n2017-12-11 09:36:21.274589: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n[       OK ] ArrayElementwiseOpTestParamCount/ArrayElementwiseOpTestParamCount.SquareManyValues/1 (73 ms)\r\n[----------] 1 test from ArrayElementwiseOpTestParamCount/ArrayElementwiseOpTestParamCount (73 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 6 tests from 2 test cases ran. (146 ms total)\r\n[  PASSED  ] 5 tests.\r\n[  FAILED  ] 1 test, listed below:\r\n[  FAILED  ] ArrayElementwiseOpTest.IsFiniteR1F32s\r\n\r\n 1 FAILED TEST\r\n```\r\n  ", "@drpngx any comments ?", "Sorry, are you saying that the test is wrong?", "No actually. \r\nJFYI - I have shared the sub-tests details which are failing on ppc64le.\r\n\r\nI just wanted your thoughts on this (What would be the reason of these failures with` xla_cpu_parallel`?).\r\n\r\nThanks!", "@tatatodd I am not super familiar with that. I am suspecting that no matter what, having a different behavior is wrong. Note that this is little endian.", "@andydavis1 @sanjoy know more about the parallel cpu backend.", "I'm not sure what is causing the test failures, and do not have a PPC to reproduce this issue on.\r\nHowever, we are deprecating the parallel CPU backend, and have added intra-op parallelism to the regular CPU backend. So please use the default CPU backend for XLA.", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "OK, closing since this is a deprecated feature and there is a recommended way out.", "Hi @andydavis1 ,\r\n\r\n>We are deprecating the parallel CPU backend, and have added intra-op parallelism to the regular CPU backend. So please use the default CPU backend for XLA.\r\n\r\nFor `bazel test -c opt //tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu test ` , `xla_backend_extra_options=\"\"` is empty and test is running successfully.\r\n\r\nHowever , for `bazel test -c opt //tensorflow/compiler/xla/tests:array_elementwise_ops_test_cpu_parallel`,  the` '--xla_backend_extra_options=xla_cpu_parallel' ` option is being set automatically and the test is failing.\r\n\r\nI think this is using default CPU backend , or do you want me to set some options before running the test. Can you please provide some more details ?\r\n\r\nThanks!\r\n", "If xla_backend_extra_options=xla_cpu_parallel is set, then it uses the parallel CPU backend. We will be disabling the tests that use this option.", "@andydavis1 thanks for the information !"]}, {"number": 15265, "title": "Test remove __force_inline [DO NOT MERGE]", "body": "This is only a test for fixing https://github.com/tensorflow/tensorflow/issues/10521, please don't merge this PR.", "comments": ["http://ci.tensorflow.org/view/TF%20pull%20requests/job/tensorflow-pr-win-bazel/50/console", "@gunan ", "http://ci.tensorflow.org/view/TF%20pull%20requests/job/tensorflow-pr-win-bazel/buildTimeTrend\r\nThis looks quite good result, a clean build and test took 24min after removing __force_inline (Before is around 55min).\r\nAnd it didn't introduce any new breakages.\r\n", "https://github.com/tensorflow/tensorflow/issues/10521 is fixed"]}, {"number": 15264, "title": "Support empty input tensor for some ops (fix #14657)", "body": "Cudnn kernels doesn't work for empty input tensors.\r\nThis PR adds support for empty input tensor for FusedBatchNorm,FusedBatchNormGrad,Conv2DBackpropFilter, and cudnn pooling. (fix #14657)", "comments": ["Can one of the admins verify this patch?", "When input tensor has zero elements, the reference BN forward implementation in the test script gives NaN for mean/variance, however I return zeros.\r\nNaN seems to make more sense in terms of math, though it will then require some special treatment in the actual training. If there is no objections I'll switch to NaNs.", "@zhangyaobit Could you comment on this: https://github.com/tensorflow/tensorflow/pull/15264#issuecomment-352926246 please?", "@tensorflow-jenkins test this please", "I haven't switched from returning zeros to returning NaNs, so the test is failing. I'll do that later", "Let's wait for @zhangyaobit 's comment on this first then.", "@ppwwyyxx, could you comment on the use case of an empty input (e.g. [0, 64, 64, 3])? Should we require the input is non-empty?", "In object detection we may run CNN on patches (object candidates) cropped from an image with predicted boxes. When the image has no candidates we'll get zero patches. In training we can filter out these data but still can't avoid it in testing. A workaround is to use `tf.cond` but I hope the op can support it by itself.\r\n\r\nIn fact, I just found that the existing CPU (eigen) implementation of `fused_batch_norm` can work with empty input and returns NaNs for mean/variance.", "Thanks! This sounds good. Could you make the behavior of GPU implementation consistent with Eigen (return NaNs)? Please let me know once the PR is ready for review.", "@zhangyaobit The changes have been made. Could you review it when you have a time? Thanks!", "@tensorflow-jenkins test this please", "The implementation of `FillFunctor`, `SetZeroFunctor`, etc, are split in two bazel targets: `:fill_functor` and `:constant_op`. However `:constant_op` depends on a lot of stuff: it depends on `:transpose_functor` which depends on `conv2d` (I saw a TODO for this by @yzhwang). But `conv2d_grad_filter` needs to use `SetZeroFunctor` which ends up being a cyclic reference.", "```diff\r\n-    deps = ARRAY_DEPS,\r\n+    deps = [\r\n+        \"//tensorflow/core:array_grad\",\r\n+        \"//tensorflow/core:array_ops_op_lib\",\r\n+        \"//tensorflow/core:framework\",\r\n+        \"//tensorflow/core:lib\",\r\n+        \"//third_party/eigen3\",\r\n+        \":bounds_check\",\r\n+        \":fill_functor\",\r\n+        \":ops_util\",\r\n+    ],\r\n```\r\nCherry-picking the dependencies seems to make this PR build, but doesn't sound like an ideal solution. In general I guess functors should not depend on ops, but here `fill_functor` is actually in `:constant_op`, and `:transpose_functor` depends on `:conv_ops`.", "Jenkins, test this please.", "Thanks @drpngx for help! However I haven't yet fixed the cyclic dependency error mentioned above. I \r\nthink a proper fix would be one of the following:\r\n1. Move GPU implementation of `fill_functor` to target `:fill_functor`.\r\n2. Don't let `:transpose_functor` depend on `:conv_ops`.\r\n\r\nThe first one seems to be within my reach. I can give it a try but not sure if there is any reason why this is not done before.", "Yeah, I would go for moving the stuff out of the constant op to the fill functor. If you feel that it is a generic library that could be used by more than one, then you could extract it out of constant op and have both depend on it.", "@yifeif for some reason, this is stuck with `kokoro-run`. There are others PRs as well.", "Ah if a PR has ran Kokoro tests before, it will need the force-run tag :). ", "Oh, makes sense, of course.", "@ppwwyyxx there are some build breakages on GPU, could you check?", "The build was failing because of the bazel dependency problem, which should've been fixed now after I moved implementations to `:fill_functor` target.", "Jenkins, test this please.", "/CC @gunan ran out of devmapper space on the Jenkins build.\r\n```\r\ndevmapper: Thin Pool has 968455 free data blocks which is less than minimum required 983040 free data blocks. Create more free space in thin pool or use dm.min_free_space option to change behavior\r\nERROR: docker build failed. Dockerfile is at /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/tensorflow/tools/ci_build/Dockerfile.cpu\r\n```\r\n\r\nJenkins, test this please.", "Why is 'XLA' build failing without details?", "OK, looks like @yifeif might have fixed it. We just ran out of space on the machine.", "@ppwwyyxx looks like an internal infra failure. I kicked it off again.", "Merged. Woohoo!"]}, {"number": 15263, "title": "Cannot parse tensor from proto: dtype: DT_INT32 when using tf.extract_image_patches and tf.reshape", "body": "Hi,\r\nI'm experiencing a problem when using TensorFlow to extract image patches and then reshape the output. I'm using TensorFlow 1.3.0, what am i doing wrong?\r\n\r\nThat's my code:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nc = 3\r\nh = 1024\r\np = 32\r\n\r\nimage = tf.random_normal([h,h,c])\r\npatch_size = [1,p,p,1]\r\npatches = tf.extract_image_patches([image],\r\n   patch_size, patch_size, [1, 1, 1, 1], 'VALID')\r\npatches = tf.reshape(patches, [h, p, p, c])\r\n\r\nsess = tf.Session()\r\nI,P,R_n = sess.run([image,patches])\r\nprint(I.shape)\r\nprint(P.shape)\r\n```\r\n\r\nThe error i'm getting is this:\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n~\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_call(self, fn, *args)\r\n   1326     try:\r\n-> 1327       return fn(*args)\r\n   1328     except errors.OpError as e:\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1305                                    feed_dict, fetch_list, target_list,\r\n-> 1306                                    status, run_metadata)\r\n   1307 \r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\contextlib.py in __exit__(self, type, value, traceback)\r\n     65             try:\r\n---> 66                 next(self.gen)\r\n     67             except StopIteration:\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py in raise_exception_on_not_ok_status()\r\n    465           compat.as_text(pywrap_tensorflow.TF_Message(status)),\r\n--> 466           pywrap_tensorflow.TF_GetCode(status))\r\n    467   finally:\r\n\r\nInvalidArgumentError: Cannot parse tensor from proto: dtype: DT_INT32\r\ntensor_shape {\r\n  dim {\r\n    size: 3\r\n  }\r\n}\r\ntensor_content: \"\\000\\004\\000\\000\\000\\004\\000\\000\\003\\000\\000\\000\"\r\n\r\n\t [[Node: random_normal_21/shape = Const[dtype=DT_INT32, value=Tensor<type: int32 shape: [3] values: 1024 1024 3>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-159-454594e78931> in <module>()\r\n     12 \r\n     13 sess = tf.Session()\r\n---> 14 I,P,R_n = sess.run([image,patches])\r\n     15 print(I.shape)\r\n     16 print(P.shape)\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    893     try:\r\n    894       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 895                          run_metadata_ptr)\r\n    896       if run_metadata:\r\n    897         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1122     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1123       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1124                              feed_dict_tensor, options, run_metadata)\r\n   1125     else:\r\n   1126       results = []\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1319     if handle is None:\r\n   1320       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\r\n-> 1321                            options, run_metadata)\r\n   1322     else:\r\n   1323       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_call(self, fn, *args)\r\n   1338         except KeyError:\r\n   1339           pass\r\n-> 1340       raise type(e)(node_def, op, message)\r\n   1341 \r\n   1342   def _extend_graph(self):\r\n\r\nInvalidArgumentError: Cannot parse tensor from proto: dtype: DT_INT32\r\ntensor_shape {\r\n  dim {\r\n    size: 3\r\n  }\r\n}\r\ntensor_content: \"\\000\\004\\000\\000\\000\\004\\000\\000\\003\\000\\000\\000\"\r\n\r\n\t [[Node: random_normal_21/shape = Const[dtype=DT_INT32, value=Tensor<type: int32 shape: [3] values: 1024 1024 3>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n\r\nCaused by op 'random_normal_21/shape', defined at:\r\n  File \"C:\\Users\\koko\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"C:\\Users\\koko\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\koko\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\r\n    app.launch_new_instance()\r\n  File \"C:\\Users\\koko\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"C:\\Users\\koko\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\r\n    ioloop.IOLoop.instance().start()\r\n  File \"C:\\Users\\koko\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\r\n    super(ZMQIOLoop, self).start()\r\n  File \"C:\\Users\\koko\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\r\n    handler_func(fd_obj, events)\r\n  File \"C:\\Users\\koko\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"C:\\Users\\koko\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\r\n    self._handle_recv()\r\n  File \"C:\\Users\\koko\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"C:\\Users\\koko\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"C:\\Users\\koko\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"C:\\Users\\koko\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"C:\\Users\\koko\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"C:\\Users\\koko\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"C:\\Users\\koko\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"C:\\Users\\koko\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"C:\\Users\\koko\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"C:\\Users\\koko\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"C:\\Users\\koko\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-159-454594e78931>\", line 7, in <module>\r\n    image = tf.random_normal([h,h,c])\r\n  File \"C:\\Users\\koko\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\random_ops.py\", line 71, in random_normal\r\n    shape_tensor = _ShapeTensor(shape)\r\n  File \"C:\\Users\\koko\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\random_ops.py\", line 42, in _ShapeTensor\r\n    return ops.convert_to_tensor(shape, dtype=dtype, name=\"shape\")\r\n  File \"C:\\Users\\koko\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 611, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"C:\\Users\\koko\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 676, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"C:\\Users\\koko\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 121, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"C:\\Users\\koko\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 106, in constant\r\n    attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\r\n  File \"C:\\Users\\koko\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2630, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"C:\\Users\\koko\\AppData\\Local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): Cannot parse tensor from proto: dtype: DT_INT32\r\ntensor_shape {\r\n  dim {\r\n    size: 3\r\n  }\r\n}\r\ntensor_content: \"\\000\\004\\000\\000\\000\\004\\000\\000\\003\\000\\000\\000\"\r\n\r\n\t [[Node: random_normal_21/shape = Const[dtype=DT_INT32, value=Tensor<type: int32 shape: [3] values: 1024 1024 3>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n```", "comments": ["Hi, @talhadar . \r\nI think you should use `I, P = sess.run([image,patches])` since there are only two Tensors here `[image, patches]`.\r\n\r\nBy the way, this question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n", "Thanks, @facaiy!\r\n\r\nThis problem is indeed much better addressed on Stack Overflow, so I will close this issue. Thanks!"]}, {"number": 15262, "title": "Bug with tf.layers.Dense", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.4\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n8.0\r\n- **GPU model and memory**:\r\nGTX 1080 ti\r\n- **Exact command to reproduce**:\r\n\r\nwith tf.variable_scope('hello') as var_scope:\r\n    var = tf.get_variable('var', [3,4,5])\r\n    dense = tf.layers.Dense(5, name = 'dense_layer')\r\n    b = dense(a)\r\n\r\nAnd the error was : \r\nRuntimeError: Conversion function <function _TensorConversionFunction at 0x7f5bed522b90> for type <class 'tensorflow.python.ops.variables.Variable'> returned incompatible dtype: requested = float32_ref, actual = float32", "comments": ["Could you give a reproducible code? Thanks.", "```\r\nimport tensorflow as tf\r\nwith tf.variable_scope('hello') as var_scope:\r\n    var = tf.get_variable('var', [3,4,5])\r\n    dense = tf.layers.Dense(5, name = 'dense_layer')\r\n    b = dense(a)\r\n```", "Thanks, I think you may forget to define variable a in the code. ", "I did by `tf.get_variable` function.\r\nDo you mean that I should define variable with other function?", "Oh, if I understand you correctly, `a` is written as `var` by mistake, right?\r\n```\r\nimport tensorflow as tf\r\nwith tf.variable_scope('hello') as var_scope:\r\n    a = tf.get_variable('var', [3,4,5])\r\n    dense = tf.layers.Dense(5, name = 'dense_layer')\r\n    b = dense(a)\r\n```\r\n\r\nIf True, I'm not sure whether variable is accepted, since tf.layers.Dense requires that its input must be a tensor, see [Dense - Apply](https://www.tensorflow.org/versions/master/api_docs/python/tf/layers/Dense#apply).", "Yes, It was my mistake.\r\nOh, I didn't attention the difference between variable and tensor.\r\nThen, can I ask a question that, why Tensorflow gives such a restriction?", "Tensorflow should __not__ have such a restriction.\r\nBefore this is fixed, you can work around it by:\r\n```python\r\n    dense = tf.layers.Dense(5, name = 'dense_layer', dtype=tf.float32)\r\n```", "The question is too broad for me, \r\n\r\nAs @alextp clarified it in https://github.com/tensorflow/tensorflow/issues/14788#issuecomment-348651437\r\n> tf.Variable s are not tensors, and neither are SparseTensors\r\n\r\nI think perhaps @fchollet @alextp are more suitable to answer the question.\r\n", "Can you provide the full stack trace?", "Sure,\r\nThe original code is : \r\n```\r\nwith tf.variable_scope('test_scope') as var:\r\n    var = tf.get_variable('var', [3,4,5], dtype = tf.float32)\r\n    dense = tf.layers.Dense(5)\r\n    b = dense(var)\r\n```\r\n\r\nAnd the stack trace:\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-2-8ebaffd3a589> in <module>()\r\n      2     var = tf.get_variable('var', [3,4,5], dtype = tf.float32)\r\n      3     dense = tf.layers.Dense(5)\r\n----> 4     b = dense(var)\r\n\r\n/home/ad26kr/virenv/tensorflow1.4/lib/python2.7/site-packages/tensorflow/python/layers/base.pyc in __call__(self, inputs, *args, **kwargs)\r\n    573         if in_graph_mode:\r\n    574           self._assert_input_compatibility(inputs)\r\n--> 575         outputs = self.call(inputs, *args, **kwargs)\r\n    576 \r\n    577         if outputs is None:\r\n\r\n/home/ad26kr/virenv/tensorflow1.4/lib/python2.7/site-packages/tensorflow/python/layers/core.pyc in call(self, inputs)\r\n    149 \r\n    150   def call(self, inputs):\r\n--> 151     inputs = ops.convert_to_tensor(inputs, dtype=self.dtype)\r\n    152     shape = inputs.get_shape().as_list()\r\n    153     if len(shape) > 2:\r\n\r\n/home/ad26kr/virenv/tensorflow1.4/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in convert_to_tensor(value, dtype, name, preferred_dtype)\r\n    834       name=name,\r\n    835       preferred_dtype=preferred_dtype,\r\n--> 836       as_ref=False)\r\n    837 \r\n    838 \r\n\r\n/home/ad26kr/virenv/tensorflow1.4/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)\r\n    938           \"dtype: requested = %s, actual = %s\" %\r\n    939           (_error_prefix(name), conversion_func, base_type, dtype.name,\r\n--> 940            ret.dtype.name))\r\n    941     return ret\r\n    942   raise TypeError(\"%sCannot convert %r with type %s to Tensor: \"\r\n\r\nRuntimeError: Conversion function <function _TensorConversionFunction at 0x7f2d52a64c08> for type <class 'tensorflow.python.ops.variables.Variable'> returned incompatible dtype: requested = float32_ref, actual = float32\r\n```", "I'll submit a fix, but a temporary fix is to replace `dense(var)` with `dense(tf.identity(var))`"]}, {"number": 15261, "title": "ValueError: Variable Model/LSTMenc/rnn/basic_lstm_cell/weights does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**: ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nIf I put reuse=None while creating BasicLSTMCell in the following code, I get this error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"pretrain.py\", line 358, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"pretrain.py\", line 251, in main\r\n    valid_model = build_model(word_vocab, train=False)\r\n  File \"pretrain.py\", line 200, in build_model\r\n    dropout=FLAGS.dropout))\r\n  File \"/home/raghuram.vadapalli/styletransfer/NeuralSum/model.py\", line 218, in lstm_doc_enc\r\n    initial_state=initial_rnn_state, dtype=tf.float32)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn.py\", line 197, in static_rnn\r\n    (output, state) = call_cell()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn.py\", line 184, in <lambda>\r\n    call_cell = lambda: cell(input_, state)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 235, in __call__\r\n    with _checked_scope(self, scope or \"basic_lstm_cell\", reuse=self._reuse):\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\r\n    return self.gen.next()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 93, in _checked_scope\r\n    \"the argument reuse=True.\" % (scope_name, type(cell).__name__))\r\nValueError: Attempt to have a second RNNCell use the weights of a variable scope that already has weights: 'Model/LSTMenc/rnn/basic_lstm_cell'; and the cell was not constructed as BasicLSTMCell(..., reuse=True).  To share the weights of an RNNCell, simply reuse it in your second calculation, or create a new one with the argument reuse=True.\r\n```\r\nIf I put reuse=True, I get this error\r\n```\r\nTraceback (most recent call last):\r\n  File \"pretrain.py\", line 358, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"pretrain.py\", line 244, in main\r\n    train_model = build_model(word_vocab, train=True)\r\n  File \"pretrain.py\", line 146, in build_model\r\n    dropout=FLAGS.dropout))\r\n  File \"/home/raghuram.vadapalli/styletransfer/NeuralSum/model.py\", line 218, in lstm_doc_enc\r\n    initial_state=initial_rnn_state, dtype=tf.float32)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn.py\", line 197, in static_rnn\r\n    (output, state) = call_cell()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn.py\", line 184, in <lambda>\r\n    call_cell = lambda: cell(input_, state)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 713, in __call__\r\n    output, new_state = self._cell(inputs, state, scope)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 241, in __call__\r\n    concat = _linear([inputs, h], 4 * self._num_units, True)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 1044, in _linear\r\n    _WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size], dtype=dtype)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 1049, in get_variable\r\n    use_resource=use_resource, custom_getter=custom_getter)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 948, in get_variable\r\n    use_resource=use_resource, custom_getter=custom_getter)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 356, in get_variable\r\n    validate_shape=validate_shape, use_resource=use_resource)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 341, in _true_getter\r\n    use_resource=use_resource)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 671, in _get_single_variable\r\n    \"VarScope?\" % name)\r\nValueError: Variable Model/LSTMenc/rnn/basic_lstm_cell/weights does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n```\r\n### Source code / logs\r\n```\r\ndef lstm_doc_enc(input_cnn,\r\n                   batch_size=20,\r\n                   num_rnn_layers=2,\r\n                   rnn_size=650,\r\n                   max_doc_length=35,\r\n                   dropout=0.0):\r\n\r\n    # lstm document encoder\r\n    with tf.variable_scope('LSTMenc') as scope:\r\n        def create_rnn_cell():\r\n            cell = tf.contrib.rnn.BasicLSTMCell(rnn_size, state_is_tuple=True, forget_bias=0.0, reuse=True)\r\n            if dropout > 0.0:\r\n                cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=1.-dropout)\r\n            return cell\r\n\r\n        if num_rnn_layers > 1:\r\n            cell = tf.contrib.rnn.MultiRNNCell([create_rnn_cell() for _ in range(num_rnn_layers)], state_is_tuple=True)\r\n        else:\r\n            cell = create_rnn_cell()\r\n\r\n        initial_rnn_state = cell.zero_state(batch_size, dtype=tf.float32)\r\n\r\n        input_cnn = tf.reshape(input_cnn, [batch_size, max_doc_length, -1])\r\n        input_cnn2 = [tf.squeeze(x, [1]) for x in tf.split(input_cnn, max_doc_length, 1)]\r\n\r\n        outputs, final_rnn_state = tf.contrib.rnn.static_rnn(cell, input_cnn2,\r\n                                         initial_state=initial_rnn_state, dtype=tf.float32)\r\n\r\n    return adict(\r\n        initial_enc_state=initial_rnn_state,\r\n        final_enc_state=final_rnn_state,\r\n        enc_outputs=outputs\r\n    )\r\n\r\n```\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 15260, "title": "Fix typo", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 15259, "title": "Using tensorflow to compute gradients w.r.t. spectral decomposition error", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra 10.13.1\r\n- **TensorFlow installed from (source or binary)**: pip install tensorflow\r\n- **TensorFlow version (use command below)**: v1.2.0-5-g435cdfc 1.2.1\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: see code\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nHi,\r\n\r\nI'm having issues with evaluating the gradients of eigenvalues/eigenvectors with respect to the underlying matrix. Tensorflow evaluates the gradients, however when compared against analytical derivations the gradients are generally inconsistent (I understand numerical can be unstable). I'd like to understand is there's a bug in the gradients or if a different methodology was employed to the one cited below.\r\n\r\nWe are using tf.self_adjoint_eig to evaluate the spectral decomposition for the tensorflow variable input. We have initialised this with a symmetric matrix to satisfy the self adjoint operator property. We wish to take the derivative of individual eigenvalues or eigenvector with respect to the original matrix (note for eigenvectors we take one element from one eigenvector, e.g. element 2 in eigenvector 1 to avoid the issue of gradient aggregation for now).\r\n\r\nThe methodology for evaluating analytical gradients of eigenvalues and vectors of a symmetric real matrix can be found in the paper \"On differentiating Eigenvalues and Eigenvectors\" by Magnus (1985), Theorem 1 eqn (6) + (7). We evaluated using our input matrix the gradients under this paper and compared it to tensorflow evaluated gradients and the gradients from finite difference approximation. For the eigenvalues, the gradients are similar (identical on diagonal entries, off by a factor of 2 on off-diagonal elements), however the eigenvectors are off by quite a bit outside the diagonal entries. To start, define a matrix A as (excuse the matrix output formatting from Python)\r\n\r\nA=[[-3 -2  4]\r\n      [-2  1  1]\r\n      [ 4  1  5]]\r\n\r\nusing np.linalg.eig and tf.self_adjoint_eig on A (I simply initialised a variable with A and computed the gradient for the tf implementation)\r\n\r\nTensorflow eigenvalues: [-5.43071561  1.76904987  6.66166575]\r\nPython eigenvalues: [-5.43071561  6.66166575  1.76904987]\r\n\r\nI now wish to evaluate the gradient of eigenvalue 1 (-5.43071561) w.r.t. A. \r\n\r\nAnalytical gradient:\r\n[[ 0.75896178  0.28555906 -0.31842553]\r\n [ 0.28555906  0.10744148 -0.11980748]\r\n [-0.31842553 -0.11980748  0.13359674]]\r\n\r\nTensorflow gradient:\r\n[[[ 0.75896178,  0.        ,  0.        ],\r\n   [ 0.57111812,  0.10744148,  0.        ],\r\n  [-0.63685107, -0.23961495,  0.13359674]]]\r\n\r\nThe diagonal entries are the same but the off-diagonal entries are clearly off by a factor of 2.  Now we try and evaluate gradients for the eigenvectors.\r\n\r\nTensorflow eigenvectors:\r\n[[-0.87118413 -0.31452619 -0.37697678]\r\n [-0.32778267  0.94426587 -0.0303395 ]\r\n [ 0.36550888  0.09713517 -0.92572567]]\r\nPython eigenvectors:\r\n[[ 0.87118413  0.37697678 -0.31452619]\r\n [ 0.32778267  0.0303395   0.94426587]\r\n [-0.36550888  0.92572567  0.09713517]]\r\n\r\nWe try and find the gradient of the eigenvector 1 element 2 (+/-0.32778267). We expect the tensorflow gradient to the equivalent to the analytical gradient (after taking into account the sign difference).\r\n\r\nAnalytical gradient:\r\n[[ 0.03511309 -0.10795607 -0.01312188]\r\n [ 0.01321128 -0.04061843 -0.0049371 ]\r\n [-0.01473184  0.04529341  0.00550534]]\r\n\r\nTensorflow gradient:\r\n[[[-0.03511309,  0.        ,  0.        ],\r\n   [ 0.09474478,  0.04061843,  0.        ],\r\n   [ 0.02785372, -0.04035631, -0.00550534]]]\r\n\r\nBesides the entries being different on the off-diagonal, one issue is that tensorflow only returns the lower triangle of the gradient. Despite A being symmetric, the contribution to the gradient is not symmetric as shown in the analytical evaluation above. Thank you for reading!\r\n\r\nTL:DR, I'd like to understand where the gradient computations are coming from and why they differ substantially to the results we have been using in our research. \r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nHere is a script that reproduces the above.\r\n\r\n[eigen_decomp_examplev2.py.zip](https://github.com/tensorflow/tensorflow/files/1546631/eigen_decomp_examplev2.py.zip)\r\n", "comments": ["Thanks for your detailed explanations! Since this doesn't look very much like a bug or a feature request, though (there is a chance that a bug is in here, in which case please file another issue once you can demonstrate it), please ask your question on [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow), which is much better suited for community support questions.\r\n\r\nThanks!"]}, {"number": 15258, "title": "[no such file or directory: 'x86_64'] when building the library in TensorFlow Lite for iOS", "body": "Hi all, I encountered with an issur when trying to [setting up the environment to build TensorFlow Lite for iOS. ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/ios.md#building).\r\nBut when I was trying to build the library for all five supported architectures on iOS using `tensorflow/contrib/lite/build_ios_universal_lib.sh`, I have the follwing issue. \r\n\r\n\r\n    xcrun: error: SDK \"iphonesimulator\" cannot be located\r\n    xcrun: error: SDK \"iphonesimulator\" cannot be located\r\n    xcrun: error: unable to lookup item 'PlatformPath' in SDK 'iphonesimulator'\r\n    xcrun: error: SDK \"iphonesimulator\" cannot be located\r\n    xcrun: error: SDK \"iphonesimulator\" cannot be located\r\n    xcrun: error: unable to lookup item 'Path' in SDK 'iphonesimulator'\r\n    xcrun: error: SDK \"iphoneos\" cannot be located\r\n    xcrun: error: SDK \"iphoneos\" cannot be located\r\n    xcrun: error: unable to lookup item 'SDKVersion' in SDK 'iphoneos'\r\n    gcc --std=c++11 -O3 -DNDEBUG -miphoneos-version-min=9.0 -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -fembed-bitcode -Wno-c++11-narrowing -mno-thumb -fno-exceptions -isysroot  -arch x86_64 -O3 -I. -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/../../../ -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/ -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/allocation.cc -o /Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ios_x86_64/tensorflow/contrib/lite/allocation.o\r\n    gcc -miphoneos-version-min=9.0 -fembed-bitcode -mno-thumb -isysroot  -arch x86_64 -O3 -I. -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/../../../ -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/ -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/context.c -o /Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ios_x86_64/tensorflow/contrib/lite/context.o\r\n    gcc --std=c++11 -O3 -DNDEBUG -miphoneos-version-min=9.0 -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -fembed-bitcode -Wno-c++11-narrowing -mno-thumb -fno-exceptions -isysroot  -arch x86_64 -O3 -I. -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/../../../ -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/ -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/downloads/farmhash/src/farmhash.cc -o /Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ios_x86_64/tensorflow/contrib/lite/downloads/farmhash/src/farmhash.o\r\n    gcc --std=c++11 -O3 -DNDEBUG -miphoneos-version-min=9.0 -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -fembed-bitcode -Wno-c++11-narrowing -mno-thumb -fno-exceptions -isysroot  -arch x86_64 -O3 -I. -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/../../../ -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/ -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/error_reporter.cc -o /Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ios_x86_64/tensorflow/contrib/lite/error_reporter.o\r\n    gcc --std=c++11 -O3 -DNDEBUG -miphoneos-version-min=9.0 -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -fembed-bitcode -Wno-c++11-narrowing -mno-thumb -fno-exceptions -isysroot  -arch x86_64 -O3 -I. -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/../../../ -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/ -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/interpreter.cc -o /Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ios_x86_64/tensorflow/contrib/lite/interpreter.o\r\n    clang: error: no such file or directory: 'x86_64'\r\n    clang: warning: no such sysroot directory: '-arch' [-Wmissing-sysroot]\r\n    make: *** [/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ios_x86_64/tensorflow/contrib/lite/allocation.o] Error 1\r\n    make: *** Waiting for unfinished jobs....\r\n    clang: error: no such file or directory: 'x86_64'\r\n    clang: warning: no such sysroot directory: '-arch' [-Wmissing-sysroot]\r\n    clang: error: no such file or directory: 'x86_64'\r\n    clang: warning: no such sysroot directory: '-arch' [-Wmissing-sysroot]\r\n    make: *** [/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ios_x86_64/tensorflow/contrib/lite/context.o] Error 1\r\n    make: *** [/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ios_x86_64/tensorflow/contrib/lite/downloads/farmhash/src/farmhash.o] Error 1\r\n    clang: error: no such file or directory: 'x86_64'\r\n    clang: warning: no such sysroot directory: '-arch' [-Wmissing-sysroot]\r\n    make: *** [/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ios_x86_64/tensorflow/contrib/lite/error_reporter.o] Error 1\r\n    clang: error: no such file or directory: 'x86_64'\r\n    clang: warning: no such sysroot directory: '-arch' [-Wmissing-sysroot]\r\n    make: *** [/Users/zhangjiawei/tensorflow/tensorflow/contrib/lite/gen/obj/ios_x86_64/tensorflow/contrib/lite/interpreter.o] Error 1\r\n\r\n\r\n### System information\r\n- **OS Platform and Distribution**: macOS High Sierra 10.13.2(17C88)\r\n- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0\r\n- **Python version**: 3.6.1\r\n- **GCC/Compiler version (if compiling from source)**: GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)\r\n- **Exact command to reproduce**: `bash tensorflow/contrib/lite/build_ios_universal_lib.sh`\r\n- **Have I written custom code?**: no\r\n- **TensorFlow installed from**: source\r\n- **Bazel version**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n\r\nHas anyone else had the same problem?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory", "@tensorflowbutler thanks, I've updated that.", "@aselle can you comment on this one?", "To fix this, open in XCode > Preferences > Locations and look Command Line Tools. Mine was empty. Selecting a value fixed in that dropdown fixed this issue for me. ", "@mikeknapp Thanks, it works!", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly."]}, {"number": 15257, "title": "can't read my own tfrecords and face FailedPreconditionError", "body": "Please go to Stack Overflow for help and support:\r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 1604\r\n- **TensorFlow installed from (source or binary)**: conda tensorflow\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 2.7\r\n- **Bazel version: N/A\r\n- **CUDA/cuDNN version: N/A\r\n- **GPU model and memory: N/A\r\n\r\n### Describe the problem\r\nI'm tryting to read my own datasets. And there exists FailedPreconditionError.\r\nBut my ` test.records` are in the right directory, I don't know why it ocurred.\r\n\r\n### Source code / logs\r\nthese are the console error.\r\n```\r\nINFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.FailedPreconditionError'>, /home/tju/0_lvjc/deepnet348/data/tfrecords\r\n\t [[Node: ReaderReadV2 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](TFRecordReaderV2, input_producer)]]\r\nTraceback (most recent call last):\r\n  File \"/home/tju/Downloads/pycharm-community-2017.2.4/helpers/pydev/pydev_run_in_console.py\", line 37, in run_file\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"/home/tju/0_lvjc/deepnet348/data_input.py\", line 199, in <module>\r\n    coord.join(threads)\r\n  File \"/home/tju/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/home/tju/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/queue_runner_impl.py\", line 238, in _run\r\n    enqueue_callable()\r\n  File \"/home/tju/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1235, in _single_operation_run\r\n    target_list_as_strings, status, None)\r\n  File \"/home/tju/anaconda2/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/home/tju/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\n```\r\n\r\nthese are my codes.\r\n```\r\n#%% Convert data to TFRecord\r\n\r\nfile_dir='/home/tju/0_lvjc/datasets/SALICON/train_images/*.jpg'\r\nlabel_dir='/home/tju/0_lvjc/datasets/SALICON/salicon17_maps/train/*.png'\r\n\r\ntfrecords_file = '/home/tju/0_lvjc/deepnet348/data/tfrecords'\r\nBATCH_SIZE = 5\r\n\r\n\r\n#Convert test data: you just need to run it ONCE !\r\nname_test = 'test'\r\nimages, labels = get_file(file_dir, label_dir)\r\nconvert_to_tfrecord(images, labels, save_dir=tfrecords_file, name=name_test)\r\n\r\n\r\n#%% TO test train.tfrecord file\r\n\r\ndef plot_images(images, labels):\r\n    '''plot one batch size\r\n    '''\r\n    for i in np.arange(0, BATCH_SIZE):\r\n        plt.subplot(5, 5, i + 1)\r\n        plt.axis('off')\r\n        plt.title(chr(ord('A') + labels[i] - 1), fontsize = 14)\r\n        plt.subplots_adjust(top=1.5)\r\n        plt.imshow(images[i])\r\n    plt.show()\r\n\r\n\r\n\r\nimage_batch, label_batch = read_and_decode(tfrecords_file, batch_size=BATCH_SIZE)\r\n\r\nwith tf.Session()  as sess:\r\n    \r\n    i = 0\r\n    coord = tf.train.Coordinator()\r\n    threads = tf.train.start_queue_runners(coord=coord)\r\n    \r\n    try:\r\n        while not coord.should_stop() and i<1:\r\n            # just plot one batch size\r\n            image, label = sess.run([image_batch, label_batch])\r\n            plot_images(image, label)\r\n            i+=1\r\n            \r\n    except tf.errors.OutOfRangeError:\r\n        print('done!')\r\n    finally:\r\n        coord.request_stop()\r\n    coord.join(threads)\r\n\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "@tensorflowbutler nope."]}, {"number": 15256, "title": "Problem about tf.data.Dataset.from_sparse_tensor_slices", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Win10\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:1.4\r\n- **Python version**: 3.52\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:8.0,6.46\r\n- **GPU model and memory**:2GB\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI used the tf.data.Dataset.from_sparse_tensor_slices to built a dataset. But the website has no enought information.\r\nHere's my code\r\n`    point_cloud_feature_dataset = tf.data.Dataset.from_sparse_tensor_slices(sparse_feature)\r\n    point_cloud_feature_dataset = point_cloud_feature_dataset.shuffle(buffer_size = 100000)\r\n    point_cloud_feature_dataset = point_cloud_feature_dataset.batch(batch_size = BATCH_SIZE)\r\n    point_cloud_feature_dataset = point_cloud_feature_dataset.repeat()\r\n    iterator_feature = point_cloud_feature_dataset.make_one_shot_iterator()`\r\n\r\nwhen I called the iterator_feature.get_nest(). It return 3 Tensors of shape [none,none,1]. Instead of a SparseTensor.  The input Sparse Tensor of dataset has a shape of [1000000,300000]. Each row is a example. I hope talents can replenish the Doc. Thanks!!  \r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Could you elaborate a little bit more on what the problem is? Are you in a position to send a PR with more explanations?\r\n\r\n/CC @mrry ", "OK. thanks for your respond.\r\nAs the code I copy. When I call the `iterator_feature.get_next()`, it return (<Tensor>,<Tensor>,<Tensor>)\r\nAre they represent the element's position,value and dense shape? I guess they are. I mean you could specific them. So that we can use the lib more convinient.", "In TensorFlow 1.5, we'll deprecate `tf.data.Dataset.from_sparse_tensor_slices()` and fold its behavior into `tf.data.Dataset.from_tensor_slices()`: you'll be able to use a `tf.SparseTensor` in place of a `tf.Tensor` in that API (and many more), and it will work better with batching etc. You can try this out by installing the `tf-nightly` PIP package, or building from source. You'll be able to write:\r\n\r\n```python\r\npoint_cloud_feature_dataset = tf.data.Dataset.from_tensor_slices(sparse_feature)\r\npoint_cloud_feature_dataset = point_cloud_feature_dataset.shuffle(buffer_size = 100000)\r\npoint_cloud_feature_dataset = point_cloud_feature_dataset.batch(batch_size = BATCH_SIZE)\r\npoint_cloud_feature_dataset = point_cloud_feature_dataset.repeat()\r\niterator_feature = point_cloud_feature_dataset.make_one_shot_iterator()\r\n```\r\n\r\nFor your current code, the `Dataset.from_sparse_tensor_slices()` source yields three tensors, representing the `indices`, `values`, and `dense_shape` of a sparse tensor. However, the batching will be incorrect, because it will apply to the indices, values, and dense_shape separately, and the resulting output of the iterator might not be a valid `tf.SparseTensor`.", "Thanks for your help. Now, SparseTensor can be put into the model easily.  But I got other bug like \"index out of other\" and \"index repeat\". I think I can fix it. And doesn't the tf-nightly have GPU-version.?It is a little bit slow....", "There's a `tf-nightly-gpu` package that includes the GPU build.", "I have try the GPU version,  but the `tf.data.Dataset.from_tensor_slices` method doesn't support the sparse tensor. Will it be updated? ", "Ah, I assume you are running on Windows? It doesn't look like the packages are pushed to PyPI, but you can directly install the packages from these URLs:\r\n\r\n* Python 3.5: https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-windows/M=windows-gpu,PY=35/lastSuccessfulBuild/artifact/cmake_build/tf_python/dist/tf_nightly_gpu-1.5.0.dev20171120-cp35-cp35m-win_amd64.whl\r\n* Python 3.6: http://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-windows/M=windows-gpu,PY=36/lastSuccessfulBuild/artifact/cmake_build/tf_python/dist/tf_nightly_gpu-1.5.0.dev20171120-cp36-cp36m-win_amd64.whl", "thanks! I have solved the problem by writing a custom input function. I will try it later.", "When I tried to open URLs. It failed because of not found. And I have searched the website,but didn't found it.\r\n![default](https://user-images.githubusercontent.com/22407275/34239050-df9ad244-e63f-11e7-9b5c-4d659cb71466.JPG)\r\nDoes it be closed?\r\n@mrry  ", "Hmm, it looks like the Windows GPU Nightly is no longer being built. (I seem to recall that that build was taking a very long time, so maybe it was timing out or otherwise failing\u2014does that sound right, @av8ramit?)", "@mrry yes we had disabled the nightly windows build because of CUDA issues. I'll try and fix this.", "http://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-windows/M=windows-gpu,PY=35/", "I work very well! Thanks! But this install pkg looks like doesn't include tensorboard.. I didn't find tensorboard.exe in `Scripts`. So I can't open it by CMD. Is there any other way to open tensorboard?"]}, {"number": 15255, "title": "Add int64 support for BroadcastArgs and BroadcastGradientArgs", "body": "In `array_ops.cc`, both int32 and int64 are expected to be supported for `BroadcastArgs` and `BroadcastGradientArgs`. However, this was not the case as only int32 kernel are registered even though `T` is part of the `TypeConstraint`.\r\n\r\nThis fix adds the int64 kernel support for `BroadcastArgs` and `BroadcastGradientArgs`, and adds related test cases.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Jenkins, test this please.", "Flaky test?\r\n\r\n```\r\nE1211 01:58:36.775779505      15 server_chttp2.cc:38]        {\"created\":\"@1512957516.775699432\",\"description\":\"No address added out of total 1 resolved\",\"file\":\"external/grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc\",\"file_line\":248,\"referenced_errors\":[{\"created\":\"@1512957516.775696170\",\"description\":\"Failed to add any wildcard listeners\",\"file\":\"external/grpc/src/core/lib/iomgr/tcp_server_posix.cc\",\"file_line\":340,\"referenced_errors\":[{\"created\":\"@1512957516.775683269\",\"description\":\"Unable to configure socket\",\"fd\":13,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":200,\"referenced_errors\":[{\"created\":\"@1512957516.775666688\",\"description\":\"OS Error\",\"errno\":98,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":173,\"os_error\":\"Address already in use\",\"syscall\":\"bind\"}]},{\"created\":\"@1512957516.775695307\",\"description\":\"Unable to configure socket\",\"fd\":13,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":200,\"referenced_errors\":[{\"created\":\"@1512957516.775691914\",\"description\":\"OS Error\",\"errno\":98,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":173,\"os_error\":\"Address already in use\",\"syscall\":\"bind\"}]}]}]}\r\nE.s...............\r\n======================================================================\r\nERROR: testRemoteFunctionCrossProcess (__main__.FunctionalOpsTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/tmp/botexec/bazel-out/k8-py3-opt/bin/tensorflow/python/kernel_tests/functional_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/functional_ops_test.py\", line 588, in testRemoteFunctionCrossProcess\r\n    workers, _ = test_util.create_local_cluster(2, 1)\r\n  File \"/tmp/botexec/bazel-out/k8-py3-opt/bin/tensorflow/python/kernel_tests/functional_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1298, in create_local_cluster\r\n    for ix in range(num_ps)\r\n  File \"/tmp/botexec/bazel-out/k8-py3-opt/bin/tensorflow/python/kernel_tests/functional_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1298, in <listcomp>\r\n    for ix in range(num_ps)\r\n  File \"/tmp/botexec/bazel-out/k8-py3-opt/bin/tensorflow/python/kernel_tests/functional_ops_test.runfiles/org_tensorflow/tensorflow/python/training/server_lib.py\", line 145, in __init__\r\n    self._server_def.SerializeToString(), status)\r\n  File \"/tmp/botexec/bazel-out/k8-py3-opt/bin/tensorflow/python/kernel_tests/functional_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server\r\n\r\n----------------------------------------------------------------------\r\nRan 39 tests in 2.226s\r\n```", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 15254, "title": "CMake build on Windows (tensorflow.dll) does not include many GPU ops", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10\r\n- **TensorFlow installed from (source or binary)**:\r\nfrom source\r\n- **TensorFlow version (use command below)**:\r\n1.4\r\n\r\n- **Python version**:  3.5.2\r\n- **Bazel version (if compiling from source)**:\r\nusing CMake (3.8.2)\r\n- **GCC/Compiler version (if compiling from source)**:\r\nMSVC 14 (C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\bin\\amd64\\CL.exe)\r\n- **CUDA/cuDNN version**:\r\nCUDA 8.0, cuDNN: 6 (6.14)\r\n- **GPU model and memory**:\r\nNVidia GTX 1070 (8GB)\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI build tensorflow from source following these instructions:\r\n https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake\r\nand I use this CMake command:\r\n`cmake ..  -G \"Visual Studio 14 2015 Win64\" -T v140,host=x64 -DCMAKE_BUILD_TYPE=RelWithDebInfo -DSWIG_EXECUTABLE=e:/dev/swigwin-3.0.12/swig.exe -Dtensorflow_ENABLE_GPU=ON  -DCUDNN_HOME=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\"   -Dtensorflow_BUILD_SHARED_LIB=ON -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX2  -Dtensorflow_ENABLE_GRPC_SUPPORT=OFF\r\n`\r\nand `MSBuild /p:Configuration=RelWithDebInfo tensorflow.vcxproj`\r\n\r\n(I also tried 'Release' as Configuration - same outcome).\r\nThe result of the build process is  `tensorflow.dll`. I use a separate C++ program (using Qt) to link against the built DLL. In general, everything works fine: I can load a saved tensorflow graph and run it (=inference).\r\nThe problem is now, that many GPU-ops are not linked into tensorflow.dll (for example, `Softmax`) - in my example most ops run on GPU but Softmax on CPU - with a huge performance impact (GPU use <10%). Why I think this is the case:\r\n* `tensorflow::LogAllRegisteredKernels()` lists Softmax, but with CPU only\r\n* looking at tensorflow.dll with DependecyWalker has the same result (Softmax CPU only)\r\n* when I check `tf_core_gpu_kernels.lib`, then GPU code is there (e.g., `tf_core_gpu_kernels_generated_softmax_op_gpu.cu.cc.lib`).\r\n\r\n### Workaround\r\nAfter quite some time trying to figure this out, I found a hackish workaround:\r\nLooking at the output of MSBuild (increased verbosity level), it looks as if the python script create_def_file.py is executed without using the tf_core_kernels.lib (`C:\\Python35\\python.exe E:/dev/tensorflow/tensorflow/contrib/cmake/tools/create_def_file.py --input E:/dev/tensorflow/tensorflow/contrib/cmake/build/RelWithDebInfo/tensorflow_static.lib;E:/dev/tensorflow/tensorflow/contrib/cmake/build/RelWithDebInfo/tf_protos_cc.lib --output E:/dev/tensorflow/tensorflow/contrib/cmake/build/RelWithDebInfo/tensorflow.def --target tensorflow.dll`). \r\nWhat I did is the following:\r\n* create a def-file including the GPU kernels:\r\n`C:\\Python35\\python.exe E:/dev/tensorflow/tensorflow/contrib/cmake/tools/create_def_file.py --input E:/dev/tensorflow/tensorflow/contrib/cmake/build/RelWithDebInfo/tensorflow_static.lib;E:/dev/tensorflow/tensorflow/contrib/cmake/build/RelWithDebInfo/tf_protos_cc.lib;E:/dev/tensorflow/tensorflow/contrib/cmake/build/RelWithDebInfo/tf_core_gpu_kernels.lib --output E:/dev/tensorflow/tensorflow/contrib/cmake/build/RelWithDebInfo/tensorflow_wr.def --target tensorflow.dll`\r\n\r\n* Link tensorflow.dll with the created def file (tensorflow_wr.def). This did not work, since I got a couple of missing-unresolved-externals errors. As they were all related to LSTM/RNN, I ended up re-creating `tf_core_gpu_kernels.lib` (issuing the Lib.exe omitting tf_core_gpu_kernels_generated_gru_ops_gpu.cu.cc.obj and tf_core_gpu_kernels_generated_lstm_ops_gpu.cu.cc.obj)\r\n\r\n* Finally linking tensorflow.dll worked after I manually dropped a couple of symbols from 'tensorflow_wr.def) (unresolved external symbols).\r\n\r\nWith this workaround it works fine - all ops (including Softmax) run on GPU, performance increased by a factor of 10. I submit as an issue, since I believe it should work out of the box!\r\n\r\nSome more details on [StackOverflow](https://stackoverflow.com/questions/47636903/tensorflow-places-softmax-op-on-cpu-instead-of-gpu)\r\n", "comments": ["@werner-rammer good to know! Would you mind sending a PR to fix this?\r\n\r\n/CC @guschmue @mrry ", "this used to work when BUILD_SHARED_LIB=ON was introduced but since new ops like boosted trees have been added and the windows build (especially gpu) might need some touches to catch up.\r\nIf the missing symbols are related to lstm/gru - if I remember this correctly tensorflow.dll should not contain or reference those since they come from _lstm_ops.dll.\r\nMaybe let me do a build what errors there are.", "I am afraind I am not proficient enough with CMake et al to send a (useful) PR. Happy to help, though!", "Closing this issue due to staleness. It should be fixed in the latest TF version, please use the latest version of TensorFlow and build again.\r\nFeel free to report any issues you encounter with latest TensorFlow. Thanks!"]}, {"number": 15253, "title": "include _solib_* in pip package", "body": "See issue #15252 for detail, also should fixes issue #13711.\r\nThe problem is that when compile tensorflow with --config=mkl on virtual machines, mkl libraries won't be included because they locate under _solib_local, however, setup.py only includes _solib_k8.", "comments": ["Can one of the admins verify this patch?", "@gunan WDYT?", "@ic0n please pull rebase and push again.", "@drpngx OK, I rebased it. Hope I haven't done something silly. ", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.", "All successful, but somehow `ci.tensorflow.org` doesn't update. Ignoring."]}, {"number": 15252, "title": "vm compiled tensorflow - libmklml_intel.so ImportError", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04 VM\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\nBranch `r1.4` and branch `master`\r\n- **Python version**: \r\n3.6.3\r\n- **Bazel version (if compiling from source)**:\r\nBuild label: 0.8.1 (Install using apt repository)\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc-6 (Ubuntu 6.4.0-10ubuntu1~16.04.york0) 6.4.0 20171112\r\n\r\n- **Exact command to reproduce**:\r\npython -c \"import tensorflow as tf;\"\r\n\r\n### Describe the problem\r\nAfter compiling Tensorflow from source with tutorial: `https://www.tensorflow.org/install/install_sources` and install the compiled pip package, I import tensorflow on python console and get those errors:\r\n\r\n~~~\r\npython -c \"import tensorflow as tf;\"\r\nTraceback (most recent call last):\r\n  File \"/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/potatoman/anaconda3/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/potatoman/anaconda3/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libmklml_intel.so: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 73, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/potatoman/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/potatoman/anaconda3/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/potatoman/anaconda3/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libmklml_intel.so: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n~~~\r\n\r\nMy machine is a headless KVM virtual machine running on a ubuntu 17.10, btw.\r\n\r\nI figure out where the problem is somehow, which is the pip packaging script don't include those mkl so files, and in my case, replace `_solib_k8` in script `tensorflow/tools/pip_package/setup.py` at line 185 with `_solib_local` solves the issue.\r\n\r\n", "comments": ["OK, since you have a fix, would you mind sending a PR?\r\n\r\n/CC: @gunan ", "@drpngx  PR #15253 sent, not very clean though. \r\n\r\noh, there's a related PR #14709 .", "Thanks!", "Looks like PR #14709 had fix my problem, closing this issue now."]}, {"number": 15251, "title": "different output size for avg_pool and max_pool", "body": "Hello,\r\n\r\nI have the same bug as this user.\r\n\r\nhttps://stackoverflow.com/questions/47423172/tensorflow-why-does-avg-pool-ignore-one-stride-dimension\r\n\r\nMy version is uo-to-date\r\n\r\nHave I written custom code: Yes\r\nOS Platform and Distribution: Linux Mint 18.3, codename \"Sylvia\"\r\nTensorFlow installed from Tensorflow Website\r\nTensorFlow version 14.1.0\r\nBazel version N/A\r\nCUDA/cuDNN version N/A\r\nGPU model and memory GeForce 940MX\r\nExact command to reproduce: see link", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Thanks you. I changed it\r\n And I have a question. Why is it not possible to give the ksize of tf.nn.avg_pool or max_pool as a tensor? It makes no sense, that it has to be an int.\r\nhttps://stackoverflow.com/questions/47740967/tf-avg-pool-with-dynamic-ksize\r\n\r\nSincerely Viktor", "Related issue: Hvass-Labs/TensorFlow-Tutorials#19", "This looks like a duplicate of #14886 -- please add any further comments there."]}]