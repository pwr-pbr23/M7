[{"number": 5574, "title": "Allow deprecation decorator to specify owner", "body": "This helps the following situations:\r\n\r\n- Users or maintainers are able to reach out the person who wants to or already deprecated this functionality\r\n- Maintainers can reach out to owner of deprecation if the scheduled removal time has passed for a while but the function is still there", "comments": ["@terrytangyuan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @martinwicke and @ilblackdragon to be potential reviewers.\n", "Build was aborted somehow \n", "@tensorflow-jenkins Test this please\n"]}, {"number": 5573, "title": "Checks that quantized Tensor's elements have type tuple.", "body": "Fix for the issue #5568 \r\n\r\n", "comments": ["Can one of the admins verify this patch?\n", "@b0noI, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @mrry and @girving to be potential reviewers.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins test this please\n"]}, {"number": 5572, "title": "Deprecated array and embedding ops in contrib.learn", "body": "Same functionalities exist as specified in the deprecation description. The examples were updated though. ", "comments": ["@terrytangyuan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @ilblackdragon and @lukaszkaiser to be potential reviewers.\n", "Seems like you made a commit for this already @ilblackdragon \n"]}, {"number": 5571, "title": "Add support for decoding RGBA images in tfexample_decoder.", "body": "Tensorflow's image decoding API's support RGBA images with png/raw formats, but the tfexample_decoder module cannot handle them due to an exception that is thrown when the decode_jpg() case is evaluated with channels >3. This PR works around that limitation by eliminating JPEG as a decoding option when the number channels is >3.\r\n", "comments": ["Can one of the admins verify this patch?\n", "@xodus7, thanks for your PR! By analyzing the history of the files in this pull request, we identified @nathansilberman, @tensorflower-gardener and @vrv to be potential reviewers.\n", "@tensorflow-jenkins test this please\n", "@gunan I assume the Windows cmake failures here can be ignored?\n", "This one looks like a different failure. But I think it is a flake.\nTo make sure:\nJenkins, test this please.\n"]}, {"number": 5570, "title": "Tensorflow works in command line ipython but not in ipython notebook", "body": "This is a setup issue. Tensorflow works in ipython command line but not in ipython notebook. When import in ipython notebook, I get error:\r\n\r\n```\r\nImportError: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.19' not found (required by /usr/local/packages/python/2.7.10-anaconda/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so)\r\n\r\n\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.\r\n```\r\n\r\nThis is possibly related to https://github.com/tensorflow/tensorflow/issues/5141.\r\nScreenshots of the problem is in http://stackoverflow.com/q/40569279/1363677\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nRed Hat Enterprise Linux Server release 6.4 (Santiago)\r\n`Linux qb2 2.6.32-358.23.2.el6.x86_64 #1 SMP Sat Sep 14 05:32:37 EDT 2013 x86_64 x86_64 x86_64 GNU/Linux`\r\n\r\n\r\nInstalled version of CUDA and cuDNN: \r\n\r\n```\r\n-rw-r--r-- 1 root root  189170 Oct 11 10:13 libcudadevrt.a\r\nlrwxrwxrwx 1 root root      16 Oct 11 10:13 libcudart.so -> libcudart.so.7.5\r\nlrwxrwxrwx 1 root root      19 Oct 11 10:13 libcudart.so.7.5 -> libcudart.so.7.5.18\r\n-rwxr-xr-x 1 root root  311596 Oct 11 10:13 libcudart.so.7.5.18\r\n-rw-r--r-- 1 root root  558020 Oct 11 10:13 libcudart_static.a\r\nlrwxrwxrwx 1 root root      17 Oct 11 10:13 libcuinj32.so -> libcuinj32.so.7.5\r\nlrwxrwxrwx 1 root root      20 Oct 11 10:13 libcuinj32.so.7.5 -> libcuinj32.so.7.5.18\r\n-rwxr-xr-x 1 root root 5396088 Oct 11 10:13 libcuinj32.so.7.5.18\r\n```\r\n\r\n\r\n```\r\npython -c \"import tensorflow; print(tensorflow.__version__)\"\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.7.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.7.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.7.5 locally\r\n0.11.0rc0\r\n```\r\n\r\nI'm not sure whether this is relevant, but my ipython kernel is on a remote machine, and the notebook interface is on my local machine. The ipython kernel is started using\r\n\r\n`[user@remote ~]$ ipython notebook --no-browser --port=8889`\r\n\r\nThen at my local machine, I set up the port forwarding as\r\n\r\n`[user@local ~]$ ssh -N -L localhost:8888:localhost:8889 remote`\r\n\r\nAnd the notebook interface is started from the browser at localhost:8888\r\n\r\n\r\n@drpngx ", "comments": ["Do you use `virtualenv` or `anaconda`? If so, Yarosalv's [answer](https://github.com/tensorflow/tensorflow/issues/5141#issuecomment-255859042) might be something to try.\n\nWhich version of ubuntu or centos are you running?\n", "The system is Red Hat Enterprise Linux Server release 6.4.\n\nI tried Yarosalv's answer to use `/usr/local/packages/python/2.7.10-anaconda/bin/ipython notebook` but it doesn't work for me.\n\nI'm using `anaconda`, but there is only one root environment. From the  error message it seems that the ipython is correctly configured to use anaconda python. Moreover, tensorflow works fine for me in the terminal in ipython, it just breaks in the notebook interface.\n", "There was a [problem](https://github.com/ContinuumIO/anaconda-issues/issues/483) in Anaconda that might be related. \n\nAlso, if you see this problem during the build, make sure your [`LD_LIBRARY_PATH`](https://gcc.gnu.org/onlinedocs/libstdc++/faq.html#faq.how_to_set_paths) is set correctly.\nAnother thing, could you, please, provide the output of the `strings /usr/lib64/libstdc++.so.6 | grep GLIBCXX` - just to check the required version is installed\n\nYou could also replace the anaconda's `libstdc++.so.6` with the symlink from the OS. I am not sure where anaconda stores its environments. Make sure you backup though\n", "Indeed it's the problem of `LD_LIBRARY_PATH`. I reported the problem to my system administrator, and he offered a solution that works:\n\n`module load gcc`\n\nComparing before and after loading gcc, the `LD_LIBRARY_PATH` does change. After loading the module, the following paths have been prepended to the PATH.\n\n```\n/usr/local/compilers/gcc/4.9.0/lib/gcc/x86_64-unknown-linux-gnu/4.9.0\n/usr/local/compilers/gcc/4.9.0/lib64\n/usr/local/compilers/gcc/4.9.0/lib\n```\n"]}, {"number": 5569, "title": "Python 3 Builds in CI are failing", "body": "All our python 3 builds for pull requests on our CI are failing with the following error:\r\n```\r\nERROR: /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/external/bazel_tools/tools/build_defs/pkg/BUILD:44:1: Converting to Python 3: external/bazel_tools/tools/build_defs/pkg/build_tar.py failed: 2to3 failed: error executing command \r\n  (cd /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace && \\\r\n  exec env - \\\r\n  bazel-out/host/bin/external/bazel_tools/tools/python/2to3 --no-diffs --nobackups --write --output-dir bazel-out/host/genfiles/python3/external/bazel_tools/tools/build_defs/pkg --write-unchanged-files external/bazel_tools/tools/build_defs/pkg/build_tar.py): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\n```\r\n\r\nIt seems to be independent of the PR we are testing, and the error message above does not really makes sense. @damienmg , could you help us decode the above error\r\n@benoitsteiner , who is running into this problem.", "comments": ["I need to see more of the output but the problem is coming from depending on @bazel_tools//tools/build_defs/pkg:build_tar which is not py3 compatible and we do not have 2to3 in bazel (we should fix both on bazel side ultimately).\n\nAnyway how is it used?\n", "This is happening during testing of PRs #5560 and #5565 \nI think one of the changes we made internally is causing an issue with the build.\nHere are some build logs, the latter has slightly more information:\n\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-cpu-python3/2171/consoleFull\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-cpu-python3/2172/consoleFull\n", "Can this be bisected? There is a lot of changes in that PR...\n\nOn Mon, Nov 14, 2016 at 7:16 PM gunan notifications@github.com wrote:\n\n> This is happening during testing of PRs #5560\n> https://github.com/tensorflow/tensorflow/pull/5560 and #5565\n> https://github.com/tensorflow/tensorflow/pull/5565\n> I think one of the changes we made internally is causing an issue with the\n> build.\n> Here are some build logs, the latter has slightly more information:\n> \n> https://ci.tensorflow.org/job/tensorflow-pull-requests-cpu-python3/2171/consoleFull\n> \n> https://ci.tensorflow.org/job/tensorflow-pull-requests-cpu-python3/2172/consoleFull\n> \n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5569#issuecomment-260415202,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ADjHfx0repWtRvQEIJSIj0GVqxhQqgbiks5q-KV-gaJpZM4KwkrW\n> .\n", "Found the culprit, it is a new pkg_tar rule.\nAre there corresponding issues for the py3 compatilibility of build_tar on bazel side?\nOr would you like to keep this open to track that?\n", "I'll follow up on this one.\n\nWe need to remove pkg_tar from bazel repository (much like other rules) so\nI'll take the action there.\n\nOn Mon, Nov 14, 2016 at 8:26 PM gunan notifications@github.com wrote:\n\n> Found the culprit, it is a new pkg_tar rule.\n> Are there corresponding issues for the py3 compatilibility of build_tar on\n> bazel side?\n> Or would you like to keep this open to track that?\n> \n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5569#issuecomment-260435335,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ADjHf7iAm823H_cQdG2DMQMoiOpK0Rfsks5q-LX2gaJpZM4KwkrW\n> .\n"]}, {"number": 5568, "title": "Attempt to create quantized tf.constant results in a TypeError", "body": "### Environment info\r\nOperating System:\r\nMacOS 10.12.1\r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\nf794cd393b1e7821fcc3cdcee9b6a4400f2540bf\r\n2. The output of `bazel version`\r\nBuild label: 0.3.1-homebrew\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Sep 27 03:15:13 2016 (1474946113)\r\nBuild timestamp: 1474946113\r\nBuild timestamp as int: 1474946113\r\n\r\n### Repro code\r\n```python\r\nimport tensorflow as tf\r\n\r\nsess = tf.Session()\r\nwith sess.as_default():\r\n     m = tf.constant(3, dtype=tf.qint8)\r\n     print(m.eval())\r\n```\r\n#### result:\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 2, in <module>\r\n  File \"/Users/b0noI/src/tensorflow/_python_build/tensorflow/python/framework/constant_op.py\", line 163, in constant\r\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape))\r\n  File \"/Users/b0noI/src/tensorflow/_python_build/tensorflow/python/framework/tensor_util.py\", line 354, in make_tensor_proto\r\n    nparray = np.array(values, dtype=np_dt)\r\nTypeError: expected a readable buffer object\r\n```", "comments": ["working on PR\n", "Thanks, @b0noI .\n", "turns out that the root cause of the issue is in the way how quantized Tensors are represented. It is expected that they always will have a tuple as the representation for each of an element. On practice this leads to the inconsistent API like the next one:\n\n```\nm = tf.constant(3, dtype=tf.int8) # Evaluates correctly since the type of the tensor is not quantized.\nm = tf.constant(3, dtype=tf.qint8) # Throws TypeError since the tensor is a quantized tensor, but the element in the tensor is not tuple.\nm = tf.constant((3,), dtype=tf.qint8) # Evaluates correctly since element in the tensor is a tuple.\n```\n\nCurrent representation of the type of elements of a quantized Tensor described by following DTypes:\n\n```\n_np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n_np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n_np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n_np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n_np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n```\n\nOne of the possible solutions would be to refactor the sanity checks for the input arguments. Current sanity check assumes that quantized Tensor has same sanity checks as non-quantized Tensor:\n\n```\n dtypes.qint16: _FilterInt,\n dtypes.qint32: _FilterInt,\n dtypes.qint8: _FilterInt,\n dtypes.quint16: _FilterInt,\n```\n\nPS: Still working on PR to show the proposal of new sanity checks.\n", "done.\n"]}, {"number": 5567, "title": "Fix syntax error in UnchangedShape example code", "body": "This is the same patch as #5566, but applied to master.", "comments": ["Can one of the admins verify this patch?\n", "@dtrebbien, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @keveman and @tensorflower-gardener to be potential reviewers.\n", "@tensorflow-jenkins test this please\n"]}, {"number": 5566, "title": "Fix syntax error in UnchangedShape example code", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@dtrebbien, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @keveman and @rryan to be potential reviewers.\n", "The documentation fix needs to be applied to master as well.\n", "Thanks!  Do you want to send another PR to master ?\n", "I just created #5567 to apply the patch to master.\n"]}, {"number": 5565, "title": "Branch 138943977", "body": "", "comments": ["@benoitsteiner, thanks for your PR! By analyzing the history of the files in this pull request, we identified @caisq, @keveman and @tensorflower-gardener to be potential reviewers.\n", "Jenkins, test this please\n", "Let's abandon this and do a new push.\n"]}, {"number": 5564, "title": "tutorial - code", "body": "Hello Team,\r\n\r\nYesterday evening I completed your tutorial (https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#5) to take my first steps with tensorflow. During the process I had to complete the code as in label_image.py there is no import sys line.\r\n\r\nCheers!", "comments": ["Fixed!  Thanks for pointing that out.  It was correct in the gist, but not written in the code."]}, {"number": 5563, "title": "R0.11", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@cartmanatl, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @tensorflower-gardener and @gunan to be potential reviewers.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}, {"number": 5562, "title": "square of square root is identity", "body": "```python\r\nimport tensorflow as tf\r\n\r\ntensor = tf.constant(0.0)\r\nloss = tf.square(tf.sqrt(tensor))\r\ngrad = tf.gradients(loss, tensor)\r\n\r\nwith tf.Session() as sess:\r\n    print grad[0].eval()\r\n```\r\n\r\nThis gives `nan`. The gradient of `sqrt(x)` with respect to `x` is `inf` at `0` which is likely causing the problem.\r\n\r\nWould be useful to have automatic inference for this such that `sqrt` followed by `square` can be removed at least for the purpose of gradient computation. There are other similar cases (e.g. `exp` of `log`) where such inference would help.", "comments": ["The toy example in the previous post may be bad because it tries to compute gradient at 0 but the square root is not defined below zero. What I am actually trying to do is closer to this:\n\n``` python\nimport tensorflow as tf\n\ntensor1, tensor2 = tf.constant(1.0), tf.constant(1.0)\ntensor3, tensor4 = tf.constant(2.0), tf.constant(2.0)\ntensor_list = [tensor1 - tensor2, tensor3 - tensor4]\nloss = tf.square(tf.global_norm(tensor_list))\n\ngrad = tf.gradients(loss, tensor1)[0]\nwith tf.Session() as sess:\n    print grad.eval()\n```\n", "@andydavis1 , could you please take a look? Thanks.\n", "I don't think it's right to permit square(sqrt) for semi-negative values, except negative zero (as per https://en.wikipedia.org/wiki/Signed_zero, your first example). At zero, this is a boundary value, and your code should probably not rely on any particular behavior for the gradient, since it's undefined.\n", "> This gives nan. The gradient of sqrt(x) with respect to x is inf at 0 which is likely causing the problem.\n\nI think your second example is just a more complicated instance of the problem you are describing in your first example, cf. the sqrt here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard0/tf.global_norm.md\n\nOne way to attack this problem is that you explicitly define the desired behavior in the limit case. I think you would have to resort to defining your operator own gradient for that op because `tf.select` are not to be trusted. At least I have had bad experiences with trying to do that. Now I often I tend to use `tf.clip_by_value` to handle these problems. Ugly, but often does the trick.\n\nSide note: We can understand why we are getting a NaN in the gradient backprop computation by applying the chain rule:\n\n> ### Define\n> \n> loss(x) = sqr(sqrt(x))\n> loss(y) = sqr(y)\n> y(x) = sqrt(x)\n> d{loss}/d{x} = d{loss}/d{y} \\* d{y}/d{x} = 2 y (1/2) 1/sqrt(x).\n> \n> ### Plug x=0, y(0)=0 into the expression above,\n> \n> 2 (0) (1/2) 1/sqrt(0) = 0 (-Inf) = NaN.\n> \n> ### we could symbolically, for x>0, simplify this to\n> \n> sqrt(x) 1/sqrt(x) = 1\n> \n> ### and extend that definition to x=0, why not\n", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 5561, "title": "enable avgpooling for windows gpu builds", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@guschmue, thanks for your PR! By analyzing the history of the files in this pull request, we identified @zheng-xq, @keveman and @benoitsteiner to be potential reviewers.\n", "this enables #5532 for gpu \n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins test this please.\n", "@gunan: It looks like the PIP installation is failing: https://ci.tensorflow.org/job/tensorflow-pr-win-cmake-py/63/console\n\n```\n07:49:25 c:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build>SET PIP_EXE=\"C:\\Program Files\\Anaconda3\\Scripts\\pip.exe\" \n07:49:25 \n07:49:25 c:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build>\"C:\\Program Files\\Anaconda3\\Scripts\\pip.exe\" uninstall -y tensorflow \n07:49:26 Cannot uninstall requirement tensorflow, not installed\n07:49:26 You are using pip version 8.1.2, however version 9.0.1 is available.\n07:49:26 You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n07:49:26 \n07:49:26 c:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build>\"C:\\Program Files\\Anaconda3\\Scripts\\pip.exe\" install --upgrade c:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\tf_python\\dist\\tensorflow-0.11.0rc2_cmake_experimental-py3-none-any.whl \n07:49:26 Processing .\\tf_python\\dist\\tensorflow-0.11.0rc2_cmake_experimental-py3-none-any.whl\n07:49:28 Requirement already up-to-date: wheel>=0.26 in c:\\program files\\anaconda3\\lib\\site-packages (from tensorflow==0.11.0rc2-cmake-experimental)\n07:49:28 Collecting protobuf==3.0.0 (from tensorflow==0.11.0rc2-cmake-experimental)\n07:49:28   Using cached protobuf-3.0.0-py2.py3-none-any.whl\n07:49:28 Requirement already up-to-date: six>=1.10.0 in c:\\program files\\anaconda3\\lib\\site-packages (from tensorflow==0.11.0rc2-cmake-experimental)\n07:49:28 Requirement already up-to-date: numpy>=1.11.0 in c:\\program files\\anaconda3\\lib\\site-packages (from tensorflow==0.11.0rc2-cmake-experimental)\n07:49:29 Collecting setuptools (from protobuf==3.0.0->tensorflow==0.11.0rc2-cmake-experimental)\n07:49:29   Using cached setuptools-28.8.0-py2.py3-none-any.whl\n07:49:29 Installing collected packages: setuptools, protobuf, tensorflow\n07:49:29   Found existing installation: setuptools 23.0.0\n07:49:29 Cannot remove entries from nonexistent file c:\\program files\\anaconda3\\lib\\site-packages\\easy-install.pth\n07:49:29 You are using pip version 8.1.2, however version 9.0.1 is available.\n07:49:29 You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n```\n\nThe build _seems to be_ succeeding (if I'm reading the logs correctly), but is there any way to tell why PIP is failing to install the `tensorflow` module?\n", "I think you are right.\npip install seems to be failing with:\n`Cannot remove entries from nonexistent file c:\\program files\\anaconda3\\lib\\site-packages\\easy-install.pth`\n\nLet's rerun to see if it is a flake:\nJenkins, test this please\n", "Could it be a bad Anaconda interaction? Issue #3324 seemed to hit the same problem on Linux with Anaconda.\n", "...and this seems to affect not just TF:\n\nhttps://github.com/ContinuumIO/anaconda-issues/issues/542\n", "It was some kind of flake in windows, we are back to only reader ops test failing.\nMerging.\n"]}, {"number": 5560, "title": "Branch 138939121", "body": "", "comments": ["@benoitsteiner, thanks for your PR! By analyzing the history of the files in this pull request, we identified @caisq, @keveman and @tensorflower-gardener to be potential reviewers.\n", "Jenkins, test this please.\n", "Jenkins, test this please\n", "Should we also abandon this one?\n", "Closing in favor of #5603 please reopen if I am missing anything.\n"]}, {"number": 5559, "title": "print_selective_registration_header", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nprint_selective_registration_header is not working with TF master.\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nMacOSX\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n1. create ops_to_register.h and copy to tensorflow/\r\n2. go to tensorflow/tensorflow.bzl\r\n3. add -DSELECTIVE_REGISTRATION and -Itensorflow/ to tf_copts\r\n4. bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\nreturns with various (clustering, logging) ops missing error.\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["@petewarden \n", "@cwhipkey has anything changed with this recently?\n", "Around Oct 21, a change was made so that the format of ops_to_register.h is\nslightly different, but the print_selective_... tool should be in sync with\nthat.\n\nAlso, an \"ops missing error\" would seem more likely to happen when running\nthan during compilation.\n- What is the error that bazel prints at this point?\n- Can you share the generated ops_to_register.h?\n\nOn Fri, Nov 11, 2016 at 4:27 PM, Pete Warden notifications@github.com\nwrote:\n\n> @cwhipkey https://github.com/cwhipkey has anything changed with this\n> recently?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5559#issuecomment-260086584,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AQw4wVpWx7SenjVoId-ZWM9f4FxQTJipks5q9QfjgaJpZM4KwPzZ\n> .\n", "is this a reasonable path to pursue or the intend of use for this tool is different (no bazel build intended?) what is the proper way of using ops_to_register.h? Thanks!\n", "@emreyamangilsc Can you provide the information Chad requested?\n", "Closing due to lack of activity."]}, {"number": 5558, "title": "Add string tensor tags to new summary interface", "body": "Old/Deprecated summary ops can accept `string tensors` for `tags`. The new summary interface only allows `strings` for `names` (subsequently used as tags). This pull adds `prefix` option, which can be a `string tensor`. \r\n\r\nThis is important because sometimes we want to define the tags symbolically based on some conditions of input placeholders. ", "comments": ["@thuyen, thanks for your PR! By analyzing the history of the files in this pull request, we identified @danmane, @drpngx and @tensorflower-gardener to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "@danmane @drpngx Can one of the admins have a look at this pull request?", "@danmane knows best.", "Is there a need for the \"prefix\" argument? Why not just have the summary wrapper accept a string tensor as name, and convert it into a string?\r\n\r\nAdding prefix as another arg seems orthogonal to the main goal of the pr", "@danmane Could you tell me the function for `string_tensor to string` conversion. I couldn't find it in current API. If we have that function then we don't need this PR at all. ", "I am still not sure if such function exists since `tensor` value would be determined dynamically at run time while `string` needs static literals. ", "What would the prefix be used for? Can't you achieve the same by wrapping the call to the summary in a name_scope? ", "`name_scope` couldn't be a `string tensor`. \r\nWith the old interface, one can create a single graph for both training and validation (like the following). In the new interface, it's only possible with some new argument `prefix` (a `tensor`) because the `name` argument can only be `string`. An example:\r\n```\r\nis_training = tf.placeholder(tf.bool)\r\ninputs, labels = data(is_training)\r\noutputs = model(inputs, is_training)\r\nloss = tf.reduce_mean(tf.square(output-labels))\r\ntrain_op = ....\r\n\r\nprefix = tf.cond(is_training, lambda: tf.constant('train/'), lambda: tf.constant('valid/'))\r\ntf.summary.scalar('loss', loss, prefix=prefix)\r\nsummary_op = tf.summary.merge_all()\r\n\r\n# Each training iteration\r\nsess.run([train_op, summary_op], feed_dict={is_training: True})\r\n\r\n# Validation\r\nsess.run(summary_op, feed_dict={is_training: False})\r\n```\r\n", "+1 for implementing some way of using string tensor as summary name.\r\n\r\nI frequently used the following scenario, where I want to measure accuracy on train/devel/test data (to appear in different graphs in TensorBoard):\r\n```python\r\n    dataset_name = tf.placeholder(tf.string, [])\r\n    summary = tf.scalar_summary(self.dataset_name+\"/accuracy\", accuracy)\r\n```\r\nThat way I could use the same `summary` with any of the data:\r\n```python\r\n   summary = session.run([summary], {dataset_name:\"train\" / \"devel\" / \"test\"})\r\n```\r\nA `prefix` option would be ideal for me, but generaly any way of using string tensor is. Currently I have to create separate summary operations for every dataset I want to evaluate on.", "+1 for passing an optional string/string tensor (but not prefix) as names. This would also resolve #6150 ", "@dandelionmane is that the right way to go about this?", "@dandelionmane any advice on how to proceed here?", "@thuyen Thanks for the pull request, and sorry for the really slow review on my part.\r\n\r\nI think this is a fine proposal. I would just like more descriptive docstrings that explain why the new argument should be used.", "Adding @martinwicke for API review triage, if needed.\r\n\r\nAlso, a test is warranted here, probably.", "Please update it for the (tensor_summary)[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/summary_ops.py#L38] as well. This will be more tricky because the tensor_summary has a different implementation and things are a little inconsistent. \r\nPlease also add tests.\r\n\r\nIn the long run, I think we may want to have all of the summaries set the node_name part of the summary protobuf according to the kernels' node name, and set the tag according to things like the prefix and the original text the user provided to the summary op (as opposed to the deduplicated, name-scope'd name)", "Why the solution here is prefix instead of just giving the whole string? Giving the whole string would also fix #6150 together", "@dandelionmane, what about @ppwwyyxx's comment?", "@dandelionmane any progress?", "@dandelionmane `tensor_summary` has different interface, taking only the `tensor` as [input](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/logging_ops.cc#L68), no `tag`, and thus couldn't take a string tensor as tag. This is probably inline with the new summary interface . I should wait for more unified solution from google and close this?"]}, {"number": 5557, "title": "make RestoreV2() work for windows", "body": "RestoreV2() did not work because wildcard match had some issues.\r\nEnabled RTTI for windows as well.", "comments": ["Can one of the admins verify this patch?\n", "@guschmue, thanks for your PR! By analyzing the history of the files in this pull request, we identified @mrry, @zheng-xq and @Yangqing to be potential reviewers.\n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins test this please\n", "@rmlarsen The Windows failure here is unrelated, and we have a fix for it in the works. This one should be good to merge at your convenience....\n", "@tensorflow-jenkins test this please.\n"]}, {"number": 5556, "title": "R0.11", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@neohoon, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @tensorflower-gardener and @gunan to be potential reviewers.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}, {"number": 5555, "title": "GPU-enabled Mac build of TensorFlow version 0.11.0rc2-py2 was compiled against cuDNN v5.1", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nThis is a regression of issue #3826.\r\n\r\n### Environment info\r\nOperating System: macOS Sierra Version 10.12.1 (16B2555)\r\n\r\nInstalled version of CUDA and cuDNN: cuda_8.0.47_mac, cudnn-8.0-osx-x64-v5.1 (I explain why I am using cuDNN v5.1 rather than cuDNN v5 below.)\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n```\r\nlrwxr-xr-x  1 root  admin     13 Nov 11 17:13 /usr/local/cuda/lib/libcuda.1.dylib -> libcuda.dylib\r\n-rwxr-xr-x@ 1 root  wheel  13504 Sep 26 17:59 /usr/local/cuda/lib/libcuda.dylib\r\nlrwxr-xr-x@ 1 root  wheel     45 Sep 26 18:00 /usr/local/cuda/lib/libcudadevrt.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudadevrt.a\r\nlrwxr-xr-x@ 1 root  wheel     50 Sep 26 18:00 /usr/local/cuda/lib/libcudart.8.0.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.8.0.dylib\r\nlrwxr-xr-x@ 1 root  wheel     46 Sep 26 18:00 /usr/local/cuda/lib/libcudart.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.dylib\r\nlrwxr-xr-x@ 1 root  wheel     49 Sep 26 18:00 /usr/local/cuda/lib/libcudart_static.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart_static.a\r\nlrwxr-xr-x  1 root  admin     47 Nov 11 17:29 /usr/local/cuda/lib/libcudnn.5.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn.5.dylib\r\nlrwxr-xr-x  1 root  admin     45 Nov 11 17:29 /usr/local/cuda/lib/libcudnn.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn.dylib\r\nlrwxr-xr-x  1 root  admin     48 Nov 11 17:29 /usr/local/cuda/lib/libcudnn_static.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn_static.a\r\n```\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: `export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow-0.11.0rc2-py2-none-any.whl`\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.1.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.dylib locally\r\n0.11.0rc2\r\n```\r\n\r\nIf installed from source: Not Applicable\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n1. Set up TensorFlow according to the instructions at  \r\nhttps://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#prepare_environment_for_mac_os_x\r\n\r\n    In particular, the page says to install cuDNN v5 from the cuDNN Download page.\r\n2. Test the installation by running:\r\n\r\n        cuda-memcheck python -m tensorflow.models.image.mnist.convolutional\r\n\r\n    You should see:\r\n\r\n    > E tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded runtime CuDNN library: 5005 (compatibility version 5000) but source was compiled with 5105 (compatibility version 5100).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.\r\n\r\n### What other attempted solutions have you tried?\r\nUninstalling cuDNN v5 and installing cuDNN v5.1 (labeled \"cuDNN v5.1 (August 10, 2016), for CUDA 8.0\" on the cuDNN Download page) fixes the issue.\r\n\r\nAs before in issue #3826, the installation instructions should be corrected to specify that cuDNN v5.1 needs to be installed.", "comments": ["cc @yifeif \n\nWill update the docs,\nWhich documentation have you been using specificly?\n", "I was following:\nhttps://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#prepare_environment_for_mac_os_x\n\nThe master version of these docs appears to be the same:\nhttps://www.tensorflow.org/versions/master/get_started/os_setup.html#prepare_environment_for_mac_os_x\n", "@yifeif, did we update the documentation about this one?", "Updated the instruction at #6407. Should be up on the website tomorrow."]}, {"number": 5554, "title": "Unable to install GPU enabled TensorFlow", "body": "To install TensorFlow with GPU on an Ubuntu system, I installed CUDA v 8.0 using `cuda-repo-ubuntu1404_8.0.44-1_amd64.deb` and cuDNN using `cudnn-8.0-linux-x64-v5.1`.\r\n\r\nThe following are the environment variables in ~/.profile file: \r\n\r\n```\r\nLD_LIBRARY_PATH=/usr/local/cuda/lib64\r\nCUDA_PATH=/usr/local/cuda\r\n```\r\n\r\nOn running the ./configure command inside the tensorflow folder the following error is displayed:\r\n\r\n![image](https://cloud.githubusercontent.com/assets/17990840/20231345/07e4f14e-a82f-11e6-9552-c19876c17012.png)\r\n\r\nSeeking assistance from @drpngx \r\n\r\nAny help is appreciated.\r\n\r\n", "comments": ["Cross-linking from stackoverflow: http://stackoverflow.com/questions/40555477/unable-to-install-gpu-enabled-tensorflow?noredirect=1#comment68351563_40555477\n", "How about `sudo apt-get install libcupti-dev` ? That's required for the profiler capabilities.\n", "@drpngx: I am unable to install the `libcupti-dev` package, the following error is displayed `E: Unable to locate package libcupti-dev`\n", "It should be part of your regular ubuntu distro. Which one are you running 14, 15 or 16?\n", "I am running Ubuntu 14.04\n", "OK, then this should work for you: https://installion.co.uk/ubuntu/trusty/multiverse/l/libcupti-dev/install/index.html\n", "@drpngx: I was successfully able to install libcupti-dev, however on running ./configure command the same error is displayed again. \n![image](https://cloud.githubusercontent.com/assets/17990840/20232084/f072bf64-a833-11e6-835f-6dd852d23e86.png)\n", "We probably want to run `dpkg -S libcupti` to see where it's installed.\n", "I suggest using cuda8.0 **local** deb file to install cuda, download from here\nhttps://developer.nvidia.com/cuda-downloads\nremove all cuda pkg first before reinstall\n\n```\nsudo dpkg -i cuda-repo-ubuntu1404-8-0-local_8.0.44-1_amd64.deb\nsudo apt-get update\nsudo apt-get install cuda\n```\n\nand use\n\n```\ntar xvzf cudnn-8.0-linux-x64-v5.1.tgz\nsudo cp -P cuda/include/cudnn.h /usr/local/cuda/include\nsudo cp -P cuda/lib64/libcudnn* /usr/local/cuda/lib64\nsudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*\n```\n\nto install cuDNN,\nand use all default value when you do ./configure\n", "@willdzeng: On uninstalling cuda and reinstalling it and running the command `sudo apt-get update`, the update does not finish successfully. Please find the screenshot of the error attached.\n![image](https://cloud.githubusercontent.com/assets/17990840/20271301/59cd8da6-aa58-11e6-8de5-eddedee5b0d6.png)\n![image](https://cloud.githubusercontent.com/assets/17990840/20271319/654902fa-aa58-11e6-9c59-a10876f6bab1.png)\n", "So, did you manage to get any version of `libcupti` on your system? If so, just symlink it as instructed above.\n", "I started the entire installation process again and followed the instructions given by @willdzeng. However on performing `sudo apt-get update`, I am facing the problem mentioned above. Because of which I am unable to install CUDA and cuDNN. Can you guide me with it?\n", "@drpngx: On running `./configure` command the following error is displayed:\n\n![image](https://cloud.githubusercontent.com/assets/17990840/20279305/107d0a32-aa76-11e6-8be3-cfbc1e6ed3ed.png)\n\nAnd result of command `dpkg -S libcupti` is shown in below image:\n![image](https://cloud.githubusercontent.com/assets/17990840/20279343/3c8eb74c-aa76-11e6-96fd-fe1b1a1b9b6e.png)\n", "looks like you have serval different version of cuda installed in your system.\nUse\n`sudo apt-get purge cuda-.`\nremove all cuda first and then install the 8.0 from the local deb file.\nalso looks like you have some linkage broken with the dpkg that causing you failing the apt-get update,\nyou should go to setting/Software&Update/other software to clean your source list, especially remove those who are failing.\n\n![image](https://cloud.githubusercontent.com/assets/12994633/20282052/f0a701b2-aa67-11e6-9a4f-608feb9f8490.png)\n", "It looks like this is in `/usr/x86_64-linux-gnu`, so you can symlink it from there.\n", "Closing due to lack of activity."]}, {"number": 5553, "title": "Running own TensorFlow model on Android gives native inference error: \u201cSession was not created with a graph before Run()!\u201d", "body": "I was able to run the Inception-v3 model on Android just fine, and I now want to run my own trained TensorFlow model on Android. I'm following the approach from [TensorFlow's image recognition tutorial](https://www.tensorflow.org/versions/r0.11/tutorials/image_recognition/index.html) and the Android TensorFlow demo, and adapting as necessary. My changes include: (a) integrating Android OpenCV as part of the bazel build (b) using own model and label file and (c) adjusting parameters (img_size, input_mean, input_std, etc.) accordingly.\r\n\r\nFrom Android logcat, running my model with the tensorflow android demo app gives:\r\n\r\n```\r\nE/native: tensorflow_inference_jni.cc:202 Error during inference: Invalid argument: Session was not created with a graph before Run()!\r\n...\r\nE/native: tensorflow_inference_jni.cc:159 Output [output/Softmax:0] not found, aborting!\r\n\r\n```\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nOwn (duplicate) SO thread: http://stackoverflow.com/questions/40555749/running-own-tensorflow-model-on-android-gives-native-inference-error-session-w\r\n\r\n### Environment info\r\nOS X Yosemite (10.10.5), LGE Nexus 5 (Android 6.0.1), Android SDK 23, Android OpenCV SDK 23, Bazel 0.4.0.\r\n\r\n### Steps taken\r\n\r\n1. Saved own model's checkpoint (.ckpt) and graph definition (.pb) files separately using `tf.train.Saver()` then `tf.train.write_graph()`\r\n2. Froze graph using freeze_graph.py (using bazel), gives 227.5 MB file\r\n3. Optimized the graph using optimize_for_inference.py (additionally tried strip_unused.py)\r\n4. Copied frozen, optimized, or stripped graph to android/assets\r\n5. Doubled the total byte limit using `coded_stream.SetTotalBytesLimit()` in jni_utils.cc to handle my large model size\r\n6. Built the tensorflow android app using bazel\r\n7. Installed on android device using adb and bazel\r\n\r\nAs a sanity check, I have tested my model in C++ built with bazel following the tutorial here [label_image](https://www.tensorflow.org/versions/r0.8/how_tos/image_retraining/index.html), and my model correctly outputs a prediction. I have also tried playing with the order by which I save my graph def and checkpoint files before freezing, but no change.\r\n\r\nAny help would be great. \r\ncc @drpngx @andrewharp \r\n", "comments": ["@abdoelali Are you certain that `output/Softmax:0` is correct node for your graph? Possibly it's capitalized differently?\n", "@andrewharp Yes, pretty sure that `output/Softmax` is the correct node name, and `output/Softmax:0` the correct tensor name, as seen through the logs from `graph.get_operations()`. I've also tested both just in case, but it seems the error is occurring before (at `Session was not created with a graph before Run()!`).\n\nThe funky naming is due to using TFlearn to train my network, where the final FC layer is called in Python as: `fully_connected(self.network, len(CLASSES), activation = 'softmax', name='output')`. An export of my graph is [here](https://github.com/abdoelali/scratch/blob/master/graph_def_export.txt).\n\nAlso, this is the latest tensorflow commit I'm on: Mon Oct 31 09:15:24 2016 -0700 `3ccdb2b695586201cde65e079806c5941ae542b6`\n", "You may be running into https://github.com/tensorflow/tensorflow/issues/5111. The simple workaround is to not compress the pb in your apk by adding  `nocompress_extensions = [\".pb\",],` to the android_binary target. Updating the pb loading logic for a real fix is on my TODO list.\n\nIf that doesn't fix it, can you paste the adb logcat of a full run?  It could also be that it's either not finding the graph or TF isn't being built with all the required operators.\n\n`adb logcat *:E native:V tensorflow:V` should catch everything relevant.\n", "Adding no compression for .pb extensions seems did not help. Full run (using `optimized_cnn_graph.pb`) of adb logcat [here](https://github.com/abdoelali/scratch/blob/master/adb_logcat_tf.txt), and android tf BUILD [here](https://github.com/abdoelali/scratch/blob/master/BUILD.android_tf). Tested also on `stripped_inception_graph.pb`.\n", "Further sanity check: trained and ran a test 32-ResNet model (10e,128bs), with resultant stripped model size of 8.3MB. Same error occurs, so doesn't seem to depend on .pb size. ResNet graph def [here](https://github.com/abdoelali/scratch/blob/master/test_resnet_graph_def.txt) and accompanying adb logcat [here](https://github.com/abdoelali/scratch/blob/master/adb_logcat_resnet_tf.txt). \n", "@abdoelali  Check this line `tensorflow_inference_jni.cc:131 Creating session` in android logcat message .There is a possibility that the graph has not been created correctly. \n", "@shastakr I can't find tensorflow_inference_jni.cc:131, but I noticed that for the ResNet graph, it was failing because of `tensorflow_inference_jni.cc:133 Could not create Tensorflow Graph` (which has to do with batchnormalization), but not so for earlier CNN model. Currently testing whether models implemented in Keras also result in the same error.\n", "If there exists `tf.cond` (which is switch for train/validation at the same code), it cannot be correctly converted. So I change `tf.cond` to python if-else (like `tf.contrib.layers.batch_norm`'s `smart_cond`) for my own batchnorm layers and it works. It is difficult to deal with batchnorm.", "I had the same issue. Cost me several hours to find that out, but finally solved it. Maybe you did the same error.\r\n\r\nI changed the modelfile to:\r\n```java\r\nprivate static final String MODEL_FILE = \"my_frozen_graph.pb\";\r\n```\r\n`assetManager.open` call said it can open and read the file and tensorflow reported a success (0) and no exception and no debug message when calling `inferenceInterface.initializeTensorflow(assetManager, modelFilename)`\r\n\r\nSo I wrongly assumed that loading worked. It was no error with the input and output naming or the pb file (frozen with the freeze python script) it simply was tensorflow not finding the pb file but giving no error message.\r\n\r\nTHE SOLUTION was to change the model file to a path that the assetManager.open call does not find (on my phone) but tensorflow does.\r\n```java\r\nprivate static final String MODEL_FILE = \"file:///android_assets/my_frozen_graph.pb\";\r\n```\r\n\r\nA suggestion for tensorflow would be to improve the API to correctly report, if loading worked or if e.g. the file could not be found.", "@shastakr I'm not really working with ResNet's, so I'm leaving all matters of batch normalization alone for now", "@penguinmenac3 This can't be it, as the .pb file is certainly being read. I know this because: (a) the out of the box inception graph file works correctly (and is placed in the assets dir) and (b) changing my frozen graph file name to one that doesn't exist causes a fatal error. \r\n\r\nSo, still stuck on:\r\n\r\n` tensorflow_inference_jni.cc:202 Error during inference: Invalid argument: Session was not created with a graph before Run()!`  ", "I also have the same issue. I posted it on [stackoverflow](http://stackoverflow.com/questions/41252122/tensorflow-on-android-error-during-inference-invalid-argument-session-was-not). \r\n\r\nWhat I have noticed in [tensorflow_inference_jni.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/android/jni/tensorflow_inference_jni.cc) is that the GraphDef is instantiated and then the model is read with ReadFileToProtoOrDie function. I printed the node count after and before it and it is 0, so in my case, there is a problem with this function.\r\n\r\nMaybe there is a problem on how the pb parsing is being made. For creating my model I used Anaconda with tensorflow 0.12.0rc and protobuf 3.1.0 installed with pip but I built the Android TF Lib from source code.", "@abdoelali I noticed this line in your logcat dump which seems to explain the error you're seeing:\r\n`11-15 11:50:15.011 17053 17053 E native  : tensorflow_inference_jni.cc:133 Could not create Tensorflow Graph: Invalid argument: Input 0 of node BatchNormalization/cond/AssignMovingAvg_1/Switch was passed float from BatchNormalization/moving_variance:0 incompatible with expected float_ref.`", "@ivancruzbht Still not sure what's going on in your case. Though 7ms seems very fast to initialize (it takes me 70ms to initialize with a .5mb graph), so it probably is just failing to load the graphdef properly. What is the size of your file? Is it binary or text?\r\n\r\n@penguinmenac3 Yes, we should do a better job of failing fast and reporting initialization errors clearly -- will look into improving this.", "@andrewharp I just found out the issue. It seems that ReadFileToProtoOrDie in [jni_utils.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/android/jni/jni_utils.cc#L78) needs the prefix \"file:///android_asset/\" in the file name to use the AssetManager to open the GraphDefFile. Otherwise it will try to open the file standalone. For some reason not using the AssetManager does not open correctly the pb file. Then I had an issue in my Gradle script that was not executing the non-compression of the pb file. Once I fixed that I could load the Graph.\r\n\r\nMaybe a couple of \"File Not Found\" or \"AssetManager cannot find the file\" logs would improve the initialization :)", "@ivancruzbht Right -- if you leave off the prefix it just treats the location as a regular filepath on device (e.g. you could put your file in /sdcard/ somewhere -- there's no reason that PBs need to be bundled with the app in assets).\r\n\r\nI suppose the assumption was that proto->ParseFromCodedStream would return false if the file wasn't found, and then the CHECK on ReadFileToProtoOrDie would catch that. That's obviously not the case, so I'll look into making it more robust.", "@andrewharp the `tensorflow_inference_jni.cc:133` error might explain my ResNet experiment (which indeed gives batch normalization error), but not my CNN experiment I initially linked to (this [logcat](https://github.com/abdoelali/scratch/blob/master/adb_logcat_tf.txt)).", "@ivancruzbht Also have a [StackOverflow question](http://stackoverflow.com/questions/40555749/running-own-tensorflow-model-on-android-gives-native-inference-error-session-w) open.\r\n\r\nWill try printing the node count and inspect the ReadFileToProtoOrDie in ` jni_utils.cc`.", "I've updated ReadFileToProtoOrDie internally to help it live up to the latter part of its name in the case of pb files not being found. Also added a check that the resulting GraphDef has > 0 nodes to catch anything else slipping through. Should be getting pushed to GitHub soon, so hopefully that will help discriminate between loading errors and actual TF issues.", "With https://github.com/tensorflow/tensorflow/commit/c8abaee1e8dd9a2a654d0499e431a82e0285c5d1\r\nto check for missing files and \r\nhttps://github.com/tensorflow/tensorflow/commit/522046158048c5c6a7c12ec8975b1178eed7165c to check for 0 node counts in the GraphDef, most initialization errors should now be caught much earlier.\r\n\r\n@abdoelali If you sync again does it still give you the same error? You're also getting a suspiciously fast 7ms initialization time so I'm assuming it was a problem loading the graph.\r\n", "Additionally a non-zero return code will throw a RuntimeException with https://github.com/tensorflow/tensorflow/commit/c9722179a8ae49ca1438c957e51c737ed713087f, so I think it's unlikely any initialization problems can throw inference errors at this point..\r\n\r\n@abdoelali Did you resolve your issue?", "@andrewharp Ok, some progress! \r\n\r\nIf I sync with 522046158048c5c6a7c12ec8975b1178eed7165c and c9722179a8ae49ca1438c957e51c737ed713087f, but not c8abaee1e8dd9a2a654d0499e431a82e0285c5d1, my model crashes with following logcat:\r\n\r\n`01-05 15:48:28.062 11639-11639/? I/native: tensorflow_inference_jni.cc:82 Creating new session variables for 8dc6c085059a97b8\r\n01-05 15:48:28.062 11639-11639/? I/native: tensorflow_inference_jni.cc:105 Loading Tensorflow.\r\n01-05 15:48:28.062 11639-11639/? I/native: tensorflow_inference_jni.cc:107 Making new SessionOptions.\r\n01-05 15:48:28.062 11639-11639/? I/native: tensorflow_inference_jni.cc:110 Got config, 0 devices\r\n01-05 15:48:28.064 11639-11639/? I/native: tensorflow_inference_jni.cc:114 Session created.\r\n01-05 15:48:28.064 11639-11639/? I/native: tensorflow_inference_jni.cc:117 Graph created.\r\n01-05 15:48:28.067 11639-11639/? I/native: tensorflow_inference_jni.cc:121 Acquired AssetManager.\r\n01-05 15:48:28.067 11639-11639/? I/native: tensorflow_inference_jni.cc:123 Reading file to proto: file:///android_asset/cnn_frozen_graph.pb\r\n01-05 15:48:28.067 11639-11639/? A/native: tensorflow_inference_jni.cc:126 Check failed: tensorflow_graph.node_size() > 0 Problem loading GraphDef!\r\n01-05 15:48:28.067 11639-11639/? A/libc: Fatal signal 6 (SIGABRT), code -6 in tid 11639 (tensorflow.demo)`\r\n\r\nHowever, if I update `jni_utils.cc` with c8abaee1e8dd9a2a654d0499e431a82e0285c5d1, then (I guess) my graph loads, though incorrectly classifying (which can be another issue). To be sure, latest working logcat [here.](https://github.com/abdoelali/scratch/blob/master/adb_logcat_jniutil_sync.txt) Also, looking at logcat, shouldn't it print` \"TF init status != 0\"`?\r\n\r\nNote: I'm now compiling using newer bazel: `Build label: 0.4.3-homebrew`", "@abdoelali It's confusing why c8abaee by itself would cause your graph to load, as it strictly adds more checks to the initialization. It was the first of that series of commits, though, so my guess would be that you're regressing the entire tree to that point rather than cherry-picking commits to add?\r\n\r\nAlso it appears the code you have on the Java side of things is somewhat outdated, as you still have TF initialized messages coming from CameraConnectionFragment (which did not do any checking of return status). If at all possible I'd suggest syncing the entire tree to the latest codebase to make diagnosing the problem easier.", "@andrewharp Pleased to say that all is now working! Steps taken: full sync with latest tf build (last commit: 798ae42bab61da6c7dc001145ed574006b8b6acf) and built using bazel -v `0.4.3-homebrew`. Android SDK v23 and NDK v23. Stripped off all preprocessing (i.e., removing OpenCV from build), and just replaced my model and label file with out of box tf inception demo. Logcat now shows:\r\n\r\n`01-06 10:27:39.048 26344-26344/? I/TensorFlowImageClassifier: Reading labels from: mylabels.txt\r\n01-06 10:27:39.049 26344-26344/? I/TensorFlowImageClassifier: Read 7, 7 specified\r\n01-06 10:27:39.049 26344-26344/? I/native: tensorflow_inference_jni.cc:97 Native TF methods loaded.\r\n01-06 10:27:39.049 26344-26344/? I/TensorFlowInferenceInterface: Native methods already loaded.\r\n01-06 10:27:39.049 26344-26344/? I/native: tensorflow_inference_jni.cc:85 Creating new session variables for 25a68072eeb0d05\r\n01-06 10:27:39.049 26344-26344/? I/native: tensorflow_inference_jni.cc:113 Loading Tensorflow.\r\n01-06 10:27:39.053 26344-26344/? I/native: tensorflow_inference_jni.cc:120 Session created.\r\n01-06 10:27:39.053 26344-26344/? I/native: tensorflow_inference_jni.cc:126 Acquired AssetManager.\r\n01-06 10:27:39.053 26344-26344/? I/native: tensorflow_inference_jni.cc:128 Reading file to proto: file:///android_asset/cnn_frozen_graph.pb\r\n01-06 10:27:39.680 26344-26344/? I/native: tensorflow_inference_jni.cc:132 GraphDef loaded from file:///android_asset/cnn_frozen_graph.pb with 40 nodes.\r\n01-06 10:27:39.680 26344-26344/? I/native: stat_summarizer.cc:38 StatSummarizer found 40 nodes\r\n01-06 10:27:39.680 26344-26344/? I/native: tensorflow_inference_jni.cc:139 Creating TensorFlow graph from GraphDef.\r\n01-06 10:27:39.713 26344-26344/org.tensorflow.demo I/native: tensorflow_inference_jni.cc:151 Initialization done in 664.038ms\r\n01-06 10:27:39.714 26344-26344/org.tensorflow.demo I/tensorflow: ClassifierActivity: Sensor orientation: 90, Screen orientation: 0\r\n01-06 10:27:39.714 26344-26344/org.tensorflow.demo I/tensorflow: ClassifierActivity: Initializing at size 640x480`\r\n\r\nMy predictions appear to be incorrect (since currently no preprocessing), but that's a different issue. Thanks again!", "Glad to hear!"]}, {"number": 5552, "title": "[docs] python3 bytes -> string in example in README", "body": "The current README.md indicates the following \"first TensorFlow program\":\r\n\r\n```\r\n>>> import tensorflow as tf\r\n>>> hello = tf.constant('Hello, TensorFlow!')\r\n>>> sess = tf.Session()\r\n>>> sess.run(hello)\r\nHello, TensorFlow!\r\n>>> a = tf.constant(10)\r\n>>> b = tf.constant(32)\r\n>>> sess.run(a+b)\r\n42\r\n>>>\r\n```\r\n\r\nThis works fine under python2, but handling of strings has [changed slightly in python3](http://stackoverflow.com/a/6269785). Suggested improvement to the example program for compatibility with both python2 and python3 (we need to [convert bytes to string](http://stackoverflow.com/a/606199)):\r\n\r\n```\r\n>>> import tensorflow as tf\r\n>>> hello = tf.constant('Hello, TensorFlow!')\r\n>>> sess = tf.Session()\r\n>>> sess.run(hello).decode(\"utf-8\")\r\nHello, TensorFlow!\r\n>>> a = tf.constant(10)\r\n>>> b = tf.constant(32)\r\n>>> sess.run(a+b)\r\n42\r\n```\r\n\r\nTested from the [wheel build](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#virtualenv-installation) of 0.11.0 (final), [converted to RPM](https://copr.fedorainfracloud.org/coprs/dgoerger/yale-zoo/package/python-tensorflow/), on Fedora 24/25.\r\n\r\nThanks!", "comments": ["@dgoerger would you like to send a PR? Thanks.\n", "@dgoerger What is the error message you see with the current version?\n", "Hmm. Further testing suggests this may be more serious than I'd thought. Type inconsistency, not just documentation.\n\n```\n$ python3\n>>> type('Hello, TensorFlow!')\n<class 'str'>\n>>> type(tf.constant('Hello, TensorFlow!'))\n<class 'tensorflow.python.framework.ops.Tensor'>\n>>> type(tf.Session().run(tf.constant('Hello, TensorFlow!')))\n<class 'bytes'>\n```\n\nAccording to [the docs](https://www.tensorflow.org/versions/master/api_docs/python/client.html#Session.run), input type Tensor outputs via numpy ndarray, which appears not to output type string. Reformatting Session.run(Tensor) when output type is bytes, to new output type string, might give the intended behaviour here. Compare non-ASCII:\n\n```\n$ python3\n>>> import tensorflow, sys\n>>> type('\u00bfC\u00f3mo se ve?')\n<class 'str'>\n>>> type(tensorflow.constant('\u00bfC\u00f3mo se ve?'))\n<class 'tensorflow.python.framework.ops.Tensor'>\n>>> type(tensorflow.Session().run(tensorflow.constant('\u00bfC\u00f3mo se ve?')))\n<class 'bytes'>\n>>> type(tensorflow.Session().run(tensorflow.constant('\u00bfC\u00f3mo se ve?')).decode(sys.getdefaultencoding()))\n<class 'str'>\n>>> tensorflow.Session().run(tensorflow.constant('\u00bfC\u00f3mo se ve?'))\nb'\\xc2\\xbfC\\xc3\\xb3mo se ve?'\n>>> tensorflow.Session().run(tensorflow.constant('\u00bfC\u00f3mo se ve?')).decode(sys.getdefaultencoding())\n'\u00bfC\u00f3mo se ve?'\n```\n\n... and of course the same doesn't work on python2:\n\n```\n$ python2\n>>> import tensorflow, sys\n>>> type('\u00bfC\u00f3mo se ve?')\n<type 'str'>\n>>> type(tensorflow.constant('\u00bfC\u00f3mo se ve?'))\n<class 'tensorflow.python.framework.ops.Tensor'>\n>>> type(tensorflow.Session().run(tensorflow.constant('\u00bfC\u00f3mo se ve?')))\n<type 'str'>\n>>> type(tensorflow.Session().run(tensorflow.constant('\u00bfC\u00f3mo se ve?')).decode(sys.getdefaultencoding()))\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 0: ordinal not in range(128)\n>>> # yikes, python2 default encoding is ascii\n>>> type(tensorflow.Session().run(tensorflow.constant('\u00bfC\u00f3mo se ve?')).decode('utf-8'))\n<type 'unicode'>\n>>> tensorflow.Session().run(tensorflow.constant('\u00bfC\u00f3mo se ve?'))\n'\\xc2\\xbfC\\xc3\\xb3mo se ve?'\n>>> tensorflow.Session().run(tensorflow.constant('\u00bfC\u00f3mo se ve?')).decode('utf-8')\nu'\\xbfC\\xf3mo se ve?'\n```\n\nSo I guess fixing this discrepancy will require evaluating what type object should be output by tensorflow.Session().run(Tensor(string)). On the other hand, is it possible non-ASCII doesn't work at all under the current Session.run() python2 implementation? The python2 output seems non-ideal, but maybe I'm missing a common trick.\n", "TensorFlow converts `str` to `bytes` in most places, including `sess.run`, and this is unlikely to change.  The user is free to convert back, but unfortunately it's too large a change to add a unicode dtype to the core.  Closing as won't fix for now.\n", "For the record, it's useful to use\r\n\r\n```\r\nfrom tensorflow.python.util import compat\r\n\r\ncompat.as_bytes(...)\r\ncompat.as_text(...)\r\n```\r\n\r\nto do the conversions between bytes and string when needed."]}, {"number": 5551, "title": "Inconsistent behavior for variables_collections and outputs_collections parameters (contrib/layers)", "body": "Using layers from contrib,\r\n- The outputs_collections parameter accepts either a string or list of strings.\r\n- The variables_collections parameter requires a list of strings.\r\n\r\nGranted, the documentation does specify this\r\n```\r\nvariables_collections: optional list of collections for all the variables or\r\n      a dictionay containing a different list of collection per variable.\r\noutputs_collections: collection to add the outputs.\r\n```\r\nBut I don't think it's very obvious.  Furthermore, is it really necessary to have these two parameters behave differently?\r\n\r\nThe variables_collections parameter is passed to the model_variable function()\r\nCurrently, the model_variable() function performs this before creating the variable:\r\n```\r\n  collections = list(collections or [])\r\n  collections += [ops.GraphKeys.VARIABLES, ops.GraphKeys.MODEL_VARIABLES]\r\n```\r\nSo, if\r\n```\r\n# variables_collections = 'kernels'\r\ncollections\r\n['k', 'e', 'r', 'n', 'e', 'l', 's']\r\n```\r\n\r\n\r\nWhereas, for outputs_collections it uses the utils.collect_named_outputs() function, wherein\r\n```\r\n# variables_collections = 'kernels'\r\nnames = (names,) if isinstance(names, six.string_types) else set(names)\r\nnames\r\n('kernels',)\r\n```\r\n\r\nPerhaps the behavior of these two parameters should be unified?", "comments": ["This is a bug. Thanks for reporting.\n", "The outputs_collections behavior is clearly the correct one. Both arguments should accept the same types of input, that is, either strings or set types. \n", "We chose not to include the collections arguments when we ported the layers to core. \r\n\r\nWhen using the core layers: If you need access to the variables, use the scope. If you want to add the activations to a collection, do that using the returned tensor.\r\n\r\nI'm closing this, since the contrib layers are being deprecated in favor of those in core.", "the bug still exists in` tf.contrib.layers.batch_norm(variables_collections=['vars'], outputs_collections='vars2')`. If you do just` variables_collections='vars'` it will return an empty list. Annoying if you do not know this thread.\r\n\r\n", "Pretty sure it's not just batch_norm.\n\nOn Sat, Jun 17, 2017, 10:31 AM Robomate <notifications@github.com> wrote:\n\n> the bug still exists in\n> tf.contrib.layers.batch_norm(variables_collections=['vars'],\n> outputs_collections='vars2'). If you do just variables_collections='vars'\n> it will return an empty list. Annoying if you do not know this thread.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5551#issuecomment-309218406>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEsrOD0jGCHpZoMGUZEBoI3iuPK-eFjGks5sE-NbgaJpZM4KwHT0>\n> .\n>\n"]}, {"number": 5550, "title": "Checkpoint v2 breaking translate.py restore", "body": "The following restore won't work with new checkpoint v2.\r\nhttps://github.com/tensorflow/tensorflow/blob/816ecb7a342c25c19daa8b19627346e1fb38e56f/tensorflow/models/rnn/translate/translate.py#L134", "comments": []}, {"number": 5549, "title": "unable to generate tensorflow/go/op", "body": "Unable to generate ``tensorflow/go/op``, because of ``ld`` link errors, undefined reference to few stl methods.\r\n\r\nI've already tried adding ``-lstdc++ -lgcc -lc`` to ``LDFLAGS`` environmental variable and ``-ldflags`` parameter to ``go generate``\r\n\r\nTensorflow installed from source, commit hash ``20c3d37ecc9bef0e106002b9d01914efd548e66b``\r\n\r\n### Environment\r\nOperating system: \r\n```\r\nLinux 397f80c505a4 4.4.30-1-MANJARO #1 SMP PREEMPT Tue Nov 1 05:47:13 UTC 2016 x86_64 GNU/Linux\r\n```\r\n\r\n``go env`` output:\r\n```\r\nGOARCH=\"amd64\" GOBIN=\"\" GOEXE=\"\" GOHOSTARCH=\"amd64\" GOHOSTOS=\"linux\" GOOS=\"linux\" GOPATH=\"/go\" GORACE=\"\" GOROOT=\"/usr/local/go\" GOTOOLDIR=\"/usr/local/go/pkg/tool/linux_amd64\" CC=\"gcc\" GOGCCFLAGS=\"-fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build195639664=/tmp/go-build -gno-record-gcc-switches\" CXX=\"g++\" CGO_ENABLED=\"1\"\r\n\r\n```\r\n\r\n### Error message:\r\n\r\n```src/github.com/tensorflow/tensorflow/tensorflow/go/op/generate.go\r\ngo generate github.com/tensorflow/tensorflow/tensorflow/go/op                                                         \r\n# github.com/tensorflow/tensorflow/tensorflow/go                                          \r\n/lib/../lib/libtensorflow.so: undefined reference to `vtable for std::__cxx11::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4.21'                 \r\n/lib/../lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::find(char const*, unsigned long, unsigned long) const@GLIBCXX_3.4.21'                                                                  \r\n/lib/../lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::assign(char const*)@GLIBCXX_3.4.21'          \r\n/lib/../lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string()@GLIBCXX_3.4.21'              \r\n/lib/../lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_erase(unsigned long, unsigned long)@GLIBCXX_3.4.21'                                                                                 \r\n/lib/../lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_replace_aux(unsigned long, unsigned long, unsigned long, char)@GLIBCXX_3.4.21'                                                      \r\n/lib/../lib/libtensorflow.so: undefined reference to `std::basic_istream<char, std::char_traits<char> >& std::operator>><char, std::char_traits<char>, std::allocator<char> >(std::basic_istream<char, std::char_traits<char> >&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&)@GLIBCXX_3.4.21'                                     \r\n/lib/../lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_stringstream()@GLIBCXX_3.4.21'  \r\n/lib/../lib/libtensorflow.so: undefined reference to `typeinfo for std::__cxx11::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4.21'           \r\n/lib/../lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::rfind(char, unsigned long) const@GLIBCXX_3.4.21'                                                                                       \r\n/lib/../lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_append(char const*, unsigned long)@GLIBCXX_3.4.21'                                                                                  \r\n/lib/../lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned long, unsigned long)@GLIBCXX_3.4.21'                                                                               \r\n/lib/../lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::compare(unsigned long, unsigned long, char const*) const@GLIBCXX_3.4.21'                                                               \r\n/lib/../lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::reserve(unsigned long)@GLIBCXX_3.4.21'       \r\n/lib/../lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::resize(unsigned long, char)@GLIBCXX_3.4.21'  \r\n/lib/../lib/libtensorflow.so: undefined reference to `vtable for std::__cxx11::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4.21'              \r\n/lib/../lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::find(char, unsigned long) const@GLIBCXX_3.4.21'                                                                                        \r\n/lib/../lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::substr(unsigned long, unsigned long) const@GLIBCXX_3.4.21'                                                                             \r\n/lib/../lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_mutate(unsigned long, unsigned long, char const*, unsigned long)@GLIBCXX_3.4.21'                                                    \r\n/lib/../lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::find_last_of(char const*, unsigned long, unsigned long) const@GLIBCXX_3.4.21'                                                          \r\n/lib/../lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_replace(unsigned long, unsigned long, char const*, unsigned long)@GLIBCXX_3.4.21'                                                   \r\n/lib/../lib/libtensorflow.so: undefined reference to `std::thread::_M_start_thread(std::shared_ptr<std::thread::_Impl_base>, void (*)())@GLIBCXX_3.4.21'                            \r\n/lib/../lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::find_first_of(char const*, unsigned long, unsigned long) const@GLIBCXX_3.4.21'                                                         \r\n/lib/../lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_ostringstream()@GLIBCXX_3.4.21'\r\n/lib/../lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::_M_sync(char*, unsigned long, unsigned long)@GLIBCXX_3.4.21'                                                                        \r\n/lib/../lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::swap(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&)@GLIBCXX_3.4.21'                                 \r\n/lib/../lib/libtensorflow.so: undefined reference to `std::basic_istream<char, std::char_traits<char> >& std::getline<char, std::char_traits<char>, std::allocator<char> >(std::basic_istream<char, std::char_traits<char> >&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&, char)@GLIBCXX_3.4.21'                                  \r\n/lib/../lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::compare(unsigned long, unsigned long, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const@GLIBCXX_3.4.21'                                                                              \r\n/lib/../lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::find_first_not_of(char const*, unsigned long, unsigned long) const@GLIBCXX_3.4.21'                                                     \r\n/lib/../lib/libtensorflow.so: undefined reference to `VTT for std::__cxx11::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4.21'                 \r\n/lib/../lib/libtensorflow.so: undefined reference to `lgammaf@GLIBC_2.23'                 \r\n/lib/../lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_assign(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)@GLIBCXX_3.4.21'                      \r\n/lib/../lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::find_last_not_of(char const*, unsigned long, unsigned long) const@GLIBCXX_3.4.21'                                                      \r\n/lib/../lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::append(char const*)@GLIBCXX_3.4.21'          \r\n/lib/../lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::compare(char const*) const@GLIBCXX_3.4.21'   \r\n/lib/../lib/libtensorflow.so: undefined reference to `vtable for std::__cxx11::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4.21'             \r\n/lib/../lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_construct(unsigned long, char)@GLIBCXX_3.4.21'\r\n/lib/../lib/libtensorflow.so: undefined reference to `std::random_device::_M_init(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)@GLIBCXX_3.4.21'\r\n/lib/../lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::push_back(char)@GLIBCXX_3.4.21'\r\n/lib/../lib/libtensorflow.so: undefined reference to `VTT for std::__cxx11::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4.21'\r\n/lib/../lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::compare(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const@GLIBCXX_3.4.21'\r\n/lib/../lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_create(unsigned long&, unsigned long)@GLIBCXX_3.4.21'\r\n/lib/../lib/libtensorflow.so: undefined reference to `lgamma@GLIBC_2.23'\r\ncollect2: error: ld returned 1 exit status\r\n../genop/main.go:15: running \"sh\": exit status 1\r\nsrc/github.com/tensorflow/tensorflow/tensorflow/go/op/generate.go:15: running \"go\": exit status 1\r\n```\r\n", "comments": ["Hi @jhseu, do you know what's wrong?\n", "@MrJaqbq : Could you describe the version of the compiler being used (e.g. `g++ --version`).  I'm wondering if there is some C++ ABI change.\n\n@lukasz-pyrzyk : Can you reproduce the problem? If so, same question :)\n", "Sure, here it is\n\n```\n$ g++ --version\ng++ (GCC) 6.2.1 20160830\nCopyright (C) 2016 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n```\n\nand configuration\n\n```\n$ g++ -v\nUsing built-in specs.\nCOLLECT_GCC=g++\nCOLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-pc-linux-gnu/6.2.1/lto-wrapper\nTarget: x86_64-pc-linux-gnu\nConfigured with: /build/gcc-multilib/src/gcc/configure --prefix=/usr --libdir=/usr/lib --libexecdir=/usr/lib --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=https://bugs.archlinux.org/ --enable-languages=c,c++,ada,fortran,go,lto,objc,obj-c++ --enable-shared --enable-threads=posix --enable-libmpx --with-system-zlib --with-isl --enable-__cxa_atexit --disable-libunwind-exceptions --enable-clocale=gnu --disable-libstdcxx-pch --disable-libssp --enable-gnu-unique-object --enable-linker-build-id --enable-lto --enable-plugin --enable-install-libiberty --with-linker-hash-style=gnu --enable-gnu-indirect-function --enable-multilib --disable-werror --enable-checking=release\nThread model: posix\ngcc version 6.2.1 20160830 (GCC) \n```\n", "Will try to reproduce. Is it possible that `libtensorflow.so` was built with a different version (say 5.x or 4.x)? Or in other words, how did you build it - one the same machine? With the same compiler version?\n", "@asimshankar yeah, you were right, my bad.\n`libtensorflow.so` was built on the environment I described earlier, but `tensorflow/go/op` was generated on other machine. I rearranged my setup to build everything in one place.\nNow unresolved references to stl are gone, but I'm getting another error:\n\n```\nsrc/github.com/tensorflow/tensorflow/tensorflow/go/op/generate.go\ngo generate ../genop\n# github.com/tensorflow/tensorflow/tensorflow/go\n/tmp/go-build286363903/github.com/tensorflow/tensorflow/tensorflow/go/_obj/version.cgo2.o: In function `_cgo_bcde4d8165b9_Cfunc_TF_Version':\nsrc/github.com/tensorflow/tensorflow/tensorflow/go/version.go:60: undefined reference to `TF_Version'\ncollect2: error: ld returned 1 exit status\n../genop/main.go:15: running \"sh\": exit status 1\nsrc/github.com/tensorflow/tensorflow/tensorflow/go/op/generate.go:15: running \"go\": exit status 1\nThe command '/bin/sh -c go get github.com/tensorflow/tensorflow/tensorflow/go/op & go generate -v -x github.com/tensorflow/tensorflow/tensorflow/go/op' returned a non-zero code: 1\n```\n\nCommit hash: `77bb8d4c20ecb87a3988b24cf4c3f39e6dd78a34`\n\nNew environment `ubuntu xenial` :\n\n```\n$ go env\nGOARCH=\"amd64\" GOBIN=\"\" GOEXE=\"\" GOHOSTARCH=\"amd64\" GOHOSTOS=\"linux\" GOOS=\"linux\" GOPATH=\"/go\" GORACE=\"\" GOROOT=\"/usr/local/go\" GOTOOLDIR=\"/usr/local/go/pkg/tool/linux_amd64\" CC=\"gcc\" GOGCCFLAGS=\"-fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build477629842=/tmp/go-build -gno-record-gcc-switches\" CXX=\"g++\" CGO_ENABLED=\"1\"\n\n```\n\n```\n$ bazel version\nBuild label: 0.3.2 Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar Build time: Fri Oct 7 17:25:10 2016 (1475861110) Build timestamp: 1475861110 Build timestamp as int: 1475861110\n```\n\n```\n$ g++ --version\ng++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609 Copyright (C) 2015 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n```\n\n```\n$ g++ -v\nUsing built-in specs.\n\nCOLLECT_GCC=g++\nCOLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/5/lto-wrapper                        \nTarget: x86_64-linux-gnu                                                               \nConfigured with: ../src/configure -v --with-pkgversion='Ubuntu 5.4.0-6ubuntu1~16.04.4' --with-bugurl=file:///usr/share/doc/gcc-5/README.Bugs --enable-languages=c,ada,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-5 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-libmpx --enable-plugin --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-5-amd64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-5-amd64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-5-amd64 --with-arch-directory=amd64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --enable-objc-gc --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu                 \nThread model: posix                                                                    \ngcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4) \n```\n", "@MrJaqbq : Is it possible that you did a `git pull` in  between building `libtensorflow.so` and running `go generate`? Because `TF_Version` in the C library and `Version` in the Go API were introduced in commit https://github.com/tensorflow/tensorflow/commit/a374ea13c0c7b9598b5ada851b43655f895a578e So if you built the library on one side of this commit and the Go API on the other, then it would fail.\n", "Also, if that isn't the case, can you `git pull` to get the latest code and retry? (I'm wondering if the missing `extern` in the declaration of `TF_Version` in the C API is causing trouble. It was added soon after, but perhaps you just happened to snapshot the codebase in an unfortunate state)\n", "Seems that was the cause, after pulling latest source and compiling once again everything works. Thanks for help!"]}, {"number": 5548, "title": "Restore and predict tensorflow contrib.learn.DNNRegressor/DNNClassifier without running at least one step of training (which was a missing feature).", "body": "Hi,\r\n\r\nI'm following up on the issue that is mentioned in this thread:\r\nhttps://github.com/tensorflow/tensorflow/issues/3340\r\n\r\nCurrently the way to restore a trained model is: First created a DNNRegressor/DNNClassifier specifiyng model_dir on constructor, fit() (training step) , and restore it by creating another DNNRegressor/DNNClassifier using the same model_dir in constructor. That works for me and is not the problem.\r\n\r\nHowever, when I want to make the model portable to others, meaning I train on my machine and copy the model file it generates to others, and let them predict on new data sets without training again. It fails.\r\n\r\nAnd from a large and wide search on the issue, I find several resources point out to the same reply: \"Right now, there isn't a way to restore and predict without running at least one step of training (which is a missing feature)\"@martinwicke \r\nAnother reference: https://github.com/tensorflow/tensorflow/issues/3306\r\n\r\nSince this was the reply 3 months ago, and now I follow @michaelisard 's suggestion to open a separate  issue to track the feature of restoring without predicting.\r\n\r\nLook forward to hearing from experts who develops tensorflow, or any walk-around is welcome. Thanks!\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["The work-around is run zero steps of eval. It's a terrible hack, but it shouldn't affect you much.", "This should work using the new `tf.estimator.Estimator`. We will create `tf.estimator.DNNClassifier/Regressor` soon which should fix this issue.", "These classes are starting to appear on master. I will close this issue before I forget."]}, {"number": 5547, "title": "optional libjpeg-turbo support", "body": "This is in reference to #4807.", "comments": ["@mkolod, thanks for your PR! By analyzing the history of the files in this pull request, we identified @jart, @tensorflower-gardener and @keveman to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "@jart @tensorflower-gardener @keveman BTW, in #4807, I was asked to provide some benchmarks. When using a jpeg-decode-only graph ([source here](https://github.com/mkolod/tf_perf_eval/blob/master/io/jpeg/tensorflow/multithreaded_jpeg_decode.py)) and limiting processing to 1 thread, I got 1,257 images/sec (256x256 JPEG, specifically [this one](https://github.com/mkolod/tf_perf_eval/blob/master/io/jpeg/tensorflow/ElCapitan_256_by_256.jpg)) on a machine with an SSD and an Intel Core i7-5930K CPU @ 3.50GHz with libjpeg-turbo, and 632 images/second with libjpeg. This represents a libjpeg-turbo speedup of 1.99x over libjpeg, which is consistent with the expectations based on the [benchmarks](http://www.libjpeg-turbo.org/About/Performance) provided by the libjpeg-turbo team. The range of speed-up factors for x86-64 provided in the libjpeg-turbo team's benchmarks  is 1.89 to 4.31, depending on image characteristics, and my test shows that the speed-up factor is indeed in that range. For other platforms such as ARM v8, the speed-up factors are basically the same as for x86-64 (see [here](http://www.libjpeg-turbo.org/About/Performance)). A very exhaustive spreadsheet with lots of benchmark scenarios can be found [here](http://www.libjpeg-turbo.org/pmwiki/uploads/About/libjpegturbo-1.5.ods). My finding of a 1.99x speedup with the library called from TensorFlow itself should be convincing enough that this constitutes a major improvement.\n", "Jenkins, test this please.\n", "We're grateful that you took the time to send us this pull request @mkolod. I've just written up a change internally that upgrades TensorFlow to libjpeg-turbo, using the secret build knowledge other Googlers have figured out before me. When my change gets exported to GitHub (in probably a week tops) #4807 will be updated automatically.\n"]}, {"number": 5546, "title": "Add support for dict Input to DataFeeder, StreamingDataFeeder and allow for different TensorSignature for each of ModeKeys", "body": "# Problem\r\n\r\nThe `Estimator.fit()` function takes as argument either\r\n- (`x`, `y`, and `batch_size`) where `x` and `y` could be numpy arrays or iterators.\r\n  - **PROS**\r\n    1.   Easy to use.\r\n    2.  Allows feeding data from arbitrary source as long as problem can be decomposed into `x` and `y`.\r\n  - **CONS**\r\n    1.  No provision to provide epoch\r\n    2.  In case, `x` and `y` are arrays, the data aggregate must be available as opposed to reading on the fly ( say from database)\r\n    3. Whether array or iterator, `x` and `y` can't be dictionaries. Most complex problems cannot be reduced to input matrix and output matrix and may require multiple input features matrices.\r\n- `input_fn` - this is callback function which must return `features` and `target` tensors or dictionary of tensors.\r\n  - **PROS**\r\n    1. Allows feeding data from arbitrary source (in theory).\r\n    2. returned features and targets can be dictionary thus allowing to solve complex problems which takes multiple inputs.\r\n  - **CONS**\r\n    1. Only found support for reading files using `read_batch_examples()`, `read_batch_features()`, `read_batch_record_features()`, etc.\r\n    2. No support for passing placeholder and feed_fn to allow for arbitrary source of input data which don't require queue.\r\n# Relevant discussions\r\n1. https://github.com/tensorflow/tensorflow/pull/4696#issuecomment-253632403\r\n2. http://stackoverflow.com/questions/39855375/how-to-use-streamingdatafeeder-as-contrib-learn-estimator-fits-input-fn\r\n# Additional problem\r\n\r\nCurrently the self.feature_info (feature signature) must be same for training as well as evaluation. However, there can be cases where evaluation if done differently than training. For example, DualEncoderLSTM model where training just requires Context and Utterance, while Evaluation (Validation) requires context and multiple utterances.\r\n# Solution\r\n\r\nThis PR basically combines the pros of using (`x`, `y`, and `batch_size`) with pros of input_fn by allowing `x`, `y` to be dictionary of multiple arrays. This also works for `x` and `y` iterators returning dictionary of streaming data. \r\n\r\nAlso, added support for different feature/target signature for training and evaluation.\r\n", "comments": ["Can one of the admins verify this patch?\n", "@abhitopia, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @ilblackdragon and @martinwicke to be potential reviewers.\n", "@martinwicke - This is updated version of #5000 where I accidentally deleted my branch. I have also resolved conflicts with the main in this PR.\n", "Copied from #5000 where @ilblackdragon commented -\n\n\"Here is an update on current state:\n\nwe just split up SK compatibility from Estimator into separate class SKCompat. E.g. if one wants to use fit(x, y) they need to wrap their estimator with SKCompat(Estimator(...)). Otherwise preferred way will be .\nthere is a pending change (hopefully goes in today) to add pandas_input_fn that would allow you to est.fit(pandas_input_fn(x_df, y_series), ...) which would pass all the columns of the data frame as dict of features. It will also use local queue to feed data, which increases performance a bit compared to current data feeder solution.\nGoing forward we are thinking to remove data feeders, and rely on input functions similar to pandas_input_fn for other types of data. And SKCompat interface can just seamlessly wrap the passed data into this functions.\"\n", "@ilblackdragon - fine if you remove the DataFeeders completely. What will be the equivalent for `StreamingDatafeeder` then? Last I checked, `input_fn` only support input from TFRecords files. \nFor people like me, who have data coming from database, `StreamingDataFeeder` with dictionary support( this #5546) is a blessing. \n\nIn any case, even you plan to introduce `SKCompat` and want to let go of DataFeeders in future, I think for now this PR could still be relevant?\n\n@martinwicke - it's been over a month. :(\n", "@abhitopia +1\n", "`input_fn` is not limited to any particular data source. You can look at the `pandas_input_fn` that @ilblackdragon mentioned, it reads from pandas Dataframes. A similar function reading from a dict would be trivial. \n\nWe do want to remove data feeders, so relying on them may be short-lived. If you fix the conflicts, I'm happy to accept this change though. \n\nSorry for the wait. \n", "The idea to remove all DataFeeders - and replace them with input functions that are using [feeding_functions](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/dataframe/queues/feeding_functions.py).\n\nSee example for [pandas_input_fn](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/learn_io/pandas_io.py) - it already supports dicts passing.\n\nWhat we need now to implement is `enqueue_data` to support also an iterators and write a something like `iter_input_fn`.\n", "@martinwicke - I completely agree that we should let go of DataFeeders and have support for only for input_fn (which already supports dict). Using input_fn using [feeding_functions](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/dataframe/queues/feeding_functions.py) looks cool. \n\nHowever, meanwhile the transition is still in progress and until support for DataFeeders exists, I think it still makes sense to update DataFeeders with this PR. ( I have resolved the conflicts)\n\n@martinwicke , @ilblackdragon  - Thanks for the pointers. I will definitely checkout `pandas_input_fn` and also see if I can contribute in writing `iter_input_fn` soon.\n", "Thanks for updating the PR. I left review comments. \n\nPlease do evaluate pandas_input_fn, it is significantly easier, and should be easy to adapt into a new function dict_input_fn or similar which does exactly what you want. We'd love to have that function if you end up writing it.\n\nWe will eventually remove DataFeeder for the betterment of the word, so relying on this functionality will be brittle.\n", "@martin -  I have updated with changes you asked. And left comment on where I have question from you. Please review. Thanks.\n", "@martinwicke , @ilblackdragon - Once this is accepted, I will try and work on proposed `dict/iter_input_fn` this week and also update my private code to use same.\n", "Jenkins, test this please.\n", "@martinwicke - Made the requested changes. \n\nI am  currently fixing the failed test. Will let you know when done.\n", "@martinwicke, @ilblackdragon  - I have fixed all the bugs due to which some tests were failing. I also want to **emphasise** that this PR also adds support for different input signatures for `ModeKeys.TRAIN` and `ModeKeys.EVAL` and nicely extends it to `ModeKeys.INFER`.\n\nAs such, even if `DataFeeders` are going away ( and rightly so),  this PR still adds necessary support which makes already awesome `contrib.learn` even better.\n\nCan you please review this soon? Many thanks.\n", "Jenkins, test this please.", "Bunch of things to fix.\r\n\r\nI don't think we should allow letting the input signatures vary by mode. It seems to me this makes it more fragile -- is there a use case you have in mind for which this is useful?\r\n\r\n@ilblackdragon do you have an opinion on this?", "@martinwicke, @ilblackdragon  - I can give you at least one real life example where this is required. \r\nConsider the case of Response Selection in conversation.  ( See http://www.wildml.com/2016/07/deep-learning-for-chatbots-2-retrieval-based-model-tensorflow/)\r\n\r\n**During training:**\r\nX: context\r\ny: response\r\n\r\n**During evaluation:**\r\nX: context\r\ny: k responses which are ranked against X to give k@recall metric.\r\n\r\nAlso, I don't understand how having different signature makes it fragile. If for instance, the tensor signature in ModeKeys.EVAL is not compatible with computation graph, the error will be generated any how. Similarly, for ModeKeys.INFER the code first tries to match it up against signatures for ModeKeys.TRAIN/EVAL. If it doesn't match, warning is generated. But if the user doesn't pay heed to the warning and continues, that error should automatically get generated down the line if signature of ModeKeys.INFER is incompatible with computation graph.\r\n\r\n", "@martinwicke - I am still waiting for your response to my previous comment, meanwhile, I just ran the tests on docker container using \r\n`tensorflow/tools/ci_build/ci_build.sh CPU bazel test //tensorflow/...`\r\n\r\nand seems that 8 tests TIMEOUT, and no tests failed. I am not sure if that is the reason for some checks not passing. Any ideas on how to fix TIMEOUT tests?\r\n\r\n", "The sanity checks failed, can you fix these errors? It may be that the other tests are stale.", "@martinwicke - I had problem running the `ci_build.sh` #5759, but as I mentioned in the issue comment, there is a simple fix. Should I create a separate PR for that?", "@martinwicke, @ilblackdragon, @caisq  - I need help here. There were no errors but only two TIMEOUT errors when i run\r\n\r\n`tensorflow/tools/ci_build/ci_build.sh CPU bazel test //tensorflow/...` \r\n\r\n<img width=\"695\" alt=\"screen shot 2016-11-22 at 19 53 11\" src=\"https://cloud.githubusercontent.com/assets/12864026/20540077/7ee4c260-b0ef-11e6-9284-de503612fddc.png\">\r\n\r\nBut I am absolutely unable to figure out why sanity checks are failing only for `datafeeder.py`.  The complain seems to be about Bad indentation. But when I compare for instance other file like `graph_io.py`, indentation looks same to me. Further more, if I run `pylint` with `ci_build/pep8` configuration file, then the same indentation errors appear for pretty much all other files in Tensorflow (which I did not modify).\r\n\r\n<img width=\"545\" alt=\"screen shot 2016-11-22 at 20 12 01\" src=\"https://cloud.githubusercontent.com/assets/12864026/20540166/fe6192ca-b0ef-11e6-91b0-f411014d24f4.png\">\r\n\r\nI really need some input from you.", "@martinwicke, @abhitopia whats the status of this?", "@abhitopia Please do not worry about testing it yourself. We will run test suites on our Jenkins to test this PR before merging it. Flaky timeouts like the ones you see do occur and in this case they seem to be unrelated to your changes. ", "@tensorflow-jenkins test this please", "@abhitopia It looks like that the sanity check passed on Jenkins: https://ci.tensorflow.org/job/tensorflow-pull-requests-sanity/2181/console", "@caisq , @martinwicke - The only tests that failed were due to Python3 compatibility. I have fixed them now. Please check again.", "@caisq - Can you test this again please?", "@tensorflow-jenkins test this please.", "@caisq - Thanks for testing. It seems the error testing is a bit flaky as now one Mac OS test is failing.\r\n`//tensorflow/python/kernel_tests:cwise_ops_test                          FAILED in 1 out of 50 in 28.8s`\r\n\r\nThis test passed before, and now without any change, failed. This test is not related to this PR, \r\ncan we get it merged please? \r\n\r\nCC: @martinwicke ", "@martinwicke  - any comments please?", "MacOS tests were broken at head. I just pushed a fix for them.\r\nJenkins, test this please.", "Jenkins, test this please.", "@martinwicke , @gunan, @ilblackdragon, @caisq - Tests passing now. \ud83d\ude04  Merge?", "All the team is out on thanksgiving, and I am not very familiar with the code.\r\nI would feel more comfortable once we have approval from Martin.", "@gunan - thanks for letting me know. I agree, let's wait for @martinwicke's approval.", "@martinwicke - Can you check this please?", "Thank you, and sorry this took so long.", "@martinwicke - Thank you. ", "@martinwicke, @davidsoergel  - When I submitted, all the tests pass, and now you have reverted the changes.  :( ", "Yes, I am sorry.\u200b\n\nAll the tests passed but it turns out the tests were not comprehensive\nenough. We should try this again. Can you make another PR with exactly the\nsame changes, and assign me?\n"]}, {"number": 5545, "title": "Use of Inception v5 model to extract characters from video frames", "body": "", "comments": []}]