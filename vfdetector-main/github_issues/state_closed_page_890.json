[{"number": 26782, "title": "Check whether the file descriptor is valid", "body": "calling PosixWritableFile::Close twice causes segfaults on some posix systems. better check the file stream pointer state and return error if it's already closed.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26782) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26782) for more info**.\n\n<!-- ok -->", "Nagging Reviewer @ebrevdo: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 44 days with no activity and the `awaiting review` label has been applied."]}, {"number": 26781, "title": "Confusing documentation for  to tf.Graph in Tensorflow 2", "body": "**System information**\r\n\r\n> tensorflow-estimator-2.0-preview==1.14.0.dev2019031500\r\n> tensorflow-probability==0.5.0\r\n\r\n- Doc Link : https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/Graph\r\n\r\n**Describe the documentation issue**\r\nI believe for TF2 these docs can be more explicit that tf.Graph should not be used directly, and instead provide guidance similar to what Alex does in his talk\r\nhttps://www.youtube.com/watch?v=Up9CvRLIIIw\r\n\r\nThe wording that `A default Graph is always registered` is confusing in TF2 verbiage. and I believe the warning that the graph will not be executed eagerly should also be made more prominent, potentially by moving it up.\r\n\r\nThe confusion compounds because after using this method the user is left with a `tf.Tensor` but we're advised not to use `tf.Session` so it's not entirely clear what should be done next to evaluate the graph.\r\n\r\nSome of my confusion is visible in this thread (Big thanks to @brianwa84 for the help)\r\nhttps://github.com/pymc-devs/pymc4/pull/93#issuecomment-473490764 for reference\r\n\r\nThanks in advance!\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\nIf I can get some help on how to word things correctly then yes\r\n", "comments": ["@canyon289,\r\nThe documentation has been updated for TensorFlow v2.4. Could you please take a look at it and let us know if this is still an issue? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I doubt it is. Thanks @amahendrakar"]}, {"number": 26780, "title": "bazel  quantize:quantize_graph ERROR:quantize_graph failed to build", "body": "**System information**\r\n- Ubuntu 18.04\r\ndownload from https://github.com/tensorflow/tensorflow/\r\n- Python version: 3.6\r\n- Bazel version:0.23.2\r\n- CUDA/cuDNN version:cuda10/cudnn7\r\n\r\n**Describe the problem**\r\nbazel build tensorflow/contrib/quantize:quantize_graph\r\n\r\n`ERROR: /home/nnir712/project/tensorflow-master/tensorflow/core/grappler/costs/BUILD:281:1: C++ compilation of rule '//tensorflow/core/grappler/costs:op_level_cost_estimator' failed (Exit 1)\r\nIn file included from ./tensorflow/core/grappler/costs/op_level_cost_estimator.h:19:0,\r\n                 from tensorflow/core/grappler/costs/op_level_cost_estimator.cc:16:\r\n./tensorflow/core/grappler/costs/cost_estimator.h:58:52: error: 'INFINITY' was not declared in this scope\r\n              double intermediate_read_gb_per_sec = INFINITY,\r\n                                                    ^~~~~~~~\r\n./tensorflow/core/grappler/costs/cost_estimator.h:59:53: error: 'INFINITY' was not declared in this scope\r\n              double intermediate_write_gb_per_sec = INFINITY)\r\n                                                     ^~~~~~~~\r\n./tensorflow/core/grappler/costs/cost_estimator.h: In constructor 'tensorflow::grappler::DeviceInfo::DeviceInfo()':\r\n./tensorflow/core/grappler/costs/cost_estimator.h:46:17: error: 'INFINITY' was not declared in this scope\r\n       : gigaops(INFINITY),\r\n                 ^~~~~~~~\r\ntensorflow/core/grappler/costs/op_level_cost_estimator.cc: In member function 'virtual tensorflow::grappler::DeviceInfo tensorflow::grappler::OpLevelCostEstimator::GetDeviceInfo(const tensorflow::DeviceProperties&) const':\r\ntensorflow/core/grappler/costs/op_level_cost_estimator.cc:442:39: error: call to 'tensorflow::grappler::DeviceInfo::DeviceInfo(double, double, double, double)' uses the default argument for parameter 3, which is not yet defined\r\n   return DeviceInfo(gflops, gb_per_sec);\r\n                                       ^\r\ntensorflow/core/grappler/costs/op_level_cost_estimator.cc:442:39: error: call to 'tensorflow::grappler::DeviceInfo::DeviceInfo(double, double, double, double)' uses the default argument for parameter 4, which is not yet defined\r\nIn file included from ./tensorflow/core/platform/default/logging.h:24:0,\r\n                 from ./tensorflow/core/platform/logging.h:25,\r\n                 from ./tensorflow/core/lib/core/status.h:25,\r\n                 from ./tensorflow/core/grappler/costs/cost_estimator.h:19,\r\n                 from ./tensorflow/core/grappler/costs/op_level_cost_estimator.h:19,\r\n                 from tensorflow/core/grappler/costs/op_level_cost_estimator.cc:16:\r\n./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':\r\n./tensorflow/core/util/tensor_format.h:467:54:   required from here\r\n./tensorflow/core/util/tensor_format.h:441:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\n./tensorflow/core/util/tensor_format.h:441:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n   ^\r\nTarget //tensorflow/contrib/quantize:quantize_graph failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1340.504s, Critical Path: 68.45s\r\nINFO: 1816 processes: 1816 local.\r\n`", "comments": ["I suppose ```Graph_Transform_Tool``` can be a better alternative instead. Did you take a look at [```Graph_Transform_Tool```](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms#graph-transform-tool) yet?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 26779, "title": "[TF 2.0 API Docs] Fix missing docstrings for tf.keras.applications.MobileNetV2", "body": "Fix issue #25988 by adding a wrapper for decorated functions for `keras.applications` and adding missing docstrings for `tf.keras.applications.MobileNetV2`.", "comments": ["This should be submitted to master", "Closing this PR as this not against `master`, please open a new PR against `master` \r\nCC @mihaimaruseac"]}, {"number": 26778, "title": "[TF2.0] Possibly wrong path to decode_predictions in keras", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version: 3.6.5_1/2.7.15\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\nCurrently, Mobilenet_V2 Keras in `tensorflow/python/keras/applications/mobilenet_v2.py` references the function `decode_predictions` from `keras_applications.mobilenet_v2` [here](https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet_v2.py)\r\n \r\n**Describe the expected behavior**\r\n\r\n`decode_predictions` seems to be from `keras_applications.imagenet_utils` [here](https://github.com/keras-team/keras-applications/blob/master/keras_applications/imagenet_utils.py) instead. \r\n\r\n**Code to reproduce the issue**\r\n\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nNA\r\n\r\n**Other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nNA", "comments": ["If this is not intended, I will create a PR to fix it.", "The ```mobilenet_v2.py```  and ```imagenet_utils.py``` are referencing the decode_predictions correctly."]}, {"number": 26777, "title": "How do I allow growth with eager execution?", "body": "This is a generic question.   I am working locally from a copy of [a Colab notebook](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb), which uses eager execution.\r\n\r\nI notice that the notebook allocates all of the memory on my GPU right away.  So I can track what commands use what memory, I would like it to not do that.  According to [this StackOverflow article:](https://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory) I should do this in the code before it grabs all the memory:\r\n\r\n```\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth=True\r\nsess = tf.Session(config=config)\r\n```\r\n\r\nHowever, I notice that with eager execution, there is no Session.\r\n\r\nSo question/bug/feature request: How do I allow growth with eager execution?\r\n", "comments": ["According to the [doc](https://www.tensorflow.org/api_docs/python/tf/enable_eager_execution), `enable_eager_execution` takes a config as argument the same way your session does. Simply do:\r\n```python\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\ntf.enable_eager_execution(config=config)\r\n```", "Thanks, that answers my question!"]}, {"number": 26776, "title": "Improve testing infrastructure to prevent breaking open source builds", "body": "Recently pushes to the master branch done by tensorflow gardener frequently breaking the open source builds from the master and that is hindering the development on top of the tree by requested/suggested by google engineers through various configurations. Would it be possible to improve the testing of internal to external merges to make sure that opensource builds are not broken by a push from internal repositories?.\r\n\r\nFor example a push made today left master in broken state. In line https://github.com/tensorflow/tensorflow/blob/9a43dfeac58477c37cb356e3759b053d2bbd0247/tensorflow/tensorflow.bzl#L2189\r\nsome environment variables are referred however bazel clears the environment for genrules, we end up with an error message like below and can't build TF right now. It is possible to fix such issues but it likely to cost time for multiple people. Would it be possible to add a test that does a build with the instructions at [installation/build from source](https://www.tensorflow.org/install/source) still works.\r\n\r\n`   (cd /ssdscratch/.cache/bazel/_bazel_skama/184fadfe649ebd4ee26f07bbb482f004/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/ssdscratch/work/XLAInt8/tensorflow/build/bin:/home/skama/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/tools/build_info/gen_build_info --raw_generate \"bazel-out/host/genfiles/tensorflow/python/platform/build_info.py\" --build_config cuda --key_value  cuda_version_number=${TF_CUDA_VERSION} cudnn_version_number=${TF_CUDNN_VERSION} ')\r\nExecution platform: @bazel_tools//platforms:host_platform\r\n/bin/bash: TF_CUDA_VERSION: unbound variable\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n`", "comments": ["@samikama Could you please let us know if this is still an issue ?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26776\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26776\">No</a>\n"]}, {"number": 26775, "title": "[ROCm] Update interfaces within port.h", "body": "- rename CudaSupportsHalfMatMulAndConv to GpuSupportsHalfMatMulAndConv\r\n- introduce IsBuiltWithROCm", "comments": ["Nagging Reviewer @yifeif, @chsigg, @gunan: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "@chsigg @gunan a gentle ping"]}, {"number": 26774, "title": "[ROCm] add ROCm support for GPU types", "body": "This PR is subsequent to #26751 which adds fundamental type support on ROCm platform.\r\n\r\nRelevant codes have been running for more than 1 year on ROCm platform. We have published docker images at:\r\n\r\nhttps://hub.docker.com/r/rocm/tensorflow/tags\r\n\r\nand also PyPI packages:\r\n\r\nhttps://pypi.org/project/tensorflow-rocm/\r\n\r\nfor a sample public test run you can refer to:\r\n\r\nhttp://ml-ci.amd.com:21096/job/tensorflow-upstream-unit-tests/721/console\r\n\r\nrelevant tests:\r\n\r\n```\r\n//tensorflow/python:framework_device_test                                \u001b[0m\u001b[32mPASSED\u001b[0m in 2.4s\r\n```", "comments": []}, {"number": 26773, "title": "Correct API docs for tf.io.decode_gif", "body": "The API docs for `tf.io.decode_gif` currently state that the op only decodes the first frame of animated GIFs. But the C++ implementation actually extracts all frames from animated GIF files; see code [here](https://github.com/tensorflow/tensorflow/blob/623e6671213cd5286f3f053c2dec355282a679fb/tensorflow/core/kernels/decode_image_op.cc#L300).\r\n\r\nThis PR corrects the API docs to match the implementation.", "comments": []}, {"number": 26772, "title": "Refactor Range&Map&Take DatasetOpTests", "body": "This PR refactors `RangeDatasetOpTest`, `MapDatasetOpTest`, and `TakeDatasetOpTest` to be consistent with the unit test style. \r\n\r\ncc: @jsimsa ", "comments": []}, {"number": 26771, "title": "LSTM simple example ", "body": "Hello,\r\nI'm writing a simple LSTM function that executes the following operations on a random input x:\r\n\r\ni{t}=sigma(W{i}x{t}+R{i}h{t-1}+b{i})\r\nf{t}=sigma(W{f}x{t}+R{f}h{t-1}+b{f})\r\no{t}=sigma(W{o}x{t}+R{o}h{t-1}+b{o})\r\nc'{t}=tanh(W{c}x{t}+R{c}h{t-1}+b{c})\r\nc{t}=f{t} o c{t-1}+i{t} o c'{t}\r\nh{t}=o{t} o tanh(c{t})\r\n\r\nTo test the performance of my code I want to use Tensorflow XLA. With all the examples I found, I could not create a simple function as this ( I couldn't find out how to initialize a random size input with random values, initialize weights and define the output size)\r\nCould anyone help me implement this function using Tensorflow XLA ?\r\n", "comments": ["@jlebar ", "Hi, @RadjaHachilif.\r\n\r\nGithub Issues are for bugs in TensorFlow, not for support requests.  There are separate mailing lists for this.  See https://www.tensorflow.org/community/forums.  Or you can try StackOverflow.\r\n\r\nI know it can be frustrated to be told \"good question, wrong venue\", but if we don't keep these separate, our processes will fall apart in a bad way.\r\n\r\nWe try to be very up-front about this.  When you filed the bug, you should have seen some text  that links to https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md.\r\n\r\nTo save you a round-trip, when answering your question on a mailing list I'd need more information to help you.  What is the exact TensorFlow code you are running, and how have you attempted to enable XLA?  What version of TF are you running, and on what operating system?  That sort of thing.  (I believe some of these are actually requested when you file a github issue?)  The more details you can provide the more likely it is that you'll be able to get assistance.\r\n\r\nI'm going to close this bug.  Good luck getting help!", "(@ymodak fyi is it possible to get responses like the above when someone files a bug that's not actually a bug?  Would save some dev time.)", "@jlebar Sure will make a note of it. Thanks for taking a look."]}, {"number": 26770, "title": "Fix incorrect deprecation warning for tf.numpy_function", "body": "This fix tries to address the issue raised in #26735 where\r\nusage of tf.numpy_function in v2 generates incorrect deprecation\r\nwarning:\r\n```\r\ntf.py_func is deprecated in TF V2.\r\n```\r\n\r\nThe reason was that tf.numpy_function calls tf.py_func internal\r\nso a warning (on tf.py_func) is always in place even though\r\ntf.numpy_function is the correct usage in v2.\r\n\r\nThis fix reorders the calling so that incorrect warning could be addressed.\r\n\r\nThis fix fixes #26735.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 26769, "title": "Failed to run optimizer ArithmeticOptimizer", "body": "I am trying to build a graph which uses a conv2dTranspose using keras layers. I ran into following error:\r\n\r\nHere is a sample code which reproduces same error: \r\n```\r\nimport numpy as np\r\nfrom tensorflow.keras.layers import Conv2D, Conv2DTranspose\r\nfrom tensorflow.keras.layers import Input\r\nfrom tensorflow.keras.models import Model\r\n\r\na = Input(shape=(512, 16, 16))\r\nb = Conv2DTranspose(filters=192, kernel_size=2, strides=2, padding='same', data_format=\"channels_first\")(a)\r\nmodel = Model(inputs=a, outputs=b)\r\ninp = np.random.rand(10, 512, 16, 16)\r\nmodel.predict(inp)\r\n```\r\n\r\n```2019-03-16 03:22:24.181247: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node conv2d_transpose/strided_slice_3. Error: Pack node (conv2d_transpose/stack) axis attribute is out of bounds: 0\r\n2019-03-16 03:22:24.181306: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node conv2d_transpose/strided_slice_4. Error: Pack node (conv2d_transpose/stack) axis attribute is out of bounds: 0\r\n2019-03-16 03:22:24.181334: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node conv2d_transpose/strided_slice_5. Error: Pack node (conv2d_transpose/stack) axis attribute is out of bounds: 0\r\n2019-03-16 03:22:24.181379: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node conv2d_transpose/strided_slice_6. Error: Pack node (conv2d_transpose/stack) axis attribute is out of bounds: 0\r\n2019-03-16 03:22:24.186380: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node conv2d_transpose/strided_slice_3. Error: Pack node (conv2d_transpose/stack) axis attribute is out of bounds: 0\r\n2019-03-16 03:22:24.186435: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node conv2d_transpose/strided_slice_4. Error: Pack node (conv2d_transpose/stack) axis attribute is out of bounds: 0\r\n2019-03-16 03:22:24.186469: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node conv2d_transpose/strided_slice_5. Error: Pack node (conv2d_transpose/stack) axis attribute is out of bounds: 0\r\n2019-03-16 03:22:24.186501: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node conv2d_transpose/strided_slice_6. Error: Pack node (conv2d_transpose/stack) axis attribute is out of bounds: 0\r\n```\r\nCan anyone explain cause of this error?", "comments": ["Hi, same error using Conv2DTranspose in Win10, TF-GPU1.13, Cuda10, Python3.6", "Using GPU V100, TF 1.13.1 and Adanet\r\n```\r\nmessage:  \"Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node adanet/iteration_7/best_eval_metric_ops/strided_slice_9. Error: Pack node (adanet/iteration_7/best_eval_metric_ops/stack_9) axis attribute is out of bounds: 0\"   \r\npathname:  \"./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h\" \r\n```", "@somil55 I was able to execute your code snippet successfully using google colab with TF 1.13.1 and 2.0-alpha.\r\nCan you please make sure you have no other processes that utilize GPU?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I am having the same issue, any news on that? Same versions cited, could it be because of the nvidia driver (418.56)?", "I get the same warning with the error message `node strided_slice_23. Error: ValidateStridedSliceOp returned partial shapes [?,1,3] and [?,1,3]`\r\nAny hint what this actually means. I can rule out memory issue (as my model is super small), I am training on a GPU.", "I have the same issue.  \r\nI build c++ API with tensrflow1.13.1, and it works normally but arising W(warning) in console.   \r\n```\r\n2019-12-26 20:06:40.968728: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\nCreate session succesfully\r\nintializating...\r\n2019-12-26 20:06:46.010915: W .\\tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node clipped_boxes/strided_slice. Error: Pack node (boxes/stack) axis attribute is out of bounds: 2\r\n2019-12-26 20:06:46.019316: W .\\tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node clipped_boxes/strided_slice_2. Error: Pack node (boxes/stack) axis attribute is out of bounds: 2\r\n2019-12-26 20:06:46.028367: W .\\tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node clipped_boxes/strided_slice_4. Error: Pack node (boxes/stack) axis attribute is out of bounds: 2\r\n2019-12-26 20:06:46.037209: W .\\tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node clipped_boxes/strided_slice_6. Error: Pack node (boxes/stack) axis attribute is out of bounds: 2\r\n2019-12-26 20:06:49.505305: W .\\tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node clipped_boxes/strided_slice. Error: Pack node (boxes/stack) axis attribute is out of bounds: 2\r\n2019-12-26 20:06:49.514453: W .\\tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node clipped_boxes/strided_slice_2. Error: Pack node (boxes/stack) axis attribute is out of bounds: 2\r\n2019-12-26 20:06:49.523269: W .\\tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node clipped_boxes/strided_slice_4. Error: Pack node (boxes/stack) axis attribute is out of bounds: 2\r\n2019-12-26 20:06:49.533106: W .\\tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node clipped_boxes/strided_slice_6. Error: Pack node (boxes/stack) axis attribute is out of bounds: 2\r\nRun session seccesfully, number of object:0\r\ndetecting...\r\nRun session seccesfully, number of object:0\r\n49.3665 133.372 324.649 875.971 1 4\r\n-0.660165 -0.660165 -0.660165 -0.660165 -1 -1\r\n-0.660165 -0.660165 -0.660165 -0.660165 -1 -1\r\n-0.660165 -0.660165 -0.660165 -0.660165 -1 -1\r\n-0.660165 -0.660165 -0.660165 -0.660165 -1 -1\r\nRun session seccesfully, number of object:1\r\n4.86386 124.282 314.616 871.312 1 4\r\n192.478 525.426 241.645 599.255 0.601976 2\r\n192.841 525.105 241.857 599.59 0.417561 -1\r\n96.2017 534.41 136.276 570.967 -1 -1\r\n192.441 525.143 241.376 599.877 -1 -1  \r\n```  \r\nNot only tensorflow-cpu but also tensorflow-gpu arise this issue. It works  normally but those Warnings make me uncomfortable.   \r\nAnyone can tell me why this issue appearing and how to fix? \r\nThanks!", "I faced the same issue while running the **tensorflow object detection api for mobilenetv2** on my custom dataset.\r\n```bash\r\n2020-02-29 11:35:57.903698: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node ChangeCoordinateFrame/strided_slice. Error: Pack node (stack_10) axis attribute is out of bounds: 0\r\n2020-02-29 11:35:57.903732: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node ChangeCoordinateFrame/strided_slice_2. Error: Pack node (stack_10) axis attribute is out of bounds: 0\r\n2020-02-29 11:35:57.903742: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node ChangeCoordinateFrame/strided_slice_6. Error: Pack node (stack_10) axis attribute is out of bounds: 0\r\n2020-02-29 11:35:57.903750: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node ChangeCoordinateFrame/strided_slice_7. Error: Pack node (stack_10) axis attribute is out of bounds: 0\r\n```\r\n**Error is fixed and disappeared** after **upgrading from TF-gpu from `1.13.1` to `1.14.0`**: `pip install -U tensorflow-gpu==1.14.0` as mentioned by @LucienZhang here in this issues [issues/1264](https://github.com/matterport/Mask_RCNN/issues/1264#issuecomment-568875440)", "same issue, any updates?", "the same issue, but in my case, it trained and predict successfully.\r\n\r\nHowever, I think this problem may affect the speed of detection.\r\n\r\nWait for updates", "the same issue,i can trained and predict,\r\n\r\n", "anybody fix this issue?"]}, {"number": 26768, "title": "Patch in pcmp_eq method for altivec [ppc64le] float", "body": "Commit dcbc81277b made use of the pcmp_eq method for Packet\r\nfor the first time, then works fine on x86 but fails to compile\r\non ppc64le because the pcmp_eq method was not implemented in\r\nEigen's arch/AltiVec/Complex.h for float. This patch adds this\r\nimplementation to get around the build break on ppc64le.\r\n\r\nWe will work on upstreaming this change into the Eigen source,\r\nwe'll need to also add the pcmp_eq method for double when we do\r\nthat.", "comments": ["You can prepare PR to Eigen and assign it to @rmlarsen for review. I think it's the preferred way.", "Please copy Konstantinos Margaritis in the Eigen community as well.", "FYI, I just submitted a PR into Eigen: https://bitbucket.org/eigen/eigen/pull-requests/619/add-support-for-pcmp_eq-in-altivec/diff", "The upstream eigen PR is now merged. I'll close this PR and create a new one to update the eigen version used in TensorFlow."]}, {"number": 26767, "title": "[ROCm] Add ROCm support for the Reshape op", "body": "This minor mod adds ROCm support for the Reshape op.  \r\n\r\n#### Background info ####\r\n\r\nThese ops are fundamental to TensorFlow, and this mod has been running for more than 1 year on our ROCm port of TF. \r\n\r\nWe have published docker images at:  https://hub.docker.com/r/rocm/tensorflow/tags\r\nAnd also PyPI packages:  https://pypi.org/project/tensorflow-rocm/\r\n\r\nFor a sample ROCm test run you can refer to:\r\nhttp://ml-ci.amd.com:21096/job/tensorflow-upstream-unit-tests/721/console\r\n```\r\n//tensorflow/python/kernel_tests:reshape_op_test                PASSED in 2.5s\r\n//tensorflow/python/kernel_tests:reshape_op_test_gpu            PASSED in 2.9s\r\n```", "comments": []}, {"number": 26766, "title": "Tensorflow 2.0 Alpha, tf.concat, ConcatV2 requires tf.double input under eager mode", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): \r\n```\r\nconditions = tf.keras.layers.Lambda(\r\n            lambda x: tf.concat(x, axis=-1)\r\n        )(self.condition_inputs)\r\n```\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac\r\ndevice:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.0.0a0\r\n- Python version: 3.6.5\r\n\r\n**Describe the current behavior**\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: cannot compute ConcatV2 as input #1(zero-based) was expected to be a double tensor but is a float tensor [Op:ConcatV2] name: concat\r\n```\r\n\r\noriginal code\r\n\r\n```\r\nget_values([observations_np, actions_np])\r\n```\r\nwould casue the above error under eager. However, there is no error without eager.\r\n\r\nBut, if force the numpy inputs to be tf.double tf Variable undr eager mode, it can go through:\r\n\r\n```\r\nget_values([tf.Variable(observations_np, dtype=tf.double), tf.Variable(actions_np, dtype=tf.double)])\r\n``\r\n\r\n", "comments": ["It is caused by the inputs have two different dtypes: float32 and float64."]}, {"number": 26765, "title": "[ROCm] Add ROCm support for Reshape op [closing, will resubmit with single author commits]", "body": "Minor change to support the Reshape op on ROCm.  Note that this PR is branched from PR#26583.", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26765) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 26764, "title": "Build Error For a CPU-only build on macOS Mojave ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: macOS Mojave 10.14.3\r\n- Mobile device: NA\r\n- TensorFlow installed from (source or binary): trying to build from source (build issue)\r\n- TensorFlow version: r1.12\r\n- Python version: Python 3.6.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:42:37)\r\n- Installed using virtualenv? pip? conda?: trying to build in a conda environment\r\n- Bazel version (if compiling from source): bazel-0.15-darwin-x86_64\r\n- GCC/Compiler version (if compiling from source): LLVM version 10.0.0 (clang-1000.11.45.5)\r\n- CUDA/cuDNN version: not installed (CPU-only build)\r\n- GPU model and memory: Radeon R9 M370X 2048 (CPU-only build)\r\n\r\n**Describe the problem**\r\n\r\nIt seems like `toUpper` is defined twice. Once as a function in STL in and once as a macro in Python headers (`pyport.h`). There are other similar errors for `tolower`, `isspace` and such. See the log for more details.\r\n\r\nSteps to reproduce the problem:\r\n1. Check out the r1.12 branch of Tensorflow from Github.\r\n2. Create a conda environment with Python 3.6.5 and activate it.\r\n3. Pip install the dependencies: pip six numpy wheel mock keras_applications keras_preprocessing as described [here](https://www.tensorflow.org/install/source).\r\n4. Install bazel 0.15 from [here](https://github.com/bazelbuild/bazel/releases/tag/0.15.0).\r\n5. Navigate to the Tensorflow root folder and `./configure`.\r\n6. Use the default values for python and dist-packages (first two questions).\r\n7. Answer No to remaining questions during configuration.\r\n8. `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`.\r\n9. Wait ~2 hours for bazel to build ~7400 files.\r\n\r\nYou should see a similar error message (and other similar errors) to the following:\r\n\r\n`ERROR: /Users/siavash/clarius/tensorflow/tensorflow/python/eager/BUILD:10:1: C++ compilation of rule '//tensorflow/python/eager:pywrap_tfe_lib' failed (Exit 1)\r\nIn file included from tensorflow/python/eager/pywrap_tfe_src.cc:18:\r\nIn file included from ./tensorflow/python/eager/pywrap_tfe.h:22:\r\nIn file included from ./tensorflow/core/lib/core/status.h:23:\r\nIn file included from bazel-out/darwin-opt/genfiles/tensorflow/core/lib/core/error_codes.pb.h:9:\r\nIn file included from external/protobuf_archive/src/google/protobuf/stubs/common.h:39:\r\nIn file included from /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/iostream:38:\r\nIn file included from /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/ios:216:\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/__locale:518:15: error: C++ requires a type specifier for all declarations\r\n    char_type toupper(char_type __c) const\r\n              ^\r\nbazel-out/darwin-opt/genfiles/external/local_config_python/python_include/pyport.h:706:29: note: expanded from macro 'toupper'\r\n#define toupper(c) towupper(btowc(c))\r\n`\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n[error.log](https://github.com/tensorflow/tensorflow/files/2972514/error.log)\r\n", "comments": ["After doing a bit of digging, it seems like this is a known [bug](https://bugs.python.org/issue10910). It might be that the conda environment that I am using does not include the fix. I will investigate further.", "It seems that I missed a clean (?) step and bazel is building against my Python 3.5.2. Python 3.5.2 does not include the fix, so now I am building with Python 3.6.0 that does include the fix. I will see if this addresses the issue.", "Tensorflow built successfully:\r\n\r\n`Target //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\nINFO: Elapsed time: 6885.252s, Critical Path: 432.77s\r\nINFO: 8608 processes: 8608 local.\r\nINFO: Build completed successfully, 9125 total actions`", "The [tested configurations table](https://www.tensorflow.org/install/source#tested_build_configurations) is extremely misleading as it clearly says that Python 2.7, 3.3-3.6 are tested. However, the pyport [bug](https://bugs.python.org/issue10910) is not fixed in Python 3.5.2, which is one of the supported versions in the table.\r\n\r\nThe table should be updated to reflect this. ", "This is probably the same issue: https://github.com/tensorflow/tensorflow/issues/23096", "I am closing this as I think it only requires the configuration table to be updated.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26764\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26764\">No</a>\n"]}, {"number": 26763, "title": "[TF2.0] Embedding batch_input_shape not aware of distribute.MirroredStrategy()", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes, modified an [example from Seedbank](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/sequences/text_generation.ipynb#scrollTo=MtCrdfzEI2N0) to use with TF2.0\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip binary\r\n- TensorFlow version (use command below): ` 2.0.0-alpha0`; `git v1.12.0-9492-g2c319fb415`\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): NIL\r\n- GCC/Compiler version (if compiling from source): NIL\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: V100 16GB\r\n\r\n**Describe the current behavior**\r\n\r\n1. when `batch_input_shape` is **not** specified in `tf.keras.layers.Embedding`\r\nWith or without `distribute.MirroredStrategy()`, model works perfectly fine\r\n2. when `batch_input_shape` **is** specified in `tf.keras.layers.Embedding`\r\nWithout `distribute.MirroredStrategy()`, model works perfectly fine, But with `distribute.MirroredStrategy()`, **`tf.data.Dataset` splits the inputs to the model correctly, but the model's expected input is not correct**\r\n\r\n* The model replicas **each** expect `batchsize_per_replica * replica` (the un-split output from `Dataset`) instead of  `batchsize_per_replica` (split output from `Dataset`).\r\n* If the model's `batch_input_shape` or `Dataset` output is adjusted to match the above expectation, the keras Model immediately errors out as it expects `batchsize_per_replica * replica`, split to `batchsize_per_replica` as an input to **each** model replica.\r\n\r\n\"Illustrated Example\":\r\n```\r\n> batchsize = 4*128\r\n> batchsize_per_replica = 128\r\n> model batch_input_shape = 4*128\r\n> model replica expects 4*128 causing error\r\n\r\n> batchsize = 4*128\r\n> batchsize_per_replica = 128\r\n> model batch_input_shape = 128\r\n> model expects 4*128 causing error\r\n```\r\n\r\nIt seems like the problem is everywhere else, the batch size etc. is calculated correctly, except when `batch_input_shape` is specified in `tf.keras.layers.Embedding`. If anyone is wondering why we need to specify this, this is for stateful RNNs to work.\r\n\r\n**Describe the expected behavior**\r\n\r\nWhen the scope is `distribute.MirroredStrategy()`, `tf.keras.layers.Embedding` specified `batch_input_shape` should also be divided by the number of replicas.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nThe notebook below contains code that will reproduce the error on a **multi-GPU** system. On Colab, there is only one GPU hence it runs fine, since `distribute.MirroredStrategy()` only creates one replica. However, with two or more replicas on a multi-GPU system, the error is observed.\r\n\r\n[Notebook presented on Google Colab](https://colab.research.google.com/drive/1R3h2Jf9rKCsi952KLcg7b8PtPyyXgx6b)\r\n\r\nThere is a header/section that shows the model training with and without `distribute.MirroredStrategy()`. \r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nFollowing the Colab notebook above, when training is run on a 4 GPU system (overall `batch_size=128`, `batchsize_per_replica=32`), the error is:\r\n\r\n```\r\nInvalid input_h shape: [1,128,256] [1,32,256]\r\n\t [[{{node replica_3/unified_lstm_2/CudnnRNN}}]]\r\n\t [[replica_1/loss/dense_1_loss/loss/weighted_loss/broadcast_weights/assert_broadcastable/is_valid_shape/else/_48/has_valid_nonscalar_shape/then/_207/has_invalid_dims/concat/_374]] [Op:__inference_keras_scratch_graph_10991]\r\n```\r\n\r\nNotebook that shows the entire run resulting in the above error can be seen [here](https://nbviewer.jupyter.org/github/tlkh/arxiv-lm/blob/master/tf_distributed_embedding_bugreport.ipynb)\r\n\r\nIf you modify `batch_input_shape` to match for `batchsize_per_replica` (32):\r\n\r\n```\r\nValueError: The batch output shape of your `Dataset` is 128, which is incompatible with the specified batch size of your Input Layer: 32\r\n```\r\n\r\nIn all cases, `model.summary()` gives the same result:\r\n\r\n```\r\nModel: \"sequential_1\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nembedding_1 (Embedding)      (128, None, 128)          8320      \r\n_________________________________________________________________\r\nunified_lstm_2 (UnifiedLSTM) (128, None, 256)          394240    \r\n_________________________________________________________________\r\nunified_lstm_3 (UnifiedLSTM) (128, None, 256)          525312    \r\n_________________________________________________________________\r\ndense_1 (Dense)              (128, None, 65)           16705     \r\n=================================================================\r\nTotal params: 944,577\r\nTrainable params: 944,577\r\nNon-trainable params: 0\r\n```\r\n\r\n**The exact same code will run fine on single GPU system even with `distribute.MirroredStrategy()` as the scope**, you can [view Colab demo](https://colab.research.google.com/drive/1R3h2Jf9rKCsi952KLcg7b8PtPyyXgx6b)", "comments": ["#26245 shows a similar error, but somewhat different in nature\r\n\r\n1. That one is with Keras `multi_gpu_model` (I believe this is not recommended any more with TF2.0?)\r\n2. That one gives an error even when `batch_input_shape` is not specified, whereas in my case it works fine", "@tlkh why is `multi_gpu_model` not recommended with TF 2.0 any longer?", "@ghego With TF 2.0, tf.Keras works out of the box with `tf.distribute.MirroredStrategy`. My own experience is that it is much nicer to use. Check out: https://www.tensorflow.org/alpha/tutorials/distribute/keras\r\n", "yep, I've seen that. Thanks", "Assigning to Priya since this seems to be related with distribution strategy. Please reassign to someone who owns the embedding if the fix should be on that side.", "It should be fixed after https://github.com/tensorflow/tensorflow/commit/a3f9c59153203020dcd008527db5247deefad95a.  Could you please try and re-open if that doesn't help?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26763\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26763\">No</a>\n"]}, {"number": 26762, "title": "Print/return the result with the highest accuracy", "body": "Small hotfix to print the most accurate prediction instead of the least accurate prediction, like it was doing before!", "comments": ["Hi @urmilshroff, when you say \"like it was doing before\" what are you referring to? As far as I can tell the code that you are changing was introduced by https://github.com/tensorflow/tensorflow/pull/9261/files and never changed since.", "@jsimsa I don't remember exactly, but when I had used the `label_image.py` file earlier in my project (sometime around January 2019), it was working fine. Then I messed it up and re-downloaded the file from the TF repo, and realized it wasn't returning the proper values. I'm on vacation and will clarify further once I get my laptop, if the need be.\n", "OK. The last change to that file happened in 02/2018 so chances are something else have changed in your setup and I am hesitant to accept your \"fix\". "]}, {"number": 26761, "title": "[Cherry-pick] Prevent null pointer dereference in decode_gif.", "body": "PiperOrigin-RevId: 231841542\r\n\r\nOriginal commit: https://github.com/tensorflow/tensorflow/commit/e41cb124cd0b325821af85cdacd9d8a12e206418", "comments": []}, {"number": 26760, "title": "Update version to 1.12.1 for patch release following e41cb12", "body": "", "comments": []}, {"number": 26759, "title": "Keras HDF5 with int8/bfloat16 support", "body": "\r\n**System information**\r\n- TensorFlow version (you are using): TensorFlow 1.12\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nThe TensorFlow protobuf saved model format is aware of reduced precision (int8), but I don't think the Keras HDF5 saved format is.  \r\n\r\n**Will this change the current api? How?**\r\n\r\nI'd like to have a way to convert a Keras HDF5 model to int8. So when I do a `tf.keras.model.predict()` it will use the INT8 operations rather than the FP32.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone using the tf.Keras API.\r\n\r\n**Any Other info.**\r\nFor example, TF Lite can do this:  https://www.tensorflow.org/lite/performance/post_training_quantization\r\n", "comments": ["Just circling back on this.  Is there a way to use INT8 or BFLOAT16 with the Keras HDF5 format? If not, will this be implemented? Thanks.", "@tonyreina Could you describe a use case for this feature? Thanks!", "Sure.  \r\n\r\nSo hardware is moving toward using BFLOAT16 and INT8 operations. For example, Intel Xeon supports DL Boost which is an INT8 precision.  We have pre-trained Keras models in FP32 which we've converted to TensorFlow protobuf models to take advantage of INT8, but that's going to require users to convert every model from a Keras HDF5 saved file to TensorFlow protobuf. And, it means we can't use model.predict() with the tf. Keras API. So it makes it hard to justify the use of Keras for these use cases if the Keras saved format needs to be changed to protobufs.\r\n\r\nI believe Google already has BFLOAT16 support for TPUs (https://cloud.google.com/tpu/docs/bfloat16) but-- again-- I'm not aware of any Keras support for this. Perhaps it exists or is in development but I'm not seeing how to use it.\r\n\r\nIf TensorFlow 2.0 is moving to use Keras as the preferred API, it would help to add support for these lower precision formats.", "We would recommend you use the TF SavedModel format in this case. In TF 2.0, this will be the default saving format; in the meantime, you can use tf.keras.experimental.export_saved_model() or model.save_weights(...save_format='tf')", "I see. What will happen with tf.keras.model.predict?  Will I be able to load a TF SavedModel via it and do inference that way? ", "Yes, you should be able to use tf.keras.experimental.load_from_saved_model to reload the saved model back into python.", "Thanks Karmel.  That's good to hear.  Is the idea that eventually the tf.keras.models.save_model will actually use the SavedModel protobuf format instead of the Keras HDF5 format?  I'm thinking that the typical user will probably use the save_model and load_model functions rather than the experimental ones.", "Yes, we anticipate making the TF format the default for tf.keras.Model.save in TF 2.0.", "@tonyreina Is this still an issue? Can you check with `TF2.0` and let us know whether the issue persists with latest TF version. Thanks!", "It should not be.  I believe that with TF 2.0 the Keras API is now saved as a protobuf. So that should handle the reduced precision formats.", "I am closing the issue as it was resolved. Please feel free to open it if the issue persists again. Thanks!"]}, {"number": 26758, "title": "tensorflow-gpu using only 10% of my GPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 (1809)\r\n- TensorFlow installed from (source or binary): binary (pip install tensorflow-gpu)\r\n- TensorFlow version (use command below): b'v1.10.0-rc1-19-g656e7a2b34' 1.10.0\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: GTX 960 2GB\r\n\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nWhile trying to train a neural network with my GTX960 after installing tensorflow-gpu, and choosing my GPU with the below code, I can see on the Windows task manager that it's only using about 10% of the GPU, and thus making it way slower than training it with the CPU. \r\n\r\n\r\n```\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = False\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 1\r\nsession = tf.Session(config = config)\r\n\r\nwith tf.device(\"/device:GPU:0\"):\r\n    model = Sequential()\r\n    ...\r\n```\r\n\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\nThe GPU should be used almost if not entirely.\r\n\r\n**Code to reproduce the issue**\r\nyou'd need the whole neural network code plus the datasets and the rest...\r\n\r\n**Other info / logs**\r\n>python predictor_LSTM_all.py\r\nUsing TensorFlow backend.\r\n2019-03-15 17:38:18.822616: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-03-15 17:38:19.109114: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1405] Found device 0 with properties:\r\nname: GeForce GTX 960 major: 5 minor: 2 memoryClockRate(GHz): 1.2785\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 2.00GiB freeMemory: 1.64GiB\r\n2019-03-15 17:38:19.117459: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2019-03-15 17:38:19.527190: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-03-15 17:38:19.532341: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971]      0\r\n2019-03-15 17:38:19.535210: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:984] 0:   N\r\n2019-03-15 17:38:19.538575: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2048 MB memory) -> physical GPU (device: 0, name: GeForce GTX 960, pci bus id: 0000:01:00.0, compute capability: 5.2)\r\nEpoch 1/10\r\n2019-03-15 17:38:21.045228: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2019-03-15 17:38:21.049900: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-03-15 17:38:21.054218: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971]      0\r\n2019-03-15 17:38:21.057053: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:984] 0:   N\r\n2019-03-15 17:38:21.060264: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2048 MB memory) -> physical GPU (device: 0, name: GeForce GTX 960, pci bus id: 0000:01:00.0, compute capability: 5.2)\r\n15709/15709 [==============================] - 39s 2ms/step - loss: 33.3039 - acc: 0.1775\r\n\r\n(using the CPU only takes about 7-10 seconds to finish the first epoch while here it shows how it takes almost 40 seconds with the GPU.\r\n", "comments": ["What is the batch size you are using while training? Too small batch size makes GPU training inefficient.\r\n\r\nIn addition, are you using `tf.keras`? Are you using `LSTM` or `CuDNNLSTM`? The latter is much, much faster!", "> What is the batch size you are using while training? Too small batch size makes GPU training inefficient.\r\n\r\nI was using a batch size of 20 as my dataset is 15700 samples long only. I just tried a batch size of 1000 and the GPU usage has gone up to 20%, so thanks for that, I had no idea. (I'm gonna have to rise the epochs as well if i want similar results tho)\r\n\r\n> In addition, are you using `tf.keras`? Are you using `LSTM` or `CuDNNLSTM`? The latter is much, much faster!\r\n\r\nYes, I'm using Keras, and I'm currently using normal LSTMs so I'm going to do some research to thsoe CuDNNLSTM and see if I can modify my code slightly to use them, thanks again!", "wow @tlkh I'm in your debt for telling me about cuDNNLSTM. Not only it's faster than LSTM on CPU (which is going to save me quite some time over 111 sensors that I have to train and predict), but it's also going to save my poor CPU of quite a few rough hours. ", "Can you check with GPU-z? I discovered last week that windows task manager shows max 14%, but it is 100% utilization most of the time on GPU-z.", "Yup @realiti4 , using GPU-z does show way higher GPU usage, and while it's only about 40% it probably is because i'm using a small Batch Size for the training.\r\n\r\n![Sin t\u00edtulo](https://user-images.githubusercontent.com/17100634/54489929-b4214000-48b1-11e9-80c5-b6ada1bfd021.png)\r\n", "Hello @ismaelestalayo  , i have the same problem  ,when i run my script python ,  tensorflow-gpu using only 10% of my GPU , please tel me how can be resolve it , i see that you are using LSTM or CuDNNLSTM , please tell me how , thanks in advance.\r\n ", "Im using OS  Linux Ubuntu 18.04,  i have this problem since 2 week , please help me, ", "> Hello @ismaelestalayo , i have the same problem ,when i run my script python , tensorflow-gpu using only 10% of my GPU , please tel me how can be resolve it , i see that you are using LSTM or CuDNNLSTM , please tell me how , thanks in advance.\r\n\r\nSorry to tell you that I did not \"fix\" it per se (if it even was a bug or error of Tensorflow). \r\nAs you can see in my previous message, GPU-Z actually showed a 40% load on the GPU which is way more significant, and the other 60% missing could partly be because of very small batch sizes which are not optimal as another user suggested in this very issue. \r\n\r\nLastly, if you're using LSTM, I recommend you trying CuDNNLSTM, as it's way more efficient and fast (while getting the same results as with the basic LSTMs).", "Hi \r\nThanks for bringing up this issue. When I try to increase the batch size over 14 the training process is crushing. Any idea why? ", "> Hi\r\n> Thanks for bringing up this issue. When I try to increase the batch size over 14 the training process is crushing. Any idea why?\r\n\r\nFirst of all, what do you mean by \"crushing\"? And second, there's no fixed value that works for every dataset, the batch size is just the number of examples in each one of the epoch. The higher the batch size, the more memory you'll need", "By crushing I mean training process is failing at the very start of it. \r\nGPU RTX 2080\r\nRam 32gib\r\nI saw people with worse GPUs setting batch size to 1,000. I can set it to 14 and training process took 4 days to get to 1.5 in the loss function. ", "> By crushing I mean training process is failing at the very start of it.\r\n> GPU RTX 2080\r\n> Ram 32gib\r\n> I saw people with worse GPUs setting batch size to 1,000. I can set it to 14 and training process took 4 days to get to 1.5 in the loss function.\r\n\r\nyep, it's weird to crush with such high specs, I'd suggest you Google the specific error that TensorFlow or Python returns. it might be specific to the size of your dataset not accepting high batch sizes or something, I can't tell just by the sheer specs alone.", "> By crushing I mean training process is failing at the very start of it.\r\n> GPU RTX 2080\r\n> Ram 32gib\r\n> I saw people with worse GPUs setting batch size to 1,000. I can set it to 14 and training process took 4 days to get to 1.5 in the loss function.\r\n\r\nI think its a memory overcommitment issue. The solution is to increase the memory size from the default settings. The following link might help you (it worked for me) :\r\nhttps://stackoverflow.com/questions/57507832/unable-to-allocate-array-with-shape-and-data-type"]}, {"number": 26757, "title": "[ROCm] Enable ROCm support for the \"reverse\" op", "body": "This PR enables ROCm support for the \"reverse\" op.\r\n\r\nPR #26722 is a pre-req for this PR, and hence this PR includes commits from that PR.\r\nOnly the last commit in this PR should be reviewed here (as all others will be reviewed as part of PR #26722 )\r\n\r\n------------------\r\n\r\n@tatianashp , @whchung : just FYI", "comments": ["rebased to remove merge conflict", "Waiting on the rocprim change to get in first.", "rebased PR to account for the updates to PR #26722 ", "rebased PR to resolve merge-conflicts post the merge for PR #26722 .\r\n\r\nNote that now PR #28116 is a pre-requisite for this PR. This PR includes commits from PR #28116 ", "rebased PR to the tip of master (since all the pre-reqs are now merged).\r\n\r\nthe changes in this PR are now trivial...please approve and merge.\r\n\r\nthanks\r\n\r\ndeven"]}, {"number": 26756, "title": "[ROCm] Add ROCm support for batch_kernels", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26756) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26756) for more info**.\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26756) for more info**.\n\n<!-- ok -->", "Sorry I don't think I'm the right person to review all the ROCM changes, I don't have enough knowledge about how it works."]}, {"number": 26755, "title": "Request for Leaky Relu quantization support", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\n1.13.1\r\n- **Python version**:\r\n2.7.12\r\n- **Bazel version (if compiling from source)**:\r\n0.21\r\n- **GCC/Compiler version (if compiling from source)**:\r\n5.4.0\r\n- **CUDA/cuDNN version**:\r\nNo\r\n- **GPU model and memory**:- *\r\n*Exact command to reproduce**:\r\ntflite_convert --output_file=yolo2.tflite --graph_def_file=yolo2.pb --input_arrays=input_1 --output_arrays=conv2d_23/BiasAdd --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_dev_values=127 --default_ranges_min=0 --default_ranges_max=255\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nWhile trying to convert yolo-v2 tensorflow model to quantized tflite model, tflite_convert complains that LeakRelu quantization is not supported yet.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n2019-03-15 11:04:15.496254: F tensorflow/lite/toco/graph_transformations/quantize.cc:491] Unimplemented: this graph contains an operator of type LeakyRelu for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).\r\nAborted\r\n\r\n", "comments": ["@xiaomin05, as of now TFlite does not support this, however i have raised one PR which supports 8-bit quantization for leaky relu, till this PR is merged you can play around and test this. In case you find any issues do let me know.\r\nThe link for the PR is:-\r\nhttps://github.com/tensorflow/tensorflow/pull/27061\r\n\r\nRegards\r\nAmit", "thanks for the effort. i reviewed the code. it seems that you need to make change to quantize.cc as well, in order to make tflite_convert work:\r\n\r\nindex 2fa80f2..45873c2 100644\r\n--- a/tensorflow/lite/toco/graph_transformations/quantize.cc\r\n+++ b/tensorflow/lite/toco/graph_transformations/quantize.cc\r\n@@ -66,7 +66,8 @@ bool SupportsQuantization(const Operator& op) {\r\n          type == OperatorType::kPack || type == OperatorType::kTopK_V2 ||\r\n          type == OperatorType::kRandomUniform ||\r\n          type == OperatorType::kResizeNearestNeighbor ||\r\n-         type == OperatorType::kPRelu;\r\n+         type == OperatorType::kPRelu ||\r\n+         type == OperatorType::kLeakyRelu ;\r\n }", "@xiaomin05 , thanks for the comments, i will update this part as well, in the mean time can you please let me know if you have tested this implementation ? IF so kindly publish the results as well.\r\n\r\nRegards\r\nAmit", "Is this for post-training quantization or quantized training? I was able to do --post_training_quantize for a model with LeakyRelu from tf.nn_leaky_relu", "Hi,\r\n\r\nI am trying to perform post training integer quantization with configuration of `tf.lite.OpsSet.TFLITE_BUILTINS_INT8`. I have LeakyRelu in my model, so it throws an error of:\r\n`RuntimeError: Quantization not yet supported for op: LEAKY_RELU`. I navigated to here and found the PR. I installed `tf-nightly` (version: `2.1.0-dev20191009`) and tried to convert it again, but the same error `RuntimeError: Quantization not yet supported for op: LEAKY_RELU` is thrown. I wonder why.", "> Hi,\r\n> \r\n> I am trying to perform post training integer quantization with configuration of `tf.lite.OpsSet.TFLITE_BUILTINS_INT8`. I have LeakyRelu in my model, so it throws an error of:\r\n> `RuntimeError: Quantization not yet supported for op: LEAKY_RELU`. I navigated to here and found the PR. I installed `tf-nightly` (version: `2.1.0-dev20191009`) and tried to convert it again, but the same error `RuntimeError: Quantization not yet supported for op: LEAKY_RELU` is thrown. I wonder why.\r\n\r\nSame here. Did you find a solution?", "@zye1996 \r\n\r\nCheck https://github.com/tensorflow/tensorflow/pull/37279, https://github.com/tensorflow/tensorflow/issues/33397 for current status.", "Hi @xiaomin05 ! Can we move this issue to closed status now?  It seems concerned PR's has been merged from above [comment.](https://github.com/tensorflow/tensorflow/issues/26755#issuecomment-596315674)", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 26754, "title": "[ROCm] Enable ROCm support for \"sequence\" ops / \"reverse_sequence\" op", "body": "This PR enables ROCm support for the \"range\" / \"linspace\" / \"reverse_sequence\" ops.\r\n\r\nPR #26722 is a pre-req for this PR, and hence this PR includes commits from that PR.\r\nOnly the last two commit in this PR should be reviewed here (as all others will be reviewed as part of PR #26722 )\r\n\r\n----------------\r\n\r\n@tatianashp , @whchung : just FYI", "comments": ["rebased to remove merge conflicts", "Will get back to this in a bit. I would like to have the choice of macro sorted out in PR #26753 first.", "rebased PR to account for updates in PR #26722 ", "rebased PR to resolve merge-conflicts post the merge for PR #26722 .\r\n\r\nNote that now PR #28116 is a pre-requisite for this PR. This PR includes commits from PR #28116", "rebased PR to the tip of master (since all the pre-reqs are now merged).\r\n\r\nthe changes in this PR are now trivial...please approve and merge.\r\n\r\nthanks\r\n\r\ndeven"]}, {"number": 26753, "title": "[ROCm] Enable ROCm support for \"shape\" ops", "body": "This PR enables ROCm support for the \"shape\" ops.\r\n\r\nPR #26722 is a pre-req for this PR, and hence this PR includes commits from that PR.\r\nOnly the last commit in this PR should be reviewed here (as all others will be reviewed as part of PR #26722 )\r\n\r\n------------------\r\n\r\n@tatianashp , @whchung : just FYI", "comments": ["rebased to remove merge conflicts", "rebased PR to account for updates to PR #26722 ", "rebased PR to resolve merge-conflicts post the merge for PR #26722 .\r\n\r\nNote that now PR #28116 is a pre-requisite for this PR. This PR includes commits from PR #28116", "Removing my review request since @chsigg has reviewed.", "rebased PR to the tip of master (since all the pre-reqs are now merged).\r\n\r\nthe changes in this PR are now trivial...please approve and merge.\r\n\r\nthanks\r\n\r\ndeven"]}]