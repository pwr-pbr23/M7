[{"number": 14236, "title": "Branch 174534886", "body": "pushing internal commits", "comments": []}, {"number": 14235, "title": "Retraining the lower layers of the inception model for a better classifier. ", "body": "https://github.com/tensorflow/tensorflow/blob/4e75ae1f1e8c6479cfa86fde1a940453945e6671/tensorflow/examples/image_retraining/retrain.py#L26\r\n\r\nHi, I am trying to use the inception model to learn to classify between about 200 different object types.  I tried the approach mentioned here, which seems to only retrain the last few layer of the model, and I can't get about ~ 60% accuracy even when I am training the model. I imagine if I pass on the gradient and change the weights in the lower layers, that I will be able to improve the accuracy of the model when applied to my data set. \r\n\r\nHow do I go about doing this ? Any guidance on this being done else where would be extremely helpful. ", "comments": ["The `retrain.py` example adds a new layer to the end of an existing model. Whether a single layer is sufficient, or whether the number of parameters chosen is sufficient, or how much data you should have to train effectively, or what the most effective way to train your classifier of 200 objects is are broad questions which aren't a matter of a TensorFlow bug or feature request :)\r\n\r\nApologies, but I'm going to close this issue out since the TensorFlow github issues are meant for bugs and feature requests.\r\n", "I wasn't asking a broad question. I was asking how can I pass the gradients down to the lower layers of the pre trained Inception Model. Only the last layers are retrained, how can I retrain the lower layers of an existing model ? \r\n"]}, {"number": 14234, "title": "Add missing flags to tfdbg doc chart", "body": "Some command flags for tfdbg weren't listed in the doc chart. This commit adds the ones that are missing.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 14233, "title": "graph_editor doesn't update graph_version", "body": "Symptom:\r\n1. session.run\r\n2. modify the graph with tf.contrib.graph_editor\r\n3. session.run\r\n\r\nSession.run in step 3 will use the same graph as in step 1 (pre-modification)\r\n\r\nThe reason is that session.py looks at [self._graph.version]( https://github.com/tensorflow/tensorflow/blob/4e75ae1f1e8c6479cfa86fde1a940453945e6671/tensorflow/python/client/session.py#L1345) when deciding whether to trigger TF_ExtendGraph. Since `graph_editor` doesn't update graph_version, it will reuse the previous graph.\r\n\r\nSuggestion: every graph-modifying method in `graph_editor` should increment `version` attribute for the graph\r\n\r\n@purpledog ", "comments": ["@purpledog, can you take a look at this feature request?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "@yaroslavvb  Do you still see this as an issue? Looks like the code you're referring to has changed between r1.9 and r1.10.\r\n\r\nAlso, looking at the current document below, it seems if you are trying to edit graph in session it is not allowed.  https://www.tensorflow.org/api_guides/python/contrib.graph_editor\r\n\r\n@purpledog  can we close this if thats the case?", "@yaroslavvb , As per the information provided [here](https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md) graph editor has been deleted in the latest Tensorflow version. \r\nYou can check [this](https://www.tensorflow.org/api_docs/python/tf/Graph) documentation for anything related to Tensorflow graph. \r\nFeel free to close the issue if you don't have any further question.", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/14233\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/14233\">No</a>\n"]}, {"number": 14232, "title": "[Feature Request] Automatic ClusterSpec Propagation for multiple hosts", "body": "Currently, when launching a distributed TensorFlow job, user need to manually input all the worker hosts' IP and port number. This is not too convenient and does not scale well. It would be really nice to have the native TensorFlow functionality that workers can automatically register themselves on master service without knowing all the host IP and port beforehand. Not sure if there is existing solution to solve this problem. But I used some of the building blocks (ClusterSpec Propagation and ClusterResolver) to enable this feature on my client code. Please let me know if it is a good approach to do this, I'd love to contribute if there's interest in this functionality. \r\n\r\nThe solution I have involve the following steps:\r\n1) Master service start server with user specified port and wait for all workers to register\r\n2) Worker register themselves by sending ClusterSpec to Master service and wait for response\r\n3) Master service waits until numbers of workers registered matches requested worker number\r\n4) Master service merges ClusterSpecs and propagates to all registered workers\r\n5) Master service and workers continues", "comments": ["@mrry, can you please take a look at this feature request?\r\n", "Passing on to @saeta for ClusterSpec propagation and @frankchn for ClusterResolver.\r\n\r\nIt would be neat if we could implement this without big changes to the runtime. For example, could we use a TF server to implement step 1, and short-lived sessions targeting the master from each of the workers to implement steps 2 and 3?", "Hi @UWFrank, is your code available as open source anywhere? We are happy to take a look at it.", "Hi @frankchn, my design would require a new API for creating the server. Parameters such as num_ps_hosts, num_worker_hosts, job_name and task_index need to be provided by the user. I am working on a patch right now and can send a PR once done.", "Hi @UWFrank, would it be possible to write up a short Google Doc/Markdown doc about any proposed API changes? We try to review API changes early in the design process so to avoid extensive rewrites, etc...", "/sub", "@UWFrank This is an area of interest to us as well. Please keep us posted and let me know if I can be of any help in this area.", "Hello @frankchn I've drafted a [design proposal](https://docs.google.com/document/d/18XgUwmmxvhR-sk9A2KFD8Wom8H0sfI7PKtacVI_7jvc) that include major API changes, please let me know your thoughts. @yongtang thank you for your interest, please also feel free to comment on the design doc.", "@UWFrank In a async SGD cluster there's typically two jobs, `job_name='ps'` and `job_name='worker'`. Your API suggests to use following to create coordinator:\r\n`create_distributed_server(num_workers, job_name, task_index, master_host, is_master=False)`\r\n\r\nDoes this mean you would create two distributed servers, one for each job?\r\n\r\nFor efficient async SGD, the number of live hosts in worker job should be allowed to change over time without affecting session creation, or session run calls of remaining live hosts. This allows training to proceed even if some hosts are unavailable. Currently this is achieved using sparse job config (https://github.com/tensorflow/tensorflow/commit/8177edd7700ccbe0c831e680a1acb5275819d762). The cluster propagation design seems to preclude this use-case.\r\n\r\nIn this use case, here's what some ClusterSpecs look like for a cluster with 4 `ps` tasks and 2 `worker` tasks:\r\n\r\n```\r\njob_name=worker, task_id=1:\r\n{'worker': {1: '172.31.35.121:3000'}, 'ps': ['172.31.42.219:3000', '172.31.38.22:3000', '172.31.32.142:3000', '172.31.35.253:3000']})\r\n\r\njob_name=worker, task_id=2\r\n{'worker': {2: '172.31.41.31:3000'}, 'ps': ['172.31.42.219:3000', '172.31.38.22:3000', '172.31.32.142:3000', '172.31.35.253:3000']})\r\n\r\njob_name=ps, task_id=3:\r\n{'worker': ['172.31.40.171:3000', '172.31.35.121:3000', '172.31.41.31:3000', '172.31.36.117:3000', '172.31.38.4:3000'], 'ps': {3: '172.31.35.253:3000'}})\r\n```\r\n\r\nEach `tf.train.Server` instance (worker or ps) knows about itself. Worker servers need to know locations of all ps shards, but not of other workers. PS servers need to know about all workers but not other ps shards. As long as ps servers are only used as [workers](https://github.com/tensorflow/tensorflow/blob/9089ab59826ce656b6f8026b08471ca6696cacf9/tensorflow/core/protobuf/worker.proto), and not as [masters](https://github.com/tensorflow/tensorflow/blob/2eac53d5ea540b0b09326ba9330b6051d742532d/tensorflow/core/protobuf/master.proto), the number of live worker servers can change without affecting training. On other hand, training will stop if any ps server goes down.", "@yaroslavvb I've updated the draft to address your comment to support sparse job config.\r\nI added a new parameter 'sparse_jobs_config' to allow users to specify related job's task_index so that only those host addresses get propagated back from the master server.\r\n\r\n> Does this mean you would create two distributed servers, one for each job?\r\n\r\nEach **task** launch their own server.There should only be one master for each cluster. I should clarify that the worker referenced in 'num_workers' in the parameter is not job_name='worker', but type of server.\r\n\r\nPlease let me know your thoughts.\r\n\r\nThanks", "relevant: https://github.com/tensorflow/tensorflow/issues/14232", "Quick update, @UWFrank's implementation is almost code complete, we'll be doing some minor tweaks / testing and will target posting the implementation sometime early next month.", "Has any progress been made regarding this issue or #11896?", "@hungj @UWFrank is there any updates?", "This is an important feature when building scalable ml systems.  is it still active?", "@UWFrank  What's `ManagedTrainingSession` mentioned in [README](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/contrib/cluster_resolver/README.md) exactly refers to ?\r\nIs it a session which we can pass clusterSpec in session creation?\r\n\r\nThanks", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "Hi is there any update? I think it is an important feature for the community.", "The new way to use TensorFlow for distributed training is called distribution strategies: https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/contrib/distribute/README.md, so we do encourage you to take a look at those examples.", "Is there any updates?", "In case someone else is also confused by this, TF has the ability to propagates cluster spec automatically. For example the ps servers can boot without knowing any worker devices.\r\nSee #38519 "]}, {"number": 14231, "title": "CUDA_ERROR_OUT_OF_MEMORY on multiple and single GPU set up (1080Ti)", "body": "Hi. I am having troubles running any tensorflow code on my 4x1080Ti GPU set up.\r\n\r\nThis is the code I am running (just to reproduce the issue):\r\n----------------------------------------------------------------\r\nimport tensorflow as tf\r\na = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\nb = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\nc = tf.matmul(a, b)\r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.6) #with or without this line of code\r\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True, gpu_options=gpu_options))\r\nprint(sess.run(c))\r\nsess.close()\r\n\r\nBut eventually my python session crashes with out of memory error. The log allso indicate that.\r\nWhen I force the session to only use one GPU (using environment variable CUDA_VISIBLE_DEVICES=0), it becomes more stable but crashed later when I run an actual training code (quite basic also).\r\n\r\n------------------------\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Pro 64bits\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: b'unknown' 1.3.0\r\n- **Python version**: Python 3.5.3 :: Intel Corporation\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: cuda_8.0.61.2 / cuDNN 8.0\r\n- **GPU model and memory**: 1080Ti 11GB\r\n- **Exact command to reproduce**:\r\n\r\n\r\nHere is the log:\r\n------------------------\r\n\r\n\r\nC:\\Users\\Riad\\Desktop\\Courses>jupyter notebook\r\n[W 11:33:32.835 NotebookApp] All authentication is disabled.  Anyone who can connect to this server will be able to run code.\r\n[I 11:33:32.887 NotebookApp] Serving notebooks from local directory: C:\\Users\\Riad\\Desktop\\Courses\r\n[I 11:33:32.887 NotebookApp] 0 active kernels\r\n[I 11:33:32.888 NotebookApp] The Jupyter Notebook is running at: http://0.0.0.0:8888/\r\n[I 11:33:32.888 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\r\n[W 11:34:06.855 NotebookApp] 404 GET /nbextensions/widgets/notebook/js/extension.js?v=20171103113331 (192.168.100.9) 15.23ms referer=http://rrig01:8888/notebooks/Untitled.ipynb\r\n[I 11:34:07.181 NotebookApp] Kernel started: a846f3bb-429b-49d9-b3dc-90e8d47d67fc\r\n2017-11-03 11:34:25.408488: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-11-03 11:34:25.408577: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-11-03 11:34:26.328062: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:955] Found device 0 with properties:\r\nname: GeForce GTX 1080 Ti\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.6705\r\npciBusID 0000:01:00.0\r\nTotal memory: 11.00GiB\r\nFree memory: 9.08GiB\r\n2017-11-03 11:34:26.762169: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:523] A non-primary context 00000171274F5980 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.\r\n2017-11-03 11:34:26.763254: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:955] Found device 1 with properties:\r\nname: GeForce GTX 1080 Ti\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.6705\r\npciBusID 0000:02:00.0\r\nTotal memory: 11.00GiB\r\nFree memory: 9.08GiB\r\n2017-11-03 11:34:27.217289: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:523] A non-primary context 00000171274EB2C0 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.\r\n2017-11-03 11:34:27.218417: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:955] Found device 2 with properties:\r\nname: GeForce GTX 1080 Ti\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.6705\r\npciBusID 0000:04:00.0\r\nTotal memory: 11.00GiB\r\nFree memory: 9.08GiB\r\n2017-11-03 11:34:27.652094: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:523] A non-primary context 000001712754BC30 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.\r\n2017-11-03 11:34:27.654175: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:955] Found device 3 with properties:\r\nname: GeForce GTX 1080 Ti\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.6705\r\npciBusID 0000:05:00.0\r\nTotal memory: 11.00GiB\r\nFree memory: 9.08GiB\r\n2017-11-03 11:34:27.654302: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:847] Peer access not supported between device ordinals 0 and 1\r\n2017-11-03 11:34:27.654690: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:847] Peer access not supported between device ordinals 0 and 2\r\n2017-11-03 11:34:27.654941: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:847] Peer access not supported between device ordinals 0 and 3\r\n2017-11-03 11:34:27.655238: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:847] Peer access not supported between device ordinals 1 and 0\r\n2017-11-03 11:34:27.655512: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:847] Peer access not supported between device ordinals 1 and 2\r\n2017-11-03 11:34:27.655806: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:847] Peer access not supported between device ordinals 1 and 3\r\n2017-11-03 11:34:27.656091: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:847] Peer access not supported between device ordinals 2 and 0\r\n2017-11-03 11:34:27.656392: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:847] Peer access not supported between device ordinals 2 and 1\r\n2017-11-03 11:34:27.656680: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:847] Peer access not supported between device ordinals 2 and 3\r\n2017-11-03 11:34:27.656974: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:847] Peer access not supported between device ordinals 3 and 0\r\n2017-11-03 11:34:27.657493: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:847] Peer access not supported between device ordinals 3 and 1\r\n2017-11-03 11:34:27.657800: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:847] Peer access not supported between device ordinals 3 and 2\r\n2017-11-03 11:34:27.658247: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:976] DMA: 0 1 2 3\r\n2017-11-03 11:34:27.658286: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:986] 0:   Y N N N\r\n2017-11-03 11:34:27.658545: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:986] 1:   N Y N N\r\n2017-11-03 11:34:27.658580: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:986] 2:   N N Y N\r\n2017-11-03 11:34:27.658606: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:986] 3:   N N N Y\r\n2017-11-03 11:34:27.658660: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0)\r\n2017-11-03 11:34:27.658997: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0)\r\n2017-11-03 11:34:27.659402: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1045] Creating TensorFlow device (/gpu:2) -> (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0)\r\n2017-11-03 11:34:27.659795: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1045] Creating TensorFlow device (/gpu:3) -> (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0)\r\n2017-11-03 11:34:28.549763: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 8.63G (9267854592 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:28.986715: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 7.77G (8341068800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:29.440854: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 6.99G (7506961920 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:29.887158: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 6.29G (6756265472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:30.135809: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 8.63G (9267854592 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:30.168426: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 7.77G (8341068800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:30.201799: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 6.99G (7506961920 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:30.247813: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 6.29G (6756265472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:30.287203: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 5.66G (6080638976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:30.320556: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 5.10G (5472574976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:30.357686: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 4.59G (4925317120 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:30.394005: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 4.13G (4432785408 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:30.426176: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 3.71G (3989506816 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:30.457480: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 3.34G (3590556160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:30.491576: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 3.01G (3231500544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:30.527402: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 2.71G (2908350464 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:30.561202: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 2.44G (2617515264 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:30.594297: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 2.19G (2355763712 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:30.628747: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 1.97G (2120187392 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:30.665536: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 1.78G (1908168704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:30.707833: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 1.60G (1717351936 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:30.743971: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 1.44G (1545616640 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:30.779594: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 1.29G (1391055104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:30.810792: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 1.17G (1251949568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:30.841555: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 1.05G (1126754560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:30.873425: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 967.10M (1014079232 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:30.943823: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 870.39M (912671488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:30.977474: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 783.35M (821404416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.010872: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 705.02M (739264000 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.041557: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 634.51M (665337600 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.101954: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 571.06M (598803968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.132940: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 513.96M (538923776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.152136: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 8.63G (9267854592 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.160670: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 7.77G (8341068800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.161213: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 6.99G (7506961920 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.162127: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 6.29G (6756265472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.163461: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 5.66G (6080638976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.164258: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 5.10G (5472574976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.212887: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 4.59G (4925317120 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.214000: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 4.13G (4432785408 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.214529: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 3.71G (3989506816 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.215007: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 3.34G (3590556160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.215633: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 3.01G (3231500544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.216019: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 2.71G (2908350464 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.216421: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 2.44G (2617515264 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.216896: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 2.19G (2355763712 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.217393: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 1.97G (2120187392 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.218201: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 1.78G (1908168704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.219158: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 1.60G (1717351936 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.220334: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 1.44G (1545616640 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.223796: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 1.29G (1391055104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.224705: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 1.17G (1251949568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.228203: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 1.05G (1126754560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.228790: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 967.10M (1014079232 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.240002: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 870.39M (912671488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.240616: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 783.35M (821404416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.241031: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 705.02M (739264000 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.241440: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 634.51M (665337600 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.241912: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 571.06M (598803968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.243669: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 513.96M (538923776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.244271: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 462.56M (485031424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.370999: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 416.31M (436528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.371450: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 374.67M (392875520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.382825: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 337.21M (353587968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.383636: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 303.49M (318229248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.384740: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 273.14M (286406400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.446815: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 245.82M (257765888 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.447603: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 221.24M (231989504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.448226: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 199.12M (208790784 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.448853: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 179.21M (187911936 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.449435: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 161.29M (169120768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.450162: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 145.16M (152208896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.456715: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 130.64M (136988160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.457435: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 117.58M (123289344 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.457901: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 105.82M (110960640 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.458436: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 95.24M (99864576 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.458854: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 85.71M (89878272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.459447: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 77.14M (80890624 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.460854: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 69.43M (72801792 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.461332: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 62.49M (65521664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.461884: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 56.24M (58969600 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.462263: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 50.61M (53072640 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.462685: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 45.55M (47765504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.463065: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 41.00M (42989056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.465693: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 36.90M (38690304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.466117: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 33.21M (34821376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.466556: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 29.89M (31339264 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.474153: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 26.90M (28205568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.474574: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 24.21M (25385216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-03 11:34:31.475153: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 21.79M (22846720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0\r\n/job:localhost/replica:0/task:0/gpu:1 -> device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0\r\n/job:localhost/replica:0/task:0/gpu:2 -> device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0\r\n/job:localhost/replica:0/task:0/gpu:3 -> device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0\r\n2017-11-03 11:34:31.586968: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\direct_session.cc:300] Device mapping:\r\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0\r\n/job:localhost/replica:0/task:0/gpu:1 -> device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0\r\n/job:localhost/replica:0/task:0/gpu:2 -> device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0\r\n/job:localhost/replica:0/task:0/gpu:3 -> device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0\r\n\r\nMatMul: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\n2017-11-03 11:34:31.596219: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\simple_placer.cc:872] MatMul: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\nb: (Const): /job:localhost/replica:0/task:0/gpu:0\r\n2017-11-03 11:34:31.674827: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\simple_placer.cc:872] b: (Const)/job:localhost/replica:0/task:0/gpu:0\r\na: (Const): /job:localhost/replica:0/task:0/gpu:0\r\n2017-11-03 11:34:31.675394: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\simple_placer.cc:872] a: (Const)/job:localhost/replica:0/task:0/gpu:0\r\n[I 11:36:07.126 NotebookApp] Saving file at /Untitled.ipynb\r\n\r\n\r\n\r\n", "comments": ["Looking at your logs:\r\n```\r\n2017-11-03 11:34:28.549763: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:924] failed to allocate 8.63G (9267854592 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n```\r\nI can believe that a tensor of this size can be difficult to fit on your GPUs, if your process allocated anything else on the same device. I think this is an issue in the training code you are trying to run.", "Gunan, if you check the code I am running to reproduce  this issue, I am loading anything in memory myself, just constants and such. But TF tries to allocate 8gb and it fails. Though I have 1080Ti with 11gb memory. I also used session gpu options to limit memory allocation but same thing.\r\nI suspect a compatibility issue between my TF installation and the CUDA drivers. \r\nAny idea? ", "What if you set `allow_growth` in your GPUOptions?\n\nOn Mon, Nov 6, 2017 at 3:54 PM, riadsouissi <notifications@github.com>\nwrote:\n\n> Gunan, if you check the code I am running to reproduce this issue, I am\n> loading anything in memory myself, just constants and such. But TF tries to\n> allocate 8gb and it fails. Though I have 1080Ti with 11gb memory. I also\n> used session gpu options to limit memory allocation but same thing.\n> I suspect a compatibility issue between my TF installation and the CUDA\n> drivers.\n> Any idea?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/14231#issuecomment-342327638>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOWtR64YKc0gZbBXnOggZLmkbjRWaks5sz5wsgaJpZM4QRsD->\n> .\n>\n", "gunan, thanks for the feedback. I upgraded to TF 1.4 and used your option as well. Things are much more stable now, though training with GPU on my machine is slower than CPU, mostly due to my setup have been configured  for mining (low PCI for stability). Will play with PCI and see..."]}, {"number": 14230, "title": "Build from source documentation is incorrect", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**: **4.5**\r\n- **GCC/Compiler version (if compiling from source)**: 4.8\r\n- **CUDA/cuDNN version**: 8/6\r\n- **GPU model and memory**: nvidia\r\n- **Exact command to reproduce**: bazel build --config=opt --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\n### Describe the problem\r\nDocumentation is incorrect: https://www.tensorflow.org/install/install_sources\r\n\r\n\"Tested source configurations\" lists bazel 0.4.5 for tensorflow 1.4.0. Tensorflow 1.4.0 rejects this version of bazel with an error message:\r\n\r\nCurrent Bazel version is 0.4.5, expected at least 0.5.4\r\n\r\n### Source code / logs\r\n```\r\n$ bazel build --config=opt --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package\r\n ---> Running in 547db5b015bf\r\nWARNING: Config values are not defined in any .rc file: opt\r\nERROR: /tmp/tensorflow/WORKSPACE:41:1: Traceback (most recent call last):\r\nFile \"/tmp/tensorflow/WORKSPACE\", line 41\r\n\t\ttf_workspace()\r\n\tFile \"/tmp/tensorflow/tensorflow/workspace.bzl\", line 146, in tf_workspace\r\n\t\tcheck_version(\"0.5.4\")\r\n\tFile \"/tmp/tensorflow/tensorflow/workspace.bzl\", line 56, in check_version\r\n\t\tfail(\"\r\nCurrent Bazel version is {}, e...))\r\n\r\nCurrent Bazel version is 0.4.5, expected at least 0.5.4\r\n```\r\n", "comments": ["Thanks for pointing this out.\r\n\r\n@jugglerix @gunan : Seems like the table needs to be updated?", "I was able to successfully build with 0.5.4, instead of 0.4.5", "@av8ramit Could you check if we have the information correct in the table?", "& is 0.7.0 acceptable, because the Bazel installation guide the TensorFlow doc points to has only 0.7.0 in it's repo (unless installing from git) -- a list of acceptable releases might be good.", "Yes, unfortunately the information in that table is out of date for 1.4.0. I'll update it accordingly.", "https://github.com/tensorflow/tensorflow/pull/14298 thank you @jugglerix for pointing this out.", "https://www.tensorflow.org/install/install_sources"]}, {"number": 14229, "title": "tf.contrib.learn.Experiment - train_monitors should be renamed", "body": "Shouldn't **train_monitors** be renamed to **train_hooks**? As far as I understand hooks replaced monitors.\r\n\r\nI look at TF v1.4.0-rc1.\r\n\r\ntf.contrib.learn.Experiment(estimator, train_input_fn, eval_input_fn, eval_metrics=None, train_steps=None, eval_steps=100, **train_monitors**=None, **eval_hooks**=None, local_eval_frequency=None, eval_delay_secs=120, continuous_eval_throttle_secs=60, min_eval_frequency=None, delay_workers_by_global_step=False, export_strategies=None, train_steps_per_iteration=None, checkpoint_and_export=False)", "comments": ["There seems to be some difference between tf.estimator and tf.contrib.learn (though i think tf.estimator comes from tf.contrib.learn)\r\nThe similar issue is ```train()``` and ```fit```, it would be great if we can make two APIs compatible in these functions and arguments in the long term ", "Thanks for pointing this out.\r\n\r\nAs outlined in our [versioning guarantees policy](https://www.tensorflow.org/programmers_guide/version_compat#what_is_not_covered), things in the `tf.contrib` namespace are considered experimental and subject to change.\r\n\r\nSo we allow for differences between say `tf.estimator` and `tf.contrib.learn`, as a means of learning from feedback on functions and classes in `tf.contrib` before settling down on an API we will provide stability guarantees for going forward (`tf.estimator` in this case).\r\n\r\nIn the long term, the `tf.contrib` symbols will go away (as we either figure out that they are not worth pursuing or adapt them into ones that we will support in a non-`contrib` module).\r\n\r\nHope that makes sense.\r\n\r\nThanks!\r\n\r\n(FYI @ispirmustafa for this particular case)"]}, {"number": 14228, "title": "Feature request: an op that returns timestamp", "body": "This is useful for stats tracking in distributed TensorFlow (ie, measuring TF communication latency between workers and plotting it in TensorBoard). Perhaps it could be called `CurrentTimestamp`, `tf.current_timestamp()`\r\n\r\n```\r\n#include \"tensorflow/core/platform/env.h\"\r\nstatic EnvTime* env_time = tensorflow::EnvTime::Default();\r\nuint64 now_micros = env_time->NowMicros();\r\n```", "comments": ["If you would like to plot time stamp on tensorboard, you can use include it in tf.summary\r\nFor example,\r\nsummary_writer = tf.summary.FileWriter()\r\n\r\nsummary = tf.Summary()\r\nsummary.value.add(tag='xxx', simple_vale=yyy)\r\nsummary_writer.add_summary(summary, global_step)\r\n", "How would you plot communication latency between workers using tf.summary?\r\nYou need something like CurrentTimestamp op to execute on one worker, and then compare it against timestamp on another worker to get the latency"]}, {"number": 14227, "title": "Bulleted lists need a leading blank line.", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please"]}, {"number": 14226, "title": "decode_image resize_images workflow does not work", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: ('v1.3.0-rc2-20-g0787eee', '1.3.0')\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: \r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: Performing `resize_images` with output of `decode_image`\r\n\r\n### Describe the problem\r\nThis problem was addressed in https://github.com/tensorflow/tensorflow/issues/1029 but was closed. https://github.com/tensorflow/tensorflow/issues/8551 and https://github.com/tensorflow/tensorflow/issues/9356 are also relevant. \r\n\r\nThe basic problem is that the output of `tf.image.decode_image` cannot be passed to `tf.image.resize_images`. It raises `ValueError: 'images' contains no shape.` in the call to `resize_images`. \r\n\r\nPossible workarounds to this include using `decode_jpeg`, `decode_png` or adding `decoded_image.set_shape([None, None, None])` before calling `tf.image.resize_images`. However, as @girving pointed out in #1029, nothing about the underlying op requires knowing a static shape. \r\n\r\n### Source code / logs\r\n    decoded_image = tf.image.decode_jpeg(tf.read_file(image_filename))\r\n    tf.image.resize_images(decoded_image, (100, 100))\r\n", "comments": ["I think the issue is that in `def decode_image()`, BMP, JPEG, and PNG images would have the shape of `[height, width, num_channels]`, and GIF images would have the shape of `[num_frames, height, width, 3]`.\r\nSo the rank of the shape could be 3 or 4, depending on the image type.\r\n\r\nUnfortunately, finding out if it is GIF image will require checking the content of the input tensor which is not available until runtime.", "Even if `decode_image` can't give a compile time size, why does `tf.image.resize_images` require image size at compile time?", "It could be a solution to implement a decode_bmp_jpg_png() function besides decode_image(). This function could then return a shape like decode_bmp, decode_jpg and decode_png do.", "Or maybe add an optional argument, e.g., `candidate=\"jpg,bmp\"` to `decode_image` so that the possible image formats to be probed are restricted.\r\n\r\nEither way, if the rank of the shape is determined then I can help creating a PR to have the shape defined when returned from `decode_image`.", "I worked with these decode image ops recently with the C++ API and I remember having to code a bunch of logic into the graph to guarantee a certain shape output.\r\n\r\n@mingxingtan would it be possible to add a parameter to `tf.image.decode_image` to make this simpler? Maybe it could be `force=None` for existing behavior, `force='4d'` to add a JPEG/PNG/BMP have a fourth dimension, or `force='3d'` to truncate GIF output to only have the first frame.", "perhaps an argument for tf.image.resize_images to tell it the number of dimensions would work? or a 3D option for the other functions. I think this is an issue for mapping with datasets as the image processing functions accept a mixture of 4D and 3D tensors so hard to combine. @girving was looking at this topic in a prior thread", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "I'm going to close this one out due to inactivity. I agree that the behavior could be improved but it's probably not something we can work on right now.", "But shouldn't this stay open then? I mean, if we agree that there is a change needed, the change should be done some day eventually. When the ticket is closed, nobody will remember it and thus it will never be changed.", "I try to run the offical sample code(https://www.tensorflow.org/programmers_guide/datasets) and get  same error.\r\n\r\nerror detail:\r\n----> 4   image_resized = tf.image.resize_images(image_decoded, [28, 28])\r\n          5   return image_resized, label\r\n\r\n--> 741     raise ValueError('\\'images\\' contains no shape.')\r\n      742   # TODO(shlens): Migrate this functionality to the underlying Op's.\r\n      743   is_batch = True\r\n\r\n\r\nmy tf versiom:\r\ntensorflow-gpu (1.4.1)\r\n\r\nmy python version:\r\npython 2.7\r\n\r\n\r\n", "This is also how I stumpled onto this. When following the official instructions xiaogea01 mentioned you will run into this problem.", "`tf.image.decode_image` is a wrapper of many other ops: `decode_jpeg`, `decode_gif`, `decode_png`, etc, so the output shape depends on the image type, and it cannot determine the shape of the output until actually seeing the content at execution time.\r\n\r\nFor now, you can simply replace the `tf.image.decode_image` with more specific ops, i.e.,  `tf.image.decode_jpeg` for jpeg, `tf.image.decode_png` for png, etc.  (See https://www.tensorflow.org/api_docs/python/tf/image for more details).", "I have same problem!!\r\n\r\npython lib/src/create_face_embeddings.py \r\n/home/neamah/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nModel filename: /home/neamah/Downloads/Face_Recognition-master/lib/src/ckpt/20180408-102900.pb\r\nTraceback (most recent call last):\r\n  File \"lib/src/create_face_embeddings.py\", line 100, in <module>\r\n    main(parse_arguments(sys.argv[1:]))\r\n  File \"lib/src/create_face_embeddings.py\", line 63, in main\r\n    images_placeholder = tf.image.resize_images(images_placeholder,(160,160))\r\n  File \"/home/neamah/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/image_ops_impl.py\", line 997, in resize_images\r\n    raise ValueError('\\'images\\' contains no shape.')\r\nValueError: 'images' contains no shape.", "I have the same problem too: \r\n\r\n    for image_path in dataset_file:\r\n        if \"Null\" not in image_path:\r\n            image_path_ = image_path.strip()\r\n            image_tf_read = tf.read_file(image_path_)\r\n            image_decoded = tf.image.decode_png(image_tf_read)\r\n            images_decoded_.append(image_decoded)\r\n            print(\"IMAGE:  \"+ image_decoded.eval().shape + \"\\n\")\r\n    images_decoded_tf = tf.stack(images_decoded_,axis=0)\r\n   \r\nERORR would be: \r\n\r\n   raise ValueError('\\'images\\' contains no shape.')\r\nValueError: 'images' contains no shape.", "can't believe this issue is still not addressed after 2 years.\r\nI had this problem in 1.13.1 (most recent version, except for tf 2.0beta)", "> can't believe this issue is still not addressed after 2 years.\r\n> I had this problem in 1.13.1 (most recent version, except for tf 2.0beta)\r\n\r\nRefer to Geoffrey Irving's answer in https://github.com/tensorflow/tensorflow/issues/9356#issuecomment-309144064\r\n\r\n> decode_png and decode_jpeg now decode all formats.\r\n\r\nSimilar solution has been suggested by Nagashayan in\r\nhttps://stackoverflow.com/questions/44942729/tensorflowvalueerror-images-contains-no-shape"]}, {"number": 14225, "title": "Clean up our libcuda stub when building the GPU Docker container", "body": "", "comments": ["Welp, our builds are broken for 1.4 branch since the builds scripts aren't versioned with the branches.\r\n\r\nNew stuff was added to the ubilds which didn't exist before. Will have to force-submit this :("]}, {"number": 14224, "title": "tensorflow/tensorflow:1.4.0-devel-gpu uses libcuda.so stub at runtime", "body": "```\r\n$ nvidia-docker run -ti tensorflow/tensorflow:1.4.0-devel-gpu /bin/bash\r\nroot@2ad65cce2269:~# ldd /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n[...]\r\n        libcuda.so.1 => /usr/local/cuda/lib64/stubs/libcuda.so.1 (0x00007f9de7215000)\r\n```\r\nAs a result, you can't use CUDA in this image:\r\n```\r\n2017-11-03 17:57:11.181620: E tensorflow/stream_executor/cuda/cuda_driver.cc:406] failed call to cuInit: CUresult(-1)\r\n2017-11-03 17:57:11.181660: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: 939b58c94187\r\n2017-11-03 17:57:11.181677: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: 939b58c94187\r\n2017-11-03 17:57:11.181848: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\r\n```\r\n\r\nIssue was caused by: https://github.com/tensorflow/tensorflow/pull/13399\r\nPlease backport https://github.com/tensorflow/tensorflow/pull/13456 into `r1.4`", "comments": ["@gunan ", "@case540 can we patch #13456 into the release branch and rebuild our docker images?", "Ok, will get started on this. #14225", "@gunan this seems to be fixed, right?", "Patch was cherry-picked to 1.4. In process of rebuilding/uploading docker images for 1.4", "New dockers built and uploaded. Unsure how to verify issue is fixed.", "```\r\ngunan@dragonite:~$ nvidia-docker run -ti tensorflow/tensorflow:1.4.0-devel-gpu /bin/bash\r\n1.4.0-devel-gpu: Pulling from tensorflow/tensorflow\r\n...\r\nStatus: Downloaded newer image for tensorflow/tensorflow:1.4.0-devel-gpu\r\nroot@d39ef3933d75:~# python\r\nPython 2.7.12 (default, Nov 19 2016, 06:48:10) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.test.is_gpu_available()\r\n.....\r\nTrue\r\n>>> \r\n```\r\nLooks like GPU is now accessible on 1.4 devel docker images.\r\n\r\n"]}, {"number": 14223, "title": "what's is forget_bias? ", "body": "what's is forget_bias? I checked the basicllstmcell variable. but I found the biases are 0 althought I changed forget_bias to 0 or 1. I heard the func of it is that don't forget input of  first time .  please feedback", "comments": ["Please see the [documentation](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell). It's a bias added to the forget gate described in the paper at http://arxiv.org/abs/1409.2329 \r\n\r\nHope that helps! Such questions are better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14222, "title": "Add int64 `Tdim` support for `ExpandDims`", "body": "This fix tries to add int64 `Tdim` support for `ExpandDims`.\r\n\r\nIn `array_ops.cc`, `ExpandDims` registers both int32 and int64 support for `Tdim`. However, only int32 kernel for `ExpandDims` has been supported.\r\n\r\nThis fix addresses the discrepancy by adding the support of int64 `Tdim` for `ExpandDims`.\r\n\r\nAdditional tests has also been added to cover the changes.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 14220, "title": "64MB protobuf limit", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: 3.5\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\na = tf.constant(np.zeros(10000000))\r\nprofiler = tf.profiler.Profiler(tf.get_default_graph())\r\n```\r\n\r\n### Describe the problem\r\nRunning the above code results in the error message:\r\n```\r\n[libprotobuf ERROR C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\35\\cmake_build\\protobuf\\src\\protobuf\\src\\google\\protobuf\\io\\coded_stream.cc:208] A protocol message was rejected because it was too big (more than 67108864 bytes).  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\r\n[libprotobuf ERROR C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\35\\cmake_build\\protobuf\\src\\protobuf\\src\\google\\protobuf\\text_format.cc:298] Error parsing text-format tensorflow.GraphDef: 2:1: Interpreting non ascii codepoint 224.\r\n[libprotobuf ERROR C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\35\\cmake_build\\protobuf\\src\\protobuf\\src\\google\\protobuf\\text_format.cc:298] Error parsing text-format tensorflow.GraphDef: 2:1: Expected identifier, got: \u00e0\r\nFailed to parse graph\r\n```\r\n\r\nReading #582 suggests that this problem should have been fixed in `protobuf 3.2.0`, but that doesn't seem to be the case in my system.  This may be a Windows-specific issue?\r\n\r\n### Source code / logs\r\nHere is the output of `pip freeze` showing the installed versions of tensorflow/protobuf (this is all starting from a fresh environment, and installed via `pip install tensorflow`).\r\n\r\n```\r\nbleach==1.5.0\r\ncertifi==2016.2.28\r\nenum34==1.1.6\r\nhtml5lib==0.9999999\r\nMarkdown==2.6.9\r\nnumpy==1.13.1\r\nprotobuf==3.4.0\r\nsix==1.11.0\r\ntensorflow==1.4.0\r\ntensorflow-tensorboard==0.4.0rc2\r\nWerkzeug==0.12.2\r\nwincertstore==0.2\r\n```\r\n", "comments": ["Can you please try installing the nightly build of TensorFlow (`pip install tf-nightly`) and see if this is still a problem? \r\n\r\nIt looks like b927df57f0c09ea62a855795e340d6daf70553df upgrades TensorFlow on Windows to a newer version of protobuf, and that patch wasn't present in the 1.4 release.", "That works!  So this should be fixed in the next release (or use `tf-nightly` in the meantime).  Thanks for the quick response."]}, {"number": 14219, "title": "tf.layers.Network is in the documentation, but not the library", "body": "**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OS X 10.12.6\r\n- **TensorFlow installed from (source or binary)**: source, latest 1.4 branch\r\n- **TensorFlow version (use command below)**: v1.4.0-rc1-12-gd752244fba 1.4.0\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**: 0.7.0\r\n- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.0.0 (clang-900.0.38)\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\nsee below\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\n\r\nThe 1.4 version of the documentation mentions tf.layers.Network in its examples of using the new function based layers (here is a link to the specific doc, https://www.tensorflow.org/api_docs/python/tf/layers/Input), so all you have to do is copy/paste the example code and run it.\r\n\r\n### Source code / logs\r\nthe most basic code to verify is\r\n\r\n```\r\nimport tensorflow as tf\r\nprint(tf.layers.Network)\r\n```\r\n\r\non my system this results in \r\n\r\n```\r\nAttributeError: module 'tensorflow.python.layers.layers' has no attribute 'Network'\r\n```", "comments": ["Thanks for pointing this out. This is indeed a bug, we decided to not export `tf.layers.Network` as a symbol for the 1.4 release, but forgot to hide `tf.layers.Input` as well.\r\n\r\nWe probably will introduce a `tf.layers.Network` by 1.5 or 1.6. ( @josh11b @fchollet ?)\r\n\r\nSorry for the confusion.\r\n", "Thanks for the quick reply. Not sure about it, but it seems that without the tf.layers.Network and tf.layers.Input, the rest of the function based layers have a rather limited use. Anyway, looking forward to the future releases.", "@MadWombat : re \"the rest of the function based layers have limited use\" - the function based layers are in fact pretty extensively used already (such as in the [models repository](https://github.com/tensorflow/models/tree/master/official)).\r\n\r\nThe existing object oriented layers provide a way to structure your models (similar to Keras) as objects, even without the `Network` class. In fact, we encourage that to make models run efficiently even when [eager execution is enabled](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/g3doc/guide.md#using-keras-and-the-layers-api)", "I guess I did not phrase it right. I meant the new style function based layers. The ones that return functions rather than ops `tf.layers.Dense` rather than `tf.layers.dense`. The models seem to be using the old style ones and without tf.layers.Network, I am not sure how useful the new style layers are.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Ping @josh11b @fchollet, see comment by @asimshankar on Nov 3, 2017.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "tf.layers.Dense is the name of a class (capitalized means class in Python conventions), and so calling tf.layers.Dense is creating an object, which is what Asim is referring to as the object-oriented layers API. I think there are already some examples using them, and more on the way, @asimshankar and @fchollet would know more.", "/CC @MarkDaoust ", "Hello,\r\n\r\nThe `Network` class has been removed (that's why it wasn't part of the public API). \r\n\r\nWe recommend you use the `Model` class: `tf.keras.Model`, which has the same functionality.\r\n\r\nYou can use the `Model` class with both layers from `tf.layers` and `tf.keras.layers`. `tf.layers` is a subset of `tf.keras.layers`. We recommend you use `tf.keras.layers`."]}, {"number": 14218, "title": "TensorFlow 1.4 (gpu Linux) Py36 not built with Py36", "body": "Installed TF 1.4 with pip install --ignore-installed --upgrade  https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.4.0-cp36-cp36m-linux_x86_64.whl\r\nGives me this error on import:\r\n/home/tom/anaconda3/envs/tf4/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)\r\nSimilar error reported form Tensorboard:\r\n/home/tom/anaconda3/envs/tf4/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)\r\nTensorBoard 0.4.0rc2 at http://tomServal:6006 (Press CTRL+C to quit)\r\n", "comments": ["the same", "Closing this as a duplicate of #14182. Please follow along on that thread!"]}, {"number": 14217, "title": "ios documentation for tensorflow-experimental", "body": "Could we have a proper documentation (either in the pod files with a custom header) or in the readme?\r\nIt would help greatly to have the list of available methods and what they do (unless method name is self-explanatory).\r\nHaving to dig into ~20 objective-c files to get a grasp of what is possible in swift is not equivalent to a proper documentation", "comments": ["@petewarden : Do we have something like this on the cards, or suggestions for contributions?", "It goes beyond that.\r\nAfter a close look i now see that your ios framework is in fact a c++ library just wrapped to be downloaded by cocoapods.\r\nSo it seems we have absolutly no way to use swift with tensorflow except if we wrappe everything ourselves", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 59 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 74 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 89 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 104 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing this as we'll be switching over to the TensorFlow Lite pod for iOS rather than investing in this original TensorFlow Mobile approach."]}, {"number": 14216, "title": "[CMake] Generate `gen_audio_ops.py`.", "body": "Fixes #14004.", "comments": []}, {"number": 14215, "title": " org.tensorflow.TensorFlowException: Op type not registered 'Sum' in binary running on localhost. Make sure the Op and Kernel are registered in the binary running in this process.", "body": " \r\n\r\n### System information\r\n- **Have I written custom code**:\r\nI worte a code that is based on this : https://www.tensorflow.org/api_guides/python/contrib.signal\r\ntrying to produce mfcc works fine in python but i cannot make it work in Android\r\n\r\n- **OS Platform and Distribution  Linux Ubuntu 16.04 \r\n- **TensorFlow installed from source  **:\r\n- **TensorFlow version 1.4.2:\r\n- **Python version - 2.7 \r\n- **Bazel version  0.7.0\r\n-\r\n \r\n\r\n### Describe the problem\r\ni have reduced my graph to this - \r\n****PYTHON CODE that creates the pb *****\r\nimport sugartensor as tf\r\nfrom tensorflow.python.framework import graph_util\r\nfrom model import *\r\nimport data\r\nimport os\r\nimport sys\r\nfrom tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio\r\nimport functools\r\n\r\n\r\nfrom tensorflow.contrib.session_bundle import exporter\r\nfrom tensorflow.python.tools import freeze_graph\r\nglobal session_config\r\nexportfilename = 'out2/export_mfcc_only.pb'\r\nwith tf.device('/cpu:0'):\r\n\tsession_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\r\n\t \r\n\ttf.app.flags.DEFINE_integer ('export_version',   1,           'version number of the exported model')\r\n\tsession = tf.InteractiveSession()\r\n\t#session = tf.Session()\r\n\t# Run inference\r\n\tbatch_size = 1\r\n\tvoca_size = data.voca_size\r\n\tsample_rate = 16000.0\t\r\n\twavdata = tf.placeholder(tf.float32, [ None], name=\"wav_float_input\")\r\n\tpcm = tf.expand_dims(wavdata, 0)\r\n\tstfts = tf.contrib.signal.stft(pcm, frame_length=2048, frame_step=512,\r\n                             fft_length=2048,window_fn=functools.partial(tf.contrib.signal.hann_window, periodic=False), pad_end=True)\r\n\tspectrograms = tf.abs(stfts)\r\n\t# Warp the linear scale spectrograms into the mel-scale.\r\n\tnum_spectrogram_bins = stfts.shape[-1].value\r\n\tlower_edge_hertz, upper_edge_hertz, num_mel_bins = 0.0,8000.0, 128\r\n\tlinear_to_mel_weight_matrix = tf.contrib.signal.linear_to_mel_weight_matrix(\r\n\tnum_mel_bins, num_spectrogram_bins, sample_rate, lower_edge_hertz,\r\n\tupper_edge_hertz)\r\n\tmel_spectrograms = tf.tensordot(\r\n\tspectrograms, linear_to_mel_weight_matrix, 1)\r\n\tmel_spectrograms.set_shape(spectrograms.shape[:-1].concatenate(\r\n\tlinear_to_mel_weight_matrix.shape[-1:]))\r\n\t# Compute a stabilized log to get log-magnitude mel-scale spectrograms.\r\n\tlog_mel_spectrograms = tf.log(mel_spectrograms + 1e-6)\r\n\t# Compute MFCCs from log_mel_spectrograms and take the first 13.\r\n\tmfccs = tf.contrib.signal.mfccs_from_log_mel_spectrograms(\r\n\tlog_mel_spectrograms)[..., :20]\r\n\r\n\tseq_len =[tf.size(mfccs.sg_int().sg_sum(axis=2), name=\"output_node\" )]\r\n\r\n  \r\n\t\r\n\r\n\tsession.run(tf.local_variables_initializer() )\r\n\ttf.global_variables_initializer()\r\n\tsubgraph = tf.graph_util.extract_sub_graph(session.graph_def, [\"output_node\"])\r\n\r\n\tfrozen_graph_def = graph_util.convert_variables_to_constants(\r\n\t      session, subgraph, [\"output_node\"])\r\n\ttf.train.write_graph(\r\n\t      frozen_graph_def,\r\n\t      os.path.dirname(exportfilename),\r\n\t      os.path.basename(exportfilename),\r\n\t      as_text=False)\r\n\ttf.logging.info('Saved frozen graph to %s',exportfilename)\r\n***END OF PYTHON CODE***\r\n \r\ni manage to load it and test the created pb it in python\r\n\r\ni create libtensorflow_inference.so\r\nusing print_selective_registration_header as described in - \r\ntensorflow/tensorflow/python/tools/print_selective_registration_header.py\r\n\r\ntesting the code in Androids results in an Error when i load the model - \r\n\r\n11-03 16:15:31.790 8639-8639/org.tensorflow.demo E/native: executor.cc:651 Executor failed to create kernel. Not found: No registered 'ListDiff' OpKernel for CPU devices compatible with node Tensordot/ListDiff_1 = ListDiff[T=DT_INT32, out_idx=DT_INT32, _device=\"/device:CPU:0\"](Tensordot/range_3, Tensordot/add_3)\r\n                                                           \t.  Registered:  <no registered kernels>\r\n                                                           \r\n                                                           \t [[Node: Tensordot/ListDiff_1 = ListDiff[T=DT_INT32, out_idx=DT_INT32, _device=\"/device:CPU:0\"](Tensordot/range_3, Tensordot/add_3)]]\r\n11-03 16:15:31.790 8639-8639/org.tensorflow.demo E/native: executor.cc:651 Executor failed to create kernel. Not found: No registered 'ListDiff' OpKernel for CPU devices compatible with node Tensordot/ListDiff_1 = ListDiff[T=DT_INT32, out_idx=DT_INT32, _device=\"/device:CPU:0\"](Tensordot/range_3, Tensordot/add_3)\r\n                                                           \t.  Registered:  <no registered kernels>\r\n                                                           \r\n                                                           \t [[Node: Tensordot/ListDiff_1 = ListDiff[T=DT_INT32, out_idx=DT_INT32, _device=\"/device:CPU:0\"](Tensordot/range_3, Tensordot/add_3)]]\r\n11-03 16:15:31.800 8639-8639/org.tensorflow.demo E/native: executor.cc:651 Executor failed to create kernel. Not found: No registered 'ListDiff' OpKernel for CPU devices compatible with node Tensordot/ListDiff_1 = ListDiff[T=DT_INT32, out_idx=DT_INT32, _device=\"/device:CPU:0\"](Tensordot/range_3, Tensordot/add_3)\r\n                                                           \t.  Registered:  <no registered kernels>\r\n                                                           \r\n                                                           \t [[Node: Tensordot/ListDiff_1 = ListDiff[T=DT_INT32, out_idx=DT_INT32, _device=\"/device:CPU:0\"](Tensordot/range_3, Tensordot/add_3)]]\r\n11-03 16:15:31.810 8639-8639/org.tensorflow.demo E/native: executor.cc:651 Executor failed to create kernel. Not found: No registered 'ListDiff' OpKernel for CPU devices compatible with node Tensordot/ListDiff = ListDiff[T=DT_INT32, out_idx=DT_INT32, _device=\"/device:CPU:0\"](Tensordot/range_2, Tensordot/add_1)\r\n                                                           \t.  Registered:  <no registered kernels>\r\n                                                           \r\n                                                           \t [[Node: Tensordot/ListDiff = ListDiff[T=DT_INT32, out_idx=DT_INT32, _device=\"/device:CPU:0\"](Tensordot/range_2, Tensordot/add_1)]]\r\n11-03 16:15:31.870 8639-8639/org.tensorflow.demo E/AndroidRuntime: FATAL EXCEPTION: main\r\n                                                                   Process: org.tensorflow.demo, PID: 8639\r\n                                                                   org.tensorflow.TensorFlowException: Op type not registered 'Sum' in binary running on localhost. Make sure the Op and Kernel are registered in the binary running in this process.\r\n\r\n\r\ni see that both 'ListDiff' and 'Sum' ops are in the ops_to_register.h file that is generated by the print_selective_registration_header process\r\n\r\nis there a workaround for this?\r\n\t ", "comments": ["I looked into it, and it seems that there is a problem with the bazel compilation\r\ni changed the models a few time and then did the print_selective_registration_header process,\r\neach time the ops_to_register.h file changed but the libtensorflow_inference.so does not.\r\nI tried all sort of bazel clean and bazel dump, and not chaned the resulted libtensorflow_inference.so\r\nso i think that either there is some issue with the bazel caching or that the bug is in the print_selective_registration_header process\r\n", "Hi, I met the same problem, do you sovle it at last?", "@mtu  -  eventually i had 2 ops_to_register.h files in my project and bazel always compiled libtensorflow_inference.so according to the wrong one...", "Hi @eli99999 i have generated ops_to_register.h by using tensorflow/python/tools/print_selective_registration_header.py. And then copy it into \"tensorflow/core/framework/\", and after bazel build the libtensorflow_inference.so also changed.\r\nBut my error log is still appear.\r\n\r\nerror log:\r\nE/native: executor.cc:643 Executor failed to create kernel. Not found: No registered 'ListDiff' OpKernel for CPU devices compatible with node encoder/input_projection/Tensordot/ListDiff = ListDiff[T=DT_INT32, out_idx=DT_INT32](encoder/input_projection/Tensordot/range, encoder/input_projection/Tensordot/add_1).  Registered:  <no registered kernels>\r\n          \t [[Node: encoder/input_projection/Tensordot/ListDiff = ListDiff[T=DT_INT32, out_idx=DT_INT32](encoder/input_projection/Tensordot/range, encoder/input_projection/Tensordot/add_1)]]\r\n\r\nsys:\r\ntf: 1.4.0  (I don't know where to get the 1.4.2 version)\r\nbazel: 0.8.0\r\n\r\nadd: and  i find my error at \"executor.cc:643\" and yours at \"executor.cc:651\". If it's convenient, please let me know how to solve this problem. \r\nand where did you move to with the ops_to_register.h file.\r\nand where is the \"listdiff\" bazel compiled.\r\nthanks", "I met the same issue. Does anyone solved it?", "I'm having a similar error"]}, {"number": 14214, "title": "do fine tuning with my own image size ", "body": "I would like to do transfer learning and fine tuning with some pretrained model in tensorflow.contrib.slim. I searched some examples about this. However, almost all of these examples will resize images to the specified size that the model needs. For example, 224\u00d7224 is the size that vgg16 needed. Will it be possible to set my own image size? Like 512\u00d7512. In keras, I can do this like following:\r\n\r\n    `base_model= VGG16(include_top=False, input_shape=(512, 512, 3))` \r\n\r\nUp till now, I have not found the solution, can this be down in tensorflow and slim?\r\n", "comments": ["That sounds like a question for stackoverflow; please close this issue here. Thanks!"]}, {"number": 14213, "title": "Feature tf.image : scale image and keep aspect/ratio", "body": "Hi, I'm working on a FCN (Fully Convolutional Network) and use Tensorflow for this work. I have a lot of images in input, and they have very different dimensions, like 2340x4160 or 512x512. So my input need to be dynamic. It's why I search a way to resize images for having the greater dimension to a specific value, with respect to the aspect/ratio.\r\n\r\nUnfortunatly, Tensorflow doesn't provide this feature or, if I'm wrong, please just ignore this issue.\r\n\r\nSo I write my own code. (It's probably not the most beautiful way to write this, sorry if this code seems ugly!)\r\n\r\n``` python3\r\n#!/usr/bin/env python\r\nimport tensorflow as tf\r\n\r\nMAX_SIZE = 512\r\n\r\n# Image (for example: 2340x4160)\r\nimage_path = \"your_image.jpg\"\r\n\r\n# Open image\r\nimage_string = tf.read_file(image_path)\r\nimage        = tf.image.decode_jpeg(image_string, channels=3)\r\n\r\n# Take width/height\r\ninitial_width = tf.shape(image)[0]\r\ninitial_height = tf.shape(image)[1]\r\n\r\n# Function for resizing \r\ndef _resize(x, y):\r\n  # Take the greater value, and use it for the ratio \r\n  max_ = tf.maximum(initial_width, initial_height)\r\n  ratio = tf.to_float(max_) / tf.constant(MAX_SIZE, dtype=tf.float32)\r\n\r\n  new_width = tf.to_float(initial_width) / ratio\r\n  new_height = tf.to_float(initial_height) / ratio\r\n\r\n  return tf.to_int32(new_width), tf.to_int32(new_height)\r\n\r\n# Useless function for the next condition\r\ndef _useless(x, y):\r\n  return x, y\r\n\r\nnew_w, new_h = tf.cond(tf.logical_or(\r\n                         tf.greater(initial_width, tf.constant(MAX_SIZE)),\r\n                         tf.greater(initial_height, tf.constant(MAX_SIZE))\r\n                       ),\r\n        lambda: _resize(initial_width, initial_height),\r\n        lambda: _useless(initial_width, initial_height))\r\n\r\nresized_image = tf.image.resize_images(image, [new_w, new_h])\r\nimage_int     = tf.cast(resized_image, tf.uint8)\r\nimage_enc     = tf.image.encode_jpeg(image_int)\r\nfwrite        = tf.write_file(\"my_resized_image.jpeg\", image_enc)\r\n\r\nsess = tf.Session()\r\nsess.run([fwrite])\r\n```\r\n\r\nI think this feature could be very useful when working with image input to have a function (here: **\"tf.image.resize_image_keep_aspect\"**) which allow us to do the operation more quickly (and I hope more efficiently!)\r\n\r\n```\r\n#!/usr/bin/env python\r\nimport tensorflow as tf\r\n\r\nMAX_SIZE = 512\r\n\r\n# Image (for example: 2340x4160)\r\nimage_path = \"your_image.jpg\"\r\n\r\n# Open image\r\nimage_string  = tf.read_file(image_path)\r\nimage         = tf.image.decode_jpeg(image_string, channels=3)\r\nresized_image = tf.image.resize_image_keep_aspect(image, MAX_SIZE)\r\nimage_enc     = tf.image.encode_jpeg(resized_image)\r\nfwrite        = tf.write_file(\"my_resized_image.jpeg\", image_enc)\r\n\r\nsess = tf.Session()\r\nsess.run([fwrite])\r\n```\r\n\r\nIf the feature already exist, sorry but I search on git/stackoverflow/google and didn't find any viable solution.\r\n\r\nIf you have any question or if I'm not clear, ask to me.\r\nHave a nice day :)\r\n", "comments": ["I think this issue might be similar to #4290?", "You are right. So the feature was already proposed. But still I think it's an easy and useful feature to add! But my bad, didn't see the #4290 issue.", "@Kayoku Maybe we could close this issue and continue discussion in #4290? That may also generate more attention I assume.\r\n", "Ok :)\r\nDone!", "Thanks @Kayoku. I may looking into this issue and hopefully provide a fix if I could find some free cycles. Though I don't have a concrete timeline yet so I could not commit to get it done. Maybe someone else may provide a fix before that."]}, {"number": 14212, "title": "Suggestion: Add Image Captioning Model Example for Android", "body": "This is a suggestion to add Android example for our deep-learning based tensorflow-android app which captions live camera frames in real-time.\r\nhttps://github.com/neural-nuts/Cam2Caption\r\n\r\nThis app uses pre-trained model generated using \r\nhttps://github.com/neural-nuts/image-caption-generator", "comments": ["If I understand correctly, you're asking us to merge your project into the TensorFlow codebase? Looking at the screenshots, what you built seems really cool. However merging it here might require a fair bit of build work, since .so and .jar files appear to be checked-in to that repo. While we appreciate contributions, we also love seeing TensorFlow benefiting the developers of other projects. I'm sure anyone searching will find, and benefit from, the work that you've shared."]}, {"number": 14211, "title": "SWIGing tensorflow/python/tensorflow.i failed (Exit 1) on Centos 6.2", "body": "Hi,\r\n\r\nTrying to build on Centos 6.2 with the following command:\r\n```\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --verbose_failures\r\n```\r\ngives the following error:\r\n```\r\nINFO: $TEST_TMPDIR defined: output root default is '/tmp'.\r\n........................\r\nWARNING: /home/username/src/tensorflow/tensorflow/core/BUILD:1787:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/username/src/tensorflow/tensorflow/tensorflow.bzl:1073:30.\r\nWARNING: /home/username/src/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/username/src/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nINFO: Found 1 target...\r\nERROR: /home/username/src/tensorflow/tensorflow/python/BUILD:2974:1: SWIGing tensorflow/python/tensorflow.i failed (Exit 1): swig failed: error executing command\r\n  (cd /tmp/_bazel_username/dcca333cc36b578f4473c754fbbc85ff/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n  bazel-out/host/bin/external/swig/swig -c++ -python -module pywrap_tensorflow_internal -o bazel-out/local-py3-opt/bin/tensorflow/python/pywrap_tensorflow_internal.cc -outdir bazel-out/local-py3-opt/bin/tensorflow/python -ltensorflow/python/client/device_lib.i -ltensorflow/python/client/events_writer.i -ltensorflow/python/client/tf_session.i -ltensorflow/python/client/tf_sessionrun_wrapper.i -ltensorflow/python/framework/cpp_shape_inference.i -ltensorflow/python/framework/python_op_gen.i -ltensorflow/python/grappler/cluster.i -ltensorflow/python/grappler/cost_analyzer.i -ltensorflow/python/grappler/item.i -ltensorflow/python/grappler/model_analyzer.i -ltensorflow/python/grappler/tf_optimizer.i -ltensorflow/python/lib/core/py_func.i -ltensorflow/python/lib/core/strings.i -ltensorflow/python/lib/io/file_io.i -ltensorflow/python/lib/io/py_record_reader.i -ltensorflow/python/lib/io/py_record_writer.i -ltensorflow/python/platform/base.i -ltensorflow/python/pywrap_tfe.i -ltensorflow/python/training/quantize_training.i -ltensorflow/python/training/server_lib.i -ltensorflow/python/util/kernel_registry.i -ltensorflow/python/util/port.i -ltensorflow/python/util/py_checkpoint_reader.i -ltensorflow/python/util/stat_summarizer.i -ltensorflow/python/util/tfprof.i -ltensorflow/python/util/transform_graph.i -ltensorflow/python/util/util.i -Ibazel-out/local-py3-opt/genfiles -Iexternal/eigen_archive -Iexternal/grpc -Iexternal/protobuf_archive -Iexternal/swig -Iexternal/boringssl -Ibazel-out/local-py3-opt/genfiles/external/local_config_python -Iexternal/nsync -Iexternal/gemmlowp -Iexternal/jpeg -Iexternal/com_googlesource_code_re2 -Iexternal/jsoncpp_git -Iexternal/zlib_archive -Iexternal/highwayhash -Iexternal/gif_archive -Ibazel-out/local-py3-opt/genfiles/external/jpeg -Iexternal/lmdb -Iexternal/png_archive -Iexternal/farmhash_archive -Iexternal/local_config_cuda -Iexternal/sqlite_archive -Iexternal/swig/Lib -Iexternal/swig/Lib/cffi -Iexternal/swig/Lib/python -Iexternal/swig/Lib/std -Iexternal/swig/Lib/typemaps tensorflow/python/tensorflow.i).\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n\r\nIs there any way to at least see what the error from swig is? Running the reported command manually works fine.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Why is this not considered a bug? I get the same issue on my CentOS 7.4 machine using TensorFlow 1.12.0 and Bazel 0.20.0:\r\n\r\n```\r\nERROR: /user_data/tensorflow-1.12.0/tensorflow/contrib/lite/python/interpreter_wrapper/BUILD:22:1: SWIGing tensorflow/contrib/lite/python/interpreter_wrapper/interpreter_wrapper.i failed (Exit 1): swig failed: error executing command  \r\n\r\n  (cd /user_data/.tmp/cache.srbo/bazel/_bazel_srbo/96431eb3bb8da25a1741c1c1ac91e19b/execroot/org_tensorflow && \\ \r\n\r\n  exec env - \\ \r\n\r\n  bazel-out/host/bin/external/swig/swig -c++ -python -module tensorflow_wrap_interpreter_wrapper -o bazel-out/k8-opt/bin/tensorflow/contrib/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.cc -outdir bazel-out/k8-opt/bin/tensorflow/contrib/lite/python/interpreter_wrapper -Iexternal/eigen_archive -Iexternal/swig -Ibazel-out/k8-opt/genfiles -Ibazel-out/k8-opt/genfiles/external/local_config_python -Iexternal/gemmlowp -Iexternal/flatbuffers -Iexternal/com_google_absl -Iexternal/arm_neon_2_x86_sse -Iexternal/farmhash_archive -Iexternal/swig/Lib -Iexternal/swig/Lib/cffi -Iexternal/swig/Lib/python -Iexternal/swig/Lib/std -Iexternal/swig/Lib/typemaps tensorflow/contrib/lite/python/interpreter_wrapper/interpreter_wrapper.i) \r\n\r\nExecution platform: @bazel_tools//platforms:host_platform \r\n\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build \r\n\r\nINFO: Elapsed time: 18.267s, Critical Path: 9.44s \r\n\r\nINFO: 93 processes: 93 local. \r\n\r\nFAILED: Build did NOT complete successfully \r\n```\r\n\r\nRunning the swig command manually works (0 return code) and does not give any output. Adding use_default_shell_env=True to the swig rule as per the comment here: https://github.com/bazelbuild/bazel/issues/4053#issuecomment-343134886 fixes the issue.\r\n\r\nThe command I'm running is:\r\n```\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --incompatible_remove_native_http_archive=false --distdir=distdir\r\n```", "@SimonBoorer Getting the same error.", "@angerson Can this please be reopened? This is a bug and a patch with a fix is available"]}, {"number": 14210, "title": "Fix typo : specified", "body": "typo fix\r\nspecfied -> specified", "comments": ["Can one of the admins verify this patch?", "Could you sign the Google CLA, then we can look into reviewing the change.", "i signed Google CLA :)\r\n"]}, {"number": 14209, "title": "Why eager concat  inputs?", "body": "I am using eager to implement a seq2seq model. \r\nHere is a [toy model](https://github.com/soenkyo/toy_seq2seq_4_debug):\r\nNo error arise in forward calculate, but in backprop, I get:\r\n\r\n`Traceback (most recent call last):\r\n  File \"/home/usr/eager/test.py\", line 18, in <module>\r\n    tf.app.run()\r\n  File \"/home/usr/miniconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/home/usr/eager/test.py\", line 14, in main\r\n    empirical_loss, gradients_and_variables = cal_gradient()\r\n  File \"/home/usr/miniconda2/lib/python2.7/site-packages/tensorflow/python/eager/backprop.py\", line 362, in grad_fn\r\n    sources)\r\n  File \"/home/usr/miniconda2/lib/python2.7/site-packages/tensorflow/python/eager/imperative_grad.py\", line 230, in imperative_grad\r\n    result.append(vspace.aggregate_fn(g))\r\n  File \"/home/usr/miniconda2/lib/python2.7/site-packages/tensorflow/python/eager/backprop.py\", line 716, in _aggregate_grads\r\n    for x in indexed_slices_list], 0)\r\n  File \"/home/usr/miniconda2/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1110, in concat\r\n    return gen_array_ops._concat_v2(values=values, axis=axis, name=name)\r\n  File \"/home/usr/miniconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 719, in _concat_v2\r\n    attrs=_attrs, ctx=_ctx, name=name)\r\n  File \"/home/usr/miniconda2/lib/python2.7/site-packages/tensorflow/python/eager/execute.py\", line 67, in execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"/home/usr/miniconda2/lib/python2.7/site-packages/six.py\", line 718, in raise_from\r\n    raise value\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [16,100] vs. shape[1] = [16,400] [Op:ConcatV2] name: concat\r\n`\r\n", "comments": ["Thanks for the report. Though, without seeing more of the code, it's hard to tell what the problem might be.\r\n\r\nWould it be possible to share detailed instructions on reproducing the problem? Ideally a small, self-contained snippet of code that I can run to reproduce the problem?\r\n\r\nThanks!\r\n", "@asimshankar  I updated a toy code [here](https://github.com/soenkyo/toy_seq2seq_4_debug)", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "hi. @soenkyo ,, \"InvalidArgumentError: ConcatOp : Dimensions of inputs should match\" , I meet the same problem, Did you have some solutions? thanks!!!", "@shantianyang Did you solve the problem? I also have the same problem.\r\n"]}, {"number": 14208, "title": "java.lang.UnsatisfiedLinkError: tensorflow_native_libraries-1509708652238-0/libtensorflow_jni.so: /lib64/libc.so.6: version `GLIBC_2.16' not found (required bytensorflow_native_libraries-1509708652238-0/libtensorflow_jni.so)", "body": "The exception is caught when I run Tensorflow API in java. There is no GLIBC_2.16. The lowest version is 2.2.5. what should I do? Could I package the .so file by myself?", "comments": ["Please do try to fill in all the information asked for the in issue template (https://github.com/tensorflow/tensorflow/issues/new), without that it is somewhat hard to help.\r\n\r\nIn this particular case, what platform are you running on?  \r\nSeems like you have a very old version of glibc (2.2.5 is from 2002 I believe - as per https://sourceware.org/glibc/wiki/Glibc%20Timeline)\r\n\r\nIf that is the case, and you're unable to use a more modern version of glibc, then you could try compiling from source - though I'm not sure that it will work.\r\n\r\nTo build from sources see https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java#building-from-source \r\n\r\nAnd once you have the native libraries (`libtensorflow_framework.so` and `libtensorflow_jni.so`) built, you could simply copy them to a directory an include that directory in the `java.library.path` property provided to the JVM.\r\n\r\nHope that helps.", "Thanks. How can I specific the GLIBC version when I compile from source?  Is the GLIBC version same as GCC ?", "Can you compile on the same machine you're deploying on (the one that has the old GLIBC version?).\r\n\r\nThat said, it is quite possible that TensorFlow will not compile with an older GLIBC version. Unfortunately, we don't have the bandwidth to support that, and will have to rely on community support.\r\n", "unfortunately, I can not compile on the machine which I don't have the authority...", "I have tried to compile on the machine with lower GLIBC version. Unfortunately, The compile tool (Bazel) need one higher GLIBC[bazel: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by bazel)]. Consequently, there is no way to run the native tensorflow program on machine with lower GLIBC version\uff08<2.14\uff09..."]}, {"number": 14207, "title": "Could not find any downloads that satisfy the requirement tensorflow", "body": "I cannot install Tensorflow using `pip install tensorflow`\r\n\r\n> Installing TensorFlow on Ubuntu\r\n> \r\n> This guide explains how to install TensorFlow on Ubuntu. These instructions might also work on other Linux variants, but we have only tested (and we only support) these instructions on **Ubuntu 14.04 or higher**.\r\n\r\n\r\n```\r\n>>> python -V\r\nPython 2.7.6\r\n\r\n>>> pip -V\r\npip 1.5.4 from /usr/lib/python2.7/dist-packages (python 2.7)\r\n\r\n>>> uname -a\r\nLinux 241f7b925fda 3.16.0-36-generic #48~14.04.1-Ubuntu SMP Wed Apr 15 13:11:28 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n>>> lsb_release -a\r\nNo LSB modules are available.\r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 14.04.2 LTS\r\nRelease:\t14.04\r\nCodename:\ttrusty\r\n\r\n>>> pip install tensorflow\r\nDownloading/unpacking tensorflow\r\n  Could not find any downloads that satisfy the requirement tensorflow\r\nCleaning up...\r\nNo distributions at all found for tensorflow\r\nStoring debug log for failure in /root/.pip/pip.log\r\n\r\n>>> tail -n 20 /root/.pip/pip.log\r\n    Skipping https://pypi.python.org/packages/fd/1a/7935eb82b9a9b89a3a8ef7e54f7d538698c85d248d8bedb533eab5afd293/tensorflow-1.0.1-cp36-cp36m-manylinux1_x86_64.whl#md5=61d704d2c563b2569524545f106f0a4a (from https://pypi.python.org/simple/tensorflow/) because it is not compatible with this Python\r\n    Skipping https://pypi.python.org/packages/fd/1a/b6e78223c8e05a8bdee8f9bb20d4926f81db50e583632a1cde6e5b5ec2f0/tensorflow-1.1.0-cp35-cp35m-manylinux1_x86_64.whl#md5=fc5ed08795ef5afa60b48ae916def79c (from https://pypi.python.org/simple/tensorflow/) because it is not compatible with this Python\r\n    Skipping https://pypi.python.org/packages/fd/cb/21c20d0597cbf67e952f590355b2cdb93544b699cd6ee46a4dc9069d037d/tensorflow-1.4.0rc0-cp33-cp33m-macosx_10_11_x86_64.whl#md5=05c4cfd97ed74ddd1753312aa5b19fca (from https://pypi.python.org/simple/tensorflow/) because it is not compatible with this Python\r\n    Skipping https://pypi.python.org/packages/fe/d0/49d9d8e6e781acde22b7bcbee693a1be86bcf8f6c21e915159ab61d12bd8/tensorflow-1.3.0-cp33-cp33m-macosx_10_11_x86_64.whl#md5=2855104686160b99a64910d33935d7d8 (from https://pypi.python.org/simple/tensorflow/) because it is not compatible with this Python\r\n    Skipping https://pypi.python.org/packages/fe/dd/8764ae59e8ff74421d615ddb9c86a1b404c27708dfde3caa8f17c183788d/tensorflow-1.3.0-cp27-cp27mu-manylinux1_x86_64.whl#md5=e82e309e6af0996f2083f59cf21d392c (from https://pypi.python.org/simple/tensorflow/) because it is not compatible with this Python\r\n  Could not find any downloads that satisfy the requirement tensorflow\r\nCleaning up...\r\n  Removing temporary dir /tmp/pip_build_root...\r\nNo distributions at all found for tensorflow\r\nException information:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python2.7/dist-packages/pip/basecommand.py\", line 122, in main\r\n    status = self.run(options, args)\r\n  File \"/usr/lib/python2.7/dist-packages/pip/commands/install.py\", line 278, in run\r\n    requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle)\r\n  File \"/usr/lib/python2.7/dist-packages/pip/req.py\", line 1178, in prepare_files\r\n    url = finder.find_requirement(req_to_install, upgrade=self.upgrade)\r\n  File \"/usr/lib/python2.7/dist-packages/pip/index.py\", line 277, in find_requirement\r\n    raise DistributionNotFound('No distributions at all found for %s' % req)\r\nDistributionNotFound: No distributions at all found for tensorflow\r\n```", "comments": ["> pip 1.5.4 from /usr/lib/python2.7/dist-packages (python 2.7)\r\n\r\nSee [installation guide](https://www.tensorflow.org/install/install_linux):\r\n\r\n>  We strongly recommend version 8.1 or higher of pip or pip3.", "Yes, that's it. Thank you."]}, {"number": 14206, "title": "R1.4", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->"]}]