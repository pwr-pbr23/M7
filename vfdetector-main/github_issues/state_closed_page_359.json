[{"number": 43294, "title": "How to evluate a Model accuracy", "body": "Since there is mentioned to evaluate [Model accuracy](https://www.tensorflow.org/lite/performance/post_training_quantization#model_accuracy) I wonder how to use it for a evaluation, please do you have an example (sample) how to evaluate it?\r\n\r\nThank you.", "comments": ["It seems to me a valid Documentation/Tutorial request.", "@peter197321,\r\nCould you please take a look at these evaluation examples and let us know if it helps. \r\n - [Object Detection evaluation](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks/coco_object_detection)\r\n- [Image Classification evaluation](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification)\r\n\r\nThanks!", "Is there anything to eval accuracy for Segmentation purposes?", "@amahendrakar Also what I meant is that users are pointed directly to https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks from the docs. \r\nI suppose that we need a main Readme in the root or a similar solution.", "that would be helpful ...", "Hey! I'd like to contribute to this. I'm gonna get started on creating a README.md for [`tools/evaluation/tasks/`](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks). I'm a little busy at the moment but I'll make sure to have a PR open by this weekend.", "@bhack should I create the `README.me` under `tools/evaulation` or `tools/evaluation/tasks/`?", "I think It Is ok", "> I think It Is ok\r\n\r\nSorry, what do you mean?\r\n\r\n", "> @bhack should I create the `README.me` under `tools/evaulation` or `tools/evaluation/tasks/`?\n\n", "Currently the documentation link is https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks so it could be nice to have a general `Readme.md` there", "@peter197321 [Tools for Evaluation](https://www.tensorflow.org/lite/performance/delegates#tools_for_evaluation) was added recently which provides a great explanation of the evaluation tools.", "Thank you @Harsh188 ", "@peter197321 if your questions are fulfilled could you go ahead and close this issue?", "Closing this issue in the light of Tools for Evaluation doc update. Feel free to reopen if necessary. Thanks!\r\n> @peter197321 [Tools for Evaluation](https://www.tensorflow.org/lite/performance/delegates#tools_for_evaluation) was added recently which provides a great explanation of the evaluation tools.\r\n\r\n"]}, {"number": 43293, "title": "Usage of intermediate layer causes an exception", "body": "**System information**\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\nOS Platform and Distribution: Windows 10\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version (use command below): 2.3\r\nPython version: 3.7.5\r\nCUDA/cuDNN version: 10.1\r\nGPU model and memory: RTX 2080 Ti\r\n\r\n**Describe the current behavior**\r\nIn toy example (provided below) there is an example of usage intermediate NN layer in loss function and in accuracy validation logic. On TF 2.3 it causes the following exeption:\r\n\r\n`Traceback (most recent call last): File \"C:/PyProjects/TF_Issue/main.py\", line 53, in <module> hits += tf.reduce_sum(tf.where((output - target_batch + x_slice) < 0.01, 1, 0)).numpy() AttributeError: 'Tensor' object has no attribute 'numpy'`\r\n\r\nI marked the line which causes the error. Without usage of intermediate layer it works.\r\nPlease, note these are two lines in the script below:\r\n\r\n ```\r\n hits += tf.reduce_sum(tf.where((output - target_batch + x_slice) < 0.01, 1, 0)).numpy() # this line causes the error\r\n  # hits += tf.reduce_sum(tf.where(output - target_batch < 0.01, 1, 0)).numpy() # if x_slice is omitted script works fine\r\n```\r\n\r\nCommenting-out the first one and uncomment the second to get the script working.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nbatch_size = 128\r\n\r\ninput = tf.keras.Input(shape=(None, 1))\r\nx = tf.keras.layers.Dense(1)(input)\r\noutput = tf.keras.layers.Dense(1)(x)\r\n\r\nmodel = tf.keras.Model(inputs=input, outputs=output)\r\n\r\n# A toy dataset of points around 3 * x + 2\r\nNUM_EXAMPLES = 2000\r\ninputs = tf.random.normal([NUM_EXAMPLES])\r\nnoise = tf.random.normal([NUM_EXAMPLES])\r\noutputs = inputs * 3 + 2 + noise\r\n\r\ntraining_inputs = tf.reshape(inputs[:1500], (1500, 1))\r\ntraining_outputs = tf.reshape(outputs[:1500], (1500, 1))\r\ntraining_inputs = tf.data.Dataset.from_tensor_slices(training_inputs).batch(batch_size)\r\ntraining_outputs = tf.data.Dataset.from_tensor_slices(training_outputs).batch(batch_size)\r\ntest_inputs = tf.reshape(inputs[1500:], (500, 1))\r\ntest_outputs = tf.reshape(outputs[1500:], (500, 1))\r\ntest_inputs = tf.data.Dataset.from_tensor_slices(test_inputs).batch(batch_size)\r\ntest_outputs = tf.data.Dataset.from_tensor_slices(test_outputs).batch(batch_size)\r\n\r\n\r\ndef loss(model, inputs, targets):\r\n  outputs = model(inputs)\r\n  output = outputs[:, 0]  # take the first output (in general model can have several outputs)\r\n  global x\r\n  x_slice = x[:, 0]\r\n  error = output - targets + x_slice\r\n  return tf.reduce_mean(tf.square(error))\r\n\r\n\r\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\r\nepoch = 3\r\nfor i in range(epoch):\r\n  for input_batch, target_batch in zip(training_inputs, training_outputs):\r\n    with tf.GradientTape() as tape:\r\n      loss_value = loss(model, input_batch, target_batch)\r\n      grads = tape.gradient(loss_value, model.trainable_variables)\r\n      optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n  print('epoch #:', i)\r\n\r\n\r\nhits = 0\r\ntotal = 0\r\nfor input_batch, target_batch in zip(test_inputs, test_outputs):\r\n  outputs = model(input_batch)\r\n  output = outputs[:, 0]  # take the first output (in general model can have several outputs)\r\n  x_slice = x[:, 0]\r\n  hits += tf.reduce_sum(tf.where((output - target_batch + x_slice) < 0.01, 1, 0)).numpy() # this line causes the error\r\n  # hits += tf.reduce_sum(tf.where(output - target_batch < 0.01, 1, 0)).numpy() # if x_slice is omitted script works fine\r\n  total += input_batch.shape[0]\r\n\r\nprint(hits)\r\nprint('Accuracy: ', hits/total)\r\n\r\n```\r\nAny ideas how to fix the issues?\r\nThanks in advance!!!\r\n", "comments": ["It is not a bug. It is by design and your effort is similar to:\r\nhttps://github.com/tensorflow/tensorflow/issues/37522#issuecomment-598648414\r\n\r\nCause also `tf.data.Dataset` runs in graph mode.\r\n", "@bhack thank you for the reply!\r\nyes, you are right! \r\n\r\nMore details (and fixed version of the example above) in comment in related issue here https://github.com/tensorflow/tensorflow/issues/41200#issuecomment-695779647", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43293\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43293\">No</a>\n"]}, {"number": 43292, "title": "Error while trying to save the decoder of the Image-Captioning model into TFLite file.", "body": "**System information**\r\n- Using Google Colaboratory\r\n\r\nI am trying to convert this Image-Captioning model into a TFLite file: https://www.tensorflow.org/tutorials/text/image_captioning\r\nFor this, I am trying to make two tensorflow lite files- one from the encoder and the other from the decoder objects after training them.\r\n\r\nThe encoder gets saved using `encoder.save(...)` and then it gets converted into the corresponding TFLite file using:\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(\"saved_encoder/mera_encoder\", signature_keys=None)\r\ntflite_model = converter.convert()\r\n```\r\nbut while saving the decoder using `decoder.save(...)` , I get an error saying : _\"TypeError: call() missing 2 required positional arguments: 'features' and 'hidden'\"_ \r\n\r\nWhat should I do?\r\n\r\nThe entire error call traceback: \r\n\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-37-d437f5400f07> in <module>()\r\n----> 1 decoder.save(\"saved_encoder/mera_decoder\")\r\n\r\n24 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in save(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\r\n   1977     \"\"\"\r\n   1978     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\r\n-> 1979                     signatures, options)\r\n   1980 \r\n   1981   def save_weights(self,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/save.py in save_model(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\r\n    132   else:\r\n    133     saved_model_save.save(model, filepath, overwrite, include_optimizer,\r\n--> 134                           signatures, options)\r\n    135 \r\n    136 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saved_model/save.py in save(model, filepath, overwrite, include_optimizer, signatures, options)\r\n     78     # we use the default replica context here.\r\n     79     with distribution_strategy_context._get_default_replica_context():  # pylint: disable=protected-access\r\n---> 80       save_lib.save(model, filepath, signatures, options)\r\n     81 \r\n     82   if not include_optimizer:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/save.py in save(obj, export_dir, signatures, options)\r\n    974 \r\n    975   _, exported_graph, object_saver, asset_info = _build_meta_graph(\r\n--> 976       obj, export_dir, signatures, options, meta_graph_def)\r\n    977   saved_model.saved_model_schema_version = constants.SAVED_MODEL_SCHEMA_VERSION\r\n    978 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/save.py in _build_meta_graph(obj, export_dir, signatures, options, meta_graph_def)\r\n   1045   if signatures is None:\r\n   1046     signatures = signature_serialization.find_function_to_export(\r\n-> 1047         checkpoint_graph_view)\r\n   1048 \r\n   1049   signatures, wrapped_functions = (\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/signature_serialization.py in find_function_to_export(saveable_view)\r\n     73   # If the user did not specify signatures, check the root object for a function\r\n     74   # that can be made into a signature.\r\n---> 75   functions = saveable_view.list_functions(saveable_view.root)\r\n     76   signature = functions.get(DEFAULT_SIGNATURE_ATTR, None)\r\n     77   if signature is not None:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/save.py in list_functions(self, obj, extra_functions)\r\n    143     if obj_functions is None:\r\n    144       obj_functions = obj._list_functions_for_serialization(  # pylint: disable=protected-access\r\n--> 145           self._serialization_cache)\r\n    146       self._functions[obj] = obj_functions\r\n    147     if extra_functions:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _list_functions_for_serialization(self, serialization_cache)\r\n   2588     self.predict_function = None\r\n   2589     functions = super(\r\n-> 2590         Model, self)._list_functions_for_serialization(serialization_cache)\r\n   2591     self.train_function = train_function\r\n   2592     self.test_function = test_function\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in _list_functions_for_serialization(self, serialization_cache)\r\n   3017   def _list_functions_for_serialization(self, serialization_cache):\r\n   3018     return (self._trackable_saved_model_saver\r\n-> 3019             .list_functions_for_serialization(serialization_cache))\r\n   3020 \r\n   3021   def __getstate__(self):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saved_model/base_serialization.py in list_functions_for_serialization(self, serialization_cache)\r\n     85         `ConcreteFunction`.\r\n     86     \"\"\"\r\n---> 87     fns = self.functions_to_serialize(serialization_cache)\r\n     88 \r\n     89     # The parent AutoTrackable class saves all user-defined tf.functions, and\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py in functions_to_serialize(self, serialization_cache)\r\n     77   def functions_to_serialize(self, serialization_cache):\r\n     78     return (self._get_serialized_attributes(\r\n---> 79         serialization_cache).functions_to_serialize)\r\n     80 \r\n     81   def _get_serialized_attributes(self, serialization_cache):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py in _get_serialized_attributes(self, serialization_cache)\r\n     93 \r\n     94     object_dict, function_dict = self._get_serialized_attributes_internal(\r\n---> 95         serialization_cache)\r\n     96 \r\n     97     serialized_attr.set_and_validate_objects(object_dict)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saved_model/model_serialization.py in _get_serialized_attributes_internal(self, serialization_cache)\r\n     49     # cache (i.e. this is the root level object).\r\n     50     if len(serialization_cache[constants.KERAS_CACHE_KEY]) == 1:\r\n---> 51       default_signature = save_impl.default_save_signature(self.obj)\r\n     52 \r\n     53     # Other than the default signature function, all other attributes match with\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saved_model/save_impl.py in default_save_signature(layer)\r\n    203   original_losses = _reset_layer_losses(layer)\r\n    204   fn = saving_utils.trace_model_call(layer)\r\n--> 205   fn.get_concrete_function()\r\n    206   _restore_layer_losses(original_losses)\r\n    207   return fn\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in get_concrete_function(self, *args, **kwargs)\r\n   1165       ValueError: if this object has not yet been called on concrete values.\r\n   1166     \"\"\"\r\n-> 1167     concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)\r\n   1168     concrete._garbage_collector.release()  # pylint: disable=protected-access\r\n   1169     return concrete\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _get_concrete_function_garbage_collected(self, *args, **kwargs)\r\n   1071       if self._stateful_fn is None:\r\n   1072         initializers = []\r\n-> 1073         self._initialize(args, kwargs, add_initializers_to=initializers)\r\n   1074         self._initialize_uninitialized_variables(initializers)\r\n   1075 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    695     self._concrete_stateful_fn = (\r\n    696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 697             *args, **kwds))\r\n    698 \r\n    699     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   2853       args, kwargs = None, None\r\n   2854     with self._lock:\r\n-> 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   2856     return graph_function\r\n   2857 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   3211 \r\n   3212       self._function_cache.missed.add(call_context_key)\r\n-> 3213       graph_function = self._create_graph_function(args, kwargs)\r\n   3214       self._function_cache.primary[cache_key] = graph_function\r\n   3215       return graph_function, args, kwargs\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   3073             arg_names=arg_names,\r\n   3074             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 3075             capture_by_value=self._capture_by_value),\r\n   3076         self._function_attributes,\r\n   3077         function_spec=self.function_spec,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    984         _, original_func = tf_decorator.unwrap(python_func)\r\n    985 \r\n--> 986       func_outputs = python_func(*func_args, **func_kwargs)\r\n    987 \r\n    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    598         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    599         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 600         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    601     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    602 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saving_utils.py in _wrapped_model(*args)\r\n    132     with base_layer_utils.call_context().enter(\r\n    133         model, inputs=inputs, build_graph=False, training=False, saving=True):\r\n--> 134       outputs = model(inputs, training=False)\r\n    135 \r\n    136     # Outputs always has to be a flat dict.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n    983 \r\n    984         with ops.enable_auto_cast_variables(self._compute_dtype_object):\r\n--> 985           outputs = call_fn(inputs, *args, **kwargs)\r\n    986 \r\n    987         if self._activity_regularizer:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    300   def wrapper(*args, **kwargs):\r\n    301     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\r\n--> 302       return func(*args, **kwargs)\r\n    303 \r\n    304   if inspect.isfunction(func) or inspect.ismethod(func):\r\n\r\nTypeError: call() missing 2 required positional arguments: 'features' and 'hidden'\r\n```\r\n", "comments": ["Since the decoder instance is a tf keras model instance, you can try tf.lite.TFLiteConverter.from_keras_model path instead.", "> Since the decoder instance is a tf keras model instance, you can try tf.lite.TFLiteConverter.from_keras_model path instead.\r\n\r\n@abattery I am able to store the TFLiteConverter object using this i.e. `converter=tf.lite.TFLiteConverter.from_keras_model(decoder)` produces no error. However, while converting the TFLiteConverter object using `tflite_model = converter.convert()` , the following error pops up:\r\n\r\n_ValueError: Model <main.RNN_Decoder object at 0x000002E2E2D94CD0> cannot be saved because the input shapes have not been set. Usually, input shapes are automatically determined from calling .fit() or .predict(). To manually set the shapes, call model.build(input_shape)._\r\n\r\nThe total error message:\r\n\r\n> Traceback (most recent call last):\r\n> File \"c:/...../Image-Captioning_using_Tensorflow.py\", line 271, in\r\n> tflite_model2 = converter2.convert()\r\n> File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 803, in convert\r\n> func = _saving_utils.trace_model_call(self._keras_model, input_signature)\r\n> File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saving_utils.py\", line 122, in trace_model_call\r\n> raise_model_input_error(model)\r\n> File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saving_utils.py\", line 93, in raise_model_input_error\r\n> raise ValueError(\r\n> ValueError: Model <main.RNN_Decoder object at 0x000002E2E2D94CD0> cannot be saved because the input shapes have not been set. Usually, input shapes are automatically determined from calling .fit() or .predict(). To manually set the shapes, call model.build(input_shape)", "I was also able to reproduce the _TypeError: call() missing 2 required positional arguments: 'features' and 'hidden'_ error seen on the decoder.save(fileNameStr) call in a Jupyter Notebook environment as well as a stand-alone python script. I can provide those tracebacks if it would be helpful in solving this issue. ", "@ravikyram @Saduf2019 @gowthamkpr\r\nI am able to create a TFLiteConvertor object using `converter=tf.lite.TFLiteConverter.from_keras_model(decoder)`. However, while converting the TFLiteConverter object using `tflite_model = converter.convert()` , the same error pops up:\r\n\r\n_TypeError: **call() missing 2 required positional arguments: 'features' and 'hidden'**._\r\n\r\nP.S.: The encoder gets converted into a TFLite file without error; doing the same things as above.\r\n\r\nWhat do you think might be causing this issue with the decoder.\r\n\r\nThe entire error message: \r\n\r\n> TypeError                                 Traceback (most recent call last)\r\n> <ipython-input-35-c6f70bd3ca2b> in <module>()\r\n> ----> 1 tflite_model2 = converter2.convert()\r\n>       2 open(\"decoder.tflite\", \"wb\").write(tflite_model2)\r\n> \r\n> 11 frames\r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in convert(self)\r\n>     802 \r\n>     803     func = _saving_utils.trace_model_call(self._keras_model, input_signature)\r\n> --> 804     concrete_func = func.get_concrete_function()\r\n>     805     self._funcs = [concrete_func]\r\n>     806 \r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in get_concrete_function(self, *args, **kwargs)\r\n>    1165       ValueError: if this object has not yet been called on concrete values.\r\n>    1166     \"\"\"\r\n> -> 1167     concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)\r\n>    1168     concrete._garbage_collector.release()  # pylint: disable=protected-access\r\n>    1169     return concrete\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _get_concrete_function_garbage_collected(self, *args, **kwargs)\r\n>    1071       if self._stateful_fn is None:\r\n>    1072         initializers = []\r\n> -> 1073         self._initialize(args, kwargs, add_initializers_to=initializers)\r\n>    1074         self._initialize_uninitialized_variables(initializers)\r\n>    1075 \r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n>     695     self._concrete_stateful_fn = (\r\n>     696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n> --> 697             *args, **kwds))\r\n>     698 \r\n>     699     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n>    2853       args, kwargs = None, None\r\n>    2854     with self._lock:\r\n> -> 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n>    2856     return graph_function\r\n>    2857 \r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n>    3211 \r\n>    3212       self._function_cache.missed.add(call_context_key)\r\n> -> 3213       graph_function = self._create_graph_function(args, kwargs)\r\n>    3214       self._function_cache.primary[cache_key] = graph_function\r\n>    3215       return graph_function, args, kwargs\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n>    3073             arg_names=arg_names,\r\n>    3074             override_flat_arg_shapes=override_flat_arg_shapes,\r\n> -> 3075             capture_by_value=self._capture_by_value),\r\n>    3076         self._function_attributes,\r\n>    3077         function_spec=self.function_spec,\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n>     984         _, original_func = tf_decorator.unwrap(python_func)\r\n>     985 \r\n> --> 986       func_outputs = python_func(*func_args, **func_kwargs)\r\n>     987 \r\n>     988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n>     598         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n>     599         # the function a weak reference to itself to avoid a reference cycle.\r\n> --> 600         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n>     601     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n>     602 \r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saving_utils.py in _wrapped_model(*args)\r\n>     132     with base_layer_utils.call_context().enter(\r\n>     133         model, inputs=inputs, build_graph=False, training=False, saving=True):\r\n> --> 134       outputs = model(inputs, training=False)\r\n>     135 \r\n>     136     # Outputs always has to be a flat dict.\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n>     983 \r\n>     984         with ops.enable_auto_cast_variables(self._compute_dtype_object):\r\n> --> 985           outputs = call_fn(inputs, *args, **kwargs)\r\n>     986 \r\n>     987         if self._activity_regularizer:\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n>     300   def wrapper(*args, **kwargs):\r\n>     301     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\r\n> --> 302       return func(*args, **kwargs)\r\n>     303 \r\n>     304   if inspect.isfunction(func) or inspect.ismethod(func):\r\n> \r\n> TypeError: call() missing 2 required positional arguments: 'features' and 'hidden'", "I have met the same problem these days, it seems that the decoder model depends on encoder model's output which are \"features\" and \"hidden\", still don't fix it as you."]}, {"number": 43291, "title": "Didnt find op for builtin opcode 'RESIZE_NEAREST_NEIGHBOR' version '3'", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung A51\r\n- TensorFlow installed from (source or binary): Maven \r\n- TensorFlow version (use command below): implementation('org.tensorflow:tensorflow-lite:2.3.0'){changing=true}\r\n- Python version: n/a\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**.tflite generated** using tf.lite.TFLiteConverter.from_saved_model\r\ntf_version = 2.3.0\r\nPython Implementation for Inference works without errors.\r\n\r\n**using the same model on Android for inference gives the error**- \r\ncant create interpreter : Didnt find op for builtin opcode 'RESIZE_NEAREST_NEIGHBOR' version '3'\r\nbuild-gradle : implementation('org.tensorflow:tensorflow-lite:2.3.0'){changing=true}\r\n\r\n**Describe the expected behavior**\r\nthe Android code should run.\r\n", "comments": ["@DeepakG19,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!", "> @DeepakG19,\r\n> In order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!\r\n\r\nHi, that wont be possible due to confidentiality clauses. But the issue being - same tflite works in python but not in Android.", "> @DeepakG19,\r\n> In order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!\r\n\r\nHi @amahendrakar, any updates?", "> @DeepakG19,\r\n> In order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!\r\n\r\nHi Any Updates?", "> Hi, that wont be possible due to confidentiality clauses. But the issue being - same tflite works in python but not in Android.\r\n\r\n@DeepakG19,\r\nWithout a reproducible code it would be difficult for us to pinpoint the issue. In this case, could you please provide a dummy model and reproducible code to mimic the error you are facing? Thanks!", "Hi @srjoglekar246, seems RESIZE_BILINEAR version 3 should be supported by TFLite 2.3.0. Can you take a look?", "This is weird. @DeepakG19 could you try the latest nightly for the aar and see if it works on Android?", "> This is weird. @DeepakG19 could you try the latest nightly for the aar and see if it works on Android?\r\n\r\nAble to resolve the error. The issue was a third-party module which implicitly had tf2.2.0 and was over-writing the dependecny added using the maven in main module. Turning this module off resolved the above error.\r\n**_### But, I am facing some new errors_** -\r\n\r\n2020-09-24 07:31:59.284 16761-16761/Processor: hws2:0 2  Input SHAPE-  0\r\n2020-09-24 07:31:59.285 16761-16761/Processor: mask:0 1 1 1 1 Input SHAPE-  1\r\n2020-09-24 07:31:59.285 16761-16761/Processor: image:0 1 1 1 3 Input SHAPE-  2\r\n2020-09-24 07:31:59.285 16761-16761/Processor: hws:0 2  Input SHAPE-  3\r\n\r\n2020-09-24 07:31:59.285 16761-16761/Processor: strided_slice_1:0 1 1 3  Output SHAPE-  0\r\n\r\nafter :\r\ntflite.resizeInput(1,dim);\r\ntflite.resizeInput(2,dim);\r\ntflite.allocateTensors();\r\n\r\n2020-09-24 07:31:59.286 16761-16761/Processor: hws2:0 2 Input SHAPE-  0\r\n2020-09-24 07:31:59.286 16761-16761/Processor: mask:0 1 256 256 1 Input SHAPE-  1\r\n2020-09-24 07:31:59.286 16761-16761/Processor: image:0 1 256 256 3 Input SHAPE-  2\r\n2020-09-24 07:31:59.286 16761-16761/Processor: hws:0 2 Input SHAPE-  3\r\n\r\n2020-09-24 07:31:59.286 16761-16761/Processor: strided_slice_1:0 1 1 3  Output SHAPE-  0\r\n\r\nNotice : Output Shape doesnt changes, which I assume is the correct behavior\r\n\r\nOn Running :\r\ntflite.runForMultipleInputsOutputs(inputs, outputs);\r\n\r\nThis error comes -\r\n\r\n2020-09-24 07:31:59.442 16761-16761/com.package.deepak A/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0 in tid 16761 (service.deepak), pid 16761 (service.deepak)\r\n\r\n2020-09-24 07:31:59.731 17121-17121/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\n2020-09-24 07:31:59.732 17121-17121/? A/DEBUG: Build fingerprint: Samsung-A50\r\n2020-09-24 07:31:59.732 17121-17121/? A/DEBUG: Revision: '2'\r\n2020-09-24 07:31:59.732 17121-17121/? A/DEBUG: ABI: 'arm64'\r\n2020-09-24 07:31:59.732 17121-17121/? A/DEBUG: pid: 16761, tid: 16761, name: service.deepak  >>> com.package.deepak <<<\r\n2020-09-24 07:31:59.732 17121-17121/? A/DEBUG: signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0\r\n2020-09-24 07:31:59.732 17121-17121/? A/DEBUG: Cause: null pointer dereference\r\n\r\n2020-09-24 07:41:50.415 18015-18015/? A/DEBUG: backtrace:\r\n2020-09-24 07:41:50.415 18015-18015/? A/DEBUG:     #00 pc 000000000001dd6c  /system/lib64/libc.so (memcpy+124)\r\n2020-09-24 07:41:50.415 18015-18015/? A/DEBUG:     #01 pc 0000000000133560  /data/app/com.package.deepak-fuEaz_w7MUZ9fy7vI4iAfA==/lib/arm64/libtensorflowlite_jni.so\r\n2020-09-24 07:41:50.415 18015-18015/? A/DEBUG:     #02 pc 00000000001331e8  /data/app/com.package.deepak-fuEaz_w7MUZ9fy7vI4iAfA==/lib/arm64/libtensorflowlite_jni.so\r\n2020-09-24 07:41:50.415 18015-18015/? A/DEBUG:     #03 pc 00000000001b269c  /data/app/com.package.deepak-fuEaz_w7MUZ9fy7vI4iAfA==/lib/arm64/libtensorflowlite_jni.so\r\n2020-09-24 07:41:50.415 18015-18015/? A/DEBUG:     #04 pc 00000000001b546c  /data/app/com.package.deepak-fuEaz_w7MUZ9fy7vI4iAfA==/lib/arm64/libtensorflowlite_jni.so\r\n2020-09-24 07:41:50.415 18015-18015/? A/DEBUG:     #05 pc 0000000000046738  /data/app/com.package.deepak-fuEaz_w7MUZ9fy7vI4iAfA==/lib/arm64/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+32)\r\n2020-09-24 07:41:50.415 18015-18015/? A/DEBUG:     #06 pc 0000000000563be0  /system/lib64/libart.so (art_quick_generic_jni_trampoline+144)\r\n2020-09-24 07:41:50.416 18015-18015/? A/DEBUG:     #07 pc 000000000055ae4c  /system/lib64/libart.so (art_quick_invoke_static_stub+604)\r\n2020-09-24 07:41:50.416 18015-18015/? A/DEBUG:     #08 pc 00000000000d04e8  /system/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+232)\r\n2020-09-24 07:41:50.416 18015-18015/? A/DEBUG:     #09 pc 00000000002838ac  /system/lib64/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+344)\r\n2020-09-24 07:41:50.416 18015-18015/? A/DEBUG:     #10 pc 000000000027d8b4  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+968)\r\n2020-09-24 07:41:50.416 18015-18015/? A/DEBUG:     #11 pc 000000000052b750  /system/lib64/libart.so (MterpInvokeStatic+204)\r\n2020-09-24 07:41:50.416 18015-18015/? A/DEBUG:     #12 pc 000000000054d394  /system/lib64/libart.so (ExecuteMterpImpl+14612)\r\n2020-09-24 07:41:50.416 18015-18015/? A/DEBUG:     #13 pc 000000000021fcf4  /dev/ashmem/dalvik-classes.dex extracted in memory from /data/app/com.package.deepak-fuEaz_w7MUZ9fy7vI4iAfA==/base.apk_17926_17926 (deleted) (org.tensorflow.lite.NativeInterpreterWrapper.run+156)\r\n2020-09-24 07:41:50.416 18015-18015/? A/DEBUG:     #14 pc 00000000002575b8  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.1037722801+488)\r\n2020-09-24 07:41:50.416 18015-18015/? A/DEBUG:     #15 pc 000000000025d0ac  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)\r\n2020-09-24 07:41:50.416 18015-18015/? A/DEBUG:     #16 pc 000000000027d898  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+940)\r\n2020-09-24 07:41:50.416 18015-18015/? A/DEBUG:     #17 pc 000000000052a24c  /system/lib64/libart.so (MterpInvokeVirtual+588)\r\n2020-09-24 07:41:50.416 18015-18015/? A/DEBUG:     #18 pc 000000000054d214  /system/lib64/libart.so (ExecuteMterpImpl+14228)\r\n2020-09-24 07:41:50.417 18015-18015/? A/DEBUG:     #19 pc 000000000021f2fe  /dev/ashmem/dalvik-classes.dex extracted in memory from /data/app/com.package.deepak-fuEaz_w7MUZ9fy7vI4iAfA==/base.apk_17926_17926 (deleted) (org.tensorflow.lite.Interpreter.**runForMultipleInputsOutputs**+10)\r\n2020-09-24 07:41:50.417 18015-18015/? A/DEBUG:     #20 pc 00000000002575b8  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.1037722801+488)\r\n2020-09-24 07:41:50.417 18015-18015/? A/DEBUG:     #21 pc 000000000051aae0  /system/lib64/libart.so (artQuickToInterpreterBridge+1020)\r\n2020-09-24 07:41:50.417 18015-18015/? A/DEBUG:     #22 pc 0000000000563cfc  /system/lib64/libart.so (art_quick_to_interpreter_bridge+92)\r\n2020-09-24 07:41:50.417 18015-18015/? A/DEBUG:     #23 pc 0000000000019ba4  /dev/ashmem/dalvik-jit-code-cache_17926_17926 (deleted) (com.package.deepak.Processor.process+12052)\r\n2020-09-24 07:41:50.417 18015-18015/? A/DEBUG:     #24 pc 000000000055aedc  /system/lib64/libart.so (art_quick_osr_stub+44)", "> **_### But, I am facing some new errors_** -\r\n\r\nIn this case, could you please close this issue and submit a new one from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!\r\n", "As suggested by @amahendrakar, this issue is closed and the new submitted issue can be followed up from [here](https://github.com/tensorflow/tensorflow/issues/43657)."]}, {"number": 43290, "title": "Various Google C++ style guide fixes", "body": "I want to contribute to this amazing project and picked up some low hanging fruit in the C++ section.\r\nI found some inconsistencies with the style guide on some header files, and therefore submitted this PR.", "comments": []}, {"number": 43289, "title": "ERROR: Could not find a version that satisfies the requirement tensorflow==1.14.0", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina version 10.15.5, Windows 10 64bit\r\n- TensorFlow installed from (source or binary): pip install tensorflow==1.14.0\r\n- TensorFlow version: 1.14.0\r\n- Python version: 3.7.* (64bit)\r\n- Installed using virtualenv? pip? conda?: pip and conda\r\n- pip version: I have used 19.1.1 and 20.2.3\r\n\r\n**Describe the problem**\r\nI have created virtual environments using Anaconda on both OS(mac and windows). Then I tried to install all the modules on that. but I have faced problems that install specific tensorflow version 1.14.0. My pip only supports those version below:\r\n(from versions: 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2, 2.3.0)\r\n\r\nI also tried to install tensorflow using commands below:\r\nconda install tensorflow==1.14.0\r\n\r\nbut it wasn't worked. my source can not import tensorflow module.\r\n\r\nhow can I install that version?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\npip install tensorflow==1.14.0\r\npip install tensorflow-gpu==1.14.0\r\n\r\n\r\n**Any other info / logs**\r\nabove commands always occurs this error below:\r\nERROR: Could not find a version that satisfies the requirement tensorflow==1.14.0 (from versions: 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2, 2.3.0)\r\nERROR: No matching distribution found for tensorflow==1.14.0", "comments": ["@ChrisDongWooKim \r\nCould you please refer to [this comment](https://github.com/tensorflow/tensorflow/issues/42724#issuecomment-683311743) and let us know.\r\nYou may also refer to similar issues:\r\n#34302 [upgrade pip] [link](https://stackoverflow.com/questions/63059979/cannot-install-tensorflow-1-x) [link](https://forum.rasa.com/t/could-not-find-a-version-that-satisfies-the-requirement-tensorflow-1-15-0/21509) [please verify these packages in case the error still persist]\r\n\r\n\r\nAlso is ther any particular reason to be using 1.14 as its an older version and later versions are available, would you want to try with tf 1.15 or 2.x and let us know.", "@Saduf2019 \r\nThx for answer :D\r\nI already tried those things before posting this question.\r\nI have used several pip versions to install TensorFlow 1.14.0 but it didn't work at all. I guess it isn't related to pip version.\r\nAnd also my pip only supports version 2.2.0 to 2.3.0.\r\n\r\nI should use a specific TensorFlow version. cuz I deployed this ML model using the newest TensorFlow version before but it didn't work as same as on tensorflow 1.14.0.", "@ChrisDongWooKim \r\nWe have support for tf 1.15 and 2.x,please upgrade and let us know if you still face any issues.\r\n", "@Saduf2019 \r\nVery thx to helping me out :)\r\n\r\nActually, I have already tried to install various versions of tf such as 1.1.3, 1.1.5, and everything on Mac OS but it didn't work at all.\r\nHowever, I solved the problem that I faced on Windows OS. I re-created my virtual environments based python version 3.7.* and then It worked to install tf 1.14.0 version. \r\nbut the thing is that My Mac doesn't support installing tf 1.14.0 even on the same python version.\r\nI guess I should use my windows computer instead of.", "@ChrisDongWooKim As mentioned above, by switching to python 3.7, tensorflow 1.14 installation was possible on Windows 10. \r\nNow that you are facing a different problem, please create a new issue and close this one as we can track these issues separately. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43289\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43289\">No</a>\n"]}, {"number": 43287, "title": "Fix sanity build", "body": "", "comments": []}, {"number": 43286, "title": "Disable test that fails due to breaking change in absl_py", "body": "", "comments": []}, {"number": 43285, "title": "Update common_win.bat", "body": "Pin numpy", "comments": []}, {"number": 43284, "title": "Fix build on r2.1 branch", "body": "Disable test that fails with newer absl_py syntax", "comments": []}, {"number": 43283, "title": "Fix missing comma", "body": "", "comments": []}, {"number": 43282, "title": "[INTEL MKL] MKL DNN 0.x code clean - input cuponversion op", "body": "DNN 0.x cleanup of MKL Input Conversion Op:\r\n\r\n(1) Remove all DNN 0.x related code \r\n\r\n(2) Replace all DNN 1.x macro usages", "comments": []}, {"number": 43281, "title": "Fixing build files for broken test/builds", "body": "Fixing build scripts..", "comments": []}, {"number": 43280, "title": "[INTEL MKL] MKL DNN 0.x code cleanup - Reshape op", "body": "DNN 0.x cleanup of MKL Reshape Op:\r\n\r\n(1) Remove all DNN 0.x related code\r\n(2) Replace all DNN 1.x macro usages", "comments": []}, {"number": 43279, "title": "[TFLite] Update the operators versions supported by the reference kernels in BuiltinRefOpResolver and add some missing operators", "body": "Hi,\r\n\r\nThis PR updates the operators versions supported by the reference kernels in `BuiltinRefOpResolver` as they were quite outdated compared to the versions that these reference kernels really support.\r\n\r\nI also added some missing operators that have an adequate reference kernel implementation (I took care to leave out the ones with only an optimized kernel implementation).\r\n\r\nI checked that these reference kernels supported these new versions of the operators, but it'd be good to double check.\r\n\r\nThibaut", "comments": ["@jdduke Is register_ref.cc our legacy things? does it need to be updated?", "It's fine (and good) to update until we're no longer using elsewhere."]}, {"number": 43278, "title": "My doc build is picking up methods not in the advertised API", "body": "## Description of issue (what needs changing):\r\n\r\nThis is a bit of a difficult issue to pin down. I have a class that extends `tf.Module`, and I'm trying to build the documentation for that class using sphinx (with a couple of extensions). When I build it, I'm seeing a large number of methods in my doc build that aren't advertised in the [`tf.Module` docs](https://www.tensorflow.org/api_docs/python/tf/Module). (These are mostly inherited from the superclasses of `tf.Module`: `Autotrackable` and `Trackable`, but that might not be too important here.)\r\n\r\nI'd like to be able to show those parts of the `tf.Module` API that are intended for use by client code (including those with leading underscores that are intended for use in subclasses, i.e. 'protected' methods), and to omit those that aren't. It's not clear to me what parts of these tensorflow classes are indeed part of the public/protected API. It's also not clear to me, if the public API only includes what is shown on the website, how I can omit the rest from my doc build.\r\n\r\nThe simplest suggestion for me would be for tensorflow to remove the docstrings for all fully private functionality, but that's a lot of work, and I assume you use those docstrings.\r\n\r\nYou may be wondering why I'm raising this as an issue with tensorflow. The reason is that with the current structure of docstrings in tensorflow, I'm finding it difficult to manage my downstream doc build. Does the tensorflow team have a clear and consistent way of delineating fully private functionality, and if not, is there anything that can be done to improve docstring usability in this regard?", "comments": ["> what parts of these tensorflow classes are indeed part of the public/protected API.\r\n\r\nThe rule is, if it's on tensorflow.org it's public. otherwise it's not.\r\n\r\nThe code that hides many of those methods is here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docs/generate2.py#L177"]}, {"number": 43277, "title": " Failed to run on the given Interpreter: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.     Node number 62011 (FlexSize) failed to prepare.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): pip install tf-nightly\r\n- TensorFlow version (or github SHA if from source):\r\n2.4.0-dev20200916\r\n\r\n**Provide the text output from tflite_convert**\r\nFor **Android Studio project** that gives the following error:\r\n\r\njava.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\n    Node number 62011 (FlexSize) failed to prepare.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:163)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:360)\r\n        at edu.ilab.covid_id.localize.tflite.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:202)\r\n        at edu.ilab.covid_id.ir.ConnectFlirActivity$7.run(ConnectFlirActivity.java:673)\r\n        at android.os.Handler.handleCallback(Handler.java:789)\r\n        at android.os.Handler.dispatchMessage(Handler.java:98)\r\n        at android.os.Looper.loop(Looper.java:164)\r\n        at android.os.HandlerThread.run(HandlerThread.java:65)\r\n```\r\n# Copy and paste here\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n\r\n**Here is the colab used to convert the saved model to tflite**\r\nhttps://colab.research.google.com/drive/1_9l_DcyNuVV1NxU3PsmSMjWeI9nTEwNS?usp=sharing\r\n\r\n**Here is the drive with the saved model to tflite**\r\nhttps://drive.google.com/drive/folders/1MCvDti2ygukqw6fGE18WpnFOSzR_AanF?usp=sharing\r\n\r\n**Here is the java code which uses the tflite model**\r\n[ConnectFlirActivity.zip](https://github.com/tensorflow/tensorflow/files/5234536/ConnectFlirActivity.zip)\r\n\r\n", "comments": ["I think the above android application does not have Select TF ops. You can follow the below guides to add the Select TF ops:\r\n\r\nhttps://www.tensorflow.org/lite/guide/ops_select#android_aar\r\nhttps://www.tensorflow.org/lite/guide/reduce_binary_size", "No, it has TF ops", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43277\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43277\">No</a>\n"]}, {"number": 43276, "title": "Model trains on GPU but not on TPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Mostly stock\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux (Colab)\r\n- TensorFlow installed from (source or binary): Pre-installed\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.x\r\n- Accelerator: TPU\r\n\r\n**Describe the current behavior**\r\nI am receiving this error:-\r\n```\r\nInvalidArgumentError: 9 root error(s) found.\r\n  (0) Invalid argument: {{function_node __inference_train_function_33888}} Compilation failure: Input to reshape is a tensor with 100 values, but the requested shape has 50\r\n\t [[{{node gradient_tape/sequential_4/embedding_4/embedding_lookup/Reshape_1}}]]\r\n\tTPU compilation failed\r\n\t [[tpu_compile_succeeded_assert/_11288723609953746016/_5]]\r\n\t [[tpu_compile_succeeded_assert/_11288723609953746016/_5/_233]]\r\n  (1) Invalid argument: {{function_node __inference_train_function_33888}} Compilation failure: Input to reshape is a tensor with 100 values, but the requested shape has 50\r\n\t [[{{node gradient_tape/sequential_4/embedding_4/embedding_lookup/Reshape_1}}]]\r\n\tTPU compilation failed\r\n\t [[tpu_compile_succeeded_assert/_11288723609953746016/_5]]\r\n\t [[tpu_compile_succeeded_assert/_11288723609953746016/_5/_209]]\r\n  (2) Invalid argument: {{function_node __inference_train_function_33888}} Compilation failure: Input to reshape is a tensor with 100 values, but the requested shape has 50\r\n\t [[{{node gradient_tape/sequential_4/embedding_4/embedding_lookup/Reshape_1}}]]\r\n\tTPU compilation failed\r\n\t [[tpu_compile_succeeded_assert/_11288723609953746016/_5]]\r\n\t [[tpu_compile_succeeded_assert/_11288723609953746016/_5/_173]]\r\n  (3) Invalid argument: {{function_node __inference_train_function_33888}} Compilation failure: Input to reshape is a tensor with 100 values, but the requested shape has 50\r\n\t [[{{node gradient_tape/sequential_4/embedding_4/embedding_lookup/Reshape_1}}]]\r\n\tTPU compilation failed\r\n\t [[tpu_compile_succeeded_assert/_11288723609953746016/_5]]\r\n\t [[tpu_compile_succeeded_assert/_11288723609953746016/_5/_185]]\r\n  (4) Invalid argument: {{function_node __inference_train_function_33888}} Compilation failure: Input to reshape is a tensor with 100 values, but the requested shape has 50\r\n\t [[{{node gradient_tape/sequential_4/embedding_4/embedding_lookup/Reshape_1}}]]\r\n\tTPU compilation failed\r\n\t [[tpu_compile_succeeded_assert/_11288723609953746016/_5]]\r\n\t [[tpu_compile_succeeded_assert/_11288723609953746016/_5/_197]]\r\n  (5) Invalid argument: {{function_node __inference_train_function_33888}} Compilation failure: Input to reshape is a tensor with 100 values, but the requested shape has 50\r\n\t [[{{node gradient_tape/sequential_4/embedding_4/embedding_lookup/Reshape_1}}]]\r\n\tTPU compilation failed\r\n\t [[tpu_compile_succeeded_assert/_11288723609953746016/_5]]\r\n\t [[tpu_compile_succeeded_assert/_11288723609953746016/_5/_245]]\r\n  (6) Invalid argument: {{function_node __inference_train_function_33888}} Compilation failure: Input to reshape is a tensor with 100 values, but the requested shape has 50\r\n\t [[{{node gradient_tape/sequential_4/embedding_4/embedding_lookup/Reshape_1}}]]\r\n\tTPU compilation failed\r\n\t [[tpu_compile_succeeded_assert/_11288723609953746016/_5]]\r\n\t [[tpu_compile_succeeded_assert/_11288723609953746016/_5/_257]]\r\n  (7) Invalid argument: {{function_node __inference_train_function_33888}} Compilation failure: Input to reshape is a tensor with 100 values, but the requested shape has 50\r\n\t [[{{node gradient_tape/sequential_4/embedding_4/embed ... [truncated]\r\n```\r\nWhich indicates that there is probably some error with the shapes of my model.\r\n\r\n**Describe the expected behavior**\r\nExpected the model to run, as it runs pretty well for considerable time on GPU but ALWAYS results in that error when using TPU. This means that the Model should train and run (since no error is received on GPU and does the training of the first Epoch)\r\n\r\n**Standalone code to reproduce the issue**\r\nHere is the `model.fit` block to initiate the training:-\r\n```\r\n# Directory where the checkpoints will be saved\r\ncheckpoint_dir = '/content/drive/My Drive/HashPro/checkpoints/'\r\n\r\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\r\n\r\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint(\r\n    filepath=checkpoint_prefix)\r\n\r\nEPOCHS = 50\r\n\r\nhistory = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback], steps_per_epoch=300)  # Comment to evaluate the model\r\n```\r\nHere is how my model looks like:-\r\n```\r\n\r\nModel: \"sequential_4\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nembedding_4 (Embedding)      (1, None, 8800)           158400    \r\n_________________________________________________________________\r\ndense_42 (Dense)             (1, None, 2500)           22002500  \r\n_________________________________________________________________\r\ndropout_23 (Dropout)         (1, None, 2500)           0         \r\n_________________________________________________________________\r\ndense_43 (Dense)             (1, None, 3500)           8753500   \r\n_________________________________________________________________\r\ndense_44 (Dense)             (1, None, 5500)           19255500  \r\n_________________________________________________________________\r\ndropout_24 (Dropout)         (1, None, 5500)           0         \r\n_________________________________________________________________\r\ndense_45 (Dense)             (1, None, 7500)           41257500  \r\n_________________________________________________________________\r\ndense_46 (Dense)             (1, None, 9500)           71259500  \r\n_________________________________________________________________\r\nbatch_normalization_8 (Batch (1, None, 9500)           38000     \r\n_________________________________________________________________\r\ndropout_25 (Dropout)         (1, None, 9500)           0         \r\n_________________________________________________________________\r\ndense_47 (Dense)             (1, None, 7000)           66507000  \r\n_________________________________________________________________\r\ndense_48 (Dense)             (1, None, 7000)           49007000  \r\n_________________________________________________________________\r\ndropout_26 (Dropout)         (1, None, 7000)           0         \r\n_________________________________________________________________\r\ndense_49 (Dense)             (1, None, 1500)           10501500  \r\n_________________________________________________________________\r\ndense_50 (Dense)             (1, None, 500)            750500    \r\n_________________________________________________________________\r\ndropout_27 (Dropout)         (1, None, 500)            0         \r\n_________________________________________________________________\r\nactivation_4 (Activation)    (1, None, 500)            0         \r\n_________________________________________________________________\r\nbatch_normalization_9 (Batch (1, None, 500)            2000      \r\n=================================================================\r\nTotal params: 289,492,900\r\nTrainable params: 289,472,900\r\nNon-trainable params: 20,000\r\n\r\n```\r\nModel block for building and applying TPU strategy to run it:-\r\n```\r\ndef build_model(vocab_size, embedding_dim, mid_units, batch_size):\r\n  model = tf.keras.Sequential([tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\r\n  tf.keras.layers.Dense(2500, activation='relu'),\r\n  tf.keras.layers.Dropout(0.15),\r\n  tf.keras.layers.Dense(3500, activation='relu'),\r\n  tf.keras.layers.Dense(5500, activation='relu'),\r\n  tf.keras.layers.Dropout(0.15),\r\n\r\n  tf.keras.layers.Dense(7500, activation='relu'),\r\n  tf.keras.layers.Dense(9500, activation='relu'),\r\n  tf.keras.layers.BatchNormalization(),\r\n  tf.keras.layers.Dropout(0.15),\r\n  tf.keras.layers.Dense(mid_units, activation='relu'),\r\n  tf.keras.layers.Dense(mid_units, activation='relu'),\r\n  tf.keras.layers.Dropout(0.15),\r\n\r\n  tf.keras.layers.Dense(1500, activation='relu'),\r\n  tf.keras.layers.Dense(500, activation='relu'),\r\n  tf.keras.layers.Dropout(0.15),\r\n  \r\n  tf.keras.layers.Activation('softmax'),\r\n  tf.keras.layers.BatchNormalization()\r\n])\r\n  return model\r\n\r\ndef loss(labels, logits):\r\n  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\r\n\r\nwith strategy.scope():\r\n  model = build_model(\r\n    vocab_size = len(vocab),\r\n    embedding_dim=embedding_dim,\r\n    mid_units=7000,\r\n    batch_size=BATCH_SIZE)\r\n\r\n  model.compile(optimizer='Adam', loss=loss)\r\n\r\n#Let's see the model's organs!\r\nmodel.summary()\r\n```\r\nTPU strategy and initialization code taken from the TensorFlow website and does not produce any error at all.\r\n> It seems to me that there might be a bug here, since if the model has some shape-related issue, then training on GPU would not have worked at all. But since it works, there might be a problem in that way I am using the TPU\r\n\r\nIf you want any more info. please comment below", "comments": ["@neel04 \r\n\r\nRequest you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "@ravikyram I have attached the `py` file for all the code, however, the dataset can't be provided by me. You can open the file in Colab and inspect it with a random English dataset. (Or perhaps just make a file with a couple of words and it would do good enough for reproducibility)\r\n[code.zip](https://github.com/tensorflow/tensorflow/files/5240535/code.zip)\r\n\r\n", "@gowthamkpr Any help please? I have to finish this project quick...", "@neel04 Are you using `tf.data.Dataset` for input data? Can you try adding `drop_remainder=True` when batching it?", "@yixingfu I am changing my Input pipeline at the moment :) So will try with that. In the new one, I am indeed using `drop_remainder`. Lets see how that turns out\r\n", "Hi @neel04 please provide a simple example dataset that reproduces the error. That will greatly help us to identify what is causing this issue. Thanks!", "Tried by feeding numpy arrays directly in the model, but am getting this error:-\r\n```\r\n\r\nValueError: in user code:\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\r\n        return step_function(self, iterator)\r\n    <ipython-input-10-cab6225c487c>:33 loss  *\r\n        return tf.keras.losses.categorical_crossentropy(label, logits, from_logits=False)    #removed sparse\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper  **\r\n        return target(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:1535 categorical_crossentropy\r\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\r\n        return target(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4687 categorical_crossentropy\r\n        target.shape.assert_is_compatible_with(output.shape)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_shape.py:1134 assert_is_compatible_with\r\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\r\n\r\n    ValueError: Shapes (None, 1) and (None, 40, 500) are incompatible\r\n\r\n\r\n```\r\nAnyone know how to fix this?\r\n", "Hi @neel04 it's difficult to diagnose this issue just from the stack trace you provided above. Please provide a simple example dataset + code that reproduces the error and we can better help to troubleshoot.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43276\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43276\">No</a>\n", "I have this same issue and have tried a lot of things to fix it including manually reshaping the dataset, adding drop_remainder. Getting TPU to work has been the biggest headache.", "> \r\n> \r\n> I have this same issue and have tried a lot of things to fix it including manually reshaping the dataset, adding drop_remainder. Getting TPU to work has been the biggest headache.\r\n\r\nWhere did you add the drop remainder? I am using two Input layers to concatenate two Models and it runs with GPU but not with TPU.\r\n"]}, {"number": 43275, "title": "TensorFlow Lite (2.3.0) Converter Quantization Incompatibility for Coral Edge TPU Compiler", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 20.04 LTS with Docker\r\n- TensorFlow installed from:  pulled TensorFlow Docker image (version 19.03.12)\r\n- TensorFlow version (or github SHA if from source): v2.3.0 CPU\r\n- Coral Edge TPU Compiler: version 14.1.317412892\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport tensorflow as tf \r\nimport numpy as np\r\nfrom tensorflow import keras\r\n\r\nkeras_model = tf.keras.models.load_model(\"keras_model.h5\")\r\nkeras_model.input.set_shape((1,)+keras_model.input.shape[1:])\r\nkeras_model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\r\n#input_arrays=[\"x\"], output_arrays=[\"Identity\"],input_shapes={'x':[1,32,32,1]})\r\n\r\n\r\ndef representative_dataset_gen():\r\n  for _ in range(10):\r\n    input_array = np.random.random((1,32,32,1))\r\n    input_array = np.array(input_array,dtype=np.float32)\r\n    yield [input_array]\r\n\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8  # or tf.uint8\r\nconverter.inference_output_type = tf.uint8  # or tf.uint8\r\ntflite_model = converter.convert()\r\n\r\nwith open('keras_model_2-3-0.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n```\r\nNote: I am using garbage random data to do the post-training quantization.\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n2020-09-16 18:08:33.821326: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2020-09-16 18:08:33.825281: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2020-09-16 18:08:38.269950: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2020-09-16 18:08:38.270738: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\r\n2020-09-16 18:08:38.271532: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (e386413cb5cf): /proc/driver/nvidia/version does not exist\r\n2020-09-16 18:08:38.278651: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-09-16 18:08:38.337624: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 1799995000 Hz\r\n2020-09-16 18:08:38.345643: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4714550 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-09-16 18:08:38.345866: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nWARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\n2020-09-16 18:09:01.424650: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\n2020-09-16 18:09:28.840811: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2020-09-16 18:09:28.847221: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-09-16 18:09:28.910112: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize\r\n2020-09-16 18:09:28.910190: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 1.162ms.\r\n2020-09-16 18:09:28.910203: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.004ms.\r\n2020-09-16 18:09:31.373431: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.\r\n2020-09-16 18:09:31.373507: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.\r\n\r\nAttached is the Keras model and tflite model below:\r\n[models.zip](https://github.com/tensorflow/tensorflow/files/5234095/models.zip)\r\n\r\n```\r\n\r\n**Failure details**\r\nI am attempting to use this post-training quantized TensorFlow Lite model for input to the Coral Edge TPU compiler such that it can run on the Edge TPU.  The Edge TPU compiler fails to compile the quantized TensorFlow Lite model.  I reached to the Coral Edge TPU team to examine why the the TensorFlow Lite model will not compile.  According to their team, \"The compiler rejects the model on purpose because there are some mis matching in quantization parameter which could cause bad prediction:\r\n\r\n![image](https://user-images.githubusercontent.com/19961323/93378124-fa01f000-f829-11ea-8fd8-2a88204dd4ac.png)\r\n\r\nThe are 2 quantized op going to that same Concat layer where one has this:\r\n`scale: 0.048531219363212585 zero_point: 103 num_fxp_values: 256`\r\nand the other one:\r\n\r\n`scale: 0.033884394913911819 zero_point: 120 num_fxp_values: 256`\r\nThis seems to me like a conversion issue, I suggest reaching out to the tensorflow team for a more appropriate solution.\"\r\n\r\nWhy are there 2 quantized operations with different quantization parameters going to the same concatenation layer?  Is this an error with the TensorFlow Lite converter or an error with the Edge TPU compiler?\r\n\r\n", "comments": ["@goodwilj,\r\nIn order to expedite the trouble-shooting process, could you please provide the code to generate the `keras_model.h5` file as well as the model (i.e. the `keras_model.h5` file) itself. Thanks!", "Thanks for the reply! Attached below is the code to generate the keras_model.h5 and the model itself.\r\n\r\n[fc_densenet.zip](https://github.com/tensorflow/tensorflow/files/5239226/fc_densenet.zip)\r\n", "Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/e8d52cf7d93a47563b8f604eaa9ff446/43275.ipynb). Thanks!", "Thanks! @amahendrakar ", "Reassigned to MOT team.", "Help.\r\nThe exactly same problem occured to me. And I have tried many versions to avoid it ,but it did not worked.\r\nHow could I solve the problem as soon ? Thanks", "@goodwilj I treid to reproduce the issue but facing different error in TF 2.5. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/837dde655958409925695c086878f8f8/43275.ipynb#scrollTo=l7rcScsd21FX).Thanks!", "@saikumarchalla This actually appears to be the exact same error as detailed in the original post, so I believe you have reproduced it.  According to the Edge TPU team, the crux of the problem is that there are two quantized operations with different scales/zero-points going to the same concatenation layer:\r\n\r\n\"The are 2 quantized op going to that same Concat layer where one has this:\r\n`scale: 0.048531219363212585 zero_point: 103 num_fxp_values: 256`\r\nand the other one:\r\n`scale: 0.033884394913911819 zero_point: 120 num_fxp_values: 256` \"\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hi @goodwilj ! This issue is getting resolved in the [2.8](https://colab.sandbox.google.com/gist/mohantym/5579c86990354a36bc2a52547dbc5ff6/untitled67.ipynb) version . Can we move this issue to closed status now?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@mohantym Sounds good! Thank you for the update. We can move this to closed status.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43275\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43275\">No</a>\n"]}, {"number": 43274, "title": "[INTEL MKL] MKL DNN 0.x code cleanup - RequantizePerChannel op", "body": "DNN 0.x cleanup of MKL RequantizePerChannel Op:\r\n\r\n(1) Remove all DNN 0.x related code\r\n\r\n(2) Replace all DNN 1.x macro usages", "comments": []}, {"number": 43273, "title": "FP16 not working with NHNet", "body": "I am able to run NHNet code with batch size 8 but the accuracy numbers are no way near to what are reported in the paper. So, I thought increasing the batch size might help. But due to GPU limitation, I am not able to do it. So I tried using Mixed Precision to train the model for larger batch. I tried the following 2 things:\r\n\r\n1.  Using `tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt,\"dynamic\")`\r\nBut this gives the following error.\r\n`AttributeError: 'LossScaleOptimizer' object has no attribute '_hypers_created'`\r\n\r\n2.I also tried using:  `tf.keras.mixed_precision.experimental.set_policy('mixed_float16')`\r\nBut that gives this error:\r\n`TypeError: Tensors in list passed to 'inputs' of 'Einsum' Op have types [float16, float32] that don't all match.`\r\n\r\n**System information**\r\n- Used exact same code from NHNet repo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\n- TensorFlow installed from (source or binary): tf-nightly==2.4.0.dev20200724\r\n- TensorFlow version (use command below): v1.12.1-31004-g203aa8b634 2.2.0-dev20200501\r\n- Python version: 3.7.7\r\n- CUDA/cuDNN version: 10.1/7\r\n- GPU model and memory: TITAN RTX 24GB\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nAs soon as epoch 1 starts the code throws error as mentioned before.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe training should happen.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nExact same code mentioned here: https://github.com/tensorflow/models/blob/master/official/nlp/nhnet/trainer.py\r\n\r\nJust add one line after line no 146:\r\n`opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt,\"dynamic\")`\r\n", "comments": ["You need some adaptation also for the train_step. See:\r\nhttps://www.tensorflow.org/guide/mixed_precision?hl=en#training_the_model_with_a_custom_training_loop", "The code actually uses model.fit and hence I didn't change anything. But I changed it now, but still its not helping.\r\n\r\nMy code looks like this:\r\n\r\n```\r\nclass Trainer(tf.keras.Model):\r\n  \"\"\"A training only model.\"\"\"\r\n\r\n  def __init__(self, model, params):\r\n    super(Trainer, self).__init__()\r\n    self.model = model\r\n    self.params = params\r\n    self._num_replicas_in_sync = tf.distribute.get_strategy(\r\n    ).num_replicas_in_sync\r\n\r\n  def call(self, inputs, mode=\"train\"):\r\n    return self.model(inputs, mode)\r\n\r\n  def train_step(self, inputs):\r\n    \"\"\"The logic for one training step.\"\"\"\r\n    with tf.GradientTape() as tape:\r\n      logits, _, _ = self(inputs, mode=\"train\", training=True)\r\n      targets = models.remove_sos_from_seq(inputs[\"target_ids\"],\r\n                                           self.params.pad_token_id)\r\n      loss = transformer_metrics.transformer_loss(logits, targets,\r\n                                                  self.params.label_smoothing,\r\n                                                  self.params.vocab_size)\r\n      # Scales the loss, which results in using the average loss across all\r\n      # of the replicas for backprop.\r\n      scaled_loss = self.optimizer.get_scaled_loss(loss) / self._num_replicas_in_sync\r\n\r\n    tvars = self.trainable_variables\r\n    grads = self.optimizer.get_unscaled_gradients(tape.gradient(scaled_loss, tvars))\r\n    self.optimizer.apply_gradients(list(zip(grads, tvars)))\r\n    return {\r\n        \"training_loss\": loss,\r\n        \"learning_rate\": self.optimizer._decayed_lr(var_dtype=tf.float32)\r\n    }\r\n\r\n\r\ndef train(params, strategy, dataset=None):\r\n  \"\"\"Runs training.\"\"\"\r\n\r\n  if not dataset:\r\n    dataset = input_pipeline.get_input_dataset(\r\n        FLAGS.train_file_pattern,\r\n        FLAGS.train_batch_size,\r\n        params,\r\n        is_training=True,\r\n        strategy=strategy)\r\n\r\n  with strategy.scope():\r\n    model = models.create_model(\r\n        FLAGS.model_type, params, init_checkpoint=FLAGS.init_checkpoint)\r\n    opt = tf.keras.optimizers.Adam(learning_rate=params.learning_rate)\r\n    opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt,\"dynamic\")\r\n    trainer = Trainer(model, params)\r\n    model.global_step = opt.iterations\r\n\r\n    trainer.compile(\r\n        optimizer=opt,\r\n        experimental_steps_per_execution=FLAGS.steps_per_loop)\r\n    summary_dir = os.path.join(FLAGS.model_dir, \"summaries\")\r\n    summary_callback = tf.keras.callbacks.TensorBoard(\r\n        summary_dir, update_freq=max(100, FLAGS.steps_per_loop))\r\n    checkpoint = tf.train.Checkpoint(model=model, optimizer=opt)\r\n    checkpoint_manager = tf.train.CheckpointManager(\r\n        checkpoint,\r\n        directory=FLAGS.model_dir,\r\n        max_to_keep=10,\r\n        step_counter=model.global_step,\r\n        checkpoint_interval=FLAGS.checkpoint_interval)\r\n    if checkpoint_manager.restore_or_initialize():\r\n      logging.info(\"Training restored from the checkpoints in: %s\",\r\n                   FLAGS.model_dir)\r\n    checkpoint_callback = keras_utils.SimpleCheckpoint(checkpoint_manager)\r\n\r\n  # Trains the model.\r\n  steps_per_epoch = min(FLAGS.train_steps, FLAGS.checkpoint_interval)\r\n  epochs = FLAGS.train_steps // steps_per_epoch\r\n  history = trainer.fit(\r\n      x=dataset,\r\n      steps_per_epoch=steps_per_epoch,\r\n      epochs=epochs,\r\n      callbacks=[summary_callback, checkpoint_callback],\r\n      verbose=2)\r\n  train_hist = history.history\r\n  # Gets final loss from training.\r\n  stats = dict(training_loss=float(train_hist[\"training_loss\"][-1]))\r\n  return stats\r\n```\r\n\r\nThis still gives the same error: ` AttributeError: 'LossScaleOptimizer' object has no attribute '_hypers_created`.\r\n\r\nIf I remove \"learning_rate\": self.optimizer._decayed_lr(var_dtype=tf.float32) in the return, it throws this error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"trainer.py\", line 237, in <module>\r\n    app.run(main)\r\n  File \"/home/rishabh/.local/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/rishabh/.local/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"trainer.py\", line 231, in main\r\n    stats = run()\r\n  File \"trainer.py\", line 210, in run\r\n    stats = train(params, strategy)\r\n  File \"trainer.py\", line 176, in train\r\n    verbose=2)\r\n  File \"/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 72, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 907, in fit\r\n    tmp_logs = train_function(iterator)\r\n  File \"/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 766, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 809, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 688, in _initialize\r\n    *args, **kwds))\r\n  File \"/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2902, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3232, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3121, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 981, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 595, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 964, in wrapper\r\n    user_requested=True,\r\n  File \"/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 585, in converted_call\r\n    return _fall_back_unconverted(f, args, kwargs, options, e)\r\n  File \"/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 393, in _fall_back_unconverted\r\n    return _call_unconverted(f, args, kwargs, options)\r\n  File \"/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 343, in _call_unconverted\r\n    return f(*args, **kwargs)\r\n  File \"/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 627, in train_function\r\n    for _ in math_ops.range(self._steps_per_execution - 1):\r\n  File \"/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 552, in __iter__\r\n    self._disallow_iteration()\r\n  File \"/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 545, in _disallow_iteration\r\n    self._disallow_when_autograph_enabled(\"iterating over `tf.Tensor`\")\r\n  File \"/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 523, in _disallow_when_autograph_enabled\r\n    \" decorating it directly with @tf.function.\".format(task))\r\ntensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.\r\n```\r\n\r\n\r\nAlso beyond this, I don't think FP16 will work directly as the NHNet model has many custom layers which are not getting converted. When I run the code I get following warnings:\r\n\r\n```\r\nW0917 14:16:15.056954 140018629465920 ag_logging.py:146] AutoGraph could not transform <bound method TransformerDecoder.call of <official.nlp.nhnet.decoder.TransformerDecoder object at 0x7f573c702350>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Constant'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TransformerDecoderLayer.call of <official.nlp.modeling.layers.transformer.TransformerDecoderLayer object at 0x7f573c67f750>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Constant'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nW0917 14:16:15.077228 140018629465920 ag_logging.py:146] AutoGraph could not transform <bound method TransformerDecoderLayer.call of <official.nlp.modeling.layers.transformer.TransformerDecoderLayer object at 0x7f573c67f750>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Constant'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING:tensorflow:AutoGraph could not transform <bound method CachedAttention.call of <official.nlp.modeling.layers.attention.CachedAttention object at 0x7f573c63ead0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: 'arguments' object has no attribute 'posonlyargs'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nW0917 14:16:15.085375 140018629465920 ag_logging.py:146] AutoGraph could not transform <bound method CachedAttention.call of <official.nlp.modeling.layers.attention.CachedAttention object at 0x7f573c63ead0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: 'arguments' object has no attribute 'posonlyargs'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING:tensorflow:AutoGraph could not transform <bound method MultiChannelAttention.call of <official.nlp.modeling.layers.multi_channel_attention.MultiChannelAttention object at 0x7f573c60af90>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: 'arguments' object has no attribute 'posonlyargs'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nW0917 14:16:15.138240 140018629465920 ag_logging.py:146] AutoGraph could not transform <bound method MultiChannelAttention.call of <official.nlp.modeling.layers.multi_channel_attention.MultiChannelAttention object at 0x7f573c60af90>> and will run it as-is.\r\n```", "@nlp-sudo \r\ni see few issues with similar error, could you please refer to them if they are helpful.\r\n#37144 #35101 [link](https://stackoverflow.com/questions/60268178/error-creating-universal-sentence-encoder-embeddings-using-beam-tf-transform)", "I will handle the warning part later, that's not a big deal. It would be really helpful if you can help me out to solve the error:\r\n`AttributeError: 'LossScaleOptimizer' object has no attribute '_hypers_created.`\r\n\r\nI checked the optimizer code of keras too, it does have _hypers_created but I don't know why the error is occurring. And as I said, if I ignore the decaying of learning rate, the other error is coming. Solution to anyone of it would be really helpful.", "Can you share a very, very minimal but complete runnable example or Colab (better) to reproduce this?", "Also I see some explicit `float32` in the model definition at https://github.com/tensorflow/models/blob/master/official/nlp/nhnet/models.py. \r\nSo probably it is better that you open a ticket directly in https://github.com/tensorflow/models/  repository", "I thought the issue was related to FP16 in Keras and hence opened it here. No issues, I will open a issue in that repo. Thanks for help.", "@nlp-sudo \r\nPLease move the issue to closed status, once open in mentioned repo.", "Opened an issue here: https://github.com/tensorflow/models/issues/9262\r\n\r\nClosing this one.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43273\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43273\">No</a>\n"]}, {"number": 43272, "title": "Can't Access Tensorflow Dependencies After Install", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary):  pip install tensorflow\r\n- TensorFlow version:\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1/cudnn-10.1-windows10-x64-v7.6.5.32\r\n- GPU model and memory: nvidia rtx 2060 \r\n\r\n\r\n\r\n**Describe the problem**\r\nTensorflow not recognizing cudnn is installed. I have verified the files exist in the directories that I have added to my path variable, still says the file doesn't exist.\r\n![image](https://user-images.githubusercontent.com/21284577/93372548-a4bde280-f819-11ea-98aa-ea549bdd06e3.png)\r\n![image](https://user-images.githubusercontent.com/21284577/93372639-c5863800-f819-11ea-8a0c-3ac1c420910c.png)\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n from keras.models import Sequential\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\nfrom keras.models import Sequential\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n\r\n  File \"C:\\Users\\Luke\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\platform\\self_check.py\", line 87, in preload_check\r\n    ctypes.WinDLL(build_info.cudnn_dll_name)\r\n\r\n  File \"C:\\Users\\Luke\\anaconda3\\lib\\ctypes\\__init__.py\", line 364, in __init__\r\n    self._handle = _dlopen(self._name, mode)\r\n\r\nOSError: [WinError 126] The specified module could not be found\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-1-9c5e0a19b646>\", line 1, in <module>\r\n    from keras.models import Sequential\r\n\r\n  File \"C:\\Users\\Luke\\anaconda3\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from . import utils\r\n\r\n  File \"C:\\Users\\Luke\\anaconda3\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n\r\n  File \"C:\\Users\\Luke\\anaconda3\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n\r\n  File \"C:\\Users\\Luke\\anaconda3\\lib\\site-packages\\keras\\backend\\__init__.py\", line 1, in <module>\r\n    from .load_backend import epsilon\r\n\r\n  File \"C:\\Users\\Luke\\anaconda3\\lib\\site-packages\\keras\\backend\\load_backend.py\", line 90, in <module>\r\n    from .tensorflow_backend import *\r\n\r\n  File \"C:\\Users\\Luke\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n\r\n  File \"C:\\Users\\Luke\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n\r\n  File \"C:\\Users\\Luke\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\n  File \"C:\\Users\\Luke\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 30, in <module>\r\n    self_check.preload_check()\r\n\r\n  File \"C:\\Users\\Luke\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\platform\\self_check.py\", line 97, in preload_check\r\n    % (build_info.cudnn_dll_name, build_info.cudnn_version_number))\r\n\r\nImportError: Could not find 'cudnn64_7.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Note that installing cuDNN is a separate step from installing CUDA, and this DLL is often found in a different directory from the CUDA DLLs. You may install the necessary DLL by downloading cuDNN 7 from this URL: https://developer.nvidia.com/cudnn", "comments": ["@lkoll \r\n\r\nCan you please follow the guidelines from [here](https://www.tensorflow.org/install/gpu) and please make sure that all software and hardware requirements are met from [here](https://www.tensorflow.org/install/gpu#windows_setup).Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43272\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43272\">No</a>\n"]}, {"number": 43271, "title": "TFLite converter aborts (dumps core) in BroadcastAdd4DSlow", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Pop! OS 20.04 LTS\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 2.3.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\nhttps://colab.research.google.com/drive/1rfdr8bu_UfTzpGCXH-tQFVHMIKDreXad?usp=sharing\r\n\r\nWhen running on my Pop! OS system under gdb, I get the backtrace:\r\n```\r\ngdb python\r\n[...]\r\n(gdb) run tflite.py\r\n[...]\r\n(gdb) where\r\n#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50\r\n#1  0x00007ffff7ddb859 in __GI_abort () at abort.c:79\r\n#2  0x00007ffbf82ccb1f in tflite::reference_ops::BroadcastAdd4DSlow(tflite::ArithmeticParams const&, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float*) ()\r\n   from /home/juan/.local/lib/python3.8/site-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so\r\n#3  0x00007ffbf82d54ec in void tflite::ops::builtin::add::EvalAdd<(tflite::ops::builtin::add::KernelType)2>(TfLiteContext*, TfLiteNode*, TfLiteAddParams*, tflite::ops::builtin::add::OpData const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*) ()\r\n   from /home/juan/.local/lib/python3.8/site-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so\r\n#4  0x00007ffbf82d846d in TfLiteStatus tflite::ops::builtin::add::Eval<(tflite::ops::builtin::add::KernelType)2>(TfLiteContext*, TfLiteNode*) () from /home/juan/.local/lib/python3.8/site-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so\r\n#5  0x00007ffbf82ab1bf in tflite::optimize::calibration::(anonymous namespace)::LoggingEval(TfLiteContext*, TfLiteNode*) ()\r\n   from /home/juan/.local/lib/python3.8/site-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so\r\n#6  0x00007ffbf850f20b in tflite::impl::Subgraph::Invoke() ()\r\n   from /home/juan/.local/lib/python3.8/site-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so\r\n#7  0x00007ffbf85121c0 in tflite::impl::Interpreter::Invoke() ()\r\n   from /home/juan/.local/lib/python3.8/site-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so\r\n#8  0x00007ffbf827a581 in tflite::calibration_wrapper::CalibrationWrapper::FeedTensor(_object*) ()\r\n   from /home/juan/.local/lib/python3.8/site-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so\r\n#9  0x00007ffbf8274285 in pybind11::cpp_function::initialize<pybind11_init__pywrap_tensorflow_lite_calibration_wrapper(pybind11::module&)::{lambda(tflite::calibration_wrapper::CalibrationWrapper&, pybind11::handle&)#4}, pybind11::object, tflite::calibration_wrapper::CalibrationWrapper&, pybind11::handle&, pybind11::name, pybind11::is_method, pybind11::sibling>(pybind11_init__pywrap_tensorflow_lite_calibration_wrapper(pybind11::module&)::{lambda(tflite::calibration_wrapper::CalibrationWrapper&, pybind11::handle&)#4}&&, pybind11::object (*)(tflite::calibration_wrapper::CalibrationWrapper&, pybind11::handle&), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call) ()\r\n   from /home/juan/.local/lib/python3.8/site-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so\r\n#10 0x00007ffbf8271fe7 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) ()\r\n   from /home/juan/.local/lib/python3.8/site-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so\r\n#11 0x00000000005f17e5 in PyCFunction_Call ()\r\n#12 0x00000000005f2406 in _PyObject_MakeTpCall ()\r\n#13 0x000000000050795f in ?? ()\r\n#14 0x000000000056c475 in _PyEval_EvalFrameDefault ()\r\n#15 0x0000000000565972 in _PyEval_EvalCodeWithName ()\r\n#16 0x00000000005f1d85 in _PyFunction_Vectorcall ()\r\n#17 0x00000000005677c7 in _PyEval_EvalFrameDefault ()\r\n#18 0x0000000000565972 in _PyEval_EvalCodeWithName ()\r\n#19 0x00000000005f1d85 in _PyFunction_Vectorcall ()\r\n#20 0x0000000000507729 in ?? ()\r\n#21 0x00000000005f1107 in PyObject_Call ()\r\n#22 0x0000000000568e1f in _PyEval_EvalFrameDefault ()\r\n#23 0x000000000050712e in ?? ()\r\n#24 0x000000000056c475 in _PyEval_EvalFrameDefault ()\r\n#25 0x0000000000565972 in _PyEval_EvalCodeWithName ()\r\n#26 0x00000000005f1d85 in _PyFunction_Vectorcall ()\r\n--Type <RET> for more, q to quit, c to continue without paging--\r\n#27 0x00000000005677c7 in _PyEval_EvalFrameDefault ()\r\n#28 0x0000000000565972 in _PyEval_EvalCodeWithName ()\r\n#29 0x0000000000686053 in PyEval_EvalCode ()\r\n#30 0x00000000006753d1 in ?? ()\r\n#31 0x000000000067544f in ?? ()\r\n#32 0x0000000000675507 in PyRun_FileExFlags ()\r\n#33 0x000000000067758a in PyRun_SimpleFileExFlags ()\r\n#34 0x00000000006ae99e in Py_RunMain ()\r\n#35 0x00000000006aed29 in Py_BytesMain ()\r\n#36 0x00007ffff7ddd0b3 in __libc_start_main (main=0x4ebd20 <main>, argc=2, argv=0x7fffffffe2c8, init=<optimized out>, \r\n    fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7fffffffe2b8) at ../csu/libc-start.c:308\r\n#37 0x00000000005f62ee in _start ()\r\n```\r\n", "comments": ["@jaltmayerpizzorno Could you try the tf-nightly version instead?", "> @jaltmayerpizzorno Could you try the tf-nightly version instead?\r\n\r\nHi @abattery, I did earlier, with the same result, but will try again and report back.", "@abattery It crashed on colab again with `2.4.0-dev20200916`.  It also does so using the `tensorflow/tensorflow:nightly-gpu` (also `2.4.0-dev20200916`) docker image.  On the latter, gdb has the backtrace at:\r\n```\r\n#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51\r\n#1  0x00007fe09246f8b1 in __GI_abort () at abort.c:79\r\n#2  0x00007fdf60088eff in tflite::reference_ops::BroadcastAdd4DSlow(tflite::ArithmeticParams const&, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so\r\n#3  0x00007fdf6009136c in void tflite::ops::builtin::add::EvalAdd<(tflite::ops::builtin::add::KernelType)2>(TfLiteContext*, TfLiteNode*, TfLiteAddParams*, tflite::ops::builtin::add::OpData const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so\r\n#4  0x00007fdf600946d9 in TfLiteStatus tflite::ops::builtin::add::Eval<(tflite::ops::builtin::add::KernelType)2>(TfLiteContext*, TfLiteNode*) () from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so\r\n#5  0x00007fdf600676bf in tflite::optimize::calibration::(anonymous namespace)::LoggingEval(TfLiteContext*, TfLiteNode*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so\r\n#6  0x00007fdf60301e6b in tflite::impl::Subgraph::Invoke() ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so\r\n#7  0x00007fdf603049b0 in tflite::impl::Interpreter::Invoke() ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so\r\n#8  0x00007fdf600365c1 in tflite::calibration_wrapper::CalibrationWrapper::FeedTensor(_object*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so\r\n#9  0x00007fdf6002d5a5 in void pybind11::cpp_function::initialize<pybind11_init__pywrap_tensorflow_lite_calibration_wrapper(pybind11::module&)::{lambda(tflite::calibration_wrapper::CalibrationWrapper&, pybind11::handle&)#4}, pybind11::object, tflite::calibration_wrapper::CalibrationWrapper&, pybind11::handle&, pybind11::name, pybind11::is_method, pybind11::sibling>(pybind11_init__pywrap_tensorflow_lite_calibration_wrapper(pybind11::module&)::{lambda(tflite::calibration_wrapper::CalibrationWrapper&, pybind11::handle&)#4}&&, pybind11::object (*)(tflite::calibration_wrapper::CalibrationWrapper&, pybind11::handle&), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so\r\n#10 0x00007fdf6002e279 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so\r\n```", "Before crashing, it also logs the following... Just passing on in case it matters.\r\n```\r\nsee current operation: %174 = \"tfl.concatenation\"(%173, %128) {axis = 3 : i32, fused_activation_function = \"NONE\"} : (tensor<?x?x?x256xf32>, tensor<?x38x38x512xf32>) -> tensor<?x38x38x768xf32>\r\n2020-09-16 23:31:50.934601: W tensorflow/compiler/mlir/lite/tf_to_tfl_flatbuffer.cc:138] see current operation: %174 = \"tfl.concatenation\"(%173, %128) {axis = 3 : i32, fused_activation_function = \"NONE\"} : (tensor<?x?x?x256xf32>, tensor<?x38x38x512xf32>) -> tensor<?x38x38x768xf32>\r\n\r\nsee current operation: %193 = \"tfl.concatenation\"(%192, %77) {axis = 3 : i32, fused_activation_function = \"NONE\"} : (tensor<?x?x?x128xf32>, tensor<?x76x76x256xf32>) -> tensor<?x76x76x384xf32>\r\n2020-09-16 23:31:50.934633: W tensorflow/compiler/mlir/lite/tf_to_tfl_flatbuffer.cc:138] see current operation: %193 = \"tfl.concatenation\"(%192, %77) {axis = 3 : i32, fused_activation_function = \"NONE\"} : (tensor<?x?x?x128xf32>, tensor<?x76x76x256xf32>) -> tensor<?x76x76x384xf32>\r\n\r\nsee current operation: %216 = \"tfl.concatenation\"(%212, %213, %215) {axis = 4 : i32, fused_activation_function = \"NONE\"} : (tensor<?x76x76x3x2xf32>, tensor<?x76x76x3x2xf32>, tensor<?x76x76x3x81xf32>) -> tensor<?x76x76x3x85xf32>\r\n2020-09-16 23:31:50.934644: W tensorflow/compiler/mlir/lite/tf_to_tfl_flatbuffer.cc:138] see current operation: %216 = \"tfl.concatenation\"(%212, %213, %215) {axis = 4 : i32, fused_activation_function = \"NONE\"} : (tensor<?x76x76x3x2xf32>, tensor<?x76x76x3x2xf32>, tensor<?x76x76x3x81xf32>) -> tensor<?x76x76x3x85xf32>\r\n\r\nsee current operation: %231 = \"tfl.concatenation\"(%219, %222, %226, %230, %215) {axis = 4 : i32, fused_activation_function = \"NONE\"} : (tensor<?x76x76x3x1xf32>, tensor<?x76x76x3x1xf32>, tensor<?x76x76x3x1xf32>, tensor<?x76x76x3x1xf32>, tensor<?x76x76x3x81xf32>) -> tensor<?x76x76x3x85xf32>\r\n2020-09-16 23:31:50.934655: W tensorflow/compiler/mlir/lite/tf_to_tfl_flatbuffer.cc:138] see current operation: %231 = \"tfl.concatenation\"(%219, %222, %226, %230, %215) {axis = 4 : i32, fused_activation_function = \"NONE\"} : (tensor<?x76x76x3x1xf32>, tensor<?x76x76x3x1xf32>, tensor<?x76x76x3x1xf32>, tensor<?x76x76x3x1xf32>, tensor<?x76x76x3x81xf32>) -> tensor<?x76x76x3x85xf32>\r\n\r\nsee current operation: %242 = \"tfl.concatenation\"(%238, %239, %241) {axis = 4 : i32, fused_activation_function = \"NONE\"} : (tensor<?x38x38x3x2xf32>, tensor<?x38x38x3x2xf32>, tensor<?x38x38x3x81xf32>) -> tensor<?x38x38x3x85xf32>\r\n2020-09-16 23:31:50.934666: W tensorflow/compiler/mlir/lite/tf_to_tfl_flatbuffer.cc:138] see current operation: %242 = \"tfl.concatenation\"(%238, %239, %241) {axis = 4 : i32, fused_activation_function = \"NONE\"} : (tensor<?x38x38x3x2xf32>, tensor<?x38x38x3x2xf32>, tensor<?x38x38x3x81xf32>) -> tensor<?x38x38x3x85xf32>\r\n\r\nsee current operation: %257 = \"tfl.concatenation\"(%245, %248, %252, %256, %241) {axis = 4 : i32, fused_activation_function = \"NONE\"} : (tensor<?x38x38x3x1xf32>, tensor<?x38x38x3x1xf32>, tensor<?x38x38x3x1xf32>, tensor<?x38x38x3x1xf32>, tensor<?x38x38x3x81xf32>) -> tensor<?x38x38x3x85xf32>\r\n2020-09-16 23:31:50.934677: W tensorflow/compiler/mlir/lite/tf_to_tfl_flatbuffer.cc:138] see current operation: %257 = \"tfl.concatenation\"(%245, %248, %252, %256, %241) {axis = 4 : i32, fused_activation_function = \"NONE\"} : (tensor<?x38x38x3x1xf32>, tensor<?x38x38x3x1xf32>, tensor<?x38x38x3x1xf32>, tensor<?x38x38x3x1xf32>, tensor<?x38x38x3x81xf32>) -> tensor<?x38x38x3x85xf32>\r\n\r\nsee current operation: %268 = \"tfl.concatenation\"(%264, %265, %267) {axis = 4 : i32, fused_activation_function = \"NONE\"} : (tensor<?x19x19x3x2xf32>, tensor<?x19x19x3x2xf32>, tensor<?x19x19x3x81xf32>) -> tensor<?x19x19x3x85xf32>\r\n2020-09-16 23:31:50.934705: W tensorflow/compiler/mlir/lite/tf_to_tfl_flatbuffer.cc:138] see current operation: %268 = \"tfl.concatenation\"(%264, %265, %267) {axis = 4 : i32, fused_activation_function = \"NONE\"} : (tensor<?x19x19x3x2xf32>, tensor<?x19x19x3x2xf32>, tensor<?x19x19x3x81xf32>) -> tensor<?x19x19x3x85xf32>\r\n\r\nsee current operation: %283 = \"tfl.concatenation\"(%271, %274, %278, %282, %267) {axis = 4 : i32, fused_activation_function = \"NONE\"} : (tensor<?x19x19x3x1xf32>, tensor<?x19x19x3x1xf32>, tensor<?x19x19x3x1xf32>, tensor<?x19x19x3x1xf32>, tensor<?x19x19x3x81xf32>) -> tensor<?x19x19x3x85xf32>\r\n2020-09-16 23:31:50.934717: W tensorflow/compiler/mlir/lite/tf_to_tfl_flatbuffer.cc:138] see current operation: %283 = \"tfl.concatenation\"(%271, %274, %278, %282, %267) {axis = 4 : i32, fused_activation_function = \"NONE\"} : (tensor<?x19x19x3x1xf32>, tensor<?x19x19x3x1xf32>, tensor<?x19x19x3x1xf32>, tensor<?x19x19x3x1xf32>, tensor<?x19x19x3x81xf32>) -> tensor<?x19x19x3x85xf32>\r\n```", "I just confirmed that the issue is still there with TF 2.3.1.", "@abattery Not sure of this change pertains to Keras/RNN model conversion. Here is a [colab](https://colab.research.google.com/drive/1lQsF1DH7Ozv2WXYVm0t4HaPfkCAE_G3Z?usp=sharing) to help you debug.\r\n\r\n", "This issue is related to partial 5D support on both Add and Mul operators. I have working fixes for this problem. I will update the thread when they will arrive.", "The fix has merged into master. Could you try again with the tomorrow's nightly version?", "Sure, will do.", "I re-ran the code on the colab I posted earlier in this ticket; the crash is gone!  Nice! :)\r\n\r\nI am getting errors about operations being `neither a custom op nor a flex op` that I haven't attempted to solve yet, but I assume they are due to what's in my model and not a sign of a bug (well, at least not this bug).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43271\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43271\">No</a>\n"]}, {"number": 43270, "title": "[RNN]Cannot get saved .tflite model", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 16.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: tf-nightly 2.2.0\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n[convert_pb2_tflite.txt](https://github.com/tensorflow/tensorflow/files/5233664/convert_pb2_tflite.txt)\r\n\r\n**The output  log from the converter invocation**\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/5233693/log.txt)\r\n\r\n**Failure details**\r\nMy .pb model is trained and saved in tensorflow 1.15.0 and I want to convert it to .tflite to deploy on CPU for better performance. When I run the uploaded file convert_pb2_tflite.txt(.py actually), the log attached above seems to include no warning and errors. But there is no tflite file in my save path. What's probably going wrong? \r\nThank you.\r\n\r\n", "comments": ["tf.Sign op is currently supported by the TFLite builtin operator set. You can use the Select TF op set for covering tf.Sign op. Please refer to the following guide pages.\r\n\r\nhttps://www.tensorflow.org/lite/guide/ops_select\r\nhttps://www.tensorflow.org/lite/guide/reduce_binary_size", "Hi @abattery,\r\n\r\nThank you, I successfully convert following your instruction. But the .pb file before converting is 131MB, and converted .tflite file is 73MB, is this normal?\r\n\r\n Thank you.", "TFLite converter can trim the given graph based on input/output tensors for inference use cases. Often it can reduce overall binary size and the reduction rate can be varied depending on the given graph internal structure.", "Hi @abattery,\r\n\r\nI tried to do inference with .tflite model. \r\ninterpreter.allocate_tensors()   gives this error:\r\n'RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. Node number 3 (FlexSign) failed to prepare.'\r\n\r\nAccording to https://github.com/tensorflow/tensorflow/issues/40157#issuecomment-680798660, seems like SELECT_TF_OPS is not supported by tflite_runtime. There is LSTM layer in my model, do I have to reimplement it with keras?https://github.com/tensorflow/tensorflow/issues/40157#issuecomment-691650953\r\n\r\nThank you\r\n", "To run Select TF ops in android, the android application project requires additional dependency on org.tensorflow:tensorflow-lite-select-tf-ops.\r\n\r\nhttps://www.tensorflow.org/lite/guide/ops_select#android_aar\r\n\r\nIf you want a trimmed aar, you can follow the below page:\r\nhttps://www.tensorflow.org/lite/guide/reduce_binary_size\r\n", "Hi @abattery \uff0c\r\n\r\nI'm running on Linux PC.", "Great. TensorFlow Lite with select TensorFlow ops will be installed automatically with the [TensorFlow pip package](https://www.tensorflow.org/install/pip). You can also choose to only install the [TensorFlow Lite Interpreter pip package](https://www.tensorflow.org/lite/guide/python#install_just_the_tensorflow_lite_interpreter).", "Hi @abattery ,\r\nError\r\n'RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. Node number 3 (FlexSign) failed to prepare.'\r\n\r\nseems to caused by tf.sign op in my model since I remove that op, the error is gone.\r\nSo do I have to implement this op myself or are there other solutions?\r\n\r\nThank you", "How did you build your TFLite inference code? Are you using the PIP installer or building natively?\r\n\r\nThere are two ways to enable tf.Sign op by 1) implementing your own custom op 2) linking Flex delegate.", "Hi @abattery ,\r\n\r\nI build with pip installer. Could you please offer an instruction on how to Link Flex delegate?\r\n\r\nThank you", "Flex delegate will be automatically enabled on Linux when you use tf 2.3.0 or later versions only.", "OK, I will try tf-nightly 2.4.0. Thank you for your help and patience @abattery ", "Hi, @abattery,\r\n\r\nI update to tf-nightly2.4 try to run inference on .tflitemodel, but  error occurs. Here is the log:\r\n\r\n*******************\r\nINFO: TfLiteFlexDelegate delegate: 11 nodes delegated out of 129 nodes with 3 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 3 nodes delegated out of 24 nodes with 3 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 3 nodes delegated out of 24 nodes with 3 partitions.\r\n\r\n[{'name': 'input_x', 'index': 0, 'shape': array([ 1, 50], dtype=int32), 'shape_signature': array([-1, 50], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n[{'name': 'output/y_hat', 'index': 225, 'shape': array([   1, 2363], dtype=int32), 'shape_signature': array([  -1, 2363], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n2020-09-18 18:37:52.699048: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at tensor_array_ops.cc:1035 : Not found: Container __per_step_0 does not exist. (Could not find resource: __per_step_0/_tensor_arraysrnn/bidirectional_rnn/bw/bw/dynamic_rnn/input_0_1)\r\nTraceback (most recent call last):\r\n  File \"run_tflite.py\", line 60, in <module>\r\n    interpreter.invoke()\r\n  File \"/home/ai/anaconda5/envs/tf-nightly2.4/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py\", line 539, in invoke\r\n    self._interpreter.Invoke()\r\nRuntimeError: Container __per_step_0 does not exist. (Could not find resource: __per_step_0/_tensor_arraysrnn/bidirectional_rnn/bw/bw/dynamic_rnn/input_0_1)\r\n\t (while executing 'TensorArrayScatterV3' via Eager)Node number 129 (TfLiteFlexDelegate) failed to invoke.\r\n\r\n*******************\r\nSeems like it's the problem with BiLSTM Layer.\r\nThe code I define BiLSTM layer is: \r\n\r\nfrom tensorflow.contrib import rnn\r\n\r\nwith tf.variable_scope(\"rnn\", reuse=reuse):\r\n    lstm_fw_cell = rnn.BasicLSTMCell(hidden_size)\r\n    lstm_bw_cell = rnn.BasicLSTMCell(hidden_size)\r\n\r\n    if self.dropout_keep_prob is not None:\r\n        lstm_fw_cell = rnn.DropoutWrapper(lstm_fw_cell, output_keep_prob=self.dropout_keep_prob)\r\n        lstm_bw_cell = rnn.DropoutWrapper(lstm_bw_cell, output_keep_prob=self.dropout_keep_prob)\r\n    \r\n    outputs, _ = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, embedded_chars, dtype=tf.float32)\r\n\r\n*******************\r\n\r\nBut I refer to info in https://www.tensorflow.org/lite/convert/rnn, bidirectional_dynamic_rnn() is available.\r\n\r\nRefer to https://github.com/tensorflow/tensorflow/issues/40157#issuecomment-691650953, if lstm layer was created by keras, this error was solved. So do I have to implement BiLSTM layer in eras?\r\n\r\nThank you\r\n\r\n", "Hi @le8888e \r\n\r\nSorry for encountering the issue.\r\n\r\nTensorArrayScatterV3 op is currently not supported by either TensorFlow Lite builtin op set or Flex delegate. Unfortunately, the above TF graph should be rewritten by replacing the TensorArray with a 1-D TensorList through V2 control flow ops or concatenation operators if possible instead.\r\n\r\nWhen you are creating a TF graph, could you invoke the following call before the TF graph creation part?\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/compat/v1/enable_control_flow_v2\r\n\r\nBest regards,", "Hi @abattery ,\r\n\r\nI have tried to invoke 'tf.enable_control_flow_v2()'  before I create my TF graph. But I cannot converted trained .pb to .tflite. Here is the log:\r\n![log](https://user-images.githubusercontent.com/34771253/93551389-4bcc8680-f9a0-11ea-8057-9e521f031c30.jpeg)\r\nWithout 'tf.enable_control_flow_v2()', I can finish converting but collapse in inference like I mentioned before.\r\n\r\nI will try to replace tf.nn.bidirectional_dynamic_rnn() by tf.lite.experimental.nn.bidirectional_dynamic_rnn() and let you know the result.\r\n\r\nThank you", "Hi @abattery,\r\n\r\nI try to replace tf.contirb.cnn.BasicLSTMCell() with tf.lite.experimental.nn.TfLiteLSTMCell()\r\nand\r\ntf.nn.bidirectional_dynamic_rnn() with tf.lite.experimental.nn.bidirectional_dynamic_rnn()\r\nThen retrain my model. My tf version to train is tf 1.15.0\r\n\r\nError occurs:\r\nAttributeError: module 'tensorflow._api.v1.lite.experimental.nn' has no attribute 'TfLiteLSTMCell'\r\n\r\nSeems like tf.lite.experimental  is not supported by tf 1.* \r\nMaybe I have to transfer my code to tf 2.* or keras : )", "hi @abattery ,\r\n\r\nAfter implementing BiLSTM by Keras, I successfully convert and inference .tflite model.\r\nI want to serve .tflite model by tf-serving  on PC  with python api. Is  this feasible and is there any guidance for this?\r\n\r\nTHANK YOU", "As you already asked the question to the tf-serving project, let's see their answers at https://github.com/tensorflow/serving/issues/1736.", "Hi @abattery ,\r\n\r\nI was building tf-nightly viac pip. Would XNNPACK built by default? And how can I check it?\r\n\r\nThank you\r\n", "@le8888e \r\nserving requires model asset to be a saved model, please create a new issue for any new questions. If this resolves your issue please move this to closed status.", "@multiverse-tf Chao, could you answer @le8888e 's comment at https://github.com/tensorflow/tensorflow/issues/43270#issuecomment-696582237?", "> Hi @abattery ,\r\n> \r\n> I was building tf-nightly viac pip. Would XNNPACK built by default? And how can I check it?\r\n\r\nAs for \"XNNPACK built by default\", do you mean by \"applying XNNPACK by default\" in the interpreter itself? If yes, this hasn't been enabled yet in tf-nightly build. So, to utilize XNNPACK delegate currently, pls continue to follow this guide: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md#using-xnnpack-engine-with-tensorflow-lite-interpreter\r\n\r\n", "@le8888e\r\nPlease update, if this issue can be moved to closed status.", "@Saduf2019 ,\r\nThis issue can be closed, thank you.", "Moving the issue to closed status with confirmation"]}, {"number": 43269, "title": "Mutablehashtable lookup support full size dynamic default values.", "body": "This PR is one part of [RFC: Sparse Domain Isolation for Supporting large-scale Sparse Weights Training](https://github.com/tensorflow/community/pull/237), and original is here : [PR](https://github.com/tensorflow/tensorflow/pull/41371)\r\n@yuefengz @tanzhenyu Please review the PR, Thanks!", "comments": ["This PR is required to make Hash table trainable in TensorFlow, that is a key feature for sparse weights training on Recommender System.", "Hi @rohan100jain @yuefengz , please help review this PR, Thanks!", "Hi @rohan100jain , thank you for your reviewing, I just push a new commit that contains test cases, clearer function comments. Please review it again, thank you very much!"]}, {"number": 43268, "title": "TF-TRT Expose TRTEngineInstance class", "body": "Add `trt_engine_instance_proto_py` as a dependency of `trt_convert.py` to ensure that TF will install the `TRTEngineInstance` python class. Modify an existing test for this.\r\n\r\nThe `TRTEngineInstance` protobuf class is needed to retrieve serialized TensorRT inference engines from an asset file using Python, to support use cases similar to https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#tensorrt-plan.\r\n\r\nHere is an example how to load a TensorRT engine from the asset file:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.compiler.tf2tensorrt.utils.trt_engine_instance_pb2 import TRTEngineInstance\r\n\r\ndef get_first_serialized_engine_from_asset(asset_file):\r\n    \"\"\" Read the serialized TRT engine from an asset file, and return the buffer that contains the first engine.\r\n    Arguments: asset_file -- path to the file\r\n    \"\"\"\r\n    raw_dataset = tf.data.TFRecordDataset([asset_file])\r\n\r\n    # Note that the asset file could contain multiple engines, one for each input shape that the\r\n    # corresponding TRTEngineOp handles. Here we only return the buffer that contains the first engine.\r\n    for raw_record in raw_dataset.take(1):\r\n        engine_instance = TRTEngineInstance()\r\n        engine_instance.ParseFromString(raw_record.numpy())\r\n        #print(\"Loaded engine for shape\", engine_instance.input_shapes)\r\n        #print(repr(raw_record))\r\n        \r\n    return engine_instance.serialized_engine\r\n\r\nasset_file = '/tmp/models/trt_model/assets/trt-serialized-engine.TRTEngineOp_0_0'\r\nengine_string = get_first_serialized_engine_from_asset(asset_file)\r\n```\r\n\r\nTagging @bixia1 for review.", "comments": ["@tfeher  Can you please check @bixia1's comments and keep us posted ? Thanks!", "Thanks @bixia1, I have updated the PR description!", "I am still working in merging this into google, need to fix issues related to the translation between google and github."]}, {"number": 43267, "title": "Grammar fix", "body": "", "comments": ["Thanks for contributing on this."]}, {"number": 43266, "title": "Update delegates.md with buildin_ops.h header", "body": "Add the include file tensorflow/lite/builtin_ops.h since some of the constants referenced in the code are defined in this file", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43266) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43266) for more info**.\n\n<!-- ok -->"]}, {"number": 43265, "title": "Fix XNNPack delegate type-check for Conv2D bias.", "body": "This small typo means that the bias tensor type isn't getting checked. (The filter tensor type is already checked on line 1208 just above.)", "comments": ["Thanks for spotting this error."]}, {"number": 43264, "title": "Helm chart for TF2+GPU", "body": "Would like to see a helm chart for TF2 w/ GPUs be made avail. ", "comments": ["There are already some TF helm charts in https://github.com/helm/charts/. \r\nIf you need something alse please close this and open a new thicket on that repository directly.", "There must be some confusion, I am not looking for the helm chart repo. I\nsearched that and all I found was TF v1.16.\n\nI am looking for a Helm chart for TF2+GPU.\n\nOn Wed, Sep 16, 2020 at 8:50 AM bhack <notifications@github.com> wrote:\n\n>\n>\n> There are already some TF helm charts in https://github.com/helm/charts/.\n>\n>\n> If you need something alse please close this and open a new thicked on\n> that repository directly.\n>\n>\n>\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/43264#issuecomment-693418471>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABHVQHCO26NEIU4FKSOMKKTSGC7BPANCNFSM4RO2DZEQ>\n> .\n>\n>\n>\n", "Yes but I meant that we don't maintain Helm chart directly. \r\nIt is a Helm community initiative in the Helm repository.\r\nSo if you have any request (e.g. like having something updated for TF2 + Gpu) open an issue or a PR directly there.\r\nThanks", "Thank you.\n\nOn Wed, Sep 16, 2020 at 9:12 AM bhack <notifications@github.com> wrote:\n\n>\n>\n> Yes but I meant that we don't maintain Helm chart directly.\n>\n>\n> It is a Helm community initiative in the Helm repository.\n>\n>\n> So if you have any request (e.g. like having something updated for TF2 +\n> Gpu) open an issue or a PR directly there.\n>\n>\n> Thanks\n>\n>\n>\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/43264#issuecomment-693433159>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABHVQHFHZRNCKEIRP3AYBC3SGDBVRANCNFSM4RO2DZEQ>\n> .\n>\n>\n>\n", "Seems like this one can be closed out then, @joehoeller ?", "Yes plz.\n\nOn Wed, Sep 16, 2020 at 9:50 AM Tim Ryan <notifications@github.com> wrote:\n\n>\n>\n> Seems like this one can be closed out then, @joehoeller\n> <https://github.com/joehoeller> ?\n>\n>\n>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n>\n>\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/43264#issuecomment-693457330>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABHVQHBIJ4BY2JBFUGY67T3SGDGCDANCNFSM4RO2DZEQ>\n> .\n>\n>\n>\n"]}]