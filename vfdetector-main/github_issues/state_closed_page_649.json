[{"number": 34145, "title": "Implemented Pooling kernel for CMSIS-NN backend", "body": "Highlight:\r\n- Implemented average and max pooling kernel for cmsis-nn backend\r\n- Implemented reshape kernels for cmsis-nn backend\r\n- Encapsulated scratch buffer", "comments": ["@lamarrr Nice job! Let me know if you need input on the fallback solutions for the other cmsis-nn kernels. Or anything else regarding optimization. Cheers!", "> @lamarrr Nice job! Let me know if you need input on the fallback solutions for the other cmsis-nn kernels. Or anything else regarding optimization. Cheers!\r\n\r\n@freddan80  \r\nOk, sir. Will do!", "> @lamarrr Nice job! Let me know if you need input on the fallback solutions for the other cmsis-nn kernels. Or anything else regarding optimization. Cheers!\r\n\r\n@freddan80 \r\nany plans to implement _broadcasting add_ for the cmsis-nn _add_ kernel?\r\nI think that's the only thing left for the tflite _add_  kernel.", "> > @lamarrr Nice job! Let me know if you need input on the fallback solutions for the other cmsis-nn kernels. Or anything else regarding optimization. Cheers!\r\n> \r\n> @freddan80\r\n> any plans to implement _broadcasting add_ for the cmsis-nn _add_ kernel?\r\n> I think that's the only thing left for the tflite _add_ kernel.\r\n\r\nIt's on our todo-list, but not until beginning of next year. If you want to contribute it earlier, we'd be happy to help out. Cheers! /Fredrik"]}, {"number": 34144, "title": "Tensorflow 2.0 too slow when minimizing a custom cost function", "body": "**System information**\r\n- OS : Windows 10\r\n- TensorFlow version: v2.0.0-rc2-26-g64c3d382ca 2.0.0\r\n- Python version: 3.7.4\r\n\r\nI have a code that looks like the following, where I want to minimize a custom cost function with respect to parameters w. However, when running the code, it appears to me that it is very slow (like more than 30 times slower) compared to the same code implemented without tensorflow (by explicitly defining a function that gives the gradient of the cost).\r\n\r\nI am not sure if it's a problem with tf or if I am doing something wrong and unnecessarily re-computing the tf graph each time. I posted this issue on stackoverflow and have been advised to open an issue here.\r\n\r\nIn the following code, I am using a simple dummy cost function just as an example to show the big difference in performance.\r\n\r\n**Code with Tensorflow:**\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport time\r\n\r\nclass ExampleTF:\r\n    def __init__(self, n=100, m=10):\r\n        Z = np.random.randn(n, m)\r\n        self.Z = tf.convert_to_tensor(Z, dtype=tf.float32)\r\n        self.w = tf.Variable(np.ones((m, 1)), dtype=tf.float32)\r\n\r\n    # =====================================\r\n    def cost(self, P):\r\n        # This is a simple dummy cost function just as an example\r\n        return tf.reduce_sum((self.Z @ self.w) - P)\r\n\r\n    # =====================================\r\n    def optimize_w(self, cost_func, parameters, lr=0.01, iterations=2000):\r\n        optimizer = tf.optimizers.Adam(lr)\r\n        for _ in range(iterations):\r\n            optimizer.minimize(cost_func, var_list=parameters)\r\n\r\n    # =====================================\r\n    def update(self, P):\r\n        P = tf.convert_to_tensor(P, dtype=tf.float32)\r\n\r\n        self.optimize_w(\r\n            cost_func = lambda: self.cost(P),\r\n            parameters = [self.w]\r\n        )\r\n\r\n        #print(\"===> cost:\", self.cost(P).numpy())\r\n        #print(\"w:\", self.w.numpy().reshape(-1)[:10])\r\n\r\n# =====================================\r\nn, m = 1000, 100\r\nex_tf = ExampleTF(n, m)\r\nfor _ in range(50):\r\n    P = np.random.uniform(size=n).reshape((-1, 1))\r\n\r\n    start = time.time()\r\n    ex_tf.update(P)\r\n    elapsed = time.time() - start\r\n\r\n    print(\"elapsed time:\", elapsed)\r\n```\r\n\r\n**Code without Tensorflow (just numpy) :**\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport time\r\n\r\nclass ExampleNonTF:\r\n    def __init__(self, n=100, m=10):\r\n        self.Z = np.random.randn(n, m)\r\n        self.w = np.ones((m, 1))\r\n\r\n    # =====================================\r\n    def cost(self, P):\r\n        # This is a simple dummy cost function just as an example\r\n        return np.sum(self.Z @ self.w - P)\r\n\r\n    # =====================================\r\n    def gradient_cost(self, P):\r\n        # This is the gradient of the dummy cost function with respect to self.w\r\n        return np.sum(self.Z, axis=0).reshape(self.w.shape)\r\n\r\n    # =====================================\r\n    def optimize_w(self, P, lr=0.01, iterations=2000): # This is the ADAM optimizer\r\n        avg_grad1 = 0; avg_grad2 = 0\r\n        beta1 = 0.9; beta2 = 0.999; eps = 1e-07\r\n        for itr in range(iterations):\r\n            grad = self.gradient_cost(P)\r\n            avg_grad1 = beta1 * avg_grad1 + (1 - beta1) * grad\r\n            avg_grad2 = (beta2 * avg_grad2 + (1 - beta2) * (grad ** 2))\r\n            avg_grad1_corr = avg_grad1 / (1 - beta1 ** (itr + 1))\r\n            avg_grad2_corr = avg_grad2 / (1 - beta2 ** (itr + 1))\r\n            self.w = self.w - lr * (avg_grad1_corr / (np.sqrt(avg_grad2_corr) + eps))\r\n\r\n    # =====================================\r\n    def update(self, P):\r\n        self.optimize_w(P)\r\n\r\n        #print(\"===> cost:\", self.cost(P))\r\n        #print(\"w:\", self.w.reshape(-1)[:10])\r\n\r\n# =====================================\r\nn, m = 1000, 100\r\nex_nontf = ExampleNonTF(n, m)\r\nfor _ in range(50):\r\n    P = np.random.uniform(size=n).reshape((-1, 1))\r\n\r\n    start = time.time()\r\n    ex_nontf.update(P)\r\n    elapsed = time.time() - start\r\n\r\n    print(\"elapsed time:\", elapsed)\r\n```", "comments": ["I have tried on colab with TF version 2.0  and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/4280a01f0727d3da964e6ad750043929/untitled346.ipynb). Thanks!\r\n", "By the way, for a very big n (e.g. > 1000000), the TF code version starts to become faster than the NON-TF one.\r\n\r\nIs Tensorflow meant for large-scale computations only? I consider `n, m = 10000, 100` to be sufficiently big in my case, yet the NON-TF version is faster for these dimensions.", "Ran some tests - code & results below. Noticed bit late that your Numpy implementation is off - see \"Changelog\". Lastly, note the very last optimistic result; as I suspected, `Eager` is a culprit - will continue investigating later.\r\n\r\n<hr>\r\n\r\n**Results**:\r\n\r\n```python\r\nEAGER\r\ntf vs. np  (tf.optimizers.Adam)\r\n_____________________________________________________\r\nRatio | n, m          | iterations | Total time (sec)\r\n66.6  | 100,    100   | 50000      | (111,  1.67)\r\n26.9  | 1000,   100   | 50000      | (120,  4.46)\r\n4.36  | 10000,  100   | 20000      | (54.9, 12.6)\r\n1.34  | 100000, 100   | 2000       | (25.6, 19.1)\r\n1.02  | 100000, 1000  | 400        | (34.2, 33.6)\r\n_____________________________________________________\r\n\r\ntf.keras vs. np  (tf.keras.optimizers.Adam)\r\n_____________________________________________________\r\nRatio | n, m          | iterations | Total time (sec)\r\n64.9  | 100,    100   | 50000      | (114,  1.75)\r\n25.4  | 1000,   100   | 50000      | (115,  4.53)\r\n4.39  | 10000,  100   | 20000      | (55.3, 12.6)\r\n1.26  | 100000, 100   | 2000       | (24.4, 19.4)\r\n1.02  | 100000, 1000  | 400        | (33.9, 33.4)\r\n_____________________________________________________\r\n\r\nGRAPH\r\ntf.keras vs. np  (tf.keras.optimizers.Adam)\r\n_____________________________________________________\r\nRatio | n, m          | iterations | Total time (sec)\r\n      | 100,    100   | 50000      | \r\n      | 1000,   100   | 50000      | \r\n      | 10000,  100   | 20000      | \r\n      | 100000, 100   | 2000       | \r\n0.373 | 100000, 1000  | 400        | (12.5, 33.6)\r\n_____________________________________________________\r\n```\r\n\r\n<hr>\r\n\r\n**Testing code**:\r\n\r\n```python\r\nimport tensorflow as tf\r\n# tf.compat.v1.disable_eager_execution() # uncomment to disable Eager\r\nimport numpy as np\r\nfrom time import time\r\nprint(\"TF version: \" + tf.__version__)\r\n\r\n######################################################\r\nclass ExampleTF:\r\n    def __init__(self, n=100, m=10, iterations=2000):\r\n        Z = np.random.randn(n, m)\r\n        self.Z = tf.convert_to_tensor(Z, dtype=tf.float32)\r\n        self.w = tf.Variable(np.ones((m, 1)), dtype=tf.float32)\r\n        self.iterations = iterations\r\n        self.optimizer = tf.keras.optimizers.Adam(lr=0.01)\r\n\r\n    def cost(self, P):\r\n        return tf.reduce_sum((self.Z @ self.w) - P) # dummy cost function\r\n\r\n    def optimize_w(self, cost_func, parameters, lr=0.01):\r\n        for _ in range(self.iterations):\r\n            self.optimizer.minimize(cost_func, var_list=parameters)\r\n\r\n    def update(self, P):\r\n        P = tf.convert_to_tensor(P, dtype=tf.float32)\r\n        self.optimize_w(cost_func = lambda: self.cost(P),\r\n                        parameters = [self.w])\r\n\r\nclass ExampleNumpy:\r\n    def __init__(self, n=100, m=10, iterations=2000):\r\n        self.Z = np.random.randn(n, m)\r\n        self.w = np.ones((m, 1))\r\n        self.iterations = iterations\r\n\r\n    def cost(self, P):\r\n        return np.sum(self.Z @ self.w - P)\r\n\r\n    def gradient_cost(self, P):\r\n        return np.sum(self.Z, axis=0).reshape(self.w.shape)\r\n\r\n    def optimize_w(self, P, lr=0.01): # Adam implemented in Numpy\r\n        m_t, v_t = 0, 0\r\n        beta_1 = 0.9; beta_2 = 0.999; eps = 1e-07\r\n\r\n        for itr in range(self.iterations):\r\n            t = itr + 1\r\n            lr_t = lr * np.sqrt(1 - beta_2**t) / (1 - beta_1**t)\r\n            grad = self.gradient_cost(P)\r\n\r\n            m_t = beta_1 * m_t + (1 - beta_1) * grad\r\n            v_t = beta_2 * v_t + (1 - beta_2) * (grad**2)\r\n            self.w = self.w - lr_t * m_t / (np.sqrt(v_t) + eps)\r\n\r\n    def update(self, P):\r\n        self.optimize_w(P)\r\n\r\ndef test_performance(ex_class, P, test_name):\r\n    iters = ex_class.iterations\r\n    t0 = time()\r\n    ex_class.update(P)\r\n    iter_time = round((time() - t0) / iters, 7)\r\n\r\n    print(\"[{}] Time/iter: {} sec ({} iters avg) -- Total time: {} sec\".format(\r\n          test_name, iter_time, iters, iter_time * iters))\r\n    return iter_time\r\n######################################################\r\n\r\nn, m = 100000, 1000\r\niterations = 400\r\n\r\nP = np.random.uniform(size=n).reshape((-1, 1))\r\nex_tf = ExampleTF(n, m, iterations)\r\nex_np = ExampleNumpy(n, m, iterations)\r\n\r\ntf_iter_time = test_performance(ex_tf, P, 'tf')\r\nnp_iter_time = test_performance(ex_np, P, 'np')\r\n\r\nprint(\"[tf/np] Time/iter:\", round(tf_iter_time / np_iter_time, 3), \"(ratio)\")\r\n```\r\n\r\n<hr>\r\n\r\n**Changelog**:\r\n   - Defined self.optimizer to eliminate redundant redefinitions w/ overheads\r\n   - Rewrote Numpy's self.optimize_w to be consistent w/ TF/keras implementation (partly[1])\r\n   - Removed `range(50)`; redundant overhead w/ `P` generations; `iterations` is sufficient\r\n   - Defined `test_performance()` for convenience\r\n   - Formatting, var renaming, etc\r\n\r\n[1] `self.cost` is never used, and `gradient` is computed without it - should be fixed.  Also the dummy classes should be such that they're buildable as neural nets - i.e. involving layers w/ weights (e.g. `Dense`), a loss function (e.g. `'mae', 'mse'`), and labels (e.g. `P`).\r\n\r\n**Todo**:\r\n   - Profile Graph (disable Eager execution) performance\r\n   - Profile Numpy implementation accuracy vs. TF", "@OverLordGoldDragon  \r\nI don't know whether your optimization is completed on GPU or CPU. \r\nI find the same issue on GPU, but if you force the optimization in CPU, which is the same as numpy does, then it is faster.\r\n\r\nOn GPU:\r\n```bash\r\nTF version: 2.0.0\r\n[tf] Time/iter: 0.0888078 sec (400 iters avg) -- Total time: 35.523120000000006 sec\r\n[np] Time/iter: 0.0664769 sec (400 iters avg) -- Total time: 26.590760000000003 sec\r\n[tf/np] Time/iter: 1.336 (ratio)\r\n```\r\n\r\nOn CPU:\r\n```bash\r\nTF version: 2.0.0\r\n[tf] Time/iter: 0.0362841 sec (400 iters avg) -- Total time: 14.51364 sec\r\n[np] Time/iter: 0.0663887 sec (400 iters avg) -- Total time: 26.55548 sec\r\n[tf/np] Time/iter: 0.547 (ratio)\r\n```\r\n\r\nI don't know the particular reasons behind, maybe the loading cost from CPU to GPU?\r\n\r\nAnother related question #13603\r\n", "There are known limitations of TF eager execution performance that is being actively worked on by the team. I'd suggest seeing if you can use tf.function to improve performance."]}, {"number": 34143, "title": "Unable to convert .pb to .tflite ", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window 10\r\n- TensorFlow installed from (source or binary): python -m pip install tensorflow==1.15\r\n- TensorFlow version (or github SHA if from source): 1.15\r\n\r\nI am currently trying to convert .pb file from export_tflite_ssd_graph.py into a .tflite file and encountered these error:\r\n\r\n>2019-11-11 06:45:44.042588: F tensorflow/lite/toco/tooling_util.cc:935] Check failed: GetOpWithOutput(model, output_array) Specified output array \"'TFLite_Detection_PostProcess'\" is not produced by any op in this graph. Is it a typo? This should not happen. If you trigger this error please send a bug report (with code to reporduce this error), to the TensorFlow Lite team.\r\nFatal Python error: Aborted\r\n\r\n\r\nThis is what i try to run\r\n```\r\ntflite_convert --output_file=C:/tensorflow1/models/research/object_detection/inference_graph/detect.tflite --graph_def_file=C:/tensorflow1/models/research/object_detection/inference_graph/tflite_graph.pb --inference_type=FLOAT --inference_input_type=QUANTIZED_UINT8 --input_arrays=normalized_input_image_tensor --input_shapes=1,300,300,3 --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --mean_values=128 --std_dev_values=128 --change_concat_input_ranges=false --allow_custom_ops\r\n```\r\n\r\n\r\nLink is the full traceback:\r\nhttps://pastebin.com/xJ1vZGmd\r\n\r\nI've also check from https://stackoverflow.com/questions/52918051/how-to-convert-pb-to-tflite-format  and confirm that the output array is TFLite_Detection_PostProcess\r\n```\r\noutput name =\r\nTFLite_Detection_PostProcess\r\n\r\nInput name =\r\nnormalized_input_image_tensor\r\n```\r\n", "comments": [">Update\r\n\r\ntested my frozen_inference.pb on object_detection_tutorial.ipynb and the inference is working fine. \r\n\r\n![WhatsApp Image 2019-11-11 at 21 45 27](https://user-images.githubusercontent.com/18519104/68642815-4c0f2080-054a-11ea-9636-fac0b8c4cf21.jpeg)\r\n\r\n\r\n", "Hey @Keyrainn , did you add `--add_postprocessing_op=true` to the command when you ran `export_tflite_ssd_graph.py`? If yes, could you paste that entire command? Looks like the .pb file you have exported doesn't have the exact structure the converter expects, so something might be off.", "Hi @srjoglekar246 , i have added ```--add_postprocessing_op=true``` when running ```export_tflite_ssd_graph.py``` and it's still producing the same error.\r\n\r\nbelow is the command that i use to ran ```export_tflite_ssd_graph.py``` :\r\n```python export_tflite_ssd_graph.py --pipeline_config_path=training/ssd_mobilenet_v2_coco.config --trained_checkpoint_prefix training/model.ckpt-10647 --output_directory inference_tflite --add_postprocessing_op=true --max_detections 10```\r\n", "I see the following in your converter command: `--inference_type=FLOAT --inference_input_type=QUANTIZED_UINT8` Is your model quantized? \r\n\r\nIf yes, please provide the following instead:\r\n``` \r\n--input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --inference_type=QUANTIZED_UINT8 --mean_values=129 --std_values=128\r\n```\r\n\r\nIf not:\r\n```\r\n--input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --inference_type=FLOAT\r\n```", "I've trained an unquantized model and added as per your suggestion. but its still producing the same error\r\n\r\n```\r\ntflite_convert --graph_def_file=C:/tensorflow1/models/research/object_detection/inference_tflite/tflite_graph.pb --output_file=C:/tensorflow1/models/research/object_detection/inference_tflite/detect.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --inference_type=FLOAT --input_shapes=\"1,300, 300,3\" --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --allow_custom_ops\r\n```\r\n\r\nI've tried toco as well and its still producing the same error .\r\n``` \r\ntoco --graph_def_file=C:/tensorflow1/models/research/object_detection/inference_tflite/tflite_graph.pb --output_file=C:/tensorflow1/models/research/object_detection/inference_tflite/detect.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --inference_type=FLOAT --input_shapes=\"1,300, 300,3\" --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --allow_custom_ops\r\n```", "@srjoglekar246 i'm not sure what kind of bug i'm facing but to others that having this issue current workaround is to run it in google colab. In colab it will produce the tflite file", "That is strange. So running in colab with the same commands solved it for you as well? ", "yes it solve for me. Currently i am able to do up to training part only. all file conversion to .pb or .tflite is being done on colab ", "I guess the TF version (or the environment) you are using might have some bugs. You could try using our [Python API](https://www.tensorflow.org/lite/convert/python_api) for conversion to fit better into your training+conversion pipeline, but your error seems to indicate an issue with the `export_tflite_ssd_graph` tool. Do you run that script in colab as well?", "yes. only training done on local machine. ", "Hi, do you know any tutorial to use google colab to transform my models? I've done the training, but I have problems transforming it into lite format. \r\n\r\n> yes. only training done on local machine.\r\n\r\n", "@josantoniojl, \r\nPlease refer the Tensorflow lite document for converting model to lite.\r\nhttps://www.tensorflow.org/lite/guide/get_started#2_convert_the_model.", "i did what u said. i got this error.\r\n\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x9d in position 3: invalid start byte", "> I see the following in your converter command: `--inference_type=FLOAT --inference_input_type=QUANTIZED_UINT8` Is your model quantized?\r\n> \r\n> If yes, please provide the following instead:\r\n> \r\n> ```\r\n> --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --inference_type=QUANTIZED_UINT8 --mean_values=129 --std_values=128\r\n> ```\r\n> \r\n> If not:\r\n> \r\n> ```\r\n> --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --inference_type=FLOAT\r\n> ```\r\n\r\ni did what u said. i got this error.\r\n\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x9d in position 3: invalid start byte", "@erolgerceker Which model are you trying to convert? The error indicates something is wrong with the source (TensorFlow) model, which is strange.", "> **System information**\r\n> \r\n> * OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window 10\r\n> * TensorFlow installed from (source or binary): python -m pip install tensorflow==1.15\r\n> * TensorFlow version (or github SHA if from source): 1.15\r\n> \r\n> I am currently trying to convert .pb file from export_tflite_ssd_graph.py into a .tflite file and encountered these error:\r\n> \r\n> > 2019-11-11 06:45:44.042588: F tensorflow/lite/toco/tooling_util.cc:935] Check failed: GetOpWithOutput(model, output_array) Specified output array \"'TFLite_Detection_PostProcess'\" is not produced by any op in this graph. Is it a typo? This should not happen. If you trigger this error please send a bug report (with code to reporduce this error), to the TensorFlow Lite team.\r\n> > Fatal Python error: Aborted\r\n> \r\n> This is what i try to run\r\n> \r\n> ```\r\n> tflite_convert --output_file=C:/tensorflow1/models/research/object_detection/inference_graph/detect.tflite --graph_def_file=C:/tensorflow1/models/research/object_detection/inference_graph/tflite_graph.pb --inference_type=FLOAT --inference_input_type=QUANTIZED_UINT8 --input_arrays=normalized_input_image_tensor --input_shapes=1,300,300,3 --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --mean_values=128 --std_dev_values=128 --change_concat_input_ranges=false --allow_custom_ops\r\n> ```\r\n> \r\n> Link is the full traceback:\r\n> https://pastebin.com/xJ1vZGmd\r\n> \r\n> I've also check from https://stackoverflow.com/questions/52918051/how-to-convert-pb-to-tflite-format and confirm that the output array is TFLite_Detection_PostProcess\r\n> \r\n> ```\r\n> output name =\r\n> TFLite_Detection_PostProcess\r\n> \r\n> Input name =\r\n> normalized_input_image_tensor\r\n> ```\r\n\r\nI was getting the same error, looks like a small change can solve this.\r\nDon't use the quotes in output_arrays i.e (' TFlite_Detection_PostProcess')\r\n\r\ninstead of using\r\n --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' Use: \r\n--output_arrays=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3\r\n\r\nlet me know if the same works for you!\r\n\r\n**Edit** - You won't have to use colab for this, with tf correctly installed, it should work in your local machine.\r\nMy system uses:\r\nTensorflow Version: 1.15 (installed from pip)\r\nOperating System: Windows 10\r\n\r\nwhich is the same as mentioned in the original post of the issue, hence i am hoping it should work! ", "@Keyrainn \r\nPlease let us know if this is still an issue.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34143\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34143\">No</a>\n"]}, {"number": 34142, "title": "Combining packages from different channels", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 18.04\r\n- TensorFlow installed from (source or binary):\r\nconda (using conda-forge)\r\n- TensorFlow version:\r\n1.13.2\r\n- Python version:\r\n2.7.15\r\n- Installed using virtualenv? pip? conda?:\r\nconda\r\n- GCC/Compiler version (if compiling from source):\r\n4.8.3\r\n\r\n**Describe the problem**\r\nI am trying to get a running tensorflow 1.13.2 combined with ROOT (in particular root, root_numpy, and rootpy) in a conda environment. The issue is that there is no one conda channel that contains all of those packages at once but I need to combine two channels. I tried different orders of installing the packages but whenever I start using the second channel, I get this error: (in this case, I first installed all the root stuff, which worked and then added tensorflow)\r\n```\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/anja/anaconda3/envs/new_try/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/anja/anaconda3/envs/new_try/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/anja/anaconda3/envs/new_try/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/anja/anaconda3/envs/new_try/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/anja/anaconda3/envs/new_try/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/anja/anaconda3/envs/new_try/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: /home/anja/anaconda3/envs/new_try/lib/python2.7/site-packages/tensorflow/python/../../../../libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /home/anja/anaconda3/envs/new_try/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n```", "comments": ["@Anjaaaa, Is there a specific reason to use two different channels. \r\nPlease follow the steps mentioned below to install Tenosrflow using conda single channel.\r\nSet Up Anaconda Environments\r\n`conda create --name tf_1.13`\r\nActivate the new Environment\r\n`source activate tf_1.13`\r\nInstall Tf 2.0\r\n`conda install -c conda-forge tensorflow python=2.7`\r\nLet us know if it resolves your issue. Thanks!", "As I said, I want to install it in combination with `root`,`rootpy`, and `root-numpy` and there is no channel that combines all of them. I also tried installing everything from `conda-forge` and add what's missing using `pip` but no success.", "@Anjaaaa,\r\nSorry, but we don't provide support for issues with the conda environment.\r\nPlease follow the instructions mentioned in this [link](https://www.tensorflow.org/install/pip). Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34142\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34142\">No</a>\n"]}, {"number": 34141, "title": "input_shape to build method is None when calling compute_output_shape on layer with overridden build method", "body": "**System information**\r\ncustom code to reproduce is included below.\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 18.04.3\r\n\r\n- TensorFlow installed from (source or binary):\r\npip install tensorflow-gpu\r\n\r\n- TensorFlow version (use command below):\r\nv2.0.0-rc2-26-g64c3d38 2.0.0\r\n\r\n- Python version:\r\n3.6.8\r\n\r\n**Describe the current behavior**\r\nbuild method called with input_shape = None\r\n\r\n**Describe the expected behavior**\r\nBuild method called with correct input shape\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nclass MyLayer(tf.keras.layers.Layer):\r\n    def build(self, input_shape):\r\n\r\n        if input_shape is None:\r\n            print('error: input shape is none')\r\n        else:\r\n            print('build:', input_shape)\r\n\r\n        super().build(input_shape)\r\n\r\n    def call(self, inputs):\r\n        print('call:', inputs.shape)\r\n\r\n        return inputs * 2\r\n\r\ndef main():\r\n\r\n    l = MyLayer()\r\n\r\n    inp = tf.keras.Input(shape=(5, 5, 1))\r\n    l.compute_output_shape((None, 5, 5, 1))\r\n\r\n    o = l(inp)\r\n\r\n    print(\"input_shape:\", l.input_shape)\r\n    print(\"count_params:\", l.count_params())\r\n    print('output:', o.shape)\r\n\r\nif __name__=='__main__':\r\n    main()\r\n```\r\n\r\n**Other info / logs**\r\n\r\nWhen run, program prints this: \r\n```\r\nerror: input shape is none\r\ncall: (None, 5, 5, 1)\r\ncall: (None, 5, 5, 1)\r\ninput_shape: (None, 5, 5, 1)\r\ncount_params: 0\r\noutput: (None, 5, 5, 1)\r\n```\r\n\r\nLooking at the code for compute_output_shape, I noticed that it calls self._mabye_build method, but this method expects actual tensors as inputs, not tensor shapes. So it fumbes and passes in a None. \r\n\r\nSuggested code to fix:\r\nat https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/engine/base_layer.py#L620\r\n\r\nreplace _maybe_build with _maybe_build_from_shapes:\r\n(should probably adjust _maybe_build to call this method also)\r\n```\r\ndef _maybe_build_with_shapes(self, inputshapes):\r\n    # Check input assumptions set before layer building, e.g. input rank.\r\n    if not self.built:\r\n\r\n      # Only call `build` if the user has manually overridden the build method.\r\n      if not hasattr(self.build, '_is_default'):\r\n        # Any setup work performed only once should happen in an `init_scope`\r\n        # to avoid creating symbolic Tensors that will later pollute any eager\r\n        # operations.\r\n        with tf_utils.maybe_init_scope(self):\r\n          self.build(input_shapes)\r\n      # We must set self.built since user defined build functions are not\r\n      # constrained to set self.built.\r\n      self.built = True\r\n\r\n```\r\nI don't know what the code at the end of maybe_build is doing, so don't know if it belongs in this method or should just stay in _maybe_build:\r\n\r\n```\r\n    # Optionally load weight values specified at layer instantiation.\r\n    if getattr(self, '_initial_weights', None) is not None:\r\n      self.set_weights(self._initial_weights)\r\n      self._initial_weights = None\r\n```", "comments": ["Alternate bug fix:\r\n\r\nmove the _maybe_build call after the placeholders have been created:\r\n\r\n```def compute_output_shape(self, input_shape):\r\n    \"\"\"Computes the output shape of the layer.\r\n    If the layer has not been built, this method will call `build` on the\r\n    layer. This assumes that the layer will later be used with inputs that\r\n    match the input shape provided here.\r\n    Arguments:\r\n        input_shape: Shape tuple (tuple of integers)\r\n            or list of shape tuples (one per output tensor of the layer).\r\n            Shape tuples can include None for free dimensions,\r\n            instead of an integer.\r\n    Returns:\r\n        An input shape tuple.\r\n    \"\"\"\r\n    if context.executing_eagerly():\r\n      # In this case we build the model first in order to do shape inference.\r\n      # This is acceptable because the framework only calls\r\n      # `compute_output_shape` on shape values that the layer would later be\r\n      # built for. It would however cause issues in case a user attempts to\r\n      # use `compute_output_shape` manually with shapes that are incompatible\r\n      # with the shape the Layer will be called on (these users will have to\r\n      # implement `compute_output_shape` themselves).\r\n      with context.graph_mode():\r\n        graph = func_graph.FuncGraph('graph')\r\n        with graph.as_default():\r\n          input_shape = tf_utils.convert_shapes(input_shape, to_tuples=False)\r\n          inputs = nest.map_structure(\r\n              base_layer_utils.generate_placeholders_from_shape, input_shape)\r\n          self._maybe_build(inputs)\r\n          try:\r\n            if self._expects_training_arg:\r\n              outputs = self(inputs, training=False)\r\n            else:\r\n              outputs = self(inputs)\r\n          except TypeError:\r\n            raise NotImplementedError('We could not automatically infer '\r\n                                      'the static shape of the layer\\'s output.'\r\n                                      ' Please implement the '\r\n                                      '`compute_output_shape` method on your '\r\n                                      'layer (%s).' % self.__class__.__name__)\r\n      return nest.map_structure(lambda t: t.shape, outputs)\r\n    raise NotImplementedError\r\n```", "I have tried on colab with TF version 2.0 ,2.1.0-dev20191110 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/1bc35ad6876cb9944aa1254e27facfbe/untitled347.ipynb). Thanks!", "I want to modify the PR\r\nThere is an unnecessary file in there.", "There is an unnecessary file in there.", "@chahld Is this still an issue? Can you please check with recent `tf-nightly`. I tried and am not getting that `error: input shape is none`. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/a37cdcc0684c5191ae9d1f7a09f740ad/untitled347.ipynb). \r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "@chahld Can you please verify and close if this was resolved for you. Thanks!", "Yes it appears to be fixed in nightly.\r\n\r\nthanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34141\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34141\">No</a>\n"]}, {"number": 34140, "title": "[tf2.0] Can not load keras saved model with customized loss function", "body": "I run tensorflow.keras on colab.research.google.com. \r\n\r\nIn tf2.0, I trained a model with a customized loss function named Loss, then saved it by keras.Model.save().\r\nWhen I try loading it by \r\n`keras.models.load_model( 'filename', custom_objects = { 'Loss': Loss } )`\r\nIt raises:\r\n> /tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_utils.py in get_loss_function(loss)\r\n>    1092   return losses.LossFunctionWrapper(\r\n>    1093       loss_fn,\r\n> -> 1094       name=loss_fn.__name__,\r\n>    1095       reduction=losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE)\r\n>    1096 \r\n> \r\n> AttributeError: 'Loss' object has no attribute '__name__'\r\n\r\nEven to set Loss.__name__ = 'xxx' before calling load_model does not help.\r\n\r\nIf set the 'compile' param to True for load_model, it may load successfully. But the loaded model can not work on evaluating or predicting.\r\n\r\nTo load the files saved by keras of tf1.5 is all OK. The model is trained by the same customized loss function. The model is saved to a file by tf1.5, while to a path by tf2.0.\r\n\r\nHow can I solve it?", "comments": ["@saintthor ,\r\nCan you share a simple and standalone code to reproduce the issue? Thanks!", "> @saintthor ,\r\n> Can you share a simple and standalone code to reproduce the issue? Thanks!\r\n\r\nplease test this code:\r\n\r\n```\r\ndef Loss( y0, y ):\r\n    return K.mean( K.maximum( K.abs( y0 / ( K.abs( y ) + 0.1 )), K.abs( y / ( K.abs( y0 ) + 0.1 ))))\r\n\r\nInputLayer = tf.keras.layers.Input( dtype = 'float32', shape = ( 4, ))\r\nOutput = tf.keras.layers.Dense( 1, use_bias = True )( InputLayer )\r\nmodel = tf.keras.models.Model( inputs = [InputLayer], outputs=[Output] )\r\nmodel.compile( loss = Loss, optimizer = tf.keras.optimizers.Adam())\r\nXData = np.random.random(( 100, 4 ))\r\nYData = np.random.random(( 100, ))\r\nmodel.fit( [XData], [YData], epochs = 5, batch_size = 5, verbose=2 )\r\nmodel.save( 'mo.test' )\r\ntf.keras.models.load_model( 'mo.test' )\r\n```\r\n\r\nthe last line raises:\r\n```\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-23-23b454583e49> in <module>()\r\n     10 model.fit( [XData], [YData], epochs = 5, batch_size = 5, verbose=2 )\r\n     11 model.save( 'mo.test' )\r\n---> 12 tf.keras.models.load_model( 'mo.test' )\r\n\r\n6 frames\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/saving/save.py in load_model(filepath, custom_objects, compile)\r\n    148   if isinstance(filepath, six.string_types):\r\n    149     loader_impl.parse_saved_model(filepath)\r\n--> 150     return saved_model_load.load(filepath, compile)\r\n    151 \r\n    152   raise IOError(\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/saving/saved_model/load.py in load(path, compile)\r\n     91     if model._training_config is not None:  # pylint: disable=protected-access\r\n     92       model.compile(**saving_utils.compile_args_from_training_config(\r\n---> 93           model._training_config))  # pylint: disable=protected-access\r\n     94 \r\n     95   return model\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\r\n    334     # Prepare list of loss functions, same size of model outputs.\r\n    335     self.loss_functions = training_utils.prepare_loss_functions(\r\n--> 336         self.loss, self.output_names)\r\n    337 \r\n    338     target_tensors = self._process_target_tensor_for_compile(target_tensors)\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_utils.py in prepare_loss_functions(loss, output_names)\r\n   1349       loss_functions.append(get_loss_function(loss.get(name, None)))\r\n   1350   elif isinstance(loss, six.string_types):\r\n-> 1351     loss_functions = [get_loss_function(loss) for _ in output_names]\r\n   1352   elif isinstance(loss, collections_abc.Sequence):\r\n   1353     if len(loss) != len(output_names):\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_utils.py in <listcomp>(.0)\r\n   1349       loss_functions.append(get_loss_function(loss.get(name, None)))\r\n   1350   elif isinstance(loss, six.string_types):\r\n-> 1351     loss_functions = [get_loss_function(loss) for _ in output_names]\r\n   1352   elif isinstance(loss, collections_abc.Sequence):\r\n   1353     if len(loss) != len(output_names):\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_utils.py in get_loss_function(loss)\r\n   1092   return losses.LossFunctionWrapper(\r\n   1093       loss_fn,\r\n-> 1094       name=loss_fn.__name__,\r\n   1095       reduction=losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE)\r\n   1096 \r\n\r\nAttributeError: 'Loss' object has no attribute '__name__'\r\n```\r\n\r\nthanks.", "changing the last line to\r\n`tf.keras.models.load_model( 'mo.test', custom_objects = { 'Loss': Loss } )\r\n`raises the same error.", "@saintthor ,\r\nCan you please provide complete code so that we can reproduce the issue from our end?Thanks!", "> @saintthor ,\r\n> Can you please provide complete code so that we can reproduce the issue from our end?Thanks!\r\n\r\nThe above code is almost complete. See the follow code. I run it in google colab.\r\n```\r\n\r\n%tensorflow_version 2.x\r\nimport numpy as np\r\nimport tensorflow as tf\r\nK = tf.keras.backend\r\n\r\ndef Loss( y0, y ):\r\n    return K.mean( K.maximum( K.abs( y0 / ( K.abs( y ) + 0.1 )), K.abs( y / ( K.abs( y0 ) + 0.1 ))))\r\n\r\nInputLayer = tf.keras.layers.Input( dtype = 'float32', shape = ( 4, ))\r\nOutput = tf.keras.layers.Dense( 1, use_bias = True )( InputLayer )\r\nmodel = tf.keras.models.Model( inputs = [InputLayer], outputs=[Output] )\r\nmodel.compile( loss = Loss, optimizer = tf.keras.optimizers.Adam())\r\nXData = np.random.random(( 100, 4 ))\r\nYData = np.random.random(( 100, ))\r\nmodel.fit( [XData], [YData], epochs = 5, batch_size = 5, verbose=2 )\r\nmodel.save( 'mo.test' )\r\n#each of the follow lines raises error\r\n#tf.keras.models.load_model( 'mo.test', custom_objects = { 'Loss': Loss } )\r\ntf.keras.models.load_model( 'mo.test' )\r\n```", "Issue replicating for TF-2.0 and tf-nightly. kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/d2a2ccbc84474b6fc693ebcad55f049c/34140.ipynb) of colab.Thanks!", "@saintthor Approach to load a model when a custom object was used is little different. First, you need to load the saved model with `custom_objects` with `compile = False`. Then compile the model with the `custom_objects` as shown below.\r\n\r\n```\r\nmodel.save( 'mo.test' )\r\n#each of the follow lines raises error\r\nnew_model =  tf.keras.models.load_model( 'mo.test', custom_objects = { 'Loss': Loss },compile=False)\r\nnew_model.compile( loss = Loss, optimizer = tf.keras.optimizers.Adam())\r\n#tf.keras.models.load_model( 'mo.test' )\r\n```\r\n\r\nHope this helps. Please close the issue if the issue was resolved. Thanks!\r\n[Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/ef405b1200a5a7e087408b693273f65b/34140.ipynb) is the gist for your reference. Thanks!", "> @saintthor Approach to load a model when a custom object was used is little different. First, you need to load the saved model with `custom_objects` and `compile = False`. Then compile the model with the `custom_objects` as shown below.\r\n> \r\n> ```\r\n> model.save( 'mo.test' )\r\n> #each of the follow lines raises error\r\n> new_model =  tf.keras.models.load_model( 'mo.test', custom_objects = { 'Loss': Loss },compile=False)\r\n> new_model.compile( loss = Loss, optimizer = tf.keras.optimizers.Adam())\r\n> #tf.keras.models.load_model( 'mo.test' )\r\n> ```\r\n> \r\n> Hope this helps. Please close the issue if the issue was resolved. Thanks!\r\n> [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/ef405b1200a5a7e087408b693273f65b/34140.ipynb) is the gist for your reference. Thanks!\r\n\r\nthank you. this solution works well.\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34140\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34140\">No</a>\n", "> @saintthor Approach to load a model when a custom object was used is little different. First, you need to load the saved model with `custom_objects` with `compile = False`. Then compile the model with the `custom_objects` as shown below.\r\n> \r\n> ```\r\n> model.save( 'mo.test' )\r\n> #each of the follow lines raises error\r\n> new_model =  tf.keras.models.load_model( 'mo.test', custom_objects = { 'Loss': Loss },compile=False)\r\n> new_model.compile( loss = Loss, optimizer = tf.keras.optimizers.Adam())\r\n> #tf.keras.models.load_model( 'mo.test' )\r\n> ```\r\n> \r\n> Hope this helps. Please close the issue if the issue was resolved. Thanks!\r\n> [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/ef405b1200a5a7e087408b693273f65b/34140.ipynb) is the gist for your reference. Thanks!\r\n\r\n@jvishnuvardhan If I compile the model after loading it, won't I lose the old saved Optimizer state? With a newly intialized optimizer, if I try to continue my training process, that might cause the loss to increase again.", "@rdutta1999 In an earlier version we need to do that way. But with the recent version `tf-nightly`, we don't need pass `compile=False`. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/f555eae93e9a2d71d8f9530e0bde8f64/34140.ipynb).\r\n\r\nPlease check the guide on saving and loading [here](https://www.tensorflow.org/tutorials/keras/save_and_load#saving_custom_objects). Thanks!\r\n\r\nIf there are any further questions, please feel free to open a new issue with a standalone code to reproduce your issue so that it is better for the community who might learn from your questions. Thanks again!"]}, {"number": 34139, "title": "Only one GPU is used during .fit() validation phase", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow version (use command below): 2.0.0\r\n\r\n**Describe the current behavior**\r\n\r\nDuring validation phase of Keras' `.fit()`, only one GPU is used (while all GPUs are used during the training phase).\r\n\r\n**Code to reproduce the issue**\r\nThis issue can be easily reproduced using the code of [this official tutorial](https://www.tensorflow.org/tutorials/distribute/keras), by feeding the validation dataset `eval_dataset` to `model.fit()`.\r\n", "comments": ["How did you determine if was using only one GPU during validation? Can you also share more information about your setup? This is using MirroredStrategy? ", "cc @qlzh727 ", "@guptapriya I used `nvidia-smi` to observe GPU usage, only one GPU is used during the entire validation phase of `.fit()`. Training was launched on 4 GPUs using `tf.distribute.MirroredStrategy()`. \r\n\r\nNot sure if it's related to this issue on Keras' repo: https://github.com/keras-team/keras/issues/9404.", "from device placement logs it seems that validation happens on all GPUs under conditions mentioned in  keras-team/keras#9404 (num_data=7,num_gpu=3,batch_size=6).\r\n\r\nnvidia-smi may not be accurate if the dataset is small. Could you try again with a larger validation set (by adding a .repeat(N) to the dataset).", "Closing inactive issues. Feel free to reopen.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34139\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34139\">No</a>\n", "I seem to have the same problem. TF 2.4, keras. tf.data. Zero usage according to nvidia-smi and the validation procedure is obviously very slow. "]}, {"number": 34138, "title": "fixed KeyError issue for tf.Module.variables", "body": "Fixed `KeyError` issue caused by the dictionary that the `__iter__` function does not iterate over dictionary keys when it is assigned to a class that inherit from`tf.Module` as an attribute.\r\n\r\nUsing `dict.keys()` to iterate over dictionary keys is much safer than `dict.__iter__`, since `dict.keys()` is a common interface for `dict` and for those key/value paired containers inherited from `collections.abc.Mapping` (which is used in the file [nest.py#L194](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/util/nest.py#L194)) to retrieve keys, and its functionality is clearly stated in [cpython/_collections_abc.py#L673](https://github.com/python/cpython/blob/master/Lib/_collections_abc.py#L673), while `__iter__` is just an abstract method that have no clear definitions.\r\n\r\nHere is the minimal code that would cause the `KeyError` exception when calling `tf.Module.variables`\r\n```python\r\nimport tensorflow as tf #tf2.0\r\nfrom collections import OrderedDict\r\n\r\nclass MyNamedTuple(OrderedDict):\r\n    def __init__(self, *args, **kwargs):\r\n        super(MyNamedTuple, self).__init__(*args, **kwargs)\r\n        self.__dict__ = self\r\n    def __iter__(self):  # iterate over values (not keys)\r\n        for v in self.values():\r\n            yield v\r\n          \r\nclass MyModule(tf.Module):\r\n    def __call__(self):\r\n        self.myargs = MyNamedTuple({'arg': 'hello'}) # cause KeyError\r\n        self.v = tf.Variable([1, 2, 3]) # some variables\r\n        \r\nmodule = MyModule('my_module')\r\nmodule()\r\nmy_vars = module.variables #KeyError\r\n```\r\n\r\nAnd the exception traceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"keyerror.py\", line 16, in <module>\r\n    my_vars = module.variables #KeyError\r\n  File \"/home/joehsiao/lib/venv/tf2_env/lib/python3.6/site-packages/tensorflow_core/python/module/module.py\", line 155, in variables\r\n    return tuple(self._flatten(predicate=_is_variable))\r\n  File \"/home/joehsiao/lib/venv/tf2_env/lib/python3.6/site-packages/tensorflow_core/python/module/module.py\", line 339, in _flatten_module\r\n    for leaf_path, leaf in nest.flatten_with_tuple_paths(module_dict[key]):\r\n  File \"/home/joehsiao/lib/venv/tf2_env/lib/python3.6/site-packages/tensorflow_core/python/util/nest.py\", line 1326, in flatten_with_tuple_paths\r\n    flatten(structure, expand_composites=expand_composites)))\r\n  File \"/home/joehsiao/lib/venv/tf2_env/lib/python3.6/site-packages/tensorflow_core/python/util/nest.py\", line 1275, in yield_flat_paths\r\n    for k, _ in _yield_flat_up_to(nest, nest, is_seq):\r\n  File \"/home/joehsiao/lib/venv/tf2_env/lib/python3.6/site-packages/tensorflow_core/python/util/nest.py\", line 640, in _yield_flat_up_to\r\n    input_tree = dict(_yield_sorted_items(input_tree))\r\n  File \"/home/joehsiao/lib/venv/tf2_env/lib/python3.6/site-packages/tensorflow_core/python/util/nest.py\", line 191, in _yield_sorted_items\r\n    yield key, iterable[key]\r\nKeyError: 'hello'\r\n```", "comments": ["Thanks for both the PR and the very thorough explanation of why it is necessary.", "@robieta  Can you please take a look on this PR? Thanks!", "Ah, I realize now that it won't start the process without an approving review, even with the \"ready to pull\" label."]}, {"number": 34137, "title": "Updated DeviceSpec docstring and fixed typos", "body": "Updated docstring (as per https://github.com/tensorflow/tensorflow/issues/34124), adding instructions for using `device_spec.to_string()` method with eager execution enabled. Fixed missing parentheses in example.", "comments": ["Please make the PR against master branch, not r2.0.", "ah, silly me. Yep will fix it.\r\n", "You can add me as reviewer to it", "@mihaimaruseac https://github.com/tensorflow/tensorflow/pull/34162"]}, {"number": 34136, "title": "TensorRT Segmentation Fault During Conversion", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow):**\r\n  Yes, the code I'm running is taken from [here](https://github.com/qubvel/efficientnet). I used the script [here](https://github.com/aaroey/tensorflow/blob/tftrt20/tftrt20/test.py) provided by @aaroey and replaced **mobilenet** by **efficientnet**\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**\r\nUbuntu 16.04.3 LTS\r\n**- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:**\r\n**- TensorFlow installed from (source or binary):**\r\n**- TensorFlow version (use command below):**\r\nv2.0.0-rc2-26-g64c3d38 2.0.0\r\n**- Python version:**\r\nPython 3.5.2\r\n**- Bazel version (if compiling from source):**\r\nNot Applicable\r\n**- GCC/Compiler version (if compiling from source):**\r\n**- CUDA/cuDNN version:** \r\nCUDA: 10.0; CuDNN: 7.6.4\r\n**- GPU model and memory:**\r\nTesla M60 (AWS)\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nRunning the script attached above and provided by @aaroey with efficientnet give segmentation fault\r\n\r\n**Describe the expected behavior**\r\nRunning the same script provided above as is run smoothly\r\n\r\n**Code to reproduce the issue**\r\nRun the following code:\r\n\r\n`import datetime\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.compiler.tensorrt import trt_convert as trt\r\n\r\nmsgs = []\r\ninp = np.random.random_sample([1, 224, 224, 3]).astype(np.float32)\r\ninpconst = tf.constant(inp)\r\n\r\n\r\ndef run_and_time(saved_model_dir, ref_result=None):\r\n  \"\"\"Helper method to measure the running time of a SavedModel.\"\"\"\r\n  NUM_RUNS = 100\r\n  root = tf.saved_model.load(saved_model_dir)\r\n  concrete_func = root.signatures[\"serving_default\"]\r\n  result = None\r\n  for _ in range(2):  # warm up\r\n    concrete_func(input_1=inpconst)\r\n\r\n  start_time = datetime.datetime.now()\r\n  for i in range(NUM_RUNS):\r\n    result = concrete_func(input_1=inpconst)\r\n  end_time = datetime.datetime.now()\r\n\r\n  elapsed = end_time - start_time\r\n  result = result[list(result.keys())[0]]\r\n\r\n  msgs.append(\"------> time for %d runs: %s\" % (NUM_RUNS, str(elapsed)))\r\n  if ref_result is not None:\r\n    msgs.append(\r\n        \"------> max diff: %s\" % str(np.max(np.abs(result - ref_result))))\r\n  return result\r\n\r\n\r\n\r\nsaved_model_dir = \"/tmp/efficientnet.original\"\r\n\r\nimport efficientnet.tfkeras as efn\r\nmodel = efn.EfficientNetB0(weights='imagenet')\r\ntf.saved_model.save(model, saved_model_dir)\r\n\r\n\r\nparams = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\r\n    precision_mode='FP16',\r\n    is_dynamic_op=True,\r\n    minimum_segment_size=10)\r\n\r\nconverter = trt.TrtGraphConverterV2(\r\n    input_saved_model_dir=saved_model_dir,\r\n    conversion_params=params)\r\n\r\nconverter.convert()\r\nsaved_model_dir_trt = \"/tmp/efficientnet.trt\"\r\nconverter.save(saved_model_dir_trt)\r\nref_result = run_and_time(saved_model_dir)\r\nrun_and_time(saved_model_dir_trt, ref_result)\r\nfor m in msgs:\r\n  print(m)`\r\n\r\n\r\n**Other info / logs**\r\nusing GDB:\r\n\r\ngdb python\r\nGNU gdb (Ubuntu 7.11.1-0ubuntu1~16.5) 7.11.1\r\nCopyright (C) 2016 Free Software Foundation, Inc.\r\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\r\nThis is free software: you are free to change and redistribute it.\r\nThere is NO WARRANTY, to the extent permitted by law.  Type \"show copying\"\r\nand \"show warranty\" for details.\r\nThis GDB was configured as \"x86_64-linux-gnu\".\r\nType \"show configuration\" for configuration details.\r\nFor bug reporting instructions, please see:\r\n<http://www.gnu.org/software/gdb/bugs/>.\r\nFind the GDB manual and other documentation resources online at:\r\n<http://www.gnu.org/software/gdb/documentation/>.\r\nFor help, type \"help\".\r\nType \"apropos word\" to search for commands related to \"word\"...\r\nReading symbols from python...(no debugging symbols found)...done.\r\n(gdb) run effi.py \r\nStarting program: /home/ubuntu/tf20/bin/python effi.py\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n[New Thread 0x7ffff43e0700 (LWP 7815)]\r\n[New Thread 0x7ffff3bdf700 (LWP 7816)]\r\n[New Thread 0x7fffef3de700 (LWP 7817)]\r\n[New Thread 0x7fffecbdd700 (LWP 7818)]\r\n[New Thread 0x7fffea3dc700 (LWP 7819)]\r\n[New Thread 0x7fffe7bdb700 (LWP 7820)]\r\n[New Thread 0x7fffe53da700 (LWP 7821)]\r\n[New Thread 0x7fffe2bd9700 (LWP 7822)]\r\n[New Thread 0x7fffe03d8700 (LWP 7823)]\r\n[New Thread 0x7fffddbd7700 (LWP 7824)]\r\n[New Thread 0x7fffdd3d6700 (LWP 7825)]\r\n[New Thread 0x7fffdabd5700 (LWP 7826)]\r\n[New Thread 0x7fffda3d4700 (LWP 7827)]\r\n[New Thread 0x7fffd5bd3700 (LWP 7828)]\r\n[New Thread 0x7fffd33d2700 (LWP 7829)]\r\n[Thread 0x7fffdd3d6700 (LWP 7825) exited]\r\n[Thread 0x7fffddbd7700 (LWP 7824) exited]\r\n[Thread 0x7fffe03d8700 (LWP 7823) exited]\r\n[Thread 0x7fffe2bd9700 (LWP 7822) exited]\r\n[Thread 0x7fffd5bd3700 (LWP 7828) exited]\r\n[Thread 0x7fffda3d4700 (LWP 7827) exited]\r\n[Thread 0x7fffdabd5700 (LWP 7826) exited]\r\n[Thread 0x7fffe53da700 (LWP 7821) exited]\r\n[Thread 0x7fffe7bdb700 (LWP 7820) exited]\r\n[Thread 0x7fffd33d2700 (LWP 7829) exited]\r\n[Thread 0x7fffea3dc700 (LWP 7819) exited]\r\n[Thread 0x7fffecbdd700 (LWP 7818) exited]\r\n[Thread 0x7fffef3de700 (LWP 7817) exited]\r\n[Thread 0x7ffff3bdf700 (LWP 7816) exited]\r\n[Thread 0x7ffff43e0700 (LWP 7815) exited]\r\n[New Thread 0x7fffd33d2700 (LWP 7833)]\r\n[New Thread 0x7fffd5bd3700 (LWP 7834)]\r\n[New Thread 0x7fffda3d4700 (LWP 7835)]\r\n[New Thread 0x7fffdabd5700 (LWP 7836)]\r\n[New Thread 0x7fff7b306700 (LWP 7837)]\r\n[New Thread 0x7fff78b05700 (LWP 7838)]\r\n[New Thread 0x7fff76304700 (LWP 7839)]\r\n[New Thread 0x7fff73b03700 (LWP 7840)]\r\n[New Thread 0x7fff73302700 (LWP 7841)]\r\n[New Thread 0x7fff70b01700 (LWP 7842)]\r\n[New Thread 0x7fff6e300700 (LWP 7843)]\r\n[New Thread 0x7fff6daff700 (LWP 7844)]\r\n[New Thread 0x7fff672fe700 (LWP 7845)]\r\n[New Thread 0x7fff64afd700 (LWP 7846)]\r\n[New Thread 0x7fff642fc700 (LWP 7847)]\r\n2019-11-10 08:50:49.862529: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n[New Thread 0x7fff52cd2700 (LWP 7848)]\r\n2019-11-10 08:50:49.866720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-10 08:50:49.867387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: Tesla M60 major: 5 minor: 2 memoryClockRate(GHz): 1.1775\r\npciBusID: 0000:00:1e.0\r\n2019-11-10 08:50:49.872638: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-11-10 08:50:49.879677: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-11-10 08:50:49.885164: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-11-10 08:50:49.889850: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-11-10 08:50:49.898681: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-11-10 08:50:49.907196: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-11-10 08:50:49.916626: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-11-10 08:50:49.916725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-10 08:50:49.917360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-10 08:50:49.918021: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-11-10 08:50:49.918338: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n[New Thread 0x7fff52257700 (LWP 7849)]\r\n[New Thread 0x7fff51a56700 (LWP 7850)]\r\n[New Thread 0x7fff51255700 (LWP 7851)]\r\n[New Thread 0x7fff50a54700 (LWP 7852)]\r\n[New Thread 0x7fff0d9c1700 (LWP 7853)]\r\n[New Thread 0x7fff0d1c0700 (LWP 7854)]\r\n[New Thread 0x7fff0c9bf700 (LWP 7855)]\r\n[New Thread 0x7ffeeffff700 (LWP 7856)]\r\n[New Thread 0x7ffee7fff700 (LWP 7857)]\r\n[New Thread 0x7ffeef7fe700 (LWP 7858)]\r\n[New Thread 0x7ffeeeffd700 (LWP 7859)]\r\n[New Thread 0x7ffeee7fc700 (LWP 7860)]\r\n[New Thread 0x7ffeedffb700 (LWP 7861)]\r\n[New Thread 0x7ffeed7fa700 (LWP 7862)]\r\n[New Thread 0x7ffeecff9700 (LWP 7863)]\r\n[New Thread 0x7ffee77fe700 (LWP 7864)]\r\n[New Thread 0x7ffee6ffd700 (LWP 7865)]\r\n[New Thread 0x7ffee67fc700 (LWP 7866)]\r\n[New Thread 0x7ffee5ffb700 (LWP 7867)]\r\n2019-11-10 08:50:49.927704: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300095000 Hz\r\n[Thread 0x7ffee67fc700 (LWP 7866) exited]\r\n[New Thread 0x7ffee67fc700 (LWP 7868)]\r\n[New Thread 0x7ffee57fa700 (LWP 7869)]\r\n[New Thread 0x7ffee4ff9700 (LWP 7870)]\r\n[New Thread 0x7ffeaffff700 (LWP 7871)]\r\n[New Thread 0x7ffeaf7fe700 (LWP 7872)]\r\n[New Thread 0x7ffeaeffd700 (LWP 7873)]\r\n[New Thread 0x7ffeae7fc700 (LWP 7874)]\r\n[New Thread 0x7ffeadffb700 (LWP 7875)]\r\n[New Thread 0x7ffead7fa700 (LWP 7876)]\r\n[New Thread 0x7ffeacff9700 (LWP 7877)]\r\n[New Thread 0x7ffe8ffff700 (LWP 7878)]\r\n[New Thread 0x7ffe8f7fe700 (LWP 7879)]\r\n[New Thread 0x7ffe8effd700 (LWP 7880)]\r\n[New Thread 0x7ffe8e7fc700 (LWP 7881)]\r\n[New Thread 0x7ffe8dffb700 (LWP 7882)]\r\n[New Thread 0x7ffe8d7fa700 (LWP 7883)]\r\n2019-11-10 08:50:49.929881: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5181b90 executing computations on platform Host. Devices:\r\n2019-11-10 08:50:49.929908: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n[New Thread 0x7ffe8cff9700 (LWP 7884)]\r\n[New Thread 0x7ffe6ffff700 (LWP 7885)]\r\n[New Thread 0x7ffe6f7fe700 (LWP 7886)]\r\n[New Thread 0x7ffe6effd700 (LWP 7887)]\r\n[New Thread 0x7ffe6e7fc700 (LWP 7888)]\r\n2019-11-10 08:50:49.991708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-10 08:50:49.992298: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x51c4a30 executing computations on platform CUDA. Devices:\r\n2019-11-10 08:50:49.992329: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla M60, Compute Capability 5.2\r\n[Thread 0x7ffe6f7fe700 (LWP 7886) exited]\r\n2019-11-10 08:50:49.992513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-10 08:50:49.993023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: Tesla M60 major: 5 minor: 2 memoryClockRate(GHz): 1.1775\r\npciBusID: 0000:00:1e.0\r\n2019-11-10 08:50:49.993088: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-11-10 08:50:49.993123: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-11-10 08:50:49.993156: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-11-10 08:50:49.993185: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-11-10 08:50:49.993214: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-11-10 08:50:49.993241: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-11-10 08:50:49.993307: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-11-10 08:50:49.993392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-10 08:50:49.993922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-10 08:50:49.994402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-11-10 08:50:49.994475: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-11-10 08:50:49.995502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-11-10 08:50:49.995527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-11-10 08:50:49.995545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-11-10 08:50:49.995994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-10 08:50:49.996530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-10 08:50:49.997043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7162 MB memory) -> physical GPU (device: 0, name: Tesla M60, pci bus id: 0000:00:1e.0, compute capability: 5.2)\r\n[New Thread 0x7ffe6f7fe700 (LWP 7889)]\r\n[New Thread 0x7ffe6dffb700 (LWP 7890)]\r\n[New Thread 0x7ffe6d7fa700 (LWP 7891)]\r\n[New Thread 0x7ffe6cff9700 (LWP 7892)]\r\n[New Thread 0x7ffe4ffff700 (LWP 7893)]\r\n[New Thread 0x7ffe4f7fe700 (LWP 7894)]\r\n[New Thread 0x7ffe4effd700 (LWP 7895)]\r\n[New Thread 0x7ffe4e7fc700 (LWP 7896)]\r\n[New Thread 0x7ffe4dffb700 (LWP 7897)]\r\n[New Thread 0x7ffe4d7fa700 (LWP 7898)]\r\n[New Thread 0x7ffe4cff9700 (LWP 7899)]\r\n[New Thread 0x7ffe2ffff700 (LWP 7900)]\r\n[New Thread 0x7ffe2f7fe700 (LWP 7901)]\r\n[New Thread 0x7ffe2effd700 (LWP 7902)]\r\n[New Thread 0x7ffe2e7fc700 (LWP 7903)]\r\n[New Thread 0x7ffe2dffb700 (LWP 7904)]\r\n[New Thread 0x7ffe2d7fa700 (LWP 7905)]\r\n[New Thread 0x7ffe2cff9700 (LWP 7906)]\r\n[New Thread 0x7ffe07a4e700 (LWP 7907)]\r\n[New Thread 0x7ffdf2027700 (LWP 7908)]\r\n[Thread 0x7ffdf2027700 (LWP 7908) exited]\r\n2019-11-10 08:51:09.523160: W tensorflow/python/util/util.cc:299] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW1110 08:51:22.750380 140737353942784 deprecation.py:506] From /home/ubuntu/tf20/lib/python3.5/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\n2019-11-10 08:51:28.819439: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.5\r\n[New Thread 0x7ffdf2027700 (LWP 7909)]\r\n[New Thread 0x7ffdd5d34700 (LWP 7910)]\r\n[New Thread 0x7ffdcffff700 (LWP 7911)]\r\n[Thread 0x7ffdcffff700 (LWP 7911) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 7910) exited]\r\n[Thread 0x7ffdf2027700 (LWP 7909) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 7912)]\r\n[New Thread 0x7ffdd5d34700 (LWP 7913)]\r\n[New Thread 0x7ffdf2027700 (LWP 7914)]\r\n[Thread 0x7ffdd5d34700 (LWP 7913) exited]\r\n[Thread 0x7ffdcffff700 (LWP 7912) exited]\r\n[Thread 0x7ffdf2027700 (LWP 7914) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 7915)]\r\n[New Thread 0x7ffdd5d34700 (LWP 7916)]\r\n[New Thread 0x7ffdcffff700 (LWP 7917)]\r\n[Thread 0x7ffdcffff700 (LWP 7917) exited]\r\n[Thread 0x7ffdf2027700 (LWP 7915) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 7916) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 7918)]\r\n[New Thread 0x7ffdd5d34700 (LWP 7919)]\r\n[New Thread 0x7ffdf2027700 (LWP 7920)]\r\n[Thread 0x7ffdf2027700 (LWP 7920) exited]\r\n[Thread 0x7ffdcffff700 (LWP 7918) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 7919) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 7921)]\r\n[New Thread 0x7ffdd5d34700 (LWP 7922)]\r\n[New Thread 0x7ffdcffff700 (LWP 7923)]\r\n[Thread 0x7ffdd5d34700 (LWP 7922) exited]\r\n[Thread 0x7ffdf2027700 (LWP 7921) exited]\r\n[Thread 0x7ffdcffff700 (LWP 7923) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 7924)]\r\n[New Thread 0x7ffdd5d34700 (LWP 7925)]\r\n[New Thread 0x7ffdf2027700 (LWP 7926)]\r\n[Thread 0x7ffdf2027700 (LWP 7926) exited]\r\n[Thread 0x7ffdcffff700 (LWP 7924) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 7925) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 7927)]\r\n[New Thread 0x7ffdd5d34700 (LWP 7928)]\r\n[New Thread 0x7ffdcffff700 (LWP 7929)]\r\n[Thread 0x7ffdcffff700 (LWP 7929) exited]\r\n[Thread 0x7ffdf2027700 (LWP 7927) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 7928) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 7930)]\r\n[New Thread 0x7ffdd5d34700 (LWP 7931)]\r\n[New Thread 0x7ffdf2027700 (LWP 7932)]\r\n[Thread 0x7ffdf2027700 (LWP 7932) exited]\r\n[Thread 0x7ffdcffff700 (LWP 7930) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 7931) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 7933)]\r\n[New Thread 0x7ffdd5d34700 (LWP 7934)]\r\n[New Thread 0x7ffdcffff700 (LWP 7935)]\r\n[Thread 0x7ffdcffff700 (LWP 7935) exited]\r\n[Thread 0x7ffdf2027700 (LWP 7933) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 7934) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 7936)]\r\n[New Thread 0x7ffdd5d34700 (LWP 7937)]\r\n[New Thread 0x7ffdf2027700 (LWP 7938)]\r\n[Thread 0x7ffdf2027700 (LWP 7938) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 7937) exited]\r\n[Thread 0x7ffdcffff700 (LWP 7936) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 7939)]\r\n[New Thread 0x7ffdd5d34700 (LWP 7940)]\r\n[New Thread 0x7ffdcffff700 (LWP 7941)]\r\n[Thread 0x7ffdcffff700 (LWP 7941) exited]\r\n[Thread 0x7ffdf2027700 (LWP 7939) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 7940) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 7942)]\r\n[New Thread 0x7ffdd5d34700 (LWP 7943)]\r\n[New Thread 0x7ffdf2027700 (LWP 7944)]\r\n[Thread 0x7ffdf2027700 (LWP 7944) exited]\r\n[Thread 0x7ffdcffff700 (LWP 7942) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 7943) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 7945)]\r\n[New Thread 0x7ffdd5d34700 (LWP 7946)]\r\n[New Thread 0x7ffdcffff700 (LWP 7947)]\r\n[Thread 0x7ffdcffff700 (LWP 7947) exited]\r\n[Thread 0x7ffdf2027700 (LWP 7945) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 7946) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 7948)]\r\n[New Thread 0x7ffdd5d34700 (LWP 7949)]\r\n[New Thread 0x7ffdf2027700 (LWP 7950)]\r\n[Thread 0x7ffdf2027700 (LWP 7950) exited]\r\n[Thread 0x7ffdcffff700 (LWP 7948) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 7949) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 7951)]\r\n[New Thread 0x7ffdd5d34700 (LWP 7952)]\r\n[New Thread 0x7ffdcffff700 (LWP 7953)]\r\n[Thread 0x7ffdcffff700 (LWP 7953) exited]\r\n[Thread 0x7ffdf2027700 (LWP 7951) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 7952) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 7954)]\r\n[New Thread 0x7ffdd5d34700 (LWP 7955)]\r\n[New Thread 0x7ffdf2027700 (LWP 7956)]\r\n[Thread 0x7ffdf2027700 (LWP 7956) exited]\r\n[Thread 0x7ffdcffff700 (LWP 7954) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 7955) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 7957)]\r\n[New Thread 0x7ffdd5d34700 (LWP 7958)]\r\n[New Thread 0x7ffdcffff700 (LWP 7959)]\r\n[Thread 0x7ffdcffff700 (LWP 7959) exited]\r\n[Thread 0x7ffdf2027700 (LWP 7957) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 7958) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 7960)]\r\n[New Thread 0x7ffdd5d34700 (LWP 7961)]\r\n[New Thread 0x7ffdf2027700 (LWP 7962)]\r\n[Thread 0x7ffdf2027700 (LWP 7962) exited]\r\n[Thread 0x7ffdcffff700 (LWP 7960) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 7961) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 7963)]\r\n[New Thread 0x7ffdd5d34700 (LWP 7964)]\r\n[New Thread 0x7ffdcffff700 (LWP 7965)]\r\n[Thread 0x7ffdcffff700 (LWP 7965) exited]\r\n[Thread 0x7ffdf2027700 (LWP 7963) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 7964) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 7966)]\r\n[New Thread 0x7ffdd5d34700 (LWP 7967)]\r\n[New Thread 0x7ffdf2027700 (LWP 7968)]\r\n[Thread 0x7ffdf2027700 (LWP 7968) exited]\r\n[Thread 0x7ffdcffff700 (LWP 7966) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 7967) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 7969)]\r\n[New Thread 0x7ffdd5d34700 (LWP 7970)]\r\n[New Thread 0x7ffdcffff700 (LWP 7971)]\r\n[Thread 0x7ffdcffff700 (LWP 7971) exited]\r\n[Thread 0x7ffdf2027700 (LWP 7969) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 7970) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 7972)]\r\n[New Thread 0x7ffdd5d34700 (LWP 7973)]\r\n[New Thread 0x7ffdf2027700 (LWP 7974)]\r\n[Thread 0x7ffdf2027700 (LWP 7974) exited]\r\n[Thread 0x7ffdcffff700 (LWP 7972) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 7973) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 7975)]\r\n[New Thread 0x7ffdd5d34700 (LWP 7976)]\r\n[New Thread 0x7ffdcffff700 (LWP 7977)]\r\n[Thread 0x7ffdcffff700 (LWP 7977) exited]\r\n[Thread 0x7ffdf2027700 (LWP 7975) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 7976) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 7978)]\r\n[New Thread 0x7ffdd5d34700 (LWP 7979)]\r\n[New Thread 0x7ffdf2027700 (LWP 7980)]\r\n[Thread 0x7ffdf2027700 (LWP 7980) exited]\r\n[Thread 0x7ffdcffff700 (LWP 7978) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 7979) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 7981)]\r\n[New Thread 0x7ffdd5d34700 (LWP 7982)]\r\n[New Thread 0x7ffdcffff700 (LWP 7983)]\r\n[Thread 0x7ffdcffff700 (LWP 7983) exited]\r\n[Thread 0x7ffdf2027700 (LWP 7981) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 7982) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 7984)]\r\n[New Thread 0x7ffdd5d34700 (LWP 7985)]\r\n[New Thread 0x7ffdf2027700 (LWP 7986)]\r\n[Thread 0x7ffdf2027700 (LWP 7986) exited]\r\n[Thread 0x7ffdcffff700 (LWP 7984) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 7985) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 7987)]\r\n[New Thread 0x7ffdd5d34700 (LWP 7988)]\r\n[New Thread 0x7ffdcffff700 (LWP 7989)]\r\n[Thread 0x7ffdcffff700 (LWP 7989) exited]\r\n[Thread 0x7ffdf2027700 (LWP 7987) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 7988) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 7990)]\r\n[New Thread 0x7ffdd5d34700 (LWP 7991)]\r\n[New Thread 0x7ffdf2027700 (LWP 7992)]\r\n[Thread 0x7ffdf2027700 (LWP 7992) exited]\r\n[Thread 0x7ffdcffff700 (LWP 7990) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 7991) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 7993)]\r\n[New Thread 0x7ffdd5d34700 (LWP 7994)]\r\n[New Thread 0x7ffdcffff700 (LWP 7995)]\r\n[Thread 0x7ffdcffff700 (LWP 7995) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 7994) exited]\r\n[Thread 0x7ffdf2027700 (LWP 7993) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 7996)]\r\n[New Thread 0x7ffdd5d34700 (LWP 7997)]\r\n[New Thread 0x7ffdf2027700 (LWP 7998)]\r\n[Thread 0x7ffdd5d34700 (LWP 7997) exited]\r\n[Thread 0x7ffdcffff700 (LWP 7996) exited]\r\n[Thread 0x7ffdf2027700 (LWP 7998) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 7999)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8000)]\r\n[New Thread 0x7ffdcffff700 (LWP 8001)]\r\n[Thread 0x7ffdcffff700 (LWP 8001) exited]\r\n[Thread 0x7ffdf2027700 (LWP 7999) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8000) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8002)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8003)]\r\n[New Thread 0x7ffdf2027700 (LWP 8004)]\r\n[Thread 0x7ffdd5d34700 (LWP 8003) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8002) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8004) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8005)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8006)]\r\n[New Thread 0x7ffdcffff700 (LWP 8007)]\r\n[Thread 0x7ffdcffff700 (LWP 8007) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8005) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8006) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8008)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8009)]\r\n[New Thread 0x7ffdf2027700 (LWP 8010)]\r\n[Thread 0x7ffdf2027700 (LWP 8010) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8008) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8009) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8011)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8012)]\r\n[New Thread 0x7ffdcffff700 (LWP 8013)]\r\n[Thread 0x7ffdcffff700 (LWP 8013) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8011) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8012) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8014)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8015)]\r\n[New Thread 0x7ffdf2027700 (LWP 8016)]\r\n[Thread 0x7ffdf2027700 (LWP 8016) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8014) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8015) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8017)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8018)]\r\n[New Thread 0x7ffdcffff700 (LWP 8019)]\r\n[Thread 0x7ffdd5d34700 (LWP 8018) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8017) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8019) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8020)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8021)]\r\n[New Thread 0x7ffdf2027700 (LWP 8022)]\r\n[Thread 0x7ffdf2027700 (LWP 8022) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8020) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8021) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8023)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8024)]\r\n[New Thread 0x7ffdcffff700 (LWP 8025)]\r\n[Thread 0x7ffdcffff700 (LWP 8025) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8023) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8024) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8026)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8027)]\r\n[New Thread 0x7ffdf2027700 (LWP 8028)]\r\n[Thread 0x7ffdd5d34700 (LWP 8027) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8026) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8028) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8029)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8030)]\r\n[New Thread 0x7ffdcffff700 (LWP 8031)]\r\n[Thread 0x7ffdcffff700 (LWP 8031) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8029) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8030) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8032)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8033)]\r\n[New Thread 0x7ffdf2027700 (LWP 8034)]\r\n[Thread 0x7ffdd5d34700 (LWP 8033) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8032) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8034) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8035)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8036)]\r\n[New Thread 0x7ffdcffff700 (LWP 8037)]\r\n[Thread 0x7ffdcffff700 (LWP 8037) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8035) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8036) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8038)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8039)]\r\n[New Thread 0x7ffdf2027700 (LWP 8040)]\r\n[Thread 0x7ffdf2027700 (LWP 8040) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8038) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8039) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8041)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8042)]\r\n[New Thread 0x7ffdcffff700 (LWP 8043)]\r\n[Thread 0x7ffdcffff700 (LWP 8043) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8041) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8042) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8044)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8045)]\r\n[New Thread 0x7ffdf2027700 (LWP 8046)]\r\n[Thread 0x7ffdf2027700 (LWP 8046) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8044) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8045) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8047)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8048)]\r\n[New Thread 0x7ffdcffff700 (LWP 8049)]\r\n[Thread 0x7ffdd5d34700 (LWP 8048) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8047) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8049) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8050)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8051)]\r\n[New Thread 0x7ffdf2027700 (LWP 8052)]\r\n[Thread 0x7ffdf2027700 (LWP 8052) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8050) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8051) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8053)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8054)]\r\n[New Thread 0x7ffdcffff700 (LWP 8055)]\r\n[Thread 0x7ffdcffff700 (LWP 8055) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8053) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8054) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8056)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8057)]\r\n[New Thread 0x7ffdf2027700 (LWP 8058)]\r\n[Thread 0x7ffdf2027700 (LWP 8058) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8056) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8057) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8059)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8060)]\r\n[New Thread 0x7ffdcffff700 (LWP 8061)]\r\n[Thread 0x7ffdcffff700 (LWP 8061) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8059) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8060) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8062)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8063)]\r\n[New Thread 0x7ffdf2027700 (LWP 8064)]\r\n[Thread 0x7ffdf2027700 (LWP 8064) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8062) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8063) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8065)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8066)]\r\n[New Thread 0x7ffdcffff700 (LWP 8067)]\r\n[Thread 0x7ffdcffff700 (LWP 8067) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8065) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8066) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8068)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8069)]\r\n[New Thread 0x7ffdf2027700 (LWP 8070)]\r\n[Thread 0x7ffdf2027700 (LWP 8070) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8068) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8069) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8071)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8072)]\r\n[New Thread 0x7ffdcffff700 (LWP 8073)]\r\n[Thread 0x7ffdcffff700 (LWP 8073) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8071) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8072) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8074)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8075)]\r\n[New Thread 0x7ffdf2027700 (LWP 8076)]\r\n[Thread 0x7ffdf2027700 (LWP 8076) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8074) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8075) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8077)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8078)]\r\n[New Thread 0x7ffdcffff700 (LWP 8079)]\r\n[Thread 0x7ffdcffff700 (LWP 8079) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8077) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8078) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8080)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8081)]\r\n[New Thread 0x7ffdf2027700 (LWP 8082)]\r\n[Thread 0x7ffdf2027700 (LWP 8082) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8080) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8081) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8083)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8084)]\r\n[New Thread 0x7ffdcffff700 (LWP 8085)]\r\n[Thread 0x7ffdcffff700 (LWP 8085) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8083) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8084) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8086)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8087)]\r\n[New Thread 0x7ffdf2027700 (LWP 8088)]\r\n[Thread 0x7ffdf2027700 (LWP 8088) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8086) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8087) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8089)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8090)]\r\n[New Thread 0x7ffdcffff700 (LWP 8091)]\r\n[Thread 0x7ffdcffff700 (LWP 8091) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8090) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8089) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8092)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8093)]\r\n[New Thread 0x7ffdf2027700 (LWP 8094)]\r\n[Thread 0x7ffdf2027700 (LWP 8094) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8092) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8093) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8095)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8096)]\r\n[New Thread 0x7ffdcffff700 (LWP 8097)]\r\n[Thread 0x7ffdcffff700 (LWP 8097) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8095) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8096) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8098)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8099)]\r\n[New Thread 0x7ffdf2027700 (LWP 8100)]\r\n[Thread 0x7ffdf2027700 (LWP 8100) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8098) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8099) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8101)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8102)]\r\n[New Thread 0x7ffdcffff700 (LWP 8103)]\r\n[Thread 0x7ffdcffff700 (LWP 8103) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8101) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8102) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8104)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8105)]\r\n[New Thread 0x7ffdf2027700 (LWP 8106)]\r\n[Thread 0x7ffdf2027700 (LWP 8106) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8104) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8105) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8107)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8108)]\r\n[New Thread 0x7ffdcffff700 (LWP 8109)]\r\n[Thread 0x7ffdcffff700 (LWP 8109) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8107) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8108) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8110)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8111)]\r\n[New Thread 0x7ffdf2027700 (LWP 8112)]\r\n[Thread 0x7ffdf2027700 (LWP 8112) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8110) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8111) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8113)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8114)]\r\n[New Thread 0x7ffdcffff700 (LWP 8115)]\r\n[Thread 0x7ffdcffff700 (LWP 8115) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8113) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8114) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8116)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8117)]\r\n[New Thread 0x7ffdf2027700 (LWP 8118)]\r\n[Thread 0x7ffdf2027700 (LWP 8118) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8116) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8117) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8119)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8120)]\r\n[New Thread 0x7ffdcffff700 (LWP 8121)]\r\n[Thread 0x7ffdcffff700 (LWP 8121) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8119) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8120) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8122)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8123)]\r\n[New Thread 0x7ffdf2027700 (LWP 8124)]\r\n[Thread 0x7ffdf2027700 (LWP 8124) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8122) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8123) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8125)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8126)]\r\n[New Thread 0x7ffdcffff700 (LWP 8127)]\r\n[Thread 0x7ffdcffff700 (LWP 8127) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8125) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8126) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8128)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8129)]\r\n[New Thread 0x7ffdf2027700 (LWP 8130)]\r\n[Thread 0x7ffdf2027700 (LWP 8130) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8128) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8129) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8131)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8132)]\r\n[New Thread 0x7ffdcffff700 (LWP 8133)]\r\n[Thread 0x7ffdcffff700 (LWP 8133) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8131) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8132) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8134)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8135)]\r\n[New Thread 0x7ffdf2027700 (LWP 8136)]\r\n[Thread 0x7ffdf2027700 (LWP 8136) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8134) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8135) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8137)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8138)]\r\n[New Thread 0x7ffdcffff700 (LWP 8139)]\r\n[Thread 0x7ffdcffff700 (LWP 8139) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8137) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8138) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8140)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8141)]\r\n[New Thread 0x7ffdf2027700 (LWP 8142)]\r\n[Thread 0x7ffdf2027700 (LWP 8142) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8140) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8141) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8143)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8144)]\r\n[New Thread 0x7ffdcffff700 (LWP 8145)]\r\n[Thread 0x7ffdcffff700 (LWP 8145) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8143) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8144) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8146)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8147)]\r\n[New Thread 0x7ffdf2027700 (LWP 8148)]\r\n[Thread 0x7ffdf2027700 (LWP 8148) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8146) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8147) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8149)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8150)]\r\n[New Thread 0x7ffdcffff700 (LWP 8151)]\r\n[Thread 0x7ffdcffff700 (LWP 8151) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8149) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8150) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8152)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8153)]\r\n[New Thread 0x7ffdf2027700 (LWP 8154)]\r\n[Thread 0x7ffdf2027700 (LWP 8154) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8152) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8153) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8155)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8156)]\r\n[New Thread 0x7ffdcffff700 (LWP 8157)]\r\n[Thread 0x7ffdcffff700 (LWP 8157) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8155) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8156) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8158)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8159)]\r\n[New Thread 0x7ffdf2027700 (LWP 8160)]\r\n[Thread 0x7ffdf2027700 (LWP 8160) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8158) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8159) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8161)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8162)]\r\n[New Thread 0x7ffdcffff700 (LWP 8163)]\r\n[Thread 0x7ffdcffff700 (LWP 8163) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8161) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8162) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8164)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8165)]\r\n[New Thread 0x7ffdf2027700 (LWP 8166)]\r\n[Thread 0x7ffdf2027700 (LWP 8166) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8164) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8165) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8167)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8168)]\r\n[New Thread 0x7ffdcffff700 (LWP 8169)]\r\n[Thread 0x7ffdcffff700 (LWP 8169) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8167) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8168) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8170)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8171)]\r\n[New Thread 0x7ffdf2027700 (LWP 8172)]\r\n[Thread 0x7ffdf2027700 (LWP 8172) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8170) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8171) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8173)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8174)]\r\n[New Thread 0x7ffdcffff700 (LWP 8175)]\r\n[Thread 0x7ffdcffff700 (LWP 8175) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8173) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8174) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8176)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8177)]\r\n[New Thread 0x7ffdf2027700 (LWP 8178)]\r\n[Thread 0x7ffdf2027700 (LWP 8178) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8176) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8177) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8179)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8180)]\r\n[New Thread 0x7ffdcffff700 (LWP 8181)]\r\n[Thread 0x7ffdcffff700 (LWP 8181) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8179) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8180) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8182)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8183)]\r\n[New Thread 0x7ffdf2027700 (LWP 8184)]\r\n[Thread 0x7ffdf2027700 (LWP 8184) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8182) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8183) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8185)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8186)]\r\n[New Thread 0x7ffdcffff700 (LWP 8187)]\r\n[Thread 0x7ffdcffff700 (LWP 8187) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8185) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8186) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8188)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8189)]\r\n[New Thread 0x7ffdf2027700 (LWP 8190)]\r\n[Thread 0x7ffdf2027700 (LWP 8190) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8188) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8189) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8191)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8192)]\r\n[New Thread 0x7ffdcffff700 (LWP 8193)]\r\n[Thread 0x7ffdcffff700 (LWP 8193) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8191) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8192) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8194)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8195)]\r\n[New Thread 0x7ffdf2027700 (LWP 8196)]\r\n[Thread 0x7ffdf2027700 (LWP 8196) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8194) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8195) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8197)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8198)]\r\n[New Thread 0x7ffdcffff700 (LWP 8199)]\r\n[Thread 0x7ffdcffff700 (LWP 8199) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8197) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8198) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8200)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8201)]\r\n[New Thread 0x7ffdf2027700 (LWP 8202)]\r\n[Thread 0x7ffdf2027700 (LWP 8202) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8200) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8201) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8203)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8204)]\r\n[New Thread 0x7ffdcffff700 (LWP 8205)]\r\n[Thread 0x7ffdcffff700 (LWP 8205) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8203) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8204) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8206)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8207)]\r\n[New Thread 0x7ffdf2027700 (LWP 8208)]\r\n[Thread 0x7ffdf2027700 (LWP 8208) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8206) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8207) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8209)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8210)]\r\n[New Thread 0x7ffdcffff700 (LWP 8211)]\r\n[Thread 0x7ffdcffff700 (LWP 8211) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8209) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8210) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8212)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8213)]\r\n[New Thread 0x7ffdf2027700 (LWP 8214)]\r\n[Thread 0x7ffdf2027700 (LWP 8214) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8212) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8213) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8215)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8216)]\r\n[New Thread 0x7ffdcffff700 (LWP 8217)]\r\n[Thread 0x7ffdcffff700 (LWP 8217) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8215) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8216) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8218)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8219)]\r\n[New Thread 0x7ffdf2027700 (LWP 8220)]\r\n[Thread 0x7ffdf2027700 (LWP 8220) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8218) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8219) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8221)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8222)]\r\n[New Thread 0x7ffdcffff700 (LWP 8223)]\r\n[Thread 0x7ffdcffff700 (LWP 8223) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8221) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8222) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8224)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8225)]\r\n[New Thread 0x7ffdf2027700 (LWP 8226)]\r\n[Thread 0x7ffdf2027700 (LWP 8226) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8224) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8225) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8227)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8228)]\r\n[New Thread 0x7ffdcffff700 (LWP 8229)]\r\n[Thread 0x7ffdcffff700 (LWP 8229) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8227) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8228) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8230)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8231)]\r\n[New Thread 0x7ffdf2027700 (LWP 8232)]\r\n[Thread 0x7ffdf2027700 (LWP 8232) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8230) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8231) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8233)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8234)]\r\n[New Thread 0x7ffdcffff700 (LWP 8235)]\r\n[Thread 0x7ffdcffff700 (LWP 8235) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8233) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8234) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8236)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8237)]\r\n[New Thread 0x7ffdf2027700 (LWP 8238)]\r\n[Thread 0x7ffdf2027700 (LWP 8238) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8236) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8237) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8239)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8240)]\r\n[New Thread 0x7ffdcffff700 (LWP 8241)]\r\n[Thread 0x7ffdcffff700 (LWP 8241) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8239) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8240) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8242)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8243)]\r\n[New Thread 0x7ffdf2027700 (LWP 8244)]\r\n[Thread 0x7ffdf2027700 (LWP 8244) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8242) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8243) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8245)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8246)]\r\n[New Thread 0x7ffdcffff700 (LWP 8247)]\r\n[Thread 0x7ffdcffff700 (LWP 8247) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8245) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8246) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8248)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8249)]\r\n[New Thread 0x7ffdf2027700 (LWP 8250)]\r\n[Thread 0x7ffdf2027700 (LWP 8250) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8248) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8249) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8251)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8252)]\r\n[New Thread 0x7ffdcffff700 (LWP 8253)]\r\n[Thread 0x7ffdcffff700 (LWP 8253) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8251) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8252) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8254)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8255)]\r\n[New Thread 0x7ffdf2027700 (LWP 8256)]\r\n[Thread 0x7ffdf2027700 (LWP 8256) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8254) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8255) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8257)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8258)]\r\n[New Thread 0x7ffdcffff700 (LWP 8259)]\r\n[Thread 0x7ffdcffff700 (LWP 8259) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8257) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8258) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8260)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8261)]\r\n[New Thread 0x7ffdf2027700 (LWP 8262)]\r\n[Thread 0x7ffdf2027700 (LWP 8262) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8260) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8261) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8263)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8264)]\r\n[New Thread 0x7ffdcffff700 (LWP 8265)]\r\n[Thread 0x7ffdcffff700 (LWP 8265) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8263) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8264) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8266)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8267)]\r\n[New Thread 0x7ffdf2027700 (LWP 8268)]\r\n[Thread 0x7ffdf2027700 (LWP 8268) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8266) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8267) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8269)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8270)]\r\n[New Thread 0x7ffdcffff700 (LWP 8271)]\r\n[Thread 0x7ffdcffff700 (LWP 8271) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8269) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8270) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8272)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8273)]\r\n[New Thread 0x7ffdf2027700 (LWP 8274)]\r\n[Thread 0x7ffdf2027700 (LWP 8274) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8272) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8273) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8275)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8276)]\r\n[New Thread 0x7ffdcffff700 (LWP 8277)]\r\n[Thread 0x7ffdcffff700 (LWP 8277) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8275) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8276) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8278)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8279)]\r\n[New Thread 0x7ffdf2027700 (LWP 8280)]\r\n[Thread 0x7ffdf2027700 (LWP 8280) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8278) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8279) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8281)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8282)]\r\n[New Thread 0x7ffdcffff700 (LWP 8283)]\r\n[Thread 0x7ffdcffff700 (LWP 8283) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8281) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8282) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8284)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8285)]\r\n[New Thread 0x7ffdf2027700 (LWP 8286)]\r\n[Thread 0x7ffdf2027700 (LWP 8286) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8284) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8285) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8287)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8288)]\r\n[New Thread 0x7ffdcffff700 (LWP 8289)]\r\n[Thread 0x7ffdcffff700 (LWP 8289) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8287) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8288) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8290)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8291)]\r\n[New Thread 0x7ffdf2027700 (LWP 8292)]\r\n[Thread 0x7ffdf2027700 (LWP 8292) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8290) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8291) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8293)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8294)]\r\n[New Thread 0x7ffdcffff700 (LWP 8295)]\r\n[Thread 0x7ffdcffff700 (LWP 8295) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8293) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8294) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8296)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8297)]\r\n[New Thread 0x7ffdf2027700 (LWP 8298)]\r\n[Thread 0x7ffdf2027700 (LWP 8298) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8296) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8297) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8299)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8300)]\r\n[New Thread 0x7ffdcffff700 (LWP 8301)]\r\n[Thread 0x7ffdcffff700 (LWP 8301) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8299) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8300) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8302)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8303)]\r\n[New Thread 0x7ffdf2027700 (LWP 8304)]\r\n[Thread 0x7ffdf2027700 (LWP 8304) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8302) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8303) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8305)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8306)]\r\n[New Thread 0x7ffdcffff700 (LWP 8307)]\r\n[Thread 0x7ffdcffff700 (LWP 8307) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8305) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8306) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8308)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8309)]\r\n[New Thread 0x7ffdf2027700 (LWP 8310)]\r\n[Thread 0x7ffdf2027700 (LWP 8310) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8308) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8309) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8311)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8312)]\r\n[New Thread 0x7ffdcffff700 (LWP 8313)]\r\n[Thread 0x7ffdcffff700 (LWP 8313) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8311) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8312) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8314)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8315)]\r\n[New Thread 0x7ffdf2027700 (LWP 8316)]\r\n[Thread 0x7ffdf2027700 (LWP 8316) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8314) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8315) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8317)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8318)]\r\n[New Thread 0x7ffdcffff700 (LWP 8319)]\r\n[Thread 0x7ffdcffff700 (LWP 8319) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8317) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8318) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8320)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8321)]\r\n[New Thread 0x7ffdf2027700 (LWP 8322)]\r\n[Thread 0x7ffdf2027700 (LWP 8322) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8320) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8321) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8323)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8324)]\r\n[New Thread 0x7ffdcffff700 (LWP 8325)]\r\n[Thread 0x7ffdcffff700 (LWP 8325) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8323) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8324) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8326)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8327)]\r\n[New Thread 0x7ffdf2027700 (LWP 8328)]\r\n[Thread 0x7ffdf2027700 (LWP 8328) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8326) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8327) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8329)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8330)]\r\n[New Thread 0x7ffdcffff700 (LWP 8331)]\r\n[Thread 0x7ffdcffff700 (LWP 8331) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8329) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8330) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8332)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8333)]\r\n[New Thread 0x7ffdf2027700 (LWP 8334)]\r\n[Thread 0x7ffdf2027700 (LWP 8334) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8332) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8333) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8335)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8336)]\r\n[New Thread 0x7ffdcffff700 (LWP 8337)]\r\n[Thread 0x7ffdcffff700 (LWP 8337) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8335) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8336) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8338)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8339)]\r\n[New Thread 0x7ffdf2027700 (LWP 8340)]\r\n[Thread 0x7ffdf2027700 (LWP 8340) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8338) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8339) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8341)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8342)]\r\n[New Thread 0x7ffdcffff700 (LWP 8343)]\r\n[Thread 0x7ffdcffff700 (LWP 8343) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8341) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8342) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8344)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8345)]\r\n[New Thread 0x7ffdf2027700 (LWP 8346)]\r\n[Thread 0x7ffdf2027700 (LWP 8346) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8344) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8345) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8347)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8348)]\r\n[New Thread 0x7ffdcffff700 (LWP 8349)]\r\n[Thread 0x7ffdcffff700 (LWP 8349) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8347) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8348) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8350)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8351)]\r\n[New Thread 0x7ffdf2027700 (LWP 8352)]\r\n[Thread 0x7ffdf2027700 (LWP 8352) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8350) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8351) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8353)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8354)]\r\n[New Thread 0x7ffdcffff700 (LWP 8355)]\r\n[Thread 0x7ffdcffff700 (LWP 8355) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8353) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8354) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8356)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8357)]\r\n[New Thread 0x7ffdf2027700 (LWP 8358)]\r\n[Thread 0x7ffdf2027700 (LWP 8358) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8356) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8357) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8359)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8360)]\r\n[New Thread 0x7ffdcffff700 (LWP 8361)]\r\n[Thread 0x7ffdcffff700 (LWP 8361) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8359) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8360) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8362)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8363)]\r\n[New Thread 0x7ffdf2027700 (LWP 8364)]\r\n[Thread 0x7ffdf2027700 (LWP 8364) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8362) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8363) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8365)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8366)]\r\n[New Thread 0x7ffdcffff700 (LWP 8367)]\r\n[Thread 0x7ffdcffff700 (LWP 8367) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8365) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8366) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8368)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8369)]\r\n[New Thread 0x7ffdf2027700 (LWP 8370)]\r\n[Thread 0x7ffdf2027700 (LWP 8370) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8368) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8369) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8371)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8372)]\r\n[New Thread 0x7ffdcffff700 (LWP 8373)]\r\n[Thread 0x7ffdcffff700 (LWP 8373) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8371) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8372) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8374)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8375)]\r\n[New Thread 0x7ffdf2027700 (LWP 8376)]\r\n[Thread 0x7ffdf2027700 (LWP 8376) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8374) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8375) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8377)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8378)]\r\n[New Thread 0x7ffdcffff700 (LWP 8379)]\r\n[Thread 0x7ffdcffff700 (LWP 8379) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8377) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8378) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8380)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8381)]\r\n[New Thread 0x7ffdf2027700 (LWP 8382)]\r\n[Thread 0x7ffdf2027700 (LWP 8382) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8380) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8381) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8383)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8384)]\r\n[New Thread 0x7ffdcffff700 (LWP 8385)]\r\n[Thread 0x7ffdcffff700 (LWP 8385) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8383) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8384) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8386)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8387)]\r\n[New Thread 0x7ffdf2027700 (LWP 8388)]\r\n[Thread 0x7ffdf2027700 (LWP 8388) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8386) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8387) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8389)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8390)]\r\n[New Thread 0x7ffdcffff700 (LWP 8391)]\r\n[Thread 0x7ffdcffff700 (LWP 8391) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8389) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8390) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8392)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8393)]\r\n[New Thread 0x7ffdf2027700 (LWP 8394)]\r\n[Thread 0x7ffdf2027700 (LWP 8394) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8392) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8393) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8395)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8396)]\r\n[New Thread 0x7ffdcffff700 (LWP 8397)]\r\n[Thread 0x7ffdcffff700 (LWP 8397) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8395) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8396) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8398)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8399)]\r\n[New Thread 0x7ffdf2027700 (LWP 8400)]\r\n[Thread 0x7ffdf2027700 (LWP 8400) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8398) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8399) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8401)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8402)]\r\n[New Thread 0x7ffdcffff700 (LWP 8403)]\r\n[Thread 0x7ffdcffff700 (LWP 8403) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8401) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8402) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8404)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8405)]\r\n[New Thread 0x7ffdf2027700 (LWP 8406)]\r\n[Thread 0x7ffdf2027700 (LWP 8406) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8404) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8405) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8407)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8408)]\r\n[New Thread 0x7ffdcffff700 (LWP 8409)]\r\n[Thread 0x7ffdcffff700 (LWP 8409) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8407) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8408) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8410)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8411)]\r\n[New Thread 0x7ffdf2027700 (LWP 8412)]\r\n[Thread 0x7ffdf2027700 (LWP 8412) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8410) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8411) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8413)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8414)]\r\n[New Thread 0x7ffdcffff700 (LWP 8415)]\r\n[Thread 0x7ffdcffff700 (LWP 8415) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8413) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8414) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8416)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8417)]\r\n[New Thread 0x7ffdf2027700 (LWP 8418)]\r\n[Thread 0x7ffdf2027700 (LWP 8418) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8416) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8417) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8419)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8420)]\r\n[New Thread 0x7ffdcffff700 (LWP 8421)]\r\n[Thread 0x7ffdcffff700 (LWP 8421) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8419) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8420) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8422)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8423)]\r\n[New Thread 0x7ffdf2027700 (LWP 8424)]\r\n[Thread 0x7ffdf2027700 (LWP 8424) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8422) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8423) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8425)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8426)]\r\n[New Thread 0x7ffdcffff700 (LWP 8427)]\r\n[Thread 0x7ffdcffff700 (LWP 8427) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8425) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8426) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8428)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8429)]\r\n[New Thread 0x7ffdf2027700 (LWP 8430)]\r\n[Thread 0x7ffdf2027700 (LWP 8430) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8428) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8429) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8431)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8432)]\r\n[New Thread 0x7ffdcffff700 (LWP 8433)]\r\n[Thread 0x7ffdcffff700 (LWP 8433) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8431) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8432) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8434)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8435)]\r\n[New Thread 0x7ffdf2027700 (LWP 8436)]\r\n[Thread 0x7ffdf2027700 (LWP 8436) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8434) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8435) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8437)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8438)]\r\n[New Thread 0x7ffdcffff700 (LWP 8439)]\r\n[Thread 0x7ffdcffff700 (LWP 8439) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8437) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8438) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8440)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8441)]\r\n[New Thread 0x7ffdf2027700 (LWP 8442)]\r\n[Thread 0x7ffdf2027700 (LWP 8442) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8440) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8441) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8443)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8444)]\r\n[New Thread 0x7ffdcffff700 (LWP 8445)]\r\n[Thread 0x7ffdcffff700 (LWP 8445) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8443) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8444) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8446)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8447)]\r\n[New Thread 0x7ffdf2027700 (LWP 8448)]\r\n[Thread 0x7ffdf2027700 (LWP 8448) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8446) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8447) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8449)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8450)]\r\n[New Thread 0x7ffdcffff700 (LWP 8451)]\r\n[Thread 0x7ffdcffff700 (LWP 8451) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8449) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8450) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8452)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8453)]\r\n[New Thread 0x7ffdf2027700 (LWP 8454)]\r\n[Thread 0x7ffdf2027700 (LWP 8454) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8452) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8453) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8455)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8456)]\r\n[New Thread 0x7ffdcffff700 (LWP 8457)]\r\n[Thread 0x7ffdcffff700 (LWP 8457) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8455) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8456) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8458)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8459)]\r\n[New Thread 0x7ffdf2027700 (LWP 8460)]\r\n[Thread 0x7ffdf2027700 (LWP 8460) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8458) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8459) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8461)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8462)]\r\n[New Thread 0x7ffdcffff700 (LWP 8463)]\r\n[Thread 0x7ffdcffff700 (LWP 8463) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8461) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8462) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8464)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8465)]\r\n[New Thread 0x7ffdf2027700 (LWP 8466)]\r\n[Thread 0x7ffdf2027700 (LWP 8466) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8464) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8465) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8467)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8468)]\r\n[New Thread 0x7ffdcffff700 (LWP 8469)]\r\n[Thread 0x7ffdcffff700 (LWP 8469) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8467) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8468) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8470)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8471)]\r\n[New Thread 0x7ffdf2027700 (LWP 8472)]\r\n[Thread 0x7ffdf2027700 (LWP 8472) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8470) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8471) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8473)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8474)]\r\n[New Thread 0x7ffdcffff700 (LWP 8475)]\r\n[Thread 0x7ffdcffff700 (LWP 8475) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8473) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8474) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8476)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8477)]\r\n[New Thread 0x7ffdf2027700 (LWP 8478)]\r\n[Thread 0x7ffdf2027700 (LWP 8478) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8476) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8477) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8479)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8480)]\r\n[New Thread 0x7ffdcffff700 (LWP 8481)]\r\n[Thread 0x7ffdcffff700 (LWP 8481) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8479) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8480) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8482)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8483)]\r\n[New Thread 0x7ffdf2027700 (LWP 8484)]\r\n[Thread 0x7ffdf2027700 (LWP 8484) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8482) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8483) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8485)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8486)]\r\n[New Thread 0x7ffdcffff700 (LWP 8487)]\r\n[Thread 0x7ffdcffff700 (LWP 8487) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8485) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8486) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8488)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8489)]\r\n[New Thread 0x7ffdf2027700 (LWP 8490)]\r\n[Thread 0x7ffdf2027700 (LWP 8490) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8488) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8489) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8491)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8492)]\r\n[New Thread 0x7ffdcffff700 (LWP 8493)]\r\n[Thread 0x7ffdcffff700 (LWP 8493) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8491) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8492) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8494)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8495)]\r\n[New Thread 0x7ffdf2027700 (LWP 8496)]\r\n[Thread 0x7ffdf2027700 (LWP 8496) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8494) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8495) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8497)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8498)]\r\n[New Thread 0x7ffdcffff700 (LWP 8499)]\r\n[Thread 0x7ffdcffff700 (LWP 8499) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8497) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8498) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8500)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8501)]\r\n[New Thread 0x7ffdf2027700 (LWP 8502)]\r\n[Thread 0x7ffdf2027700 (LWP 8502) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8500) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8501) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8503)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8504)]\r\n[New Thread 0x7ffdcffff700 (LWP 8505)]\r\n[Thread 0x7ffdcffff700 (LWP 8505) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8503) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8504) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8506)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8507)]\r\n[New Thread 0x7ffdf2027700 (LWP 8508)]\r\n[Thread 0x7ffdf2027700 (LWP 8508) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8506) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8507) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8509)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8510)]\r\n[New Thread 0x7ffdcffff700 (LWP 8511)]\r\n[Thread 0x7ffdcffff700 (LWP 8511) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8509) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8510) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8512)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8513)]\r\n[New Thread 0x7ffdf2027700 (LWP 8514)]\r\n[Thread 0x7ffdf2027700 (LWP 8514) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8512) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8513) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8515)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8516)]\r\n[New Thread 0x7ffdcffff700 (LWP 8517)]\r\n[Thread 0x7ffdcffff700 (LWP 8517) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8515) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8516) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8518)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8519)]\r\n[New Thread 0x7ffdf2027700 (LWP 8520)]\r\n[Thread 0x7ffdf2027700 (LWP 8520) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8518) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8519) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8521)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8522)]\r\n[New Thread 0x7ffdcffff700 (LWP 8523)]\r\n[Thread 0x7ffdcffff700 (LWP 8523) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8521) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8522) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8524)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8525)]\r\n[New Thread 0x7ffdf2027700 (LWP 8526)]\r\n[Thread 0x7ffdf2027700 (LWP 8526) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8524) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8525) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8527)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8528)]\r\n[New Thread 0x7ffdcffff700 (LWP 8529)]\r\n[Thread 0x7ffdcffff700 (LWP 8529) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8527) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8528) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8530)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8531)]\r\n[New Thread 0x7ffdf2027700 (LWP 8532)]\r\n[Thread 0x7ffdf2027700 (LWP 8532) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8530) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8531) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8533)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8534)]\r\n[New Thread 0x7ffdcffff700 (LWP 8535)]\r\n[Thread 0x7ffdcffff700 (LWP 8535) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8533) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8534) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8536)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8537)]\r\n[New Thread 0x7ffdf2027700 (LWP 8538)]\r\n[Thread 0x7ffdf2027700 (LWP 8538) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8536) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8537) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8539)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8540)]\r\n[New Thread 0x7ffdcffff700 (LWP 8541)]\r\n[Thread 0x7ffdcffff700 (LWP 8541) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8539) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8540) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8542)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8543)]\r\n[New Thread 0x7ffdf2027700 (LWP 8544)]\r\n[Thread 0x7ffdf2027700 (LWP 8544) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8542) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8543) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8545)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8546)]\r\n[New Thread 0x7ffdcffff700 (LWP 8547)]\r\n[Thread 0x7ffdcffff700 (LWP 8547) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8545) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8546) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8548)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8549)]\r\n[New Thread 0x7ffdf2027700 (LWP 8550)]\r\n[Thread 0x7ffdf2027700 (LWP 8550) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8548) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8549) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8551)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8552)]\r\n[New Thread 0x7ffdcffff700 (LWP 8553)]\r\n[Thread 0x7ffdcffff700 (LWP 8553) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8551) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8552) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8554)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8555)]\r\n[New Thread 0x7ffdf2027700 (LWP 8556)]\r\n[Thread 0x7ffdf2027700 (LWP 8556) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8554) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8555) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8557)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8558)]\r\n[New Thread 0x7ffdcffff700 (LWP 8559)]\r\n[Thread 0x7ffdcffff700 (LWP 8559) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8557) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8558) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8560)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8561)]\r\n[New Thread 0x7ffdf2027700 (LWP 8562)]\r\n[Thread 0x7ffdf2027700 (LWP 8562) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8560) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8561) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8563)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8564)]\r\n[New Thread 0x7ffdcffff700 (LWP 8565)]\r\n[Thread 0x7ffdcffff700 (LWP 8565) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8563) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8564) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8566)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8567)]\r\n[New Thread 0x7ffdf2027700 (LWP 8568)]\r\n[Thread 0x7ffdf2027700 (LWP 8568) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8566) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8567) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8569)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8570)]\r\n[New Thread 0x7ffdcffff700 (LWP 8571)]\r\n[Thread 0x7ffdcffff700 (LWP 8571) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8569) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8570) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8572)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8573)]\r\n[New Thread 0x7ffdf2027700 (LWP 8574)]\r\n[Thread 0x7ffdf2027700 (LWP 8574) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8572) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8573) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8575)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8576)]\r\n[New Thread 0x7ffdcffff700 (LWP 8577)]\r\n[Thread 0x7ffdcffff700 (LWP 8577) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8575) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8576) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8578)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8579)]\r\n[New Thread 0x7ffdf2027700 (LWP 8580)]\r\n[Thread 0x7ffdf2027700 (LWP 8580) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8578) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8579) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8581)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8582)]\r\n[New Thread 0x7ffdcffff700 (LWP 8583)]\r\n[Thread 0x7ffdcffff700 (LWP 8583) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8581) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8582) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8584)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8585)]\r\n[New Thread 0x7ffdf2027700 (LWP 8586)]\r\n[Thread 0x7ffdf2027700 (LWP 8586) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8584) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8585) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8587)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8588)]\r\n[New Thread 0x7ffdcffff700 (LWP 8589)]\r\n[Thread 0x7ffdcffff700 (LWP 8589) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8587) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8588) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8590)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8591)]\r\n[New Thread 0x7ffdf2027700 (LWP 8592)]\r\n[Thread 0x7ffdf2027700 (LWP 8592) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8590) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8591) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8593)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8594)]\r\n[New Thread 0x7ffdcffff700 (LWP 8595)]\r\n[Thread 0x7ffdcffff700 (LWP 8595) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8593) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8594) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8596)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8597)]\r\n[New Thread 0x7ffdf2027700 (LWP 8598)]\r\n[Thread 0x7ffdf2027700 (LWP 8598) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8596) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8597) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8599)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8600)]\r\n[New Thread 0x7ffdcffff700 (LWP 8601)]\r\n[Thread 0x7ffdcffff700 (LWP 8601) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8599) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8600) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8602)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8603)]\r\n[New Thread 0x7ffdf2027700 (LWP 8604)]\r\n[Thread 0x7ffdf2027700 (LWP 8604) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8602) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8603) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8605)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8606)]\r\n[New Thread 0x7ffdcffff700 (LWP 8607)]\r\n[Thread 0x7ffdcffff700 (LWP 8607) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8605) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8606) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8608)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8609)]\r\n[New Thread 0x7ffdf2027700 (LWP 8610)]\r\n[Thread 0x7ffdf2027700 (LWP 8610) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8608) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8609) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8611)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8612)]\r\n[New Thread 0x7ffdcffff700 (LWP 8613)]\r\n[Thread 0x7ffdcffff700 (LWP 8613) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8611) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8612) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8614)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8615)]\r\n[New Thread 0x7ffdf2027700 (LWP 8616)]\r\n[Thread 0x7ffdf2027700 (LWP 8616) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8615) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8614) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8617)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8618)]\r\n[New Thread 0x7ffdcffff700 (LWP 8619)]\r\n[Thread 0x7ffdcffff700 (LWP 8619) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8617) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8618) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8620)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8621)]\r\n[New Thread 0x7ffdf2027700 (LWP 8622)]\r\n[Thread 0x7ffdf2027700 (LWP 8622) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8620) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8621) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8623)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8624)]\r\n[New Thread 0x7ffdcffff700 (LWP 8625)]\r\n[Thread 0x7ffdcffff700 (LWP 8625) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8623) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8624) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8626)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8627)]\r\n[New Thread 0x7ffdf2027700 (LWP 8628)]\r\n[Thread 0x7ffdf2027700 (LWP 8628) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8626) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8627) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8629)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8630)]\r\n[New Thread 0x7ffdcffff700 (LWP 8631)]\r\n[Thread 0x7ffdcffff700 (LWP 8631) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8629) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8630) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8632)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8633)]\r\n[New Thread 0x7ffdf2027700 (LWP 8634)]\r\n[Thread 0x7ffdf2027700 (LWP 8634) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8632) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8633) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8635)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8636)]\r\n[New Thread 0x7ffdcffff700 (LWP 8637)]\r\n[Thread 0x7ffdcffff700 (LWP 8637) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8635) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8636) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8638)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8639)]\r\n[New Thread 0x7ffdf2027700 (LWP 8640)]\r\n[Thread 0x7ffdf2027700 (LWP 8640) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8638) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8639) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8641)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8642)]\r\n[New Thread 0x7ffdcffff700 (LWP 8643)]\r\n[Thread 0x7ffdcffff700 (LWP 8643) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8641) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8642) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8644)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8645)]\r\n[New Thread 0x7ffdf2027700 (LWP 8646)]\r\n[Thread 0x7ffdf2027700 (LWP 8646) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8644) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8645) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8647)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8648)]\r\n[New Thread 0x7ffdcffff700 (LWP 8649)]\r\n[Thread 0x7ffdcffff700 (LWP 8649) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8647) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8648) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8650)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8651)]\r\n[New Thread 0x7ffdf2027700 (LWP 8652)]\r\n[Thread 0x7ffdf2027700 (LWP 8652) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8650) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8651) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8653)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8654)]\r\n[New Thread 0x7ffdcffff700 (LWP 8655)]\r\n[Thread 0x7ffdcffff700 (LWP 8655) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8653) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8654) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8656)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8657)]\r\n[New Thread 0x7ffdf2027700 (LWP 8658)]\r\n[Thread 0x7ffdf2027700 (LWP 8658) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8656) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8657) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8659)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8660)]\r\n[New Thread 0x7ffdcffff700 (LWP 8661)]\r\n[Thread 0x7ffdcffff700 (LWP 8661) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8659) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8660) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8662)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8663)]\r\n[New Thread 0x7ffdf2027700 (LWP 8664)]\r\n[Thread 0x7ffdf2027700 (LWP 8664) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8662) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8663) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8665)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8666)]\r\n[New Thread 0x7ffdcffff700 (LWP 8667)]\r\n[Thread 0x7ffdcffff700 (LWP 8667) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8665) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8666) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8668)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8669)]\r\n[New Thread 0x7ffdf2027700 (LWP 8670)]\r\n[Thread 0x7ffdf2027700 (LWP 8670) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8668) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8669) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8671)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8672)]\r\n[New Thread 0x7ffdcffff700 (LWP 8673)]\r\n[Thread 0x7ffdcffff700 (LWP 8673) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8671) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8672) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8674)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8675)]\r\n[New Thread 0x7ffdf2027700 (LWP 8676)]\r\n[Thread 0x7ffdf2027700 (LWP 8676) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8674) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8675) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8677)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8678)]\r\n[New Thread 0x7ffdcffff700 (LWP 8679)]\r\n[Thread 0x7ffdcffff700 (LWP 8679) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8677) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8678) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8680)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8681)]\r\n[New Thread 0x7ffdf2027700 (LWP 8682)]\r\n[Thread 0x7ffdf2027700 (LWP 8682) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8680) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8681) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8683)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8684)]\r\n[New Thread 0x7ffdcffff700 (LWP 8685)]\r\n[Thread 0x7ffdcffff700 (LWP 8685) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8683) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8684) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8686)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8687)]\r\n[New Thread 0x7ffdf2027700 (LWP 8688)]\r\n[Thread 0x7ffdf2027700 (LWP 8688) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8686) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8687) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8689)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8690)]\r\n[New Thread 0x7ffdcffff700 (LWP 8691)]\r\n[Thread 0x7ffdcffff700 (LWP 8691) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8689) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8690) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8692)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8693)]\r\n[New Thread 0x7ffdf2027700 (LWP 8694)]\r\n[Thread 0x7ffdf2027700 (LWP 8694) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8692) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8693) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8695)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8696)]\r\n[New Thread 0x7ffdcffff700 (LWP 8697)]\r\n[Thread 0x7ffdcffff700 (LWP 8697) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8695) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8696) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8698)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8699)]\r\n[New Thread 0x7ffdf2027700 (LWP 8700)]\r\n[Thread 0x7ffdf2027700 (LWP 8700) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8698) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8699) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8701)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8702)]\r\n[New Thread 0x7ffdcffff700 (LWP 8703)]\r\n[Thread 0x7ffdcffff700 (LWP 8703) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8701) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8702) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8704)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8705)]\r\n[New Thread 0x7ffdf2027700 (LWP 8706)]\r\n[Thread 0x7ffdf2027700 (LWP 8706) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8704) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8705) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8707)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8708)]\r\n[New Thread 0x7ffdcffff700 (LWP 8709)]\r\n[Thread 0x7ffdcffff700 (LWP 8709) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8707) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8708) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8710)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8711)]\r\n[New Thread 0x7ffdf2027700 (LWP 8712)]\r\n[Thread 0x7ffdf2027700 (LWP 8712) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8710) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8711) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8713)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8714)]\r\n[New Thread 0x7ffdcffff700 (LWP 8715)]\r\n[Thread 0x7ffdcffff700 (LWP 8715) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8713) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8714) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8716)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8717)]\r\n[New Thread 0x7ffdf2027700 (LWP 8718)]\r\n[Thread 0x7ffdf2027700 (LWP 8718) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8716) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8717) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8719)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8720)]\r\n[New Thread 0x7ffdcffff700 (LWP 8721)]\r\n[Thread 0x7ffdcffff700 (LWP 8721) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8719) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8720) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8722)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8723)]\r\n[New Thread 0x7ffdf2027700 (LWP 8724)]\r\n[Thread 0x7ffdf2027700 (LWP 8724) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8722) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8723) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8725)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8726)]\r\n[New Thread 0x7ffdcffff700 (LWP 8727)]\r\n[Thread 0x7ffdcffff700 (LWP 8727) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8725) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8726) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8728)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8729)]\r\n[New Thread 0x7ffdf2027700 (LWP 8730)]\r\n[Thread 0x7ffdf2027700 (LWP 8730) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8728) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8729) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8731)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8732)]\r\n[New Thread 0x7ffdcffff700 (LWP 8733)]\r\n[Thread 0x7ffdcffff700 (LWP 8733) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8731) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8732) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8734)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8735)]\r\n[New Thread 0x7ffdf2027700 (LWP 8736)]\r\n[Thread 0x7ffdf2027700 (LWP 8736) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8734) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8735) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8737)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8738)]\r\n[New Thread 0x7ffdcffff700 (LWP 8739)]\r\n[Thread 0x7ffdcffff700 (LWP 8739) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8737) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8738) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8740)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8741)]\r\n[New Thread 0x7ffdf2027700 (LWP 8742)]\r\n[Thread 0x7ffdf2027700 (LWP 8742) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8740) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8741) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8743)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8744)]\r\n[New Thread 0x7ffdcffff700 (LWP 8745)]\r\n[Thread 0x7ffdcffff700 (LWP 8745) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8743) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8744) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8746)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8747)]\r\n[New Thread 0x7ffdf2027700 (LWP 8748)]\r\n[Thread 0x7ffdf2027700 (LWP 8748) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8746) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8747) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8749)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8750)]\r\n[New Thread 0x7ffdcffff700 (LWP 8751)]\r\n[Thread 0x7ffdcffff700 (LWP 8751) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8749) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8750) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8752)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8753)]\r\n[New Thread 0x7ffdf2027700 (LWP 8754)]\r\n[Thread 0x7ffdf2027700 (LWP 8754) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8752) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8753) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8755)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8756)]\r\n[New Thread 0x7ffdcffff700 (LWP 8757)]\r\n[Thread 0x7ffdcffff700 (LWP 8757) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8755) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8756) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8758)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8759)]\r\n[New Thread 0x7ffdf2027700 (LWP 8760)]\r\n[Thread 0x7ffdf2027700 (LWP 8760) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8758) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8759) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8761)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8762)]\r\n[New Thread 0x7ffdcffff700 (LWP 8763)]\r\n[Thread 0x7ffdcffff700 (LWP 8763) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8761) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8762) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8764)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8765)]\r\n[New Thread 0x7ffdf2027700 (LWP 8766)]\r\n[Thread 0x7ffdf2027700 (LWP 8766) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8764) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8765) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8767)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8768)]\r\n[New Thread 0x7ffdcffff700 (LWP 8769)]\r\n[Thread 0x7ffdcffff700 (LWP 8769) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8767) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8768) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8770)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8771)]\r\n[New Thread 0x7ffdf2027700 (LWP 8772)]\r\n[Thread 0x7ffdf2027700 (LWP 8772) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8770) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8771) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8773)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8774)]\r\n[New Thread 0x7ffdcffff700 (LWP 8775)]\r\n[Thread 0x7ffdcffff700 (LWP 8775) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8773) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8774) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8776)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8777)]\r\n[New Thread 0x7ffdf2027700 (LWP 8778)]\r\n[Thread 0x7ffdf2027700 (LWP 8778) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8776) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8777) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8779)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8780)]\r\n[New Thread 0x7ffdcffff700 (LWP 8781)]\r\n[Thread 0x7ffdcffff700 (LWP 8781) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8779) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8780) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8782)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8783)]\r\n[New Thread 0x7ffdf2027700 (LWP 8784)]\r\n[Thread 0x7ffdf2027700 (LWP 8784) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8782) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8783) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8785)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8786)]\r\n[New Thread 0x7ffdcffff700 (LWP 8787)]\r\n[Thread 0x7ffdcffff700 (LWP 8787) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8785) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8786) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8788)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8789)]\r\n[New Thread 0x7ffdf2027700 (LWP 8790)]\r\n[Thread 0x7ffdf2027700 (LWP 8790) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8788) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8789) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8791)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8792)]\r\n[New Thread 0x7ffdcffff700 (LWP 8793)]\r\n[Thread 0x7ffdcffff700 (LWP 8793) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8791) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8792) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8794)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8795)]\r\n[New Thread 0x7ffdf2027700 (LWP 8796)]\r\n[Thread 0x7ffdf2027700 (LWP 8796) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8794) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8795) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8797)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8798)]\r\n[New Thread 0x7ffdcffff700 (LWP 8799)]\r\n[Thread 0x7ffdcffff700 (LWP 8799) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8797) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8798) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8800)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8801)]\r\n[New Thread 0x7ffdf2027700 (LWP 8802)]\r\n[Thread 0x7ffdf2027700 (LWP 8802) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8800) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8801) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8803)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8804)]\r\n[New Thread 0x7ffdcffff700 (LWP 8805)]\r\n[Thread 0x7ffdcffff700 (LWP 8805) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8803) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8804) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8806)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8807)]\r\n[New Thread 0x7ffdf2027700 (LWP 8808)]\r\n[Thread 0x7ffdf2027700 (LWP 8808) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8807) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8806) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8809)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8810)]\r\n[New Thread 0x7ffdcffff700 (LWP 8811)]\r\n[Thread 0x7ffdcffff700 (LWP 8811) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8809) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8810) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8812)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8813)]\r\n[New Thread 0x7ffdf2027700 (LWP 8814)]\r\n[Thread 0x7ffdf2027700 (LWP 8814) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8812) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8813) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8815)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8816)]\r\n[New Thread 0x7ffdcffff700 (LWP 8817)]\r\n[Thread 0x7ffdcffff700 (LWP 8817) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8815) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8816) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8818)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8819)]\r\n[New Thread 0x7ffdf2027700 (LWP 8820)]\r\n[Thread 0x7ffdf2027700 (LWP 8820) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8818) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8819) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8821)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8822)]\r\n[New Thread 0x7ffdcffff700 (LWP 8823)]\r\n[Thread 0x7ffdcffff700 (LWP 8823) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8821) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8822) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8824)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8825)]\r\n[New Thread 0x7ffdf2027700 (LWP 8826)]\r\n[Thread 0x7ffdf2027700 (LWP 8826) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8824) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8825) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8827)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8828)]\r\n[New Thread 0x7ffdcffff700 (LWP 8829)]\r\n[Thread 0x7ffdcffff700 (LWP 8829) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8827) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8828) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8830)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8831)]\r\n[New Thread 0x7ffdf2027700 (LWP 8832)]\r\n[Thread 0x7ffdf2027700 (LWP 8832) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8830) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8831) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8833)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8834)]\r\n[New Thread 0x7ffdcffff700 (LWP 8835)]\r\n[Thread 0x7ffdcffff700 (LWP 8835) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8833) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8834) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8836)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8837)]\r\n[New Thread 0x7ffdf2027700 (LWP 8838)]\r\n[Thread 0x7ffdf2027700 (LWP 8838) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8836) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8837) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8839)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8840)]\r\n[New Thread 0x7ffdcffff700 (LWP 8841)]\r\n[Thread 0x7ffdcffff700 (LWP 8841) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8839) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8840) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8842)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8843)]\r\n[New Thread 0x7ffdf2027700 (LWP 8844)]\r\n[Thread 0x7ffdf2027700 (LWP 8844) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8842) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8843) exited]\r\n[New Thread 0x7ffdf2027700 (LWP 8845)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8846)]\r\n[New Thread 0x7ffdcffff700 (LWP 8847)]\r\n[Thread 0x7ffdcffff700 (LWP 8847) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8845) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8846) exited]\r\n[New Thread 0x7ffdcffff700 (LWP 8848)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8849)]\r\n[New Thread 0x7ffdf2027700 (LWP 8850)]\r\n[Thread 0x7ffdf2027700 (LWP 8850) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8848) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8849) exited]\r\n2019-11-10 08:51:40.041023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-10 08:51:40.041373: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n[New Thread 0x7ffdf2027700 (LWP 8851)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8852)]\r\n2019-11-10 08:51:40.041879: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n[New Thread 0x7ffdcffff700 (LWP 8853)]\r\n[New Thread 0x7ffdd5533700 (LWP 8854)]\r\n[Thread 0x7ffdd5d34700 (LWP 8852) exited]\r\n[Thread 0x7ffdf2027700 (LWP 8851) exited]\r\n[New Thread 0x7ffdd5d34700 (LWP 8855)]\r\n[New Thread 0x7ffdf2027700 (LWP 8856)]\r\n[New Thread 0x7ffdd4d32700 (LWP 8857)]\r\n2019-11-10 08:51:40.043093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-10 08:51:40.043334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: Tesla M60 major: 5 minor: 2 memoryClockRate(GHz): 1.1775\r\npciBusID: 0000:00:1e.0\r\n2019-11-10 08:51:40.043393: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-11-10 08:51:40.043418: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-11-10 08:51:40.043439: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-11-10 08:51:40.043456: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-11-10 08:51:40.043476: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-11-10 08:51:40.043490: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-11-10 08:51:40.043506: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-11-10 08:51:40.043563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-10 08:51:40.043824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-10 08:51:40.044044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-11-10 08:51:40.232781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-11-10 08:51:40.232845: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-11-10 08:51:40.232866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-11-10 08:51:40.233149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-10 08:51:40.233473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-10 08:51:40.233707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7162 MB memory) -> physical GPU (device: 0, name: Tesla M60, pci bus id: 0000:00:1e.0, compute capability: 5.2)\r\n[New Thread 0x7ffdcf7fe700 (LWP 8858)]\r\n[New Thread 0x7ffdceffd700 (LWP 8859)]\r\n[New Thread 0x7ffdce7fc700 (LWP 8860)]\r\n[New Thread 0x7ffdcdffb700 (LWP 8861)]\r\n[New Thread 0x7ffdcd7fa700 (LWP 8862)]\r\n[New Thread 0x7ffdccff9700 (LWP 8863)]\r\n[New Thread 0x7ffda7fff700 (LWP 8864)]\r\n[New Thread 0x7ffda77fe700 (LWP 8865)]\r\n[New Thread 0x7ffda6ffd700 (LWP 8866)]\r\n[New Thread 0x7ffda67fc700 (LWP 8867)]\r\n[New Thread 0x7ffda5ffb700 (LWP 8868)]\r\n[New Thread 0x7ffda57fa700 (LWP 8869)]\r\n[New Thread 0x7ffda4ff9700 (LWP 8870)]\r\n[New Thread 0x7ffd87fff700 (LWP 8871)]\r\n[New Thread 0x7ffd877fe700 (LWP 8872)]\r\n[New Thread 0x7ffd86ffd700 (LWP 8873)]\r\n2019-11-10 08:51:40.320845: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize\r\n2019-11-10 08:51:40.320889: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: Graph size after: 1676 nodes (1362), 3172 edges (2858), time = 50.794ms.\r\n2019-11-10 08:51:40.320900: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.832ms.\r\n[New Thread 0x7ffd867fc700 (LWP 8874)]\r\n[Thread 0x7ffdd5533700 (LWP 8854) exited]\r\n[Thread 0x7ffdcffff700 (LWP 8853) exited]\r\n[Thread 0x7ffd867fc700 (LWP 8874) exited]\r\n[Thread 0x7ffda57fa700 (LWP 8869) exited]\r\n[Thread 0x7ffda7fff700 (LWP 8864) exited]\r\n[Thread 0x7ffdcdffb700 (LWP 8861) exited]\r\n[Thread 0x7ffdcf7fe700 (LWP 8858) exited]\r\n[Thread 0x7ffd86ffd700 (LWP 8873) exited]\r\n[Thread 0x7ffda6ffd700 (LWP 8866) exited]\r\n[Thread 0x7ffdceffd700 (LWP 8859) exited]\r\n[Thread 0x7ffda4ff9700 (LWP 8870) exited]\r\n[Thread 0x7ffda5ffb700 (LWP 8868) exited]\r\n[Thread 0x7ffdccff9700 (LWP 8863) exited]\r\n[Thread 0x7ffdcd7fa700 (LWP 8862) exited]\r\n[Thread 0x7ffdce7fc700 (LWP 8860) exited]\r\n[Thread 0x7ffd877fe700 (LWP 8872) exited]\r\n[Thread 0x7ffda77fe700 (LWP 8865) exited]\r\n[Thread 0x7ffd87fff700 (LWP 8871) exited]\r\n[Thread 0x7ffda67fc700 (LWP 8867) exited]\r\n[Thread 0x7ffdd5d34700 (LWP 8855) exited]\r\n[Thread 0x7ffdd4d32700 (LWP 8857) exited]\r\nRun TRT optimizer in Grappler to convert the graph\r\n\r\n2019-11-10 08:51:41.984437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-10 08:51:41.984749: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n[New Thread 0x7ffdd4d32700 (LWP 8875)]\r\n[New Thread 0x7ffdd5d34700 (LWP 8876)]\r\n2019-11-10 08:51:41.985406: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n[New Thread 0x7ffd86ffd700 (LWP 8877)]\r\n[New Thread 0x7ffd877fe700 (LWP 8878)]\r\n[Thread 0x7ffdd5d34700 (LWP 8876) exited]\r\n[Thread 0x7ffdd4d32700 (LWP 8875) exited]\r\n[New Thread 0x7ffdd5d34700 (LWP 8879)]\r\n[New Thread 0x7ffdd4d32700 (LWP 8880)]\r\n2019-11-10 08:51:41.986303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-10 08:51:41.986526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: Tesla M60 major: 5 minor: 2 memoryClockRate(GHz): 1.1775\r\npciBusID: 0000:00:1e.0\r\n2019-11-10 08:51:41.986594: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-11-10 08:51:41.986621: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-11-10 08:51:41.986645: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-11-10 08:51:41.986667: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-11-10 08:51:41.986690: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-11-10 08:51:41.986712: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-11-10 08:51:41.986736: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-11-10 08:51:41.986788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-10 08:51:41.987035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-10 08:51:41.987240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-11-10 08:51:41.987279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-11-10 08:51:41.987294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-11-10 08:51:41.987301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-11-10 08:51:41.987447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-10 08:51:41.987699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-10 08:51:41.987917: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7162 MB memory) -> physical GPU (device: 0, name: Tesla M60, pci bus id: 0000:00:1e.0, compute capability: 5.2)\r\n[New Thread 0x7ffdd5533700 (LWP 8881)]\r\n[New Thread 0x7ffdcffff700 (LWP 8882)]\r\n[New Thread 0x7ffdcf7fe700 (LWP 8883)]\r\n[New Thread 0x7ffdceffd700 (LWP 8884)]\r\n[New Thread 0x7ffdce7fc700 (LWP 8885)]\r\n[New Thread 0x7ffdcdffb700 (LWP 8886)]\r\n[New Thread 0x7ffdcd7fa700 (LWP 8887)]\r\n[New Thread 0x7ffdccff9700 (LWP 8888)]\r\n[New Thread 0x7ffda7fff700 (LWP 8889)]\r\n[New Thread 0x7ffda77fe700 (LWP 8890)]\r\n[New Thread 0x7ffda6ffd700 (LWP 8891)]\r\n[New Thread 0x7ffda67fc700 (LWP 8892)]\r\n[New Thread 0x7ffda5ffb700 (LWP 8893)]\r\n[New Thread 0x7ffda57fa700 (LWP 8894)]\r\n[New Thread 0x7ffda4ff9700 (LWP 8895)]\r\n[New Thread 0x7ffd87fff700 (LWP 8896)]\r\n[New Thread 0x7ffd867fc700 (LWP 8897)]\r\n[New Thread 0x7ffd85ffb700 (LWP 8898)]\r\n[New Thread 0x7ffd857fa700 (LWP 8899)]\r\n[New Thread 0x7ffd84ff9700 (LWP 8900)]\r\n[New Thread 0x7ffd67fff700 (LWP 8901)]\r\n[New Thread 0x7ffd677fe700 (LWP 8902)]\r\n[New Thread 0x7ffd66ffd700 (LWP 8903)]\r\n[New Thread 0x7ffd667fc700 (LWP 8904)]\r\n[New Thread 0x7ffd65ffb700 (LWP 8905)]\r\n[New Thread 0x7ffd657fa700 (LWP 8906)]\r\n[New Thread 0x7ffd64ff9700 (LWP 8907)]\r\n[New Thread 0x7ffd47fff700 (LWP 8908)]\r\n[New Thread 0x7ffd477fe700 (LWP 8909)]\r\n[New Thread 0x7ffd46ffd700 (LWP 8910)]\r\n[New Thread 0x7ffd467fc700 (LWP 8911)]\r\n[New Thread 0x7ffd45ffb700 (LWP 8912)]\r\n[New Thread 0x7ffd457fa700 (LWP 8913)]\r\n[New Thread 0x7ffd44ff9700 (LWP 8914)]\r\n[New Thread 0x7ffd27fff700 (LWP 8915)]\r\n[New Thread 0x7ffd277fe700 (LWP 8916)]\r\n[New Thread 0x7ffd26ffd700 (LWP 8917)]\r\n[New Thread 0x7ffd267fc700 (LWP 8918)]\r\n[New Thread 0x7ffd25ffb700 (LWP 8919)]\r\n[New Thread 0x7ffd257fa700 (LWP 8920)]\r\n[New Thread 0x7ffd24ff9700 (LWP 8921)]\r\n[New Thread 0x7ffd07fff700 (LWP 8922)]\r\n[New Thread 0x7ffd077fe700 (LWP 8923)]\r\n[New Thread 0x7ffd06ffd700 (LWP 8924)]\r\n[New Thread 0x7ffd067fc700 (LWP 8925)]\r\n[New Thread 0x7ffd05ffb700 (LWP 8926)]\r\n[New Thread 0x7ffd057fa700 (LWP 8927)]\r\n[New Thread 0x7ffd04ff9700 (LWP 8928)]\r\n2019-11-10 08:51:42.597091: I tensorflow/compiler/tf2tensorrt/segment/segment.cc:460] There are 70 ops of 7 different types in the graph that are not converted to TensorRT: Identity, Placeholder, NoOp, Pack, Shape, StridedSlice, Reshape, (For more information see https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#supported-ops).\r\n2019-11-10 08:51:42.982417: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:633] Number of TensorRT candidate segments: 17\r\n2019-11-10 08:51:43.059702: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:734] TensorRT node StatefulPartitionedCall/efficientnet-b0/TRTEngineOp_0 added for segment 0 consisting of 41 nodes succeeded.\r\n2019-11-10 08:51:43.060007: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:734] TensorRT node StatefulPartitionedCall/efficientnet-b0/TRTEngineOp_1 added for segment 1 consisting of 41 nodes succeeded.\r\n2019-11-10 08:51:43.060271: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:734] TensorRT node StatefulPartitionedCall/efficientnet-b0/TRTEngineOp_2 added for segment 2 consisting of 43 nodes succeeded.\r\n2019-11-10 08:51:43.060530: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:734] TensorRT node StatefulPartitionedCall/efficientnet-b0/TRTEngineOp_3 added for segment 3 consisting of 41 nodes succeeded.\r\n2019-11-10 08:51:43.060768: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:734] TensorRT node StatefulPartitionedCall/efficientnet-b0/TRTEngineOp_4 added for segment 4 consisting of 43 nodes succeeded.\r\n2019-11-10 08:51:43.061007: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:734] TensorRT node StatefulPartitionedCall/efficientnet-b0/TRTEngineOp_5 added for segment 5 consisting of 41 nodes succeeded.\r\n2019-11-10 08:51:43.061313: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:734] TensorRT node StatefulPartitionedCall/efficientnet-b0/TRTEngineOp_6 added for segment 6 consisting of 43 nodes succeeded.\r\n2019-11-10 08:51:43.061518: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:734] TensorRT node StatefulPartitionedCall/efficientnet-b0/TRTEngineOp_7 added for segment 7 consisting of 43 nodes succeeded.\r\n2019-11-10 08:51:43.061728: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:734] TensorRT node StatefulPartitionedCall/efficientnet-b0/TRTEngineOp_8 added for segment 8 consisting of 41 nodes succeeded.\r\n2019-11-10 08:51:43.061976: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:734] TensorRT node StatefulPartitionedCall/efficientnet-b0/TRTEngineOp_9 added for segment 9 consisting of 43 nodes succeeded.\r\n2019-11-10 08:51:43.062207: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:734] TensorRT node StatefulPartitionedCall/efficientnet-b0/TRTEngineOp_10 added for segment 10 consisting of 43 nodes succeeded.\r\n2019-11-10 08:51:43.062447: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:734] TensorRT node StatefulPartitionedCall/efficientnet-b0/TRTEngineOp_11 added for segment 11 consisting of 41 nodes succeeded.\r\n2019-11-10 08:51:43.062683: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:734] TensorRT node StatefulPartitionedCall/efficientnet-b0/TRTEngineOp_12 added for segment 12 consisting of 43 nodes succeeded.\r\n2019-11-10 08:51:43.062900: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:734] TensorRT node StatefulPartitionedCall/efficientnet-b0/TRTEngineOp_13 added for segment 13 consisting of 43 nodes succeeded.\r\n2019-11-10 08:51:43.063120: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:734] TensorRT node StatefulPartitionedCall/efficientnet-b0/TRTEngineOp_14 added for segment 14 consisting of 43 nodes succeeded.\r\n2019-11-10 08:51:43.063297: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:734] TensorRT node TRTEngineOp_15 added for segment 15 consisting of 40 nodes succeeded.\r\n\r\nThread 1 \"python\" received signal SIGSEGV, Segmentation fault.\r\n0x00007fff8bb8b820 in tensorflow::Node::name() const () from /home/ubuntu/tf20/lib/python3.5/site-packages/tensorflow_core/python/../libtensorflow_framework.so.2\r\n(gdb) bt\r\n#0  0x00007fff8bb8b820 in tensorflow::Node::name() const () from /home/ubuntu/tf20/lib/python3.5/site-packages/tensorflow_core/python/../libtensorflow_framework.so.2\r\n#1  0x00007fff919b8186 in tensorflow::tensorrt::convert::UpdateToEngineNode(std::vector<tensorflow::tensorrt::convert::EngineInfo, std::allocator<tensorflow::tensorrt::convert::EngineInfo> > const&, unsigned long, std::vector<tensorflow::Node*, std::allocator<tensorflow::Node*> > const&, bool, std::string const&, tensorflow::Node**, int*) ()\r\n   from /home/ubuntu/tf20/lib/python3.5/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#2  0x00007fff919ba270 in tensorflow::tensorrt::convert::CreateTRTNode(tensorflow::tensorrt::convert::ConversionParams const&, std::vector<tensorflow::tensorrt::convert::EngineInfo, std::allocator<tensorflow::tensorrt::convert::EngineInfo> > const&, int, int, tensorflow::Graph*, nvinfer1::IGpuAllocator*, std::vector<tensorflow::Node*, std::allocator<tensorflow::Node*> >*) ()\r\n   from /home/ubuntu/tf20/lib/python3.5/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#3  0x00007fff919bf334 in tensorflow::tensorrt::convert::ConvertAfterShapes(tensorflow::tensorrt::convert::ConversionParams const&) ()\r\n   from /home/ubuntu/tf20/lib/python3.5/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#4  0x00007fff919f7b06 in tensorflow::tensorrt::convert::TRTOptimizationPass::Optimize(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem const&, tensorflow::GraphDef*) ()\r\n   from /home/ubuntu/tf20/lib/python3.5/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#5  0x00007fff94d674ac in tensorflow::grappler::MetaOptimizer::RunOptimizer(tensorflow::grappler::GraphOptimizer*, tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem*, tensorflow::GraphDef*, tensorflow::grappler::MetaOptimizer::GraphOptimizationResult*) () from /home/ubuntu/tf20/lib/python3.5/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#6  0x00007fff94d68695 in tensorflow::grappler::MetaOptimizer::OptimizeGraph(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem const&, tensorflow::GraphDef*) ()\r\n   from /home/ubuntu/tf20/lib/python3.5/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#7  0x00007fff94d6a050 in tensorflow::grappler::MetaOptimizer::Optimize(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem const&, tensorflow::GraphDef*) ()\r\n   from /home/ubuntu/tf20/lib/python3.5/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#8  0x00007fff8eda048f in TF_OptimizeGraph(GCluster, tensorflow::ConfigProto const&, tensorflow::MetaGraphDef const&, bool, std::string const&, TF_Status*) ()\r\n   from /home/ubuntu/tf20/lib/python3.5/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#9  0x00007fff8eda77c4 in _wrap_TF_OptimizeGraph () from /home/ubuntu/tf20/lib/python3.5/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#10 0x00000000004ea10f in PyCFunction_Call ()\r\n#11 0x0000000000536d94 in PyEval_EvalFrameEx ()\r\n#12 0x000000000053fc97 in ?? ()\r\n#13 0x000000000053b83f in PyEval_EvalFrameEx ()\r\n#14 0x000000000053b294 in PyEval_EvalFrameEx ()\r\n#15 0x000000000053fc97 in ?? ()\r\n#16 0x000000000053b83f in PyEval_EvalFrameEx ()\r\n#17 0x000000000053fc97 in ?? ()\r\n#18 0x00000000005409bf in PyEval_EvalCode ()\r\n#19 0x000000000060cb42 in ?? ()\r\n#20 0x000000000060efea in PyRun_FileExFlags ()\r\n#21 0x000000000060f7dc in PyRun_SimpleFileExFlags ()\r\n#22 0x0000000000640256 in Py_Main ()\r\n#23 0x00000000004d0001 in main ()\r\n(gdb) \r\n\r\n\r\n", "comments": ["@arielbenitah sorry about the trouble. It seems you're using efficientnet, is it possible for you to share the model for investigation purposes?", "I think I might have fixed the segfault in 9bbb12d481de11e2611fa6fbb4dc13cb7bb09176.\r\nBut the real problem is probably that something goes wrong when converting the graphs...", "@aaroey You're right! I'm using EfficientNetB0. The model is written above in the description of the issue. \r\nThanks fo your help", "@olesalscheider Thanks, I'll take a look at your fix as soon as possible and let you know.", "I believe I am running into the same issue:\r\nWith tensorflow2.0 I get the following error:\r\n2020-02-03 14:01:22.902124: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:737] Replaced segment 0 consisting of 46 nodes by TRTEngineOp_0.\r\n\r\nProcess finished with exit code 139 (interrupted by signal 11: SIGSEGV)\r\n\r\nUsing tf-nightly-gpu the error changes to:\r\n\r\n2020-02-03 13:51:56.895040: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:638] Number of TensorRT candidate segments: 3\r\n2020-02-03 13:51:56.900299: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:739] Replaced segment 0 consisting of 46 nodes by TRTEngineOp_0.\r\n2020-02-03 13:51:56.900528: F tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:310] Node StatefulPartitionedCall/Identity_1 not found in any engine.\r\n\r\nProcess finished with exit code 134 (interrupted by signal 6: SIGABRT)\r\n", "@bixia1 @sanjoy could you help to investigate this?", "@arielbenitah,\r\nIs this still an issue?\r\n\r\nCould you please update TensorFlow to the latest stable version v.2.4.1 and check if you are facing the same error. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34136\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34136\">No</a>\n"]}, {"number": 34135, "title": "Failed to load delegate from libedgetpu.so.1.0", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nRaspbian\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nRaspberry Pi\r\n- TensorFlow installed from (source or binary):\r\nBinary\r\n- TensorFlow version:\r\n- Python version:\r\n3.5\r\n- Installed using virtualenv? pip? conda?:\r\npip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the problem**\r\n libedgetpu.so.1.0 fails to load for certain user.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n  interpreter = Interpreter(args.model, experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\r\n\r\nIt works perfectly for the user I can sudo. But I want to use the library with a different user (that cannot do sudo). I found in another issue that I need to add this user to the plugdev group. Which I did with the following command:\r\nsudo usermod -aG plugdev [your username] \r\nThis did not solve the problem.\r\n\r\n**Any other info / logs**\r\n    interpreter = Interpreter(args.model, experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\r\n  File \"/home/certain-user/.local/lib/python3.5/site-packages/tflite_runtime/interpreter.py\", line 168, in load_delegate\r\n    library, str(e)))\r\nValueError: Failed to load delegate from libedgetpu.so.1.0\r\n", "comments": ["It seems it's not documented. At least, I don't remember seeing it mentioned. Using delegate is kinda API/ABI dependent, if there is data structure or function parameters changes, loading a delegate could fail. A safe way to make sure you can use the Edge TPU delegate is to check the source code by Edge TPU team. E.g., the [latest source code](https://github.com/google-coral/edgetpu) you can find now is for diploria2. And the TensorFlow version used by it is TensorFlow https://github.com/tensorflow/tensorflow/commit/5d0b55dd4a00c74809e5b32217070a26ac6ef823. You can find corresponding version in Edge TPU repo's WORKSPACE file, check [here](https://github.com/google-coral/edgetpu/blob/master/WORKSPACE#L5-L8)", "Hi Team,\r\nI am also getting the same error while I am trying to access the python script from a php block through a browser. The program is running fine while run from a terminal. For an example \"python runProg.py\" works fine from the terminal. Also, I can run the php block separately which is calling the python block. Like \"php start.php\" works fine. But While I am running it from the browser like 192.168.XX.XX/start.php, the browser is throwing the error. I did not find the solution through the internet. If anyone got any suggestion that would be helpful.", "With MacOS restarting the computer solved the problem.", "Hi Olli,\r\nI am using a Raspberry Pi. The problem persists for last one month. I did at least 100 restarts in the meantime. ", "I am also using a Raspberry Pi and I still have not solved this issue.", "@DebojitBiswas @bsddemon make sure you guys upgrade the `tflite_runtime` package from [here](https://www.tensorflow.org/lite/guide/python)\r\nMy rpi3 runs consistently without issues ", "> With MacOS restarting the computer solved the problem.\r\n\r\nI'm having the same problem and restarting my machine does nothing. :(", "For me I tried this also on a RPI4 - 4GB and I still get this error when I connect the TPU to a USB3.0 port. If I use a USB2.0 port it works. ", "@Syirrus I believe this USB3.0 issues is actually a bug in rpi4's HW. Check [here](https://www.raspberrypi.org/documentation/hardware/raspberrypi/usb/README.md)", "I encounter this problem on the Coral Edge TPU boards as well.\r\nSetup:\r\n1. Board is updated (not that it matters since no new software has been released for some time.)\r\n2. I am trying to run the classification example from [this repository](https://github.com/google-coral/tflite/tree/master/python/examples/classification)\r\n3. I have installed the wheel for tflite_runtime-2.1.0.post1-cp35-cp35m. Newer wheels refuse to install with errors:\r\n```\r\n$ pip3 install https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp38-cp38-linux_aarch64.whl\r\ntflite_runtime-2.1.0.post1-cp38-cp38-linux_aarch64.whl is not a supported wheel on this platform.\r\n```\r\n4. I haven't made any changes, I'm just trying to run the hello-world of this device and confirm stuff is working. \r\n\r\n\r\nFull error is below:\r\n```\r\ntflite/python/examples/classification$ python3 classify_image.py \\\r\n>   --model models/mobilenet_v2_1.0_224_inat_bird_quant.tflite \\\r\n>   --labels models/inat_bird_labels.txt \\\r\n>   --input images/parrot.jpg\r\nTraceback (most recent call last):\r\n  File \"classify_image.py\", line 122, in <module>\r\n    main()\r\n  File \"classify_image.py\", line 99, in main\r\n    interpreter = make_interpreter(args.model)\r\n  File \"classify_image.py\", line 73, in make_interpreter\r\n    {'device': device[0]} if device else {})\r\n  File \"/usr/local/lib/python3.5/dist-packages/tflite_runtime/interpreter.py\", line 161, in load_delegate\r\n    delegate = Delegate(library, options)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tflite_runtime/interpreter.py\", line 90, in __init__\r\n    self._library = ctypes.pydll.LoadLibrary(library)\r\n  File \"/usr/lib/python3.5/ctypes/__init__.py\", line 425, in LoadLibrary\r\n    return self._dlltype(name)\r\n  File \"/usr/lib/python3.5/ctypes/__init__.py\", line 347, in __init__\r\n    self._handle = _dlopen(self._name, mode)\r\nOSError: libedgetpu.so.1: cannot open shared object file: No such file or directory\r\nException ignored in: <bound method Delegate.__del__ of <tflite_runtime.interpreter.Delegate object at 0xffff77fb1e80>>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tflite_runtime/interpreter.py\", line 125, in __del__\r\n    if self._library is not None:\r\nAttributeError: 'Delegate' object has no attribute '_library'\r\n```\r\n\r\nOther information\r\n```\r\n$ uname -a\r\nLinux t2pu 4.9.51-imx #1 SMP PREEMPT Thu Jan 31 01:58:26 UTC 2019 aarch64 GNU/Linux\r\n\r\n$ pip3 list\r\nchardet (2.3.0)\r\ncycler (0.10.0)\r\nCython (0.25.2)\r\ndecorator (4.0.11)\r\nedgetpu (1.2.0)\r\nedgetpuvision (1.0)\r\nhttplib2 (0.9.2)\r\nipython (5.1.0)\r\nipython-genutils (0.1.0)\r\nmatplotlib (2.0.0)\r\nnumpy (1.12.1)\r\npexpect (4.2.1)\r\npickleshare (0.7.4)\r\nPillow (4.0.0)\r\npip (9.0.1)\r\nprompt-toolkit (1.0.9)\r\nprotobuf (3.0.0)\r\nptyprocess (0.5.1)\r\npycurl (7.43.0)\r\nPygments (2.2.0)\r\npygobject (3.22.0)\r\nPyJWT (1.4.2)\r\npyparsing (2.1.10)\r\npython-apt (1.4.1)\r\npython-dateutil (2.5.3)\r\npython-debian (0.1.30)\r\npython-debianbts (2.6.1)\r\npytz (2016.7)\r\nreportbug (7.1.7)\r\nrequests (2.12.4)\r\nscipy (0.18.1)\r\nsetuptools (33.1.1)\r\nsimplegeneric (0.8.1)\r\nsix (1.10.0)\r\ntflite-runtime (2.1.0.post1)\r\ntraitlets (4.3.1)\r\nunattended-upgrades (0.1)\r\nurllib3 (1.19.1)\r\nwcwidth (0.1.7)\r\nwheel (0.29.0)\r\n\r\n```\r\nAnd an ipython3 session:\r\n```\r\n$ ipython3\r\nPython 3.5.3 (default, Sep 27 2018, 17:25:39) \r\nType \"copyright\", \"credits\" or \"license\" for more information.\r\n\r\nIPython 5.1.0 -- An enhanced Interactive Python.\r\n?         -> Introduction and overview of IPython's features.\r\n%quickref -> Quick reference.\r\nhelp      -> Python's own help system.\r\nobject?   -> Details about 'object', use 'object??' for extra details.\r\n\r\n\r\nIn [2]: import tflite_runtime as tf\r\n\r\nIn [3]: tf.__version__\r\nOut[3]: '2.1.0.post1'\r\n\r\n\r\n```", "@youngelf \r\n```\r\nOSError: libedgetpu.so.1: cannot open shared object file: No such file or directory\r\n```\r\nMeans that you son't have libedgetpu installed. \r\nAlso, can you share your outputs of:\r\n```\r\ncat /etc/os-release\r\ndpkg -l | grep edgetpu\r\n```\r\nIt looks like you are running a very old version of mendel OS to even have python3.5 as default. I know there are a lot of tutorials out there that had older versions but I definitely suggest installing new OS. Currently, we're at mendel 5.0, try this: https://coral.ai/docs/dev-board/reflash/#flash-a-new-system-image", "Thanks, namburger. I flashed it last weekend, here we go:\r\n\r\n```\r\n$ cat /etc/os-release\r\nPRETTY_NAME=\"Mendel GNU/Linux 5 (Eagle)\"\r\nNAME=\"Mendel GNU/Linux\"\r\nID=mendel\r\nID_LIKE=debian\r\nHOME_URL=\"https://coral.ai/\"\r\nSUPPORT_URL=\"https://coral.ai/\"\r\nBUG_REPORT_URL=\"https://coral.ai/\"\r\nVERSION_CODENAME=\"eagle\"\r\n\r\n$ dpkg -l |grep edgetpu\r\nii  edgetpu-examples                     14.1                                all          Example code for Edge TPU Python API\r\nii  edgetpudemo                          3-1                                 all          Edge TPU demo script\r\nii  libedgetpu-dev                       14.1                                arm64        Development files for libedgetpu\r\nii  libedgetpu1-std:arm64                14.1                                arm64        Support library for Edge TPU\r\nii  python3-edgetpu                      14.1                                arm64        Edge TPU Python API\r\nii  python3-edgetpuvision                6-1                                 arm64        EdgeTPU camera API\r\n\r\n\r\n# wget https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp37-cp37m-linux_aarch64.whl--2020-11-05 17:56:48--  https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp37-cp37m-linux_aarch64.whl\r\nResolving dl.google.com (dl.google.com)... 172.217.5.110, 2607:f8b0:4007:803::200e\r\nConnecting to dl.google.com (dl.google.com)|172.217.5.110|:443... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: 1940683 (1.9M) [application/octet-stream]\r\nSaving to: \u2018tflite_runtime-2.1.0.post1-cp37-cp37m-linux_aarch64.whl\u2019\r\n\r\ntflite_runtime-2.1.0.post1 100%[=======================================>]   1.85M  2.89MB/s    in 0.6s\r\n\r\n2020-11-05 17:56:49 (2.89 MB/s) - \u2018tflite_runtime-2.1.0.post1-cp37-cp37m-linux_aarch64.whl\u2019 saved [1940683/1940683]\r\n\r\n# pip3 install tflite_runtime-2.1.0.post1-cp37-cp37m-linux_aarch64.whl\r\nProcessing ./tflite_runtime-2.1.0.post1-cp37-cp37m-linux_aarch64.whl\r\nRequirement already satisfied: numpy>=1.12.1 in /usr/lib/python3/dist-packages (from tflite-runtime==2.1.0.post1) (1.16.2)\r\nInstalling collected packages: tflite-runtime\r\nSuccessfully installed tflite-runtime-2.1.0.post1\r\n\r\n\r\n$ ipython3\r\nPython 3.7.3 (default, Dec 20 2019, 18:57:59)\r\nType \"copyright\", \"credits\" or \"license\" for more information.\r\n\r\nIPython 5.8.0 -- An enhanced Interactive Python.\r\n?         -> Introduction and overview of IPython's features.\r\n%quickref -> Quick reference.\r\nhelp      -> Python's own help system.\r\nobject?   -> Details about 'object', use 'object??' for extra details.\r\n\r\nIn [1]: import tflite_runtime as tf\r\n\r\nIn [2]: tf.__version__\r\nOut[2]: '2.1.0.post1'\r\n\r\n\r\n~/coral/tflite/python/examples/classification$ python3 classify_image.py \\\r\n> --model models/mobilenet_v2_1.0_224_inat_bird_quant_edgetpu.tflite \\\r\n> --labels models/inat_bird_labels.txt \\\r\n> --input images/parrot.jpg\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/dist-packages/tflite_runtime/interpreter.py\", line 161, in load_delegate\r\n    delegate = Delegate(library, options)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tflite_runtime/interpreter.py\", line 120, in __init__\r\n    raise ValueError(capture.message)\r\nValueError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"classify_image.py\", line 122, in <module>\r\n    main()\r\n  File \"classify_image.py\", line 99, in main\r\n    interpreter = make_interpreter(args.model)\r\n  File \"classify_image.py\", line 73, in make_interpreter\r\n    {'device': device[0]} if device else {})\r\n  File \"/usr/local/lib/python3.7/dist-packages/tflite_runtime/interpreter.py\", line 164, in load_delegate\r\n    library, str(e)))\r\nValueError: Failed to load delegate from libedgetpu.so.1\r\n```\r\n\r\n(I suspect hardware damage, as on this board the Wifi is detected with 'lshw' but nmcli doesn't think it exists.)\r\n\r\n```\r\n# nmcli d\r\nDEVICE  TYPE      STATE      CONNECTION         \r\neth0    ethernet  connected  Wired connection 1 \r\nusb0    ethernet  connected  gadget0            \r\nusb1    ethernet  connected  gadget1            \r\nlo      loopback  unmanaged  --             \r\n\r\n```\r\nFull output of 'lshw' attached here:\r\n[lshw.txt](https://github.com/tensorflow/tensorflow/files/5498462/lshw.txt)\r\n\r\n", "@youngelf does \r\nls /dev/apex_0 \r\nshows anything? ", "Nope, nothing. \r\n\r\n```\r\n# ls /dev/\r\nautofs           input             mxc_hantro          random     tty22  tty46    ttymxc2   vcs2\r\nblock            ion               net                 rtc        tty23  tty47    ttyp0     vcs3\r\n... \r\ninitctl          mqueue            ptypf               tty21      tty45  ttymxc1  vcs1\r\n\r\n```\r\n\r\nNothing called apex.", "@youngelf apex is literally how the software interact with the edgetpu lol\r\nCould you ping us an email to coral-support@google.com? \r\nLooks like some how the edgetpu completely disappear from the board, we can issue a replacement", "Super, thanks. Will do.", "Got the updated boards, and I was able to run the inference in the classify_image.py example from [here](https://coral.ai/docs/dev-board/get-started#6-run-a-model-using-the-pycoral-api).\r\n\r\nThanks so much!", "@bsddemon Could you please try to  upgrade the **`tflite_runtime`** package from [link](https://www.tensorflow.org/lite/guide/python) and try to execute your code using latest stable version of TF **`2.6.0`** .Please let us know if helps?Thanks!", "I have also faced this issue (and #32743) and I think one more possible reason is that the user may not be the part of `apex` group. So, adding user to the `apex` group can fix this issue. This can be done using:\r\n\r\n`sudo usermod -a -G apex <user_name>`\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34135\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34135\">No</a>\n"]}, {"number": 34134, "title": "R1.13", "body": "merge master an 1.13\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34134) for more info**.\n\n<!-- need_sender_cla -->", "Hi Davari393@, \r\n\r\nI'm not sure what you're trying to do here, but I'l pretty sure this isn't the right way to do it.\r\n\r\nIs there some fix from the 1.13 branch that never got merged to master?"]}, {"number": 34133, "title": "logits and labels must have the same first dimension", "body": "**System information**\r\n- MAC OS 10.14.6\r\n- Tensorflow 1.14\r\n\r\n**Working model**\r\n```python\r\ninput_layer=Input(shape=(X.shape[1],))\r\nmodel=Embedding(input_dim=len(vocab)+1,output_dim=32,input_length=X.shape[1])(input_layer)\r\nmodel = Bidirectional(LSTM(units = 50, return_sequences=True, recurrent_dropout=0.2))(model)\r\noutput_layer= Dense(3, activation=\"softmax\")(model)\r\n\r\nmodel = Model(input_layer,output_layer)\r\nmodel.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])\r\nmodel.summary()\r\n```\r\n\r\n**Trying to create same model with tf-lite layers of bi-directional LSTM**\r\n\r\n```python\r\n\r\nimport os\r\nos.environ['TF_ENABLE_CONTROL_FLOW_V2'] = '1'\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.lite.experimental.examples.lstm.rnn import bidirectional_dynamic_rnn\r\n\r\ndef build_LSTM_layer(num_layers):\r\n    lstm_layers=[]\r\n    for i in range(num_layers):\r\n        lstm_layers.append(tf.lite.experimental.nn.TFLiteLSTMCell(num_units=50,name='rnn{}'.format(i),forget_bias=1.0))\r\n    final_lstm_layer=tf.keras.layers.StackedRNNCells(lstm_layers)\r\n    return final_lstm_layer\r\ndef build_bidirectional(inputs,num_layers,use_dynamic_rnn=True):\r\n    lstm_inputs=transposed_inp=tf.transpose(inputs,[1,0,2])\r\n    outputs,output_states=bidirectional_dynamic_rnn(build_LSTM_layer(num_layers),build_LSTM_layer(num_layers),lstm_inputs,dtype=\"float\",time_major=True)\r\n    fw_lstm_output,bw_lstm_output=outputs\r\n    final_out=tf.concat([fw_lstm_output,bw_lstm_output],axis=2)\r\n    \r\n    final_out=tf.unstack(final_out,axis=0)\r\n   \r\n    resultant_out=final_out[-1]\r\n    \r\n    return resultant_out\r\n\r\ntf.reset_default_graph()\r\n\r\nmodel_tf = tf.keras.models.Sequential([\r\n  tf.keras.layers.Input(shape=(X.shape[1],), name='input'),  \r\n  tf.keras.layers.Embedding(input_dim=len(vocab)+1,output_dim=32,input_length=X.shape[1]),\r\n  tf.keras.layers.Lambda(build_bidirectional, arguments={'num_layers' : 2, 'use_dynamic_rnn': True}),\r\n  tf.keras.layers.Flatten(),\r\n  tf.keras.layers.Dense(3, activation=tf.nn.softmax, name='output')\r\n])\r\nmodel_tf.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\r\nmodel_tf.summary()\r\n```\r\n\r\n\r\n**Inputs are token sequence and output should be NER tags which i get from keras model but not from above model**\r\n```python\r\nX.shape = (30, 16)\r\ny.shape = (30, 16, 1)\r\n\r\nI/P = array([[15., 10., 38.,  4., 32., 57., 39.,  0.,  0.,  0.,  0.,  0.,  0., 0.,  0.,  0.],...])\r\nO/P = array([[[1.],[1.],[1.],[1.],[2.],[1.],[1.],[0.],[0.],[0.],\r\n         [0.],[0.],[0.],[0.],[0.],[0.]],...])\r\n```\r\n\r\n\r\n\r\n\r\n**Output logs**\r\n```Error\r\nEpoch 1/10\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-503-89e5191da57e> in <module>\r\n      2 train_x,test_x,train_y,test_y = train_test_split(X,y,test_size=0.2)\r\n      3 \r\n----> 4 history = model_tf.fit(train_x,train_y,epochs=10,batch_size=3)\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    778           validation_steps=validation_steps,\r\n    779           validation_freq=validation_freq,\r\n--> 780           steps_name='steps_per_epoch')\r\n    781 \r\n    782   def evaluate(self,\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\r\n    361 \r\n    362         # Get outputs.\r\n--> 363         batch_outs = f(ins_batch)\r\n    364         if not isinstance(batch_outs, list):\r\n    365           batch_outs = [batch_outs]\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in __call__(self, inputs)\r\n   3290 \r\n   3291     fetched = self._callable_fn(*array_vals,\r\n-> 3292                                 run_metadata=self.run_metadata)\r\n   3293     self._call_fetch_callbacks(fetched[-len(self._fetches):])\r\n   3294     output_structure = nest.pack_sequence_as(\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py in __call__(self, *args, **kwargs)\r\n   1456         ret = tf_session.TF_SessionRunCallable(self._session._session,\r\n   1457                                                self._handle, args,\r\n-> 1458                                                run_metadata_ptr)\r\n   1459         if run_metadata:\r\n   1460           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\nInvalidArgumentError: logits and labels must have the same first dimension, got logits shape [3,3] and labels shape [48]\r\n\t [[{{node loss/output_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]\r\n```\r\n", "comments": ["Hi @gadagashwini ,\r\n\r\nI tried your two codes, and I found out that there is no error under tensorflow==1.14.0. Could you give me more information?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34133\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34133\">No</a>\n"]}, {"number": 34132, "title": "BatchNorm doesn't work in custom Model or Layer.", "body": "**System information**\r\n-Windows10\r\n- TensorFlow: 2.0.0\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\n\r\n`InaccessibleTensorError: The tensor 'Tensor(\"batch_normalization/batch_normalization_trainable:0\", dtype=bool)' cannot be accessed here: it is defined in another function or code block. Use return values, explicit Python locals or TensorFlow collections to access it. Defined in: FuncGraph(name=build_graph, id=3078774714504); accessed from: FuncGraph(name=keras_graph, id=3077450685512).`\r\n\r\n**Code to reproduce the issue**\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, LeakyReLU\r\n\r\ndef get_norm(norm_type):\r\n    if norm_type == \"batch\":\r\n        return BatchNormalization()\r\n    else:\r\n        raise ValueError(f\"Unrecognized norm_type {norm_type}\")\r\n\r\nclass Discriminator(Model):\r\n    def __init__(self,\r\n                 base_filters=32,\r\n                 lrelu_alpha=0.2,\r\n                 pad_type=\"same\",\r\n                 norm_type=\"batch\"):\r\n        super(Discriminator, self).__init__(name=\"Discriminator\")\r\n        # 1------------------------------------------\r\n        self.conv1 = Conv2D(\r\n                filters=base_filters, # 32\r\n                kernel_size=3,     \r\n                padding=pad_type\r\n                )\r\n        self.relu1 = LeakyReLU(alpha=lrelu_alpha)\r\n        \r\n        # 2-------------------------------------------\r\n        self.conv2a = Conv2D(\r\n                filters=base_filters*2, # 64\r\n                strides=2,\r\n                kernel_size=3, \r\n                padding=pad_type\r\n                )\r\n        self.relu2a = LeakyReLU(alpha=lrelu_alpha)\r\n        self.conv2b = Conv2D(\r\n                filters=base_filters*4, # 128\r\n                kernel_size=3, \r\n                padding=pad_type\r\n                )\r\n        self.norm2 = get_norm(norm_type)\r\n        self.relu2b = LeakyReLU(alpha=lrelu_alpha)\r\n        \r\n        # 3-----------------------------------------------\r\n        self.conv3a = Conv2D(\r\n                filters=base_filters*4, # 128\r\n                strides=2,\r\n                kernel_size=3, \r\n                padding=pad_type\r\n                )\r\n        self.relu3a = LeakyReLU(alpha=lrelu_alpha)\r\n        self.conv3b = Conv2D(\r\n                filters=base_filters*8, # 256\r\n                kernel_size=3, \r\n                padding=pad_type\r\n                )\r\n        self.norm3 = get_norm(norm_type)\r\n        self.relu3b = LeakyReLU(alpha=lrelu_alpha)\r\n        \r\n        # 4----------------------------------------------\r\n        self.conv4 = Conv2D(\r\n                filters=base_filters*8, # 256\r\n                kernel_size=3, \r\n                padding=pad_type\r\n                )\r\n        self.norm4 = get_norm(norm_type)\r\n        self.relu4 = LeakyReLU(alpha=lrelu_alpha)\r\n        \r\n        # final--------------------------------------------\r\n        self.conv_final = Conv2D(\r\n                filters=1, # 256\r\n                kernel_size=3, \r\n                padding=pad_type\r\n                )\r\n\r\n    def build(self, input_shape):\r\n        super(Discriminator, self).build(input_shape)\r\n\r\n    def call(self, input_tensor, training=False):\r\n        # 1---------------------------------------------------\r\n        x = self.conv1(input_tensor, training=training)\r\n        x = self.relu1(x, training=training)\r\n        \r\n        # 2--------------------------------------------------\r\n        x = self.conv2a(x, training=training)\r\n        x = self.relu2a(x, training=training)\r\n        x = self.conv2b(x, training=training)\r\n        x = self.norm2(x, training=training)\r\n        x = self.relu2b(x, training=training)\r\n        \r\n        # 3------------------------------------------------\r\n        x = self.conv3a(x, training=training)\r\n        x = self.relu3a(x, training=training)\r\n        x = self.conv3b(x, training=training)\r\n        x = self.norm3(x, training=training)\r\n        x = self.relu3b(x, training=training)\r\n        \r\n        # 4-------------------------------------------------\r\n        x = self.conv4(x, training=training)\r\n        x = self.norm4(x, training=training)\r\n        x = self.relu4(x, training=training)\r\n        \r\n        # final--------------------------------------------\r\n        x = self.conv_final(x, training=training)\r\n        \r\n        return x\r\n\r\nif __name__ == \"__main__\":\r\n    import numpy as np\r\n\r\n    shape = (1, 128, 128, 3)\r\n    nx = np.random.rand(*shape).astype(np.float32)\r\n    t = tf.keras.Input(shape=nx.shape[1:], batch_size=nx.shape[0])\r\n\r\n    #tf.keras.backend.clear_session()\r\n    d = Discriminator()\r\n    out = d(t)\r\n    d.summary()\r\n    print(f\"Input  Shape: {t.shape}\")\r\n    print(f\"Output Shape: {out.shape}\")\r\n```\r\n", "comments": ["@FancyVin,\r\nWhen trying to reproduce your error, an intermediate error, shown below has occurred,  `ModuleNotFoundError: No module named 'keras_contrib'`. Can you please help us reproduce the error. Thanks!\r\n", "Related / Duplicate of [32477](https://github.com/tensorflow/tensorflow/issues/32477) I guess.", "@FancyVin,\r\nAs @cecabert mentioned, it is a duplicate of [32477](https://github.com/tensorflow/tensorflow/issues/32477). Please confirm if we can close this issue as it's being tracked there. Thanks!", "@rmothukuru\r\n> @FancyVin,\r\n> When trying to reproduce your error, an intermediate error, shown below has occurred, `ModuleNotFoundError: No module named 'keras_contrib'`. Can you please help us reproduce the error. Thanks!\r\n\r\nI updated the code, it should work now.", "Closing the issue as it is the duplicate of [32477](https://github.com/tensorflow/tensorflow/issues/32477). Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34132\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34132\">No</a>\n"]}, {"number": 34131, "title": "Negative sampling / candidate sampling with Keras API", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nIt looks like TF Keras has no built in functions to do candidate sampling like in TF 1.0. As far as I can tell, the official keras word2vec example doesn't have negative sampling https://www.tensorflow.org/tutorials/text/word_embeddings even though it's an integral part for producing quality word embeddings. \r\n\r\n**Will this change the current api? How?**\r\n\r\nThe original tensorflow losses had options for negative sampling, including different distributions to select from. So perhaps some new loss functions and candidate sampling options will need to be added. \r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone who is training embedding models. \r\n\r\n\r\n", "comments": ["@Santosh-Gupta What do you mean by \"The original tensorflow losses had options for negative sampling, including different distributions to select from\" whereas TF2 doesn't have it?\r\nAn example of \"keras equivalent\" of negative sampling can be checked [here](https://adventuresinmachinelearning.com/word2vec-keras-tutorial/)", "Closing this. If the above article doesn't help you, let us know!"]}, {"number": 34130, "title": "Improve error message of RaggedTensor by showing data type explicitly", "body": "While working on writing a tf.data pipeline with RaggedTensor the following\r\nerror showed up:\r\n```\r\n\r\n    def raise_from(value, from_value):\r\n>       raise value\r\nE       InvalidArgumentError: Expected splits Tensor dtype: 9, found: 3 [Op:RaggedTensorFromVariant]\r\n\r\n/usr/local/lib/python2.7/dist-packages/six.py:737: InvalidArgumentError\r\n```\r\n\r\nIt is not very obvious about the exact type that needs. Until found out in\r\n`tensorflow/core/framework/types.proto` that `3` is `int32` and `9` is `int64`.\r\n\r\nThis PR enhance the error message by explictily print out the DataType in string,\r\nso the message will be:\r\n```\r\nE       InvalidArgumentError: Expected splits Tensor dtype: int64, found: int32 [Op:RaggedTensorFromVariant]\r\n```\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 34129, "title": "Looking for a simple gray scale and threshold function in tflite with flutter", "body": "Hey everybody,\r\nI know it's a wrong place to ask this question but I couldn't anywhere else active like GitHub, so here it goes.\r\nI'm creating a flutter app that needs to get the stream of the frames from the camera and do some image processing on it, actually, just turning it into grayscale and do a threshold operation.\r\nI couldn't find any library that can do that in Flutter.\r\nI used to do these operations with Opencv and numpy in python, but with flutter, it's a whole other story.\r\nI thought that maybe flutter \"tflite\" has this feature in it.\r\n\r\nThanks a lot and I'm sorry to post this question here.", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\n Thanks!\r\n"]}, {"number": 34128, "title": "Exponential decay - missing argument step()", "body": "I am facing the below error while using the exponential decay in Adam optimizer - \r\n\r\n**Code** \r\n\r\n`lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\r\n    initial_learning_rate = 0.0001,\r\n    decay_steps=100000,\r\n    decay_rate=0.96,\r\n    staircase=True)\r\n\r\nmodel = sm.Unet(\r\n    'resnet50', \r\n    classes=4,\r\n    input_shape=(320, 480, 3),\r\n    activation='sigmoid'\r\n)\r\nmodel.compile(optimizer = Adam(lr = lr_schedule) ,  loss      = bce_dice_loss, metrics   = [dice_coef])\r\n`\r\n\r\n**Error** \r\nTypeError: __call__() missing 1 required positional argument: 'step'\r\n\r\nWhat am I missing?\r\nPlease suggest.", "comments": ["Not sure which TF version you are using, but I think the problem is with the \"lr\" parameter in Adam optimizer.\r\nThe lr parameter is only there for backward compatibility and it doesn't support learning rate schedule.\r\n\r\nYou should use learning_rate parameter instead as follows:\r\n\r\n`model.compile(optimizer = Adam(learning_rate = lr_schedule) , loss = bce_dice_loss, metrics = [dice_coef])`\r\n\r\nHope this works!", "Okay, thanks. \r\nFor the moment, I have found an alternative , but this error is within function ExponentialDecay - \r\nI dig deeper into its documentation and there is an argument **Step** which is causing this error.\r\nUnfortunately, I could not understand it from the documentation.\r\nThanks", "**Step** argument is the number of global steps of the optimizer, it's passed to the **call** method of the learning rate schedule to calculate the current learning rate.\r\nIt's supposed to be done implicitly by the optimizer during training, however good thing you find an alternative to your problem.", "Yes, this is the doubt here - **Step** is an implicit argument, then how do we alter it to get the execution correct.", "@PN12 \r\n\r\nWhich TensorFlow version you are using?.Thanks!", "Version is '1.15.0'\r\n", "@PN12 Can you please share a simple standalone code to reproduce the issue? Thanks!", "Okay, I will share the same, at the moment an alternative has been used as a fix.\r\n", "@PN12 Can you please share a simple standalone code to reproduce the issue? Thanks!\r\n\r\nPlease close the issue If the issue was already resolved. Thanks!", "@jvishnuvardhan \r\n\r\nClosing the issue at the moment.\r\n\r\nThanks.\r\n", "@ghost\r\nHow did you solve this? What is your alternative? I met the exactly same error. \r\n\r\n> Okay, thanks.\r\n> For the moment, I have found an alternative , but this error is within function ExponentialDecay -\r\n> I dig deeper into its documentation and there is an argument **Step** which is causing this error.\r\n> Unfortunately, I could not understand it from the documentation.\r\n> Thanks\r\n\r\n", "> @ghost\r\n> How did you solve this? What is your alternative? I met the exactly same error.\r\n> \r\n> > Okay, thanks.\r\n> > For the moment, I have found an alternative , but this error is within function ExponentialDecay -\r\n> > I dig deeper into its documentation and there is an argument **Step** which is causing this error.\r\n> > Unfortunately, I could not understand it from the documentation.\r\n> > Thanks\r\n\r\nHi, have you found the solution? I have same error.", "I have the same problem"]}, {"number": 34127, "title": "Impossible to use tf.keras.callbacks.ModelCheckpoint in distributed training", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- TensorFlow version (use command below): 2.0.0\r\n\r\n**Describe the current behavior**\r\nIt is not possible to use `tf.keras.callbacks.ModelCheckpoint` in distributed training:\r\n`RuntimeError: `add_update` was called in a cross-replica context. This is not expected. If you require this feature, please file an issue.`\r\n\r\n**Code to reproduce the issue**\r\nSee [this Colab notebook](https://colab.research.google.com/drive/11CmG16-x9z-MMKoSeKaEKZ0zrnBdeMMV).", "comments": ["Note: no error if we use `.h5` instead of TF checkpoint format. However, just a few hours ago, @karmel and @jvishnuvardhan [recommended me to use TF format](https://github.com/tensorflow/tensorflow/issues/34016) (because `.h5` can cause issues, okay fine). So, which one should I use finally? None is working? I am lost... \r\n\r\n", "Hello @netw0rkf10w, sorry about the confusion. I was trying to repro locally with your example, but unfortunately there was some issue downloading the dataset. When I tried some other dataset I have the program could finish just fine. Would you mind providing a minimal repro with some dummy data so I can quickly repro? Thank you.", "Hi @rchao. Could you tell me what kind of issues did you have? I have just tested again and there was no issue with the download of data.\r\n\r\nNote that in the notebook, I created the dataset and fit the model in the same code cell, so maybe the error you got was from the training part and not from the dataset download part. For clarity, I have split them into different cells. Please try it again. Thanks.", "Hello @netw0rkf10w, sorry for the late response. I made a copy of your updated colab and used nightly instead:\r\n\r\n```\r\n%tensorflow_version 2.x\r\n!pip uninstall -y -q tensorflow\r\n!pip install -U -q tf-nightly\r\n```\r\n\r\nand it could work fine without the problem. Can you take a look if that fixes your issue? The TF version I verified to work at is 2.1.0-dev20191202.", "Closing for lack of activity. Please reopen if the issue still exists. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34127\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34127\">No</a>\n"]}, {"number": 34126, "title": "Fixed typo", "body": "Fixed typo causing invalid code syntax in description. \r\n--> fixed missing parentheses.", "comments": []}, {"number": 34125, "title": "Replace effect with affect in docstring", "body": "In the docstring of `tf.io.TFRecordOptions` initializer, effect should be replaced with affect", "comments": ["We will not be encouraging one liner changes as this is expensive process, thank you for your interest.\r\nCC @mihaimaruseac"]}, {"number": 34124, "title": "[Docs] Doc example of DeviceSpec doesn't work with tf 2.0", "body": "Doc link: https://www.tensorflow.org/api_docs/python/tf/DeviceSpec\r\n```python\r\nimport tensorflow as tf\r\ndevice_spec = tf.DeviceSpec(job=\"ps\", device_type=\"CPU\", device_index=0)\r\nwith tf.device(device_spec):\r\n  pass\r\n```\r\nGot error:\r\n```\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/context.py in __enter__(self)\r\n   1508     try:\r\n-> 1509       new_device_name, new_device_spec = _device_parsing_cache[cache_key]\r\n   1510     except TypeError:\r\n\r\nKeyError: ('', <tensorflow.python.framework.device_spec.DeviceSpecV2 object at 0x7f34c74de528>)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n1 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/context.py in __enter__(self)\r\n   1517         if not isinstance(new_device_name, six.string_types):\r\n   1518           raise ValueError(\"Expecting a string device name. Got %s(%s)\" %\r\n-> 1519                            (type(new_device_name), new_device_name))\r\n   1520         device_spec = pydev.DeviceSpec.from_string(new_device_name)\r\n   1521         if old_device_name:\r\n\r\nValueError: Expecting a string device name. Got <class 'tensorflow.python.framework.device_spec.DeviceSpecV2'>(<tensorflow.python.framework.device_spec.DeviceSpecV2 object at 0x7f34c74de528>)\r\n```\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): 2.0.0\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Syntax works fine on TensorFlow 1.15.0 (all 1.x): [gist](https://colab.research.google.com/gist/nikochiko/7cefa862d487ff9d6d31c9c2034a3cb8/34124.ipynb#scrollTo=6jMLFtVeeJpX)\r\nThrows error in TensorFlow 2.x: [gist](https://colab.research.google.com/drive/1nkoSsW4hINcp03EPwLC0NFQhpuJURUbW#scrollTo=tB54LogAgL1D)\r\n", "Update: found out the problem lies in using this command with eager execution. \r\nFind [gist](https://colab.research.google.com/gist/nikochiko/0de00bb23b936f1ce714bf969aae5f1b/34124_fix.ipynb) here. Must be called with `spec_name.to_string()` function in eager execution.\r\nWill update the docs. Issue can be assigned to me. Will put a PR soon.", "Thank you @VoVAllen for bringing this issue to attention. ", "This issue is fixed with latest version TF nightly build '2.1.0-dev20191111'. Thanks!", "However I would consider it as an implementation bug instead of a documentation error. Thanks for your help!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34124\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34124\">No</a>\n"]}, {"number": 34123, "title": "How can I know that the accuracy is good or not using RMSE for regression tasks?", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):1.13.1\r\n- Python version:3.4.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:9.0/7.4\r\n- GPU model and memory:Nvidia Geforce 840m\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nHello, \r\nMy question is very simple, I have trained a model for regression task ( detect eye region with landmarks ) using training dataset of 160000 images and evaluate it after 50000 steps using eval_mse that based on `tf.metrics.root_mean_squared_error`.\r\nI would like to know how can I know that the accuracy is good by viewing the value of the RMSE using the `Eval_mode` and `Tensorboard`\r\n\r\nThat was some screenshots:\r\n![losse](https://user-images.githubusercontent.com/19480228/68531460-91e1a400-0312-11ea-8352-380751600678.PNG)\r\n\r\n![graphhh](https://user-images.githubusercontent.com/19480228/68531464-9908b200-0312-11ea-822c-987a88f1528b.PNG)\r\n\r\n\r\nHow can I understand the value of the `eval_mse` and the scalar of the RMSE?\r\n\r\n\r\n", "comments": ["@abdou31 Please refer to the following [link](https://stats.stackexchange.com/questions/56302/what-are-good-rmse-values) to understand what RMSE values are good for your problem.\r\n\r\nAlso please post questions like these in stack overflow as github is only meant for bug/performance, build/install and docs related issues. Thanks!"]}, {"number": 34122, "title": "Build failed on matrix_square_root_op using GCC 7.4.0 and Ubuntu 18.04", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n`\r\nDISTRIB_ID=Ubuntu\r\nDISTRIB_RELEASE=18.04\r\nDISTRIB_CODENAME=bionic\r\nDISTRIB_DESCRIPTION=\"Ubuntu 18.04.3 LTS\"\r\nNAME=\"Ubuntu\"\r\nVERSION=\"18.04.3 LTS (Bionic Beaver)\"\r\nID=ubuntu\r\nID_LIKE=debian\r\nPRETTY_NAME=\"Ubuntu 18.04.3 LTS\"\r\nVERSION_ID=\"18.04\"\r\nHOME_URL=\"https://www.ubuntu.com/\"\r\nSUPPORT_URL=\"https://help.ubuntu.com/\"\r\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\r\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\r\nVERSION_CODENAME=bionic\r\nUBUNTU_CODENAME=bionic\r\n`\r\n\r\n- TensorFlow installed from (source or binary):\r\nTensorflow github hash: ce0b9aa347979cfbe4291440a3aeeb7af3dda41d\r\n- TensorFlow version:\r\n2.0\r\n\r\n- Python version:\r\nPython 3.6.8\r\n\r\n- Installed using virtualenv? pip? conda?:\r\nBuilding\r\n\r\n- Bazel version (if compiling from source):\r\nbazel 0.29.1\r\n\r\n- GCC/Compiler version (if compiling from source):\r\n`\r\nUsing built-in specs.\r\nCOLLECT_GCC=gcc\r\nCOLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/7/lto-wrapper\r\nOFFLOAD_TARGET_NAMES=nvptx-none\r\nOFFLOAD_TARGET_DEFAULT=1\r\nTarget: x86_64-linux-gnu\r\nConfigured with: ../src/configure -v --with-pkgversion='Ubuntu 7.4.0-1ubuntu1~18.04.1' --with-bugurl=file:///usr/share/doc/gcc-7/README.Bugs --enable-languages=c,ada,c++,go,brig,d,fortran,objc,obj-c++ --prefix=/usr --with-gcc-major-version-only --program-suffix=-7 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-libmpx --enable-plugin --enable-default-pie --with-system-zlib --with-target-system-zlib --enable-objc-gc=auto --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none --without-cuda-driver --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu\r\nThread model: posix\r\ngcc version 7.4.0 (Ubuntu 7.4.0-1ubuntu1~18.04.1) \r\n`\r\n\r\n- CUDA/cuDNN version:\r\nNA\r\n\r\n- GPU model and memory:\r\nNA\r\n\r\n**Describe the problem**\r\n\r\nI'm attempting to build Tensorflow from source to enable AVX2 and FMA instructions. I get the following error, which appears to be a compiler bug???\r\n\r\n/projects/tensorflow/tensorflow/core/kernels/BUILD:3559:1: C++ compilation of rule '//tensorflow/core/kernels:matrix_square_root_op' failed (Exit 4)\r\ngcc: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n\r\n\r\n", "comments": ["Please refer to this [table](https://www.tensorflow.org/install/source.md#linux) for tested build configurations. Bazel 0.29.1 is not supported, use Bazel 0.26.1 instead.", "When you run ./configure with bazel 0.26.1 you are told that you must install a higher version of bazel.", "Can you send a more detailed error log? ", "@nlbutts, Please Provide the exact sequence of commands / steps that you executed before running into the problem and also include the full error log. Thanks!", "`gcc: internal compiler error: Killed (program cc1plus)` is a common error seen at random spots when you don't have enough memory on the system your compiling TensorFlow on. Try using the `--local_resources` flag to limit resource usage during the build.\r\n\r\nSomething like:\r\n\r\n```\r\nbazel build -c opt --local_resources 4096,4.0,1.0 //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n", "OK, I got it working on one of my computers. I uninstalled Bazel 0.29.1 and installed Bazel 0.26.1. Then I limited the amount of memory allowed to be used with this command:\r\n`\r\nbazel build --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --local_ram_resources=8192 -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both -k //tensorflow/tools/pip_package:build_pip_package\r\n`\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34122\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34122\">No</a>\n"]}, {"number": 34121, "title": "Update documentation of tf.debugging.assert_shapes", "body": "", "comments": ["@amitkumarj441 Can you please resolve conflicts? Thanks!", "@amitkumarj441 Could you please address the reviewer comments and resolve conflicts? Thanks!", "@amitkumarj441: I'm really sorry but I think I had a typo in my previous review. My comment should have read `Though we accept tuples in v2 we only allow dictionaries in v1`. Oops.\r\n\r\nAs the code has changed quite a bit, it seems like the v2 documentation has already been fixed. If you want you can go ahead and change the v1 documentation https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/check_ops.py#L1661 to:\r\n\r\n```\r\n  tf.assert_shapes([\r\n    (x, ('N', 'Q')),\r\n    (y, ('N', 'D')),\r\n    (param, ('Q',)),\r\n    (scalar, ())\r\n  ])\r\n```\r\n\r\nSorry again for the confusion.", "> @amitkumarj441: I'm really sorry but I think I had a typo in my previous review. My comment should have read `Though we accept tuples in v2 we only allow dictionaries in v1`. Oops.\r\n> \r\n> As the code has changed quite a bit, it seems like the v2 documentation has already been fixed. If you want you can go ahead and change the v1 documentation https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/check_ops.py#L1661 to:\r\n> \r\n> ```\r\n>   tf.assert_shapes([\r\n>     (x, ('N', 'Q')),\r\n>     (y, ('N', 'D')),\r\n>     (param, ('Q',)),\r\n>     (scalar, ())\r\n>   ])\r\n> ```\r\n> \r\n> Sorry again for the confusion.\r\n\r\nI have seen the changes in code, and is fixed here [L1602/L1609](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/check_ops.py#L1602) as well. Isn't it? \r\nI'll update the v1 documentation.", "> @amitkumarj441 Could you please address the reviewer comments and resolve conflicts? Thanks!\r\n\r\n@gbaned @jaingaurav Any updates?", "I'll investigate why it didn't get automatically merged"]}, {"number": 34120, "title": "The latest version(2.0.0) of TensorFlow import protobuf error", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):2.0.0\r\n- Python version:3.6.0\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\nv2.0.0-rc2-26-g64c3d382ca 2.0.0\r\n\r\n**Describe the current behavior**\r\n\r\nImporting tensorflow gives me following error.\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"E:\\ProgramData\\Anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\__init__.py\", line 98, in <module>\r\n    from tensorflow_core import *\r\n  File \"E:\\ProgramData\\Anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"E:\\ProgramData\\Anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"E:\\ProgramData\\Anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"E:\\ProgramData\\Anaconda3\\envs\\ai\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"E:\\ProgramData\\Anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 52, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"E:\\ProgramData\\Anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow_core\\core\\framework\\graph_pb2.py\", line 7, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"E:\\ProgramData\\Anaconda3\\envs\\ai\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 47, in <module>\r\n    from google.protobuf.pyext import _message\r\nImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u7a0b\u5e8f\u3002\r\n```\r\n**Solution**\r\nI already found a solution to this problem. It is related to the pip package protobuf 3.6.0. I solved this problem by issuing following commands:\r\n```\r\npip uninstall protobuf\r\npip install protobuf==3.6.0\r\n```\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\nimport tensorflow\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@chenquan \r\n\r\nLooks like you found solution for this issue. Can you please confirm, whether we can close this issue as it was resolved. Thanks!", "> @chenquan\r\n> \r\n> Looks like you found solution for this issue. Can you please confirm, whether we can close this issue as it was resolved. Thanks!\r\n\r\nYes! According to the installation log, it is caused by `protobuf > = 3.6.1`.The installation of version 3.6.0 can work normally.", "I am closing this issue, since it was resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34120\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34120\">No</a>\n"]}, {"number": 34119, "title": "TF2.0 Calling set_session before fit_generator causes training to freeze when using multiprocessing", "body": "**System information**\r\n- Have I written custom code: No\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- Mobile device if the issue happens on mobile device: N/A\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- Python version: 3.6\r\n- Bazel version: N/A\r\n- GCC/Compiler version: N/A\r\n- CUDA/cuDNN version: 10.0 / 7\r\n- GPU model and memory: GTX 1080Ti\r\n\r\n**Describe the current behavior**\r\nI am trying to modify TF session parameters in combination with Keras (e.g. the allowed GPU memory) and use `set_session()` to store these parameters. However, if `set_session()` is called  before `fit_generator()`, it causes the training to freeze when using multiprocessing. To reproduce the error use the following code. The Colab gist to reproduce the error can be found [here](https://colab.research.google.com/gist/moberweger/2553560a5deeb2eaa5e3cfc23516ef34/untitled217.ipynb). The colab execution of the code in question is not responding, so I stopped executing after 10 min, which ultimately gives me the error. This problems seems to occur also in 1.14, but not 1.15 !? as indicated in #33973. The gist might work when executed the first time, but the error can be reproduce after running the script a few times.\r\n\r\n**Describe the expected behavior**\r\nNo freeze should occur.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.keras.backend as K\r\nfrom tensorflow.keras.layers import Input, Conv2D\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.utils import Sequence\r\n\r\n\r\ndef get_model(input_shape, output_shape, compile_batch_size):\r\n    assert K.image_data_format() == 'channels_last'\r\n    m_input = Input(shape=input_shape, batch_size=compile_batch_size)\r\n    m_output = Conv2D(output_shape[-1], 1, activation=None)(m_input)\r\n    model = Model(inputs=m_input, outputs=m_output)\r\n    return model\r\n\r\n\r\nclass DataGenerator(Sequence):\r\n    def __init__(self, batch_size, image_size, num_batches):\r\n        assert K.image_data_format() == 'channels_last'\r\n\r\n        self.batch_size = batch_size\r\n        self.image_size = image_size\r\n        self.num_batches = num_batches\r\n        self.on_epoch_end()\r\n\r\n    def __len__(self):\r\n        assert self.num_batches > 0\r\n        return self.num_batches\r\n\r\n    def __getitem__(self, index):\r\n        return np.zeros((self.batch_size,) + self.image_size).astype('float32'), np.zeros(\r\n            (self.batch_size,) + self.image_size).astype('float32')\r\n\r\n    def on_epoch_end(self):\r\n        pass\r\n\r\n\r\nif __name__ == '__main__':\r\n    print(tf.version.GIT_VERSION, tf.version.VERSION)\r\n\r\n    # set to True to trigger infinite wait\r\n    if True:\r\n        # limit GPU memory\r\n        config = tf.compat.v1.ConfigProto()\r\n        config.gpu_options.allow_growth = True\r\n        config.gpu_options.per_process_gpu_memory_fraction = 0.5\r\n        tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))\r\n\r\n    inp_shape = (128, 64, 2)\r\n    outp_shape = (128, 64, 2)\r\n    batch_size = 16\r\n\r\n    gen = DataGenerator(batch_size, inp_shape, 10)\r\n    model = get_model(inp_shape, outp_shape, batch_size)\r\n    model.summary()\r\n    model.compile(loss=[\"mse\"], optimizer=\"adam\", metrics=[\"accuracy\"])\r\n    model.fit_generator(gen, steps_per_epoch=10, epochs=5, workers=2, use_multiprocessing=True)\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nTraceback (most recent call last)\r\n\r\n<ipython-input-4-26fa57f6f7f8> in <module>()\r\n     55     model.summary()\r\n     56     model.compile(loss=[\"mse\"], optimizer=\"adam\", metrics=[\"accuracy\"])\r\n---> 57     model.fit_generator(gen, steps_per_epoch=10, epochs=5, workers=2, use_multiprocessing=True)\r\n\r\n7 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\r\n   1295         shuffle=shuffle,\r\n   1296         initial_epoch=initial_epoch,\r\n-> 1297         steps_name='steps_per_epoch')\r\n   1298 \r\n   1299   def evaluate_generator(self,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\r\n    219     step = 0\r\n    220     while step < target_steps:\r\n--> 221       batch_data = _get_next_batch(generator)\r\n    222       if batch_data is None:\r\n    223         if is_dataset:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_generator.py in _get_next_batch(generator)\r\n    361   \"\"\"Retrieves the next batch of input data.\"\"\"\r\n    362   try:\r\n--> 363     generator_output = next(generator)\r\n    364   except (StopIteration, errors.OutOfRangeError):\r\n    365     return None\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/utils/data_utils.py in get(self)\r\n    777     try:\r\n    778       while self.is_running():\r\n--> 779         inputs = self.queue.get(block=True).get()\r\n    780         self.queue.task_done()\r\n    781         if inputs is not None:\r\n\r\n/usr/lib/python3.6/multiprocessing/pool.py in get(self, timeout)\r\n    636 \r\n    637     def get(self, timeout=None):\r\n--> 638         self.wait(timeout)\r\n    639         if not self.ready():\r\n    640             raise TimeoutError\r\n\r\n/usr/lib/python3.6/multiprocessing/pool.py in wait(self, timeout)\r\n    633 \r\n    634     def wait(self, timeout=None):\r\n--> 635         self._event.wait(timeout)\r\n    636 \r\n    637     def get(self, timeout=None):\r\n\r\n/usr/lib/python3.6/threading.py in wait(self, timeout)\r\n    549             signaled = self._flag\r\n    550             if not signaled:\r\n--> 551                 signaled = self._cond.wait(timeout)\r\n    552             return signaled\r\n    553 \r\n\r\n/usr/lib/python3.6/threading.py in wait(self, timeout)\r\n    293         try:    # restore state no matter what (e.g., KeyboardInterrupt)\r\n    294             if timeout is None:\r\n--> 295                 waiter.acquire()\r\n    296                 gotit = True\r\n    297             else:\r\n```\r\n", "comments": ["@moberweger I think this was resolved in recent `tf-nightly`.  I ran your code with `tf-nightly` and I cannot reproduce the error you had faced with `TF2.0`. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/951378d5d7eabb359453dd5557d75dab/untitled37.ipynb).\r\n\r\nThere is a warning by the code that suggest you to update data pipelines with `tf.data` for better performance. Thanks!\r\n\r\n`WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.`\r\n\r\nPlease close the issue if this was resolved for you. Thanks!", "Thanks @jvishnuvardhan for checking.\r\nAlthough your notebook does not show the specific error, it is still deadlocked, ie stuck at `Epoch 1/5`.\r\n\r\nSeems that there is now a valid warning, and the `Model.fit_generator` interface is deprecated. So assuming there is no impact of this problem after the interface got removed in the future, I close this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34119\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34119\">No</a>\n", "@moberweger Code was taking longer time. So I cancelled after some time. Please open a new bug if you face any issues in updating input pipeline. Thanks! "]}, {"number": 34118, "title": "An strange error about `tf.estimator.BoostedTreesClassifier`", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n    * no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n    * windows 10 and Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n    * source\r\n- TensorFlow version (use command below):\r\n    * Tensorflow-gpu 2.0 and tensorflow2.0\r\n- Python version:\r\n    * python 3.6 and python 3.7\r\n- Bazel version (if compiling from source):\r\n    * no\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n    * cuda10\r\n- GPU model and memory:\r\n    * GTX 1060 6G\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI run the [boosted_trees_model_understanding tutorials](https://tensorflow.google.cn/tutorials/estimator/boosted_trees_model_understanding?hl=en), but it  high probability will run error, and my python will shutdown! I need to say, **that doesn`t run error every times**, but nearly 90%+ will run error. And this is the most strange things. \r\n![1](https://user-images.githubusercontent.com/15049049/68524375-293dfb80-0301-11ea-9bdc-9e3ca0684f03.png)\r\n![2](https://user-images.githubusercontent.com/15049049/68524377-2c38ec00-0301-11ea-9f1f-ff9d64fd55c1.png)\r\n![3](https://user-images.githubusercontent.com/15049049/68524378-2d6a1900-0301-11ea-8ff2-76eb6f0ca940.png)\r\n\r\n**Describe the expected behavior**\r\nIt should run like the  [boosted_trees_model_understanding tutorials](https://tensorflow.google.cn/tutorials/estimator/boosted_trees_model_understanding?hl=en)\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```python \r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom IPython.display import clear_output\r\n\r\n# Load dataset.\r\ndftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv')\r\ndfeval = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv')\r\ny_train = dftrain.pop('survived')\r\ny_eval = dfeval.pop('survived')\r\n\r\nimport tensorflow as tf\r\ntf.random.set_seed(123)\r\n\r\nfc = tf.feature_column\r\nCATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck',\r\n                       'embark_town', 'alone']\r\nNUMERIC_COLUMNS = ['age', 'fare']\r\n\r\ndef one_hot_cat_column(feature_name, vocab):\r\n    return tf.feature_column.indicator_column(\r\n        tf.feature_column.categorical_column_with_vocabulary_list(feature_name,\r\n                                                 vocab))\r\nfeature_columns = []\r\nfor feature_name in CATEGORICAL_COLUMNS:\r\n    # Need to one-hot encode categorical features.\r\n    vocabulary = dftrain[feature_name].unique()\r\n    feature_columns.append(one_hot_cat_column(feature_name, vocabulary))\r\n\r\nfor feature_name in NUMERIC_COLUMNS:\r\n    feature_columns.append(tf.feature_column.numeric_column(feature_name,\r\n                                           dtype=tf.float32))\r\n\r\n# Use entire batch since this is such a small dataset.\r\nNUM_EXAMPLES = len(y_train)\r\n\r\ndef make_input_fn(X, y, n_epochs=None, shuffle=True):\r\n    def input_fn():\r\n        dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))\r\n        if shuffle:\r\n            dataset = dataset.shuffle(NUM_EXAMPLES)\r\n        # For training, cycle thru dataset as many times as need (n_epochs=None).\r\n        dataset = dataset.repeat(n_epochs)\r\n        # In memory training doesn't use batching.\r\n        dataset = dataset.batch(NUM_EXAMPLES)\r\n        return dataset\r\n    return input_fn\r\n\r\n# Training and evaluation input functions.\r\ntrain_input_fn = make_input_fn(dftrain, y_train)\r\neval_input_fn = make_input_fn(dfeval, y_eval, shuffle=False, n_epochs=1)\r\n\r\nparams = {\r\n  'n_trees': 50,\r\n  'max_depth': 3,\r\n  'n_batches_per_layer': 1,\r\n  # You must enable center_bias = True to get DFCs. This will force the model to\r\n  # make an initial prediction before using any features (e.g. use the mean of\r\n  # the training labels for regression or log odds for classification when\r\n  # using cross entropy loss).\r\n  'center_bias': True\r\n}\r\n\r\nest = tf.estimator.BoostedTreesClassifier(feature_columns, **params)\r\n# Train model.\r\nest.train(train_input_fn, max_steps=100)\r\n\r\n# Evaluation.\r\nresults = est.evaluate(eval_input_fn)\r\nclear_output()\r\nprint(pd.Series(results))\r\n```\r\n\r\nbut if i test [boosted_trees tutorials](https://tensorflow.google.cn/tutorials/estimator/boosted_trees?hl=en), and just the param changed. It will run fluently:\r\n```python\r\nn_batches = 1\r\nest = tf.estimator.BoostedTreesClassifier(feature_columns,\r\n                                          n_batches_per_layer=n_batches)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nError information on my win10 1903 computer with tensorflow-gpu 2.0\r\n```python\r\n2019-11-09 15:16:16.038790: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\nWARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\sha\\AppData\\Local\\Temp\\tmp4qz0t419\r\nWARNING:tensorflow:From C:\\Users\\sha\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\boosted_trees.py:369: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\r\nWARNING:tensorflow:From C:\\Users\\sha\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\nWARNING:tensorflow:From C:\\Users\\sha\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\training\\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\n2019-11-09 15:16:17.766728: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2019-11-09 15:16:19.033200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1060 major: 6 minor: 1 memoryClockRate(GHz): 1.6705\r\npciBusID: 0000:01:00.0\r\n2019-11-09 15:16:19.033405: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-11-09 15:16:19.034037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\nWARNING:tensorflow:From C:\\Users\\sha\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\feature_column\\feature_column_v2.py:4276: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\r\nWARNING:tensorflow:From C:\\Users\\sha\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\boosted_trees.py:214: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.cast` instead.\r\nWARNING:tensorflow:From C:\\Users\\sha\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\head.py:437: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.cast` instead.\r\nWARNING:tensorflow:Issue encountered when serializing resources.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\n'_Resource' object has no attribute 'name'\r\n2019-11-09 15:16:19.632757: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-11-09 15:16:19.931364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1060 major: 6 minor: 1 memoryClockRate(GHz): 1.6705\r\npciBusID: 0000:01:00.0\r\n2019-11-09 15:16:19.931565: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-11-09 15:16:19.932914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-11-09 15:16:20.627630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-11-09 15:16:20.627777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-11-09 15:16:20.627863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-11-09 15:16:20.628605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4708 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nWARNING:tensorflow:Issue encountered when serializing resources.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\n'_Resource' object has no attribute 'name'\r\nWARNING:tensorflow:Issue encountered when serializing resources.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\n'_Resource' object has no attribute 'name'\r\nWARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\r\n\r\nProcess finished with exit code -1073740940 (0xC0000374)\r\n```", "comments": ["If i change the param like following:\r\n```python\r\nparams = {\r\n  'n_trees': 100,\r\n  'max_depth': 6,\r\n  'n_batches_per_layer': 1,\r\n  # You must enable center_bias = True to get DFCs. This will force the model to\r\n  # make an initial prediction before using any features (e.g. use the mean of\r\n  # the training labels for regression or log odds for classification when\r\n  # using cross entropy loss).\r\n  'center_bias': True\r\n}\r\n\r\nest = tf.estimator.BoostedTreesClassifier(feature_columns, **params)\r\n```\r\n\r\nIt will run fluently, so strange!", "@shazhongcheng \r\nI have tried on colab with TF version 2.0  and i am not seeing any error message. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/9998fa6cb4a59584a35a0456c3420584/untitled349.ipynb).Please, let me know if i miss something. Thanks!", "> @shazhongcheng\r\n> I have tried on colab with TF version 2.0 and i am not seeing any error message. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/9998fa6cb4a59584a35a0456c3420584/untitled349.ipynb).Please, let me know if i miss something. Thanks!\r\n\r\nThank you for your reply. You can see my screenshot, it indeed turn out error, and the outcome is my python will shutdown.\r\nI also try on colab, it indeed run fluently as you say, [like following](https://colab.research.google.com/drive/1mphVcOnkg82xZAMtvVZU52kjL-J_f_xP?hl=en#scrollTo=9bSneJbA1vWd):\r\n![image](https://user-images.githubusercontent.com/15049049/68577489-2b43be00-04ab-11ea-8a44-ea0353babccf.png)\r\n\r\nBut when i try on my jupyter, my python shutdown! Like this:\r\n![image](https://user-images.githubusercontent.com/15049049/68577809-c341a780-04ab-11ea-9501-e3b749de0999.png)\r\nAnd the same things turn out on my Pycharm:\r\n![image](https://user-images.githubusercontent.com/15049049/68578135-6692bc80-04ac-11ea-953e-a5490ea9a32c.png)\r\n\r\nI know google colab is very usefully, But sometimes the same code will turn out different results. Can you test this code on jupyter or pycharm? Pleas!", "The strange things is if i try use the default params, like:\r\n```python\r\nparams = {\r\n  'n_trees': 100,\r\n  'max_depth': 6,\r\n  'n_batches_per_layer': 1,\r\n  # You must enable center_bias = True to get DFCs. This will force the model to\r\n  # make an initial prediction before using any features (e.g. use the mean of\r\n  # the training labels for regression or log odds for classification when\r\n  # using cross entropy loss).\r\n  'center_bias': True\r\n}\r\n\r\nest = tf.estimator.BoostedTreesClassifier(feature_columns, **params)\r\n``` \r\nJust change the n_trees and max_depth, it will run fluently, so strange.", "Since you are able to run the script successfully on reducing the no of trees and depth of your model, you may try limiting gpu memory usage for more large params.\r\nYou may try this snippet at the top of your script;\r\n```python\r\nimport tensorflow as tf\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(gpus[0],True)\r\n```", "Ok it`s work! Thank you!\r\nThe code:\r\n```python\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom IPython.display import clear_output\r\nimport tensorflow as tf\r\ntf.random.set_seed(123)\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(gpus[0],True)\r\n\r\n# Load dataset.\r\ndftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv')\r\ndfeval = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv')\r\ny_train = dftrain.pop('survived')\r\ny_eval = dfeval.pop('survived')\r\n\r\n\r\n\r\n\r\nfc = tf.feature_column\r\nCATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck',\r\n                       'embark_town', 'alone']\r\nNUMERIC_COLUMNS = ['age', 'fare']\r\n\r\ndef one_hot_cat_column(feature_name, vocab):\r\n    return tf.feature_column.indicator_column(\r\n        tf.feature_column.categorical_column_with_vocabulary_list(feature_name,\r\n                                                 vocab))\r\nfeature_columns = []\r\nfor feature_name in CATEGORICAL_COLUMNS:\r\n    # Need to one-hot encode categorical features.\r\n    vocabulary = dftrain[feature_name].unique()\r\n    feature_columns.append(one_hot_cat_column(feature_name, vocabulary))\r\n\r\nfor feature_name in NUMERIC_COLUMNS:\r\n    feature_columns.append(tf.feature_column.numeric_column(feature_name,\r\n                                           dtype=tf.float32))\r\n\r\n# Use entire batch since this is such a small dataset.\r\nNUM_EXAMPLES = len(y_train)\r\n\r\ndef make_input_fn(X, y, n_epochs=None, shuffle=True):\r\n    def input_fn():\r\n        dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))\r\n        if shuffle:\r\n            dataset = dataset.shuffle(NUM_EXAMPLES)\r\n        # For training, cycle thru dataset as many times as need (n_epochs=None).\r\n        dataset = dataset.repeat(n_epochs)\r\n        # In memory training doesn't use batching.\r\n        dataset = dataset.batch(NUM_EXAMPLES)\r\n        return dataset\r\n    return input_fn\r\n\r\n# Training and evaluation input functions.\r\ntrain_input_fn = make_input_fn(dftrain, y_train)\r\neval_input_fn = make_input_fn(dfeval, y_eval, shuffle=False, n_epochs=1)\r\n\r\nparams = {\r\n  'n_trees': 50,\r\n  'max_depth': 3,\r\n  'n_batches_per_layer': 1,\r\n  # You must enable center_bias = True to get DFCs. This will force the model to\r\n  # make an initial prediction before using any features (e.g. use the mean of\r\n  # the training labels for regression or log odds for classification when\r\n  # using cross entropy loss).\r\n  'center_bias': True\r\n}\r\n\r\nest = tf.estimator.BoostedTreesClassifier(feature_columns, **params)\r\n# Train model.\r\nest.train(train_input_fn, max_steps=100)\r\n\r\n# Evaluation.\r\nresults = est.evaluate(eval_input_fn)\r\nclear_output()\r\nprint(pd.Series(results))\r\n```\r\n\r\n[But in my other error](https://github.com/tensorflow/tensorflow/issues/34094), it doesn`t work.", "Same issue here with GTX 1660 Ti ", "> Same issue here with GTX 1660 Ti\r\n\r\nadd this code, and i solve the problem!\r\n```python\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(gpus[0],True)\r\n```\r\n\r\n", "In case someone has the same issue, do also check this [link](https://github.com/tensorflow/tensorflow/issues/30319). In my case, this is the issue causing due to a lack of tcmalloc package."]}, {"number": 34117, "title": "Build failure: undefined reference to protobuf symbols", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.3\r\n- Python version:  Tried with python 3.7.3 and python 3.8\r\n- Installed using virtualenv? pip? conda?: conda python 3.7.3 and virtualenv python 3.8\r\n- Bazel version (if compiling from source): 0.29.1\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: CUDA 10.0 and cuDNN 7.6.5\r\n- GPU model and memory: Nvidia RTX 2080 Ti\r\n\r\n**Describe the problem**\r\nBuild fails most of the way in to build.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\ngit checkout -b mybranch         (make up a branch to checkout head)\r\nbazel build --config=opt --config=cuda --config=v2 --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package\r\n\r\nHere is the last part of the terminal's output (attached text file):\r\n[tensorflow_build_fails.txt](https://github.com/tensorflow/tensorflow/files/3826936/tensorflow_build_fails.txt)\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@dbonner, Did you run `./configure`. If yes please attach the output of `./configure`. Thanks!", "https://www.tensorflow.org/install/source#linux\r\n\r\nTested build configuration differs from the OP. \r\n\r\nTested build uses:\r\ngcc 7.3.1\r\ncuDNN 7.4", "@gadagashwini \r\nHere is the output as requested:\r\n./configure\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.29.1 installed.\r\nPlease specify the location of python. [Default is /home/daniel/anaconda3/envs/tfgpu/bin/python]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /home/daniel/anaconda3/envs/tfgpu/lib/python3.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/daniel/anaconda3/envs/tfgpu/lib/python3.7/site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: Y\r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: N\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: N\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: y\r\nTensorRT support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.0 in:\r\n    /usr/local/cuda/lib64\r\n    /usr/local/cuda/include\r\nFound cuDNN 7 in:\r\n    /usr/lib/x86_64-linux-gnu\r\n    /usr/include\r\nFound TensorRT 6 in:\r\n    /usr/lib/x86_64-linux-gnu\r\n    /usr/include/x86_64-linux-gnu\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 7.5\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: N\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: \r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=ngraph      \t# Build with Intel nGraph support.\r\n\t--config=numa        \t# Build with NUMA support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n\t--config=v2          \t# Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nohdfs      \t# Disable HDFS support.\r\n\t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n", "I've also not been able to compile TF from source for the last 4 days because of this.", "I see the same errors too.", "I also see the same problem. I tried bazel 0.27.1 and 0.27.2. It seems related to protobuf. What version of protobuf is TF using?", "Issue title is misleading, first error is not `undefined reference to tensorflow::register_op` but\r\n\r\n```\r\nbazel-out/host/bin/tensorflow/core/libfunctional_ops_op_lib.lo(functional_ops.o): In function `tensorflow::Status tensorflow::errors::InvalidArgument<char const*, unsigned long, char const*, int>(char const*, unsigned long, char const*, int)':\r\nfunctional_ops.cc:(.text._ZN10tensorflow6errors15InvalidArgumentIJPKcmS3_iEEENS_6StatusEDpT_[_ZN10tensorflow6errors15InvalidArgumentIJPKcmS3_iEEENS_6StatusEDpT_]+0x42): undefined reference to `tensorflow::strings::FastInt32ToBufferLeft(int, char*)'\r\nfunctional_ops.cc:(.text._ZN10tensorflow6errors15InvalidArgumentIJPKcmS3_iEEENS_6StatusEDpT_[_ZN10tensorflow6errors15InvalidArgumentIJPKcmS3_iEEENS_6StatusEDpT_]+0x82): undefined reference to `tensorflow::strings::FastUInt64ToBufferLeft(unsigned long long, char*)'\r\nfunctional_ops.cc:(.text._ZN10tensorflow6errors15InvalidArgumentIJPKcmS3_iEEENS_6StatusEDpT_[_ZN10tensorflow6errors15InvalidArgumentIJPKcmS3_iEEENS_6StatusEDpT_]+0xd4): undefined reference to `tensorflow::strings::StrCat[abi:cxx11](tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)'\r\nfunctional_ops.cc:(.text._ZN10tensorflow6errors15InvalidArgumentIJPKcmS3_iEEENS_6StatusEDpT_[_ZN10tensorflow6errors15InvalidArgumentIJPKcmS3_iEEENS_6StatusEDpT_]+0xef): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::string_view)'\r\n...\r\n```\r\n\r\nIn any case, can you try building again from a fresh clone and attach the entire log of `bazel build -s --config=opt --config=cuda --config=v2 --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package &> log.txt`?", "The first error I see is this one: \r\n\r\n```\r\nERROR: /bench/tensorflow-source/tensorflow/python/BUILD:2317:1: Linking of rule '//tensorflow/python:gen_boosted_trees_ops_py_wrappers_cc' failed (Exit 1)\r\nbazel-out/host/bin/tensorflow/core/libop_gen_lib.a(op_gen_lib.o): In function `google::protobuf::internal::ArenaStringPtr::CreateInstance(google::protobuf::Arena*, std::__cxx11::basic_string<char, std::char_trait\r\ns<char>, std::allocator<char> > const*)':\r\nop_gen_lib.cc:(.text._ZN6google8protobuf8internal14ArenaStringPtr14CreateInstanceEPNS0_5ArenaEPKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE[_ZN6google8protobuf8internal14ArenaStringPtr14CreateInstanceEPN\r\nS0_5ArenaEPKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE]+0x36): undefined reference to `google::protobuf::internal::ArenaImpl::AllocateAlignedAndAddCleanup(unsigned long, void (*)(void*))'\r\nop_gen_lib.cc:(.text._ZN6google8protobuf8internal14ArenaStringPtr14CreateInstanceEPNS0_5ArenaEPKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE[_ZN6google8protobuf8internal14ArenaStringPtr14CreateInstanceEPN\r\nS0_5ArenaEPKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE]+0xc0): undefined reference to `google::protobuf::Arena::OnArenaAllocation(std::type_info const*, unsigned long) const'\r\n```", "@mihaimaruseac \r\nlog.txt is 119 MB so can't be uploaded to github.\r\nYou can download from this link:\r\nhttps://www.dropbox.com/s/c7xc9y37k617z93/log.txt?dl=0\r\nMany thanks for helping with this issue.", "First error is\r\n\r\n```\r\nERROR: /home/daniel/tensorflow/tensorflow/python/BUILD:2599:1: Linking of rule '//tensorflow/python:gen_tpu_ops_py_wrappers_cc' failed (Exit 1)\r\nbazel-out/host/bin/tensorflow/core/libop_gen_lib.a(op_gen_lib.o): In function `google::protobuf::internal::ArenaStringPtr::CreateInstance(google::protobuf::Arena*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const*)':\r\nop_gen_lib.cc:(.text._ZN6google8protobuf8internal14ArenaStringPtr14CreateInstanceEPNS0_5ArenaEPKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE[_ZN6google8protobuf8internal14ArenaStringPtr14CreateInstanceEPNS0_5ArenaEPKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE]+0x36): undefined reference to `google::protobuf::internal::ArenaImpl::AllocateAlignedAndAddCleanup(unsigned long, void (*)(void*))'\r\nop_gen_lib.cc:(.text._ZN6google8protobuf8internal14ArenaStringPtr14CreateInstanceEPNS0_5ArenaEPKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE[_ZN6google8protobuf8internal14ArenaStringPtr14CreateInstanceEPNS0_5ArenaEPKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE]+0xc0): undefined reference to `google::protobuf::Arena::OnArenaAllocation(std::type_info const*, unsigned long) const'\r\n```", "@pkanwar23 , can you please help to drive this one?  It is blocking several people on our team.", "> Issue title is misleading, first error is not `undefined reference to tensorflow::register_op` but\r\n> \r\n> ```\r\n> bazel-out/host/bin/tensorflow/core/libfunctional_ops_op_lib.lo(functional_ops.o): In function `tensorflow::Status tensorflow::errors::InvalidArgument<char const*, unsigned long, char const*, int>(char const*, unsigned long, char const*, int)':\r\n> functional_ops.cc:(.text._ZN10tensorflow6errors15InvalidArgumentIJPKcmS3_iEEENS_6StatusEDpT_[_ZN10tensorflow6errors15InvalidArgumentIJPKcmS3_iEEENS_6StatusEDpT_]+0x42): undefined reference to `tensorflow::strings::FastInt32ToBufferLeft(int, char*)'\r\n> functional_ops.cc:(.text._ZN10tensorflow6errors15InvalidArgumentIJPKcmS3_iEEENS_6StatusEDpT_[_ZN10tensorflow6errors15InvalidArgumentIJPKcmS3_iEEENS_6StatusEDpT_]+0x82): undefined reference to `tensorflow::strings::FastUInt64ToBufferLeft(unsigned long long, char*)'\r\n> functional_ops.cc:(.text._ZN10tensorflow6errors15InvalidArgumentIJPKcmS3_iEEENS_6StatusEDpT_[_ZN10tensorflow6errors15InvalidArgumentIJPKcmS3_iEEENS_6StatusEDpT_]+0xd4): undefined reference to `tensorflow::strings::StrCat[abi:cxx11](tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)'\r\n> functional_ops.cc:(.text._ZN10tensorflow6errors15InvalidArgumentIJPKcmS3_iEEENS_6StatusEDpT_[_ZN10tensorflow6errors15InvalidArgumentIJPKcmS3_iEEENS_6StatusEDpT_]+0xef): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::string_view)'\r\n> ...\r\n> ```\r\n> \r\n> In any case, can you try building again from a fresh clone and attach the entire log of `bazel build -s --config=opt --config=cuda --config=v2 --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package &> log.txt`?\r\n\r\nJust tried this command on the latest master, and got the same error.\r\nI uploaded the log here: https://drive.google.com/open?id=1MobJh4K6bk_ySptRn77F1dKl4srY3k0I\r\n\r\nThe first error seems to be this one: \r\n```\r\nERROR: /bench/tensorflow-source/tensorflow/cc/BUILD:506:1: Linking of rule '//tensorflow/cc:ops/training_ops_gen_cc' failed (Exit 1)\r\nbazel-out/host/bin/tensorflow/core/libop_gen_lib.a(op_gen_lib.o): In function `google::protobuf::internal::ArenaStringPtr::CreateInstance(google::protobuf::Arena*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const*)':\r\nop_gen_lib.cc:(.text._ZN6google8protobuf8internal14ArenaStringPtr14CreateInstanceEPNS0_5ArenaEPKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE[_ZN6google8protobuf8internal14ArenaStringPtr14CreateInstanceEPNS0_5ArenaEPKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE]+0x36): undefined reference to `google::protobuf::internal::ArenaImpl::AllocateAlignedAndAddCleanup(unsigned long, void (*)(void*))'\r\nop_gen_lib.cc:(.text._ZN6google8protobuf8internal14ArenaStringPtr14CreateInstanceEPNS0_5ArenaEPKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE[_ZN6google8protobuf8internal14ArenaStringPtr14CreateInstanceEPNS0_5ArenaEPKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE]+0xc0): undefined reference to `google::protobuf::Arena::OnArenaAllocation(std::type_info const*, unsigned long) const'\r\nbazel-out/host/bin/tensorflow/core/libop_gen_lib.a(op_gen_lib.o): In function `tensorflow::(anonymous namespace)::MergeArg(tensorflow::ApiDef_Arg*, tensorflow::ApiDef_Arg const&)':\r\nop_gen_lib.cc:(.text._ZN10tensorflow12_GLOBAL__N_18MergeArgEPNS_10ApiDef_ArgERKS1_+0x4c): undefined reference to `google::protobuf::internal::fixed_address_empty_string[abi:cxx11]'\r\nop_gen_lib.cc:(.text._ZN10tensorflow12_GLOBAL__N_18MergeArgEPNS_10ApiDef_ArgERKS1_+0x77): undefined reference to `google::protobuf::internal::fixed_address_empty_string[abi:cxx11]'\r\nbazel-out/host/bin/tensorflow/core/libop_gen_lib.a(op_gen_lib.o): In function `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > absl::strings_internal::JoinRange<google::protobuf::RepeatedPtrField<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >(google::protobuf::RepeatedPtrField<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > const&, absl::string_view)':\r\nop_gen_lib.cc:(.text._ZN4absl16strings_internal9JoinRangeIN6google8protobuf16RepeatedPtrFieldINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEEEESA_RKT_NS_11string_viewE[_ZN4absl16strings_internal9JoinRangeIN6google8protobuf16RepeatedPtrFieldINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEEEESA_RKT_NS_11string_viewE]+0x21): undefined reference to `google::protobuf::RepeatedPtrField<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >::end() const'\r\n```\r\n\r\n", "That's the exact error I'm seeing. I'm compiling merely with:\r\n--config=opt --config=cuda\r\nI tried bazel 0.27, 0.29, 1.0, and 1.1\r\n", "Just noticed this: if I checkout a random commit from Oct 2, I see the linker errors when I build with bazel 0.27.1, but **not** when I build with bazel 0.24.1.", "I've come to the same conclusion.\r\nupto 17a23945abe427cf2fe6677d26e65558109f6f10 (Nov 6) I can build with 0.24.1, but not with 0.27.x", "Seems we're observing the similar issue after recent Bazel version upgrade.", "I have also problems with this, trying now bazel 0.24.1\r\nI'm in jetson nano, tensorflow2 has been compiled from source there (I have not tried it yet):\r\nhttps://jkjung-avt.github.io/build-tensorflow-2.0.0/", "Can you try adding --noincompatible_remove_legacy_whole_archive?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34117\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34117\">No</a>\n", "--noincompatible_remove_legacy_whole_archive doesn't work when using bazel 0.27.1 and up", "concur with @mengdong\r\nI tried  a73d7ace8e3c5a59d2325b95f3b02e225a977ff2 with bazel 1.1.0 and get the same errors as before. ", "I also tried with 0.27.1 and still fails similarly.", "It does work with bazel 0.24.1\r\n", "Yes, 0.27.1 still fails with the patch.\r\nIt builds with bazel 0.26.1.\r\nYou have to run ./configure with bazel 0.27.1 or higher.  Otherwise it won't let you run ./configure.\r\nThen uninstall bazel (sudo apt remove bazel)\r\nThen install bazel 0.26.1\r\nThen it will build successfully.", "> Yes, 0.27.1 still fails with the patch.\r\n> It builds with bazel 0.26.1.\r\n> You have to run ./configure with bazel 0.27.1 or higher. Otherwise it won't let you run ./configure.\r\n> Then uninstall bazel (sudo apt remove bazel)\r\n> Then install bazel 0.26.1\r\n> Then it will build successfully.\r\n\r\nAnd I call this a solution! Here is a simple patch that can be used to lower the bazel requirements down to 0.26.1. Then bazel 0.26.1 worked for me. Hope we'll find a more clever fix.\r\n\r\n```\r\ndiff --git a/configure.py b/configure.py\r\nindex d29f3d4464..cd3ce61dec 100644\r\n--- a/configure.py\r\n+++ b/configure.py\r\n@@ -49,7 +49,7 @@ _TF_BAZELRC_FILENAME = '.tf_configure.bazelrc'\r\n _TF_WORKSPACE_ROOT = ''\r\n _TF_BAZELRC = ''\r\n _TF_CURRENT_BAZEL_VERSION = None\r\n-_TF_MIN_BAZEL_VERSION = '0.27.1'\r\n+_TF_MIN_BAZEL_VERSION = '0.26.1'\r\n _TF_MAX_BAZEL_VERSION = '0.29.1'\r\n\r\n NCCL_LIB_PATHS = [\r\n```\r\n\r\nFYI I'm running configure with the following setting: \r\n\r\n```\r\nPYTHON_BIN_PATH=\"/usr/local/bin/python\" PYTHON_LIB_PATH=\"/usr/local/lib/python3.6/dist-packages\" TF_ENABLE_XLA=\"y\" TF_NEED_OPENCL_SYCL=\"n\" TF_NEED_ROCM=\"n\" TF_NEED_CUDA=\"y\" TF_DOWNLOAD_CLANG=\"n\" GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc\" CC_OPT_FLAGS=\"-march=native -Wno-sign-compare\" TF_SET_ANDROID_WORKSPACE=\"n\" TF_NEED_TENSORRT=n TF_CUDA_COMPUTE_CAPABILITIES=\"3.5,6.0\" TF_CUDA_CLANG=n TF_NEED_MPI=n ./configure;\r\n```    \r\n", "We have a fix for this issue on TF side. However given that the build succeeds with 0.26.1 and is broken with 0.27.1, we should locate the change in Bazel that caused this behavior.\r\nAn incompatible change that modifies the linking order was introduced in 0.27 through [--incompatible_do_not_split_linking_cmdline](https://github.com/bazelbuild/bazel/issues/7687). Can someone please try building with 0.27.1 or newer version adding --noincompatible_do_not_split_linking_cmdline ?", "> Yes, 0.27.1 still fails with the patch.\r\n> It builds with bazel 0.26.1.\r\n\r\n@dbonner can you please clarify -- was the build with 0.26.1 successful before [a73d7ac](https://github.com/tensorflow/tensorflow/commit/a73d7ace8e3c5a59d2325b95f3b02e225a977ff2)?\r\nWe're seeing failures at HEAD for windows and macos due to this commit, it'll most likely be at least temporarily reverted. I'd like to know if it impacts this issue in any way.", "building at a73d7ace8e3c5a59d2325b95f3b02e225a977ff2 with bazel 0.27.2 with --noincompatible_do_not_split_linking_cmdline produces the same linker errors.\r\n", "@bmzhao has a fix for this which would be incoming soon.", "@scentini \r\nI don't know if the build would have been successful with bazel 0.26.1 before patch a73d7ac.  I tested building after the patch with bazel 0.26.1 and it worked.\r\nI am building the r2.1 branch with bazel 0.26.1 at the moment.  It has the same error when you try to build it with bazel 0.27.1.", "Hello!\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/5caa9e83798cb510c9b49acee8a64efdb746207c is now in master. I've manually tested building with it using bazel 1.1, with the following command:\r\n\r\n```\r\nbazel build -c opt --config=cuda --config=v2 --host_force_python=PY3 //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\nINFO: Elapsed time: 2067.380s, Critical Path: 828.19s\r\nINFO: 12942 processes: 51 remote cache hit, 12891 local.\r\nINFO: Build completed successfully, 14877 total actions\r\n```\r\n\r\n@dbonner can you confirm if Tensorflow head now builds for you as well?", "@bmzhao, I was able to successfully build TF with bazel 1.1 at 5caa9e83798cb510c9b49acee8a64efdb746207c on Ubuntu 18.04\r\nI do see some failures for windows and mac in the CI, like the ones mentioned by @scentini before.\r\nDoes this mean this change could be reverted?", "Hey @bas-aarts,\r\n\r\nAfter double checking with our buildcop, it looks like the current Windows and Mac breakages' root causes are other commits (not https://github.com/tensorflow/tensorflow/commit/5caa9e83798cb510c9b49acee8a64efdb746207c). Therefore, I don't expect this change to be rolled back.", "Is this error related?\r\n\r\n```\r\nERROR: /home/vai/repo/tensorflow/tensorflow/python/keras/api/BUILD:115:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1)\r\n2019-11-20 13:15:41.004621: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2\r\nTraceback (most recent call last):\r\n  File \"/home/vai/.cache/bazel/_bazel_vai/964c7018fd2d0d2d2cf98e15f592d3c8/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 771, in <module>\r\n    main()\r\n  File \"/home/vai/.cache/bazel/_bazel_vai/964c7018fd2d0d2d2cf98e15f592d3c8/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 767, in main\r\n    lazy_loading, args.use_relative_imports)\r\n  File \"/home/vai/.cache/bazel/_bazel_vai/964c7018fd2d0d2d2cf98e15f592d3c8/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 625, in create_api_files\r\n    api_version, compat_api_versions, lazy_loading, use_relative_imports)\r\n  File \"/home/vai/.cache/bazel/_bazel_vai/964c7018fd2d0d2d2cf98e15f592d3c8/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 502, in get_api_init_text\r\n    _, attr = tf_decorator.unwrap(attr)\r\n  File \"/home/vai/.cache/bazel/_bazel_vai/964c7018fd2d0d2d2cf98e15f592d3c8/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/util/tf_decorator.py\", line 219, in unwrap\r\n    elif _has_tf_decorator_attr(cur):\r\n  File \"/home/vai/.cache/bazel/_bazel_vai/964c7018fd2d0d2d2cf98e15f592d3c8/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/util/tf_decorator.py\", line 124, in _has_tf_decorator_attr\r\n    hasattr(obj, '_tf_decorator') and\r\n  File \"/home/vai/.cache/bazel/_bazel_vai/964c7018fd2d0d2d2cf98e15f592d3c8/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/util/lazy_loader.py\", line 62, in __getattr__\r\n    module = self._load()\r\n  File \"/home/vai/.cache/bazel/_bazel_vai/964c7018fd2d0d2d2cf98e15f592d3c8/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/util/lazy_loader.py\", line 45, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/home/vai/anaconda3/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/home/vai/.cache/bazel/_bazel_vai/964c7018fd2d0d2d2cf98e15f592d3c8/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/wrap_py_utils.py\", line 28, in <module>\r\n    _wrap_py_utils = swig_import_helper()\r\n  File \"/home/vai/.cache/bazel/_bazel_vai/964c7018fd2d0d2d2cf98e15f592d3c8/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/wrap_py_utils.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_wrap_py_utils', fp, pathname, description)\r\n  File \"/home/vai/anaconda3/lib/python3.7/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/vai/anaconda3/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\n  File \"<frozen importlib._bootstrap>\", line 696, in _load\r\n  File \"<frozen importlib._bootstrap>\", line 670, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 583, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 1043, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: /home/vai/.cache/bazel/_bazel_vai/964c7018fd2d0d2d2cf98e15f592d3c8/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/_wrap_py_utils.so: undefined symbol: _ZN10tensorflow3Env7DefaultEv\r\n----------------\r\nNote: The failure of target //tensorflow/python/keras/api:create_tensorflow.python_api_1_keras_python_api_gen_compat_v1 (with exit code 1) may have been caused by the fact that it is a Python 2 program that was built in the host configuration, which uses Python 3. You can change the host configuration (for the entire build) to instead use Python 2 by setting --host_force_python=PY2.\r\n\r\nIf this error started occurring in Bazel 0.27 and later, it may be because the Python toolchain now enforces that targets analyzed as PY2 and PY3 run under a Python 2 and Python 3 interpreter, respectively. See https://github.com/bazelbuild/bazel/issues/7899 for more information.\r\n----------------\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /home/vai/repo/tensorflow/tensorflow/python/tools/BUILD:141:1 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1)\r\n```", "@alanpurple could you file a separate issue including all relevant information to reproduce the error? Please see https://github.com/tensorflow/tensorflow/blob/master/ISSUE_TEMPLATE.md#system-information", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34117\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34117\">No</a>\n", "I am still in the process of building with bazel 1.1.0 and will report back how it goes.\r\nIf successful, can we update configure.py to allow maximum bazel version of 1.1.0 please?", "I believe the undefined symbols errors are caused by 2 different Bazel flags:\r\n`--incompatible_remove_legacy_whole_archive`\r\n`--incompatible_do_not_split_linking_cmdline`\r\n\r\n@bmzhao confirmed that `--incompatible_do_not_split_linking_cmdline` allows for a successful build even without the fix [5caa9e8](https://github.com/tensorflow/tensorflow/commit/5caa9e83798cb510c9b49acee8a64efdb746207c)\r\n\r\nThe `--incompatible_remove_legacy_whole_archive` has proven difficult to stick due to regressions and undefined symbols at runtime. @alanpurple, I suspect that the linking issue you're running into comes from it. Due to lack of test coverage it can happen that the codebase regressed wrt this flag in the meantime.\r\n\r\nI am working on setting the default for `--incompatible_remove_legacy_whole_archive` to false. [a73d7ac](https://github.com/tensorflow/tensorflow/commit/a73d7ace8e3c5a59d2325b95f3b02e225a977ff2) did that, but was reverted in [5caa9e8](https://github.com/tensorflow/tensorflow/commit/5caa9e83798cb510c9b49acee8a64efdb746207c) because it caused windows and macos failures due to multiply defined symbols. I'm looking into fixing that now.\r\n\r\n@dbonner [configure.py](https://github.com/tensorflow/tensorflow/blob/master/configure.py#L53) already sets the max version to 1.1.0. Or is there another configure.py somewhere?", "JFYI\r\nFor our similar case the problem is not reproducible with TensorFlow master since yesterday (looks like there were some fixes). Bazel version: 0.29.1.\r\nFor example, this TensorFlow commit (95051b9) is built successfully.", "I was having the same issue building https://github.com/tensorflow/tensorflow/commit/76f94a5f0777abe8697cdfc7e1c79484760fd1b5 with Bazel 1.1.0 but adding --noincompatible_do_not_split_linking_cmdline as @scentini suggested allowed to build successfully (thank you!).", "@mihaimaruseac \r\nLooks like `--noincompatible_do_not_split_linking_cmdline` is still needed with bazel 1.1.0 in order to compile the master as of an hour ago.", "cc @hlopko to deal with this from Bazel side.", "I also hit a related issue that I need the flag `--noincompatible_do_not_split_linking_cmdline` to pass XLA tests.\r\n\r\nFor example:\r\n`bazel test -c opt --config=cuda --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --noincompatible_do_not_split_linking_cmdline //tensorflow/compiler/xla/service/gpu:horizontal_fusion_test`\r\n\r\nIf I do no add the flag, I hit the below error due to linkage change.\r\n\r\n```\r\n2019-12-05 02:01:06.565142: F tensorflow/compiler/xla/tests/hlo_test_base.cc:53] Non-OK-status: result.status() status: Not found: Could not find registered platform with name: \"interpreter\"could not get interpreter platform\r\n\r\n*** Received signal 6 ***\r\n\r\n*** BEGIN MANGLED STACK TRACE ***\r\n\r\n```"]}, {"number": 34116, "title": "ImportError: DLL load failed TF 1.13", "body": "\r\n**System information**\r\n- win 10 x64  18362 (1903):\r\n- pc:\r\n- VS community tensorflow-gpu 1.13.1:\r\n- tensorflow-gpu 1.13.1:\r\n- Python 3.7:\r\n- pip in VS community:\r\n- all version 9.0 10.1/cudnn-10.1-windows10-x64-v7.6.5.32:\r\n- 1060 6gb:\r\n```\r\n\r\n```\r\n> \r\n> Using TensorFlow backend.\r\n> Traceback (most recent call last):\r\n>   File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\imp.py\", line 242, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\imp.py\", line 342, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: \u041d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d \u0443\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043c\u043e\u0434\u0443\u043b\u044c.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"c:\\program files (x86)\\microsoft visual studio\\2019\\community\\common7\\ide\\extensions\\microsoft\\python\\core\\ptvsd_launcher.py\", line 119, in <module>\r\n>     vspd.debug(filename, port_num, debug_id, debug_options, run_as)\r\n>   File \"c:\\program files (x86)\\microsoft visual studio\\2019\\community\\common7\\ide\\extensions\\microsoft\\python\\core\\Packages\\ptvsd\\debugger.py\", line 39, in debug\r\n>     run()\r\n>   File \"c:\\program files (x86)\\microsoft visual studio\\2019\\community\\common7\\ide\\extensions\\microsoft\\python\\core\\Packages\\ptvsd\\__main__.py\", line 316, in run_file\r\n>     runpy.run_path(target, run_name='__main__')\r\n>   File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\runpy.py\", line 263, in run_path\r\n>     pkg_name=pkg_name, script_name=fname)\r\n>   File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\runpy.py\", line 96, in _run_module_code\r\n>     mod_name, mod_spec, pkg_name, script_name)\r\n>   File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\runpy.py\", line 85, in _run_code\r\n>     exec(code, run_globals)\r\n>   File \"C:\\Users\\sc20k\\source\\repos\\Test yay\\Test yay\\Test_yay.py\", line 1, in <module>\r\n>     from imageai.Detection import VideoObjectDetection\r\n>   File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\imageai\\Detection\\__init__.py\", line 2, in <module>\r\n>     from imageai.Detection.keras_retinanet.models.resnet import resnet50_retinanet\r\n>   File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\imageai\\Detection\\keras_retinanet\\models\\resnet.py\", line 19, in <module>\r\n>     import keras\r\n>   File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n>     from . import utils\r\n>   File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\r\n>     from . import conv_utils\r\n>   File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\r\n>     from .. import backend as K\r\n>   File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\keras\\backend\\__init__.py\", line 1, in <module>\r\n>     from .load_backend import epsilon\r\n>   File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\keras\\backend\\load_backend.py\", line 90, in <module>\r\n>     from .tensorflow_backend import *\r\n>   File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\r\n>     import tensorflow as tf\r\n>   File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n>   File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n>   File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n>     raise ImportError(msg)\r\n> ImportError: Traceback (most recent call last):\r\n>   File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\imp.py\", line 242, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\imp.py\", line 342, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: \u041d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d \u0443\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043c\u043e\u0434\u0443\u043b\u044c.\r\n> \r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n> \r\n> See https://www.tensorflow.org/install/errors\r\n> \r\n> for some common reasons and solutions.  Include the entire stack trace\r\n> above this error message when asking for help.\r\n> \r\n> \r\n```\r\n\r\n```", "comments": ["You need to install cuda 10.0 and add cuda, cupti, cudnn path to the environment variable.\r\nPlease take a look at [gpu setup for windows guide](https://www.tensorflow.org/install/gpu#windows_setup) to know more.", "@sc20ka, Did you get a chance to try @ymodak's solution.\r\nThanks!", "@sc20ka, Install CUDA 10.0 and Follow the following steps to install Tensorflow. Also set the CUDA path. \r\n$pip install virtualenv\r\n$virtualenv tf_1.13   # tf_1.13 is virtual env name\r\n$source tf_1.13/bin/activate\r\ntf_1.13 $ pip install tensorflow-gpu==1.13.1\r\nThanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34116\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34116\">No</a>\n", "I would like to reopen this issue as my problem is the same.\r\n\r\nI have installed CUDA 10.1 and cudnn and added the things from here (https://www.tensorflow.org/install/gpu#windows_setup) into the PATH-Variable but I still get the error that \"Failed to load the native TensorFlow runtime.\".\r\n\r\nThis is my complete stack-trace:\r\n\r\n```\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\tensorflow\\pyth\r\non\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\tensorflow\\pyth\r\non\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\tensorflow\\pyth\r\non\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\Anaconda3\\envs\\imageai\\lib\\imp.py\", line 242, in load_mo\r\ndule\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Anaconda3\\envs\\imageai\\lib\\imp.py\", line 342, in load_dy\r\nnamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Documents\\ImageAI\\CustomDetectionTraining.py\", line 1, i\r\nn <module>\r\n    from imageai.Detection.Custom import DetectionModelTrainer\r\n  File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\imageai\\Detecti\r\non\\__init__.py\", line 2, in <module>\r\n    from imageai.Detection.keras_retinanet.models.resnet import resnet50_retinan\r\net\r\n  File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\imageai\\Detecti\r\non\\keras_retinanet\\models\\resnet.py\", line 19, in <module>\r\n    import keras\r\n  File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\keras\\__init__.\r\npy\", line 3, in <module>\r\n    from . import utils\r\n  File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\keras\\utils\\__i\r\nnit__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n  File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\keras\\utils\\con\r\nv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n  File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\keras\\backend\\_\r\n_init__.py\", line 1, in <module>\r\n    from .load_backend import epsilon\r\n  File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\keras\\backend\\l\r\noad_backend.py\", line 90, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\keras\\backend\\t\r\nensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\tensorflow\\__in\r\nit__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-im\r\nport\r\n  File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\tensorflow\\pyth\r\non\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\tensorflow\\pyth\r\non\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\tensorflow\\pyth\r\non\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\tensorflow\\pyth\r\non\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Anaconda3\\envs\\imageai\\lib\\site-packages\\tensorflow\\pyth\r\non\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\Anaconda3\\envs\\imageai\\lib\\imp.py\", line 242, in load_mo\r\ndule\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Anaconda3\\envs\\imageai\\lib\\imp.py\", line 342, in load_dy\r\nnamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n", "@sc20ka, and @Snickbrack  Does your CPU support AVX instruction sets. ", "@Snickbrack, Closing issue due to no activity. Please free to open new issue with information asked in template. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34116\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34116\">No</a>\n"]}]