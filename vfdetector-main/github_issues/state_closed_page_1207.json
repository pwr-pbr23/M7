[{"number": 16963, "title": "Improve transform_graph tool dependencies", "body": "### System information\r\n- **TensorFlow installed from (source or binary)**: source\r\n\r\nIs there any way we can improve this dependency chain? The transform graph tool really shoudn't need to know about kernel implementations as far as I can tell.  Perhaps the appropriate place to cut this is at @org_tensorflow//tensorflow/core:tensorflow_opensource. Maintainer thoughts?\r\n```\r\nbazel query 'somepath(@org_tensorflow//tensorflow/tools/graph_transforms:transform_graph,@org_tensorflow//tensorflow/core/kernels:strided_slice_op)'\r\n@org_tensorflow//tensorflow/tools/graph_transforms:transform_graph\r\n@org_tensorflow//tensorflow/tools/graph_transforms:transform_graph_main_lib\r\n@org_tensorflow//tensorflow/tools/graph_transforms:transforms_lib\r\n@org_tensorflow//tensorflow/core:tensorflow\r\n@org_tensorflow//tensorflow/core:tensorflow_opensource\r\n@org_tensorflow//tensorflow/core:all_kernels\r\n@org_tensorflow//tensorflow/core/kernels:array\r\n@org_tensorflow//tensorflow/core/kernels:strided_slice_op\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 16962, "title": "Added controlled logging for estimator.py", "body": "Added extra argument `logging_every_n_iter` to the `fit` function of Estimator in estimator.py to control logging display.", "comments": ["Nagging Assignee @protoget: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "contrib.learn is deprecated, I'd rather not add features to it. "]}, {"number": 16961, "title": "Add instructions for building CUDA-enabled Android TensorFlow", "body": "", "comments": ["Still tweaking, please don't merge yet.", "This should be ready for review now, please take a look!"]}, {"number": 16960, "title": "Build error introduced by 1baac78627. Can't build sources (master).", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian Jessie\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: Master\r\n- **Python version**: 2.7.14\r\n- **Bazel version (if compiling from source)**: 0.10.0\r\n- **GCC/Compiler version (if compiling from source)**: GCC 4.9.2-10\r\n- **CUDA/cuDNN version**: Cuda8 / Cudnn7\r\n- **GPU model and memory**: 1080Ti\r\n- **Exact command to reproduce**:\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow .\r\nexport PYTHON_BIN_PATH=/path/to/python ## python 2.7.14\r\nexport USE_DEFAULT_PYTHON_LIB_PATH=1\r\nexport TF_NEED_JEMALLOC=1\r\nexport TF_NEED_GCP=0\r\nexport TF_NEED_HDFS=1\r\nexport TF_ENABLE_XLA=1\r\nexport TF_NEED_OPENCL=0\r\nexport TF_NEED_S3=0\r\nexport TF_NEED_GDR=0\r\nexport TF_NEED_VERBS=0\r\nexport TF_NEED_OPENCL_SYCL=0\r\nexport TF_NEED_CUDA=1\r\nexport TF_CUDA_VERSION=8.0\r\nexport CUDA_TOOLKIT_PATH=/path/to/cuda\r\nexport TF_CUDNN_VERSION=7\r\nexport CUDNN_INSTALL_PATH=/path/to/cudnn\r\nexport TF_CUDA_COMPUTE_CAPABILITIES=\"3.5,5.2,6.0,6.1\"\r\nexport TF_CUDA_CLANG=0\r\nexport GCC_HOST_COMPILER_PATH=/path/to/gcc\r\nexport TF_NEED_MPI=1\r\nexport MPI_HOME=/path/to/openmpi\r\nexport CC_OPT_FLAGS=\"-march=native\"\r\nexport TF_SET_ANDROID_WORKSPACE=0\r\n./configure\r\nbazel build --config=mkl --config=opt --config=cuda \\\r\n          //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n### Describe the problem\r\nFailure to build sources:\r\n```\r\nERROR: /opt/tensorflow/tensorflow/core/BUILD:2077:1: C++ compilation of rule '//tensorflow/core:core_cpu_impl' failed (Exit 1)\r\nIn file included from tensorflow/core/common_runtime/threadpool_device.cc:32:0:\r\n./tensorflow/core/common_runtime/mkl_cpu_allocator.h: In member function 'virtual void tensorflow::MklCPUAllocator::AddAllocVisitor(tensorflow::VisitableAllocator::Visitor)':\r\n./tensorflow/core/common_runtime/mkl_cpu_allocator.h:123:17: error: 'class tensorflow::Allocator' has no member named 'AddAllocVisitor'\r\n     allocator_->AddAllocVisitor(visitor);\r\n                 ^\r\n./tensorflow/core/common_runtime/mkl_cpu_allocator.h: In member function 'virtual void tensorflow::MklCPUAllocator::AddFreeVisitor(tensorflow::VisitableAllocator::Visitor)':\r\n./tensorflow/core/common_runtime/mkl_cpu_allocator.h:127:17: error: 'class tensorflow::Allocator' has no member named 'AddFreeVisitor'\r\n     allocator_->AddFreeVisitor(visitor);\r\n                 ^\r\n```\r\nClass `MklCPUAllocator` has a member `allocator_` which is of type `tensorflow::Allocator` which does not have the member functions accessed. ", "comments": ["I would be happy to provide a fix if get a pointer on what needs to be done. \ud83d\ude04 \r\nHere is the commit --> https://github.com/tensorflow/tensorflow/commit/1baac7862739525351d25202800dc04e8ec3868b", "@poxvoculi it looks like your change added the AddAllocVisitor() and AddFreeVisitor() methods to MklCPUAllocator, then delegated their functionality to the allocator_ data member. But wouldn't you also need to change the declaration of allocator_ to be a VisitableAllocator* itself, so that the delegation would work? ", "@cy89 \r\nThe class `VisitableAllocator` I believe cannot be constructed. So I guessed the intention was to use `BFCAllocator* allocator_`. ", "@cy89, yes.  I don't know how this passed the presubmit build tests, but it did.  Must be missing some mkl coverage.   MklCPUAllocator::allocator_ should now have type VisitableAllocator* not Allocator*.\r\nThe implementation is a BFCAllocator, as here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/mkl_cpu_allocator.h#L95", "Thank you!!\r\nI will close my PR.", "I can see an internal CL that fixes this. Will close, expecting it'll get merged."]}, {"number": 16959, "title": "Remove header dependence on cuda_config.h to fix opensource custom op\u2026", "body": "\u2026 support.\r\n\r\nFixes #14454\r\nFixes #12860\r\n\r\nPiperOrigin-RevId: 185194924", "comments": ["The test failure is unrelated. Merging"]}, {"number": 16958, "title": "Update tb-nightly dep to >= 1.7.0a0, < 1.8.0a0", "body": "Now that tf-nightly is at 1.7.0+ on PyPI and tb-nightly has been updated to publish at 1.7.0+ as well, synchronize the deps so that current tf-nightly depends on current tb-nightly.", "comments": []}, {"number": 16957, "title": "Update data_flow_ops.py", "body": "Fixes #16948", "comments": []}, {"number": 16956, "title": "[1.6] Cherrypicks for RC1", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "I also had `3c2e6f883e87df550521b11edbd837a2479780fc #184075365` in my list, but it looks like it is already in the branch."]}, {"number": 16955, "title": "Row wise lookup table in Tensorflow", "body": "Currently I have a matrix in which each row is a lookup table. Corresponding to it I have a coded matrix with the same number of rows as the lookup table. e.g.\r\n\r\n> LookupTable (matrix size 100, 32)\r\n\r\n> CodedMatrix (matrix size 100, 1000)\r\n\r\nSo the lookup table values match to the corresponding row of the coded matrix. The code matrix contains numbers from 0 to 31 which have a corresponding value in the Lookup table for that specific row.\r\n\r\nThe final output of this should be \r\n\r\n> DecodedMatrix(matrix size 100, 1000)\r\n\r\nIn which each value of the row is replaced with it's corresponding lookup output.\r\n\r\nCurrently in numpy I use a for loop, because at the end I sum up the decoded matrix along the row axis for the final output. The code looks like this\r\n\r\n       out = sum([C[L] for C,L in zip(CodedMatrix, LookupTable)])\r\n\r\nwhich is a still inefficient. But in Tensorflow I use\r\n\r\n     nRows = tf.constant(100, name=\"nRows\")\r\n     n     = tf.Variable(tf.constant(0))\r\n\r\n     def cond(n, out):\r\n         return n < nRows\r\n\r\n     def body(n, out):  \r\n         out = out + tf.gather(LookupTable[m,:], CodedMatrix[m,:])\r\n         return n+1, out\r\n\r\n     out = tf.while_loop(cond, body, [n, out])[1]\r\n\r\nThis execution takes a lot of time, because each time a new tensor is created and using the loop isn't very efficient.\r\n\r\nIs there a way to do this without using while loop? Does tf.gather have any setup to do lookup like this?\r\n\r\nHave I written custom code - Yes\r\nOS Platform and Distribution - Mac OS X High Sierra\r\nTensorFlow installed from - Source\r\nTensorFlow version - 1.5.0-rc0\r\nBazel version - 0.5.4\r\nCUDA/cuDNN version - N/A\r\nGPU model and memory - N/A\r\nExact command to reproduce - Provided aboce", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Updated", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 16954, "title": "Iterator.get_next() documentation improvement request", "body": "### System information\r\nN/A\r\n### Describe the problem\r\nRecently I've written some code using Dataset API and I would like to request a problem with documentation (IMO).  Instead of hardcoding comments [here](https://github.com/tensorflow/tensorflow/blob/3ee1721b46d0e61097d0ee72f01e3de9739f0b6f/tensorflow/python/data/ops/iterator_ops.py#L31), please, move @mrry's annotation about `Iterator.get_next()` and `GET_NEXT_CALL_WARNING_THRESHOLD` into `get_next()` method documentation. \r\n\r\nI don't know why I didn't get that beautiful warning on my console output but I think I'm not the first person with funny thread-bomb running and consuming system resources. :) You know about that also (see comment).\r\nSo... It would be great if you could move all critical annotation into main documentation. I'm thinking now about all ML newcomers rather than me (Yeah, I actually found solution by myself :)) \r\nThat's all. \r\n### Source code / logs\r\nN/A", "comments": []}, {"number": 16953, "title": "TESTING DO NOT MERGE", "body": "", "comments": []}, {"number": 16952, "title": "Branch 185398372", "body": "", "comments": []}, {"number": 16951, "title": "TypeError: int() argument must be a string, a bytes-like object or a number, not 'Tensor'", "body": "### System information\r\n- **Have I written custom code**: Yes\r\n- **OS Platform and Distribution**: Ubuntu 16.04\r\n- **TensorFlow installed from**: binary\r\n- **TensorFlow version**: 1.4.1\r\n- **Python version**: 3.5.2\r\n- **Bazel version:** Not compiled from source\r\n- **GCC/Compiler version**: Not compiled from source\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: GeForce GTX 1080 (8GB x 4)\r\n- **Exact command to reproduce**: N/A\r\n\r\nI'm trying to convert a the initializer form from `tf.Variable` to `tf.get_variable` for `Cudnn_GRU` but I keep getting this error. I have to convert because tensorflow does not allow initializing in loop/control-flow functions and only allow lambda initializers or through `tf.get_variable`\r\n\r\nI have reduced the problem into the following minimal example:\r\n```\r\nimport tensorflow as tf\r\ne = tf.random_uniform_initializer(-0.1, 0.1)\r\ni = tf.constant(0)\r\ndef func():\r\n    gru_fw = tf.contrib.cudnn_rnn.CudnnGRU(num_layers=1, num_units=75, input_size=25)\r\n    # original line: commented out and working if not under a control flow mechanism\r\n    # param_fw = tf.Variable(tf.random_uniform([gru_fw.params_size()], -0.1, 0.1), validate_shape=False)\r\n    # converted line\r\n    param_fw = tf.get_variable(\"abcd\", shape=[gru_fw.params_size()],initializer=e, validate_shape=False)\r\n    return param_fw\r\n\r\ndef func2():\r\n    ### repeat the same thing from func1\r\n    pass\r\n\r\nresult = tf.cond(tf.equal(i, tf.constant(0)),func,func2)\r\n```\r\nThe traceback is as follows:\r\n```\r\nTraceback (most recent call last):\r\n\tFile \"test_run_error.py\", line 16, in <module>\r\n\t\tresult = tf.cond(tf.equal(i, tf.constant(0)),func,func2)\r\n\tFile \"/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\", line 316, in new_func\r\n\t\treturn func(*args, **kwargs)\r\n\tFile \"/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1855, in cond\r\n\t\torig_res_t, res_t = context_t.BuildCondBranch(true_fn)\r\n\tFile \"/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1725, in BuildCondBranch\r\n\t\toriginal_result = fn()\r\n\tFile \"test_run_error.py\", line 9, in func\r\n\t\tparam_fw = tf.get_variable(\"abcd\", shape=[gru_fw.params_size()],initializer=e, validate_shape=False)\r\n\tFile \"/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 1203, in get_variable\r\n\t\tconstraint=constraint)\r\n\tFile \"/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 1092, in get_variable\r\n\t\tconstraint=constraint)\r\n\tFile \"/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 425, in get_variable\r\n\t\tconstraint=constraint)\r\n\tFile \"/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 394, in _true_getter\r\n\t\tuse_resource=use_resource, constraint=constraint)\r\n\tFile \"/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 730, in _get_single_variable\r\n\t\tshape = tensor_shape.as_shape(shape)\r\n\tFile \"/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\", line 849, in as_shape\r\n\t\treturn TensorShape(shape)\r\n\tFile \"/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\", line 455, in __init__\r\n\t\tself._dims = [as_dimension(d) for d in dims_iter]\r\n\tFile \"/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\", line 455, in <listcomp>\r\n\t\tself._dims = [as_dimension(d) for d in dims_iter]\r\n\tFile \"/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\", line 397, in as_dimension\r\n\t\treturn Dimension(value)\r\n\tFile \"/home/search/snetP/snet/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\", line 32, in __init__\r\n\t\tself._value = int(value)\r\nTypeError: int() argument must be a string, a bytes-like object or a number, not 'Tensor'\r\n```", "comments": ["The exception is raised as Tensor is passed as `shape` argument:\r\n\r\n```python\r\nimport tensorflow as tf\r\ngru_fw = tf.contrib.cudnn_rnn.CudnnGRU(num_layers=1, num_units=75, input_size=25)\r\ne = tf.random_uniform_initializer(-0.1, 0.1)\r\nparam_fw = tf.get_variable(\"abcd\", shape=[gru_fw.params_size()],initializer=e, validate_shape=False)\r\n```", "Yeah, I solved it by using `shape` as argument in initializer and moving the initializer outside the condition and discarding the shape argument from `tf.get_variable`. It is to be noted that the same Tensor can be passed as a `shape` argument to initializer, though."]}, {"number": 16950, "title": "Windows Installation tutorial has wrong cuDNN version requirement, 7 required for latest version.", "body": "**Inaccuracies in the documentation**.\r\n\r\nIn the installation documentation Tensorflow in Windows says that the system must be running a version of CUDA 9.0 (correctly) and cuDNN 6.0 (not correct) because on the Nvidia website we can download and install only:\r\n- **cuDNN** v7.0.5 , for **CUDA** 8.0/9.0/9.1 ;\r\n- **cuDNN** v7.0.4 , for **CUDA** 8.0/9.0 ;\r\n- **cuDNN** v6.0 , for **CUDA** 8.0 .\r\n\r\nIn https://developers.googleblog.com/2018/01/announcing-tensorflow-15.html said that \r\n\r\n> If you are using GPU Acceleration on **Windows** or Linux, TensorFlow 1.5 now has CUDA 9 and **cuDNN 7** support built-in. \r\n\r\nIn issue #16477 (https://github.com/tensorflow/tensorflow/issues/16477)  as @gunan  said \r\n> Windows also requires cuDNN 7, looks like we missed that.\r\n\r\nIn proof of his words let me give you output logs:\r\n\r\n(tensorflow15) C:\\Windows\\system32>python\r\nPython 3.5.4 |Anaconda, Inc.| (default, Nov  8 2017, 14:34:30) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n      import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\User\\AppData\\Local\\conda\\conda\\envs\\tensorflow15\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\", line 87, in preload_check\r\n    ctypes.WinDLL(build_info.cudnn_dll_name)\r\n  File \"C:\\Users\\User\\AppData\\Local\\conda\\conda\\envs\\tensorflow15\\lib\\ctypes\\__init__.py\", line 351, in __init__\r\n    self._handle = _dlopen(self._name, mode)\r\nOSError: [WinError 126] \u041d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d \u0443\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043c\u043e\u0434\u0443\u043b\u044c (The specified module was not found)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\User\\AppData\\Local\\conda\\conda\\envs\\tensorflow15\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\User\\AppData\\Local\\conda\\conda\\envs\\tensorflow15\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\User\\AppData\\Local\\conda\\conda\\envs\\tensorflow15\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 30, in <module>\r\n    self_check.preload_check()\r\n  File \"C:\\Users\\User\\AppData\\Local\\conda\\conda\\envs\\tensorflow15\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\", line 97, in preload_check\r\n    % (build_info.cudnn_dll_name, build_info.cudnn_version_number))\r\nImportError: Could not find **'cudnn64_7.dll'**. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Note that installing cuDNN is a separate step from installing CUDA, and this DLL is often found in a different directory from the CUDA DLLs. You may install the necessary DLL by downloading **cuDNN 7** from this URL: https://developer.nvidia.com/cudnn\r\n\r\n\r\nIn conclusion.\r\n\r\nNeed to replace:\r\n\r\n> **cuDNN  v6.0**. For details, see NVIDIA's documentation. Note that cuDNN is typically installed in a different location from the other CUDA DLLs. Ensure that you add the directory where you installed the cuDNN DLL to your %PATH% environment variable.\r\n\r\nto\r\n\r\n> **cuDNN v7.0**. For details, see NVIDIA's documentation. Note that cuDNN is typically installed in a different location from the other CUDA DLLs. Ensure that you add the directory where you installed the cuDNN DLL to your %PATH% environment variable.\r\n\r\non the web page https://www.tensorflow.org/install/install_windows\r\n", "comments": ["This looks to me like it's been fixed in the repository. \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/install/install_windows.md\r\n\r\nPlease reopen if I've misread things.\r\n", "And @nikitaverbis, thanks for reporting this!"]}, {"number": 16949, "title": "Controlled logging for Estimator", "body": "Added `logging_every_n_iter` argument for Estimator.fit. This controls the frequency of the display of metrics.", "comments": ["Nagging Assignee @protoget: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "contrib.learn is deprecated, I'd rather not add features to it. Please switch to tf.estimator.*"]}, {"number": 16948, "title": "tf.QueueBase.dequeue_many returns list instead of tuple", "body": "The dequeue_many operation returns a **list** of Tensors, while the documentation states that it should be a **tuple** (which is more sensible).\r\n\r\n> Returns:\r\n> The tuple of concatenated tensors that was dequeued.\r\n(https://www.tensorflow.org/api_docs/python/tf/QueueBase)\r\n\r\nVersion: 1.5.0 from PIP, Python3, on OS/X", "comments": ["@mrry, I think that _dequeue_return_value was originally Matthieu's routine, but it looks like that routine is built to return a list not a tuple. Could you please comment on whether we should fix the documentation or fix the implementation, and perhaps assign someone to implement? \r\n\r\nMarking \"docs\" for now. ", "I don't think we can fix the implementation: although I agree it would make more sense to return a tuple here, there might be code that depends on it being a list. \r\n\r\nIn a future major version of TensorFlow, it might make sense to use the `nest` library to allow nested structures (including lists, tuples, dictionaries, and recursive nests of those) to be stored in a queue.\r\n\r\nFor now, we'd welcome a fix to the documentation. I'm marking it as contributions welcome, since there is nobody specific working on that API right now.", "I see, that's unfortunate but understandable. I submitted a pull request. #16957 "]}, {"number": 16947, "title": "Using P100 on different generations of CPUs causes training to slow down on TF1.5", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: RedHat 7.3\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.5\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: From yum installation.\r\n- **GCC/Compiler version (if compiling from source)**: 4.8\r\n- **CUDA/cuDNN version**: 7.0.5\r\n- **GPU model and memory**: P100-PCIe 16GB\r\n- **Exact command to reproduce**: bazel build options: bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package \r\n\r\n\r\n### Source code / logs\r\n[Alexnet_PSCPU_GPU1_fp32_18_02_08_18_01.log](https://github.com/tensorflow/tensorflow/files/1716272/Alexnet_PSCPU_GPU1_fp32_18_02_08_18_01.log)\r\n[Inception3_PSCPU_GPU1_fp32_18_02_08_18_01.log](https://github.com/tensorflow/tensorflow/files/1716273/Inception3_PSCPU_GPU1_fp32_18_02_08_18_01.log)\r\n[Resnet50_PSCPU_GPU1_fp32_18_02_08_18_01.log](https://github.com/tensorflow/tensorflow/files/1716274/Resnet50_PSCPU_GPU1_fp32_18_02_08_18_01.log)\r\n\r\nHi all,\r\n\r\nWhen I use tensorflow benchmark to the model performance will see the decreasing performance.\r\n\r\nThe following is the performance result of alexnet, we can observe weird results in the performance will continue to decrease on Broadwell CPU.\r\nStep\tImg/sec\ttotal_loss\r\n1\timages/sec: 2935.5 +/- 0.0 (jitter = 0.0)\t7.198\r\n10\timages/sec: 2934.3 +/- 0.9 (jitter = 2.8)\t7.201\r\n20\timages/sec: 2936.5 +/- 0.8 (jitter = 1.9)\t7.199\r\n30\timages/sec: 2936.5 +/- 0.6 (jitter = 2.2)\t7.201\r\n40\timages/sec: 2936.4 +/- 0.5 (jitter = 2.3)\t7.200\r\n50\timages/sec: 2935.4 +/- 1.1 (jitter = 2.5)\t7.199\r\n60\timages/sec: 2935.4 +/- 0.9 (jitter = 2.7)\t7.200\r\n70\timages/sec: 2935.4 +/- 0.8 (jitter = 3.0)\t7.202\r\n80\timages/sec: 2935.5 +/- 0.7 (jitter = 3.0)\t7.201\r\n90\timages/sec: 2935.7 +/- 0.7 (jitter = 3.6)\t7.200\r\n100\timages/sec: 2935.8 +/- 0.6 (jitter = 3.2)\t7.200\r\n110\timages/sec: 2936.0 +/- 0.6 (jitter = 3.7)\t7.198\r\n120\timages/sec: 2936.0 +/- 0.5 (jitter = 3.6)\t7.198\r\n130\timages/sec: 2936.0 +/- 0.5 (jitter = 3.3)\t7.199\r\n140\timages/sec: 2935.9 +/- 0.5 (jitter = 3.2)\t7.198\r\n150\timages/sec: 2936.2 +/- 0.4 (jitter = 3.2)\t7.198\r\n160\timages/sec: 2936.3 +/- 0.4 (jitter = 3.2)\t7.198\r\n170\timages/sec: 2936.1 +/- 0.4 (jitter = 3.3)\t7.199\r\n180\timages/sec: 2936.1 +/- 0.4 (jitter = 3.2)\t7.199\r\n190\timages/sec: 2936.1 +/- 0.4 (jitter = 3.2)\t7.199\r\n200\timages/sec: 2936.0 +/- 0.4 (jitter = 3.1)\t7.200\r\n210\timages/sec: 2936.1 +/- 0.4 (jitter = 3.1)\t7.199\r\n220\timages/sec: 2936.0 +/- 0.3 (jitter = 3.0)\t7.199\r\n230\timages/sec: 2935.9 +/- 0.3 (jitter = 3.1)\t7.199\r\n240\timages/sec: 2935.9 +/- 0.3 (jitter = 3.2)\t7.199\r\n250\timages/sec: 2935.9 +/- 0.3 (jitter = 3.2)\t7.199\r\n260\timages/sec: 2936.0 +/- 0.3 (jitter = 3.2)\t7.198\r\n270\timages/sec: 2936.0 +/- 0.3 (jitter = 3.1)\t7.199\r\n280\timages/sec: 2936.1 +/- 0.3 (jitter = 3.4)\t7.197\r\n290\timages/sec: 2936.3 +/- 0.3 (jitter = 3.6)\t7.198\r\n300\timages/sec: 2936.3 +/- 0.3 (jitter = 3.4)\t7.200\r\n310\timages/sec: 2936.4 +/- 0.3 (jitter = 3.4)\t7.199\r\n320\timages/sec: 2936.4 +/- 0.3 (jitter = 3.5)\t7.199\r\n330\timages/sec: 2936.4 +/- 0.3 (jitter = 3.5)\t7.198\r\n340\timages/sec: 2936.5 +/- 0.3 (jitter = 3.6)\t7.200\r\n350\timages/sec: 2936.5 +/- 0.3 (jitter = 3.7)\t7.200\r\n360\timages/sec: 2936.5 +/- 0.2 (jitter = 3.6)\t7.200\r\n370\timages/sec: 2936.5 +/- 0.2 (jitter = 3.7)\t7.200\r\n380\timages/sec: 2936.5 +/- 0.2 (jitter = 3.7)\t7.199\r\n390\timages/sec: 2936.6 +/- 0.2 (jitter = 3.7)\t7.199\r\n400\timages/sec: 2936.6 +/- 0.2 (jitter = 3.6)\t7.198\r\n410\timages/sec: 2936.7 +/- 0.2 (jitter = 3.6)\t7.200\r\n420\timages/sec: 2936.7 +/- 0.2 (jitter = 3.6)\t7.197\r\n430\timages/sec: 2936.7 +/- 0.2 (jitter = 3.6)\t7.198\r\n440\timages/sec: 2936.7 +/- 0.2 (jitter = 3.6)\t7.198\r\n450\timages/sec: 2936.7 +/- 0.2 (jitter = 3.6)\t7.198\r\n460\timages/sec: 2936.7 +/- 0.2 (jitter = 3.6)\t7.199\r\n470\timages/sec: 2936.6 +/- 0.2 (jitter = 3.5)\t7.200\r\n480\timages/sec: 2936.6 +/- 0.2 (jitter = 3.6)\t7.199\r\n490\timages/sec: 2936.6 +/- 0.2 (jitter = 3.6)\t7.196\r\n500\timages/sec: 2936.6 +/- 0.2 (jitter = 3.6)\t7.198\r\n510\timages/sec: 2936.6 +/- 0.2 (jitter = 3.6)\t7.199\r\n520\timages/sec: 2935.8 +/- 0.3 (jitter = 3.7)\t7.198\r\n530\timages/sec: 2933.9 +/- 0.7 (jitter = 3.8)\t7.200\r\n540\timages/sec: 2931.4 +/- 1.0 (jitter = 3.9)\t7.197\r\n550\timages/sec: 2925.7 +/- 1.9 (jitter = 4.0)\t7.197\r\n560\timages/sec: 2920.8 +/- 2.4 (jitter = 4.2)\t7.198\r\n570\timages/sec: 2916.1 +/- 2.7 (jitter = 4.3)\t7.199\r\n580\timages/sec: 2911.0 +/- 3.0 (jitter = 4.4)\t7.200\r\n590\timages/sec: 2903.0 +/- 3.7 (jitter = 4.6)\t7.196\r\n600\timages/sec: 2894.5 +/- 4.3 (jitter = 4.7)\t7.196\r\n610\timages/sec: 2887.1 +/- 4.7 (jitter = 4.8)\t7.200\r\n620\timages/sec: 2879.7 +/- 5.0 (jitter = 5.0)\t7.200\r\n630\timages/sec: 2872.6 +/- 5.3 (jitter = 5.2)\t7.199\r\n640\timages/sec: 2865.7 +/- 5.6 (jitter = 5.4)\t7.198\r\n650\timages/sec: 2852.2 +/- 6.5 (jitter = 5.6)\t7.197\r\n660\timages/sec: 2832.6 +/- 7.7 (jitter = 5.7)\t7.200\r\n670\timages/sec: 2814.1 +/- 8.6 (jitter = 5.8)\t7.198\r\n680\timages/sec: 2799.0 +/- 9.2 (jitter = 6.0)\t7.198\r\n690\timages/sec: 2785.8 +/- 9.7 (jitter = 6.1)\t7.200\r\n700\timages/sec: 2773.1 +/- 10.0 (jitter = 6.3)\t7.199\r\n710\timages/sec: 2760.9 +/- 10.4 (jitter = 6.5)\t7.198\r\n....\r\n9940\timages/sec: 1442.7 +/- 4.2 (jitter = 84.0)\t7.197\r\n9950\timages/sec: 1442.6 +/- 4.2 (jitter = 83.8)\t7.196\r\n9960\timages/sec: 1442.6 +/- 4.2 (jitter = 83.6)\t7.197\r\n9970\timages/sec: 1442.6 +/- 4.2 (jitter = 83.4)\t7.197\r\n9980\timages/sec: 1442.6 +/- 4.2 (jitter = 83.3)\t7.197\r\n9990\timages/sec: 1442.6 +/- 4.2 (jitter = 83.0)\t7.197\r\n10000\timages/sec: 1442.3 +/- 4.2 (jitter = 83.0)\t7.198\r\n\r\n\r\nBut we test the same environment on Purley platform, we could get the normal performance on training result.\r\n\r\nTensorflow (img/sec)         | AlexNet | InceptionV3 | RenNet50\r\n SKL   -   P100 GPU x 1       | 2919.01 | 139.12 | 219.91\r\nBWD -    P100GPU x 1        | 1441.96 | 77.87   | 120.8\r\n\r\nWhat is the major problem on this strange results? \r\n\r\n", "comments": ["Can you please specify more details of your run, such as tf_cnn_benchmark commit #, the exact command line to run the benchmark? It seems the speed stabilized after 100 steps. Maybe GPU overheating? ", "Hi @bignamehyp ,\r\n\r\nThe following are the exact command we used on tf_cnn_benchmark. \r\nAlexNet:\r\npython tf_cnn_benchmarks.py --local_parameter_device=cpu --num_gpus=1 \\\r\n--batch_size=512 --num_batches=10000 --model=alexnet --variable_update=replicated --all_reduce_spec nccl\r\nInceptionV3:\r\npython tf_cnn_benchmarks.py --local_parameter_device=cpu --num_gpus=1 \\\r\n--batch_size=64 --num_batches=10000 --model=inception3 --variable_update=replicated --all_reduce_spec nccl \r\nResNet50:\r\npython tf_cnn_benchmarks.py --local_parameter_device=cpu --num_gpus=1 \\\r\n--batch_size=64 --num_batches=10000 --model=resnet50 --variable_update=replicated --all_reduce_spec nccl\r\n\r\n\r\nWe observed that the GPU temperature found the results are within a reasonable range.\r\n\r\nGPUID                     | GPU0 | GPU1 | GPU2 | GPU3 | GPU4 | GPU5 | GPU6 | GPU7\r\n\r\nAverage                 | 76.22   |77.21  | 76.79  | 76.69 | 77.70 | 75.67  | 77.30 | 76.83\r\nMax Temp.             | 80       | 80      | 80      | 80      | 80      | 80       | 80      | 80\r\nSlow down Temp   | 92       | 92      | 92      | 92      | 92      | 92       | 92      | 92 \r\n\r\n![broadwell_gpu_temp](https://user-images.githubusercontent.com/7540820/36650062-25fcfce6-1adc-11e8-9c23-37a576bfc2e9.png)\r\n", "hi @bignamehyp ,\r\n\r\nThanks for your tips. When I changed  the \"nvidia-smi -l 1 \" to the \"nvidia-smi -lms 1\".\r\n I saw the shorter interval timer had recorded the rumetime temperature which are out of slow down temperature. \r\n\r\nWhen I changed the fans speed, can solve this problem."]}, {"number": 16946, "title": "tensorflow lite converter(toco) build error ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Windows 10(64bit)\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**: tensorflow 1.5.0\r\n- **Python version**: Python 2.7/3.6\r\n- **Bazel version (if compiling from source)**:  bazel 0.9.0\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: No GPU model\r\n\r\n### Describe the problem\r\nI try to build the toco that is tensorflow lite converter. \r\nBut I can not success to build. please see below for the details.\r\n\r\n### Source code / logs\r\n`C:\\tensorflow>bazel build //tensorflow/contrib/lite/toco:toco`\r\nThe following error message appears.\r\n> ERROR: Skipping '//tensorflow/contrib/lite/toco:toco': error loading package 'tensorflow/contrib/lite/toco': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': Traceback (most recent call last):\r\n        File \"C:/tensorflow/third_party/repo.bzl\", line 88\r\n                _apply_patch(ctx, ctx.attr.patch_file)\r\n        File \"C:/tensorflow/third_party/repo.bzl\", line 59, in _apply_patch\r\n                _execute_and_check_ret_code(ctx, cmd)\r\n        File \"C:/tensorflow/third_party/repo.bzl\", line 44, in _execute_and_check_ret_code\r\n                fail(\"Non-zero return code({1}) when ...))\r\nNon-zero return code(3) when executing 'C:\\tools\\msys64\\usr\\bin\\bash.exe -c patch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch':\r\nStdout: patching file src/google/protobuf/compiler/cpp/cpp_file.cc\r\nStderr: Assertion failed: hunk, file ../patch-2.5.9-src/patch.c, line 354\r\nThis application has requested the Runtime to terminate it in an unusual way.\r\nPlease contact the application's support team for more information.\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow/contrib/lite/toco': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': Traceback (most recent call last):\r\n        File \"C:/tensorflow/third_party/repo.bzl\", line 88\r\n                _apply_patch(ctx, ctx.attr.patch_file)\r\n        File \"C:/tensorflow/third_party/repo.bzl\", line 59, in _apply_patch\r\n                _execute_and_check_ret_code(ctx, cmd)\r\n        File \"C:/tensorflow/third_party/repo.bzl\", line 44, in _execute_and_check_ret_code\r\n                fail(\"Non-zero return code({1}) when ...))\r\nNon-zero return code(3) when executing 'C:\\tools\\msys64\\usr\\bin\\bash.exe -c patch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch':\r\nStdout: patching file src/google/protobuf/compiler/cpp/cpp_file.cc\r\nStderr: Assertion failed: hunk, file ../patch-2.5.9-src/patch.c, line 354\r\nThis application has requested the Runtime to terminate it in an unusual way.\r\nPlease contact the application's support team for more information.\r\nINFO: Elapsed time: 27.852s\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/contrib/lite/toco\r\n\r\n- plus info.\r\nThe following message appears when I input like this in command line. (for test)\r\n`patch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch`\r\n> patching file src/google/protobuf/compiler/cpp/cpp_file.cc\r\nAssertion failed: hunk, file ../patch-2.5.9-src/patch.c, line 354\r\nThis application has requested the Runtime to terminate it in an unusual way.\r\nPlease contact the application's support team for more information.\r\n\r\n`patch -p1 -d C:/users/r3pc/appdata/local/temp/_bazel_r3pc/x1e5egqw/external/protobuf_archive -i C:/tensorflow/third_party/protobuf/add_noinlines.patch --binary`\r\n> patching file src/google/protobuf/compiler/cpp/cpp_file.cc\r\nHunk #1 succeeded at 750 with fuzz 1 (offset 193 lines).\r\nHunk #2 succeeded at 825 (offset 169 lines).\r\nHunk #3 succeeded at 906 with fuzz 2 (offset 169 lines).\r\n\r\nI don't know how to add --binary option to script...\r\nref. https://github.com/tensorflow/tensorflow/issues/10435\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "That problem is solved.\r\nadd \"--binary\" to  58 line in repo.bzl\r\nlike this, \r\n` ctx, [\"patch\", \"-p1\", \"-d\", ctx.path(\".\"), \"-i\", ctx.path(patch_file) ,\"--binary\"])`"]}, {"number": 16945, "title": "Fix some warnings with keep_dims in `tf.contrib.distributions`", "body": "This fix fixes some warnings with keep_dims in `tf.contrib.distributions` and `math_ops_tests`.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 16944, "title": "Tensorflow (0.11.0) returns error while running training and testing scripts. ", "body": "Due to some constraint i am using Tensorflow version 0.11.0, which i installed using bellow commands:\r\nexport TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\r\npip install --ignore-installed --upgrade $TF_BINARY_URL\r\n\r\nAfter installation i am successfully able to import Tensorflow in python:\r\n```\r\ncvsion@cvsion:~$ python -c 'import tensorflow as tf; print(tf.__version__)'\r\n0.11.0\r\n\r\ncvsion@cvsion:~/Deeplearning/DeepTensorFlow/tensorlayer$ python \r\nPython 2.7.12 (default, Dec  4 2017, 14:50:18) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> \r\n```\r\n\r\nbut when i am running my training script its giving error:\r\n\r\n> Traceback (most recent call last):\r\n>   File \"./testing_nvidia.py\", line 8, in <module>\r\n>     import tensorlayer as tl\r\n>   File \"/home/cvsion/Deeplearning/DeepTensorFlow/tensorlayer/tensorlayer/__init__.py\", line 23, in <module>\r\n>     from . import distributed\r\n>   File \"/home/cvsion/Deeplearning/DeepTensorFlow/tensorlayer/tensorlayer/distributed.py\", line 9, in <module>\r\n>     from tensorflow.python.training import session_run_hook\r\n> ImportError: cannot import name session_run_hook\r\n\r\nAny idea to resolve above issue", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code = Yes\r\nOS Platform and Distribution=  Ubuntu 16.04\r\nTensorFlow installed from = export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\r\npip install --ignore-installed --upgrade $TF_BINARY_URL\r\nBazel version= N/A\r\nCUDA/cuDNN version = CUDA 8.0/cuDNN 6.1\r\nGPU model and memory: 12GB\r\nExact command to reproduce: python ./testing_nvidia.py", "I'm sorry, but 0.11.0 is past our support window. ", "As we do not support 0.11.0 , will be closing this request. Please try latest versions. Thank you."]}, {"number": 16943, "title": "tensorflow serving, add_meta_graph_and_variables() legacy_init_op doesn't surpport a tf.group of multi ops. ", "body": "the new nmt model in tutorial [https://github.com/tensorflow/nmt](url) using index_table_from_file and the datasets API make_initializable_iterator. So when I want to deploy the model as serving,  in builder.add_meta_graph_and_variables(), I assigned legacy_init_op with an tf.group(tf.tables_initializer(), infer_model.iterator.initializer) or use tf.control_dependencies, when I export the model and run it, I met a error: Failed precondition: Table not initialized.\r\n\r\n```# Build the signature_def_map.\r\n    builder = tf.saved_model.builder.SavedModelBuilder(export_path)\r\n    tensor_info_src = tf.saved_model.utils.build_tensor_info(\r\n        infer_model.src_placeholder)\r\n    tensor_info_sample_words = tf.saved_model.utils.build_tensor_info(\r\n        loaded_infer_model.sample_words)\r\n\r\n    prediction_signature = (\r\n        tf.saved_model.signature_def_utils.build_signature_def(\r\n            inputs={'src': tensor_info_src},\r\n            outputs={'sample_words': tensor_info_sample_words},\r\n            method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))\r\n\r\n    with tf.control_dependencies([tf.tables_initializer()]):\r\n         op1 = tf.no_op()\r\n    with tf.control_dependencies([op1, infer_model.iterator.initializer]):\r\n         op2 = tf.no_op(name='legacy_init_op')\r\n\r\n    legacy_init_op = tf.group(tf.tables_initializer(), infer_model.iterator.initializer,\r\n                              name='legacy_init_op')\r\n    print(legacy_init_op)\r\n\r\n    table_init_op = tf.group(tf.tables_initializer(),\r\n                             name='legacy_init_op')\r\n    print(table_init_op)\r\n\r\n    builder.add_meta_graph_and_variables(\r\n        session, [tf.saved_model.tag_constants.SERVING],\r\n        signature_def_map={\r\n            'predict_chat': prediction_signature\r\n        },\r\n        legacy_init_op=op2)\r\n    builder.save()\r\n    print('Done exporting.')```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I have the same issue.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing because the template is not filled out.\r\n\r\n@airaria if you still have this issue, please file a new issue with the issue template filled out.\r\n\r\n"]}, {"number": 16942, "title": "wrapping op in while_loop makes it run faster, even with single iteration", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: v1.5.0-0-g37aa430 (1.5.0)\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: 0.9.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: CUDA 9.1, CuDNN 7\r\n- **GPU model and memory**: GeForce GTX 1050, 4042MiB\r\n- **Exact command to reproduce**: See below\r\n\r\n### Describe the problem\r\n\r\nI am working on an object detector. I have one entry point to my code that detects objects in a single image, and another that reads in a batch of images. The batch version uses `tf.while_loop` to apply the same inference code to each image in the batch. I can't put all the images into a single tensor because they are all likely to be different dimensions, so I am using a `TensorArray`. The final results work as expected, but the timing behavior is odd.\r\n\r\nThe first `sess.run` call in the loop over the dataset is always slower than the rest, presumably due to caching, memory allocation, etc. However, the first call in the non-batch version is WAY slower than the first call in the batch version. I distilled this down to the minimal examples below. The first version does a dot product in a straightforward way. On my machine, it takes about 13 seconds to run. The second version does the same dot product, but wrapped in a `while_loop` and writing the result to an intermediate `TensorArray` and then reading it out again. On my machine, it takes about 4 seconds to run. Presumably, version 2 has to do the same data copying, memory allocation, etc. as version 1, in addition to the overhead of the `while_loop` and `TensorArray` calls. So, the timing behavior is unexpected.\r\n\r\nVersion 2 can even be modified to compute the same dot product 10 (or more) times, and it will still run faster. This behavior holds even if the loop body is modified to use a random vector every time, so I don't think it is due to caching. It also holds even if `parallel_iterations` is held to `1`, so I don't think it is due to parallelism. Is this a bug, or is something else going on under the hood?\r\n\r\n### Source code / logs\r\n\r\n```python\r\n# version 1 (~13 seconds on my machine)\r\ngraph = tf.Graph()\r\nwith graph.as_default(), tf.device('/gpu:0'):\r\n    nums = tf.range(200000000, dtype=tf.float32)\r\n    dot_product = tf.reduce_sum((nums/2) * (nums-5))\r\nwith tf.Session(graph=graph) as sess:\r\n    print(sess.run(dot_product))\r\n```\r\n\r\n```python\r\n# version 2 (~4 seconds on my machine)\r\ngraph = tf.Graph()\r\nwith graph.as_default(), tf.device('/gpu:0'):\r\n    arr = tf.TensorArray(size=1, dtype=tf.float32)\r\n    nums = tf.range(200000000, dtype=tf.float32)\r\n    i = tf.constant(0)\r\n    def body(i, arr):\r\n        arr = arr.write(i, tf.reduce_sum((nums/2) * (nums-5)))\r\n        return i+1, arr\r\n    i, arr = tf.while_loop(\r\n        cond=lambda i, x: i < 1,\r\n        body=body,\r\n        loop_vars=[i, arr],\r\n        parallel_iterations=1)\r\n    dot_product = arr.read(0)\r\nwith tf.Session(graph=graph) as sess:\r\n    print(sess.run(dot_product))\r\n```\r\n", "comments": ["I find the same behaviour.\r\nI include a slight modified copy-pastable MVCE.\r\nI get 9s and 3 s with the below on CPU.\r\n\r\n## MVCE\r\n``` python\r\nimport tensorflow as tf\r\nfrom timeit import default_timer\r\n\r\ndef func(nums):\r\n    return tf.reduce_sum(tf.multiply(tf.add(nums,1), nums))\r\n\r\ndef v1():\r\n    graph = tf.Graph()\r\n    with graph.as_default(), tf.device('/cpu:0'):\r\n        nums = tf.range(200000000, dtype=tf.float32)\r\n        dot_product = func(nums)\r\n    with tf.Session(graph=graph) as sess:\r\n        print(sess.run(dot_product))\r\n\r\ndef v2():\r\n    graph = tf.Graph()\r\n    with graph.as_default(), tf.device('/cpu:0'):\r\n        arr = tf.TensorArray(size=1, dtype=tf.float32)\r\n        nums = tf.range(200000000, dtype=tf.float32)\r\n        i = tf.constant(0)\r\n        def body(i, arr):\r\n            arr = arr.write(i, func(nums))\r\n            return i+1, arr\r\n        i, arr = tf.while_loop(\r\n            cond=lambda i, x: i < 1,\r\n            body=body,\r\n            loop_vars=[i, arr],\r\n            parallel_iterations=1)\r\n        dot_product = arr.read(0)\r\n    with tf.Session(graph=graph) as sess:\r\n        print(sess.run(dot_product))\r\n\r\nif __name__ == '__main__':\r\n    t1 = default_timer()\r\n    v1()\r\n    print(\"time v1: {}\".format(default_timer() - t1))\r\n    t1 = default_timer()\r\n    v2()\r\n    print(\"time v2: {}\".format(default_timer() - t1))\r\n```", "@prb12 can you shed any light on what the tf.while_loop does to performance? ", "I'll try wrapping in a profiler context to see if I can spot the difference.", "Seems strange - since v2 should be a superset of the ops in v1.  Perhaps a difference in something like constant folding?   (maybe try changing the optimization level in the session options) \r\n\r\nProbably easiest to just grab a timeline of each (or use the profiler) and see which ops actually get run.\r\n\r\nAlso, several of the tensors involved are 1.6GB so this *might* cause some interesting differences in memory allocation?", "The first `session.run` call being slower is almost certainly due to running the constant folder to optimize the graph.  When `session.run` is called for the first time on a set of feeds/fetches, the constant folder will run and parts of the graph will be replaced with a `tf.constant` node that has a protobuf containing the constant values.  Potential constants are always evaluated on the CPU device.  (Also, if the resulting value is larger than a certain threshold -- around 10s of MB iirc -- then it is discarded and the graph is left alone).\r\n\r\nIt it likely that when the same expression is computed in a loop the constant folder doesn't consider it a candidate.\r\n\r\nIn your code snippets above, you only time a single session.run call for each variant...   I would expect constant folding overheads (and many other overheads) to only affect the first execution of each graph.  \r\n\r\nIf you do a warmup `sess.run(dot_product)` call, then put a loop around multiple calls of `sess.run(dot_product)` and evaluate it repeatedly, do you still see the loop version running faster?   If so then this is worth looking into further!\r\n\r\ni.e. something like:\r\n```\r\nwith tf.Session(graph=graph) as sess:\r\n   print(sess.run(dot_product))\r\n   t1 = default_timer()\r\n   for _ in xrange(10):\r\n       sess.run(dot_product)\r\n   print(\"time v1: {}\".format(default_timer() - t1))\r\n```\r\n(Note that if the size of `dot_product` was bigger than a scalar then I would also be concerned about repeatedly fetching the result...  you can use `session.run(dot_product.op)` when benchmarking)", "So I ran sess.run first and then 10 runs in a timing loop with the constant variable, and also with a random variable. I also created a profile timeline for each.\r\n\r\nConstant:\r\nV1 timing: 0.00014590219943784178 / run 10 runs\r\nV2 timing: 0.2790762079996057 / run 10 runs\r\n\r\nRandom:\r\nV1 timing: 1.2271329755021725 / run 10 runs\r\nV2 timing: 1.209720030199969 / run 10 runs\r\n\r\nSo it seems while loop is slower as expected after initial graph run.\r\n\r\n``` python\r\nimport tensorflow as tf\r\nfrom timeit import default_timer\r\n\r\ndef func(nums):\r\n    return tf.reduce_sum(tf.multiply(tf.add(nums,1), nums))\r\n\r\ndef v1(random=False):\r\n    graph = tf.Graph()\r\n    with graph.as_default(), tf.device('/cpu:0'):\r\n        if random:\r\n            nums = tf.random_normal(shape=(200000000,),dtype=tf.float32)\r\n        else:\r\n            nums = tf.range(200000000, dtype=tf.float32)\r\n        dot_product = func(nums)\r\n    with tf.Session(graph=graph) as sess:\r\n        sess.run(dot_product)\r\n        t1 = default_timer()\r\n        for i in range(10):\r\n            sess.run(dot_product)\r\n        print(\"V1 timing: {} / run 10 runs\".format((default_timer() - t1)/10.))\r\n\r\n        profiler = tf.profiler.Profiler(sess.graph)\r\n        run_meta = tf.RunMetadata()\r\n        sess.run(dot_product,\r\n                options = tf.RunOptions(trace_level = tf.RunOptions.FULL_TRACE),\r\n                run_metadata = run_meta)\r\n        profiler.add_step(0,run_meta)\r\n        opts = (tf.profiler.ProfileOptionBuilder(\r\n                 tf.profiler.ProfileOptionBuilder.time_and_memory())\r\n                 .with_step(0)\r\n                 .with_timeline_output('timeline_v1_{}'.format('random' if random else 'const')).build())\r\n        profiler.profile_graph(options=opts)\r\n\r\n\r\n\r\ndef v2(random=False):\r\n    graph = tf.Graph()\r\n    with graph.as_default(), tf.device('/cpu:0'):\r\n        arr = tf.TensorArray(size=1, dtype=tf.float32)\r\n        if random:\r\n            nums = tf.random_normal(shape=(200000000,),dtype=tf.float32)\r\n        else:\r\n            nums = tf.range(200000000, dtype=tf.float32)\r\n        i = tf.constant(0)\r\n        def body(i, arr):\r\n            arr = arr.write(i, func(nums))\r\n            return i+1, arr\r\n        i, arr = tf.while_loop(\r\n            cond=lambda i, x: i < 1,\r\n            body=body,\r\n            loop_vars=[i, arr],\r\n            parallel_iterations=1)\r\n        dot_product = arr.read(0)\r\n    with tf.Session(graph=graph) as sess:\r\n        sess.run(dot_product)\r\n        t1 = default_timer()\r\n        for i in range(10):\r\n            sess.run(dot_product)\r\n        print(\"V2 timing: {} / run 10 runs\".format((default_timer() - t1)/10.))\r\n\r\n        profiler = tf.profiler.Profiler(sess.graph)\r\n        run_meta = tf.RunMetadata()\r\n        sess.run(dot_product,\r\n                options = tf.RunOptions(trace_level = tf.RunOptions.FULL_TRACE),\r\n                run_metadata = run_meta)\r\n        profiler.add_step(0,run_meta)\r\n        opts = (tf.profiler.ProfileOptionBuilder(\r\n                 tf.profiler.ProfileOptionBuilder.time_and_memory())\r\n                 .with_step(0)\r\n                 .with_timeline_output('timeline_v2_{}'.format('random' if random else 'const')).build())\r\n        profiler.profile_graph(options=opts)\r\n\r\nif __name__ == '__main__':\r\n    v1(False)\r\n    v2(False)\r\n    v1(True)\r\n    v2(True)\r\n```", "OK, thanks - I don't see anything unusual in those results now other than v1 random being slower than v2 random.  I'd expect that running the RNG each step was substantially slower than the tf.range, but can't explain the -ve loop overhead unless it's just run-to-run variation. (this can happen based on other things running on the machine, heap allocation order, etc. etc.)\r\n\r\nIf the difference is repeatable, could you post the timelines and profile data?", "I'm able to repeat each time. \r\nV2 random is always a bit faster. \r\nHere are the timelines for the four situations.\r\n\r\n[timelines.tar.gz](https://github.com/tensorflow/tensorflow/files/1729333/timelines.tar.gz)\r\n\r\n", "I think this should be closed, unless you still want to follow up that second case with random tensors in while loop being faster. Run MVCE to see if you get the same.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "I'm going to close this, since the explanation above about the constant folder explains the initial behavior that I was seeing. Feel free to re-open if needed."]}, {"number": 16941, "title": "Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed /usr/bin/ld.gold: fatal error: bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/quantize_and_dequantize_op/tensorflow/core/kernels/quantize_and_dequantize_op.pic.o: read: Input/output error", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n1.5.0\r\n- **Python version**:\r\n2.7.12 \r\n- **Bazel version (if compiling from source)**:\r\n0.10.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\n5.4.0\r\n- **Exact command to reproduce**:\r\nbazel build --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --copt=-march=native //tensorflow/tools/pip_package:build_pip_package\r\n### Describe the problem\r\nI am trying to install Tensorflow on my laptop which has i5 and 8GB RAM.\r\n\r\nWhen I am building the Tensorflow from source \r\nI use\r\n\r\n    bazel build --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --copt=-march=native //tensorflow/tools/pip_package:build_pip_package\r\n\r\nEverything seems to go well but at the end I get this error\r\n\r\n    `ERROR: /home/siladittya/tensorflow/tensorflow/python/BUILD:3166:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1)\r\n\r\n    /usr/bin/ld.gold: fatal error: bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/quantize_and_dequantize_op/tensorflow/core/kernels/quantize_and_dequantize_op.pic.o: read: Input/output error\r\n\r\n    collect2: error: ld returned 1 exit status\r\n\r\n    Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n`\r\n\r\nI looked up a few posts in some forums, but they are saying that it can be solved using `--local_resources` but when I used that argument,\r\n\r\n    `bazel build --local_resources 8000,2.0,2.0 --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --config=opt //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nI am getting the same error again.\r\n \r\nBut it doesn't seem to be a error which can be solved using `--local_resources` because it doesn't work.\r\n\r\nWhat else can be the solution to this problem?\r\n", "comments": ["Well, it seems that going into recovery mode and doing `fsck` on the system partition `/dev/sda1` did the job for me.\r\nIt solved the problem with bazel."]}, {"number": 16940, "title": "TypeError: ('Keyword argument not understood:', 'adjustment') when passing slim.batch_norm as normalizer_fn to slim.conv2d", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version**:\r\n1.4.0 CPU\r\n- **Python version**: \r\n3.6\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n\r\n\r\n### Describe the problem\r\nI encountered a problem using batch norm in `slim.conv2d`. Whenever I pass `slim.batch_norm` as the `normalizer_fn` for `slim.conv2d`, I encounter this error:\r\n\r\n```\r\n  File \"...\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\framework\\python\\ops\\arg_scope.py\", line 181, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"...\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py\", line 1059, in convolution\r\n    outputs = normalizer_fn(outputs, **normalizer_params)\r\n  File \"...\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\framework\\python\\ops\\arg_scope.py\", line 181, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"...\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py\", line 650, in batch_norm\r\n    fused=fused)\r\n  File \"...\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\normalization.py\", line 118, in __init__\r\n    name=name, trainable=trainable, **kwargs)\r\n  File \"...\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 98, in __init__\r\n    raise TypeError('Keyword argument not understood:', kwarg)\r\nTypeError: ('Keyword argument not understood:', 'adjustment')\r\n```", "comments": ["Actually, even just using `slim.batch_norm` throws the same error.", "Closing this issue now since I already solved it. I forgot that I modified the layers.py that I'm using to test something out and caused that error."]}, {"number": 16939, "title": "tensorflow 1.3,1.4,1.5,1.6 DLL load failed with CUDA 9.1, CUDnn-7.05, Windows 10", "body": "import tensorflow strack trace\r\n\r\n```\r\n> py lib\\_learn\\tensorflow\\versions\\versiontest.py\r\nTraceback (most recent call last):\r\n  File \"lib/tensorflow_gpu_130\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\g\\TEST\\lib\\python\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 914, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"lib/tensorflow_gpu_130\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"lib/tensorflow_gpu_130\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"lib/tensorflow_gpu_130\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\g\\TEST\\lib\\python\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"lib\\_learn\\tensorflow\\versions\\versiontest.py\", line 4, in <module>\r\n    import tensorflow as tf\r\n  File \"lib/tensorflow_gpu_130\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"lib/tensorflow_gpu_130\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"lib/tensorflow_gpu_130\\tensorflow\\python\\pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"lib/tensorflow_gpu_130\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\g\\TEST\\lib\\python\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 914, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"lib/tensorflow_gpu_130\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"lib/tensorflow_gpu_130\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"lib/tensorflow_gpu_130\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\g\\TEST\\lib\\python\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "OS Platform and Distribution: Windows 10\r\nTensorFlow installed from: .whl files from : https://pypi.python.org/\r\nTensorFlow version: 1.4,1.5,1.6\r\nBazel version: UNKNOWN\r\nCUDA/cuDNN version: 9.1, 7.05\r\nGPU model and memory: NVIDIA, QUADRO, K5100M\r\nExact command to reproduce: import tensorflow\r\n", "With version <1.3.0 I was able to load using the following\r\n\r\nsys.path.insert(0, r'lib\\tensorflow_gpu_130')\r\nsys.path.append('lib')\r\nos.environ['PATH']=r'lib\\tensorflow_gpu_130;'+os.environ['PATH']\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\nVersion 1.3.0 and later this scheme no longer works, the approach to find the mname value in pywrap_tensorflow_internal.py appears to be the same, Reviewing now.", "v1.3 and 1.4 prebuilt binaries work with buda 8, and 1.5 and 1.6 binaries work with cuda 9.0.\r\nTo make any of these releases work with cuda 9.1, you need to build from sources.\r\nTherefore, this is working as expected.", "cmake appears to have problems detecting and supporting CUDA 9.1\n\nCMake Error at C:/Program\nFiles/CMake/share/cmake-3.10/Modules/FindPackageHandleStandardArgs.cmake:137\n(message):\n  Could NOT find CUDA: Found unsuitable version \"9.1\", but required is exact\n  version \"9.0\" (found C:/Program Files/NVIDIA GPU Computing\n  Toolkit/CUDA/v9.1)\nCall Stack (most recent call first):\n  C:/Program\nFiles/CMake/share/cmake-3.10/Modules/FindPackageHandleStandardArgs.cmake:376\n(_FPHSA_FAILURE_MESSAGE)\n  C:/Program Files/CMake/share/cmake-3.10/Modules/FindCUDA.cmake:1080\n(find_package_handle_standard_args)\n  CMakeLists.txt:310 (find_package)\n\n\nOn Mon, Feb 12, 2018 at 5:41 PM, Gunhan Gulsoy <notifications@github.com>\nwrote:\n\n> v1.3 and 1.4 prebuilt binaries work with buda 8, and 1.5 and 1.6 binaries\n> work with cuda 9.0.\n> To make any of these releases work with cuda 9.1, you need to build from\n> sources.\n> Therefore, this is working as expected.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/16939#issuecomment-365087539>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABXVTWRNxxuZ4HcukuDgEw6fuGzE_ov4ks5tUL4lgaJpZM4SBfVz>\n> .\n>\n", "You need to set this to 9.1:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/CMakeLists.txt#L40\r\n\r\nYou can do it through the command line by `-Dtensorflow_CUDA_VERSION=9.1`", "Thank you, that made it past that error, I am testing or trying to use\r\nMicrosoft Visual Studio 2017 Community.\r\n\r\n-- Found CUDA: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.1\r\n(found suitable exact version \"9.1\")\r\nCMake Error at CMakeLists.txt:322 (message):\r\n  Selected compiler (or version) is not supported for CUDA\r\n\r\n\r\nClCompile:\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.12.25827\\bin\\HostX86\\x64\\CL.exe /c /nologo /W0 /WX- /diagnostics:classic /Od /D _MBCS /Gm- /EHsc /RTC1 /MDd /GS /fp:precise /Zc:wchar_t /Zc:forScope /Zc:inline /Fo\"Debug\\\\\" /Fd\"Debug\\vc141.pdb\" /Gd /TC /errorReport:queue CMakeCCompilerId.c\r\n  CMakeCCompilerId.c\r\n\r\nFYI\r\n__VSCMD_PREINIT_LIBPATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.12.25827\\ATLMFC\\lib\\x64;.........\r\n\r\n\r\nOn Tue, Feb 13, 2018 at 4:44 PM, Gunhan Gulsoy <notifications@github.com>\r\nwrote:\r\n\r\n> You need to set this to 9.1:\r\n> https://github.com/tensorflow/tensorflow/blob/master/\r\n> tensorflow/contrib/cmake/CMakeLists.txt#L40\r\n>\r\n> You can do it through the command line by -Dtensorflow_CUDA_VERSION=9.1\r\n>\r\n> \u2014\r\n> You are receiving this because you authored the thread.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/16939#issuecomment-365413959>,\r\n> or mute the thread\r\n> <https://github.com/notifications/unsubscribe-auth/ABXVTXhUo7m_zzcIWpZgHJXbXo3do_iDks5tUgJMgaJpZM4SBfVz>\r\n> .\r\n>\r\n", "https://devtalk.nvidia.com/default/topic/1027299/cuda-setup-and-installation/cuda-9-failed-to-support-the-latest-visual-studio-2017-version-15-5/\r\n\r\nLooks like cuda still does not work with visual studio 2017?", "http://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html\n\nOn Wed, Feb 14, 2018 at 2:46 AM, Gunhan Gulsoy <notifications@github.com>\nwrote:\n\n> https://devtalk.nvidia.com/default/topic/1027299/cuda-\n> setup-and-installation/cuda-9-failed-to-support-the-latest-\n> visual-studio-2017-version-15-5/\n>\n> Looks like cuda still does not work with visual studio 2017?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/16939#issuecomment-365521655>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABXVTSQ79t6DrQJhhpdWXLPH_qN7cQ3gks5tUo9WgaJpZM4SBfVz>\n> .\n>\n"]}, {"number": 16938, "title": "Fix typos in profiler.h", "body": "Fixed 3 typos in profiler.h", "comments": ["3 changes in a single file", "Why this pull request is not merged yet?"]}, {"number": 16937, "title": "there is no idct implementation in tensorflow", "body": "Hi\r\nI need idct implementation in tensorflow, anyone aware of any implementation on this?\r\nthanks\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Have I written custom code: Yes\r\nOS Platform and Distribution: Windows 10\r\nTensorFlow installed from: pypi\r\nTensorFlow version: 1.6.0\r\nCUDA/cuDNN version: latest\r\nGPU model and memory: NVIDIA GTX 780, 6G\r\n\r\nThere is no tf.spectral.idct but only tf.spectral.dct in tensorflow.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 16936, "title": "Complement cmake script to compile tensorflow with mkl and mkldnn on Windows", "body": "- Add a make.bat to build project conveniently on Windows.\r\n- Modify cmake script to enable SIMD and MKL support on Windows.\r\n\r\n**Note**\r\nOne has to manually copy `mkl_rt.dll`, `mkl_core.dll`, `mkl_avx2.dll`, `mkl_sequential.dll` and `mkldnn.dll` (if compiled with mkldnn) to the executable directory", "comments": ["I signed it!", "- Revert to keep the meaning of `tensprflow_WIN_CPU_SIMD_OPTIONS` unchanged. Instead use `tensorflow_ENABLE_MKL_SUPPORT` to include MKL.\r\n- Fix a mistake of CUDA path in make.bat\r\n", "@LoSealL Thanks for the contribution! Just confirming that your changes are tested on Linux. One needs to build with --config=mkl to build with MKL. Check this for details: https://www.tensorflow.org/performance/performance_guide#tensorflow_with_intel_mkl_dnn", "Hi @nharada1 , sorry for late reply. I confirm that build with `--config=mkl` in bazel succeeds on Linux (Ubuntu 14.04)", "Nagging Assignee @mrry: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@gunan Any updates should I make?", "@gunan Make all the comments here.\r\n>Just for consistency, would you mind adding the default here too:\r\noption(tensorflow_WIN_CPU_SIMD_OPTIONS \"Enables CPU SIMD instructions\" OFF)\r\n\r\nSure\r\n\r\n>This is a highly specific script that will only work on your machine, with all the tools installed in the locations you installed them.\r\nPlease remove and expand README.md in this folder.\r\n\r\nIt meant to be a general script on windows, which auto searches requirements on the machine, helps developers to quickly build tensorflow with cmake specifically on Windows.  I will delete the confused path string and expand the `README.md` about this script.\r\n\r\n>Is this meant to be a comment?\r\nor is it mistakenly commented out?\r\n\r\nI will delete this comment.\r\n\r\n>should we wrap this with \"ifndef _Win32\" instead?\r\n\r\nSure.\r\n\r\n>Any updates here gzmkl ?\r\n\r\nI can wrap with a \"ifdef _Win32\"\r\n\r\n>I feel like all these typedefs need to be merged into one.\r\n\r\nDefinitely.\r\n\r\nI will update ASAP.", "@gunan Hi, any updates?", "Waiting for final confirmations from @mrry and @claynerobison ", "Nagging Assignee @mrry: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This PR passed our internal QA at Intel. \ud83d\udc4d Merge it!"]}, {"number": 16935, "title": "TensorFlow1.5.0 absl.flags._exceptions.UnparsedFlagAccessError in Jupyter Notebook", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.5.0\r\n- **Python version**:  3.6\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: 9.0/7.0.5\r\n- **GPU model and memory**:GeForce 950M(4Gb)\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI run the function `cifar10.maybe_download_and_extract()` of `tensorflow/models/tutorials/image/cifar10.py` file in python command and jupyter notebook .The code runs successfully in python command,  but it causes `absl.flags._exceptions.UnparsedFlagAccessError`from `cifar10.FLAGS.data_dir` in jupyter notebook.\r\n\r\nThe jupyter notebook has already **configured the tensorflow kernel**, other tensorflow examples could run smoothly on jupyter notebook.\r\n\r\nSignificant code in `cifar10.py` file is:\r\n\r\n    import tensorflow as tf\r\n    import cifar10_input\r\n    FLAGS = tf.app.flags.FLAGS\r\n\r\n    # Basic model parameters.\r\n    tf.app.flags.DEFINE_integer('batch_size', 128,\r\n                            \"\"\"Number of images to process in a batch.\"\"\")\r\n    tf.app.flags.DEFINE_string('data_dir', '/tmp/cifar10_data',\r\n                           \"\"\"Path to the CIFAR-10 data directory.\"\"\")\r\n    tf.app.flags.DEFINE_boolean('use_fp16', False,\r\n                            \"\"\"Train the model using fp16.\"\"\")\r\n    # .....\r\n    def maybe_download_and_extract():\r\n        \"\"\"Download and extract the tarball from Alex's website.\"\"\"\r\n        dest_directory = FLAGS.data_dir\r\n        if not os.path.exists(dest_directory):\r\n            os.makedirs(dest_directory)\r\n    # ......\r\n\r\nThe exception happened from the code statement `dest_directory = FLAGS.data_dir`.\r\n\r\nThe successful output in python command is as follows:\r\n\r\n    >>> cifar10.maybe_download_and_extract()\r\n    >> Downloading cifar-10-binary.tar.gz 100.0%\r\n    Successfully downloaded cifar-10-binary.tar.gz 170052171 bytes.\r\n\r\n\r\nThe detailed error in jupyter notebook is as follows:\r\n\r\n    UnrecognizedFlagError                     Traceback (most recent call last)\r\n    <ipython-input-6-ddaee26bfd14> in <module>()\r\n    ----> 1 cifar10.maybe_download_and_extract()\r\n    ~/models/tutorials/image/cifar10/cifar10.py in maybe_download_and_extract()\r\n    381 def maybe_download_and_extract():\r\n    382   \"\"\"Download and extract the tarball from Alex's website.\"\"\"\r\n    --> 383   dest_directory = FLAGS.data_dir\r\n    384   if not os.path.exists(dest_directory):\r\n    385     os.makedirs(dest_directory)\r\n    ~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/platform/flags.py in \r\n    __getattr__(self, name)\r\n    82     # a flag.\r\n    83     if not wrapped.is_parsed():\r\n    ---> 84       wrapped(_sys.argv)\r\n    85     return wrapped.__getattr__(name)\r\n    86 \r\n    ~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/absl/flags/_flagvalues.py in __call__(self, argv, known_only)\r\n    628       suggestions = _helpers.get_flag_suggestions(name, list(self))\r\n    629       raise _exceptions.UnrecognizedFlagError(\r\n    --> 630           name, value, suggestions=suggestions)\r\n    631 \r\n    632     self.mark_as_parsed()\r\n    UnrecognizedFlagError: Unknown command line flag 'f'\r\n\r\n### Source code / logs\r\n\r\n    #git clone https://github.com/tensorflow/models.git\r\n    #cd models/tutorials/image/cifar10\r\n    import cifar10\r\n    cifar10.maybe_download_and_extract()", "comments": ["I do not have much experience with Jupyter myself.\r\nMaybe @caisq or @random-forests may know, or may know who to redirect this to?\r\n\r\n", "Closing this issue due to staleness. Please test with latest version of TF and reopen this issue if necessary. Thanks!"]}, {"number": 16934, "title": "Can not create confusion_matrix", "body": "This is my code for the network. I am using tensorflow 1.4 and I am using my nvidia for training. The code is working. I want to create a confusion matrix and I do it like this \r\n\r\n\r\n\r\n    with tf.device('/device:GPU:0'):    \r\n        images = tf.placeholder(tf.float32, [None, IMAGE_WIDTH, IMAGE_LENGTH, 3])\r\n        labels = tf.placeholder(tf.int64, [None])\r\n\r\n        flat = tf.layers.flatten(labels)\r\n        logits = tf.layers.dense(flat, len(set(train_labels_arr)), tf.nn.relu)\r\n\r\n        predicted_labels = tf.argmax(tf.nn.softmax(logits), 1)\r\n\r\n        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits,\r\n                                                                   labels = labels)\r\n        loss = tf.reduce_mean(cross_entropy)\r\n\r\n        optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\r\n        correct_prediction = tf.equal(predicted_labels, labels)\r\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n\r\n        init = tf.global_variables_initializer()\r\n\r\nI want to create a confusion matrix and I do it like this:\r\n\r\n     confusion_matrix = tf.confusion_matrix(labels=labels, predictions=predicted_labels,\r\n                                          num_classes=len(set(train_labels_arr)),dtype=tf.int64)\r\nWhen I call: \r\n\r\n    session = tf.Session()\r\n    session.run(init) \r\n\r\nI got very weird error, but if remove the line with the confusion_matrix everything is ok: InvalidArgumentError: Cannot assign a device for operation 'confusion_matrix_4/assert_non_negative/assert_less_equal/Assert/AssertGuard/Assert': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n\t [[Node: confusion_matrix_4/assert_non_negative/assert_less_equal/Assert/AssertGuard/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT64], summarize=3, _device=\"/device:GPU:0\"](confusion_matrix_4/assert_non_negative/assert_less_equal/Assert/AssertGuard/Assert/Switch, confusion_matrix_4/assert_non_negative/assert_less_equal/Assert/AssertGuard/Assert/data_0, confusion_matrix_4/assert_non_negative/assert_less_equal/Assert/AssertGuard/Assert/data_1, confusion_matrix_4/assert_non_negative/assert_less_equal/Assert/AssertGuard/Assert/data_2, confusion_matrix_4/assert_non_negative/assert_less_equal/Assert/AssertGuard/Assert/Switch_1)]]\r\n\r\nCaused by op u'confusion_matrix_4/assert_non_negative/assert_less_equal/Assert/AssertGuard/Assert', defined at:\r\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\r\n    \"__main__\", fname, loader, pkg_name)\r\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\r\n    exec code in run_globals\r\n  File \"/home/lachaka/tensorflow/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 478, in start\r\n    self.io_loop.start()\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\r\n    super(ZMQIOLoop, self).start()\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tornado/ioloop.py\", line 888, in start\r\n    handler_func(fd_obj, events)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\r\n    self._handle_recv()\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-27-64238c75dfbf>\", line 41, in <module>\r\n    num_classes=len(set(train_labels_arr)),dtype=tf.int64)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/confusion_matrix.py\", line 162, in confusion_matrix\r\n    labels, message='`labels` contains negative values')],\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/check_ops.py\", line 237, in assert_non_negative\r\n    return assert_less_equal(zero, x, data=data, summarize=summarize)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/check_ops.py\", line 443, in assert_less_equal\r\n    return control_flow_ops.Assert(condition, data, summarize=summarize)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py\", line 107, in wrapped\r\n    return _add_should_use_warning(fn(*args, **kwargs))\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 134, in Assert\r\n    condition, no_op, true_assert, name=\"AssertGuard\")\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py\", line 316, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1864, in cond\r\n    orig_res_f, res_f = context_f.BuildCondBranch(false_fn)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1725, in BuildCondBranch\r\n    original_result = fn()\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 132, in true_assert\r\n    condition, data, summarize, name=\"Assert\")\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_logging_ops.py\", line 47, in _assert\r\n    name=name)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: Yes, I am trying to classify images\r\nOS Platform and Distribution: Ubuntu 16.04 x64\r\nTensorFlow installed from: pip\r\nTensorFlow version: 1.4\r\nBazel version: N/A\r\nCUDA/cuDNN version: 7.5\r\nGPU model and memory: Nvidia 740M 2GB\r\nRAM: 6GB\r\nExact commnad to reproduce: N/A I am using jupyter", "@lachaka just to do a little fault isolation, if you place the confusion matrix calculation on CPU, does the code work?", "Yeah, if I use CPU the is no error.", "More sanity checking: it kinda looks like you're getting the weird failure because one of the assert operations isn't implemented on the GPU. But the assertion line that's blowing is at :162 of confusion_matrix.py: \r\nhttps://github.com/tensorflow/tensorflow/blob/b2a0f1c45b2283910548ebd88ee5aaf4b6fc6077/tensorflow/python/ops/confusion_matrix.py#L162\r\n\r\nCan you copy the assertion that labels and predictions are non-negative to just before your confusion matrix code, and see if it fires on GPU but not CPU? That would suggest that you're getting different calculated values on CPU and GPU, and maybe help us find the root cause.\r\n", "\r\nInvalidArgumentError: Cannot assign a device for operation 'assert_non_negative_1/assert_less_equal/Assert/AssertGuard/Assert': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n\t [[Node: assert_non_negative_1/assert_less_equal/Assert/AssertGuard/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT64], summarize=3, _device=\"/device:GPU:0\"](assert_non_negative_1/assert_less_equal/Assert/AssertGuard/Assert/Switch, assert_non_negative_1/assert_less_equal/Assert/AssertGuard/Assert/data_0, assert_non_negative_1/assert_less_equal/Assert/AssertGuard/Assert/data_1, assert_non_negative_1/assert_less_equal/Assert/AssertGuard/Assert/data_2, assert_non_negative_1/assert_less_equal/Assert/AssertGuard/Assert/Switch_1)]]\r\n\r\nCaused by op u'assert_non_negative_1/assert_less_equal/Assert/AssertGuard/Assert', defined at:\r\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\r\n    \"__main__\", fname, loader, pkg_name)\r\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\r\n    exec code in run_globals\r\n  File \"/home/lachaka/tensorflow/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 478, in start\r\n    self.io_loop.start()\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\r\n    super(ZMQIOLoop, self).start()\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tornado/ioloop.py\", line 888, in start\r\n    handler_func(fd_obj, events)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\r\n    self._handle_recv()\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-11-794d9350ef1e>\", line 39, in <module>\r\n    labels, message='`labels` contains negative values')],\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/check_ops.py\", line 237, in assert_non_negative\r\n    return assert_less_equal(zero, x, data=data, summarize=summarize)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/check_ops.py\", line 443, in assert_less_equal\r\n    return control_flow_ops.Assert(condition, data, summarize=summarize)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py\", line 107, in wrapped\r\n    return _add_should_use_warning(fn(*args, **kwargs))\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 134, in Assert\r\n    condition, no_op, true_assert, name=\"AssertGuard\")\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py\", line 316, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1864, in cond\r\n    orig_res_f, res_f = context_f.BuildCondBranch(false_fn)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1725, in BuildCondBranch\r\n    original_result = fn()\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 132, in true_assert\r\n    condition, data, summarize, name=\"Assert\")\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_logging_ops.py\", line 47, in _assert\r\n    name=name)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"/home/lachaka/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'assert_non_negative_1/assert_less_equal/Assert/AssertGuard/Assert': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n\t [[Node: assert_non_negative_1/assert_less_equal/Assert/AssertGuard/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT64], summarize=3, _device=\"/device:GPU:0\"](assert_non_negative_1/assert_less_equal/Assert/AssertGuard/Assert/Switch, assert_non_negative_1/assert_less_equal/Assert/AssertGuard/Assert/data_0, assert_non_negative_1/assert_less_equal/Assert/AssertGuard/Assert/data_1, assert_non_negative_1/assert_less_equal/Assert/AssertGuard/Assert/data_2, assert_non_negative_1/assert_less_equal/Assert/AssertGuard/Assert/Switch_1)]]", "OK, so it looks like the assert itself, running on the GPU, is what's not implemented. \r\n\r\nAnother debugging step: can you run the assert on the CPU (so we know it works), but generate the predictions and labels on the GPU, so we can tell whether the assert is correctly firing based on funky GPU-generated data, or whether the data is good, and it's just the assertion that needs to be implemented on GPU (or mapped back to CPU, which TF should have done).", "Well, I execute the tf.confusion_matrix on CPU and It works. It creates the matrix. The other operations like generating predictions and so on are calculated by the GPU. ", "Sounds like the values generated by GPU are good, but some part of the assertion isn't implemented on GPU. "]}]