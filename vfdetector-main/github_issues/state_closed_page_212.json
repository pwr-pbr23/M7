[{"number": 48292, "title": "Custom Layer Using Packages Other Than Tensorflow", "body": "**System information**\r\n- TensorFlow version (2.4.1):\r\n- Are you willing to contribute it (If you pay yes):\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently you can't use packages other than tensorflow to build a layer \r\n\r\n**Will this change the current api? How?**\r\nI don't know\r\n\r\n**Who will benefit with this feature?**\r\nEveryone who wants to use packages like graph analysis or topological data analysis to build a custom layer for specific works .", "comments": ["@Javadcs \r\nThis is a great idea with the perspective that TensorFlow can incorporate things without any constraints. Unfortunately, this is not the case, actually far from it. Reason is (sort of) simple - Graph mode and associated things. Models and Layers in TensorFlow usually work with Graph mode on, which means that they need to be traced before execution. Now, we cannot trace non-TensorFlow constructs in the graph, it's just the way it works. The primary reason being compatibility and performance. Compatibility- TF's core is made of predefined ops and kernels, which can be extended by the user if need arises, but are in my opinion difficult to manipulate. Performance- by making such a graph and using TF constructs, TF is very fast, even for very complex operations like training a model. Thus, adding a feature allowing custom packages is not currently possible in TensorFlow, unless there are some **MAJOR** changes. \r\nAnd by the way, nobody's getting paid here (atleast most of them). All of them are here for the love of it :) .", "Thanks @AdityaKane2001 for the reply.\r\n\r\n@Javadcs, could you give a more specific example about what exact issue you are facing? A small reproducible example might help us better understand you issue.", "> @Javadcs\r\n> This is a great idea with the perspective that TensorFlow can incorporate things without any constraints. Unfortunately, this is not the case, actually far from it. Reason is (sort of) simple - Graph mode and associated things. Models and Layers in TensorFlow usually work with Graph mode on, which means that they need to be traced before execution. Now, we cannot trace non-TensorFlow constructs in the graph, it's just the way it works. The primary reason being compatibility and performance. Compatibility- TF's core is made of predefined ops and kernels, which can be extended by the user if need arises, but are in my opinion difficult to manipulate. Performance- by making such a graph and using TF constructs, TF is very fast, even for very complex operations like training a model. Thus, adding a feature allowing custom packages is not currently possible in TensorFlow, unless there are some **MAJOR** changes.\r\n> And by the way, nobody's getting paid here (atleast most of them). All of them are here for the love of it :) .\r\n\r\nHi, thanks for answering me I appreciated it, that was a great idea to improve performance in this area which had two winters that both of them were related to it.\r\nCan you guide me to know what is the structure of TensorFlow again I would appreciate it.\r\n\r\nThat was a joke for my friend I didn't intend to write it seriously I hope you and other researchers didn't take any offense\r\nGood luck and good wishes.", "> Thanks @AdityaKane2001 for the reply.\r\n> \r\n> @Javadcs, could you give a more specific example about what exact issue you are facing? A small reproducible example might help us better understand you issue.\r\n\r\nHi thanks for your concern, I wanted to have a custom topological layer in a CNN but as @AdityaKane2001 said for improving performance which is notable it is impossible right now.", "@Javadcs \r\nWhat do you want to know about TensorFlow's structure? Any specific parts?\r\nI'm also new to this, but I'll try my best.\r\n\r\n", "@JKhodadadi Is there any actionable item like raising PR? \r\n\r\nPlease note that Keras development moved to another repository to focus entirely on only keras. Could you please repost this issue on [keras-team/keras repo](https://github.com/keras-team/keras/issues). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 48291, "title": "Enable do_pylint only and --incremental", "body": "This will try to recover `--pylint` only step and `--incremental`\r\n\r\nYou can check with\r\n`docker run --rm -it -v $PWD:/tensorflow -w /tensorflow tensorflow/tensorflow:devel bash -c \"apt-get -y install python3.8 && python3.8 -m pip install pylint==2.7.2 && tensorflow/tools/ci_build/ci_sanity.sh --pylint --incremental\"`\r\n\r\nFixes https://github.com/tensorflow/tensorflow/issues/27903", "comments": ["/cc @lc0"]}, {"number": 48290, "title": "Problems with tf.function", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 20.10\r\n- TensorFlow version: 2.4.1\r\n- Python version : 3.8\r\n\r\n**Describe the problem**\r\n\r\nI would like to run inference on a microcontroller above by using a model characterized by Conv1D layers.  I thought I would use something like this:\r\n\r\n`// Pull in only needed operations (should match NN layers).`\r\n`// Template parameter <n> is number of ops to be added. Available ops:`\r\n`// tensorflow/lite/micro/kernels/micro_ops.h`\r\n\r\n`static tflite::MicroMutableOpResolver <1> micro_op_resolver;`\r\n`tflite_status = micro_op_resolver.Conv1D();`\r\n \r\n `if (tflite_status != kTfLiteOk) {`\r\n   `error_reporter->Report(\"Could not add Conv1D op\");`\r\n`while(1);`\r\n`}`\r\n\r\nHowever, the operation above shouldn't be supported by TensorFlow Lite.\r\nThen if I use this simple code:\r\n\r\n`import tensorflow as tf`\r\n`tf.config.run_functions_eagerly(True)`\r\n\r\n`input_shape = (1, 7, 1)`\r\n`x = tf.random.normal(input_shape)`\r\n\r\n`@tf.function`\r\n`def convol1d():`\r\n   `y=tf.keras.layers.Conv1D(1, 3, input_shape=input_shape[1:], name=\"Conv1D\")(x)`\r\n`return y`\r\n\r\n`data = convol1d()`\r\n`print(\"\\n\\n data is:\", data)`\r\n\r\n`tflite_model_name = 'convol1d' `\r\n`converter = tf.lite.TFLiteConverter.from_concrete_functions([convol1d.get_concrete_function()])`\r\n`converter.allow_custom_ops = True`\r\n`tflite_model = converter.convert()`\r\n`open(tflite_model_name + '.tflite', 'wb').write(tflite_model)`\r\n\r\nIf I run it, it appears the following error:\r\n\r\n`ValueError: tf.function-decorated function tried to create variables on non-first call.`\r\n\r\nInstead, I would expect it:\r\n\r\n`Error:\r\nDidn't find custom operator for name 'Conv1D'`\r\n`Registration failed.`\r\n\r\n\r\nThanks in advance.\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["Try to reinstall tensorflow and see if it works fine with previous version.It seems like it couldn't find the 'Conv1D' file in keras.", "Hi @iteachmachines, thanks for your response, but unfortunately I had problems even by using a previous version.", "Did you try converting your model with Conv1D layer into TF Lite format?\r\nSee [example](https://github.com/tensorflow/tensorflow/issues/39823) where user could successfully convert it into tflite.\r\n", "Thanks @ymodak for your reply. If I don't use @tf.function decorator, I can convert my model into TF Lite format; if I do, it apperas the ValueError error. Maybe should I try to download the TF-nightly version to solve this error?", "@Lucy20211 Conv1D will be converted to a combination of layers including ExpandDims and Conv2D. Unfortunately, the ExpandDims layer is currently unsupported in TF Lite Micro so you have to use a workaround. See [my comment ](https://github.com/tensorflow/tensorflow/issues/43141#issuecomment-813046876)in #43141 for more details.", "Hi @jensmeder, thanks for your reply. Your solution can be very useful; however, I need to use convolutions which are causal (I had not specified this requirement). ", "@Lucy20211 I would suggest to open a new github issue thread for discussing topic other than the original query. This way it helps future users to access the appropriate information easily. Can you please post a new issue if your original question has been answered? Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48290\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48290\">No</a>\n"]}, {"number": 48289, "title": "Autograph produces code with invalid syntax when transforming expressions containing tensor slicing with a colon in tf 2.6.0", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian 11\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.1-54030-g23c3e3c4ad1 2.6.0-dev20210331\r\n- Python version: 3.9.2\r\n- CUDA/cuDNN version: using a CPU\r\n- GPU model and memory: using a CPU\r\n\r\n**Describe the current behavior**\r\nWhen transforming expressions containing tensor slicing, autograph puts the contents of the brackets in parentheses (wraps them in a tuple). This produces invalid syntax if one of the characters in the brackets is a colon.\r\n\r\n**Describe the expected behavior**\r\nAutograph should transform expressions involving tensor slicing with colons without producing code that contains invalid syntax. \r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef test_function(x):\r\n    return x[:,0]\r\n\r\ntest_function(tf.ones((1, 1)))\r\n```\r\n\r\n\r\n**Other info / logs**\r\n\r\nThe above code produces the following warning:\r\n```\r\nWARNING:tensorflow:AutoGraph could not transform <function test_function at 0x7fd9511f7040> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: invalid syntax (tmpna4wrl96.py, line 12)\r\n```\r\nFull output is attached as [output.log](https://github.com/tensorflow/tensorflow/files/6252266/output.log).\r\n\r\nThe contents of the temporary file produced by autograph are:\r\n```\r\n# coding=utf-8\r\ndef outer_factory():\r\n\r\n    def inner_factory(ag__):\r\n\r\n        def tf__test_function(x):\r\n            with ag__.FunctionScope('test_function', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\r\n                do_return = False\r\n                retval_ = ag__.UndefinedReturnValue()\r\n                try:\r\n                    do_return = True\r\n                    retval_ = ag__.ld(x)[(:, 0)]\r\n                except:\r\n                    do_return = False\r\n                    raise\r\n                return fscope.ret(retval_, do_return)\r\n        return tf__test_function\r\n    return inner_factory\r\n```\r\n\r\nThe syntax error is in the expression ```retval_ = ag__.ld(x)[(:, 0)]```. Removing the parentheses around ```:, 0``` fixes the error.\r\n\r\nThis problem does not appear when using tf 2.2.0. I haven't tested with other versions.", "comments": ["@lshug,\r\nI did not face any warnings while running the code with [TF v2.4](https://colab.research.google.com/gist/amahendrakar/36dc1a339008e4ec6d8b60d4d252d2be/48289.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/a5c77d368133c0c9a1570c9a2f361c47/48289-tf-nightly.ipynb). Please check the linked gist for reference. \r\n\r\nPlease try running the code in a new virtual environment and let us know if you are facing the same issue. Also, take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/47802#issuecomment-799555152) from a similar issue and check it it helps.\r\n\r\nThanks!", "@amahendrakar I think the issue is not being reproduced on your TF-nightly colab because it's using python 3.7. I encountered this issue on python 3.9. [Here's a colab reproducing the issue](https://colab.research.google.com/drive/1h4AoNcJz7uVHgUQrppl7bL1r5l1AXdy8?usp=sharing).  \r\n\r\nAs for suppressing the warning, I'm already avoiding it altogether by selectively not converting functions that contain tensor slicing with a colon by using the @tf.autograph.experimental.do_not_convert. The problem is the behavior, not the warning. Since autograph fails on such functions, they are not compiled JIT and are run as is (as a generated graph from the function's inputs to its outputs), which can cause tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError when using things that are allowed in tf.functions but not in graph, such as using a tensor as a bool in a conditional. I've included an example of this at the end of the linked colab.", "@lshug,\r\nThank you for the gist. \r\n\r\n@ymodak,\r\nI was able to reproduce the issue with TensorFlow v2.5.0-rc1 and TF-nightly on Python 3.9. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/975ceaf88aff83ac5e0b44af1cd1b5c5/48289-tf-nightly-3-9.ipynb#scrollTo=C2V7xk9PiDE5&line=1&uniqifier=1). Thanks!", "This should be fixed by https://github.com/tensorflow/tensorflow/commit/885861a9cada78487cd868f5acdbd7f2938d7a87, and should be visible in the next nightly. We'll try to patch it into the next 2.5 RC.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48289\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48289\">No</a>\n"]}, {"number": 48288, "title": "UnboundLocalError 'strategy' referenced before assignment during SSD mobilenet FPN evaulation ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no custom code. Using stock example in tensorflow\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: 10.1 \r\n- GPU model and memory: RTX 2080, 16GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI used this `python model_main_tf2.py --model_dir=models\\my_mobilenet640v2 --pipeline_config_path=models\\my_mobilenet640v2\\pipeline.config --checkpoint_dir=models\\my_mobilenet640v2 --wait_interval=60 --eval_timeout=60` in stock model_main_tf2.py to evaluate a new trained object detection model using my own custom set of image data while training on top of a pre-trained model.\r\n\r\nIt throw a error 'UnboundLocalerror: local variable 'strategy' referenced before assignment.\r\npastebin of error: https://pastebin.com/rjfY6dhp\r\n**Describe the expected behavior**\r\nstrategy scope should not have have error\r\nWill ignoring this error cause any potential problems when exporting to tflite using export_tflite_ssd_graph.py ?\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": [">no custom code. Using stock example in tensorflow\r\n\r\n@Ethylbenzol,\r\nCould you please provide a minimal code snippet to reproduce the issue reported here or share the link of the example.\r\n\r\nAlso, please update TensorFlow to the latest stable version v2.4.1 and check if you are facing the same error. Thanks!", "@amahendrakar  Hey man thanks for the follow up.\r\n\r\n> Could you please provide a minimal code snippet to reproduce the issue reported here or share the link of the example.\r\n\r\nI used the exact code here at https://github.com/tensorflow/models/blob/master/research/object_detection/model_main_tf2.py \r\n\r\nI did however modified it a bit using tensorflow GPU ram control default code, and reduce the wait time from 300 to 120. You can read my code at https://pastebin.com/c3X3dvGY \r\nI am sure that will not affect the evaluation, and all of it was grabbed  from official tensorflow website help.\r\n\r\n>  please update TensorFlow to the latest stable version v2.4.1 and check if you are facing the same error. \r\n\r\nI will make an upgrade to to TF 2.4.1 and make a follow up on your recommendation to see if anything fixes it.\r\n\r\n", "I'm actually using tensorflow object detection and that officially says it is still in TF2.2/ 2,3? Will there be issues when upgrading the tensorflow package to 2.4.1? ", "Updated to 2.4.1. Error is resolved", "@Ethylbenzol,\r\nThank you for the update. Closing the issue as it is resolved. Please feel free to re-open if necessary. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48288\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48288\">No</a>\n"]}, {"number": 48287, "title": "Embedding Projector lacks scrolling on Nearest points in the original space.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: No\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Chrome Browser\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**: N/A\r\n-   **TensorFlow installed from (source or binary)**: N/A\r\n-   **TensorFlow version (use command below)**: N/A\r\n-   **Python version**: N/A\r\n-   **Bazel version (if compiling from source)**: N/A\r\n-   **GCC/Compiler version (if compiling from source)**: N/A\r\n-   **CUDA/cuDNN version**: N/A\r\n-   **GPU model and memory**: N/A\r\n-   **Exact command to reproduce**: Go to projector.tensorflow.org and select a word. Check the Nearest points in the original space.\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 48286, "title": "[INTEL MKL] Enable back MKL auto_mixed_precision", "body": "This PR fixes an issue in the auto_mixed_precision_test and enables back MKL auto_mixed_precision in the single binary build.", "comments": ["Thank you both for reviewing/approving the PR. This time the `MacOS CPU Python3` error seems to be unrelated to the changes. Please let me know if I need to take a look."]}, {"number": 48284, "title": "Update Estimator Version to 2.5.0-rc0", "body": "", "comments": []}, {"number": 48283, "title": "2.5-rc1 cherry-pick request: Disable GPU SparseToDense", "body": "The GPU version of SparseToDense is introduced in 2.5, but sometimes crashes the process when run, which will crash previously-working models. This cherry-pick disables the GPU version of SparseToDense to fix the issue.\r\n\r\n/CC @sanjoy", "comments": []}, {"number": 48282, "title": "[RNN] input->type != output->type (UINT8 != FLOAT32)Node number 18 (TANH) failed to prepare", "body": "### 1. System information\r\n\r\n- Windows 10\r\n- pip install tf-nightly\r\n- 2.6.0.dev20210330\r\n\r\n### 2. Code\r\nThe following code converts and quantizes a stateless lstm layer with additional inputs and outputs to handle the state by the user program. This was suggested at https://www.tensorflow.org/lite/convert/rnn\r\n> It is still possible to model a stateful Keras LSTM layer using the underlying stateless Keras LSTM layer and managing the state explicitly in the user program. Such a TensorFlow program can still be converted to TensorFlow Lite using the feature being described here.\r\n\r\n```\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nrng = np.random.default_rng()\r\n\r\n\r\ndef representative_dataset():\r\n    yield [rng.random((1, 1024), dtype=np.float32),\r\n           rng.random((1, 1, 1024), dtype=np.float32),\r\n           rng.random((1, 1024), dtype=np.float32)]\r\n\r\n\r\ndef create_keras_model(keras_file_name, stateful=True):\r\n    input = keras.Input(shape=(1, 1024,), name=\"input\")\r\n    hidden_state = keras.Input(shape=(1024,), name=\"lstm_hidden_state\")\r\n    cell_state = keras.Input(shape=(1024,), name=\"lstm_cell_state\")\r\n    output, new_hidden_state, new_cell_state = layers.LSTM(units=1024, return_sequences=True, unroll=True,\r\n                                                           return_state=True,\r\n                                                           name=\"lstm\")(input, initial_state=[hidden_state, cell_state])\r\n    model = keras.Model([input, hidden_state, cell_state],\r\n                        [output, new_hidden_state, new_cell_state])\r\n    model.summary()\r\n    model.compile()\r\n    model.save(keras_file_name)\r\n\r\n\r\ndef convert_to_tflite(keras_file_name, tflite_filename):\r\n    converter = tf.lite.TFLiteConverter.from_saved_model(keras_file_name)\r\n    #converter.experimental_new_converter = True\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.representative_dataset = representative_dataset\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n    converter.target_spec.supported_types = [tf.int8]\r\n    converter.inference_input_type = tf.uint8\r\n    converter.inference_output_type = tf.uint8\r\n    tflite_model = converter.convert()\r\n    with open(tflite_filename, 'wb') as f:\r\n        f.write(tflite_model)\r\n\r\n\r\ndef main():\r\n    keras_file_name = \"example_model\"\r\n    tflite_filename = \"example_model.tflite\"\r\n    create_keras_model(keras_file_name)\r\n    convert_to_tflite(keras_file_name, tflite_filename)\r\n\r\n    # run model with random input\r\n    interpreter = tf.lite.Interpreter(tflite_filename)\r\n    interpreter.allocate_tensors()\r\n    for input_detail in interpreter.get_input_details():\r\n        scale, zero_point = input_detail['quantization']\r\n        input_tensor = rng.random(input_detail[\"shape\"], dtype=np.float32)\r\n        input_tensor = input_detail[\"dtype\"](input_tensor / scale + zero_point)\r\n        interpreter.set_tensor(input_detail[\"index\"], input_tensor)\r\n\r\n    interpreter.invoke()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\n![grafik](https://user-images.githubusercontent.com/16699443/113432571-e7be5c80-93dd-11eb-8bff-629fcaf1b275.png)\r\n\r\n\r\n### 3. Failure after conversion\r\n\r\nRunning the model after conversion fails with:\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:/Users/user/PycharmProjects/Project/LSTM_tflite_playground.py\", line 62, in <module>\r\n    main()\r\n  File \"C:/Users/user/PycharmProjects/Project/LSTM_tflite_playground.py\", line 51, in main\r\n    interpreter.allocate_tensors()\r\n  File \"C:\\Users\\user\\PycharmProjects\\Project\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py\", line 408, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\nRuntimeError: tensorflow/lite/kernels/activations.cc:393 input->type != output->type (UINT8 != FLOAT32)Node number 18 (TANH) failed to prepare.\r\n```\r\nA normal stateless model without the additional in and outputs runs fine. Is there any workaround? \r\n", "comments": ["I confirmed that putting `converter.experimental_new_quantizer = False` will resolve the problem.", "Thanks! The only problem is that without the new quantizer there isn't support for ops like \"ELU\". So using the old quantizer is only a workaround for the toy example above. Any other ideas for a workaround to archive a quantized stateful lstm with tflite?\r\n\r\nExample from above with an additional elu activation layer:\r\n```python\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.keras import activations\r\n\r\nrng = np.random.default_rng()\r\n\r\n\r\ndef representative_dataset():\r\n    yield [rng.random((1, 1024), dtype=np.float32),\r\n           rng.random((1, 1, 1024), dtype=np.float32),\r\n           rng.random((1, 1024), dtype=np.float32)]\r\n\r\n\r\ndef create_keras_model(keras_file_name):\r\n    input = keras.Input(shape=(1, 1024,), name=\"input\")\r\n    hidden_state = keras.Input(shape=(1024,), name=\"lstm_hidden_state\")\r\n    cell_state = keras.Input(shape=(1024,), name=\"lstm_cell_state\")\r\n    lstm_output, new_hidden_state, new_cell_state = layers.LSTM(units=1024, return_sequences=True, unroll=True,\r\n                                                           return_state=True,\r\n                                                           name=\"lstm\")(input, initial_state=[hidden_state, cell_state])\r\n    output = layers.Activation(activations.elu)(lstm_output)\r\n    model = keras.Model([input, hidden_state, cell_state],\r\n                        [output, new_hidden_state, new_cell_state])\r\n    model.summary()\r\n    model.compile()\r\n    model.save(keras_file_name)\r\n    keras.utils.plot_model(model, to_file='example.png', show_shapes=True)\r\n\r\n\r\ndef convert_to_tflite(keras_file_name, tflite_filename):\r\n    converter = tf.lite.TFLiteConverter.from_saved_model(keras_file_name)\r\n    converter.experimental_new_quantizer = False  # https://github.com/tensorflow/tensorflow/issues/48282\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.representative_dataset = representative_dataset\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n    converter.target_spec.supported_types = [tf.int8]\r\n    converter.inference_input_type = tf.uint8\r\n    converter.inference_output_type = tf.uint8\r\n    tflite_model = converter.convert()\r\n    with open(tflite_filename, 'wb') as f:\r\n        f.write(tflite_model)\r\n\r\n\r\ndef main():\r\n    keras_file_name = \"example_model\"\r\n    tflite_filename = \"example_model.tflite\"\r\n    create_keras_model(keras_file_name)\r\n    convert_to_tflite(keras_file_name, tflite_filename)\r\n\r\n    # run model with random input\r\n    interpreter = tf.lite.Interpreter(tflite_filename)\r\n    interpreter.allocate_tensors()\r\n    for input_detail in interpreter.get_input_details():\r\n        scale, zero_point = input_detail['quantization']\r\n        input_tensor = rng.random(input_detail[\"shape\"], dtype=np.float32)\r\n        input_tensor = input_detail[\"dtype\"](input_tensor / scale + zero_point)\r\n        interpreter.set_tensor(input_detail[\"index\"], input_tensor)\r\n\r\n    interpreter.invoke()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```", "fvollmer@, can you confirmed that the issue is resolved with tf-nightly?", "@liufengdb Seems to work fine. I can't reproduce the problem anymore. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48282\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48282\">No</a>\n", "@fvollmer Great example how to handle the state by the user program. Do you have a hint how to handle the states of multiple LSTM layers? "]}, {"number": 48280, "title": "The new Apple M1 MLcompute Tensorflow2.4 not compatible with Numpy 1.20.1, after attempting installing ~>1.19.2, got error", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac Air M1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Github Apple MLcompute repo\r\n- TensorFlow version: 2.4\r\n- Python version: 3.9\r\n- Installed using virtualenv? pip? conda?: conda, miniforge3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nAfter #48237, I uninstalled 1.20.1 and 1.18.5 and tried to install 1.19.2 or ~>1.19.2. error \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI uninstalled numpy 1.20.1 and 1.18.5 and tried to install numpy~=1.19.2 and got this error:\r\n```\r\nERROR: Failed building wheel for numpy\r\nFailed to build numpy\r\nERROR: Could not build wheels for numpy which use PEP 517 and cannot be installed directly\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@xxl4tomxu98 \r\nCan you please refer to this link [#47782, #47691, #44751] and move this to closed status as this is more apt to pen this issue on the mentioned repo.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48280\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48280\">No</a>\n"]}, {"number": 48279, "title": "tf.while loop in MultiWorkerMirroredStrategy leads to training error: Complete shape not known for Adam/allreduce/CollectiveReduce", "body": "**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.2 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: cuda 11.0/cudnn 8\r\n- GPU model and memory: NVIDIA GTX 1080 8GB\r\n\r\n**Describe the current behavior:**\r\n\r\nIn a cutom tf.keras.Model using a tf.while loop within train_step works fine when using non ditributed training or tf.distribute.MirroredStrategy.\r\nWhen using tf.distribute.MultiWorkerMirroredStrategy following error is shown upon training startup which then leads to NaN loss when training a complex model:\r\n\r\n> 2021-04-02 12:56:07.219137: W tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:455] error: Aborted: Complete shape not known for Adam/allreduce/CollectiveReduce\r\n> 2021-04-02 12:56:07.219172: W tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:1138] error: Aborted: Complete shape not known for Adam/allreduce/CollectiveReduce\r\n> 2021-04-02 12:56:07.219180: E tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:1155] ScopedAllocatorOptimizer: Aborted: Complete shape not known for Adam/allreduce/CollectiveReduce\r\n> 2021-04-02 12:56:07.219187: W tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:928] error: Aborted: Complete shape not known for Adam/allreduce/CollectiveReduce\r\n\r\nI have prepared an example to reproduce the problem in the code below. The issue is present for GPU and CPU. Due to the simplified example, it does not lead to immediate NaN loss as it does in my more complex model. However, I hope it is sufficient to reproduce and identify the issue. When switching to SGD the error message does not appear. Nevertheless, when using tf.distribute.MultiWorkerMirroredStrategy it goes to immediate NaN loss in my more complex model as well.\r\n\r\n**Describe the expected behavior:**\r\n\r\nTraining should work with tf.distribute.MultiWorkerMirroredStrategy as it does for standalone training or training when using tf.distribute.MirroredStrategy.\r\n\r\n**Standalone code to reproduce the issue**\r\nNeeded files are provided as well in this zipfile:\r\n[example.zip](https://github.com/tensorflow/tensorflow/files/6249694/example.zip)\r\nUse code below or unzip the files in a directory and run python test_worker_0.py on one instance and python test_worker_1.py on another instance of the same machine.\r\n\r\nCode for TestModel.py:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nloss_tracker = tf.keras.metrics.Mean(name=\"loss\")\r\n\r\n\r\nclass TestModel(tf.keras.Model):\r\n    def __init__(self, *args, **kwargs):\r\n        super(TestModel, self).__init__(*args, **kwargs)\r\n        self.Dense = tf.keras.layers.Dense(80)\r\n        self.MSE = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\r\n\r\n    def train_step(self, data):\r\n        batch_size = tf.squeeze(tf.slice(tf.shape(data), [0], [1]), -1)\r\n        #tried hardcoding batch_size to e.g. 16 with two workers -> does not resolve the issue\r\n        max_length = 10\r\n        i_start = tf.constant(0, dtype=tf.int32)\r\n        dummy_inputs = tf.zeros([batch_size, 1, 200])\r\n        gen_outputs = tf.zeros([batch_size, 0, 80], tf.float32)\r\n\r\n        def body(i, input, output_full):\r\n            output_single = self.Dense(input)\r\n            output_full = tf.concat([output_full, output_single], 1)\r\n            i_next = i + 1\r\n            return i_next, input, output_full\r\n\r\n        with tf.GradientTape() as tape:\r\n            _, _, gen_data = tf.while_loop(cond=lambda i,\r\n                                                       input,\r\n                                                       output_full: tf.less(i, max_length),\r\n                                           body=body,\r\n                                           loop_vars=(i_start,\r\n                                                      dummy_inputs,\r\n                                                      gen_outputs,),\r\n                                           shape_invariants=(i_start.get_shape(),\r\n                                                             tf.TensorShape([None, None, 200]),\r\n                                                             tf.TensorShape([None, None, 80])))\r\n            loss = self.MSE(data, gen_data)\r\n        grads = tape.gradient(loss, self.trainable_variables)\r\n        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\r\n        loss_tracker.update_state(loss)\r\n\r\n        return {\"loss\": loss_tracker.result()}\r\n\r\n\r\ndef get_dummy_data():\r\n    x = np.random.random((128, 10, 80))\r\n    dataset = tf.data.Dataset.from_tensor_slices(x)\r\n    dataset.options().experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\r\n    dataset = dataset.batch(32)\r\n    return dataset\r\n\r\n\r\ndef get_model():\r\n    model = TestModel()\r\n    optimizer = tf.keras.optimizers.Adam()\r\n    model.compile(optimizer=optimizer)\r\n    return model\r\n\r\n```\r\n\r\nCode for test_worker_0.py:\r\n```\r\nimport os\r\nimport json\r\nimport tensorflow as tf\r\nos.environ.pop('TF_CONFIG', None)\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\r\ntf_config = {\r\n    'cluster': {\r\n        'worker': ['localhost:12345', 'localhost:23456']\r\n    },\r\n    'task': {'type': 'worker', 'index': 0}\r\n}\r\nos.environ['TF_CONFIG'] = json.dumps(tf_config)\r\n\r\ncommunication_options = tf.distribute.experimental.CommunicationOptions(\r\n    implementation=tf.distribute.experimental.CommunicationImplementation.RING)\r\nstrategy = tf.distribute.MultiWorkerMirroredStrategy(communication_options=communication_options)\r\n\r\nimport TestModel\r\n\r\nwith strategy.scope():\r\n    model = TestModel.get_model()\r\n\r\ndata = TestModel.get_dummy_data()\r\nmodel.fit(data, epochs=4)\r\n```\r\n\r\nCode for test_worker_1.py:\r\n```\r\nimport os\r\nimport json\r\nimport tensorflow as tf\r\nos.environ.pop('TF_CONFIG', None)\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\r\ntf_config = {\r\n    'cluster': {\r\n        'worker': ['localhost:12345', 'localhost:23456']\r\n    },\r\n    'task': {'type': 'worker', 'index': 1}\r\n}\r\nos.environ['TF_CONFIG'] = json.dumps(tf_config)\r\n\r\ncommunication_options = tf.distribute.experimental.CommunicationOptions(\r\n    implementation=tf.distribute.experimental.CommunicationImplementation.RING)\r\nstrategy = tf.distribute.MultiWorkerMirroredStrategy(communication_options=communication_options)\r\n\r\nimport TestModel\r\n\r\nwith strategy.scope():\r\n    model = TestModel.get_model()\r\n\r\ndata = TestModel.get_dummy_data()\r\nmodel.fit(data, epochs=4)\r\n```\r\n\r\n**Other info / logs**\r\nFull logs from a run of the example:\r\n\r\n> 2021-04-02 12:56:04.115797: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n> 2021-04-02 12:56:04.797812: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n> 2021-04-02 12:56:04.798297: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n> 2021-04-02 12:56:04.819525: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n> 2021-04-02 12:56:04.819543: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: test\r\n> 2021-04-02 12:56:04.819548: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: test\r\n> 2021-04-02 12:56:04.819598: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 460.39.0\r\n> 2021-04-02 12:56:04.819613: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.39.0\r\n> 2021-04-02 12:56:04.819618: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 460.39.0\r\n> 2021-04-02 12:56:04.819910: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\n> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n> 2021-04-02 12:56:04.821691: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n> 2021-04-02 12:56:04.823450: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n> 2021-04-02 12:56:04.827242: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12345, 1 -> localhost:23456}\r\n> 2021-04-02 12:56:04.828921: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:23456\r\n> 2021-04-02 12:56:04.885062: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n> 2021-04-02 12:56:04.885517: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3700075000 Hz\r\n> Epoch 1/4\r\n> 2021-04-02 12:56:07.219137: W tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:455] error: Aborted: Complete shape not known for Adam/allreduce/CollectiveReduce\r\n> 2021-04-02 12:56:07.219172: W tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:1138] error: Aborted: Complete shape not known for Adam/allreduce/CollectiveReduce\r\n> 2021-04-02 12:56:07.219180: E tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:1155] ScopedAllocatorOptimizer: Aborted: Complete shape not known for Adam/allreduce/CollectiveReduce\r\n> 2021-04-02 12:56:07.219187: W tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:928] error: Aborted: Complete shape not known for Adam/allreduce/CollectiveReduce\r\n> 4/4 [==============================] - 2s 3ms/step - loss: 0.3312\r\n> Epoch 2/4\r\n> 4/4 [==============================] - 0s 3ms/step - loss: 0.3290\r\n> Epoch 3/4\r\n> 4/4 [==============================] - 0s 3ms/step - loss: 0.3270\r\n> Epoch 4/4\r\n> 4/4 [==============================] - 0s 3ms/step - loss: 0.3250\r\n\r\n", "comments": ["> Due to the simplified example, it does not lead to immediate NaN loss as it does in my more complex model. However, I hope it is sufficient to reproduce and identify the issue.\r\n\r\n@antonaldo,\r\nI did not face the NaN loss issue on running the code for 100 epochs. Please check the attached screenshot for reference. \r\n![Screenshot 2021-04-06 12 26 26 AM](https://user-images.githubusercontent.com/57165142/113613608-17d05e80-966f-11eb-8545-0dd039f0f9ae.png)\r\n", "> 2021-04-02 12:56:04.819525: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n\r\nAlso, looks like TensorFlow failed to detect the GPU on your machine. Could you please share the output of the below code with us?\r\n\r\n```\r\nimport tensorflow as tf\r\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\r\n```\r\n\r\nThanks!", "@amahendrakar \r\n\r\nThanks for looking into this. Please let me clarify the example and the issue.\r\n\r\nIdea of the example was to reproduce this error message in a simple way:\r\n\r\n> 2021-04-02 12:56:07.219180: E tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:1155] ScopedAllocatorOptimizer: Aborted: Complete shape not known for Adam/allreduce/CollectiveReduce\r\n\r\nThis example is related to the allreduce error observed. It is a very simplified example, so I am not sure if it will go into NaN-loss.\r\nCan you reproduce the error message related to allreduce with the example?\r\nGPU was turned off on purpose in my example, so that it becomes clear that this error message is not necessarily GPU. However, the allreduce error message shows op on GPU as well. This happens if I use the example one one machine or distributed on two different machines with GPU or CPU.\r\nThis only happens when using tf.distribute.MultiWorkerMirroredStrategy.\r\nWhen using tf.distribute.MirroredStrategy or non ditributed training the error does not show up. So there seems to be an issue with tf.distribute.MultiWorkerMirroredStrategy when used in the context of the example provided.\r\n\r\nI hope this clarifies the problem.\r\n\r\n", "Hi @antonaldo, your model trains fine with `MirroredStrategy` but then when you use `MultiWorkerMirroredStrategy` it still trains, but you see the message `ScopedAllocatorOptimizer: Aborted: Complete shape not known for Adam/allreduce/CollectiveReduce`. Have I understood this correctly?", "@nikitamaia\r\nYes, you are correct, when using `MultiWorkerMirroredStrategy` the model performs the training loop, but the error message is displayed at the beginning. However, when using my full model with `MultiWorkerMirroredStrategy` loss is not decreasing, but ends up in NaN loss after a few steps.\r\nWhen using `MirroredStrategy` training works as expected with my full model (loss decreases).\r\nI assume that the observed error message is the reason for this unexpected behavior when `MultiWorkerMirroredStrategy` is used.", "So that message means that an optimization failed and your model training might run slower. I don't know that it would cause a nan loss though. A few questions:\r\n- Do you still have the nan loss if you use a different optimizer?\r\n- Can you also provide a screenshot of the logs for your full example?\r\n- are you scaling your batch size by the number of replicas? `global_batch_size = per_replica_batch_size * strategy.num_replicas_in_sync`", "Quick update, looks like you should not encounter this ScopedAllocator issue in TF 2.5.\r\nCan you try your code with 2.5 or nightly?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@nikitamaia:\r\nI have tried it with nightly and the error is no longer displayed. Therefore, this problem is resolved an I will close this issue.\r\nI realized that the observed NaN-loss in my full model is most likely related to use of `tf.keras.layers.experimental.SyncBatchNormalization()`. I will look into this and file a new issue if needed.\r\nThanks for your support!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48279\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48279\">No</a>\n"]}, {"number": 48278, "title": "Issue: tf.function not working when dealing with tf.stack", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow version 2.3.0\r\n\r\n\r\n**Describe the current behavior**\r\ntf.function is not working in converting functions containing tf.stack.\r\n\r\n**Describe the expected behavior**\r\nI would expect tf.function to actually manage to convert the function\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\nc = tf.Variable([[1., 5.], [2., 4.]])\r\n\r\n@tf.function\r\ndef toy_fct(x):\r\n    y = tf.stack([x[0,:], x[1,:]], axis=0)\r\n    return y\r\n\r\ntoy_fct(c)\r\n```\r\n\r\n**Other info / logs**\r\nThe warning given is:\r\n\r\n> WARNING:tensorflow:AutoGraph could not transform <function toy_fct at 0x000001DC9A13A670> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Index'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING: AutoGraph could not transform <function toy_fct at 0x000001DC9A13A670> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Index'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\r\narray([[1., 5.],\r\n       [2., 4.]], dtype=float32)>\r\n", "comments": ["The issue seems much broader as the real problem is tf.function fails to deal with tensor slicing. Closing the issue since there are many more out there on the same subject.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48278\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48278\">No</a>\n", "Similar issue was recently [fixed]( https://github.com/tensorflow/tensorflow/issues/48289#issuecomment-820789715) and should be available with upcoming TF 2.5.0-rc2 release."]}, {"number": 48277, "title": "TFLite Android 'Didn't find op for builtin opcode 'RESIZE_NEAREST_NEIGHBOR' version '3''", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Huawei Nova 3i\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.3\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI am trying to convert this YoloV3 model, written for tensorflow v2.0 to a tflite model. https://github.com/YunYang1994/tensorflow-yolov3\r\nThe model is first compiled and saved. Then, it is converted.\r\nHere is my code for converting the model:\r\n```\r\nimport tensorflow as tf\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(\"model\")\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\ntflite_model = converter.convert()\r\nopen(\"converted_model/model.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\nThe tflite model is imported into Android Studio using New > Other > Tflite Model\r\nHowever, when running the model an error occurred:\r\n```\r\nRegistration failed.\r\n    \r\n        at android.app.ActivityThread.deliverResults(ActivityThread.java:5078)\r\n        at android.app.ActivityThread.handleSendResult(ActivityThread.java:5120)\r\n        at android.app.servertransaction.ActivityResultItem.execute(ActivityResultItem.java:49)\r\n        at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:108)\r\n        at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:68)\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2199)\r\n        at android.os.Handler.dispatchMessage(Handler.java:112)\r\n        at android.os.Looper.loop(Looper.java:216)\r\n        at android.app.ActivityThread.main(ActivityThread.java:7625)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:524)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:987)\r\n     Caused by: java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find op for builtin opcode 'RESIZE_NEAREST_NEIGHBOR' version '3'\r\n```\r\nI am using\r\n```\r\n    implementation 'org.tensorflow:tensorflow-lite-support:0.1.0'\r\n    implementation 'org.tensorflow:tensorflow-lite-metadata:0.1.0'\r\n    implementation 'org.tensorflow:tensorflow-lite-gpu:2.4.0'\r\n```\r\n\r\nThanks.\r\n", "comments": ["Could you use the tf 2.4.1 or tf-nightly version?\r\n\r\n'org.tensorflow:tensorflow-lite-gpu:2.4.1'\r\n\r\nor\r\n\r\norg.tensorflow:tensorflow-lite-gpu:0.0.0-nightly", "@abattery \r\nMay I know what is the repository for 'org.tensorflow:tensorflow-lite-gpu:2.4.1'?\r\nThanks", "I figure out that you need to add the following dependency as well. \r\n\r\norg.tensorflow:tensorflow-lite:2.4.0\r\n\r\nIt needs to add the tensorflow-lite-gpu package alongside the existing tensorflow-lite package in the existing dependencies block.\r\n\r\nSee also the dependency part in https://www.tensorflow.org/lite/performance/gpu#android_with_android_studio", "While trying with org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly, it seems to be working.\r\nHowever, a new issue arises\r\n```\r\nat android.app.ActivityThread.deliverResults(ActivityThread.java:5078)\r\n        at android.app.ActivityThread.handleSendResult(ActivityThread.java:5120)\r\n        at android.app.servertransaction.ActivityResultItem.execute(ActivityResultItem.java:49)\r\n        at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:108)\r\n        at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:68)\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2199)\r\n        at android.os.Handler.dispatchMessage(Handler.java:112)\r\n        at android.os.Looper.loop(Looper.java:216)\r\n        at android.app.ActivityThread.main(ActivityThread.java:7625)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:524)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:987)\r\n     Caused by: java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/reshape.cc:58 stretch_dim != -1 (0 != -1)\r\n    Node number 168 (RESHAPE) failed to prepare.\r\n```\r\n\r\nThe java code which is causing the issue:\r\n```\r\nModel model = Model.newInstance(context);\r\n\r\n            TensorBuffer feature = TensorBuffer.createFixedSize(new int[]{1, 416, 416, 3}, DataType.FLOAT32);\r\n            feature.loadBuffer(tImage.getBuffer());\r\n\r\nLine 44 -->            Model.Outputs outputs = model.process(feature);\r\n```\r\n\r\nEDIT: I'm unsure if I should open a new issue tracker for this.", "We recommend using the same TF versions for both conversion and android applications. There is possibility that the model is not well converted with the old TF version. Could you try conversion with tf-nightly version and try again at the android application?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@Ean244 could you share how to reproduce the conversion? If possible, sharing a form of the gist will be very helpful.", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48277\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48277\">No</a>\n"]}, {"number": 48276, "title": "fix in tf.keras.applications.EfficientNetXX", "body": "in tensorflow/tensorflow/python/keras/applications/efficientnet.py\r\nat line 316,317 there is\r\n\r\nx = layers.Rescaling(1. / 255.)(x)\r\nx = layers.Normalization(axis=bn_axis)(x)\r\n\r\nWe normally put normalized data as input, and other networks(like mobilenet and resnet) other than this don't have such layers.\r\nI think these two layers should be removed.", "comments": ["https://github.com/tensorflow/tensorflow/blob/d8df26c4311abc332cc04d463d8154ab60c19014/tensorflow/python/keras/applications/efficientnet.py#L160-L165\r\n\r\nI suppose this requires to re-save trained model right? ", "Please see for more details about why we doing this.\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/47808#pullrequestreview-612848161", "> Please see for more details about why we doing this.\r\n> \r\n> [#47808 (review)](https://github.com/tensorflow/tensorflow/pull/47808#pullrequestreview-612848161)\r\n\r\n@qlzh727  Thanks for the reply! However, I am concerned that users who used backbone, such as mobilenet v2 and resnet, which did not have preprocessing layers, may not notice this change just like me. To make sure everyone who utilize tf.keras.applications.EfficientNet, MobileNetV3 realize this change, I think it would be better to provide a parameter that gives the user a choice whether or not to include a preprocessing layer. \r\n![image](https://user-images.githubusercontent.com/30307587/113530403-ede05300-9600-11eb-8b08-c326ca39a235.png)\r\nThank you!\r\n", "Ack. Will send a fix soon.", "@joonb14. just checked with API owner @fchollet about this. Adding a new flag for whether to include preprocessing will make the API surface misaligned with other models. Currently the only proper usage of all the models are:\r\n\r\n```\r\nxxx = application_module\r\nmodel = xxx.Model()(xxx.process_input(data))\r\n```\r\n\r\nFor you concern about other models that doesn't include preprocessing, it will very likely to get a low performance if user didn't invoke process_input(), which is exactly why we want to include the processing in the model."]}, {"number": 48275, "title": "tensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux bullseye/sid\r\n- TensorFlow installed from (source or binary): Not sure (I did not install it myself)\r\n- TensorFlow version (use command below): 2.5.0-dev20210323\r\n- Python version: Python 3.9.2\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: Tesla V100\r\n\r\nYou can also obtain the TensorFlow version with:\r\nv1.12.1-53554-gb725e835c68 2.5.0-dev20210323\r\n\r\n\r\n**Describe the current behavior**\r\nThere was an update on multiple aspects of the machine I train my models and after that I get the following error.  **Not sure what version exactly is not matching or not working, although due to the connected_components in the logs Im guessing it might have something to do with TF-Addons ('0.13.0-dev').**\r\n\r\nWhen trying to train the model I get the following error:\r\n```\r\n  File \"/home/pduque/.local/lib/python3.9/site-packages/comet_ml/monkey_patching.py\", line 317, in wrapper\r\n    return_value = original(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/keras/engine/training.py\", line 1154, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/def_function.py\", line 872, in __call__\r\n    result = self._call(*args, **kwds) \r\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/def_function.py\", line 933, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py\", line 3023, in __call__\r\n    return graph_function._call_flat(  \r\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py\", line 1960, in _call_flat\r\n    return self._build_call_outputs(self._inference_function.call(\r\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py\", line 591, in call          \r\n    outputs = execute.execute(                                                                                          \r\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute               tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name, \r\n\r\ntensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.                                             (0) Internal:  invalid resource handle                                                                                \r\n         [[{{node PartitionedCall/connected_components/Unique}}]]\r\n         [[ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_bool_Squeeze_1/_72]]\r\n  (1) Internal:  invalid resource handle\r\n         [[{{node PartitionedCall/connected_components/Unique}}]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_10900]\r\n\r\nFunction call stack:\r\ntrain_function -> train_function\r\n\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nModel should train normally without this error.\r\n\r\n**Standalone code to reproduce the issue**\r\nI use connected_components in the loss function, other than that is a regular model.\r\n```\r\ndef lession_recall(y_true, y_pred):\r\n    conn_comp_true = tensorflow_addons.image.connected_components(tf.cast(tf.squeeze(y_true, axis=[-1]), tf.bool))\r\n    conn_comp_pred = conn_comp_true * tf.cast(tf.squeeze(y_pred, axis=[-1]), tf.int32)\r\n\r\n    n_conn_comp_true, _ = tf.unique(backend.flatten(conn_comp_true))\r\n    n_conn_comp_pred, _ = tf.unique(backend.flatten(conn_comp_pred))\r\n    n_conn_comp_true = tf.size(input=n_conn_comp_true) - 1\r\n    n_conn_comp_pred = tf.size(input=n_conn_comp_pred) - 1\r\n\r\n    recall = tf.cond(pred=tf.equal(n_conn_comp_true, 0),\r\n                     true_fn=lambda: tf.cast(1.0, dtype=tf.float64), false_fn=lambda: n_conn_comp_pred / n_conn_comp_true)\r\n    return recall\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48275\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48275\">No</a>\n"]}, {"number": 48274, "title": "Adjust adjust unique hash table size to decrease memory bound .", "body": "I found a small performance optimization point about unique_op.\r\n>My test data: Input: size:20\\*1000k, uniqued size:5\\*1000k.\r\n\r\n> test A(Origin):\r\n>> Origin: The initial size of the hash table in unique_op is 2*input_size\r\n>> Origin: spend: 2.517s\r\n\r\n> I find heavy DRAM Bound by vtune .\r\n> so I decrease hash size from default value(2*input_size) to custom value(1*input).\r\n\r\n> test B(My test):\r\n>> My test: The initial size of the hash table in unique_op is 1*input_size(change source code\"tensorflow/core/kernels/unique_op.cc\").\r\n>> My test: spend: 1.709s**(\u219132.1%)**\r\n\r\nSo I think we should adjust default hash table size from default value to dynamic size.\r\n\r\n", "comments": ["Data size and data repetition rate should be stable in every steps.  My codes should be used in stable environment .\r\nMy change maybe get bad performance in unstable data repetition. so I add a switch(default: use 2 * input_size), that will ensure stability(switch:2*input_size) and improve performance(Heuristic adjust hash size).\r\n", "link issue:https://github.com/tensorflow/tensorflow/issues/47947", "@zhaozheng09 Can you please check @mrry's, @jpienaar's comments and keep us posted ? Thanks!", "> @zhaozheng09 Can you please check @mrry's, @jpienaar's comments and keep us posted ? Thanks!\r\n\r\nOk, I've been a little busy at work these two days, I will modify it in the near future. thx~", "@zhaozheng09  Any update on this PR? Please. Thanks!\r\n"]}, {"number": 48273, "title": "Heuristic adjust hash table size in unique_op", "body": "I find unique hash table size has a trouble performance in my test.\r\n>My test: Input: size:20\\*1000k, uniqued size:5\\*1000k.\r\n\r\nI find heavy DRAM Bound by vtune .\r\n\r\nso I decrease hash size from default value(2*input_size) to custom value(1*input).\r\n> cmp test:\r\n\r\n>> Origin: The initial size of the hash table in unique_op is 2*input_size\r\n\r\n>> My test: The initial size of the hash table in unique_op is 1*input_size(change source code\"tensorflow/core/kernels/unique_op.cc\").\r\n\r\n> cmp result:\r\n>> Origin: spend: 2.517s\r\n>> My test: spend: 1.709s(\u219132.1%)\r\n\r\nSo I think we should adjust default hash table size from default value to dynamic size.\r\n\r\n", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48273) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48273) for more info**.\n\n<!-- need_author_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48273) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 48272, "title": "No need for layers.Rescaling(1. / 255.) and layers.Normalization(axis=bn_axis) in efficientnet.py", "body": "in tensorflow/tensorflow/python/keras/applications/efficientnet.py\r\nat line 316,317 there is \r\n\r\nx = layers.Rescaling(1. / 255.)(x)\r\nx = layers.Normalization(axis=bn_axis)(x)\r\n\r\nWe normally put normalized data as input, and other networks(like mobilenet and resnet) other than efficientnet don't have such layers.\r\nI think these two layers should be removed.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48272) for more info**.\n\n<!-- need_sender_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48272) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48272) for more info**.\n\n<!-- need_author_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48272) for more info**.\n\n<!-- need_author_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48272) for more info**.\n\n<!-- need_author_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48272) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 48271, "title": "Compatibility with numpy 1.20", "body": "Hi\r\nMay I know if there is any note about the numpy version and tensorflow? It seems that the master branch doesn't work with numpy==1.20.0.\r\n\r\nIs that right? If yes, is there any workaround for that?", "comments": ["@mahmoodn \r\nNot for 2.4.x. For nightly or future releases please subscribe to #47691 which is WIP.\r\nCould you please refer to these issues #47691, #47878.\r\n\r\nKindly move this to closed status if it answers your question, as this is already tracked."]}, {"number": 48270, "title": "Fix a bug about opengles(AARCH64 Ubuntu 18.04)", "body": "### Notes:\r\nAs \"include <EGL/egl.h>\" import a macro 'Status' from there:</br>\r\n&nbsp; In file EGL/egl.h: #include <EGL/eglplatform.h></br>\r\n&nbsp; In file EGL/eglplatform.h: #include <X11/Xlib.h></br>\r\n&nbsp; In file X11/Xlib.h(line:83): #define Status int</br>\r\n\r\n### Why:\r\nAfter we import it, if you define a var like 'absl::Status TestVar', the compiler will precompile it to 'absl::int TestVar'. It is a disaster.\r\nwe should undef it to avoid this disaster.\r\n\r\n### Dependent Issues\r\nhttps://github.com/tensorflow/tensorflow/issues/46650\r\nhttps://github.com/google/mediapipe/issues/200\r\n\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48270) for more info**.\n\n<!-- need_sender_cla -->", "> You are pulling in X11 headers which have #define Status int. rather than doing this, you should exclude those headers with preprocessor macros.\r\n> \r\n> For anyone who is reviewing this PR, please do NOT approve.\r\n\r\nI can't understand it. \r\n\r\nHow can I exclude those headers with preprocessor macros ? Could you please give me an example? Thanks.\r\n\r\nYou mean like this ?\r\n`#ifdef  X11_XXXXXX`\r\n`#include <X11/Xlib.h>`\r\n`#endif `\r\n\r\nThe 'include <EGL/egl.h>' is written by file like 'portable_gl31.h' in tflite. The 'EGL/...' and 'X11/...' are a third-part open source lib. \r\n\r\nIn my mind, there are three solutions :\r\n- 1 undef macro 'Status' in 'tflite'.\r\n- 2 undef macro 'Status' in 'libgles*-mesa'.\r\n- 3 undef macro 'Status' in 'libx11-dev'.\r\n\r\n\r\nWhen I compile MediaPipe 0.8.3.2, it will be paused by this problem. ", "You might need things like `-DMESA_EGL_NO_X11_HEADERS -DEGL_NO_X11` to make it work.  You may have to dig deeper into your version of X11 headers to find out the exact name of the variable though.", "OK! Sorry! It's my fault. I will close this PR and delete it. \r\n\r\nThe 'MESA_EGL_NO_X11_HEADERS' and 'EGL_NO_X11' aren't worked for me, because the 'eglplatform.h' is supplied by my supplier, it isn't supplied by libegl1-mesa-dev. The supplier modifies it , then it doesn't include macro 'EGL_NO_X11'.\r\n\r\nlike this:\r\n`> dpkg-query -S eglplatform.h`\r\n`libmali-rk-dev:arm64: /usr/include/EGL/eglplatform.h`\r\n`libmali-rk-dev:arm64: /usr/include/eglplatform.h`\r\n\r\n`apt-cache madison libmali-rk-dev`\r\n`libmali-rk-dev |      1.7-1 | http://wiki.t-firefly.com/firefly-rk3399-repo bionic/main arm64 Packages`\r\n\r\nThanks."]}, {"number": 48269, "title": "tf.signal.rFFT in TFLITE is not working in JAVA", "body": "**System information**\r\n- OS Platform and Distribution (Android 10,11):\r\n- TensorFlow installed from : source\r\n- TensorFlow version: 2.4.1\r\n\r\nI am simply trying to make a model which takes time series data as input and compute rFFT/irFFT and return time series data.\r\nI used tf.signal.rfft and tf.signal.irfft for computations. I have converted it to TFLITE and its working perfectly fine in python. But when I load it into android studio it gave me Null in interpreter. Following is code for python:\r\n\r\n```\r\nfrom tensorflow.keras.layers import  Lambda, Input\r\nimport tensorflow as tf\r\n\r\ninp = keras.Input(shape=((197429)))\r\nO = Lambda(stftLayer)(inp)\r\nZ = Lambda(istftLayer)(O)\r\nmodel = keras.Model(inputs=inp, outputs=Z, name=\"fft_model\")\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS ]\r\ntflite_model = converter.convert()\r\nwith open('FFT.tflite', 'wb') as f:\r\n    f.write(tflite_model)\r\n```\r\n\r\nWhen I give voice signal to this TFLITE model on python it returns same signal perfectly. But When I Load it into Android studio and run following code, it do not load TFLITE model properly and show NULL in Interpreter :\r\n\r\n    private MappedByteBuffer loadModelFile() throws IOException {\r\n    AssetFileDescriptor fileDescriptor = this.getAssets().openFd(\"FFT.tflite\");\r\n    FileInputStream fileInputStream = new FileInputStream(fileDescriptor.getFileDescriptor());\r\n    FileChannel fileChannel = fileInputStream.getChannel();\r\n    long startOffSets = fileDescriptor.getStartOffset();\r\n    long declaredLength = fileDescriptor.getDeclaredLength();\r\n    return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffSets, declaredLength);\r\n    \r\n    }\r\n    import org.tensorflow.lite.Interpreter;\r\n    Interpreter tflite;\r\n    \r\n    try {\r\n    tflite = new Interpreter(loadModelFile());\r\n    \r\n        } catch (Exception e) {\r\n            e.printStackTrace();\r\n        }\r\n\r\nWhen I run model it gives \r\n**Caused by: java.lang.NullPointerException: Attempt to invoke virtual method 'void org.tensorflow.lite.Interpreter.run(java.lang.Object, java.lang.Object)' on a null object reference**\r\n\r\nFollowing is gradle setting:\r\n\r\n    android {\r\n        compileSdkVersion 30\r\n        buildToolsVersion \"30.0.3\"\r\n    \r\n        defaultConfig {\r\n            applicationId \"com.example.dtln_test\"\r\n            minSdkVersion 27\r\n            targetSdkVersion 30\r\n            versionCode 1\r\n            versionName \"1.0\"\r\n    \r\n            testInstrumentationRunner \"androidx.test.runner.AndroidJUnitRunner\"\r\n        }\r\n    \r\n        buildTypes {\r\n            release {\r\n                minifyEnabled false\r\n                proguardFiles getDefaultProguardFile('proguard-android-optimize.txt'), 'proguard-rules.pro'\r\n            }\r\n        }\r\n        compileOptions {\r\n            sourceCompatibility JavaVersion.VERSION_1_8\r\n            targetCompatibility JavaVersion.VERSION_1_8\r\n        }\r\n        aaptOptions{\r\n            noCompress = \"tflite\"\r\n    \r\n        }\r\n        buildFeatures {\r\n            mlModelBinding true\r\n        }\r\n    }\r\n    \r\n    dependencies {\r\n        implementation fileTree(dir: 'libs', include: ['*.jar'])\r\n    \r\n        //implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\n        implementation 'androidx.appcompat:appcompat:1.1.0'\r\n        implementation 'androidx.constraintlayout:constraintlayout:1.1.3'\r\n        implementation 'org.tensorflow:tensorflow-lite:+'\r\n        implementation 'org.tensorflow:tensorflow-lite-support:+'\r\n        ///////////////////////////////////////////////////////////\r\n       // implementation 'org.tensorflow:tensorflow-lite-support:0.1.0-rc1'\r\n      //  implementation 'org.tensorflow:tensorflow-lite-metadata:0.1.0-rc1'\r\n    //    implementation 'org.tensorflow:tensorflow-lite-gpu:2.2.0'\r\n        testImplementation 'junit:junit:4.12'\r\n        implementation 'com.google.android.material:material:1.2.0-alpha03'\r\n        androidTestImplementation 'androidx.test.ext:junit:1.1.1'\r\n        androidTestImplementation 'androidx.test.espresso:espresso-core:3.2.0'\r\n    }\r\n\r\nAny help will be greatly appreciated !!\r\nThanks\r\n", "comments": ["@gargn @ravikyram Please review this.", "FYI, I added my answer at the same post at https://stackoverflow.com/questions/66915970", "Thankyou @abattery \r\nIssue Resolved. While Making TFLITE with custom functions/Lambda Layers, do these steps while making tflite.\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS ]\r\ntflite_model = converter.convert()\r\nand in android studio add these in dependicies:\r\n\r\nimplementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly-SNAPSHOT'\r\n\r\nimplementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly-SNAPSHOT'", "Glad to see it resolved. Have a nice day!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48269\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48269\">No</a>\n"]}, {"number": 48268, "title": "XLA compilation bug in TPU involving indirection through index tensors", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["**System information**\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\nTensorFlow installed from (source or binary): binary, pre-installed on Colab\r\nTensorFlow version (use command below): 2.4.1\r\nPython version: 3.7?\r\nBazel version (if compiling from source): None\r\nGCC/Compiler version (if compiling from source): None\r\nCUDA/cuDNN version: Colab\r\nGPU model and memory: Colab \r\n\r\n**Describe the current behavior**\r\n\r\nThe following code works on CPU and GPU, but fails under TPU. The compilation error follows the code.\r\n\r\n**Describe the expected behavior**\r\n\r\nShould run on all three Colab platforms.\r\n\r\n**Note**\r\nKeras layer should return the input data, randomly shuffled.\r\nThe code uses tf.shuffle() to create a random set of indexes to rearrange an input array.\r\nIt is necessary to create an indirection index; this is for a project that performs the same shuffle on two different tensors. (There might be a way to do this by controlling the seed on every batch, but I have not found it.)\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras.backend as K\r\nfrom tensorflow.python.framework import smart_cond\r\n\r\nimport numpy as np\r\n\r\ntf.config.optimizer.set_jit(True) # Enable XLA.\r\n\r\n\"\"\"## Establish TPU access\r\n(lifted from efficientnet example)\r\n\"\"\"\r\n\r\ntry:\r\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\r\n    print(\"Running on TPU \", tpu.cluster_spec().as_dict()[\"worker\"])\r\n    tf.config.experimental_connect_to_cluster(tpu)\r\n    tf.tpu.experimental.initialize_tpu_system(tpu)\r\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\r\nexcept ValueError:\r\n    print(\"Not connected to a TPU runtime. Using CPU/GPU strategy\")\r\n    strategy = tf.distribute.MirroredStrategy()\r\n\r\n\"\"\"## Keras class to demonstrate problem\"\"\"\r\n\r\nclass TPUBug(tf.keras.layers.Layer):\r\n\r\n    def __init__(self, batchsize=32, **kwargs):\r\n        super(TPUBug, self).__init__(**kwargs)\r\n        self.batchsize = batchsize\r\n  \r\n    def build(self, input_shape):\r\n        self.data_len = input_shape.as_list()[1]\r\n        self.built = True\r\n\r\n    def call(self, inputs):\r\n        indices = [i for i in range(self.data_len)]\r\n        indices = tf.convert_to_tensor(indices, dtype='int32')\r\n        random_indices = tf.random.shuffle(indices)\r\n        stacked = []\r\n        for i in range(self.data_len):\r\n            indexed = inputs[:, random_indices[i]]\r\n            stacked.append(indexed)\r\n\r\n        output = tf.stack(stacked, axis=-1)\r\n        return output\r\n  \r\n    def compute_output_shape(self, input_shape):\r\n        return input_shape\r\n\r\n\"\"\"## Simple test\"\"\"\r\n\r\nwith strategy.scope():\r\n    inputs = tf.keras.layers.Input(shape=(17,))\r\n    outputs = TPUBug(batchsize=32)(inputs)\r\n\r\n    model = tf.keras.Model(inputs, outputs)\r\n    # is this needed?\r\n    model.compile(\r\n        optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\r\n    )\r\n\r\nraw = np.arange(17, dtype='float32')\r\ndata = np.asarray([raw, raw + 100])\r\nprint('data:', data)\r\n\r\nmodel.summary()\r\n\r\nshuffled = model.predict(data)\r\nprint('shuffled:', shuffled)\r\n```\r\nCompilation error on TPU:\r\n```\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-1-76b0dd665776> in <module>()\r\n     83 model.summary()\r\n     84 \r\n---> 85 shuffled = model.predict(data)\r\n     86 print('shuffled:', shuffled)\r\n     87 \r\n\r\n4 frames\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py in predict(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\r\n   1640                   outputs, batch_outputs)\r\n   1641             end_step = step + data_handler.step_increment\r\n-> 1642             callbacks.on_predict_batch_end(end_step, {'outputs': batch_outputs})\r\n   1643       if batch_outputs is None:\r\n   1644         raise ValueError('Expect x to be a non-empty array or dataset.')\r\n\r\n/usr/lib/python3.7/contextlib.py in __exit__(self, type, value, traceback)\r\n    117         if type is None:\r\n    118             try:\r\n--> 119                 next(self.gen)\r\n    120             except StopIteration:\r\n    121                 return False\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py in catch_stop_iteration(self)\r\n   1162     try:\r\n   1163       yield\r\n-> 1164       context.async_wait()\r\n   1165     except (StopIteration, errors.OutOfRangeError):\r\n   1166       if self._inferred_steps is None:\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py in async_wait()\r\n   2328   an error state.\r\n   2329   \"\"\"\r\n-> 2330   context().sync_executors()\r\n   2331 \r\n   2332 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py in sync_executors(self)\r\n    643     \"\"\"\r\n    644     if self._context_handle:\r\n--> 645       pywrap_tfe.TFE_ContextSyncExecutors(self._context_handle)\r\n    646     else:\r\n    647       raise ValueError(\"Context is not initialized.\")\r\n\r\nInvalidArgumentError: 9 root error(s) found.\r\n  (0) Invalid argument: {{function_node __inference_predict_function_1491}} Compilation failure: The number of output elements 2 has to equal to number of input elements that are sliced 1 when input indices are not constant.\r\n\t [[{{node model/tpu_bug/strided_slice_1}}]]\r\n\tTPU compilation failed\r\n\t [[tpu_compile_succeeded_assert/_13533889726789507083/_4]]\r\n\t [[tpu_compile_succeeded_assert/_13533889726789507083/_4/_301]]\r\n  (1) Invalid argument: {{function_node __inference_predict_function_1491}} Compilation failure: The number of output elements 2 has to equal to number of input elements that are sliced 1 when input indices are not constant.\r\n\t [[{{node model/tpu_bug/strided_slice_1}}]]\r\n\tTPU compilation failed\r\n\t [[tpu_compile_succeeded_assert/_13533889726789507083/_4]]\r\n\t [[tpu_compile_succeeded_assert/_13533889726789507083/_4/_281]]\r\n  (2) Invalid argument: {{function_node __inference_predict_function_1491}} Compilation failure: The number of output elements 2 has to equal to number of input elements that are sliced 1 when input indices are not constant.\r\n\t [[{{node model/tpu_bug/strided_slice_1}}]]\r\n\tTPU compilation failed\r\n\t [[tpu_compile_succeeded_assert/_13533889726789507083/_4]]\r\n\t [[tpu_compile_succeeded_assert/_13533889726789507083/_4/_241]]\r\n  (3) Invalid argument: {{function_node __inference_predict_function_1491}} Compilation failure: The number of output elements 2 has to equal to number of input elements that are sliced 1 when input indices are not constant.\r\n\t [[{{node model/tpu_bug/strided_slice_1}}]]\r\n\tTPU compilation failed\r\n\t [[tpu_compile_succeeded_assert/_13533889726789507083/_4]]\r\n\t [[tpu_compile_succeeded_assert/_13533889726789507083/_4/_271]]\r\n  (4) Invalid argument: {{function_node __inference_predict_function_1491}} Compilation failure: The number of output elements 2 has to equal to number of input elements that are sliced 1 when input indices are not constant.\r\n\t [[{{node model/tpu_bug/strided_slice_1}}]]\r\n\tTPU compilation failed\r\n\t [[tpu_compile_succeeded_assert/_13533889726789507083/_4]]\r\n\t [[cluster_predict_function/control_after/_1/_361]]\r\n  (5) Invalid argument: {{function_node __inference_predict_function_1491}} Compilation failure: The number of output elements 2 has to equal to number of input elements that are sliced 1 when input indices are not constant.\r\n\t [[{{node model/tpu_bug/strided_slice_1}}]]\r\n\tTPU compilation failed\r\n\t [[tpu_compile_succeeded_assert/_13533889726789507083/_4]]\r\n\t [[tpu_compile_succeeded_assert/_13533889726789507083/_4/_261]]\r\n  (6) Invalid argument: {{function_node __inference_predict_function_1491}} Compilation failure: The number of output elements 2 has to equal to number of input elements that are sliced 1 when input indices are not constant.\r\n\t [[{{node model/tpu_bug/strided_slice_1}}]]\r\n\tTPU compilation failed\r\n\t [[tpu_compile_succeeded_assert/_13533889726789507083/_4]]\r\n\t [[tpu_compile_succeeded_assert/_13533889726789507083/_4/_251]]\r\n  (7) Invalid argument: {{function_node __inference_predict_function_1491}} Compilation failure: The number of output elements 2 ... [truncated]\r\n```", "Under CPU and GPU, the output looks like this:\r\n```\r\nNot connected to a TPU runtime. Using CPU/GPU strategy\r\nWARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\r\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\r\ndata: [[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\r\n   14.  15.  16.]\r\n [100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111. 112. 113.\r\n  114. 115. 116.]]\r\nModel: \"model_2\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_3 (InputLayer)         [(None, 17)]              0         \r\n_________________________________________________________________\r\ntpu_bug_2 (TPUBug)           (None, 17)                0         \r\n=================================================================\r\nTotal params: 0\r\nTrainable params: 0\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nshuffled: [[  8.   9.   3.   6.  10.   4.   1.   7.   5.  16.  12.  11.   2.  13.\r\n    0.  15.  14.]\r\n [108. 109. 103. 106. 110. 104. 101. 107. 105. 116. 112. 111. 102. 113.\r\n  100. 115. 114.]]\r\n```", "Looking at this now, I see a fundamental problem that is not true on CPU or GPU: the output of tf.shuffle() is common across each batch. Under TPU (and multi-GPU training) each executor only sees a \"sub-batch\". For this to fully work on TPU, I think I need to supply the indirection indexes as input values in parallel to the data samples that are to be shuffled.\r\n\r\nIs this the problem with the above code? ", "@ymodak,\r\nCode runs fine on [CPU](https://colab.research.google.com/gist/amahendrakar/95e7c982f80b8548155d75534a45639b/48268-cpu.ipynb) and [GPU](https://colab.research.google.com/gist/amahendrakar/eb47427b62617f22ba852bc1f491c711/48268-gpu.ipynb). Whereas using TPUs, I was able to reproduce the error.\r\n\r\n With [TF v2.4 + TPU](https://colab.research.google.com/gist/amahendrakar/f2563c8ff45afdd3dfb625f993df5d1d/48268.ipynb#scrollTo=uSd7RJKb8KzO), the error is \r\n```\r\nCompilation failure: The number of output elements 2 has to equal to number of input elements that are sliced 1 when input indices are not constant.\r\n```\r\n\r\nand with TF v2.5.0-rc0 and [TF-nightly + TPU](https://colab.research.google.com/gist/amahendrakar/2db09a96874795dc1f1ffa76ddabd95f/48268-tpu-nightly.ipynb), the error is \r\n```\r\nNotFoundError: Op type not registered 'XlaSetDynamicDimensionSize' in binary running on n-ba3356ad-w-0.\r\n```\r\n\r\nPlease check the linked gist for reference. Thanks!", "Assigning to TPU oncall.", "Code works on 2.6.0-rc1 on TPU. Closed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48268\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48268\">No</a>\n"]}, {"number": 48267, "title": "ImportError: No module named '_pywrap_tensorflow_internal'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 64amd\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:1.5.0\r\n- Python version:3.5.2 , 3.6,3.7,3.8 & 3.9.2 64amd \r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nC:\\Users\\Vanshika Gupta\\AppData\\Local\\Programs\\Python\\Python39\\sign-language-gesture-recognition-master>python retrain.py --bottleneck_dir=bottlenecks --summaries_dir=training_summaries/long --output_graph=retrained_graph.pb --output_labels=retrained_labels.txt --image_dir=train_frames\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Vanshika Gupta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"C:\\Users\\Vanshika Gupta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Vanshika Gupta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Vanshika Gupta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\Vanshika Gupta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Vanshika Gupta\\AppData\\Local\\Programs\\Python\\Python39\\sign-language-gesture-recognition-master\\retrain.py\", line 132, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Vanshika Gupta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\Vanshika Gupta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Vanshika Gupta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"C:\\Users\\Vanshika Gupta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Vanshika Gupta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Vanshika Gupta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\Vanshika Gupta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@vanshika-653 \r\nWe see that you are using an old version of TensorFlow for which there is no support, can you please upgrade to 2.x [2.4] and let us know if you till face the issue reported.\r\nYou may refer to few old issues with same error: #22512, [link](https://github.com/tensorflow/tensorflow/issues/7529), [link1](https://github.com/tensorflow/tensorflow/issues/40668#issuecomment-647885727)", "I am getting a similar error: \r\n\r\nwhen running in jupyter notebook:\r\n\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\n\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nwhen just trying to import if va cmd: \r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\n\r\n\r\nAny solutions would be great! thanks", "@grantcroft \r\nPlease share your code such that we can replicate the issue reported, or a colab gist with the error and version used.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48267\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48267\">No</a>\n", "I am also facing the same error.\r\n\r\npython --version gave\r\nPython 3.7.0\r\n\r\nTensorflow version is\r\n\r\n(0.12.0)\r\n\r\nI have already installed Microsoft Visual C++ Redistribution (x64) 9.0.30729.17/.6161 / .21022 /(x86) 30729.17/ (x86) 30729.6161\r\n\r\nMicrosoft Visual C++ Redistribution2015-2019 (x64) 14.25.28508\r\nMicrosoft Visual C++ Redistribution2015-2019 (x64) 14.31.31103\r\n\r\nI have included path to all dlls to environment variable path.\r\n\r\nOS is Win 7 Ultimate.\r\n\r\nWhen I do\r\nimport tensorflow as tf\r\n\r\nI get following error\r\n`---------------------------------------------------------------------------\r\nImportError Traceback (most recent call last)\r\nh:\\python installed\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in swig_import_helper()\r\n17 try:\r\n---> 18 fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(file)])\r\n19 except ImportError:\r\n\r\nh:\\python installed\\lib\\imp.py in find_module(name, path)\r\n296 else:\r\n--> 297 raise ImportError(_ERR_MSG.format(name), name=name)\r\n298\r\n\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nModuleNotFoundError Traceback (most recent call last)\r\nh:\\python installed\\lib\\site-packages\\tensorflow\\python_init_.py in\r\n53 # use dlopen() for dynamic loading.\r\n---> 54 from tensorflow.python import pywrap_tensorflow\r\n55 except ImportError:\r\n\r\nh:\\python installed\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in\r\n27 return _mod\r\n---> 28 _pywrap_tensorflow = swig_import_helper()\r\n29 del swig_import_helper\r\n\r\nh:\\python installed\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in swig_import_helper()\r\n19 except ImportError:\r\n---> 20 import _pywrap_tensorflow\r\n21 return _pywrap_tensorflow\r\n\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError Traceback (most recent call last)\r\nin\r\n----> 1 import tensorflow as tf\r\n2 from tensorflow.keras.utils import to_categorical\r\n\r\nh:\\python installed\\lib\\site-packages\\tensorflow_init_.py in\r\n22\r\n23 # pylint: disable=wildcard-import\r\n---> 24 from tensorflow.python import *\r\n25 # pylint: enable=wildcard-import\r\n26\r\n\r\nh:\\python installed\\lib\\site-packages\\tensorflow\\python_init_.py in\r\n58 please exit the tensorflow source tree, and relaunch your python interpreter\r\n59 from there.\"\"\" % traceback.format_exc()\r\n---> 60 raise ImportError(msg)\r\n61\r\n62 # Protocol buffers\r\n\r\nImportError: Traceback (most recent call last):\r\nFile \"h:\\python installed\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\nfp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(file)])\r\nFile \"h:\\python installed\\lib\\imp.py\", line 297, in find_module\r\nraise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\nFile \"h:\\python installed\\lib\\site-packages\\tensorflow\\python_init_.py\", line 54, in\r\nfrom tensorflow.python import pywrap_tensorflow\r\nFile \"h:\\python installed\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in\r\n_pywrap_tensorflow = swig_import_helper()\r\nFile \"h:\\python installed\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\nimport _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\nError importing tensorflow. Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.`"]}, {"number": 48266, "title": "Failed to get convolution algorithm", "body": "**System information**\r\n- OS Platform and Distribution Linux Ubuntu 18.04 aws dlami\r\n- TensorFlow version : tensorflow==2.5.0rc0\r\n- Python version:  3.6\r\n- CUDA/cuDNN version: cuda 11.0\r\n- GPU model and memory:\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\r\n| N/A   31C    P0    32W /  70W |   6220MiB / 15109MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A      2685      C   python                           6217MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n** current behaviour**\r\n```2021-04-02 05:38:06.872091: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\r\n2021-04-02 05:38:06.888553: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2499995000 Hz\r\n2021-04-02 05:38:07.028741: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-04-02 05:38:07.528481: E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] Loaded runtime CuDNN library: 8.0.5 but source was compiled with: 8.1.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\r\n2021-04-02 05:38:07.531403: E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] Loaded runtime CuDNN library: 8.0.5 but source was compiled with: 8.1.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\r\n2021-04-02 05:38:07.532382: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at conv_ops_fused_impl.h:698 : Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/ubuntu/anaconda3/envs/tf_pytorch_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1727, in predict\r\n    tmp_batch_outputs = self.predict_function(iterator)\r\n  File \"/home/ubuntu/anaconda3/envs/tf_pytorch_p36/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 872, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/ubuntu/anaconda3/envs/tf_pytorch_p36/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 940, in _call\r\n    filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access\r\n  File \"/home/ubuntu/anaconda3/envs/tf_pytorch_p36/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1961, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/home/ubuntu/anaconda3/envs/tf_pytorch_p36/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 596, in call\r\n    ctx=ctx)\r\n  File \"/home/ubuntu/anaconda3/envs/tf_pytorch_p36/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[node model/block1_conv1/Relu (defined at <stdin>:1) ]] [Op:__inference_predict_function_618]\r\n\r\nFunction call stack:\r\npredict_function\r\n```\r\n\r\n\r\n\r\nI am getting error with latest tensorflow 2.5.0 its working fine with tensorflow 2.4.0. Also it's work with tensorflow 2.5.0 in colab but in my server is giving me error.", "comments": ["Please try with cuda 11.2 for TF 2.5.X versions.", "hey @ymodak \r\nI have changed cuda version but still getiing same error\r\n\r\ncommand for installation : \r\n```\r\nsudo apt install cuda-11.2\r\nsudo apt-get install --no-install-recommends nvidia-driver-460\r\n```\r\n\r\nnvidia-smi\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 460.39       Driver Version: 460.39       CUDA Version: 11.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla T4            Off  | 00000000:00:1E.0 Off |                    0 |\r\n| N/A   32C    P0    26W /  70W |  10091MiB / 15109MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A      3040      C   python                           6229MiB |\r\n|    0   N/A  N/A      3224      C   python                           3859MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n```", "> Loaded runtime CuDNN library: 8.0.5 but source was compiled with: 8.1.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.\r\n\r\nCan you also upgrade your cudnn to 8.1.0?\r\nAlso see TF 2.5 rc0 [release notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.5.0-rc0)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48266\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48266\">No</a>\n"]}, {"number": 48265, "title": "What does it mean Trackable objects? ", "body": "According to the read me, it [says](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/saving/saved_model/README.md#:~:text=Trackable%20objects%20and%20TensorFlow%20functions,wrapped%20as%20trackable%20objects%2Ftf.)\r\n\r\n> Trackable objects and TensorFlow functions are represented as nodes in the trackable object graph, and each node in the graph stores information about their python properties. Since many attributes in Keras Layers/Models are not Trackable objects or tf. functions, these attributes are wrapped as trackable objects/tf.\r\n\r\nBy this, I understand one thing that, it represents a node in the `tf` graph. But this information is not enough. What does it mean exactly? ", "comments": ["@innat \r\nThis question is more suited for Stack Overflow rather than here. Still, I will try to answer it.\r\nTrackable objects are, in my opinion, objects that are kept on check by the TensorFlow runtime. This includes things like trainable variables. They need to be updated after every epoch, right? Also, things like losses and metrics have states, which are managed by the runtime. ", "You find a definition in https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint?hl=es-419#used-in-the-notebooks_1", "@bhack I don't see a definition on that page. All I see is the following, which gives examples of trackable objects without defining them.\r\n\r\n> TensorFlow objects may contain trackable state, such as tf.Variables, tf.keras.optimizers.Optimizer implementations, tf.data.Dataset iterators, tf.keras.Layer implementations, or tf.keras.Model implementations. These are called trackable objects.\r\n\r\nAm I missing something?", "You can find something more in https://www.tensorflow.org/guide/checkpoint#object_tracking"]}, {"number": 48264, "title": "ERROR: An error occurred during the fetch of repository 'local_config_python':", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.4.1\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 2.7.2\r\n- GCC/Compiler version (if compiling from source): not sure how to check this\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n\r\n\r\n**Describe the problem**\r\nBuilding TF with bazel is not working. The cause seems to be:\r\n**ERROR: An error occurred during the fetch of repository 'local_config_python'**:\r\nI am not sure what exactly the error means or how to fix it. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n(build_tf_apr21) C:\\git_repositories\\BlinkGuard\\code\\tensorflow>python configure.py\r\nYou have bazel 3.7.2 installed.\r\nPlease specify the location of python. [Default is C:\\Users\\gerri\\anaconda3\\envs\\build_tf_apr21\\python.exe]:\r\n\r\nFound possible Python library paths:\r\n  C:\\Users\\gerri\\anaconda3\\envs\\build_tf_apr21\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\gerri\\anaconda3\\envs\\build_tf_apr21\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: n\r\nNot overriding eigen strong inline, some compilations could take more than 20 mins.\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n\r\n(build_tf_apr21) C:\\git_repositories\\BlinkGuard\\code\\tensorflow>bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-\r\nmsse4.1 --copt=-msse4.2 -k //tensorflow/tools/pip_package:build_pip_package\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=152\r\nINFO: Reading rc options for 'build' from c:\\git_repositories\\blinkguard\\code\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/gerri/anaconda3/envs/build_tf_apr21/python.exe\r\nINFO: Reading rc options for 'build' from c:\\git_repositories\\blinkguard\\code\\tensorflow\\.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_to\r\nolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --a\r\nnnounce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --d\r\nefine=with_xla_support=true --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from c:\\git_repositories\\blinkguard\\code\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/gerri/anaconda3/envs/build_tf_apr21/python.exe --action_env PYTHON_LIB_PATH=C:/Users/gerri/anac\r\nonda3/envs/build_tf_apr21/lib/site-packages --python_path=C:/Users/gerri/anaconda3/envs/build_tf_apr21/python.exe --copt=/d2ReducedOptimizeHugeFunctions\r\n --host_copt=/d2ReducedOptimizeHugeFunctions\r\nINFO: Found applicable config definition build:short_logs in file c:\\git_repositories\\blinkguard\\code\\tensorflow\\.bazelrc: --output_filter=DONT_MATCH_AN\r\nYTHING\r\nINFO: Found applicable config definition build:v2 in file c:\\git_repositories\\blinkguard\\code\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_en\r\nv=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:windows in file c:\\git_repositories\\blinkguard\\code\\tensorflow\\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DE\r\nFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_\r\nLEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkop\r\nt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\git_repositories\\blinkguard\\code\\tensorflow\\.bazelrc: --define framework_shared_obj\r\nect=false\r\nDEBUG: C:/users/gerri/_bazel_gerri/wwc5pzrd/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA prop\r\nrietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree t\r\no the terms of the license agreement, do not use the software.\r\nWARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/40548a2974f1aea06215272d9c2b47a14a24e556.tar.gz failed: class c\r\nom.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nINFO: Build options --copt, --define, and --host_copt have changed, discarding analysis cache.\r\nINFO: Repository local_execution_config_python instantiated at:\r\n  C:/git_repositories/blinkguard/code/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  C:/git_repositories/blinkguard/code/tensorflow/tensorflow/workspace2.bzl:1099:19: in workspace\r\n  C:/git_repositories/blinkguard/code/tensorflow/tensorflow/workspace2.bzl:85:27: in _tf_toolchains\r\n  C:/users/gerri/_bazel_gerri/wwc5pzrd/external/tf_toolchains/toolchains/remote_config/configs.bzl:6:28: in initialize_rbe_configs\r\n  C:/users/gerri/_bazel_gerri/wwc5pzrd/external/tf_toolchains/toolchains/remote_config/rbe_config.bzl:158:27: in _tensorflow_local_config\r\nRepository rule local_python_configure defined at:\r\n  C:/git_repositories/blinkguard/code/tensorflow/third_party/py/python_configure.bzl:275:41: in <toplevel>\r\nINFO: Repository local_config_python instantiated at:\r\n  C:/git_repositories/blinkguard/code/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  C:/git_repositories/blinkguard/code/tensorflow/tensorflow/workspace2.bzl:1099:19: in workspace\r\n  C:/git_repositories/blinkguard/code/tensorflow/tensorflow/workspace2.bzl:95:21: in _tf_toolchains\r\nRepository rule python_configure defined at:\r\n  C:/git_repositories/blinkguard/code/tensorflow/third_party/py/python_configure.bzl:294:35: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'local_execution_config_python':\r\n   Traceback (most recent call last):\r\n        File \"C:/git_repositories/blinkguard/code/tensorflow/third_party/py/python_configure.bzl\", line 209, column 22, in _create_local_python_reposito\r\nry\r\n                _check_python_bin(repository_ctx, python_bin)\r\n        File \"C:/git_repositories/blinkguard/code/tensorflow/third_party/py/python_configure.bzl\", line 143, column 52, in _check_python_bin\r\n                result = raw_exec(repository_ctx, [get_bash_bin(repository_ctx), \"-c\", cmd])\r\n        File \"C:/git_repositories/blinkguard/code/tensorflow/third_party/remote_config/common.bzl\", line 77, column 26, in get_bash_bin\r\n                bash_bin_path = which(repository_ctx, \"bash\")\r\n        File \"C:/git_repositories/blinkguard/code/tensorflow/third_party/remote_config/common.bzl\", line 27, column 23, in which\r\n                return execute(\r\n        File \"C:/git_repositories/blinkguard/code/tensorflow/third_party/remote_config/common.bzl\", line 219, column 13, in execute\r\n                fail(\r\nError in fail: Repository command failed\r\nINFO: Could not find files for the given pattern(s).\r\nERROR: An error occurred during the fetch of repository 'local_config_python':\r\n   Traceback (most recent call last):\r\n        File \"C:/git_repositories/blinkguard/code/tensorflow/third_party/py/python_configure.bzl\", line 267, column 40, in _python_autoconf_impl\r\n                _create_local_python_repository(repository_ctx)\r\n        File \"C:/git_repositories/blinkguard/code/tensorflow/third_party/py/python_configure.bzl\", line 209, column 22, in _create_local_python_reposito\r\nry\r\n                _check_python_bin(repository_ctx, python_bin)\r\n        File \"C:/git_repositories/blinkguard/code/tensorflow/third_party/py/python_configure.bzl\", line 143, column 52, in _check_python_bin\r\n                result = raw_exec(repository_ctx, [get_bash_bin(repository_ctx), \"-c\", cmd])\r\n        File \"C:/git_repositories/blinkguard/code/tensorflow/third_party/remote_config/common.bzl\", line 77, column 26, in get_bash_bin\r\n                bash_bin_path = which(repository_ctx, \"bash\")\r\n        File \"C:/git_repositories/blinkguard/code/tensorflow/third_party/remote_config/common.bzl\", line 27, column 23, in which\r\n                return execute(\r\n        File \"C:/git_repositories/blinkguard/code/tensorflow/third_party/remote_config/common.bzl\", line 219, column 13, in execute\r\n                fail(\r\nError in fail: Repository command failed\r\nINFO: Could not find files for the given pattern(s).\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -040\r\n0\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  C:/git_repositories/blinkguard/code/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  C:/git_repositories/blinkguard/code/tensorflow/tensorflow/workspace0.bzl:108:34: in workspace\r\n  C:/users/gerri/_bazel_gerri/wwc5pzrd/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories\r\nRepository rule git_repository defined at:\r\n  C:/users/gerri/_bazel_gerri/wwc5pzrd/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nWARNING: errors encountered while analyzing target '//tensorflow/tools/pip_package:build_pip_package': it will not be built\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 4017 targets configured).\r\nINFO: Found 0 targets...\r\nERROR: command succeeded, but not all targets were analyzed\r\nINFO: Elapsed time: 4.195s, Critical Path: 0.00s\r\nINFO: 1 process: 1 internal.\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["Update: \r\n\r\nI wondered if the actual problem could have been:\r\nWARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/40548a2974f1aea06215272d9c2b47a14a24e556.tar.gz failed: class c\r\nom.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\n\r\nI tried \r\nbazel clean --expunge\r\n\r\nThen I repeated bazel build using a VPN (I am **not** in China). This time I get no download warning but still the same error as before:\r\n\r\n(build_tf_apr21) C:\\git_repositories\\BlinkGuard\\code\\tensorflow>bazel clean --expunge\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=152\r\nINFO: Reading rc options for 'clean' from c:\\git_repositories\\blinkguard\\code\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  Inherited 'build' options: --python_path=C:/Users/gerri/anaconda3/envs/build_tf_apr21/python.exe\r\nINFO: Reading rc options for 'clean' from c:\\git_repositories\\blinkguard\\code\\tensorflow\\.bazelrc:\r\n  Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolch\r\nain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone\r\n-c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_\r\nconfig --define=with_xla_support=true --config=short_logs --config=v2\r\nINFO: Reading rc options for 'clean' from c:\\git_repositories\\blinkguard\\code\\tensorflow\\.tf_configure.bazelrc:\r\n  Inherited 'build' options: --action_env PYTHON_BIN_PATH=C:/Users/gerri/anaconda3/envs/build_tf_apr21/python.exe --action_env PYTHON_LIB_PATH=C:/Users/\r\ngerri/anaconda3/envs/build_tf_apr21/lib/site-packages --python_path=C:/Users/gerri/anaconda3/envs/build_tf_apr21/python.exe --copt=/d2ReducedOptimizeHug\r\neFunctions --host_copt=/d2ReducedOptimizeHugeFunctions\r\nINFO: Found applicable config definition build:short_logs in file c:\\git_repositories\\blinkguard\\code\\tensorflow\\.bazelrc: --output_filter=DONT_MATCH_AN\r\nYTHING\r\nINFO: Found applicable config definition build:v2 in file c:\\git_repositories\\blinkguard\\code\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_en\r\nv=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:windows in file c:\\git_repositories\\blinkguard\\code\\tensorflow\\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DE\r\nFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_\r\nLEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkop\r\nt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\git_repositories\\blinkguard\\code\\tensorflow\\.bazelrc: --define framework_shared_obj\r\nect=false\r\nINFO: Starting clean.\r\n\r\n(build_tf_apr21) C:\\git_repositories\\BlinkGuard\\code\\tensorflow>bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-\r\nmsse4.1 --copt=-msse4.2 -k //tensorflow/tools/pip_package:build_pip_package\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=152\r\nINFO: Reading rc options for 'build' from c:\\git_repositories\\blinkguard\\code\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/gerri/anaconda3/envs/build_tf_apr21/python.exe\r\nINFO: Reading rc options for 'build' from c:\\git_repositories\\blinkguard\\code\\tensorflow\\.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_to\r\nolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --a\r\nnnounce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --d\r\nefine=with_xla_support=true --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from c:\\git_repositories\\blinkguard\\code\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/gerri/anaconda3/envs/build_tf_apr21/python.exe --action_env PYTHON_LIB_PATH=C:/Users/gerri/anac\r\nonda3/envs/build_tf_apr21/lib/site-packages --python_path=C:/Users/gerri/anaconda3/envs/build_tf_apr21/python.exe --copt=/d2ReducedOptimizeHugeFunctions\r\n --host_copt=/d2ReducedOptimizeHugeFunctions\r\nINFO: Found applicable config definition build:short_logs in file c:\\git_repositories\\blinkguard\\code\\tensorflow\\.bazelrc: --output_filter=DONT_MATCH_AN\r\nYTHING\r\nINFO: Found applicable config definition build:v2 in file c:\\git_repositories\\blinkguard\\code\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_en\r\nv=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:windows in file c:\\git_repositories\\blinkguard\\code\\tensorflow\\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DE\r\nFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_\r\nLEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkop\r\nt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\git_repositories\\blinkguard\\code\\tensorflow\\.bazelrc: --define framework_shared_obj\r\nect=false\r\nDEBUG: C:/users/gerri/_bazel_gerri/wwc5pzrd/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA prop\r\nrietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree t\r\no the terms of the license agreement, do not use the software.\r\nINFO: Repository local_execution_config_python instantiated at:\r\n  C:/git_repositories/blinkguard/code/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  C:/git_repositories/blinkguard/code/tensorflow/tensorflow/workspace2.bzl:1099:19: in workspace\r\n  C:/git_repositories/blinkguard/code/tensorflow/tensorflow/workspace2.bzl:85:27: in _tf_toolchains\r\n  C:/users/gerri/_bazel_gerri/wwc5pzrd/external/tf_toolchains/toolchains/remote_config/configs.bzl:6:28: in initialize_rbe_configs\r\n  C:/users/gerri/_bazel_gerri/wwc5pzrd/external/tf_toolchains/toolchains/remote_config/rbe_config.bzl:158:27: in _tensorflow_local_config\r\nRepository rule local_python_configure defined at:\r\n  C:/git_repositories/blinkguard/code/tensorflow/third_party/py/python_configure.bzl:275:41: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'local_execution_config_python':\r\n   Traceback (most recent call last):\r\n        File \"C:/git_repositories/blinkguard/code/tensorflow/third_party/py/python_configure.bzl\", line 209, column 22, in _create_local_python_reposito\r\nry\r\n                _check_python_bin(repository_ctx, python_bin)\r\n        File \"C:/git_repositories/blinkguard/code/tensorflow/third_party/py/python_configure.bzl\", line 143, column 52, in _check_python_bin\r\n                result = raw_exec(repository_ctx, [get_bash_bin(repository_ctx), \"-c\", cmd])\r\n        File \"C:/git_repositories/blinkguard/code/tensorflow/third_party/remote_config/common.bzl\", line 77, column 26, in get_bash_bin\r\n                bash_bin_path = which(repository_ctx, \"bash\")\r\n        File \"C:/git_repositories/blinkguard/code/tensorflow/third_party/remote_config/common.bzl\", line 27, column 23, in which\r\n                return execute(\r\n        File \"C:/git_repositories/blinkguard/code/tensorflow/third_party/remote_config/common.bzl\", line 219, column 13, in execute\r\n                fail(\r\nError in fail: Repository command failed\r\nINFO: Could not find files for the given pattern(s).\r\nINFO: Repository local_config_python instantiated at:\r\n  C:/git_repositories/blinkguard/code/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  C:/git_repositories/blinkguard/code/tensorflow/tensorflow/workspace2.bzl:1099:19: in workspace\r\n  C:/git_repositories/blinkguard/code/tensorflow/tensorflow/workspace2.bzl:95:21: in _tf_toolchains\r\nRepository rule python_configure defined at:\r\n  C:/git_repositories/blinkguard/code/tensorflow/third_party/py/python_configure.bzl:294:35: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'local_config_python':\r\n   Traceback (most recent call last):\r\n        File \"C:/git_repositories/blinkguard/code/tensorflow/third_party/py/python_configure.bzl\", line 267, column 40, in _python_autoconf_impl\r\n                _create_local_python_repository(repository_ctx)\r\n        File \"C:/git_repositories/blinkguard/code/tensorflow/third_party/py/python_configure.bzl\", line 209, column 22, in _create_local_python_reposito\r\nry\r\n                _check_python_bin(repository_ctx, python_bin)\r\n        File \"C:/git_repositories/blinkguard/code/tensorflow/third_party/py/python_configure.bzl\", line 143, column 52, in _check_python_bin\r\n                result = raw_exec(repository_ctx, [get_bash_bin(repository_ctx), \"-c\", cmd])\r\n        File \"C:/git_repositories/blinkguard/code/tensorflow/third_party/remote_config/common.bzl\", line 77, column 26, in get_bash_bin\r\n                bash_bin_path = which(repository_ctx, \"bash\")\r\n        File \"C:/git_repositories/blinkguard/code/tensorflow/third_party/remote_config/common.bzl\", line 27, column 23, in which\r\n                return execute(\r\n        File \"C:/git_repositories/blinkguard/code/tensorflow/third_party/remote_config/common.bzl\", line 219, column 13, in execute\r\n                fail(\r\nError in fail: Repository command failed\r\nINFO: Could not find files for the given pattern(s).\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -040\r\n0\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  C:/git_repositories/blinkguard/code/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  C:/git_repositories/blinkguard/code/tensorflow/tensorflow/workspace0.bzl:108:34: in workspace\r\n  C:/users/gerri/_bazel_gerri/wwc5pzrd/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories\r\nRepository rule git_repository defined at:\r\n  C:/users/gerri/_bazel_gerri/wwc5pzrd/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nWARNING: errors encountered while analyzing target '//tensorflow/tools/pip_package:build_pip_package': it will not be built\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (234 packages loaded, 4017 targets configured).\r\nINFO: Found 0 targets...\r\nERROR: command succeeded, but not all targets were analyzed\r\nINFO: Elapsed time: 727.072s, Critical Path: 0.03s\r\nINFO: 1 process: 1 internal.\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48264\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48264\">No</a>\n", "resolved. I was missing MSYS2.\r\n\r\nSuggestions:\r\n\r\n1) On https://www.tensorflow.org/install/source link to the instructions for windows. I was following the instructions on the URL linked before which gives the impression as the same steps apply to windows, just that windows is not actively supported. Despite that there is a guide for windows: https://www.tensorflow.org/install/source_windows \r\n2) Print a descriptive error message that MSYS2 is missing. The current error message does not point the user towards what is the exact cause or fix of the error. "]}, {"number": 48257, "title": "The mbed project generation bits might be a bit out of date.", "body": "The mbed project generation bits might be a bit out of date.\r\n\r\nTagging @MatthiasHertel80 to check what the current recommended approach is for using TFLM with mbed.\r\n\r\nFrom my perspective (as a TFLM maintainer, but not the one maintaining the mbed integration), you should be able to rename [this folder](https://github.com/tensorflow/tensorflow/blob/8ddc7459a8f7d2526249f8b9d496d0812a7aff55/tensorflow/lite/micro/examples/micro_speech/arduino/Makefile.inc#L1) to mbed,\r\n\r\nand modify this line:\r\nhttps://github.com/tensorflow/tensorflow/blob/8ddc7459a8f7d2526249f8b9d496d0812a7aff55/tensorflow/lite/micro/examples/micro_speech/disco_f746ng/Makefile.inc#L2\r\n\r\nto check for the target (instead of tage) similar to arduino:\r\nhttps://github.com/tensorflow/tensorflow/blob/8ddc7459a8f7d2526249f8b9d496d0812a7aff55/tensorflow/lite/micro/examples/micro_speech/arduino/Makefile.inc#L1\r\n\r\n\r\nThen `TARGET=mbed OPTIMIZED_KERNEL_DIR=cmsis_nn` should do the trick.\r\n\r\n_Originally posted by @advaitjain in https://github.com/tensorflow/tensorflow/issues/46721#issuecomment-772749626_", "comments": ["@tensorflow/micro\r\n\r\nAgree with @advaitjain on the comment of mbed project generation, as \r\n\r\n`TARGET=mbed` in command line will not enable Makefile to include .cc files in subfolders with the target name of `micro_speech/disco_f746ng/` (or `hello_world/disco_f746ng/`, etc) according to the [Makefile line #643](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/tools/make/Makefile#L643)\r\n\r\n`MICROLITE_CC_SRCS := $(call substitute_specialized_implementations,$(MICROLITE_CC_SRCS),$(TARGET))`", "With [PR #48659 Update micro_speech mbed generation for disco_f746ng](https://github.com/tensorflow/tensorflow/pull/48659) for micro_speech example, and [PR #48740 Fix/stm32f746 examples](https://github.com/tensorflow/tensorflow/pull/48740) plus [Solved issue #48741](https://github.com/tensorflow/tensorflow/issues/48741) for hello_world example, this issue can be closed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48257\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48257\">No</a>\n"]}, {"number": 48256, "title": "Data cardinality", "body": "Hi, I am training a neural network use two inputs with different numbers of images, like this:\r\n\r\ninput1 = Input((500,500,3), name='input1')\r\ninput2 = Input((700, 500, 3,), name='input2')\r\n\r\nl1 = Dense(64, activation='relu')(input1)\r\nl1 = Dense(1, activation='sigmoid', name='output1')(l1)\r\n\r\nl2 = Dense(64, activation='relu')(input2)\r\nl2 = Dense(1, activation='sigmoid', name='output2')(l2)\r\n\r\nmodel = keras.Model(inputs=[input1, input2], outputs=[l1, l2])\r\nkeras.utils.plot_model(model, 'multi_input_and_output_model.png', show_shapes=True)\r\nmodel.compile(optimizer='adam', loss={'output1':keras.losses.MeanSquaredError(),'output2':keras.losses.MeanSquaredError()})\r\nmodel.summary()\r\n\r\nand you can check it by som dummy x1 and x2 but with different first dimension, for example:\r\nx1.shape = (50, 500, 500, 3)\r\ny1.shape = (50,)\r\nx2.shape = (100,700,500,3)\r\ny2.shape = (100,)\r\nmodel.fit({'input1':x1, 'input2':x2}, {'output1':y1, 'output2':y2}, epochs=10)\r\n\r\nI got the ValueError: Data cardinality is ambiguous: x sizes: 50, 100 y sizes: 50, 100 Please provide data which shares the same first dimension.\r\n\r\nCan anyone help me to solve the problem?\r\nAny help will be appreciated!\r\n", "comments": ["@KaterinaBao \r\nYou can have the model train on same number of samples. Thus, as per your example, you will be able to train on 50 examples only. The reason being, that on each step, your model requires a batch of data for *all* the inputs, and not only one of them. Thus, it is not possible to train on different sized datasets that go into a single model.\r\nHope this helps.", "@KaterinaBao,\r\nPlease take a look at @AdityaKane2001's comment and check if it helps. \r\n\r\nAlso, this question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a TensorFlow bug or feature request. There is a larger community that reads questions there. Thanks!", "> @KaterinaBao\r\n> You can have the model train on same number of samples. Thus, as per your example, you will be able to train on 50 examples only. The reason being, that on each step, your model requires a batch of data for _all_ the inputs, and not only one of them. Thus, it is not possible to train on different sized datasets that go into a single model.\r\n> Hope this helps.\r\n\r\nThank, that information really useful", "> @KaterinaBao,\r\n> Please take a look at @AdityaKane2001's comment and check if it helps.\r\n> \r\n> Also, this question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a TensorFlow bug or feature request. There is a larger community that reads questions there. Thanks!\r\n\r\nYes, that's helpful. Thanks", "@KaterinaBao\r\nRequesting to close the issue if the query is resolved.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48256\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48256\">No</a>\n"]}, {"number": 48254, "title": "rsync error with TFLM github CI", "body": "With https://github.com/tensorflow/tensorflow/pull/48241, we started getting an rsync error (after the TFLM CI build passed all the checks):\r\n```\r\nFinished all micro tests at Fri Apr  2 01:09:41 UTC 2021\r\n\r\n\r\n[ID: 3784634] Build finished after 1341 secs, exit value: 0\r\n\r\n\r\nWarning: Permanently added 'localhost' (ECDSA) to the list of known hosts.\r\nrsync: send_files failed to open \"/tmpfs/src/github/tensorflow/tensorflow/lite/micro/tools/make/downloads/flatbuffers/include/flatbuffers/flexbuffers.h\": Permission denied (13)\r\nrsync error: some files/attrs were not transferred (see previous errors) (code 23) at main.c(1677) [generator=3.1.3]\r\n```\r\n\r\nSee this build log for one example:\r\nhttps://source.cloud.google.com/results/invocations/44cbfc69-3cc3-4fbb-90a5-fa500a7c79bb/log\r\n\r\nThe exact reason for this is unclear and it is not reproducible with the internal kokoro builds (which is why #48241 got merged.\r\n\r\nThere doesn't seem to be a good reason to roll-back #48241 so we will attempt a quick fix-forward. If that does not work, #48241 will be rolled back.", "comments": []}]