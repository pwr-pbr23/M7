[{"number": 8663, "title": "R1.1", "body": "r1.1", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->"]}, {"number": 8662, "title": "Support self_adjoint_eig to complex64 and complex128", "body": "This fix adds the complex64 and complex128 support for `self_adjoint_eig`, as was raised in #5190.\r\n\r\nThis fix fixes #5190.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please.", "Hi @martinwicke, great contribution! just ran into this issue myself few days ago.\r\nI've compiled fork with you'r fix, and I think I've found some strange problem,\r\nWhen i try to use tf.self_adjoint_eig over a large tensor of type tf.float64 (large is [2e7, 4, 4]), the work gets distributed between my 8 CPU cores. \r\nBut when do the same with tensor of type tf.complex64 (with the same values) tf uses only single CPU core.\r\nWhat could be the cause of this problem?\r\n\r\nTest code:\r\n\r\n```\r\ndef eig_test(H, w):\r\n    H_tens = tf.constant(H[:, :, np.newaxis], dtype=tf.float64) \r\n    w_tens = tf.constant(w, dtype=tf.float64)\r\n    H_tens = tf.multiply(w_tens, H_tens)\r\n    H_tens = tf.transpose(H_tens, perm=[2, 0, 1])\r\n    \r\n    (e_tens, v_tens) = tf.self_adjoint_eig(H_tens)\r\n    \r\n    sess = tf.Session()\r\n    return sess.run(e_tens)\r\n\r\nH = np.array([[1, 2, 3, 4], [2, 1, 5, 6], [3, 5, 1, 7], [4, 6, 7, 1]])\r\nw = np.random.rand(20000000)\r\n    \r\nstart_time = time()\r\nres = eig_test(H, w)\r\nprint 'duration', time() - start_time\r\n```\r\n\r\nTest results:\r\nwith type tf.complex64 - test duration was 45 sec\r\nwith type tf.float64 - test duration was 10 sec", "@yongtang would you be able to make the requested changes to fix the tests?", "Thanks @rmlarsen for the reminder. I will address the issue shortly.", "@tensorflow-jenkins test this please", "@rmlarsen good to go?", "@drpngx @yongtang I thought @yongtang was going to fix/enable the gradient tests for complex? I guess we can merge this and address it in a followup.", "@rmlarsen you are right. @yongtang any chance you can address this?", "@drpngx Ooops, I missed your update. I agree that it would be better to address before merging.", "Thanks @rmlarsen @drpngx for the review. I am actively looking to fixing the complex issue. I tried to fix the https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_grad.py#L215\r\nas mentioned in the review comment.\r\nHowever, it seems there are other places that needs to be fixed as well and I am still working on it.\r\n\r\nFor that I think it probably make sense to merge this one first. I can create another follow up PR to address the complex gradients issue once I figure out.", "@yongtang thanks for looking into this. It makes sense to do it in a followup. \r\n@drpngx feel free to squash & merge.", "SG, thanks guys!"]}, {"number": 8661, "title": "graph_editor.graph_replace produces WARNING", "body": "Since version 1.0.0 the method `tensorflow.contrib.graph_editor.graph_replace` raises the following warning:\r\n\r\nWARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\r\n\r\n \r\n\r\n### Minimal reproducible example \r\n```python \r\nimport tensorflow as tf\r\nimport tensorflow.contrib.graph_editor as ge\r\na = tf.constant(1)\r\nb = tf.constant(2)\r\nc = tf.constant(3)\r\nd = a +  b\r\ne = ge.graph_replace([d], {a: c})\r\n```\r\n\r\n### Output log\r\nINFO:tensorflow:Copying op: add\r\nWARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\r\nWARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\r\nINFO:tensorflow:Finalizing op: add\r\n\r\n", "comments": ["@purpledog", "Since this is in contrib, we do not officially support this. @purpledog is the original author, but it should be easy to search in the code and find how to switch silence this warning. So a PR would be most welcome. Thank you!", "As Contrib folder is deprecated , this issue is not relevant.closing this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/8661\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/8661\">No</a>\n"]}, {"number": 8660, "title": "Error when creating a summary inside a tf.while_loop in a multi-GPU setup (like CIFAR-10)", "body": "Why diagnosing a problem with batch normalized RNNs, I tried to add a summary of the population statistics in my batch normalized cell, but then when I try to run the merged summaries I get the following error:\r\n\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: The node 'Merge/MergeSummary' has inputs from different frames. The input 'tower_2/rnn/while/multi_rnn_cell/cell_4/gru_cell/gates/r/rnn/multi_rnn_cell/cell_4/gru_cell/gates/r/pop_var_0' is in frame 'tower_2/rnn/while/tower_2/rnn/while/'. The input 'tower_3/rnn/while/multi_rnn_cell/cell_4/gru_cell/gates/r/rnn/multi_rnn_cell/cell_4/gru_cell/gates/r/pop_var_0' is in frame 'tower_3/rnn/while/tower_3/rnn/while/'.\r\n\r\nThis is on TensorFlow 1.0. I'll try to get a reduced testcase.", "comments": ["This only happens when I have multiple towers. Masking GPUs with CUDA_VISIBLE_DEVICES so that only tower_0 is created makes the problem go away.", "@dandelionmane, is it expected that summaries cannot be created in while loops?\r\n", "I have the same problem when I use `summary.merge` for variables which were created with `while_loop `\r\n\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: The node 'Merge/MergeSummary' has inputs from different frames. The input 'tower_0/while/o_rnn_3/kern/HistogramSummary_1' is in frame 'tower_0/while/tower_0/while/'. The input 'g_embatch_size/emb_x_0' is in frame ''", "@itsmeolivia, why was this closed? It looks like a similar but different problem than #10059.", "oops, the tags were wrong. reopening and re-tagging", "Yes, this is a known issue that conditional ops don't play well with summary ops. \r\n@alextp @josh11b @ali01 \r\n\r\nI'm removing the `tensorboard` tag because it's not a tensorboard issue, it's a problem with core tensorflow.", "when I used summary.merge(loss) to generate a summary string, it raise the type error 'Tensor' object is not iterable. ' It seems while loop and any iterator are not supported by the merge method? is there any solutions, or alternatives?", "@cuter9 The [document](https://www.tensorflow.org/api_docs/python/tf/summary/merge) says: inputs: A list of __string Tensor__ objects containing serialized __Summary protocol buffers__.", "The following is my scrips of a function which is called iteratively thru for loop, still don't work, and raise type error \" 'Tensor' object is not iterable\", and the dubuger points to the problem raising from tf.summary.merge(x_summary) statement.\r\n\r\ndef summary_4_graph(data_x):\r\n    x_summary = tf.summary.scalar(data_x.name, data_x)\r\n    x_summary_str = tf.summary.merge(x_summary)\r\n    return x_summary_str", "A __list__ of string Tensor objects containing serialized Summary protocol buffers.", "done! thanks!", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Closing since it looks like there is a resolution."]}, {"number": 8659, "title": "My tensorboard can not show anything. ", "body": "(The code didn't show any errors and my Brower is firefox)What's wrong with it ?", "comments": ["You really need to post more information. Can you use the template provided by the tensorflow folks to give an idea of the issue.\r\nYou are creating summaries etc. aren't you? Please give us some environment info, tf version and your code.", "Closing the issue due to inactivity."]}, {"number": 8658, "title": "Saving best models instead of most recent models with tf.train.Saver.", "body": "Most of the time I want to save the best models instead of the most recent models. Doing so using tf.train.Saver requires to choose when to save a model and to delete the worst model (which might not be the oldest) \"manually\".\r\n\r\nA method to save the N best models (according to some user defined value) would be nice.", "comments": ["@skye I'd like to work on this. Is this a beginner-friendly feature?", "Hi @sahildua2305 . I haven't worked on the Saver code myself, but looking through it this seems like a reasonable project for someone new to TensorFlow (assuming you have prior Python experience already). I think this will be a small-to-medium-sized change, and that the tricky part will be refactoring the existing code to allow both kinds of deletion modes (don't forget to maintain backwards API compatibility). Apologies if I'm sending you down a rabbit hole :) Looking forward to a PR!", "@skye I see! I do have good Python as well as OSC experience.\r\n\r\nIf you think there's another better issue for a beginner (to Tensorflow), please tag me there.", "@sherrym ", "Another possibility is to have a tool that runs independent of Saver. IE, something that accepts current save path, an evaluation function and `best_checkpoints` target path. Like an eval script, it would run in a loop, each time evaluating latest checkpoint, and copying it into \"best_checkpoints\" directory if necessary.\r\n\r\n@vrv posted some guidelines one when it makes sense to integrate something into core library in https://github.com/tensorflow/tensorflow/pull/9305#issuecomment-298469132\r\n\r\nAbout integrating something like this into saver, the \"Saver\" would need to know how \"good\" a model is, so this is adding neural networks-specific functionality into a module that can be used more generally to save some variables.", "It may also be something that a monitored session hook should do, if one doesn't already exist. ", "I doubt that Saver is the right spot -- the first layer of abstraction that would understand metrics and evaluation results is Estimator. We're working on a solution that allows users to have their best checkpoint in a training run kept and exported.", "Hi @martinwicke !\r\nIs there any progress on this feature? \r\nIf not, is it possible to write \"Save Best Model Hook\" for TF.Estimator to solve this issue?", "It is absolutely possible. The easiest would be to use `tf.estimator.train_and_evaluate` with your `Estimator`,  and add an `Exporter` in our `EvalSpec`. The [`Exporter`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/exporter.py) is a simple class which\u00a0you derive from to implement whatever behavior you want. It receives each eval result and can decide based on that whether to perform an export (or not). It can also decide what \"export\" means. So, if you want to save the best checkpoints, you would keep the best eval results as state, and if you see a better one, overwrite your exported checkpoint with the current one.", "@martinwicke Do you mind giving a detailed tutorial(lines of code) for this method? It's still unclear to me how to implement this.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "I just wrote a light-weight class to handle exactly this use case.  It should be very easy to drop in.  No Estimator necessary.  I have a paper deadline end of this week.  Early next week I'll move the code into a public repo and post a link in here.", "So... I think it works.\r\n\r\nhttps://github.com/vonclites/checkmate\r\n\r\nDefinitely not a professional product.", "@martinwicke I tried extending the Exporter class but I can only use it to export the model to the SavedModel format which I cannot use to restore and continue training. Am I missing something? I also do not have access to the Estimator session so I can not use a Saver.", "The `Exporter` gets the checkpoint, so you shouldn't need to use the `Saver`.  \r\n\r\nYou can do with the checkpoint whatever you want (see e.g. @vonclites' solution, which is nice).", "Any updates on this feature and whether it will be included in tensorflow?", "This makes my life a bit easier.\r\nhttps://www.tensorflow.org/api_docs/python/tf/estimator/BestExporter\r\n", "> This makes my life a bit easier.\r\n> https://www.tensorflow.org/api_docs/python/tf/estimator/BestExporter\r\n\r\nDoes BestExporter work with TPUs ?", "I'm new to tensorflow and have had trouble running inference on saved models (using the object detection API).  I don't have any trouble using an inference graph exported using the export tool, so I just want to collect the model files for the best checkpoints.\r\n\r\nAnyhow, I threw together a little exporter that simply copies checkpoint files from the model directory based on an eval metric and comparison function (and cleans up worse performing ones, maintaining up to the given number of checkpoints): https://github.com/bluecamel/best_checkpoint_copier", "```python\r\nimport tensorflow as tf\r\nimport tensorflow.logging as logging\r\n\r\n\r\nclass BestExporter(tf.estimator.BestExporter):\r\n\r\n    def export(self, estimator, export_path, checkpoint_path, eval_result,\r\n               is_the_final_export):\r\n        if self._best_eval_result is None or \\\r\n                self._compare_fn(self._best_eval_result, eval_result):\r\n            logging.info(\r\n                'Exporting a better model ({} instead of {})...'.format(\r\n                    eval_result, self._best_eval_result))\r\n            result = self._saved_model_exporter.export(\r\n                estimator, export_path, checkpoint_path, eval_result,\r\n                is_the_final_export)\r\n            self._best_eval_result = eval_result\r\n            self._garbage_collect_exports(export_path)\r\n            return result\r\n        else:\r\n            logging.info(\r\n                'Keeping the current best model ({} instead of {}).'.format(\r\n                    self._best_eval_result, eval_result))\r\n\r\n# tf.estimator.EvalSpec(\u2026, exporters=[BestExporter(\u2026)])\r\n```", "Is there a solution to keep the best Checkpoints and not use the SavedModel format, all while using the Estimator API ?", "@laetitaoist just use @Ivanukhov's solution but just copy `checkpoint_path` instead of exporting. \r\n\r\nI'll close this issue, I think the solution is exactly what was initially asked for. ", "This is working great! My modified class where I just keep all of the best checkpoints copied to another folder : \r\n\r\n```python\r\nimport shutil, glob, os\r\nimport tensorflow as tf\r\nimport tensorflow.logging as logging\r\n\r\nclass BestCheckpointsExporter(tf.estimator.BestExporter):\r\n\r\n    def export(self, estimator, export_path, checkpoint_path, eval_result,\r\n               is_the_final_export):\r\n        if self._best_eval_result is None or \\\r\n                self._compare_fn(self._best_eval_result, eval_result):\r\n            tf.logging.info(\r\n                'Exporting a better model ({} instead of {})...'.format(\r\n                    eval_result, self._best_eval_result))\r\n            # copy the checkpoints files *.meta *.index, *.data* each time there is a better result, no cleanup for max amount of files here\r\n            for name in glob.glob(checkpoint_path + '.*'):\r\n                shutil.copy(name, os.path.join(BEST_CHECKPOINTS_PATH, os.path.basename(name)))\r\n           # also save the text file used by the estimator api to find the best checkpoint\r\n            with open(os.path.join(BEST_CHECKPOINTS_PATH, \"checkpoint\"), 'w') as f:\r\n                f.write(\"model_checkpoint_path: \\\"{}\\\"\".format(os.path.basename(checkpoint_path))))\r\n            self._best_eval_result = eval_result\r\n        else:\r\n            tf.logging.info(\r\n                'Keeping the current best model ({} instead of {}).'.format(\r\n                    self._best_eval_result, eval_result))\r\n```\r\n", "> This is working great! My modified class where I just keep all of the best checkpoints copied to another folder :\r\n> \r\n> ```python\r\n> import shutil, glob, os\r\n> import tensorflow as tf\r\n> import tensorflow.logging as logging\r\n> \r\n> class BestCheckpointsExporter(tf.estimator.BestExporter):\r\n> \r\n>     def export(self, estimator, export_path, checkpoint_path, eval_result,\r\n>                is_the_final_export):\r\n>         if self._best_eval_result is None or \\\r\n>                 self._compare_fn(self._best_eval_result, eval_result):\r\n>             tf.logging.info(\r\n>                 'Exporting a better model ({} instead of {})...'.format(\r\n>                     eval_result, self._best_eval_result))\r\n>             # copy the checkpoints files *.meta *.index, *.data* each time there is a better result, no cleanup for max amount of files here\r\n>             for name in glob.glob(checkpoint_path + '.*'):\r\n>                 shutil.copy(name, os.path.join(BEST_CHECKPOINTS_PATH, os.path.basename(name)))\r\n>            # also save the text file used by the estimator api to find the best checkpoint\r\n>             with open(os.path.join(BEST_CHECKPOINTS_PATH, \"checkpoint\"), 'w') as f:\r\n>                 f.write(\"model_checkpoint_path: \\\"{}\\\"\".format(os.path.basename(checkpoint_path))))\r\n>             self._best_eval_result = eval_result\r\n>         else:\r\n>             tf.logging.info(\r\n>                 'Keeping the current best model ({} instead of {}).'.format(\r\n>                     self._best_eval_result, eval_result))\r\n> ```\r\n\r\nDo you have a tutorial on how to call this class method in the train.py?", "> ```python\r\n> import tensorflow as tf\r\n> import tensorflow.logging as logging\r\n> \r\n> \r\n> class BestExporter(tf.estimator.BestExporter):\r\n> \r\n>     def export(self, estimator, export_path, checkpoint_path, eval_result,\r\n>                is_the_final_export):\r\n>         if self._best_eval_result is None or \\\r\n>                 self._compare_fn(self._best_eval_result, eval_result):\r\n>             logging.info(\r\n>                 'Exporting a better model ({} instead of {})...'.format(\r\n>                     eval_result, self._best_eval_result))\r\n>             result = self._saved_model_exporter.export(\r\n>                 estimator, export_path, checkpoint_path, eval_result,\r\n>                 is_the_final_export)\r\n>             self._best_eval_result = eval_result\r\n>             self._garbage_collect_exports(export_path)\r\n>             return result\r\n>         else:\r\n>             logging.info(\r\n>                 'Keeping the current best model ({} instead of {}).'.format(\r\n>                     self._best_eval_result, eval_result))\r\n> \r\n> # tf.estimator.EvalSpec(\u2026, exporters=[BestExporter(\u2026)])\r\n> ```\r\n\r\nHi, do you have a tutorial on how to call this class method in (TFOD API) train.py file?", "I use this BestCheckpointsExporter in EvalSpec like this:\r\n```\r\nbest_exporter = BestCheckpointsExporter(serving_input_receiver_fn=serving_input_receiver_fn)\r\neval_spec = tf.estimator.EvalSpec(input_fn=eval_fn.run, exporters=best_exporter)\r\ntf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n```\r\n\r\nThis BestCheckpointsExporter class is very basic, You can modify it to save the top N best checkpoints for example.", "> I use this BestCheckpointsExporter in EvalSpec like this:\r\n> \r\n> ```\r\n> best_exporter = BestCheckpointsExporter(serving_input_receiver_fn=serving_input_receiver_fn)\r\n> eval_spec = tf.estimator.EvalSpec(input_fn=eval_fn.run, exporters=best_exporter)\r\n> tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n> ```\r\n> This BestCheckpointsExporter class is very basic, You can modify it to save the top N best checkpoints for example.\r\n\r\nThanks for your prompt reply. May i know did you ever add this block of codes to TensorFlow Object Detection API? If yes, where do you modify?", "I don't know this project but if they use the Estimator API during training it should be easy to drop in.", "I can not understand why we need the serving input receiver to save the best model based on validation error. would please describe what is it for?\r\n\r\ncan you put the complete code of how should we create that function and use?\r\n\r\nalso, how can I load the saved model in another computer to predict?", "> \u6211\u5728EvalSpec\u4e2d\u4f7f\u7528\u6b64BestCheckpointsExporter\uff0c\u5982\u4e0b\u6240\u793a\uff1a\r\n> \r\n> ```\r\n> best_exporter = BestCheckpointsExporter(serving_input_receiver_fn=serving_input_receiver_fn)\r\n> eval_spec = tf.estimator.EvalSpec(input_fn=eval_fn.run, exporters=best_exporter)\r\n> tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n> ```\r\n> \r\n> \u8be5BestCheckpointsExporter\u7c7b\u975e\u5e38\u57fa\u7840\uff0c\u4f8b\u5982\uff0c\u60a8\u53ef\u4ee5\u5bf9\u5176\u8fdb\u884c\u4fee\u6539\u4ee5\u4fdd\u5b58\u524dN\u4e2a\u6700\u4f73\u68c0\u67e5\u70b9\u3002\r\n\r\n@laetitiaoist What does \u2018serving_input_receiver_fn\u2019 mean here\uff0cHow do I set it up\u3002\r\nBestCheckpointsExporter  and BestExporter Is it the same thing?", "Hi! My serving_input_receiver_fn for the best checkpoints exporter was like that :\r\nBut the tensorflow 2 export script tells me I need to replace tf.placeholder.\r\nHow can I update this to work with tensorflow 2.0 without needing compat v1 symbols?\r\n\r\n@martinwicke \r\n```python\r\ndef serving_input_receiver_fn(image_shape):\r\n    input_images = tf.placeholder(tf.float32, [None] + image_shape)\r\n    features = input_images\r\n    receiver_tensors = input_images\r\n    return tf.estimator.export.TensorServingInputReceiver(features, receiver_tensors)\r\n\r\n\r\n\r\n", "I *believe* that you should be able to use \r\n\r\n```python\r\ndef serving_input_receiver_fn(image_shape):\r\n  input_images = tf.zeros([7] + image_shape, tf.float32)  # 7 is arbitrary and will be eliminated\r\n  return tf.estimator.export.build_raw_serving_input_receiver_fn(input_images)\r\n```\r\n\r\nNow, that still uses placeholders internally, but you don't have to have any in your code.", "@martinwicke On tensorflow 1.15 I needd to switch the first parameters :of the first line\r\n\r\ndef serving_input_receiver_fn(image_shape):\r\n  input_images = tf.constant([7] + image_shape, tf.float32)  # 7 is arbitrary and will be eliminated\r\n  return tf.estimator.export.build_raw_serving_input_receiver_fn(input_images)\r\n\r\nBut what is this 7? This is not very clear to me. It could be any number?", "Ah, sorry for the mistake. I corrected it above. You should use zeros, not constant, and flip the args.\r\n\r\nThe 7 is the batch size, but when creating the serving graph, the batch size will be eliminated anyway, so you can safely use any number you want, it won't affect the result at all.\r\n", "Hi all,\r\n\r\nThis is a \"trick\" I found to save the best checkpoints.\r\nI am writing a small code which regularly downloads a specfic loss graphic in tensorbord page. The downloaded data are stored in text file in JSONArray format. Each JSON has : \r\n-  time\r\n-  step\r\n-  loss\r\n\r\nFor instance, if you are interested in the \"Total loss\", use this curl command (make sure tensorboard is launched and working !):\r\ncurl \"http://IP:PORT/data/plugin/scalars/scalars?tag=Losses%2FTotalLoss&run=.&experiment=\" >> TotalLoss.txt\r\nIt is the same link you use when you want to download the JSON directly from your web page (\"right click\", \"copy link\"s adress\")\r\n\r\nThe \"hard\" part is to find a good JSON parser to consume all the data, and to regularly copy the best chekpoint to another folder ;-)\r\nPS : I'm using TF 1.12", "Hi @laetitiaoist \r\nI try your `serving_input_receiver_fn` function but i got this error :\r\n```\r\nTypeError: serving_input_receiver_fn() takes exactly 1 argument (0 given)\r\n```\r\nin Object Detection API (when training with main.py)\r\nin file model_lib.py  I add the class BestExporter as above \r\nand i change\r\n```\r\nexporter = tf.estimator.FinalExporter -> exporter = BestExporter\r\n```\r\nwith your function as `serving_input_receiver_fn=serving_input_receiver_fn`\r\ndo you know what I did wrong?\r\n\r\nI want to export the model with input_type as image_tensor \r\nThanks for the help ", "Hi! I actually changed this BestCheckpointsExporter class to inherit directly from tf.estimator.Exporter. Because I am only exporting checkpoints and not SavedModel, I don't need the serving_input_receiver_fn anymore.\r\n@mbenami I think you can fix your problem by calling :\r\nserving_input_receiver_fn=serving_input_receiver_fn(image_shape) ", "Hi @laetitiaoist \r\nthanks for the quick replay \r\nI had a typo in the calling function \r\n\r\nanyway I create new `serving_input_receiver_fn` \r\nas I had other issues with the export of the best model as \r\nsaved_model.pb with input as image_tensor \r\nif someone needs this you can find it in this issue :\r\nhttps://github.com/tensorflow/models/issues/4941\r\nas it more related to the object detection API \r\n\r\nand its work locally and on cloud ", "> ```python\r\n> import tensorflow as tf\r\n> import tensorflow.logging as logging\r\n> \r\n> \r\n> class BestExporter(tf.estimator.BestExporter):\r\n> \r\n>     def export(self, estimator, export_path, checkpoint_path, eval_result,\r\n>                is_the_final_export):\r\n>         if self._best_eval_result is None or \\\r\n>                 self._compare_fn(self._best_eval_result, eval_result):\r\n>             logging.info(\r\n>                 'Exporting a better model ({} instead of {})...'.format(\r\n>                     eval_result, self._best_eval_result))\r\n>             result = self._saved_model_exporter.export(\r\n>                 estimator, export_path, checkpoint_path, eval_result,\r\n>                 is_the_final_export)\r\n>             self._best_eval_result = eval_result\r\n>             self._garbage_collect_exports(export_path)\r\n>             return result\r\n>         else:\r\n>             logging.info(\r\n>                 'Keeping the current best model ({} instead of {}).'.format(\r\n>                     self._best_eval_result, eval_result))\r\n> \r\n> # tf.estimator.EvalSpec(\u2026, exporters=[BestExporter(\u2026)])\r\n> ```\r\n\r\n@IvanUkhov  sir, your code is a great work. Would you mind showing more detail about this class and how to initialize it ? Hope for it", "Found the solution. In `object_detection/model_lib.py` file, `create_train_and_eval_specs` function can be modified to include the best exporter:\r\n\r\n```python\r\nfinal_exporter = tf.estimator.FinalExporter(\r\n    name=final_exporter_name, serving_input_receiver_fn=predict_input_fn)\r\n\r\nbest_exporter = tf.estimator.BestExporter(\r\n    name=\"best_exporter\",\r\n    serving_input_receiver_fn=predict_input_fn,\r\n    event_file_pattern='eval_eval/*.tfevents.*',\r\n    exports_to_keep=5)\r\nexporters = [final_exporter, best_exporter]\r\n\r\ntrain_spec = tf.estimator.TrainSpec(\r\n    input_fn=train_input_fn, max_steps=train_steps)\r\n\r\neval_specs = [\r\n    tf.estimator.EvalSpec(\r\n        name=eval_spec_name,\r\n        input_fn=eval_input_fn,\r\n        steps=eval_steps,\r\n        exporters=exporters)\r\n]\r\n```", "> This is working great! My modified class where I just keep all of the best checkpoints copied to another folder :\r\n> \r\n> ```python\r\n> import shutil, glob, os\r\n> import tensorflow as tf\r\n> import tensorflow.logging as logging\r\n> \r\n> class BestCheckpointsExporter(tf.estimator.BestExporter):\r\n> \r\n>     def export(self, estimator, export_path, checkpoint_path, eval_result,\r\n>                is_the_final_export):\r\n>         if self._best_eval_result is None or \\\r\n>                 self._compare_fn(self._best_eval_result, eval_result):\r\n>             tf.logging.info(\r\n>                 'Exporting a better model ({} instead of {})...'.format(\r\n>                     eval_result, self._best_eval_result))\r\n>             # copy the checkpoints files *.meta *.index, *.data* each time there is a better result, no cleanup for max amount of files here\r\n>             for name in glob.glob(checkpoint_path + '.*'):\r\n>                 shutil.copy(name, os.path.join(BEST_CHECKPOINTS_PATH, os.path.basename(name)))\r\n>            # also save the text file used by the estimator api to find the best checkpoint\r\n>             with open(os.path.join(BEST_CHECKPOINTS_PATH, \"checkpoint\"), 'w') as f:\r\n>                 f.write(\"model_checkpoint_path: \\\"{}\\\"\".format(os.path.basename(checkpoint_path))))\r\n>             self._best_eval_result = eval_result\r\n>         else:\r\n>             tf.logging.info(\r\n>                 'Keeping the current best model ({} instead of {}).'.format(\r\n>                     self._best_eval_result, eval_result))\r\n> ```\r\n\r\nDear iteal,\r\n   your codes are very nice.\r\n   But, I am very new to tensorflow, I don't know how to apply your codes into tf.Estimator, could you share us a full codes application example in tf1.*?\r\nthanks very much!!!", "\r\n\r\n\r\n> This is working great! My modified class where I just keep all of the best checkpoints copied to another folder :\r\n> \r\n> ```python\r\n> import shutil, glob, os\r\n> import tensorflow as tf\r\n> import tensorflow.logging as logging\r\n> \r\n> class BestCheckpointsExporter(tf.estimator.BestExporter):\r\n> \r\n>     def export(self, estimator, export_path, checkpoint_path, eval_result,\r\n>                is_the_final_export):\r\n>         if self._best_eval_result is None or \\\r\n>                 self._compare_fn(self._best_eval_result, eval_result):\r\n>             tf.logging.info(\r\n>                 'Exporting a better model ({} instead of {})...'.format(\r\n>                     eval_result, self._best_eval_result))\r\n>             # copy the checkpoints files *.meta *.index, *.data* each time there is a better result, no cleanup for max amount of files here\r\n>             for name in glob.glob(checkpoint_path + '.*'):\r\n>                 shutil.copy(name, os.path.join(BEST_CHECKPOINTS_PATH, os.path.basename(name)))\r\n>            # also save the text file used by the estimator api to find the best checkpoint\r\n>             with open(os.path.join(BEST_CHECKPOINTS_PATH, \"checkpoint\"), 'w') as f:\r\n>                 f.write(\"model_checkpoint_path: \\\"{}\\\"\".format(os.path.basename(checkpoint_path))))\r\n>             self._best_eval_result = eval_result\r\n>         else:\r\n>             tf.logging.info(\r\n>                 'Keeping the current best model ({} instead of {}).'.format(\r\n>                     self._best_eval_result, eval_result))\r\n> ```\r\n\r\n@ iteal Where is `BEST_CHECKPOINTS_PATH` defined? How shall we pass `BEST_CHECKPOINTS_PATH` to `BestCheckpointsExporter`?", "Hi @marchss ! This is a global variable in order to keep this code sample short, but you can make a constructor to pass a variable."]}, {"number": 8657, "title": "Numeric stability CPU vs GPU", "body": "### Numerical stability CPU vs GPU\r\nMy lab partner and I found that in some cases, our scripts would run into numerical instability issues when using GPU, while they would run fine using a CPU. To minimally reproduce this issue, we've come up with the following script:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    t = tf.placeholder(tf.float32, [])\r\n    division = tf.constant(1, dtype='float32') / tf.pow(1.5, -t)\r\n\r\n    sess = tf.Session()\r\n    for count in range(100000000):\r\n        val = sess.run(division, feed_dict={t: count})\r\n\r\n        if np.isnan(val) or np.isinf(val) or val < 0:\r\n            print(count)\r\n            break\r\n\r\n```\r\nIf we run this with CUDA_VISIBLE_DEVICES=\"\", we see that the script reaches 219 steps before being either `nan` or `inf`. If we use GPU instead, we see that the script reaches **only 216** steps before being either `nan` or `inf`.\r\n\r\n### Operating system\r\nUbuntu 16.04\r\n\r\nInstalled version of CUDA and cuDNN:\r\n```\r\n-rw-r--r-- 1 root root   560184 sep 11  2016 /usr/local/cuda-8.0/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 sep 11  2016 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 sep 11  2016 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\r\n-rwxr-xr-x 1 root root   394472 sep 11  2016 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.27\r\n-rw-r--r-- 1 root root   737516 sep 11  2016 /usr/local/cuda-8.0/lib64/libcudart_static.a\r\n-rwxr-xr-x 1 root root 79337624 nov  5 12:57 /usr/local/cuda-8.0/lib64/libcudnn.so\r\n-rwxr-xr-x 1 root root 79337624 nov  5 12:57 /usr/local/cuda-8.0/lib64/libcudnn.so.5\r\n-rwxr-xr-x 1 root root 79337624 nov  5 12:57 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5\r\n-rw-r--r-- 1 root root 69756172 nov  5 12:57 /usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n```\r\nTensorflow version (from binary):\r\n0.12.0", "comments": ["This is as expected. Since they are different devices with different implementations, they are going to be slightly different. Any robust machine learning applications needs to be robust enough to handle this. If not, there is no hope that perturbations of training data will work at all."]}, {"number": 8656, "title": "Inconsistent behaviour between CPU and GPU gradient step operation", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nNone\r\n\r\n### Environment info\r\nOperating System: Linux Mint 17.2 Rafaela\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\nls -l /usr/local/cuda/lib64/libcud*\r\n/usr/local/cuda/lib64/libcudadevrt.a\r\n/usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\n/usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61\r\n /usr/local/cuda/lib64/libcudart.so.8.0.61\r\n/usr/local/cuda/lib64/libcudart_static.a\r\n/usr/local/cuda/lib64/libcudnn.so\r\n/usr/local/cuda/lib64/libcudnn.so.5\r\n/usr/local/cuda/lib64/libcudnn.so.5.1.10\r\n/usr/local/cuda/lib64/libcudnn_static.a\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n1.1.0-rc0\r\nThis bug appeared also on the current TF 1.0 Release when installed via pip install tensorflow-gpu\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\ngit rev-parse HEAD: 49380d638bdc2983722c9a2831ca74770dc6ba43\r\n2. The output of `bazel version`\r\nBuild label: 0.4.5\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\r\n\r\nimport tensorflow as tf\r\na = tf.Variable(1.0)\r\nloss = (a-2.0)**2\r\noptimizer = tf.train.GradientDescentOptimizer(1.0)\r\ntrain_op = optimizer.minimize(loss)\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nprint(sess.run([train_op,a]))\r\nprint(sess.run(a))\r\n```\r\nThe two print statements evaluate to \r\n```\r\n[None, 3.0]\r\n3.0\r\n```\r\n\r\nWhen allowing GPU computation to happen by commenting out the second line above, the two print statements evaluate to:\r\n\r\n```\r\n[None, 1.0]\r\n3.0\r\n```\r\nSo apparently when using the GPU, the Variable `a` is evaluated before the gradient op is executed - and the other way around on CPU. I am not entirely sure what the desired behaviour is supposed to be, but I'm pretty sure they should not be inconsistent. \r\n\r\n### What other attempted solutions have you tried?\r\nA few things I have observed:\r\n```\r\nimport os\r\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\r\n\r\nimport tensorflow as tf\r\n\r\nwith tf.device(\"/cpu:0\"):\r\n    a = tf.Variable(1.0)\r\nloss = (a-2.0)**2\r\noptimizer = tf.train.GradientDescentOptimizer(1.0)\r\ntrain_op = optimizer.minimize(loss)\r\ninit_op = tf.global_variables_initializer()\r\n\r\nsess = tf.Session()\r\nsess.run(init_op)\r\nprint(sess.run([train_op,a]))\r\nprint(sess.run(a))\r\n```\r\nevaluates to \r\n```\r\n[None, 3.0]\r\n3.0\r\n```\r\n\r\nThe following code\r\n```\r\nimport os\r\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\r\n\r\nimport tensorflow as tf\r\n\r\na = tf.Variable(1.0)\r\nloss = (a-2.0)**2\r\noptimizer = tf.train.GradientDescentOptimizer(1.0)\r\ntrain_op = optimizer.minimize(loss)\r\ninit_op = tf.global_variables_initializer()\r\nwith tf.control_dependencies([train_op]):\r\n    a = tf.identity(a)\r\nsess = tf.Session()\r\nsess.run(init_op)\r\nprint(sess.run(a))\r\n```\r\nevaluates to \r\n`3.0`. This seems to \"enforce\" the behaviour of CPU-only computation when using the GPU.\r\n\r\n\r\nAlso the initial example has been run on three different machines, all with the same TitanX GPU Model.\r\n\r\nWhat am I missing? Any help would be greatly appreciated.\r\n\r\nMatthias\r\n", "comments": ["That's not good. I might take a look later.\r\n\r\n@rmlarsen FYI\r\n\r\n@gunan ", "I think this is not a bug:  Consider what the data flow graph looks like, and the fact that fetching [train_op, a] says nothing about which value of 'a' should be fetched: the one before train_op complete or after it completes (fetching [train_op, a] is the same as [a, train_op]).  Because there is no contract, there is no inconsistency.  Due to the timing of the execution of the graphs, my guess is you're likely to get one answer more than another in a given configuration, but that's not a guarantee.\r\n\r\nThe control dependency enforces the order, so I'd expect that to be consistent no matter what devices ops are on.\r\n\r\nAt least, that's what I am interpreting from the code.\r\n", "@vrv is right, the code is not the same! You need to have the same `tf.control_dependencies`. Closing. Feel free to reopen if the error persists.", "I see. By not exactly specifying the computation graph, there is ambiguity.\r\nStill I wonder if this ambiguity should be resolved the same way in both configurations. \r\n"]}, {"number": 8655, "title": "Bazel does not support execution of Python interpreters via labels yet", "body": "I following tutorial https://www.tensorflow.org/tutorials/image_retraining\r\nwhen I try to use the retrainer I got this error\r\nBazel does not support execution of Python interpreters via labels yet error\r\n\r\nWhat can I do ?\r\n![screen shot 2017-03-23 at 6 10 04 pm](https://cloud.githubusercontent.com/assets/18695558/24242844/0fa487aa-0ff4-11e7-9cf6-3fe8cadde0ae.png)\r\n\r\n\r\nMy setting:\r\nPython 2.7.13\r\nTensorflow Release 1.0.1\r\nbazel Build label: 0.4.5-homebrew\r\n", "comments": ["@jart, do you have any ideas on this?", "@aselle I already fixed by changing bazel-bin/tensorflow/examples/image_retraining/retrain \r\nin line 51 to return PYTHON_BINARY. ", "@star277hk Can you tell me how you fixed this? Line 51 on tensorflow/tensorflow/examples/image_retraining/retrain.py is a comment", "Got it, I had to remove the first slash from PYTHON_BINARY in bazel-bin/tensorflow/path/to/executable", "As above comments mentioned, this error message occurs when `PYTHON_BINARY` starts with `//` . This happened for me as my python binary was `//anaconda/bin/python`. \r\n"]}, {"number": 8654, "title": "AttributeError: module 'tensorflow' has no attribute 'layers'", "body": "Hi there,\r\n\r\nThe source code of tutorial\r\n* https://www.tensorflow.org/tutorials/layers\r\n* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py\r\n\r\ncannot run on my MacAir.\r\n\r\nIt just reported: AttributeError: module 'tensorflow' has no attribute 'layers'\r\nIs that a version problem?", "comments": ["Can you please post the output of `python -c 'import tensorflow as tf; print(tf.__version__)' `\r\n\r\nLayers was only recently introduced out of contrib. Please post output of above and we can go from there.", "OK. That is 0.12.1", "It looks like you are using tutorials that require TensorFlow 1.0 or later. Either upgrading to the latest version of TensorFlow, or using a tutorial from the 0.12 branch should fix things.", "Hello,\r\n            I am using Mac OS. Currently I have tensorflow 0.12 running on my laptop. Can you help me how to upgrade tensorflow to latest version, which has attribute 'layers' in it.\r\n", "For me it is 2.0.0 but still that problem ", "**AttributeError: module 'tensorflow' has no attribute 'layers'** means that tensorflow has not this kind of any command in that. it might have two cases.\r\n\r\n1. your syntax may wrong. (for example with new update in tensorflow syntax of layers may change)\r\n in that case syntax is : `tf.keras.layers.Layer`  for tensorflow 2.1\r\n\r\n2. If your syntax is right then you might have installation problem or path error. If you use GPU version, then check version of CUDA, CdNN.\r\n\r\nin my case I install wrong CUDA version.\r\ncuda 10.1\r\npython 3.7.\r\nIt might be helpful to @Hasnat65 @shuuchen "]}, {"number": 8653, "title": "Add the sched_getaffinity support for Android.", "body": "Now the NumSchedulableCPUs() always chooses to  return 4 cores for Android, other than try the Bionic supported sched_getaffinity to get the actual available cores. Here I remove the \"!defined(__ANDROID__)\" macro and just keep the \" __linux__\" macro to work for both pure-linux and android-linux platform. \r\nThe reason why we can drop the \"__ANDROID__\" marco is that the GNU toolchain would always has the system-specific predefined macros \"__linux__\"  setting to 1. \r\n ", "comments": ["Can one of the admins verify this patch?", "1. Information about the GNU toolchain's \"system-specific predefined macros\" can be get from:\\\r\n[https://gcc.gnu.org/onlinedocs/cpp/System-specific-Predefined-Macros.html](url)\r\n\r\n2. How can we infer that the __linux__ is always set to 1 by the GNU cpp?\r\n\r\n(1) gcc-4.9/gcc/c-family/c-cppbuiltin.c: in c_cpp_builtins()\r\n`1041   TARGET_OS_CPP_BUILTINS ();`\r\n\r\n(2) gcc-4.9/gcc/config/aarch64/aarch64-linux-android.h: \r\n`#undef TARGET_OS_CPP_BUILTINS\r\n#define TARGET_OS_CPP_BUILTINS()                \\\r\n  do                                            \\\r\n    {                                           \\\r\n        GNU_USER_TARGET_OS_CPP_BUILTINS();      \\\r\n        ANDROID_TARGET_OS_CPP_BUILTINS();       \\\r\n    }                                           \\\r\n  while (0)\r\n`\r\n\r\n(3) gcc-4.9/gcc/config/linux.h:\r\n`#define GNU_USER_TARGET_OS_CPP_BUILTINS()                       \\\r\n    do {                                                        \\\r\n        if (OPTION_GLIBC)                                       \\\r\n          builtin_define (\"__gnu_linux__\");                     \\\r\n        builtin_define_std (\"linux\");                           \\\r\n        builtin_define_std (\"unix\");                            \\\r\n        builtin_assert (\"system=linux\");                        \\\r\n        builtin_assert (\"system=unix\");                         \\\r\n        builtin_assert (\"system=posix\");                        \\\r\n    } while (0)\r\n`\r\n\r\n(4) gcc-4.9/gcc/c-family/c-cppbuiltin.c: in builtin_define_std() routine, the \"linux\" macro will be converted to \"__linux__\"\r\n", "Maybe pete@petewarden.com<mailto:pete@petewarden.com> can help to review.\r\nThanks!\r\n\r\n\r\n-------------------------------------------------------------------------------------------------------------------------------------\r\nTHIS E-MAIL INCLUDING ANY ATTACHMENTS CONTAINS SPREADTRUM\u2019S PROPRIETARY CONFIDENTIAL INFORMATION THAT IS HIGHLY CONFIDENTIAL AND PRIVILEGED.\r\n\r\nFrom: Tensorflow Jenkins [mailto:notifications@github.com]\r\nSent: Thursday, March 23, 2017 5:44 PM\r\nTo: tensorflow/tensorflow\r\nCc: Ji Qiu (\u90b1\u5409); Author\r\nSubject: Re: [tensorflow/tensorflow] Add the sched_getaffinity support for Android. (#8653)\r\n\r\n\r\nCan one of the admins verify this patch?\r\n\r\n\u2014\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/pull/8653#issuecomment-288666071>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AXR6un-flMzXuaLFquG0DStQTDTiLKqxks5roj7GgaJpZM4Mmese>.\r\n", "@tensorflow-jenkins Test this, please.", "Please don't merge this yet; while testing this internally I discovered a build issue. Looking into it now.", "@andrewharp working now?", "@drpngx Not yet. We need to replicate sched_getaffinity() as the function isn't present in the NDK/API combination we use internally for building.", "@andrewharp is this good to go now?", "@martinwicke No, it can't go in as-is since we need a workaround internally. sched_getaffinity() isn't provided by our Android NDK so we need to replicate the system calls it uses. I'll update once I get a chance to properly test the fix.", "I talked to Andrew offline and it sounds like due to internal API differences, this change sort of has to be completely done differently, and Andrew has decided to take this on, so I'm going to close this PR.\r\n\r\nI wish I had a better answer for you, but the TF team doesn't control the NDK versions that are built inside all of Google, so the best we can do is do the work required to solve this problem for you."]}, {"number": 8652, "title": "AttributeError: 'NoneType' object has no attribute 'TF_NewStatus", "body": "In the latest sources the issue: https://github.com/tensorflow/tensorflow/issues/3388 is fixed in the thensorflow/python/client/session.py, But I now receive the following error instead:\r\n```\r\nException ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x7fb2e08cee48>>\r\nTraceback (most recent call last):\r\n  File \"$HOME/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 595, in __del__\r\nAttributeError: 'NoneType' object has no attribute 'TF_NewStatus'\r\n\r\n```\r\nI compiled the model manually using the latest version sources and using Bazel 0.4.5. The same seems to be true as with the issue 3388: The error is not always popping up and the run seems to have ended normally. However, now and then to above error message is displayed. ", "comments": ["@naatje80 can you provide a script to repro this?", "Dear skye, \r\n\r\nthank you for your response. Triggered by your question, I found out that this issue is only occurring when using the Keras module. When I use the Tensorflow module directly I'm not able to reproduce the issue. I assumed that the tensorflow was the issue, as issue #3388 was also occurring at the same moment when I ran my Keras test scripts. Next to that, the error message also reported the issue was occurring in the session.py python script that is part of tensorflow. I will report the issue at the Keras project. Sorry for the inconvenience....", "No problem, thanks for digging into this!", "Hi @skye and @naatje80 I got the same issue. After installing tensorflow 1.0.1 from source, using bazel 0.4.5, I no longer get the error. Used keras v2 with MNIST dataset, it took 799s using the terminal. (GeForce 1070)\r\n\r\nFor some reason, when I use jupyter notebook, it gets blocked. From terminal using py files, everything works ok", "Hi, I also have this issue. It pops up from time to time. Keras 2.0.3 and tensorflow 1.1.0.", "This issue reproduces with the following code, which is based on [this example](https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py):\r\n\r\n```\r\nimport sys\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.keras as keras\r\n\r\ndef main(argv):\r\n\t# Get data into numpy arrays of size (?,num_unroll) where ? is the number of\r\n\t# data points and num_unroll is the number of time steps...\r\n\tx_train = ...\r\n\tx_test = ...\r\n\ty_train = ...\r\n\ty_test = ...\r\n\r\n\t# Build and run the model\r\n\tmodel = keras.models.Sequential()\r\n\tmodel.add(keras.layers.Embedding(MAX_FEATURES, 128))\r\n\tmodel.add(keras.layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2))\r\n\tmodel.add(keras.layers.Dense(1, activation='sigmoid'))\r\n\tmodel.compile(loss='binary_crossentropy',\r\n\t              optimizer='adam',\r\n\t              metrics=['accuracy'])\r\n\tprint('Train...')\r\n\tmodel.fit(x_train, y_train,\r\n\t          batch_size=FLAGS.batch_size,\r\n\t          epochs=15,\r\n\t          validation_data=(x_test, y_test))\r\n\tscore, acc = model.evaluate(x_test, y_test,\r\n\t                            batch_size=FLAGS.batch_size)\r\n\tprint('Validation score:', score)\r\n\tprint('Validation accuracy:', acc)\r\n\r\nif __name__ == \"__main__\":\r\n\tmain(sys.argv)\r\n```\r\n\r\nI don't get the error when I `import keras` instead of `import tensorflow.contrib.keras as keras`.  I'm using tensorflow 1.1.0 in Python 3 from `pip3 install tensorflow`.  **Note:** I also don't get the error if I run my script with Python 2 instead of Python 3.  Maybe could be some sort of 2/3 compatibility issue?", "It happens to me as well, from time to time.\r\nI used tensorflow-gpu", "It's happening in Tensorflow 1.1.0 tensorflow-gpu \r\n**Exception ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x000001A62301DC50>>\r\nTraceback (most recent call last):\r\n  File \"D:\\Programs\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 587, in __del__\r\nAttributeError: 'NoneType' object has no attribute 'TF_NewStatus'**\r\n", "Getting similar error. Has only happened in last 24-36 hours or so:\r\n\r\n**AttributeError: 'NoneType' object has no attribute 'update'**\r\n\r\n", "I had the same problem and checking the code in the file /lib/python3.5/site-packages/tensorflow/python/client/session.py shows this:\r\n```\r\ntry:\r\n        status = tf_session.TF_NewStatus()\r\n        tf_session.TF_DeleteDeprecatedSession(self._session, status)\r\n```\r\n\r\nthe error could be avoided with something like this:\r\n```\r\ntry:\r\n        if tf_session is not None:\r\n            status = tf_session.TF_NewStatus()\r\n            tf_session.TF_DeleteDeprecatedSession(self._session, status)\r\n```", "windows, python 3.5 in anaconda, tensorflow 1.1.0, keras 2.0.5\r\nwhen I try to run example perceptron from documentation (https://keras.io/getting-started/sequential-model-guide/) get error \r\n```\r\nException ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x0000026A0153E978>>\r\nTraceback (most recent call last):\r\n  File \"C:\\Miniconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 587, in __del__\r\nAttributeError: 'NoneType' object has no attribute 'TF_NewStatus' \r\n```\r\nCode that I run\r\n```\r\nimport keras\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout, Activation\r\nfrom keras.optimizers import SGD\r\n\r\n# Generate dummy data\r\nimport numpy as np\r\nx_train = np.random.random((1000, 20))\r\ny_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)\r\nx_test = np.random.random((100, 20))\r\ny_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\r\n\r\nmodel = Sequential()\r\n# Dense(64) is a fully-connected layer with 64 hidden units.\r\n# in the first layer, you must specify the expected input data shape:\r\n# here, 20-dimensional vectors.\r\nmodel.add(Dense(64, activation='relu', input_dim=20))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(64, activation='relu'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(10, activation='softmax'))\r\n\r\nsgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\r\nmodel.compile(loss='categorical_crossentropy',\r\n              optimizer=sgd,\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train,\r\n          epochs=20,\r\n          batch_size=128)\r\nscore = model.evaluate(x_test, y_test, batch_size=128)\r\n```", "I'm running windows, python 3.5 in anaconda, tensorflow 1.1.0, keras 2.0.**6**.\r\nWhen I run the code @jabacrack quoted, I **do not** get the error.  However I **do** get the error when running keras/examples/pretrained_word_embeddings.py.  FYI.", "@baloodevil, I get this error sometimes, even if I run some code several times.", "@Pr0duktiv's solution worked for me. ", "any ideas? I only get when I LOAD a model on Py27 Keras 2.0.4 TF 1.1 and then use a callback function on epoch end. I am trying the solution provided by Pr0duktiv, will get back to whether it has worked. ", "This happens to me as well, but python guide warns about usage of `__del__` method, that the other variables can be already garbage collected and thus set to `None`\r\n\r\nhttps://docs.python.org/3/reference/datamodel.html#object.__del__\r\n\r\nEDIT: Sorry I was actually hitting #3388\r\n", "I'm not sure if this is the same error or not; it's saying not callable rather than the attribute, but seems to be in the same method anyway...\r\n\r\n```\r\nException ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x7f4cfbc42e10>>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 696, in __del__\r\nTypeError: 'NoneType' object is not callable\r\n```\r\n\r\nusing tensorflow 1.4.1 in docker, using `tensorflow/tensorflow:latest-gpu-py3`\r\n*not* using keras, but I am using `sess = tf.Session()` and `sess.close()` instead of `with tf.Session() as sess:` but that doesn't seem to be sufficient to recreate the error reliably in say ipython3, it only happens when I run my script. perhaps some sort of race condition?\r\n", "I'm using tensorflow 1.5.0 via docker (gcr.io/tensorflow/tensorflow:1.5.0-devel-gpu-py3) and keras 2.1.3. The following exception sometimes occurs:\r\n\r\n```\r\nException ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x7fa52147f898>>\r\nTraceback (most recent call last): \r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 702, in __del__\r\nTypeError: 'NoneType' object is not callable\r\n```\r\n\r\nI'm not sure if it's related, but other times my script does not exit and keeps hanging. This makes automatic parameter tuning a hassle.", "Same here. I'm using python 3.5, tensorflow 1.4.0 and keras 2.1.0 and I get the following error:\r\n\r\n```\r\nException ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x7fe252d53dd8>>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 696, in __del__\r\nTypeError: 'NoneType' object is not callable\r\n```\r\n\r\nI noticed that in the relative code block only the AttributeError exception is catched, but not the TypeError we are getting, probably it should be added.\r\n\r\n```\r\nif self._session is not None:\r\n    try:\r\n        status = c_api_util.ScopedTFStatus()\r\n        if self._created_with_new_api:\r\n            tf_session.TF_DeleteSession(self._session, status)\r\n        else:\r\n            tf_session.TF_DeleteDeprecatedSession(self._session, status)\r\n    except AttributeError:\r\n        # At shutdown, `c_api_util` or `tf_session` may have been garbage\r\n        # collected, causing the above method calls to fail. In this case,\r\n        # silently leak since the program is about to terminate anyway.\r\n        pass\r\n    self._session = None\r\n```", "Reproduced on an EC2 p2.xlarge (GPU) instance, running ubuntu 16.04.3, python 3.5.2 keras 2.1.3, tensorflow 1.5.1\r\n\r\n```\r\nException ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x7f37d83267b8>>\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/safedk/dl_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 587, in __del__\r\nAttributeError: 'NoneType' object has no attribute 'TF_NewStatus'\r\n```", "I get the same error.\r\n\r\n```python\r\nfrom keras import backend as K\r\nK.clear_session()\r\n```\r\n\r\nUsing this line at the end of the code seems to fix it\r\nIt feels like a work around though.", " It worked for me too. Thanks\r\n", "Same problem. I was able to fix this (tensorflow 1.7.0 keras 2.1.5) by simply adding `import tensorflow as tf` before importing keras.", "@naatje80 Did you ever get to open an issue in the Keras repo? Can you please post the link?\r\n\r\nThanks in advance. \r\n", "@assif I am also running my job on AWS p2 instance and having the same issue. Did you solve it?", "@xiaohk I used the snippet provided by @Nimi42 and it worked.\r\n(using k.clear_session() at the end of the code).\r\nI agree it's an ugly hack though :)", "@assif Thanks! I will try it when I get connection to AWS again. I think I have cuda 8.0 on P2 and I am using Tensorflow 1.4. Are you using Tensorflow 1.5 with cuda 8.0?", "@xiaohk Indeed. I'm using tensorflow 1.5.1 with CUDA 8.0.61", "Im using tf 1.4 with keras 2.1.5\r\n```\r\nException ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x7fda61cd9828>>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 696, in __del__\r\nTypeError: 'NoneType' object is not callable\r\n```\r\nthis message pops up about every 5 or 6 times when I run my code, even with the same code. but besides\r\nthis message, nothing really effects me, my code still can run correctly with this error message.", "Using python 3.5.2, tf 1.9, and tensorflow.contrib.keras\r\n\r\nSeeing this issue intermittently as well. It is fairly annoying as it sometimes splits my output in half.  ", "Solution of Nimi42 worked for me as well to solve this issue. I added:\r\n\r\nfrom keras import backend as K\r\nK.clear_session()", "Closing this out since I understand it to be resolved, but please let me know if I'm mistaken."]}, {"number": 8651, "title": "how to add conditions to tf.Print", "body": "can I add a certain condition to the tf.Print op?\r\nsomething like print after certain iterations or something similar\r\nI have tf.Print to print out the losses during training, therefore there is too many lines I couldn't possibly read", "comments": ["You could do something like:\r\n```\r\ndef print_if_step(pass_through_tensor, print_list, step, print_step, prefix):\r\n    pass_through_tensor = tf.cond(tf.equal(print_step,step), lambda: tf.Print(pass_through_tensor, [step] + print_list, \"step \" + prefix), lambda :pass_through_tensor)\r\n    return pass_through_tensor\r\n```\r\nBut I think these questions should be asked on stackoverflow.\r\nM", "Yes, let's move any further discussion to  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow). Thanks!"]}, {"number": 8650, "title": "Add distributed aggregation for embedding lookup sparse", "body": "#5928 \r\nthe function `embedding_lookup` becomes too complex...but hard to split.", "comments": ["Can one of the admins verify this patch?", "I find `embedding_lookup` too complex too. \r\n\r\nMaybe it would be a good idea to refactor it into something easier to apprehend for newcomers.", "@tgy I tried to add a new function for aggregation, but it was a mess. I need to modify the function `embedding_lookup` or I need to copy a lot of code from `embedding_lookup`.", "I think I prefer a function in contrib with sadly duplicated code from embedding lookup over changing its API at this point.\r\n\r\nI am just worried that maybe use_aggregation=True is not how we'd like this to look like forever. I'd rather let this sit in contrib for a bit (even with the pain of duplication) and see how it's used.\r\n\r\nThoughts?", "@alextp it's better to add new functions, the current modification is too ugly...", "Jenkins, test this please.", "@alextp I can't see the detail of the log of the failed test case.", "The embedding_ops_test fails in python3 because the code uses xrange instead of range:\r\n\r\n```\r\nFAIL: //tensorflow/contrib/layers:embedding_ops_test (see /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/testlogs/tensorflow/contrib/layers/embedding_ops_test/test.log).\r\nINFO: From Testing //tensorflow/contrib/layers:embedding_ops_test:\r\n==================== Test output for //tensorflow/contrib/layers:embedding_ops_test:\r\n2017-03-29 22:09:06.648837: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-03-29 22:09:06.648870: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-03-29 22:09:06.648874: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-03-29 22:09:06.648876: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-03-29 22:09:06.648879: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nEE../var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/embedding_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/partitioned_variables.py:280: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\r\n  \"create_partitioned_variables is deprecated.  Use \"\r\nWARNING:tensorflow:create_partitioned_variables is deprecated.  Use tf.get_variable with a partitioner set, or tf.get_partitioned_variable_list, instead.\r\n/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/embedding_ops_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/embedding_ops.py:96: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\r\n  logging.warn(\"The default value of combiner will change from \\\"mean\\\" \"\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\n.WARNING:tensorflow:create_partitioned_variables is deprecated.  Use tf.get_variable with a partitioner set, or tf.get_partitioned_variable_list, instead.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\n.WARNING:tensorflow:create_partitioned_variables is deprecated.  Use tf.get_variable with a partitioner set, or tf.get_partitioned_variable_list, instead.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\n.WARNING:tensorflow:create_partitioned_variables is deprecated.  Use tf.get_variable with a partitioner set, or tf.get_partitioned_variable_list, instead.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\n.WARNING:tensorflow:create_partitioned_variables is deprecated.  Use tf.get_variable with a partitioner set, or tf.get_partitioned_variable_list, instead.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\n.WARNING:tensorflow:create_partitioned_variables is deprecated.  Use tf.get_variable with a partitioner set, or tf.get_partitioned_variable_list, instead.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\n.WARNING:tensorflow:create_partitioned_variables is deprecated.  Use tf.get_variable with a partitioner set, or tf.get_partitioned_variable_list, instead.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\n.WARNING:tensorflow:create_partitioned_variables is deprecated.  Use tf.get_variable with a partitioner set, or tf.get_partitioned_variable_list, instead.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\n.WARNING:tensorflow:create_partitioned_variables is deprecated.  Use tf.get_variable with a partitioner set, or tf.get_partitioned_variable_list, instead.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\n.WARNING:tensorflow:create_partitioned_variables is deprecated.  Use tf.get_variable with a partitioner set, or tf.get_partitioned_variable_list, instead.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\n........WARNING:tensorflow:create_partitioned_variables is deprecated.  Use tf.get_variable with a partitioner set, or tf.get_partitioned_variable_list, instead.\r\n.WARNING:tensorflow:create_partitioned_variables is deprecated.  Use tf.get_variable with a partitioner set, or tf.get_partitioned_variable_list, instead.\r\nWARNING:tensorflow:From /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/embedding_ops_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/embedding_ops.py:344: calling sparse_feature_cross (from tensorflow.contrib.layers.python.ops.sparse_feature_cross_op) with hash_key=None is deprecated and will be removed after 2016-11-20.\r\nInstructions for updating:\r\nThe default behavior of sparse_feature_cross is changing, the default\r\nvalue for hash_key will change to SPARSE_FEATURE_CROSS_DEFAULT_HASH_KEY.\r\nFrom that point on sparse_feature_cross will always use FingerprintCat64\r\nto concatenate the feature fingerprints. And the underlying\r\n_sparse_feature_cross_op.sparse_feature_cross operation will be marked\r\nas deprecated.\r\n../var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/embedding_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/embedding_ops.py:95: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\r\n  if params is None or params == []:  # pylint: disable=g-explicit-bool-comparison\r\n./var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/embedding_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/embedding_ops.py:95: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\r\n  if params is None or params == []:  # pylint: disable=g-explicit-bool-comparison\r\n./var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/embedding_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/partitioned_variables.py:280: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\r\n  \"create_partitioned_variables is deprecated.  Use \"\r\nWARNING:tensorflow:create_partitioned_variables is deprecated.  Use tf.get_variable with a partitioner set, or tf.get_partitioned_variable_list, instead.\r\nWARNING:tensorflow:From /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/embedding_ops_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/embedding_ops.py:344: calling sparse_feature_cross (from tensorflow.contrib.layers.python.ops.sparse_feature_cross_op) with hash_key=None is deprecated and will be removed after 2016-11-20.\r\nInstructions for updating:\r\nThe default behavior of sparse_feature_cross is changing, the default\r\nvalue for hash_key will change to SPARSE_FEATURE_CROSS_DEFAULT_HASH_KEY.\r\nFrom that point on sparse_feature_cross will always use FingerprintCat64\r\nto concatenate the feature fingerprints. And the underlying\r\n_sparse_feature_cross_op.sparse_feature_cross operation will be marked\r\nas deprecated.\r\n.WARNING:tensorflow:create_partitioned_variables is deprecated.  Use tf.get_variable with a partitioner set, or tf.get_partitioned_variable_list, instead.\r\nWARNING:tensorflow:From /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/embedding_ops_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/embedding_ops.py:344: calling sparse_feature_cross (from tensorflow.contrib.layers.python.ops.sparse_feature_cross_op) with hash_key=None is deprecated and will be removed after 2016-11-20.\r\nInstructions for updating:\r\nThe default behavior of sparse_feature_cross is changing, the default\r\nvalue for hash_key will change to SPARSE_FEATURE_CROSS_DEFAULT_HASH_KEY.\r\nFrom that point on sparse_feature_cross will always use FingerprintCat64\r\nto concatenate the feature fingerprints. And the underlying\r\n_sparse_feature_cross_op.sparse_feature_cross operation will be marked\r\nas deprecated.\r\n.WARNING:tensorflow:create_partitioned_variables is deprecated.  Use tf.get_variable with a partitioner set, or tf.get_partitioned_variable_list, instead.\r\nWARNING:tensorflow:From /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/embedding_ops_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/embedding_ops.py:344: calling sparse_feature_cross (from tensorflow.contrib.layers.python.ops.sparse_feature_cross_op) with hash_key=None is deprecated and will be removed after 2016-11-20.\r\nInstructions for updating:\r\nThe default behavior of sparse_feature_cross is changing, the default\r\nvalue for hash_key will change to SPARSE_FEATURE_CROSS_DEFAULT_HASH_KEY.\r\nFrom that point on sparse_feature_cross will always use FingerprintCat64\r\nto concatenate the feature fingerprints. And the underlying\r\n_sparse_feature_cross_op.sparse_feature_cross operation will be marked\r\nas deprecated.\r\n.WARNING:tensorflow:create_partitioned_variables is deprecated.  Use tf.get_variable with a partitioner set, or tf.get_partitioned_variable_list, instead.\r\nWARNING:tensorflow:From /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/embedding_ops_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/embedding_ops.py:344: calling sparse_feature_cross (from tensorflow.contrib.layers.python.ops.sparse_feature_cross_op) with hash_key=None is deprecated and will be removed after 2016-11-20.\r\nInstructions for updating:\r\nThe default behavior of sparse_feature_cross is changing, the default\r\nvalue for hash_key will change to SPARSE_FEATURE_CROSS_DEFAULT_HASH_KEY.\r\nFrom that point on sparse_feature_cross will always use FingerprintCat64\r\nto concatenate the feature fingerprints. And the underlying\r\n_sparse_feature_cross_op.sparse_feature_cross operation will be marked\r\nas deprecated.\r\n.WARNING:tensorflow:create_partitioned_variables is deprecated.  Use tf.get_variable with a partitioner set, or tf.get_partitioned_variable_list, instead.\r\nWARNING:tensorflow:From /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/embedding_ops_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/embedding_ops.py:344: calling sparse_feature_cross (from tensorflow.contrib.layers.python.ops.sparse_feature_cross_op) with hash_key=None is deprecated and will be removed after 2016-11-20.\r\nInstructions for updating:\r\nThe default behavior of sparse_feature_cross is changing, the default\r\nvalue for hash_key will change to SPARSE_FEATURE_CROSS_DEFAULT_HASH_KEY.\r\nFrom that point on sparse_feature_cross will always use FingerprintCat64\r\nto concatenate the feature fingerprints. And the underlying\r\n_sparse_feature_cross_op.sparse_feature_cross operation will be marked\r\nas deprecated.\r\n..\r\n======================================================================\r\nERROR: testEmbeddingLookupSparse (__main__.EmbeddingLookupSparseWithDistributedAggregationTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/embedding_ops_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/embedding_ops_test.py\", line 722, in testEmbeddingLookupSparse\r\n    combiner=combiner)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/embedding_ops_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/embedding_ops.py\", line 640, in embedding_lookup_sparse_with_distributed_aggregation\r\n    weights=weights, idx=idx, segment_ids=segment_ids)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/embedding_ops_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/embedding_ops.py\", line 784, in _embedding_lookup_with_distributed_aggregation\r\n    for p in xrange(np):\r\nNameError: name 'xrange' is not defined\r\n\r\n======================================================================\r\nERROR: testGradientsEmbeddingLookupSparse (__main__.EmbeddingLookupSparseWithDistributedAggregationTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/embedding_ops_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/embedding_ops_test.py\", line 761, in testGradientsEmbeddingLookupSparse\r\n    combiner=combiner)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/embedding_ops_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/embedding_ops.py\", line 640, in embedding_lookup_sparse_with_distributed_aggregation\r\n    weights=weights, idx=idx, segment_ids=segment_ids)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/embedding_ops_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/embedding_ops.py\", line 784, in _embedding_lookup_with_distributed_aggregation\r\n    for p in xrange(np):\r\nNameError: name 'xrange' is not defined\r\n\r\n----------------------------------------------------------------------\r\nRan 32 tests in 6.352s\r\n\r\nFAILED (errors=2)\r\n================================================================================\r\n```", "Jenkins, test this please."]}, {"number": 8649, "title": "Who can tell me the way to use tf.confusion_matrix()?", "body": "I want to craete a confusion matrix to get the result of test data performance, I found that there is a function 'tf.confusion_matrix()', but I don't know how to use it, anybody can tell me ?", "comments": ["I can help you with this. But this is not a feature or bug issue with Tensorflow. Please create a stackover flow question, post it here, then close the issue. I will come and answer it for you.\r\n\r\nThanks", "I have already fix this in another way, thx anyway ~"]}, {"number": 8648, "title": "No module named tensorflow occurs, Windows 7  no GPU involved", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nSame problem on Linux or MacOS.\r\n\r\n### Environment info\r\nOperating System: Windows 7\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\nNo\r\n\r\nTensorflow is well installed to a Windows 7 before posting this question. \r\nD:\\Python35\\Scripts>pip show tensorflow\r\n---\r\nMetadata-Version: 2.0\r\nName: tensorflow\r\nVersion: 1.0.1\r\nSummary: TensorFlow helps the tensors flow\r\nHome-page: http://tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: opensource@google.com\r\nInstaller: pip\r\nLicense: Apache 2.0\r\nLocation: d:\\python35\\lib\\site-packages\r\nRequires: numpy, six, protobuf, wheel\r\nClassifiers:\r\n  Development Status :: 4 - Beta\r\n  Intended Audience :: Developers\r\n  Intended Audience :: Education\r\n  Intended Audience :: Science/Research\r\n  License :: OSI Approved :: Apache Software License\r\n  Programming Language :: Python :: 2.7\r\n  Topic :: Scientific/Engineering :: Mathematics\r\n  Topic :: Software Development :: Libraries :: Python Modules\r\n  Topic :: Software Development :: Libraries\r\nEntry-points:\r\n  [console_scripts]\r\n  tensorboard = tensorflow.tensorboard.tensorboard:main\r\nYou are using pip version 8.1.1, however version 9.0.1 is available.\r\nYou should consider upgrading via the 'python -m pip install --upgrade pip' comm\r\nand.\r\n\r\n\r\nPackages are all well installed (just for me cause this is the first day to Tensorflow.\r\nAlso, I've tried to install A ipython and it seems running well.\r\n[1]: import tensorflow as tf\r\n[2]: (I don't know what to do here and give an exit).\r\n\r\n\r\n", "comments": ["I do not really see an error message.\r\nIt looks like import command ran fine?", "I mean, if you are seeing this:\r\n```\r\n[1]: import tensorflow as tf\r\n[2]:\r\n```\r\nThat means TensorFlow is installed and imported OK.", "@gunan Thanks for this reply.\r\nIt gives this error after import\r\nC:\\Users\\Administrator>python\r\nPython 2.7.12 (v2.7.12:d33e0cf91556, Jun 27 2016, 15:19:22) [MSC v.1500 32 bit (\r\nIntel)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nImportError: No module named tensorflow\r\n>>>", "Could you try running ipython, and then importing tensorflow?\r\nipython is simply an improved python shell, so if it works there, it may be simply due to python paths not being setup correctly.", "Thank you, @gunan .\r\nIt looks like a version problem? I have both 2.7 and 3.5 installed in different drive and the terminal launches 2.7 in default. Paths of python3.5 are also added to the env. I launched python under d:\\python35 and perform import command which gives no output but another prompt simply as what I did in ipython.\r\n```\r\n>>> import tensorflow as tf\r\n>>>\r\n```\r\n```\r\n", "That completely explains the problem.\r\nTensorFlow only support python version 3.5 on windows.\r\nhttps://www.tensorflow.org/install/install_windows\r\nNo output when you run the import command means success in python.\r\n\r\nThe import error in python 2.7 is completely expected.\r\n\r\nI will close this issue, as it looks like you have a successful python 3.5 installation with tensorflow, and python 2.7 is not working as expected.\r\n", "On windows I am not sure how you would setup your path to always start python 3.5 when you run python command in your terminal. But you can probably look around online to see how people setup their development envronments."]}, {"number": 8647, "title": "Try to call tf.select but get AttributeError: 'module' object has no attribute 'select'", "body": "Here is the traceback information:\r\nTraceback (most recent call last):\r\n  File \"guidedBack.py\", line 149, in <module>\r\n    grads = K.gradients(model.layers[-2].output[0, 0], model.layers[-6].layers[-2].output)[0]\r\n  File \"/N/u/zehzhang/myTensorflow/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 2108, in gradients\r\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\r\n  File \"/N/u/zehzhang/myTensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py\", line 482, in gradients\r\n    in_grads = grad_fn(op, *out_grads)\r\n  File \"guidedBack.py\", line 25, in _GuidedReluGrad\r\n    return tf.select(0. < grad, gen_nn_ops._relu_grad(grad, op.outputs[0]), tf.zeros(grad.get_shape()[1:]))\r\nAttributeError: 'module' object has no attribute 'select'\r\n\r\nAnyone has any idea?", "comments": ["Is Tensorflow imported correctly as tf in the file throwing the error? \r\nDoes the top of that file read:\r\n```\r\nimport tensorflow as tf\r\n```\r\nPlease check your import statement.", "@jubjamie \r\nYes, I have imported tensorflow as tf.", "Can you please post the code of the file causing problems? I will try and reproduce the problem.", "I found in the new version of tensorflow, there was one file named select.py added to tensorflow.contrib.graph_editor and it seems tensorflow.contrib is lazily imported in __init__.py. \r\n\r\nIs this the reason?", "@jubjamie \r\nHere it is (still some cleaning work to do, but enough for debugging :) ):\r\n```\r\nfrom keras.applications.vgg16 import VGG16, preprocess_input\r\nfrom keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\r\nfrom keras.layers import Activation, Dropout, Flatten, Dense, Lambda\r\nfrom keras.models import Model, Sequential\r\nimport numpy as np\r\nfrom scipy.misc import imresize\r\nfrom scipy.misc import imsave\r\nimport tensorflow as tf\r\nimport keras.backend as K\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.ops import gen_nn_ops\r\n\r\n\r\n@ops.RegisterGradient(\"GuidedRelu\")\r\ndef _GuidedReluGrad(op, grad):\r\n    return tf.select(0. < grad, gen_nn_ops._relu_grad(grad, op.outputs[0]), tf.zeros(grad.get_shape()[1:]))\r\n\r\n\r\ndef target_category_loss(x, category_index, nb_classes):\r\n    return tf.multiply(x, K.one_hot([category_index], nb_classes))\r\n\r\n\r\ndef target_category_loss_output_shape(input_shape):\r\n    return input_shape\r\n\r\n\r\ndef normalize(x):\r\n    # utility function to normalize a tensor by its L2 norm\r\n    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\r\n\r\ndef backend_reshape(x):\r\n    shape = (my_batch_size, img_width, img_height, 3)\r\n    return K.reshape(x, shape)\r\n\r\ndef reproduce(x):\r\n    return x\r\n\r\ntuned_vgg_weights_path = './tuned_vgg_model.h5'\r\nldir = '/N/u/zehzhang/cogsci16/valid/0/'\r\nname = '01_20150530_16859_sync_frames_parent_img_06600.jpg'\r\nimg_path = ldir + name\r\nimg_width = 224\r\nimg_height = 224\r\nmy_batch_size = 1\r\n\r\n\r\nnb_classes = 24\r\ncategory_index = 0\r\nlayer_name = 'block5_conv3'\r\n\r\n\r\nwith tf.get_default_graph().gradient_override_map({'Relu': 'GuidedRelu'}):\r\n    model = Sequential()\r\n    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\r\n    model.add(base_model)\r\n    model.add(Flatten())\r\n    model.add(Dense(4096, activation='relu'))\r\n    model.add(Dense(4096, activation='relu'))\r\n    model.add(Dense(24, activation='relu'))\r\n    model.add(Activation('softmax'))\r\n    model.load_weights(tuned_vgg_weights_path)\r\n    print(model.summary())\r\n\r\n\r\n\r\nconv_output = [l for l in model.layers[-6].layers if l.name is layer_name][0].output\r\ngrads = K.gradients(model.layers[-2].output[0, 0], model.layers[-6].layers[-2].output)[0]\r\n\r\nimg = load_img(img_path, target_size=(img_width, img_height))\r\nori_w, ori_h = load_img(img_path).size\r\n\r\nimg_array = img_to_array(img)\r\ngradient_function = K.function([model.layers[0].input], [conv_output, grads])\r\n\r\noutput, grads_val = gradient_function([preprocess_input(img_array[np.newaxis, :])])\r\n```", "Would you mind editing your above comment to put it all in code with correct indentation and formatting please? Just to make sure everything is written as it should be! In the mean time I will try a little test here.", "@jubjamie \r\nYes wait a minute. In fact I'm now struggling to add the indentation.", "@jubjamie \r\nI have upated my code above with correct indentation.", "Problem solved. Can you please confirm what version of tensorflow you are using?", "@jubjamie \r\nThanks! \r\nI'm using the most recent version and I guess that's the reason because I found in the new version of tensorflow, there was one file named select.py added to tensorflow.contrib.graph_editor and it seems tensorflow.contrib is lazily imported in init.py.\r\n\r\nAm I right? If it's the reason, except using the previous version, is there any way I can solve it?", "tf.select is deprecated that's why. I can't believe I didn't spot that at first! \r\nIn TF1.0 tf.select() is not an op in the control flow group. Can I suggest that you use tf.where() and see if you get the desired results. In my brief testing you should get identical functionality.\r\n\r\nLet me know if that solves your problem.", "@jubjamie \r\nAfter running python -c 'import tensorflow as tf; print(tf.__version__)', I get:\r\n1.0.1\r\n\r\nSo the exact version should be 1.0.1 I think.", "Yeah that's fine. Try using tf.where() instead as tf.select is a deprecated control function. Maybe that's why you are seeing it in contrib? Would have to ask a Tensorflower!\r\nDoes this answer your issue?", "@jubjamie \r\nYes and thanks a lot!\r\nIt seems in the latest version, tf.select is removed and tf.where is a substitute!", "Yes. tf.where used to have a similar but different functionality. If this solved then feel free to close the issue.\r\nHope I could help!", "@jubjamie \r\nCould I ask one last-minute question?\r\nI try to call tf.zeros(grad.gets_shape()), but the shape of my grad is (?, 24) and thus it fails to create a zero tensor because tf.zeros requires a specific shape. Is there any way I can create the zero tensor I want?", "I'm out the office now but if you can create a stackoverflow question, link it here and then close the issue that would be great as Google like to keep this clear for bugs. I'll try and get back to you tomorrow.", "@jubjamie thank you for your responses! I'm gonna close this issue."]}, {"number": 8646, "title": "Doc addition to tf.gather describing validate_indices option behavior", "body": "See Issue #3638 . This helps people debugging their code, as otherwise they may assume `tf.gather` throws an error when invalid indices are present (it currently does so only when executed on CPU).", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "@Fenugreek Thank you for the fix!"]}, {"number": 8645, "title": "errors meet after django  call several times saver.restore", "body": "\r\n\r\n File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 242, in restore_op\r\n    [spec.tensor.dtype])[0])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 668, in restore_v2\r\n    dtypes=dtypes, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\n\r\n\r\nNotFoundError (see above for traceback): Key wd1/Adam_7 not found in checkpoint\r\n\t [[Node: save_3/RestoreV2_133 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_3/Const_0, save_3/RestoreV2_133/tensor_names, save_3/RestoreV2_133/shape_and_slices)]]\r\n\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc4/Adam_9 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc3/Adam_11 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc3/Adam_10 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc2/Adam_9 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc2/Adam_8 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc2/Adam_7 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc2/Adam_6 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc5/Adam_10 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc5/Adam_11 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc2/Adam_11 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc2/Adam_10 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc1/Adam_9 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc1/Adam_8 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc1/Adam_7 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc1/Adam_6 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc1/Adam_11 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc1/Adam_10 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key out/Adam_9 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key out/Adam_8 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key out/Adam_7 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key out/Adam_6 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc5/Adam_6 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc5/Adam_7 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc5/Adam_8 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key wc5/Adam_9 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key out/Adam_11 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key out/Adam_10 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key beta2_power_5 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key beta2_power_4 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key beta2_power_3 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key beta1_power_5 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key beta1_power_4 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: Key beta1_power_3 not found in checkpoint\r\n\r\n\r\n", "comments": ["I am not sure it's a bug in TensorFlow. I think you're trying to restore from a checkpoing from different models. Do you have a simpler repro? Without django? (It could be concurrent access to the web server?)", "I'm sure the models  are the same .Otherwise the front  several times would not work . I send the repro to your email. Thanks~ ", "Not sure if you have my email, but you can just use pastebin if it's public.", "@lufengguang, we cannot generally give help over private channels. You should make a reproducible example that can be posted publicly, please. That way other people than @drpngx, can help.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "@lufengguang I have same problem\u3002How did you solve it?"]}, {"number": 8644, "title": "unknown op: BestSplits (windows 10 + python 3.5.3)", "body": "After install tensorflow on windows 10 with python 3.5.3, I try to run hello world. but got some message\r\n\r\n```bash\r\nPS C:\\Users\\huzhifeng> python --version\r\nPython 3.5.3\r\nPS C:\\Users\\huzhifeng> python -m pip  install -U tensorflow\r\nCollecting tensorflow\r\n  Downloading tensorflow-1.0.1-cp35-cp35m-win_amd64.whl (14.7MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14.7MB 40kB/s\r\nCollecting protobuf>=3.1.0 (from tensorflow)\r\n  Downloading protobuf-3.2.0-py2.py3-none-any.whl (360kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 368kB 73kB/s\r\nCollecting numpy>=1.11.0 (from tensorflow)\r\n  Downloading numpy-1.12.1-cp35-none-win_amd64.whl (7.7MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7.7MB 61kB/s\r\nCollecting six>=1.10.0 (from tensorflow)\r\n  Using cached six-1.10.0-py2.py3-none-any.whl\r\nCollecting wheel>=0.26 (from tensorflow)\r\n  Downloading wheel-0.29.0-py2.py3-none-any.whl (66kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 46kB/s\r\nCollecting setuptools (from protobuf>=3.1.0->tensorflow)\r\n  Downloading setuptools-34.3.2-py2.py3-none-any.whl (389kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 399kB 119kB/s\r\nCollecting appdirs>=1.4.0 (from setuptools->protobuf>=3.1.0->tensorflow)\r\n  Downloading appdirs-1.4.3-py2.py3-none-any.whl\r\nCollecting packaging>=16.8 (from setuptools->protobuf>=3.1.0->tensorflow)\r\n  Downloading packaging-16.8-py2.py3-none-any.whl\r\nCollecting pyparsing (from packaging>=16.8->setuptools->protobuf>=3.1.0->tensorflow)\r\n  Downloading pyparsing-2.2.0-py2.py3-none-any.whl (56kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 93kB/s\r\nInstalling collected packages: six, appdirs, pyparsing, packaging, setuptools, protobuf, numpy, wheel, tensorflow\r\n  Found existing installation: setuptools 28.8.0\r\n    Uninstalling setuptools-28.8.0:\r\n      Successfully uninstalled setuptools-28.8.0\r\nSuccessfully installed appdirs-1.4.3 numpy-1.12.1 packaging-16.8 protobuf-3.2.0 pyparsing-2.2.0 setuptools-34.3.2 six-1.\r\n10.0 tensorflow-1.0.1 wheel-0.29.0\r\nPS C:\\Users\\huzhifeng> tensorflow\r\nPS C:\\Users\\huzhifeng> python\r\nPython 3.5.3 (v3.5.3:1880cb95a742, Jan 16 2017, 16:02:32) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> hello = tf.constant('Hello, TensorFlow!');\r\n>>> sess = tf.Session();\r\n>>> print(sess.run(hello));\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('\r\nop: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('\r\nop: \"CountExtremelyRandomStats\" device_type: \"CPU\"') for unknown op: CountExtremelyRandomStats\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('\r\nop: \"FinishedNodes\" device_type: \"CPU\"') for unknown op: FinishedNodes\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('\r\nop: \"GrowTree\" device_type: \"CPU\"') for unknown op: GrowTree\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('\r\nop: \"ReinterpretStringToFloat\" device_type: \"CPU\"') for unknown op: ReinterpretStringToFloat\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('\r\nop: \"SampleInputs\" device_type: \"CPU\"') for unknown op: SampleInputs\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('\r\nop: \"ScatterAddNdim\" device_type: \"CPU\"') for unknown op: ScatterAddNdim\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('\r\nop: \"TopNInsert\" device_type: \"CPU\"') for unknown op: TopNInsert\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('\r\nop: \"TopNRemove\" device_type: \"CPU\"') for unknown op: TopNRemove\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('\r\nop: \"TreePredictions\" device_type: \"CPU\"') for unknown op: TreePredictions\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('\r\nop: \"UpdateFertileSlots\" device_type: \"CPU\"') for unknown op: UpdateFertileSlots\r\nb'Hello, TensorFlow!'\r\n>>> print(sess.run(hello));\r\nb'Hello, TensorFlow!'\r\n>>> print(sess.run(hello));\r\nb'Hello, TensorFlow!'\r\n```\r\n\r\n", "comments": ["/cc: @guschmue @mrry in case they know if these ops are in windows and why we would be getting these messages.", "I think this happens only in the 1.0 branch, master is fine. Maybe some commit missing into the 1.0 branch? I can take a look tomorrow unless @mrry knows.", "@netroby Can you try 1.0.1?", "had the same thought and just tried ... message is still there in 1.0.1", "Thanks @guschmue for checking!\r\n\r\n@gunan @yifeif I don't know which commit that would be, and if it would be worth a cherry pick.", "I think this was resolved 1 day after 1.0 branch cut.\r\nYou can try nightlies, or you can wait for 1.1", "Yes, the `BestSplits` kernel has become [infamous](http://stackoverflow.com/q/42217532/3574081). Closing as resolved/a duplicate."]}, {"number": 8643, "title": "AOT graph gen script failed to open files in binary mode", "body": "This script will fail on Python 3 if the binary flag is not enabled during writing.\r\n\r\nFixes #8612", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "CLA signed", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please.", "Sorry for the delay."]}, {"number": 8642, "title": "Graph with tf.cond can not be serialized correctly after calling remove_training_nodes.", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n#3667 may be the same problem? But that thread is closed without a clear answer.\r\n\r\n### Environment info\r\nOperating System:\r\n\r\n- Tensorflow 1.0.0\r\n- Python 2.7\r\n- Ubuntu 14.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\nCUDA 8.0\r\ncuDNN 5.1.10\r\n*This problem occurs in both GPU and CPU environment.\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nThe following code is to produce a `.pb` file that can be used for `tf.import_graph_def`.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import graph_util\r\n\r\ninput_jpeg = tf.placeholder(tf.string, name='DecodeJpeg/contents')\r\nimage = tf.image.decode_jpeg(input_jpeg, channels=3)\r\nimage = tf.image.convert_image_dtype(image, dtype=tf.float32)\r\n\r\nheight = tf.constant(224)\r\nwidth = tf.constant(224)\r\ninput_h = tf.shape(image)[0]\r\ninput_w = tf.shape(image)[1]\r\n\r\ndef resize_t(image_t, h=height, w=width):\r\n  image2 = tf.expand_dims(image_t, axis=0)\r\n  return image2\r\n\r\nimage_checked = tf.cond(\r\n    (tf.less(input_h, height)), \r\n    lambda:resize_t(image), lambda:image)\r\n\r\nshape_t = tf.shape(image_checked, name='shape_t')\r\n\r\nwith tf.Session() as sess:\r\n  # freeze the graph and export\r\n  output_graph_def = sess.graph_def\r\n  output_graph_def = graph_util.convert_variables_to_constants(\r\n        sess, sess.graph_def, ['shape_t'])\r\n  output_graph_def = graph_util.remove_training_nodes(output_graph_def)\r\n  with open('out.pb', 'wb') as f:\r\n    f.write(output_graph_def.SerializeToString())\r\n```\r\n\r\nThe `out.pb` file can be generated as expected, but when I import it using the code below:\r\n\r\n```\r\nimport tensorflow as tf\r\nwith open('out.pb', 'rb') as f:\r\n  graph_content = f.read()\r\ngraph_def = tf.GraphDef()\r\ngraph_def.ParseFromString(graph_content)\r\n_ = tf.import_graph_def(graph_def, name='')\r\n```\r\n\r\nIt failed with Message:\r\n```\r\nTraceback (most recent call last):\r\n  File \"im.py\", line 6, in <module>\r\n    _ = tf.import_graph_def(graph_def, name='')\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 342, in import_graph_def\r\n    % (input_name,)))\r\nValueError: graph_def is invalid at node u'cond/ExpandDims/dim': More inputs specified ('cond/Switch:1') than the op expects..\r\n```\r\n### What other attempted solutions have you tried?\r\nwhen I removed the `graph_util.remove_training_nodes` call, everything works well.\r\nSo it seems that something goes wrong while removing the Identity nodes from the graph.\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\nExploring the `.pbtxt` generated with and without `graph_util.remove_training_nodes`,some Identity nodes are removed and the input of ExpandDims/dim is modified to connect to the output of Switch.(maybe?)\r\nPart of `.pbtxt` file content without `graph_util.remove_training_nodes`:\r\n```\r\n^Kcond/Switch^R^FSwitch^Z^DLess^Z^DLess*^G\r\n^AT^R^B0\r\n\r\n1\r\n^Mcond/switch_t^R^HIdentity^Z^Mcond/Switch:1*^G\r\n^AT^R^B0\r\n\r\n'\r\n^Lcond/pred_id^R^HIdentity^Z^DLess*^G\r\n^AT^R^B0\r\n\r\nM\r\n^Scond/ExpandDims/dim^R^EConst^Z^N^cond/switch_t*^K\r\n^Edtype^R^B0^C*^R\r\n^Evalue^R       B^G^H^C^R^@:^A^@\r\nh\r\n^Vcond/ExpandDims/Switch^R^FSwitch^Z^Mconvert_image^Z^Lcond/pred_id*^G\r\n^AT^R^B0^A*\r\n^F_class^R^V\r\n^T^R^Rloc:@convert_image\r\na\r\n^Ocond/ExpandDims^R\r\nExpandDims^Z^Xcond/ExpandDims/Switch:1^Z^Scond/ExpandDims/dim*\r\n\r\n^DTdim^R^B0^C*^G\r\n^AT^R^B0^A\r\n\r\n```\r\n\r\nPart of `.pbtxt` file content with `graph_util.remove_training_nodes`:\r\n```\r\n^Kcond/Switch^R^FSwitch^Z^DLess^Z^DLess*^G\r\n^AT^R^B0\r\n\r\nL\r\n^Scond/ExpandDims/dim^R^EConst^Z^Mcond/Switch:1*^R\r\n^Evalue^R       B^G^H^C^R^@:^A^@*^K\r\n^Edtype^R^B0^C\r\n`\r\n^Vcond/ExpandDims/Switch^R^FSwitch^Z^Mconvert_image^Z^DLess*^G\r\n^AT^R^B0^A*\r\n^F_class^R^V\r\n^T^R^Rloc:@convert_image\r\na\r\n^Ocond/ExpandDims^R\r\nExpandDims^Z^Xcond/ExpandDims/Switch:1^Z^Scond/ExpandDims/dim*\r\n\r\n^DTdim^R^B0^C*^G\r\n^AT^R^B0^A\r\n```", "comments": ["@petewarden any clue maybe?\r\n\r\nJust checking @isiosia that this is not related to `Identity` nodes?", "FYI.\r\nAs an attempt to avoid this error,I modified the code in `graph_util_impl.py` at L295:\r\n```\r\n  types_to_splice = {\"Identity\": True}\r\n  names_to_splice = {}\r\n  for node in nodes_after_removal:\r\n    if node.op in types_to_splice:\r\n      # We don't want to remove nodes that have control edge inputs, because\r\n      # they might be involved in subtle dependency issues that removing them\r\n      # will jeopardize.\r\n      has_control_edge = False\r\n      for input_name in node.input:\r\n      ''' ##      >>>>modified here>>>origin code:\r\n        if re.match(r\"^\\^\", input_name):\r\n      ''' ##      >>>>modified code:\r\n        if re.match(r\"^\\^\", input_name) or \\\r\n           re.match(r\".*/cond.*\", input_name):\r\n#    ''' ##      <<<<modefied end<<<\r\n          has_control_edge = True\r\n```\r\nand it seems working well so far.", "OK, interesting, thanks for debugging! If @petewarden is OK with this, then it might a good idea to send out a PR.", "Unfortunately using the name to detect conditionals is pretty fragile, since lots of non-conditionals could match \".*/cond.*\" too. As a workaround, it might be better to supply a list of node names or patterns not to remove as an argument to the function.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "OK, @petewarden fix does not require new code or fixes on our side, so closing. Feel free to reopen."]}, {"number": 8641, "title": "Cannot build Android demo - Bazel BUILD failed", "body": "### Description\r\n\r\nI've successfully built the [Android Camera Demo](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android) a couple days ago but I've failed to build the app today. The error message looks like this:\r\n\r\n    ERROR: /Users/hao.hu/GitHub/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3870:1: C++ compilation of rule '//tensorflow/core/kernels:android_tensorflow_kernels' failed: Process exited with status 1 [sandboxed].\r\n    tensorflow/core/kernels/split_v_op.cc:212:28: error: non-constant-expression cannot be narrowed from type 'value_type' (aka 'long long') to 'int' in initializer list [-Wc++11-narrowing]\r\n              prefix_dim_size, split_sizes_vec[i], suffix_dim_size};\r\n                               ^~~~~~~~~~~~~~~~~~\r\n    tensorflow/core/kernels/split_v_op.cc:172:12: note: in instantiation of member function 'tensorflow::SplitVOpCPU<int, long long>::Compute' requested here\r\n      explicit SplitVOpCPU(OpKernelConstruction* c) : Base(c) {}\r\n               ^\r\n    tensorflow/core/kernels/split_v_op.cc:355:19: note: in instantiation of member function 'tensorflow::SplitVOpCPU<int, long long>::SplitVOpCPU' requested here\r\n    TF_CALL_ALL_TYPES(REGISTER_SPLIT_LEN);\r\n                  ^\r\n    tensorflow/core/kernels/split_v_op.cc:212:28: note: insert an explicit cast to silence this issue\r\n              prefix_dim_size, split_sizes_vec[i], suffix_dim_size};\r\n                               ^~~~~~~~~~~~~~~~~~\r\n                               static_cast<int>( )\r\n    tensorflow/core/kernels/split_v_op.cc:212:28: error: non-constant-expression cannot be narrowed from type 'value_type' (aka 'long long') to 'int' in initializer list [-Wc++11-narrowing]\r\n              prefix_dim_size, split_sizes_vec[i], suffix_dim_size};\r\n                               ^~~~~~~~~~~~~~~~~~\r\n    tensorflow/core/kernels/split_v_op.cc:172:12: note: in instantiation of member function  'tensorflow::SplitVOpCPU<float, long long>::Compute' requested here\r\n      explicit SplitVOpCPU(OpKernelConstruction* c) : Base(c) {}\r\n               ^\r\n    tensorflow/core/kernels/split_v_op.cc:355:19: note: in instantiation of member function 'tensorflow::SplitVOpCPU<float, long long>::SplitVOpCPU' requested here\r\n    TF_CALL_ALL_TYPES(REGISTER_SPLIT_LEN);\r\n                      ^\r\n    tensorflow/core/kernels/split_v_op.cc:212:28: note: insert an explicit cast to silence this issue\r\n              prefix_dim_size, split_sizes_vec[i], suffix_dim_size};\r\n                               ^~~~~~~~~~~~~~~~~~\r\n                               static_cast<int>( )\r\n    tensorflow/core/kernels/split_v_op.cc:212:28: error: non-constant-expression cannot be narrowed from type 'value_type' (aka 'long long') to 'int' in initializer list [-Wc++11-narrowing]\r\n              prefix_dim_size, split_sizes_vec[i], suffix_dim_size};\r\n                               ^~~~~~~~~~~~~~~~~~\r\n    tensorflow/core/kernels/split_v_op.cc:172:12: note: in instantiation of member function 'tensorflow::SplitVOpCPU<tensorflow::bfloat16, long long>::Compute' requested here\r\n      explicit SplitVOpCPU(OpKernelConstruction* c) : Base(c) {}\r\n               ^\r\n    tensorflow/core/kernels/split_v_op.cc:356:1: note: in instantiation of member function 'tensorflow::SplitVOpCPU<tensorflow::bfloat16, long long>::SplitVOpCPU' requested here\r\n    REGISTER_SPLIT_LEN(bfloat16);\r\n    ^\r\n    tensorflow/core/kernels/split_v_op.cc:353:3: note: expanded from macro 'REGISTER_SPLIT_LEN'\r\n      REGISTER_SPLIT(type, int64);\r\n      ^\r\n    tensorflow/core/kernels/split_v_op.cc:349:27: note: expanded from macro 'REGISTER_SPLIT'\r\n                              SplitVOpCPU<type, len_type>);\r\n                              ^\r\n    tensorflow/core/kernels/split_v_op.cc:212:28: note: insert an explicit cast to silence this issue\r\n          prefix_dim_size, split_sizes_vec[i], suffix_dim_size};\r\n                           ^~~~~~~~~~~~~~~~~~\r\n                           static_cast<int>( )\r\n    3 errors generated.\r\n    Use --strategy=CppCompile=standalone to disable sandboxing for the failing actions.\r\n    Target //tensorflow/examples/android:tensorflow_demo failed to build\r\n    Use --verbose_failures to see the command lines of failed build steps.\r\n    INFO: Elapsed time: 468.550s, Critical Path: 454.15s", "comments": ["Can you give us more information about version (TF, OS, compiler, etc) and what changed between the last successful run and this one?\r\n\r\nCC @petewarden ", "@drpngx Thanks for your reply. I'll try to do a `git bisect` to figure out the offending commit.\r\n\r\nI'm building the Android demo from the latest master branch (as of right now) on Mac OS X. \r\n\r\nCompiler info:\r\n\r\n    \u276f gcc --version\r\n    Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1\r\n    Apple LLVM version 8.0.0 (clang-800.0.42.1)\r\n    Target: x86_64-apple-darwin16.4.0\r\n    Thread model: posix\r\n    InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n", "Which NDK is this? For best results try using [r12b](https://developer.android.com/ndk/downloads/older_releases.html) -- Bazel is known to have problems with more recent releases.", "@andrewharp It's r14b. I'll try r12b and see how it goes. :)", "OK. I just did a `bazel clean` and used `r12b` instead. Interestingly I'm receiving a different set of error messages:\r\n\r\n```\r\nERROR: /Users/hao.hu/GitHub/tensorflow/tensorflow/tensorflow/contrib/android/BUILD:29:1: C++ compilation of rule '//tensorflow/contrib/android:android_tensorflow_inference_jni' failed: Process exited with status 1 [sandboxed].\r\ntensorflow/contrib/android/asset_manager_filesystem.cc: In member function 'virtual tensorflow::Status tensorflow::{anonymous}::RandomAccessFileFromAsset::Read(tensorflow::uint64, size_t, tensorflow::StringPiece*, char*) const':\r\ntensorflow/contrib/android/asset_manager_filesystem.cc:97:69: error: 'AAsset_seek64' was not declared in this scope\r\n     off64_t new_offset = AAsset_seek64(asset.get(), offset, SEEK_SET);\r\n                                                                     ^\r\ntensorflow/contrib/android/asset_manager_filesystem.cc:98:52: error: 'AAsset_getLength64' was not declared in this scope\r\n     off64_t length = AAsset_getLength64(asset.get());\r\n                                                    ^\r\ntensorflow/contrib/android/asset_manager_filesystem.cc: In member function 'virtual tensorflow::Status tensorflow::AssetManagerFileSystem::NewReadOnlyMemoryRegionFromFile(const string&, std::unique_ptr<tensorflow::ReadOnlyMemoryRegion>*)':\r\ntensorflow/contrib/android/asset_manager_filesystem.cc:158:68: error: 'AAsset_openFileDescriptor64' was not declared in this scope\r\n   int fd = AAsset_openFileDescriptor64(asset.get(), &start, &length);\r\n                                                                    ^\r\ntensorflow/contrib/android/asset_manager_filesystem.cc:173:44: error: 'AAsset_getLength64' was not declared in this scope\r\n     length = AAsset_getLength64(asset.get());\r\n                                            ^\r\ntensorflow/contrib/android/asset_manager_filesystem.cc: In member function 'virtual tensorflow::Status tensorflow::AssetManagerFileSystem::GetFileSize(const string&, tensorflow::uint64*)':\r\ntensorflow/contrib/android/asset_manager_filesystem.cc:214:38: error: 'AAsset_getLength64' was not declared in this scope\r\n   *s = AAsset_getLength64(asset.get());\r\n                                      ^\r\n```\r\n\r\nI'll try r13b and see what I get. Maybe that was the NDK version I was using when I successfully built the Android demo.", "What command line are you using to build, and what NDK API level do you have set in your WORKSPACE file? ", "@andrewharp \r\n\r\nI'm using this command as [instructed](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android):\r\n\r\n```\r\nbazel build -c opt //tensorflow/examples/android:tensorflow_demo\r\n```\r\n\r\nAnd this is the way I set SDK and NDK API levels:\r\n\r\n```\r\nandroid_sdk_repository(\r\n  name = \"androidsdk\",\r\n  api_level = 25,\r\n  build_tools_version = \"25.0.2\",\r\n  path = \"/usr/local/Cellar/android-sdk/24.4.1_1/\",\r\n)\r\n\r\nandroid_ndk_repository(\r\n  name=\"androidndk\",\r\n  path=\"/usr/local/Cellar/android-ndk/r13b/\",\r\n  api_level=13)\r\n```", "Ah, your ndk api_level is too low: it needs to be at least 14.\r\n\r\nI'd also leave the NDK at r12b (this is not tied directly to the api_level) for the time being until Bazel is confirmed to have no issues with anything more recent. I think the narrowing cast errors you were getting earlier may have come from that incompatibility.", "Ah, I was so stupid. This fixes the problem!\r\n\r\nFor those careless guys who might meet the same problem in the future, I observed the original error message when my `android_ndk_repository` is defined like this:\r\n\r\n```\r\nandroid_ndk_repository(\r\n  name=\"androidndk\",\r\n  path=\"/usr/local/Cellar/android-ndk/r14b/\",\r\n  api_level=14)\r\n```\r\n\r\nI didn't check anything and I thought that integer value in `r%db` should match the value of `api_level`. So while using `r12b` and `r13b` I've basically changed `api_level` to 12 and 13, respectively.\r\n\r\nThis is not the case, it is specified [here](https://github.com/bazelbuild/bazel/tree/master/examples/android) in the bazel repository that \"Similarly, for the `android_ndk_repository` rule, the value of the `api_level` attribute corresponds to a directory containing the NDK libraries for that API level.\"\r\n\r\nThis config now builds successfully:\r\n\r\n```\r\nandroid_ndk_repository(\r\n  name=\"androidndk\",\r\n  path=\"/usr/local/Cellar/android-ndk/r12b/\",\r\n  api_level=14)\r\n```", "@andrewharp \r\n\r\nThanks again for your help. BTW, using `r13b` and `api_level=14` gives me the same error messages as the first post.\r\n\r\nOne more thing to add, using `r12b` and `api_level=14`, the original error message now becomes warnings like this:\r\n\r\n```\r\ntensorflow/core/kernels/split_v_op.cc: In instantiation of 'void tensorflow::SplitVOpCPU<T, Tlen>::Compute(tensorflow::OpKernelContext*) [with T = int; Tlen = long long int]':\r\ntensorflow/core/kernels/split_v_op.cc:401:1:   required from here\r\ntensorflow/core/kernels/split_v_op.cc:212:63: warning: narrowing conversion of 'split_sizes_vec.std::vector<_Tp, _Alloc>::operator[]<long long int, std::allocator<long long int> >(((std::vector<long long int, std::allocator<long long int> >::size_type)i))' from '__gnu_cxx::__alloc_traits<std::allocator<long long int> >::value_type {aka long long int}' to 'int' inside { } [-Wnarrowing]\r\n```", "Great to see this is solved!\r\n\r\nThanks @andrewharp for the help! Feel free to coordinate with @wolffg if we need to update the docs.", "I'll add some more detailed comments in the WORKSPACE file to clarify this.", "Bazel 0.4.5 should work with NDK13 and NDK14. However, there is a substantial change from NDK12. In NDK12, GCC was used by default. In NDK13 and NDK14, clang is the default. I believe that gcc and clang are treating c++11-narrowing differently.", "Builds fine with Bazel 0.5.3 too, given that we use NDK12. However, NDK 13 or higher do fail. So the key is to use NDK12.\r\n\r\nThe compilation failures have got to do with narrowing conversion as @aj-michael stated.", "Is there any plan to fix this? Seems that Tensorflow should definitely support the latest NDK version,", "@d4I3k I've submitted a change internally to fix building with NDK r13+, should show up in the next push to github. I think right now r14b is the recommended NDK for bazel, but I also tested with r15c and that seems to work fine as well.", "Bazel 0.5.4 supports up to NDK14. If you use NDK15, Bazel will pretend like it's NDK14 and may work, but there's no guarantees.", "@andrewharp I'm running into issues compiling still:\r\n\r\nSame error as: https://github.com/tensorflow/tensorflow/issues/9280\r\n\r\nhttps://travis-ci.org/luk-ai/build-tensorflow/builds/273170125#L4187", "@d4l3k When did you clone? The -fomit-frame-pointer flag added to tf_copts() in https://github.com/tensorflow/tensorflow/commit/d410a83207bdf63ead87a145f97377b7eb1f96bf should take care of the insufficient register error.\r\n\r\nI've just now re-cloned and verified that the command:\r\n`bazel build --verbose_failures //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a` works on my system using NDK r14b.", "@andrewharp I cloned shortly ~30min before posting. I was previously using https://github.com/urho3d/android-ndk, but since have switched to just downloading the standard package and extracting which seems to be working for the most part.\r\n\r\nI'm now running into https://github.com/tensorflow/tensorflow/issues/12979"]}, {"number": 8640, "title": "Java:  The TensorFlow library wasn't compiled to use AVX / SSE / FMA instructions, but these are available on your machine and could speed up CPU computations", "body": "I'm building a small project using the current java binding of TensorFlow - [tensorflow-java ](https://github.com/loretoparisi/tensorflow-java)\r\nWhen running the examples of inception network I get this warning about the platform cpu instruction set that could be used\r\n\r\n```\r\n2017-03-23 01:17:20.243925: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-03-23 01:17:20.243980: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-03-23 01:17:20.243986: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-03-23 01:17:20.243990: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-03-23 01:17:20.243994: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n```\r\n\r\nIs it possibile to build the jar [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java) enabling SSE, AVX and FMA instruction set?", "comments": ["You need to build with `--config=opt` which will do `-march=native`.\r\n\r\n@asimshankar ", "@loretoparisi To enable these you'll have to build from source using the bazel flags @drpngx suggested.\r\n\r\nUnfortunately, in the near term, the release binaries will be built without some of these (see https://github.com/tensorflow/tensorflow/issues/8266#issuecomment-285919845 for a discussion).\r\n\r\nNote that these are just warnings, there should not be any correctness issues. So if you're prototyping or are not optimizing performance of your models, then you can continue to use the release binaries.\r\n\r\nHope that helps.\r\nClosing this out for the same reasons as #8266 ", "@asimshankar @drpngx Thank you.\r\nIs it possibile to use a different Inception model? I would like to switch to a different pre-trained model like `Inception-ResNet-v2`\r\n\r\n(taken from [here](https://github.com/tensorflow/models/tree/master/slim)).", "@loretoparisi : Such questions are better asked on [StackOverflow](https://www.stackoverflow.com/questions/tagged/tensorflow). We try to keep the GitHub issues focused on bugs and feature requests."]}, {"number": 8639, "title": "Cherrypick fix windows gpu build (#8603)", "body": "Fixing Windows release build.", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Jenkins, test this please. (Re-running due to failed cmake)"]}, {"number": 8638, "title": "Support pandas Series and DataFrame as input for tf.constant", "body": "This fix tries to address the proposal raised in #2318 to support pandas Series and DataFrame as input for tf.constant.\r\n\r\nAs was explained in #2318, Series and DataFrame has `__array__` method, which is the standard API used for indicating that an object is coercible into NumPy arrays.\r\n\r\nThis fix adds the support for objects with `__array__` to be served as input for tf.constant.\r\n\r\nThis fix fixes #2318.", "comments": ["Can one of the admins verify this patch?", "@shoyer Thanks for the review. The PR has been updated with added test case of `testMockArray` which has the dummy class that implements `__array__`. Please take a look.", "Just to be clear, I'm Googler and TensorFlow contributor, but not an owner for this part of the code. Somebody else will triage this PR and assign it for review.", "Thanks @martinwicke @alextp for the review. The PR has been updated as suggested. Please take a look.", "Jenkins, test this please."]}, {"number": 8637, "title": "Add the corresponding --action_env to .bazelrc from ./configure", "body": "This will make the fetch phase reproducible so the refetch does\r\nnot happens unless people re-run configure (the environment variable\r\nare stored in the .bazelrc file).\r\n\r\nFixes #8619.", "comments": ["Only one failure on the GPU build, is it a flake? ", "That is indeed a flake. I will send a change to disable it for now.", "@gunan This change is missing from HEAD, can you check what happened? A merge mistake?", "@wicke @yifeif FYI It is almost like this change never got merged to master at all.\r\nAll gone from history, too. Not sure what happened.\r\n\r\nCould you resend the change, if you have it handy?"]}, {"number": 8636, "title": "[CMake] Fix for building tools when GPU support is enabled", "body": "Before this change, benchmark_model would not build with GPU support enabled.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.\r\n@vit-stepanovs Would you like me to test this on a windows-gpu machine, or have you already taken care of that?", "mac tests failed due to a bad machine.\r\nJust to verify,\r\nJenkins, test this please.", "@gunan I tested this locally with GPU support enabled and it worked... It would not hurt to still test it as part of the check in (note that most tools are probably not built by CI, so it would only help ensure the core is not broken after the change).", "I had to take most of our windows machines offline due to required updates to base images.\r\nI will merge this then reach out to you if I see any failures.", "http://ci.tensorflow.org/job/tf-pr-win-cmake-gpu/13/", "windows cmake issue was a missing package we forgot to reinstall during maintenance.\r\nReinstalled now."]}, {"number": 8635, "title": "Add check op for assert valid sample in Bernoulli for contrib.distributions", "body": "Title. The style for asserts seemed to vary across the distribution implementations. I followed Binomial's. It implements `_maybe_assert_valid_sample` and checks via utility functions such as `embed_check_nonnegative_discrete` in `distributions_util.py`.\r\n\r\nFixes #8583. Feedback appreciated. After merging, I can submit a PR adding similar asserts for other discrete distributions.", "comments": ["Can one of the admins verify this patch?", "Can you disable the check_integer test?  I don't think it's necessary.", "Done.", "Bump. I'm happy to make changes. Afterwards I can proceed with robustifying other distributions.", "@ebrevdo good to go?", "LGTM\n\nOn Mon, Apr 10, 2017 at 10:21 AM, drpngx <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> good to go?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/8635#issuecomment-293018504>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim1oYKPQbb1oSlwyThZfFdyRhE4FAks5rumUjgaJpZM4Ml45Q>\n> .\n>\n", "Jenkins, test this please.", "Mac fails with some worrisome error. Trying again.\r\n\r\nJenkins, test this please.\r\n\r\n```\r\nFAIL: //tensorflow/contrib/xla_tf_graph:xla_tf_graph_util_test (see /Volumes/Transcend/bazel_cache/13e370a18c169b19baeafefb05212b85/execroot/tensorflow-pull-requests-mac/bazel-out/local-opt/testlogs/tensorflow/contrib/xla_tf_graph/xla_tf_graph_util_test/test.log).\r\nINFO: From Testing //tensorflow/contrib/xla_tf_graph:xla_tf_graph_util_test:\r\n==================== Test output for //tensorflow/contrib/xla_tf_graph:xla_tf_graph_util_test:\r\ndyld: malformed mach-o: load commands size (32744) > 32768\r\nexternal/bazel_tools/tools/test/test-setup.sh: line 159: 23448 Abort trap: 6           \"${TEST_PATH}\" \"$@\"\r\n```"]}, {"number": 8634, "title": "Fixed Minor Bug in tensorflow/python/platform/googletest.py", "body": "There was a bug in the function `GetTempDir` of the module, in line 118.\r\nI found this bug when I built tensorflow from source on my system and tried to test the install and it gave the following error while simply importing tensorflow in python shell. \r\n```\r\nSyntaxError: name '_googletest_temp_dir' is used prior to global declaration\r\n```\r\nThe variable was already reference at line number : 103 so thats why the error was showing up.\r\nThe global declaration was moved to staring of the function to solve the issue.", "comments": ["Can one of the admins verify this patch?", "I also encountered this bug,\r\n\r\n```\r\n$ tensorboard --logdir=log\r\nTraceback (most recent call last):\r\n  File \"/usr/bin/tensorboard\", line 7, in <module>\r\n    from tensorflow.tensorboard.tensorboard import main\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 104, in <module>\r\n    from tensorflow.python.platform import test\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/platform/test.py\", line 38, in <module>\r\n    from tensorflow.python.framework import test_util as _test_util\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/framework/test_util.py\", line 46, in <module>\r\n    from tensorflow.python.platform import googletest\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/platform/googletest.py\", line 118\r\n    global _googletest_temp_dir\r\nSyntaxError: name '_googletest_temp_dir' is used prior to global declaration\r\n```", "Thanks for the contribution.\r\nJenkins, test this please."]}]