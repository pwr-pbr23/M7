[{"number": 39143, "title": "Empty profile overview page in Tensorboard", "body": "**System information**\r\n- Stock Tensorflow example\r\n- Linux Ubuntu 18.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version (tf-nightly-gpu): v1.12.1-28892-g90dd57bb43 2.2.0-dev20200405\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: 10.1/7.6.5\r\n- GPU model and memory: GTX 1080 8G\r\n\r\n**Describe the current behavior**\r\nI have been running the tutorial as given at https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras. I have installed the exact same TF version as used in the tutorial, the matching nightly Tensorboard version (2.3.0a20200405), and latest tensorboard_plugin_profile. \r\n\r\nThe profile page displays correctly, but I'm getting only zeroes, and empty graph \r\n![image](https://user-images.githubusercontent.com/4252621/80950251-ca5fc780-8df5-11ea-8028-18438b4fced2.png)\r\n\r\nDown the \"Performance summary\" panel, I'm also getting the message: \"WARNING: No step markers observed and hence the step time is actually unknown. This may happen if your profiling duration is shorter than the step time. In that case, you may try to profile longer.\"\r\n\r\n**Describe the expected behavior**\r\nWhen opening Tensorboard, I was expecting non-zero numbers in the \"Performance Summary\" and \"Step-time Graph\" panels as shown in the figure from the tutorial (https://github.com/tensorflow/tensorboard/blob/master/docs/images/profiler_overview_page_bad_ip.png?raw=1). \r\n\r\n**Standalone code to reproduce the issue**\r\nThe mentioned tutorial as-is.\r\n", "comments": ["I'm experiencing exactly the same error under the same circumstances. I've also followed the same tutorial.\r\n\r\nSystem information\r\n\r\nStock Tensorflow example\r\nWindows 10\r\nTensorFlow installed from: pip\r\nTensorFlow version (tf-nightly-gpu): v1.12.1-28892-g90dd57bb43 2.2.0-dev20200405\r\nPython version: 3.6.9\r\nCUDA/cuDNN version: 10.1/7.6.5\r\nGPU model and memory: RTX 2080 8G", "@pbruneau,\r\nI was able to run the code without any issues. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/7b40a39a0b3b3b59d4d54e86585f7b88/39143.ipynb).\r\n\r\nAlso, could you please check if you are using nightly builds of TensorFlow and TensorBoard and have run the uninstall cell twice, to uninstall both the 1.15.0 and 2.1.0 version of TensorFlow and TensorBoard. Thanks!", "I got the same problem. No number in overview_page with \"WARNING: No step markers observed and hence the step time is actually unknown. This may happen if your profiling duration is shorter than the step time. In that case, you may try to profile longer.\" However, there is some number in the page tensorflow_stats.\r\n\r\nSystem information\r\n\r\nUbuntu 18.04\r\nDocker : tensorflow/tensorflow   2.2.0rc0-gpu-py3-jupyter\r\nTensorFlow version: tensorflow-gpu             2.2.0rc0\r\nTensorBoard version: tensorboard                2.2.0\r\nCUDA/cuDNN version: 10.2/7.6.5\r\nGPU model and memory:GTX 1080Ti 11g", "@amahendrakar I just double-checked:\r\n- I installed the latest tf-nightly, tb-nightly and tensorboard_plugin_profile as expected\r\n- This is a fresh install (docker container), so I guess the prior uninstalls are pointless (?)\r\n- I ran the same code otherwise\r\n- Still the same result (i.e. all zeroes) on my side\r\n\r\nOn my machine steps are much shorter (7ms on average) - but this should not be an obstacle?\r\nAlso, the notebook does not tell about the underlying config - in particular, can you tell the Python, CUDA and CuDNN versions you have behind the scenes, plz?", "@pbruneau,\r\nThank you for the update. Below are the configuration details for the Colab gist\r\n- Python v3.6.9\r\n- CUDA Version 10.1.243\r\n- CUDNN_MAJOR 7\r\n  CUDNN_MINOR 6\r\n  CUDNN_PATCHLEVEL 5", "@amahendrakar OK so it seems we have same Python, CUDA and CuDNN versions... I'm really baffled by this problem :/ According to the other answers, it seems that I'm not an isolated case BTW.\r\n\r\nTo be perfectly accurate about my execution context:\r\n- I put the code from the notebook in a .py file, and run it with `python3 file.py`\r\n- The only difference in my code regards logs paths (so that my subsequent call to tensorboard in the CLI is easier), i.e. my `model.fit` goes like:\r\n        \r\n        tboard_callback = tf.keras.callbacks.TensorBoard(log_dir = \"./logs\", \r\n            histogram_freq = 1, \r\n            profile_batch = '500,520')\r\n\r\n        model.fit(ds_train, \r\n            epochs=2, \r\n            validation_data=ds_test, \r\n            callbacks = [tboard_callback])\r\n        \r\n- After I ran `file.py`, I launch `tensorboard --logdir \"./logs\" --bind_all`. I use `--bind_all` as it lives inside a docker container, and I view tensorboard on a different machine from the one where the python code ran.\r\n\r\n- I access tensorboard using Chrome on the client machine - Tensorboard opens fine (i.e. I get the display below - loads correctly, but with \"erroneous zeroes\"). NB that all other panels (e.g. scalars, histograms) display correctly.\r\n![image](https://user-images.githubusercontent.com/4252621/81175352-b35ed800-8fa3-11ea-8ecf-4639f2d912bf.png)\r\n\r\nIs it possible for you to try outside colab (e.g. regular Python3 CLI), to check and see it works similarly, please? I know there shouldn't be a difference, but at this stage I don't have another explanation...", "Hi, I've also encountered similar problem, but had notice some errors in the terminal.\r\nI'm running nightly docker image with GPU support (tensorflow/tensorflow:nightly-gpu-py3-jupyter).\r\nThe errors were as follows:\r\n```\r\n2020-05-06 22:12:04.575505: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1372] Profiler found 1 GPUs\r\n2020-05-06 22:12:04.576324: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcupti.so.10.1\r\n2020-05-06 22:12:04.677122: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1419] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\r\n2020-05-06 22:12:04.678646: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1458] function cupti_interface_->ActivityRegisterCallbacks( AllocCuptiActivityBuffer, FreeCuptiActivityBuffer)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\r\n2020-05-06 22:12:04.681355: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->EnableCallback( 0 , subscriber_, CUPTI_CB_DOMAIN_DRIVER_API, cbid)failed with error CUPTI_ERROR_INVALID_PARAMETER\r\n2020-05-06 22:12:06.055379: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-05-06 22:12:06.214657: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-05-06 22:12:16.711500: I tensorflow/core/profiler/lib/profiler_session.cc:145] Profiler session started.\r\n2020-05-06 22:12:16.711571: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1419] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI_ERROR_NOT_INITIALIZED\r\n2020-05-06 22:12:16.711590: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1458] function cupti_interface_->ActivityRegisterCallbacks( AllocCuptiActivityBuffer, FreeCuptiActivityBuffer)failed with error CUPTI_ERROR_NOT_INITIALIZED\r\n2020-05-06 22:12:27.126307: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->EnableCallback( 0 , subscriber_, CUPTI_CB_DOMAIN_DRIVER_API, cbid)failed with error CUPTI_ERROR_INVALID_PARAMETER\r\n2020-05-06 22:12:27.131846: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:217]  GpuTracer has collected 0 callback api events and 0 activity events.\r\n```\r\n\r\nI've solved my problem thanks to this issue: #35860 [(solution)](https://github.com/tensorflow/tensorflow/issues/35860#issuecomment-586558743)\r\n\r\nIn my case the problem was I've been running the Docker container with a non-root user and in an unprivileged mode. After switching back to the root user (default, no `--user`, no `--group-add`) and adding `--privileged` option the issue has gone.\r\nCheck if it is also your case!\r\n", "> I got the same problem. No number in overview_page with \"WARNING: No step markers observed and hence the step time is actually unknown. This may happen if your profiling duration is shorter than the step time. In that case, you may try to profile longer.\" However, there is some number in the page tensorflow_stats.\r\n> \r\n> System information\r\n> \r\n> Ubuntu 18.04\r\n> Docker : tensorflow/tensorflow 2.2.0rc0-gpu-py3-jupyter\r\n> TensorFlow version: tensorflow-gpu 2.2.0rc0\r\n> TensorBoard version: tensorboard 2.2.0\r\n> CUDA/cuDNN version: 10.2/7.6.5\r\n> GPU model and memory:GTX 1080Ti 11g\r\n\r\nI resolved the problem thanks to this issue too. https://github.com/tensorflow/tensorflow/issues/35860#issuecomment-586558743\r\n\r\nIn my case is using docker run option '--privileged=true'.", "@antonipelka This is so solving my problem, thanks! It works now :)\r\n\r\nI should have inspected the `model.fit` logs, I actually had 2 problems:\r\n- My `LD_LIBRARY_PATH` was not right (fixed according to https://github.com/tensorflow/tensorflow/issues/35860#issuecomment-583858751)\r\n- I also had to run my container with the `privileged=true` option (see https://github.com/tensorflow/tensorflow/issues/35860#issuecomment-586558743)"]}, {"number": 39141, "title": "-", "body": "-", "comments": ["@IsaacPatole \r\n\r\nThis is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at Stackoverflow. There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "@ravikyram thanks for the quick response, sorry this issue didnt belong here, I got confused between one another repo of tensorflow API. Have a great day"]}, {"number": 39140, "title": "Can't get nbextensions to work using Tensorflow docker gpu image", "body": "**Describe the problem**\r\nI am using a Jupyter gpu docker image on Ubuntu 18.04 LTS and I can't get nbextensions to work. I made a Dockerfile adding the nbextensions and I see them available on my screen but when I enable them nothing happens on my Jupyter Notebook. This would made my productivity if I could get these nbextensions to work. \r\n\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 2.1\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: Tensorflow Docker image\r\n- GPU model and memory: 1080Ti\r\n\r\nHere is the Docker file that I build my image\r\n```\r\nFROM tensorflow/tensorflow:2.1.0-gpu-py3-jupyter\r\n\r\nRUN pip install --upgrade pip\r\nRUN pip install jupyter_contrib_nbextensions\r\nRUN jupyter contrib nbextension install --system\r\nRUN pip install pandas \r\nRUN pip install matplotlib\r\nRUN pip install scikit-learn\r\nRUN pip install opencv-python \r\n```\r\n\r\nThen I build it \r\n`docker build -t tf21 .`\r\n\r\nThen run it \r\n`sudo docker run -u $(id -u):$(id -g) --gpus all --rm -it -p 8888:8888 -v $(pwd):/tf/notebook tf21`\r\n\r\nFrom here I open the jupyter in a browser and click on the Nbextensions tab and uncheck \r\n`disable configuration for nbextensions without explicit compatibility (they may break your notebook environment, but can be useful to show for nbextension development)` \r\nThen I start checking the extensions I would like to use.\r\n\r\nWhen I open my notebook, none of my selections work.\r\n\r\nI'm not sure if this helps but this is what outputs on my terminal when I click on the extensions I want.\r\n\r\n```\r\n[E 07:15:56.931 NotebookApp] 500 PATCH /api/config/notebook (172.17.0.1) 1.44ms referer=http://127.0.0.1:8888/tree/notebooks\r\n[E 07:15:57.742 NotebookApp] Uncaught exception PATCH /api/config/notebook (172.17.0.1)\r\n    HTTPServerRequest(protocol='http', host='127.0.0.1:8888', method='PATCH', uri='/api/config/notebook', version='HTTP/1.1', remote_ip='172.17.0.1')\r\n    Traceback (most recent call last):\r\n      File \"/usr/local/lib/python3.6/dist-packages/tornado/web.py\", line 1697, in _execute\r\n        result = method(*self.path_args, **self.path_kwargs)\r\n      File \"/usr/local/lib/python3.6/dist-packages/tornado/web.py\", line 3174, in wrapper\r\n        return method(self, *args, **kwargs)\r\n      File \"/usr/local/lib/python3.6/dist-packages/notebook/services/config/handlers.py\", line 29, in patch\r\n        section = self.config_manager.update(section_name, new_data)\r\n      File \"/usr/local/lib/python3.6/dist-packages/notebook/services/config/manager.py\", line 34, in update\r\n        return self.write_config_manager.update(section_name, new_data)\r\n      File \"/usr/local/lib/python3.6/dist-packages/notebook/config_manager.py\", line 132, in update\r\n        self.set(section_name, data)\r\n      File \"/usr/local/lib/python3.6/dist-packages/notebook/config_manager.py\", line 109, in set\r\n        self.ensure_config_dir_exists()\r\n      File \"/usr/local/lib/python3.6/dist-packages/notebook/config_manager.py\", line 66, in ensure_config_dir_exists\r\n        os.makedirs(self.config_dir, 0o755)\r\n      File \"/usr/lib/python3.6/os.py\", line 210, in makedirs\r\n        makedirs(head, mode, exist_ok)\r\n      File \"/usr/lib/python3.6/os.py\", line 220, in makedirs\r\n        mkdir(name, mode)\r\n    PermissionError: [Errno 13] Permission denied: '/.jupyter'\r\n[W 07:15:57.742 NotebookApp] Unhandled error\r\n[E 07:15:57.742 NotebookApp] {\r\n      \"Host\": \"127.0.0.1:8888\",\r\n      \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:75.0) Gecko/20100101 Firefox/75.0\",\r\n      \"Accept\": \"application/json, text/javascript, */*; q=0.01\",\r\n      \"Accept-Language\": \"en-US,en;q=0.5\",\r\n      \"Accept-Encoding\": \"gzip, deflate\",\r\n      \"Content-Type\": \"application/json\",\r\n      \"X-Xsrftoken\": \"2|92e82d6a|8f936aeda218e6eed7af91384caf3a27|1588574338\",\r\n      \"X-Requested-With\": \"XMLHttpRequest\",\r\n      \"Content-Length\": \"51\",\r\n      \"Origin\": \"http://127.0.0.1:8888\",\r\n      \"Dnt\": \"1\",\r\n      \"Connection\": \"keep-alive\",\r\n      \"Referer\": \"http://127.0.0.1:8888/tree/notebooks\",\r\n      \"Cookie\": \"username-127-0-0-1-8888=\\\"2|1:0|10:1588575705|23:username-127-0-0-1-8888|44:MTJkNzE1ZmJhZWJlNDQzZjlhZmI2ZTM5MDIxMjFjMGE=|a976c227287b6c47a53e2e8460d1df1e8da047bb0ad73f418665f60dcf55fe7f\\\"; _xsrf=2|92e82d6a|8f936aeda218e6eed7af91384caf3a27|1588574338\"\r\n    }\r\n```\r\n\r\nDoes anybody know how I can get this to work? I think it has something to do with the `/.jupyter` Permission denied.\r\n\r\nBy the way, you guys are doing an awesome job! TF2 is so much easier to use than before.", "comments": ["It seems the user you are running the docker container with (internal user) is not allowed to write to the root of the container. Maybe try a different directory / different user / grant permissions?", "Ah yes, when I took away the -u from the run it was able to write to the root of the container and work as expected.\r\n\r\n`sudo docker run --gpus all --rm -it -p 8888:8888 -v $(pwd):/tf/notebook tf21`\r\n\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39140\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39140\">No</a>\n"]}, {"number": 39139, "title": "module 'tensorflow' has no attribute 'keras'", "body": "Specifications:\r\n\r\nwin10 os\r\npython 3.6\r\ntensorflow 2.1.0\r\n\r\nWhile loading the trained model, I used:\r\ntf.keras.models.load_model\r\n\r\nBut it resulted in error, \r\nAttributeError: module 'tensorflow' has no attribute 'keras'", "comments": ["@samra-irshad \r\n\r\nCan you please try \"pip install -U tensorflow\" and follow [this link](https://stackoverflow.com/questions/51724309/attributeerror-module-tensorflow-has-no-attribute-name-scope-with-keras) for reference\r\n\r\n[link](https://github.com/tensorflow/tensorflow/issues/16614#issuecomment-488962620)\r\n[link1](https://github.com/tensorflow/tensorflow/issues/16614#issuecomment-361858036)\r\n[link2](https://github.com/tensorflow/tensorflow/issues/28213)\r\n[link3](https://github.com/tensorflow/tensorflow/issues/33215)\r\n", "I cannot uninstall/install packages myself. I am using a certain HPC facility and they have it installed already. I just load it to run my script", "@samra-irshad\r\nIs this still an issue", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 39138, "title": "can't save stacked lstm", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nthe following model cause error. The error message is \"RuntimeError: Unable to create link (name already exists)\"\r\n\r\n```python\r\n#!/usr/bin/python3\r\nimport tensorflow as tf;\r\n\r\ndef lstm():\r\n  inputs = tf.keras.Input((25,256));\r\n  results = tf.keras.layers.RNN([tf.keras.layers.LSTMCell(units = 512) for i in range(2)])(inputs);\r\n  return tf.keras.Model(inputs = inputs, outputs = results);\r\n\r\nif __name__ == \"__main__\":\r\n  m = lstm();\r\n  m.save('lstm.h5');\r\n```\r\n\r\nbut model without stacking lstm can be serialized successfully.\r\n\r\n```python\r\n#!/usr/bin/python3\r\nimport tensorflow as tf;\r\n\r\ndef lstm():\r\n  inputs = tf.keras.Input((25,256));\r\n  results = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(units = 512))(inputs);\r\n  return tf.keras.Model(inputs = inputs, outputs = results);\r\n\r\nif __name__ == \"__main__\":\r\n  m = lstm();\r\n  m.save('lstm.h5');\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nthe serialization should be processed without problem.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\n#!/usr/bin/python3\r\nimport tensorflow as tf;\r\n\r\ndef lstm():\r\n  inputs = tf.keras.Input((25,256));\r\n  results = tf.keras.layers.RNN([tf.keras.layers.LSTMCell(units = 512) for i in range(2)])(inputs);\r\n  return tf.keras.Model(inputs = inputs, outputs = results);\r\n\r\nif __name__ == \"__main__\":\r\n  m = lstm();\r\n  m.save('lstm.h5');\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n>Traceback (most recent call last):\r\n  File \"<stdin>\", line 3, in <module>\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\", line 1008, in save\r\n    signatures, options)\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/save.py\", line 112, in save_model\r\n    model, filepath, overwrite, include_optimizer)\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\", line 109, in save_model_to_hdf5\r\n    save_weights_to_hdf5_group(model_weights_group, model_layers)\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\", line 631, in save_weights_to_hdf5_group\r\n    param_dset = g.create_dataset(name, val.shape, dtype=val.dtype)\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/h5py/_hl/group.py\", line 139, in create_dataset\r\n    self[name] = dset\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/h5py/_hl/group.py\", line 373, in __setitem__\r\n    h5o.link(obj.id, self.id, name, lcpl=lcpl, lapl=self._lapl)\r\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\r\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\r\n  File \"h5py/h5o.pyx\", line 202, in h5py.h5o.link\r\nRuntimeError: Unable to create link (name already exists)\r\n", "comments": ["@breadbread1984 \r\n\r\nCan you try with the latest TF version 2.2.0-rc4 (`!pip install tensorflow==2.2-rc4`)\r\nLooks like this issue got resolved in TF 2.2.0-rc4. Please check the gist [here](https://colab.sandbox.google.com/gist/ravikyram/8822e1e08c6c9d26c8b41676df3caa0f/untitled851.ipynb).Please verify once and close the issue. Thanks!", "it solved the problem. thx", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39138\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39138\">No</a>\n"]}, {"number": 39137, "title": "Add cardinality calculation for Dataset.unbatch() when possible", "body": "This PR tries to address the issue raised in #39136 where cardinality\r\nof Dataset.unbatch() was always UNKNOWN, even if it might be known\r\nin certain situations.\r\n\r\nThis PR add the cardinality calculation in case the input cardinality\r\nis known and the leading dim of the output shape is known.\r\n\r\nThis PR fixes #39136.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @aaudiber for the review, the PR has been updated. Please take a look and let me know if there are any other issues."]}, {"number": 39136, "title": "Dataset.unbatch() sets cardinality to -2 even when batch size is known", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64 (1909)\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de410 2.1.0\r\n- Python version: 3.7.7\r\n- CUDA/cuDNN version: Using CPU\r\n- GPU model and memory: Using CPU\r\n\r\n**Describe the current behavior**\r\nDoing `Dataset.unbatch()` on dataset with known batch size resets cardinality to -2 (unknown).\r\n\r\n**Describe the expected behavior**\r\nWhen batch size of dataset is known, it should set cardinality to `batch_size * cardinality`.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nds = tf.data.Dataset.range(10) # shape=()\r\nds = ds.batch(2, drop_remainder=True) # shape=(2,)\r\nprint(tf.data.experimental.cardinality(ds)) # 5\r\nds = ds.unbatch() # shape=()\r\nprint(tf.data.experimental.cardinality(ds)) # Should be 10, but is -2 (unknown)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nAlthough cardinality is currently experimental, it is used when traning keras model.", "comments": ["Added a PR #39137 for the fix.", "Was able to reproduce the issue with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/caaefad7a4ba4579d410e203fa20057c/39136-tf-nightly.ipynb), [TF v2.2.0-rc4](https://colab.research.google.com/gist/amahendrakar/b2ef86f09489bdad388f089ab3ba315b/39136-2-2.ipynb#scrollTo=52pV0lOQX93A) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/b01f15c8b4e12c9e879f62eca2fb8395/39136-tf-nightly.ipynb). Please find the attached gist. Thanks!", "@aaudiber please review the PR from Yong.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39136\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39136\">No</a>\n"]}, {"number": 39135, "title": "mlir graph optimization is not built into tf-nightly", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: tf-nightly\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n\r\n**Describe the problem**\r\n\r\nCommit 248bc00 (@ezhulenev ) enables MLIR graph optimizations. However, it looks like this is not built as part of the tf-nightly. As a result, even though the following two methods are available:\r\n```\r\ntf.config.experimental.enable_mlir_graph_optimization\r\ntf.config.experimental.disable_mlir_graph_optimization\r\n```\r\n\r\nThere is no effect by toggle on/off the above two commands with tf-nightly installed.\r\n\r\nFrom the commit, if the commit is carried in tf-nightly, then with \r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\r\n\r\nimport tensorflow as tf\r\n...\r\n...\r\n```\r\n\r\nwe should see something like below (base on the code in commit 248bc00):\r\n\r\n```\r\n  if (!config_proto.experimental().enable_mlir_graph_optimization()) {\r\n    VLOG(1) << \"Skipping MLIR Graph Optimization Pass\"\r\n            << \", session flag not enabled\";\r\n    return Status::OK();\r\n  }\r\n```\r\n\r\n\r\nBut the logs does not show the above contents.\r\n\r\nIs there any way (e.g., pass certain build options) so that the generated pip wheel could carry the MLIR graph optimizations?\r\n\r\n\r\n\r\n\r\n", "comments": ["@jpienaar @joker-eph can you please take a look and help triage?", "@ezhulenev do you know why your optimization isn't available here? Can you look into this?", "I don't know of any special handling of pass registration in the OSS TF.\r\n\r\nDoes this static registration invoked at start time: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tensorflow/transforms/graph_optimization_pass_registration.cc#L25 ?\r\n\r\nThis registration forwarded to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/mlir_graph_optimization_pass.h, and then registered with a runtime here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/mlir_graph_optimization_pass_registration.cc#L24-L28\r\n\r\nFor now it is only registered for TF function, so if you run TF V1 graph with sessions, it will not be invoked.\r\n\r\nFor V1 you need to add `MlirV1CompatOptimizationPassRegistration` for this pass, but that's probably not going to work by default, because V1 graphs (graphs with sessions) contains control flow constructs that are not supported by MLIR importer.", "Couple of different parts here (sessions vs graphs vs functions) that confuse things, so let's start simpler. MLIR importer does support V1 control flow constructs but majority of optimizations and focus is on V2 control flow.\r\n\r\nBut I didn't see V1 references until this point, is this a V1 model? Also I haven't tried the nightly build.\r\n\r\n@ezhulenev is there an integration test of the feature that you added so that we could verify here with a bazel test command?", "No, only mlir-unit-tests for layout optimization.\r\n\r\nAnd a simple python example to test it, but the end state is not observable from python, so it can't be easily converted to end-to-end test.\r\n\r\nWhen executed with GPU available, data format transposed to NCHW.\r\n\r\n```python\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\n\r\nfrom absl import app\r\nfrom absl import flags\r\n\r\nimport numpy as np\r\n\r\nFLAGS = flags.FLAGS\r\n\r\nfrom google3.third_party.tensorflow.python.framework import config\r\n\r\n\r\ndef main(_):\r\n  # Disable layout optimizer to not change input data_format.\r\n  config.set_optimizer_experimental_options({'layout_optimizer': False})\r\n\r\n  # Enable MLIR Graph Optimizations\r\n  config.enable_mlir_graph_optimization()\r\n\r\n  input_dims = [1, 9, 9, 4]\r\n  filter_dims = [3, 3, 4, 8]\r\n  input_t = np.random.rand(*input_dims).astype('f')\r\n  filter_t = np.random.rand(*filter_dims).astype('f')\r\n\r\n  @tf.function\r\n  def fun(input, filter):\r\n    input_tensor = np.random.rand(*input_dims) * 5\r\n    x = tf.nn.convolution(\r\n        input,\r\n        filter,\r\n        padding = \"VALID\",\r\n        data_format='NHWC',\r\n    )\r\n\r\n    return x\r\n\r\n  x = fun(input_t, filter_t)\r\n  print(x)\r\n\r\n\r\nif __name__ == '__main__':\r\n  app.run(main)\r\n```", "For `tf-nightly`, the above example may need to make small modifications to change  `from google3.third_party.tensorflow.python.framework import config`.  Below is the modified one that runs on Colab:\r\n```python3\r\n!pip install tf-nightly\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n# Disable layout optimizer to not change input data_format.\r\ntf.config.optimizer.set_experimental_options({'layout_optimizer': False})\r\n\r\n# Enable MLIR Graph Optimizations\r\ntf.config.experimental.enable_mlir_graph_optimization()\r\n\r\ninput_dims = [1, 9, 9, 4]\r\nfilter_dims = [3, 3, 4, 8]\r\ninput_t = np.random.rand(*input_dims).astype('f')\r\nfilter_t = np.random.rand(*filter_dims).astype('f')\r\n\r\n@tf.function\r\ndef fun(input, filter):\r\n  input_tensor = np.random.rand(*input_dims) * 5\r\n  x = tf.nn.convolution(\r\n      input,\r\n      filter,\r\n      padding = \"VALID\",\r\n      data_format='NHWC',\r\n  )\r\n\r\n  return x\r\n\r\nx = fun(input_t, filter_t)\r\nprint(x)\r\n\r\n```\r\n", "I checked the bazel build file in `tensorflow/BUILD`, and think the issue is that\r\n```\r\n//tensorflow/compiler/mlir:mlir_graph_optimization_pass_registration\r\n```\r\n\r\nneeds to be added to the target `//tensorflow:tensorflow_framework`, as otherwise the mlir optimization will not be included in `libtensorflow_framework.so.2` that is packaged into `tf-nightly`.\r\n\r\nHowever, if I directly add `//tensorflow/compiler/mlir:mlir_graph_optimization_pass_registration` to \r\n`//tensorflow:tensorflow_framework` then a \r\n```\r\nNon-OK-status: RegisterAlreadyLocked(deferred_[i]) status: Already exists: \r\n```\r\nerror will show up.\r\n\r\nThink the issue is caused by inclusion of some components multiple times in bazel rules.", "I have created a PR #39231 to add MLIR graph optimizations registration to be part of the libtensorflow_framework.so, so that it could be packaged to tf-nightly pip install.\r\n\r\nIt didn't make code change, though it made several changes in bazel to remove the dependency of multiple copies, and also splits `import_model.[h|cc]` into 3 parts (no true code change):\r\n\r\n- new `import_base.[h|cc]` consists of ImporterBase class and common shared functions (by `import_model.[h|cc]` and `import_graphdef.[h|cc]`\r\n- new `import_graphdef.[h|cc]` consists of graph only conversion to mlir\r\n- the `import_model.[h|cc]` has also been updated to consist model only conversion to mlir\r\n\r\nThe reason for the above change is that model imports depends on some library in core and it pulled in xla and core ops multiple times.\r\n\r\nWith the change, MLIR graph optimizations registration only need `export_graphdef.cc` and `import_graphdef.cc`, and the multiple copies of xla and core ops are avoided.\r\n\r\nPlease take a look at PR #39231  and let me know if there are any issues.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39135\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39135\">No</a>\n"]}, {"number": 39134, "title": "Fix ValueError with tf.keras.metrics.Recall and float64 keras backend", "body": "\r\nThis PR fixes the issue raised in #36790 where tf.keras.metrics.Recall\r\ncauses ValueError when the backend of the keras is float64:\r\n\r\nThis PR cast the value to the dtype of var as var.assign_add\r\nis being called.\r\n\r\nThis PR fixes #36790.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n", "comments": ["Thanks @pavithrasv for the review. The PR has been updated and now all float32 cast in the function have been changed to use the dtype of the variable. Please take a look and let me know if there are any issues."]}, {"number": 39133, "title": "Implementation of relational operator for complex numbers in XLA", "body": "This is the suggestion to implement relational operators for complex numbers (<, <=, >, >=) as in numpy, i.e., the lexicographical order. In this PR only the \">\" relation was implemented as feedback from the community is important to evaluate if this functionality is wished for and makes sense.\r\n\r\nThis could be leveraged for the implementation of [np.unique in JAX](https://github.com/google/jax/pull/2760) as comparison and sorting of complex arrays is not supported by XLA but possible with numpy. Another usage could be an implementation of [complex QR decomposition in JAX](https://github.com/google/jax/issues/1274).\r\n\r\nThe remaining comparisons for complex numbers and tests for it, will be added if this functionality makes sense.", "comments": ["Matching numpy semantics makes sense to me, could we add tests?", "@joschkabraun Can you please check @cheshire's comments and keep us posted. Thanks!", "Okay, perfect! I\u2019m currently working on it and will finish this weekend!", "I have been looking through the code. Am I correct in assuming the tests for that should be placed into the file elemental_ir_emitter_test.cc?", "@joschkabraun Can you please check @cheshire's comments and keep us posted. Thanks!", "It has been 20 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@joschkabraun, Any update on this PR? Please. Thanks!", "Unfortunately, I am currently very busy and seems advisable if someone else could take this over. I am sorry for answering so late!", "@joschkabraun Thanks for the confirmation. Closing the PR. \r\n"]}, {"number": 39132, "title": "tensorflow-gpu: Could not load dynamic library 'libcudart.so.10.1'", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: \r\n```python\r\nimport tensorflow as tf\r\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n```\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ArchLinux 64bits\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: tensorflow-gpu 2.2.0rc4\r\n- **Python version**: 3.8.2\r\n- **CUDA/cuDNN version**: 10.2.89-5 (from Archlinux repo)\r\n\r\n### Describe the problem\r\nTensorFlow doesn't use my GPU because there is a bug while trying to load \"libcudart.so.10.1\". My system has libcudart.so.10.2 installed\r\nAll the other libraries load fine since they look for libcu***.so.10 and not 10.1\r\n\r\n### Source code / logs\r\n```bash\r\nPython 3.8.2 (default, Apr  8 2020, 14:31:25) \r\n[GCC 9.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n2020-05-03 22:17:42.422067: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-05-03 22:17:42.471297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-05-03 22:17:42.472040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:09:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.683GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-05-03 22:17:42.472169: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2020-05-03 22:17:42.473728: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-05-03 22:17:42.475267: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-05-03 22:17:42.475499: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-05-03 22:17:42.477078: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-05-03 22:17:42.478053: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-05-03 22:17:42.481446: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-05-03 22:17:42.481478: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\nNum GPUs Available:  0\r\n\r\n```\r\n\r\nHere is the CUDA lib I have installed:\r\n```bash\r\n/opt/cuda/doc/man/man7/libcudart.7\r\n/opt/cuda/doc/man/man7/libcudart.so.7\r\n/opt/cuda/targets/x86_64-linux/lib/libcudart.so\r\n/opt/cuda/targets/x86_64-linux/lib/libcudart.so.10\r\n/opt/cuda/targets/x86_64-linux/lib/libcudart.so.10.2\r\n/opt/cuda/targets/x86_64-linux/lib/libcudart.so.10.2.89\r\n/opt/cuda/targets/x86_64-linux/lib/libcudart_static.a\r\n\r\n```\r\n", "comments": ["@gillouche \r\nTo use CUDA 10.2 with Tensorflow 2.2. Please build the Tensorflow from source.\r\nFollow the instructions mentioned [here](https://www.tensorflow.org/install/source). And also take a look at this [comment.](https://github.com/tensorflow/tensorflow/issues/38194#issuecomment-609922803)Thanks!", "Hi @ravikyram \r\nwill the next release of tensorflow-gpu on pypi support CUDA 10.2 ? If so, I guess that I can wait a little bit. I wanted to have everything installed properly in a virtual environment (requirements.txt) so I don't want to build it myself.\r\n\r\nThe python tensorflow cuda package on Arch repository is 2.2.0rc3-2 which shouldn't be compatible either with the CUDA 10.2 version installed from Arch repository. I'll try tonight, maybe they did something to fix the compatibility issue.\r\n\r\nThanks for your help.", "Okay I tried and Archlinux does indeed fix the issue with the package tensorflow-cuda. I'll keep using that and not tensorflow-gpu with pip. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39132\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39132\">No</a>\n", "It seems that libcudart 10.1 and 10.2 are compatible. I was able to hack this on Ubuntu 20.04 by providing a symlink as mentioned by others and run tensorflow 2.2 with libcudart 10.2 (without building from source, just the symlink from a fake library). This may help:\r\nhttps://github.com/tensorflow/tensorflow/issues/38194#issuecomment-629801937", "meet this problem when using tensorflow2.2 with cuda10.2. Solved by ln -s\r\n\r\nsudo ln -s /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudart.so.10.2 /usr/lib/x86_64-linux-gnu/libcudart.so.10.1", "\"sudo ln -s /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudart.so.10.2 /usr/lib/x86_64-linux-gnu/libcudart.so.10.1\"\r\nIt works. Thank you stdcoutzyx.\r\n\r\nIf you have installed cuda-10.2 and still have problems, try this:\r\ncd /usr/local/cuda_10.2/lib64\r\nln -s libcudart.so.10.2 libcudart.so.10.1\r\n", "sudo ln -s /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudart.so.10.2 /usr/lib/x86_64-linux-gnu/libcudart.so.10.1\r\n\r\ndidn't work", "`sudo ln -s /opt/cuda/targets/x86_64-linux/lib/libcudart.so.10.2 /usr/lib/libcudart.so.10.1`\r\n worked for me.", "I'm having this message with cuda 11 installed. As I know cuda 11 should be backward compatible with everything using cuda 10. Does it have any negative effects? It also shows \"Ignore above cudart dlerror if you do not have a GPU set up on your machine.\"\r\n", "> I'm having this message with cuda 11 installed. As I know cuda 11 should be backward compatible with everything using cuda 10. Does it have any negative effects? It also shows \"Ignore above cudart dlerror if you do not have a GPU set up on your machine.\"\r\n\r\nI am facing the same problem. Even Cuda 11 successfully working but tensorflow 2.3 looking cuda 10.1", "> > I'm having this message with cuda 11 installed. As I know cuda 11 should be backward compatible with everything using cuda 10. Does it have any negative effects? It also shows \"Ignore above cudart dlerror if you do not have a GPU set up on your machine.\"\r\n> \r\n> I am facing the same problem. Even Cuda 11 successfully working but tensorflow 2.3 looking cuda 10.1\r\n\r\nSame Here. Pytorch is working fine with GPU's. But getting errors with tensorflow.", "> > I'm having this message with cuda 11 installed. As I know cuda 11 should be backward compatible with everything using cuda 10. Does it have any negative effects? It also shows \"Ignore above cudart dlerror if you do not have a GPU set up on your machine.\"\r\n> \r\n> I am facing the same problem. Even Cuda 11 successfully working but tensorflow 2.3 looking cuda 10.1\r\n\r\nSame problem and I solved it by uninstall tensorflow 2.3, and install tensorflow-gpu 2.0.0.\r\nBefore that, I tried to add path of cuda 11, and also use latest tf-nightly-gpu but they didn't work.", "> meet this problem when using tensorflow2.2 with cuda10.2. Solved by ln -s\r\n> \r\n> sudo ln -s /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudart.so.10.2 /usr/lib/x86_64-linux-gnu/libcudart.so.10.1\r\n\r\nthank you, this works for me (tensorflow2.2 with cuda10.2)", "If you are using conda to manage your envs, just specify the cuda env when installing tensorflow. This way you can keep cuda 11 installed, and still run tf with an old cuda version.\r\n\r\n`conda install tensorflow-gpu cudatoolkit=10.1`", "> Okay I tried and Archlinux does indeed fix the issue with the package tensorflow-cuda. I'll keep using that and not tensorflow-gpu with pip. Thanks!\r\n\r\nhow do you make to do python find tensorflow-cuda? I installed that metapkg but python says that there is no tensorflow installed", "> > Okay I tried and Archlinux does indeed fix the issue with the package tensorflow-cuda. I'll keep using that and not tensorflow-gpu with pip. Thanks!\r\n> \r\n> how do you make to do python find tensorflow-cuda? I installed that metapkg but python says that there is no tensorflow installed\r\n\r\nAre you using the python3 interpreter or another version installed from AUR (python3.6, python3.7, ...) ? You also have the package python-tensorflow-cuda. I am not sure about the differences between all the tensorflow packages in arch repo.", "> > > Okay I tried and Archlinux does indeed fix the issue with the package tensorflow-cuda. I'll keep using that and not tensorflow-gpu with pip. Thanks!\r\n> > \r\n> > \r\n> > how do you make to do python find tensorflow-cuda? I installed that metapkg but python says that there is no tensorflow installed\r\n> \r\n> Are you using the python3 interpreter or another version installed from AUR (python3.6, python3.7, ...) ? You also have the package python-tensorflow-cuda. I am not sure about the differences between all the tensorflow packages in arch repo.\r\n\r\nI'm using tensorflow-cuda and python3, the ofictal rep version and python doesn't detect tensorflow.", "> > > > Okay I tried and Archlinux does indeed fix the issue with the package tensorflow-cuda. I'll keep using that and not tensorflow-gpu with pip. Thanks!\r\n> > > \r\n> > > \r\n> > > how do you make to do python find tensorflow-cuda? I installed that metapkg but python says that there is no tensorflow installed\r\n> > \r\n> > \r\n> > Are you using the python3 interpreter or another version installed from AUR (python3.6, python3.7, ...) ? You also have the package python-tensorflow-cuda. I am not sure about the differences between all the tensorflow packages in arch repo.\r\n> \r\n> I'm using tensorflow-cuda and python3, the ofictal rep version and python doesn't detect tensorflow.\r\n\r\nI tried python-tensorflow-cuda and the other problem is no more but now I have a memory problem with a really tiny NN and little data with a 3070 when that NN was trained first by a 970, I understand nothing :(", "ln -s /usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudart.so.11.0.221 /usr/lib/x86_64-linux-gnu/libcudart.so.10.1", "currently it is working, I'm using the arch tensorflow pkg  ", "> If you are using conda to manage your envs, just specify the cuda env when installing tensorflow. This way you can keep cuda 11 installed, and still run tf with an old cuda version.\r\n> \r\n> `conda install tensorflow-gpu cudatoolkit=10.1`\r\n\r\nThx, this works for me. CUDA 11.0 + python 3.8", "i have same issue,  @ioannist try this  conda install tensorflow-gpu cudatoolkit=10.1\r\nbut still same issue", "> conda install tensorflow-gpu cudatoolkit=10.1\r\n\r\nThanks!", "> `sudo ln -s /opt/cuda/targets/x86_64-linux/lib/libcudart.so.10.2 /usr/lib/libcudart.so.10.1`\r\n> worked for me.\r\n\r\nIt worked for me"]}, {"number": 39131, "title": "Fix BatchNormalization issue with virtual_batch_size when shape has None", "body": "This PR tries to address the issue raised in #32380 where BatchNormalization with virtual_batch_size will throw out error if shape has None:\r\n```\r\nTypeError: Failed to convert object of type <class 'list'> to Tensor. Contents: [8, -1, None, None, 3]. Consider casting elements to a supported type.\r\n```\r\n\r\nThis PR converts None to -1 so that it could be passed as a tensor to `reshape`.\r\n\r\nThis PR fixes #32380.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @alextp for the review. The PR has been updated. Please take a look."]}, {"number": 39130, "title": "pip Install Error: don't could find version that satisfies the requirement tensorflow", "body": "hi, I have win10 and I have tried install through cmd tensorflow with:\r\npip install tensorflow\r\npip install tensorflow-cpu\r\npip3 install --upgrade tensorflow\r\n \r\nAll these pip gave me the same error:\r\n\r\n![image](https://user-images.githubusercontent.com/63893710/80923804-cbd8b380-8d53-11ea-8b1f-af41f380daab.png)\r\n\r\nwhat's happened? What I need do to fix it?\r\n\r\n", "comments": ["I have a possibly related issue:  when I use \r\n`pip install tensorflow`\r\npip installs the most recent release candidate: https://pypi.org/project/tensorflow/2.2.0rc4/\r\nI'm trying to get the most recent stable version, 2.1.0.\r\n\r\nWhen I try:\r\n`pip install tensorflow==2.1.0`\r\n\r\nI get the error message: \r\n```\r\nERROR: Could not find a version that satisfies the requirement tensorflow==2.1.0 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4)\r\nERROR: No matching distribution found for tensorflow==2.1.0\r\n```\r\nI updated pip to 20.1.  Same behavior.\r\nI tried:\r\n`pip install tensorflow>=2.0.0`\r\nAnd still got ver 2.2.0rc4\r\n\r\nWhy is pip installing only the rc's?\r\n\r\n![image](https://user-images.githubusercontent.com/5289393/80924776-c8d3c800-8d3f-11ea-923e-690abed11e28.png)\r\n", "Okay, I think I figured out my issue:  I was using Python 3.8 in my environment. I made a new environment using Python 3.7, and the install worked fine.", "@an-white,\r\nTensorFlow 2 packages require a pip version >19.0. Could you please upgrade pip using the below command and let us know if it works.\r\n\r\n`pip install --upgrade pip`.\r\n\r\nFor more information please check [this TensorFlow guide](https://www.tensorflow.org/install#download-a-package). Thanks!", "hi, I forgot say that I have python 3.8, this can affect? When I have already upgrade pip, this occurs:\r\n\r\n![image](https://user-images.githubusercontent.com/63893710/81001657-1458a580-8e16-11ea-99a5-babdb9eca323.png)\r\n\r\n", "@an-white :  I suggest creating a new env with python 3.7 and try installing TF2 there. ", "> Okay, I think I figured out my issue: I was using Python 3.8 in my environment. I made a new environment using Python 3.7, and the install worked fine.\r\n\r\nI saw this recommendation on other forums and tried it, but it didn't work for me.\r\n\r\nI've tried on 3.7.0 and 3.6.4", "> > Okay, I think I figured out my issue: I was using Python 3.8 in my environment. I made a new environment using Python 3.7, and the install worked fine.\r\n> \r\n> I saw this recommendation on other forums and tried it, but it didn't work for me.\r\n> \r\n> I've tried on 3.7.0 and 3.6.4\r\n\r\nAs I expected the problem was my fault... but I think is something that can pass unnoticed and explaining it here may help people with a similar setup and issue.\r\n\r\nI'm using Python via [pyenv-win](https://github.com/pyenv-win/pyenv-win) which is great, but the thing is that I didn't realize that `pyenv install X.Y.Z` will install the 32bit version and tensorflow **requires 64bit**. So, in my case, it was just a matter of installing an actual 64bit version with `pyenv install X.Y.Z-amd64`", "> hi, I forgot say that I have python 3.8, this can affect? When I have already upgrade pip, this occurs:\r\n\r\n@an-white,\r\nTensorFlow v2.1 is not compatible with Python 3.8. Instead, could you please try installing TF v2.2.0rc4 using the below command \r\n\r\n`pip install tensorflow==2.2.0rc4`\r\n\r\nThanks!", "@an-white,\r\nAlso, could you please check if you are using the 64 bit version of Python. TensorFlow is tested and supported only on 64-bit systems. Thanks!", "Ok, thanks I'm going to check all this recommendation and find if works", "Ready thank you so much, the problem was the version that I have of python 32 bit, and already I install the 64 bit version and only need wait for this finish\r\n\r\n![image](https://user-images.githubusercontent.com/63893710/81077241-b500a100-8eba-11ea-9e04-b8594e359657.png)\r\n \r\nNow only need wait, any other problem I'll tell you", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39130\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39130\">No</a>\n", "thanks  i", "hi..\r\nI am using python 3.8.\r\nwants to:       pip install tensorflow==1.31.1\r\nbut getting error.\r\n![Capture](https://user-images.githubusercontent.com/48278850/84350175-7acfa100-abd6-11ea-94ff-9b686f8e7893.PNG)\r\n", "> hi..\r\n> I am using python 3.8.\r\n> wants to: pip install tensorflow==1.31.1\r\n> but getting error.\r\n\r\n@biswajit310,\r\nTensorFlow 1.13 is not actively supported. Please try installing TensorFlow 2.x. If you need any help please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose), so that we can track the issue there. Thanks!", "verify that you have python 64bits , it solved my problem , i'm using python3.8 win10 64bits"]}, {"number": 39129, "title": "Real system exit()", "body": "`sys.exit(1)` works  definitely", "comments": ["Is there a bug that this fixes?", "@mihaimaruseac Sorry I don't get it completely your comment. You mean if there was a bug which causes this modification of replacing `exit()` by `sys.exit() `?\r\n\r\n- I just saw that all `python-exits` in TensorFlow except for [control_flow_grad.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/control_flow_grad.py) - because it refers to its own exit-function - are _real_ `sys.exit()` and not just `exit()`.\r\n- For me, it makes sense because it is a safe exit as well as is more convenient for python2.7.\r\n\r\n\r\nSorry if it not fits", "We no longer support python2.7 and would prefer one line fixes to be real bug fixes", "So close the request better?"]}, {"number": 39128, "title": "tensorflow in python", "body": "1.  i tried import tnsorflow , and get alot of warning messemges \r\n\r\n2. after import, i runed this code : \r\n\r\n```\r\na = tf.Variable(1, name = \"a\")\r\nb = tf.Variable(2, name = \"b\")\r\nf = a+b\r\nprint(f)\r\n```\r\n\r\nbut this was my put put : \r\nTensor(\"add:0\", shape=(), dtype=int32)\r\n\r\ncan someone know what is the problem ? and what to do ? \r\n\r\n*** i did :  pip   install --upgrade tensorflow\r\nbut it doesnt  #helped....\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@dan881818 \r\nCould you please share the tensor flow version and share complete error log for us to analyse the issue.", "how can i check wich version of tensorflow i have? \r\ni work on jupyter via anaconda, \r\non the prompt i run this code in order to install the tensorflow : \r\n\r\n**conda   create -n tpf3.7 python=3.7\r\nactivate tpf3.7\r\nconda install   tensorflow\r\npip install --upgrade   tensorflow\r\nimport tensorflow**\r\n\r\n**about the error messeges :** \r\n\r\n```\r\nimport tensorflow as tf\r\nC:\\Users\\dansh\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\nC:\\Users\\dansh\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\nC:\\Users\\dansh\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\nC:\\Users\\dansh\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\nC:\\Users\\dansh\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\nC:\\Users\\dansh\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\nC:\\Users\\dansh\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\nC:\\Users\\dansh\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\nC:\\Users\\dansh\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n_np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\nC:\\Users\\dansh\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\nC:\\Users\\dansh\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\nC:\\Users\\dansh\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\nC:\\Users\\dansh\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n\r\n```\r\n", "Please fill in issue template. Please properly format error and code blocks using proper markdown syntax.\r\n\r\nTF version can be obtained with `tf.__version__` after import, or via `pip list`. Actually, please post output of `pip list` as well as a filled in issue template.", "Mihai Maruseac  - did'nt understand what you mean .... what shuold i do in roder the tensorflow work via jupyter ?", "duplicate #31249", "(base) C:\\Users\\dansh>pip show tensorflow\r\nName: tensorflow\r\nVersion: 2.1.0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: c:\\users\\dansh\\anaconda3\\lib\\site-packages\r\nRequires: grpcio, tensorflow-estimator, scipy, wrapt, protobuf, opt-einsum, tensorboard, keras-applications, gast, keras-preprocessing, termcolor, numpy, six, google-pasta, absl-py, wheel, astor\r\nRequired-by:\r\n\r\n(base) C:\\Users\\dansh>pip show numpy\r\nName: numpy\r\nVersion: 1.18.1\r\nSummary: NumPy is the fundamental package for array computing with Python.\r\nHome-page: https://www.numpy.org\r\nAuthor: Travis E. Oliphant et al.\r\nAuthor-email: None\r\nLicense: BSD\r\nLocation: c:\\users\\dansh\\anaconda3\\lib\\site-packages\r\nRequires:\r\nRequired-by: tensorflow, tensorboard, tables, statsmodels, seaborn, scipy, scikit-learn, PyWavelets, pytest-arraydiff, patsy, pandas, opt-einsum, numexpr, numba, mkl-random, mkl-fft, matplotlib, Keras-Preprocessing, Keras-Applications, imageio, h5py, Bottleneck, bokeh, bkcharts, astropy", "hey,\r\nup is my tensorflow + numpy version\r\nName: tensorflow\r\nVersion: 2.1.0\r\nName: numpy\r\nVersion: 1.18.1\r\nwhat i need to do in order tensorflow worm in my jupyter ? \r\ntnx...", "Duplicate of #31249", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39128\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39128\">No</a>\n"]}, {"number": 39127, "title": "Removing unreachable return", "body": "Removed the unreachable `return original_index`\r\n\r\n```python\r\ndef _get_correct_mapping(original_index, nodes):\r\n  # Special handle for the index is -1 case.\r\n  # If it is -1, return the last index.\r\n  if original_index == -1:\r\n    node_indices = nodes.keys()\r\n    node_indices = sorted(node_indices)\r\n    return node_indices[-1]\r\n  else:\r\n    return original_index\r\n  return original_index\r\n```", "comments": []}, {"number": 39126, "title": "removed unreachable original_index", "body": "the last `return original_index` cannot be reached -> removed\r\n\r\n```python\r\ndef _get_correct_mapping(original_index, nodes):\r\n  # Special handle for the index is -1 case.\r\n  # If it is -1, return the last index.\r\n  if original_index == -1:\r\n    node_indices = nodes.keys()\r\n    node_indices = sorted(node_indices)\r\n    return node_indices[-1]\r\n  else:\r\n    return original_index\r\n  return original_index\r\n```", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39126) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 39125, "title": "removed unreachable original_index", "body": "the last `return original_index` cannot be reached -> removed\r\n```python\r\ndef _get_correct_mapping(original_index, nodes):\r\n  # Special handle for the index is -1 case.\r\n  # If it is -1, return the last index.\r\n  if original_index == -1:\r\n    node_indices = nodes.keys()\r\n    node_indices = sorted(node_indices)\r\n    return node_indices[-1]\r\n  else:\r\n    return original_index\r\n  return original_index\r\n```", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39125) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.", "> We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors. If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)? If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\r\n> In order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\r\n> \r\n> \u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39125) for more info**.\r\n\r\n@googlebot I fixed it."]}, {"number": 39124, "title": "Add S3 filesystem support on Windows", "body": "@mihaimaruseac \r\nThis PR adds support for S3 on windows.\r\nSince GCS is supported on windows by this PR: https://github.com/tensorflow/tensorflow/pull/38959. After this PR is merged, we can close this issue: https://github.com/tensorflow/tensorflow/issues/19297", "comments": ["Thank you for the PR", "We should close https://github.com/tensorflow/tensorflow/issues/19297", "Sounds good. Thank you"]}, {"number": 39123, "title": "Fix Huber Loss crashes due to data type mismatch", "body": "This PR tries to address the issue raised in #39004 where setting keras backend to 'float64' causes Huber Loss crash:\r\n```\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:1440 huber\r\n        math_ops.multiply(delta, linear)),\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:180 wrapper\r\n        return target(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:490 multiply\r\n        return gen_math_ops.mul(x, y, name)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:6153 mul\r\n        \"Mul\", x=x, y=y, name=name)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:506 _apply_op_helper\r\n        inferred_from[input_arg.type_attr]))\r\n\r\n    TypeError: Input 'y' of 'Mul' Op has type float64 that does not match type float32 of argument 'x'.\r\n\r\n```\r\n\r\nThis PR fixes the crash by casting delta the same way as y_pred and y_true accordingly.\r\n\r\nThis PR fixes #39004\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @qlzh727 for the review. The PR has been updated. Please take a look."]}, {"number": 39122, "title": "Error when trying to run on TPU - \"Failed to connect to all addresses\"", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, but it used to work. \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 10\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.2.0-dev20200503\r\n- Python version: 3.7.3\r\n\r\n**Describe the current behavior**\r\nHi,\r\nI'm trying to train a BERT-based model on a TPU (using TensorFlow models). It used to work perfectly a week ago, and I haven't changed my code since then, but now whenever I'm trying to run it (on a GCP VM instance connected to a TPU node) I get this:\r\n\r\n```\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py\", line 196, in _create_device_dataset\r\n    ds = _ReincarnatedPerDeviceGenerator(prototype_ds, incarnation_id)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py\", line 181, in __init__\r\n    **self._flat_structure)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 2070, in generator_dataset\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 6810, in raise_from_not_ok_status\r\n2020-05-03 15:47:19.234435: W tensorflow/core/distributed_runtime/eager/remote_tensor_handle_data.cc:76] Unable to destroy remote tensor handles. If you are running a tf.function, it usually indicates some op in the graph gets an error: failed to\r\n connect to all addresses\r\nAdditional GRPC error information:\r\n{\"created\":\"@1588520839.208445209\",\"description\":\"Failed to pick subchannel\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3937,\"referenced_errors\":[{\"created\":\"@1588520839.208443301\",\"description\":\"f\r\nailed to connect to all addresses\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":394,\"grpc_status\":14}]}\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.UnavailableError: failed to connect to all addresses\r\nAdditional GRPC error information:\r\n{\"created\":\"@1588520839.208445209\",\"description\":\"Failed to pick subchannel\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3937,\"referenced_errors\":[{\"created\":\"@1588520839.208443301\",\"description\":\"f\r\nailed to connect to all addresses\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":394,\"grpc_status\":14}]} [Op:GeneratorDataset]\r\nError in atexit._run_exitfuncs:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py\", line 2256, in async_wait\r\n    context().sync_executors()\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py\", line 652, in sync_executors\r\n    pywrap_tfe.TFE_ContextSyncExecutors(self._context_handle)\r\ntensorflow.python.framework.errors_impl.UnavailableError: failed to connect to all addresses\r\nAdditional GRPC error information:\r\n{\"created\":\"@1588520839.208445209\",\"description\":\"Failed to pick subchannel\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3937,\"referenced_errors\":[{\"created\":\"@1588520839.208443301\",\"description\":\"f\r\nailed to connect to all addresses\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":394,\"grpc_status\":14}]}\r\nException ignored in: <function IteratorResourceDeleter.__del__ at 0x7f6cc39591e0>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 539, in __del__\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1224, in delete_iterator\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 6810, in raise_from_not_ok_status\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.UnavailableError: failed to connect to all addresses\r\nAdditional GRPC error information:\r\n{\"created\":\"@1588520839.208445209\",\"description\":\"Failed to pick subchannel\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3937,\"referenced_errors\":[{\"created\":\"@1588520839.208443301\",\"description\":\"f\r\nailed to connect to all addresses\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":394,\"grpc_status\":14}]} [Op:DeleteIterator]\r\nException ignored in: <function IteratorResourceDeleter.__del__ at 0x7f6cc39591e0>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 539, in __del__\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1224, in delete_iterator\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 6810, in raise_from_not_ok_status\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.UnavailableError: failed to connect to all addresses\r\nAdditional GRPC error information:\r\n{\"created\":\"@1588520839.208445209\",\"description\":\"Failed to pick subchannel\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3937,\"referenced_errors\":[{\"created\":\"@1588520839.208443301\",\"description\":\"f\r\nailed to connect to all addresses\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":394,\"grpc_status\":14}]} [Op:DeleteIterator]\r\nException ignored in: <function IteratorResourceDeleter.__del__ at 0x7f6cc39591e0>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 539, in __del__\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1224, in delete_iterator\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 6810, in raise_from_not_ok_status\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.UnavailableError: failed to connect to all addresses\r\nAdditional GRPC error information:\r\n{\"created\":\"@1588520839.208445209\",\"description\":\"Failed to pick subchannel\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3937,\"referenced_errors\":[{\"created\":\"@1588520839.208443301\",\"description\":\"f\r\nailed to connect to all addresses\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":394,\"grpc_status\":14}]} [Op:DeleteIterator]\r\n2020-05-03 15:47:19.384278: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op \r\nID: 144, Output num: 0\r\nAdditional GRPC error information:\r\n{\"created\":\"@1588520839.384194484\",\"description\":\"Error received from peer ipv4:10.71.22.42:8470\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Unable to find the relevant tensor remote_handl\r\ne: Op ID: 144, Output num: 0\",\"grpc_status\":3}\r\n```\r\n\r\nAny ideas about what could have happened?", "comments": ["Possible duplicate of https://github.com/tensorflow/tensorflow/issues/39099", "@danieljannai \r\n\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "> @danieljannai\r\n> \r\n> Request you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!\r\n\r\nHi,\r\nUnfortunately, I can't do that as my codebase is pretty big, but I can assure you that everything worked perfectly a week ago. I tried reverting to the tf-nightly version I used but it didn't help.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39122\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39122\">No</a>\n", "Solved here:\r\nhttps://github.com/tensorflow/tensorflow/issues/39099", "This bug is still there. I have tried many versions and combinations. When I create dataset using from_tensor_slices, it works fine (but only on small datasets. after that protobuf limitations come into play). But when I try from_generator, or when I'm trying to prepare dataset on th fly (by using tokenization inside of map), it's throwing \"failed to connect to all addresses\". I think TF team needs to do something with it."]}, {"number": 39121, "title": "TF 2.2 - Train_step output control", "body": "**System information**\r\n- TensorFlow version (you are using): 2.2 rc1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nTF 2.2 allows modifying train_step() and make_train_function() to leverage the advantages of fit() with a lot of customization. However, due to train_step() being a tf.function, there is no way to extract variables from it (e.g. the model's output during training), other than by adding it to the *outputs* dictionary which goes directly to the logs. By customizing make_train_function() it is possible to do that even with multiple GPUs (concatenating accross devices), but adding that output to the logs creates a domino of problems when using callbacks.\r\n\r\nBelow is an example of mine where the model's outputs are stored with key *output* in the dict, concatenated accross devices, and returned to the dict, which is stored to the logs during each step in fit().\r\n\r\n```\r\n# In make_train_function()\r\n...\r\ndef train_function(iterator):\r\n    data = next(iterator)\r\n    output_dict = self.distribute_strategy.run(\r\n        self.train_step, args=(data,))\r\n    m_output = output_dict.pop('output')\r\n    m_output = reduce_per_replica(\r\n        m_output, self.distribute_strategy, reduction='concat')\r\n    output_dict = reduce_per_replica(\r\n        output_dict, self.distribute_strategy, reduction='first')\r\n    output_dict['output'] = m_output\r\n    return output_dict\r\n...\r\n\r\n# Then in fit (without changes)\r\n...\r\ntmp_logs = train_function(iterator)\r\nif not data_handler.inferred_steps:\r\n  context.async_wait()\r\nlogs = tmp_logs\r\n...\r\n```\r\n\r\nMy suggestion would be changing the fit() function so that instead of assigning the output of train_function() to *tmp_logs*, it allows for additional outputs from train_step(), stored in a distinct variable which can then be accessed via callbacks. Below I have modified train_function() and fit() to return a list that would allow storing desired outputs in a separate variable.\r\n\r\n```\r\ndef train_function(iterator):\r\n    data = next(iterator)\r\n    output_dict = self.distribute_strategy.run(\r\n        self.train_step, args=(data,))\r\n    output_dict = reduce_per_replica(\r\n        output_dict, self.distribute_strategy, reduction='first')\r\n    return output_dict, None\r\n\r\n#OR with user modifications to get some output\r\n\r\ndef train_function(iterator):\r\n    data = next(iterator)\r\n    output_dict = self.distribute_strategy.run(\r\n        self.train_step, args=(data,))\r\n    m_output = output_dict.pop('output')\r\n    m_output = reduce_per_replica(\r\n        m_output, self.distribute_strategy, reduction='concat')\r\n    output_dict = reduce_per_replica(\r\n        output_dict, self.distribute_strategy, reduction='first')\r\n    return output_dict, m_output\r\n\r\n# Then in fit\r\n...\r\ntmp_logs, tmp_step_output = train_function(iterator)\r\nif not data_handler.inferred_steps:\r\n  context.async_wait()\r\nlogs, step_output = tmp_logs, tmp_step_output\r\n...\r\n```\r\nWith this modification and by giving all callbacks access to *step_output* users can customise train_function() and get whatever they want outside the training graph. These modifications would give them absolute freedom over it.\r\n\r\n**Will this change the current api? How?**\r\nAs far as I can see the only changes would be the above code snippets in fit() and make_train_function(), and then adding *step_output* to the default inputs for callbacks.\r\n\r\n**Who will benefit with this feature?**\r\nEveryone. Other than model outputs it can be used to store gradients, and intermediate outcomes. It would allow for much better prototyping with minimal changes to the API.\r\n\r\n**Any Other info.**\r\n", "comments": ["@ManiadisG Thanks for the issue!\r\n\r\n> but adding that output to the logs creates a domino of problems when using callbacks.\r\n\r\nCould you provide example of the errors you've seen? It's probably best to fix the problem at this level IMO", "Thank you for your response.\r\n\r\nThe core of the issue is that the logs variable is handled in a specific way by standard Callbacks that only makes sense for losses and metrics. In my case I wanted to extract the model's output during training so the problems I encountered are based on that. The output was a (N, X) matrix where N is the batch size, and I stored it to the logs dict with a \"output\" key.\r\n\r\nI identified the following issues.\r\n1. The Progbar callback printed the \"output\" as a scalar (same as the loss), I'm assuming via reduce_mean. After an epoch was over it tried to aggregate the result which led to errors if the batches were uneven (due to the last batch not necessarily being the same size as the others).\r\n2. The Tensorboard callback registered \"output\" and presented it in the report same as the losses, again I assume via reduce_mean. I did not test if this leads to errors or if it is just an aesthetic issue, but I'm assuming it would also cause performance issues if I tried to extract larger data and Tensorboard kept track of them.\r\n3. The History callback also registered \"output\" as a scalar and again sometimes produced errors in the aggregation.\r\n\r\nSome of those issues could be solved by using almost exclusively customized Callbacks, which I tried to do with some success. I also tried a Callback that pops \"output\" from the logs dictionary but it appears that other Callbacks access the logs first so it didn't really work. I also couldn't solve possible performance issues from the model possibly having to process/store the output accross an epoch's steps. As of 2.2-rc1, CallbackList is very hard to configure outside of fit() and there is very limited user visibility and control over what the model does with logs by default.\r\n\r\nI would like to point out though that, even if that was not the case, having to rewrite every Callback negates a lot of the advantages of using fit(). And even if these errors were solved individually, not letting the users extract data from the training step without having pre-determined operations applied seems very error-prone.\r\n\r\nTo solve this at the Callback level, may I suggest defining a specific key (e.g. \"user_output\") that all standard Callbacks ignore. That way users would be able to store whatever they want in the logs dictionary under a specific key and handle it in any way they see fit with their own custom Callbacks.\r\n\r\nAlternatively, adding an exception argument to the Callback class that can be passed via fit() to CallbackList would also work. That could take the form of a list of keys (defaulting to None), so that standard Callbacks will ingore the corresponding logs.", "Update.\r\n\r\nIt turns out there is only an error when using the Progbar callback, which at the end of the epoch tries to aggregate the logs. The History and Tensorboard Callbacks do not cause errors and can be overwritten to avoid performance and aesthetic problems.\r\n\r\nBasically it appears that, in my case, the issue can be overcome by customizing Callbacks. Progbar in particular needs to be overwritten and the CallbackList object must be initialized outside fit with tensorflow.python.keras.callbacks.configure_callbacks(). However I'm not sure what would happen with other kinds of output data.\r\n\r\nRegardless, since users can now customize train_step() and make_train_step(), it seems that there should be a way to add values to the train step function's output that are are not treated as metrics by standard Callbacks. It is the best way to guarantee that errors are avoided without restricting the output type and having to rewrite multiple callbacks.", "@ManiadisG Thanks for the update!\r\n\r\nI think we should definitely fix `ProgbarLogger`, `History`, and `TensorBoard` so that they only try to handle log items that are scalars\r\n\r\nRe adding another output to `Model.train_step`, I'm wary of adding another return value to the function. One way I've seen people handle this kind of use case is to smuggle the value out by assigning it to a `tf.Variable`, something like:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nBATCH_SIZE = 2\r\n\r\nclass MyModel(tf.keras.Model):\r\n\r\n  def __init__(self):\r\n    super(MyModel, self).__init__()\r\n    self.outputs = tf.Variable(tf.zeros((BATCH_SIZE, 10)),\r\n                               trainable=False)\r\n    self.layer = tf.keras.layers.Dense(10)\r\n\r\n  def call(self, inputs):\r\n    return self.layer(inputs)\r\n\r\n  def train_step(self, data):\r\n    x, y = data\r\n    with tf.GradientTape() as tape:\r\n      y_pred = self(x, training=True)\r\n      self.outputs.assign(y_pred)\r\n      loss = self.compiled_loss(\r\n          y, y_pred, regularization_losses=self.losses)\r\n    trainable_variables = self.trainable_variables\r\n    gradients = tape.gradient(loss, trainable_variables)\r\n    self.optimizer.apply_gradients(zip(gradients, trainable_variables))\r\n    self.compiled_metrics.update_state(y, y_pred)\r\n    return {m.name: m.result() for m in self.metrics}\r\n\r\nclass MyCallback(tf.keras.callbacks.Callback):\r\n  def on_train_batch_end(self, batch, logs=None):\r\n    print(tf.reduce_sum(self.model.outputs))\r\n\r\nmodel = MyModel()\r\nmodel.compile('sgd', 'mse')\r\nx, y = np.ones((10, 100)), 10 * np.ones((10, 10))\r\nmodel.fit(x, y, batch_size=BATCH_SIZE, callbacks=[MyCallback()], verbose=2)\r\n```\r\n\r\nDo you think that pattern would work for you? I still think medium-term we should fix the built-in Callbacks to handle this use case", "Thank you for your response and for going through the trouble to propose a solution.\r\n\r\nI have solved my problem by overwritting all the Callbacks I needed. The only one that I couldn't directly overwrite and use by feeding it to fit() was Progbar. That was solved by creating the CallbackList object outside model.fit() with tensorflow.python.keras.callbacks.configure_callbacks(). BaseLogger, which looks like it would be the hardest to overwrite, doesn't look like it's causing problems.\r\n\r\nOverall, the issue can be overcome with enough callback customization until a more concrete solution is added. However, I would point out that restricting Callbacks to scalar logs is still too broad. Most Callbacks already treat logs differently based on whether the key of the dict is in the stateful_metrics list. I think it would be cleaner to expand this function to allow users to pass exceptions to fit() or to CallbackList (when it is fully integrated to the API) so that the corresponding logs are ignored by default Callbacks.\r\n\r\nWIth regard to the solution you proposed, I believe it it still has a weak point in that the use of tf.Variable and tf.assign assumes a fixed shape which is not always the case (an epoch's last batch may be smaller).", "I solved the issue at the Callback level as you suggested. I tested it in various scenarios with the 2.2 version and it worked perfectly. I wrote a Colab notebook with the code implementing a classifier which you can find here:\r\n\r\nhttps://github.com/ManiadisG/Customising-Tensorflow-2.2-Models\r\n\r\nOther than a solution to this specific problem, in the notebook I also wrote custom save/load functions for Model subclasses that, in my opinion, are more flexible and reliable than the defaults, particularly with Model subclassing."]}, {"number": 39120, "title": "TensorFlow Lite C++ API setup", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): installed from binary. (also have cloned using git clone)\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.7.6\r\n- Installed using virtualenv? pip? conda?: conda\r\n\r\n\r\n\r\nI want to setup TensorFlow Lite (TFLite) for C++ development. I searched google for it but didn't find anything. Tensorflow have posted regarding C++ development([documentation](https://www.tensorflow.org/lite/api_docs/cc)) but there's nothing about installation instruction. As of May 3, 2020 it seems to be in experimental mode. Though tensorflow has developed few [models](https://www.tensorflow.org/lite/models) out of it and build Android/iOS apps from it. I want to try those models on my own in C++. But here's a problem, to access header files required for C++  there is no direct support in tensorflow pip installation. This githubRepo of tensorflow need to be cloned which contains header files. Even after that to access some third-party header files that repository needs to be build. And I don't know how to build it. It'd be helpful if anyone working on TFLite C++ API shares how it can be done?\r\n\r\n\r\nI want to include following header file to my C++ code. \r\n`#include \"tensorflow/lite/model.h\"`\r\nBut I got error as `cannot open source file \"flatbuffers/flatbuffers.h\"`. I searched complete repository but there isn't anything as `flatbuffers.h`. But I found a folder **flatbuffers** which contains some build files. I think if I can build those files I will be able to load header files I need. But I don't know how to build them.\r\n\r\n\r\nNote : Just to be clear I want tensorflow lite C++ installation not tensorflow C api\r\n\r\nThank you.\r\n", "comments": ["@pranv12 \r\nplease refer to these links and let us know if it helps:\r\n[link1](https://developer.ridgerun.com/wiki/index.php?title=R2Inference/Supported_backends/TensorFlow-Lite)\r\n[link2](https://github.com/tensorflow/tensorflow/issues/23896)", "@Saduf2019  \r\nThank you! References you provided are very helpful. \r\n\r\nI also found a [sample project](https://github.com/infil00p/tf_world).\r\n\r\nClosing this issue for now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39120\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39120\">No</a>\n"]}, {"number": 39119, "title": "Spellcheck of filenames a-e in tensorflow/python/keras/layers/.", "body": "Minor spell checks.", "comments": []}, {"number": 39118, "title": "Can't run Tensorflow-Lite in NDK", "body": "\r\n**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- Mobile device: Samsung Galaxy S10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): 3.0.0\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI can build TFLite from binaries but I think it's not building correctly, or maybe only partially.\r\nI can include the AAR / SO files into Android Studio, but I can't seem to use TFLite successfully in the NDK.\r\n\r\nThis should be a very basic task - I just want to run a simple inference in Android's NDK.\r\nWhat am I missing here?\r\nI tried to follow the basic examples from https://www.tensorflow.org/ or this repository's c++ examples, but the TFLite library does not seems to respond.\r\n\r\nMore information available at the StackOverflow post:\r\nhttps://stackoverflow.com/questions/61523713/how-to-run-tensorflow-lite-in-android-studio-ndk\r\n", "comments": ["After failing to build the library as a shared / static one, I also tried to build the files straight from the IDE - see [more info in my post](https://github.com/tensorflow/tensorflow/issues/39148#issuecomment-628489924).\r\n\r\nBuilding fails with memory issues from toolchain / compiler.\r\nIt's possible that build system is missing some configurations - but the closest to successful build for `arm64-v8a` is done in Tensorflow's Docker container (from [here](https://www.tensorflow.org/lite/guide/build_arm64#cross-compile_for_arm64)) but the container uses a script, and not a configurable build system.\r\n\r\nHow can the TFLite library used in Android Studio's NDK environment - if not via static library?", "Hi orangesomethingorange@.\r\n\r\nIf I understand this correctly, you want some ndk code that links to the aar file?\r\n\r\nFrom what I understand this is a bit tricky:  https://stackoverflow.com/questions/51930836/android-ndk-build-cmake-include-so-library-included-in-aar-library.\r\n\r\nThis may be a solution for you. If not, I suggest exploding the aar manually and adding a dependency on the shared object in your CMakeLists.txt (shared_library) or Android.mk (using PREBUILT_SHARED_LIBRARY).\r\n\r\nI would, however, suggest that you will probably not get much performance benefit from this: the overhead of jni is probably not your bottleneck. I would take a look at possible optimizations such as quantization or pruning to reduce the model parameters.", "> Hi orangesomethingorange@.\r\n> \r\n> If I understand this correctly, you want some ndk code that links to the aar file?\r\n> \r\n> From what I understand this is a bit tricky: https://stackoverflow.com/questions/51930836/android-ndk-build-cmake-include-so-library-included-in-aar-library.\r\n> \r\n> This may be a solution for you. If not, I suggest exploding the aar manually and adding a dependency on the shared object in your CMakeLists.txt (shared_library) or Android.mk (using PREBUILT_SHARED_LIBRARY).\r\n> \r\n> I would, however, suggest that you will probably not get much performance benefit from this: the overhead of jni is probably not your bottleneck. I would take a look at possible optimizations such as quantization or pruning to reduce the model parameters.\r\n\r\nHey @daverim,\r\n\r\nCould you please elaborate on optimizations?\r\nDo you think that NDK code won't reduce running times substantially?\r\n\r\nThe link you sent is all about extracting an `.so` from an `.aar`. That's not the issue for me :)\r\nI already have both a shared library (`.so`) file and a static library (`.a`) file for the library I wish to include, which I'm adding in my Android Studio project's `CMakeLists.txt` file.\r\n\r\nThe problem is - when I include the library file, I get errors like so:\r\n`ninja: error: '/full/path/to/the/library/i/want/to/include/libtoinclude.so', needed by '.../app/build/intermediates/cmake/debug/obj/arm64-v8a/libnative-lib.so', missing and no known rule to make it`\r\n(when `libnative-lib.so` being the c++ library compiled from the NDK code).\r\n\r\nAny ideas / suggestions?", "> Do you think that NDK code won't reduce running times substantially?\r\n\r\nThat's right, inference times between Java and C++ should generally be comparable, as long as you avoid using multi-dimensional arrays on the Java side for things like image inputs/outputs. The Java API is just a lightweight wrapper around the C++ API.\r\n\r\n", "> > Do you think that NDK code won't reduce running times substantially?\r\n> \r\n> That's right, inference times between Java and C++ should generally be comparable, as long as you avoid using multi-dimensional arrays on the Java side for things like image inputs/outputs. The Java API is just a lightweight wrapper around the C++ API.\r\n\r\nI actually do have an image I'm trying to give as an input to the inference.\r\nThe output is also an image.\r\nCurrently the input image is loaded as a bitmap, and converted to an image-buffer before the inference.\r\n\r\nHow can I avoid using multi-dimensional arrays on Android's Java side?\r\nWhat other options are there for image processing?", "You can use a FloatBuffer or ByteBuffer of the appropriate size to provide your input data and extract output data. Most (maybe all?) of our examples use this approach. You can also use some of the helper utilities from the [support library](https://www.tensorflow.org/lite/guide/android) to do this, e.g., with our [image classification example](https://github.com/tensorflow/examples/blob/master/lite/examples/image_classification/android/app/src/main/java/org/tensorflow/lite/examples/classification/tflite/Classifier.java#L248).", "> You can use a FloatBuffer or ByteBuffer of the appropriate size to provide your input data and extract output data. Most (maybe all?) of our examples use this approach. You can also use some of the helper utilities from the [support library](https://www.tensorflow.org/lite/guide/android) to do this, e.g., with our [image classification example](https://github.com/tensorflow/examples/blob/master/lite/examples/image_classification/android/app/src/main/java/org/tensorflow/lite/examples/classification/tflite/Classifier.java#L248).\r\n\r\nI didn't mention it's a high definition image, my bad.\r\nIt results in memory issues, and it's not related to this issue, but yes - I actually am using ByteBuffer, thanks.", "@jdduke maybe this would better explain the issue:\r\n\r\nI created an Android project from scratch and tried to build the C / C++ source files.\r\n\r\n### steps to recreate:\r\n\r\n1. created an Android Studio Native C++ project\r\n2. created basic C/C++ source files and headers directory structure in `app/src/main/cpp` and put the (A) **tensorflow**, (B) **absl** and (C) **flatbuffers** files there, so now it looks like this:\r\n```\r\n|-- CMakeLists.txt\r\n|-- absl\r\n|   |-- base\r\n|   |   |-- attributes.h\r\n|   |   |-- config.h\r\n|   |   |-- internal\r\n|   |   |   |-- identity.h\r\n|   |   |   |-- inline_variable.h\r\n|   |   |   `-- invoke.h\r\n|   |   |-- macros.h\r\n|   |   |-- optimization.h\r\n|   |   |-- options.h\r\n|   |   |-- policy_checks.h\r\n|   |   `-- port.h\r\n|   |-- memory\r\n|   |   `-- memory.h\r\n|   |-- meta\r\n|   |   `-- type_traits.h\r\n|   |-- types\r\n|   |   |-- bad_optional_access.h\r\n|   |   |-- internal\r\n|   |   |   `-- optional.h\r\n|   |   `-- optional.h\r\n|   `-- utility\r\n|       `-- utility.h\r\n|-- flatbuffers\r\n|   |-- base.h\r\n|   |-- flatbuffers.h\r\n|   `-- stl_emulation.h\r\n|-- native-lib.cpp\r\n|-- native-lib.h\r\n`-- tensorflow\r\n    |-- core\r\n    |   `-- public\r\n    |       `-- version.h\r\n    `-- lite\r\n        |-- allocation.h\r\n        |-- c\r\n        |   |-- c_api.cc\r\n        |   |-- c_api.h\r\n        |   |-- c_api_internal.h\r\n        |   `-- common.h\r\n        |-- core\r\n        |   |-- api\r\n        |   |   |-- error_reporter.h\r\n        |   |   |-- op_resolver.h\r\n        |   |   `-- profiler.h\r\n        |   |-- macros.h\r\n        |   `-- subgraph.h\r\n        |-- delegates\r\n        |   `-- nnapi\r\n        |       `-- nnapi_delegate.h\r\n        |-- error_reporter.h\r\n        |-- experimental\r\n        |   `-- resource\r\n        |       `-- resource_base.h\r\n        |-- external_cpu_backend_context.h\r\n        |-- interpreter.h\r\n        |-- interpreter_builder.h\r\n        |-- kernels\r\n        |   `-- register.h\r\n        |-- memory_planner.h\r\n        |-- model.h\r\n        |-- model_builder.h\r\n        |-- mutable_op_resolver.h\r\n        |-- nnapi\r\n        |   |-- NeuralNetworksTypes.h\r\n        |   `-- nnapi_implementation.h\r\n        |-- schema\r\n        |   `-- schema_generated.h\r\n        |-- stderr_reporter.h\r\n        |-- string_type.h\r\n        |-- type_to_tflitetype.h\r\n        |-- util.h\r\n        `-- version.h\r\n```\r\nwith `native-lib` (**native-lib.cpp** / **native-lib.h**) being the compiled library and `CMakeLists.txt` being the **cmake** configuration file - which is left unchanged since it's \"the only library\".\r\n3. All / most of tensorflow's header files include some `#include \"tensorflow/...` lines, which couldn't be found by the compiler. So I changed all `#include \"tensorflow/...` lines to either (A) relative paths or to (B) `#include \"/tensorflow/...` paths (note the addition of a slash character, before \"tensorflow/...\").\r\n4. In the app's build.gradle I added a no-compression line for the `.tflite` file:\r\n```\r\naaptOptions {\r\n        noCompress \"tflite\"\r\n    }\r\n```\r\nand I control the build target via the `ndk` **abiFilters** option.\r\n5. I added an assets directory to the app\r\n6. In `native-lib.cpp` I add [some example code from the TFLite website](https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_c) to use:\r\n```\r\n    char filename [] = \"../assets/my_model.tflite\";\r\n\r\n    // Load the model\r\n    std::unique_ptr<tflite::FlatBufferModel> model =\r\n            tflite::FlatBufferModel::BuildFromFile(filename);\r\n\r\n    // Build the interpreter\r\n    tflite::ops::builtin::BuiltinOpResolver resolver;\r\n    std::unique_ptr<tflite::Interpreter> interpreter;\r\n    tflite::InterpreterBuilder(*model, resolver)(&interpreter);\r\n\r\n    // Resize input tensors, if desired.\r\n    interpreter->AllocateTensors();\r\n\r\n    float* input = interpreter->typed_input_tensor<float>(0);\r\n    // Fill `input`.\r\n\r\n    interpreter->Invoke();\r\n\r\n    float* output = interpreter->typed_output_tensor<float>(0);\r\n```\r\n7. Tried to build the library, source files included\r\n\r\n### errors\r\n\r\n1. Build target is **x86_64**:\r\n\r\n`/path/to/Android/Sdk/ndk/20.0.5594570/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/include/c++/v1/memory:2339: error: undefined reference to 'tflite::impl::Interpreter::~Interpreter()'\r\n`\r\n\r\nmemory, line 2339 is the \"`delete __ptr;`\" line:\r\n```\r\n_LIBCPP_INLINE_VISIBILITY void operator()(_Tp* __ptr) const _NOEXCEPT {\r\n    static_assert(sizeof(_Tp) > 0,\r\n                  \"default_delete can not delete incomplete type\");\r\n    static_assert(!is_void<_Tp>::value,\r\n                  \"default_delete can not delete incomplete type\");\r\n    delete __ptr;\r\n  }\r\n```\r\n\r\n2. Build target is **arm64-v8a**:\r\n\r\nmany errors of the following structure: \r\n```\r\n...\r\nnative-lib.cpp:32: undefined reference to `tflite::DefaultErrorReporter()'\r\nnative-lib.cpp:32: undefined reference to `tflite::FlatBufferModel::BuildFromFile(char const*, tflite::ErrorReporter*)'\r\n...\r\nnative-lib.cpp:40: undefined reference to `tflite::impl::Interpreter::AllocateTensors()'\r\nnative-lib.cpp:45: undefined reference to `tflite::impl::Interpreter::Invoke()'\r\n...\r\n```\r\nand the last one, again, being:\r\n`memory:2339: undefined reference to 'tflite::impl::Interpreter::~Interpreter()'`\r\n\r\n### additional info\r\n\r\nThe build line:\r\n```\r\n/path/to/app/build/intermediates/cmake/debug/obj/arm64-v8a/libnative-lib.so \r\n: && /path/to/Android/Sdk/ndk/21.1.6352462/toolchains/llvm/prebuilt/linux-x86_64/bin/clang++ --target=aarch64-none-linux-android29 --gcc-toolchain=/path/to/Android/Sdk/ndk/21.1.6352462/toolchains/llvm/prebuilt/linux-x86_64 --sysroot=/path/to/Android/Sdk/ndk/21.1.6352462/toolchains/llvm/prebuilt/linux-x86_64/sysroot -fPIC -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -O0 -fno-limit-debug-info  -Wl,--exclude-libs,libgcc.a -Wl,--exclude-libs,libgcc_real.a -Wl,--exclude-libs,libatomic.a -static-libstdc++ -Wl,--build-id -Wl,--fatal-warnings -Wl,--no-undefined -Qunused-arguments -shared -Wl,-soname,libnative-lib.so -o /path/to/app/build/intermediates/cmake/debug/obj/arm64-v8a/libnative-lib.so CMakeFiles/native-lib.dir/native-lib.cpp.o  /path/to/Android/Sdk/ndk/21.1.6352462/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/lib/aarch64-linux-android/29/liblog.so -latomic -lm\r\n```\r\n\r\nSee anything that might cause this **clang \"memory\"** issue?", "> with native-lib (native-lib.cpp / native-lib.h) being the compiled library\r\n\r\nCan you talk more about this? What exactly do you mean that this is the compiled library? Which library exactly? And how was it generated?\r\n\r\nNote that we've had some internal discussions over the past week, and are planning to prioritize adding native CMake support in Q3 for TFLite. This should simplify native integration into non-bazel toolchains. ", "> > with native-lib (native-lib.cpp / native-lib.h) being the compiled library\r\n> \r\n> Can you talk more about this? What exactly do you mean that this is the compiled library? Which library exactly? And how was it generated?\r\n> \r\n> Note that we've had some internal discussions over the past week, and are planning to prioritize adding native CMake support in Q3 for TFLite. This should simplify native integration into non-bazel toolchains.\r\n\r\nThese are great news :)\r\nNon-bazel toolchains integration would be excellent.\r\nAlso - I couldn't get Bazel to build TFL for arm64-v8a either, I was only able to build a static (.a) library for x86_64.\r\n\r\nRegarding your question:\r\n`native-lib.cpp` and `native-lib.h` are the C++ source file and header file, respectively, that I wrote to execute my program. They're compiled by Android Studio's build system (gradle) via the NDK extension (using `cmake`), and are-referred-to as `native-lib`.\r\nThis is standard in Android Studio, and is being configured by default, when creating a Native Project in Android Studio.\r\nSo what I was trying to do is to get gradle (Android Studio's build system) to build two libraries: (1) `native-lib` (my code) and (2) `tensorflow-lite` - and have my code in `native-lib` call the `tensorflow-lite` functions.\r\nIn the last trial depicted above, the `native-lib.cpp` did not have any special functionality - I was only trying to call the Tensorflow-Lite functions, to see that I can use TFL in native code.", "Just for reference:\r\n\r\n1. What I'm trying to do is to run a Tensorflow-Lite inference, from Android Studio's NDK.\r\n2. The reason I was trying to build the `tensorflow-lite` library myself, was because it failed to be compiled from a static / shared library (for `arm64-v8a`)\r\n3. Before trying to build and compile the code with gradle, I also tried using Android Studio's Bazel plugin to build the TFL library, with no apparent success.\r\n4. Using a shared library is preferred, but I couldn't build an `.so` for `arm64-v8a`, or failed to have it successfully compiled / linked into the project.", "@jdduke I tried running TFL in NDK using the C-API via [these instructions](https://www.tensorflow.org/lite/guide/android#use_tflite_c_api) and [version 2.2](https://bintray.com/google/tensorflow/download_file?file_path=org%2Ftensorflow%2Ftensorflow-lite%2F2.2.0%2Ftensorflow-lite-2.2.0.aar) seems to work ok :)\r\n**I assume I have you to thank for - so thanks**!\r\n\r\nI still can't see any result from the inference but I just assume it's my bad somehow... regarding this issue specifically, I think that TFL is currently running in NDK, so I'll close it.\r\n\r\nI still want to run the C++ API ([this](https://github.com/tensorflow/tensorflow/issues/39148) issue) and also check out GPU configuration capabilities of TFL, but I'll just open another issue for that.", "> @jdduke I tried running TFL in NDK using the C-API via these instructions and version 2.2 seems to work ok :)\r\n\r\nGreat news! Thanks for the feedback.\r\n\r\n> I still want to run the C++ API (this issue) and also check out GPU configuration capabilities of TFL, but I'll just open another issue for that.\r\n\r\nSounds good. Our CMake work this quarter should hopefully make it much easier to build from source directly from within Android Studio."]}, {"number": 39117, "title": "trying again: Big performance gains for Go NewTensor and Value", "body": "After some helpful comments in https://github.com/tensorflow/tensorflow/pull/36578#issuecomment-622308355 I'm trying again with this performance improvement.\r\n\r\nPlease please please if it gets rolled back again because of \"internal test\" failures, please get some kind of debug information. Stack traces or something. Any kind of clue.\r\n\r\n```\r\nname                                  old time/op    new time/op    delta\r\nTensor/New/[150528]int32-16             1.78ms \u00b1 4%    0.13ms \u00b110%   -92.63%  (p=0.000 n=8+7)\r\nTensor/New/[100][100][100]int32-16      13.1ms \u00b1 1%     0.9ms \u00b153%   -92.81%  (p=0.000 n=8+8)\r\nTensor/New/[]float32-16                 3.72ms \u00b1 1%    0.97ms \u00b130%   -74.04%  (p=0.000 n=8+8)\r\nTensor/New/[][]float32-16               4.83ms \u00b1 2%    1.32ms \u00b1 8%   -72.69%  (p=0.000 n=8+8)\r\nTensor/New/[][][]float32-16             4.81ms \u00b1 1%    1.32ms \u00b1 4%   -72.51%  (p=0.001 n=8+6)\r\nTensor/New/[]string-16                   466ms \u00b1 1%      34ms \u00b1 4%   -92.60%  (p=0.001 n=7+7)\r\nTensor/New/[][]string-16                 460ms \u00b1 1%      35ms \u00b1 1%   -92.45%  (p=0.000 n=8+8)\r\nTensor/New/[][][]string-16               462ms \u00b1 2%      36ms \u00b1 5%   -92.14%  (p=0.000 n=8+8)\r\nTensor/Value/[150528]int32-16            647\u00b5s \u00b1 3%      82\u00b5s \u00b1 1%   -87.28%  (p=0.000 n=8+8)\r\nTensor/Value/[100][100][100]int32-16    6.43ms \u00b1 1%    0.99ms \u00b1 3%   -84.63%  (p=0.000 n=8+8)\r\nTensor/Value/[]float32-16               5.57ms \u00b1 3%    1.04ms \u00b1 7%   -81.26%  (p=0.000 n=8+8)\r\nTensor/Value/[][]float32-16             6.84ms \u00b1 1%    1.51ms \u00b1 1%   -77.96%  (p=0.000 n=8+8)\r\nTensor/Value/[][][]float32-16           6.87ms \u00b1 1%    1.52ms \u00b1 3%   -77.80%  (p=0.001 n=7+7)\r\nTensor/Value/[]string-16                 268ms \u00b1 3%      20ms \u00b1 2%   -92.45%  (p=0.000 n=8+8)\r\nTensor/Value/[][]string-16               269ms \u00b1 2%      20ms \u00b1 1%   -92.46%  (p=0.000 n=8+7)\r\nTensor/Value/[][][]string-16             271ms \u00b1 2%      20ms \u00b1 1%   -92.55%  (p=0.000 n=8+8)\r\n\r\nname                                  old alloc/op   new alloc/op   delta\r\nTensor/New/[150528]int32-16              606kB \u00b1 0%       0kB \u00b1 0%   -99.99%  (p=0.000 n=8+8)\r\nTensor/New/[100][100][100]int32-16      4.16MB \u00b1 0%    0.00MB \u00b1 0%  -100.00%  (p=0.000 n=7+8)\r\nTensor/New/[]float32-16                 4.01MB \u00b1 0%    0.00MB \u00b1 0%  -100.00%  (p=0.002 n=7+8)\r\nTensor/New/[][]float32-16               4.48MB \u00b1 0%    0.00MB \u00b1 0%  -100.00%  (p=0.000 n=8+8)\r\nTensor/New/[][][]float32-16             4.48MB \u00b1 0%    0.00MB \u00b1 0%  -100.00%  (p=0.002 n=7+8)\r\nTensor/New/[]string-16                  48.0MB \u00b1 0%     0.0MB \u00b1 0%  -100.00%  (p=0.000 n=7+8)\r\nTensor/New/[][]string-16                48.3MB \u00b1 0%     0.0MB \u00b1 0%  -100.00%  (p=0.000 n=7+8)\r\nTensor/New/[][][]string-16              48.3MB \u00b1 0%     0.0MB \u00b1 0%  -100.00%  (p=0.000 n=7+8)\r\nTensor/Value/[150528]int32-16           1.21MB \u00b1 0%    0.61MB \u00b1 0%   -50.00%  (p=0.000 n=8+8)\r\nTensor/Value/[100][100][100]int32-16    9.23MB \u00b1 0%    4.25MB \u00b1 0%   -53.93%  (p=0.000 n=8+8)\r\nTensor/Value/[]float32-16               8.01MB \u00b1 0%    4.01MB \u00b1 0%   -50.00%  (p=0.000 n=8+7)\r\nTensor/Value/[][]float32-16             9.21MB \u00b1 0%    4.25MB \u00b1 0%   -53.82%  (p=0.000 n=8+8)\r\nTensor/Value/[][][]float32-16           9.23MB \u00b1 0%    4.25MB \u00b1 0%   -53.93%  (p=0.000 n=8+8)\r\nTensor/Value/[]string-16                56.0MB \u00b1 0%    23.0MB \u00b1 0%   -58.91%  (p=0.000 n=8+7)\r\nTensor/Value/[][]string-16              58.5MB \u00b1 0%    23.3MB \u00b1 0%   -60.23%  (p=0.000 n=8+8)\r\nTensor/Value/[][][]string-16            58.5MB \u00b1 0%    23.3MB \u00b1 0%   -60.25%  (p=0.001 n=7+7)\r\n\r\nname                                  old allocs/op  new allocs/op  delta\r\nTensor/New/[150528]int32-16               4.00 \u00b1 0%      2.00 \u00b1 0%   -50.00%  (p=0.000 n=8+8)\r\nTensor/New/[100][100][100]int32-16       10.0k \u00b1 0%      0.0k \u00b1 0%   -99.96%  (p=0.000 n=8+8)\r\nTensor/New/[]float32-16                   4.00 \u00b1 0%      2.00 \u00b1 0%   -50.00%  (p=0.000 n=8+8)\r\nTensor/New/[][]float32-16                20.0k \u00b1 0%      0.0k \u00b1 0%   -99.99%  (p=0.000 n=8+8)\r\nTensor/New/[][][]float32-16              20.0k \u00b1 0%      0.0k \u00b1 0%   -99.98%  (p=0.000 n=8+8)\r\nTensor/New/[]string-16                   4.00M \u00b1 0%     0.00M \u00b1 0%  -100.00%  (p=0.000 n=8+8)\r\nTensor/New/[][]string-16                 4.01M \u00b1 0%     0.00M \u00b1 0%  -100.00%  (p=0.000 n=8+8)\r\nTensor/New/[][][]string-16               4.01M \u00b1 0%     0.00M \u00b1 0%  -100.00%  (p=0.000 n=8+8)\r\nTensor/Value/[150528]int32-16             7.00 \u00b1 0%      2.00 \u00b1 0%   -71.43%  (p=0.000 n=8+8)\r\nTensor/Value/[100][100][100]int32-16     40.2k \u00b1 0%      0.0k \u00b1 0%   -99.99%  (p=0.000 n=8+8)\r\nTensor/Value/[]float32-16                 7.00 \u00b1 0%      2.00 \u00b1 0%   -71.43%  (p=0.000 n=8+8)\r\nTensor/Value/[][]float32-16              40.0k \u00b1 0%      0.0k \u00b1 0%   -99.99%  (p=0.000 n=8+8)\r\nTensor/Value/[][][]float32-16            40.2k \u00b1 0%      0.0k \u00b1 0%   -99.99%  (p=0.000 n=8+8)\r\nTensor/Value/[]string-16                 5.00M \u00b1 0%     0.00M \u00b1 0%  -100.00%  (p=0.000 n=8+8)\r\nTensor/Value/[][]string-16               5.02M \u00b1 0%     0.00M \u00b1 0%  -100.00%  (p=0.000 n=8+8)\r\nTensor/Value/[][][]string-16             5.02M \u00b1 0%     0.00M \u00b1 0%  -100.00%  (p=0.000 n=8+8)\r\n```", "comments": ["@jhseu Can you please take a look this PR ? Thanks!", "Dammit, fat-fingered git...", "Created a new PR here: https://github.com/tensorflow/tensorflow/pull/40761. Sorry folks."]}, {"number": 39116, "title": " Installation broken - Tensorflow 2.1 Cuda 10.1 Ubuntu 18.04", "body": "Hi,\r\n\r\nI'm struggling with the same issue as [here](https://github.com/tensorflow/tensorflow/issues/36121) when trying to install TF 2.1 with Cuda 10.1. I followed the [instructions](https://www.tensorflow.org/install/gpu) for Ubuntu 18.04 twice on two different machines, worked like a charm a few weeks ago, and now on this one I get stuck as described above when trying to install `cuda-10-1` (the other libraries can be installed, and `nvidia-smi` shows the gpus):\r\n```\r\n$ sudo apt-get install cuda-10-1\r\nThe following packages have unmet dependencies:\r\n cuda-10-1 : Depends: cuda-runtime-10-1 (>= 10.1.243) but it is not going to be installed\r\n             Depends: cuda-demo-suite-10-1 (>= 10.1.243) but it is not going to be installed\r\nE: Unable to correct problems, you have held broken packages.\r\n```\r\nWhen trying to install deps:\r\n```\r\n$ sudo apt-get install cuda-runtime-10-1\r\nThe following packages have unmet dependencies:\r\n cuda-runtime-10-1 : Depends: cuda-drivers (>= 418.87) but 410.0-0~dummy18.04 is to be installed\r\nE: Unable to correct problems, you have held broken packages.\r\n```\r\nFinally:\r\n```\r\n$ sudo apt-get install cuda-drivers\r\ncuda-drivers is already the newest version (410.0-0~dummy18.04).\r\n```\r\nI read the other thread but I'm not sure how @igorhoogerwoord  managed to go beyond this. I'd also avoid installing `tf-nightly` for now as @HardRockDude did if I can...\r\n\r\nThanks in advance for any help!", "comments": ["@jchwenger,\r\nCould you please check [this comment](https://askubuntu.com/a/598616) from a similar issue and let us know if it works. Thanks!", "Hi,\r\nThanks for this! \r\nWell, as mentioned in my post I could not install `cuda-runtime-10-1`. After much tinkering (I'm afraid it's hard for me to recall all steps now, mostly purging everything and installing again many times over, sometimes trying to install the dependencies manually), I managed to get it working.\r\n\r\nI *think* what made a difference in the end was a full upgrade.\r\n```\r\nsudo apt -y full-upgrade\r\n```\r\nAfter that the steps indicated on the official [page](https://www.tensorflow.org/install/gpu#ubuntu_1804_cuda_101) lead me to the same state as the other machines: things seem to work now with TF 2.1, as well as previous ones like 1.14 in another conda env, except for Tensor RT, I'll post another question now if I don't find an answer.", "@jchwenger,\r\nIs this still an issue? Please feel free to close the issue if resolved. Thanks!", "My apologies, yes all done, thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39116\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39116\">No</a>\n"]}, {"number": 39115, "title": "conv2d calculation results are inconsistent with pytorch", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): \r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): \r\n- Python version: 3.6.5\r\n- CUDA/cuDNN version: - GPU model and memory:10.1\r\n\r\n**Describe the current behavior**\r\nFor some specific data, the calculation results of tensorflow and pytorch are inconsistent\r\n**Describe the expected behavior**\r\nThe calculation results of tensorflow and pytorch are consistent or the difference is small\r\n**Standalone code to reproduce the issue** \r\n\r\n        if target_interface == 'conv1':\r\n            weights_torch = torch.from_numpy(np.full((11, 11, 3, 1), 1, np.float64).transpose((3, 2, 0, 1)))\r\n            output_pytorch_cpu = torch.from_numpy(\r\n                F.conv2d(torch.from_numpy(input_pytorch.numpy().transpose((0, 3, 1, 2))), weights_torch, padding=0,\r\n                         stride=4).numpy().transpose((0, 2, 3, 1)))\r\n**Other info / logs** \r\ntensorflow_output:\r\n31.42857361,31.4285183,31.42857361\r\n69.24489594,69.24489594,59.3673439\r\n100.89792633,100.89792633,100.89792633\r\n133.26530457,133.26530457,133.26530457\r\n60.22449875,60.22449875,60.22449875\r\n187.46939087,187.46939087,187.46939087\r\n205.918396,205.918396,205.918396\r\n96.42860413,96.42860413,96.42860413\r\n61.42854309,61.42854309,61.42854309\r\n148.89811707,148.89811707,148.89811707\r\n96.28572083,96.28572083,96.28572083\r\n83.02045441,83.02045441,83.02045441\r\n87.89794922,87.89794922,87.89794922\r\n135.12240601,135.12240601,135.12240601\r\n\r\n\r\npytorch_output\r\n31.428574,31.428518,31.428574\r\n59.367344,69.244896,69.244896\r\n100.89793,100.89793,100.89793\r\n133.2653,133.2653,133.2653\r\n60.2245,60.2245,60.2245\r\n187.46939,187.46939,187.46939\r\n205.9184,205.9184,205.9184\r\n96.428604,96.428604,96.428604\r\n61.428543,61.428543,61.428543\r\n148.89812,148.89812,148.89812\r\n96.28572,96.28572,96.28572\r\n83.020454,83.020454,83.020454\r\n87.89795,87.89795,87.89795\r\n135.1224,135.1224,135.1224\r\n\r\n The version of tensorflow is 1.90. The error operators are conv2d () and avg_pooling (). Thank you.\r\n\r\nThis error is a fact, and it can be reproduced by using a large number of random tensors as input to conv2d () or avg_pooling (). Thank you.", "comments": ["@lvjunsetup \r\nWe see that the code provided is incomplete, please share simple stand alone code to replicate the issue faced along with the tensorflow version on which the error was faced.", "> @lvjunsetup\r\n> We see that the code provided is incomplete, please share simple stand alone code to replicate the issue faced along with the tensorflow version on which the error was faced.\r\n\r\nHello, our project is so big that it is difficult to take out the code alone now, I will sort it out and send it to you.", "@lvjunsetup\r\nAlso i see you are using an old version of tensorflow , could you please try on later version as 1.15 or 2.1 and let us know if that helps.\r\n", "@lvjunsetup\r\nPlease update as per above comment", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39115\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39115\">No</a>\n"]}, {"number": 39114, "title": "Using dynamic=True in a keras Metric constructor should run the metric methods in eager mode", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.2.0rc4\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently, when using `dynamic=True` in a Metric constructor, it has no effect. All the methods are run in graph mode. Since this feature is [already present in keras Layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#arguments_20), and that a Metric is a subclass of Layer, one would expect a metric to run in eager mode if `dynamic=True` is provided.\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo change to the API. dynamic=True is already accepted in a Metric constructor. The change would be in behavior only. The flag should trigger the eager mode. Currently it doesn't do anything.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nUsers who want to make custom metrics which need eager mode and want to distribute them.\r\n\r\n**Any Other info.**\r\n\r\nHere is a minimal notebook reproducing the problem. With this feature implemented, the error should not happen:\r\n\r\nhttps://colab.research.google.com/drive/11EuSPoyedceHySz_uxxqjZJbU2uatpfk\r\n\r\n", "comments": ["Intuitively it might be easiest to just pass dynamic arg to constructor. Assuming that the Metric class inherits all relevant infrastructure from the Layer class. But looking at the Layer class code it seems that approach wouldn't work.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/engine/base_layer.py#L357-L360\r\n\r\nLooking at the other metrics that inherit from Metric, it seems they use metrics_utils to handle to handle the heavy lifting. \r\nFor example in the case of the Recall class:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/metrics.py#L1382-L1392\r\nSo that could be the path forward.", "@gabrieldemarmiesse This feature not yet implemented. Is there any actionable item like raising PR? \r\n\r\nPlease note that Keras development moved to another repository to focus entirely on only keras. Could you please repost this issue on [keras-team/keras repo](https://github.com/keras-team/keras/issues). Thanks!", "I supposed the path forward would be to make a pull request. It's been quite a while since I last worked with deep learning so I'll let users who are interested implement it. ", "I opened this issue in keras-team/keras. Let's track the progress there https://github.com/keras-team/keras/issues/15758"]}, {"number": 39113, "title": "cannot download micro speech pre-trained model ", "body": "@tensorflow/micro\r\n\r\n**System information**\r\nwindows 10 enterprise \r\nfirefox 75.0\r\n\r\n**Describe the problem**\r\nwhen trying to download micro speech pre-trained model an xml with the following error is received:\r\n\r\n`Anonymous caller does not have storage.objects.get access to download.tensorflow.org/models/tflite/micro/speech_commands_2020_04_13.zip`\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n1. open readme.md: [https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/micro_speech/train#trained-models](url)\r\n2. click the download link: [https://storage.googleapis.com/download.tensorflow.org/models/tflite/micro/speech_commands_2020_04_13.zip](url)\r\n3. get following error:\r\n![image](https://user-images.githubusercontent.com/61374608/80912183-db500080-8d43-11ea-9710-386a7c45f179.png)\r\n\r\n", "comments": ["@idog-ceva,\r\nI was able to download the `speech_commands.zip` file without any issues from the [README.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/micro_speech/train/README.md) page.\r\n\r\nThe link you have provided is \r\nhttps://storage.googleapis.com/download.tensorflow.org/models/tflite/micro/speech_commands_2020_04_13.zip\r\n\r\nwhereas the link to the zip file is\r\nhttps://storage.googleapis.com/download.tensorflow.org/models/tflite/micro/micro_speech_2020_04_13.zip\r\n\r\nCould you please check this and let us know if it works? Thanks!", "@amahendrakar\r\nupdated link works, thanks."]}]