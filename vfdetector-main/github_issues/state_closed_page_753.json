[{"number": 30974, "title": "AttributeError: module 'tensorflow.python.ops.summary_op_util' has no attribute 'skip_summary'", "body": "\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\ndevice:\r\n- TensorFlow installed from (source or binary): i use pip\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: i tried 3.7.3 and 3.6\r\n- Bazel version (if compiling from source):  i dont know\r\n- GCC/Compiler version (if compiling from source): i dont know\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: 1070ti\r\n\r\ni try to use TecoGAN (https://github.com/thunil/TecoGAN)\r\n\r\n\r\nTraceback (most recent call last):\r\nFile \"main.py\", line 284, in \r\nNet = TecoGAN( rdata.s_inputs, rdata.s_targets, FLAGS )\r\nFile \"/home/ksjin/\ubc14\ud0d5\ud654\uba74/TecoGAN-master/lib/Teco.py\", line 500, in TecoGAN\r\ngif_sum = [ gif_summary('LR', r_inputs, max_outputs=max_outputs, fps=3),\r\nFile \"/home/ksjin/\ubc14\ud0d5\ud654\uba74/TecoGAN-master/lib/ops.py\", line 507, in gif_summary\r\nif summary_op_util.skip_summary():\r\nAttributeError: module 'tensorflow.python.ops.summary_op_util' has no attribute 'skip_summary'\r\n\r\n\r\n\r\nbut i got this error  how can i fix it\r\n\r\n", "comments": ["@skyzombie89 \r\nIn order to expedite the trouble-shooting process, please provide a minimal code snippet to reproduce the issue reported here. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 30973, "title": " Updated tf.set_random_seed to tf.random.set_seed", "body": "Deprecated api updated", "comments": ["Can one of the admins verify this patch?", "@siju-samuel thank you for your contribution , contrib folder will be removed in 2.0 and we will not be accepting new changes here , closing  this."]}, {"number": 30972, "title": "Excessive overhead with dense layer", "body": "Under normal circumstances, propagation through the dense layer should reduce to a matmul operation and possibly an activation operation. However, in tensorflow 1.14, it sometimes does not. This situation arises if the input to the dense layer is at least 3D and its dimensions are not fully determined at the time of graph construction.\r\nIf this happens, the framework calls the method tensordot(), which calls the internal method _tensordot_reshape(), which then attempts to determine how to reshape the tensor before calling matmul.\r\nThe GPU trace of the whole thing ends up looking like so:\r\n\r\n```\r\n\r\n[CUDA memcpy HtoD]\r\nvoid tensorflow::GatherOpKernel<int, int, bool=1>(int const *, int const *, tensorflow::GatherOpKernel<int, int, bool=1>*, __int64, __int64, __int64, __int64)\r\nvoid tensorflow::GatherOpKernel<int, int, bool=1>(int const *, int const *, tensorflow::GatherOpKernel<int, int, bool=1>*, __int64, __int64, __int64, __int64)\r\n[CUDA memcpy DtoH]\r\nvoid tensorflow::functor::BlockReduceKernel<int*, int*, int=256, tensorflow::functor::Prod<int>>(int*, int*, int, int, std::iterator_traits<tensorflow::functor::BlockReduceKernel<int*, int*, int=256, tensorflow::functor::Prod<int>>>::value_type)\r\n[CUDA memcpy DtoH]\r\n[CUDA memcpy DtoH]\r\nvolta_fp16_s884gemm_fp16_256x128_ldg8_f2f_nn\r\n```\r\n\r\nNote that these memcpy's are all for 4-8 bytes, but each one involves a syscall and a pointer validity check, so, they are not exactly free. \r\n\r\nAnd it is all unnecessary, because the framework does know how big the tensor is, on the CPU side, and something along these lines https://github.com/ekuznetsov139/tensor2tensor/commit/079f51f2c736d547cbd2b4bcfdcedb039827b93a would compute the shape without the use of the GPU.\r\n\r\nIt might seem trivial, but, in a graph with lots of small dense layers and especially when tensor cores are in play (so that the actual convolution is fairly fast), the overhead may become significant.\r\n\r\nThe above change to the tensor2tensor project reduces training time per step (NVIDIA Tesla V100, fp16) by at least 10%. Since the issue is fundamental and not specific to tensor2tensor, it would make sense to address it at the framework level.", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. \r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "TensorFlow 1.14, Linux, observed on Debian and Ubuntu.\r\n\r\nYou may reproduce the effect with the following simple script:\r\n```\r\nfrom __future__ import absolute_import, division, print_function\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\n\r\ninput_shape=(40,40)\r\nfix=False\r\ninp=keras.layers.Input(shape=input_shape)\r\nfeaturesL1 = 4\r\nif fix:\r\n\tsh=tf.shape(inp)\r\n\tin_shape=tf.concat([sh[0:1]*sh[1:2], sh[-1:] ],axis=0)\r\n\tout_shape=tf.concat([sh[:-1], [featuresL1]], axis=0)\r\n\tL1inp=tf.reshape(inp,in_shape)\r\nelse:\r\n\tL1inp = inp\r\nL1=keras.layers.Dense(featuresL1,use_bias=False,activation=tf.nn.relu,name='main')(L1inp)\r\nif fix:\r\n\tL1=tf.reshape(L1,out_shape)\r\nL3=keras.layers.Flatten()(L1)\r\nL4=keras.layers.Lambda(lambda x: tf.math.reduce_mean(x, axis=1))(L3)\r\nL5=keras.layers.Flatten()(L4)\r\nmodel=keras.models.Model(inputs=inp,outputs=L5)\r\noptimizer = tf.compat.v1.train.RMSPropOptimizer(0.001)\r\ntrain_data=[]\r\ntrain_labels=[]\r\nfor x in range(100):\r\n\ttrain_data.append(tf.constant(np.random.random(input_shape)))\r\n\ttrain_labels.append(np.random.random())\r\ntrain_data=tf.stack(train_data,axis=0)\r\n\r\nmodel.compile(loss='mse', optimizer=optimizer)\r\nmodel.summary()\r\nmodel.fit(x=train_data,y=train_labels,epochs=10,verbose=1,steps_per_epoch=10,shuffle=True)\r\n\r\n```\r\nTo do the profiling, launch \"nvprof --profile-all-processes --print-gpu-trace --log-file nvprof%p.txt\" in another terminal on the same system. You will observe that, if 'fix' is 'False', the resulting profile log contains repeated invocations of tensorflow::GatherOpKernel , but, if 'fix' is 'True', the log is about 10% shorter and GatherOpKernel is not used at all.\r\n\r\nNote that the problem occurs even though the input shape is fully specified as (40,40). I believe that is because the graph does not know the batch size (which is implicitly prepended as dimension 0) at construction time.\r\n", "Apologies for the delay in response. This issue is better surfaced on **tensor2tensor** repository, since changes you are suggesting belong to https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_layers.py\r\nPerhaps you can raise issue on https://github.com/tensorflow/tensor2tensor/issues\r\nThanks!", "The problem is not specific to tensor2tensor, it is demonstrated by the code snippet above that has nothing to do with tensor2tensor.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 30971, "title": "Refactor MapAndBatchDatasetOp", "body": "This PR refactors `MapAndBatchDatasetOp` and adds the tests.\r\n\r\ncc: @jsimsa ", "comments": ["@rachellim please take a look", "Thanks for the change, @feihugis ", "Thanks for the review, @rachellim! We are refactoring the test style to avoid the repeated code. This PR will be updated accordingly. Will ping you once I finish the change.", "Waiting for #31592 to be merged. Once merged, the failures in `Ubuntu CC` will disappear. ", "@rachellim The refactoring of the test style is finished. Could you please take a look at the change(https://github.com/tensorflow/tensorflow/pull/30971/commits/18cb0f08d873fbae678f0115e850bd3e78fd5f8c) when you have time?", "@rachellim The internal checks failed. Could you please help check the logs and paste them here? Thanks!", "`MapAndBatchDatasetOpTest.InvalidArguments` is giving SIGSEV. Trace:\r\n\r\n```\r\nPC: @     0x7fe2e28a51b4  (unknown)  tensorflow::UniqueTensorReferences::~UniqueTensorReferences()\r\n    @     0x7fe2dba65d51       1648  FailureSignalHandler()\r\n    @     0x7fe2db2649a0  245665408  (unknown)\r\n    @     0x7fe2e283ee99        144  tensorflow::OpKernelContext::~OpKernelContext()\r\n    @     0x7fe2e44ea939        224  tensorflow::data::DatasetOpsTestBase::CreateOpKernelContext()\r\n    @     0x7fe2e44ecaa8         64  tensorflow::data::DatasetOpsTestBase::CreateDatasetContext()\r\n    @     0x7fe2e4523306        112  tensorflow::data::experimental::(anonymous namespace)::MapAndBatchDatasetOpTest::Initialize()\r\n    @     0x7fe2e45324ec       2224  tensorflow::data::experimental::(anonymous namespace)::MapAndBatchDatasetOpTest_InvalidArguments_Test::TestBody()\r\n    @     0x7fe2e2bd392a         48  testing::Test::Run()\r\n    @     0x7fe2e2bd4ab0         96  testing::TestInfo::Run()\r\n    @     0x7fe2e2bd54a7         80  testing::TestSuite::Run()\r\n    @     0x7fe2e2be4df7        208  testing::internal::UnitTestImpl::RunAllTests()\r\n    @     0x7fe2e2be4415         64  testing::UnitTest::Run()\r\n    @     0x7fe2e450336e         32  main\r\n    @     0x7fe2db0d2bbd        208  __libc_start_main\r\n    @     0x5645316cf4a9  (unknown)  ../sysdeps/x86_64/start.S:108 _start\r\n```", "@rachellim Thanks for the detailed logs! The SIGSEV issue is addressed here (https://github.com/tensorflow/tensorflow/pull/30971/commits/97ae500d64f64b4ed056a2f7ea5e333ed7408620). Could you please take another look?", "Can one of the admins verify this patch?"]}, {"number": 30970, "title": "graph.get_operation() returns operations that are not from the respective name_scope", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): conda\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nfunction get_operations() \r\nfrom class Graph()\r\nis not returning the operations from a namescope enclosing the tensorflow operations from respective namescope but is returning the operations for all the graph\r\n\r\n**Describe the expected behavior**\r\n\r\nReturns only the operations from name scope, and preferable shorted by construction just like in the original list.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\ntf.reset_default_graph()\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n  with tf.variable_scope('aa'):\r\n    c = tf.constant(10.0,name='const')\r\n    a = tf.Variable(c,name='nomon')\r\n  with tf.variable_scope('ab'):\r\n    c = tf.constant(10.0,name='const')\r\n    a = tf.Variable(c,name='nomon')  \r\n\r\nwith graph.as_default():    \r\n  with tf.name_scope('aa/'):\r\n    print(graph.get_operations()[-1].name)\r\n``` \r\n**Other info / logs**\r\n\r\nI know this seems to be a fairly trivial problem to be circumvented (a.k.a search for the operations by the name with if and elses). But this bug brings out a possible ugly spagethi code, and seems fairly easy to fiz in the api, without having to go to the backend.", "comments": ["And, use graph.get_operation_by_name(name)\r\nis dully since i would have to inspect op names inside the scope and use if and elses too\r\n", "Was able to reproduce the issue with Tensorflow 1.14.0 on Colab. Please find the gist [here](https://colab.research.google.com/drive/1naJ37I98gPub_G22x1vfN7W17Dd8JKsB). Thanks!", "Perhaps this can be a Feature Request since we already have [get_operation_by_name](https://www.tensorflow.org/api_docs/python/tf/Graph#get_operation_by_name)", "graph.get_operations() is not intended to depend on the current thread-local name-scope, so this is working as intended.", "So tensorflow is lacking a function that can know what name-space its talking about, unless you have another ideia for namespaces functions, the only thing that resides is that they are appropriated names for a graph of ops. \r\n\r\nAlso, i thinked of collocate_with as an option, however it cant see the name scope but only the op.", "You can always get the operations and filter them yourself.\n\nOn Thu, Jul 25, 2019 at 9:14 AM Daniel Penalva <notifications@github.com>\nwrote:\n\n> So tensorflow is lacking a function that can know what name-space its\n> talking about, unless you have another ideia for namespaces functions, the\n> only thing that resides is that they are appropriated names for a graph of\n> ops.\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRN65M7GWNFQEZZNMW3QBHGQDA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2Z6HPY#issuecomment-515105727>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRPY7TSCJDSDMH6S6HDQBHGQDANCNFSM4IGKWYJA>\n> .\n>\n\n\n-- \n - Alex\n", "What is the point of being working inside a namespace that you defined by a\npurpose, if you have to \"filter the ops by yourself\" ?\n\n\n\nOn Thu, Jul 25, 2019 at 1:27 PM Alexandre Passos <notifications@github.com>\nwrote:\n\n> You can always get the operations and filter them yourself.\n>\n> On Thu, Jul 25, 2019 at 9:14 AM Daniel Penalva <notifications@github.com>\n> wrote:\n>\n> > So tensorflow is lacking a function that can know what name-space its\n> > talking about, unless you have another ideia for namespaces functions,\n> the\n> > only thing that resides is that they are appropriated names for a graph\n> of\n> > ops.\n> >\n> > \u2014\n> > You are receiving this because you modified the open/close state.\n> > Reply to this email directly, view it on GitHub\n> > <\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRN65M7GWNFQEZZNMW3QBHGQDA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2Z6HPY#issuecomment-515105727\n> >,\n> > or mute the thread\n> > <\n> https://github.com/notifications/unsubscribe-auth/AAABHRPY7TSCJDSDMH6S6HDQBHGQDANCNFSM4IGKWYJA\n> >\n> > .\n> >\n>\n>\n> --\n> - Alex\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAOOCXZ2CFMHZKG3LFLKGSDQBHH6ZA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2Z7HLQ#issuecomment-515109806>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAOOCX64RSANGFT7EJXORBDQBHH6ZANCNFSM4IGKWYJA>\n> .\n>\n", "I do not intend to be arrogant, but i can see the if else, or even lambda\r\nbut will be a lot of, proliferating if i wanna to derive an architecture\r\nfrom Inception V3 and do Transfer Learning for Research.\r\nThat's why i see immense importance in the issue.\r\n\r\nOn Thu, Jul 25, 2019 at 1:59 PM Daniel Penalva <dkajah@gmail.com> wrote:\r\n\r\n> What is the point of being working inside a namespace that you defined by\r\n> a purpose, if you have to \"filter the ops by yourself\" ?\r\n>\r\n>\r\n>\r\n> On Thu, Jul 25, 2019 at 1:27 PM Alexandre Passos <notifications@github.com>\r\n> wrote:\r\n>\r\n>> You can always get the operations and filter them yourself.\r\n>>\r\n>> On Thu, Jul 25, 2019 at 9:14 AM Daniel Penalva <notifications@github.com>\r\n>> wrote:\r\n>>\r\n>> > So tensorflow is lacking a function that can know what name-space its\r\n>> > talking about, unless you have another ideia for namespaces functions,\r\n>> the\r\n>> > only thing that resides is that they are appropriated names for a graph\r\n>> of\r\n>> > ops.\r\n>> >\r\n>> > \u2014\r\n>> > You are receiving this because you modified the open/close state.\r\n>> > Reply to this email directly, view it on GitHub\r\n>> > <\r\n>> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRN65M7GWNFQEZZNMW3QBHGQDA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2Z6HPY#issuecomment-515105727\r\n>> >,\r\n>> > or mute the thread\r\n>> > <\r\n>> https://github.com/notifications/unsubscribe-auth/AAABHRPY7TSCJDSDMH6S6HDQBHGQDANCNFSM4IGKWYJA\r\n>> >\r\n>> > .\r\n>> >\r\n>>\r\n>>\r\n>> --\r\n>> - Alex\r\n>>\r\n>> \u2014\r\n>> You are receiving this because you authored the thread.\r\n>> Reply to this email directly, view it on GitHub\r\n>> <https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAOOCXZ2CFMHZKG3LFLKGSDQBHH6ZA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2Z7HLQ#issuecomment-515109806>,\r\n>> or mute the thread\r\n>> <https://github.com/notifications/unsubscribe-auth/AAOOCX64RSANGFT7EJXORBDQBHH6ZANCNFSM4IGKWYJA>\r\n>> .\r\n>>\r\n>\r\n", "tf.name_scope is not a way for you to define an isolated namespace within a\ngraph. It just means that ops created inside that block have names that\nstart with the name of the scope.\n\nOn Thu, Jul 25, 2019 at 10:16 AM Daniel Penalva <notifications@github.com>\nwrote:\n\n> I do not intend to be arrogant, but i can see the if else, or even sigma\n> but will be a lot of, proliferating if i wanna to derive an architecture\n> from Inception V3 and do Transfer Learning for Research.\n> That's why i see immense importance in the issue.\n>\n> On Thu, Jul 25, 2019 at 1:59 PM Daniel Penalva <dkajah@gmail.com> wrote:\n>\n> > What is the point of being working inside a namespace that you defined by\n> > a purpose, if you have to \"filter the ops by yourself\" ?\n> >\n> >\n> >\n> > On Thu, Jul 25, 2019 at 1:27 PM Alexandre Passos <\n> notifications@github.com>\n> > wrote:\n> >\n> >> You can always get the operations and filter them yourself.\n> >>\n> >> On Thu, Jul 25, 2019 at 9:14 AM Daniel Penalva <\n> notifications@github.com>\n> >> wrote:\n> >>\n> >> > So tensorflow is lacking a function that can know what name-space its\n> >> > talking about, unless you have another ideia for namespaces functions,\n> >> the\n> >> > only thing that resides is that they are appropriated names for a\n> graph\n> >> of\n> >> > ops.\n> >> >\n> >> > \u2014\n> >> > You are receiving this because you modified the open/close state.\n> >> > Reply to this email directly, view it on GitHub\n> >> > <\n> >>\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRN65M7GWNFQEZZNMW3QBHGQDA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2Z6HPY#issuecomment-515105727\n> >> >,\n> >> > or mute the thread\n> >> > <\n> >>\n> https://github.com/notifications/unsubscribe-auth/AAABHRPY7TSCJDSDMH6S6HDQBHGQDANCNFSM4IGKWYJA\n> >> >\n> >> > .\n> >> >\n> >>\n> >>\n> >> --\n> >> - Alex\n> >>\n> >> \u2014\n> >> You are receiving this because you authored the thread.\n> >> Reply to this email directly, view it on GitHub\n> >> <\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAOOCXZ2CFMHZKG3LFLKGSDQBHH6ZA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2Z7HLQ#issuecomment-515109806\n> >,\n> >> or mute the thread\n> >> <\n> https://github.com/notifications/unsubscribe-auth/AAOOCX64RSANGFT7EJXORBDQBHH6ZANCNFSM4IGKWYJA\n> >\n> >> .\n> >>\n> >\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRJFOGGIKJ3RZWXGNADQBHNWXA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22DYZY#issuecomment-515128423>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRLFCGOHXD6GLJAAWVLQBHNWXANCNFSM4IGKWYJA>\n> .\n>\n\n\n-- \n - Alex\n", "you use variable scope, and them  reuse it through variable scope, thats\nok.\n\nBut to me seems that you are evading the question, how to you do require an\nop, in a big model, by knowing the var/name space block and by knowing that\nthis op is the one that outputs tensors of interest.\n\nTensorflow nowadays donst do that. I see you are member of Tensorflow, but\nplease dosnt evade the question because of that, it donst make sense since\nthe software is open source ...\nThank you\n\nOn Thu, Jul 25, 2019 at 3:16 PM Alexandre Passos <notifications@github.com>\nwrote:\n\n> tf.name_scope is not a way for you to define an isolated namespace within a\n> graph. It just means that ops created inside that block have names that\n> start with the name of the scope.\n>\n> On Thu, Jul 25, 2019 at 10:16 AM Daniel Penalva <notifications@github.com>\n> wrote:\n>\n> > I do not intend to be arrogant, but i can see the if else, or even sigma\n> > but will be a lot of, proliferating if i wanna to derive an architecture\n> > from Inception V3 and do Transfer Learning for Research.\n> > That's why i see immense importance in the issue.\n> >\n> > On Thu, Jul 25, 2019 at 1:59 PM Daniel Penalva <dkajah@gmail.com> wrote:\n> >\n> > > What is the point of being working inside a namespace that you defined\n> by\n> > > a purpose, if you have to \"filter the ops by yourself\" ?\n> > >\n> > >\n> > >\n> > > On Thu, Jul 25, 2019 at 1:27 PM Alexandre Passos <\n> > notifications@github.com>\n> > > wrote:\n> > >\n> > >> You can always get the operations and filter them yourself.\n> > >>\n> > >> On Thu, Jul 25, 2019 at 9:14 AM Daniel Penalva <\n> > notifications@github.com>\n> > >> wrote:\n> > >>\n> > >> > So tensorflow is lacking a function that can know what name-space\n> its\n> > >> > talking about, unless you have another ideia for namespaces\n> functions,\n> > >> the\n> > >> > only thing that resides is that they are appropriated names for a\n> > graph\n> > >> of\n> > >> > ops.\n> > >> >\n> > >> > \u2014\n> > >> > You are receiving this because you modified the open/close state.\n> > >> > Reply to this email directly, view it on GitHub\n> > >> > <\n> > >>\n> >\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRN65M7GWNFQEZZNMW3QBHGQDA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2Z6HPY#issuecomment-515105727\n> > >> >,\n> > >> > or mute the thread\n> > >> > <\n> > >>\n> >\n> https://github.com/notifications/unsubscribe-auth/AAABHRPY7TSCJDSDMH6S6HDQBHGQDANCNFSM4IGKWYJA\n> > >> >\n> > >> > .\n> > >> >\n> > >>\n> > >>\n> > >> --\n> > >> - Alex\n> > >>\n> > >> \u2014\n> > >> You are receiving this because you authored the thread.\n> > >> Reply to this email directly, view it on GitHub\n> > >> <\n> >\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAOOCXZ2CFMHZKG3LFLKGSDQBHH6ZA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2Z7HLQ#issuecomment-515109806\n> > >,\n> > >> or mute the thread\n> > >> <\n> >\n> https://github.com/notifications/unsubscribe-auth/AAOOCX64RSANGFT7EJXORBDQBHH6ZANCNFSM4IGKWYJA\n> > >\n> > >> .\n> > >>\n> > >\n> >\n> > \u2014\n> > You are receiving this because you modified the open/close state.\n> > Reply to this email directly, view it on GitHub\n> > <\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRJFOGGIKJ3RZWXGNADQBHNWXA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22DYZY#issuecomment-515128423\n> >,\n> > or mute the thread\n> > <\n> https://github.com/notifications/unsubscribe-auth/AAABHRLFCGOHXD6GLJAAWVLQBHNWXANCNFSM4IGKWYJA\n> >\n> > .\n> >\n>\n>\n> --\n> - Alex\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAOOCX6PFZTZTT73M3UZED3QBHUZNA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22KCAA#issuecomment-515154176>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAOOCX323VU7EEDKYOGRPQLQBHUZNANCNFSM4IGKWYJA>\n> .\n>\n", "I'm not evading the question. I'm just telling you that tf.name_scope is\nnot the same thing as tf.variable_scope, and doesn't have (and never was\nintended to have) the functionality you seem to want it to have, which is\neasy to implement on top of the functionality it does have.\n\nOn Thu, Jul 25, 2019 at 11:39 AM Daniel Penalva <notifications@github.com>\nwrote:\n\n> you use variable scope, and them reuse it through variable scope, thats\n> ok.\n>\n> But to me seems that you are evading the question, how to you do require an\n> op, in a big model, by knowing the var/name space block and by knowing that\n> this op is the one that outputs tensors of interest.\n>\n> Tensorflow nowadays donst do that. I see you are member of Tensorflow, but\n> please dosnt evade the question because of that, it donst make sense since\n> the software is open source ...\n> Thank you\n>\n> On Thu, Jul 25, 2019 at 3:16 PM Alexandre Passos <notifications@github.com\n> >\n> wrote:\n>\n> > tf.name_scope is not a way for you to define an isolated namespace\n> within a\n> > graph. It just means that ops created inside that block have names that\n> > start with the name of the scope.\n> >\n> > On Thu, Jul 25, 2019 at 10:16 AM Daniel Penalva <\n> notifications@github.com>\n> > wrote:\n> >\n> > > I do not intend to be arrogant, but i can see the if else, or even\n> sigma\n> > > but will be a lot of, proliferating if i wanna to derive an\n> architecture\n> > > from Inception V3 and do Transfer Learning for Research.\n> > > That's why i see immense importance in the issue.\n> > >\n> > > On Thu, Jul 25, 2019 at 1:59 PM Daniel Penalva <dkajah@gmail.com>\n> wrote:\n> > >\n> > > > What is the point of being working inside a namespace that you\n> defined\n> > by\n> > > > a purpose, if you have to \"filter the ops by yourself\" ?\n> > > >\n> > > >\n> > > >\n> > > > On Thu, Jul 25, 2019 at 1:27 PM Alexandre Passos <\n> > > notifications@github.com>\n> > > > wrote:\n> > > >\n> > > >> You can always get the operations and filter them yourself.\n> > > >>\n> > > >> On Thu, Jul 25, 2019 at 9:14 AM Daniel Penalva <\n> > > notifications@github.com>\n> > > >> wrote:\n> > > >>\n> > > >> > So tensorflow is lacking a function that can know what name-space\n> > its\n> > > >> > talking about, unless you have another ideia for namespaces\n> > functions,\n> > > >> the\n> > > >> > only thing that resides is that they are appropriated names for a\n> > > graph\n> > > >> of\n> > > >> > ops.\n> > > >> >\n> > > >> > \u2014\n> > > >> > You are receiving this because you modified the open/close state.\n> > > >> > Reply to this email directly, view it on GitHub\n> > > >> > <\n> > > >>\n> > >\n> >\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRN65M7GWNFQEZZNMW3QBHGQDA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2Z6HPY#issuecomment-515105727\n> > > >> >,\n> > > >> > or mute the thread\n> > > >> > <\n> > > >>\n> > >\n> >\n> https://github.com/notifications/unsubscribe-auth/AAABHRPY7TSCJDSDMH6S6HDQBHGQDANCNFSM4IGKWYJA\n> > > >> >\n> > > >> > .\n> > > >> >\n> > > >>\n> > > >>\n> > > >> --\n> > > >> - Alex\n> > > >>\n> > > >> \u2014\n> > > >> You are receiving this because you authored the thread.\n> > > >> Reply to this email directly, view it on GitHub\n> > > >> <\n> > >\n> >\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAOOCXZ2CFMHZKG3LFLKGSDQBHH6ZA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2Z7HLQ#issuecomment-515109806\n> > > >,\n> > > >> or mute the thread\n> > > >> <\n> > >\n> >\n> https://github.com/notifications/unsubscribe-auth/AAOOCX64RSANGFT7EJXORBDQBHH6ZANCNFSM4IGKWYJA\n> > > >\n> > > >> .\n> > > >>\n> > > >\n> > >\n> > > \u2014\n> > > You are receiving this because you modified the open/close state.\n> > > Reply to this email directly, view it on GitHub\n> > > <\n> >\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRJFOGGIKJ3RZWXGNADQBHNWXA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22DYZY#issuecomment-515128423\n> > >,\n> > > or mute the thread\n> > > <\n> >\n> https://github.com/notifications/unsubscribe-auth/AAABHRLFCGOHXD6GLJAAWVLQBHNWXANCNFSM4IGKWYJA\n> > >\n> > > .\n> > >\n> >\n> >\n> > --\n> > - Alex\n> >\n> > \u2014\n> > You are receiving this because you authored the thread.\n> > Reply to this email directly, view it on GitHub\n> > <\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAOOCX6PFZTZTT73M3UZED3QBHUZNA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22KCAA#issuecomment-515154176\n> >,\n> > or mute the thread\n> > <\n> https://github.com/notifications/unsubscribe-auth/AAOOCX323VU7EEDKYOGRPQLQBHUZNANCNFSM4IGKWYJA\n> >\n> > .\n> >\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRMY33OYDA463A6ZEFLQBHXP3A5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22MGNI#issuecomment-515162933>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRIZG6ZP7CY2IVRTETDQBHXP3ANCNFSM4IGKWYJA>\n> .\n>\n\n\n-- \n - Alex\n", "suppose you have:\n/name_1/insidename1/Add\n/name_1/insidename1/Flat\n/name_1/insidename1/ininsidename1/Conv\n/name_1/insidename1/ininsidename1/Flat\n/name_1/insidename1/ininsidename1_1/SomeOp\n/name_2/insidename1/Add\n/name_2/insidename1/Flat\n/name_2/insidename1/ininsidename1/Conv\n/name_2/insidename1/ininsidename1/Flat\n/name_2/insidename1/ininsidename1_1/SomeOp\n\ni wanna the last operation of the first block(i.e. name_1)\n\nSo what are suggesting as a solution ? var scope name parsing and if\nelse  ?\n (note that this is a simple example, a if-else solution could be much more\ncomplicated than that in more sofisticated models)\n\nthere is a better option implemented ? could you suggest something to use ?\n\nOn Thu, Jul 25, 2019 at 4:34 PM Alexandre Passos <notifications@github.com>\nwrote:\n\n> I'm not evading the question. I'm just telling you that tf.name_scope is\n> not the same thing as tf.variable_scope, and doesn't have (and never was\n> intended to have) the functionality you seem to want it to have, which is\n> easy to implement on top of the functionality it does have.\n>\n> On Thu, Jul 25, 2019 at 11:39 AM Daniel Penalva <notifications@github.com>\n> wrote:\n>\n> > you use variable scope, and them reuse it through variable scope, thats\n> > ok.\n> >\n> > But to me seems that you are evading the question, how to you do require\n> an\n> > op, in a big model, by knowing the var/name space block and by knowing\n> that\n> > this op is the one that outputs tensors of interest.\n> >\n> > Tensorflow nowadays donst do that. I see you are member of Tensorflow,\n> but\n> > please dosnt evade the question because of that, it donst make sense\n> since\n> > the software is open source ...\n> > Thank you\n> >\n> > On Thu, Jul 25, 2019 at 3:16 PM Alexandre Passos <\n> notifications@github.com\n> > >\n> > wrote:\n> >\n> > > tf.name_scope is not a way for you to define an isolated namespace\n> > within a\n> > > graph. It just means that ops created inside that block have names that\n> > > start with the name of the scope.\n> > >\n> > > On Thu, Jul 25, 2019 at 10:16 AM Daniel Penalva <\n> > notifications@github.com>\n> > > wrote:\n> > >\n> > > > I do not intend to be arrogant, but i can see the if else, or even\n> > sigma\n> > > > but will be a lot of, proliferating if i wanna to derive an\n> > architecture\n> > > > from Inception V3 and do Transfer Learning for Research.\n> > > > That's why i see immense importance in the issue.\n> > > >\n> > > > On Thu, Jul 25, 2019 at 1:59 PM Daniel Penalva <dkajah@gmail.com>\n> > wrote:\n> > > >\n> > > > > What is the point of being working inside a namespace that you\n> > defined\n> > > by\n> > > > > a purpose, if you have to \"filter the ops by yourself\" ?\n> > > > >\n> > > > >\n> > > > >\n> > > > > On Thu, Jul 25, 2019 at 1:27 PM Alexandre Passos <\n> > > > notifications@github.com>\n> > > > > wrote:\n> > > > >\n> > > > >> You can always get the operations and filter them yourself.\n> > > > >>\n> > > > >> On Thu, Jul 25, 2019 at 9:14 AM Daniel Penalva <\n> > > > notifications@github.com>\n> > > > >> wrote:\n> > > > >>\n> > > > >> > So tensorflow is lacking a function that can know what\n> name-space\n> > > its\n> > > > >> > talking about, unless you have another ideia for namespaces\n> > > functions,\n> > > > >> the\n> > > > >> > only thing that resides is that they are appropriated names for\n> a\n> > > > graph\n> > > > >> of\n> > > > >> > ops.\n> > > > >> >\n> > > > >> > \u2014\n> > > > >> > You are receiving this because you modified the open/close\n> state.\n> > > > >> > Reply to this email directly, view it on GitHub\n> > > > >> > <\n> > > > >>\n> > > >\n> > >\n> >\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRN65M7GWNFQEZZNMW3QBHGQDA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2Z6HPY#issuecomment-515105727\n> > > > >> >,\n> > > > >> > or mute the thread\n> > > > >> > <\n> > > > >>\n> > > >\n> > >\n> >\n> https://github.com/notifications/unsubscribe-auth/AAABHRPY7TSCJDSDMH6S6HDQBHGQDANCNFSM4IGKWYJA\n> > > > >> >\n> > > > >> > .\n> > > > >> >\n> > > > >>\n> > > > >>\n> > > > >> --\n> > > > >> - Alex\n> > > > >>\n> > > > >> \u2014\n> > > > >> You are receiving this because you authored the thread.\n> > > > >> Reply to this email directly, view it on GitHub\n> > > > >> <\n> > > >\n> > >\n> >\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAOOCXZ2CFMHZKG3LFLKGSDQBHH6ZA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2Z7HLQ#issuecomment-515109806\n> > > > >,\n> > > > >> or mute the thread\n> > > > >> <\n> > > >\n> > >\n> >\n> https://github.com/notifications/unsubscribe-auth/AAOOCX64RSANGFT7EJXORBDQBHH6ZANCNFSM4IGKWYJA\n> > > > >\n> > > > >> .\n> > > > >>\n> > > > >\n> > > >\n> > > > \u2014\n> > > > You are receiving this because you modified the open/close state.\n> > > > Reply to this email directly, view it on GitHub\n> > > > <\n> > >\n> >\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRJFOGGIKJ3RZWXGNADQBHNWXA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22DYZY#issuecomment-515128423\n> > > >,\n> > > > or mute the thread\n> > > > <\n> > >\n> >\n> https://github.com/notifications/unsubscribe-auth/AAABHRLFCGOHXD6GLJAAWVLQBHNWXANCNFSM4IGKWYJA\n> > > >\n> > > > .\n> > > >\n> > >\n> > >\n> > > --\n> > > - Alex\n> > >\n> > > \u2014\n> > > You are receiving this because you authored the thread.\n> > > Reply to this email directly, view it on GitHub\n> > > <\n> >\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAOOCX6PFZTZTT73M3UZED3QBHUZNA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22KCAA#issuecomment-515154176\n> > >,\n> > > or mute the thread\n> > > <\n> >\n> https://github.com/notifications/unsubscribe-auth/AAOOCX323VU7EEDKYOGRPQLQBHUZNANCNFSM4IGKWYJA\n> > >\n> > > .\n> > >\n> >\n> > \u2014\n> > You are receiving this because you modified the open/close state.\n> > Reply to this email directly, view it on GitHub\n> > <\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRMY33OYDA463A6ZEFLQBHXP3A5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22MGNI#issuecomment-515162933\n> >,\n> > or mute the thread\n> > <\n> https://github.com/notifications/unsubscribe-auth/AAABHRIZG6ZP7CY2IVRTETDQBHXP3ANCNFSM4IGKWYJA\n> >\n> > .\n> >\n>\n>\n> --\n> - Alex\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAOOCX7PVJODCEYMJ4AS7Y3QBH525A5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22QWJI#issuecomment-515181349>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAOOCX7NQW4WWYU4NL5FSJLQBH525ANCNFSM4IGKWYJA>\n> .\n>\n", "You seemed to want earlier that graph.get_operations() inside a namescope\nreturns instead [x for x in graph.get_operations() if\nx.name.startswith(scope)].\n\nIn general if you want to get to an operation in the TF graph you should\nkeep a reference to it or give it a globally unique name (and keep a\nreference to that).\n\nOn Thu, Jul 25, 2019 at 2:05 PM Daniel Penalva <notifications@github.com>\nwrote:\n\n> suppose you have:\n> /name_1/insidename1/Add\n> /name_1/insidename1/Flat\n> /name_1/insidename1/ininsidename1/Conv\n> /name_1/insidename1/ininsidename1/Flat\n> /name_1/insidename1/ininsidename1_1/SomeOp\n> /name_2/insidename1/Add\n> /name_2/insidename1/Flat\n> /name_2/insidename1/ininsidename1/Conv\n> /name_2/insidename1/ininsidename1/Flat\n> /name_2/insidename1/ininsidename1_1/SomeOp\n>\n> i wanna the last operation of the first block(i.e. name_1)\n>\n> So what are suggesting as a solution ? var scope name parsing and if\n> else ?\n> (note that this is a simple example, a if-else solution could be much more\n> complicated than that in more sofisticated models)\n>\n> there is a better option implemented ? could you suggest something to use ?\n>\n> On Thu, Jul 25, 2019 at 4:34 PM Alexandre Passos <notifications@github.com\n> >\n> wrote:\n>\n> > I'm not evading the question. I'm just telling you that tf.name_scope is\n> > not the same thing as tf.variable_scope, and doesn't have (and never was\n> > intended to have) the functionality you seem to want it to have, which is\n> > easy to implement on top of the functionality it does have.\n> >\n> > On Thu, Jul 25, 2019 at 11:39 AM Daniel Penalva <\n> notifications@github.com>\n> > wrote:\n> >\n> > > you use variable scope, and them reuse it through variable scope, thats\n> > > ok.\n> > >\n> > > But to me seems that you are evading the question, how to you do\n> require\n> > an\n> > > op, in a big model, by knowing the var/name space block and by knowing\n> > that\n> > > this op is the one that outputs tensors of interest.\n> > >\n> > > Tensorflow nowadays donst do that. I see you are member of Tensorflow,\n> > but\n> > > please dosnt evade the question because of that, it donst make sense\n> > since\n> > > the software is open source ...\n> > > Thank you\n> > >\n> > > On Thu, Jul 25, 2019 at 3:16 PM Alexandre Passos <\n> > notifications@github.com\n> > > >\n> > > wrote:\n> > >\n> > > > tf.name_scope is not a way for you to define an isolated namespace\n> > > within a\n> > > > graph. It just means that ops created inside that block have names\n> that\n> > > > start with the name of the scope.\n> > > >\n> > > > On Thu, Jul 25, 2019 at 10:16 AM Daniel Penalva <\n> > > notifications@github.com>\n> > > > wrote:\n> > > >\n> > > > > I do not intend to be arrogant, but i can see the if else, or even\n> > > sigma\n> > > > > but will be a lot of, proliferating if i wanna to derive an\n> > > architecture\n> > > > > from Inception V3 and do Transfer Learning for Research.\n> > > > > That's why i see immense importance in the issue.\n> > > > >\n> > > > > On Thu, Jul 25, 2019 at 1:59 PM Daniel Penalva <dkajah@gmail.com>\n> > > wrote:\n> > > > >\n> > > > > > What is the point of being working inside a namespace that you\n> > > defined\n> > > > by\n> > > > > > a purpose, if you have to \"filter the ops by yourself\" ?\n> > > > > >\n> > > > > >\n> > > > > >\n> > > > > > On Thu, Jul 25, 2019 at 1:27 PM Alexandre Passos <\n> > > > > notifications@github.com>\n> > > > > > wrote:\n> > > > > >\n> > > > > >> You can always get the operations and filter them yourself.\n> > > > > >>\n> > > > > >> On Thu, Jul 25, 2019 at 9:14 AM Daniel Penalva <\n> > > > > notifications@github.com>\n> > > > > >> wrote:\n> > > > > >>\n> > > > > >> > So tensorflow is lacking a function that can know what\n> > name-space\n> > > > its\n> > > > > >> > talking about, unless you have another ideia for namespaces\n> > > > functions,\n> > > > > >> the\n> > > > > >> > only thing that resides is that they are appropriated names\n> for\n> > a\n> > > > > graph\n> > > > > >> of\n> > > > > >> > ops.\n> > > > > >> >\n> > > > > >> > \u2014\n> > > > > >> > You are receiving this because you modified the open/close\n> > state.\n> > > > > >> > Reply to this email directly, view it on GitHub\n> > > > > >> > <\n> > > > > >>\n> > > > >\n> > > >\n> > >\n> >\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRN65M7GWNFQEZZNMW3QBHGQDA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2Z6HPY#issuecomment-515105727\n> > > > > >> >,\n> > > > > >> > or mute the thread\n> > > > > >> > <\n> > > > > >>\n> > > > >\n> > > >\n> > >\n> >\n> https://github.com/notifications/unsubscribe-auth/AAABHRPY7TSCJDSDMH6S6HDQBHGQDANCNFSM4IGKWYJA\n> > > > > >> >\n> > > > > >> > .\n> > > > > >> >\n> > > > > >>\n> > > > > >>\n> > > > > >> --\n> > > > > >> - Alex\n> > > > > >>\n> > > > > >> \u2014\n> > > > > >> You are receiving this because you authored the thread.\n> > > > > >> Reply to this email directly, view it on GitHub\n> > > > > >> <\n> > > > >\n> > > >\n> > >\n> >\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAOOCXZ2CFMHZKG3LFLKGSDQBHH6ZA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2Z7HLQ#issuecomment-515109806\n> > > > > >,\n> > > > > >> or mute the thread\n> > > > > >> <\n> > > > >\n> > > >\n> > >\n> >\n> https://github.com/notifications/unsubscribe-auth/AAOOCX64RSANGFT7EJXORBDQBHH6ZANCNFSM4IGKWYJA\n> > > > > >\n> > > > > >> .\n> > > > > >>\n> > > > > >\n> > > > >\n> > > > > \u2014\n> > > > > You are receiving this because you modified the open/close state.\n> > > > > Reply to this email directly, view it on GitHub\n> > > > > <\n> > > >\n> > >\n> >\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRJFOGGIKJ3RZWXGNADQBHNWXA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22DYZY#issuecomment-515128423\n> > > > >,\n> > > > > or mute the thread\n> > > > > <\n> > > >\n> > >\n> >\n> https://github.com/notifications/unsubscribe-auth/AAABHRLFCGOHXD6GLJAAWVLQBHNWXANCNFSM4IGKWYJA\n> > > > >\n> > > > > .\n> > > > >\n> > > >\n> > > >\n> > > > --\n> > > > - Alex\n> > > >\n> > > > \u2014\n> > > > You are receiving this because you authored the thread.\n> > > > Reply to this email directly, view it on GitHub\n> > > > <\n> > >\n> >\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAOOCX6PFZTZTT73M3UZED3QBHUZNA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22KCAA#issuecomment-515154176\n> > > >,\n> > > > or mute the thread\n> > > > <\n> > >\n> >\n> https://github.com/notifications/unsubscribe-auth/AAOOCX323VU7EEDKYOGRPQLQBHUZNANCNFSM4IGKWYJA\n> > > >\n> > > > .\n> > > >\n> > >\n> > > \u2014\n> > > You are receiving this because you modified the open/close state.\n> > > Reply to this email directly, view it on GitHub\n> > > <\n> >\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRMY33OYDA463A6ZEFLQBHXP3A5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22MGNI#issuecomment-515162933\n> > >,\n> > > or mute the thread\n> > > <\n> >\n> https://github.com/notifications/unsubscribe-auth/AAABHRIZG6ZP7CY2IVRTETDQBHXP3ANCNFSM4IGKWYJA\n> > >\n> > > .\n> > >\n> >\n> >\n> > --\n> > - Alex\n> >\n> > \u2014\n> > You are receiving this because you authored the thread.\n> > Reply to this email directly, view it on GitHub\n> > <\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAOOCX7PVJODCEYMJ4AS7Y3QBH525A5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22QWJI#issuecomment-515181349\n> >,\n> > or mute the thread\n> > <\n> https://github.com/notifications/unsubscribe-auth/AAOOCX7NQW4WWYU4NL5FSJLQBH525ANCNFSM4IGKWYJA\n> >\n> > .\n> >\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRMZE27YIPJXKDPJ3DLQBIIRHA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22YBOI#issuecomment-515211449>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRLGFA3LOHGT2MLXP33QBIIRHANCNFSM4IGKWYJA>\n> .\n>\n\n\n-- \n - Alex\n", "agree with you, the problem is keeping track of the reference, it will not\nbe easy when same ops appear in embeded varscopes with the same root name\ni think this possible if you considwr the redundant ops in the same root as\nwith diferent subnames as edges of diferemt size\n\nEm qui, 25 de jul de 2019 18:38, Alexandre Passos <notifications@github.com>\nescreveu:\n\n> You seemed to want earlier that graph.get_operations() inside a namescope\n> returns instead [x for x in graph.get_operations() if\n> x.name.startswith(scope)].\n>\n> In general if you want to get to an operation in the TF graph you should\n> keep a reference to it or give it a globally unique name (and keep a\n> reference to that).\n>\n> On Thu, Jul 25, 2019 at 2:05 PM Daniel Penalva <notifications@github.com>\n> wrote:\n>\n> > suppose you have:\n> > /name_1/insidename1/Add\n> > /name_1/insidename1/Flat\n> > /name_1/insidename1/ininsidename1/Conv\n> > /name_1/insidename1/ininsidename1/Flat\n> > /name_1/insidename1/ininsidename1_1/SomeOp\n> > /name_2/insidename1/Add\n> > /name_2/insidename1/Flat\n> > /name_2/insidename1/ininsidename1/Conv\n> > /name_2/insidename1/ininsidename1/Flat\n> > /name_2/insidename1/ininsidename1_1/SomeOp\n> >\n> > i wanna the last operation of the first block(i.e. name_1)\n> >\n> > So what are suggesting as a solution ? var scope name parsing and if\n> > else ?\n> > (note that this is a simple example, a if-else solution could be much\n> more\n> > complicated than that in more sofisticated models)\n> >\n> > there is a better option implemented ? could you suggest something to\n> use ?\n> >\n> > On Thu, Jul 25, 2019 at 4:34 PM Alexandre Passos <\n> notifications@github.com\n> > >\n> > wrote:\n> >\n> > > I'm not evading the question. I'm just telling you that tf.name_scope\n> is\n> > > not the same thing as tf.variable_scope, and doesn't have (and never\n> was\n> > > intended to have) the functionality you seem to want it to have, which\n> is\n> > > easy to implement on top of the functionality it does have.\n> > >\n> > > On Thu, Jul 25, 2019 at 11:39 AM Daniel Penalva <\n> > notifications@github.com>\n> > > wrote:\n> > >\n> > > > you use variable scope, and them reuse it through variable scope,\n> thats\n> > > > ok.\n> > > >\n> > > > But to me seems that you are evading the question, how to you do\n> > require\n> > > an\n> > > > op, in a big model, by knowing the var/name space block and by\n> knowing\n> > > that\n> > > > this op is the one that outputs tensors of interest.\n> > > >\n> > > > Tensorflow nowadays donst do that. I see you are member of\n> Tensorflow,\n> > > but\n> > > > please dosnt evade the question because of that, it donst make sense\n> > > since\n> > > > the software is open source ...\n> > > > Thank you\n> > > >\n> > > > On Thu, Jul 25, 2019 at 3:16 PM Alexandre Passos <\n> > > notifications@github.com\n> > > > >\n> > > > wrote:\n> > > >\n> > > > > tf.name_scope is not a way for you to define an isolated namespace\n> > > > within a\n> > > > > graph. It just means that ops created inside that block have names\n> > that\n> > > > > start with the name of the scope.\n> > > > >\n> > > > > On Thu, Jul 25, 2019 at 10:16 AM Daniel Penalva <\n> > > > notifications@github.com>\n> > > > > wrote:\n> > > > >\n> > > > > > I do not intend to be arrogant, but i can see the if else, or\n> even\n> > > > sigma\n> > > > > > but will be a lot of, proliferating if i wanna to derive an\n> > > > architecture\n> > > > > > from Inception V3 and do Transfer Learning for Research.\n> > > > > > That's why i see immense importance in the issue.\n> > > > > >\n> > > > > > On Thu, Jul 25, 2019 at 1:59 PM Daniel Penalva <dkajah@gmail.com\n> >\n> > > > wrote:\n> > > > > >\n> > > > > > > What is the point of being working inside a namespace that you\n> > > > defined\n> > > > > by\n> > > > > > > a purpose, if you have to \"filter the ops by yourself\" ?\n> > > > > > >\n> > > > > > >\n> > > > > > >\n> > > > > > > On Thu, Jul 25, 2019 at 1:27 PM Alexandre Passos <\n> > > > > > notifications@github.com>\n> > > > > > > wrote:\n> > > > > > >\n> > > > > > >> You can always get the operations and filter them yourself.\n> > > > > > >>\n> > > > > > >> On Thu, Jul 25, 2019 at 9:14 AM Daniel Penalva <\n> > > > > > notifications@github.com>\n> > > > > > >> wrote:\n> > > > > > >>\n> > > > > > >> > So tensorflow is lacking a function that can know what\n> > > name-space\n> > > > > its\n> > > > > > >> > talking about, unless you have another ideia for namespaces\n> > > > > functions,\n> > > > > > >> the\n> > > > > > >> > only thing that resides is that they are appropriated names\n> > for\n> > > a\n> > > > > > graph\n> > > > > > >> of\n> > > > > > >> > ops.\n> > > > > > >> >\n> > > > > > >> > \u2014\n> > > > > > >> > You are receiving this because you modified the open/close\n> > > state.\n> > > > > > >> > Reply to this email directly, view it on GitHub\n> > > > > > >> > <\n> > > > > > >>\n> > > > > >\n> > > > >\n> > > >\n> > >\n> >\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRN65M7GWNFQEZZNMW3QBHGQDA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2Z6HPY#issuecomment-515105727\n> > > > > > >> >,\n> > > > > > >> > or mute the thread\n> > > > > > >> > <\n> > > > > > >>\n> > > > > >\n> > > > >\n> > > >\n> > >\n> >\n> https://github.com/notifications/unsubscribe-auth/AAABHRPY7TSCJDSDMH6S6HDQBHGQDANCNFSM4IGKWYJA\n> > > > > > >> >\n> > > > > > >> > .\n> > > > > > >> >\n> > > > > > >>\n> > > > > > >>\n> > > > > > >> --\n> > > > > > >> - Alex\n> > > > > > >>\n> > > > > > >> \u2014\n> > > > > > >> You are receiving this because you authored the thread.\n> > > > > > >> Reply to this email directly, view it on GitHub\n> > > > > > >> <\n> > > > > >\n> > > > >\n> > > >\n> > >\n> >\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAOOCXZ2CFMHZKG3LFLKGSDQBHH6ZA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2Z7HLQ#issuecomment-515109806\n> > > > > > >,\n> > > > > > >> or mute the thread\n> > > > > > >> <\n> > > > > >\n> > > > >\n> > > >\n> > >\n> >\n> https://github.com/notifications/unsubscribe-auth/AAOOCX64RSANGFT7EJXORBDQBHH6ZANCNFSM4IGKWYJA\n> > > > > > >\n> > > > > > >> .\n> > > > > > >>\n> > > > > > >\n> > > > > >\n> > > > > > \u2014\n> > > > > > You are receiving this because you modified the open/close state.\n> > > > > > Reply to this email directly, view it on GitHub\n> > > > > > <\n> > > > >\n> > > >\n> > >\n> >\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRJFOGGIKJ3RZWXGNADQBHNWXA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22DYZY#issuecomment-515128423\n> > > > > >,\n> > > > > > or mute the thread\n> > > > > > <\n> > > > >\n> > > >\n> > >\n> >\n> https://github.com/notifications/unsubscribe-auth/AAABHRLFCGOHXD6GLJAAWVLQBHNWXANCNFSM4IGKWYJA\n> > > > > >\n> > > > > > .\n> > > > > >\n> > > > >\n> > > > >\n> > > > > --\n> > > > > - Alex\n> > > > >\n> > > > > \u2014\n> > > > > You are receiving this because you authored the thread.\n> > > > > Reply to this email directly, view it on GitHub\n> > > > > <\n> > > >\n> > >\n> >\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAOOCX6PFZTZTT73M3UZED3QBHUZNA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22KCAA#issuecomment-515154176\n> > > > >,\n> > > > > or mute the thread\n> > > > > <\n> > > >\n> > >\n> >\n> https://github.com/notifications/unsubscribe-auth/AAOOCX323VU7EEDKYOGRPQLQBHUZNANCNFSM4IGKWYJA\n> > > > >\n> > > > > .\n> > > > >\n> > > >\n> > > > \u2014\n> > > > You are receiving this because you modified the open/close state.\n> > > > Reply to this email directly, view it on GitHub\n> > > > <\n> > >\n> >\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRMY33OYDA463A6ZEFLQBHXP3A5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22MGNI#issuecomment-515162933\n> > > >,\n> > > > or mute the thread\n> > > > <\n> > >\n> >\n> https://github.com/notifications/unsubscribe-auth/AAABHRIZG6ZP7CY2IVRTETDQBHXP3ANCNFSM4IGKWYJA\n> > > >\n> > > > .\n> > > >\n> > >\n> > >\n> > > --\n> > > - Alex\n> > >\n> > > \u2014\n> > > You are receiving this because you authored the thread.\n> > > Reply to this email directly, view it on GitHub\n> > > <\n> >\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAOOCX7PVJODCEYMJ4AS7Y3QBH525A5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22QWJI#issuecomment-515181349\n> > >,\n> > > or mute the thread\n> > > <\n> >\n> https://github.com/notifications/unsubscribe-auth/AAOOCX7NQW4WWYU4NL5FSJLQBH525ANCNFSM4IGKWYJA\n> > >\n> > > .\n> > >\n> >\n> > \u2014\n> > You are receiving this because you modified the open/close state.\n> > Reply to this email directly, view it on GitHub\n> > <\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRMZE27YIPJXKDPJ3DLQBIIRHA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22YBOI#issuecomment-515211449\n> >,\n> > or mute the thread\n> > <\n> https://github.com/notifications/unsubscribe-auth/AAABHRLGFA3LOHGT2MLXP33QBIIRHANCNFSM4IGKWYJA\n> >\n> > .\n> >\n>\n>\n> --\n> - Alex\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAOOCXYQV7I7RXPPAEVBHGDQBIMPDA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD222S5I#issuecomment-515221877>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAOOCX2GNR3MDRPMXPI6LU3QBIMPDANCNFSM4IGKWYJA>\n> .\n>\n", "that is, if you uniq embbeded ids as notion of distamce from macro id you\ncan track the real ops\n\n\nEm qui, 25 de jul de 2019 20:51, Daniel Penalva <dkajah@gmail.com> escreveu:\n\n> agree with you, the problem is keeping track of the reference, it will not\n> be easy when same ops appear in embeded varscopes with the same root name\n> i think this possible if you considwr the redundant ops in the same root\n> as with diferent subnames as edges of diferemt size\n>\n> Em qui, 25 de jul de 2019 18:38, Alexandre Passos <\n> notifications@github.com> escreveu:\n>\n>> You seemed to want earlier that graph.get_operations() inside a namescope\n>> returns instead [x for x in graph.get_operations() if\n>> x.name.startswith(scope)].\n>>\n>> In general if you want to get to an operation in the TF graph you should\n>> keep a reference to it or give it a globally unique name (and keep a\n>> reference to that).\n>>\n>> On Thu, Jul 25, 2019 at 2:05 PM Daniel Penalva <notifications@github.com>\n>> wrote:\n>>\n>> > suppose you have:\n>> > /name_1/insidename1/Add\n>> > /name_1/insidename1/Flat\n>> > /name_1/insidename1/ininsidename1/Conv\n>> > /name_1/insidename1/ininsidename1/Flat\n>> > /name_1/insidename1/ininsidename1_1/SomeOp\n>> > /name_2/insidename1/Add\n>> > /name_2/insidename1/Flat\n>> > /name_2/insidename1/ininsidename1/Conv\n>> > /name_2/insidename1/ininsidename1/Flat\n>> > /name_2/insidename1/ininsidename1_1/SomeOp\n>> >\n>> > i wanna the last operation of the first block(i.e. name_1)\n>> >\n>> > So what are suggesting as a solution ? var scope name parsing and if\n>> > else ?\n>> > (note that this is a simple example, a if-else solution could be much\n>> more\n>> > complicated than that in more sofisticated models)\n>> >\n>> > there is a better option implemented ? could you suggest something to\n>> use ?\n>> >\n>> > On Thu, Jul 25, 2019 at 4:34 PM Alexandre Passos <\n>> notifications@github.com\n>> > >\n>> > wrote:\n>> >\n>> > > I'm not evading the question. I'm just telling you that tf.name_scope\n>> is\n>> > > not the same thing as tf.variable_scope, and doesn't have (and never\n>> was\n>> > > intended to have) the functionality you seem to want it to have,\n>> which is\n>> > > easy to implement on top of the functionality it does have.\n>> > >\n>> > > On Thu, Jul 25, 2019 at 11:39 AM Daniel Penalva <\n>> > notifications@github.com>\n>> > > wrote:\n>> > >\n>> > > > you use variable scope, and them reuse it through variable scope,\n>> thats\n>> > > > ok.\n>> > > >\n>> > > > But to me seems that you are evading the question, how to you do\n>> > require\n>> > > an\n>> > > > op, in a big model, by knowing the var/name space block and by\n>> knowing\n>> > > that\n>> > > > this op is the one that outputs tensors of interest.\n>> > > >\n>> > > > Tensorflow nowadays donst do that. I see you are member of\n>> Tensorflow,\n>> > > but\n>> > > > please dosnt evade the question because of that, it donst make sense\n>> > > since\n>> > > > the software is open source ...\n>> > > > Thank you\n>> > > >\n>> > > > On Thu, Jul 25, 2019 at 3:16 PM Alexandre Passos <\n>> > > notifications@github.com\n>> > > > >\n>> > > > wrote:\n>> > > >\n>> > > > > tf.name_scope is not a way for you to define an isolated namespace\n>> > > > within a\n>> > > > > graph. It just means that ops created inside that block have names\n>> > that\n>> > > > > start with the name of the scope.\n>> > > > >\n>> > > > > On Thu, Jul 25, 2019 at 10:16 AM Daniel Penalva <\n>> > > > notifications@github.com>\n>> > > > > wrote:\n>> > > > >\n>> > > > > > I do not intend to be arrogant, but i can see the if else, or\n>> even\n>> > > > sigma\n>> > > > > > but will be a lot of, proliferating if i wanna to derive an\n>> > > > architecture\n>> > > > > > from Inception V3 and do Transfer Learning for Research.\n>> > > > > > That's why i see immense importance in the issue.\n>> > > > > >\n>> > > > > > On Thu, Jul 25, 2019 at 1:59 PM Daniel Penalva <\n>> dkajah@gmail.com>\n>> > > > wrote:\n>> > > > > >\n>> > > > > > > What is the point of being working inside a namespace that you\n>> > > > defined\n>> > > > > by\n>> > > > > > > a purpose, if you have to \"filter the ops by yourself\" ?\n>> > > > > > >\n>> > > > > > >\n>> > > > > > >\n>> > > > > > > On Thu, Jul 25, 2019 at 1:27 PM Alexandre Passos <\n>> > > > > > notifications@github.com>\n>> > > > > > > wrote:\n>> > > > > > >\n>> > > > > > >> You can always get the operations and filter them yourself.\n>> > > > > > >>\n>> > > > > > >> On Thu, Jul 25, 2019 at 9:14 AM Daniel Penalva <\n>> > > > > > notifications@github.com>\n>> > > > > > >> wrote:\n>> > > > > > >>\n>> > > > > > >> > So tensorflow is lacking a function that can know what\n>> > > name-space\n>> > > > > its\n>> > > > > > >> > talking about, unless you have another ideia for namespaces\n>> > > > > functions,\n>> > > > > > >> the\n>> > > > > > >> > only thing that resides is that they are appropriated names\n>> > for\n>> > > a\n>> > > > > > graph\n>> > > > > > >> of\n>> > > > > > >> > ops.\n>> > > > > > >> >\n>> > > > > > >> > \u2014\n>> > > > > > >> > You are receiving this because you modified the open/close\n>> > > state.\n>> > > > > > >> > Reply to this email directly, view it on GitHub\n>> > > > > > >> > <\n>> > > > > > >>\n>> > > > > >\n>> > > > >\n>> > > >\n>> > >\n>> >\n>> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRN65M7GWNFQEZZNMW3QBHGQDA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2Z6HPY#issuecomment-515105727\n>> > > > > > >> >,\n>> > > > > > >> > or mute the thread\n>> > > > > > >> > <\n>> > > > > > >>\n>> > > > > >\n>> > > > >\n>> > > >\n>> > >\n>> >\n>> https://github.com/notifications/unsubscribe-auth/AAABHRPY7TSCJDSDMH6S6HDQBHGQDANCNFSM4IGKWYJA\n>> > > > > > >> >\n>> > > > > > >> > .\n>> > > > > > >> >\n>> > > > > > >>\n>> > > > > > >>\n>> > > > > > >> --\n>> > > > > > >> - Alex\n>> > > > > > >>\n>> > > > > > >> \u2014\n>> > > > > > >> You are receiving this because you authored the thread.\n>> > > > > > >> Reply to this email directly, view it on GitHub\n>> > > > > > >> <\n>> > > > > >\n>> > > > >\n>> > > >\n>> > >\n>> >\n>> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAOOCXZ2CFMHZKG3LFLKGSDQBHH6ZA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2Z7HLQ#issuecomment-515109806\n>> > > > > > >,\n>> > > > > > >> or mute the thread\n>> > > > > > >> <\n>> > > > > >\n>> > > > >\n>> > > >\n>> > >\n>> >\n>> https://github.com/notifications/unsubscribe-auth/AAOOCX64RSANGFT7EJXORBDQBHH6ZANCNFSM4IGKWYJA\n>> > > > > > >\n>> > > > > > >> .\n>> > > > > > >>\n>> > > > > > >\n>> > > > > >\n>> > > > > > \u2014\n>> > > > > > You are receiving this because you modified the open/close\n>> state.\n>> > > > > > Reply to this email directly, view it on GitHub\n>> > > > > > <\n>> > > > >\n>> > > >\n>> > >\n>> >\n>> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRJFOGGIKJ3RZWXGNADQBHNWXA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22DYZY#issuecomment-515128423\n>> > > > > >,\n>> > > > > > or mute the thread\n>> > > > > > <\n>> > > > >\n>> > > >\n>> > >\n>> >\n>> https://github.com/notifications/unsubscribe-auth/AAABHRLFCGOHXD6GLJAAWVLQBHNWXANCNFSM4IGKWYJA\n>> > > > > >\n>> > > > > > .\n>> > > > > >\n>> > > > >\n>> > > > >\n>> > > > > --\n>> > > > > - Alex\n>> > > > >\n>> > > > > \u2014\n>> > > > > You are receiving this because you authored the thread.\n>> > > > > Reply to this email directly, view it on GitHub\n>> > > > > <\n>> > > >\n>> > >\n>> >\n>> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAOOCX6PFZTZTT73M3UZED3QBHUZNA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22KCAA#issuecomment-515154176\n>> > > > >,\n>> > > > > or mute the thread\n>> > > > > <\n>> > > >\n>> > >\n>> >\n>> https://github.com/notifications/unsubscribe-auth/AAOOCX323VU7EEDKYOGRPQLQBHUZNANCNFSM4IGKWYJA\n>> > > > >\n>> > > > > .\n>> > > > >\n>> > > >\n>> > > > \u2014\n>> > > > You are receiving this because you modified the open/close state.\n>> > > > Reply to this email directly, view it on GitHub\n>> > > > <\n>> > >\n>> >\n>> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRMY33OYDA463A6ZEFLQBHXP3A5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22MGNI#issuecomment-515162933\n>> > > >,\n>> > > > or mute the thread\n>> > > > <\n>> > >\n>> >\n>> https://github.com/notifications/unsubscribe-auth/AAABHRIZG6ZP7CY2IVRTETDQBHXP3ANCNFSM4IGKWYJA\n>> > > >\n>> > > > .\n>> > > >\n>> > >\n>> > >\n>> > > --\n>> > > - Alex\n>> > >\n>> > > \u2014\n>> > > You are receiving this because you authored the thread.\n>> > > Reply to this email directly, view it on GitHub\n>> > > <\n>> >\n>> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAOOCX7PVJODCEYMJ4AS7Y3QBH525A5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22QWJI#issuecomment-515181349\n>> > >,\n>> > > or mute the thread\n>> > > <\n>> >\n>> https://github.com/notifications/unsubscribe-auth/AAOOCX7NQW4WWYU4NL5FSJLQBH525ANCNFSM4IGKWYJA\n>> > >\n>> > > .\n>> > >\n>> >\n>> > \u2014\n>> > You are receiving this because you modified the open/close state.\n>> > Reply to this email directly, view it on GitHub\n>> > <\n>> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRMZE27YIPJXKDPJ3DLQBIIRHA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22YBOI#issuecomment-515211449\n>> >,\n>> > or mute the thread\n>> > <\n>> https://github.com/notifications/unsubscribe-auth/AAABHRLGFA3LOHGT2MLXP33QBIIRHANCNFSM4IGKWYJA\n>> >\n>> > .\n>> >\n>>\n>>\n>> --\n>> - Alex\n>>\n>> \u2014\n>> You are receiving this because you authored the thread.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAOOCXYQV7I7RXPPAEVBHGDQBIMPDA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD222S5I#issuecomment-515221877>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/AAOOCX2GNR3MDRPMXPI6LU3QBIMPDANCNFSM4IGKWYJA>\n>> .\n>>\n>\n", "The real solution is to not use name_scopes / variable_scopes to retrieve\nops later. Either manually keep them in a list, use context managers which\nactually track ops, or use tf.function / tf.compat.v1.wrap_function to get\nsemantically scoped subgraphs.\n\nOn Thu, Jul 25, 2019 at 5:11 PM Daniel Penalva <notifications@github.com>\nwrote:\n\n> that is, if you uniq embbeded ids as notion of distamce from macro id you\n> can track the real ops\n>\n>\n> Em qui, 25 de jul de 2019 20:51, Daniel Penalva <dkajah@gmail.com>\n> escreveu:\n>\n> > agree with you, the problem is keeping track of the reference, it will\n> not\n> > be easy when same ops appear in embeded varscopes with the same root name\n> > i think this possible if you considwr the redundant ops in the same root\n> > as with diferent subnames as edges of diferemt size\n> >\n> > Em qui, 25 de jul de 2019 18:38, Alexandre Passos <\n> > notifications@github.com> escreveu:\n> >\n> >> You seemed to want earlier that graph.get_operations() inside a\n> namescope\n> >> returns instead [x for x in graph.get_operations() if\n> >> x.name.startswith(scope)].\n> >>\n> >> In general if you want to get to an operation in the TF graph you should\n> >> keep a reference to it or give it a globally unique name (and keep a\n> >> reference to that).\n> >>\n> >> On Thu, Jul 25, 2019 at 2:05 PM Daniel Penalva <\n> notifications@github.com>\n> >> wrote:\n> >>\n> >> > suppose you have:\n> >> > /name_1/insidename1/Add\n> >> > /name_1/insidename1/Flat\n> >> > /name_1/insidename1/ininsidename1/Conv\n> >> > /name_1/insidename1/ininsidename1/Flat\n> >> > /name_1/insidename1/ininsidename1_1/SomeOp\n> >> > /name_2/insidename1/Add\n> >> > /name_2/insidename1/Flat\n> >> > /name_2/insidename1/ininsidename1/Conv\n> >> > /name_2/insidename1/ininsidename1/Flat\n> >> > /name_2/insidename1/ininsidename1_1/SomeOp\n> >> >\n> >> > i wanna the last operation of the first block(i.e. name_1)\n> >> >\n> >> > So what are suggesting as a solution ? var scope name parsing and if\n> >> > else ?\n> >> > (note that this is a simple example, a if-else solution could be much\n> >> more\n> >> > complicated than that in more sofisticated models)\n> >> >\n> >> > there is a better option implemented ? could you suggest something to\n> >> use ?\n> >> >\n> >> > On Thu, Jul 25, 2019 at 4:34 PM Alexandre Passos <\n> >> notifications@github.com\n> >> > >\n> >> > wrote:\n> >> >\n> >> > > I'm not evading the question. I'm just telling you that\n> tf.name_scope\n> >> is\n> >> > > not the same thing as tf.variable_scope, and doesn't have (and never\n> >> was\n> >> > > intended to have) the functionality you seem to want it to have,\n> >> which is\n> >> > > easy to implement on top of the functionality it does have.\n> >> > >\n> >> > > On Thu, Jul 25, 2019 at 11:39 AM Daniel Penalva <\n> >> > notifications@github.com>\n> >> > > wrote:\n> >> > >\n> >> > > > you use variable scope, and them reuse it through variable scope,\n> >> thats\n> >> > > > ok.\n> >> > > >\n> >> > > > But to me seems that you are evading the question, how to you do\n> >> > require\n> >> > > an\n> >> > > > op, in a big model, by knowing the var/name space block and by\n> >> knowing\n> >> > > that\n> >> > > > this op is the one that outputs tensors of interest.\n> >> > > >\n> >> > > > Tensorflow nowadays donst do that. I see you are member of\n> >> Tensorflow,\n> >> > > but\n> >> > > > please dosnt evade the question because of that, it donst make\n> sense\n> >> > > since\n> >> > > > the software is open source ...\n> >> > > > Thank you\n> >> > > >\n> >> > > > On Thu, Jul 25, 2019 at 3:16 PM Alexandre Passos <\n> >> > > notifications@github.com\n> >> > > > >\n> >> > > > wrote:\n> >> > > >\n> >> > > > > tf.name_scope is not a way for you to define an isolated\n> namespace\n> >> > > > within a\n> >> > > > > graph. It just means that ops created inside that block have\n> names\n> >> > that\n> >> > > > > start with the name of the scope.\n> >> > > > >\n> >> > > > > On Thu, Jul 25, 2019 at 10:16 AM Daniel Penalva <\n> >> > > > notifications@github.com>\n> >> > > > > wrote:\n> >> > > > >\n> >> > > > > > I do not intend to be arrogant, but i can see the if else, or\n> >> even\n> >> > > > sigma\n> >> > > > > > but will be a lot of, proliferating if i wanna to derive an\n> >> > > > architecture\n> >> > > > > > from Inception V3 and do Transfer Learning for Research.\n> >> > > > > > That's why i see immense importance in the issue.\n> >> > > > > >\n> >> > > > > > On Thu, Jul 25, 2019 at 1:59 PM Daniel Penalva <\n> >> dkajah@gmail.com>\n> >> > > > wrote:\n> >> > > > > >\n> >> > > > > > > What is the point of being working inside a namespace that\n> you\n> >> > > > defined\n> >> > > > > by\n> >> > > > > > > a purpose, if you have to \"filter the ops by yourself\" ?\n> >> > > > > > >\n> >> > > > > > >\n> >> > > > > > >\n> >> > > > > > > On Thu, Jul 25, 2019 at 1:27 PM Alexandre Passos <\n> >> > > > > > notifications@github.com>\n> >> > > > > > > wrote:\n> >> > > > > > >\n> >> > > > > > >> You can always get the operations and filter them yourself.\n> >> > > > > > >>\n> >> > > > > > >> On Thu, Jul 25, 2019 at 9:14 AM Daniel Penalva <\n> >> > > > > > notifications@github.com>\n> >> > > > > > >> wrote:\n> >> > > > > > >>\n> >> > > > > > >> > So tensorflow is lacking a function that can know what\n> >> > > name-space\n> >> > > > > its\n> >> > > > > > >> > talking about, unless you have another ideia for\n> namespaces\n> >> > > > > functions,\n> >> > > > > > >> the\n> >> > > > > > >> > only thing that resides is that they are appropriated\n> names\n> >> > for\n> >> > > a\n> >> > > > > > graph\n> >> > > > > > >> of\n> >> > > > > > >> > ops.\n> >> > > > > > >> >\n> >> > > > > > >> > \u2014\n> >> > > > > > >> > You are receiving this because you modified the\n> open/close\n> >> > > state.\n> >> > > > > > >> > Reply to this email directly, view it on GitHub\n> >> > > > > > >> > <\n> >> > > > > > >>\n> >> > > > > >\n> >> > > > >\n> >> > > >\n> >> > >\n> >> >\n> >>\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRN65M7GWNFQEZZNMW3QBHGQDA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2Z6HPY#issuecomment-515105727\n> >> > > > > > >> >,\n> >> > > > > > >> > or mute the thread\n> >> > > > > > >> > <\n> >> > > > > > >>\n> >> > > > > >\n> >> > > > >\n> >> > > >\n> >> > >\n> >> >\n> >>\n> https://github.com/notifications/unsubscribe-auth/AAABHRPY7TSCJDSDMH6S6HDQBHGQDANCNFSM4IGKWYJA\n> >> > > > > > >> >\n> >> > > > > > >> > .\n> >> > > > > > >> >\n> >> > > > > > >>\n> >> > > > > > >>\n> >> > > > > > >> --\n> >> > > > > > >> - Alex\n> >> > > > > > >>\n> >> > > > > > >> \u2014\n> >> > > > > > >> You are receiving this because you authored the thread.\n> >> > > > > > >> Reply to this email directly, view it on GitHub\n> >> > > > > > >> <\n> >> > > > > >\n> >> > > > >\n> >> > > >\n> >> > >\n> >> >\n> >>\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAOOCXZ2CFMHZKG3LFLKGSDQBHH6ZA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2Z7HLQ#issuecomment-515109806\n> >> > > > > > >,\n> >> > > > > > >> or mute the thread\n> >> > > > > > >> <\n> >> > > > > >\n> >> > > > >\n> >> > > >\n> >> > >\n> >> >\n> >>\n> https://github.com/notifications/unsubscribe-auth/AAOOCX64RSANGFT7EJXORBDQBHH6ZANCNFSM4IGKWYJA\n> >> > > > > > >\n> >> > > > > > >> .\n> >> > > > > > >>\n> >> > > > > > >\n> >> > > > > >\n> >> > > > > > \u2014\n> >> > > > > > You are receiving this because you modified the open/close\n> >> state.\n> >> > > > > > Reply to this email directly, view it on GitHub\n> >> > > > > > <\n> >> > > > >\n> >> > > >\n> >> > >\n> >> >\n> >>\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRJFOGGIKJ3RZWXGNADQBHNWXA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22DYZY#issuecomment-515128423\n> >> > > > > >,\n> >> > > > > > or mute the thread\n> >> > > > > > <\n> >> > > > >\n> >> > > >\n> >> > >\n> >> >\n> >>\n> https://github.com/notifications/unsubscribe-auth/AAABHRLFCGOHXD6GLJAAWVLQBHNWXANCNFSM4IGKWYJA\n> >> > > > > >\n> >> > > > > > .\n> >> > > > > >\n> >> > > > >\n> >> > > > >\n> >> > > > > --\n> >> > > > > - Alex\n> >> > > > >\n> >> > > > > \u2014\n> >> > > > > You are receiving this because you authored the thread.\n> >> > > > > Reply to this email directly, view it on GitHub\n> >> > > > > <\n> >> > > >\n> >> > >\n> >> >\n> >>\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAOOCX6PFZTZTT73M3UZED3QBHUZNA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22KCAA#issuecomment-515154176\n> >> > > > >,\n> >> > > > > or mute the thread\n> >> > > > > <\n> >> > > >\n> >> > >\n> >> >\n> >>\n> https://github.com/notifications/unsubscribe-auth/AAOOCX323VU7EEDKYOGRPQLQBHUZNANCNFSM4IGKWYJA\n> >> > > > >\n> >> > > > > .\n> >> > > > >\n> >> > > >\n> >> > > > \u2014\n> >> > > > You are receiving this because you modified the open/close state.\n> >> > > > Reply to this email directly, view it on GitHub\n> >> > > > <\n> >> > >\n> >> >\n> >>\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRMY33OYDA463A6ZEFLQBHXP3A5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22MGNI#issuecomment-515162933\n> >> > > >,\n> >> > > > or mute the thread\n> >> > > > <\n> >> > >\n> >> >\n> >>\n> https://github.com/notifications/unsubscribe-auth/AAABHRIZG6ZP7CY2IVRTETDQBHXP3ANCNFSM4IGKWYJA\n> >> > > >\n> >> > > > .\n> >> > > >\n> >> > >\n> >> > >\n> >> > > --\n> >> > > - Alex\n> >> > >\n> >> > > \u2014\n> >> > > You are receiving this because you authored the thread.\n> >> > > Reply to this email directly, view it on GitHub\n> >> > > <\n> >> >\n> >>\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAOOCX7PVJODCEYMJ4AS7Y3QBH525A5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22QWJI#issuecomment-515181349\n> >> > >,\n> >> > > or mute the thread\n> >> > > <\n> >> >\n> >>\n> https://github.com/notifications/unsubscribe-auth/AAOOCX7NQW4WWYU4NL5FSJLQBH525ANCNFSM4IGKWYJA\n> >> > >\n> >> > > .\n> >> > >\n> >> >\n> >> > \u2014\n> >> > You are receiving this because you modified the open/close state.\n> >> > Reply to this email directly, view it on GitHub\n> >> > <\n> >>\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRMZE27YIPJXKDPJ3DLQBIIRHA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22YBOI#issuecomment-515211449\n> >> >,\n> >> > or mute the thread\n> >> > <\n> >>\n> https://github.com/notifications/unsubscribe-auth/AAABHRLGFA3LOHGT2MLXP33QBIIRHANCNFSM4IGKWYJA\n> >> >\n> >> > .\n> >> >\n> >>\n> >>\n> >> --\n> >> - Alex\n> >>\n> >> \u2014\n> >> You are receiving this because you authored the thread.\n> >> Reply to this email directly, view it on GitHub\n> >> <\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAOOCXYQV7I7RXPPAEVBHGDQBIMPDA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD222S5I#issuecomment-515221877\n> >,\n> >> or mute the thread\n> >> <\n> https://github.com/notifications/unsubscribe-auth/AAOOCX2GNR3MDRPMXPI6LU3QBIMPDANCNFSM4IGKWYJA\n> >\n> >> .\n> >>\n> >\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRIBRMFQVYXP7FPU27DQBI6LNA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD23DMVQ#issuecomment-515257942>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRJ55QUUW5GHU7LQ7ELQBI6LNANCNFSM4IGKWYJA>\n> .\n>\n\n\n-- \n - Alex\n", "this is a 2.0beta feature only, it only works in eager mode, but that's\nfine for now, will be dedicating to the other issue that seems to have more\nconcise precision.\n\nthanks.\n\nOn Fri, Jul 26, 2019 at 12:28 PM Alexandre Passos <notifications@github.com>\nwrote:\n\n> The real solution is to not use name_scopes / variable_scopes to retrieve\n> ops later. Either manually keep them in a list, use context managers which\n> actually track ops, or use tf.function / tf.compat.v1.wrap_function to get\n> semantically scoped subgraphs.\n>\n> On Thu, Jul 25, 2019 at 5:11 PM Daniel Penalva <notifications@github.com>\n> wrote:\n>\n> > that is, if you uniq embbeded ids as notion of distamce from macro id you\n> > can track the real ops\n> >\n> >\n> > Em qui, 25 de jul de 2019 20:51, Daniel Penalva <dkajah@gmail.com>\n> > escreveu:\n> >\n> > > agree with you, the problem is keeping track of the reference, it will\n> > not\n> > > be easy when same ops appear in embeded varscopes with the same root\n> name\n> > > i think this possible if you considwr the redundant ops in the same\n> root\n> > > as with diferent subnames as edges of diferemt size\n> > >\n> > > Em qui, 25 de jul de 2019 18:38, Alexandre Passos <\n> > > notifications@github.com> escreveu:\n> > >\n> > >> You seemed to want earlier that graph.get_operations() inside a\n> > namescope\n> > >> returns instead [x for x in graph.get_operations() if\n> > >> x.name.startswith(scope)].\n> > >>\n> > >> In general if you want to get to an operation in the TF graph you\n> should\n> > >> keep a reference to it or give it a globally unique name (and keep a\n> > >> reference to that).\n> > >>\n> > >> On Thu, Jul 25, 2019 at 2:05 PM Daniel Penalva <\n> > notifications@github.com>\n> > >> wrote:\n> > >>\n> > >> > suppose you have:\n> > >> > /name_1/insidename1/Add\n> > >> > /name_1/insidename1/Flat\n> > >> > /name_1/insidename1/ininsidename1/Conv\n> > >> > /name_1/insidename1/ininsidename1/Flat\n> > >> > /name_1/insidename1/ininsidename1_1/SomeOp\n> > >> > /name_2/insidename1/Add\n> > >> > /name_2/insidename1/Flat\n> > >> > /name_2/insidename1/ininsidename1/Conv\n> > >> > /name_2/insidename1/ininsidename1/Flat\n> > >> > /name_2/insidename1/ininsidename1_1/SomeOp\n> > >> >\n> > >> > i wanna the last operation of the first block(i.e. name_1)\n> > >> >\n> > >> > So what are suggesting as a solution ? var scope name parsing and if\n> > >> > else ?\n> > >> > (note that this is a simple example, a if-else solution could be\n> much\n> > >> more\n> > >> > complicated than that in more sofisticated models)\n> > >> >\n> > >> > there is a better option implemented ? could you suggest something\n> to\n> > >> use ?\n> > >> >\n> > >> > On Thu, Jul 25, 2019 at 4:34 PM Alexandre Passos <\n> > >> notifications@github.com\n> > >> > >\n> > >> > wrote:\n> > >> >\n> > >> > > I'm not evading the question. I'm just telling you that\n> > tf.name_scope\n> > >> is\n> > >> > > not the same thing as tf.variable_scope, and doesn't have (and\n> never\n> > >> was\n> > >> > > intended to have) the functionality you seem to want it to have,\n> > >> which is\n> > >> > > easy to implement on top of the functionality it does have.\n> > >> > >\n> > >> > > On Thu, Jul 25, 2019 at 11:39 AM Daniel Penalva <\n> > >> > notifications@github.com>\n> > >> > > wrote:\n> > >> > >\n> > >> > > > you use variable scope, and them reuse it through variable\n> scope,\n> > >> thats\n> > >> > > > ok.\n> > >> > > >\n> > >> > > > But to me seems that you are evading the question, how to you do\n> > >> > require\n> > >> > > an\n> > >> > > > op, in a big model, by knowing the var/name space block and by\n> > >> knowing\n> > >> > > that\n> > >> > > > this op is the one that outputs tensors of interest.\n> > >> > > >\n> > >> > > > Tensorflow nowadays donst do that. I see you are member of\n> > >> Tensorflow,\n> > >> > > but\n> > >> > > > please dosnt evade the question because of that, it donst make\n> > sense\n> > >> > > since\n> > >> > > > the software is open source ...\n> > >> > > > Thank you\n> > >> > > >\n> > >> > > > On Thu, Jul 25, 2019 at 3:16 PM Alexandre Passos <\n> > >> > > notifications@github.com\n> > >> > > > >\n> > >> > > > wrote:\n> > >> > > >\n> > >> > > > > tf.name_scope is not a way for you to define an isolated\n> > namespace\n> > >> > > > within a\n> > >> > > > > graph. It just means that ops created inside that block have\n> > names\n> > >> > that\n> > >> > > > > start with the name of the scope.\n> > >> > > > >\n> > >> > > > > On Thu, Jul 25, 2019 at 10:16 AM Daniel Penalva <\n> > >> > > > notifications@github.com>\n> > >> > > > > wrote:\n> > >> > > > >\n> > >> > > > > > I do not intend to be arrogant, but i can see the if else,\n> or\n> > >> even\n> > >> > > > sigma\n> > >> > > > > > but will be a lot of, proliferating if i wanna to derive an\n> > >> > > > architecture\n> > >> > > > > > from Inception V3 and do Transfer Learning for Research.\n> > >> > > > > > That's why i see immense importance in the issue.\n> > >> > > > > >\n> > >> > > > > > On Thu, Jul 25, 2019 at 1:59 PM Daniel Penalva <\n> > >> dkajah@gmail.com>\n> > >> > > > wrote:\n> > >> > > > > >\n> > >> > > > > > > What is the point of being working inside a namespace that\n> > you\n> > >> > > > defined\n> > >> > > > > by\n> > >> > > > > > > a purpose, if you have to \"filter the ops by yourself\" ?\n> > >> > > > > > >\n> > >> > > > > > >\n> > >> > > > > > >\n> > >> > > > > > > On Thu, Jul 25, 2019 at 1:27 PM Alexandre Passos <\n> > >> > > > > > notifications@github.com>\n> > >> > > > > > > wrote:\n> > >> > > > > > >\n> > >> > > > > > >> You can always get the operations and filter them\n> yourself.\n> > >> > > > > > >>\n> > >> > > > > > >> On Thu, Jul 25, 2019 at 9:14 AM Daniel Penalva <\n> > >> > > > > > notifications@github.com>\n> > >> > > > > > >> wrote:\n> > >> > > > > > >>\n> > >> > > > > > >> > So tensorflow is lacking a function that can know what\n> > >> > > name-space\n> > >> > > > > its\n> > >> > > > > > >> > talking about, unless you have another ideia for\n> > namespaces\n> > >> > > > > functions,\n> > >> > > > > > >> the\n> > >> > > > > > >> > only thing that resides is that they are appropriated\n> > names\n> > >> > for\n> > >> > > a\n> > >> > > > > > graph\n> > >> > > > > > >> of\n> > >> > > > > > >> > ops.\n> > >> > > > > > >> >\n> > >> > > > > > >> > \u2014\n> > >> > > > > > >> > You are receiving this because you modified the\n> > open/close\n> > >> > > state.\n> > >> > > > > > >> > Reply to this email directly, view it on GitHub\n> > >> > > > > > >> > <\n> > >> > > > > > >>\n> > >> > > > > >\n> > >> > > > >\n> > >> > > >\n> > >> > >\n> > >> >\n> > >>\n> >\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRN65M7GWNFQEZZNMW3QBHGQDA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2Z6HPY#issuecomment-515105727\n> > >> > > > > > >> >,\n> > >> > > > > > >> > or mute the thread\n> > >> > > > > > >> > <\n> > >> > > > > > >>\n> > >> > > > > >\n> > >> > > > >\n> > >> > > >\n> > >> > >\n> > >> >\n> > >>\n> >\n> https://github.com/notifications/unsubscribe-auth/AAABHRPY7TSCJDSDMH6S6HDQBHGQDANCNFSM4IGKWYJA\n> > >> > > > > > >> >\n> > >> > > > > > >> > .\n> > >> > > > > > >> >\n> > >> > > > > > >>\n> > >> > > > > > >>\n> > >> > > > > > >> --\n> > >> > > > > > >> - Alex\n> > >> > > > > > >>\n> > >> > > > > > >> \u2014\n> > >> > > > > > >> You are receiving this because you authored the thread.\n> > >> > > > > > >> Reply to this email directly, view it on GitHub\n> > >> > > > > > >> <\n> > >> > > > > >\n> > >> > > > >\n> > >> > > >\n> > >> > >\n> > >> >\n> > >>\n> >\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAOOCXZ2CFMHZKG3LFLKGSDQBHH6ZA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2Z7HLQ#issuecomment-515109806\n> > >> > > > > > >,\n> > >> > > > > > >> or mute the thread\n> > >> > > > > > >> <\n> > >> > > > > >\n> > >> > > > >\n> > >> > > >\n> > >> > >\n> > >> >\n> > >>\n> >\n> https://github.com/notifications/unsubscribe-auth/AAOOCX64RSANGFT7EJXORBDQBHH6ZANCNFSM4IGKWYJA\n> > >> > > > > > >\n> > >> > > > > > >> .\n> > >> > > > > > >>\n> > >> > > > > > >\n> > >> > > > > >\n> > >> > > > > > \u2014\n> > >> > > > > > You are receiving this because you modified the open/close\n> > >> state.\n> > >> > > > > > Reply to this email directly, view it on GitHub\n> > >> > > > > > <\n> > >> > > > >\n> > >> > > >\n> > >> > >\n> > >> >\n> > >>\n> >\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRJFOGGIKJ3RZWXGNADQBHNWXA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22DYZY#issuecomment-515128423\n> > >> > > > > >,\n> > >> > > > > > or mute the thread\n> > >> > > > > > <\n> > >> > > > >\n> > >> > > >\n> > >> > >\n> > >> >\n> > >>\n> >\n> https://github.com/notifications/unsubscribe-auth/AAABHRLFCGOHXD6GLJAAWVLQBHNWXANCNFSM4IGKWYJA\n> > >> > > > > >\n> > >> > > > > > .\n> > >> > > > > >\n> > >> > > > >\n> > >> > > > >\n> > >> > > > > --\n> > >> > > > > - Alex\n> > >> > > > >\n> > >> > > > > \u2014\n> > >> > > > > You are receiving this because you authored the thread.\n> > >> > > > > Reply to this email directly, view it on GitHub\n> > >> > > > > <\n> > >> > > >\n> > >> > >\n> > >> >\n> > >>\n> >\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAOOCX6PFZTZTT73M3UZED3QBHUZNA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22KCAA#issuecomment-515154176\n> > >> > > > >,\n> > >> > > > > or mute the thread\n> > >> > > > > <\n> > >> > > >\n> > >> > >\n> > >> >\n> > >>\n> >\n> https://github.com/notifications/unsubscribe-auth/AAOOCX323VU7EEDKYOGRPQLQBHUZNANCNFSM4IGKWYJA\n> > >> > > > >\n> > >> > > > > .\n> > >> > > > >\n> > >> > > >\n> > >> > > > \u2014\n> > >> > > > You are receiving this because you modified the open/close\n> state.\n> > >> > > > Reply to this email directly, view it on GitHub\n> > >> > > > <\n> > >> > >\n> > >> >\n> > >>\n> >\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRMY33OYDA463A6ZEFLQBHXP3A5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22MGNI#issuecomment-515162933\n> > >> > > >,\n> > >> > > > or mute the thread\n> > >> > > > <\n> > >> > >\n> > >> >\n> > >>\n> >\n> https://github.com/notifications/unsubscribe-auth/AAABHRIZG6ZP7CY2IVRTETDQBHXP3ANCNFSM4IGKWYJA\n> > >> > > >\n> > >> > > > .\n> > >> > > >\n> > >> > >\n> > >> > >\n> > >> > > --\n> > >> > > - Alex\n> > >> > >\n> > >> > > \u2014\n> > >> > > You are receiving this because you authored the thread.\n> > >> > > Reply to this email directly, view it on GitHub\n> > >> > > <\n> > >> >\n> > >>\n> >\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAOOCX7PVJODCEYMJ4AS7Y3QBH525A5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22QWJI#issuecomment-515181349\n> > >> > >,\n> > >> > > or mute the thread\n> > >> > > <\n> > >> >\n> > >>\n> >\n> https://github.com/notifications/unsubscribe-auth/AAOOCX7NQW4WWYU4NL5FSJLQBH525ANCNFSM4IGKWYJA\n> > >> > >\n> > >> > > .\n> > >> > >\n> > >> >\n> > >> > \u2014\n> > >> > You are receiving this because you modified the open/close state.\n> > >> > Reply to this email directly, view it on GitHub\n> > >> > <\n> > >>\n> >\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRMZE27YIPJXKDPJ3DLQBIIRHA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD22YBOI#issuecomment-515211449\n> > >> >,\n> > >> > or mute the thread\n> > >> > <\n> > >>\n> >\n> https://github.com/notifications/unsubscribe-auth/AAABHRLGFA3LOHGT2MLXP33QBIIRHANCNFSM4IGKWYJA\n> > >> >\n> > >> > .\n> > >> >\n> > >>\n> > >>\n> > >> --\n> > >> - Alex\n> > >>\n> > >> \u2014\n> > >> You are receiving this because you authored the thread.\n> > >> Reply to this email directly, view it on GitHub\n> > >> <\n> >\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAOOCXYQV7I7RXPPAEVBHGDQBIMPDA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD222S5I#issuecomment-515221877\n> > >,\n> > >> or mute the thread\n> > >> <\n> >\n> https://github.com/notifications/unsubscribe-auth/AAOOCX2GNR3MDRPMXPI6LU3QBIMPDANCNFSM4IGKWYJA\n> > >\n> > >> .\n> > >>\n> > >\n> >\n> > \u2014\n> > You are receiving this because you modified the open/close state.\n> > Reply to this email directly, view it on GitHub\n> > <\n> https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAABHRIBRMFQVYXP7FPU27DQBI6LNA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD23DMVQ#issuecomment-515257942\n> >,\n> > or mute the thread\n> > <\n> https://github.com/notifications/unsubscribe-auth/AAABHRJ55QUUW5GHU7LQ7ELQBI6LNANCNFSM4IGKWYJA\n> >\n> > .\n> >\n>\n>\n> --\n> - Alex\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/30970?email_source=notifications&email_token=AAOOCX24R7GJB7UTEXXJWP3QBMJZPA5CNFSM4IGKWYJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD245KZI#issuecomment-515495269>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAOOCX3VY7ZDOTPPIJR76JDQBMJZPANCNFSM4IGKWYJA>\n> .\n>\n"]}, {"number": 30969, "title": "TF-TRT does not return  correct batch size.", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):1.13.1\r\n- Python version:2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.1\r\n- GPU model and memory:T4\r\n\r\n\r\n**Describe the current behavior**\r\nTF-TRT  seems to return max_batch_size instead of returning the feed batch size.  This breaks current code badly.\r\n\r\n**Describe the expected behavior**\r\nReturn the batch size as plain TF.\r\n\r\n**Code to reproduce the issue**\r\nShould be simple to reproduce. \r\n\r\n**Other info / logs**\r\nNone\r\n", "comments": ["@sgambient ,\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "Ok, will try to . It is related to pyfunc that returns  2 arrays of different  first dimension ( e.g.  (25,5) (1,) ) .\r\n", "@sgambient ,\r\nPlease provide the code to reproduce the issue.Thanks!", "Found a workaround, but it is beyond ugly.   Could not find any  clean way of getting specific output from multi-output networks, although TF  does assign a unique name to each output.  The code is attached. \r\n\r\n```python\r\nimport tensorflow as tf                                                                                                                                           [24/1007]\r\nimport tensorflow.contrib.tensorrt as trt                                                                                                                                  \r\nimport numpy as np                                                                                                                                                         \r\nimport os                                                                                                                                                                  \r\ninput_names  = []                                                                                                                                                          \r\noutput_names = []                                                                                                                                                          \r\noutput_indexes = []                                                                                                                                                        \r\noutput_nodes = []                                                                                                                                                          \r\n                                                                                                                                                                           \r\ndef netw():                                                                                                                                                                \r\n    global output_names                                                                                                                                                    \r\n    global input_names                                                                                                                                                     \r\n                                                                                                                                                                           \r\n    x = tf.placeholder(tf.float32, shape=(None,), name ='x')                                                                                                               \r\n    ys = tf.py_func(pyfunc1, [x], [tf.float32, tf.float32], name = 'y')                                                                                                    \r\n    print (\"x\", x)                                                                                                                                                         \r\n    print (\"ys\", ys)                                                                                                                                                       \r\n    input_names = [x.name]                                                                                                                                                 \r\n    for y in ys:                                                                                                                                                           \r\n        output_names.append(y.name.split(\":\")[0])                                                                                                                          \r\n        output_indexes.append(int(y.name.split(\":\")[1]))                                                                                                                   \r\n                                                                                                                                                                           \r\ndef netw2(x):                                                                                                                                                              \r\n    global output_names                                                                                                                                                    \r\n    z = tf.math.square(x, name='z')                                                                                                                                        \r\n    output_names.append(z.name.split(\":\")[0])\r\n    output_indexes.append(int(z.name.split(\":\")[1]))\r\n    output_nodes.append(z)\r\n\r\ndef pyfunc1(x):\r\n    return np.sinh(x), np.array (x.shape, dtype=np.float32)\r\n\r\ndef run_meta():\r\n    TENSORBOARD_DIR = os.environ.get('TENSORBOARD_DIR')\r\n    # Inference with TF-TRT `MetaGraph` and checkpoint files workflow:\r\n    graph = tf.Graph()\r\n    with graph.as_default():\r\n        netw()\r\n        with tf.Session() as sess:\r\n            # First create a `Saver` object (for saving and rebuilding a\r\n            # model) and import your `MetaGraphDef` protocol buffer into it:\r\n            #saver = tf.train.import_meta_graph(\"~/path/to/your/model.ckpt.meta\")\r\n            # Then restore your training data from checkpoint files:\r\n            #saver.restore(sess, \"~/path/to/your/model.ckptx\")\r\n            # Finally, freeze the graph:\r\n            frozen_graph = tf.graph_util.convert_variables_to_constants(\r\n                            sess,\r\n                            tf.get_default_graph().as_graph_def(),\r\n                            output_node_names=output_names\r\n                            )\r\n            # Now you can create a TensorRT inference graph from your\r\n            # frozen graph:\r\n            trt_graph = trt.create_inference_graph(\r\n                            input_graph_def=frozen_graph,\r\n                            outputs=output_names,\r\n                            max_batch_size=25,\r\n                            max_workspace_size_bytes=1024*1024*1024,\r\n                            precision_mode=\"FP16\")\r\n            # Import the TensorRT graph into a new graph and run:\r\n            trtnodes = tf.import_graph_def(\r\n                        trt_graph,\r\n                        return_elements=input_names+output_names)\r\n            for n in trtnodes:\r\n                print ( \"n \", n)\r\n            x = np.full ((10,), 0.5, dtype=np.float)\r\n            feed_dict = {trtnodes[0]: x}\r\n            for i, n in enumerate (trtnodes[len(input_names):] ):\r\n                idx = output_indexes[i]\r\n                #idx = 0\r\n                output_nodes.append(n.outputs[idx])\r\n            netw2(output_nodes[0])\r\n            file_writer = tf.summary.FileWriter(TENSORBOARD_DIR, sess.graph)\r\n            ys = sess.run(output_nodes, feed_dict=feed_dict)\r\n            print (\"x\", x, \" ys \", ys)\r\n    print (\"done run_meta\")\r\n\r\nif __name__ == '__main__':\r\n    run_meta()\r\n\r\n```", "Please find the gist of [collab](https://colab.research.google.com/drive/1AmdeHVVcWFKhvmZRHFUR00UIsZQx42p1) executed for the given code TF version-1.13.Thanks!", "@sgambient I tried with your script but it doesn't output any errors. By `TF-TRT seems to return max_batch_size instead of returning the feed batch size. This breaks current code badly`, could you elaborate? E.g. what does it mean  by `return max_batch_size` and how does it `break current code`?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30969\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30969\">No</a>\n"]}, {"number": 30968, "title": "[INTEL MKL] Fix Conv3D output tensor shape when the tensor is empty.", "body": "Also added a unit test and fixed some existing Clang issues in mkl_conv_ops.cc.", "comments": []}, {"number": 30967, "title": "TensorFlow 1.14 custom estimator model slower than TensorFlow 1.13", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo\r\n- TensorFlow installed from (source or binary):\r\nBinary\r\n- TensorFlow version (use command below):\r\nTensorFlow 1.14\r\n- Python version:\r\nPython 2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\nCUDA 10, CuDNN 7.4.2\r\n- GPU model and memory:\r\nGeForce GTX 1080Ti\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nI was trying to upgrade my tensorflow from 1.13 to tensorflow 1.14. I noticed that the training speed for my models dropped a lot.\r\n\r\nThen I tried the tensorflow custom estimator and reproduced the problem.\r\nhttps://github.com/tensorflow/models/blob/master/samples/core/get_started/custom_estimator.py\r\n\r\nThis is the speed with tf 1.13.\r\nINFO:tensorflow:loss = 2.3727872, step = 0\r\nINFO:tensorflow:global_step/sec: 293.209\r\nINFO:tensorflow:loss = 0.17633303, step = 100 (0.341 sec)\r\nINFO:tensorflow:global_step/sec: 337.629\r\nINFO:tensorflow:loss = 0.09756016, step = 200 (0.296 sec)\r\nINFO:tensorflow:global_step/sec: 342.554\r\nINFO:tensorflow:loss = 0.10544465, step = 300 (0.292 sec)\r\nINFO:tensorflow:global_step/sec: 342.051\r\nINFO:tensorflow:loss = 0.049246334, step = 400 (0.292 sec)\r\nINFO:tensorflow:global_step/sec: 347.739\r\nINFO:tensorflow:loss = 0.09701028, step = 500 (0.288 sec)\r\nINFO:tensorflow:global_step/sec: 347.722\r\nINFO:tensorflow:loss = 0.063723646, step = 600 (0.287 sec)\r\nINFO:tensorflow:global_step/sec: 329.121\r\nINFO:tensorflow:loss = 0.045053717, step = 700 (0.304 sec)\r\nINFO:tensorflow:global_step/sec: 295.816\r\nINFO:tensorflow:loss = 0.061943192, step = 800 (0.338 sec)\r\nINFO:tensorflow:global_step/sec: 295.844\r\nINFO:tensorflow:loss = 0.029347159, step = 900 (0.338 sec)\r\n\r\nThis is the speed with tf 1.14.\r\nI0722 22:54:59.609863 139637823923968 basic_session_run_hooks.py:262] loss = 1.0876435, step = 0\r\nI0722 22:54:59.962240 139637823923968 basic_session_run_hooks.py:692] global_step/sec: 283.369\r\nI0722 22:54:59.963597 139637823923968 basic_session_run_hooks.py:260] loss = 0.13201241, step = 100 (0.354 sec)\r\nI0722 22:55:00.268446 139637823923968 basic_session_run_hooks.py:692] global_step/sec: 326.584\r\nI0722 22:55:00.270031 139637823923968 basic_session_run_hooks.py:260] loss = 0.09678831, step = 200 (0.306 sec)\r\nI0722 22:55:00.601743 139637823923968 basic_session_run_hooks.py:692] global_step/sec: 299.99\r\nI0722 22:55:00.602967 139637823923968 basic_session_run_hooks.py:260] loss = 0.07743741, step = 300 (0.333 sec)\r\nI0722 22:55:00.933660 139637823923968 basic_session_run_hooks.py:692] global_step/sec: 301.274\r\nI0722 22:55:00.935039 139637823923968 basic_session_run_hooks.py:260] loss = 0.08071006, step = 400 (0.332 sec)\r\nI0722 22:55:01.271045 139637823923968 basic_session_run_hooks.py:692] global_step/sec: 296.425\r\nI0722 22:55:01.272447 139637823923968 basic_session_run_hooks.py:260] loss = 0.10450166, step = 500 (0.337 sec)\r\nI0722 22:55:01.605127 139637823923968 basic_session_run_hooks.py:692] global_step/sec: 299.334\r\nI0722 22:55:01.606523 139637823923968 basic_session_run_hooks.py:260] loss = 0.06578785, step = 600 (0.334 sec)\r\nI0722 22:55:01.946224 139637823923968 basic_session_run_hooks.py:692] global_step/sec: 293.167\r\nI0722 22:55:01.947628 139637823923968 basic_session_run_hooks.py:260] loss = 0.09801171, step = 700 (0.341 sec)\r\nI0722 22:55:02.284446 139637823923968 basic_session_run_hooks.py:692] global_step/sec: 295.649\r\nI0722 22:55:02.285842 139637823923968 basic_session_run_hooks.py:260] loss = 0.05814831, step = 800 (0.338 sec)\r\nI0722 22:55:02.621150 139637823923968 basic_session_run_hooks.py:692] global_step/sec: 297.002\r\nI0722 22:55:02.622456 139637823923968 basic_session_run_hooks.py:260] loss = 0.056445364, step = 900 (0.337 sec)\r\n\r\n**Describe the expected behavior**\r\nI would expect tensorflow has similar or better performance after the upgrade.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nhttps://github.com/tensorflow/models/blob/master/samples/core/get_started/custom_estimator.py\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@dwangnuro This question is better asked on [tenosrflow/model](https://github.com/tensorflow/models/issues) repository for faster resolution. Thanks!\r\n\r\n", "@gadagashwini Why is that? It is the same demo code but became slower after upgrade. And I saw my other code became 50% more slower after upgrade. Is it a regression on tensorflow?", "@dwangnuro As this is more related to TF models, please post this in [TF model](https://github.com/tensorflow/models/issues) repo. Thanks!", "@gadagashwini I think there are some misunderstanding. In general, I noticed some of my model become slower after upgrade to tf 1.14. And the model in this issue is just an example that shows tf 1.14 is slower than tf 1.13. It is a performance issue. It isn't the model in the TF model become slower.", "@dwangnuro If some of your custom model became slower after upgrading to 1.14, might be regression issue in 1.14 version. Will take a look. Thanks! ", "@dwangnuro I see that time taken by both is almost similar. (0.001 sec difference in last step). This doesn't look like a regression issue. Perhaps you can use profiling tool and investigate which TF operation consumes longer time than usual to execute. This will help us to troubleshoot quicker.  We will reopen the issue when new information is available. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30967\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30967\">No</a>\n", "On a different RNN model that I'm working on. The training speed is 50% slower. I tried the profiling tool but TF 1.14 and TF 1.13 generates very different profiling result.\r\n1. TF 1.14 put data ops into the graph so there are a lot of profiling result for them which are not in the TF 1.13 profiling graph. I cannot tell if it is cause for the slowness.\r\n2. In TF 1.13, there's only one CPU compute section. In TF 1.14, I saw two CPU compute section. It seems that every op runs on the GPU also appears in one of the CPU compute section. Is it intended?"]}, {"number": 30966, "title": "Restoring tf.py_funcs used inside of a tf.data.Dataset for use at inference time", "body": "**tensorflow gpu version:** 1.13.1\r\n\r\nI'm currently trying to use the Datasets API at inference time but I am having trouble restoring the model and the iterator/py_func ops. I'm setting up the dataset while training like this and I'm using two py_funcs in my input pipeline:\r\n```python\r\npadding_const_str = tf.constant(\"<PAD>\")\r\npadding_const_int = tf.constant(0, dtype=tf.int64)\r\ndataset_type = tf.placeholder(tf.string, shape=[])\r\ndataset = tf.data.TextLineDataset(dataset_type)\r\ndataset = dataset.map(lambda map_el: tuple(tf.py_func(preprocess, [map_el], [tf.string, tf.string, tf.string, tf.string, tf.int64, tf.int64], name=\"preprocess_py_func\")), num_parallel_calls=16)\r\ndataset = dataset.padded_batch(batch_size=FLAGS.batch_size, padded_shapes=([None],[None],[None],[None],[None],[None]), padding_values=(padding_const_str, padding_const_str, padding_const_str, padding_const_str, padding_const_int, padding_const_int))\r\ndataset = dataset.map(lambda labels, tokens, shapes, chars, seq_lens, tok_len : str_to_id((labels, tokens, shapes, chars, seq_lens, tok_len), labels_str_id_lookup, vocab_str_id_lookup, shape_str_id_lookup, char_str_id_lookup),  num_parallel_calls=16)\r\ndataset = dataset.map(lambda labels, tokens, shapes, padded_chars, seq_len_batch, tok_len : tuple(tf.py_func(postprocess, [labels, tokens, shapes, padded_chars, seq_len_batch, tok_len], [labels.dtype, tokens.dtype, shapes.dtype, padded_chars.dtype, seq_len_batch.dtype, tok_len.dtype, tf.float32, tf.int64, tf.int64, tf.int64], name=\"postprocess_py_func\")), num_parallel_calls=16)\r\ndataset = dataset.prefetch(3)\r\niterator = tf.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)\r\ndataset_init_op = iterator.make_initializer(dataset, name='dataset_init')\r\n```\r\n\r\nI am trying to load the model and iterator at inference time by doing this, however I'm running into an issue. \r\n```python\r\nwith tf.Graph().as_default():\r\n\r\n    saver = tf.train.import_meta_graph(FLAGS.load_dir + '/dilated-cnn.tf.meta')\r\n    ckpt = tf.train.get_checkpoint_state(os.path.dirname(FLAGS.load_dir + '/checkpoint'))\r\n    \r\n    with tf.Session() as sess:\r\n        saver.restore(sess, ckpt.model_checkpoint_path)\r\n\r\n        graph = tf.get_default_graph()\r\n        dataset_init_op = graph.get_operation_by_name('dataset_init')\r\n        dataset_type = graph.get_tensor_by_name('Placeholder:0')\r\n        predictions = graph.get_tensor_by_name('predictions/ArgMax:0')\r\n\r\n        sess.run(dataset_init_op, feed_dict={dataset_type : \"sample_text.txt\"})\r\n        print(sess.run(predictions))\r\n```\r\n\r\nI keep getting an error saying: \r\n```\r\nValueError: callback pyfunc_0 is not found\r\n\t [[{{node PyFunc}}]]\r\n\t [[{{node IteratorGetNext}}]]\r\n\t [[{{node IteratorGetNext}}]]\r\n```\r\n\r\nI understand this is happening because py_funcs are inherently not stored on the computation graph as they are simply pure Python functions. However, even after looking at this [post](https://stackoverflow.com/questions/43644506/setting-up-py-func-op-after-importing-graphdef-in-tensorflow) and creating two py_func skeleton functions, I'm still getting a similar error. \r\n\r\nMy question is, does Tensorflow support using the Datasets API at inference time (without using feed dict i.e feeding in the tensors directly into the model without also recreating the model). I'm trying to do it using iterators and a modular dataset but I can't get around this py_func issue. Is there a way to properly restore an iterator that uses py_funcs at inference time or should I focus on converting all my preprocessing to native TF ops (which seems to be taking longer than the py_func version weirdly)?", "comments": ["@Alaska47 ,\r\nIn order to expedite the trouble-shooting process, please provide complete code snippet to reproduce the issue reported here. Thanks!", "Like I said, it occurs when I'm trying to get the value of my output tensor. Specifically with this code: \r\n```python\r\n\r\ndef preprocess(map_el):\r\n    # this function takes in a list of strings\r\n    lines = re.findall(r\"((?:\\-{2,}|\\.{2,}|(?:\\.\\s){2,}\\.)|(?=[^\\(\\\"\\`{\\[:;&\\#\\*@\\)}\\]\\-,])\\S+?(?=\\s|$|(?:[?!)\\\";}\\]\\*:@\\'\\({\\[])|(?:\\-{2,}|\\.{2,}|(?:\\.\\s){2,}\\.)|(?=$|\\s|(?:[?!)\\\";}\\]\\*:@\\'\\({\\[])|(?:\\-{2,}|\\.{2,}|(?:\\.\\s){2,}\\.)))|\\S)\", map_el[0].decode(\"utf-8\"))\r\n    sent_len = len(lines)\r\n    max_word_len = max(map(len, [word for word in lines]))\r\n    max_len_with_pad = 2 + sent_len\r\n\r\n    tokens = np.empty(max_len_with_pad, dtype=\"S\" + str(max_word_len))\r\n    shapes = np.empty(max_len_with_pad, dtype=\"S5\")\r\n    chars = np.empty(max_len_with_pad*max_word_len, dtype=\"S5\")\r\n    sent_lens = np.empty(1, dtype=np.int64)\r\n    tok_lens = np.empty(max_len_with_pad, dtype=np.int64)\r\n    sent_lens[0] = sent_len\r\n\r\n    pad_width = 1\r\n\r\n    tokens[:pad_width] = \"<PAD>\"\r\n    shapes[:pad_width] = \"<PAD>\"\r\n    chars[:pad_width] = \"<PAD>\"\r\n    tok_lens[:pad_width] = 1\r\n\r\n    char_start = pad_width\r\n    idx = pad_width\r\n\r\n    def shape(string):\r\n        if all(c.isupper() for c in string):\r\n            return \"AA\"\r\n        if string[0].isupper():\r\n            return \"Aa\"\r\n        if any(c for c in string if c.isupper()):\r\n            return \"aAa\"\r\n        else:\r\n            return \"a\"\r\n\r\n    for token_str in lines:\r\n        token_shape = shape(token_str)\r\n        token_str_normalized = re.sub(\"\\d\", \"0\", token_str)\r\n\r\n        tok_lens[idx] = len(token_str)\r\n        tokens[idx] = token_str_normalized\r\n        shapes[idx] = token_shape\r\n\r\n        chars[char_start:char_start+tok_lens[idx]] = [char for char in token_str]\r\n\r\n        char_start += tok_lens[idx]\r\n        idx += 1\r\n\r\n    tokens[idx:idx + pad_width] = \"<PAD>\"\r\n    shapes[idx:idx + pad_width] = \"<PAD>\"\r\n    chars[char_start:char_start + pad_width] = \"<PAD>\"\r\n    char_start += pad_width\r\n    tok_lens[idx:idx + pad_width] = 1\r\n    idx += pad_width\r\n\r\n    padded_len = 1*(len(sent_lens)+1)*pad_width+sum(sent_lens)\r\n    tokens = tokens[:padded_len]\r\n    shapes = shapes[:padded_len]\r\n    chars = chars[:sum(tok_lens)]\r\n\r\n    labels = None\r\n    if(len(map_el) == 1):\r\n        labels = np.array([\"O\"] * sent_len)\r\n    else:\r\n        labels = np.array((\"<PAD> \" + map_el[1].decode('utf-8') + \" <PAD>\").split(\" \"))\r\n    return labels, tokens, shapes, chars, sent_lens, tok_lens\r\n\r\ndef postprocess(labels, tokens, shapes, chars, seq_lens, tok_len):\r\n    # this function takes in input vectors and returns more input vectors\r\n    batch_size, batch_seq_len = tokens.shape\r\n    mask_batch = np.zeros((batch_size, batch_seq_len), dtype=np.float32)\r\n    actual_seq_lens = np.add(np.sum(seq_lens, axis=1), 2)\r\n\r\n    for i, seq_len in enumerate(actual_seq_lens):\r\n        mask_batch[i, :seq_len] = 1\r\n\r\n    char_lens = np.sum(tok_len, axis=1)\r\n    max_char_len = np.max(tok_len)\r\n    padded_chars = np.zeros((batch_size, max_char_len * batch_seq_len), dtype=np.int64)\r\n    for b in range(batch_size):\r\n        char_indices = [item for sublist in [range(i * max_char_len, i * max_char_len + d) for i, d in\r\n                                             enumerate(tok_len[b])] for item in sublist]\r\n        padded_chars[b, char_indices] = chars[b][:char_lens[b]]\r\n\r\n    # tf.int64, tf.int64, tf.int64, tf.int64, tf.int64, tf.int64, tf.int64, tf.int32, tf.int32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32\r\n    return labels, tokens, shapes, padded_chars, seq_lens, tok_len, mask_batch, batch_size, batch_seq_len, max_char_len\r\n\r\nwith tf.Graph().as_default():\r\n\r\n    saver = tf.train.import_meta_graph(FLAGS.load_dir + '/dilated-cnn.tf.meta')\r\n    ckpt = tf.train.get_checkpoint_state(os.path.dirname(FLAGS.load_dir + '/checkpoint'))\r\n    \r\n    # Even with the py_funcs redefined, I still get the same error\r\n    # py_func1 = lambda map_el: tuple(tf.py_func(preprocess, [map_el], [tf.string, tf.string, tf.string, tf.string, tf.int64, tf.int64], name=\"preprocess_py_func\"))\r\n    # py_func2 = lambda labels, tokens, shapes, padded_chars, seq_len_batch, tok_len : tuple(tf.py_func(postprocess, [labels, tokens, shapes, padded_chars, seq_len_batch, tok_len], [labels.dtype, tokens.dtype, shapes.dtype, padded_chars.dtype, seq_len_batch.dtype, tok_len.dtype, tf.float32, tf.int64, tf.int64, tf.int64], name=\"postprocess_py_func\"))\r\n\r\n    with tf.Session() as sess:\r\n        saver.restore(sess, ckpt.model_checkpoint_path)\r\n\r\n        graph = tf.get_default_graph()\r\n        dataset_init_op = graph.get_operation_by_name('dataset_init')\r\n        dataset_type = graph.get_tensor_by_name('Placeholder:0')\r\n        predictions = graph.get_tensor_by_name('predictions/ArgMax:0')\r\n\r\n        sess.run(dataset_init_op, feed_dict={dataset_type : \"sample_text.txt\"})\r\n\r\n         # ERROR OCCURS HERE\r\n        print(sess.run(predictions))\r\n```\r\n\r\n\r\n", "@Alaska47 ,\r\nWhen tried executing the code given `NameError: name 'FLAGS' is not defined` was faced.Can you provide minimal required executable code?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 30965, "title": "LITE: Interpreter wrapper without libtensorflow_framework.so.2 dependency", "body": "One should be able to use tensorflow lite python wrapper without having to import the entire libtensorflow (really costly on ARM boards such as raspberry), and use only the libtensorflow-lite as dependency.\r\n\r\n", "comments": ["Was able to generate tflite_runtime without everything by using the following commands:\r\n\r\n```\r\nsudo apt-get install -y swig curl zlib1g-dev zlib1g-dev zlib1g-dev zlib1g-dev\r\npip3 install --upgrade wheel setuptools\r\ncd\r\ncd esos_data\r\ngit clone -b master --depth=1 http://github.com/tensorflow/tensorflow\r\ncd tensorflow\r\n./tensorflow/lite/tools/make/download_dependencies.sh\r\nsed -i 's/PYTHON=.*/PYTHON=python3/' ./tensorflow/lite/tools/pip_package/build_pip_package.sh\r\necho '${PYTHON} setup.py install' >> ./tensorflow/lite/tools/pip_package/build_pip_package.sh\r\n./tensorflow/lite/tools/pip_package/build_pip_package.sh\r\n\r\n```"]}, {"number": 30964, "title": "Redefinition of \"__name__\" when importing keras callbacks or optimizers", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 2.7.15+\r\n- CUDA/cuDNN version: 10.0\r\n\r\n**Describe the current behavior**\r\nWhen importing all keras callbacks or optimizers via `from tensorflow.keras.callbacks import *` or `from tensorflow.keras.optimizers import *`, the variable `__name__` gets redefined to `tensorflow.keras.callbacks` and `tensorflow.keras.optimizers`.\r\n\r\n**Describe the expected behavior**\r\nVariable `__main__` should keep its value `\"__main__\"` so it can be used to determine if the python code is imported as a module/library or the entry point of a script.\r\n\r\n**Code to reproduce the issue**\r\n```Python\r\nfrom tensorflow.keras.callbacks import *\r\nprint(__name__)\r\n```\r\n\r\n**Other info / logs**\r\nOnly occurs with `tensorflow.keras`, not with the standalone Keras.", "comments": ["I am able to reproduce the issue with Tenosrflow 1.14.0 on Colab. Take a look at Colab gist [here](https://colab.research.google.com/drive/1et_k56o8CMt8lWEmwjS6imZKtudiAc1J). Thanks!", "I don't have access to the gist that you are referring to.\r\nI can reproduce this on a virtual Ubuntu 18.04 machine with Python 2 and 3, and on a CentOS 7 server with Python 2.", "This is fixed with tf-nightly version '1.15.0-dev20190731'\r\n```python\r\nfrom tensorflow.keras.optimizers import *\r\nprint(__name__)\r\n__main__\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30964\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30964\">No</a>\n"]}, {"number": 30963, "title": "update self_check.py", "body": "fixes #30935", "comments": ["@mrry  thank you , can we merge this as well please ?", "Looks good to me. Is there anything else I need to do now that it's approved?", "This change is against r2.0 branch , can we merge this ?\r\ncc @mihaimaruseac @bananabowl ", "I think this needs to go on master first and maybe then cherry-picked. Adding @goldiegadde and @gunan", "2.0 will be manually fast forwarded anyway.\r\nPlease make the change to master."]}, {"number": 30962, "title": " [INTEL_MKL] Updated the test analyzer_cli_test.py to be compatible with recent changes in the graph rewrite logic for MKL-DNN support, which changes the name of and attributes of some ops, such as MatMul. etc.", "body": "\u2026 the node name and attribute change in graphrewrite path  in MKL path", "comments": []}, {"number": 30961, "title": "Inconsistent behavior with and without distributed scope", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Darwin Kernel Version 16.7.0\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): installed from pypi\r\n- TensorFlow version (use command below):  v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nTraining on mac os, with CPU only, with and without distributed scope for mnist example. The loss and accuracy is incorrect with distributed scope turned off\r\n\r\nWith distributed scope on:\r\nTrain on 550.0 steps, validate on 50 steps\r\nEpoch 1/5\r\n550/550 [==============================] - 6s 10ms/step - loss: 4.5770 - accuracy: 0.7565 - val_loss: 1.0736 - val_accuracy: 0.7990\r\nEpoch 2/5\r\n550/550 [==============================] - 2s 3ms/step - loss: 0.7952 - accuracy: 0.8072 - val_loss: 0.6584 - val_accuracy: 0.8282\r\nEpoch 3/5\r\n550/550 [==============================] - 2s 3ms/step - loss: 0.5272 - accuracy: 0.8367 - val_loss: 0.5347 - val_accuracy: 0.8438\r\nEpoch 4/5\r\n550/550 [==============================] - 2s 3ms/step - loss: 0.4477 - accuracy: 0.8501 - val_loss: 0.4830 - val_accuracy: 0.8572\r\nEpoch 5/5\r\n550/550 [==============================] - 2s 3ms/step - loss: 0.4037 - accuracy: 0.8591 - val_loss: 0.4780 - val_accuracy: 0.8456\r\n\r\nWithout distributed scope:\r\nEpoch 1/5\r\n550/550 [==============================] - 3s 6ms/step - loss: 14.5062 - accuracy: 0.1000 - val_loss: 14.4869 - val_accuracy: 0.1012\r\nEpoch 2/5\r\n550/550 [==============================] - 2s 4ms/step - loss: 14.5080 - accuracy: 0.0999 - val_loss: 14.4869 - val_accuracy: 0.1012\r\nEpoch 3/5\r\n550/550 [==============================] - 2s 4ms/step - loss: 14.5080 - accuracy: 0.0999 - val_loss: 14.4869 - val_accuracy: 0.1012\r\nEpoch 4/5\r\n550/550 [==============================] - 2s 4ms/step - loss: 14.5080 - accuracy: 0.0999 - val_loss: 14.4869 - val_accuracy: 0.1012\r\nEpoch 5/5\r\n550/550 [==============================] - 2s 4ms/step - loss: 14.5080 - accuracy: 0.0999 - val_loss: 14.4869 - val_accuracy: 0.1012\r\n\r\n**Describe the expected behavior**\r\nThe training is expected to work correctly even with distributed scope turned off. This was working correctly in the tf2.0 alpha release but is an issue in the tf2.0 beta1 release. \r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nimport contextlib\r\nimport sys\r\n\r\ndistributed=False\r\nif len(sys.argv) > 1 and sys.argv[1] == \"distributed\":\r\n    distributed=True\r\n    print(\"asdf enabled distributed trainer\")\r\n\r\nclass NoOpScope:\r\n    def scope(self):\r\n        return contextlib.suppress()\r\n\r\ndistribution_strategy = tf.distribute.MirroredStrategy() if distributed else NoOpScope()\r\n\r\nwith distribution_strategy.scope():\r\n    model = tf.keras.models.Sequential()\r\n    model.add(tf.keras.layers.Flatten(input_shape=[28, 28]))\r\n    model.add(tf.keras.layers.Dense(300, activation=\"relu\"))\r\n    model.add(tf.keras.layers.Dense(100, activation=\"relu\"))\r\n    model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\r\n    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\r\n\r\n(x_train_full, y_train_full), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\r\nx_dev, x_train = x_train_full[:5000], x_train_full[5000:]\r\ny_dev, y_train = y_train_full[:5000], y_train_full[5000:]\r\ntrain_data = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(55000).repeat().batch(100)\r\ndev_data = tf.data.Dataset.from_tensor_slices((x_dev, y_dev)).batch(100)\r\n\r\nmodel.fit(train_data, \r\n          epochs=5,\r\n          steps_per_epoch=55000/100,\r\n          validation_data=dev_data)\r\n```\r\n\r\nPlease copy to a script and run as:\r\nWithout distributed scope: python3 script.py\r\nWith distributed scope : python3 script.py distributed\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "comments": ["@anirudhraju Looks like the code is incomplete. Please provide the complete code to reproduce the issue. Thanks!", "Reformatted the above code blurb with python markdown for readability. Please copy to a script and run as:\r\nWithout distributed scope: python3 script.py\r\nWith distributed scope : python3 script.py distributed", "I am able to reproduce the issue on Colab with Tensorflow version 2.0.0.beta1. Please find the [gist](https://colab.research.google.com/drive/1NsqVwxKeBy6zZOA4KCC0etgzy7NUZYy9) of colab. Thanks!", "I believe you are missing this one step in the input pipeline:\r\n`x_train_full = x_train_full / 255.0` \r\nAfter adding this, the code without strategy seems to work fine too. \r\n\r\nSee this tutorial for a very similar example: \r\nhttps://www.tensorflow.org/beta/tutorials/keras/basic_classification\r\n\r\nI don't understand though why\r\n- with strategy it seems to work without needing this extra step ( i will look into that) \r\n- why it was working for you before beta. (are you sure you didn't have that division at the time?) \r\n\r\n", ">why it was working for you before beta. (are you sure you didn't have that division at the time?)\r\n\r\nYes, this works ok in TF2.0 alpha0 version without the distributed scope, and without the division by 255 operation. \r\nIn beta, the behavior is inconsistent with and without the distributed scope", "@qlzh727  Any update on this issue? Do we know why this behavior is inconsistent between tf2.0 alpha and beta?", "Thanks for the notice. let me take a look today.", "I wasn't able to reproduce the issue with the snippet above on the latest head. The issue seems to be already fixed by the new execution path.", "Closing this issue due to the lack of activity. Feel free to reopen it if you still see the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30961\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30961\">No</a>\n"]}, {"number": 30960, "title": "Make Github templates more intuitive", "body": "Hi,\r\nFrequently I stumble upon issues on the Tensorflow githubpage which do not fill out the template properly. I was curious whether it's possible to make the filling in of the template more intuitive. \r\n\r\nFor instance I think it would be nice if there were field to fill in the asked questions right after each question.\r\nFurthermore it would be nice to be required to at least fill some part of the form before being able to submit it.\r\n\r\nWhat do you think? Is it possible to change the template on github in such a way?\r\nI believe that this would improve the speed of resolving issues.", "comments": ["@lufol Thanks for your suggestion. We have an understanding here. Unfortunately at this point of time, GitHub interface has limitation where we cannot enforce users to fill all information asked by the template before posting their issue. However we do highly recommend them to do so.  We strive to help TensorFlow community to the best of our efforts and we revert to the issues wherever we can provide suitable resolution. Thanks!"]}, {"number": 30959, "title": "What can replace 'initializer = tf.contrib.layers.xavier_initializer_conv2d()' in tensorflow 2.0?", "body": "Now that tensorflow has no attribute 'contrib'. Thanks a lot!", "comments": ["You can find the Glorot initializer which is a different name for the Xavier initializer here: `tf.keras.initializers.GlorotNormal`\r\n\r\nI hope this helps!", "@lufol It really works. Thank you!", "Can we close this issue as it was resolved.Thanks!", "I am closing this issue as it was resolved.Thanks!", "tf.contrib.layers.xavier_initializer_conv2d not working in tf2. Resolution?\r\n"]}, {"number": 30958, "title": "fit and save methods fail with a subclassed model using a SequenceFeatures layer with sequence_numeric_column", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6\r\n- TensorFlow installed from (source or binary): from pip install\r\n- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1\r\n- Python version: v3.6.7:6ec5cf24b7, Oct 20 2018, 03:02:14\r\n\r\n**Describe the current behavior**\r\n\r\n`tf.keras.Model.fit` and `tf.keras.Model.save` method fail and return an error when applied to a model created by subclassing the `tf.keras.Model` class which uses a `SequenceFeatures` layer created by calling `SequenceFeatures.__init__` on a feature colmun created with `tf.feature_column.sequence_numeric_column`. Note that the issue does not occur when the `SequenceFeatures` layer is created by calling `SequenceFeatures.__init__` on a feature colmun created with `tf.feature_column.embedding_column`, as my code to reproduce the issue shows below.\r\n\r\nNote also that the `call` method of the subclassed model using `sequence_numeric_column` works fine, in eager execution or in graph mode, see also the example code.\r\n\r\nThe error message I get with `fit` and `export_saved_model` is \r\n```\r\nValueError: Cannot convert a partially known TensorShape to a Tensor: (None, 9)\r\n```\r\nHence it seems to be occuring because I did not explicitely set the batch size. But the last batch usually have a size less than the batch size so I am not interested in setting it to a fixed value before training. As a side note, I did not find how to do it anyway, my experiments with passing a `batch_input_shape` keyword argument to the `__init__` method of `SequenceFeatures` have failed, as if the argument was always ignored, and this combines poorly with nested structures of inputs like the one I have here with a dictionary of tensors.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe case when I use `embedding_column`, which is the case when the functions do not fail, suggests that the case with `sequence_numeric_column` should behave similarly and should not make the functions fail. It does not make sense to me that not knowing the batch size is relevant in one case and not relevant in the other. It may be due to the use, somewhere, of `tf.Tensor.shape` (which is static and returns the `None`) instead of `tf.shape` (which is dynamic). Additionally, it seems a standard practice to let the batch dimension to `None` (for the reason given above) so this should be supported.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import Model\r\nfrom tensorflow.python.feature_column.feature_column_v2 import EmbeddingColumn\r\nfrom tensorflow.keras.layers import LSTM, Dense\r\nfrom tensorflow.keras.experimental import SequenceFeatures\r\n\r\nprint('Using Tensorflow version {} (git version {})'.format(tf.version.VERSION, tf.version.GIT_VERSION))\r\n\r\nclass Toy(Model):\r\n\r\n    def __init__(self,\r\n                 fc_list,\r\n                 nb_features,\r\n                 name='toy_model',\r\n                 **kwargs):\r\n        super(Toy, self).__init__(name=name, **kwargs)\r\n        self.fc_list = fc_list\r\n        self.dict_layers = {}\r\n        for fc in self.fc_list:\r\n            fc_name = fc.name\r\n            self.dict_layers[fc_name] = SequenceFeatures(fc)\r\n        self.lstm = LSTM(64, return_sequences=False)\r\n        self.output_layer = Dense(nb_features, activation='softmax')\r\n        \r\n    def call(self, inputs, training=None):\r\n        dict_apply_layers = {}\r\n        for fc in self.fc_list:\r\n            fc_name = fc.name\r\n            if type(fc) == EmbeddingColumn:\r\n                dict_apply_layers[fc_name] = self.dict_layers[fc_name](inputs)[0]\r\n            else:\r\n                # we need to convert inputs[fc_name] to a sparse tensor, see https://github.com/tensorflow/tensorflow/issues/29879\r\n                zero = tf.constant(0, dtype=tf.float32)\r\n                dense = inputs[fc_name]\r\n                indices = tf.where(tf.not_equal(dense, zero))\r\n                values = tf.gather_nd(dense, indices)\r\n                sparse = tf.SparseTensor(indices, values, dense.shape)\r\n                dict_apply_layers[fc_name] = self.dict_layers[fc_name]({fc_name: sparse})[0]\r\n        x = tf.concat([v for _, v in dict_apply_layers.items()], axis=-1)\r\n        x = self.lstm(x)\r\n        x = self.output_layer(x)\r\n        return x\r\n    \r\n#Dataset Parameters\r\nnb_batches = 15\r\nbatch_size = 24\r\nsequence_length = 9\r\nnb_features = 10\r\n\r\n#Dataset construction\r\ninput_dense = tf.constant(np.random.normal(0, 1, (nb_batches, batch_size, sequence_length)))\r\ninput_dense = tf.cast(input_dense, dtype=tf.float32)\r\ninput_cat = tf.constant(np.random.randint(0, nb_features, (nb_batches, batch_size, sequence_length)))\r\ninput_dict = {'dense': input_dense, 'categorical': input_cat}\r\ninput_dataset = tf.data.Dataset.from_tensor_slices(input_dict)\r\n\r\ntarget_cat = tf.constant(np.random.randint(0, high=nb_features, size=(nb_batches, batch_size)))\r\ntarget_dataset = tf.data.Dataset.from_tensor_slices(target_cat)\r\n\r\ntraining_dataset = tf.data.Dataset.zip((input_dataset, target_dataset))\r\n\r\n#Feature columns definition\r\nfc_dense = tf.feature_column.sequence_numeric_column('dense')\r\nfc_cat = tf.feature_column.sequence_categorical_column_with_identity('categorical', nb_features)\r\nembedding_units = 16\r\nfc_cat = tf.feature_column.embedding_column(fc_cat, embedding_units)\r\n    \r\n#Model Parameters\r\nrnn_units = 64\r\n\r\n#Training Parameters\r\nepochs = 2\r\n\r\n#Try the model with the sequence_numeric_column feature column\r\nmodel = Toy([fc_dense,], nb_features)\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['sparse_categorical_accuracy'])\r\ntry:\r\n    model.fit(x=training_dataset, epochs=epochs)\r\n    model.evaluate(x=training_dataset)\r\nexcept ValueError as e:\r\n    print(e)\r\ntry:\r\n    path = 'tmp/test'\r\n    model.save(path)\r\n    new_model = tf.keras.models.load_model(path)\r\n    new_model.evaluate(x=training_dataset)\r\nexcept ValueError as e:\r\n    print(e)\r\n    \r\n#Try the call method with the sequence_numeric_column feature column\r\nmodel = Toy([fc_dense], nb_features)\r\ntry:\r\n    for x, y in training_dataset:\r\n        model(x)\r\n    print('call worked')\r\nexcept ValueError as e:\r\n    print(e)\r\n\r\n#Try the call method in graph mode with the sequence_numeric_column feature column\r\n@tf.function\r\ndef call_graph(model, inputs):\r\n    return model(inputs)\r\nmodel = Toy([fc_dense], nb_features)\r\ntry:\r\n    for x, y in training_dataset:\r\n        call_graph(model, x)\r\n    print('call in graph mode worked')\r\nexcept ValueError as e:\r\n    print(e)\r\n    \r\n#Try the model with the embedding_column feature column\r\nmodel = Toy([fc_cat,], nb_features)\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['sparse_categorical_accuracy'])\r\ntry:\r\n    model.fit(x=training_dataset, epochs=epochs)\r\n    model.evaluate(x=training_dataset)\r\nexcept ValueError as e:\r\n    print(e)\r\ntry:\r\n    path = 'tmp/test'\r\n    model.save(path)\r\n    new_model = tf.keras.models.load_model(path)\r\n    new_model.evaluate(x=training_dataset)\r\nexcept ValueError as e:\r\n    print(e)\r\n```\r\n\r\nNote that trying to build the model first, by inserting the lines\r\n```\r\nfor x, y in training_dataset.take(1):\r\n    model(x)\r\n```\r\njust before calling `fit` does not change anything.", "comments": ["I edited my message to talk about the `save` method instead of `tf.keras.experimental.export_saved_model`. `export_saved_model` has the same issue but has additional problems with subclassed models making it useless even in the case where `fit` and `save` do not fail (that is, the case with `embedding_column`). Indeed, `export_saved_model` do not save saved_model.json because the subclassed model do not  have a `get_config` method, see the following warning when calling `export_saved_model`:\r\n```\r\nW0723 19:17:28.631285 140736192209792 saved_model.py:171] Skipped saving model JSON, subclassed model does not have get_config() defined.\r\n```\r\nHence we cannot rebuild the model with `tf.keras.experimental.load_from_saved_model` and we get this error instead:\r\n```\r\nNotFoundError: tmp/test/assets/saved_model.json; No such file or directory\r\n```", "Was able to reproduce the issue on Colab with Tensorflow version 2.0.0.beta1. Please find the gist [here](https://colab.research.google.com/drive/17M78uFxsUg82xjqOYPdMb4Hm2TqB-J9J). Thanks!", "Sorry, can you clarify the current code and error? The example above is fairly complicated; can you reduce to the minimal example required to reproduce the error?", "There is no need to clarify my code, because I have found the cause of the issue. It was my use of the static `dense.shape` instead of the dynamic `tf.shape(dense)`.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30958\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30958\">No</a>\n"]}, {"number": 30957, "title": "Segfault when passing empty Tensor to cholesky_solve", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 (Linux 93fd36b9ffb5 4.9.125-linuxkit #1 SMP Fri Sep 7 08:20:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0\r\n- Python version: Python 3.6.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nWhen passing an empty Tensor as the second argument to the `tf.cholesky_solve` function a segfault is encountered.\r\n\r\nValgrind reports the following:\r\n\r\n```\r\n==8399== Process terminating with default action of signal 11 (SIGSEGV)\r\n==8399==  Access not within mapped region at address 0x0\r\n==8399==    at 0x1896E2B0: tensorflow::MatrixTriangularSolveOp<double>::ComputeMatrix(tensorflow::OpKernelContext*, absl::InlinedVector<Eigen::Map<Eigen::Matrix<double, -1, -1, 1, -1, -1> const, 0, Eigen::Stride<0, 0> >, 4ul, std::allocator<Eigen::Map<Eigen::Matrix<double, -1, -1, 1, -1, -1> const, 0, Eigen::Stride<0, 0> > > > const&, absl::InlinedVector<Eigen::Map<Eigen::Matrix<double, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, 4ul, std::allocator<Eigen::Map<Eigen::Matrix<double, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > > >*) (in /home/nferguson/venv_cholesky_segfault/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n==8399==    by 0x18A4A661: tensorflow::LinearAlgebraOp<double>::ComputeTensorSlice(tensorflow::OpKernelContext*, long long, absl::InlinedVector<tensorflow::Tensor const*, 4ul, std::allocator<tensorflow::Tensor const*> > const&, absl::InlinedVector<tensorflow::TensorShape, 4ul, std::allocator<tensorflow::TensorShape> > const&, absl::InlinedVector<tensorflow::Tensor*, 4ul, std::allocator<tensorflow::Tensor*> > const&, absl::InlinedVector<tensorflow::TensorShape, 4ul, std::allocator<tensorflow::TensorShape> > const&) (in /home/nferguson/venv_cholesky_segfault/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n==8399==    by 0x18A4A996: std::_Function_handler<void (long long, long long), tensorflow::LinearAlgebraOp<double>::Compute(tensorflow::OpKernelContext*)::{lambda(long long, long long)#1}>::_M_invoke(std::_Any_data const&, long long, long long) (in /home/nferguson/venv_cholesky_segfault/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n==8399==    by 0x214615A3: tensorflow::thread::ThreadPool::Impl::ParallelFor(long long, long long, std::function<void (long long, long long)>) (in /home/nferguson/venv_cholesky_segfault/lib/python3.6/site-packages/tensorflow/libtensorflow_framework.so.1)\r\n==8399==    by 0x2146181E: tensorflow::thread::ThreadPool::ParallelFor(long long, long long, std::function<void (long long, long long)>) (in /home/nferguson/venv_cholesky_segfault/lib/python3.6/site-packages/tensorflow/libtensorflow_framework.so.1)\r\n==8399==    by 0x212C6E80: tensorflow::Shard(int, tensorflow::thread::ThreadPool*, long long, long long, std::function<void (long long, long long)>) (in /home/nferguson/venv_cholesky_segfault/lib/python3.6/site-packages/tensorflow/libtensorflow_framework.so.1)\r\n==8399==    by 0x18A46E44: tensorflow::LinearAlgebraOp<double>::Compute(tensorflow::OpKernelContext*) (in /home/nferguson/venv_cholesky_segfault/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n==8399==    by 0x213C1D93: tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) (in /home/nferguson/venv_cholesky_segfault/lib/python3.6/site-packages/tensorflow/libtensorflow_framework.so.1)\r\n==8399==    by 0x213C20E9: std::_Function_handler<void (), tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(absl::InlinedVector<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, 8ul, std::allocator<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode> > const&, tensorflow::(anonymous namespace)::ExecutorState::TaggedNodeReadyQueue*)::{lambda()#1}>::_M_invoke(std::_Any_data const&) (in /home/nferguson/venv_cholesky_segfault/lib/python3.6/site-packages/tensorflow/libtensorflow_framework.so.1)\r\n==8399==    by 0x2145F6D3: Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) (in /home/nferguson/venv_cholesky_segfault/lib/python3.6/site-packages/tensorflow/libtensorflow_framework.so.1)\r\n==8399==    by 0x2145E543: std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) (in /home/nferguson/venv_cholesky_segfault/lib/python3.6/site-packages/tensorflow/libtensorflow_framework.so.1)\r\n==8399==    by 0x2212D9DF: ??? (in /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.25)\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nAlthough we clearly shouldn't be passing an empty Tensor to this method, I would expect an exception to occur (in Python) explaining that the input is invalid, rather than a segfault.\r\n\r\n**Code to reproduce the issue**\r\n\r\nRun the following code:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef go():\r\n\r\n    with tf.Graph().as_default():\r\n\r\n        with tf.Session() as sess:\r\n\r\n            X = tf.placeholder(shape=None, dtype=tf.float64)\r\n\r\n            rhs = X[1:]\r\n            rhs = tf.reshape(rhs, shape=(-1, 1))\r\n\r\n            chol = tf.ones(shape=(1, 1), dtype=tf.float64)\r\n\r\n            iP0 = tf.cholesky_solve(chol, rhs)\r\n\r\n            feed_dict = {X: np.asarray([1.])}\r\n            print(sess.run(iP0, feed_dict=feed_dict))\r\n\r\n            print('SUCCESS!')\r\n\r\n\r\ngo()\r\n```\r\n\r\nHere the `rhs` argument to `cholesky_solve` ends up as an empty Tensor (it has a shape of (0, 1)), which appears to cause the segfault. \r\n", "comments": ["I am able to reproduce the issue.Thanks!", "@nfergu There was some updates related to this issue. Can you please check whether the issue still persists. Please use `tf-nightly` and let us know the status. Thanks!", "I think it was resolved. Closing due to lack of recent activity. Please open new ticket if you see similar issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30957\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30957\">No</a>\n"]}, {"number": 30956, "title": "Segfault in custom op with TensorFlow 1.14, Python 3.6 and GCC 5.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 (Linux 93fd36b9ffb5 4.9.125-linuxkit #1 SMP Fri Sep 7 08:20:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0\r\n- Python version: Python 3.6.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): g++-5 (Ubuntu 5.5.0-12ubuntu1) 5.5.0 20171010\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nWhen compiling the example custom op from https://www.tensorflow.org/guide/extend/op with GCC 5, a segfault is encountered when running with TensorFlow 1.14 and Python 3.6 (on Linux).\r\n\r\nThe same code works fine when running with either Python 3.7 (instead of Python 3.6) or compiling with GCC 4.8 (instead of GCC 5). The code also works fine on Mac.\r\n\r\nValgrind shows the following:\r\n\r\n```\r\n==8208== Process terminating with default action of signal 11 (SIGSEGV)\r\n==8208==  Access not within mapped region at address 0x88\r\n==8208==    at 0x40292BC0: {lambda(tensorflow::shape_inference::InferenceContext*)#1}::_FUN(tensorflow::shape_inference::InferenceContext*) (in /tmp/tmp1vwpnrsl)\r\n==8208==    by 0x40292D0F: std::_Function_handler<tensorflow::Status (tensorflow::shape_inference::InferenceContext*), tensorflow::Status (*)(tensorflow::shape_inference::InferenceContext*)>::_M_invoke(std::_Any_data const&, tensorflow::shape_inference::InferenceContext*&&) (in /tmp/tmp1vwpnrsl)\r\n==8208==    by 0x2119ED1C: tensorflow::shape_inference::InferenceContext::Run(std::function<tensorflow::Status (tensorflow::shape_inference::InferenceContext*)> const&) (in /home/nferguson/venv_cholesky_segfault/lib/python3.6/site-packages/tensorflow/libtensorflow_framework.so.1)\r\n==8208==    by 0x1BF7871F: tensorflow::ShapeRefiner::RunShapeFn(tensorflow::Node const*, tensorflow::OpRegistrationData const*, tensorflow::ExtendedInferenceContext*) (in /home/nferguson/venv_cholesky_segfault/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n==8208==    by 0x1BF7A247: tensorflow::ShapeRefiner::AddNode(tensorflow::Node const*) (in /home/nferguson/venv_cholesky_segfault/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n==8208==    by 0x19932F19: TF_FinishOperation (in /home/nferguson/venv_cholesky_segfault/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n==8208==    by 0x179A3DB5: _wrap_TF_FinishOperation (in /home/nferguson/venv_cholesky_segfault/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n==8208==    by 0x4F858C: ??? (in /usr/bin/python3.6)\r\n==8208==    by 0x4F98C6: _PyEval_EvalFrameDefault (in /usr/bin/python3.6)\r\n==8208==    by 0x4F7A27: ??? (in /usr/bin/python3.6)\r\n==8208==    by 0x4F876C: ??? (in /usr/bin/python3.6)\r\n==8208==    by 0x4F98C6: _PyEval_EvalFrameDefault (in /usr/bin/python3.6)\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nMy understanding is that both GCC 5 and Python 3.6 are both supported when building and running custom ops, so the code that uses the op should be able to run fine without segfaulting.\r\n\r\n**Code to reproduce the issue**\r\n\r\nMake a file called `zero_out.cc` with the following contents (as per the example from https://www.tensorflow.org/guide/extend/op):\r\n\r\n```\r\n#include \"tensorflow/core/framework/op.h\"\r\n#include \"tensorflow/core/framework/shape_inference.h\"\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n\r\n\r\nusing namespace tensorflow;\r\n\r\nREGISTER_OP(\"ZeroOut\")\r\n    .Input(\"to_zero: int32\")\r\n    .Output(\"zeroed: int32\")\r\n    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\r\n      c->set_output(0, c->input(0));\r\n      return Status::OK();\r\n    });\r\n\r\nclass ZeroOutOp : public OpKernel {\r\n public:\r\n  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}\r\n\r\n  void Compute(OpKernelContext* context) override {\r\n    // Grab the input tensor\r\n    const Tensor& input_tensor = context->input(0);\r\n    auto input = input_tensor.flat<int32>();\r\n\r\n    // Create an output tensor\r\n    Tensor* output_tensor = NULL;\r\n    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),\r\n                                                     &output_tensor));\r\n    auto output_flat = output_tensor->flat<int32>();\r\n\r\n    // Set all but the first element of the output tensor to 0.\r\n    const int N = input.size();\r\n    for (int i = 1; i < N; i++) {\r\n      output_flat(i) = 0;\r\n    }\r\n\r\n    // Preserve the first input value if possible.\r\n    if (N > 0) output_flat(0) = input(0);\r\n  }\r\n};\r\n\r\nREGISTER_KERNEL_BUILDER(Name(\"ZeroOut\").Device(DEVICE_CPU), ZeroOutOp);\r\n```\r\n\r\nMake another file called `test_zero_out.py` with the following contents: \r\n\r\n```\r\nimport os\r\nfrom tempfile import NamedTemporaryFile\r\n\r\nimport tensorflow as tf\r\nfrom numpy.testing import assert_array_equal\r\nimport numpy as np\r\n\r\nBUILD_COMMAND = 'g++-5 -std=c++11 -shared zero_out.cc -o {0} -fPIC {1} {2} -O2'\r\n\r\n\r\ndef test_zero_out():\r\n\r\n    with NamedTemporaryFile() as temp_file:\r\n\r\n        compile_args = ' '.join(tf.sysconfig.get_compile_flags())\r\n        link_args = ' '.join(tf.sysconfig.get_link_flags()).replace(\r\n            '-l:libtensorflow_framework.1.dylib', '-ltensorflow_framework.1')\r\n\r\n        os.system(BUILD_COMMAND.format(temp_file.name, compile_args, link_args))\r\n\r\n        zero_out_module = tf.load_op_library(temp_file.name)\r\n        with tf.Graph().as_default():\r\n            with tf.Session().as_default():\r\n                result = zero_out_module.zero_out([5, 4, 3, 2, 1])\r\n                assert_array_equal(result.eval(), np.asarray([5, 0, 0, 0, 0]))\r\n                print('SUCCESS')\r\n\r\n\r\ntest_zero_out()\r\n```\r\n\r\nEnsure that GCC 5 is installed, along with Python 3.6 (on Ubuntu 18.04), and execute the following:\r\n\r\n```\r\npython3 test_zero_out.py\r\n```\r\n\r\nThis compiles and runs the custom op. This should cause a segfault.\r\n\r\nEnsure that Python 3.7 is installed and try the same thing:\r\n\r\n```\r\npython3.7 test_zero_out.py\r\n```\r\n\r\nThis should run successfully and print \"SUCCESS\".\r\n\r\nNow create an equivalent test file for GCC 4.8 called `test_zero_out-4.8.py`\r\n\r\n```\r\nimport os\r\nfrom tempfile import NamedTemporaryFile\r\n\r\nimport tensorflow as tf\r\nfrom numpy.testing import assert_array_equal\r\nimport numpy as np\r\n\r\nBUILD_COMMAND = 'g++-4.8 -std=c++11 -shared zero_out.cc -o {0} -fPIC {1} {2} -O2'\r\n\r\n\r\ndef test_zero_out():\r\n\r\n    with NamedTemporaryFile() as temp_file:\r\n\r\n        compile_args = ' '.join(tf.sysconfig.get_compile_flags())\r\n        link_args = ' '.join(tf.sysconfig.get_link_flags()).replace(\r\n            '-l:libtensorflow_framework.1.dylib', '-ltensorflow_framework.1')\r\n\r\n        os.system(BUILD_COMMAND.format(temp_file.name, compile_args, link_args))\r\n\r\n        zero_out_module = tf.load_op_library(temp_file.name)\r\n        with tf.Graph().as_default():\r\n            with tf.Session().as_default():\r\n                result = zero_out_module.zero_out([5, 4, 3, 2, 1])\r\n                assert_array_equal(result.eval(), np.asarray([5, 0, 0, 0, 0]))\r\n                print('SUCCESS')\r\n\r\n\r\ntest_zero_out()\r\n\r\n```\r\n\r\nEnsure GCC 4.8 is installed and run:\r\n\r\n```\r\npython3 test_zero_out-4.8.py\r\n```\r\n\r\nThis will compile and run the same code with GCC 4.8. This should succeed and print \"SUCCESS\".", "comments": ["Unfortunately, at the moment this is due to GCC5.\r\nYou have to use the same toolchains we are providing to build your custom ops, otherwise they will be ABI incompatible with our prebuilt binaries.\r\nFor our new guide to build the custom ops, please see here:\r\nhttps://github.com/tensorflow/custom-op\r\n\r\n@yifeif @lamberta I see that the website still does not point to the new custom op building guide:\r\nhttps://www.tensorflow.org/guide/extend/op#build_the_op_library\r\nWe have to fix that. Maybe \"Building the op library\" section should just point people to the git repo: https://github.com/tensorflow/custom-op", "The link to the custom-op repo was added at the bottom of the page: https://www.tensorflow.org/guide/extend/op#build_a_pip_package_for_your_custom_op\r\n\"Build a pip package for your custom op\" as opposed to source.\r\nHappy to point the *Building the op library* section to this repo, if that's accurate.\r\nThat page is [here in GitHub](https://github.com/tensorflow/docs/blob/master/site/en/guide/extend/op.md) and we also need to figure how much of this comes over for TF2.", "@nfergu,\r\nSorry for the delayed response. Can you please refer the documentation to [Create a New Op](https://www.tensorflow.org/guide/create_op) and the [Github Repository to build a Custom Op](https://github.com/tensorflow/custom-op) and let us know if your issue is resolved? Thanks!", "Hi @nfergu ! Could you  please confirm with above response ?. Also , We see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.6 version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30956\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30956\">No</a>\n"]}, {"number": 30955, "title": "Subclassing Model prevents the computation of intermediate values in graph mode", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nGoogle Colab\r\n- TensorFlow version (use command below):\r\nTensorflow 2.0 beta 1\r\n\r\n**Describe the current behavior**\r\nComputing intermediate values within a subclassed model doesn't work in graph mode.\r\n\r\n**Describe the expected behavior**\r\nIt should most likely work, as it was possible in TensorFlow 1.x.\r\n\r\n**Code to reproduce the issue**\r\nTry it directly in Google Colab. Setting 'use_eager_mode' to True/False switches between the two cases.\r\nhttps://gist.github.com/DeepBlender/6ab324ab3b14552109979a97bf4acb8f", "comments": ["I tried executing the given code on colab and I am able to reproduce the issue with `use_eager_mode=False`. Thanks!", "The code provided snippet while printing intermediate results fails with same message in TF 2.X (eager mode disabled) as well as TF 1.X (eager disabled by default). \r\n```python\r\nAttributeError: Layer conv2d_1 has no inbound nodes.\r\n```\r\nCan you please confirm if you are able to execute this script successfully in  TF 1.X?", "@ymodak sorry for the confusion, I meant to say similar functionality, but that does not seem to be accurate either.\r\nIt has been my understanding that in TensorFlow 2.0 it should be possible to switch between graph and eager mode flawlessly. This is a relatively simple example where it doesn't work.", "Even though, the example I presented has a workaround, a minor change will eliminate this workaround. If you add \"x = x - 0.5\" in \"call\", the code simply doesn't work anymore and there is no way to compute the intermediate results.\r\n\r\nAs this way of using the Model class is presented as pretty much the \"TensorFlow 2.0 way\", this issue is reducing the flexibility quite significantly.\r\n\r\nEdit: I just realized that a workaround is to use Lambda. However, that's not suggested anywhere and nothing in the error message would help me to figure that out.", "@ymodak I'm also having trouble creating subgraph for model using the subclassing API (on tensorflow-gpu==2.0.0-beta1). \r\n\r\nGoogle Colab: https://colab.research.google.com/drive/1kRbgCtF2CIfiK3F75FjDZiI241cfPC-f\r\n\r\nCode below:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nNUM_CLASSES = 10\r\nTARGET_LAYER_NAME = 'target_layer_name'\r\n\r\n# Create the Subclassing API Class\r\nclass SubclassedModel(tf.keras.Model):\r\n  def __init__(self, name='subclassed'):\r\n    super(SubclassedModel, self).__init__(name=name)\r\n    self.conv_1 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')\r\n    self.conv_2 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', name=TARGET_LAYER_NAME)\r\n    self.maxpool_1 = tf.keras.layers.MaxPool2D(pool_size=(2, 2))\r\n\r\n    self.flatten = tf.keras.layers.Flatten()\r\n\r\n    self.dense_1 = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\r\n\r\n  def call(self, inputs, **kwargs):\r\n    x = inputs\r\n    for layer in [self.conv_1, self.conv_2, self.maxpool_1, self.flatten, self.dense_1]:\r\n        x = layer(x)\r\n\r\n    return x\r\n\r\n  def compute_output_shape(self, input_shape):\r\n    shape = tf.TensorShape(input_shape).as_list()\r\n    return tf.TensorShape([shape[0], NUM_CLASSES])\r\n\r\n# Initialize a model using the subclassing API\r\nmodel = SubclassedModel()\r\nmodel(np.random.random((4, 28, 28, 1)).astype('float32'))  # Sample call to build the model\r\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\r\n\r\n# Trying to fit some random data, all goes well\r\ntraining_size = 256\r\nsample_x = np.random.random((training_size, 28, 28, 1)).astype('float32')\r\nsample_y = np.eye(NUM_CLASSES)[np.random.choice(NUM_CLASSES, training_size)]\r\nhistory = model.fit(sample_x, sample_y, epochs=3, verbose=0)\r\n\r\n# Trying to extract a subgraph\r\nsubmodel = tf.keras.Model([model.inputs], [model.get_layer(TARGET_LAYER_NAME).output])\r\nsubmodel.summary()\r\n\r\n# Trying the same thing with sequential API -- it works\r\n\r\nmodel_seq = tf.keras.Sequential([\r\n  tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\r\n  tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', name=TARGET_LAYER_NAME),\r\n  tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\r\n  tf.keras.layers.Flatten(),\r\n  tf.keras.layers.Dense(NUM_CLASSES, activation='softmax'),\r\n])\r\nmodel_seq.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\r\nmodel_seq.fit(sample_x, sample_y, epochs=3, verbose=0)\r\n\r\nsubmodel_seq = tf.keras.Model([model_seq.inputs], [model_seq.get_layer(TARGET_LAYER_NAME).output])\r\nsubmodel_seq.summary()\r\n```\r\n\r\nStacktrace is:\r\n![image](https://user-images.githubusercontent.com/12402673/62143005-921fdb80-b2ef-11e9-9c10-ac7285bf4a2d.png)\r\n\r\nIs there a workaround for this?\r\n\r\nEdit: Error also appears on tf 1.14", "@DeepBlender @RaphaelMeudec \r\n\r\nIn general it's not possible to extract subgraphs of subclassed Models. For those use cases, please use Functional API models\r\n\r\nFunctional API models are described by their DAG, but for subclassed Models, the code is the definition and in general it's not possible to extract a DAG from them\r\n\r\nPlease note that even in 2.0, `model.fit` executes inside its own `tf.function` (similar to 1.x graph style). To run `model.fit` eagerly, compile your Model with `model.compile(..., run_eagerly=True)`", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30955\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30955\">No</a>\n", "Thanks for the clarification!\r\n\r\nAt this point, the documentation feels very misleading in the sense that \"Everything is Possible in TensorFlow 2.0!\" kind of sense. For me, it reads like: Do whatever you want. (Intentionally exaggerated of course!)\r\nHowever, those details are missing (as far as I can see), but they are certainly very important. Should I create a new issue to point to this deficiency in the documentation out or can we repurpose this issue?", "Has there been any update on this limitation? ", "> Thanks for the clarification!\r\n> \r\n> At this point, the documentation feels very misleading in the sense that \"Everything is Possible in TensorFlow 2.0!\" kind of sense. For me, it reads like: Do whatever you want. (Intentionally exaggerated of course!)\r\n> However, those details are missing (as far as I can see), but they are certainly very important. Should I create a new issue to point to this deficiency in the documentation out or can we repurpose this issue?\r\n\r\nAs it allows DAG definition, it will be confusing to have output bound to layers in the functional and subclassing model api. \r\nFor example, consider a simple param sharing case:\r\n```\r\ndense = Dense(256, activation='relu')\r\nx1 = dense(x)\r\nx2 = dense(x1)\r\n```\r\nWhat should be bound to the output of the 'dense' layer? A list seems work, but that conflict with layer having multiple outputs.\r\nIn sequential api, the model is linear, and things are simple. In the DAG case, it'd be better to operate manually."]}, {"number": 30952, "title": "`tf.keras.layers.Embedding` causes memory leak", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Linux Mint 19.1\r\n- TensorFlow installed from: binary (using pip)\r\n- TensorFlow version: 2.0.0-beta1 (v2.0.0-beta0-16-g1d91213fe7)\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.0 / 7.5\r\n- GPU model and memory: Nvidia Quadro P1000 - 4 GB GDDR5\r\n\r\n**Describe the current behavior**\r\n\r\nA GPU (_edit: CPU as well, see addendum below_) memory leak (rapidly) emerges from using (high-dimensional) `tf.keras.layers.Embedding` layers.\r\n\r\nTo be more precise, I am working on Transformer networks, and found out that when I try to fit one, _e.g._ on the portuguese-to-english translation task presented in [this official tutorial](https://www.tensorflow.org/beta/tutorials/text/transformer), a GPU memory leak emerges after a few iterations. Based on [this StackOverflow post](https://stackoverflow.com/questions/42499592/resourceexhaustederror-oom-when-allocating-tensor-with-shape#42512916), I rapidly came to suspect that the issue comes from the (learnable) embedding layers at the base of both the encoder and decoder parts of the network.\r\n\r\nTo further assess the issue and its source, I implemented a pseudo-Transformer network (see code linked below) that is stripped of most technical components the actual model embarks (_e.g._ I removed positional encoding, residual connections, masking mechanisms, etc.) - the rationale being to provide a more condense (and faster-run) code to document this issue, but also to confirm that the leak does not come from custom layers or any \"complex\" data processing mechanism.\r\n\r\nThe provided code includes a data pre-processing pipeline entirely based on the aforementioned [tutorial](https://www.tensorflow.org/beta/tutorials/text/transformer), a model-construction function that makes use of the keras functional API, and a main function to call the former and start the fitting process. On my computer, everything runs fine and I can see the first few fitting iterations pass, until an ugly stack of allocation error messages show up (see full log linked below), whose informative part seems to be `W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at cwise_ops_common.cc:70 : Resource exhausted: OOM when allocating tensor`\r\n\r\n**Addendum**: I re-ran the provided code disabling access to the GPU, and it turns out there also is a high memory usage when running on CPU. During the first epoch (and mostly during its first half), memory usage goes up multiple GB (in my case, from 2 to 10 GB, with an increase from 2 to 7 within the first 60 train steps out of 704), and keeps slowly increasing throughout the following epochs (with minor decreases between increases, thus displaying local plateaux which I would guess are related to the loading / discarding of data batches). Although it is a bit less of a problem than with GPU since it is relatively common to have quite some RAM available (plus some swap space, on linux), it still does not feel right that fitting the fake model on a dataset which can be fully loaded in memory (creating a list of Eager Tensors from the `tf.data.Dataset` object containing the batched, padded training set results in a marginal usage of around 100 MB of RAM) would end up using 16GB or RAM. I would also like to note that calling `gc.collect` after training _does not_ empty the used RAM, which is only freed (instantly) when ending the python process.\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nThe fitting process should go one fine, and the memory should not get saturated (I would expect some tensors to be de-allocated as iterations pass).\r\n\r\n**Code to reproduce the issue**\r\n\r\nThe script I wrote to illustrate the issue is publicly accessible as a gist [here](https://gist.github.com/pandrey-fr/c4ba8022c5dd956388e984f49c89ce61).\r\n\r\n**Other info / logs**\r\n\r\nThe full error stack (_with GPU enabled_) is publicly accessible as a gist [here](https://gist.github.com/pandrey-fr/ff004b4cdd6d22b9cd84f82ef4e3a5ac)", "comments": ["@pandrey-fr Hi Paul,\r\nthis is not related to your problem. However, I am stuck at one step before you, since you have helped me earlier so I thought maybe you can give me some suggestions. \r\nI am training my `tf.keras` model on multiple GPUs (2 exactly) and following TF2.0's [example](https://www.tensorflow.org/beta/guide/distribute_strategy#using_tfdistributestrategy_with_keras) for doing that. I am adding these two lines\r\n`mirrored_strategy = tf.distribute.MirroredStrategy()`\r\n`with mirrored_strategy.scope():`\r\n before the model's definition and compilations as also shown in the link. Now, I am receiving errors. My question from you is if there is anything else that has to be added in order to train it on multiple GPUs?  I have my own `tf.data.Dataset` separates into train and test sets after padding and batching.\r\nThank you for your helpful reply. I already opened an [issue](https://github.com/tensorflow/tensorflow/issues/30843#event-2502562470) on TF but haven't gotten any response. ", "After additional testing, I found out the high memory usage is not exclusive to the GPU, and update the initial post accordingly.", "@rishabhsahrawat Hi, I unfortunately have no experience with GPU distribution strategies, and only have 1-GPU machines at my current disposal, hence I would not know how to help you... Sorry :/", "I could reproduce the reported issue on Colab with Tensorflow version 2.0.0.beta1. Please take a look at gist of [colab](https://colab.research.google.com/drive/1ubO2h45rdehaPuoKiX4MzeKrYF-HIeG5). Thanks!", "Additional information: since [this aforementioned post](https://stackoverflow.com/questions/42499592/resourceexhaustederror-oom-when-allocating-tensor-with-shape#42512916) recommends taking the embedding lookup out of the training loop, I ran a modified version of the code where the embedding layers are declared outside of the instantiated keras Model (which now takes pre-embedded Tensors as inputs) and applied to the datasets at the time of their reshaping (in the `reformat_inputs` function).\r\n\r\nThis does not resolve the issue, and the GPU runs out of memory just as fast. However, if I try to load the entire training set in (non-GPU) memory (_e.g._ `data = list(iter(trainset))`), it works, but the memory used (which is greater than when loading the non-embedded data, which makes sense) is _not_ freed upon deletion. _I.e._ it appears that every time the data is loaded, memory is allocated that cannot be de-allocated - and apparently, more data loading occurs when fitting the model than I would expect (since the increase (with GPU disabled) is far greater when running `model.fit` than when listing the contents of the `trainset` object).", "With some effort, I found a way to export and reload the datasets after their creation (which requires Eager execution), so that I was able to run `setup_dataset`, dump the results, restart python, `tf.compat.v1.disable_eager_execution`, reload the datasets, `setup_model` and fit it without Eager.\r\n\r\nLong story short, it turns out **the issue does not show up when Eager is disabled** (and the fitting goes slightly faster - on CPU, 250s / epoch instead of 280s ; and obviously enabling GPU use makes for a great runtime gain, with less than 80s /epoch).\r\n\r\nSo, Eager execution messes things up badly... Why does that seem to be the endpoint of each and every issue I encounter these days? Anyway, I hope someone can find out where things go wrong with Eager enabled, and how to fix this (because disabling Eager is not exactly a fix, just a workaround for the times being - and an option I would personally like to keep in the future, outside of the `compat` sub-module, but that is another question).\r\n\r\nCode to reproduce (not including the functions defined in the aforeshared [gist](https://gist.github.com/pandrey-fr/c4ba8022c5dd956388e984f49c89ce61))\r\n\r\nFirst session - Eager execution is enabled.\r\n```python\r\n# Use aforeshared code to define setup_dataset\r\nimport numpy as np\r\n\r\ntrain, valid, inp_voc_size, tar_voc_size = setup_dataset()\r\nnp.save('train.npy', [(x.numpy(), y.numpy()) for x, y in train])\r\nnp.save('valid.npy', [(x.numpy(), y.numpy()) for x, y in valid])\r\n\r\n# I also ran commands to get the constants and note them somewhere.\r\n# In the second run, I therefore hard-code them for simplicity.\r\n# input vocab size is 8443, target vocab size is 8356\r\n# train set comprises 704 batches, validation set has 17\r\n```\r\n\r\nSecond session\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ntf.compat.v1.disable_eager_execution()\r\n\r\ndef reload_dataset(path):\r\n    \"\"\"Reload a dumped dataset and finish formatting it.\"\"\"\r\n    data = np.load(path, allow_pickle=True).tolist() \r\n    def generator(): \r\n        for inputs, target in data: \r\n            yield ((inputs, target[:, :-1]), target[:, 1:]) \r\n    types = ((tf.int64, tf.int64), tf.int64) \r\n    shape = (((None, None), (None, None)), (None, None)) \r\n    dataset = tf.data.Dataset.from_generator(generator, types, shape) \r\n    return dataset \r\n\r\n\r\n# use aforeshared code to define setup_model\r\n\r\n\r\ndef main():\r\n    train = reload_dataset('train.npy')\r\n    valid = reload_dataset('valid.npy')\r\n    model = setup_model(8443, 8356)\r\n    model.fit(\r\n        epochs=10, x=train.repeat(), steps_per_epoch=704,\r\n        validation_data=valid.repeat(), validation_steps=17,\r\n    )\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```", "Oh, and for the sake of it: I tried fitting a model with Eager enabled after reloading the data from the .npy dumps, and the memory issue is still there (_i.e._ it is not caused by the use of dataset transformations in `setup_dataset`).", "As I am still hoping that someone will pick up this issue, I conducted (yet) additional testing, namely replacing `tf.keras.layers.Embedding` with a subclass that overrides the `call` method in order to use one-hot-encoding and dot-product to retrieve embeddings instead of `tf.nn.lookup` (see code below). This does not fix the issue, which therefore seems to be a general input tensors non-discarding issue in Eager execution (regardless of how they are created).\r\n\r\nThe class I used to replace `tf.keras.layers.Embedding` inside the `setup_pseudo_model` function:\r\n```python\r\nclass OneHotEmbedding(tf.keras.layers.Embedding):\r\n    \"Embedding layer with one-hot dot-product retrieval mechanism.\"\"\"\r\n\r\n    def call(self, inputs):\r\n        \"\"\"Embed some inputs.\"\"\"\r\n        one_hot = tf.one_hot(inputs, depth=self.input_dim, dtype=tf.float32)\r\n        return tf.keras.backend.dot(one_hot, self.embeddings)\r\n```\r\n\r\nAgain, disabling Eager has everything run as smoothly as I want...", "I am happy to see some activity popping (@robieta, you seem to be quite the expert on this kind of issue!), however I see the `type:bug` tag was removed which in my humble opinion is incorrect; this is not just a performance issue (as is the case, e.g. in #30561), this is a bug, in the sense that training is not just slower, it is made altogether impossible on some configurations (namely when a GPU is visible or the amount of RAM is under 16 GB, which it really should not given the size of the model and dataset).", "Hi,\r\n\r\nThanks for the report and the reproduction script.\r\n\r\nI am not able to reproduce the memory leak with the beta1 release on CPU, with the script provided. I will try GPU next.\r\n\r\nThe tutorial script itself does not seem to feature a memory leak either on CPU or GPU.\r\n\r\nPlease try your reproduction script with your local configuration with the TF2.0 nightly build: https://pypi.org/project/tf-nightly-2.0-preview/", "So, I was actually able to reproduce the problem on GPU (but not CPU). Will investigate further.", "It seems that updating the TF version from beta1 to the latest nightly fixes the issue for me. Could you check if the update works for you as well?\r\n\r\n```\r\npip install tf-nightly-gpu-2.0-preview\r\n```", "Hey, I just want to say that I experienced the same problems with memory leaks (very high memory usage during first epoch) and installing the nightly version instead of beta1 fixed it for me.", "Hi,\r\nThank you for the follow-up on this. I currently am on the move and will only have access to the machine I ran those tests on next week, but hopefully the nightly build comes with the rightful fixes indeed. I will post the results (and hopefully close this issue) next week.", "@fchollet \r\n\r\nAs suggested, I installed a gpu-enabled 2.0 nightly build (from binary, using pip ; version `2.0.0-dev20190731` / git version `v1.12.1-7529-g3e0ad8a004`) and ran the test script again (without the line disabling Eager execution), both with and without GPU; unfortunately, I still encounter the same issue.\r\n\r\nWhen using the GPU, I get the following error after the first 38 training steps:\r\n```\r\nResourceExhaustedError: 2 root error(s) found.\r\n  (0) Resource exhausted:  OOM when allocating tensor with shape[64,37,8356] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[node loss/dense_2_loss/clip_by_value/Minimum (defined at /home/pandrey/Documents/tfnightly/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1686) ]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\t [[training/Adam/gradients/gradients/embedding_1/embedding_lookup_grad/Reshape/_34]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n  (1) Resource exhausted:  OOM when allocating tensor with shape[64,37,8356] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[node loss/dense_2_loss/clip_by_value/Minimum (defined at /home/pandrey/Documents/tfnightly/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1686) ]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_keras_scratch_graph_1551]\r\n\r\nFunction call stack:\r\nkeras_scratch_graph -> keras_scratch_graph\r\n```\r\n\r\nWhen running solely on CPU, the training runs but the RAM usage goes up as before, i.e. very much during the first steps, and more slowly but still up as further steps are run (at the end of the first epoch, I reached nearly 12 GB of RAM usage). The amount of RAM used remained stable during the second epoch (with small up-and-down fluctuations seemingly related to the loading and discard of data batches, which is normal).", "I also re-ran the tests adding the `tf.compat.v1.disable_eager_execution()` line, which again avoids triggering the issue. On CPU, RAM usage fluctuates between 2 and 2.4 GB, and on GPU the available 4GB of dedicated memory are not exhausted (and training goes way faster than the first steps run with both GPU and Eager enabled).", "@pandrey-fr This was resolved in `2.0.0rc0` and was tested internally. Can you check and let us know whether it was resolved for you when you use `!pip install tensorflow==2.0.0rc0`. Thanks!", "Awesome! I will test on my custom model tomorrow, but as for the example case I initially shared, it is indeed running just fine with `2.0.0rc0`. Many thanks and congratulations on the work already achieved towards the 2.0 release :-)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30952\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30952\">No</a>\n", "As a conclusive note:\r\n\r\nI ran my actual model with 2.0 rc0, comparing performances with and without disabling Eager execution. Most importantly, I am happy to report that leaving Eager enabled no longer causes memory issues. Regarding fitting runtimes, disabling Eager still yields a slight gain (122 seconds per epoch, versus 145 for the first and 135 for the following ones when Eager is left enabled - so, around 10 % runtime difference), but this is a relatively small gap compared to what I encountered in 2.0b1.\r\n\r\nOn the overall, Eager now seems much more stable than a couple of months ago - an impressive progress which must have taken a lot of hard work from all people involved, so many thanks and congrats for that!"]}, {"number": 30951, "title": "Missing input after saving/loading Keras Model", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 2.7.12\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: GeForce FTX 1080\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI am using [keras functional API](https://keras.io/getting-started/functional-api-guide/) to create a model. During training it has 2 inputs - one is actual input and second one is ground truth mask, I am applying using `Lambda` layer (see code example). When saving the model (not the weights) (via `ModelCheckpoint` callback or manually) and loading it via `keras.models.load_model` I get model without second input.\r\n\r\n**Describe the expected behavior**\r\nModel has both inputs after loading.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\n\r\n\r\ninput_size = 30\r\noutput_size = 10\r\n\r\ninp = keras.layers.Input((input_size,))\r\nmask = keras.layers.Input((output_size,), dtype=tf.bool)\r\n\r\nx = keras.layers.Dense(output_size)(inp)\r\nx = keras.layers.Lambda(lambda x: x, mask=mask)(x)\r\n\r\nmodel = keras.models.Model(inputs=[inp, mask], outputs=[x])\r\nmodel.compile(loss='mean_squared_error', optimizer='sgd')\r\n\r\nprint('Model inputs: {}'.format(model.inputs)) # Prints: \"Model inputs: [<tf.Tensor 'input_1:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'input_2:0' shape=(?, 10) dtype=bool>]\"\r\n\r\nbatch_size = 20\r\nx_train = np.random.rand(batch_size, input_size)\r\ny_train = np.random.rand(batch_size, output_size)\r\ny_train_mask = np.random.rand(batch_size, output_size)\r\ny_train_mask = y_train_mask > .5\r\n\r\nmodel.fit([x_train, y_train_mask], y_train)\r\n\r\ncheckpoint = './model.h5'\r\nmodel.save(checkpoint)\r\n\r\nmodel = keras.models.load_model(checkpoint)\r\nprint('Model inputs: {}'.format(model.inputs)) # Prints: \"Model inputs: [<tf.Tensor 'input_1_1:0' shape=(?, 30) dtype=float32>]\"\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "comments": ["I was able to reproduce the issue with TF version 1.12.\r\nPlease find Colab [Gist](https://colab.research.google.com/gist/oanush/8ef6232c7efe46a022abc644180188c5/30951.ipynb).Thanks!", "Another thing to consider: running the exact code snippet with TF version 1.14 (built from source, git tag 1.14 with Bazel 0.25.2 on Mac OS, running with miniconda python 3.7.3) I get an unhandled exception in `compile` method:\r\n```\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0729 19:48:53.413664 4525299136 deprecation.py:506] From /Users/binpord/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nTraceback (most recent call last):\r\n  File \"/Users/binpord/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1864, in _create_c_op\r\n    c_op = c_api.TF_FinishOperation(op_desc)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Can not squeeze dim[1], expected a dimension of 1, got 10 for 'loss/lambda_loss/weighted_loss/Squeeze' (op: 'Squeeze') with input shapes: [?,10].\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"example.py\", line 16, in <module>\r\n    model.compile(loss='mean_squared_error', optimizer='sgd')\r\n  File \"/Users/binpord/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/Users/binpord/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 337, in compile\r\n    self._compile_weights_loss_and_weighted_metrics()\r\n  File \"/Users/binpord/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/Users/binpord/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1710, in _compile_weights_loss_and_weighted_metrics\r\n    self.total_loss = self._prepare_total_loss(masks)\r\n  File \"/Users/binpord/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1774, in _prepare_total_loss\r\n    reduction=losses_utils.ReductionV2.NONE)\r\n  File \"/Users/binpord/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/utils/losses_utils.py\", line 199, in compute_weighted_loss\r\n    losses, None, sample_weight)\r\n  File \"/Users/binpord/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/utils/losses_utils.py\", line 101, in squeeze_or_expand_dimensions\r\n    sample_weight = array_ops.squeeze(sample_weight, [-1])\r\n  File \"/Users/binpord/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 180, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/Users/binpord/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/Users/binpord/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 3162, in squeeze\r\n    return gen_array_ops.squeeze(input, axis, name)\r\n  File \"/Users/binpord/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 9666, in squeeze\r\n    \"Squeeze\", input=input, squeeze_dims=axis, name=name)\r\n  File \"/Users/binpord/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/Users/binpord/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/Users/binpord/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\r\n    op_def=op_def)\r\n  File \"/Users/binpord/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 2027, in __init__\r\n    control_input_ops)\r\n  File \"/Users/binpord/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1867, in _create_c_op\r\n    raise ValueError(str(e))\r\nValueError: Can not squeeze dim[1], expected a dimension of 1, got 10 for 'loss/lambda_loss/weighted_loss/Squeeze' (op: 'Squeeze') with input shapes: [?,10].\r\n```\r\nThis means, that I am doing something wrong. I tried debugging the issue, but got completely confused with all the abstraction levels. What I succeeded in grasping is, `compile` calls `losses_utils.compute_weighted_loss` function, which tries to equalise `losses` and `sample_weight` dimensions. Ok, the fact that output mask ends up in something called `sample_weight` seems already weird enough to me. Moreover, it completely fails, as `losses` has shape of `(None, 1)` (the mean squared error loss) and the `sample_weight` has shape of `(None, 10)`, hence cannot be squashed.\r\n\r\nHow I see it, output mask should work this way: NN predicts some tensor (vector of 10 elements in the snippet) and mask should point to elements in this vector, which should be excluded from loss computation. However, this seems not to be the case. What am I doing wrong?\r\n\r\nIn actual use case, I am interested in, I try to predict some 2D image-like tensor and I would like to mask loss in some areas, where prediction is meaningless and, hence, should not affect training. Maybe I should use some other mechanism instead of `Lambda` layer mask?", "@Binpord Thanks for the issue!\r\n\r\nThere seem to be some errors with how the Model was constructed, this worked for me though:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\n\r\n\r\ninput_size = 30\r\noutput_size = 10\r\n\r\ninp = keras.layers.Input((input_size,))\r\nmask = keras.layers.Input((output_size,), dtype=tf.bool)\r\nx = keras.layers.Dense(output_size)(inp)\r\nx = x * tf.cast(mask, dtype=tf.float32)\r\n\r\nmodel = keras.models.Model(inputs=[inp, mask], outputs=x)\r\nmodel.compile(loss='mean_squared_error', optimizer='sgd')\r\n\r\nprint('Model inputs: {}'.format(model.inputs)) # Prints: \"Model inputs: [<tf.Tensor 'input_1:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'input_2:0' shape=(?, 10) dtype=bool>]\"\r\n\r\nbatch_size = 20\r\nx_train = np.random.rand(batch_size, input_size)\r\ny_train = np.random.rand(batch_size, output_size)\r\ny_train_mask = np.random.rand(batch_size, output_size)\r\ny_train_mask = y_train_mask > .5\r\n\r\nmodel.fit([x_train, y_train_mask], y_train)\r\n\r\ncheckpoint = './model.h5'\r\nmodel.save(checkpoint)\r\n\r\nmodel = keras.models.load_model(checkpoint)\r\nprint('Model inputs: {}'.format(model.inputs))\r\n```\r\n\r\nClosing as I think the issue was in the model construction", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30951\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30951\">No</a>\n"]}, {"number": 30950, "title": "Transfer Learning tutorial Attribute Error: 'GFile' object has no attribute 'seekable'", "body": "Following the tutorial on downloading and using data sets found here: https://www.tensorflow.org/beta/tutorials/images/transfer_learning\r\nI get the error \r\n\r\n> Attribute Error: 'GFile' object has no attribute 'seekable'\r\n\r\nIt breaks when I run tfds.load\r\n\r\nYou can reproduce it with the following syntax:\r\n\r\n'import tensorflow_datasets as tfds\r\ntfds.disable_progress_bar()\r\nSPLIT_WEIGHTS = (8, 1, 1)\r\nsplits = tfds.Split.TRAIN.subsplit(weighted=SPLIT_WEIGHTS)\r\n\r\n(raw_train, raw_validation, raw_test), metadata = tfds.load(\r\n    cats_vs_dogs', split=list(splits),\r\n    with_info=True, `as_supervised=True)`\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- Platform: Windows\r\n- Tensorflow Datasets version 1.0.2\r\n- TensorFlow installed from (source or binary): Anaconda\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 10.0/7.6", "comments": ["I manually deleted all of the tensorflow files from my computer and reinstalled them and this is no longer a problem"]}, {"number": 30949, "title": "Dense layer that ingests both sparse and dense inputs", "body": "", "comments": ["Clear example on how I used this custom Layer in subclassing : https://medium.com/dailymotion/how-to-design-deep-learning-models-with-sparse-inputs-in-tensorflow-keras-fd5e754abec1", "@SharoneDayan thanks for your contribution , can you please add a test case ?\r\n\r\nAlso please add some description as why this change is needed.", "Hello, thank you for your comment ! I create a test case file to test a model that supports this layer.\r\nI also added a brief description. Thank you !", "Adding Francois who is the api owner.\r\n\r\nFrom API perspective, we don't want to have layers that is specially designed to handle certain inputs, otherwise the number of layer will be exploded.\r\n\r\nAlso, in the meantime, we are adding composite tensor support across keras, which will also handle sparse input.  ", "Closing this PR based on the comment above. Feel free to reopen if there is any updates. Thanks.", "The current Keras Dense layer already supports sparse tensor."]}, {"number": 30948, "title": "The use of List and Tuple is vague in Tensorflow tf.data.Dataset.map.", "body": "System information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version (use command below): 1.14.0\r\nPython version: 3.7.3\r\nCUDA/cuDNN version: 10.1.168 / 7.6.0 but also failing in CPU-only mode\r\nGPU model and memory: GTX 1080 8G\r\nExact command to reproduce:\r\n```\r\nimport tensorflow as tf\r\ninput = tf.constant([10,20,30])\r\nds = tf.data.Dataset.from_tensor_slices(input)\r\n#Out: <DatasetV1Adapter shapes: (), types: tf.int32>\r\n\r\nds1=ds.map(lambda x: [x+1, x+2, x+3])\r\n#Out: <DatasetV1Adapter shapes: ((), (), ()), types: (tf.int32, tf.int32, tf.int32)>\r\nds2=ds.map(lambda x: [[x+1, x+2, x+3]])\r\n#Out: <DatasetV1Adapter shapes: ((3,),), types: (tf.int32,)>\r\nds3=ds.map(lambda x: ([x+1, x+2, x+3]))\r\n#Out: <DatasetV1Adapter shapes: ((), (), ()), types: (tf.int32, tf.int32, tf.int32)>\r\n```\r\nI think tensorflow treats tuple as nested structures of Tensor and list as tensor, as shown in [link](https://github.com/tensorflow/tensorflow/issues/20481). \r\nHowever, `tf.data.Dataset.map` doesn't behave like this in above code. I expect `ds1, d2, ds3` to be of shape `[3, ], [1, 3], ([3, ], )` respectively,  rather than `((), (), ()), ((3,),), ((), (), ())` since top-level list [] should be treated as tensor rather than nested structure.\r\nA [solution](https://stackoverflow.com/questions/57142462/something-strange-about-output-tensor-shape-of-tf-data-dataset-map/57146524?noredirect=1#comment100834064_57146524) is that I use `tf.convert_to_tensor` to force convert `[x+1, x+2, x+3]` to a single tensor but it's not elegant.\r\n\r\nps: It's confusing to output shape only with tuple, why not using list to represent shape and using tuple to represent nested structure of tensor jsut like \r\nthe output of Tensor.shape and Tensor.get_shape. e.g. `#Out: <DatasetV1Adapter shapes: ([],[],[]), types: (tf.int32, tf.int32, tf.int32)>`", "comments": ["@kzhangair \r\nIs this still an issue", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30948\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30948\">No</a>\n"]}, {"number": 30946, "title": "tf.keras.layers.Dropout does not accept noise_shape with None-dimension", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux openSUSE 42.2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary (via conda)\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 9.0 / cuDNN 7.6.0\r\n- GPU model and memory: GeForce GTX 1080 Ti (10.92 GB)\r\n\r\n**Describe the current behavior**\r\n\r\nThe Dropout layer accepts a `noise_shape` argument. If set, this shape has to include the batch size, which is usually not known at model construction time. Thus, this argument is actually only useful if `None` can be specified for any axis and is then expanded to the actual size of that axis in the input tensor passed to the layer.\r\n\r\nThis is currently perfectly possible in standard `keras` but raises an error in `tf.keras` (see below).\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should be possible to include `None`-sized axes in the `noise_shape` argument to `tf.keras.layers.Dropout`, which are expanded at run-time to the actual size of that axis in the input tensor of the layer.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nfrom tensorflow import keras\r\n\r\ninp_ = keras.layers.Input((2,3,4))\r\ndropout = keras.layers.Dropout(0.5, noise_shape=(None,1,1,1))\r\nx = dropout(inp_, training=True)\r\ndropfun = keras.backend.function([inp_], [x])\r\n```\r\n\r\nThis code works completely fine if the first line is replaced with `import keras`.\r\n\r\n**Other info / logs**\r\n\r\nTraceback:\r\n```pytb\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-3-1601abc84b49> in <module>\r\n      1 inp_ = keras.layers.Input((2,3,4))\r\n      2 dropout = keras.layers.Dropout(0.5, noise_shape=(None,1,1,1))\r\n----> 3 x = dropout(inp_, training=True)\r\n      4 dropfun = keras.backend.function([inp_], [x])\r\n\r\n~/lib/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    632                     outputs = base_layer_utils.mark_as_return(outputs, acd)\r\n    633                 else:\r\n--> 634                   outputs = call_fn(inputs, *args, **kwargs)\r\n    635 \r\n    636             except TypeError as e:\r\n\r\n~/lib/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py in call(self, inputs, training)\r\n    160     output = tf_utils.smart_cond(training,\r\n    161                                  dropped_inputs,\r\n--> 162                                  lambda: array_ops.identity(inputs))\r\n    163     return output\r\n    164 \r\n\r\n~/lib/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py in smart_cond(pred, true_fn, false_fn, name)\r\n     56         pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n     57   return smart_module.smart_cond(\r\n---> 58       pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n     59 \r\n     60 \r\n\r\n~/lib/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/framework/smart_cond.py in smart_cond(pred, true_fn, false_fn, name)\r\n     52   if pred_value is not None:\r\n     53     if pred_value:\r\n---> 54       return true_fn()\r\n     55     else:\r\n     56       return false_fn()\r\n\r\n~/lib/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py in dropped_inputs()\r\n    156           noise_shape=self._get_noise_shape(inputs),\r\n    157           seed=self.seed,\r\n--> 158           rate=self.rate)\r\n    159 \r\n    160     output = tf_utils.smart_cond(training,\r\n\r\n~/lib/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    505                 'in a future version' if date is None else ('after %s' % date),\r\n    506                 instructions)\r\n--> 507       return func(*args, **kwargs)\r\n    508 \r\n    509     doc = _add_deprecated_arg_notice_to_docstring(\r\n\r\n~/lib/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py in dropout(x, keep_prob, noise_shape, seed, name, rate)\r\n   4168     raise ValueError(\"You must provide a rate to dropout.\")\r\n   4169 \r\n-> 4170   return dropout_v2(x, rate, noise_shape=noise_shape, seed=seed, name=name)\r\n   4171 \r\n   4172 \r\n\r\n~/lib/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py in dropout_v2(x, rate, noise_shape, seed, name)\r\n   4247     # and subtract 1.0.\r\n   4248     random_tensor = random_ops.random_uniform(\r\n-> 4249         noise_shape, seed=seed, dtype=x.dtype)\r\n   4250     keep_prob = 1 - rate\r\n   4251     scale = 1 / keep_prob\r\n\r\n~/lib/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/ops/random_ops.py in random_uniform(shape, minval, maxval, dtype, seed, name)\r\n    237     maxval = 1\r\n    238   with ops.name_scope(name, \"random_uniform\", [shape, minval, maxval]) as name:\r\n--> 239     shape = _ShapeTensor(shape)\r\n    240     minval = ops.convert_to_tensor(minval, dtype=dtype, name=\"min\")\r\n    241     maxval = ops.convert_to_tensor(maxval, dtype=dtype, name=\"max\")\r\n\r\n~/lib/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/ops/random_ops.py in _ShapeTensor(shape)\r\n     42   else:\r\n     43     dtype = None\r\n---> 44   return ops.convert_to_tensor(shape, dtype=dtype, name=\"shape\")\r\n     45 \r\n     46 \r\n\r\n~/lib/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype, dtype_hint)\r\n   1085   preferred_dtype = deprecation.deprecated_argument_lookup(\r\n   1086       \"dtype_hint\", dtype_hint, \"preferred_dtype\", preferred_dtype)\r\n-> 1087   return convert_to_tensor_v2(value, dtype, preferred_dtype, name)\r\n   1088 \r\n   1089 \r\n\r\n~/lib/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)\r\n   1143       name=name,\r\n   1144       preferred_dtype=dtype_hint,\r\n-> 1145       as_ref=False)\r\n   1146 \r\n   1147 \r\n\r\n~/lib/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors, accept_composite_tensors)\r\n   1222 \r\n   1223     if ret is None:\r\n-> 1224       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1225 \r\n   1226     if ret is NotImplemented:\r\n\r\n~/lib/anaconda3/envs/tf114/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in _tensor_shape_tensor_conversion_function(s, dtype, name, as_ref)\r\n    324   if not s.is_fully_defined():\r\n    325     raise ValueError(\r\n--> 326         \"Cannot convert a partially known TensorShape to a Tensor: %s\" % s)\r\n    327   s_list = s.as_list()\r\n    328   int64_value = 0\r\n\r\nValueError: Cannot convert a partially known TensorShape to a Tensor: (?, 1, 1, 1)\r\n```\r\n\r\nThe bug is probably due to the `tf.keras.layers.Dropout._get_noise_shape` function, which explicitly performs the `None`-expansion in [standard keras](https://github.com/keras-team/keras/blob/master/keras/layers/core.py#L110).\r\n", "comments": ["Looking into the code history, I think this issue might have been fixed in: ab2644e", "Looks good. Thanks for pointing me to this and sorry for redundant reporting."]}, {"number": 30945, "title": "/home/hdp_teu_dia/guesslike/user/suwenyuan/emotion/bert-master/chinese_L-12_H-768_A-12", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Hi,\r\n\r\nCould you please describe the problem you are experiencing currently and fill out the form you posted above with some information?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 30944, "title": "Keras' performance is worse than pure TF", "body": "Hello, \r\nI am trying to update my pure Tensorflow code to incorporate Keras. But what I find is that my Keras implementation of the same NN structure does not converge as much as the pure TF code for the same number of iteration. In fact, Keras implementation does very poorly.  I have attached my code and data set [here](https://github.com/ronyeapen/TF-and-Keras). It is a Regression model.  No response here with regards to it. \r\nhttps://stackoverflow.com/questions/57137235/tf-low-level-api-vs-tf-keras-performance-difference\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30944\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30944\">No</a>\n"]}, {"number": 30943, "title": "TensorFlow C API Nightly URLs", "body": "[This README](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/README.md) mentions the following URLs for nightly builds of libtensorflow:\r\n- https://storage.googleapis.com/tensorflow-nightly/github/tensorflow/lib_package/libtensorflow-cpu-linux-x86_64.tar.gz\r\n- https://storage.googleapis.com/tensorflow-nightly/github/tensorflow/lib_package/libtensorflow-gpu-linux-x86_64.tar.gz\r\n- https://storage.googleapis.com/tensorflow-nightly/github/tensorflow/lib_package/libtensorflow-cpu-darwin-x86_64.tar.gz\r\n\r\nAre there any versions of these including dates? Maybe something matching the naming convention used by the nightly pip packages (e.g. `libtensorflow-cpu-linux-x86_64-1.15.0.dev20190722.tar.gz`)?\r\n\r\nI looked through some issues and RFCs  (including https://github.com/tensorflow/tensorflow/issues/21524 and https://github.com/tensorflow/community/blob/master/rfcs/20181026-tf-nightly.md), but didn't see anything about versioned nightly builds.\r\n\r\nThank you!", "comments": ["@VivekPanyam We are checking to see if you still need help on this issue. If it is resolved then please feel free to move this issue to close status ? Thanks!", "The links in the readme are all wrong.\r\n\r\nMaybe just delete them and point users to the in stall guide, which has better links.\r\n\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30943\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30943\">No</a>\n"]}, {"number": 30942, "title": "unable to load trained model", "body": "I trained the NMT model on my RTX2080 ti. I saved the model by\r\n\r\ncheckpoint_dir = './training_checkpoints'\r\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\r\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,\r\n                                 encoder=encoder,\r\n                                 decoder=decoder)\r\n\r\nand in the training step \r\n\r\nif (epoch + 1) % 2 == 0:\r\n   checkpoint.save(file_prefix = checkpoint_prefix)\r\n\r\nand after training, I am loading the trained model by\r\n\r\ncheckpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\r\n\r\nBut my model's output is random. \r\n\r\n### System information\r\n- ** TensorFlow version == 1.14.0(installed through \" conda install -c anaconda tensorflow-gpu \")\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 18.04)**:\r\n- **Python version == 3.7\r\n- **CUDA/cuDNN version**:== 10\r\n- **GPU model and memory**: 11GB\r\n\r\n\r\nand I am getting this at the end\r\n\r\nW0723 07:13:25.044782 140413660407616 util.py:244] Unresolved object in checkpoint: (root).optimizer.beta1_power\r\nW0723 07:13:25.044880 140413660407616 util.py:244] Unresolved object in checkpoint: (root).optimizer.beta2_power\r\nW0723 07:13:25.044922 140413660407616 util.py:248] Unused attribute in object (root).decoder.embedding: ['OBJECT_CONFIG_JSON']\r\nW0723 07:13:25.044954 140413660407616 util.py:248] Unused attribute in object (root).decoder.gru: ['OBJECT_CONFIG_JSON']\r\nW0723 07:13:25.044982 140413660407616 util.py:248] Unused attribute in object (root).decoder.fc: ['OBJECT_CONFIG_JSON']\r\nW0723 07:13:25.045008 140413660407616 util.py:248] Unused attribute in object (root).decoder.W1: ['OBJECT_CONFIG_JSON']\r\nW0723 07:13:25.045034 140413660407616 util.py:248] Unused attribute in object (root).decoder.W2: ['OBJECT_CONFIG_JSON']\r\nW0723 07:13:25.045060 140413660407616 util.py:248] Unused attribute in object (root).decoder.V: ['OBJECT_CONFIG_JSON']\r\nW0723 07:13:25.045086 140413660407616 util.py:248] Unused attribute in object (root).encoder.embedding: ['OBJECT_CONFIG_JSON']\r\nW0723 07:13:25.045122 140413660407616 util.py:248] Unused attribute in object (root).encoder.gru: ['OBJECT_CONFIG_JSON']\r\nW0723 07:13:25.045150 140413660407616 util.py:252] A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\r\n", "comments": ["@ankitgc1 ,\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "@anush-o \r\n\r\nI changed the model from NMT to for chatbot here are some code snippet.\r\n\r\ntraining phase \r\n![Screenshot from 2019-07-25 14-39-49](https://user-images.githubusercontent.com/35380717/61861896-7e890500-aeea-11e9-9632-867c823b4f5a.png)\r\n\r\n\r\nrestoring phase\r\n\r\n![Screenshot from 2019-07-25 14-39-59](https://user-images.githubusercontent.com/35380717/61861921-8b0d5d80-aeea-11e9-8a04-082905f3a953.png)\r\n", "@ankitgc1 ,\r\nCan you please provide executable code.Thanks!", "@anush-o \r\nyou can find all files and code here\r\nhttps://github.com/ankitgc1/Chatbot-in-tensorflow\r\n", "@ankitgc1 ,\r\nWhen tried executing the code by cloning, I got the output as per the screenshot attached.\r\n![Screenshot from 2019-07-31 13-59-46](https://user-images.githubusercontent.com/52397990/62197282-32800980-b39d-11e9-8c10-d800c1029afe.png)\r\nThanks!\r\n", "@anush-o \r\nthis is random output. first you have to train the model. you can do this thing by just uncommenting the commenting part.\r\n\r\nthanks.\r\n", "This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}]