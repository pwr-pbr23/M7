[{"number": 53416, "title": "Fixed pip package build with cmake due to missing numpy include path", "body": null, "comments": ["@terryheo could you PTAL?"]}, {"number": 53415, "title": "Added lowerings for mhlo.sine and mhlo.cosine to mhlo preprocessing", "body": "We can decompose sine and cosine operations to additional sine, cosing, and\r\nexponential operattions. Includes tests for both operations.", "comments": ["@rsuderman  Can you please resolve conflicts? Thanks!"]}, {"number": 53412, "title": "[Go] Stabilize generated wrappers", "body": "Multiple commits each day make white-space only changes to `/tensorflow/go/op/wrappers.go` with commit message `Go: Update generated wrapper functions for TensorFlow ops.`  Example: tensorflow/tensorflow@82c3b11\r\n\r\nPR stabilizes output from the genop library, which generates these wrappers.  The source of the current instability is the text marshaler in google.golang.org/protobuf, which purposefully adds [random whitespace](https://github.com/protocolbuffers/protobuf-go/blob/v1.27.1/internal/encoding/text/encode.go#L226).\r\n- Catch and remove superfluous whitespace when marshaling proto message\r\n- Sort op list (and thus wrapper functions) lexicographically by name\r\n- Re-enable test TestGeneratedOp which was previously disabled due to output instability", "comments": []}, {"number": 53411, "title": "[r2.6][PR] Cannot find real source of dependency `No such file or directory`  error when building from scratch", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r2.6\r\n- Python version: 3.9.5\r\n- Installed using virtualenv? pip? conda?: `miniconda`\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n\r\n**Describe the problem**\r\n\r\nI am currently working on a relatively large PR to [introduce ZSTD support in TF](https://github.com/tensorflow/tensorflow/pull/53385) for `TFRecordWriter`, which currently supports `ZLIB` and `GZIP`.\r\n\r\nTo introduce my changes, I am of course writing some tests, and as I am working my way up to the dependencies chain, I have stumbled upon this error:\r\n\r\n```\r\n(tensorflow) ubuntu@tensorflow-compression-build-1:~/Workspace/tensorflow$ bazel test //tensorflow/core/lib/io/zstd:zstd_test --test_filter=* --verbose_failures\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=143\r\nINFO: Reading rc options for 'test' from /home/ubuntu/Workspace/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'test' from /home/ubuntu/Workspace/tensorflow/.bazelrc:\r\n  Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true\r\nINFO: Reading rc options for 'test' from /home/ubuntu/Workspace/tensorflow/.tf_configure.bazelrc:\r\n  Inherited 'build' options: --action_env PYTHON_BIN_PATH=/home/ubuntu/miniconda3/envs/tensorflow/bin/python --action_env PYTHON_LIB_PATH=/home/ubuntu/miniconda3/envs/tensorflow/lib/python3.9/site-packages --python_path=/home/ubuntu/miniconda3/envs/tensorflow/bin/python\r\nINFO: Reading rc options for 'test' from /home/ubuntu/Workspace/tensorflow/.tf_configure.bazelrc:\r\n  'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium\r\nINFO: Found applicable config definition build:short_logs in file /home/ubuntu/Workspace/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/ubuntu/Workspace/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition test:v2 in file /home/ubuntu/Workspace/tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only\r\nINFO: Found applicable config definition build:linux in file /home/ubuntu/Workspace/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/ubuntu/Workspace/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ef409778dc6f6079679ade72bc957ee0/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.\r\nINFO: Analyzed target //tensorflow/core/lib/io/zstd:zstd_test (1 packages loaded, 70 targets configured).\r\nINFO: Found 1 test target...\r\nERROR: /home/ubuntu/Workspace/tensorflow/tensorflow/core/BUILD:1610:16: C++ compilation of rule '//tensorflow/core:framework_internal_impl' failed (Exit 1): gcc failed: error executing command \r\n  (cd /home/ubuntu/.cache/bazel/_bazel_ubuntu/ef409778dc6f6079679ade72bc957ee0/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/home/ubuntu/.cache/bazelisk/downloads/bazelbuild/bazel-3.7.2-linux-x86_64/bin:/home/ubuntu/.vscode-server/bin/7db1a2b88f7557e0a43fec75b6ba7e50b3e9f77e/bin:/home/ubuntu/miniconda3/envs/tensorflow/bin:/home/ubuntu/miniconda3/condabin:/home/ubuntu/.vscode-server/bin/7db1a2b88f7557e0a43fec75b6ba7e50b3e9f77e/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/home/ubuntu/miniconda3/envs/tensorflow/bin/python \\\r\n    PYTHON_LIB_PATH=/home/ubuntu/miniconda3/envs/tensorflow/lib/python3.9/site-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -MD -MF bazel-out/k8-opt/bin/tensorflow/core/_objs/framework_internal_impl/events_writer.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/core/_objs/framework_internal_impl/events_writer.pic.o' -fPIC -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -iquote. -iquotebazel-out/k8-opt/bin -iquoteexternal/com_google_protobuf -iquotebazel-out/k8-opt/bin/external/com_google_protobuf -iquoteexternal/eigen_archive -iquotebazel-out/k8-opt/bin/external/eigen_archive -iquoteexternal/com_google_absl -iquotebazel-out/k8-opt/bin/external/com_google_absl -iquoteexternal/nsync -iquotebazel-out/k8-opt/bin/external/nsync -iquoteexternal/gif -iquotebazel-out/k8-opt/bin/external/gif -iquoteexternal/libjpeg_turbo -iquotebazel-out/k8-opt/bin/external/libjpeg_turbo -iquoteexternal/com_googlesource_code_re2 -iquotebazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquoteexternal/farmhash_archive -iquotebazel-out/k8-opt/bin/external/farmhash_archive -iquoteexternal/fft2d -iquotebazel-out/k8-opt/bin/external/fft2d -iquoteexternal/highwayhash -iquotebazel-out/k8-opt/bin/external/highwayhash -iquoteexternal/zlib -iquotebazel-out/k8-opt/bin/external/zlib -iquoteexternal/double_conversion -iquotebazel-out/k8-opt/bin/external/double_conversion -iquoteexternal/snappy -iquotebazel-out/k8-opt/bin/external/snappy -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem external/gif -isystem bazel-out/k8-opt/bin/external/gif -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -isystem external/double_conversion -isystem bazel-out/k8-opt/bin/external/double_conversion -w -DAUTOLOAD_DYNAMIC_KERNELS '-std=c++14' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DTENSORFLOW_USE_XLA=1' -DINTEL_MKL -msse3 -pthread '-DTENSORFLOW_USE_XLA=1' '-DINTEL_MKL=1' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/core/util/events_writer.cc -o bazel-out/k8-opt/bin/tensorflow/core/_objs/framework_internal_impl/events_writer.pic.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nIn file included from ./tensorflow/core/lib/io/record_writer.h:29:0,\r\n                 from ./tensorflow/core/util/events_writer.h:23,\r\n                 from tensorflow/core/util/events_writer.cc:16:\r\n./tensorflow/core/lib/io/zstd/zstd_outputbuffer.h:19:10: fatal error: zstd.h: No such file or directory\r\n #include <zstd.h>\r\n          ^~~~~~~~\r\ncompilation terminated.\r\nTarget //tensorflow/core/lib/io/zstd:zstd_test failed to build\r\nINFO: Elapsed time: 0.926s, Critical Path: 0.28s\r\nINFO: 5 processes: 5 internal.\r\nFAILED: Build did NOT complete successfully\r\n//tensorflow/core/lib/io/zstd:zstd_test                         FAILED TO BUILD\r\n\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nAnd it did happen before, so I just went into the `BUILD` file that the error references, and look if there is somewhere I might have missed a dependency somehow. I have yet to find the real source of the dependency, and I have gone through every possible place where I should have put my dependencies.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nClone my fork and go to my branch, with commit sha `9d7b6a5e223c59d8d9687ce128dd1ebc5a6ab908` or `build-issue-adrian-compression-r2.6`:\r\n\r\n```\r\ngit clone https://github.com/IAL32/tensorflow\r\ngit checkout 9d7b6a5e223c59d8d9687ce128dd1ebc5a6ab908\r\n```\r\n\r\nBuild from source however you like, and execute the test:\r\n\r\n```\r\nbazel test //tensorflow/core/lib/io/zstd:zstd_test\r\n```\r\n\r\nYou should now see the error above.\r\n\r\nFinally, if you revert the last commit, which changes `tensorflow/core/lib/io/record_writer.cc` and `tensorflow/core/lib/io/record_writer.h` to accommodate my `ZSTD` class, launching the test again works fine.\r\n\r\nAn overview of the stuff I have changed can be found here: https://github.com/tensorflow/tensorflow/compare/r2.6...IAL32:build-issue-adrian-compression-r2.6?expand=1\r\n\r\nI believe this can be solved by including some header as a dependency, but I have been struggling to understand **where** exactly.\r\n\r\nThanks for any help!", "comments": ["Hi @Saduf2019! Could you please look at this issue?", "@IAL32 \r\nWith respect to the error \"\" can you refer to this [link](https://stackoverflow.com/questions/57881232/why-does-configure-error-zstd-library-not-found-appear-when-i-configure-gree), also please downgrade your python and let us know if this is still an issue.\r\nPlease refer to similar error issues:[Link](https://github.com/bazelbuild/bazel/issues/591),[link1](https://github.com/tensorflow/serving/issues/1573),[link2](https://stackoverflow.com/questions/68441018/getting-this-error-c-compilation-of-rule-org-tensorflow-tensorflow-core-ker)\r\n", "Hi @Saduf2019 , thanks for the answer! Although, none of the links you have provided really answer my issue, or have much to do with it.\r\n\r\nThe \"header missing\" error does not come from the lack of the library, which is correctly provided and installed in `third_party/zstd.BUILD` (and of course is __not__ supposed to be installed on the local machine).\r\n\r\nWhat I am having issues with is not the procedure of building itself, but this specific behavior of the build system when a new dependency is introduced in `reader_writer.cc`.", "I think I have fixed it, although I really don't know why.\r\n\r\nI have just moved the declaration for the `ZstdOutputBuffer` from the header to the cpp file:\r\nhttps://github.com/IAL32/tensorflow/commit/efef02f0026ec3b2a6e590337d15e88e859e9a20#diff-5ab5acb26dbf183efb7a3707d0b15f82287cc4d21720caa91ca08d215b140838\r\n\r\nI will close the issue, although it is apparent that this should not be done, it serves my purposes for now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53411\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53411\">No</a>\n"]}, {"number": 53409, "title": "Patch absl for cuda 11.6 compatibility.", "body": "Works around the following compiler errors.\r\n```\r\nerror: expression must have a constant value\r\nnote #2721-D: expression cannot be interpreted\r\n```\r\nAttn: @sanjoy ", "comments": []}, {"number": 53408, "title": "Tuner tutorial rebuilds model rather than saving it", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/tutorials/keras/keras_tuner\r\n\r\n## Description of issue (what needs changing):\r\nThe bottom of this tutorial says `Re-instantiate the hypermodel and train it with the optimal number of epochs from above.` But there's no reason to train the model twice when you could use the callback to save the model at the best epoch. I and others have speculated [here](https://twitter.com/HamelHusain/status/1470492047404580865) that the call to re-train the model is supposed to be on the full data (with no validation split). If that's the intention, the resulting change would be to remove `validation_split=0.2` from the final `fit` call.\r\n\r\nOtherwise, it seems you should add a `ModelCheckpoint` callback to the save the best model when you first fit the model... and then not refit the model.", "comments": ["@dansbecker ,\r\nPlease post this issue on [keras-team/keras](https://github.com/keras-team/keras/issues) repo.\r\nTo know more refer to:\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999\r\n", "Thanks @tilakrayal . I've submitted [the issue there](https://github.com/keras-team/keras/issues/15789). I'll leave it to you to decide if this issue should be closed.", "@dansbecker ,\r\nPlease feel free to move this issue to closed status as it has been tracking in Keras repo.Thanks!"]}, {"number": 53407, "title": "Bazel Tensorflow installation with TensorRT from source throws error although there is output file", "body": "I want to build Tensorflow with TensorRT support with Bazel from source.\r\n\r\nSystem specifications:\r\n\r\nTensorflow 2.7\r\nPython 3.8.0\r\nCUDA 11.5\r\nUbuntu 18.04\r\nGeforce RTX 2060\r\nI tried the following bazel versions: 3.7.2, 4.2.1, 4.2.2 and all threw the same error.\r\n\r\nIt starts the building process but then throws the following 2 errors regarding the BUILD file:\r\n\r\n```\r\nERROR: /home/ros/.cache/bazel/_bazel_ros/f58d1d645696850c769c1c78103710da/external/local_config_python/BUILD:78:8: declared output 'external/local_config_python/*.*' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)\r\nERROR: /home/ros/.cache/bazel/_bazel_ros/f58d1d645696850c769c1c78103710da/external/local_config_python/BUILD:78:8: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\nHere is the BUILD file from which the error occurs\r\n\r\n```\r\nlicenses([\"restricted\"])\r\n\r\npackage(default_visibility = [\"//visibility:public\"])\r\n\r\nload(\"@bazel_tools//tools/python:toolchain.bzl\", \"py_runtime_pair\")\r\n\r\npy_runtime(\r\n    name = \"py2_runtime\",\r\n    interpreter_path = \"/home/ros/tfenv/bin/python3\",\r\n    python_version = \"PY2\",\r\n)\r\n\r\npy_runtime(\r\n    name = \"py3_runtime\",\r\n    interpreter_path = \"/home/ros/tfenv/bin/python3\",\r\n    python_version = \"PY3\",\r\n)\r\n\r\npy_runtime_pair(\r\n    name = \"py_runtime_pair\",\r\n    py2_runtime = \":py2_runtime\",\r\n    py3_runtime = \":py3_runtime\",\r\n)\r\n\r\ntoolchain(\r\n    name = \"py_toolchain\",\r\n    toolchain = \":py_runtime_pair\",\r\n    toolchain_type = \"@bazel_tools//tools/python:toolchain_type\",\r\n    target_compatible_with = [],\r\n    exec_compatible_with = [],\r\n)\r\n\r\ncc_import(\r\n    name = \"python_lib\",\r\n    interface_library = select({\r\n        \":amd64\": \":python_import_lib\",\r\n        # A placeholder for Unix platforms which makes --no_build happy.\r\n        \"//conditions:default\": \"not-existing.lib\",\r\n    }),\r\n    system_provided = 1,\r\n)\r\n\r\ncc_library(\r\n    name = \"python_headers\",\r\n    hdrs = [\":python_include\"],\r\n    deps = select({\r\n        \":amd64\": [\":python_lib\"],\r\n        \"//conditions:default\": [],\r\n    }),\r\n    includes = [\"python_include\"],\r\n)\r\n\r\n\r\nalias(\r\n    name = \"headers\",\r\n    actual = \":python_headers\",\r\n)\r\n\r\ncc_library(\r\n    name = \"numpy_headers\",\r\n    hdrs = [\":numpy_include\"],\r\n    includes = [\"numpy_include\"],\r\n)\r\n\r\nconfig_setting(\r\n    name = \"amd64\",\r\n    values = {\"cpu\": \"amd64\"},\r\n    visibility = [\"//visibility:public\"],\r\n)\r\n\r\ngenrule(\r\n    name = \"python_include\",\r\n    outs = [\r\n        \"*.*\"\r\n    ],\r\n    cmd = \"\"\"\r\n\r\n   \"\"\",\r\n)\r\n\r\ngenrule(\r\n    name = \"numpy_include\",\r\n    outs = [\r\n        \"numpy_include/numpy/__multiarray_api.h\",\r\n        \"numpy_include/numpy/__ufunc_api.h\",\r\n        \"numpy_include/numpy/_neighborhood_iterator_imp.h\",\r\n        \"numpy_include/numpy/_numpyconfig.h\",\r\n        \"numpy_include/numpy/arrayobject.h\",\r\n        \"numpy_include/numpy/arrayscalars.h\",\r\n        \"numpy_include/numpy/halffloat.h\",\r\n        \"numpy_include/numpy/libdivide/LICENSE.txt\",\r\n        \"numpy_include/numpy/libdivide/libdivide.h\",\r\n        \"numpy_include/numpy/multiarray_api.txt\",\r\n        \"numpy_include/numpy/ndarrayobject.h\",\r\n        \"numpy_include/numpy/ndarraytypes.h\",\r\n        \"numpy_include/numpy/noprefix.h\",\r\n        \"numpy_include/numpy/npy_1_7_deprecated_api.h\",\r\n        \"numpy_include/numpy/npy_3kcompat.h\",\r\n        \"numpy_include/numpy/npy_common.h\",\r\n        \"numpy_include/numpy/npy_cpu.h\",\r\n        \"numpy_include/numpy/npy_endian.h\",\r\n        \"numpy_include/numpy/npy_interrupt.h\",\r\n        \"numpy_include/numpy/npy_math.h\",\r\n        \"numpy_include/numpy/npy_no_deprecated_api.h\",\r\n        \"numpy_include/numpy/npy_os.h\",\r\n        \"numpy_include/numpy/numpyconfig.h\",\r\n        \"numpy_include/numpy/old_defines.h\",\r\n        \"numpy_include/numpy/oldnumeric.h\",\r\n        \"numpy_include/numpy/random/bitgen.h\",\r\n        \"numpy_include/numpy/random/distributions.h\",\r\n        \"numpy_include/numpy/ufunc_api.txt\",\r\n        \"numpy_include/numpy/ufuncobject.h\",\r\n        \"numpy_include/numpy/utils.h\",\r\n    ],\r\n    cmd = \"\"\"\r\ncp -f \"/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/__multiarray_api.h\" \"$(@D)/numpy_include/numpy/__multiarray_api.h\" && cp -f \"/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/__ufunc_api.h\" \"$(@D)/numpy_include/numpy/__ufunc_api.h\" && cp -f \"/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/_neighborhood_iterator_imp.h\" \"$(@D)/numpy_include/numpy/_neighborhood_iterator_imp.h\" && cp -f \"/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/_numpyconfig.h\" \"$(@D)/numpy_include/numpy/_numpyconfig.h\" && cp -f \"/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/arrayobject.h\" \"$(@D)/numpy_include/numpy/arrayobject.h\" && cp -f \"/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/arrayscalars.h\" \"$(@D)/numpy_include/numpy/arrayscalars.h\" && cp -f \"/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/halffloat.h\" \"$(@D)/numpy_include/numpy/halffloat.h\" && cp -f \"/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/libdivide/LICENSE.txt\" \"$(@D)/numpy_include/numpy/libdivide/LICENSE.txt\" && cp -f \"/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/libdivide/libdivide.h\" \"$(@D)/numpy_include/numpy/libdivide/libdivide.h\" && cp -f \"/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/multiarray_api.txt\" \"$(@D)/numpy_include/numpy/multiarray_api.txt\" && cp -f \"/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/ndarrayobject.h\" \"$(@D)/numpy_include/numpy/ndarrayobject.h\" && cp -f \"/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/ndarraytypes.h\" \"$(@D)/numpy_include/numpy/ndarraytypes.h\" && cp -f \"/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/noprefix.h\" \"$(@D)/numpy_include/numpy/noprefix.h\" && cp -f \"/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h\" \"$(@D)/numpy_include/numpy/npy_1_7_deprecated_api.h\" && cp -f \"/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/npy_3kcompat.h\" \"$(@D)/numpy_include/numpy/npy_3kcompat.h\" && cp -f \"/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/npy_common.h\" \"$(@D)/numpy_include/numpy/npy_common.h\" && cp -f \"/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/npy_cpu.h\" \"$(@D)/numpy_include/numpy/npy_cpu.h\" && cp -f \"/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/npy_endian.h\" \"$(@D)/numpy_include/numpy/npy_endian.h\" && cp -f \"/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/npy_interrupt.h\" \"$(@D)/numpy_include/numpy/npy_interrupt.h\" && cp -f \"/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/npy_math.h\" \"$(@D)/numpy_include/numpy/npy_math.h\" && cp -f \"/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/npy_no_deprecated_api.h\" \"$(@D)/numpy_include/numpy/npy_no_deprecated_api.h\" && cp -f \"/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/npy_os.h\" \"$(@D)/numpy_include/numpy/npy_os.h\" && cp -f \"/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/numpyconfig.h\" \"$(@D)/numpy_include/numpy/numpyconfig.h\" && cp -f \"/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/old_defines.h\" \"$(@D)/numpy_include/numpy/old_defines.h\" && cp -f \"/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/oldnumeric.h\" \"$(@D)/numpy_include/numpy/oldnumeric.h\" && cp -f \"/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/random/bitgen.h\" \"$(@D)/numpy_include/numpy/random/bitgen.h\" && cp -f \"/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/random/distributions.h\" \"$(@D)/numpy_include/numpy/random/distributions.h\" && cp -f \"/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/ufunc_api.txt\" \"$(@D)/numpy_include/numpy/ufunc_api.txt\" && cp -f \"/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/ufuncobject.h\" \"$(@D)/numpy_include/numpy/ufuncobject.h\" && cp -f \"/home/ros/tfenv/lib/python3.8/site-packages/numpy/core/include/numpy/utils.h\" \"$(@D)/numpy_include/numpy/utils.h\"\r\n   \"\"\",\r\n)\r\n```", "comments": ["Hi @sanatmpa1! Could you please look in this issue?", "@lenkei-mark,\r\n\r\nI noticed that you're using `CUDA 11.5`, whereas the tested build configuration supports `CUDA 11.2` for `TF 2.7.0`. Can you take a look at this [tested build configurations](https://www.tensorflow.org/install/source#gpu) and modify your enivronment accordingly. Let us know if the same issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53407\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53407\">No</a>\n"]}, {"number": 53406, "title": "throw exception :TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float64 of argument 'x'. when tf.keras.backend.set_floatx(\"float64\")", "body": "**my code**\r\n``` python\r\nimport tensorflow as tf\r\ntf.keras.backend.set_floatx(\"float64\")\r\n.....\r\nmodel.fit(.....)\r\n```\r\nthe tf version is `v2.6.1-9-gc2363d6d025 2.6.2`\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `model.fit`\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Ubuntu 16.04`\r\n- TensorFlow installed from (source or binary): `docker`\r\n- TensorFlow version (use command below): `2.6.2`\r\n- Python version: 3.6.9\r\n\r\n**throw exception:**\r\n```\r\n/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:853 train_function  *\r\n        return step_function(self, iterator)\r\n    /usr/local/lib/python3.6/dist-packages/keras/engine/training.py:842 step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/parameter_server_strategy_v2.py:863 _call_for_each_replica\r\n        args, kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/mirrored_run.py:104 call_for_each_replica\r\n        return _call_for_each_replica(strategy, fn, args, kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/mirrored_run.py:246 _call_for_each_replica\r\n        coord.join(threads)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/coordinator.py:389 join\r\n        six.reraise(*self._exc_info_to_raise)\r\n    /usr/local/lib/python3.6/dist-packages/six.py:703 reraise\r\n        raise value\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/coordinator.py:297 stop_on_exception\r\n        yield\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/mirrored_run.py:346 run\r\n        self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/keras/engine/training.py:835 run_step  **\r\n        outputs = model.train_step(data)\r\n    /usr/local/lib/python3.6/dist-packages/keras/engine/training.py:792 train_step\r\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\r\n    /usr/local/lib/python3.6/dist-packages/keras/engine/compile_utils.py:457 update_state\r\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\r\n    /usr/local/lib/python3.6/dist-packages/keras/utils/metrics_utils.py:73 decorated\r\n        update_op = update_state_fn(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/keras/metrics.py:177 update_state_fn\r\n        return ag_update_state(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/keras/metrics.py:2292 update_state  **\r\n        label_weights=label_weights)\r\n    /usr/local/lib/python3.6/dist-packages/keras/utils/metrics_utils.py:636 update_confusion_matrix_variables\r\n        thresholds_with_epsilon=thresholds_with_epsilon)\r\n    /usr/local/lib/python3.6/dist-packages/keras/utils/metrics_utils.py:391 _update_confusion_matrix_variables_optimized\r\n        true_labels = tf.multiply(y_true, weights)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper\r\n        return target(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:530 multiply\r\n        return gen_math_ops.mul(x, y, name)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:6246 mul\r\n        \"Mul\", x=x, y=y, name=name)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:558 _apply_op_helper\r\n        inferred_from[input_arg.type_attr]))\r\n\r\n    TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float64 of argument 'x'.\r\n```\r\n**Describe the expected behavior**\r\n\r\n- Do you want to contribute a PR? (yes/no): `yes`\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/c2363d6d025981c661f8cbecf4c73ca7fbf38caf/tensorflow/python/keras/utils/metrics_utils.py#L379-L413\r\n\r\ncast the `weights` to y_true.dtype\u3002\r\n``` python\r\nweights = tf.cast(weights, dtype=y_true.dtype)\r\ntrue_labels = math_ops.multiply(y_true, weights)\r\nfalse_labels = math_ops.multiply((1.0 - y_true), weights)\r\n```", "comments": ["@shipoyewu \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53406\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53406\">No</a>\n"]}, {"number": 53405, "title": "throw exception :TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float64 of argument 'x'.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@shipoyewu ,\r\nCan you please take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/49460#issuecomment-845923088) from the issue and [SO link](https://stackoverflow.com/questions/36210887/how-to-fix-matmul-op-has-type-float64-that-does-not-match-type-float32-typeerror) with the similar error.It helps.Thanks!", "\u60a8\u597d\uff0c\u60a8\u7684\u90ae\u4ef6\u6211\u5df2\u6536\u5230!\u795d\uff1a\u5de5\u4f5c\u5546\u797a!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53405\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53405\">No</a>\n"]}, {"number": 53404, "title": "ERROR: Node number 27 (AVERAGE_POOL_2D) failed to invoke", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Android10\r\n- TensorFlow installed from (source or binary):source \r\n- TensorFlow version (or github SHA if from source):github tensorflow-r.2.3.1\r\n\r\nAll the way,\r\nthe sample label_image and the sample model  test on Ubuntu18.04 is OK ,and On Android use NNAPI is OK .\r\nbut **use tflite to  test on Android10 is wrong**  .\r\n\r\n**Provide the text output from tflite_convert**\r\n**when Label_image run mobilenet_v1_1.0_224_quant.tflite on Android ,AVERAGE_POOL_2D failed to invoke**\r\n**label_image**: tensorflow\\lite\\examples\\label_image\\label_image.cc\r\n**mobilenet_v1_1.0_224_quant**.tflite:http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_1.0_224_quant.tgz\r\n```\r\n# Copy and paste here\r\nud710_2h10:/data/local/tmp/android-test # **./label_image_1210   -m model/tf_mobilenet_v1_1.0_224_quant.tflite    -i data/grace_hopper.bmp -l label/labels.txt -a 0 -d 0 -f 0 -g 0  -b 0 -s 1  -p 1  -v 1**\r\nLoaded model model/tf_mobilenet_v1_1.0_224_quant.tflite\r\nresolved reporter\r\nINFO: Initialized TensorFlow Lite runtime.\r\nload tflite mode\r\naverage time: 1.559 ms\r\ntensors size: 89\r\nnodes size: 31\r\ninputs: 1\r\ninput(0) name: input\r\n0: MobilenetV1/Logits/AvgPool_1a/AvgPool, 1024, 3, 0.0235285, 0\r\n1: MobilenetV1/Logits/Conv2d_1c_1x1/BiasAdd, 1001, 3, 0.166099, 66\r\n2: MobilenetV1/Logits/Conv2d_1c_1x1/Conv2D_bias, 4004, 2, 0.000117327, 0\r\n3: MobilenetV1/Logits/Conv2d_1c_1x1/weights_quant/FakeQuantWithMinMaxVars, 1025024, 3, 0.0049866, 74\r\n4: MobilenetV1/Logits/SpatialSqueeze, 1001, 3, 0.166099, 66\r\n5: MobilenetV1/Logits/SpatialSqueeze_shape, 8, 2, 0, 0\r\n6: MobilenetV1/MobilenetV1/Conv2d_0/Conv2D_Fold_bias, 128, 2, 0.000170521, 0\r\n7: MobilenetV1/MobilenetV1/Conv2d_0/Relu6, 401408, 3, 0.0235285, 0\r\n8: MobilenetV1/MobilenetV1/Conv2d_0/weights_quant/FakeQuantWithMinMaxVars, 864, 3, 0.0218267, 151\r\n9: MobilenetV1/MobilenetV1/Conv2d_10_depthwise/Relu6, 100352, 3, 0.0235285, 0\r\n10: MobilenetV1/MobilenetV1/Conv2d_10_depthwise/depthwise_Fold_bias, 2048, 2, 0.000572435, 0\r\n11: MobilenetV1/MobilenetV1/Conv2d_10_depthwise/weights_quant/FakeQuantWithMinMaxVars, 4608, 3, 0.0243294, 134\r\n12: MobilenetV1/MobilenetV1/Conv2d_10_pointwise/Conv2D_Fold_bias, 2048, 2, 0.000227253, 0\r\n13: MobilenetV1/MobilenetV1/Conv2d_10_pointwise/Relu6, 100352, 3, 0.0235285, 0\r\n14: MobilenetV1/MobilenetV1/Conv2d_10_pointwise/weights_quant/FakeQuantWithMinMaxVars, 262144, 3, 0.00965865, 99\r\n15: MobilenetV1/MobilenetV1/Conv2d_11_depthwise/Relu6, 100352, 3, 0.0235285, 0\r\n16: MobilenetV1/MobilenetV1/Conv2d_11_depthwise/depthwise_Fold_bias, 2048, 2, 0.000455672, 0\r\n17: MobilenetV1/MobilenetV1/Conv2d_11_depthwise/weights_quant/FakeQuantWithMinMaxVars, 4608, 3, 0.0193668, 106\r\n18: MobilenetV1/MobilenetV1/Conv2d_11_pointwise/Conv2D_Fold_bias, 2048, 2, 0.000128159, 0\r\n19: MobilenetV1/MobilenetV1/Conv2d_11_pointwise/Relu6, 100352, 3, 0.0235285, 0\r\n20: MobilenetV1/MobilenetV1/Conv2d_11_pointwise/weights_quant/FakeQuantWithMinMaxVars, 262144, 3, 0.00544699, 153\r\n21: MobilenetV1/MobilenetV1/Conv2d_12_depthwise/Relu6, 25088, 3, 0.0235285, 0\r\n22: MobilenetV1/MobilenetV1/Conv2d_12_depthwise/depthwise_Fold_bias, 2048, 2, 0.00018436, 0\r\n23: MobilenetV1/MobilenetV1/Conv2d_12_depthwise/weights_quant/FakeQuantWithMinMaxVars, 4608, 3, 0.00783559, 126\r\n24: MobilenetV1/MobilenetV1/Conv2d_12_pointwise/Conv2D_Fold_bias, 4096, 2, 0.000192445, 0\r\n25: MobilenetV1/MobilenetV1/Conv2d_12_pointwise/Relu6, 50176, 3, 0.0235285, 0\r\n26: MobilenetV1/MobilenetV1/Conv2d_12_pointwise/weights_quant/FakeQuantWithMinMaxVars, 524288, 3, 0.00817923, 130\r\n27: MobilenetV1/MobilenetV1/Conv2d_13_depthwise/Relu6, 50176, 3, 0.0235285, 0\r\n28: MobilenetV1/MobilenetV1/Conv2d_13_depthwise/depthwise_Fold_bias, 4096, 2, 0.00296857, 0\r\n29: MobilenetV1/MobilenetV1/Conv2d_13_depthwise/weights_quant/FakeQuantWithMinMaxVars, 9216, 3, 0.126169, 211\r\n30: MobilenetV1/MobilenetV1/Conv2d_13_pointwise/Conv2D_Fold_bias, 4096, 2, 0.000424646, 0\r\n31: MobilenetV1/MobilenetV1/Conv2d_13_pointwise/Relu6, 50176, 3, 0.0235285, 0\r\n32: MobilenetV1/MobilenetV1/Conv2d_13_pointwise/weights_quant/FakeQuantWithMinMaxVars, 1048576, 3, 0.0180482, 95\r\n33: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/Relu6, 401408, 3, 0.0235285, 0\r\n34: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise_Fold_bias, 128, 2, 0.006875, 0\r\n35: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/weights_quant/FakeQuantWithMinMaxVars, 288, 3, 0.292199, 110\r\n36: MobilenetV1/MobilenetV1/Conv2d_1_pointwise/Conv2D_Fold_bias, 256, 2, 0.000715759, 0\r\n37: MobilenetV1/MobilenetV1/Conv2d_1_pointwise/Relu6, 802816, 3, 0.0235285, 0\r\n38: MobilenetV1/MobilenetV1/Conv2d_1_pointwise/weights_quant/FakeQuantWithMinMaxVars, 2048, 3, 0.0304209, 121\r\n39: MobilenetV1/MobilenetV1/Conv2d_2_depthwise/Relu6, 200704, 3, 0.0235285, 0\r\n40: MobilenetV1/MobilenetV1/Conv2d_2_depthwise/depthwise_Fold_bias, 256, 2, 0.00947663, 0\r\n41: MobilenetV1/MobilenetV1/Conv2d_2_depthwise/weights_quant/FakeQuantWithMinMaxVars, 576, 3, 0.402773, 130\r\n42: MobilenetV1/MobilenetV1/Conv2d_2_pointwise/Conv2D_Fold_bias, 512, 2, 0.000356414, 0\r\n43: MobilenetV1/MobilenetV1/Conv2d_2_pointwise/Relu6, 401408, 3, 0.0235285, 0\r\n44: MobilenetV1/MobilenetV1/Conv2d_2_pointwise/weights_quant/FakeQuantWithMinMaxVars, 8192, 3, 0.0151482, 104\r\n45: MobilenetV1/MobilenetV1/Conv2d_3_depthwise/Relu6, 401408, 3, 0.0235285, 0\r\n46: MobilenetV1/MobilenetV1/Conv2d_3_depthwise/depthwise_Fold_bias, 512, 2, 0.00142435, 0\r\n47: MobilenetV1/MobilenetV1/Conv2d_3_depthwise/weights_quant/FakeQuantWithMinMaxVars, 1152, 3, 0.0605373, 160\r\n48: MobilenetV1/MobilenetV1/Conv2d_3_pointwise/Conv2D_Fold_bias, 512, 2, 0.000323645, 0\r\n49: MobilenetV1/MobilenetV1/Conv2d_3_pointwise/Relu6, 401408, 3, 0.0235285, 0\r\n50: MobilenetV1/MobilenetV1/Conv2d_3_pointwise/weights_quant/FakeQuantWithMinMaxVars, 16384, 3, 0.0137555, 94\r\n51: MobilenetV1/MobilenetV1/Conv2d_4_depthwise/Relu6, 100352, 3, 0.0235285, 0\r\n52: MobilenetV1/MobilenetV1/Conv2d_4_depthwise/depthwise_Fold_bias, 512, 2, 0.000394292, 0\r\n53: MobilenetV1/MobilenetV1/Conv2d_4_depthwise/weights_quant/FakeQuantWithMinMaxVars, 1152, 3, 0.0167581, 123\r\n54: MobilenetV1/MobilenetV1/Conv2d_4_pointwise/Conv2D_Fold_bias, 1024, 2, 0.00017886, 0\r\n55: MobilenetV1/MobilenetV1/Conv2d_4_pointwise/Relu6, 200704, 3, 0.0235285, 0\r\n56: MobilenetV1/MobilenetV1/Conv2d_4_pointwise/weights_quant/FakeQuantWithMinMaxVars, 32768, 3, 0.00760185, 151\r\n57: MobilenetV1/MobilenetV1/Conv2d_5_depthwise/Relu6, 200704, 3, 0.0235285, 0\r\n58: MobilenetV1/MobilenetV1/Conv2d_5_depthwise/depthwise_Fold_bias, 1024, 2, 0.000965968, 0\r\n59: MobilenetV1/MobilenetV1/Conv2d_5_depthwise/weights_quant/FakeQuantWithMinMaxVars, 2304, 3, 0.0410553, 129\r\n60: MobilenetV1/MobilenetV1/Conv2d_5_pointwise/Conv2D_Fold_bias, 1024, 2, 0.000151326, 0\r\n61: MobilenetV1/MobilenetV1/Conv2d_5_pointwise/Relu6, 200704, 3, 0.0235285, 0\r\n62: MobilenetV1/MobilenetV1/Conv2d_5_pointwise/weights_quant/FakeQuantWithMinMaxVars, 65536, 3, 0.00643161, 122\r\n63: MobilenetV1/MobilenetV1/Conv2d_6_depthwise/Relu6, 50176, 3, 0.0235285, 0\r\n64: MobilenetV1/MobilenetV1/Conv2d_6_depthwise/depthwise_Fold_bias, 1024, 2, 0.000316712, 0\r\n65: MobilenetV1/MobilenetV1/Conv2d_6_depthwise/weights_quant/FakeQuantWithMinMaxVars, 2304, 3, 0.0134608, 122\r\n66: MobilenetV1/MobilenetV1/Conv2d_6_pointwise/Conv2D_Fold_bias, 2048, 2, 0.000215785, 0\r\n67: MobilenetV1/MobilenetV1/Conv2d_6_pointwise/Relu6, 100352, 3, 0.0235285, 0\r\n68: MobilenetV1/MobilenetV1/Conv2d_6_pointwise/weights_quant/FakeQuantWithMinMaxVars, 131072, 3, 0.00917122, 109\r\n69: MobilenetV1/MobilenetV1/Conv2d_7_depthwise/Relu6, 100352, 3, 0.0235285, 0\r\n70: MobilenetV1/MobilenetV1/Conv2d_7_depthwise/depthwise_Fold_bias, 2048, 2, 0.000869019, 0\r\n71: MobilenetV1/MobilenetV1/Conv2d_7_depthwise/weights_quant/FakeQuantWithMinMaxVars, 4608, 3, 0.0369348, 132\r\n72: MobilenetV1/MobilenetV1/Conv2d_7_pointwise/Conv2D_Fold_bias, 2048, 2, 0.000124702, 0\r\n73: MobilenetV1/MobilenetV1/Conv2d_7_pointwise/Relu6, 100352, 3, 0.0235285, 0\r\n74: MobilenetV1/MobilenetV1/Conv2d_7_pointwise/weights_quant/FakeQuantWithMinMaxVars, 262144, 3, 0.00530005, 140\r\n75: MobilenetV1/MobilenetV1/Conv2d_8_depthwise/Relu6, 100352, 3, 0.0235285, 0\r\n76: MobilenetV1/MobilenetV1/Conv2d_8_depthwise/depthwise_Fold_bias, 2048, 2, 0.00100255, 0\r\n77: MobilenetV1/MobilenetV1/Conv2d_8_depthwise/weights_quant/FakeQuantWithMinMaxVars, 4608, 3, 0.0426099, 94\r\n78: MobilenetV1/MobilenetV1/Conv2d_8_pointwise/Conv2D_Fold_bias, 2048, 2, 0.000116779, 0\r\n79: MobilenetV1/MobilenetV1/Conv2d_8_pointwise/Relu6, 100352, 3, 0.0235285, 0\r\n80: MobilenetV1/MobilenetV1/Conv2d_8_pointwise/weights_quant/FakeQuantWithMinMaxVars, 262144, 3, 0.00496329, 127\r\n81: MobilenetV1/MobilenetV1/Conv2d_9_depthwise/Relu6, 100352, 3, 0.0235285, 0\r\n82: MobilenetV1/MobilenetV1/Conv2d_9_depthwise/depthwise_Fold_bias, 2048, 2, 0.000667241, 0\r\n83: MobilenetV1/MobilenetV1/Conv2d_9_depthwise/weights_quant/FakeQuantWithMinMaxVars, 4608, 3, 0.0283589, 127\r\n84: MobilenetV1/MobilenetV1/Conv2d_9_pointwise/Conv2D_Fold_bias, 2048, 2, 0.000182837, 0\r\n85: MobilenetV1/MobilenetV1/Conv2d_9_pointwise/Relu6, 100352, 3, 0.0235285, 0\r\n86: MobilenetV1/MobilenetV1/Conv2d_9_pointwise/weights_quant/FakeQuantWithMinMaxVars, 262144, 3, 0.0077709, 89\r\n87: MobilenetV1/Predictions/Reshape_1, 1001, 3, 0.00390625, 0\r\n88: input, 150528, 3, 0.0078125, 128\r\nlen: 940650\r\nwidth, height, channels: 517, 606, 3\r\ninput: 88\r\nnumber of inputs: 1\r\nnumber of outputs: 1\r\nInterpreter has 90 tensors and 31 nodes\r\nInputs: 88\r\nOutputs: 87\r\n\r\nTensor   0 MobilenetV1/Logits/AvgPool_1a/AvgPool kTfLiteUInt8  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 1 1 1024\r\nTensor   1 MobilenetV1/Logits/Conv2d_1c_1x1/BiasAdd kTfLiteUInt8  kTfLiteArenaRw       1001 bytes ( 0.0 MB)  1 1 1 1001\r\nTensor   2 MobilenetV1/Logits/Conv2d_1c_1x1/Conv2D_bias kTfLiteInt32   kTfLiteMmapRo       4004 bytes ( 0.0 MB)  1001\r\nTensor   3 MobilenetV1/Logits/Conv2d_1c_1x1/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo    1025024 bytes ( 1.0 MB)  1001 1 1 1024\r\nTensor   4 MobilenetV1/Logits/SpatialSqueeze kTfLiteUInt8  kTfLiteArenaRw       1001 bytes ( 0.0 MB)  1 1001\r\nTensor   5 MobilenetV1/Logits/SpatialSqueeze_shape kTfLiteInt32   kTfLiteMmapRo          8 bytes ( 0.0 MB)  2\r\nTensor   6 MobilenetV1/MobilenetV1/Conv2d_0/Conv2D_Fold_bias kTfLiteInt32   kTfLiteMmapRo        128 bytes ( 0.0 MB)  32\r\nTensor   7 MobilenetV1/MobilenetV1/Conv2d_0/Relu6 kTfLiteUInt8  kTfLiteArenaRw     401408 bytes ( 0.4 MB)  1 112 112 32\r\nTensor   8 MobilenetV1/MobilenetV1/Conv2d_0/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo        864 bytes ( 0.0 MB)  32 3 3 3\r\nTensor   9 MobilenetV1/MobilenetV1/Conv2d_10_depthwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     100352 bytes ( 0.1 MB)  1 14 14 512\r\nTensor  10 MobilenetV1/MobilenetV1/Conv2d_10_depthwise/depthwise_Fold_bias kTfLiteInt32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  11 MobilenetV1/MobilenetV1/Conv2d_10_depthwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo       4608 bytes ( 0.0 MB)  1 3 3 512\r\nTensor  12 MobilenetV1/MobilenetV1/Conv2d_10_pointwise/Conv2D_Fold_bias kTfLiteInt32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  13 MobilenetV1/MobilenetV1/Conv2d_10_pointwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     100352 bytes ( 0.1 MB)  1 14 14 512\r\nTensor  14 MobilenetV1/MobilenetV1/Conv2d_10_pointwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo     262144 bytes ( 0.2 MB)  512 1 1 512\r\nTensor  15 MobilenetV1/MobilenetV1/Conv2d_11_depthwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     100352 bytes ( 0.1 MB)  1 14 14 512\r\nTensor  16 MobilenetV1/MobilenetV1/Conv2d_11_depthwise/depthwise_Fold_bias kTfLiteInt32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  17 MobilenetV1/MobilenetV1/Conv2d_11_depthwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo       4608 bytes ( 0.0 MB)  1 3 3 512\r\nTensor  18 MobilenetV1/MobilenetV1/Conv2d_11_pointwise/Conv2D_Fold_bias kTfLiteInt32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  19 MobilenetV1/MobilenetV1/Conv2d_11_pointwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     100352 bytes ( 0.1 MB)  1 14 14 512\r\nTensor  20 MobilenetV1/MobilenetV1/Conv2d_11_pointwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo     262144 bytes ( 0.2 MB)  512 1 1 512\r\nTensor  21 MobilenetV1/MobilenetV1/Conv2d_12_depthwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw      25088 bytes ( 0.0 MB)  1 7 7 512\r\nTensor  22 MobilenetV1/MobilenetV1/Conv2d_12_depthwise/depthwise_Fold_bias kTfLiteInt32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  23 MobilenetV1/MobilenetV1/Conv2d_12_depthwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo       4608 bytes ( 0.0 MB)  1 3 3 512\r\nTensor  24 MobilenetV1/MobilenetV1/Conv2d_12_pointwise/Conv2D_Fold_bias kTfLiteInt32   kTfLiteMmapRo       4096 bytes ( 0.0 MB)  1024\r\nTensor  25 MobilenetV1/MobilenetV1/Conv2d_12_pointwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw      50176 bytes ( 0.0 MB)  1 7 7 1024\r\nTensor  26 MobilenetV1/MobilenetV1/Conv2d_12_pointwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo     524288 bytes ( 0.5 MB)  1024 1 1 512\r\nTensor  27 MobilenetV1/MobilenetV1/Conv2d_13_depthwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw      50176 bytes ( 0.0 MB)  1 7 7 1024\r\nTensor  28 MobilenetV1/MobilenetV1/Conv2d_13_depthwise/depthwise_Fold_bias kTfLiteInt32   kTfLiteMmapRo       4096 bytes ( 0.0 MB)  1024\r\nTensor  29 MobilenetV1/MobilenetV1/Conv2d_13_depthwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo       9216 bytes ( 0.0 MB)  1 3 3 1024\r\nTensor  30 MobilenetV1/MobilenetV1/Conv2d_13_pointwise/Conv2D_Fold_bias kTfLiteInt32   kTfLiteMmapRo       4096 bytes ( 0.0 MB)  1024\r\nTensor  31 MobilenetV1/MobilenetV1/Conv2d_13_pointwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw      50176 bytes ( 0.0 MB)  1 7 7 1024\r\nTensor  32 MobilenetV1/MobilenetV1/Conv2d_13_pointwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo    1048576 bytes ( 1.0 MB)  1024 1 1 1024\r\nTensor  33 MobilenetV1/MobilenetV1/Conv2d_1_depthwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     401408 bytes ( 0.4 MB)  1 112 112 32\r\nTensor  34 MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise_Fold_bias kTfLiteInt32   kTfLiteMmapRo        128 bytes ( 0.0 MB)  32\r\nTensor  35 MobilenetV1/MobilenetV1/Conv2d_1_depthwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo        288 bytes ( 0.0 MB)  1 3 3 32\r\nTensor  36 MobilenetV1/MobilenetV1/Conv2d_1_pointwise/Conv2D_Fold_bias kTfLiteInt32   kTfLiteMmapRo        256 bytes ( 0.0 MB)  64\r\nTensor  37 MobilenetV1/MobilenetV1/Conv2d_1_pointwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     802816 bytes ( 0.8 MB)  1 112 112 64\r\nTensor  38 MobilenetV1/MobilenetV1/Conv2d_1_pointwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  64 1 1 32\r\nTensor  39 MobilenetV1/MobilenetV1/Conv2d_2_depthwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     200704 bytes ( 0.2 MB)  1 56 56 64\r\nTensor  40 MobilenetV1/MobilenetV1/Conv2d_2_depthwise/depthwise_Fold_bias kTfLiteInt32   kTfLiteMmapRo        256 bytes ( 0.0 MB)  64\r\nTensor  41 MobilenetV1/MobilenetV1/Conv2d_2_depthwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo        576 bytes ( 0.0 MB)  1 3 3 64\r\nTensor  42 MobilenetV1/MobilenetV1/Conv2d_2_pointwise/Conv2D_Fold_bias kTfLiteInt32   kTfLiteMmapRo        512 bytes ( 0.0 MB)  128\r\nTensor  43 MobilenetV1/MobilenetV1/Conv2d_2_pointwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     401408 bytes ( 0.4 MB)  1 56 56 128\r\nTensor  44 MobilenetV1/MobilenetV1/Conv2d_2_pointwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo       8192 bytes ( 0.0 MB)  128 1 1 64\r\nTensor  45 MobilenetV1/MobilenetV1/Conv2d_3_depthwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     401408 bytes ( 0.4 MB)  1 56 56 128\r\nTensor  46 MobilenetV1/MobilenetV1/Conv2d_3_depthwise/depthwise_Fold_bias kTfLiteInt32   kTfLiteMmapRo        512 bytes ( 0.0 MB)  128\r\nTensor  47 MobilenetV1/MobilenetV1/Conv2d_3_depthwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo       1152 bytes ( 0.0 MB)  1 3 3 128\r\nTensor  48 MobilenetV1/MobilenetV1/Conv2d_3_pointwise/Conv2D_Fold_bias kTfLiteInt32   kTfLiteMmapRo        512 bytes ( 0.0 MB)  128\r\nTensor  49 MobilenetV1/MobilenetV1/Conv2d_3_pointwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     401408 bytes ( 0.4 MB)  1 56 56 128\r\nTensor  50 MobilenetV1/MobilenetV1/Conv2d_3_pointwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo      16384 bytes ( 0.0 MB)  128 1 1 128\r\nTensor  51 MobilenetV1/MobilenetV1/Conv2d_4_depthwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     100352 bytes ( 0.1 MB)  1 28 28 128\r\nTensor  52 MobilenetV1/MobilenetV1/Conv2d_4_depthwise/depthwise_Fold_bias kTfLiteInt32   kTfLiteMmapRo        512 bytes ( 0.0 MB)  128\r\nTensor  53 MobilenetV1/MobilenetV1/Conv2d_4_depthwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo       1152 bytes ( 0.0 MB)  1 3 3 128\r\nTensor  54 MobilenetV1/MobilenetV1/Conv2d_4_pointwise/Conv2D_Fold_bias kTfLiteInt32   kTfLiteMmapRo       1024 bytes ( 0.0 MB)  256\r\nTensor  55 MobilenetV1/MobilenetV1/Conv2d_4_pointwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     200704 bytes ( 0.2 MB)  1 28 28 256\r\nTensor  56 MobilenetV1/MobilenetV1/Conv2d_4_pointwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo      32768 bytes ( 0.0 MB)  256 1 1 128\r\nTensor  57 MobilenetV1/MobilenetV1/Conv2d_5_depthwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     200704 bytes ( 0.2 MB)  1 28 28 256\r\nTensor  58 MobilenetV1/MobilenetV1/Conv2d_5_depthwise/depthwise_Fold_bias kTfLiteInt32   kTfLiteMmapRo       1024 bytes ( 0.0 MB)  256\r\nTensor  59 MobilenetV1/MobilenetV1/Conv2d_5_depthwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo       2304 bytes ( 0.0 MB)  1 3 3 256\r\nTensor  60 MobilenetV1/MobilenetV1/Conv2d_5_pointwise/Conv2D_Fold_bias kTfLiteInt32   kTfLiteMmapRo       1024 bytes ( 0.0 MB)  256\r\nTensor  61 MobilenetV1/MobilenetV1/Conv2d_5_pointwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     200704 bytes ( 0.2 MB)  1 28 28 256\r\nTensor  62 MobilenetV1/MobilenetV1/Conv2d_5_pointwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo      65536 bytes ( 0.1 MB)  256 1 1 256\r\nTensor  63 MobilenetV1/MobilenetV1/Conv2d_6_depthwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw      50176 bytes ( 0.0 MB)  1 14 14 256\r\nTensor  64 MobilenetV1/MobilenetV1/Conv2d_6_depthwise/depthwise_Fold_bias kTfLiteInt32   kTfLiteMmapRo       1024 bytes ( 0.0 MB)  256\r\nTensor  65 MobilenetV1/MobilenetV1/Conv2d_6_depthwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo       2304 bytes ( 0.0 MB)  1 3 3 256\r\nTensor  66 MobilenetV1/MobilenetV1/Conv2d_6_pointwise/Conv2D_Fold_bias kTfLiteInt32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  67 MobilenetV1/MobilenetV1/Conv2d_6_pointwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     100352 bytes ( 0.1 MB)  1 14 14 512\r\nTensor  68 MobilenetV1/MobilenetV1/Conv2d_6_pointwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo     131072 bytes ( 0.1 MB)  512 1 1 256\r\nTensor  69 MobilenetV1/MobilenetV1/Conv2d_7_depthwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     100352 bytes ( 0.1 MB)  1 14 14 512\r\nTensor  70 MobilenetV1/MobilenetV1/Conv2d_7_depthwise/depthwise_Fold_bias kTfLiteInt32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  71 MobilenetV1/MobilenetV1/Conv2d_7_depthwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo       4608 bytes ( 0.0 MB)  1 3 3 512\r\nTensor  72 MobilenetV1/MobilenetV1/Conv2d_7_pointwise/Conv2D_Fold_bias kTfLiteInt32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  73 MobilenetV1/MobilenetV1/Conv2d_7_pointwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     100352 bytes ( 0.1 MB)  1 14 14 512\r\nTensor  74 MobilenetV1/MobilenetV1/Conv2d_7_pointwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo     262144 bytes ( 0.2 MB)  512 1 1 512\r\nTensor  75 MobilenetV1/MobilenetV1/Conv2d_8_depthwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     100352 bytes ( 0.1 MB)  1 14 14 512\r\nTensor  76 MobilenetV1/MobilenetV1/Conv2d_8_depthwise/depthwise_Fold_bias kTfLiteInt32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  77 MobilenetV1/MobilenetV1/Conv2d_8_depthwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo       4608 bytes ( 0.0 MB)  1 3 3 512\r\nTensor  78 MobilenetV1/MobilenetV1/Conv2d_8_pointwise/Conv2D_Fold_bias kTfLiteInt32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  79 MobilenetV1/MobilenetV1/Conv2d_8_pointwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     100352 bytes ( 0.1 MB)  1 14 14 512\r\nTensor  80 MobilenetV1/MobilenetV1/Conv2d_8_pointwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo     262144 bytes ( 0.2 MB)  512 1 1 512\r\nTensor  81 MobilenetV1/MobilenetV1/Conv2d_9_depthwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     100352 bytes ( 0.1 MB)  1 14 14 512\r\nTensor  82 MobilenetV1/MobilenetV1/Conv2d_9_depthwise/depthwise_Fold_bias kTfLiteInt32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  83 MobilenetV1/MobilenetV1/Conv2d_9_depthwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo       4608 bytes ( 0.0 MB)  1 3 3 512\r\nTensor  84 MobilenetV1/MobilenetV1/Conv2d_9_pointwise/Conv2D_Fold_bias kTfLiteInt32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  85 MobilenetV1/MobilenetV1/Conv2d_9_pointwise/Relu6 kTfLiteUInt8  kTfLiteArenaRw     100352 bytes ( 0.1 MB)  1 14 14 512\r\nTensor  86 MobilenetV1/MobilenetV1/Conv2d_9_pointwise/weights_quant/FakeQuantWithMinMaxVars kTfLiteUInt8   kTfLiteMmapRo     262144 bytes ( 0.2 MB)  512 1 1 512\r\nTensor  87 MobilenetV1/Predictions/Reshape_1 kTfLiteUInt8  kTfLiteArenaRw       1001 bytes ( 0.0 MB)  1 1001\r\nTensor  88 input                kTfLiteUInt8  kTfLiteArenaRw     150528 bytes ( 0.1 MB)  1 224 224 3\r\nTensor  89 (null)               kTfLiteUInt8  kTfLiteArenaRw     338688 bytes ( 0.3 MB)  1 112 112 27\r\n\r\nNode   0 Operator Builtin Code   3 CONV_2D\r\n  Inputs: 88 8 6\r\n  Outputs: 7\r\n  Temporaries: 89\r\nNode   1 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Inputs: 7 35 34\r\n  Outputs: 33\r\nNode   2 Operator Builtin Code   3 CONV_2D\r\n  Inputs: 33 38 36\r\n  Outputs: 37\r\nNode   3 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Inputs: 37 41 40\r\n  Outputs: 39\r\nNode   4 Operator Builtin Code   3 CONV_2D\r\n  Inputs: 39 44 42\r\n  Outputs: 43\r\nNode   5 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Inputs: 43 47 46\r\n  Outputs: 45\r\nNode   6 Operator Builtin Code   3 CONV_2D\r\n  Inputs: 45 50 48\r\n  Outputs: 49\r\nNode   7 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Inputs: 49 53 52\r\n  Outputs: 51\r\nNode   8 Operator Builtin Code   3 CONV_2D\r\n  Inputs: 51 56 54\r\n  Outputs: 55\r\nNode   9 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Inputs: 55 59 58\r\n  Outputs: 57\r\nNode  10 Operator Builtin Code   3 CONV_2D\r\n  Inputs: 57 62 60\r\n  Outputs: 61\r\nNode  11 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Inputs: 61 65 64\r\n  Outputs: 63\r\nNode  12 Operator Builtin Code   3 CONV_2D\r\n  Inputs: 63 68 66\r\n  Outputs: 67\r\nNode  13 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Inputs: 67 71 70\r\n  Outputs: 69\r\nNode  14 Operator Builtin Code   3 CONV_2D\r\n  Inputs: 69 74 72\r\n  Outputs: 73\r\nNode  15 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Inputs: 73 77 76\r\n  Outputs: 75\r\nNode  16 Operator Builtin Code   3 CONV_2D\r\n  Inputs: 75 80 78\r\n  Outputs: 79\r\nNode  17 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Inputs: 79 83 82\r\n  Outputs: 81\r\nNode  18 Operator Builtin Code   3 CONV_2D\r\n  Inputs: 81 86 84\r\n  Outputs: 85\r\nNode  19 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Inputs: 85 11 10\r\n  Outputs: 9\r\nNode  20 Operator Builtin Code   3 CONV_2D\r\n  Inputs: 9 14 12\r\n  Outputs: 13\r\nNode  21 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Inputs: 13 17 16\r\n  Outputs: 15\r\nNode  22 Operator Builtin Code   3 CONV_2D\r\n  Inputs: 15 20 18\r\n  Outputs: 19\r\nNode  23 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Inputs: 19 23 22\r\n  Outputs: 21\r\nNode  24 Operator Builtin Code   3 CONV_2D\r\n  Inputs: 21 26 24\r\n  Outputs: 25\r\nNode  25 Operator Builtin Code   4 DEPTHWISE_CONV_2D\r\n  Inputs: 25 29 28\r\n  Outputs: 27\r\nNode  26 Operator Builtin Code   3 CONV_2D\r\n  Inputs: 27 32 30\r\n  Outputs: 31\r\nNode  27 Operator Builtin Code   1 AVERAGE_POOL_2D\r\n  Inputs: 31\r\n  Outputs: 0\r\nNode  28 Operator Builtin Code   3 CONV_2D\r\n  Inputs: 0 3 2\r\n  Outputs: 1\r\nNode  29 Operator Builtin Code  22 RESHAPE\r\n  Inputs: 1 5\r\n  Outputs: 4\r\nNode  30 Operator Builtin Code  25 SOFTMAX\r\n  Inputs: 4\r\n  Outputs: 87\r\n**ERROR: tensorflow/lite/kernels/pooling.cc:173 optimized_ops::AveragePool(op_params, GetTensorShape(input), GetTensorData<uint8_t>(input), GetTensorShape(output), GetTensorData<uint8_t>(output)) was not true.\r\nERROR: Node number 27 (AVERAGE_POOL_2D) failed to invoke.**\r\n\r\nFailed to invoke tflite!\r\ninvoked\r\naverage time: 36.978 ms\r\n1: 941 941:spaghetti squash\r\n1: 924 924:plate\r\n1: 886 886:velvet\r\n1: 833 833:stupa, tope\r\n1: 649 649:medicine chest, medicine cabinet\r\n```\r\n\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n\r\n", "comments": ["@Niuxiaotong \r\nPlease refer to similar issues and let us know:#38285,#31739", "@Saduf2019 according to the link:\r\nhttps://github.com/tensorflow/tensorflow/issues/38285#issuecomment-789096556\r\nit means that now tensorflow lite only support input/output uint8 in inference tflite model,\r\n the middle layer such as AVG_POOL which is uint8 can't be supported?\r\n\r\nand others question ,tensorflow quantization model such as  mobilenet_v1_1.0_224_quant.tflite which be quantized int8 but input and output is uint8,right?", "> @Niuxiaotong Please refer to similar issues and let us know:#38285,#31739\r\n\r\n@Saduf2019 according to the link:\r\n#38285 (comment)\r\nit means that now tensorflow lite only support input/output uint8 in inference tflite model,\r\nthe middle layer such as AVG_POOL which is uint8 can't be supported?\r\n\r\nand others question ,tensorflow quantization model such as mobilenet_v1_1.0_224_quant.tflite which be quantized int8 but input and output is uint8,right?", "> > @Niuxiaotong Please refer to similar issues and let us know:#38285,#31739\r\n> \r\n> @Saduf2019 according to the link: #38285 (comment) it means that now tensorflow lite only support input/output uint8 in inference tflite model, the middle layer such as AVG_POOL which is uint8 can't be supported?\r\n> \r\n> and others question ,tensorflow quantization model such as mobilenet_v1_1.0_224_quant.tflite which be quantized int8 but input and output is uint8,right?\r\n\r\n@jvishnuvardhan what about this question?", "@Niuxiaotong Sorry for the late response. Can you please check with recent TF versions and let us know whether the issue persists with recent versions. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53404\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53404\">No</a>\n", "> @Niuxiaotong Sorry for the late response. Can you please check with recent TF versions and let us know whether the issue persists with recent versions. Thanks!\r\n\r\nsee system info, TensorFlow version (or github SHA if from source):github tensorflow-r.2.3.1", "@Niuxiaotong Can you try with recent TF versions like TF2.8 or `tf-nightly`? Thanks", "> @Niuxiaotong Can you try with recent TF versions like TF2.8 or `tf-nightly`? Thanks\r\nTF2.8 Compared with TF2.3, is there some new feature to support Uint8 in Midder layer such as AVG_POOL ?\r\n"]}, {"number": 53402, "title": "When \bI use   tensorflow_lite(1.13.1) it has report error  it looks ...", "body": "error tip : Model provided has model identifier 'AFRG', should be 'TFL3'\r\n\r\nI have model.tflite when I creat     \r\nmodel = tflite::FlatBufferModel::BuildFromBuffer(newmodel_data, modellength);\r\n\r\nHow Can I do?\r\n\r\n\r\n", "comments": ["@csdf-ssm ,\r\nWe see that you are using tf version 1.13, 1.x is not actively supported, please update to latest stable tf v2.7 and let us know if you are facing same issue.", "I had download  tensorflow.git and use demo in /tensorflow/tensorflow/lite/examples/ios/camera  directory ,run pod and it pod.lock \r\nPODS:\r\n  - TensorFlowLite (1.13.1)\r\n\r\nDEPENDENCIES:\r\n  - TensorFlowLite (= 1.13.1)\r\n\r\nSPEC REPOS:\r\n  trunk:\r\n    - TensorFlowLite\r\n\r\nSPEC CHECKSUMS:\r\n  TensorFlowLite: 8b9dc4eb32eac0f8cb660c66bca7604da56dcc5a\r\n\r\nPODFILE CHECKSUM: 1f44a2ab814725d3675508d4a66a5efdb4140d58\r\n\r\nCOCOAPODS: 1.11.2\r\n\r\n\r\nthen  I   check my pods version and  it shows last version is 1.13.1 \r\n-> TensorFlowLite (1.13.1)\r\n   TensorFlow Lite\r\n   pod 'TensorFlowLite', '~> 1.13.1'\r\n   - Homepage: https://www.tensorflow.org/lite/\r\n   - Source:  \r\n   https://dl.google.com/dl/cpdc/a3cc8a8fb2aec8f6/TensorFlowLite-1.13.1.tar.gz\r\n   - Versions: 1.13.1, 1.12.0, 1.11.0, 1.10.1, 1.10.0, 1.9.0, 0.1.7, 0.0.3,\r\n   0.0.2 [master repo]\r\n\r\n\r\n", "> @csdf-ssm , We see that you are using tf version 1.13, 1.x is not actively supported, please update to latest stable tf v2.7 and let us know if you are using same issue.\r\n\r\n@tilakrayal   \r\nI had download tensorflow.git and use demo in /tensorflow/tensorflow/lite/examples/ios/camera directory ,run pod and it pod.lock\r\nPODS:\r\n\r\nTensorFlowLite (1.13.1)\r\nDEPENDENCIES:\r\n\r\nTensorFlowLite (= 1.13.1)\r\nSPEC REPOS:\r\ntrunk:\r\n- TensorFlowLite\r\n\r\nSPEC CHECKSUMS:\r\nTensorFlowLite: 8b9dc4eb32eac0f8cb660c66bca7604da56dcc5a\r\n\r\nPODFILE CHECKSUM: 1f44a2ab814725d3675508d4a66a5efdb4140d58\r\n\r\nCOCOAPODS: 1.11.2\r\n\r\nthen I check my pods version and it shows last version is 1.13.1\r\n-> TensorFlowLite (1.13.1)\r\nTensorFlow Lite\r\npod 'TensorFlowLite', '~> 1.13.1'\r\n\r\nHomepage: https://www.tensorflow.org/lite/\r\nSource:\r\nhttps://dl.google.com/dl/cpdc/a3cc8a8fb2aec8f6/TensorFlowLite-1.13.1.tar.gz\r\nVersions: 1.13.1, 1.12.0, 1.11.0, 1.10.1, 1.10.0, 1.9.0, 0.1.7, 0.0.3,\r\n0.0.2 [master repo]", "@tilakrayal  and I also download tensorflowSwift demo  ImageClassification run success  but I do not want  load model.tllite filepath to creat Model,You know I mean? show code:\r\n\r\n        interpreter = try Interpreter.init(modelPath: modelPath, options: options, delegates: nil)\r\nand Interpreter Class \r\n  public init(modelPath: String, options: Options? = nil, delegates: [Delegate]? = nil) throws {\r\n    **guard let model = Model(filePath: modelPath) else { throw InterpreterError.failedToLoadModel }**\r\n    guard let cInterpreterOptions = TfLiteInterpreterOptionsCreate() else {\r\n      throw InterpreterError.failedToCreateInterpreter\r\n    }\r\nand  in Model Class :\r\n   ///   - filePath: The local file path to a TensorFlow Lite model.\r\n  init?(filePath: String) {\r\n    **guard !filePath.isEmpty, let cModel = TfLiteModelCreateFromFile(filePath) else { return nil }**\r\n    self.cModel = cModel\r\n  }\r\nthis \r\n    **guard !filePath.isEmpty, let cModel = TfLiteModelCreateFromFile(filePath) else { return nil }**\r\n\r\n i want to use that method \r\n// Returns a model from the provided buffer, or null on failure.\r\n**TFL_CAPI_EXPORT extern TfLiteModel* TfLiteModelCreate(const void* model_data,\r\n                                                      size_t model_size);**\r\nbut  run  error  ,I do not know  where  I have make a mistake;\r\nin my demo  \r\n        let data =  try! Data.init(contentsOf: URL.init(fileURLWithPath: modelPath))\r\n        var bytes = [UInt8](data)\r\n        let unsafeMutableRawBufferPointer = bytes.withUnsafeMutableBytes { $0 }\r\n        let unsafeMutablePointer = unsafeMutableRawBufferPointer.baseAddress\r\n\r\n\r\nthis is my question ,sorry  The code is a mess,but  do you know what i mean? if you can ,can show me a demo to load model with  TfLiteModel* TfLiteModelCreate(const void* model_data,\r\n                                                      size_t model_size); \r\nThanks!", "@csdf-ssm ,\r\nAs commented tf v1.x is not supported  actively, please update to 2.x and let us know if you are facing same issue.Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53402\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53402\">No</a>\n"]}, {"number": 53401, "title": "AFRG", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Hi @csdf-ssm ! Could you elaborate the exact issue in above template? Thanks!"]}, {"number": 53400, "title": "00M report after I upgrade tensorflow for tf2.5 to tf2.7", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip install tensorflow\r\n- TensorFlow version (use command below): tf2.5 and tf 2.7\r\n- Python version: \r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:  CUDA11.2.2/cudnn8.1.0.77 for both tf2.5 and tf2.7 \r\n- GPU model and memory: NVIDIA RTX A6000, 43660 MB memory\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI upgrade tensorflow from version 2.5 to 2.7. The code that can run in tf2.5 enviroment cannot run in tf2.7 enviroment since when run in tf2.7, the GPU memory exhaust.\r\n**Describe the expected behavior**\r\nThe code can also run in tf2.7 without reporting OOM happens.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nThe link of code script is here   [https://colab.research.google.com/drive/1zb_SAqKZPGinzdWWl5CUySwzX5T_CatU?usp=sharing](url). It cannot reporduce the isuue in colab since the code needs about 50G RAM memory.\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n2021-12-13 13:04:10.364463: W tensorflow/core/common_runtime/bfc_allocator.cc:474] *************************************************************************************************___\r\n2021-12-13 13:04:10.364557: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at gather_nd_op.cc:47 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[772,753,768,1,1] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\nTraceback (most recent call last):\r\n  File \"/home/wangwei/T7/GitLab/incomplete-CT/fan-beam/fan_line_parker/incomplete_2_parker_line_150.py\", line 220, in <module>\r\n    train(epoch, batch, iternum, restore=0,\r\n  File \"/home/wangwei/T7/GitLab/incomplete-CT/fan-beam/fan_line_parker/incomplete_2_parker_line_150.py\", line 210, in train\r\n    hist = Model.fit(x, y, batch_size=batch, epochs=epoch, validation_split=0.1,\r\n  File \"/home/wangwei/.conda/envs/tf2.7/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"/home/wangwei/.conda/envs/tf2.7/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 58, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError:  OOM when allocating tensor with shape[772,753,768,1,1] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n         [[node GatherNd_35\r\n (defined at /media/wangwei/T7/GitLab/incomplete-CT/fan-beam/fan_line_parker/fan_radon_iradon_line.py:8)\r\n]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\r\n [Op:__inference_train_function_12544]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node GatherNd_35:\r\nIn[0] mul_129:\r\nIn[1] GatherNd_5/indices:\r\n\r\nOperation defined at: (most recent call last)\r\n>>>   File \"/home/wangwei/T7/GitLab/incomplete-CT/fan-beam/fan_line_parker/incomplete_2_parker_line_150.py\", line 220, in <module>\r\n>>>     train(epoch, batch, iternum, restore=0,\r\n>>> \r\n>>>   File \"/home/wangwei/T7/GitLab/incomplete-CT/fan-beam/fan_line_parker/incomplete_2_parker_line_150.py\", line 210, in train\r\n>>>     hist = Model.fit(x, y, batch_size=batch, epochs=epoch, validation_split=0.1,\r\n>>> \r\n>>>   File \"/home/wangwei/.conda/envs/tf2.7/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\r\n>>>     return fn(*args, **kwargs)\r\n>>> \r\n>>>   File \"/home/wangwei/.conda/envs/tf2.7/lib/python3.8/site-packages/keras/engine/training.py\", line 1216, in fit\r\n>>>     tmp_logs = self.train_function(iterator)\r\n>>> \r\n>>>   File \"/home/wangwei/.conda/envs/tf2.7/lib/python3.8/site-packages/keras/engine/training.py\", line 878, in train_function\r\n>>>     return step_function(self, iterator)\r\n>>> \r\n>>>   File \"/home/wangwei/.conda/envs/tf2.7/lib/python3.8/site-packages/keras/engine/training.py\", line 867, in step_function\r\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n>>> \r\n>>>   File \"/home/wangwei/.conda/envs/tf2.7/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in run_step\r\n>>>     outputs = model.train_step(data)\r\n>>> \r\n>>>   File \"/home/wangwei/T7/GitLab/incomplete-CT/fan-beam/fan_line_parker/incomplete_2_parker_line_150.py\", line 109, in train_step\r\n>>>     predictions = self(x, training=1)\r\n>>> \r\n>>>   File \"/home/wangwei/.conda/envs/tf2.7/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\r\n>>>     return fn(*args, **kwargs)\r\n>>> \r\n>>>   File \"/home/wangwei/.conda/envs/tf2.7/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1083, in __call__\r\n>>>     outputs = call_fn(inputs, *args, **kwargs)\r\n>>> \r\n>>>   File \"/home/wangwei/.conda/envs/tf2.7/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\r\n>>>     return fn(*args, **kwargs)\r\n>>> \r\n>>>   File \"/home/wangwei/T7/GitLab/incomplete-CT/fan-beam/fan_line_parker/incomplete_2_parker_line_150.py\", line 98, in call\r\n>>>     for i in range(1, len(self.oneiterstack) - 1):\r\n>>> \r\n>>>   File \"/home/wangwei/T7/GitLab/incomplete-CT/fan-beam/fan_line_parker/incomplete_2_parker_line_150.py\", line 102, in call\r\n>>>     Au = self.radon(u)\r\n>>> \r\n>>>   File \"/home/wangwei/T7/GitLab/incomplete-CT/fan-beam/fan_line_parker/incomplete_2_parker_line_150.py\", line 82, in radon\r\n>>>     self.w11,self.w12,self.w21,self.w22,u)\r\n>>> \r\n>>>   File \"/media/wangwei/T7/GitLab/incomplete-CT/fan-beam/fan_line_parker/fan_radon_iradon_line.py\", line 8, in radon_fan_line\r\n>>>     pf = tf.gather_nd(f * 1.0, ind11) * w11 + \\\r\n>>> \r\n\r\nFunction call stack:\r\ntrain_function -> call\r\n", "comments": ["@wangwei-cmd \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "@sushreebarsa \r\nThe code script is here [https://github.com/tensorflow/tensorflow/issues/url](url). It cannot reporduce the isuue in colab since the code needs about 50G RAM memory and colab only give me 12.69G . Besides, colab doesn't has the RTX A6000.\r\n\r\nWhen debugging the code, I found a strange phenomenon. When the number (not batch size) of training samples is small, the code can successfully run in tf 2.7, but when the number become large, 00M error was raised. \r\nFor example, when the shapes of img_label and img_ini in the code script  are [10, 512, 512, 1] and the shapes of  sin_label and sin_ini are [10, 753, 768, 1], the script can run in tf2.7. But when the number becomes 1000 ([1000, 512, 512, 1] and  [1000, 753, 768, 1]),  00M error was thrown.\r\nIn tf2.5, no matter the number is 10 or 1000, the code can run.", "@wangwei-cmd In the above [comment](https://github.com/tensorflow/tensorflow/issues/53400#issuecomment-992187042) could you please check  code [script URL](https://github.com/tensorflow/tensorflow/issues/url) and provide the correct one?Could you please refer to the [guide](https://www.tensorflow.org/guide/profiler) and let us know if it helps?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "i have the same issue.  model trains on 2.5.2, and fails on 2.6.2 and 2.7 latest.  same cuda and drivers for all.  something is eating a lot more memory in post 2.5.2 releases.", "For future lookers, you can fix the issue of OOM with > 2.5 versions by using tf.dataset, it is the only way to pass data to fit and have it not run out of memory.  I was using numpy in memory arrays and fit failed for oom, using tf.dataset from_generator I no longer fail.  I don't know why and this seems like a significant bug, but this is the fix.", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53399, "title": "tf.reduce_logsumexp throws exception when reducing over RaggedTensor", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  **Ubuntu 18.04.6 LTS**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **v2.7.0-rc1-69-gc256c071bb2 2.7.0**\r\n- Python version: **Python 3.9.0**\r\n- Bazel version (if compiling from source): **N/A**\r\n- GCC/Compiler version (if compiling from source): **N/A**\r\n- CUDA/cuDNN version: **N/A**\r\n- GPU model and memory: **N/A**\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**: When reducing over a ragged dimension, `tf.reduce_logsumexp` throws a `ValueError: TypeError: object of type 'RaggedTensor' has no len()`.\r\n\r\n**Describe the expected behavior**: When reducing over a ragged dimension, `tf.reduce_logsumexp` should successfully reduce and return a non-ragged tensor. This matches the behavior of other reduce operations like `tf.reduce_sum` or `tf.reduce_max`/\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): **no**\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**:\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\na = tf.ragged.constant([[0, 0, 0], [0, 0]], dtype=tf.float32)\r\na.shape  # TensorShape([2, None])\r\ntf.reduce_sum(a, axis=1)\r\n# >>> <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>\r\ntf.reduce_logsumexp(a, axis=1)  \r\n# >>> ValueError: TypeError: object of type 'RaggedTensor' has no len()\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nThis same error shows up when you try to transpose a ragged tensor.\r\n\r\nA simple workaround:\r\n```python\r\na_max = tf.reduce_max(a, axis=1, keepdims=True)\r\ntf.math.log(tf.math.reduce_sum(tf.math.exp(a - a_max), axis=1)) + a_max\r\n# >>> <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.0986123, 0.6931472], dtype=float32)>\r\n```\r\n```", "comments": ["@abhmul ,\r\nCan you please take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/45569#issuecomment-744527190) from the issue with similar error.It helps.Thanks!", "@tilakrayal Thanks for linking. That comment has a good workaround as well. It looks like the issue was fixed on tf-nightly so I'll go ahead and close.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53399\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53399\">No</a>\n"]}, {"number": 53397, "title": "[BUG] Retrained model on new session cause garbage for saving ", "body": "I still get this issues at Tensorflow 2.5+, Tensorflow-GPU 2.5+, Keras-2.4.3. h5py=3.1.0\r\nEven reinstalling Tensorflow, keras, and h5py does not resolve the problem.\r\n\r\nThe model I made is just a stack of Dense layer without anything special. \r\nSimilar to all people, I can evaluate and predict good in the same training kernel after training. But saving a model by `model.save()` after training, and then reload the trained model for re-training cause weight reinitialization. Despite the arg `compile=True` or `compile=False`, or even `model.reset_states()` or not, The error still occurred\r\n\r\nNote 1: This error appeared too long, from 2016 till now but it is not resolved\r\nNote 2: Test have been made on TensorFlow 2.5, 2.6, ... but errors still being found\r\n\r\nSimilar Issue: # 4875 in Keras (keras-team/keras#4875)\r\n\r\nThe pipeline is similar. (This model is pure of stack of Dense layers)\r\n1) Create the model\r\n2) Compile\r\n2) Train model\r\n3) Save the model. \r\n\r\n-> The model still fine for prediction\r\n\r\n---------------------------------------------------------------------------------\r\n\r\nNEW KERNEL:\r\n4) Re-load the model (3)\r\n5) Save the model (4)\r\n-> Still ok\r\n\r\nBUT\r\n---------------------------------------------------------------------------------\r\n\r\nNEW KERNEL:\r\n4) Re-load the model (3)\r\n5) Retrain the model by `model.fit()` (4) -> Cause new additional weight initializer: \r\nI believe this is made by weight += new_initialized_weights\r\n6) Save the model --> Model used the weight from beginning at the first step of (5) after retraining\r\n\r\nAlthough I have digged into the source code but no sign of this behaviour `weight += new_initialized_weights` was found. \r\nBut the Keras loss at the first batch during re-training phase is large enough so that I could notify this error. I believe this may come from the global variables state maintained in the source, \r\n\r\nThe callbacks contained the same configuration ModelCheckpoint at two phases\r\n\r\n", "comments": ["@IchiruTake \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "> @IchiruTake In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n\r\nThe codebase is relatively large. Attempt basic work on Colab did not represent the behaviour. I am trying to search where does the bug came from and notice you if found.", "> @IchiruTake In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n\r\nUnfortunately, I cannot reproduce the issue: Not by the code but reloading  model after stop and reset the kernel \r\nLink: https://colab.research.google.com/drive/1aUs3j4aMto9deH6AJBCifhTBFV9gOMS5#scrollTo=RxO3-QcfBmo7\r\n\r\nThe funny thing that until we reset all kernels and repeat loading the process and retrain model, this issue occured. I have found several issues related and some reported this issue can be found. \r\n\r\nChanging the backend from `tf.keras` to `keras` or `keras` to `tf` does not simply resolve solution. Some suggests, reinstalling or upgrading, but it does not resolve properly, or even re-define the global variables or behavior but no progress made. \r\n\r\nI have tested the work on `TensorBoard` and `clear_session()` ON and OFF (4 different cases), even saving in `.tf` format (`.ckpt`) does not resolve the issues. Even after re-training, in the same running instance, I load a NEW model from the saved model file `.h5`, it worked as expected. But in the different running instance, it broke.\r\n \r\nRelated Issue: keras-team/keras#2378 keras-team/keras#12263\r\n", "> @IchiruTake In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n\r\nAfter cleaning and rebuild all the environments (same TensorFlow, Keras version), I have resolved this issue. Thus, it should be noticed that this issue should be closed. Thank you for your help, I am so appreciated.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53397\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53397\">No</a>\n", "@IchiruTake May I ask if you were able to get this working on google colab? I am encountering the same issue while working on Colab as well as running locally, how did you manage to solve it?", "Yes. First, the error is made due to the difference process between inference stage and training stage. If error is still available, reinstall and clear all cache would help\r\nYou should publish your code to let us know the issue explicitly\r\n\r\nGet Outlook for Android<https://aka.ms/AAb9ysg>\r\n________________________________\r\nFrom: zaneeto ***@***.***>\r\nSent: Tuesday, January 11, 2022 10:40:13 PM\r\nTo: tensorflow/tensorflow ***@***.***>\r\nCc: Ichiru Take ***@***.***>; Mention ***@***.***>\r\nSubject: Re: [tensorflow/tensorflow] [BUG] Retrained model on new session cause garbage for saving (Issue #53397)\r\n\r\n\r\n@IchiruTake<https://github.com/IchiruTake> May I ask if you were able to get this working on google colab? I am encountering the same issue while working on Colab as well as running locally, how did you manage to solve it?\r\n\r\n\u2014\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/53397#issuecomment-1010086525>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AP2KBABZYQSO4D6JRMZ7OP3UVRFN3ANCNFSM5J5DSN6A>.\r\nTriage notifications on the go with GitHub Mobile for iOS<https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675> or Android<https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\r\nYou are receiving this because you were mentioned.Message ID: ***@***.***>\r\n", "I'm new to this so I don't really know what's wrong, but I'm not using any custom layers or anything like that.\r\nMy training code is \r\n`model = keras.models.Sequential([\r\n    keras.layers.Dense(30000,activation='relu'),\r\n    keras.layers.Dense(100,activation='relu'),\r\n    keras.layers.Dense(1,activation='relu')\r\n])\r\n\r\nmodel.compile(loss='mse',optimizer='Adam')\r\ntrainingval = model.fit(X_train, y_train, epochs=30, validation_data=[X_valid,y_valid])`\r\n\r\nand then I'm saving it with \r\n`model.save('my_model')`\r\n\r\nThen, I load it with \r\n`model_new = tf.keras.models.load_model('my_model')`\r\n\r\nand then attempt to test a dataset using \r\n`testing = model_new.evaluate(datatesting_X, datatesting_y)`\r\n\r\nI've been working on it in a single file before and it's always been working, but now that I need to save and load it (since split into two files) it's just reinitializing the weights and giving the loss that I would expect for the first epoch. \r\nI'm working on Google Colab too, so I don't think it's anything to do with the environment. Is there a very obvious solution I'm missing? I've tried most of the solutions I've seen.\r\n\r\n\r\nEDIT: Tried with RMSProp and SGD optimizers just in case it is an issue with the optimizer, but still doesn't work. ", "> I'm new to this so I don't really know what's wrong, but I'm not using any custom layers or anything like that. My training code is `model = keras.models.Sequential([ keras.layers.Dense(30000,activation='relu'), keras.layers.Dense(100,activation='relu'), keras.layers.Dense(1,activation='relu') ])\r\n> \r\n> model.compile(loss='mse',optimizer='Adam') trainingval = model.fit(X_train, y_train, epochs=30, validation_data=[X_valid,y_valid])`\r\n> \r\n> and then I'm saving it with `model.save('my_model')`\r\n> \r\n> Then, I load it with `model_new = tf.keras.models.load_model('my_model')`\r\n> \r\n> and then attempt to test a dataset using `testing = model_new.evaluate(datatesting_X, datatesting_y)`\r\n> \r\n> I've been working on it in a single file before and it's always been working, but now that I need to save and load it (since split into two files) it's just reinitializing the weights and giving the loss that I would expect for the first epoch. I'm working on Google Colab too, so I don't think it's anything to do with the environment. Is there a very obvious solution I'm missing? I've tried most of the solutions I've seen.\r\n> \r\n> EDIT: Tried with RMSProp and SGD optimizers just in case it is an issue with the optimizer, but still doesn't work.\r\n\r\nI don't know, if you are facing this issue, re-install TensorFlow. I have tested on TF 2.5 on local and TF 2.7, TF 2.8 on Colab but it still worked. You may want to check whether input pipeline between training and fine-tuning are identical"]}, {"number": 53396, "title": "module 'tensorflow.python.compiler.tensorrt.utils' has no attribute 'versionTupleToString'", "body": "# Tensorrt conversion fails on Ubuntu \r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): 2.7 binary (via pip)\r\n- TensorFlow version (use command below): 2.7\r\n- Python version: v2.7.0-rc1-69-gc256c071bb2 2.7.0\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.5\r\n- GPU model and memory:  Geforce RTX 2060 Super\r\n\r\n**Describe the current behavior**\r\nThe tensorrt conversion fails when the installed version 7.2.3 is higher than the linked version 7.2.2, because the python code in the logger statements is wrong.\r\n\r\n**Describe the expected behavior**\r\nTensorrt conversion is expected to succeed.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nNot needed here, just look at `trt_convert.py:229ff` - the function `trt_utils.versionTupleToString` is called multiple times but is not defined, there is only `trt_utils._version_tuple_to_string`. The logger only gets called when there is a version mismatch.  A simple \r\n\r\n```python\r\nversionTupleToString = _version_tuple_to_string\r\n```\r\nin `utils.py` will solve it.\r\n\r\nFile `trt_convert.py:229ff`:\r\n```python\r\n  def raise_trt_version_deprecated(version_type, trt_version):\r\n    assert version_type in [\r\n        \"linked\", \"loaded\"\r\n    ], (\"Incorrect value received for version_type: %s. Accepted: ['linked', \"\r\n        \"'loaded']\") % version_type\r\n\r\n    logging.error(\r\n        \"The {version_type} version of TensorRT: `{trt_version}` has now \"\r\n        \"been removed. Please upgrade to TensorRT 7 or more recent.\".format(\r\n            version_type=version_type,\r\n            trt_version=trt_utils.versionTupleToString(trt_version)))\r\n\r\n    raise RuntimeError(\"Incompatible %s TensorRT versions\" % version_type)\r\n\r\n  if not trt_utils.is_linked_tensorrt_version_greater_equal(7, 0, 0):\r\n    raise_trt_version_deprecated(\"linked\", linked_version)\r\n\r\n  if not trt_utils.is_loaded_tensorrt_version_greater_equal(7, 0, 0):\r\n    raise_trt_version_deprecated(\"loaded\", loaded_version)\r\n\r\n  if (loaded_version[0] != linked_version[0] or\r\n      not trt_utils.is_loaded_tensorrt_version_greater_equal(*linked_version)):\r\n    logging.error(\r\n        \"Loaded TensorRT %s but linked TensorFlow against TensorRT %s. A few \"\r\n        \"requirements must be met:\\n\"\r\n        \"\\t-It is required to use the same major version of TensorRT during \"\r\n        \"compilation and runtime.\\n\"\r\n        \"\\t-TensorRT does not support forward compatibility. The loaded \"\r\n        \"version has to be equal or more recent than the linked version.\",\r\n        trt_utils.versionTupleToString(loaded_version),\r\n        trt_utils.versionTupleToString(linked_version))\r\n    raise RuntimeError(\"Incompatible TensorRT major version\")\r\n\r\n  elif loaded_version != linked_version:\r\n    logging.info(\r\n        \"Loaded TensorRT %s and linked TensorFlow against TensorRT %s. This is \"\r\n        \"supported because TensorRT minor/patch upgrades are backward \"\r\n        \"compatible.\", trt_utils.versionTupleToString(loaded_version),\r\n        trt_utils.versionTupleToString(linked_version))\r\n\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Did you try with `Tensorrt` version 7.2.2, also check the compatibility and limitations of Tensorrt 7.2.2 [here](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/tensorrt-7.html#rel_7-2-2).", "@sachinprasadhs - I didn't check w/t 7.2.2, but if you look at the code above, you see that the function `trt_utils.versionTupleToString` is only being called if the runtime version is different than the linked version. The problem is, that this function doesn't exist, adding it to `utils.py` will solve the issue. The code says, it should be working with the same major version and any equal or larger minor version in the runtime. Which it did for me when I made the quick fix described above.  ", "@sachinprasadhs, @sanjoy  - do you have any update on this? Could you please acknowledge that this is a bug and will get fixed. It's basically a blocker when you want to upgrade to tf 2.7 and you support tensorrt.  ", "is this fixed in tf2.8?", "@sachinprasadhs, @sanjoy  - do you have any update on this? ", "it was fixed in r2.8 and 2.8.0\r\n- `utils._version_tuple_to_string` was renamed to `utils.version_tuple_to_string`\r\n- `trt_convert.py` uses `trt_utils.version_tuple_to_string` instead of `trt_utils.versionTupleToString`.", "As mentioned by @apivovarov , It was. fixed in TF 2.8, you can find the source code below.\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.8.0/tensorflow/python/compiler/tensorrt/trt_convert.py#L261-L262\r\nHere is the fix which resolved the issue. https://github.com/tensorflow/tensorflow/commit/b523f96d480fd6783aa16c1ce55419a6e4cd9cc6\r\n\r\n", "Can it be backported to r2.7? (since the issue was introduced in v2.7.0 first)", "Cherrypicks will be usually done when there is any security vulnerability, in this case you can use Tensorflow 2.8, since the fix is already available and Tensorflow 2.9 release is around the corner as well.  ", "Thanks @sachinprasadhs!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53396\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53396\">No</a>\n"]}, {"number": 53395, "title": "tf.sparse.sparse_dense_matmul vs tf.matmul has numerical error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution: MacOs Cataline 10.15.7\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): tested with 2.4.1, 2.6.0 2.7.0\r\n- Python version: 3.7.12\r\n- CUDA/cuDNN version: Used cpu\r\n- GPU model and memory:\r\n\r\n**Bug**\r\n\r\nNumerical errors appear when using `tf.sparse.sparse_dense_matmul` compared to `tf.matmul`. This happens when the number of sparse elements becomes large ~100. Code to reproduce:\r\n\r\n```\r\nd = 10\r\nmatrix = tf.reshape(tf.linspace(0.,1.,d*d), [d,d])\r\nsparse_matrix = tf.sparse.from_dense(matrix)\r\n\r\nx = tf.reshape(tf.linspace(0.,1.,d), [d,1])\r\n\r\ny = tf.matmul(matrix,x)\r\nsy = tf.sparse.sparse_dense_matmul(sparse_matrix,x)\r\n\r\nerror = tf.reduce_mean(tf.abs(y-sy))\r\n=> error = 1e-8\r\n```\r\n\r\nCouldn't seem to find this issue anywhere apart from [here](https://www.gitclear.com/open_repos/tensorflow/tensorflow/release/v2.5.0-rc2), where they mention that there is truly random noise when using tf.sparse.sparse_dense_matmul using GPU. But it seems like here it also appears with CPU\r\n\r\n\r\n", "comments": ["@jocampo2 \r\nPlease refer to [the gist](https://colab.research.google.com/gist/Saduf2019/78cdd298fd13ee3e3d8518b087acb202/untitled652.ipynb) attached for tf 2.7", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "anyone?", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53395\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53395\">No</a>\n"]}, {"number": 53394, "title": "Not able to restore the model from config with tf.einsum operation", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.4.3 (also reproduced with 2.5.0 and 2.7.0)\r\n- Python version: 3.8.10\r\n- CUDA/cuDNN version: 11.0\r\n\r\n**Describe the current behavior**\r\nWhen restoring the model from config getting\r\n`ValueError: Got 0 inputs for equation \"bmhwf,bmoh->bmowf\", expecting 2`\r\nAlthough if the `tf.einsum` op is wrapped as a Keras Lambda layer, it works (able to dump to config and restore).\r\n**Describe the expected behavior**\r\nShould be able to restore the model from config.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): Yes\r\n- Briefly describe your candidate solution(if contributing): Not sure of how the solution might look like.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/10X2dDb_EGLL64w-MyMU4g9dSrHLw9PvI?usp=sharing\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\n\r\nx1 = keras.Input(shape=(2, 4, 4, 1))\r\nx2 = keras.Input(shape=(2, 2, 4))\r\nx = tf.einsum('bmhwf,bmoh->bmowf', x1, x2)\r\nmodel = keras.Model(inputs=[x1, x2], outputs=x)\r\nmodel = tf.keras.Model.from_config(model.get_config())\r\n```\r\n\r\n**Other info / logs**\r\nLog from colab\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-1-5a94aa47793c> in <module>()\r\n      7 x = tf.einsum('bmhwf,bmoh->bmowf', x1, x2)\r\n      8 model = keras.Model(inputs=[x1, x2], outputs=x)\r\n----> 9 model = tf.keras.Model.from_config(model.get_config())\r\n\r\n4 frames\r\n/usr/local/lib/python3.7/dist-packages/keras/engine/training.py in from_config(cls, config, custom_objects)\r\n   2446     with generic_utils.SharedObjectLoadingScope():\r\n   2447       input_tensors, output_tensors, created_layers = (\r\n-> 2448           functional.reconstruct_from_config(config, custom_objects))\r\n   2449       # Initialize a model belonging to `cls`, which can be user-defined or\r\n   2450       # `Functional`.\r\n\r\n/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py in reconstruct_from_config(config, custom_objects, created_layers)\r\n   1336         while layer_nodes:\r\n   1337           node_data = layer_nodes[0]\r\n-> 1338           if process_node(layer, node_data):\r\n   1339             layer_nodes.pop(0)\r\n   1340           else:\r\n\r\n/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py in process_node(layer, node_data)\r\n   1280         input_tensors = (\r\n   1281             base_layer_utils.unnest_if_single_tensor(input_tensors))\r\n-> 1282       output_tensors = layer(input_tensors, **kwargs)\r\n   1283 \r\n   1284       # Update node index map.\r\n\r\n/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)\r\n     65     except Exception as e:  # pylint: disable=broad-except\r\n     66       filtered_tb = _process_traceback_frames(e.__traceback__)\r\n---> 67       raise e.with_traceback(filtered_tb) from None\r\n     68     finally:\r\n     69       del filtered_tb\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/special_math_ops.py in _einsum_v2_parse_and_resolve_equation(equation, input_shapes)\r\n   1279   if len(input_shapes) != len(input_labels):\r\n   1280     raise ValueError('Got {} inputs for equation \"{}\", expecting {}'.format(\r\n-> 1281         len(input_shapes), equation, len(input_labels)))\r\n   1282 \r\n   1283   # Special case: if there are no '->', then we create output subscripts from\r\n\r\nValueError: Exception encountered when calling layer \"tf.einsum\" (type TFOpLambda).\r\n\r\nGot 0 inputs for equation \"bmhwf,bmoh->bmowf\", expecting 2\r\n\r\nCall arguments received:\r\n  \u2022 equation='bmhwf,bmoh->bmowf'\r\n  \u2022 inputs=<class 'inspect._empty'>\r\n  \u2022 kwargs=<class 'inspect._empty'>\r\n```\r\n", "comments": ["@negvet \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53394\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53394\">No</a>\n"]}, {"number": 53393, "title": "Can't run inference with YAMNET based model that's been converted to TFLite", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes and No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device: Not a Mobile Device\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.7.0-rc1-69-gc256c071bb2 2.7.0\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: using CPU\r\n- GPU model and memory: using CPU\r\n\r\n**Describe the current behavior**\r\nI used [this tutorial](https://www.tensorflow.org/tutorials/audio/transfer_learning_audio) to train a model.\r\nThen I combined the classification model and YAMNet model using the instructions in the tutorial.\r\nNext, I converted the model to TFLite using int8 quantization.\r\nThe first problem I faced was that the input shape of YAMNet is variable and the TFLite model's shape is now 1.\r\n![image](https://user-images.githubusercontent.com/1261332/145704398-248d5793-8441-4a4c-a4c5-08c1286474d8.png)\r\n\r\nSo I had to resize the input to accept 16k audio samples.\r\n\r\n```\r\nmodel.resize_tensor_input(\r\n    input_details[0]['index'], (16000,))\r\n```\r\n\r\nBut then I encountered another error I couldn't fix:\r\n\r\n**Pad value has to be greater than equal to 0.Node number 17 (PAD) failed to invoke.**\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom yamnet_commons import filenames, model_path, load_wav_for_map, AUTOTUNE, commands\r\n\r\nfiles_ds = tf.data.Dataset.from_tensor_slices(filenames)\r\nfiles_ds = files_ds.map(load_wav_for_map, num_parallel_calls=AUTOTUNE)\r\nfiles_ds = files_ds.cache()\r\n\r\nmodel = tf.lite.Interpreter(str(model_path.joinpath('./model.tflite')))\r\ninput_details = model.get_input_details()\r\noutput_details = model.get_output_details()\r\n\r\nmodel.resize_tensor_input(\r\n    input_details[0]['index'], (16000,))\r\n\r\nmodel.allocate_tensors()\r\n\r\nresults = {}\r\n\r\nfor frame_data, lbl_idx in files_ds:\r\n    lbl_idx = lbl_idx.numpy()\r\n    model.set_tensor(input_details[0]['index'],\r\n                     tf.cast(frame_data * 255 - 128, tf.int8))\r\n    model.invoke()\r\n    prediction = model.get_tensor(output_details[0]['index'])\r\n    print(prediction)\r\n    sm = tf.nn.softmax(prediction.astype(np.float) / 128)\r\n    idx = np.argmax(sm)\r\n\r\n    if lbl_idx not in results:\r\n        results[lbl_idx] = {\r\n            'n_total': 0,\r\n            'n_ok': 0\r\n        }\r\n\r\n    results[lbl_idx]['n_total'] += 1\r\n    if idx == lbl_idx:\r\n        results[lbl_idx]['n_ok'] += 1\r\n\r\n\r\nprint(results)\r\n\r\nfor lbl, v in results.items():\r\n    print(\"{0} accuracy: {1}\".format(commands[lbl], v['n_ok']/v['n_total']))\r\n```\r\n", "comments": ["I've seen a similar issue https://github.com/tensorflow/models/issues/10235 but I've no idea how to fix mine.", "I resized to 15600 and the error is gone.\r\nBut faced a new Error:\r\n\r\n**RuntimeError: tensorflow/lite/kernels/complex_support.cc:43 output->type != kTfLiteFloat32 (INT8 != FLOAT32)Node number 46 (COMPLEX_ABS) failed to prepare.**\r\n\r\nseems that TFLite doesn't support complex and can't generate spectrogram using it.", "You need to provide input as float32, it is failing in the code check [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/conv.cc#L458).\r\nAlso, refer [this](https://stackoverflow.com/questions/63806975/get-fully-qunatized-tflite-model-also-with-in-and-output-on-int8) issue for int8 and uint8 input and output.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "> You need to provide input as float32, it is failing in the code check [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/conv.cc#L458). Also, refer [this](https://stackoverflow.com/questions/63806975/get-fully-qunatized-tflite-model-also-with-in-and-output-on-int8) issue for int8 and uint8 input and output.\r\n\r\nI had no problem converting other models to a fully quantized model.\r\nBut YAMNET is a pain.\r\n\r\nIt seems that there are OPs not supported by TFLite or some OPs cannot be fully quantized?!", "Yes, all the OPs can not be quantized since those Ops don't have TFLite equivalents. You can check the ops_compatibility [here](https://www.tensorflow.org/lite/guide/ops_compatibility). \r\nAlso, check our [roadmap](https://www.tensorflow.org/lite/guide/roadmap#optimization) on tflite.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@sachinprasadhs \r\nThank You", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53393\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53393\">No</a>\n"]}, {"number": 53392, "title": "exit code 409 when trying to run through tensorflow example in pycharm", "body": "I am trying to go through the DCGAN example on the tensorflow website https://www.tensorflow.org/tutorials/generative/dcgan. it seems to run fine up until the step where it uses the generator generated_image = generator(noise, training=False). At that point it exits with error code Process finished with exit code -1073740791 (0xC0000409).\r\n\r\nI am running on Windows 10 using pycharm. I have tried messing with the batch size in case this is a memory issue, but even setting it to 1 gives the same results. I have also tried running pycharm as administrator.", "comments": ["@skywo1f \r\nCan you please try on colab and see if you face the same issue, you use gpu option aswell.", "I am using the gpu option, I installed cuda/cudnn/drivers and made sure that tensorflow did not throw any errors with regards to that. Running the code on google's colab does not replicate the error", "pycharm defaults to python 3.9. is it possible that tensorflow is not compatible with python 3.9?", "@skywo1f \r\nCould you downgrade to 3.7/3.8 and see if you still face the issue. you may refer to similar compatibility issues.\r\n#51573,#44485\r\nAs explained earlier unless we have all the required information we cannot help you, you may open this issue with pycharm or at the discussion forum.\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53392\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53392\">No</a>\n"]}, {"number": 53391, "title": "AttributeError: 'Sequential' object has no attribute 'predict_classes'", "body": "```\r\nseed_text = \"I've got a bad feeling about this\"\r\nnext_words = 100\r\n  \r\nfor _ in range(next_words):\r\n\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\r\n\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\r\n\tpredicted = model.predict_classes(token_list, verbose=0)\r\n\toutput_word = \"\"\r\n\tfor word, index in tokenizer.word_index.items():\r\n\t\tif index == predicted:\r\n\t\t\toutput_word = word\r\n\t\t\tbreak\r\n\tseed_text += \" \" + output_word\r\nprint(seed_text)\r\n```\r\n\r\nThis codeblock is giving me the error shown below:\r\n\r\n\r\n```\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-52-70399a5f93d3> in <module>()\r\n      5         token_list = tokenizer.texts_to_sequences([seed_text])[0]\r\n      6         token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\r\n----> 7         predicted = model.predict_classes(token_list, verbose=0)\r\n      8         output_word = \"\"\r\n      9         for word, index in tokenizer.word_index.items():\r\n\r\nAttributeError: 'Sequential' object has no attribute 'predict_classes'\r\n```\r\n\r\nHow can I solve this error? Please help.\r\n\r\nHere is the link of the full code:\r\nhttps://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbDVSTGdJczNoa2ZBeXpObWxobGJldGd5Q2pLd3xBQ3Jtc0trSVRobkVBM2EtV3lTajlQNzhXbEhGTjN3WXhFTGo3d1U4b3h1Rjl4Rk9ubUVVWG95cUNfaWFoTHpmRlpEM1VmM2NrOXFqc21WYkRKZ0c3WWgtcE5VRV9kZF9pZDZMUDVmWnRtNUtMS3NzUnZodFB3RQ&q=https%3A%2F%2Fgoo.gle%2F3aSTLGx", "comments": ["@aritraMandal02 \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),\r\nThanks!", "I used keras issue template.\r\n", "@aritraMandal02 \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Okay posted there.\r\n", "@aritraMandal02 Thank you for confirming ! Could you please move this issue to closed status as we will track the other one in [keras-team/keras repo](https://github.com/keras-team/keras/issues). Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53391\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53391\">No</a>\n", "Okay done. Thank you for helping "]}, {"number": 53390, "title": "Cuda 10.2 compatible with which tensorlow-gpu version?", "body": "I am getting error while importing tensorflow where it is looking cuda 11.0 version, I have cuda with 10.2, unable to find compatible TF version with 10.2.\r\n\r\nCan anyone confirm the TF version ?\r\n\r\n", "comments": ["@pn12 \r\nWe see that you have not mentioned the tf version used, please use issue template a sit helps us help you.\r\nPlease refer to this link to use the [correct compatibility](https://www.tensorflow.org/install/source_windows#gpu).", "@Saduf2019 - It's resolved now, there was a version compatibility issue among cuda , tf , cudnn.\r\nThanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53390\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53390\">No</a>\n"]}, {"number": 53389, "title": "added download per month badge", "body": null, "comments": ["I am unsure we should be adding so many badges. Let me discuss this with OSS leads at a future meeting (when it gets planned)"]}, {"number": 53388, "title": "cl.exe failed on fast_module_type.so exit 2", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 19044.1387\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.6\r\n- Python version: 3.9.2\r\n- Installed using virtualenv? pip? conda?: bazel\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): VS2019 (14.29.30133)\r\n- CUDA/cuDNN version: 11.2 / 8.2\r\n- CPU model and memory: R7 2600X 16GB DDR4\r\n- GPU model and memory: GTX 2060 6GB GDDR4\r\n\r\n\r\n\r\n**Describe the problem**\r\nI'm trying to compile tensorflow from source to deploy a model in C++ trained on Google Collab. I tried to compile different versions of tensorflow from v2.4.0 to master and r2.4 to master but all of them give me error, now I'm trying to compile r2.6.\r\nI just need TF on CPU so when I run configure.py select No on cuda an cuDNN support. I already use python 3.8, 3.9, 3.9.2, VS2019, VS2017, between each test use `bazel clean` and then `bazel clean --expunge` and also tried with a clean install of Windows 10 for compiling but all of them give the exact same error\r\nAlso followed issue #50950 and #51573\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nClonning repository\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\ngit checkout r2.6\r\n```\r\n\r\nRunning ` python .\\configure.py`\r\n```\r\nYou have bazel 3.7.2 installed.\r\nPlease specify the location of python. [Default is C:\\Users\\sixto\\AppData\\Local\\Programs\\Python\\Python39\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  C:\\Users\\sixto\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\sixto\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]:\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\r\nEigen strong inline overridden.\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n```\r\n\r\nInstalling requirements which I copy from `tensorflow\\tools\\pip_package\\setup.py`\r\n`pip3 install -r requirements.txt`\r\n\r\nAnd compile just for CPU\r\n`bazel --output_user_root=D:\\tmp_tensorflow build --config=opt //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nAt first run it give me this error\r\n```\r\nERROR: D:/tensorflow/tensorflow/compiler/xla/service/BUILD:340:11: C++ compilation of rule '//tensorflow/compiler/xla/service:hlo_evaluator' failed (Exit 2): cl.exe failed: error executing command\r\n  cd D:/tmp_tensorflow/26orbg4z/execroot/org_tensorflow\r\n  SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\include;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\cppwinrt\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\MSBuild\\Current\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.8 Tools\\x64\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\Tools\\devinit;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.19041.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\\\MSBuild\\Current\\Bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\Tools\\;;C:\\Windows\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/sixto/AppData/Local/Programs/Python/Python39/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/sixto/AppData/Local/Programs/Python/Python39/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\sixto\\AppData\\Local\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TMP=C:\\Users\\sixto\\AppData\\Local\\Temp\r\n```\r\n\r\nThen re-run the same compiling command and give me this error\r\n```\r\nERROR: D:/tensorflow/tensorflow/python/util/BUILD:610:27: C++ compilation of rule '//tensorflow/python/util:fast_module_type.so' failed (Exit 2): cl.exe failed: error executing command\r\n  cd D:/tmp_tensorflow/26orbg4z/execroot/org_tensorflow\r\n  SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\include;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\cppwinrt\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\MSBuild\\Current\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.8 Tools\\x64\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\Tools\\devinit;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.19041.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\\\MSBuild\\Current\\Bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\Tools\\;;C:\\Windows\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/sixto/AppData/Local/Programs/Python/Python39/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/sixto/AppData/Local/Programs/Python/Python39/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\sixto\\AppData\\Local\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TMP=C:\\Users\\sixto\\AppData\\Local\\Temp\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n[second_run.log](https://github.com/tensorflow/tensorflow/files/7696321/second_run.log)\r\n[first_run.log](https://github.com/tensorflow/tensorflow/files/7696322/first_run.log)\r\n\r\n", "comments": ["Hi @chunduriv! Could you please look at this issue?", "@orlando9427,\r\n\r\nCould you please refer similar issues [#36576](https://github.com/tensorflow/tensorflow/issues/36576) and let us know if it helps.Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53388\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53388\">No</a>\n"]}, {"number": 53387, "title": "\ud83d\udc1b\ud83e\udeb2", "body": "> <em>Please make sure that this is a bug. As per our\r\n\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@Chalo0 \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53387\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53387\">No</a>\n"]}, {"number": 53386, "title": "[oneDNN] Upgrade oneDNN version to 2.5-rc", "body": "This PR is to upgrade oneDNN version to 2.5-rc.", "comments": []}, {"number": 53383, "title": "I think nvidia-driver is not working.", "body": "I think nvidia-driver is not working.\r\n\r\nYou have to check command `nvidia-smi` \r\n\r\nIf these command is not working, you have reboot your computer.\r\n\r\nMaybe `nvidia-smi` works correctly.\r\n\r\n_Originally posted by @SnowMasaya in https://github.com/tensorflow/tensorflow/issues/255#issuecomment-384846967_", "comments": ["Hi @Sameerbobde ! \r\nCould you please update the template too as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks!", "bash: /usr/bin/nvidia-smi: No such file or directory\r\ni have install nvidia-cuda-toolkit but that it is not showing", "Hi @Sameerbobde ! Could you try again after pointing your file from Cuda installation location through [environment variable ](https://www.tensorflow.org/install/gpu#linux_setup). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53383\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53383\">No</a>\n"]}, {"number": 53382, "title": "FAILED: Build did NOT complete successfully on windows 10", "body": "<em>I am facing error trying to build tensorflow from source in windows 10. My goal is to use tensorflow API in C++. I am following [this doc](https://www.tensorflow.org/install/source_windows) for that. But whenever I run `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package` I get `ERROR: C:/users/black ops/_bazel_black ops/r7janahi/external/com_google_protobuf/BUILD:301:11: Compiling src/google/protobuf/compiler/zip_writer.cc failed: (Exit 1): cl.exe failed: error executing command`. I have also posted an issue where I am getting error message while trying to build tensorflow in Ubuntu 18.04 [here](https://github.com/tensorflow/tensorflow/issues/53379).</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.7\r\n- Python version: 3.8\r\n- Bazel version: 4.2.2\r\n\r\n\r\n**Describe the problem**\r\nI have tried to build tensorflow in Ubuntu 18.04 created in Oracle VM VirtualBox. When I tried to run the `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package` command to build the pip package tensorflow cpu-only version, The following error message persists:\r\n```\r\nERROR: C:/users/black ops/_bazel_black ops/r7janahi/external/com_google_protobuf/BUILD:301:11: Compiling src/google/protobuf/compiler/zip_writer.cc failed: (Exit 1): cl.exe failed: error executing command\r\n  cd C:/users/black ops/_bazel_black ops/r7janahi/execroot/org_tensorflow\r\n  SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\winrt;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\Tools;C:\\Program Files (x86)\\HTML Help Workshop;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v8.1A\\bin\\NETFX 4.5.1 Tools\\x64\\;;C:\\WINDOWS\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=I:/Anaconda/envs/tf_c++/python.exe\r\n    SET PYTHON_LIB_PATH=I:/Anaconda/envs/tf_c++/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\BLACKO~1\\AppData\\Local\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TMP=C:\\Users\\BLACKO~1\\AppData\\Local\\Temp\r\n  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/cl.exe @bazel-out/x64_windows-opt/bin/external/com_google_protobuf/_objs/protoc_lib/zip_writer.obj.params\r\nExecution platform: @local_execution_config_platform//:platform\r\ncl : Command line warning D9002 : ignoring unknown option '/experimental:preprocessor'\r\nfatal error C1007: unrecognized flag '-ReducedOptimizeHugeFunctions' in 'p2'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 1.472s, Critical Path: 0.74s\r\nINFO: 10 processes: 10 internal.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nAccording to [the doc](https://www.tensorflow.org/install/source_windows) I have created a conda environment of Python 3.8 and installed the TensorFlow package dependencies:\r\n`pip3 install -U six numpy wheel`\r\n`pip3 install -U keras_preprocessing --no-deps`\r\nThen I have installed Bazel 4.2.2 from [bazel github](https://github.com/bazelbuild/bazel/releases?page=6) using _bazel-4.2.2-windows-x86_64.exe_.\r\nThen I have installed _MSYS2_ and run the following command:\r\n`pacman -S git patch unzip`\r\nAfter that, I have cloned the [Tensorflow repo](https://github.com/tensorflow/tensorflow.git) and checked into _r2.7_ branch.\r\nThen I have run `python ./configure.py` and the following is the output:\r\n```\r\nYou have bazel 4.2.2 installed.\r\nPlease specify the location of python. [Default is I:\\Anaconda\\envs\\tf_c++\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  I:\\Anaconda\\envs\\tf_c++\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [I:\\Anaconda\\envs\\tf_c++\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]:\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\r\nEigen strong inline overridden.\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]:\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n\r\n```\r\n\r\nAfter that, I have run `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package` command and the following is the output:\r\n```\r\nWARNING: Output user root \"C:/Users/BLACK OPS/_bazel_BLACK OPS\" contains a space. This will probably break the build. You should set a different --output_user_root.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=171\r\nINFO: Reading rc options for 'build' from i:\\tf_for_c_env\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=I:/Anaconda/envs/tf_c++/python.exe\r\nINFO: Reading rc options for 'build' from i:\\tf_for_c_env\\tensorflow\\.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true\r\nINFO: Reading rc options for 'build' from i:\\tf_for_c_env\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=I:/Anaconda/envs/tf_c++/python.exe --action_env PYTHON_LIB_PATH=I:/Anaconda/envs/tf_c++/lib/site-packages --python_path=I:/Anaconda/envs/tf_c++/python.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true\r\nINFO: Reading rc options for 'build' from i:\\tf_for_c_env\\tensorflow\\.bazelrc:\r\n  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils\r\nINFO: Found applicable config definition build:short_logs in file i:\\tf_for_c_env\\tensorflow\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file i:\\tf_for_c_env\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:opt in file i:\\tf_for_c_env\\tensorflow\\.tf_configure.bazelrc: --copt=/arch:AVX --host_copt=/arch:AVX\r\nINFO: Found applicable config definition build:windows in file i:\\tf_for_c_env\\tensorflow\\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file i:\\tf_for_c_env\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/43d6991c2a4cc2ac374e68c029634f2b59ffdfdf.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nWARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1596824487 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  I:/tf_for_c_env/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  I:/tf_for_c_env/tensorflow/tensorflow/workspace0.bzl:108:34: in workspace\r\n  C:/users/black ops/_bazel_black ops/r7janahi/external/bazel_toolchains/repositories/repositories.bzl:35:23: in repositories\r\nRepository rule git_repository defined at:\r\n  C:/users/black ops/_bazel_black ops/r7janahi/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nERROR: C:/users/black ops/_bazel_black ops/r7janahi/external/com_google_protobuf/BUILD:301:11: Compiling src/google/protobuf/compiler/zip_writer.cc failed: (Exit 1): cl.exe failed: error executing command\r\n  cd C:/users/black ops/_bazel_black ops/r7janahi/execroot/org_tensorflow\r\n  SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\winrt;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\Tools;C:\\Program Files (x86)\\HTML Help Workshop;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v8.1A\\bin\\NETFX 4.5.1 Tools\\x64\\;;C:\\WINDOWS\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=I:/Anaconda/envs/tf_c++/python.exe\r\n    SET PYTHON_LIB_PATH=I:/Anaconda/envs/tf_c++/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\BLACKO~1\\AppData\\Local\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TMP=C:\\Users\\BLACKO~1\\AppData\\Local\\Temp\r\n  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/cl.exe @bazel-out/x64_windows-opt/bin/external/com_google_protobuf/_objs/protoc_lib/zip_writer.obj.params\r\nExecution platform: @local_execution_config_platform//:platform\r\ncl : Command line warning D9002 : ignoring unknown option '/experimental:preprocessor'\r\nfatal error C1007: unrecognized flag '-ReducedOptimizeHugeFunctions' in 'p2'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 1.472s, Critical Path: 0.74s\r\nINFO: 10 processes: 10 internal.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n**Any other info / logs**\r\nI have Visual Studio 2019 installed from previous uses in my Windows environment, so I have skipped the [Install Visual C++ Build Tools 2019](https://www.tensorflow.org/install/source_windows#install_visual_c_build_tools_2019) part from the doc.", "comments": ["@nitolpalak Could you please refer to the similar [issue1](https://github.com/tensorflow/tensorflow/issues/53054), [issue2](https://github.com/tensorflow/tensorflow/issues/51841) and this [comment](https://github.com/tensorflow/tensorflow/issues/53007#issuecomment-965028450) ?Please let us know if it helps?Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hello @sushreebarsa I have checked the above issues and comment as you have mentioned.\r\nAccording to [issue1](https://github.com/tensorflow/tensorflow/issues/53054), The problem seems to be in CUDA installation, But in my case I am trying to install the CPU-only version.\r\nAccording to [issue2](https://github.com/tensorflow/tensorflow/issues/51841), the problem was solved after enabling Long Path of NTFS, but in my case the following output occurs after enabling Long Path of NTFS:\r\n```\r\nERROR: E:/client_projects/jasmy_recommender/tfc++/tensorflow/tensorflow/tools/pip_package/BUILD:278:10: //tensorflow/tools/pip_package:build_pip_package depends on //tensorflow/compiler/mlir/tensorflow:gen_mlir_passthrough_op_py in repository @ which failed to fetch. no such package '@llvm-project//mlir': Failed to execute overlay script: 'E:/Python/python.exe C:/users/bjit/_bazel_bjit/hkqdhmuf/external/llvm-raw/utils/bazel/overlay_directories.py --src C:/users/bjit/_bazel_bjit/hkqdhmuf/external/llvm-raw --overlay C:/users/bjit/_bazel_bjit/hkqdhmuf/external/llvm-raw/utils/bazel/llvm-project-overlay --target .'\r\nExited with code 1\r\nstdout:\r\n\r\nstderr:\r\nTraceback (most recent call last):\r\n  File \"C:/users/bjit/_bazel_bjit/hkqdhmuf/external/llvm-raw/utils/bazel/overlay_directories.py\", line 92, in <module>\r\n    main(parse_arguments())\r\n  File \"C:/users/bjit/_bazel_bjit/hkqdhmuf/external/llvm-raw/utils/bazel/overlay_directories.py\", line 80, in main\r\n    _symlink_abs(os.path.join(args.overlay, relpath),\r\n  File \"C:/users/bjit/_bazel_bjit/hkqdhmuf/external/llvm-raw/utils/bazel/overlay_directories.py\", line 64, in _symlink_abs\r\n    os.symlink(os.path.abspath(from_path), os.path.abspath(to_path))\r\nOSError: [WinError 1314] A required privilege is not held by the client: 'C:\\\\users\\\\bjit\\\\_bazel_bjit\\\\hkqdhmuf\\\\external\\\\llvm-raw\\\\utils\\\\bazel\\\\llvm-project-overlay\\\\.bazelignore' -> 'C:\\\\users\\\\bjit\\\\_bazel_bjit\\\\hkqdhmuf\\\\external\\\\llvm-project\\\\.bazelignore'\r\n\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed\r\nINFO: Elapsed time: 4.767s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (51 packages loaded, 14 targets configured)\r\n    currently loading: tensorflow/lite/tools ... (3 packages)\r\n```\r\n", "According to the [comment](https://github.com/tensorflow/tensorflow/issues/53007#issuecomment-965028450)-\r\nMy Processor is `Intel(R) Core(TM) i7-9700 CPU @ 3.00 GHz`\r\nI have installed MSVC++ Redistributable from [this link](https://docs.microsoft.com/en-US/cpp/windows/latest-supported-vc-redist?view=msvc-170)\r\nMy Python info: `Python 3.8.0 (tags/v3.8.0:fa919fd, Oct 14 2019, 19:37:50) [MSC v.1916 64 bit (AMD64)] on win32`\r\nMy CPU is also x64 based processor\r\nPreviously, I was trying on a Conda environment which is not supported  according to the comment. So I have installed python 3.8.0 and created a python virtual environment using `virtualenv` and tried to build tensorflow 2.7, but I still get the following error.\r\n```\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed\r\nINFO: Elapsed time: 8.085s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (52 packages loaded, 14 targets configured)\r\n    currently loading: tensorflow/compiler/mlir/tensorflow ... (2 packages)\r\n```\r\nPlease let me know if I have missed anything.", "@nitolpalak Could you please have a look at the [install errors](https://www.tensorflow.org/install/errors) for some common reasons and solutions ,please let us know if it helps?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53382\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53382\">No</a>\n"]}, {"number": 53381, "title": " bazel-genfiles/ and *.pb.h  not been generated  with bazel-building tensorflow2.4.1 from source", "body": "**System information**\r\n- OS Platform and Distribution ( Linux Ubuntu 18.04):\r\n- TensorFlow installed from (source)\r\n- TensorFlow version:        Tags v2.4.1\r\n- Python version:      Python 3.7.6\r\n- Installed using virtualenv? pip? conda?:       conda\r\n- Bazel version (if compiling from source):        have tried bazel 3.4.0 \u3001 bazel 3.1.0\r\n- GCC/Compiler version (if compiling from source):  gcc (GCC) 7.4.0\r\n- CUDA/cuDNN version:        cuda_11.1 , cudnn 8.0.5\r\n- GPU model and memory:   rtx3060\r\n- nvidia-driver version:     460.91\r\n- eigen  version :      eigen-3.3.90\r\n\r\n\r\nI want to build  tensorflow c++ api with bazel, no errors were reported during  bazel-build process .\r\nthe command I used: `bazel build --config=opt --config=cuda //tensorflow:libtensorflow_cc.so`\r\n\r\nhowever,while compiling a C++ program, I got the following error:\r\n`fatal error: tensorflow/core/framework/device_attributes.pb.h: No such file or directory\r\n  #include \"tensorflow/core/framework/device_attributes.pb.h\"\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\n`  \r\n\r\nAfter that I  checked the  tensorflow/core/framework  folder and **found no *.pb.h  files in it ,only   some .proto files,**(etc. device_attributes.proto ) .  **Not even the bazel-genfiles folder.** \r\nThe *.pb.h files should be generated by protobuf ,  what's wrong with my protobuf ?\r\nthe protobuf version and url refer to [workspace.bzl](https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/workspace.bzl)\r\n\r\n\r\nI don't  know how to solve it.  Thanks for your help.\r\n\r\n\r\n\r\n", "comments": ["@no-define ,\r\nCould you please update TensorFlow to the latest stable version v2.7 and let us know if you are facing the same error. Also please take a look at this issue [1](https://github.com/tensorflow/tensorflow/issues/47902) and [2](https://github.com/tensorflow/tensorflow/issues/46786) with the similar error.It helps.Thanks!", "@tilakrayal  hi,I've tried  manually generating pb.h by command  `sudo protoc  <protofile>  --cpp_out=./ `\r\n, now the original error has been fixed.  \r\n\r\nNow the error is        `  tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:42: error: #include nested too deeply\r\n #include \"unsupported/Eigen/CXX11/Tensor\"  `\r\n\r\nI'm going to find a fix for this problem. \r\n Thank you anyway!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53381\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53381\">No</a>\n", "> unsupported/Eigen/CXX11/Tensor\r\n\r\nYou could see the solution in https://blog.csdn.net/wd1603926823/article/details/92843830 because the author solved the similar problem like the following shows:\r\n`./unsupported/Eigen/CXX11/Tensor:1:42: error: #include nested too deeply\r\ntensorflow/core/lib/core/threadpool.cc:88:49: error: expected template-name before \u2018<\u2019 token\r\n struct ThreadPool::Impl : Eigen::ThreadPoolTempl<EigenEnvironment> {\r\n                                                 ^\r\ntensorflow/core/lib/core/threadpool.cc:88:49: error: expected \u2018{\u2019 before \u2018<\u2019 token\r\ntensorflow/core/lib/core/threadpool.cc:88:49: error: expected unqualified-id before \u2018<\u2019 token\r\ntensorflow/core/lib/core/threadpool.cc:217:1: error: expected \u2018}\u2019 at end of input\r\n }  // namespace tensorflow\r\n ^\r\ntensorflow/core/lib/core/threadpool.cc:217:1: error: expected \u2018}\u2019 at end of input\r\n`", "> \r\n\r\n`tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:42: error: #include nested too deeply #include \"unsupported/Eigen/CXX11/Tensor\"`\r\n\r\nway to solve this problem :   https://stackoverflow.com/questions/54779775/issues-including-eigen-for-simple-c-tensorflow-lite-test-program\r\n\r\n\r\n`find . -name \"eigen*\" -type d `   \r\n\r\nthen edit your CMakeList.txt  :\r\n` include_directories(\r\n        ${CMAKE_CURRENT_SOURCE_DIR}/tf/eigen\r\n)\r\n`\r\n\r\ntry replace eigen path listed one by one.                                                                                                                             \r\n"]}, {"number": 53380, "title": "UnsatisfiedLinkError when attempting to use HexagonDelegate", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Using example from https://www.tensorflow.org/lite/performance/hexagon_delegate\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 11\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: OnePlus 7T Pro\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): `org.tensorflow:tensorflow-lite:0.0.0-nightly-SNAPSHOT` and `org.tensorflow:tensorflow-lite-hexagon:0.0.0-nightly-SNAPSHOT` with hexagon libraries `v1.20.0.1`\r\n- Python version: n/a\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a, but the 7T Pro has a Snapdragon 855+ with a Hexagon 690\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nAttempting to create `HexagonDelegate` results in a `java.lang.UnsatisfiedLinkError`\r\n\r\n**Describe the expected behavior**\r\n\r\nHexagonDelegate should work on the device in question\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nCreate a new project, and add the Java API as described here: https://www.tensorflow.org/lite/performance/hexagon_delegate#hexagon_delegate_java_api\r\n\r\nYou should end up with the three skel binaries in app/src/main/jniLibs, and the tensorflow-lite & tensorflow-lite-hexagon nightly AARs downloaded and built into the app, resulting in the following lib folder after installation:\r\n\r\n```\r\nOnePlus7TPro:/data/app/~~an-4OVJU49vmn0JQVRH4TQ==/<pname>-QzF_0vIwaxg6M2SwPMqdug==/lib/arm64 #\r\nls\r\nlibhexagon_interface.so  libhexagon_nn_skel_v65.so  libtensorflowlite_hexagon_jni.so\r\nlibhexagon_nn_skel.so    libhexagon_nn_skel_v66.so  libtensorflowlite_jni.so\r\n```\r\n\r\nAdd code to create a HexagonDelegate, as follows:\r\n\r\n```\r\ntry {\r\n   HexagonDelegate(context)\r\n}catch (e: UnsupportedOperationException){\r\n   //Fall back to NNAPI delegate\r\n}\r\n```\r\n\r\nRun the app, it will crash with an exception:\r\n\r\n```\r\n12-10 01:21:39.599 20658 20658 E AndroidRuntime: FATAL EXCEPTION: main\r\n12-10 01:21:39.599 20658 20658 E AndroidRuntime: Process: <pname>, PID: 20658\r\n12-10 01:21:39.599 20658 20658 E AndroidRuntime: java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol \"dlopen\" referenced by \"/data/app/~~an-4OVJU49vmn0JQVRH4TQ==/<pname>-QzF_0vIwaxg6M2SwPMqdug==/lib/arm64/libtensorflowlite_hexagon_jni.so\"...\r\n12-10 01:21:39.599 20658 20658 E AndroidRuntime:        at java.lang.Runtime.loadLibrary0(Runtime.java:1087)\r\n12-10 01:21:39.599 20658 20658 E AndroidRuntime:        at java.lang.Runtime.loadLibrary0(Runtime.java:1008)\r\n12-10 01:21:39.599 20658 20658 E AndroidRuntime:        at java.lang.System.loadLibrary(System.java:1664)\r\n12-10 01:21:39.599 20658 20658 E AndroidRuntime:        at org.tensorflow.lite.HexagonDelegate.ensureNativeLibraryLoaded(HexagonDelegate.java:66)\r\n12-10 01:21:39.599 20658 20658 E AndroidRuntime:        at org.tensorflow.lite.HexagonDelegate.<init>(HexagonDelegate.java:35)\r\n12-10 01:21:39.599 20658 20658 E AndroidRuntime:        at <line of HexagonDelegate creation>(....kt:89)\r\n```\r\n\r\nAs shown with the `ls` above, all the required libraries are present in that directory.\r\n\r\nThis behaviour is identical to #51872, which was closed as stale without a solution. The linked issues from the comments on that issue do not solve this problem, as for issue 1 the crash is occurring before the actual DSP check, which would be caught and handled by my sample code. Issue 2 is for Ubuntu Mobile and Python, and thus totally irrelevant.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Hi @KieronQuinn ! Did you check with this[ thread](https://github.com/tensorflow/tensorflow/issues/36804#issuecomment-591571178) yet?", "> Hi @KieronQuinn ! Did you check with this[ thread](https://github.com/tensorflow/tensorflow/issues/36804#issuecomment-591571178) yet?\r\n\r\nYes, like I said in the issue that topic is for Python and is not relevant. I'm already using the nightly. ", "@sachinprasadhs ! Could you please look at this issue ?", "Can you make sure that you're using the latest nightlies - may be clear the gradle cash ?\r\n\r\nI just cloned tensorflow examples and updated image classification example to include hexagon. I followed the [guide](https://www.tensorflow.org/lite/performance/hexagon_delegate#hexagon_delegate_java_api)\r\n\r\nby adding the gradle dep in build.gradle\r\n```\r\nimplementation 'org.tensorflow:tensorflow-lite-hexagon:0.0.0-nightly-SNAPSHOT'\r\n```\r\n\r\nand created directory jniLibs/arm64-v8a and included the files i got from the libhexagon_nn_skel extraction. as explained [here](https://www.tensorflow.org/lite/performance/hexagon_delegate#add_the_shared_library_to_your_app)\r\n\r\nAlso, can you share the log ?\r\n\r\nThanks", "I cleared the cache and made sure it's using the nightly, it is. \r\n\r\nThe JNI folder (app\\src\\main\\jniLibs\\arm64-v8a) looks like this:\r\n\r\n![image](https://user-images.githubusercontent.com/3430869/145914145-1c5e2a57-b07d-4639-9aca-973a4a039e5c.png)\r\n\r\nAnd the log is in the issue but here it is again:\r\n\r\n```\r\n12-14 01:03:12.924  7339  7339 E AndroidRuntime: Process: <pname>, PID: 7339\r\n12-14 01:03:12.924  7339  7339 E AndroidRuntime: java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol \"dlopen\" referenced by \"/data/app/~~uTBWoFr6BqliVZ652EhEiQ==/<pname>-h930g9ig2G8v9bcinuLslg==/lib/arm64/libtensorflowlite_hexagon_jni.so\"...\r\n12-14 01:03:12.924  7339  7339 E AndroidRuntime:        at java.lang.Runtime.loadLibrary0(Runtime.java:1087)\r\n12-14 01:03:12.924  7339  7339 E AndroidRuntime:        at java.lang.Runtime.loadLibrary0(Runtime.java:1008)\r\n12-14 01:03:12.924  7339  7339 E AndroidRuntime:        at java.lang.System.loadLibrary(System.java:1664)\r\n12-14 01:03:12.924  7339  7339 E AndroidRuntime:        at org.tensorflow.lite.HexagonDelegate.ensureNativeLibraryLoaded(HexagonDelegate.java:66)\r\n12-14 01:03:12.924  7339  7339 E AndroidRuntime:        at org.tensorflow.lite.HexagonDelegate.<init>(HexagonDelegate.java:35)\r\n```", "Can you provide all the information about the ndk and api version you are using and android version too.\r\n\r\nThanks", "I'm not using the NDK in this project as I'm using the prebuilts - if that's incorrect please let me know as the guide does not mention the NDK. \r\n\r\nAPI wise I'm targeting 31, and the phone is running Android 11 (30)", "I just tried our image classification [example](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android) and added code to use hexagon delegate.\r\n\r\nTo make sure we have similar setup, can you share your project, or try from the example as i mentioned earlier.\r\n\r\nIf you don't want to share here, you can send it to me on email - same name as here at google dot com\r\n\r\nThanks", "I've uploaded and shared with your email on Google Drive my project setup, as well as a precompiled APK. I'll have a look at the example too, see if I can reproduce it on that also, thanks. \r\n\r\nThe crash happens on launch, so long as the device is *possibly* capable of Hexagon usage (it should fall back to NNAPI with an UnsupportedOperationException), the delegate code is in `TapTapTfClassifier.kt`", "Update on the above, by using the example, adding the hexagon gradle dependency and the libs and changing the code using\r\n\r\n```\r\ntfliteOptions.addDelegate(new HexagonDelegate(activity));\r\n```\r\n\r\n(and commenting out the CPU switch block) at line 225 of Classifier.java, the same exception happens with my config. Could I possibly be missing a step somewhere? ", "Thanks. Can you share the example project you updated  - it will be easier to look into it.\r\n\r\nThanks", "Absolutely, here you go: https://drive.google.com/file/d/1vs3ghix6SzL9CG6EtbERY5VKZSzSHdbC/view?usp=sharing", "Thanks for the updated example code with the fix, unfortunately it doesn't help my original project.\r\n\r\nStarted by running a comparison between the before and after example to identify what had been changed, to try to replicate it in my own project, nothing really jumped out other than updating gradle (which I've done - no change), and adding Sonatype to the sources (again, no change).\r\n\r\nI've copied all the dependency lines from the gradle again, made sure all the libs are correct (even copying the known-working libs from the fixed example), I've even decompiled the resulting APK and replaced the libs to no avail, always the same crash. \r\n\r\nI think I may just give up on using Hexagon entirely and just use NNAPI as Hexagon seems to have very limited support (it doesn't work on my OnePlus 7T Pro even with selinux disabled for example)", "I've encountered what appears to be this exact issue on my own project.  I'm trying to use the hexagon delegate on a Samsung Galaxy Tab S7 (snapdragon 865) and when I load the delegate I encounter a runtime crash with the same exception and identical stack trace as reported in this issue above.  From my program:\r\n\r\n```\r\nE/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: com.example.legonetandroid, PID: 30171\r\n    java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol \"dlopen\" referenced by \"/data/app/~~dVPIFpHRrBcjznj_lAHvhA==/com.example.legonetandroid-zcRv5ajrqr355wCmT88zeA==/base.apk!/lib/arm64-v8a/libtensorflowlite_hexagon_jni.so\"...\r\n        at java.lang.Runtime.loadLibrary0(Runtime.java:1077)\r\n        at java.lang.Runtime.loadLibrary0(Runtime.java:998)\r\n        at java.lang.System.loadLibrary(System.java:1656)\r\n        at org.tensorflow.lite.HexagonDelegate.ensureNativeLibraryLoaded(HexagonDelegate.java:66)\r\n        at org.tensorflow.lite.HexagonDelegate.<init>(HexagonDelegate.java:35)\r\n```\r\n\r\nI attempted to reproduce the issue using the tensorflow image classification example (modified to use the hexagon delegate), and I encounter the same runtime crash as I do on my own project.\r\n\r\n> Thanks for the updated example code with the fix, unfortunately it doesn't help my original project.\r\n\r\n@karimnosseir or @KieronQuinn , could you please post the example code fix mentioned above for the benefit of the community?  While it appears to not have been helpful for the original poster, perhaps it will be helpful for others.", "Absolutely, here you go, this is the example @karimnosseir sent to me. https://drive.google.com/file/d/1ZY6Ku9VE10Qt-eeLgAtk7aiGSzRbWGqw/view?usp=sharing\r\n\r\nAs I said above, I was unable to solve the issue on my project, and ended up scrapping Hexagon support. If you get yours working, please let me know how so I can see if the same works for me! ", "Thank you @KieronQuinn !", "@deanh-sr Please let us know if you have any questions. \r\nThanks @KieronQuinn for sharing.\r\nI am sorry the example wasn't enough to know what's the problem in your case.\r\n\r\nThanks", "I'm currently using the fixed version of the image classification example provided by @KieronQuinn above, and it doesn't appear to actually work correctly for me.  On the app GUI I am able to select the HEXAGON device, and it appears to continue classifying.  However, if I inspect the run logs I find the following:\r\n```\r\nD/tensorflow: CameraActivity: Updating  device: HEXAGON\r\nD/tensorflow: ClassifierActivity: Closing classifier.\r\nD/tensorflow: ClassifierActivity: Creating classifier (model=QUANTIZED_EFFICIENTNET, device=HEXAGON, numThreads=1)\r\nE/tflite: Failed to load libhexagon_interface.so, Error: dlopen failed: library \"libadsprpc.so\" not found: needed by /data/app/~~HRSw6P2_Mad6sIxoxJc_5Q==/org.tensorflow.lite.examples.classification-fjRemGNRXWPUEZgIsd26Pg==/lib/arm64/libhexagon_interface.so in namespace classloader-namespace\r\nI/tflite: Hexagon Delegate is not supported.\r\nD/ClassifierWithSupport: Hexagon delegate is not supported on this device.\r\nD/ClassifierWithSupport: Created a Tensorflow Lite Image Classifier.\r\n```\r\nIt looks like loading the hexagon delegate failed, but classification continued using another mechanism?  In any case, it's a quiet failure, but appears to fail to load the hexagon delegate.\r\n\r\nI was able to address the specific error by providing the missing library file libadsprpc.so (from the Qualcomm Hexagon SDK), however now I see the following message instead:\r\n```\r\nD/tensorflow: CameraActivity: Updating  device: HEXAGON\r\nV/ClassifierWithSupport: Timecost to load the image: 3\r\nV/ClassifierWithSupport: Timecost to run model inference: 12\r\nD/tensorflow: ClassifierActivity: Closing classifier.\r\nD/tensorflow: ClassifierActivity: Creating classifier (model=QUANTIZED_EFFICIENTNET, device=HEXAGON, numThreads=1)\r\nW/tflite: Failed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide.\r\nI/tflite: Hexagon Delegate is not supported.\r\nD/ClassifierWithSupport: Hexagon delegate is not supported on this device.\r\nD/ClassifierWithSupport: Created a Tensorflow Lite Image Classifier.\r\n```\r\nAgain, this is a quiet failure and from the app it appears that classifications continued.\r\n\r\nIt's not clear to me what's missing at this point.  I haven't changed the build.gradle file or the hexagon library files that were included in the project, so I'm doubtful that the problem is indeed due to incompatible versions.\r\n\r\n@karimnosseir or @KieronQuinn , are you certain that this updated example app was loading the hexagon delegate correctly in your testing?  Is it possible that it was failing quietly for you as well?\r\n", "@deanh-sr \r\n\r\n1) This is an OS provided library, you should not add this library. What device are you using ? are you sure it has supported QC SoC ?\r\n\r\n2) Please re-download the libhexagon latest version and use them with the nightly to make sure you are using compatible versions. https://www.tensorflow.org/lite/performance/hexagon_delegate#step_2_add_hexagon_libraries_to_your_android_app\r\n\r\nRe \"are you certain that this updated example app was loading the hexagon delegate correctly in your testing\"\r\n\r\nYes", "Thanks for the speedy reply @karimnosseir !\r\n\r\n> This is an OS provided library, you should not add this library. \r\n\r\nOk, good to know.  I checked the device file system, and the library does exist under /system/vendor/lib.  So I'm not sure why it isn't found correctly, I'll have to figure that out.\r\n\r\n> What device are you using ? are you sure it has supported QC SoC ?\r\n\r\nThe device is a Samsung Galaxy Tab S7, which has a Qualcomm 865+ (Hexagon 698).  It is not explicitly listed as a supported device, however I believed it would be supported based on this thread: https://github.com/tensorflow/tensorflow/issues/39035\r\nIs there a way to determine explicitly if a device is supported?  Alternatively, do you know if this device is or is not supported?\r\n\r\n> Please re-download the libhexagon latest version and use them with the nightly to make sure you are using compatible\r\n\r\nOk, I re-downloaded hexagon_nn_skel_v1.20.0.1.  The lib_support build.gradle is configured to use the TensorFlow Lite nightly snapshots:\r\n```\r\n    // Build off of nightly TensorFlow Lite\r\n    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly-SNAPSHOT'\r\n    implementation 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly-SNAPSHOT'\r\n    implementation 'org.tensorflow:tensorflow-lite-hexagon:0.0.0-nightly-SNAPSHOT'\r\n\r\n    implementation 'org.tensorflow:tensorflow-lite-support:0.3.0'\r\n```\r\n(I also tried using the nightly snapshot for tensorflow-lite-support)\r\n\r\nI still get the same error.\r\n\r\n> Re \"are you certain that this updated example app was loading the hexagon delegate correctly in your testing\"\r\nYes\r\n\r\nOk, thank you for confirming.  Just wanted to check as the failure was not immediately obvious on my device.", "Can you run these commands with the device plugged in and share the output\r\n\r\n```\r\nadb shell cat /proc/cpuinfo | grep Hardware\r\nadb shell cat /sys/devices/soc0/soc_id\r\nadb shell cat /sys/devices/system/soc/soc0/id\r\n```", "```\r\ngts7lwifi:/ $ cat /proc/cpuinfo | grep Hardware\r\nHardware        : Qualcomm Technologies, Inc KONA\r\ngts7lwifi:/ $ cat /sys/devices/soc0/soc_id\r\n356\r\ngts7lwifi:/ $ cat /sys/devices/system/soc/soc0/id\r\ncat: /sys/devices/system/soc/soc0/id: No such file or directory\r\n```\r\nI checked the full output of `cat /proc/cpuinfo` and there wasn't anything on the next line.", "Thanks for sharing the info. This SoC should be supported by the delegate.\r\n\r\nCan you try the [benchmark](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark) tool and share the log and commands used.\r\n\r\n\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53380\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53380\">No</a>\n"]}]