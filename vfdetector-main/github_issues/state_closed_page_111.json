[{"number": 51718, "title": "Symmetric convolved with symmetric returns non-symmetric", "body": "**System information**\r\n- OS Platform and Distribution: Arch Linux 5.13.12\r\n- TensorFlow installed from: pip\r\n- TensorFlow version: v2.6.0-rc2-32-g919f693420e 2.6.0\r\n- Python version: 3.9\r\n- CUDA/cuDNN: running on CPU for now\r\n\r\nWhen convolving a symmetric kernel with itself I expect the result to be also symmetric. This is what I get when using numpy, scipy or JAX, but for some reason it is not the case when using TensorFlow. I don't know if I could not understand something in the documentation, if it is like this on purpose, but it is surely not obvious. \r\n\r\nIf this is not a bug, why does TensorFlow work like this, and how could I overcome this behaviour to get a 2D cross-correlation/convolution right?\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nSee comparison bellow considering a (symmetric) laplacian filter being convolved with itself:\r\n\r\n```python\r\nimport numpy as np\r\n\r\ndef test_convolution_jax(filt):\r\n    from jax import numpy as jnp, lax\r\n    filt = jnp.asarray(filt)\r\n    return lax.conv(filt[:, :, None, None],\r\n                    filt[:, :, None, None],\r\n                    window_strides=(1, 1),\r\n                    padding='SAME')[:, :, 0, 0]\r\n\r\ndef test_convolution_tf(filt):\r\n    import tensorflow as tf\r\n    filt = tf.Variable(filt)\r\n    return tf.nn.convolution(filt[:, :, None, None],\r\n                             filt[:, :, None, None],\r\n                             strides=(1, 1),\r\n                             padding='SAME')[:, :, 0, 0]\r\n\r\nfilt = np.asarray([[0,  1, 0],\r\n                   [1, -4, 1],\r\n                   [0,  1, 0]],\r\n                   dtype=np.float32)\r\n\r\nprint('JAX:', test_convolution_jax(filt), sep='\\n', end=2 * '\\n')\r\nprint('TF:', test_convolution_tf(filt), sep='\\n',)\r\n```\r\n\r\nOutput:\r\n```\r\nJAX:\r\n[[ 1. -4.  1.]\r\n [-4. 18. -4.]\r\n [ 1. -4.  1.]]\r\n\r\nTF:\r\ntf.Tensor(\r\n[[ 1. -4.  1.]\r\n [-8. 18. -8.]\r\n [ 1. -4.  1.]], shape=(3, 3), dtype=float32)\r\n```", "comments": ["Sorry, only now I realized that dimensions work differently here. Closing this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51718\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51718\">No</a>\n"]}, {"number": 51717, "title": "Fix crash of tf.image.pad_to_bounding_box with large input value.", "body": "This PR tries to address one of the issues raised in #46890\r\nwhere tf.image.pad_to_bounding_box will crash with large input\r\nvalue.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 51716, "title": "Grappler graph optimizations may add nondeterministic behavior", "body": "**System information**\r\n- OS windows 10\r\n- tensorflow-gpu==2.5.1\r\n- python=3.7\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: 471.68-desktop-win10-win11-64bit-international-dch-whql\r\n\r\n**Got non-determinism on GPU.I found out it's the problem of Grappler graph optimizations,then i set this option false,but the train is more slower than cpu.**\r\noptions = {'implementation_selector':False}\r\ntf.config.optimizer.set_experimental_options(options)\r\n\r\nhttps://github.com/tensorflow/community/blob/master/rfcs/20210119-determinism.md#grappler-changes\r\n**I found out this document may help me to fix this problem ,but don't know how to disabled timeout with a RewriteConfig option on tensorflow-gpu==2.5.1.**  \r\n", "comments": ["@jackyesf,\r\n\r\nCan you try adding this line in your code and see if it helps in any way,\r\n\r\n```python\r\nimport os\r\nos.environ['TF_CUDNN_DETERMINISTIC']='1'\r\n```\r\n\r\nAlso please share a simple reproducible code to expedite the trouble-shooting process? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 51715, "title": "Fix crash with tf.summary.create_file_writer when non-scalar values are passed", "body": "This PR tries to fix the issue raised in #46909 where\r\ntf.summary.create_file_writer crashes when non-scalar values are passed.\r\n\r\nThis PR fixes #46909.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 51714, "title": "R2.2", "body": "Hii ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51714) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 51713, "title": "How to modify ckpt value", "body": "### First,\r\nI can get variable name by  `tf.train.list_variables('./ckpt/ckpt-30')`\r\n\r\n### Second,\r\n[https://www.tensorflow.org/guide/checkpoint#manual_checkpointing](url)\r\nI can get value by :\r\n```\r\nreader = tf.train.load_checkpoint('./ckpt/ckpt-30')\r\nshape_from_key = reader.get_variable_to_shape_map()\r\ndtype_from_key = reader.get_variable_to_dtype_map()\r\nsorted(shape_from_key.keys())\r\nkey = 'net/l1/kernel/.ATTRIBUTES/VARIABLE_VALUE'\r\nreader.get_tensor(key)\r\n```\r\n\r\nresult is `array([[4.715253 , 4.832879 , 4.754512 , 4.891385 , 4.8498716]], dtype=float32)`\r\n\r\n### So\r\nI want to modify ckpt value,  but I don't know how to do it.\r\n\r\n```\r\nmodel = Model()\r\nckpt = tf.train.Checkpoint(model=model)\r\nstatus = ckpt.restore('./ckpt/ckpt-30')\r\n\r\n```\r\nCan I get the value from ckpt, and modify value, and  ckpt.save() or ckpt.write(),   get a new ckpt file?\r\nPlease show me simple code, thanks!!!!!!!\r\n", "comments": ["so you want to modify a single weight? Is this correct? ", "If this is the case I thin the best and most convenient solution is to load the model form the checkpoint ,modify the layer with set_weights() and save the model in a checkpoint ", "> If this is the case I thin the best and most convenient solution is to load the model form the checkpoint ,modify the layer with set_weights() and save the model in a checkpoint\r\n\r\nI want to merge conv and BN manually, so I need load checkpoint and modify some value of conv,  delete BN value and save a new checkpoint. How can I implement this operation\uff1f Thank you for reply!", "Hi @qinhuan !\r\nThis is not a bug or feature request, for any further queries you may open this issue in TF discussion[ forum](https://discuss.tensorflow.org/) as there is a larger community there.Thank you!", "> If this is the case I thin the best and most convenient solution is to load the model form the checkpoint ,modify the layer with set_weights() and save the model in a checkpoint\r\n\r\n@vulkomilev Thanks for reply!  I  found a way to solve my problem.\r\nfirst, \r\n```\r\nmodel = Model()\r\ndir(model)\r\nmodel.variables\r\n```\r\n\r\nand we need inputs, and inference; then `model.variables` will have values.\r\n```\r\ninputs=[tf.random.uniform([10000,3],dtype=tf.float32)*60.0, tf.constant([10000],dtype=tf.int32)]\r\n_ = model(inputs)\r\ntype(model.variables) \r\n<class 'list'>\r\n```\r\nload ckpt file, `model.variables `will be changed;\r\n```\r\nckpt = tf.train.Checkpoint(model=model)\r\nstatus = ckpt.restore('./ckpt/ckpt-30')\r\n```\r\n\r\nnow, we can modify the value by` assgin`.\r\n\r\n```\r\ntmp1 = tf.zeros([8])\r\n<tf.Tensor: shape=(8,), dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>\r\nmodel.variables[58].assign(tmp1)\r\nckpt.save('/path/to/save/ckpt')\r\n```\r\n"]}, {"number": 51712, "title": "[tf2.6] Loaded subclassed-Model fails to be retrained", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\n-- Dockerfile: \r\n```\r\nFROM python:3.8\r\n\r\nWORKDIR /usr/src/app\r\n\r\nCOPY requirements.txt ./\r\nRUN pip install --no-cache-dir -r requirements.txt\r\n\r\nCOPY . .\r\n\r\nCMD [ \"python\", \"./test.py\" ]\r\n```\r\n-- requirements.txt:\r\n```\r\nnumpy\r\ntensorflow\r\n```\r\nand\r\n`docker build -t python-docker .`\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not tested\r\n- TensorFlow installed from (source or binary): Docker on Mac mini (2018)\r\n- TensorFlow version (use command below): TF2.6\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): Not tested, unlikely to be related to this issue\r\n- GCC/Compiler version (if compiling from source): Not tested, unlikely to be related to this issue\r\n- CUDA/cuDNN version: No CUDA on this Mac mini\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nTyping `docker run -it --rm -v \"$PWD\":/usr/src/app -w /usr/src/app python-docker python test.py` in my environment will end up getting the following error. More specifically, `predict(x, y)` works perfectly, however, `fit(x, y)` is the cause.\r\n\r\n```\r\nValueError: No gradients provided for any variable: ['abs:0', 'non_distributional_model/dense_1/kernel:0', 'non_distributional_model/dense_1/bias:0', 'non_distributional_model/dense_2/kernel:0', 'non_distributional_model/dense_2/bias:0', 'non_distributional_model/output_layer/kernel:0', 'non_distributional_model/output_layer/bias:0'].\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe loaded model should be retrainable.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom tensorflow.keras.layers import Dense\r\n\r\nBATCH_SIZE = 2\r\nNUM_ACTION = 11\r\nSTATE_DIM = 1\r\n\r\n\"\"\"Customer Loss functions.\"\"\"\r\ndef _huber_loss(y_true, y_pred, max_grad=1.):\r\n    a = tf.abs(y_true - y_pred)\r\n    less_than_max = 0.5 * tf.square(a)\r\n    greater_than_max = max_grad * (a - 0.5 * max_grad)\r\n    return tf.where(a <= max_grad, x=less_than_max, y=greater_than_max)\r\n\r\n@tf.keras.utils.register_keras_serializable()\r\nclass MeanHuberLoss(keras.losses.Loss):\r\n    def __init__(self, name='mean_huber_loss', **kwargs):\r\n        super(MeanHuberLoss, self).__init__(name=name, **kwargs)\r\n\r\n    def call(self, y_true, y_pred):\r\n        error = _huber_loss(y_true, y_pred)\r\n\r\n        # The reduce_mean is automatically done as default\r\n        return error\r\n\r\n@tf.keras.utils.register_keras_serializable()\r\nclass DirectMappingForAbs(keras.metrics.Metric):\r\n    def __init__(self, name='direct_map_for_abs', **kwargs):\r\n        super(DirectMappingForAbs, self).__init__(name=name, **kwargs)\r\n        self.output_value = tf.Variable(initial_value=[], name='abs', shape=(None, ), validate_shape=False, dtype=tf.float32)\r\n\r\n    def update_state(self, values, sample_weight=None):\r\n        self.output_value.assign(values)\r\n\r\n    def result(self):\r\n        return self.output_value\r\n\r\n    def reset_state(self):\r\n        self.output_value.assign([])\r\n\r\n@tf.keras.utils.register_keras_serializable()\r\nclass NonDistributionalModel(keras.Model):\r\n    def __init__(self, \r\n                 name=\"non_distributional_model\",\r\n                 num_output=None,\r\n                 trainable=True):\r\n        super(NonDistributionalModel, self).__init__(name=name)\r\n\r\n        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\r\n        self.abs_metric = DirectMappingForAbs(name=\"abs\") # Returns a tensor with the same shape of the input tensors\r\n        self.criterion = MeanHuberLoss()\r\n\r\n        self.layer_1 = Dense(10, trainable=trainable, activation='relu', name=\"dense_1\")  \r\n        self.layer_2 = Dense(10, trainable=trainable, activation='relu', name=\"dense_2\")\r\n        self.output_layer = Dense(num_output, trainable=trainable, activation=None, name=\"output_layer\")\r\n\r\n    def call(self, inputs):\r\n        inputs = tf.cast(inputs, tf.float32)\r\n        layer_1 = self.layer_1(inputs)\r\n        layer_2 = self.layer_2(layer_1)\r\n        output = self.output_layer(layer_2)\r\n\r\n        return output\r\n\r\n    @tf.function\r\n    def train_step(self, data):\r\n        states, targets = data\r\n\r\n        targets = tf.stop_gradient(targets)\r\n        with tf.GradientTape() as tape:\r\n            logits = self(states, training=True)  # Forward pass\r\n            loss = self.criterion(targets, logits)\r\n\r\n        trainable_vars = self.trainable_variables\r\n        grads = tape.gradient(loss, trainable_vars)\r\n        self.optimizer.apply_gradients(zip(grads, trainable_vars))\r\n\r\n        self.loss_tracker.update_state(loss)\r\n        self.abs_metric.update_state(tf.cast(tf.reduce_mean(tf.math.abs(targets - logits), axis=-1), tf.float32))\r\n        \r\n        return {\"loss\": self.loss_tracker.result(), \"abs\": self.abs_metric.result()}  \r\n\r\n    @property\r\n    def metrics(self):\r\n        return [self.loss_tracker, self.abs_metric]   \r\n\r\nclass History(keras.callbacks.Callback):\r\n    def on_train_begin(self, logs={}):\r\n        self.losses = []\r\n        self.abs = []\r\n\r\n    def on_train_batch_end(self, batch, logs={}):\r\n        self.losses.append(logs.get('loss'))\r\n        self.abs.append(logs.get('abs'))\r\n\r\nclass _DQN_Model:\r\n    def __init__(self, \r\n                 alpha, \r\n                 batch_size, \r\n                 num_output, \r\n                 trainable=True):\r\n        self.alpha = alpha\r\n        self.batch_size = batch_size\r\n        self.num_output = num_output\r\n        self.trainable = trainable\r\n\r\n        self.model = self._build_model()\r\n\r\n    def _build_model(self):\r\n        lr_schedule = keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=self.alpha,\r\n                                                                       first_decay_steps=1000)\r\n\r\n        model = NonDistributionalModel(num_output=self.num_output, trainable=self.trainable)\r\n        model.compile(optimizer=Adam(lr_schedule))\r\n\r\n        return model\r\n\r\n    def predict(self, state):\r\n        return self.model.predict(state)\r\n\r\n    def train(self, states, targets):\r\n        history = History()\r\n\r\n        return self.model.fit(states, targets, batch_size=self.batch_size, epochs=1, verbose=0, callbacks=[history])\r\n\r\nclass Critic(object):\r\n    def __init__(self, alpha, batch_size):\r\n        self.alpha = alpha\r\n        self.batch_size = batch_size\r\n\r\n        self._eval_model = _DQN_Model(alpha=alpha, batch_size=batch_size, num_output=NUM_ACTION, trainable=True)\r\n\r\n    def learn(self):\r\n        x = np.random.random((self.batch_size, 1))\r\n        y = np.random.random((self.batch_size, NUM_ACTION))\r\n        history = self._eval_model.train(x, y)\r\n\r\n        return tf.squeeze(history.history['loss'])\r\n\r\n\r\ncritic = Critic(alpha=0.1, batch_size=BATCH_SIZE)\r\ncritic.learn()\r\n\r\ncritic._eval_model.model.save(\"model_saved\")\r\nloaded_model = keras.models.load_model(\"model_saved\")\r\nprint(\"Saved Model Weights: {}, Type: {}\".format(len(critic._eval_model.model.get_weights()), type(critic._eval_model.model)))\r\nprint(\"Loaded Model Weights: {}, Type: {}\".format(len(loaded_model.get_weights()), type(loaded_model)))\r\n\r\ncritic._eval_model.model.summary()\r\nloaded_model.summary()\r\n\r\nx=np.random.random((BATCH_SIZE, 1))\r\ny=np.random.random((BATCH_SIZE, NUM_ACTION))\r\n\r\n# Let's check:\r\nnp.testing.assert_allclose(\r\n    critic._eval_model.predict(x), loaded_model.predict(x)\r\n)\r\n\r\nprint(\"Continue to train the loaded model\\n\")\r\nhistory = History()\r\nresult = loaded_model.fit(x, y, batch_size=BATCH_SIZE, epochs=1, verbose=0, callbacks=[history])\r\n\r\n```\r\n\r\n**Other info / logs** \r\n```\r\n2021-08-27 12:28:38.619800: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2021-08-27 12:28:38.619860: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2021-08-27 12:28:40.667183: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2021-08-27 12:28:40.667238: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\r\n2021-08-27 12:28:40.667269: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (a3f95dfc3a77): /proc/driver/nvidia/version does not exist\r\n2021-08-27 12:28:40.667437: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-08-27 12:28:40.722464: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\r\nWARNING:tensorflow:Gradients do not exist for variables ['abs:0'] when minimizing the loss.\r\n2021-08-27 12:28:41.133216: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nSaved Model Weights: 9, Type: <class '__main__.NonDistributionalModel'>\r\nLoaded Model Weights: 9, Type: <class 'keras.saving.saved_model.load.Custom>NonDistributionalModel'>\r\nModel: \"non_distributional_model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\ndense_1 (Dense)              multiple                  20\r\n_________________________________________________________________\r\ndense_2 (Dense)              multiple                  110\r\n_________________________________________________________________\r\noutput_layer (Dense)         multiple                  121\r\n=================================================================\r\nTotal params: 253\r\nTrainable params: 251\r\nNon-trainable params: 2\r\n_________________________________________________________________\r\nModel: \"non_distributional_model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\ndense_1 (Dense)              multiple                  20\r\n_________________________________________________________________\r\ndense_2 (Dense)              multiple                  110\r\n_________________________________________________________________\r\noutput_layer (Dense)         multiple                  121\r\n=================================================================\r\nTotal params: 253\r\nTrainable params: 251\r\nNon-trainable params: 2\r\n_________________________________________________________________\r\nContinue to train the loaded model\r\n\r\nTraceback (most recent call last):\r\n  File \"test2.py\", line 182, in <module>\r\n    result = loaded_model.fit(x, y, batch_size=BATCH_SIZE, epochs=1, verbose=0, callbacks=[history])\r\n  File \"/usr/local/lib/python3.8/site-packages/keras/engine/training.py\", line 1184, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n  File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 885, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 933, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 759, in _initialize\r\n    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3066, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3463, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3298, in _create_graph_function\r\n    func_graph_module.func_graph_from_py_func(\r\n  File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 1007, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 668, in wrapped_fn\r\n    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 994, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nValueError: in user code:\r\n\r\n    /usr/local/lib/python3.8/site-packages/keras/engine/training.py:853 train_function  *\r\n        return step_function(self, iterator)\r\n    /usr/local/lib/python3.8/site-packages/keras/engine/training.py:842 step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    /usr/local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /usr/local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /usr/local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /usr/local/lib/python3.8/site-packages/keras/engine/training.py:835 run_step  **\r\n        outputs = model.train_step(data)\r\n    /usr/local/lib/python3.8/site-packages/keras/engine/training.py:791 train_step\r\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\r\n    /usr/local/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:522 minimize\r\n        return self.apply_gradients(grads_and_vars, name=name)\r\n    /usr/local/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:622 apply_gradients\r\n        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\r\n    /usr/local/lib/python3.8/site-packages/keras/optimizer_v2/utils.py:72 filter_empty_gradients\r\n        raise ValueError(\"No gradients provided for any variable: %s.\" %\r\n\r\n    ValueError: No gradients provided for any variable: ['abs:0', 'non_distributional_model/dense_1/kernel:0', 'non_distributional_model/dense_1/bias:0', 'non_distributional_model/dense_2/kernel:0', 'non_distributional_model/dense_2/bias:0', 'non_distributional_model/output_layer/kernel:0', 'non_distributional_model/output_layer/bias:0'].\r\n```\r\n\r\nThank you for your time.", "comments": ["@OniReimu \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThank you!", "@OniReimu Could you please close this issue here if you have posted the issue in keras repo, you'll get the right help there ? Thank you!", "Do you want me to post the answer here once I get the response from there?", "@OniReimu Thanks for the quick response! We want you to close this ticket if you have posted the issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues).Thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51712\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51712\">No</a>\n"]}, {"number": 51711, "title": "Fix crash with tf.range when start is large/negative", "body": "This PR tries to address the issue raised in 46899 where\r\ntf.range will crash when start is large/negative.\r\n\r\nThis PR fixes #46899.\r\n\r\nThis PR also fixes #46889.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 51710, "title": "Separable Conv2D allows dilation_rate!=1 and strides !=1", "body": "Hey Tensorflow Community :-),\r\n\r\nNothing serious :).\r\n\r\nI just noticed in tensorflow 2.4 (sry can not update to 2.7 currently since I am stuck to this version because of other compatibilty issues, so I can't tell if this also affects your newest version) that the layer Separable Conv2D states in it's documentations that if I assign another value for the dilation rate than 1 the strides are not to be allowed to have another value than 1 and vice versa. \r\n\r\n_dilation_rate: An integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1._\r\n\r\nhttps://keras.io/api/layers/convolution_layers/separable_convolution2d/\r\n\r\nBut in fact it is possible to set both parameters with values > 1. \r\n(Since this is also true (and you will get an exception if you try to apply other values than 1 for both parameters) for the normal covolutional layers I think it is either a bug or a documentation copycat issue.)\r\n\r\nCheers :)", "comments": ["@tilakrayal ", "@pokecheater,\r\n \r\nThis looks like as an issue related to keras component. Can you please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues),To know more see; [https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)", "@pokecheater,\r\n\r\nAs you have already opened an issue with `Keras` repo, please confirm if we are good to close this issue here? Thanks!", "The docs are fixed with [b2f6e34](https://github.com/keras-team/keras/commit/81b6a9b369234aff58a975c5a9c5e0101b6f93a9) Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51710\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51710\">No</a>\n"]}, {"number": 51709, "title": "'@local_config_cc//:toolchain' does not contain a toolchain for cpu 'arm64-v8a'", "body": "Hi,\r\n   I want to compiler the run_eval for android edgetu device test. but encouter the \"does not contain a toolchain for cpu 'arm64-v8a'\". Could you reference the following fail and then give me some suggestions about this?\r\nThank a lot!\r\n\r\n**System information**\r\n- OS Platform: ubuntu 20.04\r\n- Bazel version : 3.7.2\r\n\r\n** Setup environment **\r\nAndroid SDK:\r\n    - sudo apt-get install android-sdk\r\nAndroid NDK\r\n    -  wget -q https://dl.google.com/android/repository/android-ndk-r21e-linux-x86_64.zip\r\nSetup the $PATH evironment:\r\n    - echo $PATH\r\n/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:$:/usr/lib/android-sdk//tools:/usr/lib/android-sdk//platform-tools:/usr/lib/android-sdk//tools:/usr/lib/android-sdk//platform-tools:::/home/yihsin_hung/android-ndk-r21e\r\n\r\n** Build command **\r\n    cd tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification\r\n    bazel build -c opt --config=android_arm64 --cxxopt='--std=c++17' //tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification:run_eval\r\n\r\n** Build fail Log **\r\n\r\nw/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils\r\nERROR: /home/yihsin_hung/.cache/bazel/_bazel_yihsin_hung/d610790c850d0b74b4f90630e4b19108/external/local_config_cc/BUILD:47:19: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'arm64-v8a'\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1596824487 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  /home/yihsin_hung/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  /home/yihsin_hung/tensorflow/tensorflow/workspace0.bzl:108:34: in workspace\r\n  /home/yihsin_hung/.cache/bazel/_bazel_yihsin_hung/d610790c850d0b74b4f90630e4b19108/external/bazel_toolchains/repositories/repositories.bzl:35:23: in repositories\r\nRepository rule git_repository defined at:\r\n  /home/yihsin_hung/.cache/bazel/_bazel_yihsin_hung/d610790c850d0b74b4f90630e4b19108/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nERROR: Analysis of target '//tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification:run_eval' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed\r\nINFO: Elapsed time: 0.267s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)\r\n\r\n\r\n", "comments": ["Hi @yishinhung , \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].\r\n\r\nAlso please feel free to look  at this similar [issue ](https://github.com/tensorflow/tensorflow/issues/34520) for answers.", "> Hi @yishinhung ,\r\n> We see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].\r\n> \r\n> Also please feel free to look at this similar [issue ](https://github.com/tensorflow/tensorflow/issues/34520) for answers.\r\n\r\nHi @mohantym:\r\n     I use the ./configure  recommended by (https://github.com/tensorflow/tensorflow/issues/34520) but encounter issue about the Android build tool version(debian '27.0.1') conflict with bazel 3.7.2.   \r\nI search the error and find the https://github.com/bazelbuild/bazel/commit/dd666acb9d6e9ae80952baa11726a909d5fc7d79 patches. Is it mean the android build only support 26.0.1?\r\nPlease give me some suggestion about this?\r\nThank a lot!\r\n\r\n** tf version master branch**\r\ngit clone https://github.com/tensorflow/tensorflow\r\n\r\n** Execute the confgiure recommended by (https://github.com/tensorflow/tensorflow/issues/34520)**\r\nyihsin_hung@xuanhao-TS700-E7-RS8:~/tensorflow$ ./configure\r\nYou have bazel 3.7.2 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python3]:\r\nFound possible Python library paths:\r\n  /usr/lib/python3/dist-packages\r\n  /usr/local/lib/python3.8/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with CUDA support? [y/N]:\r\nNo CUDA support will be enabled for TensorFlow.\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]:\r\nClang will not be downloaded.\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -Wno-sign-compare]:\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: y\r\nSearching for NDK and SDK installations.\r\nPlease specify the home path of the Android NDK to use. [Default is /home/yihsin_hung/Android/Sdk/ndk-bundle]: /home/yihsin_hung/android-ndk-r21e\r\nPlease specify the (min) Android NDK API level to use. [Available levels: ['16', '17', '18', '19', '21', '22', '23', '24', '26', '27', '28', '29', '30']] [Default is 21]:\r\nPlease specify the home path of the Android SDK to use. [Default is /home/yihsin_hung/Android/Sdk]: /usr/lib/android-sdk\r\nPlease specify the Android SDK API level to use. [Available levels: ['23']] [Default is 23]:\r\nPlease specify an Android build tools version to use. [Available versions: ['27.0.1', 'debian']] [Default is debian]:\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n\r\n** Fail build log**\r\n~/tensorflow/tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification$ bazel build -c opt --config=android_arm64 --cxxopt='--std=c++17' //tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification:run_eval\r\n\r\nINFO: Build option --action_env has changed, discarding analysis cache.\r\nERROR: **Analysis of target '//tensorflow/lite/tools/evaluation/tasks/**imagenet_image_classification:run_eval' failed; build aborted: Bazel does not recognize Android build tools version debian****\r\nINFO: Elapsed time: 1.348s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (7 packages loaded, 11 targets configured)\r\n    currently loading: @bazel_tools//tools/jdk ... (2 packages)\r\n\r\n", "Hi @Saduf2019 ,Could you please look into this issue?", "@yishinhung \r\nCan you please refer to these issues and let us know: [link](https://github.com/tensorflow/tensorflow/issues/34520#issuecomment-557533555),[link1](https://github.com/tensorflow/tensorflow/issues/9060),[link2](https://stackoverflow.com/questions/51808001/tf-lite-android-no-default-toolchain-found-for-cpu-arm64-v8a) and let us know.", "> @yishinhung\r\n> Can you please refer to these issues and let us know: [link](https://github.com/tensorflow/tensorflow/issues/34520#issuecomment-557533555),[link1](https://github.com/tensorflow/tensorflow/issues/9060),[link2](https://stackoverflow.com/questions/51808001/tf-lite-android-no-default-toolchain-found-for-cpu-arm64-v8a) and let us know.\r\n\r\nHi @Saduf2019 \r\n     Excuse me! I don't get some information about the following log.\r\nCould you suggest us to build the \"imagenet_image_classification:run_eva\" by which version of bazel and android build tools?\r\nThank a lot!\r\n\r\n** Fail log**\r\nERROR: Analysis of target '//tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification:run_eval' failed; build aborted: Bazel does not recognize Android build tools version debian", "Did you run `./configure` script ?\r\nPlz check the following.\r\nhttps://www.tensorflow.org/lite/guide/build_android#configure_workspace_and_bazelrc", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51709\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51709\">No</a>\n"]}, {"number": 51708, "title": "Error while trying to convert TF-Hub MRCNN model to tflite", "body": "System infoemartion:\r\n\r\n- Ubuntu 18.04:\r\n- TensorFlow :2.1.0\r\n\r\n\r\nGot tf model from https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1\r\n\r\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\ninputs = tf.keras.Input(shape=(1024, 1024, 3),batch_size=1,dtype=tf.uint8)\r\nm_l = hub.KerasLayer(\"https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1\")\r\nx = m_l(inputs)\r\nmodel = tf.keras.Model(inputs=inputs, outputs=x, name=\"mrcnn\")\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\n\r\n\r\nWhen I run above python code:\r\ngot error as 2021-08-27 12:49:16.478141: F tensorflow/lite/toco/tooling_util.cc:2277] Check failed: array.data_type == array.final_data_type Array \u201cinput_1\u201d has mis-matching actual and final data types (data_type=uint8, final_data_type=float).\r\nFatal Python error: Aborted\r\n\r\n", "comments": ["@ArunaKote Can you please update the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose), it will help us  to expedite the trouble-shooting process .Thanks!", "\r\n\r\ntensorflow version is 2.7.0\r\n\r\nUsed below code to get saved_model.pb  : \r\n\r\n\r\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\nmodel_url = \"https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1\"\r\nloaded_model = hub.load(model_url)\r\ncall = loaded_model.__call__.get_concrete_function(\r\ntf.TensorSpec(shape=(1, 1024, 1024, 3), dtype=tf.uint8))\r\nsignatures = {'predict': call}\r\ntf.saved_model.save(loaded_model,'/data/aruna/maskedrnn', signatures=signatures)\r\n\r\n\r\nWhen I ran python code saved_model.pb got generated.\r\n\r\nWhen I tried with tflite_convert --saved_model_dir=$PWD --output_file=$PWD/mrcnn.tflite\r\n\r\n\r\n\r\nGot error as :\r\n\r\n\r\nloc(callsite(callsite(callsite(\"CropAndResize/CropAndResize@__inference___call___42606\" at \"StatefulPartitionedCall@__inference_restored_function_body_108980\") at \"StatefulPartitionedCall@__inference_signature_wrapper_110151\") at \"StatefulPartitionedCall\")): error: 'tf.CropAndResize' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(callsite(\"CropAndResize_1/CropAndResize@__inference___call___42606\" at \"StatefulPartitionedCall@__inference_restored_function_body_108980\") at \"StatefulPartitionedCall@__inference_signature_wrapper_110151\") at \"StatefulPartitionedCall\")): error: 'tf.CropAndResize' op is neither a custom op nor a flex op\r\nerror: failed while converting: 'main':\r\nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select\r\nTF Select ops: CropAndResize\r\nDetails:\r\ntf.CropAndResize(tensor<1x64x64x1088xf32>, tensor<?x4xf32>, tensor<100xi32>, tensor<2xi32>) -> (tensor<100x17x17x1088xf32>) : {T = f32, device = \"\", extrapolation_value = 0.000000e+00 : f32, method = \"bilinear\"}\r\ntf.CropAndResize(tensor<1x64x64x1088xf32>, tensor<?x4xf32>, tensor<300xi32>, tensor<2xi32>) -> (tensor<300x17x17x1088xf32>) : {T = f32, device = \"\", extrapolation_value = 0.000000e+00 : f32, method = \"bilinear\"}\r\nTraceback (most recent call last):\r\nFile \"/home/ubuntu/anaconda3/bin/tflite_convert\", line 8, in\r\nsys.exit(main())\r\nFile \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py\", line 697, in main\r\napp.run(main=run_main, argv=sys.argv[:1])\r\nFile \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/absl/app.py\", line 303, in run\r\n_run_main(main, args)\r\nFile \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\r\nsys.exit(main(argv))\r\nFile \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py\", line 680, in run_main\r\n_convert_tf2_model(tflite_flags)\r\nFile \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py\", line 295, in _convert_tf2_model\r\ntflite_model = converter.convert()\r\nFile \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 766, in wrapper\r\nreturn self._convert_and_export_metrics(convert_func, *args, **kwargs)\r\nFile \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 752, in _convert_and_export_metrics\r\nresult = convert_func(self, *args, **kwargs)\r\nFile \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 1032, in convert\r\nresult = _convert_saved_model(**converter_kwargs)\r\nFile \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/convert_phase.py\", line 223, in wrapper\r\nraise converter_error from None # Re-throws the exception.\r\nFile \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/convert_phase.py\", line 216, in wrapper\r\nreturn func(*args, **kwargs)\r\nFile \"/home/ubuntu/dkd/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 852, in convert_saved_model\r\nenable_mlir_converter=True)\r\nFile \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 315, in toco_convert_protos\r\nraise converter_error\r\ntensorflow.lite.python.convert_phase.ConverterError: :0: error: loc(callsite(callsite(callsite(\"CropAndResize/CropAndResize@__inference___call___42606\" at \"StatefulPartitionedCall@__inference_restored_function_body_108980\") at \"StatefulPartitionedCall@__inference_signature_wrapper_110151\") at \"StatefulPartitionedCall\")): 'tf.CropAndResize' op is neither a custom op nor a flex op\r\n:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n:0: note: loc(callsite(callsite(callsite(\"CropAndResize/CropAndResize@__inference___call___42606\" at \"StatefulPartitionedCall@__inference_restored_function_body_108980\") at \"StatefulPartitionedCall@__inference_signature_wrapper_110151\") at \"StatefulPartitionedCall\")): Error code: ERROR_NEEDS_FLEX_OPS\r\n:0: error: loc(callsite(callsite(callsite(\"CropAndResize_1/CropAndResize@__inference___call___42606\" at \"StatefulPartitionedCall@__inference_restored_function_body_108980\") at \"StatefulPartitionedCall@__inference_signature_wrapper_110151\") at \"StatefulPartitionedCall\")): 'tf.CropAndResize' op is neither a custom op nor a flex op\r\n:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n:0: note: loc(callsite(callsite(callsite(\"CropAndResize_1/CropAndResize@__inference___call___42606\" at \"StatefulPartitionedCall@__inference_restored_function_body_108980\") at \"StatefulPartitionedCall@__inference_signature_wrapper_110151\") at \"StatefulPartitionedCall\")): Error code: ERROR_NEEDS_FLEX_OPS\r\n:0: error: failed while converting: 'main':\r\nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select\r\nTF Select ops: CropAndResize\r\nDetails:\r\ntf.CropAndResize(tensor<1x64x64x1088xf32>, tensor<?x4xf32>, tensor<100xi32>, tensor<2xi32>) -> (tensor<100x17x17x1088xf32>) : {T = f32, device = \"\", extrapolation_value = 0.000000e+00 : f32, method = \"bilinear\"}\r\ntf.CropAndResize(tensor<1x64x64x1088xf32>, tensor<?x4xf32>, tensor<300xi32>, tensor<2xi32>) -> (tensor<300x17x17x1088xf32>) : {T = f32, device = \"\", extrapolation_value = 0.000000e+00 : f32, method = \"bilinear\"}", "Please consider enabling the Select TF option. https://www.tensorflow.org/lite/guide/ops_select", "\r\ntensorflow version is 2.7.0\r\n\r\nI tried  using:\r\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\ninputs = tf.keras.Input(shape=(1024, 1024, 3),batch_size=1,dtype=tf.uint8)\r\nm_l = hub.KerasLayer(\"https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1\")\r\nx = m_l(inputs)\r\nmodel = tf.keras.Model(inputs=inputs, outputs=x, name=\"mrcnn\")\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\n  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n]\r\ntflite_model = converter.convert()\r\n\r\n\r\nGot error as :\r\nWARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 125). These functions will not be directly callable after loading.\r\n2021-08-27 18:05:37.755900: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.\r\n2021-08-27 18:05:37.755958: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.\r\n2021-08-27 18:05:37.756566: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpyehqq98p\r\n2021-08-27 18:05:37.880386: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\r\n2021-08-27 18:05:37.880441: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: /tmp/tmpyehqq98p\r\n2021-08-27 18:05:38.838597: I tensorflow/cc/saved_model/loader.cc:210] Restoring SavedModel bundle.\r\n2021-08-27 18:05:40.653891: I tensorflow/cc/saved_model/loader.cc:194] Running initialization op on SavedModel bundle at path: /tmp/tmpyehqq98p\r\n2021-08-27 18:05:41.407781: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 3651218 microseconds.\r\n2021-08-27 18:05:48.846461: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1890] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\r\nFlex ops: FlexCropAndResize\r\nDetails:\r\n\ttf.CropAndResize(tensor<1x64x64x1088xf32>, tensor<?x4xf32>, tensor<100xi32>, tensor<2xi32>) -> (tensor<100x17x17x1088xf32>) : {T = f32, device = \"\", extrapolation_value = 0.000000e+00 : f32, method = \"bilinear\"}\r\n\ttf.CropAndResize(tensor<1x64x64x1088xf32>, tensor<?x4xf32>, tensor<300xi32>, tensor<2xi32>) -> (tensor<300x17x17x1088xf32>) : {T = f32, device = \"\", extrapolation_value = 0.000000e+00 : f32, method = \"bilinear\"}\r\nSee instructions: https://www.tensorflow.org/lite/guide/ops_select\r\nWARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\r\n", "The above message is just an warning. The conversion was succeeded"]}, {"number": 51707, "title": "07502269190", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n[git_test-master.zip](https://github.com/tensorflow/tensorflow/files/7064174/git_test-master.zip)\r\n", "comments": ["@kurdsh1993 ,\r\nIn order to expedite the trouble-shooting process, could you please provide a minimal code snippet and the TensorFlow version you are using.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51707\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51707\">No</a>\n"]}, {"number": 51706, "title": "[tflite] fix default of xnnpack_delegate_provider", "body": "xnnpack delegate was enabled by default on Android ARM platforms in 0b78d40a54a991e0f2b67e2a9aa5224609536552. But the corresponding default setting of xnnpack delegate provider is not changed, so users of xnnpack delegate provider, e.g., benchmark_model doesn't get right value. E.g., when `benchmark_model --help`, I got\r\n\r\n```\r\n...\r\n\t--use_xnnpack=true                       \tbool\toptional\tuse XNNPack\r\n\r\n```\r\n\r\nBut actually, it uses tflite with xnnpack enabled already. That's a bit confusing.\r\n\r\n\r\ntag @multiverse-tf who is the author of 0b78d40", "comments": ["@multiverse-tf Can you please review this PR ? Thanks!", "@multiverse-tf Can you please review this PR ? Thanks!", "@multiverse-tf Can you please review this PR ? Thanks!", "> xnnpack delegate was enabled by default on Android ARM platforms in [0b78d40](https://github.com/tensorflow/tensorflow/commit/0b78d40a54a991e0f2b67e2a9aa5224609536552). But the corresponding default setting of xnnpack delegate provider is not changed, so users of xnnpack delegate provider, e.g., benchmark_model doesn't get right value. E.g., when `benchmark_model --help`, I got\r\n> \r\n> ```\r\n> ...\r\n> \t--use_xnnpack=true                       \tbool\toptional\tuse XNNPack\r\n> ```\r\n> \r\n> But actually, it uses tflite with xnnpack enabled already. That's a bit confusing.\r\n\r\nSorry for the confusion and the late reply. The '--use_xnnpack\" option provided by the delegate provider of TFLite tooling is different from the XNNPACK-by-default feature in TFLite. Setting \"--use_xnnpack=true\" will actually cause a duplicate application of the XNNPACK delegate. I'll clarify this further in the doc asap.\r\n\r\n> \r\n> tag @multiverse-tf who is the author of [0b78d40](https://github.com/tensorflow/tensorflow/commit/0b78d40a54a991e0f2b67e2a9aa5224609536552)\r\n", "@freedomtan Can you please check @multiverse-tf's comments and keep us posted ? Thanks!", "close this since @multiverse-tf clarified it in e73d864d57dbc66e52aca7414615ac202ed79b42"]}, {"number": 51705, "title": "[PluggableDevice] Add the option to force memory growth", "body": "It's currently possible to force memory growth either from the session or the `TF_FORCE_GPU_ALLOW_GROWTH` environment variable, but some pluggable devices may need even more control than that. On some architectures and devices, allocating the memory all at once in a single is simply not an option so the pluggable devices should have the tools to prevent that from being overridden by the user.", "comments": ["@saxenasaurabh Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!", "@PatriceVignola  Can you please resolve conflicts? Thanks!"]}, {"number": 51704, "title": "Tensorflow not installing correctly", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): not sure\r\n- TensorFlow version: current\r\n- Python version: 3\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: unknown\r\n- GPU model and memory: unknown, using online IDE\r\n\r\n\r\n\r\n**Describe the problem**\r\nTensorflow not installing correctly\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nTried to import the tensorflow module\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\nEnvCommandError\r\n\r\n  Command ['/opt/virtualenvs/python3/bin/pip', 'install', '--no-deps', 'file:///home/runner/.cache/pypoetry/artifacts/0a/85/65/9562a53ef2caf00c826443d78d6f2fae460ab81e7bcf44770c09799f9c/tensorflow-2.6.0-cp38-cp38-manylinux2010_x86_64.whl'] errored with the following return code 1, and output: \r\n  Processing /home/runner/.cache/pypoetry/artifacts/0a/85/65/9562a53ef2caf00c826443d78d6f2fae460ab81e7bcf44770c09799f9c/tensorflow-2.6.0-cp38-cp38-manylinux2010_x86_64.whl\r\n  ERROR: Could not install packages due to an EnvironmentError: [Errno 122] Disk quota exceeded: '/tmp/pip-install-q4y7r3fp/tensorflow/tensorflow/_api/v2/mlir/experimental'\r\n  \r\n  WARNING: You are using pip version 19.3.1; however, version 21.2.4 is available.\r\n  You should consider upgrading via the 'pip install --upgrade pip' command.\r\n  \r\n\r\n  at /opt/virtualenvs/python3/lib/python3.8/site-packages/poetry/utils/env.py:1075 in _run\r\n      1071\u2502                 output = subprocess.check_output(\r\n      1072\u2502                     cmd, stderr=subprocess.STDOUT, **kwargs\r\n      1073\u2502                 )\r\n      1074\u2502         except CalledProcessError as e:\r\n    \u2192 1075\u2502             raise EnvCommandError(e, input=input_)\r\n      1076\u2502 \r\n      1077\u2502         return decode(output)\r\n      1078\u2502 \r\n      1079\u2502     def execute(self, bin, *args, **kwargs):\r\n\r\nexit status 1\r\n```\r\n", "comments": ["Hi @staticalliam7 , this  error stack trace looks similar to this [issue](https://github.com/Floozutter/buzzspike/issues/1) . It is related to poetry install and  this [comment](https://github.com/python-poetry/poetry/issues/1895#issuecomment-605676232) suggests to set environment variable containing installation from python.org    as it might be using python.exe from windows store. \r\n\r\nAlso try a fresh installation of Tensorflow after upgrading pip .", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51704\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51704\">No</a>\n", "I figured out the issue, I ran out of disk space on the partion I was using. Thanks for all of your help"]}, {"number": 51703, "title": "The following upstream commits caused a 15% performance degradation o\u2026", "body": "\u2026n the resnet50 model for ROCm:\r\n\r\n4193058f98378f6f67db0cc67fb68332f8396d41: increase row tiling to 64\r\n37ddd35d129280e80a847f09264118c7a0cbd894: enable tree reduction by default\r\n\r\nThe changes were \"undone\" for ROCm by guarding the previous implementation details with the TENSORFLOW_USE_ROCM macro.\r\n\r\nThe commit which increased row tiling to 64 made substantial changes to the reduce_unnested.hlo unit test to accomodate the change so the test fails on ROCm with this commit.\r\n\r\nTherefore, the reduce_unnested.hlo unit test has been disabled.", "comments": ["/cc @cheshire \r\n\r\nTwo XLA:GPU commits caused a performance regression on ROCm.\r\n\r\nThis commit undoes those specifically for ROCm.", "Hi,\r\n\r\nWe can't really undo this: tree reduction becomes necessary for correctness for many current (C128 reductions) and future (variadic reductions, fusing the output of the reductions). Could you share the HLO which has regressed?", "@cheshire \r\n\r\nThere was 1 specific XLA (fusion) kernel that regressed.\r\n\r\nI have attached the before and after HLO and also the AMD GPU specific instructions for that kernel.\r\n\r\n[fusion_235_hlo_fast.txt](https://github.com/tensorflow/tensorflow/files/7063400/fusion_235_hlo_fast.txt)\r\n[fusion_235_hlo_slow.txt](https://github.com/tensorflow/tensorflow/files/7063401/fusion_235_hlo_slow.txt)\r\n[fusion_235_amd_gpu_fast.txt](https://github.com/tensorflow/tensorflow/files/7063402/fusion_235_amd_gpu_fast.txt)\r\n[fusion_235_amd_gpu_slow.txt](https://github.com/tensorflow/tensorflow/files/7063403/fusion_235_amd_gpu_slow.txt)\r\n", "That's LLVM IR, not HLO.", "Given that we don't have AMD hardware, do you think you could investigate this and figure out what causes the slowdown?", "@cheshire \r\n\r\nMy apologies.  Here is the HLO for the slow fusion_235.\r\n\r\n[fusion_235_hlo.zip](https://github.com/tensorflow/tensorflow/files/7063576/fusion_235_hlo.zip)\r\n\r\nI updated the attachment to include all files from the HLO dump with fusion_235.\r\n", "That's the fused computation only, could you also post the whole thing?\n\nOn Thu, Aug 26, 2021 at 6:13 PM rsanthanam-amd ***@***.***>\nwrote:\n\n> @cheshire <https://github.com/cheshire>\n>\n> My apologies. Here is the HLO for the slow fusion_235.\n>\n> fusion_235_hlo.txt\n> <https://github.com/tensorflow/tensorflow/files/7063495/fusion_235_hlo.txt>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/51703#issuecomment-906847572>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AACVGH332OW4TX5PKHSNW73T63RDLANCNFSM5C4K3ERA>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n>\n", "The archive file size was too large.  I emailed it.", "I've looked at the dump on my A6000. Let me know if you get radically different numbers.\r\n\r\nThe overall runtime with and without\u00a0tree reduction seems to be within noise difference: 471ms vs. 476ms.\r\nLooking concretely at\u00a0`fusion_235`, it runs within 67% of the roofline with tree reduction (not great, but really acceptable), and within 87% without (again better, but not enough to justify lack of determinism).\r\n\r\nIf you do get radically different numbers on AMD, it seems the only way forward would be for you to try to debug this further where the slowdown is coming from. Maybe, AMD GPUs have a different amount of registers and reductions start to spill? This was a large problem for us when tuning these heuristics.\r\nThe proper fix would be to tune the heuristics for AMD, not to disable the optimization altogether.\r\n\r\nWe don't have deterministic output without tree reductions, and this is a must-have for many customers.", "@cheshire I am not familiar with how to measure an XLA kernel such as fusion_235 with and without tree reduction.  Please let me know how I can make similar measurements on ROCm.  Also, I am not familiar with what is meant by \"67% of roofline\".", " - \"roofline\" is the best-possible performance. This is determined by\nbandwidth of the GPU (e.g. 650GB/s for Titan V), and the size of the\ninput/output buffers.\nAs an example, if we are reducing an array of size 650GB on Titan V, and\nthe operation took one second, we can't possibly do better (100% roofline).\nIf it takes 2 seconds, that's 50% roofline, etc.\n\n> I am not familiar with how to measure an XLA kernel such as fusion_235\nwith and without tree reduction\nYou can set env XLA_FLAG=--xla_gpu_deterministic_reductions=false ...\n\nOn Mon, Aug 30, 2021 at 2:43 PM rsanthanam-amd ***@***.***>\nwrote:\n\n> @cheshire <https://github.com/cheshire> I am not familiar with how to\n> measure an XLA kernel such as fusion_235 with and without tree reduction.\n> Please let me know how I can make similar measurements on ROCm. Also, I am\n> not familiar with what is meant by \"67% of roofline\".\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/51703#issuecomment-908721250>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AACVGH63UZP3ZQ2CES5UIBDT7P3QHANCNFSM5C4K3ERA>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n>\n", "@cheshire Sorry, I have should been more specific.  I am not sure how to measure the XLA kernel performance as you did (471ms vs. 476ms etc.).", "Hi,\n\nYou could use https://www.tensorflow.org/tensorboard, where the trace view\nshould tell you how much time each kernel takes (the kernel name will\ncorrespond to the fusion name).\n\nAlternatively, given that you work for AMD, I'm sure that AMD has profilers\nand tools to give you the trace view which would show you each kernel on\nthe timeline, and that you can get access to that expertise better than I\ncan.\n\nOn Tue, Aug 31, 2021 at 4:07 AM rsanthanam-amd ***@***.***>\nwrote:\n\n> @cheshire <https://github.com/cheshire> Sorry, I have should been more\n> specific. I am not sure how to measure the XLA kernel performance as you\n> did (471ms vs. 476ms etc.).\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/51703#issuecomment-909134865>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AACVGHYAD67WEPRXGC3E36TT7SZXBANCNFSM5C4K3ERA>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n>\n", "@cheshire I measured a large difference in execution time for fusion_235 on ROCm:\r\n\r\n| Configuration | fusion_235 execution time |\r\n| --------------- | ------------------------------|\r\n| tree reduction disabled and row tiling size = 16 | 4.3 ms |\r\n| tree reduction enabled and row tiling size = 64 | 17.6 ms |\r\n\r\nThis was captured using a very short model run.", "OK but could you debug on what is actually going on? Why is it slow? Is it spilling registers? How far are these numbers from theoretical roofline?", "@cheshire I am currently in debugging mode.\r\n\r\nBy inspecting the HLO, I noticed that on CUDA, XLA uses NHWC for fp16 but on ROCm, it uses NCHW for fp16.\r\n\r\nI'm wondering if at a high level, that could explain the performance difference on ROCm and the lack of a performance difference on CUDA.\r\n", "What %% of the overall benchmark runtime is spent in convolutions?\n\nOn Wed, Sep 1, 2021 at 4:29 AM rsanthanam-amd ***@***.***>\nwrote:\n\n> @cheshire <https://github.com/cheshire> I am currently in debugging mode.\n>\n> By inspecting the HLO, I noticed that on CUDA, XLA uses NHWC for fp16 but\n> on ROCm, it uses NCHW for fp16.\n>\n> I'm wondering if at a high level, that could explain the performance\n> difference on ROCm and the lack of a performance difference on CUDA.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/51703#issuecomment-910195483>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AACVGH7QO527CGIM6TGJCATT7YFBLANCNFSM5C4K3ERA>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n>\n", "@cheshire \r\n\r\nHere is the % of time spent in convolution kernels on ROCm with this resnet50 model:\r\n\r\nNo Tree Reductions/Row tiling size = 16: 23.6%\r\nTree Reductions/Row tiling size = 64: 19.3%\r\n", "@cheshire \r\n\r\nTree reduction cracks a single reduction op into 2 reduction ops to avoid the need for atomics.\r\n\r\nHow does this guarantee deterministic output?\r\n", "Because at most one block writes to one output location.\n\nOn Thu, Sep 2, 2021 at 5:53 AM rsanthanam-amd ***@***.***>\nwrote:\n\n> @cheshire <https://github.com/cheshire>\n>\n> Tree reduction cracks a single reduction op into 2 reduction ops to avoid\n> the need for atomics.\n>\n> How does this guarantee deterministic output?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/51703#issuecomment-911647191>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AACVGH5IV2I4IVK7PWNJ6I3T75XUTANCNFSM5C4K3ERA>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n>\n", "@rsanthanam-amd Can you please resolve conflicts? Thanks!", "@gbaned Done.", "@gbaned This is not awaiting response.\r\n\r\n@rsanthanam-amd Sorry for the confusion. Deterministic output is a requirement, we can't conditionally disable it for AMD. Do you think you could try to figure out where exactly the slowdown is coming from?", "@cheshire I am still working on that.", "@cheshire So the reason why the performance is bad is because there are not enough registers and this is causing excessive register spills (exactly what you conjectured above).\r\n\r\nThe source of this is the row tiling size of 64 (before 4193058f98378f6f67db0cc67fb68332f8396d41, the row tiling size was 16 for ROCm).\r\n\r\nIn the context of the bad performing fusion kernel, increasing the row tiling size to 64 causes the num_threads_x to be set to 64 in this logic from ir_emitter_unnested.cc:\r\n\r\n```  \r\nint64_t num_threads_x = [&] {\r\n    if (reduction_dimensions.is_row_reduction) {\r\n      // Use 512 as default block size (threads per block) for row reductions.\r\n      // For multi-output fusions, reduce the block size further to decrease\r\n      // register pressure when multiple outputs are computed by each thread.\r\n      int64_t fan_out = fusion.getFusionRoots().size();\r\n      int64_t max_block_size =\r\n          std::max(kMinThreadsXRowReduction,\r\n                   static_cast<int64_t>(512LL / NearestPowerOfTwo(fan_out)));\r\n      return std::min(\r\n          max_block_size,\r\n          RoundUpToNearest(CeilOfRatio(reduction_dimensions.dimensions[2],\r\n                                       reduction_tiling[2]),\r\n                           kWarpSize));\r\n    }\r\n    return kWarpSize;\r\n  }();\r\n```\r\n\r\nHowever, if the row tiling size is set to 16, then num_threads_x is set to 224.  This results in a larger work group size which allows the AMD compiler to generate better code to avoid register spills.\r\n\r\nI came up with several ways to address this but I not sure if any of them will be acceptable:\r\n\r\n1. Keep the row tiling size to 16 and increase the value of `kMinThreadsXRowReduction` to 1024 to prevent bad HLO from being generated because of the following code:\r\n\r\n```\r\nbool ReductionIsRaceFree(const ReductionDimensions& reduction_dimensions,\r\n                         const std::array<int64_t, 3>& reduction_tiling) {\r\n  return (reduction_dimensions.is_row_reduction &&\r\n          reduction_dimensions.dimensions[2] <=\r\n              kMinThreadsXRowReduction * reduction_tiling[2] &&\r\n          reduction_dimensions.dimensions[0] <=\r\n              kBatchedReductionRaceFreeBound) ||\r\n         (!reduction_dimensions.is_row_reduction &&\r\n          reduction_dimensions.dimensions[1] <=\r\n              kWarpSize * reduction_tiling[1]);\r\n}\r\n```\r\n\r\n2. Keep the row tiling size set to 64 and make the following change:\r\n\r\n```  \r\nint64_t num_threads_x = [&] {\r\n    if (reduction_dimensions.is_row_reduction) {\r\n      // Use 512 as default block size (threads per block) for row reductions.\r\n      // For multi-output fusions, reduce the block size further to decrease\r\n      // register pressure when multiple outputs are computed by each thread.\r\n      int64_t fan_out = fusion.getFusionRoots().size();\r\n      int64_t max_block_size =\r\n          std::max(kMinThreadsXRowReduction,\r\n                   static_cast<int64_t>(512LL / NearestPowerOfTwo(fan_out)));\r\n      return std::min(\r\n          max_block_size,\r\n          RoundUpToNearest(CeilOfRatio(reduction_dimensions.dimensions[2],\r\n#ifdef TENSORFLOW_USE_ROCM\r\n                                       static_cast<int64_t>(16)),\r\n#else\r\n                                       reduction_tiling[2]),\r\n#endif\r\n                           kWarpSize));\r\n    }\r\n    return kWarpSize;\r\n  }();\r\n```\r\n\r\nIf neither of these are acceptable, please suggest an alternative approach to addressing this issue.\r\n", "> @cheshire So the reason why the performance is bad is because there are not enough registers and this is causing excessive register spills (exactly what you conjectured above).\r\n\r\nCan we get ROCm to log in these cases? We do such logging for ptxas.\r\n\r\nHow many reductions are fused together in the problematic case? Is it just one?\r\n\r\nAlso this seems like a bug in register allocation algorithm, right? With the unrolled tile, there should be no need to keep all the values alive?", "@cheshire Actually, I don't think there is a bug in the register allocation algorithm.\r\n\r\nWhen row tiling size is increased, that causes more values to be kept live therefore increases register usage.\r\n\r\nThe register spilling can be eliminated by enabling vectorization and memory coalescing but that is not main issue - the main issue is that the XLA reduction heuristics do not allocate enough x-threads for the fusion kernel of interest.\r\n\r\nI have updated the PR by first reverting the original proposed fix with a new one which enables tree reduction, keeps row tiling at 16, and changes kMinThreadsXRowReduction to 1024 to prevent bad HLO from being generated.\r\n\r\nThese changes restore most of the performance loss (while preserving tree reduction).\r\n\r\nHere is a picture of our thread tracing tool which shows instructions over time in a wavefront (warp) in the poor performance case:\r\n\r\n![pic1](https://user-images.githubusercontent.com/70280935/138703893-c17f3d43-461a-4ab1-a52c-69a7d2e4f868.png)\r\n\r\nThe computation is very sparse since the wave is spending most of its time waiting for data to and from memory.\r\n\r\nThe illustration show multiple waves but very little computation can be overlapped since there are not enough threads allocated for the kernel.\r\n\r\nHere is a picture of the instructions over time with the proposed fix:\r\n\r\n![fix](https://user-images.githubusercontent.com/70280935/138704086-4780a715-022f-4a9f-8753-321b0fa816f5.png)\r\n\r\nClearly, much more computation can be overlapped per unit time since more threads are in flight and memory latency can be hidden by occupying the GPU with useful work from more threads.\r\n\r\nHere is an depiction of overall CU usage corresponding to the poor performance case:\r\n\r\n![cu_bad](https://user-images.githubusercontent.com/70280935/138704274-568fca45-bbdb-4ea8-b646-61090a4596e1.png)\r\n\r\nHere is a depiction of overall CU usage corresponding to the proposed fix case:\r\n\r\n![cu_fix](https://user-images.githubusercontent.com/70280935/138704575-4e66a8d9-fc83-4f7f-acb4-ae960f88fbeb.png)\r\n\r\nNotice the significantly better CU utilization for the proposed fix.\r\n\r\n", "Thanks a lot, this seems exactly the right fix! Would you need to update the tree reduction test since bounds have changed?", "> Thanks a lot, this seems exactly the right fix! Would you need to update the tree reduction test since bounds have changed?\r\n\r\nThe tree reduction rewriter test passes without modification on ROCm.", "Nice! Probably because you up the # of threads by factor of 4, but also reduce the tile size by a factor of 4."]}, {"number": 51702, "title": "[oneDNN] Add Conv3D fusions", "body": "_FusedConv3D op and kernel\r\nConv3D + Bias fusion\r\nConv3D + Bias + activation fusion\r\nConv3D + squeeze + Bias fusion", "comments": ["@penpornk Can you please review this PR ? Thanks!"]}, {"number": 51700, "title": "How to use StreamingFilesDataset", "body": "**System information**\r\n- TensorFlow version (you are using):\r\n2.6\r\n- Python version (you are using):\r\n3.7\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI open a colab tpu runtime and try to run this [notebook](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/keras_recipes/ipynb/creating_tfrecords.ipynb?hl=en)\r\n\r\nI use your sample code of tpu setting and follow the instructions of\r\n**StreamingFilesDataset** in **tensorflow.python.tpu.datasets.py**\r\nand create similar function get_dataset2() like original one.\r\n\r\nBut it didn't work. [My adapted notebook](https://colab.research.google.com/drive/1lEUjjWffl5Vsh1JuGLzshl2bYaduCZD6?usp=sharing)\r\nIt raised errors like\r\n1. InvalidArgumentError: `cycle_length` must be > 0 [Op:LegacyParallelInterleaveDatasetV2]\r\nor\r\n2. source_iterator has no attribute string_handle()\r\n\r\n\r\nYou can quickly find my adaptions about tpu setting and get_dataset2()\r\nfrom table. \r\n\r\nI have checked some issue reports and know that it probably is a conflict of version. But I downgrade the tensorflow version to 1.x, it still didn't work.\r\n\r\nWould you help me find the solution of using StreamingFilesDataset().\r\nThanks a lot!", "comments": ["@triper1022 ,\r\nCan you please refer this link [1](https://github.com/tensorflow/models/issues/3394) and [2](https://github.com/tensorflow/tensorflow/blob/9b82752179bd4a61a9d33a638b8d9cb06adcf9e0/tensorflow/python/tpu/datasets.py#L50) for information regarding StreamingFilesDataset.It helps.Thanks!", "Thanks a lot for reply.\r\nAlthough I studied for hours in the references, I think that I am not very familiar with TF 1.x backend. I will try my best to understand it."]}, {"number": 51699, "title": "kernel error", "body": "WARNING:tensorflow:AutoGraph could not transform <bound method SequencePoolingLayer.call of <deepctr.layers.sequence.SequencePoolingLayer object at 0x134a4d520>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Index'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n5 6\r\nWARNING:tensorflow:AutoGraph could not transform <bound method DNN.call of <deepctr.layers.core.DNN object at 0x134af03a0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Index'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n2021-08-26 19:43:01.304618: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-08-26 19:43:01.304771: W tensorflow/core/platform/profile_utils/cpu_utils.cc:126] Failed to get CPU frequency: 0 Hz\r\n2021-08-26 19:43:02.016427: I tensorflow/compiler/tf2mlcompute/kernels/mlc_subgraph_op.cc:537] Compute: Failed in processing TensorFlow graph model/prediction_layer/MLCSubgraphOp_0_24 with error: Internal: AddLayerToMLCGraph: Failed to add layer to MLCGraph for node: model/prediction_layer/Reshape (Reshape). (error will be reported 5 times unless TF_MLC_LOGGING=1).\r\n2021-08-26 19:43:02.017183: I tensorflow/compiler/tf2mlcompute/kernels/mlc_subgraph_op.cc:537] Compute: Failed in processing TensorFlow graph gradient_tape/model/prediction_layer/MLCSubgraphOp_0_26 with error: Internal: PerformGradientPassNodeRoutine: Failed to find forward-pass output for node: model/prediction_layer/Reshape (error will be reported 5 times unless TF_MLC_LOGGING=1).\r\n1/4 [======>.......................] - ETA: 2s - loss: 0.69472021-08-26 19:43:02.020231: I tensorflow/compiler/tf2mlcompute/kernels/mlc_subgraph_op.cc:537] Compute: Failed in processing TensorFlow graph model/prediction_layer/MLCSubgraphOp_0_24 with error: Internal: AddLayerToMLCGraph: Failed to add layer to MLCGraph for node: model/prediction_layer/Reshape (Reshape). (error will be reported 5 times unless TF_MLC_LOGGING=1).\r\n/Users/tal/miniforge3/envs/jianliang/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '", "comments": ["Hi @shijianliang001 !We see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 51698, "title": "kernel shutdown, error", "body": "when i run this code(recall model)\r\n\r\nfrom deepmatch.models import *\r\nmodel = DSSM(user_feature_columns, item_feature_columns)\r\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\r\nhistory = model.fit(train_model_input, train_label,  batch_size=128, epochs=1, verbose=1 )\r\n\r\nkernel shutdown\r\n\r\nWARNING:tensorflow:AutoGraph could not transform <bound method SequencePoolingLayer.call of <deepctr.layers.sequence.SequencePoolingLayer object at 0x15b884c10>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Index'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING: AutoGraph could not transform <bound method SequencePoolingLayer.call of <deepctr.layers.sequence.SequencePoolingLayer object at 0x15b884c10>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Index'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING:tensorflow:AutoGraph could not transform <bound method DNN.call of <deepctr.layers.core.DNN object at 0x15b8f2c10>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Index'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING: AutoGraph could not transform <bound method DNN.call of <deepctr.layers.core.DNN object at 0x15b8f2c10>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Index'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n1/8 [==>...........................] - ETA: 5s - loss: 0.6866", "comments": ["@shijianliang001 Can you please fill the performance issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 51697, "title": "TF-TRT Set binding dimensions even if tensor is not an execution tensor", "body": "According to [nvinfer1::ITensor::isExecutionTensor](https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_tensor.html#a472db0d89262ae20c1ad247b3a6d64fa) API description:\r\n\r\n> A tensor with isShapeTensor() == false and isExecutionTensor() == false can still show up as an input to the engine if its dimensions are required.\r\n\r\nThis PR fixes optimization profile definition for that case, by always calling profile->setDimension().\r\n\r\nA unit test is added to for such a case.", "comments": ["Closing in favor of 52181"]}, {"number": 51696, "title": "Add GpuSelectFlagged helper function and test", "body": "This will be used later in GPU op implementations.\r\n\r\ncc @nluehr ", "comments": []}, {"number": 51695, "title": "Refactor SparseSliceOp to use a functor", "body": "This is in preparation for adding a GPU implementation.\r\nNo functional change.\r\nAlso adds a few tests for empty input/output sparse tensors.\r\n\r\ncc @nluehr ", "comments": ["I'm not sure how to fix this error in the MacOS CI log (probably need to list the header explicitly in the BUILD file somewhere?):\r\n```\r\nERROR: /Volumes/BuildData/tmpfs/src/github/tensorflow/tensorflow/lite/delegates/flex/test/BUILD:16:24: Couldn't build file tensorflow/lite/delegates/flex/test/_objs/test_flex_delegate_tensorflow_lib/sparse_slice_op.o: undeclared inclusion(s) in rule '//tensorflow/lite/delegates/flex/test:test_flex_delegate_tensorflow_lib':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/sparse_slice_op.cc':\r\n  'tensorflow/core/kernels/sparse_slice_op.h'\r\n```", "I've changed the new test to run under v2, and I also worked out the TFLite build issue."]}, {"number": 51694, "title": "Training Mask R Cnn ", "body": "I find Mask r cnn in TF hub. But It looks like I can't training this model. Only Using trained model. \r\nHow can train the my dataset at model Mask R cnn?\r\n\r\n[TF hub](https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1)\r\n[Code in Colab](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/tf2_object_detection.ipynb#scrollTo=Gb_siXKcnnGC)\r\n\r\n", "comments": ["@cjfghk5697 ,\r\nThis issue is more suitable for TensorFlow Models repo. Please post it on Tensorflow Models repo from [here](https://github.com/tensorflow/models/issues/new/choose). Thanks!", "@tilakrayal ,\r\nI'm so sorry about my mistake. And thank you."]}, {"number": 51692, "title": "TFLite for conv1D and DepthwiseConv1D", "body": "**System information**\r\n- OS Platform and Distribution  Linux Ubuntu 16.04:\r\n- TensorFlow installed from binary:\r\n- TensorFlow version (tf-nightly 2.7.0.dev20210824):\r\n\r\nThe h5 is:\r\n![image](https://user-images.githubusercontent.com/4407779/130913616-d18620ec-cdab-45e5-8db8-dc25849cc155.png)\r\nWhen I convert it to tflite. It becomes \r\n![image](https://user-images.githubusercontent.com/4407779/130913708-57624af5-60d8-4551-a7b2-3cd3b1413c04.png)\r\n\r\nMy input is only a speech spectrum. I have to add one additional dim if I use conv2d. So I think conv1d is enough and use conv1d/depthwiseconv1d to generate model.  But the problem is that I still get conv2d and ds2d in tflite. It isn't helpful to deal with my high MIPS issue if it is still conv2d.  Any idea on it? Thanks. ", "comments": ["@wenjingyang In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "[ds1d.tar.gz](https://github.com/tensorflow/tensorflow/files/7063668/ds1d.tar.gz)\r\nThe H5 and tflite model is in the attachment. \r\nConvert code:\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n converter.allow_custom_ops = True\r\n converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n tflite_model = converter.convert()\r\nMy problem is that I need conv1d while it is still conv2d in tflite", "@ymodak Was able to reproduce the issue on colab using TF v2.6.0 and tf-nightly,Please find the gist[ here](https://colab.research.google.com/gist/sushreebarsa/9d11750d9899691386d2035890987872/untitled412.ipynb#scrollTo=gDKUQig6mTNu) for reference.Thank you!", "For your information, this is not a bug since the generated graph also works.", "My concern is that why it isn't CONV1D in tflite.  We can see a lot of reshape/expandim inside. Anyway, it could work. Thanks. ", "Conv1D is implemented using conv2d as mentioned already. \r\n\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51692\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51692\">No</a>\n"]}, {"number": 51691, "title": "Error while converting mrcnn to tf dialect", "body": "Actually I wanted to convert mrcnn model to tf dialect .I downloaded model from TensorFlow Hub. and I tried adding signatures to model using import tensorflow.compat.v2 as tf\r\nloaded_model = tf.saved_model.load(\u2019/maskedrcnn\u2019)\r\ncall = loaded_model.call.get_concrete_function(\r\ntf.TensorSpec(shape=(1, 1024, 1024, 3), dtype=tf.uint8))\r\nsignatures = {\u2018predict\u2019: call}\r\ntf.saved_model.save(loaded_model,\u2019/ex\u2019, signatures=signatures)\r\n\r\nI got warning as Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 125). These functions will not be directly callable after loading.           \r\nIf I use tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate --savedmodel-objectgraph-to-mlir  /data/maskedrcnn -o sample.mlir\r\n I am getting error as tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate: Symbol `_ZN10tensorflow35_DeviceAttributes_default_instance_E' has different size in shared object, consider re-linking\r\ntensorflow/bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate: Symbol `_ZN10tensorflow43_ConfigProto_Experimental_default_instance_E' has different size in shared object, consider re-linking\r\ntensorflow/bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate: Symbol `_ZTVN10tensorflow4data11DatasetBaseE' has different size in shared object, consider re-linking\r\ntensorflow/bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate: symbol lookup error:tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate: undefined symbol: _ZN10tensorflow28CollectiveParamResolverLocal22GetOrCreateInstanceRecEPKNS0_8GroupRecEPNS_16CollectiveParamsEPb\r\n                                                                                                                                                                                                                           ", "comments": ["@ArunaKote ,\r\nIn order to expedite the trouble-shooting process, could you please provide a complete code and the TensorFlow version you are using.\r\n", "above error got solved when load model using hub.load.I am using below code to get mlir from saved model. I am using tensorflow 2.1.0\r\n\r\n\r\nI used :\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\r\n\r\nmodel_url = \u201chttps://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1\u201d\r\nimported = hub.load(model_url)\r\n\r\nconcrete_func = imported.call.get_concrete_function( tf.TensorSpec(shape=(1, 1024, 1024, 3), dtype=tf.uint8))\r\n#concrete_func = imported.signatures[\u2019__saved_model_init_op\u2019]\r\nfrozen_func = convert_variables_to_constants_v2(concrete_func, lower_control_flow=False)\r\ngraph_def = frozen_func.graph.as_graph_def(add_shapes=True)\r\nwith tf.Graph().as_default() as inferred_graph:\r\ntf.import_graph_def(graph_def, name=\"\")\r\n\r\nprint(tf.mlir.experimental.convert_graph_def(\r\ngraph_def, pass_pipeline=\u2018tf-standard-pipeline\u2019\r\n))\r\n\r\nmlir file got generated .and then tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt --tf-executor-to-functional-conversion --tf-shape-inference -xla-legalize-tf --print-ir-before-all --print-ir-after-all &>sample ex.mlir\r\nI got error as error: unknown TensorFlow type: uint8\r\n%1106 = \u201ctf.Placeholder\u201d() {_output_shapes = [\u201ctfshape$dim { size: 1 } dim { size: -1 } dim { size: -1 } dim { size: 3 }\u201d], device = \u201c\u201d, dtype = \u201ctfdtype$DT_UINT8\u201d, name = \u201cinput_tensor\u201d, shape = \u201ctfshape$dim { size: 1 } dim { size: -1 } dim { size: -1 } dim { size: 3 }\u201d} : () \u2192 tensor<1x?x?x3x!tf.uint8>\r\n\r\n\r\nThanks", "@ArunaKote ,\r\nCan you please try the code in latest tensorflow v2.5 or v2.6 and let us know if the issue still persists.Thanks!", "I am using tensorflow v2.6. \r\n\r\n\r\n I am getting error as:\r\n\r\n tf_out.mlir:289:102: error: tf dialect has no types, potentially meant !tf_type.string\r\n    %cst_284 = \"tf.Const\"() {value = dense<\"Incorrect field size: actual vs expected.\"> : tensor<!tf.string>} : () -> tensor<!tf.string>\r\n", "@ArunaKote ,\r\n Code shared is full of indentation errors, please share a colab gist with issue reported or simple stand alone indented code with all dependencies such that we can replicate the issue reported.Thanks", "pip install tensorflow-hub\r\npip install tensorflow\r\n\r\nI have attached python file\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/7133302/model.zip)\r\n", "@ArunaKote ,\r\nI was able to execute the mentioned code in tf v2.6 without any error.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/ee5a6f3dd57020ec626c7594a0246ee1/untitled722.ipynb).Please try to test the code in new virtual environment.Thanks!", "Thank You.I able to run the above code.I used tf  2.7.0-dev20210831.I able to save mlir into tf.mlir\r\n\r\nThen I used tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt -canonicalize  --tf-shape-inference -xla-legalize-tf  --print-ir-before-all &>1  tf.mlir \r\n\r\nGot Error as: \r\n\r\nex_13_09.mlir:3:3: error: The following operations cannot be legalized: tf.Const (count: 1); tf.CropAndResize (count: 2); tf.NonMaxSuppressionV5 (count: 91); tf.Placeholder (count: 1); tf.ResizeBilinear (count: 1); tf.Squeeze (count: 4); tf.TopKV2 (count: 4); tf.Where (count: 3). These legalization failure(s) may be due to missing TF to HLO lowerings and/or unsupported attributes, etc.\r\n  builtin.func @main() {\r\n  ^\r\nex_13_09.mlir:3:3: error: Emitting more detail about one op that failed to legalize...\r\n  builtin.func @main() {\r\n  ^\r\nex_13_09.mlir:289:16: error: 'tf.Const' op is not legalizable\r\n    %cst_284 = \"tf.Const\"() {value = dense<\"Incorrect field size: actual vs expected.\"> : tensor<!tf_type.string>} : () -> tensor<!tf_type.string>\r\n               ^\r\nex_13_09.mlir:289:16: note: see current operation: %704 = \"tf.Const\"() {value = dense<\"Incorrect field size: actual vs expected.\"> : tensor<!tf_type.string>} : () -> tensor<!tf_type.string>\r\n", "@ArunaKote ,\r\nPlease feel free to move this issue to closed status and please submit a new issue from this [link](https://github.com/tensorflow/models/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51691\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51691\">No</a>\n"]}, {"number": 51690, "title": "NameError:", "body": "\r\n![1](https://user-images.githubusercontent.com/26819449/130900977-670c52ed-7327-4ce8-96f7-e782fe5c4a69.JPG)\r\n\r\n![1 2](https://user-images.githubusercontent.com/26819449/130900986-0880d879-619e-425d-8e72-0cc208284706.jpg)\r\n![1 3](https://user-images.githubusercontent.com/26819449/130901034-a2bc3c90-1edb-4cf3-881d-c5bfd8a120a2.JPG)\r\n\r\nCan I add a direct link to the GitHub copy path like src/janggu/resources/pseudo_genome.fa.\r\nOr do I have to download the files and then upload them?\r\nCan you tell me how I should upload it then?\r\nBoth the ways would be fine for now.\r\nThank you\r\n\r\n", "comments": ["Hi @starboyvarun ,\r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks", "Hello @mohantym ,\r\nActually, I cannot take the screenshot while scrolling so I split it. No code is missing in this.\r\nThe tf version is 2.2\r\nI was trying to implement github repo Janggu", "ok , Could please  attach the Github link to source code?", "Link:  https://github.com/BIMSBbioinfo/janggu/blob/master/docs/tutorial.rst\r\nI was implementing Part(II) Building a neural network with Janggu and its subpart Fit a neural network on DNA sequences.\r\nThank you.\r\n\r\n", "This issue is not related to Tensorflow . Add `\"from pkg_resources import resource_filename\" `at start of program. For further queries please raise issue in respective repository. Thanks!", "Can you tell me where should I raise this issue? But I was using TensorFlow version 2.2 because the repository which I was using tell me to raise the issue in TensorFlow respectively.\r\n\"from pkg_resources import resource_filename\"\r\nDo I have to use permalink or copy the path of GitHub of particular files or just the \r\n![1](https://user-images.githubusercontent.com/26819449/130933788-1c54ae2e-9b95-436b-a33f-d6f9f6299411.JPG)\r\n?", "You might post queries  related to janggu [here](https://github.com/BIMSBbioinfo/janggu/issues) . ", "hey, can you help here if possible I already posted there. They just close the issue without even letting me reply.  ", " Use like this `REFGENOME = resource_filename('janggu', 'resources/pseudo_genome.fa')`and refer their documentation properly. Please feel free to close this issue as this is not related to tensorflow.", "okay thanks"]}, {"number": 51687, "title": "[INTEL MKL] Fix MKL related conv_ops_3d_test unit test failure", "body": "Fix the issue by adding sanity check for the corner case of \"zero element of filter\" in mkl_conv_ops.cc. With this fix, MKL conv op  returns a right error message which a newly added conv3d unit test (within python/kernel_tests/conv_ops_3d_test.py) expects. \r\n\r\nThe fix just does the same as what the Eigen implements (core/kernels/conv_ops_3d.cc).", "comments": ["@penpornk  Can you please review this PR ? Thanks!"]}, {"number": 51686, "title": "MaybeLockVariableInputMutexesInOrder() from training_op_helpers.h is broken when trying to lock only a subset of the input variables", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary from PyPI\r\n- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0\r\n- Python version: 3.7.5\r\n- CUDA/cuDNN version: 11.2, 8.1\r\n\r\n**Describe the current behavior**\r\n\r\nOn current master `MaybeLockVariableInputMutexesInOrder()` does not behave correctly if I pass in `std::vector<int>{1}` for the argument `input_ids`.\r\n\r\nRather than try to acquire a lock on the variable associated to input id `1` here, it tries to acquire one for that with input id `0`, which may already be locked of course and shouldn't be touched at all by this function.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/3899a80bafd3fbb95edb717dded9ffd64af58b11/tensorflow/core/kernels/training_op_helpers.h#L176\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should lock the right variable.\r\n\r\nAt the highlighted line in the source the mutex is pulled in for the wrong input_id. I think it would make more sense here to use the mutex pointer that has already been put into the `mutexes` vector a couple of lines above.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): not really\r\n- Briefly describe your candidate solution(if contributing): see under \"describe the expected behavior\", but I haven't tested that", "comments": ["Hi @Saduf2019 ,Could you please look into this issue.", "Hi @jvishnuvardhan, \r\nhave you had a chance to look into this? An improvement here would make it substantially easier for outside contributors to add custom ops that work with resource variables. An example application would be implementing novel kinds of optimizers.", "Thanks for the report! The fix seems to be easy enough given that Max has already identified the buggy line. I agree we can change that to `mutexes[input]` (and with some renaming for clarifications) as a very quick fix. \r\n\r\nI'll take a closer look.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51686\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51686\">No</a>\n"]}, {"number": 51685, "title": "ABSL_HAVE_ADDRESS_SANITIZER use in Tensorflow headers breaks build for MSVC 16.11 with /fsanitize=address", "body": "**System information**\r\n- TensorFlow installed from (source or binary): source, but does not really matter\r\n- TensorFlow version (use command below): 2.5\r\n\r\n**Describe the current behavior**\r\nWhen compiling in MSVC with address sanitizer TF enables code guarded by ABSL_HAVE_ADDRESS_SANITIZER.\r\nThat guarded code (header includes, calls of functions) break the compile.\r\n\r\nI would guess this happens since till recently only clang and gcc had sanitizers, and I presume they support all the functionality TF uses. \r\n\r\n**Describe the expected behavior**\r\nEither ABSL_HAVE_ADDRESS_SANITIZER  should be fixed to be 1 only on gcc/clang or TF should use the ABSL_HAVE_ADDRESS_SANITIZER  only when gcc/clang are used. On MSVC this causes a ton of problems.\r\nAlternatively ask MSFT if they plan to add all the headers soon, I doubt it, but if they do you can close as WONTFIX.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing): I would discuss this with Abseil team and decide to either fix a macro in Abseil, or define a TF_SOMETHING macro that logically maps to ABSL_HAVE_ADDRESS_SANITIZER==1  &&  compiler_is_not_msvc\r\n\r\n**Standalone code to reproduce the issue**\r\nInclude any header that will transitively include headers that use ABSL_HAVE_ADDRESS_SANITIZER  and compile on MSVC 16.11 with /fsanitize=address\r\nExample of problematic header:\r\ncpu\\include\\absl\\base\\internal\\dynamic_annotations.h\r\n\r\n", "comments": ["P.S. \r\nreading \r\nhttps://devblogs.microsoft.com/cppblog/address-sanitizer-for-msvc-now-generally-available/\r\n\r\nSeems it is possible that MSVC has the correct headers, but they are not just in default search paths, this person asked, but MSFT never replied to him:\r\n![image](https://user-images.githubusercontent.com/47703951/130849016-5a13cc2e-a2b7-45ec-8795-f654c4dd140a.png)\r\n", "@libbooze In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "This is not strictly related to TensforFlow - I get the same error when trying to use abseil directly in my project with ASan enabled on MSVC.", "I've also reported this issue to [Microsoft](https://developercommunity.visualstudio.com/t/ASan-API-headers-not-in-include-path-whe/1517192?space=62&q=sanitizer%2Fcommon_interface_defs.h&entry=myfeedback). Feel free to upvote the report.", "@DoDoENT Thank you very much for taking time to report this and make MS issue. \r\nDid you use the latest Abseil?\r\nIf so I can skip the request from @sushreebarsa  for testing 2.6 since if it is not fixed in latest Abseil it is for sure still broken in TF.", "I'm using 202103.24.2. However, after hardcoding the include directory path in my `CMakeLists.txt` to `C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\VC\\Tools\\MSVC\\14.29.30133\\crt\\src` just to test the build in case MSVC added that path correctly to default includes, but abseil still doesn't build. \r\n\r\nThis time I get the following error:\r\n\r\n```\r\nabsl/container/fixed_array.h(432): error C3646: '__attribute__': unknown override specifier\r\n```\r\n\r\nAs far as I see, the line 432 of `fixed_array.h` is \r\n\r\n```\r\n   private:\r\n    ABSL_ADDRESS_SANITIZER_REDZONE(redzone_begin_);  // line 432\r\n    alignas(StorageElement) char buff_[sizeof(StorageElement[inline_elements])];\r\n    ABSL_ADDRESS_SANITIZER_REDZONE(redzone_end_);\r\n```\r\n\r\nand `ABSL_ADDRESS_SANITIZER_REDZONE` is defined as\r\n\r\n```\r\n#define ABSL_ADDRESS_SANITIZER_REDZONE(name) \\\r\n  struct {                                   \\\r\n    char x[8] __attribute__((aligned(8)));   \\\r\n  } name\r\n```\r\n\r\nin case when `ABSL_HAVE_ADDRESS_SANITIZER` is defined (i.e. when ASan is detected). I think this attribute should be set differently with MSVC - ideally with `alignas` specifier, which is compiler agnostic.\r\n\r\nI believe this is an abseil bug that needs to be resolved.", "This is not related to TF, so I'd move we close this", "I agree. I reported that directly to [abseil](https://github.com/abseil/abseil-cpp/issues/1010).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51685\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51685\">No</a>\n", "When ABSL releases a new stable version with this patched, we'll patch it into TF"]}]