[{"number": 3948, "title": "how solve : ImportError: cannot import name nest", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:Ubuntu 16.04\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n### Steps to reproduce\n\n1.\n2.\n3.\n### What have you tried?\n\n1.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["Please provide information requested in the template (you just ignored it). You provide no context, we unfortunately cannot provide help.\n", "Automatically closing due to lack of recent activity. Please reopen if further information becomes available.\n", "sorry,I  had solved that problem.Using the method with re-download and re-setuo.Thank you!\n"]}, {"number": 3947, "title": "Slicing a Sparse Tensor?", "body": "Currently, sparse tensors don't support indexing, and the tf.sparse_split() operation can't split a sparse tensor according to a given proportion. If I want to select a portion of the training set (which is a sparse tensor) as the validation set, is it possible? \n", "comments": ["This is a general question better suited to StackOverflow.  Could you please re-ask there.\n", "Duplicate of https://github.com/tensorflow/tensorflow/issues/1588\n", "@haruna1998 I think there maybe a workaround.\r\nYou can split a sparse tensor then concatenate some parts of it.\r\nFor example:\r\n```\r\nindices = [[0, 0], [1,4], [2, 3], [3, 2]]\r\nvalues = [1.0, 2.0, 3.0, 4.0]\r\n# Take the first 3 rows as training set, and the last 1 row as validation set\r\nnum_training_samples, num_validation_samples = 3, 1\r\nst = tf.SparseTensor(indices, values, dense_shape=(4, 5))\r\nst_rows = tf.sparse_split(sp_input=st, num_split=4, axis=0)\r\ntraining_sparse_tensor = tf.sparse_concat(axis=0, sp_inputs=st_rows[:num_training_samples])\r\nvalidation_sparse_tensor = tf.sparse_concat(axis=0, sp_inputs=st_rows[-num_validation_samples:])\r\n```\r\nNote: not tested. But it should work.", "FYI: there is now a tf.sparse_slice, see https://www.tensorflow.org/api_docs/python/tf/sparse_slice"]}, {"number": 3946, "title": "Tensorboard URL for visiting", "body": "Run last week into the 'issue' that one of the users tried to go to `http://0.0.0.0:6006` as instructed on the command line.\n\n``` bash\n$ python tensorboard.py --logdir ~/tmp/logs/\nStarting TensorBoard b'23' on port 6006\n(You can navigate to http://0.0.0.0:6006)\n```\n\nModified the message that if `FLAGS.host` is `0.0.0.0` it identifies the correct IP-address for showing in the message (in this case the local-IP is `172.16.3.13`)\n\n``` bash\n$ python tensorboard.py --logdir ~/tmp/logs/\nStarting TensorBoard b'23' on port 6006\n(You can navigate to http://172.16.3.13:6006)\n```\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "|I signed it!|\n\nOp 21-08-16 om 12:17 schreef googlebot:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project. Before we can look at\n> your pull request, you'll need to sign a Contributor License Agreement\n> (CLA).\n> \n> \ud83d\udcdd _Please visit https://cla.developers.google.com/ to sign._\n> \n> Once you've signed, please reply here (e.g. |I signed it!|) and we'll\n> verify. Thanks.\n> \n> ---\n> - If you've already signed a CLA, it's possible we don't have your\n>   GitHub username or you're using a different email address. Check\n>   your existing CLA data https://cla.developers.google.com/clas\n>   and verify that your email is set on your git commits\n>   https://help.github.com/articles/setting-your-email-in-git/.\n> - If you signed the CLA as a corporation, please let us know the\n>   company's name.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/3946#issuecomment-241249326,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AGwylexxcg4oLkzApWx0p4xcOS3j8p3Rks5qiCWygaJpZM4JpRH-.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins test this please\n", "@danmane this looks fine to me. OK to merge?\n", "Yup! Thanks for the PR @paderijk!\n", "@danmane you're welcome! \n\nWill definitely look if I can contribute in the future more code/modifications.\n"]}, {"number": 3945, "title": "Feature request: extend tf.select to broadcast a scalar condition", "body": "Consider allowing tensorflow.select to accept scalar conditions.  I.e., `tf.select(condition, t, e, name=None)` with `condition` a scalar, and `t`, `e` having the same shape.\n", "comments": ["I think that is perhaps better accomplished via tf.cond?  https://www.tensorflow.org/versions/r0.10/api_docs/python/control_flow_ops.html#cond\n\n(The reason it is \"better\" is that, if implemented carefully, tf.cond doesn't evaluate both branches, whereas tf.select with this feature would always evaluate both branches).  Please comment if you disagree!\n", "Isn't `cond` done on the CPU?  I mean, even if the condition is a placeholder, cond has to copy the result of `pred` back to the CPU and then call either `fn1` or `fn2` \u2014 all on the CPU (and using the Python runtime).  `select` seems better if everything is already on the GPU and needs to stay on the GPU?  Also, evaluating both branches is not an issue for me.\n", "That's a good point, and select does do some amount of broadcasting already (it can broadcast row-wise when it is a vector), so it seems reasonable to extend that all the way to the scalar case.\n\nThis probably isn't super high priority though, but would be a welcome improvement if someone in the community wanted to implement this, so marking this as contributions welcome.  In the mean time, you could probably use tf.tile to broadcast the scalar into a vector of the right length without the same penalty as the full broadcast.\n", "Thanks for reopening the issule, could you point me to where select is defined in the source?  My search for \"def select\" didn't find it.\n", "You'd have to implement the change in math_ops.py _SelectShape to allow the new shape signature, math_ops.cc's ShapeFn to do the same, and then you'd have to edit cwise_op_select.cc and cwise_op_gpu_select.cu.cc to implement the new feature, and then probably cwise_ops_test.py to validate the change. \n", "Okay, thanks.  I may work around this for now using `expand_dims` on my condition.\n", "Taking a look at this. The additional case when the condition is just a scalar it will return either the full tensor from `t` or `e` based on the single value of `cond`, right? \n", "Yup!\n", "Can someone comment on the state here? This was reverted? Why?\r\n\r\nWhat broadcasting is supposed to work?\r\n\r\nE.g. I get the error:\r\n\r\n`Dimension 2 in both shapes must be equal, but are 9001 and 1 for '.../Select' (op: 'Select') with input shapes: [?,?,1], [?,?,9001], [1,1,1]`\r\n\r\nSo the `cond` can not be broadcasted?\r\n\r\nEven when I have `cond` in the correct shape, I still get the error:\r\n\r\n`ValueError: Dimension 2 in both shapes must be equal, but are 9001 and 1 for '.../Select' (op: 'Select') with input shapes: [?,?,?], [?,?,9001], [1,1,1].`\r\n\r\nSo also `e` cannot be broadcasted?\r\n\r\nWhat can be broadcasted?\r\n", "@drpngx Why was it reverted?", "What was reverted? I think it's still there. It may have been reverted temporarily. I spot checked some code https://github.com/Mistobaan/tensorflow/blob/master/tensorflow/core/ops/math_ops.cc#L943 and it's there.", "Ok, the history of commits referenced in this issue suggests that something was reverted. This is a bit confusing.\r\n\r\nAnyway, I think the broadcasting is still very much too limited. This feature request should also maybe be renamed to allow any broadcasting. I guess people would expect the straightforward behavior (which is the same in `numpy.where` I guess).\r\n", "Sounds good. Would someone be interested in implementing this? Maybe starting by replying on this thread with some snippets of test code to show all of the cases that need to be covered?", "@drpngx It looks like you reverted it in the tensorflow codebase.  (Of course it's still on Mistobaan's codebase.)  Am I misunderstanding?", "@NeilGirdhar yes it looks like it, but it was put back at a later stage AFAIK. What code are you looking at?", "I see now, you're right.", "If I may, should `tf.select / where` not be able to support more general broadcasting? \r\nIgnoring the casting problem, it is completely equivalent to the operation `cond * t  + (1  - cond) * e` which supports much more general input shapes. For instance, I find it quite useful when `t` is a Tensor and `e` a scalar for performing a simple update of some values in the former. \r\n", "Just wondering about the state here. Broadcasting in `tf.where` would really be useful.", ">  it is completely equivalent to the operation cond * t + (1 - cond) * e\u2026\r\n\r\nNot completely, because when one of `t` and `e` is `nan`, your expression always returns `nan`.  Also, `numpy`'s `where` supports mixed dimensions.", "PR #15982 might be related.", "Looks like the original issue reported is fixed. I will close this issue, but for broadcasting in where/select ops, if we still  need fixes please file a new issue.", "No, this is still not fixed.\r\nRelated (or maybe duplicate): #9284"]}, {"number": 3944, "title": "Not found: Op type not registered 'SigmoidGrad'", "body": "### Environment info\n\nOperating System:\nMac OS X 10.11.6 (15G31)\n\nInstalled version of CUDA and cuDNN: \nno cuda\n\nInstalled from sources,\ngit clone https://github.com/tensorflow/tensorflow.git --branch v0.10.0rc0 --single-branch Tensorflow\n1. The commit hash (`git rev-parse HEAD`)\n   3cb39956e622b322e43547cf2b6e337020643f21\n2. The output of `bazel version`\n   ........\n   Build label: 0.2.1-homebrew\n   Build target: bazel-out/local_darwin-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\n   Build time: Fri Apr 1 00:35:17 2016 (1459470917)\n   Build timestamp: 1459470917\n   Build timestamp as int: 1459470917\n3. create pip package and install\n4. make tensorflow graph.ph file\n5. make libtensorflow.so with\n   bazel build :libtensorflow.so\n6. create c++ program and try to load this pb file\n   status = ReadBinaryProto(Env::Default(), dir + \"/\" + tfGraphFilename, &graph_def);\n7. error at runtime\n   Not found: Op type not registered 'SigmoidGrad'\n\nI tried tf.sigmoid and tf.nn.sigmoid, same error\ntensorflow version 0.8 works fine\n", "comments": ["What if you use `:libtensorflow_cc.so` instead? \n", ":libtensorflow_cc.so solves the problem\n"]}, {"number": 3943, "title": "Link to \"iris_monitor.py\" broken ", "body": "The link to [iris_monitor.py](https://www.tensorflow.org/versions/examples/tutorials/monitors/iris_monitors.py) returns `not found` on Tensorflow tutorial [logging-and-monitoring-basics](https://www.tensorflow.org/versions/r0.10/tutorials/monitors/index.html#logging-and-monitoring-basics-with-tf-contrib-learn). Should it be redirected to Github [there](https://github.com/tensorflow/tensorflow/raw/r0.10/tensorflow/examples/tutorials/monitors/iris_monitors.py) ?\n", "comments": ["Looks like something got lost in translation between the internal and external code repositories.\n", "Apologies for the error @fansgit. The correct link is:\n\nhttps://www.tensorflow.org/code/tensorflow/examples/tutorials/monitors/iris_monitors.py\n\nI actually already fixed this on 8/19 in 4a510c177. It just needs to be folded into the r0.10 branch and published out to the website. I'll make a PR for that.\n", "This seems to be all fixed now. Closing."]}, {"number": 3942, "title": "tf.app.flags issue for multiple file AttributeError(name)", "body": "python train.py \nmessi\nTraceback (most recent call last):\n  File \"train.py\", line 24, in <module>\n    import model \n  File \"/home/gezi/mine/tensorflow-exp/tests/flags/model.py\", line 23, in <module>\n    print(FLAGS.batch_size)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/flags.py\", line 43, in **getattr**\n    raise AttributeError(name)\nAttributeError: batch_size\ngezi:~/mine/tensorflow-exp/test\nAttributeError: batch_size\n\nI have 3 .pys, running tensorlow 0.10\n# -----------------------------------------train.py\n\nfrom **future** import absolute_import\nfrom **future** import division\nfrom **future** import print_function\n\nimport sys,os\nimport tensorflow as tf\n\nflags = tf.app.flags \nFLAGS = tf.app.flags.FLAGS \n\nflags.DEFINE_integer('step', 5, '')\n\nimport util\nimport model \n# ------------------------model.py\n\nfrom **future** import absolute_import\nfrom **future** import division\nfrom **future** import print_function\n\nimport sys,os\nimport tensorflow as tf\n\nflags = tf.app.flags \nFLAGS = tf.app.flags.FLAGS \n\nflags.DEFINE_integer('batch_size', 100, '')\n\nprint(FLAGS.batch_size)\n# --------------------------util.py\n\nfrom **future** import absolute_import\nfrom **future** import division\nfrom **future** import print_function\n\nimport tensorflow as tf\n\nflags = tf.app.flags \nFLAGS = tf.app.flags.FLAGS \n\nflags.DEFINE_string('name', 'messi', '')\n\nprint(FLAGS.name)\n\nThese demo code will not cause error, if I switch to use gflags.\nAnother problem:\n python train.py --help\nusage: train.py [-h] [--step STEP] [--name NAME]\n\noptional arguments:\n  -h, --help   show this help message and exit\n  --step STEP\n  --name NAME\n\nWhy not show default values as c version gflags do ?\n", "comments": ["I think if you have multiple files like this, tf.app.flags must be used with 'tf.app.run()'.\n\ntf.app.run() is where the flags you have declared are parsed, so if run code that inspects FLAGS before tf.app.run(), it won't work.\n\nMy suggestion would be to set it up so that:\n\nmodel.py and util.py define functions but don't inspect FLAGS on import\nAnd then your 'train.py' would define its code in a main function \n\n```\ndef main(argv):\n  .. your code ...\n```\n\nThen at the bottom:\n\n```\nif __name__ == \"__main__\":\n  tf.app.run()\n```\n\nAs for the default values, I'll have to look\n", "@vrv  Got the point now, it works, thanks!\n", "If someone wants to change flags.py to use https://docs.python.org/3/library/argparse.html#argparse.ArgumentDefaultsHelpFormatter when calling argparser so that it shows defaults, that would be great.  Closing the bug as is though.\n"]}, {"number": 3941, "title": "cifar10_train can't use all CPUs", "body": "### Environment info\n\nOperating System: Linux 14.04\n\nInstalled version of CUDA and cuDNN:  No\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n\nhttps://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\n1. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\n0.10.0rc\n### Steps to reproduce\n1. clone the tensorflow main repo\n2. run `python cifar10_train.py`\n   This will only occupy ~6 CPUs on a machine with 8 CPUs (`top` command shows the CPU usage is ~600%)\n### What have you tried?\n1. set  `intra_op_parallelism_threads` and  `inter_op_parallelism_threads` in the `ConfigProto` when creating the session.  This doesn't help at all.\n2. use multi-thread training like:\n\n```\nclass Cifar10(object):\n  def __init__(self, sess):\n    print(\"In construction\")\n    self.global_step = tf.Variable(0, trainable=False)\n    self.images, self.labels = cifar10.distorted_inputs()\n    self.logits = cifar10.inference(self.images)\n    self.loss = cifar10.loss(self.logits, self.labels)\n    self.train_op = cifar10.train(self.loss, self.global_step)\n\n    self.sess = sess\n    init = tf.initialize_all_variables()\n    self.sess.run(init)\n    self.start = time.time()\n    tf.train.start_queue_runners(sess=self.sess)\n    print(\"Construction Done\")\n    self.saver = tf.train.Saver()\n\n  def train(self):\n    for step in xrange(FLAGS.max_steps):\n      self.sess.run(self.train_op)\n\n  def run(self, sess):\n    workers = []\n    for _ in xrange(4):\n      t = threading.Thread(target=self.train)\n      t.start()\n      workers.append(t)\n\n    for t in workers:\n      t.join()\n\ndef main(argv=None):  # pylint: disable=unused-argument\n  cifar10.maybe_download_and_extract()\n  with tf.Graph().as_default(): \n    with tf.Session() as sess:\n      model = Cifar10(sess)\n      for _ in xrange(10000):\n        gs = sess.run(model.global_step)\n        checkpoint_path = os.path.join(FLAGS.train_dir, 'model.ckpt')\n        model.saver.save(sess, checkpoint_path, global_step=gs)\n        model.run(sess)\n\nif __name__ == '__main__':\n  tf.app.run()\n```\n\nThe complete code:\n[cifar10_multi_thread.txt](https://github.com/tensorflow/tensorflow/files/428362/cifar10_multi_thread.txt)\n\nThis did help. The CPU usage became 800% and the training process did go faster.\n\nI assume that the data feeding process should not be the bottleneck because in the multi-thread training the input queue is not adjusted. Then how does TensorFlow parallelize the training process? Is there any parameter I should try to increase the CPU usage with the original code?\n\nI also observed that if I train the multi-threads version in a distributed setting (I'm running 3 workers async, each runs a multi-threads training process), the precision on testing data looks like:\n![image](https://cloud.githubusercontent.com/assets/7953637/17831863/d1d4b4d8-6727-11e6-8501-441796b8a70e.png)\n\nBut if I use the original version, the performance looks like:\n![image](https://cloud.githubusercontent.com/assets/7953637/17831884/a478c1c2-6728-11e6-988b-59f51e9284a1.png)\n\nI guess that might be caused by the async updates, is there any suggestion to avoid this?\n\nThanks\n", "comments": ["I don't think it is right to use cpu to training CNN ...\n", "This is a general discussion more suited to StackOverflow.  Please can you re-ask your question there.\n"]}, {"number": 3940, "title": "one more import error(new)", "body": "Here is the error messages:\n\n```\nIn [1]: import tensorflow\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\n<ipython-input-1-a649b509054f> in <module>()\n----> 1 import tensorflow\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py in <module>()\n     21 from __future__ import print_function\n     22 \n---> 23 from tensorflow.python import *\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py in <module>()\n     96 from tensorflow.python.platform import resource_loader\n     97 from tensorflow.python.platform import sysconfig\n---> 98 from tensorflow.python.platform import test\n     99 \n    100 from tensorflow.python.util.all_util import make_all\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/test.py in <module>()\n     75 import sys\n     76 if sys.version_info.major == 2:\n---> 77   import mock                # pylint: disable=g-import-not-at-top,unused-import\n     78 else:\n     79   from unittest import mock  # pylint: disable=g-import-not-at-top\n\n/usr/local/lib/python2.7/dist-packages/mock/__init__.py in <module>()\n      1 from __future__ import absolute_import\n----> 2 import mock.mock as _mock\n      3 from mock.mock import *\n      4 __all__ = _mock.__all__\n      5 #import mock.mock as _mock\n\n/usr/local/lib/python2.7/dist-packages/mock/mock.py in <module>()\n     67 import six\n     68 from six import wraps\n---> 69 from pbr.version import VersionInfo\n     70 \n     71 _v = VersionInfo('mock').semantic_version()\n\nImportError: No module named pbr.version\n```\n\nPlease help. According to the related no module alike errors I have updated the mock module and six(now 1.10.0), but the same error appears. Thanks very much. \n", "comments": ["I thought it is mock that causes the problem since importing mock can reproduce the error.\n", "Yeah, I tackled the problem by removing and reinstalling the mock module. \n"]}, {"number": 3939, "title": "when I try to install tensorflow from source by bazel, I got this problem:", "body": "the commond is : bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\nERROR is:\nundeclared inclusion(s) in rule '@zlib_archive//:zlib':\nthis rule is missing dependency declarations for the following files included by 'external/zlib_archive/zlib-1.2.8/uncompr.c':\n  '/usr/local/lib/gcc/x86_64-pc-linux-gnu/6.1.0/include-fixed/limits.h'\n  '/usr/local/lib/gcc/x86_64-pc-linux-gnu/6.1.0/include-fixed/syslimits.h'\n  '/usr/local/lib/gcc/x86_64-pc-linux-gnu/6.1.0/include/stddef.h'\n  '/usr/local/lib/gcc/x86_64-pc-linux-gnu/6.1.0/include/stdarg.h'.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nhow can I deal with it?Thanks!\n", "comments": ["Also having this issue.\n", "@reesepathak \n#2109 will help you, and I fixed it by adding '/usr/local/lib/gcc/x86_64-pc-linux-gnu/6.1.0/include' and '/usr/local/lib/gcc/x86_64-pc-linux-gnu/6.1.0/include-fixed' in tensorflow/third_party/gpus/crosstool/CROSSTOOL file,I think this will be work for you.\n"]}, {"number": 3938, "title": "After upgrade protobuf for the 64Mb limited , segment fault occurred", "body": "After doing \"pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/protobuf-3.0.0b2.post2-cp27-none-linux_x86_64.whl\" as said in \"https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#protobuf-library-related-issues\"\n, my tensorflow did not work and got a segment fault after import tensorflow.\nCould some one give me a right pip install version of >64MB limited protobuf?Thanks!\n", "comments": ["Please can you provide the OS and software version information and exact command lines you used, as requested in the TensorFlow issues template.\n", "@prb12 \nCentOS Linux release 7.2.1511 (Core)\ntensorflow 0.8.0\npython 2.7\nprotobuf-3.0.0-beta-2\nused commond:\npip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl\npip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/protobuf-3.0.0b2.post2-cp27-none-linux_x86_64.whl\n\nAfter doing this , I got the segment fault when import tensorflow.\n", "Not sure what to suggest.\nHave you tried using a more recent version of TensorFlow?  (e.g. 0.9 or 0.10rc ?)\nDoes the output of `pip show protobuf` look like this:\n\n```\n~/tensorflow.triage$ pip show protobuf\n---\nName: protobuf\nVersion: 3.0.0b2\nLocation: /usr/local/lib/python2.7/dist-packages\nRequires: six, setuptools\n```\n\nYou're definitely running your tensorflow program with python 2.7 not accidentally using python 3 ?  (in which case you'd need to **pip3** install the protobuf package)\n\n@martinwicke Any ideas?\n", "keveman@, do we have to upgrade the pre-built protobuf to 3.0.0 now that we're on 3.0.0 (no beta) for our repo and the binaries are built with it?\n", "@keveman, any update? @haibarasiao any luck solving this on your own?\n", "no good solving, I have to reduce the number of weights to solve it.\n", "I am assuming now this issue is obsolete.\r\n@haibarasiao are you still running into this?", "no\uff0cI use mxnet now. \r\n------------------ Original ------------------\r\nFrom: \"gunan\"<notifications@github.com>\r\nDate: Wed, Dec 21, 2016 05:13 PM\r\nTo: \"tensorflow/tensorflow\"<tensorflow@noreply.github.com>;\r\nCc: \"haibarasiao\"<fuxiao@wtweiqi.com>;\"Mention\"<mention@noreply.github.com>;\r\nSubject: Re: [tensorflow/tensorflow] After upgrade protobuf for the 64Mblimited , segment fault occurred (#3938)\r\n\r\n\r\n\r\nI am assuming now this issue is obsolete.\r\n @haibarasiao are you still running into this?\r\n \r\n&mdash;\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub, or mute the thread."]}, {"number": 3937, "title": "tf.add_check_numerics_ops() causes numeric errors for gradients of tf.pow", "body": "### Environment info\n\nOperating System: Ubuntu 16.04\n\nInstalled version of CUDA and cuDNN: None\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   [`https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl`](https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl)\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   `0.10.0rc0`\n### Steps to reproduce\n\n``` python\nimport tensorflow as tf\nx = tf.Variable(0.)\nx_grad = tf.gradients(x**2, x)\ninit = tf.initialize_all_variables()\ncheck_op = tf.add_check_numerics_ops()\nsess = tf.InteractiveSession()\nsess.run(init)\nsess.run(x_grad + [check_op])\n# => InvalidArgumentError: gradients/pow_grad/Log:0 : Tensor had Inf values\nsess.run(x_grad)\n# => [0.0]\n```\n\nMy guess is that the check_op forces the two factors in `grad(x**2) = (e**(2log x)) * (1./x)` to separately evaluate. Then 1./x causes an `Inf`.\n", "comments": ["That's normal functionality of check_gradient -- you run it on a code which\r\nshouldn't have any NaN or inf values, and it fails with\r\nIllegalArgumentError on the first inf/nan it finds.\r\n\r\nHowever, having NaN or Inf in your computation isn't necessarily a problem.\r\nYou can divide by inf to get 0, or you can add extra logic to replace it\r\nwith regular value, which is what Pow gradient is doing\r\n\r\nBelow is implementation of Pow grad\r\n(tensorflow/python/ops/math_grad.py#L544). You can see it'll computes Log\r\nof 0, but there's a \"where\" clause which replaces negative infinities with\r\n0's so they don't break the result\r\n\r\n```\r\n  # Avoid false singularity at x = 0\r\n  log_x = math_ops.select(x > 0, math_ops.log(x), array_ops.zeros_like(x))\r\n\r\n```\r\nOn Sat, Aug 20, 2016 at 1:35 AM, Shi Jiaxin notifications@github.com\r\nwrote:\r\n\r\n> Environment info\r\n> \r\n> Operating System: Ubuntu 16.04\r\n> \r\n> Installed version of CUDA and cuDNN: None\r\n> (please attach the output of ls -l /path/to/cuda/lib/libcud*):\r\n> \r\n> If installed from binary pip package, provide:\r\n> 1. Which pip package you installed. https://storage.googleapis.\r\n>    com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-\r\n>    none-linux_x86_64.whl\r\n>    https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\r\n> 2. The output from python -c \"import tensorflow;\r\n>    print(tensorflow.**version**)\". 0.10.0rc0\r\n> \r\n> Steps to reproduce\r\n> \r\n> import tensorflow as tf\r\n> x = tf.Variable(0.)\r\n> x_grad = tf.gradients(x**2, x)\r\n> init = tf.initialize_all_variables()\r\n> check_op = tf.add_check_numerics_ops()\r\n> sess = tf.InteractiveSession()\r\n> sess.run(init)\r\n> sess.run(x_grad + [check_op])# => InvalidArgumentError: gradients/pow_grad/Log:0 : Tensor had Inf values\r\n> sess.run(x_grad)# => [0.0]\r\n> \r\n> My guess is that the check_op forces the two factors in grad(x**2) =\r\n> (e**(2log x)) \\* (1./x) to separately evaluate. Then 1./x causes an Inf.\r\n> \r\n> \u2014\r\n> You are receiving this because you are subscribed to this thread.\r\n> Reply to this email directly, view it on GitHub\r\n> https://github.com/tensorflow/tensorflow/issues/3937, or mute the thread\r\n> https://github.com/notifications/unsubscribe-auth/AABaHGTAjWdQnsYYTdRjuMe_NripnAcdks5qhrw-gaJpZM4JpB78\r\n> .\r\n", "Thanks for explanation. Yep. But this makes it hard for someone who uses tf.pow in their code but still want to find out other NaNs, right? And it also feels a little uncomfortable to have different behaviors between tf.pow (y=2) and tf.square when using check_op.\n", "It's always possible to manually insert individual check_numerics ops - and probably more sensible to insert them in strategic places when debugging rather than everywhere.  I think it is unlikely that this behavior is going to be changed.\n", "I strongly disagree with the sentiment here. \r\n\r\nThis is how I've spent ~6 hours before I found this issue\r\n\r\n* Run big hairy model for a couple of hours, everything is fine. \r\n* Suddenly, NaNs everywhere.\r\n* Google tensorflow debug nan. Tells me to insert `add_check_numerics_ops`. Great! Then I can at least know which tensor first get's the NaNs.\r\n* Run model again. Immediately get NaN warning `gradients/pow_grad/Log:0`. Spend hours debugging where that tensor comes from.\r\n* Track it down to a MSE calculation. \r\n* Check all inputs for NaNs.\r\n* Find this issue :/\r\n\r\nI don't see the point of `add_check_numerics_ops` as long as this issue is not resolved. Which brings me back to my original issue: how do i debug NaNs in the code?\r\n\r\nDo i have to add individual check_numeric_ops to every single operation (except pows ofc.). That cannot be the best answer.\r\n\r\nFor now I've replaced all x**2 with x*x...", "btw there's also dedicated `tf.square(x)` op. Another problem with `add_check_numerics` is that ops created by optimizers are not checked https://github.com/tensorflow/tensorflow/issues/2288\r\n\r\n", "@rasmusbergpalm the reason you are seeing this error is because gradient is because of how gradient of `pow` is implemented in [math_grad.py](https://github.com/tensorflow/tensorflow/blob/64edd34ce69b4a8033af5d217cb8894105297d8a/tensorflow/python/ops/math_grad.py#L686)\r\n\r\nThere's this line\r\n\r\n`log_x = array_ops.where(x > 0, math_ops.log(x), array_ops.zeros_like(x))`\r\n\r\nSo what happens is that `check_numerics` adds a a numeric check to both branches, so there's an infinity produced during evaluation of your graph, but presumably you don't care about it because it doesn't affect your result.\r\n\r\nNot sure there's an easy general solution here -- both branches of select are evaluated here, so if there are side-effects, then then this infinity may make its way into final result. IE, you could have control dependencies or something like this.\r\n\r\n`log_x = array_ops.where(x > 0, y.assign(math_ops.log(x)), z.assign(array_ops.zeros_like(x)))`\r\n\r\nIn this case the one branch's value is assigned to `log_x`, but the second branch is still evaluated, so z is updated and infinity there could be important.\r\n\r\nPerhaps the way to fix it would be to have a fused implementation of gradient of `Pow` op", "Thanks for the explanation @yaroslavvb \r\n\r\nI think I understand why this is happening. I still think it's pretty bad, regardless of why it is so.\r\n\r\nUnfortunately, I don't have any opinion on how to fix it, as I'm not familiar with the TF source code.", "It's pretty hard to come up with a generic 'magically debug my big hairy numeric algorithm' solution when internally code is written to produce and mask numeric errors.  It's even harder to do this without destroying performance on GPU.\r\n\r\nAt one point in distant history, I remember reading the code of an inception model which only added explicit CheckNumerics ops just before writing gradients back to the model variables (hence preventing state corruption).   \r\n\r\nIf somebody has spare time on their hands, here is what I would do:\r\na) Change the `ApplyGradients` method of the `Optimizer` class to add `CheckNumerics` ops.\r\nb) Wrap training steps in an exception handler which catches any NaN exceptions, and uses the model checkpoint code to write the equivalent of a 'core dump'. \r\nc) Use the TF debugger code, partial evaluation or just explicit feed/fetch to reload the checkpoint and then repeatedly execute the step to examine the values of (all) intermediate tensors in the graph.\r\n\r\nThis technique relies on deterministic execution, so wouldn't work if there were Queues or any other stateful ops in the graph, and would rely on feeding the same training data.\r\n", "I also just spent hours on this. I solved it by replacing x**2.0 by tf.square(x).", "Just a +me-too here. The great thing with `tf.add_check_numerics` is that it allows one to relatively quickly identify where NaNs and Infs appear first, which is usually the place one is most interested in. However, this assumption fails for graphs, where NaN's and Infs a are expected and later explicitly handled, such as in the implementation of `tf.pow`. The problem is, as soon as I have one such case in my graph, I cannot use the great `tf.add_check_numerics` anymore, even when it would be perfectly fine for the 99 % remainder of the graph.\r\nThe \"perfect\" generic \"magicaly debug my big hairy numeric algorithm\" solution would probably require something like a \"non-signalling\" assert, i.e. an exception that propagates through the graph and only is reported if it makes it to the end; exactly like fp non-signalling NaNs and Infs, but with the additional traceback/debugging information being pulled along, in order to still be able to identify the source. Unfortunately, that would be very difficult to implement.\r\nOne workaround that would at least allow me to continue to use `tf.add_check_numerics` would be a way to explicitly silence/remove those checks from operations that I have explicitly identified as being unproblematic. Is there a way to accomplish this?\r\nIn any case, a warning in the documentation of `tf.add_check_numerics` (ideally with a list of well known operations with this problem) would probably save many people hours of debugging such issues. Of course, a fused implementation of the gradient of pow would avoid this particular case, as would an optimiser identifying and replacing the very common case of  `x**2`.", "One year later and none the wiser... I again spent hours debugging a big hairy numeric algorithm with NaNs, this time apparently with `tf.metrics.accuracy` (I did verify/assert that all dimensions are positive)? Any news on how to more effectively debug NaNs? `add_check_numerics` is just useless this way, but would be very useful if there was a way for selectively silencing certain sub-trees. Unfortunately, most other debugging help one finds on google is either outdated or doesn't work for my case: Training an `Estimator` from an IPython Notebook. `tfdbg` doesn't help here either..."]}, {"number": 3936, "title": "Video summary", "body": "Hey, \nJust wondering if there are any plans to add video summaries to tensorboard??\nAlex\n", "comments": ["@danmane can comment further, but there is work to make tensorboard more extensible so that people could add additional summaries on their own.\n", "We're sure to do this eventually, although it's not on the immediate roadmap. As andrew said, we are currently working on a plugin system for TensorBoard.\n\nIf any other contributors are interested in working on this, we could get this launched a lot sooner with some help. I'm marking it contributions welcome.\n", "I have done some ad hoc video visualization in the context of video prediction (https://github.com/tensorflow/models/issues/553). I am interested to help implement this as a plugin (much like the embedding visualizer) via GIF (and/or a web-compatible video format).", "I've migrated this to the TensorBoard repo."]}, {"number": 3935, "title": "Update(mnist):ps/worker cluster framework", "body": "I updated the cluster setter function of `mnist_replica.py` with the latest and easy-understanding way in distributed tutorial [here](https://www.tensorflow.org/versions/r0.9/how_tos/distributed/index.html#putting-it-all-together-example-trainer-program).\n1. `worker_grpc_url` -> ps_hosts / worker_hosts / server.target\n2.Add the `FLAGS.job_name` to distinguish the `ps job` from `worker job`\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "@rmlarsen Thanks for your reply and test\n", "@mrry please review this at your convenience.\n", "@rmlarsen Thank you !\n@mrry Could you have a look at this request? I think it's a useful update\n", "@mrry friendly ping :-)\n", "@mrry You are right! I have updated all your comments::\n- Delete the useless FLAGS arguments and info outputs\n- Delete the `cluster_setter()` and inline it at the call site\n- Move the `if` statement to the same block where the `tf.train.SyncReplicasOptimizer' is constructed \n\nAdd two changes:\n- FLAGS.worker_index -> FLAGS.task_index\n- Add the GPU allocation \n\nPlease check the latest commit! Thanks for your comments.\n", "@mrry  I have updated again, and the latest version fixed the bugs as below:\n- Delete the redundant argument `FLAGS.num_workers` \n- `num_gpu` -> `num_gpus`\n- Support to use `/cpu:0` instead of `/gpu:#` when setting `num_gpus=0` \n- Combine the `if FLAGS.sync_replicas` statement to `SyncReplicasOptimizer` block.\n- Add `FLAGS.job_name`, `FLAGS.task_index` and `FLAGS.num_gpus` available check. That is, it will raise an exception if any of them is not set, or unknown\n", "@mrry Please check the updates\n", "OK, these changes all look good to me. However, I think they'll cause the existing distributed tests to fail, because they assume that the old flags are valid. I'm not sure if the automated tests cover these, so I'll assign this to @caisq, who wrote the test and can confirm.\n", "@tensorflow-jenkins test this please.\n", "@mrry Thanks for your reply!\n", "@caisq @rmlarsen  @mrry  Please Check the updates :(\n", "@danmane Could you help me check and merge the PR? It has been a long time to wait for reviews, but no assignees come. Moreover, mrry has reviewed and checked the code before, and you can look through the  PR timeline~ :)\n", "@DjangoPeng Before we can submit this, please update the [distributed tests](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/dist_test) that use this script. In particular [`dist_mnist_test.sh`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/scripts/dist_mnist_test.sh) makes assumptions about the flags that `mnist_replica.py` accepts. @caisq can advise on how to run this test.\n", "@mrry I'll update it today. It would be better If @caisq can give some suggestions\n", "LGTM except for the default value of sync_replicas, which should be False until the issue is resolved. We can merge this PR after @DjangoPeng fixes that. Then I will prepare a PR to update dist_mnist_test.sh accordingly myself.\n", "@caisq @mrry I have updated the `dist_mnist_test.sh` and it works well in my test. You can check and run the script. \nSome change notes for `dist_mnist_test.sh`:\n- Delete `num-workers` and `num-ps` arguments\n- Add `ps-hosts`, `worker-hosts` and `num-gpus` arguments\n- According to FLAGS.ps_hosts/worker_hosts, change the delimiter to \",\" in place of space.\n- `PS_ARRAY` and `WORKER_ARRAY` keep the delimiter as space, which for the log info `echo HOST`\n- Get `N_WORKERS` and `N_PS` by counting the IP addresses of `WORKER_HOSTS` and `IP_HOSTS`\n- `sync-replicas`=1 -> FLAGS.sync_replicas=True; \n- `sync-replicas`=0 -> FLAGS.sync_replicas=False;\n", "@caisq I have reverted the change of `sync_replicas` argument in `dist_mnist_test.sh`. Now, it would exit and output ERROR INFO in case of `sync_replicas`\n", "Thanks @DjangoPeng and thanks @caisq for undertaking to fix the test (although hopefully less work will be necessary with the latest changes). This looks good to me. I'll run the tests and then we should be good to merge!\n", "@tensorflow-jenkins test this please.\n", "@mrry Thanks for your reply. I hope my update of `dist_mnist_test.sh` work well, if so, @caisq don't need to fix for the latest changes.  \n", "Hi,\nI have a question regarding the mnist_replica.py program. \nIf I use only one machine (localhost) and set internally multiple workers everything runs fine. \nBut if I want to use e.g. 2 worker running on different machines only the worker with index 0 runs through (does not wait for the other worker with index 1) and the worker with index 1 complains about global Variables not set.\nI hope you can point me in the right direction!\n", "@icklerly  What TF version are you using? Could you show me your parameters?\n"]}, {"number": 3934, "title": "Branch 130790705", "body": "", "comments": ["@danmane I assume this PR is obsolete?\n", "Yup.\n"]}, {"number": 3933, "title": " configure script crashing in virtuealenv due to issues with site.getsitepackages()", "body": "In the current master, the line https://github.com/tensorflow/tensorflow/blob/master/util/python/python_config.sh#L63\n\ncauses a crash stating \n\n> AttributeError: module 'site' has no attribute 'getsitepackages'\n\n if running confgure inside virtualenv Python environment.\n\nThe underlying issue is that virtualenv uses an older site package that doesn't have getsitepackages. It is still an open bug in virtualenv after 4 years (!): https://github.com/pypa/virtualenv/issues/355.\n\nFor reference, I am using Python 3, virtualenv and virtualenvwrapper on Ubuntu 16.04 with CUDA 8.0. The v0.10.0rc0 release works for me, but this recent fix in python_config.sh breaks virtualenv usage.\n", "comments": ["I'm facing the same issue\nI'm using Python 2.7.5 \n", "@danmane, you merged this change I believe, but it was originally authored by @itsmeolivia. Could @itsmeolivia look at fixing this so it can work with the older site interface so that virtualenv doesn't break. Thanks!\n", "I ran it and it doesn't crash because of the AttributeError.  It crashes because it expects user input for the python library path and defaults to providing nothing if the python library path cannot be found nor provided by the user.  Right now it is working as intended but the installation can always be made a little smoother.\n\nHere's a quick workaround to find your path manually while I change the script a lil more:\n\nIn your virtualenv run:\n`$ python`\n`>>> from distutils.sysconfig import get_python_lib`\n`>>> print(get_python_lib())` \n\nCopy that output and when you run `./configure` paste it into the line that says \n`Please input the desired Python library path to use.  Default is []`\n", "looks that the issue is still there. I'm using virtualenv 15.0.1 and the latest tensorflow code (ae62a692ddb53253462c1f79702fbc45baeb4ae3) on ubuntu 16.04 x64.\r\n\r\n$virtualenv -p python3 ~/pyenv/tfbuild\r\n$source ~/pyenv/tfbuild/bin/activate\r\n(tfbuild) yguo18@yguo18-skl-u1604:/work/ml/tensorflow_work/tensorflow$ ./configure \r\nWARNING: ignoring http_proxy in environment.\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.15.0 installed.\r\nPlease specify the location of python. [Default is /home/yguo18/pyenv/tfbuild/bin/python]: \r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nAttributeError: module 'site' has no attribute 'getsitepackages'\r\nFound possible Python library paths:\r\n  /home/yguo18/pyenv/tfbuild/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/yguo18/pyenv/tfbuild/lib/python3.6/site-packages]\r\n/home/yguo18/pyenv/tfbuild/lib/python3.6/site-packages\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: n\r\nNo jemalloc as malloc support will be enabled for TensorFlow.\r\n...\r\n"]}, {"number": 3932, "title": "l2_normalize broken in master", "body": "`tf.nn.l2_normalize(tf.constant(np.ones([2, 2, 2, 2])), [0, 1, 2])` crashes with the following error:\n`ValueError: Shape (1, 3) must have rank at most 1`\n\nThe problem is that l2_normalize function converts the dimension list into `[[0, 1, 2]]` (double brackets) and it stopped being supported recently. It works fine in 0.10 branch as far as I can tell.\n\nShorter repro to get the same error:\n`tf.reduce_sum(tf.constant(np.ones([2, 2, 2, 2])), [[0, 1, 2]])`\n", "comments": ["Thanks, the bug is in l2_normalize.\n\ntf.reduce_sum is working correctly, the dims argument must be a scalar or a vector.  I'll work on a fix + test\n\nThe change that broke this was that reduce_sum was not checking its shape contract, and now it is.\n"]}, {"number": 3931, "title": "tensorflow protobuf problem about:A protocolmessage was rejected because it was too big (more than 67108864 bytes) ", "body": "I got this problem when trying to run a chief worker of distribute tensorflow. The tensorflow was installed by pip,not from source. Then I uninstalled protobuf by pip and then download the source and changed 256 << 20 in coded_stream.h .\nAnd I installed protobuf as Readme said.But the same problem occurred, It seems that tensorflow did not use the new compiled libprotobuf.  How could I fixed this issue? If I have to reinstall tensorflow? Or someone can give me a updated protobuf that can fix the 64m limit? Thanks If someone could help me, I am going to mad about this issue(T_T)\n", "comments": ["Duplicate of #3938   \n\nPlease see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#protobuf-library-related-issues\n"]}, {"number": 3930, "title": "missing API documentation to write user ops", "body": "For example, how to fill a tensor with all zeros. Is it built in with `context -> allocate_output` or we can get the internal memory and do `memset`?\n", "comments": ["We likely will not document information on how to implement internals. Fortunately there are many examples to learn from. \n\nGo see definition of ones for example\nhttps://github.com/tensorflow/tensorflow/blob/8de7608075c6dd6fcaf8f5c3d921f3611f8e7611/tensorflow/python/ops/array_ops.py#L1205\n\nIt uses the constant op\nhttps://github.com/tensorflow/tensorflow/blob/5df4c71c86b28c2a4dd746bd67f00fc0281bd24f/tensorflow/core/kernels/constant_op.cc\n\nplease look at that for guidance.\n", "Thanks for this information.\n"]}, {"number": 3929, "title": "No giflib at ufpr.dl.sourceforge.net - because no ufpr.dl.sourceforge.net", "body": "Maybe this is a transient issue and I'll try again later.\n\n```\npatfla@patfla-N550JV:~/code/tensorflow$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\nSending SIGTERM to previous Bazel server (pid=12785)... done.\n.\nERROR: /home/patfla/code/tensorflow/tensorflow/core/platform/default/build_config/BUILD:56:1: no such package '@gif_archive//': Error downloading from http://ufpr.dl.sourceforge.net/project/giflib/giflib-5.1.4.tar.gz to /home/patfla/.cache/bazel/_bazel_patfla/411134e9cd8b53ea4deaf22318a2a19e/external/gif_archive: Error downloading http://ufpr.dl.sourceforge.net/project/giflib/giflib-5.1.4.tar.gz to /home/patfla/.cache/bazel/_bazel_patfla/411134e9cd8b53ea4deaf22318a2a19e/external/gif_archive/giflib-5.1.4.tar.gz: ufpr.dl.sourceforge.net and referenced by '//tensorflow/core/platform/default/build_config:platformlib'.\nERROR: Analysis of target '//tensorflow/cc:tutorials_example_trainer' failed; build aborted.\nINFO: Elapsed time: 12.010s\n```\n\n```\npatfla@patfla-N550JV:~/code/tensorflow/tensorflow$ grep -ir ufpr *\ncontrib/cmake/external/gif.cmake:set(gif_URL http://ufpr.dl.sourceforge.net/project/giflib/giflib-5.1.4.tar.gz)\nworkspace.bzl:    url = \"http://ufpr.dl.sourceforge.net/project/giflib/giflib-5.1.4.tar.gz\",\n\n```\n\n```\npatfla@patfla-N550JV:~/code/tensorflow/tensorflow$ ping ufpr.dl.sourceforge.net\nping: unknown host ufpr.dl.sourceforge.net\n\n```\n", "comments": ["And now, a short while later, mysteriously - it works.\n\nClose issue.\n", "I'm experiencing the exact same issue - same file, host and lack of connectivity. I'll try again later.\n", "Yes, sad truth, a whole build depends on sourceforge availability :/\n", "Here's a temporary fix, I've put the giflib tarball at https://deepdetect.com/stuff/giflib-5.1.4.tar.gz\nThen modify the two files below by replacing the sourceforge URL with the one above.\n- `tensorflow/workspace.bzl`\n- `tensorflow/contrib/cmake/external/gif.cmake`\n  cc @Mike-Dax \n", "Turned out several packages, giflib, swig (both on sourceforge) and zlib (on their site) are having network problems - so I grabbed them from various other sources.\n\nIt also turns out the JVM doesn't like letsencrypt, so I rehosted it all myself locally and it seems to be working now.\n\nHere's the SSL error for those who will inevitably google this later.\n\n```\nsun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target and referenced by '//tensorflow/core/platform/default/build_config:platformlib'.\n```\n\nThanks for your help @beniz \n", "faced with the same issue, cannot connect to ufpr.dl.sourceforge.net, main sourceforge site is available\n", "Same issue with `http://ufpr.dl.sourceforge.net/project/giflib/giflib-5.1.4.tar.gz`\n"]}, {"number": 3928, "title": "Fix broken link for gif_archive", "body": "", "comments": ["@tensorflow-jenkins test this please\n"]}, {"number": 3927, "title": "Docs(Queue): Fix a PriorityQueue API missing bug", "body": "As we all know there are 4 subclass of `class QueueBase`, but only 3 subclass(FIFOQueue, PaddingFIFOQueue, RandomShuffleQueue) descriptions in the API Docs [here](https://www.tensorflow.org/versions/r0.10/api_docs/python/io_ops.html#QueueBase)\n\nSo I add the description of  `class PriorityQueue()` which are the comments in ../python/ops/data_flow_ops.py.\nStart line 714 to 759 and you can check [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/data_flow_ops.py#L714)\n", "comments": ["Thanks for the pull request! Unless I'm much mistaken, these files should be autogenerated from the source in data_flow_ops. @josh11b can you comment on what the right fix is?\n", "Can one of the admins verify this patch?\n", "The source for the documentation of tf.PriorityQueue lives in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/data_flow_ops.py#L715\n\ntensorflow/g3doc/api_docs/python/io_ops.md is automatically generated.  Any changes to it will be overwritten the next time the documentation generator runs.  In fact, its very first line is:\n\n<!-- This file is machine generated: DO NOT EDIT! -->\n\nThe fact that PriorityQueue is not being generated is an issue, which we can hopefully fix by adding PriorityQueue to the list here:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/io_ops.py#L82\n(And hopefully then the doc generator will generate docs from the data_flow_ops.py file -- see https://www.tensorflow.org/versions/r0.10/how_tos/documentation/index.html for instructions on how to run the doc generator yourself. TL/DR: tools/docs/gen_docs.sh)\n\nIf, once the documentation is being generated, there are still improvements to be made, those changes should be made to data_flow_ops.py.\n", "@josh11b So it is a missing mistake of the list, and when could you run the doc generator script? \nAnd do you mean I don't need to push this commits this time?\n", "@DjangoPeng @josh11b I think Josh's point was that if you add the correct entry to\n\n https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/io_ops.py#L82\n\nthen the documentation will get automatically generated (visibility pending our push). Please go ahead and make that change as well as any documentation updates to data_flow_ops.py in this PR.\n", "@rmlarsen I have added `PriorityQueue` into the list, plz check\n", "@DjangoPeng  Can you please revert the changes from io_ops.md, since this is an autogenerated file, and instead update the documentation in \n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/data_flow_ops.py#L715\n\nif needed.\n", "@rmlarsen I have reverted the change from `io_ops.md`, and it's unnecessary to update the `data__flow_ops.py` this time. Plz check. \n", "@tensorflow-jenkins test this please\n"]}, {"number": 3926, "title": "Failing to restore net", "body": "### Environment info\n\nOperating System:\nMac OS X 10.11.4\nTensorflow installed from pre-built pip (no CUDA):\n0.10.0rc0\n### Problem\n\nI have problems with restoring my net [(from SO)](http://stackoverflow.com/questions/39035041/trouble-restoring-checkpointed-tensorflow-net). I have created a short [test program](https://github.com/tensorflow/tensorflow/files/427240/bug_report.zip) that have the same problem as my real program.\n\nThe program train a net, run one inference with it and then checkpoints the model. Then it loads the checkpointed model and run the inference with the same data and compare the result. I expected the results to be very similar but they weren't:\n\n```\nRestoring graph from /tmp/bugreport/model.ckpt-0\nInference after training gave 2.40740537643\nInference after restoring net gave 62579.6210938\n```\n\n``` python\nimport os\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.layers as contrib\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string('tmp_dir', '/tmp/bugreport', \"\"\"Temp dir\"\"\")\n\nSIZE = 5\nNUM_DATA = 10\nBATCH_SIZE = 1\nDEPTH = 1\n\ndef inference(input_tensor, is_training):\n    \"Define a stupid net\"\n    bn_params = {\n            \"is_training\": is_training,\n            \"center\": True,\n            \"scale\": True\n            }\n    tensor = contrib.convolution2d(input_tensor, 8, 3,\n            normalizer_fn=contrib.batch_norm,\n            normalizer_params=bn_params,\n            scope=\"conv1\")\n    tensor = tf.reduce_sum(tensor)\n    return tensor\n\ndef training(input_data, label_data, test_data):\n    \"\"\"1. Train the net\n    2. Do an inference pass with the trained net\n    3. Checkpoint the trained net\n    \"\"\"\n    with tf.Graph().as_default():\n        input_tensor = tf.placeholder(tf.float32, [BATCH_SIZE, SIZE, SIZE, DEPT#\n        label_tensor = tf.placeholder(tf.float32, [BATCH_SIZE, DEPTH], name=\"la#\n\n        output = inference(input_tensor, tf.constant(True))\n        loss = tf.nn.l2_loss(output - label_tensor, name=\"loss\")\n        train_op = tf.train.AdamOptimizer(0.9999).minimize(loss)\n        init = tf.initialize_all_variables()\n        saver = tf.train.Saver(tf.all_variables())\n\n        with tf.Session() as sess:\n            sess.run([init])\n            for i in range(NUM_DATA):\n                _ = sess.run(train_op, { input_tensor: input_data[i],\n                                         label_tensor: label_data[i] })\n            training_out = sess.run(output, { input_tensor: test_data })\n            cp_path = os.path.join(FLAGS.tmp_dir, \"model.ckpt\")\n            saver.save(sess, cp_path,\n                       global_step=0, write_meta_graph=None)\n    return training_out\n\ndef use_restored_net(test_data):\n    \"\"\"1. Load checkpointed net\n    2. Do an inference pass with the trained net\n    \"\"\"\n    with tf.Graph().as_default():\n        input_tensor = tf.placeholder(tf.float32, [BATCH_SIZE, SIZE, SIZE, DEPT#\n\n        output = inference(input_tensor, tf.constant(False))\n        init = tf.initialize_all_variables()\n\n        ckpt = tf.train.get_checkpoint_state(FLAGS.tmp_dir)\n        if ckpt and ckpt.model_checkpoint_path:\n            print \"Restoring graph from {}\".format(ckpt.model_checkpoint_path)\n        else:\n            raise ValueError(\"Could not find a checkpointed model\")\n        saver = tf.train.Saver(tf.all_variables())\n        with tf.Session() as sess:\n            sess.run([init])\n            saver.restore(sess, ckpt.model_checkpoint_path)\n            inference_out = sess.run(output, { input_tensor: test_data })\n    return inference_out\n\ndef main(argv):\n    if tf.gfile.Exists(FLAGS.tmp_dir):\n        tf.gfile.DeleteRecursively(FLAGS.tmp_dir)\n    tf.gfile.MakeDirs(FLAGS.tmp_dir)\n\n    input_data = np.random.rand(NUM_DATA, BATCH_SIZE, SIZE, SIZE, DEPTH)\n    label_data = np.random.rand(NUM_DATA, BATCH_SIZE, DEPTH)\n    test_data = np.random.rand(BATCH_SIZE, SIZE, SIZE, DEPTH)\n\n    training_out = training(input_data, label_data, test_data)\n    inference_out = use_restored_net(test_data)\n    print \"Inference after training gave {}\".format(training_out)\n    print \"Inference after restoring net gave {}\".format(inference_out)\n\n\nif __name__ == '__main__':\n    tf.app.run()\n\n```\n", "comments": ["Perhaps due to difference in models? (ie, is_training=True vs is_training=False)\n", "No, same thing when setting is_training=True on the restored net.\n\nAnd BTW, isn't the purpose of the `is_training` argument to batch normalization to \"integrate\" the learned mean and std into the layer during interference?\n", "@yaroslavvb I was too quick when I said it was the same result (I must have forgotten to save the file of something). When I set `is_training=True` I get the same result when doing the inference with the restored net.\n\nBut I still don't understand the purpose of the `is_training` parameter since setting it to `False` essentially throws away the learned mean and variance, replace them with 0.0 and 1.0 and then do the normalization. If this is the intended behavior of `is_training` (but the doc string indicates that it isn't) perhaps it should be called `use_learned_data` instead.\n", "During training there are two sources of mean and variable -- statistics over mini-batch, and global statistics computed  using geometric averaging. The idea is that while training you use mini-batch statistics to normalize, and just save global statistics for later. During inference you may not have mini-batches, so you global statistics instead. The name `is_training` is also used for dropout layers which has different behavior depending on whether you are training\n", "Yaroslav , thank your for taking the time to answer my questions on a weekend! :-)\n\nOK, I have made another [test program](https://github.com/tensorflow/tensorflow/files/428839/bug_report2.zip) that only uses a batch normalizer and I train beta and gamma to give the the distribution I want.\n\nI send in noise with distribution Norm(m=2,v=10) and then learn the parameters beta and gamma so the output from the batch normalization is distributed as Norm(m=7,v=5). Then I run an inference pass with test data directly after training I get the expected mean and variance (7 resp. 5).\nBut when I restore the net and send in the same test data I get some weird values that don't make any sense at all. Even if the input data is generated with Norm(m=2,v=10) which BatchNorm should have learnt by now. The moving mean and variance don't appear to be updated and the only reason that BN work when I run it in training mode is that it uses the batch mean/variance.\n\n```\ntest data:\n    mean = [ 1.99910051  2.00070378]\n    variance = [  9.99379123  10.02349832]\nAfter training:\n    calculated mean=[ 7.01989126  7.01992226]\n    calculated variance=[ 4.9753685   4.97816467]\n    beta=[ 7.01995707  7.0199585 ]\n    gamma=[ 2.23007703  2.23052549]\n    moving mean=[ 0.  0.]\n    moving variance=[ 1.  1.]\nAfter restore:\n    calculated mean=[ 11.47598934  11.4803896 ]\n    calculated variance=[ 49.58828735  49.75190735]\n    beta=[ 7.01995707  7.0199585 ]\n    gamma=[ 2.23007703  2.23052549]\n    moving mean=[ 0.  0.]\n    moving variance=[ 1.  1.]\n```\n\nTest program:\n\n``` python\nimport os\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.layers as contrib\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string('tmp_dir', '/tmp/bugreport', \"\"\"Temp dir\"\"\")\n\nSIZE = 500\nNUM_ITERS = 100\nBATCH_SIZE = 5\nDEPTH = 2\nMEAN = 2\nVARIANCE = 10\n\ndef _randn(*shape):\n    \"Create a tensor with values drawn from a gaussion distribution (MEAN, VARIANCE)\"\n    return np.sqrt(VARIANCE) * np.random.randn(*shape) + MEAN\n\n\ndef _get_bn_vars(sess):\n    \"Return values of internal BatchNorm variables\"\n    beta = None\n    gamma = None\n    moving_mean = None\n    moving_variance = None\n    for v in tf.all_variables():\n        if v.name == \"BatchNorm/beta:0\":\n            beta = sess.run(v)\n        elif v.name == \"BatchNorm/gamma:0\":\n            gamma = sess.run(v)\n        elif v.name == \"BatchNorm/moving_mean:0\":\n            moving_mean = sess.run(v)\n        elif v.name == \"BatchNorm/moving_variance:0\":\n            moving_variance = sess.run(v)\n    return beta, gamma, moving_mean, moving_variance\n\n\ndef inference(input_tensor, is_training):\n    \"Define a stupid net\"\n    bn_out = contrib.batch_norm(input_tensor,\n            center=True,\n            scale=True,\n            is_training=is_training)\n    return bn_out\n\n\ndef training(test_data):\n    \"\"\"1. Train the net\n    2. Do an inference pass with the trained net\n    3. Checkpoint the trained net\n    \"\"\"\n    with tf.Graph().as_default():\n        tf.set_random_seed(123)\n        input_tensor = tf.placeholder(tf.float32, [BATCH_SIZE, SIZE, SIZE, DEPTH], name=\"input\")\n\n        bn_out = inference(input_tensor, tf.constant(True))\n        mean, variance = tf.nn.moments(bn_out, [0, 1, 2])\n        loss = tf.nn.l2_loss(mean-7) + tf.nn.l2_loss(variance - 5)\n        train_op = tf.train.AdamOptimizer(0.9999).minimize(loss)\n        init = tf.initialize_all_variables()\n        saver = tf.train.Saver(tf.all_variables())\n\n        with tf.Session() as sess:\n            sess.run([init])\n            for _ in range(NUM_ITERS):\n                input_data = _randn(BATCH_SIZE, SIZE, SIZE, DEPTH)\n                _ = sess.run(train_op, { input_tensor: input_data } )\n            mean_val, variance_val = sess.run([mean, variance], { input_tensor: test_data })\n            cp_path = os.path.join(FLAGS.tmp_dir, \"model.ckpt\")\n            saver.save(sess, cp_path,\n                       global_step=0, write_meta_graph=None)\n            bn_vars = _get_bn_vars(sess)\n    return mean_val, variance_val, bn_vars\n\n\ndef use_restored_net(test_data):\n    \"\"\"1. Load checkpointed net\n    2. Do an inference pass with the trained net\n    \"\"\"\n    with tf.Graph().as_default():\n        input_tensor = tf.placeholder(tf.float32, [BATCH_SIZE, SIZE, SIZE, DEPTH], name=\"input\")\n\n        bn_out = inference(input_tensor, tf.constant(False))\n        mean, variance = tf.nn.moments(bn_out, [0, 1, 2])\n        init = tf.initialize_all_variables()\n\n        ckpt = tf.train.get_checkpoint_state(FLAGS.tmp_dir)\n        if not(ckpt and ckpt.model_checkpoint_path):\n            raise ValueError(\"Could not find a checkpointed model\")\n        saver = tf.train.Saver(tf.all_variables())\n        with tf.Session() as sess:\n            sess.run([init])\n            saver.restore(sess, ckpt.model_checkpoint_path)\n            bn_vars = _get_bn_vars(sess)\n            mean_val, variance_val = sess.run([mean, variance], { input_tensor: test_data })\n    return mean_val, variance_val, bn_vars\n\n\ndef main(argv):\n    if tf.gfile.Exists(FLAGS.tmp_dir):\n        tf.gfile.DeleteRecursively(FLAGS.tmp_dir)\n    tf.gfile.MakeDirs(FLAGS.tmp_dir)\n\n    np.random.seed(123)\n    test_data = _randn(BATCH_SIZE, SIZE, SIZE, DEPTH)\n    print \"test data:\"\n    print \"    mean = {}\".format(np.mean(test_data, axis=(0, 1, 2)))\n    print \"    variance = {}\".format(np.var(test_data, axis=(0, 1, 2)))\n\n    test_mean, test_variance, train_bn = training(test_data)\n    print \"After training:\"\n    print \"    calculated mean={}\".format(test_mean)\n    print \"    calculated variance={}\".format(test_variance)\n    print \"    beta={}\".format(train_bn[0])\n    print \"    gamma={}\".format(train_bn[1])\n    print \"    moving mean={}\".format(train_bn[2])\n    print \"    moving variance={}\".format(train_bn[3])\n    restored_mean, restored_variance, restored_bn = use_restored_net(test_data)\n    print \"After restore:\"\n    print \"    calculated mean={}\".format(restored_mean)\n    print \"    calculated variance={}\".format(restored_variance)\n    print \"    beta={}\".format(restored_bn[0])\n    print \"    gamma={}\".format(restored_bn[1])\n    print \"    moving mean={}\".format(restored_bn[2])\n    print \"    moving variance={}\".format(restored_bn[3])\n\n\nif __name__ == '__main__':\n    tf.app.run()\n\n```\n", "So you are saying that moving_mean and moving_variance don't get updated when training? You can see the code in [layers.py](https://github.com/tensorflow/tensorflow/blob/008bcaea38815f46804fc3f56492f4dd93837a56/tensorflow/contrib/layers/python/layers/layers.py#L229). There's a node there \"update_moving_mean\" and as far as I can see, it should be triggered whenever you read output of batchnorm. You could add some passthrough Print nodes to that code (ie, do `import inspect;\ninspect.getsourcefile(tf.contrib.layers)` and modify `layers.py` in that directory) to double-check that those nodes get called\n", "Yes, I guess that is was I'm saying.\n\nI added the print outs and nothing got printed out. But when I changed this lines in `optimizer.Optimizer.apply_gradients()`\n\n``` python\nupdate_ops = []\n```\n\nto\n\n``` python\nupdate_ops = list(ops.get_collection(ops.GraphKeys.UPDATE_OPS))\n```\n\nthe batch normalizer did learn the distribution of its input (and got correctly restored)! I don't know if this is the correct place to fetch the `UPDATE_OPS` or if there needs to be more checks or whatever.\n\nThe [`optimize_loss()`](https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.layers.html#optimize_loss) function manually adds a dependency to `UPDATE_OPS` so I guess that is one way of solving it (and I'll probably go with the `optimize_loss()` anyway).\n", "an alternative way to save and restore network is save the parameters into npz file.\nexample code here: http://tensorlayer.readthedocs.io/en/latest/modules/files.html#load-and-save-network\n\nhope it help\n", "I don't think there's a bug here, expect for possibly a documentation issue. I'd be happy about PRs to improve that.\n"]}, {"number": 3925, "title": "[bug] tensorflow/models/image/mnist/convolutional.py doesn't converge with CUDA7.5+cuDNN4", "body": "### Environment info\n\nOperating System: Ubuntu 14.04 64bit\nActually it is a docker env\nnvidia/cuda:7.5-cudnn4-devel\n\nInstalled version of CUDA and cuDNN: CUDA7.5 and cuDNN4\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n``` bash\n# ls -l /usr/lib/x86_64-linux-gnu/libcud* \nlrwxrwxrwx 1 root root       29 Aug 12 05:12 /usr/lib/x86_64-linux-gnu/libcudnn.so -> /etc/alternatives/libcudnn_so\nlrwxrwxrwx 1 root root       17 Feb  9  2016 /usr/lib/x86_64-linux-gnu/libcudnn.so.4 -> libcudnn.so.4.0.7\n-rw-r--r-- 1 root root 61453024 Feb  9  2016 /usr/lib/x86_64-linux-gnu/libcudnn.so.4.0.7\nlrwxrwxrwx 1 root root       32 Aug 12 05:12 /usr/lib/x86_64-linux-gnu/libcudnn_static.a -> /etc/alternatives/libcudnn_stlib\n-rw-r--r-- 1 root root 62025862 Feb  9  2016 /usr/lib/x86_64-linux-gnu/libcudnn_static_v4.a\n```\n\n``` bash\n# ls -la /usr/local/nvidia/lib64/libcud*   \nlrwxrwxrwx 1  999  998      17 Aug 16 13:46 /usr/local/nvidia/lib64/libcuda.so -> libcuda.so.367.35\nlrwxrwxrwx 1  999  998      17 Aug 16 13:46 /usr/local/nvidia/lib64/libcuda.so.1 -> libcuda.so.367.35\n-rw-r--r-- 2 root root 8121032 Jul 12 05:51 /usr/local/nvidia/lib64/libcuda.so.367.35\n```\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n\nUbuntu/Linux 64-bit, GPU enabled, Python 2.7\nRequires CUDA toolkit 7.5 and CuDNN v4. For other versions, see \"Install from sources\" below.\n$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\n1. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\n``` bash\n# python -c \"import tensorflow; print(tensorflow.__version__)\"\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n0.10.0rc0\n```\n### Steps to reproduce\n1. run docker\n   docker run -it nvidia/cuda:7.5-cudnn4-devel\n2. install tensorflow as described in the doc using pip\n3. run the example, and it does not converge, logs are pasted in the end\n   `python /usr/local/lib/python2.7/dist-packages/tensorflow/models/image/mnist/convolution.py`\n### What have you tried?\n1. Install and use the CPU version\n   The mnist example converges well\n### Logs or other output that would be helpful\n\n``` bash\n# python /usr/local/lib/python2.7/dist-packages/tensorflow/models/image/mnist/convolutional.py\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 1080\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.8225\npciBusID 0000:01:00.0\nTotal memory: 7.92GiB\nFree memory: 7.17GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\nInitialized!\nStep 0 (epoch 0.00), 184.2 ms\nMinibatch loss: 12.054, learning rate: 0.010000\nMinibatch error: 90.6%\nValidation error: 90.2%\nStep 100 (epoch 0.12), 4.6 ms\nMinibatch loss: 5.381, learning rate: 0.010000\nMinibatch error: 85.9%\nValidation error: 88.7%\nStep 200 (epoch 0.23), 4.1 ms\nMinibatch loss: 5.389, learning rate: 0.010000\nMinibatch error: 89.1%\nValidation error: 88.7%\nStep 300 (epoch 0.35), 4.2 ms\nMinibatch loss: 5.360, learning rate: 0.010000\nMinibatch error: 93.8%\nValidation error: 88.7%\nStep 400 (epoch 0.47), 4.2 ms\nMinibatch loss: 5.287, learning rate: 0.010000\nMinibatch error: 85.9%\nValidation error: 88.7%\nStep 500 (epoch 0.58), 4.1 ms\nMinibatch loss: 5.267, learning rate: 0.010000\nMinibatch error: 85.9%\nValidation error: 88.7%\n\n... ... (too long so deleted)\n\nStep 8000 (epoch 9.31), 4.6 ms\nMinibatch loss: 3.924, learning rate: 0.006302\nMinibatch error: 89.1%\nValidation error: 88.7%\nStep 8100 (epoch 9.43), 4.9 ms\nMinibatch loss: 3.925, learning rate: 0.006302\nMinibatch error: 93.8%\nValidation error: 88.7%\nStep 8200 (epoch 9.54), 5.0 ms\nMinibatch loss: 3.898, learning rate: 0.006302\nMinibatch error: 82.8%\nValidation error: 88.7%\nStep 8300 (epoch 9.66), 4.6 ms\nMinibatch loss: 3.882, learning rate: 0.006302\nMinibatch error: 81.2%\nValidation error: 88.7%\nStep 8400 (epoch 9.77), 4.9 ms\nMinibatch loss: 3.881, learning rate: 0.006302\nMinibatch error: 85.9%\nValidation error: 88.7%\nStep 8500 (epoch 9.89), 4.7 ms\nMinibatch loss: 3.877, learning rate: 0.006302\nMinibatch error: 87.5%\nValidation error: 88.7%\nTest error: 88.7%\n```\n", "comments": ["it is a strange problem, could you try the code from wrapper first? then you will know whether it is caused by cuda or something else.\n\nthis is a code you don't need to install the wrapper package.\n\ndescirption \nhttp://tensorlayer.readthedocs.io/en/latest/user/tutorial.html#understand-the-mnist-example\ncode:\nhttps://github.com/zsdonghao/tensorlayer/blob/master/tutorial_mnist.py\n\nhope it help\n", "You have a GTX 1080 card, but you use CUDA 7.5.\n\nFor GTX 1080 to work, you need CUDA 8.0, see these Issues:\n#3507\n#3068\n", "Yes- it seems likely this is a GTX10x0 related problem.  Please reopen if this does not fix the problem.\n", "This issue still exists when using GTX 1080 with CUDA 8 and cuDNN 5.1, I have tried all the TF docker versions, which did not help.\nPS: I am using Ubuntu 15.04 LTS, python 2.7.\nI found convolution cannot work, however, using just softmax works fine!\n", "Found out it is due to `cuDNN`.\n\nWhen you download `cuDNN`, there are texts showing:\n\n> Please check your framework documentation to determine the recommended version of cuDNN.\n> If you are using cuDNN with a Pascal (GTX 1080, GTX 1070), version 5 or later is required.\n\nI tried compiling tensorflow from source with CUDA8.0+cuDNN5.1. Everything works fine.\n\n@physicso please try TF docker with `cuDNN-5x`.\n", "Thank you very much @shiquanwang ! It turns out this issue is caused by Tensorflow itself. I tried the newest TF docker image version nightly-devel-gpu, it works!\n"]}, {"number": 3924, "title": "max of two tf.Dimension objects undetermined", "body": "### Environment info\n\nOperating System: ubuntu 16.04\n\nInstalled version of CUDA and cuDNN: None\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   0.10.0rc0\n### Steps to reproduce\n\n``` python\nimport tensorflow as tf\nmax(tf.Dimension(1), tf.Dimension(None))\n# => Dimension(1)\nmax(tf.Dimension(None), tf.Dimension(1))\n# => Dimension(None)\n```\n\nI've looked at the code of Dimension class. It defines the comparison between integers and None to return None, causing this undetermined behavior when using max. I suggest that a better way could be to raise an Error when comparing with None or to always set integers to be larger, like python does. \n", "comments": ["Unfortunately raising an error in the `__lt__` (etc.) operators would break many shape functions, which rely on the comparison operations being tri-state (`True` and `False` when the values are both known; `None` when either value is unknown). It would be nice to raise an error on `max()` if either dimension is unknown, but I'm not sure of a way to do that without breaking the existing behavior of the comparison operators.\n", "Thanks, so that's the difficulty. But as far as I know, there seems no way to raise an error on max() outside the `__lt__` operators.\n", "I'm going to close this issue as \"not feasible at present\". We might revisit it when all the Python shape functions move to C++, as the rationale for tri-state `__lt__()` (etc.) will have been eliminated by then.\n"]}, {"number": 3923, "title": "How to let tensorflow uses the new installed protobuf?", "body": "When I installed tensorflow with \"pip install\",I meet the \"A protocol message was rejected because it was too big (more than 67108864 bytes) \",and I uninstall protobuf with pip,then download a new protobuf,install it with changing 256 << 20 in coded_stream.h as \"Readme\" said,but the same problem occurred,it seems that tensorflow hasn't used the new protobuf,how could I deal with this problem?\n", "comments": ["Sounds like a protobuf version mismatch, do you get the same problem if you do a fresh install in virtualenv using the [official instructions](https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html)?\n", "@yaroslavvb sure I do ,but it didn't work too. I also tried to upgrade the > 64 version limited as said in \" https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#protobuf-library-related-issues\"\n,but after I upgraded , I can't import tensorflow , reason is segment fault. My tensorflow is installed by pip,\nnot from source.Could some one can give me a  > 64 limited protobuf version that can be upgrade by pip?Thanks!\n", "Duplicate of #3938\n"]}, {"number": 3922, "title": "Loaded runtime CuDNN library error", "body": "### Problem\n\nFirst I was having this problem\n\n```\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:347] Loaded runtime CuDNN library: 5103 (compatibility version 5100) but source was compiled with 4007 (compatibility version 4000).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.\n```\n\nAnd I run configure script again, and this time I made sure I input specific version.\nAnd compile. And here we are again.\n\n```\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:347] Loaded runtime CuDNN library: 5103 (compatibility version 5100) but source was compiled with 5005 (compatibility version 5000).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.\n```\n\nSo I doubt there are some problems with configuration. So I open configure script and read. It says configuration is stored in cuda.config file.\nHere is cuda.config\n\n```\n# CUDA_TOOLKIT_PATH refers to the CUDA toolkit.\nCUDA_TOOLKIT_PATH=\"/usr/local/cuda\"\n# CUDNN_INSTALL_PATH refers to the cuDNN toolkit. The cuDNN header and library\n# files can be either in this directory, or under include/ and lib64/\n# directories separately.\nCUDNN_INSTALL_PATH=\"/usr/local/cuda-7.5\"\n\n# The Cuda SDK version that should be used in this build (empty to use libcudart.so symlink)\nTF_CUDA_VERSION=7.5\n\n# The Cudnn version that should be used in this build\nTF_CUDNN_VERSION=5.1.3\n```\n\nSo **why** I have that problem? I already input version, but it failed **again**. I really can't understand.\n### Environment info\n\nOperating System: Ubuntu 14.04\n\nInstalled version of CUDA and cuDNN:  7.5 5.1.3\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\ndst@dst-desktop:~$ ls -l /usr/local/cuda/lib64/libcud*\n-rw-r--r-- 1 root root   322936  8\u6708 16  2015 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16  8\u6708 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root       19  8\u6708 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root   383336  8\u6708 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root   720192  8\u6708 16  2015 /usr/local/cuda/lib64/libcudart_static.a\n-rwxr-xr-x 1 root root 60696704  8\u6708 19 12:50 /usr/local/cuda/lib64/libcudnn.so\n-rwxr-xr-x 1 root root 60696704  8\u6708 19 12:50 /usr/local/cuda/lib64/libcudnn.so.5\n-rwxr-xr-x 1 root root 59823168  7\u6708  4 20:17 /usr/local/cuda/lib64/libcudnn.so.5.0.4\n-rwxr-xr-x 1 root root 59909104  7\u6708  4 21:22 /usr/local/cuda/lib64/libcudnn.so.5.0.5\n-rwxr-xr-x 1 root root 60696704  8\u6708 19 12:50 /usr/local/cuda/lib64/libcudnn.so.5.1.3\n-rwxr-xr-x 1 root root 48217000  9\u6708 27  2015 /usr/local/cuda/lib64/libcudnn.so.7.0\n-rwxr-xr-x 1 root root 48217000  9\u6708 27  2015 /usr/local/cuda/lib64/libcudnn.so.7.0.64\n-rw-r--r-- 1 root root 59715990  8\u6708 19 12:50 /usr/local/cuda/lib64/libcudnn_static.a\n```\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n\n```\ndst@dst-desktop:~/Github/tensorflow$ git rev-parse HEAD\nd8dddca5b11212ec6e8fe372d774d60f452dab24\n```\n1. The output of `bazel version`\n\n```\ndst@dst-desktop:~/Github/tensorflow$ bazel version\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp as int: 0\n```\n### Steps to reproduce\n1. ./configure\n2. bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n3. bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\n   4.pip install that package\n### What have you tried?\n1. recompile\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["I have find the problem. It is caused by copying libcudnn.so incorrectly.\nRecommend using `sudo nautilus`.\n"]}, {"number": 3921, "title": "MemoryError in ubuntu, not mac", "body": "Hello, my script gets killed by MemoryError, running in an aws EC2 instance, with 7.5G memory.\nHowever, it has no problem running in my Mac (8g memory) !!! they use same TF, but numpy version is slightly different.\n\nPython 2.7.10 (default, Oct 23 2015, 19:19:21) \n[GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.0.59.5)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n> > > import numpy as np\n> > > np.**version**\n> > > '1.11.1'\n\nPython 2.7.6 (default, Jun 22 2015, 17:58:13) \n[GCC 4.8.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n> > > import numpy as np\n> > > np.**version**\n> > > '1.11.0'\n\nI have absolutely no idea what happened... and my data is not likely to require that much memory either...  i did memory profiling in EC2, error occurs at line 290.\n# Line #    Mem usage    Increment   Line Contents\n\n   275  412.289 MiB    0.000 MiB       @profile\n   276                                 def test_naive(self, mx, yls, fn_model=None, W=None, b=None):\n   277                                     \"\"\"load variables and calculate acc\"\"\"\n   278 5117.520 MiB 4705.230 MiB           mx_array = mx.toarray()\n   279 5117.520 MiB    0.000 MiB           if fn_model is not None:\n   280                                         reader = tf.train.NewCheckpointReader(fn_model)\n   281                                         logger.info(\"loaded saved param from \" + fn_model)\n   282                                         W, b = reader.get_tensor(\"W\"), reader.get_tensor(\"b\")\n   283                                     else:\n   284 5117.520 MiB    0.000 MiB               assert (W is not None) and (b is not None)\n   285  \n   286 5117.523 MiB    0.004 MiB           print 'read', type(W), type(b), mx_array.shape\n   287 5117.523 MiB    0.000 MiB           x, _ = self.init_placeholder()\n   288 5117.523 MiB    0.000 MiB           with tf.Session() as sess:\n   289 5117.523 MiB    0.000 MiB               tf.initialize_all_variables()\n   290 5130.176 MiB   12.652 MiB               y = sess.run(tf.nn.softmax(tf.matmul(x, W) + b), feed_dict={x: mx_array})\n\n  File \"meshtags/model_bow.py\", line 290, in test_naive\n    y = sess.run(tf.nn.softmax(tf.matmul(x, W) + b), feed_dict={x: mx_array})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 372, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 619, in _run\n    np_val = np.array(subfeed_val, dtype=subfeed_dtype)\nMemoryError\n\nany comments are welcome ! thanks a lot.\n", "comments": ["There were some memory problems that are fixed by running with tcmalloc -- http://goog-perftools.sourceforge.net/doc/tcmalloc.html Also, with tcmalloc you can turn on [heap profiler](http://goog-perftools.sourceforge.net/doc/heap_profiler.html) and get a more detailed statistic of where the memory is going\n", "@aimeida, please let us know if tcmalloc helps.\n", "thanks a lot for the response ! The \"sad\" truth is that there is NO bug, the program simply runs out of memory :-) I didn't realize that EC2 instance doesn't have swap -- that's why it works on Mac. \nVery sorry for the inconvenience :-(\n", "Thanks for letting us know, glad you figured out the issue.\n"]}, {"number": 3920, "title": "about attention_seq2seq function without embedding", "body": "we already have a function tf.nn.seq2seq.embedding_attention_seq2seq(), but if i want to use an embedding trained from other model, the function is not convenient, do we have a function like tf.nn.seq2seq.attention_seq2seq() that need an embedding para or the inputs embedded.\nThanks!\n", "comments": ["it seem that the seq2seq folder have different classes, but i haven't try.\nlet me know when you solve the problem xD\n", "This does not appear to be a bug report, but rather a general question.  Please can you reask on StackOVerflow.\n"]}, {"number": 3919, "title": "Tutorial Sample not working : TypeError: argument of type 'float' is not iterable", "body": "Running the python code with the following syntax\npython wide_n_deep_tutorial.py --model_type=wide_n_deep\n\nProduces the below error\n\n` python wide_n_deep_tutorial.py --model_type=wide_n_deep\n\nTraining data is downloaded to /tmp/tmpMD6Clq\nTest data is downloaded to /tmp/tmpfG7CwC\nTraceback (most recent call last):\n  File \"wide_n_deep_tutorial.py\", line 210, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"wide_n_deep_tutorial.py\", line 206, in main\n    train_and_eval()\n  File \"wide_n_deep_tutorial.py\", line 192, in train_and_eval\n    df_train[LABEL_COLUMN] = (df_train['income_bracket'].apply(lambda x: '>50K' in x)).astype(int)\n  File \"/usr/lib/python2.7/dist-packages/pandas/core/series.py\", line 2023, in apply\n    mapped = lib.map_infer(values, f, convert=convert_dtype)\n  File \"inference.pyx\", line 920, in pandas.lib.map_infer (pandas/lib.c:44780)\n  File \"wide_n_deep_tutorial.py\", line 192, in <lambda>\n    df_train[LABEL_COLUMN] = (df_train['income_bracket'].apply(lambda x: '>50K' in x)).astype(int)\nTypeError: argument of type 'float' is not iterable'\n", "comments": ["Please can you provide the information requested in the TensorFLow issues template.- namely installation method, OS/ packages versions and full repro instructions.\n", "is there any progress?\n", "Environment info\nCompute Optimized C4 [No GPU]\nhttps://aws.amazon.com/ec2/instance-types/\n\nOperating System:\n**>>> 14.04.4 LTS (GNU/Linux 3.13.0-91-generic x86_64)**\n\nInstalled version of CUDA and cuDNN: (please attach the output of ls -l /path/to/cuda/lib/libcud_):\nCuda is not installed\n*_>>> ls: cannot access /path/to/cuda/lib/libcud_: No such file or directory_*\n\nIf installed from binary pip package, provide:\n\nA link to the pip package you installed:\nThe output from python -c \"import tensorflow; print(tensorflow.**version**)\".\nIf installed from source, provide\n**>>> 0.10.0rc0**\n\nThe commit hash (git rev-parse HEAD)\nThe output of bazel version\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n**>>>python wide_n_deep_tutorial.py --model_type=wide_n_deep**\n\nWhat other attempted solutions have you tried?\n\n```\nTest data is downloaded to /tmp/tmpmM0UO0\nTraceback (most recent call last):\n  File \"wide_n_deep_tutorial.py\", line 212, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"wide_n_deep_tutorial.py\", line 208, in main\n    train_and_eval()\n  File \"wide_n_deep_tutorial.py\", line 193, in train_and_eval\n    df_train[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n  File \"/usr/lib/python2.7/dist-packages/pandas/core/series.py\", line 2023, in apply\n    mapped = lib.map_infer(values, f, convert=convert_dtype)\n  File \"inference.pyx\", line 920, in pandas.lib.map_infer (pandas/lib.c:44780)\n  File \"wide_n_deep_tutorial.py\", line 193, in <lambda>\n    df_train[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\nTypeError: argument of type 'float' is not iterable\n\n```\n\nLogs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n", "Amazon ec2 compute optimized servers do not use nvidia gpus, I havent had much luck getting cuda working on ec2 large g4 instance. I am able to run other TensorFlow examples without Cuda, does this one require Cuda\n", "python wide_n_deep_tutorial.py --model_type=wide\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nTraining data is downloaded to /tmp/tmp2NRlQI\nTest data is downloaded to /tmp/tmp1b0NGh\nTraceback (most recent call last):\n  File \"wide_n_deep_tutorial.py\", line 212, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"wide_n_deep_tutorial.py\", line 208, in main\n    train_and_eval()\n  File \"wide_n_deep_tutorial.py\", line 193, in train_and_eval\n    df_train[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n  File \"/usr/lib/python2.7/dist-packages/pandas/core/series.py\", line 2023, in apply\n    mapped = lib.map_infer(values, f, convert=convert_dtype)\n  File \"inference.pyx\", line 920, in pandas.lib.map_infer (pandas/lib.c:44780)\n  File \"wide_n_deep_tutorial.py\", line 193, in <lambda>\n    df_train[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\nTypeError: argument of type 'float' is not iterable\n", "I tried the version 0.8.0 and 0.10.0 of tensorflow, both failed\n", "I find the reason, it is because of the data file's format. there is the empty line at the end of file, just delete it \n", "I tried using the wide_n_deep_tutorial code on my personal data set. I am uploading my data file and the test file from AWS S3 and I have ensured that my data does not have an additional spaces at the end. \nHowever, I am still getting the same error message.\n\n![argument_of_type_ffloat_is_not_iterable](https://cloud.githubusercontent.com/assets/17990840/18753723/383e64a8-80b4-11e6-9a8e-c5957cf68c61.PNG)\n\nAny help would be appreciated. \n", "vasantivmahajan - did you figure out the issue? I have exactly the same problem.\n", "since my previous comment, I have changed the classification/label data from a number (1/0) to string data (win/lose) and this has resolved this problem\n", "Awesome, I will check it out\n\nOn Sat, Sep 24, 2016 at 12:06 PM, simon789 notifications@github.com wrote:\n\n> since my previous comment, I have changed the classification/label data\n> from a number (1/0) to string data (win/lose) and this has resolved this\n> problem\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3919#issuecomment-249381771,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AFn-MJkRm9xG20qbTXGeLJgMUS7R-ZSHks5qtXSygaJpZM4JoOSv\n> .\n\n## \n\nRegards\nChandan Maruthi\n", "Hey,\n\nThank you for your response, I really appreciate it. I was able to resolve\nthat issue. However, while running the program I am getting another error\n\"TypeError: Signature mismatch. Keys must be dtype <dtype: 'string'>, got\n<dtype:'int64'>\"\n\nhttp://stackoverflow.com/questions/39730528/typeerror-signature-mismatch-keys-must-be-dtype-dtype-string-got-dtype\n\nDid you receive this error during your execution or do you know what could\nbe the reason for it?\n\nWarm Regards,\nVasanti Mahajan\n\nOn Sat, Sep 24, 2016 at 3:06 PM, simon789 notifications@github.com wrote:\n\n> since my previous comment, I have changed the classification/label data\n> from a number (1/0) to string data (win/lose) and this has resolved this\n> problem\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3919#issuecomment-249381771,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ARKEuDnMO-8fCUfDQdhQ601JkfCR2oN7ks5qtXSygaJpZM4JoOSv\n> .\n", "vasantivmahajan - I have taken a guess and posted it at the original link.\n", "one other thing...once I corrected my script I had to delete the model directory as it had been corrupted by the early termination of the programme. If you dont, you may get another error message.\n", "@chandanmaruthi : Yeah, it seems the blank lines at the end are a problem.\n\n@hengtze : Does it make sense to update [`wide_n_deep_tutorial.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/wide_n_deep_tutorial.py)  to discard trailing blank lines before parsing it? Ideally we would be able to use `skip_blank_lines`, but that seems to exist [only in newer versions of panda](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html), while Ubuntu 14.04 seems to use [pandas 0.13](http://pandas.pydata.org/pandas-docs/version/0.13.1/generated/pandas.read_csv.html?highlight=read_csv#pandas.read_csv), which doesn't have that. So we can either use `skip_footer` or explicitly trim empty lines after downloading.\n", "Is this still broken?", "Closing due to inactivity. Feel free to re-open if you would like us to look again.", "def input_fn(data_file, num_epochs, shuffle):\r\n  \"\"\"Input builder function.\"\"\"\r\n  df_data = pd.read_csv(\r\n      tf.gfile.Open(data_file),\r\n      names=CSV_COLUMNS,\r\n      skipinitialspace=True,\r\n      engine=\"python\",\r\n      skiprows=1)\r\n  # remove NaN elements\r\n  df_data = df_data.dropna(how=\"any\", axis=0)\r\n  labels = df_data[\"disease\"].apply(lambda x: \"no\" in x).astype(str)\r\n  return tf.estimator.inputs.pandas_input_fn(\r\n      x=df_data,\r\n      y=labels,\r\n      batch_size=10,\r\n      num_epochs=num_epochs,\r\n      shuffle=shuffle,\r\n      num_threads=2)\r\nI'm getting the following error for above code\r\n\r\nUnimplementedError (see above for traceback): Cast string to float is not supported\r\n\t [[Node: linear/head/ToFloat = Cast[DstT=DT_FLOAT, SrcT=DT_STRING, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](linear/head/labels)]]\r\n\r\nCan some one help to resolve it"]}]