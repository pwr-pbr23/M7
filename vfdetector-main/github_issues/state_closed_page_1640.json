[{"number": 3708, "title": "Can not access GPU when run python script with bazel", "body": "When we're using TensorFlow serving to train our model, we have found that the program is not able to access all the GPUs in that machine. This can be 100% reproduced and we add the following code for testing.\n\n```\nfrom tensorflow.python.client import device_lib\n\ndef get_available_gpus():\n    local_device_protos = device_lib.list_local_devices()\n    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n\nprint(get_available_gpus())\n```\n\nIf we run the script with `python`, it prints all the GPUs normally. If we run with `bazel build` and `bazel run` for the same script, it prints the empty list.\n### Environment info\n\nOperating System: Ubuntu 14.04\n\nInstalled version of CUDA and cuDNN: \nCUDA 7.5 and cuDNN 4.0\n\nIf installed from binary pip package, provide:\nTensorFlow 0.9.0\n### Steps to reproduce\n1. `git clone https://github.com/tensorflow/tensorflow && cd tensorflow/`\n2. Add the code to print GPUs or placements of the operations in mnist_with_summaries.py\n3. `python tensorflow/examples/tutorials/mnist/mnist_with_summaries.py` and we can see all the GPUs in this server.\n4. `bazel build //tensorflow/examples/tutorials/mnist:mnist_with_summaries`\n5. `bazel-bin/tensorflow/examples/tutorials/mnist/mnist_with_summaries` and it use CPU only and print nothing about GPUs.\n", "comments": ["I suspect the problem is that, when you run the result of `bazel build`, the resulting binary uses a newly-compiled version of TensorFlow that has whatever compilation options you specify in the build command. By default, `bazel` will build a CPU-only version, and you need to specify `--config=cuda` to get a GPU version.\n\nCan you try the following commands instead?\n\n```\n$ bazel build --config=cuda //tensorflow/examples/tutorials/mnist:mnist_with_summaries\n$ bazel-bin/tensorflow/examples/tutorials/mnist/mnist_with_summaries\n```\n", "Thanks @mrry . It works like exactly what you suspect and now the process is able to \"see\" the GPUs.\n\nBut I have another problem about the cuda compute capability version. I'm running this in AWS g2.8xlarge instance with NVIDIA K520 GPU which is cuda compute capability 3.0. I will try `TF_UNOFFICIAL_SETTING=1 ./configure` which may fix this.\n\n![screen shot 2016-08-10 at 09 54 20](https://cloud.githubusercontent.com/assets/2715000/17539521/6cb405ac-5ee1-11e6-8c32-869cca5bf1d9.png)\n", "After `TF_UNOFFICIAL_SETTING=1 ./configure` and setting the min compute capability as 3.0, it works like running `python` in my local environment. Not a bug of `bazel` or `TensorFlow`.\n\nThanks @mrry for detailed explaination and it would be better to add more documents about this.\n"]}, {"number": 3707, "title": "Fix TypeError in gradient clipping ops", "body": "Two gradient clipping ops, `tf.clip_by_global_norm` and `tf.clip_by_average_norm`, raise TypeError if `clip_norm` is a Tensor. This PR fixes the issue.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@ebrevdo Can you please review this?\n", "Jenkins, test this please.\n", "@tensorflow-jenkins test this please\n", "Is this related?  From the python3 failure:\n\n```\n======================================================================\nFAIL: testRNN (__main__.RNNTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/contrib/learn/rnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/rnn_test.py\", line 104, in testRNN\n    self.assertAllClose(predictions, np.array([1, 0]))\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/contrib/learn/rnn_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 447, in assertAllClose\n    np.testing.assert_allclose(a, b, rtol=rtol, atol=atol)\n  File \"/usr/local/lib/python3.4/dist-packages/numpy/testing/utils.py\", line 1183, in assert_allclose\n    verbose=verbose, header=header)\n  File \"/usr/local/lib/python3.4/dist-packages/numpy/testing/utils.py\", line 644, in assert_array_compare\n    raise AssertionError(msg)\nAssertionError: \nNot equal to tolerance rtol=1e-06, atol=1e-06\n\n(mismatch 100.0%)\n x: array([0, 0])\n y: array([1, 0])\n\n----------------------------------------------------------------------\nRan 3 tests in 106.213s\n\nFAILED (failures=1)\nnot close where =  (array([0]),)\nnot close lhs =  [0]\nnot close rhs =  [1]\nnot close dif =  [1]\nnot close tol =  [  2.00000000e-06]\n================================================================================\n```\n\nCan you try and replicate locally?\n", "@ebrevdo I have tested (rnn_test.py) on cpu from python3 environment. There are so many warnings, but the test is passed (please see the below log).\n\n```\n(dev-python3) shwang@digits:~/engines/contribute/tensorflow/tensorflow/contrib/learn/python/learn/estimators$ python rnn_test.py \nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmplu8ozsd6\nWARNING:tensorflow:Using default config.\nWARNING:tensorflow:Setting feature info to TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None), Dimension(5)]), is_sparse=False)\nWARNING:tensorflow:Setting targets info to TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None), Dimension(2)]), is_sparse=False)\n/home/shwang/engines/dev-python3/lib/python3.4/site-packages/tensorflow/python/ops/rnn_cell.py:285: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n  \"deprecated.  Use state_is_tuple=True.\", self)\nWARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f01697c5c50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\nWARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f015c4fcb00>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\nWARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f0157524d30>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\nWARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f014411e128>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp3ozfyqs2\nWARNING:tensorflow:Using default config.\nWARNING:tensorflow:Setting feature info to TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None), Dimension(5)]), is_sparse=False)\nWARNING:tensorflow:Setting targets info to TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None), Dimension(2)]), is_sparse=False)\nWARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f014445aa58>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\nWARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f0144465438>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n/home/shwang/engines/dev-python3/lib/python3.4/site-packages/tensorflow/contrib/rnn/python/ops/rnn_cell.py:712: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n  \"deprecated.  Use state_is_tuple=True.\" % self)\nWARNING:tensorflow:<tensorflow.contrib.rnn.python.ops.rnn_cell.AttentionCellWrapper object at 0x7f014443a9e8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\nWARNING:tensorflow:<tensorflow.contrib.rnn.python.ops.rnn_cell.AttentionCellWrapper object at 0x7f01443d29b0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\nWARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f0116229780>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\nWARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f01161de978>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\nWARNING:tensorflow:<tensorflow.contrib.rnn.python.ops.rnn_cell.AttentionCellWrapper object at 0x7f01161ffac8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\nWARNING:tensorflow:<tensorflow.contrib.rnn.python.ops.rnn_cell.AttentionCellWrapper object at 0x7f01161fff28>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n.WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp0uj5p96w\nWARNING:tensorflow:Using default config.\nWARNING:tensorflow:Setting feature info to TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None), Dimension(5)]), is_sparse=False)\nWARNING:tensorflow:Setting targets info to TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None), Dimension(2)]), is_sparse=False)\nWARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f01153a5b70>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\nWARNING:tensorflow:load_variable (from tensorflow.contrib.learn.python.learn.utils.checkpoints) is deprecated and will be removed after 2016-08-22.\nInstructions for updating:\nPlease use tf.contrib.framework.load_variable instead\nWARNING:tensorflow:load_variable (from tensorflow.contrib.learn.python.learn.utils.checkpoints) is deprecated and will be removed after 2016-08-22.\nInstructions for updating:\nPlease use tf.contrib.framework.load_variable instead\nWARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f0115181fd0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp8s4iuocx\nWARNING:tensorflow:Using default config.\nWARNING:tensorflow:Setting feature info to TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None), Dimension(5)]), is_sparse=False)\nWARNING:tensorflow:Setting targets info to TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None), Dimension(2)]), is_sparse=False)\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpkzhb54ml\nWARNING:tensorflow:Using default config.\nWARNING:tensorflow:Setting feature info to TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None), Dimension(5)]), is_sparse=False)\nWARNING:tensorflow:Setting targets info to TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None), Dimension(2)]), is_sparse=False)\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp67cz6hor\nWARNING:tensorflow:Using default config.\nWARNING:tensorflow:Setting feature info to TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None), Dimension(5)]), is_sparse=False)\nWARNING:tensorflow:Setting targets info to TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None)]), is_sparse=False)\nWARNING:tensorflow:load_variable (from tensorflow.contrib.learn.python.learn.utils.checkpoints) is deprecated and will be removed after 2016-08-22.\nInstructions for updating:\nPlease use tf.contrib.framework.load_variable instead\nWARNING:tensorflow:load_variable (from tensorflow.contrib.learn.python.learn.utils.checkpoints) is deprecated and will be removed after 2016-08-22.\nInstructions for updating:\nPlease use tf.contrib.framework.load_variable instead\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp1kckb3eq\nWARNING:tensorflow:Using default config.\nWARNING:tensorflow:Setting feature info to TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None), Dimension(5)]), is_sparse=False)\nWARNING:tensorflow:Setting targets info to TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None), Dimension(2)]), is_sparse=False)\nWARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f0157813390>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\nWARNING:tensorflow:<tensorflow.contrib.rnn.python.ops.rnn_cell.AttentionCellWrapper object at 0x7f015c345710>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\nWARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f0157664a20>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\nWARNING:tensorflow:<tensorflow.contrib.rnn.python.ops.rnn_cell.AttentionCellWrapper object at 0x7f0157348320>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n..\n----------------------------------------------------------------------\nRan 3 tests in 57.738s\n\nOK\n```\n", "@tensorflow-jenkins test this please\n@ebrevdo looks like a flaky test, but let's run the test again, just to keep the jenkins workers warm.\n", "@tensorflow-jenkins test this please\n"]}, {"number": 3706, "title": "Error: the tensor's graph is different from the session's graph", "body": "I am using my previously trained models to generate deep dream images as decscribe in one of the [tutorials](https://render.githubusercontent.com/view/ipynb?commit=0685152a02f1fd4dbd711a023c1ccea5a8b8262b&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f74656e736f72666c6f772f74656e736f72666c6f772f303638353135326130326631666434646264373131613032336331636365613561386238323632622f74656e736f72666c6f772f6578616d706c65732f7475746f7269616c732f64656570647265616d2f64656570647265616d2e6970796e62&nwo=tensorflow%2Ftensorflow&path=tensorflow%2Fexamples%2Ftutorials%2Fdeepdream%2Fdeepdream.ipynb&repository_id=45717250#deepdream). But when I try to call the render_deepdream method I am getting below error:\n\n> I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \n> I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:806] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:83:00.0)\n> # Succesfully loaded model from /data/model_cache/model.ckpt-39 at step=39.\n> \n> (3648, 5472)\n> Traceback (most recent call last):\n>   File \"imagenet_eval.py\", line 46, in <module>\n>     tf.app.run()\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n>     sys.exit(main(sys.argv))\n>   File \"imagenet_eval.py\", line 42, in main\n>     inception_eval.evaluate(dataset)\n>   File \"/home/ubuntu/experiment/models/inception/inception/inception_eval.py\", line 192, in evaluate\n>     _eval_once(saver, summary_writer, top_1_op, top_5_op, summary_op)\n>   File \"/home/ubuntu/experiment/models/inception/inception/inception_eval.py\", line 156, in _eval_once\n>     render_deepdream(tf.square(T('inception_v3/mixed_8x8x2048b/concat',graph)), img0=img0, session=sess)\n>   File \"/home/ubuntu/experiment/models/inception/inception/inception_eval.py\", line 94, in render_deepdream\n>     lo = resize(img, np.int32(np.float32(hw)/octave_scale), session=session)\n>   File \"/home/ubuntu/experiment/models/inception/inception/inception_eval.py\", line 66, in wrapper\n>     return out.eval(dict(zip(placeholders, args)), session=kw.get('session'))\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 555, in eval\n>     return _eval_using_default_session(self, feed_dict, self.graph, session)\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3495, in _eval_using_default_session\n>     raise ValueError(\"Cannot use the given session to evaluate tensor: \"\n> ValueError: Cannot use the given session to evaluate tensor: the tensor's graph is different from the session's graph.\n\n---\n\nBelow is the code that is causing the issue:\n\n```\ndef tffunc(*argtypes):\n    '''Helper that transforms TF-graph generating function into a regular one.\n    See \"resize\" function below.\n    '''\n    placeholders = list(map(tf.placeholder, argtypes))\n    def wrap(f):\n        out = f(*placeholders)\n        def wrapper(*args, **kw):\n            #from IPython import embed\n            #embed()\n            return out.eval(dict(zip(placeholders, args)), session=kw.get('session'))\n        return wrapper\n    return wrap\n\ndef resize(img, size):\n    img = tf.expand_dims(img, 0)\n    return tf.image.resize_bilinear(img, size)[0,:,:,:]\nresize = tffunc(np.float32, np.int32)(resize)\n```\n\nThis is the line causing issue: https://github.com/umerebryx/Inference/blob/master/inference_eval.py#L60\nYou can also view full code in above mentioned repo. Any help would be much appreciated as I am stuck at this for days now. Thanks in advance. \n", "comments": ["Please use markdown to format code so it is easier to read (I edited your comment to do that just now).\n", "This is unlikely to be a bug in TensorFlow.  I would suggest printing out `session.graph`.  Presumably it will be different from `out.graph`.  This is probably fixable by passing `graph=something` when making the session, but further details should be asked about on StackOverflow, not here.\n"]}, {"number": 3705, "title": "Distributed seq2seq model stuck at the session.run()", "body": "I'm applying the distributed seq2seq model:\n   1 ps\n   2 workers\nbut stuck at the \n\n> sess.run(output_feed, input_feed) \n> in the seq2seq_model.py, even though my params are very small(1 layers of 10 units)\n\nmy problem like the question on the below in stackoverflow:\n(http://stackoverflow.com/questions/38319953/tensorflow-applying-syncreplicasoptimizer-to-seq2seq-model-with-buckets).\n\nTensorflow version: 0.9-GPU_Support\n\nThis is the log output:\n\n> start running session\n> I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 7293 get requests, put_count=3583 evicted_count=1000 eviction_rate=0.279096 and unsatisfied allocation rate=0.659537\n> I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110\n\nAnd it seems like it's still running , the events file is updated as well:\n\n> total 85560\n> drwxr-xr-x 2 dl dl     4096 Aug  9 16:04 .\n> drwxrwxr-x 9 dl dl     4096 Aug  9 15:59 ..\n> -rw-rw-r-- 1 dl dl      129 Aug  9 16:03 checkpoint\n> -rw-rw-r-- 1 dl dl 23232412 Aug  9 **16:09 events.out.tfevents.1470772996.dl129**\n> -rw-rw-r-- 1 dl dl 40464613 Aug  9 16:02 graph.pbtxt\n> -rw-rw-r-- 1 dl dl   645001 Aug  9 16:03 model.ckpt-0\n> -rw-rw-r-- 1 dl dl 23249486 Aug  9 16:04 model.ckpt-0.meta\n> dl@dl129:~/PJT/train_logs$ ll\n> total 85560\n> drwxr-xr-x 2 dl dl     4096 Aug  9 16:13 .\n> drwxrwxr-x 9 dl dl     4096 Aug  9 15:59 ..\n> -rw-rw-r-- 1 dl dl      129 Aug  9 16:13 checkpoint\n> -rw-rw-r-- 1 dl dl 23232512 Aug  9 **16:13 events.out.tfevents.1470772996.dl129**\n> -rw-rw-r-- 1 dl dl 40464613 Aug  9 16:02 graph.pbtxt\n> -rw-rw-r-- 1 dl dl   645001 Aug  9 16:13 model.ckpt-0\n> -rw-rw-r-- 1 dl dl 23249486 Aug  9 16:14 model.ckpt-0.meta\n> dl@dl129:~/PJT/train_logs$ \n", "comments": ["If the events file is being updated, it's unlikely that the code is hanging inside `session.run`.  Why do you think it is hanging there?\n", "@girving Thx for your comment.\nI find the Supervisor class, and it's **init** is as below:\n\n>  def **init**(self, graph=None, ready_op=USE_DEFAULT, is_chief=True,\n>                init_op=USE_DEFAULT, init_feed_dict=None,\n>                local_init_op=USE_DEFAULT, logdir=None,\n>                summary_op=USE_DEFAULT, saver=USE_DEFAULT,\n>                global_step=USE_DEFAULT, save_summaries_secs=120,\n>                save_model_secs=600, recovery_wait_secs=30, stop_grace_secs=120,\n>                checkpoint_basename=\"model.ckpt\", session_manager=None,\n>                summary_writer=USE_DEFAULT, init_fn=None):\n\nThe arguments `save_model_secs` and `save_summaries_secs` are the time interval to save checkpoint file and event file separately. \nSo I guess at least the sv is still running, but I'm not sure whether something wrong or waiting for data in the `session.run`\n\n", "@DjangoPeng I don't understand how your comment is related to my question.  What is your evidence that it's hanging inside `sess.run()`?  It's possible I'm missing something obvious, but as is we don't have enough information to help.\n", "@girving Ok\uff0clet me be clear.\nI add two `prints` between the `session.run`:\n\n```\n# Output feed: depends on whether we do a backward step or not.\nif not forward_only:\n  output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\n                 self.gradient_norms[bucket_id],  # Gradient norm.\n                 self.losses[bucket_id]]  # Loss for this batch.\nelse:\n  output_feed = [self.losses[bucket_id]]  # Loss for this batch.\n  for l in xrange(decoder_size):  # Output logits.\n    output_feed.append(self.outputs[bucket_id][l])\n\nprint(\"------------------------------------------------------------------------------\")\nprint(\"------------------------------------------------------------------------------\")\n\nprint(\"start running session\")\noutputs = session.run(output_feed, input_feed)\nprint(\"finish running session\")\nif not forward_only:\n  return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\nelse:\n  return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\n```\n\nAnd this is the standard ouput:\n\n> reading data line 100000\n>    reading data line 200000\n>    finish reading data\n>    ready to train\n>    start training loop:0\n>    start_time:1470835889.1598\n> \n> ---\n> \n> ---\n> \n>    start running session\n>    I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 7061 get requests, put_count=3604 evicted_count=1000 eviction_rate=0.277469 and unsatisfied allocation rate=0.645376\n>    I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110\n\nAs you can see, the **finish running session** is not printed at the std.out, cause it's hanging inside the `sess.run()`.\n", "Now I add the `init_tokens_op` and `chief_queue_runner`.  It cause a AttributeError!\n\n```\nTraceback (most recent call last):\n  File \"rnn/translate/translate.py\", line 378, in <module>\n    tf.app.run()\n  File \"/home/dl/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"rnn/translate/translate.py\", line 375, in main\n    train()\n  File \"rnn/translate/translate.py\", line 197, in train\n    sv.start_queue_runners(sess, [model.chief_queue_runner])\n  File \"/home/dl/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 717, in start_queue_runners\n    threads.extend(qr.create_threads(sess, coord=self._coord, daemon=True,\nAttributeError: 'function' object has no attribute 'create_threads'\n```\n\nseq2seq_model.py:\n\n```\n    #part of __init__()\n    ...\n    params = tf.trainable_variables()\n    if not forward_only:\n      self.gradient_norms = []\n      self.updates = []\n      opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n\n      opt = tf.train.SyncReplicasOptimizer(\n        opt,\n        replicas_to_aggregate=num_workers,\n        replica_id=task_id,\n        total_num_replicas=num_workers)\n        #variable_averages=self.exp_moving_averager,\n        #variable_to_average=self.variable_to_average)\n\n\n      for b in xrange(len(buckets)):\n        gradients = tf.gradients(self.losses[b], params)\n        clipped_gradients, norm = tf.clip_by_global_norm(gradients,\n                                                         max_gradient_norm)\n        self.gradient_norms.append(norm)\n        self.updates.append(opt.apply_gradients(\n              zip(clipped_gradients, params), global_step=self.global_step))\n\n\n    self.saver = tf.train.Saver(tf.all_variables())\n    print(\"finish creating model\")\n\n\n    self.init_tokens_op = opt.get_init_tokens_op\n    self.chief_queue_runner = opt.get_chief_queue_runner\n    ...\n\n```\n\ntranslate.py:\n\n```\n          ...\n          model = seq2seq_model.Seq2SeqModel(\n                FLAGS.en_vocab_size, FLAGS.fr_vocab_size, _buckets,\n                FLAGS.size, FLAGS.num_layers, FLAGS.max_gradient_norm, FLAGS.batch_size,\n                FLAGS.learning_rate, FLAGS.learning_rate_decay_factor,\n                forward_only=False, is_chief=is_chief, \n                num_workers=FLAGS.num_workers, task_id=FLAGS.task_index)\n\n          init_op = tf.initialize_all_variables()\n          summary_op = tf.merge_all_summaries()\n\n          sv = tf.train.Supervisor(is_chief=is_chief,\n                                  logdir=logdir,\n                                  init_op=init_op,\n                                  summary_op = None,\n                                  saver=model.saver,\n                                  save_model_secs=FLAGS.save_interval_secs,\n                                  global_step=model.global_step)          \n\n          sess_config = tf.ConfigProto(\n              allow_soft_placement=True,\n              log_device_placement=True)\n\n          with sv.managed_session(server.target, config=sess_config) as sess:\n              if FLAGS.sync_replicas and is_chief:\n                sv.start_queue_runners(sess, [model.chief_queue_runner])\n                sess.run(model.init_tokens_op)\n                ...\n```\n", "I believe `sv.chief_queue_runner` is supposed to be set to a `QueueRunner`, not a function that returns a `QueueRunner`.  Try adding empty parentheses so that the function gets called.\n", "@girving thx for you reply. Actually, I'm learning how to use the distributed by reading the `inception`code.\nI don't know if it's necessary to use the `sv.chief_queue_runner`, because I find comments in the `supervisor.py` [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/supervisor.py#L693). While, the [distributed inception](https://github.com/tensorflow/models/blob/master/inception/inception/inception_distributed_train.py#L269) and [mnist_replica](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/python/mnist_replica.py#L227) both use the `chief_queue_runner`\u00b7 and `init_tokens_op`. \n", "It looks like this is a question about how to use TensorFlow, not a Github issue, so I'm going to close this for now.  Questions about how to use TensorFlow are better asked on StackOverflow.  I'm happy to reopen if it turns out this is actually a bug.\n", "@DjangoPeng Did you find where your problem is? I have met similar problem and stuck for several days. Thanks very much."]}, {"number": 3704, "title": "scalar_summary within map_fn hangs when run. ", "body": "On tf 0.9, it seems that I cannot collect summaries from within map_fn. The following code would reproduce the problem: \n\n```\n\nimport tensorflow as tf\n\nsummaries = []\ndef summarize_and_plus_one(inp):\n    summaries.append(tf.scalar_summary('inp_%s' % inp, inp))\n    inp += 1\n    return inp\n\na = tf.constant([1,2,3,4])\nb = tf.map_fn(summarize_and_plus_one, a)\n\nwith tf.Session() as sess:\n    print(sess.run(b))\n    sess.run(tf.merge_all_summaries()) # The code would hang here. \n\n```\n\nIf I unpack first and apply the function without map_fn, the code finishes as expected: \n\n```\n\nimport tensorflow as tf\n\nsummaries = []\ndef summarize_and_plus_one(inp):\n    summaries.append(tf.scalar_summary('inp_%s' % inp, inp))\n    inp += 1\n    return inp\n\na = tf.constant([1,2,3,4])\n\nres = []\nfor x in tf.unpack(a):\n    res.append(summarize_and_plus_one(x))\nb = tf.pack(res)\n\nwith tf.Session() as sess:\n    print(sess.run(b))\n    sess.run(tf.merge_all_summaries())\n\n```\n\nIs this a known issue? It seems the problem is with map_fn (or while_loop). \n", "comments": ["http://stackoverflow.com/questions/37571017/tensorflow-stuck-into-endless-loop-using-tf-while-loop\n\nThis might be related. As map_fn is in fact using while_loop internally, `fn` must NOT have side-effects and `scalar_summary` is in this case a side-effect. Is that right? If the dimension along which I need to split is unknown at graph construction time, is there any work-around that would let me collect the summaries from within the callable?\n", "@liusiqi43 Actually `scalar_summary` is just an op that returns a string; it doesn't have any side effects.  The problem is that you can't use `tf.merge_all_summaries` to link together outputs from across the while loop.  If you want this to work, you'd have to manually merge the summaries during the while loop using `tf.merge_summaries`.\n\nHowever, hanging seems like unfortunate behavior.  @yuanbyu Is there anything we can do on our end to throw an error rather than hanging in this case?\n", "Closing due to inactivity. Feel free to open a new github issue if the problem still persists in recent versions."]}, {"number": 3703, "title": "tf.contrib.learn.TensorFlowRNNRegressor: \"inputs must be a list!\"", "body": "> import numpy as np\n> from tensorflow.contrib.learn import TensorFlowRNNRegressor\n> import tensorflow as tf\n> X = [[1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]\n> y = [1, 1, 1]\n> regr = TensorFlowRNNRegressor(rnn_size=5)\n> regr.fit(np.array(X), np.array(y))\n\n**throws error:\nTraceback (most recent call last):\n  File \"rnn_test.py\", line 9, in <module>\n    regr.fit(np.array(X), np.array(y))\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/learn/python/learn/estimators/base.py\", line 160, in fit\n    monitors=monitors)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 449, in _train_model\n    train_op, loss_op = self._get_train_ops(features, targets)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 673, in _get_train_ops\n    _, loss, train_op = self._call_model_fn(features, targets, ModeKeys.TRAIN)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 656, in _call_model_fn\n    features, targets, mode=mode)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/learn/python/learn/estimators/base.py\", line 369, in _model_fn\n    predictions, loss = model_fn(features, targets)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/learn/python/learn/estimators/rnn.py\", line 205, in _model_fn\n    self.initial_state)(X, y)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/learn/python/learn/models.py\", line 399, in rnn_estimator\n    initial_state=initial_state)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/rnn.py\", line 98, in rnn\n    raise TypeError(\"inputs must be a list\")\nTypeError: inputs must be a list**\n\nBut no problem with DNNRegressor\nThank you in advance for your time\n", "comments": ["The issue template asks for some information, including the version of TensorFlow, platform, etc.  Can you add this information?  TensorFlow at HEAD has a different error message, so it's possible something has been fixed (or changed) in the mean time.\n", "I just reproduced this error using 0.8.0\n\nSome system info:\n\nCPython 3.4.4\nIPython 4.1.2\n\ncompiler   : GCC 4.4.7 20120313 (Red Hat 4.4.7-1)\nsystem     : Linux (Ubuntu 16.04)\nrelease    : 4.4.0-31-generic\nmachine    : x86_64\nprocessor  : x86_64\nCPU cores  : 4\ninterpreter: 64bit\n", "@jayswinney 0.8.0 is two releases ago.  Could you try again with 0.10 or HEAD?\n", "upgrading to '0.10.0rc0' gives a similar error to the original\n\n```\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-8-7914af068076> in <module>()\n      2 y = [1, 1, 1]\n      3 regr = TensorFlowRNNRegressor(rnn_size=5)\n----> 4 regr.fit(np.array(X), np.array(y))\n\n/home/jay/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/base.py in fit(self, x, y, steps, monitors, logdir)\n    164                       feed_fn=self._data_feeder.get_feed_dict_fn(),\n    165                       steps=steps or self.steps,\n--> 166                       monitors=monitors)\n    167     return self\n    168 \n\n/home/jay/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py in _train_model(self, input_fn, steps, feed_fn, init_op, init_feed_fn, init_fn, device_fn, monitors, log_every_steps, fail_on_nan_loss, max_steps)\n    548       features, targets = input_fn()\n    549       self._check_inputs(features, targets)\n--> 550       train_op, loss_op = self._get_train_ops(features, targets)\n    551 \n    552       # Add default monitors.\n\n/home/jay/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py in _get_train_ops(self, features, targets)\n    818       Tuple of train `Operation` and loss `Tensor`.\n    819     \"\"\"\n--> 820     _, loss, train_op = self._call_model_fn(features, targets, ModeKeys.TRAIN)\n    821     return train_op, loss\n    822 \n\n/home/jay/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py in _call_model_fn(self, features, targets, mode)\n    801         return self._model_fn(features, targets, mode=mode, params=self.params)\n    802       else:\n--> 803         return self._model_fn(features, targets, mode=mode)\n    804     return self._model_fn(features, targets)\n    805 \n\n/home/jay/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/base.py in _model_fn(features, targets, mode)\n    376       if self.class_weight is not None:\n    377         constant_op.constant(self.class_weight, name='class_weight')\n--> 378       predictions, loss = model_fn(features, targets)\n    379       if isinstance(self.learning_rate, types.FunctionType):\n    380         learning_rate = self.learning_rate(contrib_framework.get_global_step())\n\n/home/jay/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/rnn.py in _model_fn(self, x, y)\n    236                                 models.linear_regression, self.sequence_length,\n    237                                 self.initial_state, self.attn_length,\n--> 238                                 self.attn_size, self.attn_vec_size)(x, y)\n    239 \n    240   @property\n\n/home/jay/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/models.py in rnn_estimator(x, y)\n    410                            dtype=dtypes.float32,\n    411                            sequence_length=sequence_length,\n--> 412                            initial_state=initial_state)\n    413     return target_predictor_fn(encoding, y)\n    414 \n\n/home/jay/.local/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py in rnn(cell, inputs, initial_state, dtype, sequence_length, scope)\n    128     raise TypeError(\"cell must be an instance of RNNCell\")\n    129   if not nest.is_sequence(inputs):\n--> 130     raise TypeError(\"inputs must be a sequence\")\n    131   if not inputs:\n    132     raise ValueError(\"inputs must not be empty\")\n\nTypeError: inputs must be a sequence\n```\n", "So the error says an argument must be a sequence, but you're passing a numpy array.  What happens if you pass a sequence (such as a list)?\n", "```\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-14-eeb41cd0423a> in <module>()\n      2 y = [1, 1, 1]\n      3 regr = TensorFlowRNNRegressor(rnn_size=5)\n----> 4 regr.fit(tf.unpack(np.array(X), axis=1), tf.unpack(np.array(y), axis=0))\n\n/home/jay/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/base.py in fit(self, x, y, steps, monitors, logdir)\n    160       self._model_dir = logdir\n    161     self._data_feeder = setup_train_data_feeder(\n--> 162         x, y, n_classes=self.n_classes, batch_size=self.batch_size)\n    163     self._train_model(input_fn=self._data_feeder.input_builder,\n    164                       feed_fn=self._data_feeder.get_feed_dict_fn(),\n\n/home/jay/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py in setup_train_data_feeder(x, y, n_classes, batch_size, shuffle, epochs)\n    114     return StreamingDataFeeder(x, y, n_classes, batch_size)\n    115   return data_feeder_cls(\n--> 116       x, y, n_classes, batch_size, shuffle=shuffle, epochs=epochs)\n    117 \n    118 \n\n/home/jay/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py in __init__(self, x, y, n_classes, batch_size, shuffle, random_state, epochs)\n    236       output_dtype: dtype of output.\n    237     \"\"\"\n--> 238     x_dtype = np.int64 if x.dtype == np.int64 else np.float32\n    239     y_dtype = (\n    240         np.int64 if n_classes is not None and n_classes > 1 else np.float32)\n\nAttributeError: 'list' object has no attribute 'dtype'\n```\n", "@martinwicke This is probably user error, but it at least seems like a bug that we don't autoconvert lists to numpy arrays here.\n", "The TensorFlow\\* Estimator classes are deprecated. We don't have a direct replacement for it yet (we should soon). @ilblackdragon or @terrytangyuan may have advice in the meantime.\n", "If anyone comes here looking for an RNN to run the above example, this is about the shortest script I could come up with:\n\n``` python\nimport numpy as np\nimport tensorflow as tf\n\nEPOCHS = 10000\nPRINT_STEP = 1000\n\ndata = np.array([[1, 2, 3, 4, 5], [ 2, 3, 4, 5, 6], [3, 4, 5, 6, 7]])\ntarget = np.array([[6], [7], [8]])\n\nx_ = tf.placeholder(tf.float32, [None, data.shape[1]])\ny_ = tf.placeholder(tf.float32, [None, 1])\n\ncell = tf.nn.rnn_cell.BasicRNNCell(num_units=data.shape[1])\n\noutputs, states = tf.nn.rnn(cell, [x_], dtype=tf.float32)\noutputs = outputs[-1]\n\nW = tf.Variable(tf.random_normal([data.shape[1], 1]))     \nb = tf.Variable(tf.random_normal([1]))\n\ny = tf.matmul(outputs, W) + b\n\ncost = tf.reduce_mean(tf.square(y - y_))\ntrain_op = tf.train.RMSPropOptimizer(0.005, 0.2).minimize(cost)\n\nwith tf.Session() as sess:\n    tf.initialize_all_variables().run()\n    for i in range(EPOCHS):\n        sess.run(train_op, feed_dict={x_:data, y_:target})\n        if i % PRINT_STEP == 0:\n            c = sess.run(cost, feed_dict={x_:data, y_:target})\n            print('training cost:', c)\n\n    response = sess.run(y, feed_dict={x_:data})\n    print(response)\n\n```\n\nrun on version 0.10.0rc0\n", "I also across this problem. Could anyone tell me how to solve this ?\n", "I've come across the problem as well, it looks like np.array's are not collections.Sequence's I tried implementing a hacky wrapper around list that also has a dtype argument but that didn't fix the problem either.  It looks to me (through code tracing) that for some reason a Tensor is being passed into is_sequence, and the tensor is not a sequence.  Therefore regardless of the input provided the method will fail (I'm trying to use tf.contrib.learn.TensorFlowRNNClassifier() -> .fit())\n", "Well I find that if you use tf.split() to split your array into a list of arrays, then actually it will work. So I guess a list of arrays is what the machine means \"sequence\".\n", "``` python\nimport numpy as np\nfrom tensorflow.contrib.learn import TensorFlowRNNRegressor\nimport tensorflow as tf\n\ndef rnn_input_fn(x):\n    return tf.split(1, 5, x)\n\nX = [[1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]\ny = [1, 1, 1]\n\nX = np.array(list(X), dtype=np.int32)\ny = np.array(list(y), dtype=np.int32)\n\nregr = TensorFlowRNNRegressor(n_classes=2, rnn_size=10, input_op_fn=rnn_input_fn)\nregr.fit(X, y)\n```\n\nThis is how I make this example work.\n", "you can get example code here, this is how I made it work (and figured out what the problem was) https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/rnn_test.py\n\nThe main thing to notice is that you MUST have an input_op_fn even though it is a python optional (the default operation currently does not work).\n\nhttps://github.com/tensorflow/tensorflow/pull/4608\n", "Note that I am in the process of deleting TensorFlow*Estimator, so I\nwouldn't spend too much effort on it. It's been deprecated for a while --\nplease use Estimator directly.\nOn Sat, Oct 15, 2016 at 09:32 Christophe Faucon notifications@github.com\nwrote:\n\n> you can get example code here, this is how I made it work (and figured out\n> what the problem was)\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/rnn_test.py\n> \n> The main thing to notice is that you MUST have an input_op_fn even though\n> it is a python optional (the default operation currently does not work).\n> \n> #4608 https://github.com/tensorflow/tensorflow/pull/4608\n> \n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3703#issuecomment-253994791,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAjO_UJezhIHrTKl9l4dyK5coBdwJFiPks5q0QA2gaJpZM4JfsKJ\n> .\n", "when you say *Estimator do you mean *Regressor?\n", "All subclasses of TensorFlowEstimator\n\nOn Saturday, October 15, 2016, Christophe Faucon notifications@github.com\nwrote:\n\n> when you say *Estimator do you mean *Regressor?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3703#issuecomment-254013031,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AEEnSg2Y4khpHlUHwMvaOLHVnqTgulQoks5q0Um1gaJpZM4JfsKJ\n> .\n"]}, {"number": 3702, "title": "Add link to tensorflow-rust in resources", "body": "", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 3701, "title": "tensorflow won't release system memroy", "body": "Hi all,\n\nI started to use tensorflow one month ago. I briefly use it to train neural machine translation models.\nI find that after several times of training,  there were a huge amount of memory not released. I have to reboot my machine after some numbers of training to release those memory.\n\nI use the sample code( tensorflow/models/rnn/translate python translate.py) to train the model.\n### Environment info\n\nOperating System: Ubuntu 14.04.4 LTS\n\ntensorflow version: r.0.10\n\nCUDA version: libcudnn.so.4.0\nIf installed from binary pip package, provide:\n### What have you tried?\n\n1.\nAdd a maximum step number to terminate training normally.\n1. replace \n   \n   with tf.Session() as sess:\n\nwith the following code:\n\n  config = tf.ConfigProto()\n  config.gpu_options.allow_growth=True\n  sess = tf.Session(config=config)\n\n  with tf.Graph().as_default(),sess:\n1. close the session after training\n   sess.close()\n   del sess\n2. the version of my nvidia driver is 352.79 now.\n   I will update it to the latest version next week. \n\nI have no idea what to do now.\n\nThanks!\nBest,\nxiaorongfan\n", "comments": ["1. Add a maximum step number to terminate training normally.\n2. replace \n   \n   with tf.Session() as sess:\n\nwith the following code:\n\n  config = tf.ConfigProto()\n  config.gpu_options.allow_growth=True\n  sess = tf.Session(config=config)\n\n  with tf.Graph().as_default(),sess:\n3. close the session after training\n    sess.close()\n  del sess\n4. the version of my nvidia driver is 352.79 now.\n  I will update it to the latest version next week. \n", "I had a same issue like you. After upgrading the GPU driver from 352.79 to 367.35 (the newest one), the problem disappeared.\n", "@xiaorongfan Can you check if that driver update fixes it for you too?\n", "kmu-leeky\nThank you for you information. Because the administrator is on vacation now, I can only upgrade GPU driver next week. I will report the result later.\n", "@xiaorongfan my pleasure\n@girving Out of curiosity, do you know what is the root cause for this issue? How does the old version of GPU driver and Tensorflow system memory management conflicts? If you can redirect me to an article to elaborate it, it would be appreciated.\n", "@kmu-leeky Unfortunately I do not know the reason, or of such an article.\n", "My guess is that we aggressively use host-pinned GPU memory to make GPU->CPU transfers faster, and there was probably a driver bug that leaks the memory allocated in this fashion.  But impossible to know for sure since cuda driver is closed source :)\n"]}, {"number": 3700, "title": "Cuda8", "body": "", "comments": []}, {"number": 3699, "title": "inline compiler switch for __memcpy_inline?", "body": "Trying to build tensorflow on Ubuntu 16.04 with cuda 7.5 and gcc 4.9.3.\n\n```\nINFO: From Compiling tensorflow/core/kernels/batchtospace_op_gpu.cu.cc:\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\n/usr/include/string.h: In function 'void* __mempcpy_inline(void*, const void*, size_t)':\n/usr/include/string.h:652:42: error: 'memcpy' was not declared in this scope\n   return (char *) memcpy (__dest, __src, __n) + __n;\n                                          ^\n```\n\nI think __memcpy_inline needs an inline compiler switch turned on but this is the first time I've used bazel, and just what where?\n", "comments": ["@martinwicke Have you seen this before?\n", "Never. I think switches would be in the crosstool, but it's weird we should need them (or rather, you should need some that we apparently don't). @damienmg any idea?\n", "Sorry - \"inline\" ended up getting used in two contexts - in the code function name itself but I used it in the sense of: is adding __n to memcpy(a,b,n) valid in itself or does it need a special compiler switch?  Now that I think about it memcpy returns a pointer so this is just standard C pointer arithmetic in which case you don't need a complier switch.\n\nIf so then the question becomes: why is memcpy not declared in this scope?  Still this is not in the tensorflow codebase but rather /usr/include/string.h and so the problem is perhaps with my installation.  That said, the error pops up well into the build (at about [?] 1200 out of the 2500 files) which would see to indicate that for the most part my installation is working fine.  If there's a problem with one's installation one usually (note: usually) sees it right away.\n", "As I iteratively re-execute the build for tutorials_example_trainer I find that the target at which it fails can change:\n\n```\npatfla@patfla-N550JV:~/code/tensorflow$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\n.\nINFO: Found 1 target...\nINFO: From Compiling tensorflow/core/kernels/batchtospace_op_gpu.cu.cc:\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\n/usr/include/string.h: In function 'void* __mempcpy_inline(void*, const void*, size_t)':\n/usr/include/string.h:652:42: error: 'memcpy' was not declared in this scope\n   return (char *) memcpy (__dest, __src, __n) + __n;\n                                          ^\nERROR: /home/patfla/code/tensorflow/tensorflow/core/kernels/BUILD:1498:1: output 'tensorflow/core/kernels/_objs/batchtospace_op_gpu/tensorflow/core/kernels/batchtospace_op_gpu.cu.o' was not created.\nERROR: /home/patfla/code/tensorflow/tensorflow/core/kernels/BUILD:1498:1: not all outputs were created.\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 43.125s, Critical Path: 36.73s\npatfla@patfla-N550JV:~/code/tensorflow$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\nINFO: Found 1 target...\nINFO: From Compiling tensorflow/core/kernels/cwise_op_gpu_mod.cu.cc:\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\n/usr/include/string.h: In function 'void* __mempcpy_inline(void*, const void*, size_t)':\n/usr/include/string.h:652:42: error: 'memcpy' was not declared in this scope\n   return (char *) memcpy (__dest, __src, __n) + __n;\n                                          ^\nERROR: /home/patfla/code/tensorflow/tensorflow/core/kernels/BUILD:1089:1: output 'tensorflow/core/kernels/_objs/cwise_op_gpu/tensorflow/core/kernels/cwise_op_gpu_mod.cu.o' was not created.\nERROR: /home/patfla/code/tensorflow/tensorflow/core/kernels/BUILD:1089:1: not all outputs were created.\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 36.292s, Critical Path: 35.57s\n```\n\nThat makes me begin to question how bazel keeps state.\n\n```\npatfla@patfla-N550JV:~/code/tensorflow$ bazel dump --action_cache | wc -l\n1164439\n```\n\nThat's not a good indication (less state is usually better) and makes me wonder just what I'm investigating here.  Certainly not, neural net architectures.\n\nMaybe a different version of bazel would help?\n\n```\npatfla@patfla-N550JV:~/code/tensorflow$ bazel version\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp as int: 0\npatfla@patfla-N550JV:~/code/tensorflow$ grep -i bazel /var/log/dpkg.log\n2016-08-07 19:47:13 install bazel:amd64 <none> 0.3.1\n2016-08-07 19:47:13 status half-installed bazel:amd64 0.3.1\n2016-08-07 19:47:13 status unpacked bazel:amd64 0.3.1\n2016-08-07 19:47:13 status unpacked bazel:amd64 0.3.1\n2016-08-07 19:47:13 configure bazel:amd64 0.3.1 <none>\n2016-08-07 19:47:13 status unpacked bazel:amd64 0.3.1\n2016-08-07 19:47:13 status half-configured bazel:amd64 0.3.1\n2016-08-07 19:47:13 status installed bazel:amd64 0.3.1\npatfla@patfla-N550JV:~/code/tensorflow$ \n```\n\nI seem to be at bazel 0.3.1.\n", "On 16.04, you should use CUDA 8.0 instead.\n", "I tried that but backed out because something somewhere (don't remember exactly what) told me in effect: you can't get there from here.\n\nOh yes, it was probably because I didn't find an 8.0-compatible cudNN ...\n\n```\nhttps://developer.nvidia.com/rdp/cudnn-download\n\n\nDownload cuDNN v5.1 RC (June 19, 2016), for CUDA 8.0 RC\n\ncuDNN 5.1 RC for CUDA 8RC will be available soon - please check back again.\n\n```\n\nYes, that's what sent me back to 7.5.\n\nAh wait a min:\n\n`Download cuDNN v5 (May 27, 2016), for CUDA 8.0 RC\n`\n\nOh well, it doesn't hurt to go through these drills multiple times.\n\nThanks @flx42 \n", "Hmm - installed all of 8.0.  The build gets much further but then this:\n\n```\npatfla@patfla-N550JV:~/code/tensorflow$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer \nINFO: Found 1 target...\nERROR: /home/patfla/code/tensorflow/tensorflow/cc/BUILD:199:1: Linking of rule '//tensorflow/cc:tutorials_example_trainer' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/local_linux-opt/bin/tensorflow/cc/tutorials_example_trainer ... (remaining 805 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n```\n\ncrosstool_wrapper_driver_is_not_gcc failed\n", "That's not an actual error, there's should be more info about the true\nerror from the child script.\n\nThere was similar configuration problem in the SO question below, solved by\ndoing `bazel clean`\n\nhttp://stackoverflow.com/questions/38794497/tensorflow-bazel-0-3-0-build-cuda-8-0-gtx-1070-fails/38795005#comment64963046_38795005\n\nOn Tue, Aug 9, 2016 at 5:47 PM, patfla notifications@github.com wrote:\n\n> Hmm - installed all of 8.0. The build gets much further but then this:\n> \n> patfla@patfla-N550JV:~/code/tensorflow$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\n> INFO: Found 1 target...\n> ERROR: /home/patfla/code/tensorflow/tensorflow/cc/BUILD:199:1: Linking of rule '//tensorflow/cc:tutorials_example_trainer' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/local_linux-opt/bin/tensorflow/cc/tutorials_example_trainer ... (remaining 805 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n> \n> crosstool_wrapper_driver_is_not_gcc failed\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3699#issuecomment-238735855,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHNuJAucUutnv8tFiVzUMal8vZf_lks5qeR-MgaJpZM4JfT_W\n> .\n", "bazel clean worked and I now have tensorflow-0.10.0rc0 installed and running.\n\nHow do I tell when bazel clean finishes?  I used bazel clean before; couldn't tell when it finished; and ended up wiping the tree and doing a git clone again. \n", "You don't need to wait, bazel clean; bazel build should work\n\nOn Wed, Aug 10, 2016 at 1:38 PM, patfla notifications@github.com wrote:\n\n> bazel clean worked and I now have tensorflow-0.10.0rc0 installed and\n> running.\n> \n> How do I tell when bazel clean finishes? I used bazel clean before;\n> couldn't tell when it finished; and ended up wiping the tree and doing a\n> git clone again.\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3699#issuecomment-238996258,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHFkXDeS76kkBwDfjodv1eIjmyA_Eks5qejbFgaJpZM4JfT_W\n> .\n", "Thanks @yaroslavvb for the pointer.  @damienmg Could you comment on whether there is a fixable bazel issue here (or in how we use bazel), or should we close?\n", "Sorry, just back from holidays. I don't see any current issues in Bazel, we did had some change in Bazel structure that caused some error on the cache and could have corrupted his cache.\n\nThe recommended clean solution is `bazel clean --expunge` to wipe out everything.\n", "Thanks @damienmg.  Closing for now.\n"]}, {"number": 3698, "title": "Cuda8", "body": "", "comments": []}, {"number": 3697, "title": "Cuda8", "body": "", "comments": []}, {"number": 3696, "title": "conv3d_transpose not freeing memory", "body": "This issue might be the same as #3128 (which is still awaiting Googler), but it is occurring in a different way. The above issue was run on a CPU. I have since upgrade to a GTX 1070 with 8GB RAM. \n\nInstead of a Segmentation Fault, the program now runs until it runs out of memory -- in my case it runs for roughly 2000 iterations. Running on a CPU was spitting our a `free()` error, which leads me to believe this is the same underlying issue. \n\nThe error log looks like this...\n\n```\nI tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (134217728):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (268435456):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:656] Bin for 14.36MiB was 8.00MiB, Chunk State: \nI tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10005a00000 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10005a00100 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10005a00200 of size 256\n\n\n... a ton of other chunks\n\nI tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x101a1686e00 of size 100362240\nI tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x101a763d600 of size 15054336\nI tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x101a8498c00 of size 199075840\nI tensorflow/core/common_runtime/bfc_allocator.cc:683] Free at 0x10005c7cd00 of size 64000\nI tensorflow/core/common_runtime/bfc_allocator.cc:683] Free at 0x10005d66600 of size 1638400\nI tensorflow/core/common_runtime/bfc_allocator.cc:683] Free at 0x100063fcc00 of size 196608\nI tensorflow/core/common_runtime/bfc_allocator.cc:683] Free at 0x10193467b00 of size 6272768\nI tensorflow/core/common_runtime/bfc_allocator.cc:689]      Summary of in-use Chunks by size: \nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 30 Chunks of size 256 totalling 7.5KiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 4 Chunks of size 512 totalling 2.0KiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 4 Chunks of size 3072 totalling 12.0KiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 4 Chunks of size 16128 totalling 63.0KiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 4 Chunks of size 65536 totalling 256.0KiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 4 Chunks of size 262144 totalling 1.00MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 5 Chunks of size 614400 totalling 2.93MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 4 Chunks of size 1638400 totalling 6.25MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 1 Chunks of size 3136512 totalling 2.99MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 1061 Chunks of size 6272768 totalling 6.20GiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 3 Chunks of size 15054336 totalling 43.07MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 3 Chunks of size 100362240 totalling 287.14MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 1 Chunks of size 199075840 totalling 189.85MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] Sum Total of in-use chunks: 6.72GiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:698] Stats: \nLimit:                  7223063348\nInUse:                  7214891776\nMaxInUse:               7214891776\nNumAllocs:                  312188\nMaxAllocSize:            199075840\n\nW tensorflow/core/common_runtime/bfc_allocator.cc:270] ***************************************************************************************************x\nW tensorflow/core/common_runtime/bfc_allocator.cc:271] Ran out of memory trying to allocate 14.36MiB.  See logs for memory state.\nW tensorflow/core/framework/op_kernel.cc:940] Resource exhausted: OOM when allocating tensor with shape[1,32,3,198,198]\nTraceback (most recent call last):\n  File \"CNN-seg-3D.py\", line 208, in <module>\n    keep_prob: dropout})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 710, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 908, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 958, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 978, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shape[1,32,3,198,198]\n     [[Node: opt/gradients/conv1/MaxPool3D_grad/MaxPool3DGrad = MaxPool3DGrad[T=DT_FLOAT, ksize=[1, 2, 2, 2, 1], padding=\"SAME\", strides=[1, 2, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](conv1/Relu, conv1/MaxPool3D, opt/gradients/conv2/Conv3D_grad/tuple/control_dependency)]]\nCaused by op u'opt/gradients/conv1/MaxPool3D_grad/MaxPool3DGrad', defined at:\n  File \"CNN-seg-3D.py\", line 181, in <module>\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 196, in minimize\n    grad_loss=grad_loss)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 253, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py\", line 476, in gradients\n    in_grads = _AsList(grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_grad.py\", line 130, in _MaxPool3DGrad\n    padding=op.get_attr(\"padding\"))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1182, in max_pool3d_grad\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2317, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1239, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op u'conv1/MaxPool3D', defined at:\n  File \"CNN-seg-3D.py\", line 168, in <module>\n    pred = conv_net(x, weights, biases, keep_prob)\n  File \"CNN-seg-3D.py\", line 97, in conv_net\n    conv1 = maxpool3d(conv1, k=2)\n  File \"CNN-seg-3D.py\", line 78, in maxpool3d\n    padding='SAME')\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1150, in max_pool3d\n    strides=strides, padding=padding, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2317, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1239, in __init__\n    self._traceback = _extract_stack()\n```\n", "comments": []}, {"number": 3695, "title": "Added code to raise ValueError if parent directory of  path Saver.save\u2026", "body": "Current version of Saver.save(session, path) fails with a rather mysterious error code when parent directory of path doesn't exist. It throws a `NotFoundError`, listing a bunch of nodes variables, e.g.:\n\n```\n(...)\ntensorflow.python.framework.errors.NotFoundError: /tmp/tensorflow/autoencoder/cifar/cifar.ckpt.tempstate2833829603959566611\n   [[Node: save/save = SaveSlices[T=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/save/tensor_names, save/save/shapes_and_slices, Variable, Variable/Adam, Variable/Adam_1, beta1_power, beta2_power, biases, biases/Adam, biases/Adam_1, biases_1, biases_1/Adam, biases_1/Adam_1)]]\n```\n\nThis makes user believe problem is with some ops not being found by `Saver` object and doesn't point to path problem at all.\n\nI added code that checks that parent folder of output path exists and if not, fails with a `ValueError` and appropriate message.\n", "comments": ["Can one of the admins verify this patch?\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "@PuchatekwSzortach please satisfy CLABot and we'll move forward with review.\n", "@danmane I did sign CLA on both accounts I used to write the code, but for one of them after the pull request was submitted. I assume this didn't trigger CLABot reevaluation, as the action was taken outside of Github. Is there a way to force CLABot to run its check again?\n", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "@danmane @googlebot I authored all commits for this pull request, though some code comes from a different account I use. If needed I can post a comment testifying to that with the other account. If there is a another \"proper\" way of satisfying the CLA requirements apart for already having sign it with all participating accounts, please let me know how.\n", "If you rewrite the email address to the one you signed the CLA with (git commit --amend --author=\"Your Name <your email>\"), then push, the CLA bot will be happy\n", "Try squashing your commits\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "In addition to the line comment - can you add a test to saver_test.py that verifies that the expected exception is raised when saver is given a non-existent path?\n", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Jenkins, test this please.\n", "@tensorflow-jenkins test this please\n", "Jenkins, test this please.\n"]}, {"number": 3694, "title": "Fail to build from source ", "body": "Hi all,\n\nI just try to build tensorflow from source according to the steps described step by step, but I get the following error, anyone can help on it?\nThanks!\n\nalvin@hal-dev:~/tensorflow-r0.9$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\nFound stale PID file (pid=29629). Server probably died abruptly, continuing...\n.\nunexpected pipe read status: Connection refused\nServer presumed dead. Now printing '/home/alvin/.cache/bazel/_bazel_alvin/098c27509f757cf22af83c9a7d1d8ec7/server/jvm.out':\njava.lang.AssertionError: java.security.NoSuchAlgorithmException: MD5 MessageDigest not available\n    at com.google.common.hash.MessageDigestHashFunction.getMessageDigest(MessageDigestHashFunction.java:79)\n    at com.google.common.hash.MessageDigestHashFunction.<init>(MessageDigestHashFunction.java:40)\n    at com.google.common.hash.Hashing$Md5Holder.<clinit>(Hashing.java:191)\n    at com.google.common.hash.Hashing.md5(Hashing.java:187)\n    at com.google.devtools.build.lib.analysis.BlazeDirectories.checkMD5(BlazeDirectories.java:95)\n    at com.google.devtools.build.lib.analysis.BlazeDirectories.<init>(BlazeDirectories.java:74)\n    at com.google.devtools.build.lib.runtime.BlazeRuntime.newRuntime(BlazeRuntime.java:926)\n    at com.google.devtools.build.lib.runtime.BlazeRuntime.createBlazeRPCServer(BlazeRuntime.java:781)\n    at com.google.devtools.build.lib.runtime.BlazeRuntime.serverMain(BlazeRuntime.java:726)\n    at com.google.devtools.build.lib.runtime.BlazeRuntime.main(BlazeRuntime.java:519)\n    at com.google.devtools.build.lib.bazel.BazelMain.main(BazelMain.java:56)\nCaused by: java.security.NoSuchAlgorithmException: MD5 MessageDigest not available\n    at sun.security.jca.GetInstance.getInstance(GetInstance.java:159)\n    at java.security.Security.getImpl(Security.java:695)\n    at java.security.MessageDigest.getInstance(MessageDigest.java:167)\n    at com.google.common.hash.MessageDigestHashFunction.getMessageDigest(MessageDigestHashFunction.java:77)\n    ... 10 more\n", "comments": ["This looks like a bazel issue.  @damienmg Do you know what's going wrong?\n", "while bazel is running, will it connect to google server?\n", "@AlvinChen13 Bazel uses a client/server architecture for distributed builds, but both the client and the server are on your local machine in this case.\n", "@girving Is that to say I need run a bazel server in my local firstly? But after I install bazel, I haven't got server application.\n", "@AlvinChen13 No, running bazel starts both client and server.  This seems like a bazel issue, which is why I'm checking with @damienmg.\n", "Anyone can help on it? Very appreciated!\n", "This is a Bazel issue, tracking on https://github.com/bazelbuild/bazel/issues/1638.\n", "Solve the issue by re-install the JDK instead of deb.\n"]}, {"number": 3693, "title": "MNIST TensorFlow Website Example problem", "body": "Hi! When i run the example of MNIST Deep model (got on the tensorflow website) in a jupyter notebook i get this error: The kernel appears to have died. It will restart automatically.\nThis happens in the line: \nprint(\"test accuracy %g\"%accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n\nCOMPLETE CODE TENSORFLOW WEBSITE:\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n\nfrom **future** import absolute_import\nfrom **future** import division\nfrom **future** import print_function\n\nimport gzip\nimport os\nimport tempfile\n\nimport numpy\nfrom six.moves import urllib\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\nfrom tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n\nsess = tf.InteractiveSession()\n\nx = tf.placeholder(tf.float32, shape=[None, 784])\ny_ = tf.placeholder(tf.float32, shape=[None, 10])\nW = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))\nsess.run(tf.initialize_all_variables())\n\ny = tf.nn.softmax(tf.matmul(x,W) + b)\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ \\* tf.log(y), reduction_indices=[1]))\ntrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\nfor i in range(1000):\n  batch = mnist.train.next_batch(100)\n  train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n\ncorrect_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nprint(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n\ndef weight_variable(shape):\n  initial = tf.truncated_normal(shape, stddev=0.1)\n  return tf.Variable(initial)\n\ndef bias_variable(shape):\n  initial = tf.constant(0.1, shape=shape)\n  return tf.Variable(initial)\ndef conv2d(x, W):\n  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\ndef max_pool_2x2(x):\n  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                        strides=[1, 2, 2, 1], padding='SAME')\n\nW_conv1 = weight_variable([5, 5, 1, 32])\nb_conv1 = bias_variable([32])\n\nx_image = tf.reshape(x, [-1,28,28,1])\n\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\nh_pool1 = max_pool_2x2(h_conv1)\n\nW_conv2 = weight_variable([5, 5, 32, 64])\nb_conv2 = bias_variable([64])\n\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)\n\nW_fc1 = weight_variable([7 \\* 7 \\* 64, 1024])\nb_fc1 = bias_variable([1024])\n\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7_7_64])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\nkeep_prob = tf.placeholder(tf.float32)\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\nW_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\n\ny_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ \\* tf.log(y_conv), reduction_indices=[1]))\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\ncorrect_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\nsess.run(tf.initialize_all_variables())\n\nfor i in range(200):\n  batch = mnist.train.next_batch(50)\n  if i%100 == 0:\n    train_accuracy = accuracy.eval(feed_dict={\n        x:batch[0], y_: batch[1], keep_prob: 1.0})\n    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n\nprint(\"test accuracy %g\"%accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n\nPlease, help! Thanks!!\n", "comments": ["I don't think we have enough information.  Is that the full error message?\n", "Yes, jupyter notebook shows just this message on the screen. I don't know if it has a different kind of log error output.\n\nI did a little change in the code, specific in that last line. The new line is:\n\nteste = mnist.test.next_batch(1500)\nprint(\"test accuracy %g\"%accuracy.eval(feed_dict={x:teste[0], y_: teste[1], keep_prob: 1.0}))\n\nAnd the code ran fine. \nI am suspicious that is a memory problem. \nWhat do you think?\n", "I have no familiarity with Jupyter notebooks.  @colah Any guesses?\n", "Hi Guys!!\nThe problem is solved. Really, it was a memory problem. \nI installed ubuntu in my notebook (8 GB RAM) and the code ran fine (total memory used during the execution).\npreviously i was running in a virtual machine.\nThanks!!\n", "Glad you figured it out!\n", "This was not a memory issue - the issue arises when you install other packages , i've not worked out the details yet but I think either PIL (pillow) or pycat can cause some sort of conflict if installed in the same environment as tensorflow specifically causing this line:\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nto cause the kernel to die. I've seen reports that cuda also causes this but it isn't a package i have installed.\r\nMy reasons for saying this is that I had a notebook working fine until i tried to add functionality - this led me to install pycat and pillow.  Now that line causes the kernel to hang until it dies even if it is the first line in the notebook - unfortunately the installs are difficult to reverse and i'm still looking for a solution. \r\n"]}, {"number": 3692, "title": "tf.tile() doesn't propagate shapes when `multiples` argument depends on a placeholder", "body": "From [this Stack Overflow question](http://stackoverflow.com/q/38806136/3574081):\n\n> I have a variable a of dimension (1, 5) which I want to 'tile' as many times as the size of my mini-batch. For example, if the mini-batch size is 32 then I want to construct a tensor `c` of dimension (32, 5) where each row has values the same as the original (1, 5) variable `a`.\n> \n> But I only know the mini-batch size at run time: it's the size of dimension 0 of a placeholder `b`: `tf.shape(b)[0]`\n> \n> Here's my code to construct c:\n> \n> ``` python\n> a  = tf.Variable(np.random.uniform(size=(1,5)))\n> b = tf.placeholder(shape=[None, 12], dtype=tf.float32)\n> batch_size = tf.shape(b)[0]\n> c = tf.tile(a, tf.pack([batch_size, 1]))\n> ```\n> \n> This runs fine. However `c.get_shape()` returns `(?, ?)`. I don't understand why this doesn't return `(?, 5)` instead.\n> \n> This is causing an issue later in my code when I construct a matrix variable `W` with number of columns `c.get_shape()[1]` which I expect to return 5 rather than ?.\n> \n> Any help would be appreciated. Thanks.\n\nThe solution is to use `tensor_util.constant_value_as_shape()` in the shape function for `tf.tile()`. I have a fix pending.\n", "comments": ["Is there a solution to this? Same issue here.", "@sirgogo The original problem was solved six months ago. If you were facing the same problem, upgrading to a new version should fix it; otherwise please post a new issue describing a complete reproducible example and we'll take a look.", "Thanks @mrry . I realized that I should use tf.shape(X)[#] rather than X.shape[#].", "@sirgogo you saved my butt. thanks!\r\n\r\n@mrry what's the distinction between tf.shape(X) and X.get_shape() ? I'm sure there's a good reason why they are different although I intuitively would understand them to be the same . . ?", "@evanthebouncy See this Stack Overflow answer: https://stackoverflow.com/a/37096395/3574081"]}, {"number": 3691, "title": "Benchmarking example for iOS profiling", "body": "", "comments": ["@martinwicke, pete nominated you as reviewer :)\n"]}, {"number": 3690, "title": "MNIST Prediction", "body": "I used the MNIST-EXPERT.py to create a model by adding this to the end:\n\n`save_path = saver.save(sess, \"model.ckpt\")`\n`print (\"Model saved in file: \", save_path)`\n\nNow that I have saved a model, how do I give it a picture of handwriting so it can figure it out?\n\nThanks\n", "comments": ["This should be asked on StackOverflow, since it is not an issue in TensorFlow requiring improvement.\n"]}, {"number": 3689, "title": "AttributeError: 'module' object has no attribute 'session'", "body": "not an issue. please delete\n", "comments": []}, {"number": 3688, "title": "Error when trying to run tensorboard --logdir=some_path", "body": "Hi,\n\nI am using python 2.7 on Mac 10.11.4.\n\nI get errors when I am trying to run tensorboard --logdir=path\n\nwhere path is some path I have specified, i.e. 'visualizations'\n\nIt correctly creates the directory visualizations in my current working directory, but\n\nif I run: tensorboard --logdir=path, I get:\nSyntaxError: can't assign to operator\n\nIf I try: tensorboard --logdir=visualizations, I get:\nSyntaxError: can't assign to operator\n\nIf I try: tensorboard --logdir=/visualizations/, I get:\nSyntaxError: invalid syntax\n\nPlease help me! And I would really appreciate an explanation for this, as I am relatively new to all this.\n\nThanks a lot in advance!\n", "comments": ["What are the full error messages?\n", "What was the solution to this? I'm facing the same issue. \n", "You need to run it in the terminal! \n", "inside jupyter....\n\n%%sh\ntensorboard --logdir=log_simple_graph # shell command\n", "Run this :\r\ntensorboard --logdir==training:your_log_dir --host=127.0.0.1 \r\nand then open in chrome ", "I faced the same issue,\r\nThen below command managed to open tensorboard, though crashed Chrome. This is an external command to run w/o running python shell.\r\n\r\npython -m tensorboard.main --logdir visualizations --host localhost --port 8088", "Hi there, \r\nall worked fine for  me until I accidently double-clicked the \"Reset Spyder Settings.py\"...\r\nI got the error messages described above, when trying to serve the Tensorboard directly through Spyder console.\r\nSolution for me: run the tensorboard call directly from Anaconda Powershell."]}, {"number": 3687, "title": "slice shape inference bug", "body": "At current HEAD 32bd3d024f33e920a67a1081bc0ae0048350fdee with the following code,\n\n```\nA = tf.placeholder(tf.float32, [None, 10])\nB = A[:5, :]\nB.get_shape()\n```\n\nThe first dimension of B is unknown, but should be known. I have to use the `tf.slice` function to get the shape back:\n\n```\nA = tf.placeholder(tf.float32, [None, 10])\nB = tf.slice(A, [0,0], [5, -1])\nB.get_shape()\n```\n", "comments": ["@aselle Sounds like `strided_slice`.\n", "I assume you are expecting to get [5,None] from the inference.\nIn numpy style, if the slice range exceeds the original shape, it is clamped to the largest size possible.\nThis means that for tensors shaped 0, 1, 2, 3, 4 you will get tensors that are 0xNone, 1xNone, 2xNone, 3xNone, 4xNone. Only if the the placeholder is 5x10 or larger will you see a fixed value of 5.\n", "@aselle you are right. Thanks.\n"]}, {"number": 3686, "title": "C++ API Usage :No such file or directory", "body": "Hello\n\nI am trying to run the image recognition library but have the error below. \n\nI run :\n\nwget https://storage.googleapis.com/download.tensorflow.org/models/inception_dec_2015.zip -O tensorflow/examples/label_image/data/inception_dec_2015.zip\n\nError \n\ntensorflow/examples/label_image/data/inception_dec_2015.zip: No such file or directory\n", "comments": ["This sounds like an issue with how you are using wget, not an issue with tensorflow.  Maybe the directory doesn't exist?\n"]}, {"number": 3685, "title": "Unable to generate signed APK for project based on Android demo", "body": "I wrote a simple app based on the Android demo that I got running successfully on my own device for debugging, using Bazel, and I'm planning on releasing it, but generating a signed APK fails.\n### Environment info\n\nOperating System: Ubuntu 14.04 LTS 64-bit\n\nInstalled version of CUDA and cuDNN: none\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`): fc9162975e52978d3af38549b570cc3cc5f0ab66\n2. The output of `bazel version`\n\n```\nBuild label: 0.3.0\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Jun 10 11:38:23 2016 (1465558703)\nBuild timestamp: 1465558703\nBuild timestamp as int: 1465558703\n```\n### Steps to reproduce\n1. Follow instructions in #3444 up to setting up the Android Studio project\n2. Run `./studio.sh` to open Android Studio\n3. Open the Android demo as existing Android Studio project\n4. (no prompt to configure appears)\n5. Open \"File/Project Structure\"\n6. If src and res aren't set as source and resource directories respectively, do so\n7. Select \"Generate Signed APK\" under Build, fails\n### What have you tried?\n1. Found the apks built by Bazel and tried to upload them. They were rejected by the Play Store.\n2. Originally, the \"Generate Signed APK\" option wasn't available, but was fixed by deleting the project, reinstalling Android Studio, and then copying the project back in from a backup.\n3. Cleaning. No change.\n4. Solutions for similar errors suggest modifying the build tools version in the Gradle file, but this project doesn't use Gradle...\n### Logs or other output\n\n```\nError:Android Dex: [android] Exception in thread \"main\" java.lang.UnsupportedClassVersionError: com/android/dx/command/dexer/Main : Unsupported major.minor version 52.0\nError:Android Dex: [android] at java.lang.ClassLoader.defineClass1(Native Method)\nError:Android Dex: [android] at java.lang.ClassLoader.defineClass(ClassLoader.java:803)\nError:Android Dex: [android] at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\nError:Android Dex: [android] at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)\nError:Android Dex: [android] at java.net.URLClassLoader.access$100(URLClassLoader.java:71)\nError:Android Dex: [android] at java.net.URLClassLoader$1.run(URLClassLoader.java:361)\nError:Android Dex: [android] at java.security.AccessController.doPrivileged(Native Method)\nError:Android Dex: [android] at java.net.URLClassLoader$1.run(URLClassLoader.java:355)\nError:Android Dex: [android] at java.lang.ClassLoader.loadClass(ClassLoader.java:358)\nError:Android Dex: [android] at java.net.URLClassLoader.findClass(URLClassLoader.java:354)\nError:Android Dex: [android] at java.lang.ClassLoader.loadClass(ClassLoader.java:425)\nError:Android Dex: [android] at org.jetbrains.android.compiler.tools.AndroidDxRunner.loadDex(AndroidDxRunner.java:79)\nError:Android Dex: [android] at org.jetbrains.android.compiler.tools.AndroidDxRunner.runDex(AndroidDxRunner.java:136)\nError:Android Dex: [android] at org.jetbrains.android.compiler.tools.AndroidDxRunner.main(AndroidDxRunner.java:336)\n```\n", "comments": ["@petewarden Do you know if this is an issue on our end?\n", "Update: I've found an alternate way to generate a signed APK using Bazel ([see here](https://github.com/bazelbuild/bazel/issues/1613)). I haven't figured out the issue in this thread though.\n", "@cardshuffle Have you managed to solve the issue? If not, http://stackoverflow.com/questions/35990995/com-android-dx-command-main-unsupported-major-minor-version-52-0 looks related. Maybe try updating your sdk settings as suggested there?\n", "I tried out the suggested solution, but I wasn't able to find the Build Tools Version setting mentioned, and since the demo doesn't use Gradle, there isn't a Gradle settings file to change the version in.\n", "@cardshuffle I have a build.gradle file incoming that may help with this -- it will automate calling out to Bazel to build the native libs and then copy them into the appropriate directory. You can try editing the buildToolsVersion there once I get it checked in (if it's still giving you problems then).\n", "@cardshuffle build.gradle commited in in bd73f2e3b88ddd6a89d5ee08f83e65a29fdba8df. Hopefully this makes thigs a bit easier.\n", "Closing; please reopen or create a new issue if the build.gradle approach gives you any problems.\n"]}, {"number": 3684, "title": "ExponentialMovingAverage.variables_to_restore() treats all vars like they have averages", "body": "Use:\n\n```\nema = tf.train.ExponentialMovingAverage(0.9)\ntest = tf.Variable(0.1)\nprint(ema.variables_to_restore([test])\n```\n\noutputs `{u'Variable/ExponentialMovingAverage': <tensorflow.python.ops.variables.Variable object at 0x1139d1bd0>}`\n\nI expect `variables_to_restore` not to append `/ExponentialMovingAverage` to the names of variables that do not have averages tracked.\n\nI checked that `ema._averages` and `tf.get_collection(tf.GraphKeys.MOVING_AVERAGE_VARIABLES)` were indeed both empty.  Calling `ema.average_name(test)` produces the modified name and this appears to be done [specifically](https://github.com/tensorflow/tensorflow/blob/73ced9d797056c7e67a06ed2098dd809d85ec44a/tensorflow/python/training/moving_averages.py#L344) (perhaps for another reason).  So `variables_to_restore` incorrectly modifies the name because [it uses](https://github.com/tensorflow/tensorflow/blob/73ced9d797056c7e67a06ed2098dd809d85ec44a/tensorflow/python/training/moving_averages.py#L387) `average_name`.\n", "comments": ["@sherrym Is this a bug?  The behavior of `variables_to_restore` seems somewhat elaborate.\n", "Closing due to inactivity. Feel free to open a new github issue if the problem still persists in recent versions.", "Not sure if this is the best place to put this piece of feedback, but I'm finding the behavior of variables_to_restore() to a bit ... bizarre and not intuitive.  Currently variables_to_restore() includes all trainable variables in the graph.  Being an instance method I would have expected variables_to_restore() to return something related to the specific instance of ExponentialMovingAverages.  But if you look at the code it has no dependency whatsoever on the instance.  Why isn't this a class method?\r\n\r\nAt a minimum I would expect the documentation to specify that even though you're calling this method on a specific instance you're going to get all variables in the graph.  I had to look through the source code to get through my misunderstanding. ", "Tensorflow 1.2 has another way of saving moving average and using them for inference: [tf.contrib.opt.MovingAverageOptimizer](https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/contrib/opt/MovingAverageOptimizer).  \r\n\r\nFor me that has been a very good work around to this issue."]}, {"number": 3683, "title": "query - neuraltranslation.py", "body": "I was trying to execute neural_translation.py but it is generating the below error\n![aa3385a2-5c29-11e6-8cd4-90113fca1e45](https://cloud.githubusercontent.com/assets/18217467/17460473/ca2b6b7a-5c85-11e6-83fc-01191ba64098.png)\n", "comments": ["@ilblackdragon Is this an error on our end?\n", "Yes....it is working in tensorflow 7\n", "Closing due to inactivity. Feel free to open a new github issue if the problem still persists in recent versions. If you chose to do so, please include a small repro snippet. Thanks!"]}, {"number": 3682, "title": "Consider adding expm1 and log1p", "body": "`expm1` is useful for inverting `tf.nn.softplus`:\n\n```\ndef inverse_softplus(x):\n    return tf.select(tf.gt(x, 80.0),\n                     x,\n                     tf.log(tf.expm1(x)))\n```\n\n`log1p` is useful for implementing `logaddexp`:\n\n```\ndef logaddexp(x, y):\n    temp = x - y\n    return tf.select(tf.gt(temp, 0.0),\n                     x + tf.log1p(tf.exp(-temp)),\n                     y + tf.log1p(tf.exp(temp)))\n```\n", "comments": ["Yep, these would both be nice to have.  They would hopefully be straightforward to add alongside the existing unary ops, though I haven't checked whether they exist in Eigen yet.\n", "I've had a quick look at Eigen to see whether these two are already supported as functors.\n`log1p` is there, but `expm1` is missing.\nThat means a good start for `expm1` would be to add it to Eigen.\nI think some extra work in Eigen is also needed to get `log1p` to work with GPU ops (e.g. adding it to `Eigen/src/Core/arch/CUDA/MathFunctions.h`)\n\n**Edit:** I've opened a PR to Eigen with the necessary changes for `log1p` here: https://bitbucket.org/eigen/eigen/pull-requests/217/add-log1p-support-for-cuda-and-half-floats/diff. I'll update this issue once the changes end up in TensorFlow.\n", "As a potential alternative, we have implemented a python-only framework for adding custom operators to tensorflow, the Operator Vectorization Library: \nhttps://github.com/opveclib/opveclib. Multi-threaded C++ and CUDA code is automatically generated from the python operators, which can be used in a Tensorflow application with only a binary install of tensorflow. We have implemented the log1p and expm1 operators as an example. See documentation at http://opveclib.readthedocs.io/en/latest/expm1.html\n", "@kbrems Interesting!\n", "This should be fixed by 6731e21360ac2fd2fa4999a12e87ae72acfcb2d2."]}, {"number": 3681, "title": "Segmentation Fault When Importing Tensorflow", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System: CentOS 6.8/RedHat 6.8\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\nCUDA 7.5 + cuDNN v4\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n### Steps to reproduce\n1. Install tensorflow by using anaconda (conda command)\n2. Install glibc \"locally\" and add the path /glibc-2.14/lib to LD_LIBRARY_PATH\n3. Open python and use the command \"import tensorflow as tf\", and then you will get the message \"Segmentation Fault\". No other messages.\n### What have you tried?\n1. Use gdb to see what causes the segmentation fault.\n### Logs or other output that would be helpful\n\ngdb messages:\n[New Thread 0x7f653bf9f700 (LWP 71825)]\n[New Thread 0x7f653b59e700 (LWP 71826)]\n[New Thread 0x7f6538b9d700 (LWP 71827)]\n[New Thread 0x7f653619c700 (LWP 71828)]\n[New Thread 0x7f653379b700 (LWP 71829)]\n[New Thread 0x7f6530d9a700 (LWP 71830)]\n[New Thread 0x7f652e399700 (LWP 71831)]\n[New Thread 0x7f652b998700 (LWP 71832)]\n[New Thread 0x7f6528f97700 (LWP 71833)]\n[New Thread 0x7f6526596700 (LWP 71834)]\n[New Thread 0x7f6523b95700 (LWP 71835)]\n[New Thread 0x7f6521194700 (LWP 71836)]\n[New Thread 0x7f651e793700 (LWP 71837)]\n[New Thread 0x7f651bd92700 (LWP 71838)]\n[New Thread 0x7f6519391700 (LWP 71839)]\n[New Thread 0x7f6516990700 (LWP 71840)]\n[New Thread 0x7f6513f8f700 (LWP 71841)]\n[New Thread 0x7f651158e700 (LWP 71842)]\n[New Thread 0x7f650eb8d700 (LWP 71843)]\n[New Thread 0x7f650c18c700 (LWP 71844)]\n[New Thread 0x7f650978b700 (LWP 71845)]\n[New Thread 0x7f6506d8a700 (LWP 71846)]\n[New Thread 0x7f6504389700 (LWP 71847)]\n[New Thread 0x7f6501988700 (LWP 71848)]\n[New Thread 0x7f64fef87700 (LWP 71849)]\n[New Thread 0x7f64fc586700 (LWP 71850)]\n[New Thread 0x7f64f9b85700 (LWP 71851)]\n[New Thread 0x7f64f5184700 (LWP 71852)]\n[New Thread 0x7f64f4783700 (LWP 71853)]\n[New Thread 0x7f64f1d82700 (LWP 71854)]\n[New Thread 0x7f64ef381700 (LWP 71855)]\n[New Thread 0x7f64ec980700 (LWP 71856)]\n[New Thread 0x7f64e9f7f700 (LWP 71857)]\n[New Thread 0x7f64e757e700 (LWP 71858)]\n[New Thread 0x7f64e4b7d700 (LWP 71859)]\n[New Thread 0x7f64e217c700 (LWP 71860)]\n[New Thread 0x7f64df77b700 (LWP 71861)]\n[New Thread 0x7f64dcd7a700 (LWP 71862)]\n[New Thread 0x7f64da379700 (LWP 71863)]\n[New Thread 0x7f64d7978700 (LWP 71864)]\n[New Thread 0x7f64d4f77700 (LWP 71865)]\n[New Thread 0x7f64d2576700 (LWP 71866)]\n[New Thread 0x7f64cfb75700 (LWP 71867)]\n[New Thread 0x7f64cd174700 (LWP 71868)]\n[New Thread 0x7f64ca773700 (LWP 71869)]\n[New Thread 0x7f64c7d72700 (LWP 71870)]\n[New Thread 0x7f64c5371700 (LWP 71871)]\n[New Thread 0x7f64c2970700 (LWP 71872)]\n[New Thread 0x7f64bff6f700 (LWP 71873)]\n[New Thread 0x7f64bd56e700 (LWP 71874)]\n[New Thread 0x7f64bab6d700 (LWP 71875)]\n[New Thread 0x7f64b816c700 (LWP 71876)]\n[New Thread 0x7f64b576b700 (LWP 71877)]\n[New Thread 0x7f64b2d6a700 (LWP 71878)]\n[New Thread 0x7f64b0369700 (LWP 71879)]\n\nProgram received signal SIGSEGV, Segmentation fault.\n__pthread_init_static_tls (map=0x0) at allocatestack.c:1196\n1196        init_one_static_tls (list_entry (runp, struct pthread, list), map);\nfrom /home/abc/Libraries/glibc-2.14/install/lib/libpthread.so.0\n\nBecause they are clusters, it is hard (or nearly impossible in the near future) to upgrade the OSs. I also have no root privilege.\nAt first, I got an error telling that I don't have glibc_2.14.so because RedHat/CentOS 6.8 only comes with glibc 2.12. Therefore, I locally compiled the glibc and added it to LD_LIBRARY_PATH. After then, the glibc_2.14 error disappeared and was replaced by another error \"Segmentation Fault\" (no other messages). Something like this:\nimport tensorflow as tf\nSegmentation Fault\n(tensorflow) bash-4.1$\n\nI tried to use gdb to see what causes this error, and it turns out it is \"libpthread.so.0\".\n\nAny ideas?\n\nThanks.\n", "comments": ["Unfortunately, this isn't enough information to help.  However, it sounds like you have dueling versions of glibc, and you're almost certainly going to need to compile your own version of TensorFlow to make this work.  Since we do not officially support CentOS, there's no guarantee that it will be solvable.\n", "I encountered this problem too. This problem can be solved in a different way.\r\n[https://anaconda.org/jjhelmus/tensorflow](url)", "The above anaconda link is dead, and I'm getting the exact messages that denru01 is getting, ie \"Segmentation Fault\" with no other messages. I got that after \"locally\" installing glibc 2.14 and pointing LD_LIBRARY_PATH to it. Anything else I can try to get this to work on centos 6.8?", "Try this one [https://anaconda.org/jjhelmus/tensorflow](https://anaconda.org/jjhelmus/tensorflow)", "No luck :-/, same result, even after uninstalling tensorflow...thanks for the link though!", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 3680, "title": "error: can't copy 'tensorflow/models/embedding/gen_word2vec.py': doesn't exist", "body": "I was installing TF from source like I always did but I'm having the following issue this time when running `bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg` I got\n\n``` bash\n~/python/tensorflow$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\nSat Aug 6 16:07:12 EDT 2016 : === Using tmpdir: /tmp/tmp.TNJr7rCgpe\n/tmp/tmp.TNJr7rCgpe ~/python/tensorflow\nSat Aug 6 16:07:12 EDT 2016 : === Building wheel\nerror: can't copy 'tensorflow/models/embedding/gen_word2vec.py': doesn't exist or not a regular file\n```\n\nplus, I can't find that file anywhere... did anything change or am I missing something?\n", "comments": ["@martinwicke Do we have a pip czar?  We don't have one listed in the spreadsheet.\n", "That looks like a build dependency is missing: pip_package should depend on that gen_word2vec.py file, but doesn't. PRs welcome.\n", "Yuefeng, assigning to you, build system fun.\n", "@EderSantana Hi Eder, it worked in my Mac with OSX 10.11.6. What is your environment? Did you successfully run the \"bazel run\"? Which version of bazel are you using?\n", "@YuefengZhou \nI'm using ubuntu 14.04 and the GPU is a K80. I ran `./configure` and \n`bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\nwith success\n\nthe problem was on that very last step\n", "Actually, I just tried to run everything and noticed that I got the following message when running `bazel build` with `--verbose_failures`:\n\n``` bash\nedersantana@miner:~/python/tensorflow$ bazel build --verbose_failures -c opt --config=cuda\n\n...\n\nERROR: /home/edersantana/python/tensorflow/tensorflow/core/kernels/BUILD:281:1: undeclared inclusion(s) in rule '//tensorflow/core/kernels:split_lib_gpu':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/split_lib_gpu.cu.cc':\n...\n```\n\nso the problem might be something else, but I still can't figure. I'll give it a try on other versions and see what happens.\n", "@EderSantana Your problem looks very similar to another issue, you probably can try the workaround there: https://github.com/tensorflow/tensorflow/issues/1157#issuecomment-192552759\n", "I have reproduced the problem in my Ubuntu 14.04 desktop and fixed it with this workaround. So I am closing this issue.\n", "You mean this, right:\n\n> Adding\n> cxx_builtin_include_directory: \"/usr/local/cuda/targets/x86_64-linux/include\"\n> to file tensorflow/third_party/gpus/crosstool/CROSSTOOL\n> seem to be a workaround.\n", "Hi,\r\nif someone can help me. I have a similar problem on Windows 10 (a friend has the same issue for Windows aswell) with tensorflow 0.12.0-rc0:\r\nImportError: No module named 'tensorflow.models'.\r\nAnd the file gen_word2vec.py doesn't exist in tensorflow/models/embedding so I cannot even import it manually (from cloned GitHub).\r\nI did the installation via pip for CPU, but even the folder /tensorflow/models is missing in that case.\r\n", "We are moving model/ out of this repo altogether and moving it to github.com/tensorflow/models. This is probably accidental, but it's a sign of the things to come."]}, {"number": 3679, "title": "Fixes #3677: Added missing save_summary_steps in BaseEstimator", "body": "Reported in #3677\n", "comments": ["@martinwicke @ilblackdragon it looks like this could be reviewed in like 30 seconds, and then we'd have one fewer open pull requests! :)\n"]}]