[{"number": 38107, "title": " https://www.tensorflow.org/install/errors", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@Ruma-Dhanawate \r\nplease update the template, we cannot find the tensorflow version, steps followed before you faced the error and error logs for us to analyze the issue.\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "@Ruma-Dhanawate\r\nplease update as per above comment", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 38106, "title": "implicit conversion from 'int' to 'float' changes value from INT_MAX to INT_MAX+1", "body": "Clang warns\r\n\r\n```\r\ntensorflow/lite/micro/micro_utils.cc:108:19: warning: implicit conversion from 'int' to 'float' changes value from 2147483647 to 2147483648 [-Wimplicit-int-float-conversion]\r\n  if (quantized > INT_MAX) {\r\n                ~ ^~~~~~~\r\n```\r\nabout this code\r\nhttps://github.com/tensorflow/tensorflow/blob/bda5ba03da606b0d41fa3282de0108c3e98cff8e/tensorflow/lite/micro/micro_utils.cc#L106-L115\r\n\r\nSo, if `round(value/scale)` ever happens to be INT_MAX+1, it is not saturated as intended, instead has undefined wrap-around behavior (overflow is undefined for signed int,).", "comments": ["This specified file has been moved to tensorflow/tflite-micro, and also method you are referring has been moved/removed. Thank you", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38106\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38106\">No</a>\n"]}, {"number": 38105, "title": "Information about Keras Tuner is missing in tensorflow.org website", "body": "## URL(s) with the issue: \r\nhttps://youtu.be/aNrqaOAt5P4?list=PLQY2H8rRoyvzuJw20FG82Lgm2SZjTdIXU&t=660\r\n\r\n## Description of issue (what needs changing): \r\nIn the [TF Dev Summit 2020](https://youtu.be/aNrqaOAt5P4?list=PLQY2H8rRoyvzuJw20FG82Lgm2SZjTdIXU&t=660\r\n), Paige Bailey has talked about **Keras Tuner** and has shown its implementation. I liked the functionality but I couldn't information/documentation about it in [tensorflow.org site](https://www.tensorflow.org/).\r\n\r\n### Clear description: \r\nThis being a New Functionality, the documentation about that functionality in the Website would help the Community.\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct? : N/A\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?: N/A\r\n\r\n### Returns defined\r\n\r\nAre return values defined? : N/A\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? : N/A\r\n\r\n### Usage example\r\n\r\nIs there a usage example? N/A", "comments": ["@rakeshmothukuru1,\r\n[This TensorFlow Blog post](https://blog.tensorflow.org/2020/01/hyperparameter-tuning-with-keras-tuner.html) about Keras Tuner points to the official [GitHub repo](https://github.com/keras-team/keras-tuner), which has the link to the [Keras Tuner documentation](https://keras-team.github.io/keras-tuner/). \r\nCould you please check these link and let us know if it helps? Thanks!", "@amahendrakar ,\r\n In my opinion, it will be better to add the documentation in Tensorflow.org site because many people refer that site rather than many different sites. If this Functionality has to be used along with Tensorflow, then its documentation should be present in the Official Tensorflow Website.", "@rakeshmothukuru1 Keras tuner is a separate package that is not integrated in tensorflow but we can use it by installing and importing it. So, that might be the reason why its not included in tensorflow.org", "@anirudh161 is working on this.", "We have an example on the site now: https://www.tensorflow.org/tutorials/keras/keras_tuner"]}, {"number": 42797, "title": " padded_batch() missing 1 required positional argument: 'padded_shapes' in line train_data = train_data.padded_batch(BATCH_SIZE)", "body": " padded_batch() missing 1 required positional argument: 'padded_shapes' in line train_data = train_data.padded_batch(BATCH_SIZE)", "comments": ["train_data = train_data.padded_batch(BATCH_SIZE,padded_shapes=([None],[])) \r\nchange code like this worked well , i close this issue."]}, {"number": 38104, "title": "post_training_quant does not change weights from fp32 to int8", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\nNO\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): \r\nubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): \r\nconda  tensorflow version: tested on tf1.15 and 2.2.0-dev20200325.\r\n- Python version: - Bazel\r\nversion (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory:\r\n\r\n**Describe the current behavior**\r\ni follow link:<https://www.tensorflow.org/lite/performance/post_training_quant> ,use code in the doc,\r\nwhen i use [netron](https://github.com/lutzroeder/netron) open mnist_model_quant.tflite,found that weights/bias of layer conv2d are still float32.\r\n![image](https://user-images.githubusercontent.com/6041743/78113170-e99fb980-7431-11ea-8e3b-8cccfec1ec57.png)\r\n\r\n\r\n**Describe the expected behavior**\r\nafter post-training quantize, weights of conv2d should be int8 rather than float32\r\n\r\n\r\n\r\n", "comments": ["You may want to try post training integer quantization instead.\r\nhttps://www.tensorflow.org/lite/performance/post_training_integer_quant#convert_to_a_tensorflow_lite_model\r\n<img width=\"1514\" alt=\"Screen Shot 2020-04-01 at 5 51 17 PM\" src=\"https://user-images.githubusercontent.com/42785357/78199748-68502300-7441-11ea-90da-f9d3a80e6ee8.png\">\r\n", "@sdu2011 I tried the same model as in the example and find int8 for the weights of conv2d as shown below. Did you change any part of the code and did you use quantized model? \r\n\r\n[Screenshot](https://screenshot.googleplex.com/ZptgJAm6tg3).", "> You may want to try post training integer quantization instead.\r\n> https://www.tensorflow.org/lite/performance/post_training_integer_quant#convert_to_a_tensorflow_lite_model\r\n> <img alt=\"Screen Shot 2020-04-01 at 5 51 17 PM\" width=\"1514\" src=\"https://user-images.githubusercontent.com/42785357/78199748-68502300-7441-11ea-90da-f9d3a80e6ee8.png\">\r\n\r\ni know if i do inter quantize, weights would be converted from fp32 to int8, i had already tested it\uff0e\u3000my question is why post-training weight quantization does not change conv2d weights from fp32 to int8\uff0e", "> @sdu2011 I tried the same model as in the example and find int8 for the weights of conv2d as shown below. Did you change any part of the code and did you use quantized model?\r\n> \r\n> [Screenshot](https://screenshot.googleplex.com/ZptgJAm6tg3).\r\n\r\n```\r\nimport logging\r\nlogging.getLogger(\"tensorflow\").setLevel(logging.DEBUG)\r\n\r\ntry:\r\n  # %tensorflow_version only exists in Colab.\r\n  import tensorflow.compat.v2 as tf\r\nexcept Exception:\r\n  pass\r\ntf.enable_v2_behavior()\r\n\r\nfrom tensorflow import keras\r\nimport numpy as np\r\nimport pathlib\r\n\r\n# Load MNIST dataset\r\nmnist = keras.datasets.mnist\r\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\r\n\r\n# Normalize the input image so that each pixel value is between 0 to 1.\r\ntrain_images = train_images / 255.0\r\ntest_images = test_images / 255.0\r\n\r\n# Define the model architecture\r\nmodel = keras.Sequential([\r\n  keras.layers.InputLayer(input_shape=(28, 28)),\r\n  keras.layers.Reshape(target_shape=(28, 28, 1)),\r\n  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation=tf.nn.relu),\r\n  keras.layers.MaxPooling2D(pool_size=(2, 2)),\r\n  keras.layers.Flatten(),\r\n  keras.layers.Dense(10, activation=tf.nn.softmax)\r\n])\r\n\r\n# Train the digit classification model\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\nmodel.fit(\r\n  train_images,\r\n  train_labels,\r\n  epochs=1,\r\n  validation_data=(test_images, test_labels)\r\n)\r\n\r\n#\u3000\u53ea\u505a\u4e86\u683c\u5f0f\u8f6c\u6362\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\ntflite_models_dir = pathlib.Path(\"./mnist_tflite_models/\")\r\ntflite_models_dir.mkdir(exist_ok=True, parents=True)\r\ntflite_model_file = tflite_models_dir/\"mnist_model.tflite\"\r\ntflite_model_file.write_bytes(tflite_model)\r\n\r\n# \u628aweights\u8f6c\u6362\u6210int8  \u5b9e\u9645\u6d4b\u8bd5\u51fa\u6765\u53ea\u6709\u5168\u8fde\u63a5\u5c42\u7684weights\u8f6c\u6210\u4e86int8\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\ntflite_quant_model = converter.convert()\r\ntflite_model_quant_file = tflite_models_dir/\"mnist_model_quant.tflite\"\r\ntflite_model_quant_file.write_bytes(tflite_quant_model)\r\n```\r\nno,i did not change code.", "For accuracy reasons, we don't quantize tensors smaller than 1024.", "I am closing please reopen, if you have issues or questions (or create new issue)\r\n\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38104\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38104\">No</a>\n", "I'm also having the same issue, with tensors larger than 1024\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/40129"]}, {"number": 38103, "title": "custom training logic in subclassing model not saved", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g.,Linux Ubuntu 16.04): Google Colab (Windows/Linux as well)\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): TensorFlow 2.2.0RC2\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nHi all,\r\nI'm using the 2.2 new feature: custom training logic with Model.fit by overriding Model.train_step. It's working well so far. I have the following issue (maybe it's not a feature, you'll tell me):\r\nWhen I save my model (Model.save) with the custom training logic and then I want to load it, the custom training loop is not saved and when I apply a Model.fit for my loaded model, it comes back to the default one.\r\n\r\n**Describe the expected behavior**\r\nI'd expect to save as well the model.train_step (and also test_step and predict_step).\r\n\r\n**Standalone code to reproduce the issue** \r\n[Gist](https://colab.research.google.com/gist/quetil/5e92fb9af7cc959f8fe62f1090ef31e7/custom-train_step-in-subclassing-model.ipynb)\r\n\r\n**Others**\r\nCan we expect in the future that the whole class will be saved? (Even variables declared in the `__init__` for instance).\r\n\r\nThank you in advance :-)\r\n", "comments": ["@quetil \r\nI have executed the code shared by you and get different error please refer to [this gist](https://colab.sandbox.google.com/gist/Saduf2019/936ac77698c63786e9c563e07da7797e/untitled121.ipynb).\r\n\r\nwith respect to the error faced there is a similar resolved issue please refer to this [comment](https://github.com/tensorflow/tensorflow/issues/37141#issuecomment-592737155)  and let us know if it helps.\r\n#27949 [link1](https://stackoverflow.com/questions/41689451/valueerror-no-gradients-provided-for-any-variable) [link2](https://en.dev4app.com/archives/51966406-no-gradients-provided-for-any-variable-check-your-graph-for-ops-that-do-not-support-gradients.html)", "Hi @Saduf2019, thanks for the quick reply.\r\n\r\nPlease refer to [this new gist](https://colab.research.google.com/gist/quetil/5e92fb9af7cc959f8fe62f1090ef31e7/custom-train_step-in-subclassing-model.ipynb) (updated in the first post). \r\nI used the last tf-nigthly. The train_step is not saved and once loaded, it's using the default train_step.\r\n\r\nThanks", "i have replicated the issue reported as per gist shared above, please find the [gist here](https://colab.sandbox.google.com/drive/1K28D5mcVCiFrTwXXDopYU38ZJrEt6JNx#scrollTo=fliohxc90O-p)", "please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/37b047e54f8caa63fb8064e8f11b9f33/untitled122.ipynb)", "Any news about such feature?\r\nPlanned in TF 2.4 ? :-)", "Currently we do not plan to add this, if you want to retain the train_step function, decorate it with `@tf.function(input_signature=...)`", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38103\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38103\">No</a>\n", "@k-w-w Decorating with `@tf.function(input_signature=...)` presents its own problems:\r\n\r\n```import numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.python.eager import backprop\r\n\r\nclass TestModel(tf.keras.models.Model):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.x = layers.Input((10,))\r\n        self.output_ = layers.Dense(1)\r\n\r\n    def call(self, inputs):\r\n        return self.output_(inputs)\r\n\r\n    @tf.function(\r\n        input_signature=[(tf.TensorSpec(shape=(None, 10), dtype=tf.float32),\r\n                          tf.TensorSpec(shape=(None, 1), dtype=tf.int64))])\r\n    def train_step(self, data):\r\n        tf.print('Flag')\r\n        x, y = data\r\n        with backprop.GradientTape() as tape:\r\n            y_pred = self(x, training=True)\r\n            loss = self.compiled_loss(\r\n                y, y_pred, regularization_losses=self.losses)\r\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\r\n        self.compiled_metrics.update_state(y, y_pred)\r\n        return {m.name: m.result() for m in self.metrics}\r\n\r\n    def get_config(self):\r\n        config = super().get_config()\r\n        config.update({'train_step': self.train_step})\r\n        return config\r\n\r\noriginal_model = TestModel()\r\noriginal_model.compile(loss='mse', optimizer='sgd')\r\n\r\nfor i in range(3):\r\n    original_model.train_on_batch(np.random.rand(6, 10), np.arange(6))\r\n    print('Completed train loop (original model)')\r\n\r\nsave_path = 'tmp'\r\noriginal_model.save(save_path)\r\n\r\nloaded_model = tf.keras.models.load_model(save_path)\r\n\r\nfor i in range(3):\r\n    loaded_model.train_on_batch(np.random.rand(6, 10), np.arange(6))\r\n    print('Completed train loop (loaded model)')```\r\n\r\nGives:\r\n\r\n```import numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.python.eager import backprop\r\n\r\n\r\nclass TestModel(tf.keras.models.Model):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.x = layers.Input((10,))\r\n        self.output_ = layers.Dense(1)\r\n\r\n    def call(self, inputs):\r\n        return self.output_(inputs)\r\n\r\n    @tf.function(\r\n        input_signature=[(tf.TensorSpec(shape=(None, 10), dtype=tf.float32),\r\n                          tf.TensorSpec(shape=(None, 1), dtype=tf.int64))])\r\n    def train_step(self, data):\r\n        tf.print('Flag')\r\n        x, y = data\r\n        with backprop.GradientTape() as tape:\r\n            y_pred = self(x, training=True)\r\n            loss = self.compiled_loss(\r\n                y, y_pred, regularization_losses=self.losses)\r\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\r\n        self.compiled_metrics.update_state(y, y_pred)\r\n        return {m.name: m.result() for m in self.metrics}\r\n\r\n    def get_config(self):\r\n        config = super().get_config()\r\n        config.update({'train_step': self.train_step})\r\n        return config\r\n\r\n\r\noriginal_model = TestModel()\r\noriginal_model.compile(loss='mse', optimizer='sgd')\r\n\r\nfor i in range(3):\r\n    original_model.train_on_batch(np.random.rand(6, 10), np.arange(6))\r\n    print('Completed train loop (original model)')\r\n\r\nsave_path = 'tmp'\r\noriginal_model.save(save_path)\r\n\r\nloaded_model = tf.keras.models.load_model(save_path)\r\n\r\nfor i in range(3):\r\n    loaded_model.train_on_batch(np.random.rand(6, 10), np.arange(6))\r\n    print('Completed train loop (loaded model)')\r\n```\r\n\r\nIf you modify the signature from\r\n\r\n``` \r\n@tf.function(\r\n        input_signature=[(tf.TensorSpec(shape=(None, 10), dtype=tf.float32),\r\n                          tf.TensorSpec(shape=(None, 1), dtype=tf.int64))])\r\n```\r\n\r\nto\r\n\r\n``` \r\n@tf.function(\r\n        input_signature=[tf.TensorSpec(shape=(None, 10), dtype=tf.float32),\r\n                         tf.TensorSpec(shape=(None, 1), dtype=tf.int64)])\r\n```\r\n\r\n(get rid of a pair of parenthesis), we get:\r\n\r\n```\r\nTypeError: tf__train_step() takes 2 positional arguments but 3 were given\r\n```\r\n\r\nIf we only pass the shape of the inputs (x) values:\r\n\r\n```\r\n@tf.function(\r\n        input_signature=[\r\n            tf.TensorSpec(shape=(None, 10), dtype=tf.float32)])\r\n```\r\n\r\nwe get\r\n\r\n```\r\nTypeError: tf__train_step() missing 1 required positional argument: 'data'\r\n```\r\n\r\nAnd if we do not decorate, ```train_step``` is not serialised and so loaded models revert to the ```train_step``` of the base class.\r\n\r\nThere is not a lot of documentation on how to modify train_step such that it can be serialised. Am I getting the wrong end of the stick here?", "A model can be successfully run and saved with a `@tf.function(input_signature=...)` decoration, but the model either doesn't serialise the custom `train_step` or doesn't load it properly:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.python.eager import backprop\r\n\r\n\r\nclass TestModel(tf.keras.models.Model):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.x = layers.Input((10,))\r\n        self.output_ = layers.Dense(1)\r\n\r\n    def call(self, inputs):\r\n        return self.output_(inputs)\r\n\r\n    @tf.function(input_signature=[tuple(((\r\n            [tf.TensorSpec((None, 10), tf.float32),\r\n             tf.TensorSpec((None,), tf.int64)])))])\r\n    def train_step(self, data):\r\n        tf.print('Flag')\r\n        x, y = data\r\n        assert isinstance(x, tf.Tensor)\r\n        assert isinstance(y, tf.Tensor)\r\n        with backprop.GradientTape() as tape:\r\n            y_pred = self(x, training=True)\r\n            loss = self.compiled_loss(\r\n                y, y_pred, regularization_losses=self.losses)\r\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\r\n        self.compiled_metrics.update_state(y, y_pred)\r\n        return {m.name: m.result() for m in self.metrics}\r\n\r\n    def get_config(self):\r\n        config = super().get_config()\r\n        config.update({'train_step': tf.function(self.train_step)})\r\n        return config\r\n\r\noriginal_model = TestModel()\r\noriginal_model.compile(loss='mse', optimizer='sgd')\r\n\r\nfor i in range(3):\r\n    original_model.train_on_batch(\r\n        np.random.rand(6, 10), np.arange(6))\r\n    print('Completed train loop (original model)')\r\n\r\nsave_path = 'tmp'\r\noriginal_model.save(save_path)\r\n\r\nloaded_model = tf.keras.models.load_model(save_path, compile=False)\r\nloaded_model.compile(loss='mse', optimizer='sgd')\r\n\r\nfor i in range(3):\r\n    loaded_model.train_on_batch(np.random.rand(6, 10), np.arange(6))\r\n    print('Completed train loop (loaded model)')\r\n```\r\n\r\nThis gives:\r\n\r\n```\r\nCompleted train loop (original model)\r\nFlag\r\nCompleted train loop (original model)\r\nFlag\r\nCompleted train loop (original model)\r\n<(LOADING INFORMATION)>\r\nCompleted train loop (loaded model)\r\nCompleted train loop (loaded model)\r\nCompleted train loop (loaded model)\r\n```\r\n\r\nAs you can see, `train_step` has reverted to that of the parent class (tf.keras.models.Model). How can we properly serialise vital methods like this?\r\n\r\nThe decoration argument for `@tf.function(input_signature=[tuple((([tf.TensorSpec((None, 10), tf.float32), tf.TensorSpec((None,), tf.int64)])))])` is absurdly convoluted, but it is the only way I could get the code to accept the arguments to `train_on_batch` (and therefore `fit`). Surely this is not expected behaviour.", "Hi @jscant \r\nI'm having the same problem but with a model with two inputs and one output only on the test_step. How would you rephrase the above input_signature? Should something like this work? \r\n\r\n`input_signature=[tuple((([(tf.TensorSpec((None, 224, 224, 3), tf.float32), tf.TensorSpec((None, 224, 224, 3), tf.float32)), tf.TensorSpec((None,), tf.int64)])))])`\r\n\r\nThanks", "May I reopen this issue to ask the following question:\r\n\r\nDoes the new feature inside TF2.4 (the `tf.keras.Model.save(..., save_traces=True)`, with the `save_trace=True` argument), can be used to save the custom training logic (modifying the `train_step()` function and train the model with a model.fit, see initial post) ? If yes, how ?\r\n\r\nThank you in advance, "]}, {"number": 38102, "title": "tensorflow1.15,  Here is a list of operators for which you will need custom implementations: ReorderAxes.RandomStandardNormal", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04)  : ubuntu18.04\r\n- TensorFlow installed from (source or binary):   conda\r\n- TensorFlow version (or github SHA if from source): 1.15\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n# Copy and paste here\r\n\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n", "comments": ["Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONV_2D, FLOOR_DIV, FULLY_CONNECTED, GATHER, LEAKY_RELU, MEAN, MUL, PACK, PAD, RESHAPE, RSQRT, SHAPE, SQUARE, STRIDED_SLICE, SUB, SUM, TILE, TRANSPOSE, TRANSPOSE_CONV. Here is a list of operators for which you will need custom implementations: ReorderAxes.\\\r\n\r\n\r\n\r\n\r\n\r\n#############model  ops\r\n tf.nn.conv2d(x, tf.cast(w, x.dtype), data_format='NHWC', strides=[1,1,1,1], padding='SAME')", "    converter = tf.lite.TFLiteConverter.from_saved_model(PATH)\r\n    print(dir(converter))\r\n    #converter.experimental_new_converter = True  ####\r\n    #converter.allow_custom_ops = True\r\n    \r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.target_spec.supported_types = [tf.float16]\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                           tf.lite.OpsSet.SELECT_TF_OPS]\r\n    tflite_model = converter.convert()", "    PATH  = './savedmodel'\r\n    converter = tf.lite.TFLiteConverter.from_saved_model(PATH)\r\n    print(dir(converter))\r\n \r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.target_spec.supported_types = [tf.float16]\r\n    tflite_model = converter.convert()\r\n    open(\"./model/andriod/lite/pkl2pt_G_synthesis/converted_model3.tflite\", \"wb\").write(tflite_model)\r\n\r\n\r\n\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONV_2D, FLOOR_DIV, FULLY_CONNECTED, GATHER, LEAKY_RELU, MEAN, MUL, PACK, PAD, RESHAPE, RSQRT, SHAPE, SQUARE, STRIDED_SLICE, SUB, SUM, TILE, TRANSPOSE, TRANSPOSE_CONV. Here is a list of operators for which you will need custom implementations: RandomStandardNormal.", "Can you try enabling the custom_ops flag ?\r\n`converter.allow_custom_ops = True`", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@fcqfcq \r\n\r\nAny update on this issue please. Thanks!", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 38101, "title": "How to build op in tensorflow's static graph for unknown rank tensor", "body": "## Problem 1\r\n```python\r\ntensor a = [1,2,3,4]\r\n```\r\nI want to split it into:\r\n```python\r\nb1 = [1] \r\nb2 = [2] \r\nb3 = [3] \r\nb4 = [4]\r\n```\r\nHowever, if the tensor's shape is None, such as\r\n```python\r\na = tf.placeholder(tf.float32, [1, None], \"a\")\r\n```\r\nHow to build this **op** in tensorflow's static graph?\r\n\r\n## Problem 2\r\nIn addition\uff0c\r\n```python\r\ntensor x = [1,2,3,4]\r\n```\r\nI want to build a series tensor based on `tensor x`:\r\n```python\r\nx1 = [1]\r\nx2 = [1,1]\r\nx3 = [1,1,1]\r\nx4 = [1,1,1,1]\r\n```\r\nJust the same as the Problem 1, if the tensor's shape is None, how to build this *op* in tensorflow's static graph?\r\n", "comments": ["@MachineJeff,\r\nCould you please check out the [tf.split](https://www.tensorflow.org/api_docs/python/tf/split) function and let us know if it helps? \r\n\r\nAttached [gist](https://colab.research.google.com/gist/amahendrakar/605e97853dfa61458a334e75e4129ae8/38101.ipynb) for reference. Thanks!", "@amahendrakar,\r\n**tf.split** No it does not work for unknown rank tensor such as **tf.palceholder**", "@amahendrakar \r\nI have done it using tf.while_loop() \r\nThanks."]}, {"number": 38100, "title": "tensorflow training stuck after \"Successfully opened dynamic library libcublas.so.10.0\"", "body": "I'm using TF 1.13 with cuda 10. I'm training a model but the TF code freezes after printing `Successfully opened dynamic library libcublas.so.10.0`. The GPU memory was all occupied and the CPU was also fully occupied meaning that it was not a deadlock or something (my problem I think is different than issue #32017).\r\nThe training script:\r\n\r\n```\r\nCUDA_AVAILABLE_DEVICES=0,1,2,3 t2t-trainer \\\r\n  --data_dir=~/t2t_data \\\r\n  --output_dir=train_outputs/div2k \\\r\n  --problem=img2img_div2k \\\r\n  --model=img2img_transformer \\\r\n  --hparams_set=img2img_transformer \\\r\n  --train_steps=2000000 \\\r\n  --eval_steps=5000 --local_eval_frequency=5000 --worker_gpu=4\r\n```\r\n\r\nOutput:\r\n```\r\nINFO:tensorflow:Building model body\r\nINFO:tensorflow:Transforming body output with identity_modality.top\r\nINFO:tensorflow:Transforming feature 'inputs' with identity_modality.bottom\r\nINFO:tensorflow:Transforming feature 'targets' with identity_modality.targets_bottom\r\nINFO:tensorflow:Building model body\r\nINFO:tensorflow:Transforming body output with identity_modality.top\r\nINFO:tensorflow:Base learning rate: 0.200000\r\nINFO:tensorflow:Trainable Variables Total size: 47118592\r\nINFO:tensorflow:Non-trainable variables Total size: 5\r\nINFO:tensorflow:Using optimizer Adam\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\n^[[AINFO:tensorflow:Graph was finalized.\r\n2020-04-01 05:48:23.753790: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-04-01 05:48:25.319599: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x13dc3370 executing computations on platform CUDA. Devices:\r\n2020-04-01 05:48:25.319671: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla P40, Compute Capability 6.1\r\n2020-04-01 05:48:25.319694: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (1): Tesla P40, Compute Capability 6.1\r\n2020-04-01 05:48:25.319713: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (2): Tesla P40, Compute Capability 6.1\r\n2020-04-01 05:48:25.319732: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (3): Tesla P40, Compute Capability 6.1\r\n2020-04-01 05:48:25.325520: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2593990000 Hz\r\n2020-04-01 05:48:25.330211: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x197d480 executing computations on platform Host. Devices:\r\n2020-04-01 05:48:25.330255: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2020-04-01 05:48:25.330426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:\r\nname: Tesla P40 major: 6 minor: 1 memoryClockRate(GHz): 1.531\r\npciBusID: 0b1c:00:00.0\r\ntotalMemory: 22.38GiB freeMemory: 22.23GiB\r\n2020-04-01 05:48:25.330494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 1 with properties:\r\nname: Tesla P40 major: 6 minor: 1 memoryClockRate(GHz): 1.531\r\npciBusID: 246c:00:00.0\r\ntotalMemory: 22.38GiB freeMemory: 22.23GiB\r\n2020-04-01 05:48:25.330551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 2 with properties:\r\nname: Tesla P40 major: 6 minor: 1 memoryClockRate(GHz): 1.531\r\npciBusID: 4de1:00:00.0\r\ntotalMemory: 22.38GiB freeMemory: 22.23GiB\r\n2020-04-01 05:48:25.330608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 3 with properties:\r\nname: Tesla P40 major: 6 minor: 1 memoryClockRate(GHz): 1.531\r\npciBusID: 61fa:00:00.0\r\ntotalMemory: 22.38GiB freeMemory: 22.23GiB\r\n2020-04-01 05:48:25.330925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0, 1, 2, 3\r\n2020-04-01 05:48:25.337584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-01 05:48:25.337621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 1 2 3\r\n2020-04-01 05:48:25.337644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N N N N\r\n2020-04-01 05:48:25.337663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 1:   N N N N\r\n2020-04-01 05:48:25.337681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 2:   N N N N\r\n2020-04-01 05:48:25.337700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 3:   N N N N\r\n2020-04-01 05:48:25.337915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21773 MB memory) -> physical GPU (device: 0, name: Tesla P40, pci bus id: 0b1c:00:00.0, compute capability: 6.1)\r\n2020-04-01 05:48:25.338239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 21773 MB memory) -> physical GPU (device: 1, name: Tesla P40, pci bus id: 246c:00:00.0, compute capability: 6.1)\r\n2020-04-01 05:48:25.338604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 21773 MB memory) -> physical GPU (device: 2, name: Tesla P40, pci bus id: 4de1:00:00.0, compute capability: 6.1)\r\n2020-04-01 05:48:25.339518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 21773 MB memory) -> physical GPU (device: 3, name: Tesla P40, pci bus id: 61fa:00:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 0 into train_outputs/model.ckpt.\r\n2020-04-01 05:50:09.675984: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\r\n\r\n```\r\nI enabled `TF_CPP_MIN_VLOG_LEVEL=2` and here is some part of the log which is recurring but with different values everytime:\r\n\r\n```\r\n2020-04-01 05:30:33.932882: I tensorflow/core/common_runtime/executor.cc:1804] Synchronous kernel done: 8 step -361974947767986987 {{node ToInt64}} = Cast[DstT=DT_INT64, SrcT=DT_FLOAT, Truncate=false, _device=\"/device:CPU:0\"](resize/Squeeze) device: /job:localhost/replica:0/task:0/device:CPU:0\r\n2020-04-01 05:30:33.933436: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorOutput { step_id: -882416251439127608 kernel_name: \"resize/Squeeze\" tensor { dtype: DT_FLOAT shape { dim {\r\nsize: 8 } dim { size: 8 } dim { size: 3 } } allocation_description { requested_bytes: 768 allocator_name: \"cpu\" ptr: 140093260055424 } } }\r\n2020-04-01 05:30:33.933454: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2020-04-01 05:30:33.931442: I tensorflow/core/common_runtime/executor.cc:1804] Synchronous kernel done: 4 step -2826364024402348450 {{node ParseSingleExample/Reshape}} = Const[dtype=DT_INT64, value=Tensor<type:\r\nint64 shape: [1] values: 0>, _device=\"/device:CPU:0\"]() device: /job:localhost/replica:0/task:0/device:CPU:0\r\n2020-04-01 05:30:33.931993: I tensorflow/core/common_runtime/executor.cc:1661] Process node: 12 step -9020318172928929357 {{node LogicalOr}} = LogicalOr[_device=\"/device:CPU:0\"](Equal, Equal_1) device: /job:localhost/replica:0/task:0/device:CPU:0\r\n2020-04-01 05:30:33.932017: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorOutput { step_id: -7271963003795482065 kernel_name: \"TensorDataset\" tensor { dtype: DT_VARIANT shape { } al\r\nlocation_description { requested_bytes: 8 allocator_name: \"cpu\" ptr: 140092371056192 } } }\r\n2020-04-01 05:30:33.933553: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorOutput { step_id: -9020318172928929357 kernel_name: \"LogicalOr\" tensor { dtype: DT_BOOL shape { } allocation_description { requested_bytes: 1 allocator_name: \"cpu\" ptr: 140093042046528 } } }\r\n2020-04-01 05:30:33.931544: I tensorflow/core/common_runtime/executor.cc:1804] Synchronous kernel done: 7 step -1925456608919473175 {{node resize/Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[0], _device=\"/device:CPU:0\"](resize/ResizeArea) device: /job:localhost/replica:0/task:0/device:CPU:0\r\n2020-04-01 05:30:33.933059: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2020-04-01 05:30:33.933089: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2020-04-01 05:30:33.933094: I tensorflow/core/common_runtime/executor.cc:1804] Synchronous kernel done: 10 step -7022465578876499613 {{node Equal}} = Equal[T=DT_STRING, _device=\"/device:CPU:0\"](ParseSingleExample/ParseSingleExample:2, Equal/y) device: /job:localhost/replica:0/task:0/device:CPU:0\r\n2020-04-01 05:30:33.933676: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorOutput { step_id: -3410101109738072632 kernel_name: \"ParseSingleExample/ParseSingleExample\" tensor { dtype: DT_INT64 shape { dim { size: 1 } } allocation_description { requested_bytes: 8 allocator_name: \"cpu\" ptr: 140092035786688 } } }\r\n2020-04-01 05:30:33.932257: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorAllocation { step_id: -4867250882158238528 kernel_name: \"TensorDataset\" tensor { dtype: DT_VARIANT shape {\r\n} allocation_description { requested_bytes: 8 allocator_name: \"cpu\" ptr: 140092975893504 } } }\r\n2020-04-01 05:30:33.933150: I tensorflow/core/common_runtime/executor.cc:1804] Synchronous kernel done: 4 step -8238964452958482503 {{node ParseSingleExample/Reshape}} = Const[dtype=DT_INT64, value=Tensor<type:\r\nint64 shape: [1] values: 0>, _device=\"/device:CPU:0\"]() device: /job:localhost/replica:0/task:0/device:CPU:0\r\n2020-04-01 05:30:33.933757: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorOutput { step_id: -4867250882158238528 kernel_name: \"TensorDataset\" tensor { dtype: DT_VARIANT shape { } allocation_description { requested_bytes: 8 allocator_name: \"cpu\" ptr: 140092975893504 } } }\r\n2020-04-01 05:30:33.933777: I tensorflow/core/common_runtime/executor.cc:1661] Process node: 5 step -8238964452958482503 {{node ParseSingleExample/Const_1}} = Const[dtype=DT_STRING, value=Tensor<type: string shape: [0] values: >, _device=\"/device:CPU:0\"]() device: /job:localhost/replica:0/task:0/device:CPU:0\r\n2020-04-01 05:30:33.933793: I tensorflow/core/common_runtime/executor.cc:1804] Synchronous kernel done: 9 step -4867250882158238528 {{node TensorDataset}} = TensorDataset[Toutput_types=[DT_UINT8, DT_INT64], outp\r\nut_shapes=[[?,?,3], [8,8,3]], _device=\"/device:CPU:0\"](arg0, ToInt64) device: /job:localhost/replica:0/task:0/device:CPU:0\r\n2020-04-01 05:30:33.933814: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorOutput { step_id: -8238964452958482503 kernel_name: \"ParseSingleExample/Const_1\" tensor { dtype: DT_STRING\r\nshape { dim { } } } }\r\n2020-04-01 05:30:33.932821: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2020-04-01 05:30:33.933305: I tensorflow/core/common_runtime/executor.cc:1661] Process node: 8 step -5799046217055003931 {{node ToInt64}} = Cast[DstT=DT_INT64, SrcT=DT_FLOAT, Truncate=false, _device=\"/device:CPU\r\n:0\"](resize/Squeeze) device: /job:localhost/replica:0/task:0/device:CPU:0\r\n2020-04-01 05:30:33.933380: I tensorflow/core/common_runtime/executor.cc:1804] Synchronous kernel done: 6 step -200002097097137980 {{node resize/ResizeArea}} = ResizeArea[T=DT_UINT8, align_corners=false, _device\r\n=\"/device:CPU:0\"](resize/ExpandDims, resize/size) device: /job:localhost/replica:0/task:0/device:CPU:0\r\n2020-04-01 05:30:33.933917: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorAllocation { step_id: -5799046217055003931 kernel_name: \"ToInt64\" tensor { dtype: DT_INT64 shape { dim { si\r\nze: 8 } dim { size: 8 } dim { size: 3 } } allocation_description { requested_bytes: 1536 allocator_name: \"cpu\" ptr: 140093916398656 } } }\r\n2020-04-01 05:30:33.933939: I tensorflow/core/common_runtime/executor.cc:1661] Process node: 7 step -200002097097137980 {{node resize/Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[0], _device=\"/device:CPU:0\"](re\r\nsize/ResizeArea) device: /job:localhost/replica:0/task:0/device:CPU:0\r\n2020-04-01 05:30:33.933954: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorOutput { step_id: -5799046217055003931 kernel_name: \"ToInt64\" tensor { dtype: DT_INT64 shape { dim { size:\r\n8 } dim { size: 8 } dim { size: 3 } } allocation_description { requested_bytes: 1536 allocator_name: \"cpu\" ptr: 140093916398656 } } }\r\n2020-04-01 05:30:33.933479: I tensorflow/core/common_runtime/executor.cc:1804] Synchronous kernel done: 7 step -882416251439127608 {{node resize/Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[0], _device=\"/device:CPU:0\"](resize/ResizeArea) device: /job:localhost/replica:0/task:0/device:CPU:0\r\n2020-04-01 05:30:33.933489: I tensorflow/core/common_runtime/executor.cc:1661] Process node: 9 step -361974947767986987 {{node TensorDataset}} = TensorDataset[Toutput_types=[DT_UINT8, DT_INT64], output_shapes=[[\r\n?,?,3], [8,8,3]], _device=\"/device:CPU:0\"](arg0, ToInt64) device: /job:localhost/replica:0/task:0/device:CPU:0\r\n2020-04-01 05:30:33.933519: I tensorflow/core/common_runtime/executor.cc:1661] Process node: 5 step -2826364024402348450 {{node ParseSingleExample/Const_1}} = Const[dtype=DT_STRING, value=Tensor<type: string sha\r\npe: [0] values: >, _device=\"/device:CPU:0\"]() device: /job:localhost/replica:0/task:0/device:CPU:0\r\n2020-04-01 05:30:33.934058: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorAllocation { step_id: -361974947767986987 kernel_name: \"TensorDataset\" tensor { dtype: DT_VARIANT shape { } allocation_description { requested_bytes: 8 allocator_name: \"cpu\" ptr: 140092242034688 } } }\r\n2020-04-01 05:30:33.934073: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorOutput { step_id: -2826364024402348450 kernel_name: \"ParseSingleExample/Const_1\" tensor { dtype: DT_STRING\r\nshape { dim { } } } }\r\n```\r\nI literally have no idea what the problem is. I tried with another dataset and the model started training but with this particular datasets the model is constantly in executor.cc:1161 and executor.cc:1804. Is it doing computations on CPU rather than GPU maybe? but the GPU memory is all occupied and the TF has recognized the GPU devices. \r\n\r\n**P.S**: I also added a signal handler to my python code to print the stack trace upon receiving a specific signal (to see where in the python code it is spending the time), but after a certain point, i guess the signal handlers are overridden as the code does not respond to my manual signal. It does no longer even respond to SIGINT as well.\r\n\r\nany idea?", "comments": ["@py4, Can you provide the standalone code snippet to replicate the reported issue here. Thanks!", "I am having the same issue.\r\nWith TF 1.13.1 + CUDA 10.1 + Win10", "@gadagashwini it's not really possible. Some part of it is a confidential code (I replaced our own dataset with div2k). Isn't there any clue in the TF C++ logs?", "Hi @py4, I solved it by reducing the batch size. Also maybe, check your verbosity level?", "@py4, Can you try @jetjodh's suggestion and let us know if that helps in resolving your issue. Thanks", "@py4, Were you able to resolve this issue.?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@py4, \r\nplease update on the above comment", "Closing as stale. Please reopen if you'd like to work on this further.\n", "> @py4, Can you try @jetjodh's suggestion and let us know if that helps in resolving your issue. Thanks\r\n\r\nReducing the `batch_size` to 1 **didn't** work out for me. \r\n\r\n```bash\r\nHPARAMS_SET=transformer_tpu\r\nMODEL=transformer\r\n\r\nHPARAMS=\"batch_size=1,num_hidden_layers=16\"\r\n```", "I'm trying to run tf object detection model. Getting the same issue; Stuck after Successfully opened dynamic library libcuda.so.1", "When I reduce the batch size and number of gpus, it works", "Still having the exact same issue :'(", "Still having the exact same issue :'( with tf2.4", "same issue with CUDA=10.0 CUDNN=7.4 Tensorflow=1.14.0\r\nstuck at `I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10`\r\nanyone figue it out?"]}, {"number": 38098, "title": "when i use tf.contrib.lite.Interpreter(model_path=\"xxxx.lite\") ", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\ntf.contrib.lite.Interpreter(model_path=\"XXXX.lite\")\r\ninterpreter.allocate_tensors()\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nreturn _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_InputIndices(self)\r\nValueError: Interpreter was not initialized.\r\n```\r\n\r\n\r\n\r\n**Failure details**\r\nwhen i use ```converter.convert()``` without  ```converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE] ``` it works. But i use converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE],it appears the error.\r\nthis model is from torch\u2018s to tensorflow.May be related to this\r\n\r\n", "comments": ["@sibadakesi \r\n\r\nCan you please let us know which Tensorflow version you are using? .Also, request you to share simple standalone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "Thinks\uff0ci use the higher Tensorflow version(1.15.0) it works. I realized if i  quantizing this torch\u2018s models  i must use than or equal 1.15.0 version of Tensorflow.  ", "@sibadakesi \r\n\r\nGlad to know that worked with Latest versions.Please close this thread if it solves your question. Thanks!."]}, {"number": 38097, "title": "How to create a custom distribution strategy on Tensorflow", "body": "I know this question should be asked on Stackoverflow. But it seems that no one has the courage to answer [this](https://stackoverflow.com/questions/57852705/how-to-create-a-custom-distribution-strategy-on-tensorflow) question for half an year. So I'm trying some luck on the github, hoping there are someone who has the expertise that could give some instructions on how to realize the model parallelism strategy, specifically the communication part among different jobs?", "comments": ["@sjtusmartboy,\r\nCould you please take a look at [this](https://www.tensorflow.org/guide/distributed_training) official TensorFlow guide for distributed training and the [overview](https://www.tensorflow.org/api_docs/python/tf/distribute) for tf.distribute and let us know if it helps? Thanks!\r\n\r\n\r\n", "@amahendrakar Thank you for your materials. I've checked these materials and find they teach user how to use the distribution strategies, including MirroredStrategy, TPUStrategy ... Is there any docs showing the architecture of these strategies, and how they are implemented?", "@sjtusmartboy,\r\nTo know more about the architecture of these APIs you can check out their respective source code on GitHub.\r\n\r\nImplementation details can be found [here](https://www.tensorflow.org/guide/distributed_training#examples_and_tutorials_2) under the 'Examples and Tutorials' section. \r\n\r\nThanks!", "Is this still an issue? Please feel free to close the issue if resolved. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Could be related to https://github.com/tensorflow/tensorflow/issues/52197\r\n"]}, {"number": 38096, "title": "tensorflow::mutex occasionally fails on aarch64 platform", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): **`No`**\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): **`Linux Ubuntu 18.04`**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: **`NA`**\r\n- TensorFlow installed from (source or\r\nbinary):**`source`** \r\n- TensorFlow version (use command below): **`v1.15.0`**\r\n- Python version: **`3.7.5`**\r\n- Bazel version (if compiling from source):**`0.24.1`**\r\n- GCC/Compiler version (if compiling from\r\nsource): **`7.5.0`**\r\n- CUDA/cuDNN version:**`NA`** \r\n- GPU model and memory:**`NA`**\r\n\r\n**Describe the current behavior**\r\n\r\nI ran tensorflow on aarch64 platform, using ParallelMapDataset to preprocess the data, but after running normally for some time (about a few hours), the tensorflow process core dumps, each time with a different stack(here are two of them):\r\n```\r\n#0  __GI_raise (sig=11) at ../sysdeps/unix/sysv/linux/raise.c:51\r\n#1  <signal handler called>\r\n#2  0x0000ffff8f3fd024 in nsync::nsync_dll_splice_after_(nsync::nsync_dll_element_s_*, nsync::nsync_dll_element_s_*) ()\r\n   from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#3  0x0000ffff8f3fd05c in nsync::nsync_dll_make_first_in_list_(nsync::nsync_dll_element_s_*, nsync::nsync_dll_element_s_*) ()\r\n   from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#4  0x0000ffff8f3fd09c in nsync::nsync_dll_make_last_in_list_(nsync::nsync_dll_element_s_*, nsync::nsync_dll_element_s_*) ()\r\n   from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#5  0x0000ffff8f3fd25c in nsync::nsync_mu_lock_slow_(nsync::nsync_mu_s_*, nsync::waiter*, unsigned int, nsync::lock_type_s*) ()\r\n   from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#6  0x0000ffff8f3fd38c in nsync::nsync_mu_lock(nsync::nsync_mu_s_*) () from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#7  0x0000ffff86ff032c in tensorflow::CancellationManager::StartCancel() () from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/../libtensorflow_framework.so.1\r\n#8  0x0000ffff86ff05d4 in tensorflow::CancellationManager::StartCancel() () from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/../libtensorflow_framework.so.1\r\n#9  0x0000ffff86ff0a4c in tensorflow::CancellationManager::~CancellationManager() () from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/../libtensorflow_framework.so.1\r\n#10 0x0000ffff8bded294 in std::_Sp_counted_ptr_inplace<tensorflow::data::IteratorResource::State, std::allocator<tensorflow::data::IteratorResource::State>, (__gnu_cxx::_Lock_policy)2>::_M_dispose() ()\r\n   from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#11 0x0000ffff8994b43c in std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release() () from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#12 0x0000ffff8bdf0f78 in tensorflow::data::IteratorResource::~IteratorResource() () from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#13 0x0000ffff870717ec in tensorflow::ResourceMgr::Clear() () from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/../libtensorflow_framework.so.1\r\n#14 0x0000ffff8727ae24 in tensorflow::DeviceMgr::~DeviceMgr() () from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/../libtensorflow_framework.so.1\r\n#15 0x0000ffff8b9d13a0 in tensorflow::DirectSession::~DirectSession() () from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#16 0x0000ffff8b9d1484 in tensorflow::DirectSession::~DirectSession() () from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#17 0x0000ffff89991e90 in tensorflow::SessionRef::Close() () from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#18 0x0000ffff89f24218 in TF_CloseSession () from /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n```\r\n```\r\n[10:02:20]#0  0x0000ffffb7c8aad0 in raise () from /lib64/libpthread.so.0\r\n[10:02:20]#1  <signal handler called>\r\n[10:02:20]#2  0x0000ffffb7a12140 in raise () from /lib64/libc.so.6\r\n[10:02:20]#3  0x0000ffffb7a134ec in abort () from /lib64/libc.so.6\r\n[10:02:20]#4  0x0000ffff9dfa21d0 in __gnu_cxx::__verbose_terminate_handler() () from /lib64/libstdc++.so.6\r\n[10:02:20]#5  0x0000ffff9df9fae4 in ?? () from /lib64/libstdc++.so.6\r\n[10:02:20]#6  0x0000ffff9df9fb30 in std::terminate() () from /lib64/libstdc++.so.6\r\n[10:02:20]#7  0x0000ffff9df9fdec in __cxa_throw () from /lib64/libstdc++.so.6\r\n[10:02:20]#8  0x0000ffff9dfcac9c in std::__throw_bad_function_call() () from /lib64/libstdc++.so.6\r\n[10:02:20]#9  0x0000ffff9efd9a00 in tensorflow::UnboundedWorkQueue::PooledThreadFunc() ()\r\n[10:02:20]   from /usr/local/python3.7/lib/python3.7/site-packages/tensorflow_core/python/../libtensorflow_framework.so.1\r\n[10:02:20]#10 0x0000ffff9dfcd144 in ?? () from /lib64/libstdc++.so.6\r\n[10:02:20]#11 0x0000ffffb7c7f8bc in start_thread () from /lib64/libpthread.so.0\r\n[10:02:22]--Type <RET> for more, q to quit, c to continue without paging--\r\n[10:02:22]#12 0x0000ffffb7ab473c in thread_start () from /lib64/libc.so.6\r\n```\r\nThe first stack tells me that when the session exits, an invalid callback remains in a CancellationManager(use-after-free).The second stack indicates that the worker thread of the thread pool has received a corrupted task.\r\n\r\nAfter analyzing the code, I thought these were all illogical exceptions, so I tested the tensorflow interface(here are two of them):\r\n```C\r\n#include \"tensorflow/core/framework/cancellation.h\"\r\n#include <thread>\r\n\r\nusing tensorflow::CancellationManager;\r\nusing std::thread;\r\n\r\nvoid RegAndUnreg(CancellationManager& manager) {\r\n    for (int i = 0; i < 10000000; i++) {\r\n        auto token = manager.get_cancellation_token();\r\n        manager.RegisterCallback(token, nullptr);\r\n        manager.DeregisterCallback(token);\r\n    }\r\n}\r\n\r\nint main() {\r\n    int i = 0;\r\n    while (true) {\r\n        i++;\r\n        CancellationManager manager;\r\n        thread t1([&manager]() {RegAndUnreg(manager);});\r\n        thread t2([&manager]() {RegAndUnreg(manager);});\r\n        thread t3([&manager]() {RegAndUnreg(manager);});\r\n        thread t4([&manager]() {RegAndUnreg(manager);});\r\n        thread t5([&manager]() {RegAndUnreg(manager);});\r\n        thread t6([&manager]() {RegAndUnreg(manager);});\r\n        thread t7([&manager]() {RegAndUnreg(manager);});\r\n        thread t8([&manager]() {RegAndUnreg(manager);});\r\n        t1.join();\r\n        t2.join();\r\n        t3.join();\r\n        t4.join();\r\n        t5.join();\r\n        t6.join();\r\n        t7.join();\r\n        t8.join();\r\n        // I've set the callbacks_ to the public property for easy viewing\r\n        if (manager.callbacks_.size() > 0) {\r\n            LOG(FATAL) << \"Round \" << i << \"[\" << manager.callbacks_.size() << \"] callbacks left.\" << std::endl;\r\n        }\r\n    }\r\n}\r\n```\r\nThe above program soon had a core dump, sometimes because of a residual callback(I think) and sometimes because of a Check failure (token>next_cancellation_token_).\r\n\r\n```C\r\n#include \"tensorflow/core/platform/default/unbounded_work_queue.h\"\r\n#include \"tensorflow/core/platform/env.h\"\r\n\r\n#include <thread>\r\n#include <functional>\r\n\r\nusing tensorflow::UnboundedWorkQueue;\r\nusing std::thread;\r\nusing WorkFunction = std::function<void()>;\r\n\r\nconst static WorkFunction work = [](){};\r\n\r\nvoid ScheduleTask(UnboundedWorkQueue& manager) {\r\n    for (int i = 0; i < 10000000; i++) {\r\n        manager.Schedule(work);\r\n    }\r\n}\r\n\r\nint main() {\r\n    int i = 0;\r\n    while (true) {\r\n        i++;\r\n        UnboundedWorkQueue manager(tensorflow::Env::Default(), \"bugfix\");\r\n        thread t1([&manager]() {ScheduleTask(manager);});\r\n        thread t2([&manager]() {ScheduleTask(manager);});\r\n        thread t3([&manager]() {ScheduleTask(manager);});\r\n        thread t4([&manager]() {ScheduleTask(manager);});\r\n        thread t5([&manager]() {ScheduleTask(manager);});\r\n        thread t6([&manager]() {ScheduleTask(manager);});\r\n        thread t7([&manager]() {ScheduleTask(manager);});\r\n        thread t8([&manager]() {ScheduleTask(manager);});\r\n\r\n        t1.join();\r\n        t2.join();\r\n        t3.join();\r\n        t4.join();\r\n        t5.join();\r\n        t6.join();\r\n        t7.join();\r\n        t8.join();\r\n    }\r\n}\r\n\r\n```\r\nThe above program soon had a core dump, and the stack showed with successive stack_check_fail, consistent with the second core dump stack above\r\nThe exception **`disappeared`** when I replaced the locks held in the CancellationManager and UnboundedWorkQueue from tensorflow::mutex to STD ::mutex.However, I am sure that core dump can occur in other places, and it is difficult to directly replace all tensorflow::mutex, because tensorflow::mutex has some custom methods and members.\r\n\r\n**Describe the expected behavior**\r\nThese exceptions prevented me from deploying tensorflow on my server\uff0cdoes the community expect tensorflow to be deployed on aarch64 servers?\r\nAfter my test, it is true that tensorflow::mutex is faster than STD ::mutex, but sadly, it does not work properly on aarch64, which is deeply troubling me. Can you give me some Suggestions?Or does the community plan to offer some options for locks?I see the current implementation of mutex in platform/default, perhaps with platform/aarch64?\r\n\r\n**Standalone code to reproduce the issue** \r\nIt is not easy to reproduce in a production environment, such as the test case above\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\nRefer to the core dump stack information above\r\n", "comments": ["/cc", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38096\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38096\">No</a>\n", "We've also observed that the [google nsync library is unreliable on some aarch64 implementations](https://github.com/cms-sw/cmssw/issues/32899).  In our case, we saw frequent assertion failures and crashes on a Cavium ThunderX system, but were unable to reproduce the problem on an Apple Silicon M1."]}, {"number": 38095, "title": "Add int16 support for tf.math.minimum and tf.math.maximum", "body": "\r\nI was working on lossless audio data which is primary int16 PCM data,\r\nwhen trying to do a simple minimum to my surprise I noticed that in16\r\nis not supported:\r\n```\r\n>>> x = tf.constant(1, tf.int16)\r\n>>> y = tf.constant(2, tf.int16)\r\n>>> tf.math.minimum(x, y)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Library/Python/3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 5961, in minimum\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/Library/Python/3.7/site-packages/tensorflow/python/framework/ops.py\", line 6653, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.NotFoundError: Could not find valid device for node.\r\nNode:{{node Minimum}}\r\nAll kernels registered for op Minimum :\r\n  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_BFLOAT16]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_INT32]\r\n  device='CPU'; T in [DT_INT64]\r\n  device='XLA_CPU'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF]\r\n [Op:Minimum]\r\n```\r\n\r\nAs such I used a workardound which looks quite awkward:\r\n```\r\n>>> x = tf.constant(1, tf.int16)\r\n>>> y = tf.constant(2, tf.int16)\r\n>>> z = tf.stack([x, y])\r\n>>> tf.math.reduce_min(z)\r\n<tf.Tensor: shape=(), dtype=int16, numpy=1>\r\n>>>\r\n```\r\n\r\nThis PR adds int16 support for `tf.math.minimum` and `tf.math.maximum`.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @bixia1 for the review. I have updated the PR with documentation in `tensorflow/tensorflow/core/ops/ops.pbtxt`. Please take a look and let me know if there are any other issues."]}, {"number": 38094, "title": "Use vector load/store in the normalization CUDA kernel", "body": "This PR is an attempt to use vector load/store for FP16 to optimize memory bandwidth utilization on supported GPUs. The target CUDA kernel is normalization which is used in softmax ops.\r\n\r\nfyi @nluehr ", "comments": []}, {"number": 38093, "title": "Allow unused keys when a dict is passed to a single-input Functional \u2026", "body": "\u2026API model.\r\n\r\nEnsure that the key mapping to the name of the Input is used during Model\r\nexecution.\r\n\r\nPiperOrigin-RevId: 301053395\r\nChange-Id: I7f5bfffc3e034b064b3cd4129e07f000df11cb6b", "comments": []}, {"number": 38092, "title": "Remove expired forward compatibility horizons", "body": "This PR is a followup to #37801 and removes expired forward compatibility statements that always evaluate to `True`.", "comments": ["@aaudiber thanks for the fast review. It looks like I left an unused import. I fixed it in e6f54e4f0780e0dccdafd41bad955ff961fe8afe so CI  should pass now.", "@gbaned any updates on this? Would be good to get it reviewed and merged soon, before it accumulates merge conflicts", "@lgeiger Can you please resolve conflicts? Thanks!", "> Can you please resolve conflicts? Thanks!\r\n\r\nI rebased the conflicts.\r\n@gbaned It was expected that this PR will accumulate merge conflicts due to the nature of the changes and since it has been open for a month now (see my last comment). Could this PR get reviewed soon in order to prevent more duplicate work on both sides?", "Thanks for the review :+1:"]}, {"number": 38091, "title": "unclear explanation in the Text classification with TensorFlow Hub: Movie reviews example", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/tutorials/keras/text_classification_with_hub#build_the_model\r\n\r\nPlease provide a link to the documentation entry, for example:\r\n\r\nhttps://www.tensorflow.org/tutorials/keras/text_classification_with_hub#build_the_model\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe last layer in the model is `model.add(tf.keras.layers.Dense(1))`. However, in the following description, it says `The last layer is densely connected with a single output node. Using the sigmoid activation function, ...`\r\n\r\nI check the api doc and find that the default activation is none for dense layer.\r\n\r\n- Without `activation='sigmoid'`, the predictions are not in the range of (0, 1) as shown below, which is not interpretable.\r\n\r\n```\r\npred = model.predict(test_data.batch(512))\r\nprint(pred)\r\n\r\n[[-0.29496038]\r\n [ 1.2088487 ]\r\n [ 0.11580676]\r\n ...\r\n [-1.610341  ]\r\n [-0.8496179 ]\r\n [ 1.3117154 ]]\r\n```\r\n\r\nSo shall the example code be `model.add(tf.keras.layers.Dense(1, activation='sigmoid'))`?", "comments": ["fixing this issue", "Closing this issue since the associated PR has been merged. Thanks!"]}, {"number": 38090, "title": "GPU not found on Windows 10 with tf 2.1.0", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code: No\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from: pip\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.7 \r\n- CUDA/cuDNN version: 10.2.89\r\n- GPU model and memory: Nvidia GTX 1650\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I run \r\n\r\n`tf.config.list_physical_devices(None)`\r\n\r\nI get\r\n\r\n`[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]`\r\n\r\n**Describe the expected behavior**\r\n\r\n`tf.config.list_physical_devices(None)` should report a GPU device\r\n\r\n**Other info / logs**\r\n\r\nHappy to provide more info, but I'm not sure what's relevant.", "comments": ["My bad, I didn't realize tensorflow 2.1.0 didn't work with CUDA 10.2! I installed CUDA 10.1 and now my GPU is recognized. :)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38090\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38090\">No</a>\n"]}, {"number": 38089, "title": "Add reminder to test deterministic cuDNN CTC loss", "body": "@sanjoy added deterministic cuDNN CTC loss, enabled via `TF_DETERMINISTIC_OPS`, with [this commit](https://github.com/tensorflow/tensorflow/commit/9e096debc4a0909deb69970f38bee7b77e5e5f7d). This current pull request places a reminder in `cudnn_deterministic_base.py` for me to add a test for it.", "comments": ["Moved this to issue [38151](https://github.com/tensorflow/tensorflow/issues/38151), as suggested by @sanjoy. Closing."]}, {"number": 38088, "title": "Cherry-pick dlpack fix into r2.2", "body": "Cherry-pick https://github.com/tensorflow/tensorflow/pull/37999 into r2.2", "comments": ["Thank you!"]}, {"number": 38087, "title": "ImportError: DLL load failed: The specified module could not be found.", "body": "import tensorflow as tf\r\nError in callback <bound method AutoreloadMagics.post_execute_hook of <autoreload.AutoreloadMagics object at 0x00000251F0EB3708>> (for post_execute):\r\nTraceback (most recent call last):\r\n\r\n  File \"C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n\r\n  File \"C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n\r\n  File \"C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n\r\n  File \"C:\\Users\\KIIT\\anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n\r\n  File \"C:\\Users\\KIIT\\anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 538, in post_execute_hook\r\n    _, pymtime = self._reloader.filename_and_mtime(sys.modules[modname])\r\n\r\n  File \"C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 184, in filename_and_mtime\r\n    if not hasattr(module, '__file__') or module.__file__ is None:\r\n\r\n  File \"C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n\r\n  File \"C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n\r\n  File \"C:\\Users\\KIIT\\anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n\r\n  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\r\n\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n\r\n  File \"C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\r\n    from . _api.v2 import audio\r\n\r\n  File \"C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\tensorflow_core\\_api\\v2\\audio\\__init__.py\", line 10, in <module>\r\n    from tensorflow.python.ops.gen_audio_ops import decode_wav\r\n\r\n  File \"C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_audio_ops.py\", line 9, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n\r\n  File \"C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n\r\n  File \"C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n\r\n  File \"C:\\Users\\KIIT\\anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n\r\n  File \"C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\n  File \"C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\KIIT\\anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\KIIT\\anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "@rishabhgautam01,\r\nCould you please check [this](https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156) comment on a similar issue and let us know if it helps. Thanks!", "Any updates regarding this issue? Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38087\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38087\">No</a>\n"]}, {"number": 38086, "title": "Tensorboard does not show the curves", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n\r\nHello, \r\nI am using gcloud ai-platform jobs submit training to submit my training. parameters are:\r\n```\r\ngcloud ai-platform jobs submit training $JOB_NAME \\\r\n        --stream-logs \\\r\n        --runtime-version=2.1 \\\r\n        --job-dir=$JOB_DIR \\\r\n        --package-path=trainer \\\r\n        --module-name=trainer.task \\\r\n        --region=us-east1 \\\r\n        --python-version=3.5 \\\r\n        --scale-tier=basic\r\n```\r\n\r\nI am using tensorflow keras to build the model and compile it. adding the following metrics:\r\n```\r\nMETRICS = [keras.metrics.TruePositives(name='tp'), \\\r\n           keras.metrics.FalsePositives(name='fp'), \\\r\n           keras.metrics.TrueNegatives(name='tn'), \\\r\n           keras.metrics.FalseNegatives(name='fn'), \\\r\n           keras.metrics.BinaryAccuracy(name='accuracy'), \\\r\n           keras.metrics.Precision(name='precision'), \\\r\n           keras.metrics.Recall(name='recall'), \\\r\n           keras.metrics.AUC(name='auc'), \\\r\n          ]\r\n# this is the call in the task.py (ai-platform format)\r\nexecuted_model = model.make_model(input_shape,METRICS,model_choice = 5)\r\n\r\nThe following code adds the tensorboard callbacks and LambdaCallbacks:\r\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR, \\\r\n                                                        histogram_freq=1, \\\r\n                                                        write_graph=True, \\\r\n                                                        write_images=True, \\\r\n                                                        write_grads = True, \\\r\n                                                        update_freq='batch')\r\n  # writer for each one of the CM\r\n  file_writer_cm_train = tf.summary.create_file_writer(cm_dir_train)\r\n  file_writer_cm_valid = tf.summary.create_file_writer(cm_dir_valid)\r\n  file_writer_cm_test = tf.summary.create_file_writer(cm_dir_test)\r\n\r\ndef log_confusion_matrix_train(epoch,logs): \r\n    train_gen_bias.reset()\r\n    train_predictions_bias = executed_model.predict_generator(train_gen_bias, verbose=1)\r\n    for name, value in zip(executed_model.metrics_names, train_predictions_bias):\r\n      print(name, \": \",value)\r\n    print()\r\n  \r\n    save_model_param = dict(zip(executed_model.metrics_names, train_predictions_bias))\r\n    dataset = timestr+\"-train\"\r\n    saveModel(DISEASE_FOLDER,dataset,save_model_param,executed_model)\r\n\r\n    p=0.5\r\n    train_pred = [int(x>p) for x in train_predictions_bias]\r\n    train_labels_int = [CLASSES.index(x) for x in  train_labels] # train_labels is a global parameter\r\n    # Calculate the confusion matrix.\r\n    cm = sklearn.metrics.confusion_matrix(train_labels_int,train_pred)\r\n    print('cm for training is ',cm)\r\n    # Log the confusion matrix as an image summary.\r\n    figure = plot_confusion_matrix(cm, CLASSES)\r\n    cm_image = plot_to_image(figure)\r\n    # Log the confusion matrix as an image summary.\r\n    with file_writer_cm_train.as_default():\r\n      tf.summary.image(\"Confusion Matrix for training dataset\", cm_image, step='epoch')\r\n\r\n # Define the per-epoch callback.\r\n  cm_callback_train = keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix_train)\r\n  cm_callback_valid = keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix_valid) # the same like log_confusion_matrix_valid, just different dataset\r\n  cm_callback_test = keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix_test)\r\n# Define the per-batch callback.\r\n  batch_print_callback = keras.callbacks.LambdaCallback(on_batch_begin=lambda batch,logs: \\\r\n                                                        print('from callback, batch {} begins'.format(batch))) \r\n\r\n#fit model\r\n history_executed_mode = executed_model.fit(train_gen, \\\r\n                                             steps_per_epoch = STEPS_PER_EPOCH, \\\r\n                                             epochs = EPOCHS, \\\r\n                                             validation_data = validation_gen, \\\r\n                                             validation_steps = VALIDATION_STEPS, \\\r\n                                             class_weight=class_weight, \\\r\n                                             callbacks=[tensorboard_callback, \\\r\n                                                        cm_callback_train, \\\r\n                                                        cm_callback_valid, \\\r\n                                                        cm_callback_test, \\\r\n                                                        batch_print_callback])\r\n\r\n```\r\nThe execution is completed successfuly. \r\nI can see that log_confusion_matrix_train is running and generating a file with the metrics that I was checking for the training, validation and test dataset. The only issue is that tensorboard is showing black boxes for the scalar metric (See attached - the tab is called events) while hovering on them, I see numbers and the image tab is empty \r\n![Screen Shot 2020-03-31 at 1 39 19 PM](https://user-images.githubusercontent.com/30246246/78058604-8db43280-7356-11ea-9a84-ec5bdc1f28ae.png)\r\n![Screen Shot 2020-03-31 at 1 39 33 PM](https://user-images.githubusercontent.com/30246246/78058606-8e4cc900-7356-11ea-8d8c-40570a042971.png)\r\nI run the tensorboard from the google shell.\r\n\r\nMy questions are:\r\nwhat is wrong with the way I am producing the metrics?\r\nDo I need three writer for the three confusion matrix?\r\nwhy is the first tab called events and not scalars?\r\n\r\n![Screen Shot 2020-03-31 at 1 52 16 PM](https://user-images.githubusercontent.com/30246246/78058840-e388da80-7356-11ea-94be-b6ce5a79af6b.png)\r\n", "comments": ["Many thanks for any advice!\r\nthis tutorial was used for a guideline for the CM as well: https://www.tensorflow.org/tensorboard/image_summaries\r\nThanks!", "copying to datalab notebook with tensorboard 1.14, I was able to \r\n- see the scalars\r\nand was **not** able to see\r\n- graph (that I could view from the gcloud default version - not sure where to see the version)\r\n\r\nI both of them, I was not able to draw the confusion matrix as shown in the colab, for now, I have generated matplotlib figure with the model prediction data\r\n```\r\nfrom tensorboard import version; print(version.VERSION)\r\n1.14.0a20190603\r\n```\r\n\r\ncode for confusion matrix is taken from https://www.tensorflow.org/tensorboard/image_summaries \r\n\r\nWill keep on trying and will let you know, Please let me know if you have any idea how to proceed.\r\nThanks,\r\neilalan", "If the first tab is called `Events` you are using a very old version of TensorBoard (several years old).  You'll want to ensure you are on a recent version of TensorBoard, ideally 1.15 or newer.\r\n\r\nIf you're still encountering an issue when using a newer version of TensorBoard, please open an issue in the TensorBoard repository at https://github.com/tensorflow/tensorboard/issues/new/choose and provide the requested diagnostic information.\r\n"]}, {"number": 38085, "title": "Fix TFE_OpReset doesn't clear inputs completely", "body": "Cherrypick of #38000 but done manually as #38000 has multiple commits for a net of 1 line change, so it is not worth cherrypicking each commit individually.", "comments": []}, {"number": 38084, "title": "Linking tensorflow/python/_pywrap_tensorflow_internal.so failed on debug build on Windows", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version: r2.2\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): 2.0\r\n- GCC/Compiler version (if compiling from source): MSVC 19\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen build debug tensorflow on Windows. Bazel get stuck at linking tensorflow/python/_pywrap_tensorflow_internal.so. it generate a `_pywrap_tensorflow_internal.pdb` about 4,7 GB. After that, it throw an error [LNK1201](https://docs.microsoft.com/en-us/cpp/error-messages/tool-errors/linker-tools-error-lnk1201?view=vs-2019) `error writing to program database 'filename'; check for insufficient disk space, invalid path, or insufficient privilege`. In addition, Microsoft Incremental Linker consume 87% of my 32 GB Ram\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI just run the command \r\n`bazel build --config=opt -c dbg --jobs=8 //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n**Any other info / logs**\r\nIn fact, I try to build a debug version of tensorflow to resolve an issue [here](https://github.com/tensorflow/tensorflow/issues/19297). I think I only need to build the infrastructure (file system) part of tensorflow and not the logical part. Is there anyways to reduce the overall size of tensorflow ( With me, I do not need the tensorflow::ops for example )\r\n", "comments": ["I think we never tried to build with debug symbols on Windows", "Is there a way to build `py_binary` with debug symbols on Windows for bazel ?", "I think you can try building only that `py_binary`, instead of the entire pip package. But since we never ran debug builds on Python, that might also fail. In which case, TFDBG and printf-style debugging could be the only alternatives.", "@vnvo2409 \r\nplease update as per above comment", "I find a way to use printf in debug so I wil close this issue", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38084\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38084\">No</a>\n"]}, {"number": 38083, "title": "Direct feed from GPU memory in Java", "body": "- PR adds 2 things to Java API:\r\n  - allocating tensor in GPU (GPUBFCAllocator)\r\n  - directly feeding tensors allocated in GPU (makeCallable/runCallable with CallableOptions)\r\n- Originally posted as a feature request [#37909](https://github.com/tensorflow/tensorflow/issues/37909).\r\n- Testing:\r\n```\r\nbazel test //tensorflow/java:DirectSessionTest //tensorflow/java:GPUTensorTest\r\n```\r\n\r\nIt might not be the best implementation. Please, review and advise.\r\nThanks", "comments": ["I should handle these by the weekend", "Quick question, is TF JNI going to be deprecated in favor of JavaCPP?", "I will have to ask internally and see what are the plans.", "Hi, any updates, comments or advice?\r\nThanks", "As long as it's exposed to the C API, the SIG-JVM will be happy to merge this.\r\n\r\n/cc @karllessard", "We'd need to doe some ifdefs around various pieces to remove the cuda specific parts from the jni files when compiling the CPU build. For example the include of cuda_runtime_api.h isn't guarded and so probably won't compile on CPU only machines.\r\n\r\nHave you checked the behaviour of this code on CPU only builds?", "@sjamesr  Could you please have a look at this PR? Thank you!", "@gbaned , @sjamesr is not working for TensorFlow anymore and the TF Java project efforts has been migrated to [this repository](https://github.com/tensorflow/java/). It would be interesting if the following patch can be migrated to the new codebase so we (SIG JVM) can review it and merge it.", "@okdzhimiev  Can you please check @karllessard's comments and keep us posted ? Thanks!", "@gbaned I haven't followed TF development for quite a while. Would be great if somebody take this over.\r\n1. @karllessard's comments sound good.\r\n2. Also, what was missing are ifdefs for non-CUDA builds - see @Craigacp's comment.", "@okdzhimiev, @Craigacp Can you please migrate this patch to new codebase as mentioned @karllessard in the above comment.  \t\r\n", "The current PR exposes bare pointers to users and uses JNI which SIG-JVM has moved away from to JavaCPP. This patch will require essentially a complete rewrite for the new codebase. Some of the JNI portions could be converted into pure C++ which was then wrapped and exposed for use with JavaCPP, or we could have some way of checking if the JavaCPP binding for CUDA is in the classpath and then mirror the logic into Java (using reflection probably so it doesn't crash on CPUs). The Java level API would need to be different. If @okdzhimiev wants to work on porting it over to the new codebase then we can discuss it in a PR on https://github.com/tensorflow/java, otherwise we should open an issue there to provide equivalent functionality. Not sure when we'd get around to doing it though.", "@gbaned Is there now a way to access buffers in GPU memory through the stable C/C++ API? If we can do it in C/C++, we can do in Java, that's not a problem, but I was under the impression that this was still not supported, in which case we may not want to have this in TF Java, until it becomes possible with the stable C/C++ API that is.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@karllessard Can you please take a look on above comments from @Craigacp, @saudet. Thanks!", "Karl, Samuel and I are all members of SIG-JVM, but we can't do anything without moving this patch over to https://github.com/tensorflow/java and rewriting it.", "@karllessard Any update on this PR? Please. Thanks!", "@karllessard Any update on this PR? Please. Thanks!", "Hi @gbaned , like @Craigacp mentioned previously, nobody from SIG JVM is planning to work on this anytime soon. If the author (@okdzhimiev) is interested in rebasing his PR on the new repo for TF Java (https://github.com/tensorflow/java), it would certainly help but I think we should close this one."]}, {"number": 38082, "title": "'TFLiteConverter' object has no attribute 'experimental_new_quantizer' - TF 2.2.0-rc2", "body": "**System information** \r\n- Have I written custom code: See below.\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or\r\nbinary): source\r\n- TensorFlow version (use command below): 2.2.0-rc2\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version : XCode 11.4\r\n- CUDA/cuDNN version: - GPU model and memory: 10.2, GTX1080ti\r\n\r\n**Describe the current behavior**\r\nUsing the current code:\r\n`\r\n    converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(dP.model_name) \r\n    converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\n    converter.representative_dataset = representative_dataset_gen\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n    converter.inference_input_type = tf.uint8\r\n    converter.inference_output_type = tf.uint8\r\n    tflite_quant_model = converter.convert()\r\n`\r\n\r\nI get the following error and crash using TF 2.2.0-rc2:\r\n`\r\n    tflite_quant_model = converter.convert()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py\", line 1094, in convert\r\n    result, inference_input_type, inference_output_type)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py\", line 267, in _calibrate_quantize_model\r\n    inference_output_type, allow_float, self.experimental_new_quantizer)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py\", line 944, in __getattribute__\r\n    return object.__getattribute__(self, name)\r\nAttributeError: 'TFLiteConverter' object has no attribute 'experimental_new_quantizer'\r\n`\r\n\r\n**Describe the expected behavior**\r\nNOTE: This error is specific to TF 2.2.0-rc2. All works well with TF 2.2.0-rc1.\r\n\r\n", "comments": ["The issue seems to be related to this [commit](https://github.com/tensorflow/tensorflow/commit/e6895b3648595080143e8f5dd6f56c16e7852e91). In particular, line 267:\r\n\r\n` inference_output_type, allow_float, self.experimental_new_quantizer)`\r\n\r\nhas not been updated to the new private API. Should it be?:\r\n\r\n` inference_output_type, allow_float, self._experimental_new_quantizer)`", "Good catch. The cherry picked cl was not include all the changes. Let me fix it now.", "Created a fix here: https://github.com/tensorflow/tensorflow/pull/38139", "Tested with fix, and all is working now. Since the fix has been merged, I close this bug.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38082\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38082\">No</a>\n"]}, {"number": 38080, "title": "Trying to convert: RuntTimeError: MetaGraphDef associated with tags {'serve'} could not be found in SavedModel.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 \r\n- TensorFlow installed from (source or binary): anaconda installation\r\n- TensorFlow version (or github SHA if from source): 1.15\r\n\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nMetaGraphDef associated with tags {'serve'} could not be found in SavedModel.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nhttps://drive.google.com/open?id=1fI36hxl9vbIY9KL4Tjvv7HIIBJvYIfW5\r\n```\r\n\r\n\r\n**Any other info / logs**\r\n\r\nWhen I try to convert this is the error I get, so I cannot successfully convert to Lite.\r\n", "comments": ["@csongornagy, Can you please share a standalone code to reproduce the issue? Thanks!", "> @csongornagy, Can you please share a standalone code to reproduce the issue? Thanks!\r\n\r\n@gadagashwini \r\nYeah, so this is the code I am using, the official code from the tensorflow website:\r\nhttps://www.tensorflow.org/lite/guide/get_started\r\n(I have also tried the quntized version of the code, still the same error)\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n```", "I am also getting the same error during TFlite conversion. Any update on this ? \r\n", "@raghavgurbaxani No unfortunately I have not been able to resolve issue, and have not received any further updates from tensorflow either.\r\n@jvishnuvardhan Any response?", "@csongornagy Is this a frozen graph file? If it is frozen graph, then you need to provide 'input_arrays' and 'output_arrays'. Do you have access to model creation code? Can you provide 'input_arrays' and 'output_arrays'. Thanks!", "@jvishnuvardhan \r\nusing the frozen .pb file from here - https://github.com/argman/EAST/issues/296\r\n\r\nbut I get the following error. -\r\n```\r\nRuntimeError: MetaGraphDef associated with tags {'serve'} could not be found in SavedModel. To inspect available tag-sets in the SavedModel, please use the SavedModel CLI: `saved_model_cli`\r\navailable_tags: [set()]\r\n```\r\n\r\nlooks like the tag set is empty. how do I get around this ? ", "> @csongornagy Is this a frozen graph file? If it is frozen graph, then you need to provide 'input_arrays' and 'output_arrays'. Do you have access to model creation code? Can you provide 'input_arrays' and 'output_arrays'. Thanks!\r\n\r\n@jvishnuvardhan \r\nThanks for the reply! Yes it is a frozen graph model! I don't know how I can acquire the input and output arrays as you said. Where could I get these from? or How? This the code use to export the model:\r\n```\r\nimport re\r\nimport numpy as np\r\n\r\noutput_directory = './fine_tuned_model'\r\n\r\nlst = os.listdir(model_dir)\r\nlst = [l for l in lst if 'model.ckpt-' in l and '.meta' in l]\r\nsteps=np.array([int(re.findall('\\d+', l)[0]) for l in lst])\r\nlast_model = lst[steps.argmax()].replace('.meta', '')\r\n\r\nlast_model_path = os.path.join(model_dir, last_model)\r\nprint(last_model_path)\r\n!python /content/models/research/object_detection/export_inference_graph.py \\\r\n    --input_type=image_tensor \\\r\n    --pipeline_config_path={pipeline_fname} \\\r\n    --output_directory={output_directory} \\\r\n    --trained_checkpoint_prefix={last_model_path}\r\n```", "Any updates on this issue?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38080\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38080\">No</a>\n"]}, {"number": 38079, "title": "Use fixed width types consistently", "body": "int32_t* functions were being called with int* arguments. This works on most compilers, but some implementations of stdint.h define int32_t as long since long is at least 32-bits, and _technically_ int is at least 16 bits.\r\n\r\nThis is a portability issue. I have fixed all of the cases that fail with a toolchain that uses long, but I have not exhaustively gone through the codebase for this consistency in order to keep this PR as atomic as possible.\r\n", "comments": ["I should handle these by the weekend", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38079) for more info**.\n\n<!-- need_author_cla -->", "I am guessing the bot is having issues because of Github site-wide problems. \r\n\r\nLet me know if you think I need to take any action on the CLA front.", "You have commits under 2 different email addresses. Both need to sign the CLA", "Also, apologies for the delay in reviewing the PR, it has slipped through the cracks", "@googlebot I fixed it.", "@googlebot I fixed it.", "@googlebot I fixed it.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38079) for more info**.\n\n<!-- ok -->", "That was a treasure hunt. I enjoy seeing the daylight between 'git' and 'github' once in a while.\r\n\r\nNot sure how my changes could have caused the two failing checks.", "Any chance we could re-try those Windows builds? ", "Sure, retriggered them."]}, {"number": 38078, "title": "tf.data.experimental.ignore_errors: failed assertions are not logged", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Colab\r\n- TensorFlow installed from (source or\r\nbinary): binary (colab)\r\n- TensorFlow version (use command below): v2.2.0-rc1-0-gacf4951a2f 2.2.0-rc1\r\n- Python version: 3.6.9\r\n\r\n**Describe the current behavior**\r\n\r\nNo log output is emitted if an assertion fails in a dataset pipeline to which ignore_errors has been applied. While this technically matches the description of \"silently\" ignoring errors, this omission limits the usefulness of ignore_errors severely. It cannot be safely applied without the risk of hiding important errors.\r\n\r\n**Describe the expected behavior**\r\n\r\nLog output is emitted to stderr (visible in colab-jupyter.log).\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\nhttps://colab.research.google.com/gist/Andreas5739738/af7aec733d0190d41aac25c1a062fe1f/ignore_errors.ipynb#scrollTo=pve9Y2tPUDZT\r\n\r\n**Other info / logs**\r\ncolab-jupyter.log shows no failed assertions:\r\n```\r\nMar 31, 2020, 5:57:43 PM | WARNING | 2020-03-31 15:57:43.548945: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version\r\n-- | -- | --\r\nMar 31, 2020, 5:57:43 PM | WARNING | 2020-03-31 15:57:43.548902: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2371640 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\nMar 31, 2020, 5:57:43 PM | WARNING | 2020-03-31 15:57:43.548153: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2300000000 Hz\r\nMar 31, 2020, 5:57:43 PM | WARNING | 2020-03-31 15:57:43.497196: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ae332d19dc9b): /proc/driver/nvidia/version does not exist\r\nMar 31, 2020, 5:57:43 PM | WARNING | 2020-03-31 15:57:43.497116: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\nMar 31, 2020, 5:57:43 PM | WARNING | 2020-03-31 15:57:43.450050: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\nMar 31, 2020, 5:57:41 PM | WARNING | 2020-03-31 15:57:41.766459: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\nMar 31, 2020, 5:57:37 PM | INFO | Adapting to protocol v5.1 for kernel 4e9e6601-f865-41ac-ab32-8a4df7fd32b8\r\nMar 31, 2020, 5:57:35 PM | INFO | Kernel started: 4e9e6601-f865-41ac-ab32-8a4df7fd32b8\r\nMar 31, 2020, 5:33:10 PM | INFO | Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\r\nMar 31, 2020, 5:33:10 PM | INFO | http://172.28.0.2:9000/\r\nMar 31, 2020, 5:33:10 PM | INFO | The Jupyter Notebook is running at:\r\nMar 31, 2020, 5:33:10 PM | INFO | 0 active kernels\r\nMar 31, 2020, 5:33:10 PM | INFO | Serving notebooks from local directory: /\r\nMar 31, 2020, 5:33:10 PM | INFO | google.colab serverextension initialized.\r\nMar 31, 2020, 5:33:10 PM | INFO | Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret\r\nMar 31, 2020, 5:33:09 PM | WARNING | warn(\"IPython.utils.traitlets has moved to a top-level traitlets package.\")\r\nMar 31, 2020, 5:33:09 PM | WARNING | /usr/local/lib/python2.7/dist-packages/IPython/utils/traitlets.py:5: UserWarning: IPython.utils.traitlets has moved to a top-level traitlets package.```", "comments": ["I can work on this with @shahshriya as well!", "i have replicated the issue, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/6de879c5a16c1a0b0a205074b395cd29/38078.ipynb#scrollTo=Qbupa8rr-OPh)", "I think this works as intended. You either want to ignore errors (in which case you use this API) or you don't.", "Can you please describe the use case where you want to use this API and at the same time you do not want to \"ignore\" the errors?", "I think the common use case of ignore_errors is to be robust towards things like some examples not conforming to assumptions, or random data access errors/corruption occuring when loading from unreliable distributed storage systems. You don't want to kill training in such a case. But you also don't want to be oblivious of any such things occuring during training. Ideal would be to store metrics of failures. Logging to stderr would be a simple alternative that would already be very valuable.\r\n\r\nI think a better name to reflect a useful error handling mechanism would be something like \"skip_errors\", instead of \"ignore_errors\". Silently ignoring errors is something that is rarely if ever useful (see the infamous \"try: except: pass\" antipattern in Python).", "Logging the error makes sense. I would be happy to review a contribution that adds an option `log_warning` option for `ignore_errors` that controls whether the error is logged."]}, {"number": 38077, "title": "tflite with select_tf_ops enabled fails to build", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): GNU/Linux aarch64\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source): gcc 7.4.0\r\n\r\n\r\n**Describe the problem**\r\nTrying to build TensorFlow Lite shared library on custom aarch64/arm64 board with select_tf_ops enabled via bazel. Reason being a model which uses an operator that is not yet supported by the standard tflite runtime.\r\n\r\nDuring the build, an error occurs that does not seem to be related to the extended runtime options.\r\n\r\n```\r\nERROR: /workspace/tensorflow/tensorflow/core/BUILD:2310:1: C++ compilation of rule '//tensorflow/core:framework_internal_impl' failed (Exit 1)\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:161:0,\r\n                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/core/framework/tensor.h:21,\r\n                 from ./tensorflow/core/util/tensor_slice_reader.h:25,\r\n                 from tensorflow/core/util/tensor_slice_reader.cc:16:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h: In instantiation of 'TgtPacket Eigen::internal::pcast(const SrcPacket&) [with SrcPacket = __vector(4) int; TgtPacket = __vector(16) unsigned char]':\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h:61:49:   required from 'TgtPacket Eigen::PacketConverter<TensorEvaluator, SrcPacket, TgtPacket, SrcCoeffRatio, TgtCoeffRatio>::packet(Index) const [with int LoadMode = 0; Index = long int; TensorEvaluator = Eigen::TensorEvaluator<const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer>, Eigen::DefaultDevice>; SrcPacket = __vector(4) int; TgtPacket = __vector(16) unsigned char; int SrcCoeffRatio = 1; int TgtCoeffRatio = 1]'\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h:252:53:   required from 'static TargetPacket Eigen::internal::PacketConv<SrcPacket, TargetPacket, LoadMode, true, IsSameT>::run(const Eigen::TensorEvaluator<ArgType, Device>&, Eigen::Index) [with ArgType = const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer>; Device = Eigen::DefaultDevice; SrcPacket = __vector(4) int; TargetPacket = __vector(16) unsigned char; int LoadMode = 0; bool IsSameT = false; Eigen::Index = long int]'\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h:382:63:   required from 'Eigen::TensorEvaluator<const Eigen::TensorConversionOp<TargetType, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorConversionOp<TargetType, XprType>, Device>::packet(Eigen::TensorEvaluator<const Eigen::TensorConversionOp<TargetType, XprType>, Device>::Index) const [with int LoadMode = 0; TargetType = unsigned char; ArgType = const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer>; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorConversionOp<TargetType, XprType>, Device>::PacketReturnType = __vector(16) unsigned char; Eigen::TensorEvaluator<const Eigen::TensorConversionOp<TargetType, XprType>, Device>::Index = long int]'\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h:1418:18:   required from 'static void Eigen::internal::TensorBlockAssignment<Scalar, NumDims, TensorBlockExpr, IndexType>::InnerDimAssign<true, Evaluator>::Run(Scalar*, IndexType, const Evaluator&, IndexType) [with Evaluator = Eigen::TensorEvaluator<const Eigen::TensorConversionOp<unsigned char, const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer> >, Eigen::DefaultDevice>; Scalar = unsigned char; int NumDims = 8; TensorBlockExpr = Eigen::TensorConversionOp<unsigned char, const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer> >; IndexType = long int]'\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h:1523:48:   required from 'static void Eigen::internal::TensorBlockAssignment<Scalar, NumDims, TensorBlockExpr, IndexType>::Run(const Eigen::internal::TensorBlockAssignment<Scalar, NumDims, TensorBlockExpr, IndexType>::Target&, const TensorBlockExpr&) [with Scalar = unsigned char; int NumDims = 8; TensorBlockExpr = Eigen::TensorConversionOp<unsigned char, const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer> >; IndexType = long int]'\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h:174:27:   [ skipping 3 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:206:9:   required from 'static void Eigen::internal::TensorExecutor<Expression, Eigen::DefaultDevice, Vectorizable, (Eigen::internal::TiledEvaluation)1>::run(const Expression&, const Eigen::DefaultDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorSlicingOp<const Eigen::DSizes<long int, 8>, const Eigen::DSizes<long int, 8>, Eigen::TensorMap<Eigen::Tensor<unsigned char, 8, 1, long int>, 0, Eigen::MakePointer> >, const Eigen::TensorConversionOp<unsigned char, const Eigen::TensorSlicingOp<const Eigen::DSizes<long int, 8>, const Eigen::DSizes<long int, 8>, const Eigen::TensorMap<Eigen::Tensor<const int, 8, 1, long int>, 0, Eigen::MakePointer> > > >; bool Vectorizable = true]'\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h:384:65:   required from 'Eigen::TensorSlicingOp<StartIndices, Sizes, XprType>& Eigen::TensorSlicingOp<StartIndices, Sizes, XprType>::operator=(const OtherDerived&) [with OtherDerived = Eigen::TensorConversionOp<unsigned char, const Eigen::TensorSlicingOp<const Eigen::DSizes<long int, 8>, const Eigen::DSizes<long int, 8>, const Eigen::TensorMap<Eigen::Tensor<const int, 8, 1, long int>, 0, Eigen::MakePointer> > >; StartIndices = const Eigen::DSizes<long int, 8>; Sizes = const Eigen::DSizes<long int, 8>; XprType = Eigen::TensorMap<Eigen::Tensor<unsigned char, 8, 1, long int>, 0, Eigen::MakePointer>]'\r\n./tensorflow/core/util/tensor_slice_util.h:51:27:   required from 'static void tensorflow::{anonymous}::CopyThatWorksWithStringPointer<DstT>::Copy(const SrcTensor&, Shape, Shape, DstTensor&, Shape) [with SrcTensor = Eigen::TensorMap<Eigen::Tensor<const int, 8, 1, long int>, 0, Eigen::MakePointer>; DstTensor = Eigen::TensorMap<Eigen::Tensor<unsigned char, 8, 1, long int>, 0, Eigen::MakePointer>; Shape = Eigen::DSizes<long int, 8>; DstT = unsigned char]'\r\n./tensorflow/core/util/tensor_slice_util.h:181:47:   required from 'bool tensorflow::{anonymous}::CopyDataFromTensorSliceToTensorSlice(const tensorflow::TensorShape&, const tensorflow::TensorSlice&, const tensorflow::TensorSlice&, const SrcT*, DstT*) [with SrcT = int; DstT = unsigned char]'\r\n./tensorflow/core/util/tensor_slice_reader.h:184:41:   required from 'bool tensorflow::checkpoint::TensorSliceReader::CopySliceData(const string&, const tensorflow::TensorSlice&, T*) const [with T = unsigned char; std::__cxx11::string = std::__cxx11::basic_string<char>]'\r\ntensorflow/core/util/tensor_slice_reader.cc:264:5:   required from here\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h:145:10: error: invalid static_cast from type 'const __vector(4) int' to type '__vector(16) unsigned char'\r\n   return static_cast<TgtPacket>(a);\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h: In instantiation of 'TgtPacket Eigen::internal::pcast(const SrcPacket&) [with SrcPacket = __vector(4) int; TgtPacket = __vector(8) short int]':\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h:61:49:   required from 'TgtPacket Eigen::PacketConverter<TensorEvaluator, SrcPacket, TgtPacket, SrcCoeffRatio, TgtCoeffRatio>::packet(Index) const [with int LoadMode = 0; Index = long int; TensorEvaluator = Eigen::TensorEvaluator<const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer>, Eigen::DefaultDevice>; SrcPacket = __vector(4) int; TgtPacket = __vector(8) short int; int SrcCoeffRatio = 1; int TgtCoeffRatio = 1]'\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h:252:53:   required from 'static TargetPacket Eigen::internal::PacketConv<SrcPacket, TargetPacket, LoadMode, true, IsSameT>::run(const Eigen::TensorEvaluator<ArgType, Device>&, Eigen::Index) [with ArgType = const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer>; Device = Eigen::DefaultDevice; SrcPacket = __vector(4) int; TargetPacket = __vector(8) short int; int LoadMode = 0; bool IsSameT = false; Eigen::Index = long int]'\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h:382:63:   required from 'Eigen::TensorEvaluator<const Eigen::TensorConversionOp<TargetType, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorConversionOp<TargetType, XprType>, Device>::packet(Eigen::TensorEvaluator<const Eigen::TensorConversionOp<TargetType, XprType>, Device>::Index) const [with int LoadMode = 0; TargetType = short int; ArgType = const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer>; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorConversionOp<TargetType, XprType>, Device>::PacketReturnType = __vector(8) short int; Eigen::TensorEvaluator<const Eigen::TensorConversionOp<TargetType, XprType>, Device>::Index = long int]'\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h:1418:18:   required from 'static void Eigen::internal::TensorBlockAssignment<Scalar, NumDims, TensorBlockExpr, IndexType>::InnerDimAssign<true, Evaluator>::Run(Scalar*, IndexType, const Evaluator&, IndexType) [with Evaluator = Eigen::TensorEvaluator<const Eigen::TensorConversionOp<short int, const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer> >, Eigen::DefaultDevice>; Scalar = short int; int NumDims = 8; TensorBlockExpr = Eigen::TensorConversionOp<short int, const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer> >; IndexType = long int]'\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h:1523:48:   required from 'static void Eigen::internal::TensorBlockAssignment<Scalar, NumDims, TensorBlockExpr, IndexType>::Run(const Eigen::internal::TensorBlockAssignment<Scalar, NumDims, TensorBlockExpr, IndexType>::Target&, const TensorBlockExpr&) [with Scalar = short int; int NumDims = 8; TensorBlockExpr = Eigen::TensorConversionOp<short int, const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer> >; IndexType = long int]'\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h:174:27:   [ skipping 3 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:206:9:   required from 'static void Eigen::internal::TensorExecutor<Expression, Eigen::DefaultDevice, Vectorizable, (Eigen::internal::TiledEvaluation)1>::run(const Expression&, const Eigen::DefaultDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorSlicingOp<const Eigen::DSizes<long int, 8>, const Eigen::DSizes<long int, 8>, Eigen::TensorMap<Eigen::Tensor<short int, 8, 1, long int>, 0, Eigen::MakePointer> >, const Eigen::TensorConversionOp<short int, const Eigen::TensorSlicingOp<const Eigen::DSizes<long int, 8>, const Eigen::DSizes<long int, 8>, const Eigen::TensorMap<Eigen::Tensor<const int, 8, 1, long int>, 0, Eigen::MakePointer> > > >; bool Vectorizable = true]'\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h:384:65:   required from 'Eigen::TensorSlicingOp<StartIndices, Sizes, XprType>& Eigen::TensorSlicingOp<StartIndices, Sizes, XprType>::operator=(const OtherDerived&) [with OtherDerived = Eigen::TensorConversionOp<short int, const Eigen::TensorSlicingOp<const Eigen::DSizes<long int, 8>, const Eigen::DSizes<long int, 8>, const Eigen::TensorMap<Eigen::Tensor<const int, 8, 1, long int>, 0, Eigen::MakePointer> > >; StartIndices = const Eigen::DSizes<long int, 8>; Sizes = const Eigen::DSizes<long int, 8>; XprType = Eigen::TensorMap<Eigen::Tensor<short int, 8, 1, long int>, 0, Eigen::MakePointer>]'\r\n./tensorflow/core/util/tensor_slice_util.h:51:27:   required from 'static void tensorflow::{anonymous}::CopyThatWorksWithStringPointer<DstT>::Copy(const SrcTensor&, Shape, Shape, DstTensor&, Shape) [with SrcTensor = Eigen::TensorMap<Eigen::Tensor<const int, 8, 1, long int>, 0, Eigen::MakePointer>; DstTensor = Eigen::TensorMap<Eigen::Tensor<short int, 8, 1, long int>, 0, Eigen::MakePointer>; Shape = Eigen::DSizes<long int, 8>; DstT = short int]'\r\n./tensorflow/core/util/tensor_slice_util.h:181:47:   required from 'bool tensorflow::{anonymous}::CopyDataFromTensorSliceToTensorSlice(const tensorflow::TensorShape&, const tensorflow::TensorSlice&, const tensorflow::TensorSlice&, const SrcT*, DstT*) [with SrcT = int; DstT = short int]'\r\n./tensorflow/core/util/tensor_slice_reader.h:184:41:   required from 'bool tensorflow::checkpoint::TensorSliceReader::CopySliceData(const string&, const tensorflow::TensorSlice&, T*) const [with T = short int; std::__cxx11::string = std::__cxx11::basic_string<char>]'\r\ntensorflow/core/util/tensor_slice_reader.cc:265:5:   required from here\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h:145:10: error: invalid static_cast from type 'const __vector(4) int' to type '__vector(8) short int'\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h: In instantiation of 'TgtPacket Eigen::internal::pcast(const SrcPacket&) [with SrcPacket = __vector(4) int; TgtPacket = __vector(16) signed char]':\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h:61:49:   required from 'TgtPacket Eigen::PacketConverter<TensorEvaluator, SrcPacket, TgtPacket, SrcCoeffRatio, TgtCoeffRatio>::packet(Index) const [with int LoadMode = 0; Index = long int; TensorEvaluator = Eigen::TensorEvaluator<const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer>, Eigen::DefaultDevice>; SrcPacket = __vector(4) int; TgtPacket = __vector(16) signed char; int SrcCoeffRatio = 1; int TgtCoeffRatio = 1]'\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h:252:53:   required from 'static TargetPacket Eigen::internal::PacketConv<SrcPacket, TargetPacket, LoadMode, true, IsSameT>::run(const Eigen::TensorEvaluator<ArgType, Device>&, Eigen::Index) [with ArgType = const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer>; Device = Eigen::DefaultDevice; SrcPacket = __vector(4) int; TargetPacket = __vector(16) signed char; int LoadMode = 0; bool IsSameT = false; Eigen::Index = long int]'\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h:382:63:   required from 'Eigen::TensorEvaluator<const Eigen::TensorConversionOp<TargetType, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorConversionOp<TargetType, XprType>, Device>::packet(Eigen::TensorEvaluator<const Eigen::TensorConversionOp<TargetType, XprType>, Device>::Index) const [with int LoadMode = 0; TargetType = signed char; ArgType = const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer>; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorConversionOp<TargetType, XprType>, Device>::PacketReturnType = __vector(16) signed char; Eigen::TensorEvaluator<const Eigen::TensorConversionOp<TargetType, XprType>, Device>::Index = long int]'\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h:1418:18:   required from 'static void Eigen::internal::TensorBlockAssignment<Scalar, NumDims, TensorBlockExpr, IndexType>::InnerDimAssign<true, Evaluator>::Run(Scalar*, IndexType, const Evaluator&, IndexType) [with Evaluator = Eigen::TensorEvaluator<const Eigen::TensorConversionOp<signed char, const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer> >, Eigen::DefaultDevice>; Scalar = signed char; int NumDims = 8; TensorBlockExpr = Eigen::TensorConversionOp<signed char, const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer> >; IndexType = long int]'\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h:1523:48:   required from 'static void Eigen::internal::TensorBlockAssignment<Scalar, NumDims, TensorBlockExpr, IndexType>::Run(const Eigen::internal::TensorBlockAssignment<Scalar, NumDims, TensorBlockExpr, IndexType>::Target&, const TensorBlockExpr&) [with Scalar = signed char; int NumDims = 8; TensorBlockExpr = Eigen::TensorConversionOp<signed char, const Eigen::TensorMap<const Eigen::Tensor<int, 8, 1, long int>, 0, Eigen::MakePointer> >; IndexType = long int]'\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h:174:27:   [ skipping 3 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:206:9:   required from 'static void Eigen::internal::TensorExecutor<Expression, Eigen::DefaultDevice, Vectorizable, (Eigen::internal::TiledEvaluation)1>::run(const Expression&, const Eigen::DefaultDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorSlicingOp<const Eigen::DSizes<long int, 8>, const Eigen::DSizes<long int, 8>, Eigen::TensorMap<Eigen::Tensor<signed char, 8, 1, long int>, 0, Eigen::MakePointer> >, const Eigen::TensorConversionOp<signed char, const Eigen::TensorSlicingOp<const Eigen::DSizes<long int, 8>, const Eigen::DSizes<long int, 8>, const Eigen::TensorMap<Eigen::Tensor<const int, 8, 1, long int>, 0, Eigen::MakePointer> > > >; bool Vectorizable = true]'\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h:384:65:   required from 'Eigen::TensorSlicingOp<StartIndices, Sizes, XprType>& Eigen::TensorSlicingOp<StartIndices, Sizes, XprType>::operator=(const OtherDerived&) [with OtherDerived = Eigen::TensorConversionOp<signed char, const Eigen::TensorSlicingOp<const Eigen::DSizes<long int, 8>, const Eigen::DSizes<long int, 8>, const Eigen::TensorMap<Eigen::Tensor<const int, 8, 1, long int>, 0, Eigen::MakePointer> > >; StartIndices = const Eigen::DSizes<long int, 8>; Sizes = const Eigen::DSizes<long int, 8>; XprType = Eigen::TensorMap<Eigen::Tensor<signed char, 8, 1, long int>, 0, Eigen::MakePointer>]'\r\n./tensorflow/core/util/tensor_slice_util.h:51:27:   required from 'static void tensorflow::{anonymous}::CopyThatWorksWithStringPointer<DstT>::Copy(const SrcTensor&, Shape, Shape, DstTensor&, Shape) [with SrcTensor = Eigen::TensorMap<Eigen::Tensor<const int, 8, 1, long int>, 0, Eigen::MakePointer>; DstTensor = Eigen::TensorMap<Eigen::Tensor<signed char, 8, 1, long int>, 0, Eigen::MakePointer>; Shape = Eigen::DSizes<long int, 8>; DstT = signed char]'\r\n./tensorflow/core/util/tensor_slice_util.h:181:47:   required from 'bool tensorflow::{anonymous}::CopyDataFromTensorSliceToTensorSlice(const tensorflow::TensorShape&, const tensorflow::TensorSlice&, const tensorflow::TensorSlice&, const SrcT*, DstT*) [with SrcT = int; DstT = signed char]'\r\n./tensorflow/core/util/tensor_slice_reader.h:184:41:   required from 'bool tensorflow::checkpoint::TensorSliceReader::CopySliceData(const string&, const tensorflow::TensorSlice&, T*) const [with T = signed char; std::__cxx11::string = std::__cxx11::basic_string<char>]'\r\ntensorflow/core/util/tensor_slice_reader.cc:266:5:   required from here\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h:145:10: error: invalid static_cast from type 'const __vector(4) int' to type '__vector(16) signed char'\r\nTarget //tensorflow/lite/c:tensorflowlite_c failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nEnable extended runtime in tensorflow/tensorflow/lite/c/BUILD:\r\n\r\n```\r\ntflite_cc_shared_object(\r\n    name = \"tensorflowlite_c\",\r\n    linkopts = select({\r\n        \"//tensorflow:macos\": [\r\n            \"-Wl,-exported_symbols_list,$(location //tensorflow/lite/c:exported_symbols.lds)\",\r\n        ],\r\n        \"//tensorflow:windows\": [],\r\n        \"//conditions:default\": [\r\n            \"-z defs\",\r\n            \"-Wl,--version-script,$(location //tensorflow/lite/c:version_script.lds)\",\r\n        ],\r\n    }),\r\n    per_os_targets = True,\r\n    deps = [\r\n        \":c_api\",\r\n        \":c_api_experimental\",\r\n        \":exported_symbols.lds\",\r\n        \":version_script.lds\",\r\n\t\"//tensorflow/lite/delegates/flex:delegate\",\r\n    ],\r\n)\r\n```\r\nInvoke the build command from the tf root dir:\r\n`bazel build --config=monolithic --define=with_select_tf_ops=true -c opt //tensorflow/lite/c:tensorflowlite_c`\r\n\r\nSomething worth mentioning is that the default tflite runtime can be built on this system, so its probably not an issue of the bazel installation and/or the platform. Help of any kind is appreciated.", "comments": ["it looks like that the\r\n```\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h:145:10: error: invalid static_cast from type 'const __vector(4) int' to type '__vector(16) unsigned char'\r\n```\r\nerror is caused by 6e69a4c, that is, newer Eigen.\r\n\r\nan interim solution is to revert 6e69a4c (e.g., `git show 6e69a4c | patch -p1 -R`).", "@freedomtan \r\nThanks for your suggestion, the build just took a while as the board is not the fastest one around.\r\n\r\nAfter 11 hours of compiling, I get another error. \r\n\r\n```\r\nERROR: /root/.cache/bazel/_bazel_root/2c92b5569ddded7b3a6bd5e139451b60/external/aws-c-common/BUILD.bazel:12:1: C++ compilation of rule '@aws-c-common//:aws-c-common' failed (Exit 1)\r\nexternal/aws-c-common/source/arch/encoding_avx2.c:16:10: fatal error: emmintrin.h: No such file or directory\r\n #include <emmintrin.h>\r\n          ^~~~~~~~~~~~~\r\ncompilation terminated.\r\nTarget //tensorflow/lite/c:tensorflowlite_c failed to build\r\nINFO: Elapsed time: 39667.574s, Critical Path: 5512.88s\r\nINFO: 3739 processes: 3739 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\nAlso, do I have to run the configure script every time a patch is applied?", "@DocDriven if you are not using AWS, add `--config noaws` to your bazel build command, that is, do something like \r\n```\r\nbazel build --config=monolithic \\\r\n--define=with_select_tf_ops=true \\\r\n-c opt //tensorflow/lite/c:tensorflowlite_c \\\r\n--config noaws\r\n```\r\n\r\nAnd, NOPE, you don't need run `./configure` if you don't change your configuration.", "@freedomtan Thanks, that did the trick for me. I do not need AWS, so after using `--config noaws` in the build command, the build succeeded.\r\n\r\nIssue is resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38077\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38077\">No</a>\n", "@DocDriven glad it helped", "@freedomtan \r\n\r\nYou code run successfully, but **NO pip wheel** file is generated...\r\n\r\n```\r\nbazel build --config=monolithic \\\r\n--define=with_select_tf_ops=true \\\r\n-c opt //tensorflow/lite/c:tensorflowlite_c \\\r\n--config noaws\r\n```", "> @freedomtan\r\n> \r\n> You code run successfully, but **NO pip wheel** file is generated...\r\n> \r\n> ```\r\n> bazel build --config=monolithic \\\r\n> --define=with_select_tf_ops=true \\\r\n> -c opt //tensorflow/lite/c:tensorflowlite_c \\\r\n> --config noaws\r\n> ```\r\n\r\nThats because the build command is for **tensorflowlite_c**. Use `bazel build //tensorflow/tools/pip_package:build_pip_package` for building the pip package.\r\n\r\nFrom: https://www.tensorflow.org/install/source"]}]