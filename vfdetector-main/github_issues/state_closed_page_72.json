[{"number": 53050, "title": "H\u1ee3p nh\u1ea5t th\u00e0nh c\u00f4ng m\u1ed9t y\u00eau c\u1ea7u k\u00e9o c\u00f3 th\u1ec3 \u0111\u00f3ng v\u1ea5n \u0111\u1ec1 n\u00e0y", "body": "https://github.com/tensorflow/tensorflow/blob/7e5b561df1f1dbcba9216e662c345eccb46b3048/tensorflow/go/session_test.go#L252", "comments": ["@lam19892089 ,\r\nWe see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced] or if possible share a colab gist with the issue reported.\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53049, "title": "Tensorflow using Intel UHD graphics instead of Nvidia", "body": "While training my model tensorflow is using the Intel UHD graphics instead of Nvidia RTX 2060. But the thing is even if utilization is 0% the RTX memory is being used.\r\ntensorflow = 2.7.0\r\ncudaa = 11.2\r\ncudnn = 8.1\r\n![Screenshot 2021-11-13 091117](https://user-images.githubusercontent.com/69339222/141604087-f1d98ea3-3713-48e6-8c0b-b3a6a39fbdf6.png)", "comments": ["Hi @Abhinav-Bhattarai! Could you try again specifying GPU device  using [tf.device ](https://www.tensorflow.org/guide/gpu)?", "It started to use my Nvidia graphics for a bigger dataset", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53049\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53049\">No</a>\n"]}, {"number": 53048, "title": "Fix some ROCm related typos.", "body": "@cheshire @chsigg @jurahul ", "comments": []}, {"number": 53046, "title": "Different variable declaration for `inner_shape` and `outer_shape` in `Slice updates` section of `tensor_scatter_nd_update` ", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_update#slice_updates\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe `inner_shape` and `outer_shape` variables should have the definition of :\r\n\r\n`inner_shape = tensor.shape[index_depth:]`\r\n`outer_shape = tensor.shape[:index_depth]`\r\n\r\n### Clear description\r\n\r\nIn the first code block of the section the variables `inner_shape` and `outer_shape` are defined as:\r\n\r\n`inner_shape = tensor.shape[:index_depth]`\r\n`outer_shape = tensor.shape[index_depth:]`\r\n\r\nIn the fourth code block the same variables are defined as:\r\n\r\n`inner_shape = tensor.shape[index_depth:]`\r\n`outer_shape = tensor.shape[:index_depth]`\r\n\r\nNote that in the two definitions the slices are used inversely for every variable.\r\nHowever I believe that the definition of the fourth block is the correct one, namely\r\n\r\n`inner_shape = tensor.shape[index_depth:]`\r\n`outer_shape = tensor.shape[:index_depth]`\r\n\r\n\r\n#### Code example\r\n\r\nBy replicating the example in the `Slice updates` section:\r\n\r\n* the `outer_shape` should be 6 \r\n* the `inner_shape` should be 3\r\n\r\nthat is achieved with the definition\r\n\r\n`inner_shape = tensor.shape[index_depth:]`\r\n`outer_shape = tensor.shape[:index_depth]`\r\n\r\n\r\nBelow I provide the code with the corresponding output\r\n\r\n```\r\ntensor = tf.zeros([6,3], dtype=tf.int32)\r\nindices = tf.constant([[2],[4]])\r\nnum_updates, index_depth = indices.shape.as_list()\r\nprint(f\"num_updates == {num_updates}, index_depth == {index_depth}\")\r\nprint(\"\")\r\n\r\nprint(\"The outer_shape should be 6 and the inner_shape should be 3\")\r\nprint(\"\")\r\n      \r\nprint(\"If we use the definition of the first code block\")\r\nouter_shape_1st = tensor.shape[index_depth:]\r\ninner_shape_1st = tensor.shape[:index_depth]\r\nprint(f\"outer_shape is {outer_shape_1st[0]}, the inner_shape is {inner_shape_1st[0]}\")      \r\nprint(\"\")\r\n\r\n\r\nprint(\"If we use the definition of the forth code block\")\r\nouter_shape_4th = tensor.shape[:index_depth]\r\ninner_shape_4th = tensor.shape[index_depth:]\r\nprint(f\"outer_shape is {outer_shape_4th[0]}, the inner_shape is {inner_shape_4th[0]}\")\r\n```\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/35838787/141491130-d6f95932-214b-4057-8034-6652845c8894.png)\r\n\r\n\r\n", "comments": ["@inpap ,\r\nWe see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyse the issue.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53045, "title": "SpaceToDepth and DepthToSpace were not named as expected", "body": "Hello,\r\n\r\nIn the API, we can give a name for the SpaceToDepth (or DepthToSpace) operator. However, I found that whatever the name I provided, tensorflow always fixes the name of SpaceToDepth as **tf.nn.space_to_depth** (and the same problem for DepthToSpace).\r\n\r\nI'm using Windows 10, version 21H1, build 19043.1348 and Tensorflow 2.6\r\n\r\nYou can easily reproduce the issue with a minimal example as follows:\r\n```\r\nimport tensorflow as tf\r\nimport json\r\n\r\ndef sample_network(input_layer):\r\n    s2d = tf.nn.space_to_depth(input_layer, block_size=2, name=\"Space2Depth\")\r\n    d2s = tf.nn.depth_to_space(s2d, block_size=2, name=\"Depth2Space\")\r\n    return d2s\r\n\r\nif __name__ == \"__main__\":\r\n    input_net = tf.keras.Input(shape=(64, 64, 3), dtype=tf.float32)\r\n    output = sample_network(input_net)\r\n    model = tf.keras.Model(inputs=input_net, outputs=output)\r\n    model_json = json.loads(model.to_json())\r\n    with open(\"github_issue.json\", \"w\") as f:\r\n        json.dump(model_json, f, indent=2)\r\n```\r\nI would expect the name of these operators are Space2Depth and Depth2Space, but they are not.\r\n![DepthToSpace](https://user-images.githubusercontent.com/12828532/141489907-6f5d4263-abf1-4ac7-b117-e650bcbae8ac.PNG)\r\n![SpaceToDepth](https://user-images.githubusercontent.com/12828532/141489910-c9d24747-fcc6-459a-9899-a939655ed544.PNG)\r\nThank you for your consideration.\r\n", "comments": ["Hi @dvtran221! Could you try again using [tf.name_scope](https://www.tensorflow.org/api_docs/python/tf/name_scope) in the above code ? Attaching  relevant threads for reference.[link1](https://colab.sandbox.google.com/github/zaidalyafeai/Notebooks/blob/master/WeightTransfer.ipynb),[link2](https://stackoverflow.com/questions/42708989/why-do-we-use-tf-name-scope/42712602). Thank you!", "Hello @mohantym. I tried the tf.name_scope (as follows) but it does not change anything. I always observed the name tf.nn.space_to_depth and tf.nn.depth_to_space. As in the documentation, I expected they should be Space2Depth/s2d1 and Depth2Space/d2s1, but it's not the case.\r\n```\r\ndef sample_network(input_layer):\r\n    with tf.name_scope(\"Space2Depth\"):\r\n        s2d = tf.nn.space_to_depth(input_layer, block_size=2, name=\"s2d1\")\r\n    with tf.name_scope(\"Depth2Space\"):\r\n        d2s = tf.nn.depth_to_space(s2d, block_size=2, name=\"d2s1\")\r\n    print(s2d.name, d2s.name)  # print out: tf.nn.space_to_depth/SpaceToDepth:0 tf.nn.depth_to_space/DepthToSpace:0\r\n    return d2s\r\n```\r\nThank you!\r\n", "Hi @dvtran221!\r\nPlease post this issue on [keras-team/keras repo](https://github.com/keras-team/keras/issues) too.\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) . Thanks!", "Hello @mohantym \r\n\r\nI opened an issue in keras repo, you can see the link as below. \r\nI just discovered that if I disable the eager mode (by adding tf.compat.v1.disable_eager_execution()), the names are expected. I have an impression that tensorflow adds the name of operator (e.g. tf.nn.space_to_depth) as a prefix. I observed that other operators have the same problems (for example, the tf.multiply). I don't know what happen but I hope this can give you some information. \r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nimport json\r\n\r\ndef sample_network(input_layer):\r\n    s2d = tf.nn.space_to_depth(input_layer, block_size=2, name=\"Space2Depth\")\r\n    mul = tf.multiply(s2d, 10.0, name=\"Multiplication\")\r\n    d2s = tf.nn.depth_to_space(mul, block_size=2, name=\"Depth2Space\")\r\n    print(s2d.name, d2s.name, mul.name) \r\n    return d2s\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    # Disable eager mode\r\n    tf.compat.v1.disable_eager_execution()\r\n\r\n    input_net = tf.keras.Input(shape=(64, 64, 3), dtype=tf.float32, name=\"inputLayer\")\r\n    output = sample_network(input_net)\r\n    model = tf.keras.Model(inputs=input_net, outputs=output)\r\n    for layer in model.layers:\r\n        print(layer.name)\r\n```\r\n\r\nWithout tf.compat.v1.disable_eager_execution(), it prints out:\r\n```\r\ntf.nn.space_to_depth/SpaceToDepth:0 tf.nn.depth_to_space/DepthToSpace:0 tf.math.multiply/Mul:0\r\nkeras: inputLayer\r\nkeras: tf.nn.space_to_depth\r\nkeras: tf.math.multiply\r\nkeras: tf.nn.depth_to_space\r\n```\r\nAdding tf.compat.v1.disable_eager_execution():\r\n```\r\nSpace2Depth:0 Depth2Space:0 Multiplication:0\r\nkeras: inputLayer\r\nkeras: tf_op_layer_Space2Depth\r\nkeras: tf_op_layer_Multiplication\r\nkeras: tf_op_layer_Depth2Space\r\n```\r\n\r\n", "@dvtran221 ! Thanks for the confirming the same . Please close issue this here as It will be tracked in Keras repo.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53045\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53045\">No</a>\n"]}, {"number": 53044, "title": "[TFLite] Enable ApplyLazyDelegateProviders in TF-Lite Python Interpreter.", "body": "Fixed #53042 .\r\nEnable XNNPACK delegate in TensorFlow Lite Python Interpreter.", "comments": ["Hi, \r\n\r\nThanks a lot for reporting the issue and trying to put together a fix. \r\nAt a glance, it seems to me that the `ApplyLazyDelegateProviders` function is designed to be called automatically inside the `Interpreter` class and the function should probably not even be public. \r\n\r\nI'll try to find someone who is more familiar with this piece of code to review your pull request. ", "Thank you for your comment.\r\n\r\nThe InterpreterWrapper was a friend class, so I called the private method. It would have been better not to call it.\r\n\r\nI understand that the issue will be resolved. Thank you very much. I'll wait.", "> Thank you for your comment.\r\n> \r\n> The InterpreterWrapper was a friend class, so I called the private method. It would have been better not to call it.\r\n> \r\n> I understand that the issue will be resolved. Thank you very much. I'll wait.\r\n\r\nPls check out https://github.com/tensorflow/tensorflow/commit/365a3b68471f158defc3aea79a25fdaa56be4ac8 which should have fixed the issue. Thx again for catching the bug!"]}, {"number": 53043, "title": "lite Benchmark_Model build failed on ARM platform without hexgon", "body": "I'm trying to build benchmark_model with NXP M865 arm dev board, it was successfuly with v2.6.0, but failed with v2.7.0.\r\n\r\nI found it was because this patch:\r\nhttps://github.com/tensorflow/tensorflow/commit/feb49693266f444d5d8ce1a439ffbe7ff6e15e8a\r\n\r\nThis patch enabled hexgon delegate on all ARM platform, this is not make sense. Please consider rework it make sure it only compiled with platform with qualcomm chip.\r\n\r\n\r\n", "comments": ["@sunshinemyson Could you please refer to the [comment ](https://github.com/tensorflow/tensorflow/issues/52895#issuecomment-964268999) and open this issue in [TF forum](https://discuss.tensorflow.org/) where there is a larger community to help? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53043\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53043\">No</a>\n"]}, {"number": 53042, "title": "XNNPACK delegate not enabled in TensorFlow Lite Python Interpreter.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 (x64) and Raspberry Pi OS 64bit (raspios_arm64-2021-04-09)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Raspberry Pi 4\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): v2.7.0 and 7b290f9fd9fbf2ac4352b3cbe327e1067e5a3574\r\n- Python version: 3.7.3 (Raspberry Pi OS 64bit)\r\n- Bazel version (if compiling from source): - (Build with CMake)\r\n- GCC/Compiler version (if compiling from source): 8.3.0 (Raspberry Pi OS 64bit)\r\n- CUDA/cuDNN version: - \r\n- GPU model and memory: -\r\n\r\n**Describe the current behavior**\r\nXNNPACK delegate not enabled in TensorFlow Lite Python Interpreter. \r\nBuilding with either CMake or Bazel will not take effect.\r\nThe following log is not output.\r\n> INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\n  \r\n  \r\nDelegate lazy initialization was included in the 3d3c6db1ca2d50f6f07722cd800144f8f736167c commit.\r\nFor C ++ IF, Interpreter::AllocateTensors calls ApplyLazyDelegateProviders to enable the XNNPACK delegate.\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.7.0/tensorflow/lite/interpreter.cc#L176\r\n\r\nHowever, for Python IF, the XNNPACK delegate is not enabled because ApplyLazyDelegateProviders is not called in InterpreterWrapper::AllocateTensors.\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.7.0/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L259\r\n\r\n**Describe the expected behavior**\r\nThe XNNPACK delegate is enabled in the TensorFlow Lite Python Interpreter.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): Yes\r\n- Briefly describe your candidate solution(if contributing): \r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n", "comments": ["@NobuoTsukamoto ,\r\nThe issue will move to closed status once the PR is merged.", "Many thanks for catching this issue and contributing the fix! Just posted a comment to https://github.com/tensorflow/tensorflow/pull/53044/", "I was able to delegate XNNPACK with 365a3b68471f158defc3aea79a25fdaa56be4ac8 commits.\r\n- I built it with build_pip_package_with_cmake.sh on Raspberry Pi OS 64 bit.\r\n- I confirmed that the following log is output.\r\n  INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\n- The value specified for the num_threads argument of tf.lite.Interpreter was specified in the thread pool of XNNPACK.\r\n\r\n\r\nThank you very much. I would like to close this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53042\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53042\">No</a>\n"]}, {"number": 53041, "title": "TF/MLIR: lhlo to linalg conversion for BroadcastInDimOp - invalid IR", "body": "This is from the latest Tensorflow git repo 33dc8e4b4d4115b308cc55bd99ed5a9cd8b36b4c (Nov 10)\r\n\r\nOn the input:\r\n\r\n```\r\n  func @main(%in: memref<1x40x1xf16>, %out: memref<1x40x8xf16>) {\r\n    \"lmhlo.broadcast_in_dim\"(%%in, %out) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (memref<1x40x1xf16>, memref<1x40x8xf16>) -> ()\r\n    return\r\n  }\r\n```\r\n\r\n```\r\n$ tf-opt -lhlo-legalize-to-linalg lmhlo.broadcast_in_dim.mlir \r\nlmhlo.broadcast_in_dim.mlir:7:5: error: 'memref.collapse_shape' op expected collapsed type to be 'memref<1x40xf16>', but got 'memref<1x40xf16, affine_map<(d0, d1) -> (d0 * 40 + d1)>>'\r\n    \"lmhlo.broadcast_in_dim\"(%arg3, %arg5) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (memref<1x40x1xf16>, memref<1x40x8xf16>) -> ()\r\n    ^\r\nlmhlo.broadcast_in_dim.mlir:7:5: note: see current operation: %2 = \"memref.collapse_shape\"(%arg3) {reassociation = [[0], [1, 2]]} : (memref<1x40x1xf16>) -> memref<1x40xf16, affine_map<(d0, d1) -> (d0 * 40 + d1)>>\r\n```\r\n\r\nThe IR generated is invalid here. \r\n\r\nSeparately, but related to the reshaping being introduced here: it was introduced by this commit:\r\n\r\n```\r\ncommit 967782ff2a870fbc82e0db99c04eae78dded61d7\r\nAuthor: Alexander Belyaev <pifon@google.com>\r\nDate:   Fri Jun 5 06:40:28 2020 -0700\r\n\r\n    [XLA][MLIR] Insert linalg.reshape when lowering LHLO BroadcastInDimOp.\r\n    \r\n    If size-1 dimensions require expansion, we insert linalg.reshape to get rid of them.\r\n    \r\n    PiperOrigin-RevId: 314918927\r\n    Change-Id: I3e9cd380b93421858343ba299e659c0396ca7bcf\r\n```\r\n\r\nThe reshaping isn't actually needed here. Was the reason for the reshape to preserve some canonical form for linalg.generic? In that case, adding a pattern on the latter would be a much better separation and placement of concerns without the need to add such code in the lowering pattern.\r\n\r\nThe rationale for inserting the reshape isn't clear and not summarized on the commit. The above example is just a broadcast via 1x40x1 -> 1x40x8 which can and was easily handled by mapping to `linalg.generic` prior to this commit (without any extra reshapes). The above commit split the converter templated for both hlo and lhlo while adding a large amount of code to introduce the reshape. The `linalg.reshape` itself was subsequently specialized to `collapse/expand` and moved to the memref dialect (`memref.collapse/expand_shape`). After a lowering to loops,  these reshapes on memrefs would get in the way of transformations like affine fusion unless they are eliminated via \"in-placing\"/special fusion (which is separately useful to anyway exist); in the absence of such inplacing, they would just get in the way of other downstream transformations like affine fusion for example. For the lmhlo.broadcast_in_dim lowering itself, the pre-existing lowering was simple.\r\n\r\nCC: @pifon2a @joker-eph @jpienaar \r\n", "comments": ["Hi @sanatmpa1! Could you please look at this issue?", "@bondhugula \r\nUday, is this still relevant? there is no lowering from LHLO to Linalg anymore.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "As @pifon2a comments, this has now become moot.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53041\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53041\">No</a>\n"]}, {"number": 53040, "title": "How do I set up multi capabilities when run the configure file", "body": "gpu \uff1a one gtx 1060\r\nsystem: linux\r\nsource code: tensorflow/r1.15\r\nexpect capability: 5.2,6.0,6.1,7.0,7.5\r\n\r\nI have only one gpu, but i want to set mutil capabilities  when build libtensorflow.\r\n\r\n![\u4f01\u4e1a\u5fae\u4fe1\u622a\u56fe_1636692704760](https://user-images.githubusercontent.com/61573829/141411648-b523408a-ca36-42b0-8956-4b87b02c4816.png)\r\n", "comments": ["input   `5.2,6.0,6.1,7.0,7.5` or `[5.2,6.0,6.1,7.0,7.5]` or others?", "@Zhangts98 \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose), \r\nCan you please share the output [please share text in place of screenshots which will help other users as well].We see that you're using TF v1.15 which is out of support window ,so could you please try to upgrade to TF v2.7.0 and let us know the outcome?\r\nThanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53040\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53040\">No</a>\n"]}, {"number": 53039, "title": "How to specify the output layers during model conversion to TFLite in TF2?", "body": "**Describe the problem**\r\nIs there any way to remove the TFLite_Detection_PostProcess in tf2 ssd mobilenet model during conversion to tflite model\uff1fIn tf1, there is a option to do so, but it seems that in tf2 the option for specifying the output layers is removed. \r\n\r\nIn tf1, I can specify the output layers by putting the layer's name in output_array:\r\ninput_arrays=[\"normalized_input_image_tensor\"]\r\noutput_arrays=[\"raw_outputs/box_encodings\",\"raw_outputs/class_predictions\"]\r\ninput_tensor={\"normalized_input_image_tensor\":[1,320,320,1]}\r\n\r\nimport tensorflow as tf\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph('tflite_graph.pb',input_shapes = input_tensor,\r\ninput_arrays = input_arrays ,output_arrays = output_arrays)\r\n\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.allow_custom_ops = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8,tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.inference_input_type = tf.int8\r\nconverter.inference_output_type = tf.int8\r\nconverter.representative_dataset = representative_dataset_gen\r\ntflite_model = converter.convert()\r\nopen('ssd_mobilenet_v2_tf1.tflite', \"wb\").write(tflite_model)\r\n![with postprocess](https://user-images.githubusercontent.com/57311716/141403792-f97cb470-c8bb-4174-891a-1ffec7b6f402.jpg)\r\n\r\n\r\n![without postprocess](https://user-images.githubusercontent.com/57311716/141403817-7016ccf9-1249-4eda-ad56-717de818e229.jpg)\r\n\r\n\r\n", "comments": ["@qqwerter ,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and tensorflow version to reproduce the issue reported here.", "Also please take a look at this [link](https://www.tensorflow.org/lite/convert) which provides information on tflite converter.It helps.Thanks!", "The `TFLiteConverter` API in TensorFlow 2.x no longer supports specifying inputs & outputs. \r\nThe recommended approach in TensorFlow 2.x is to build `tf.function`s (or equivalent signatures in SavedModel) when building the TensorFlow model, and convert it to TensorFlow as is. \r\nIf you needs to deal with a legacy model, you can still use the `tf.compat.v1.lite.TFLiteConverter` API. \r\n\r\nLet us know if this answers your question. Please let us know if there's a concrete use case that cannot be supported now. "]}, {"number": 53038, "title": "built-in method __contains__ of dict object at 0xffff8b79a200  on aarch64\uff0c ubuntu20.04\uff0c tf\uff1av2.5.0\uff0c py: v3.8.0", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution\uff1aUbuntu 20.04):\r\n- Mobile device \uff1aaarch64\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version (use command below): V2.5.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n\r\n\r\nYou can collect some of this information using our environment capture\uff1a\r\n\r\nPython 3.8.11 (default, Aug  6 2021, 14:51:49) \r\n[GCC 10.2.0] :: Anaconda, Inc. on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from tensorflow.keras import models,layers\r\nRuntimeError: module compiled against API version 0xe but this version of numpy is 0xd\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/root/miniconda3/envs/mlsql/lib/python3.8/site-packages/tensorflow/__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"/root/miniconda3/envs/mlsql/lib/python3.8/site-packages/tensorflow/python/__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"/root/miniconda3/envs/mlsql/lib/python3.8/site-packages/tensorflow/python/eager/context.py\", line 37, in <module>\r\n    from tensorflow.python.client import pywrap_tf_session\r\n  File \"/root/miniconda3/envs/mlsql/lib/python3.8/site-packages/tensorflow/python/client/pywrap_tf_session.py\", line 23, in <module>\r\n    from tensorflow.python._pywrap_tf_session import *\r\nImportError: SystemError: <built-in method __contains__ of dict object at 0xffff8b79a200> returned a result with an error set\r\n>>> ", "comments": ["Hi @hecheng64 ! Could you try again upgrading your numpy using below command ?                                                                  `pip install numpy --upgrade` , Attaching Reference .[link1](https://issueexplorer.com/issue/google-research/ravens/15),[link2](https://github.com/freqtrade/freqtrade/issues/4281#issuecomment-770288601). Thanks!", "@mohantym tks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53038\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53038\">No</a>\n"]}, {"number": 53037, "title": "The qna model demo does not return answer results", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**Describe the current behavior**\r\nIn trying to run the qna model demo for TensorFlow.js I found that the answers always return an empty array. I did not alter any code I ran the out of the box demo direct from the Github page. I left the default information about Nikola Tesla in place and asked the question `Who is Tesla`. In the Chrome console I saw an Array with 0 results.\r\n\r\n**Describe the expected behavior**\r\nI expect answers to be returned by the model and output to the browser.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): No\r\n\r\n**Standalone code to reproduce the issue**\r\nThe reproducible example exists at the link provided from Github.\r\n\r\nhttps://storage.googleapis.com/tfjs-models/demos/mobilebert-qna/index.html\r\n\r\nThank you.", "comments": ["@torressam333 This issue is more related to **TensorFlow.js** .Could you please post the issue in [TensorFlow.js](https://github.com/tensorflow/tfjs/issues) repo  ?Thank you!", "@sushreebarsa absolutely, I will do that. Thank you.", "@torressam333 Thank you for the response! \r\nPlease move this ticket to closed status  once you open the issue in [TF.js](https://github.com/tensorflow/tfjs/issues) repo.  Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53037\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53037\">No</a>\n"]}, {"number": 53036, "title": "Facilitation of the cross-compiled unit test run on target", "body": "As of now, _-DTFLITE_KERNEL_TEST_ along with _-DTFLITE_HOST_TOOLS_DIR=DIR_ arguments can be used for the TF Lite unit tests cross-compilation using CMake. This builds ~136 unit test executables for target architecture and the description of the corresponding ~136 test runs of the CTest utility.\r\n\r\nThis PR extends this feature by **adding delegate tests** (based on the same executables) **for every enabled delegate in the CMake build run** (XNNPACK, NNAPI, external).\r\n\r\nAs propagation of environment variables to specific tests is not supported by CTest, a new helper CMake module is introduced to work around this (see the updated README file for details).", "comments": ["@terryheo Could you help to review CMake related code changes (or triage?) Thanks!", "@terryheo Can you please review this PR ? Thanks!", "@terryheo additional reviewers were automatically added due to my mistake (already fixed) in branching, please remove them. Sorry for the inconvenience."]}, {"number": 53035, "title": "Fix definition of triplet for AARCH64", "body": "Fixes https://github.com/tensorflow/tensorflow/issues/53034", "comments": []}, {"number": 53034, "title": "unit test //tensorflow/compiler/xla/service/cpu/tests:cpu_literal_caching_test fails to build on AARCH64", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: git HEAD\r\n- Python version: 3.6.9\r\n- Installed using virtualenv? pip? conda?: no\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 10.3.0\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nBuild fails with\r\nExecution platform: @local_execution_config_platform//:platform\r\nIn file included from ./tensorflow/compiler/xla/service/backend.h:31,\r\n                 from ./tensorflow/compiler/xla/tests/hlo_test_base.h:26,\r\n                 from ./tensorflow/compiler/xla/tests/codegen_test_base.h:24,\r\n                 from ./tensorflow/compiler/xla/tests/llvm_irgen_test_base.h:22,\r\n                 from ./tensorflow/compiler/xla/service/cpu/tests/cpu_codegen_test.h:19,\r\n                 from tensorflow/compiler/xla/service/cpu/tests/cpu_literal_caching_test.cc:18:\r\n./tensorflow/compiler/xla/service/transfer_manager.h:37:1: error: expected unqualified-id before 'namespace'\r\n   37 | namespace xla {\r\n      | ^~~~~~~~~\r\nIn file included from ./tensorflow/compiler/xla/tests/hlo_test_base.h:26,\r\n                 from ./tensorflow/compiler/xla/tests/codegen_test_base.h:24,\r\n                 from ./tensorflow/compiler/xla/tests/llvm_irgen_test_base.h:22,\r\n                 from ./tensorflow/compiler/xla/service/cpu/tests/cpu_codegen_test.h:19,\r\n                 from tensorflow/compiler/xla/service/cpu/tests/cpu_literal_caching_test.cc:18:\r\n./tensorflow/compiler/xla/service/backend.h:97:3: error: 'TransferManager' does not name a type\r\n   97 |   TransferManager* transfer_manager() const { return transfer_manager_; }\r\n      |   ^~~~~~~~~~~~~~~\r\n./tensorflow/compiler/xla/service/backend.h:164:11: error: 'TransferManager' has not been declared\r\n  164 |           TransferManager* transfer_manager,\r\n      |           ^~~~~~~~~~~~~~~\r\n./tensorflow/compiler/xla/service/backend.h:172:3: error: 'TransferManager' does not name a type\r\n  172 |   TransferManager* transfer_manager_;\r\n      |   ^~~~~~~~~~~~~~~\r\nINFO: Elapsed time: 707.544s, Critical Path: 172.52s\r\nINFO: 4160 processes: 63 internal, 4097 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n$ bazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --remote_http_cache=\"\"  --remote_cache_proxy=\"\" --noremote_accept_cached --config=nonccl --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --copt=-ffp-contract=off --cxxopt=-ffp-contract=off --copt=-Og --copt=-ggdb --cxxopt=-Og --cxxopt=-ggdb --verbose_failures -- //tensorflow/compiler/xla/service/cpu/...\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@cfRod @nSircombe ", "@elfringham ,\r\nThe issue will move to closed status once the PR is merged.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53034\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53034\">No</a>\n"]}, {"number": 53033, "title": "The Problem of \"UpSamples2D\u201c Layer when transfering h5 model file to tflite model file\u3002", "body": "### 1. System information\r\n\r\n- Windows 10\r\n- pip package\r\n- '2.3.1'\r\n\r\n### 2. Code\r\n\r\n    print('<-------------------------------------\u4fdd\u5b58\u4e3atflite\u6587\u4ef6:------------------------------------>\\n')\r\n    new_model = tf.keras.models.load_model(pbfile)  # path to the SavedModel directory\r\n    new_model.summary()\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(new_model)\r\n    tflite_model = converter.convert()\r\n\r\n    # Save the model.\r\n    with open(tflitefile, 'wb') as f:\r\n        f.write(tflite_model)\r\n\r\n### 3. Confused of This Code's Result\r\nThe Above Code can Successfully converted h5 model to tflite model file, But I'm Confused of the result of tflite model\r\nwhen \"UpSamples2D\u201c Layer is showed in h5 model file as following:\\\r\n```\r\n       |\r\n      \\|/\r\nUpSampling2D\r\n       |\r\n      \\|/\r\n```\r\nbut is showed in tflite model file as following:\r\n```\r\n               |\r\n              \\|/\r\n       |                  |\r\n      \\|/                |\r\n    shape           \r\n       |                  |\r\n      \\|/                |\r\n  StridedSlice    \r\n       |                  |\r\n      \\|/                |\r\n      Mul             \r\n       |                  |\r\n      \\|/                \\|/\r\n   ResizeNearestNeighbor\r\n               |\r\n              \\|/\r\n```\r\nProblem: Why UpSampling2D is divided into two route calculations?\r\n", "comments": ["Hi @pengzhikang! Could you update the Error stack trace in above template too?", "I have update comment, please Help me check the problem.", "Hi @sachinprasadhs! Could you look at this issue?", "Could you please provide the sample reproducible code to investigate on the issue. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53033\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53033\">No</a>\n"]}, {"number": 53032, "title": "Why MutableDenseHashTableV2 in tensorflow 1.10 but gone in 1.15 ?", "body": "There is some questions to ask about the file  tensorflow/core/ops/lookup_ops.cc, I see in v1.10 there is two MutableDenseHashTable, MutableDenseHashTable and MutableDenseHashTableV2, both with the same input and attributes but returning different types of output.\r\n\r\nBut in the version of 1.15, there is only MutableDenseHashTable with MutableDenseHashTableV2 deleted.\r\n\r\nMay I ask is there some OP which can substitute MutableDenseHashTableV2 in tensorflow 1.15 ?\r\n", "comments": ["@annarev @rohan100jain ", "@venusw2u We see that you are using `1.15` which is out of support window .Could you please upgrade to latest `TF v2.7.0 `and refer to the [page](https://www.tensorflow.org/api_docs/python/tf/lookup/experimental/MutableHashTable) for more details ? Let us know if it helps! Thank you!", "But, we have already actually used tf1.15 in our production environment which can not be replaced easily. @sushreebarsa \r\nCould someone can ask this question ?", "@venusw2u \r\nTF version 1.x is  not actively supported so could you please open this issue in [TF discussion forum](https://discuss.tensorflow.org/) as there is a larger community to get you the right help ?Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53031, "title": "Unable to install Tensorflow through Pip due to no versions to install from", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro and Linux Ubuntu 20.04 LTS (through WSL2)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Moto G Stylus (Running on Termux)\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version: 2.6.2, 2.7.0\r\n- Python version: 3.10.0\r\n- Installed using virtualenv? pip? conda?: Pip and Pipenv\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 11.4.153\r\n- GPU model and memory: Nvidia Quadro P4000 (8GB VRAM)\r\n\r\n**Describe the problem**\r\nWhen installing tensorflow, it seems like it can't find the selected versions to install from. I've checked for any builds, and on pypi there seems to be existing builds for windows and linux. This also happens with `pip install tensorflow==2.7.0`.\r\n\r\nSteps:\r\n1. Install Python 3.10.0 \r\n2. Upgrade pip to the latest version\r\n3. Run `pip install tensorflow`\r\n4. Wait until it returns that it can't find any versions for tensorflow to install from\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nWhen running `pip install tensorflow`, this is what it returns\r\n```\r\nERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\r\nERROR: No matching distribution found for tensorflow\r\n```", "comments": ["@No767,\r\n\r\nI've tried installing `TF 2.6.0` & `2.7.0` and both are working fine from my end. Please find the colab [gist here](https://colab.sandbox.google.com/gist/sanatmpa1/08cdb47110981cf8c3a2c54345827a8c/53031.ipynb).\r\n\r\nOne catch that I found is, you've installed `Python 3.10` whereas the [tested build configurations](https://www.tensorflow.org/install/source#tested_build_configurations) of `TF 2.7.0` supports `Python 3.7 - 3.9` for both Windows and Ubuntu. So you can try downgrading to see if it helps.\r\n\r\nAnother alternate that you can try is, upgrading your pip version using `pip3 install --upgrade pip`\r\n\r\nLet me know if any of it helps in resolving your issue. Thanks!\r\n\r\n\r\n", "Thanks for the response. I have also tested this on Python 3.8 and 3.9 and those seem working fine. I have also upgraded both pip3 and pip and seen the same result. I think it's due to 3.10 having issues, but I could be wrong. ", "@No767,\r\n\r\n`Python 3.10` is released after release cut of `TF 2.7` and so its not supported yet. But you can follow this [thread](https://discuss.tensorflow.org/t/support-python-3-10/124/5) in TF Forum for updates on the same. Thanks!", "Thanks for the info. I'll keep track of the thread on the forums.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53031\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53031\">No</a>\n", "py -3.9 -m pip install tensorflow"]}, {"number": 53030, "title": "Tensorflow 2.6 compatibility with previous versions?", "body": "Hi guys, I've trained a model using tensorflow 2.3, keras 2.3.1 and python 3.8. This model seems to be working perfectly fine in 2.4/2.5 but breaks with tf 2.6. I looked through the tensorflow 2.6 , as well as keras 2.6 release notes but couldn't find why this was breaking. \r\n\r\nThe model can be loaded in tf.2.6 and the weights are all correct. However during inference, the result is different from that of tf2.3,2.4,2.5. Is there a reason for why this is? \r\n\r\nModel is a transfer learnt efficientnet B3, with a custom top made of dense layers and batch norm. ", "comments": ["@ZiyueWangUoB \r\nFor keras related issue you may refer to this [link](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999), Unless you fill the issue template and provide us with details/error tf version log we cannot help.\r\nYou may run your code on colab and share the gist.\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53029, "title": "Adding images with no bounding box to tflite model maker object detection dataset", "body": "I want to add images without any bounding boxes to the dataset used to train an object detector using tflite model maker. \r\n\r\nAccording to the [docs](https://cloud.google.com/vision/automl/object-detection/docs/csv-format) I can add \r\n\r\n>one row for each image with no bounding box (such as row 4 below).\r\n\r\n```\r\n TRAIN,gs://folder/image1.png,car,0.1,0.1,,,0.3,0.3,,\r\n TRAIN,gs://folder/image1.png,bike,.7,.6,,,.8,.9,,\r\n UNASSIGNED,gs://folder/im2.png,car,0.1,0.1,0.2,0.1,0.2,0.3,0.1,0.3\r\n TEST,gs://folder/im3.png,,,,,,,,,\r\n```\r\n\r\nWhen doing so I am getting a ValueError \"could not convert string to float\", which is caused by an attempt to cast the None object to a float in the following [line](https://github.com/tensorflow/examples/blob/c930d0af2983ee438dcc005f9373188734f1624f/tensorflow_examples/lite/model_maker/core/data_util/object_detector_dataloader_util.py#L337-L338):\r\n\r\n```\r\n    xmin, ymin = float(line[3]) * width, float(line[4]) * height\r\n```\r\n\r\nHow can I properly add such images without bounding boxes? Possibly a more important question; are these empty images even helpful towards retraining the model (i.e. does this give higher precision to the default classifier of None or Empty)?\r\n", "comments": ["@topherbuckley \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),\r\nThanks!", "## URL(s) with the issue:\r\nhttps://cloud.google.com/vision/automl/object-detection/docs/csv-format\r\n\r\n## Description of issue (what needs changing):\r\nUnable to add images without any bounding boxes to the dataset used to train an object detector using tflite model maker.\r\n\r\n### Clear description\r\n\r\nI want to add images without any bounding boxes to the dataset used to train an object detector using tflite model maker. \r\n\r\nAccording to the [docs](https://cloud.google.com/vision/automl/object-detection/docs/csv-format) I can add \r\n\r\n>one row for each image with no bounding box (such as row 4 below).\r\n\r\n```\r\n TRAIN,gs://folder/image1.png,car,0.1,0.1,,,0.3,0.3,,\r\n TRAIN,gs://folder/image1.png,bike,.7,.6,,,.8,.9,,\r\n UNASSIGNED,gs://folder/im2.png,car,0.1,0.1,0.2,0.1,0.2,0.3,0.1,0.3\r\n TEST,gs://folder/im3.png,,,,,,,,,\r\n```\r\n\r\nWhen doing so I am getting a ValueError \"could not convert string to float\", which is caused by an attempt to cast the None object to a float in the following [line](https://github.com/tensorflow/examples/blob/c930d0af2983ee438dcc005f9373188734f1624f/tensorflow_examples/lite/model_maker/core/data_util/object_detector_dataloader_util.py#L337-L338):\r\n\r\n```\r\n    xmin, ymin = float(line[3]) * width, float(line[4]) * height\r\n```\r\n\r\nHow can I properly add such images without bounding boxes? Possibly a more important question; are these empty images even helpful towards retraining the model (i.e. does this give higher precision to the default classifier of None or Empty)?\r\n\r\n### Correct links\r\n\r\nYes / Not applicable\r\n\r\n### Parameters defined\r\n\r\nYes / Not applicable\r\n\r\n### Returns defined\r\n\r\nYes / Not applicable\r\n\r\n### Raises listed and defined\r\n\r\nYes\r\n\r\n### Usage example\r\n\r\nSee Clear description\r\n\r\n### Request visuals, if applicable\r\n\r\nNot applicable\r\n\r\n### Submit a pull request?\r\n\r\nHappy to if I know what the expected handling of None entries should be in this case. \r\n", "@topherbuckley \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "Adding @ziyeqinghan and @lu-wang-g, who may be able to answer TFLite model maker related questions. ", "> @topherbuckley In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n\r\nYou can just use the existing colab or notebook [here](https://www.tensorflow.org/lite/tutorials/model_maker_object_detection) but add an additional line to the csv file without bounding box verticies. e.g.:\r\n\r\n```\r\n[existing csv entries above]\r\nTEST,gs://cloud-ml-data/img/openimage/3/2520/3916261642_0a504acd60_o.jpg,,,,,,,,,\r\n```\r\n\r\n", "Hi, \r\nI have also ran into this problem when trying to add images without bounding boxes to a dataset. I am importing the images from a CSV stored locally on my harddrive, which was created using pandas, and am getting the same error. The images without bounding boxes have the format:\r\n\r\n> TRAIN,filepath,,,,,,,,,\r\n\r\nCan you confirm whether images without bounding boxes are able to be included in the training at all using model maker? I have been reading mixed opinions on this from stackoverflow etc. ", "Hi, please don't use images without bounding boxes. We can't accept the image without bounding box for training. We will update our doc later. Thanks a lot", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53029\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53029\">No</a>\n"]}, {"number": 53028, "title": "Remove xrange from rest of files", "body": null, "comments": []}, {"number": 53027, "title": "IDLE Time so long period.", "body": "TF version\uff1a v2.6, using distributed stragedy.\r\n\r\n![image](https://user-images.githubusercontent.com/10629930/141219850-a5cfa6d5-270a-4974-9504-c559cc83b7cf.png)\r\n\r\n![image](https://user-images.githubusercontent.com/10629930/141219519-1a3a9680-2bbe-4d3e-818e-d8028fe9ac2b.png)\r\n![image](https://user-images.githubusercontent.com/10629930/141224856-57638193-3d17-4aa2-bc87-ecdc478dc217.png)\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/10629930/141223727-f40cb68d-cb44-495a-8ec2-4c3e0561ceb3.png)\r\n\r\ntraining deepfm model by using cpu  without gpu train.\r\n\r\n\r\ninput files are tfrecords (15Mb per file)by spark. \r\n\r\nThe key  train code like this:\r\n\r\n![image](https://user-images.githubusercontent.com/10629930/141219749-49a5a82a-7c59-4605-8695-69c1fbf159b4.png)\r\n\r\nwhy the idle time is so long, and is there some ways to enchance. my problems is that using multimirrored worker can't reduce training time....", "comments": ["@berlinsaint Can you please check with `TF2.7` and `tf-nightly` and let us know whether the issue is persisting with recent `TF` versions. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53026, "title": "No supported kernel for GPU devices is available (Apple M1 - Metal GPU)", "body": "I am trying to replicate the results I obtained from a training I have already completed on both ubuntu and windows (the code works on GPU), and I also tried the code using CPU and it works properly (on MacOS). The problem is when I try to use the GPU. The error message I get is very long (I can paste it all if needed), but I believe that the core part is the following:\r\n\r\n> tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation loader/GeneratorDataset: Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n\r\nTensorflow version is 2.6.0, and I checked that the GPU is recognized:\r\n\r\n> I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\r\n> True\r\n\r\nThanks to anyone that could give hints about how to solve this issue.\r\n\r\n", "comments": ["Hi @spagliarini!\r\nCould you please the template too as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced]. Attaching similar issues for reference . [link1](https://stackoverflow.com/questions/55248631/tensorflow-cant-assign-a-device-for-operation/55455490),[link2](https://stackoverflow.com/questions/55248631/tensorflow-cant-assign-a-device-for-operation/55455490).Thanks!", "Hi @mohantym !\r\n\r\nI am using MAcOS Monterey (12.0.1), Chip Apple M1. My tensorflow version is 2.6.0  (I can correctly see the GPU if I check, and I used other scripts that run on GPU, and they run properly). \r\n\r\nThe part of the code where I run into the problem is the following: \r\n\r\n>   # Run training\r\n>   scaffold = tf.train.Scaffold(saver=tf.train.Saver(max_to_keep=2000))\r\n>   with tf.train.MonitoredTrainingSession(\r\n>   #it creates a monitored session for training. Return a monitored session which is as\r\n>   #session-like object that handles initialization, recovery and hooks\r\n>       checkpoint_dir=args.train_dir,\r\n>       save_checkpoint_secs=args.train_save_secs,\r\n>       save_summaries_secs=args.train_summary_secs,\r\n>       scaffold=scaffold) as sess:\r\n>     while True:\r\n>       # Train discriminator\r\n>       for i in xrange(args.wavegan_disc_nupdates):  #the discriminator is created 5x generator update\r\n>         sess.run(D_train_op)\r\n> \r\n>         # Enforce Lipschitz constraint for WGAN\r\n>         if D_clip_weights is not None:\r\n>           sess.run(D_clip_weights)\r\n> \r\n>       # Train generator\r\n>       sess.run(G_train_op)\r\n> \r\n\r\nI don't have a colab to reproduce the code, but the original code is from this git repository (https://github.com/chrisdonahue/wavegan): as I said, successfully used GPU version on linux/windows, and CPU version on MacOS (of course, I adapted the code to TF2 by using the \"compat.v1\" version, as it was not adapted in the linked version). \r\n\r\nThe full error message I have is the following: \r\n\r\n> WARNING:tensorflow:From /Users/silviapagliarini/miniforge3/envs/tensorflow/lib/python3.9/site-\r\n\r\n> packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> non-resource variables are not supported in the long term\r\n> Found 9356 audio files in specified directory\r\n> /Users/silviapagliarini/miniforge3/envs/tensorflow/lib/python3.9/site-packages/keras/legacy_tf_layers/core.py:236: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\r\n>   warnings.warn('`tf.layers.dense` is deprecated and '\r\n> /Users/silviapagliarini/miniforge3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/base_layer_v1.py:1676: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\r\n>   warnings.warn('`layer.apply` is deprecated and '\r\n> /Users/silviapagliarini/miniforge3/envs/tensorflow/lib/python3.9/site-packages/keras/legacy_tf_layers/convolutional.py:1660: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\r\n>   warnings.warn('`tf.layers.conv2d_transpose` is deprecated and '\r\n> Metal device set to: Apple M1\r\n> \r\n> systemMemory: 16.00 GB\r\n> maxCacheSize: 5.33 GB\r\n> \r\n> 2021-11-12 08:32:28.008642: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\r\n> 2021-11-12 08:32:28.008723: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\r\n> WARNING:tensorflow:From /Users/silviapagliarini/Documents/Python/wavegan-master/loader.py:197: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through `tf.compat.v1`. In all other situations -- namely, eager mode and inside `tf.function` -- you can consume dataset elements using `for elem in dataset: ...` or by explicitly creating iterator via `iterator = iter(dataset)` and fetching its elements via `values = next(iterator)`. Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)` to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\r\n> --------------------------------------------------------------------------------\r\n> Generator vars\r\n> [3, 16384] (49152): G/z_project/dense/kernel:0\r\n> [16384] (16384): G/z_project/dense/bias:0\r\n> [1, 25, 512, 1024] (13107200): G/upconv_0/conv2d_transpose/kernel:0\r\n> [512] (512): G/upconv_0/conv2d_transpose/bias:0\r\n> [1, 25, 256, 512] (3276800): G/upconv_1/conv2d_transpose/kernel:0\r\n> [256] (256): G/upconv_1/conv2d_transpose/bias:0\r\n> [1, 25, 128, 256] (819200): G/upconv_2/conv2d_transpose/kernel:0\r\n> [128] (128): G/upconv_2/conv2d_transpose/bias:0\r\n> [1, 25, 64, 128] (204800): G/upconv_3/conv2d_transpose/kernel:0\r\n> [64] (64): G/upconv_3/conv2d_transpose/bias:0\r\n> [1, 25, 1, 64] (1600): G/upconv_4/conv2d_transpose/kernel:0\r\n> [1] (1): G/upconv_4/conv2d_transpose/bias:0\r\n> Total params: 17476097 (66.67 MB)\r\n> /Users/silviapagliarini/miniforge3/envs/tensorflow/lib/python3.9/site-packages/keras/legacy_tf_layers/convolutional.py:263: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\r\n>   warnings.warn('`tf.layers.conv1d` is deprecated and '\r\n> --------------------------------------------------------------------------------\r\n> Discriminator vars\r\n> [25, 1, 64] (1600): D/downconv_0/conv1d/kernel:0\r\n> [64] (64): D/downconv_0/conv1d/bias:0\r\n> [25, 64, 128] (204800): D/downconv_1/conv1d/kernel:0\r\n> [128] (128): D/downconv_1/conv1d/bias:0\r\n> [25, 128, 256] (819200): D/downconv_2/conv1d/kernel:0\r\n> [256] (256): D/downconv_2/conv1d/bias:0\r\n> [25, 256, 512] (3276800): D/downconv_3/conv1d/kernel:0\r\n> [512] (512): D/downconv_3/conv1d/bias:0\r\n> [25, 512, 1024] (13107200): D/downconv_4/conv1d/kernel:0\r\n> [1024] (1024): D/downconv_4/conv1d/bias:0\r\n> [16384, 1] (16384): D/output/dense/kernel:0\r\n> [1] (1): D/output/dense/bias:0\r\n> Total params: 17427969 (66.48 MB)\r\n> --------------------------------------------------------------------------------\r\n> WARNING:tensorflow:From /Users/silviapagliarini/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/training/training_util.py:235: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\n> 2021-11-12 08:32:28.778728: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\r\n> 2021-11-12 08:32:28.778748: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\r\n> Traceback (most recent call last):\r\n>   File \"/Users/silviapagliarini/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1375, in _do_call\r\n>     return fn(*args)\r\n>   File \"/Users/silviapagliarini/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1358, in _run_fn\r\n>     self._extend_graph()\r\n>   File \"/Users/silviapagliarini/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1398, in _extend_graph\r\n>     tf_session.ExtendSession(self._session)\r\n> tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation loader/GeneratorDataset: Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> IteratorGetNext: CPU GPU \r\n> OneShotIterator: CPU \r\n> IteratorToStringHandle: CPU GPU \r\n> PrefetchDataset: CPU GPU \r\n> GeneratorDataset: CPU GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   loader/GeneratorDataset (GeneratorDataset) /device:GPU:0\r\n>   loader/PrefetchDataset_1 (PrefetchDataset) /device:GPU:0\r\n>   loader/OneShotIterator (OneShotIterator) /device:GPU:0\r\n>   loader/IteratorToStringHandle (IteratorToStringHandle) /device:GPU:0\r\n>   loader/IteratorGetNext (IteratorGetNext) /device:GPU:0\r\n> \r\n> Op: GeneratorDataset\r\n> Node attrs: Tfinalize_func_args=[DT_STRING], output_shapes=[[64,16384,1,1]], finalize_func=__inference__remote_finalize_func_466[], init_func=__inference__remote_init_func_441[], output_types=[DT_FLOAT], Tinit_func_args=[DT_STRING, DT_VARIANT], next_func=__inference__remote_next_func_454[experimental_ints_on_device=true], Tnext_func_args=[DT_STRING]\r\n> Registered kernels:\r\n>   device='GPU'\r\n>   device='CPU'\r\n>   device='XLA_CPU'\r\n> \r\n> \t [[{{node loader/GeneratorDataset}}]]\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"/Users/silviapagliarini/Documents/Python/wavegan-master/train_wavegan.py\", line 929, in <module>\r\n>     train(fps, args)\r\n>   File \"/Users/silviapagliarini/Documents/Python/wavegan-master/train_wavegan.py\", line 209, in train\r\n>     with tf.train.MonitoredTrainingSession(\r\n>   File \"/Users/silviapagliarini/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py\", line 602, in MonitoredTrainingSession\r\n>     return MonitoredSession(\r\n>   File \"/Users/silviapagliarini/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py\", line 1035, in __init__\r\n>     super(MonitoredSession, self).__init__(\r\n>   File \"/Users/silviapagliarini/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py\", line 750, in __init__\r\n>     self._sess = _RecoverableSession(self._coordinated_creator)\r\n>   File \"/Users/silviapagliarini/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py\", line 1232, in __init__\r\n>     _WrappedSession.__init__(self, self._create_session())\r\n>   File \"/Users/silviapagliarini/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py\", line 1237, in _create_session\r\n>     return self._sess_creator.create_session()\r\n>   File \"/Users/silviapagliarini/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py\", line 903, in create_session\r\n>     self.tf_sess = self._session_creator.create_session()\r\n>   File \"/Users/silviapagliarini/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py\", line 662, in create_session\r\n>     return self._get_session_manager().prepare_session(\r\n>   File \"/Users/silviapagliarini/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/training/session_manager.py\", line 327, in prepare_session\r\n>     sess.run(init_op, feed_dict=init_feed_dict)\r\n>   File \"/Users/silviapagliarini/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 967, in run\r\n>     result = self._run(None, fetches, feed_dict, options_ptr,\r\n>   File \"/Users/silviapagliarini/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1190, in _run\r\n>     results = self._do_run(handle, final_targets, final_fetches,\r\n>   File \"/Users/silviapagliarini/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1368, in _do_run\r\n>     return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n>   File \"/Users/silviapagliarini/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1394, in _do_call\r\n>     raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter\r\n> tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation loader/GeneratorDataset: Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> IteratorGetNext: CPU GPU \r\n> OneShotIterator: CPU \r\n> IteratorToStringHandle: CPU GPU \r\n> PrefetchDataset: CPU GPU \r\n> GeneratorDataset: CPU GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   loader/GeneratorDataset (GeneratorDataset) /device:GPU:0\r\n>   loader/PrefetchDataset_1 (PrefetchDataset) /device:GPU:0\r\n>   loader/OneShotIterator (OneShotIterator) /device:GPU:0\r\n>   loader/IteratorToStringHandle (IteratorToStringHandle) /device:GPU:0\r\n>   loader/IteratorGetNext (IteratorGetNext) /device:GPU:0\r\n> \r\n> Op: GeneratorDataset\r\n> Node attrs: Tfinalize_func_args=[DT_STRING], output_shapes=[[64,16384,1,1]], finalize_func=__inference__remote_finalize_func_466[], init_func=__inference__remote_init_func_441[], output_types=[DT_FLOAT], Tinit_func_args=[DT_STRING, DT_VARIANT], next_func=__inference__remote_next_func_454[experimental_ints_on_device=true], Tnext_func_args=[DT_STRING]\r\n> Registered kernels:\r\n>   device='GPU'\r\n>   device='CPU'\r\n>   device='XLA_CPU'\r\n> \r\n> \t [[node loader/GeneratorDataset (defined at /Documents/Python/wavegan-master/loader.py:192) ]]", "Hi @sachinprasadhs! Could you look at this issue?", "Could you please try to configure the GPU device with [logical_device_configuration](https://www.tensorflow.org/api_docs/python/tf/config/set_logical_device_configuration?hl=zh-cn) and set some memory to see if the code runs without any issues.\r\nOptionally, you can also try  [set_visible_devices](https://www.tensorflow.org/api_docs/python/tf/config/set_visible_devices) like below.\r\n```\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\n tf.config.set_visible_devices(physical_devices[0], 'GPU')\r\n```", "I have tried both options:\r\n* allocated some memories using the instructions on logical_device_configuration\r\n* added the set_visible_devices at the beginning of my code as suggested\r\n\r\nI end up with the same error message. \r\n\r\nIf this can help, I think the problem comes when I try to do the following:\r\n```\r\n      dataset = dataset.apply(\r\n          tf.data.experimental.prefetch_to_device(\r\n            '/device:GPU:{}'.format(prefetch_gpu_num)))\r\n```\r\n\r\nI also tried to give explicitely the device name (/device:GPU:0) but the error message does not change.", "Try the below sample code for the above module and let us know the output.\r\n\r\n```\r\nwith tf.device('/GPU'):\r\n  dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\r\n  dataset = dataset.apply(tf.data.experimental.prefetch_to_device(\"GPU:0\"))\r\n\r\nfor element in dataset:\r\n  print(f'Tensor {element} is on device {element.device}')\r\n\r\nResult:\r\n#Tensor 1 is on device /job:localhost/replica:0/task:0/device:GPU:0\r\n#Tensor 2 is on device /job:localhost/replica:0/task:0/device:GPU:0\r\n#Tensor 3 is on device /job:localhost/replica:0/task:0/device:GPU:0\r\n```\r\n", "Yes. Here is the output. \r\n\r\n```\r\nwith tf.device('/GPU'):\r\n...   dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\r\n...   dataset = dataset.apply(tf.data.experimental.prefetch_to_device(\"GPU:0\"))\r\n```\r\n\r\nMetal device set to: Apple M1\r\n\r\nsystemMemory: 16.00 GB\r\nmaxCacheSize: 5.33 GB\r\n\r\n2021-11-17 10:48:35.562861: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\r\n2021-11-17 10:48:35.563156: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\r\n\r\n```\r\nfor element in dataset:\r\n...   print(f'Tensor {element} is on device {element.device}')\r\n\r\n```\r\n2021-11-17 10:48:44.547542: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\r\n2021-11-17 10:48:44.552936: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\r\n2021-11-17 10:48:44.554274: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\r\n2021-11-17 10:48:44.574873: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\r\nTensor 1 is on device /job:localhost/replica:0/task:0/device:GPU:0\r\nTensor 2 is on device /job:localhost/replica:0/task:0/device:GPU:0\r\nTensor 3 is on device /job:localhost/replica:0/task:0/device:GPU:0", "This is working as expected by seeing the result and it is able to detect the GPU as well, the problem is not with `prefetch_to_device`.\r\nIn your code make the below changes \r\n`dataset = dataset.apply(tf.data.experimental.prefetch_to_device(\"GPU:0\"))` and it the code breaks again then we may have to debug for other details.", "Ok. So somehow there something that does not go well when I start the training with \r\n\r\n`tf.train.MonitoredTrainingSession`\r\n\r\nIf this is a problem related to how tensorflow works on Apple M1, do you have any suggestion about how to get around it? ", "`tf.train.MonitoredTrainingSession` is deprediated and you have to use with [tf.compat.v1.train.MonitoredTrainingSession](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/MonitoredTrainingSession). \r\nFor Apple related issues you can raise it in their dedicated [forum](https://developer.apple.com/forums/).", "`tf.compat.v1.train.MonitoredTrainingSession` arises the same error message. \r\n\r\nI was reading again the whole error message from the beginning. Coming back to your point about the memory, I can see this warning:\r\n\r\n`I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)`\r\n\r\nI tried both your suggestions but I always have this warning. Could it be related to the issue I have later when starting Monitored Session - I tried also Session but I run into the same error).\r\n", "Please refer to the similar issue on apple forum [here](https://developer.apple.com/forums/thread/684178), if that doesn't helps you please raise the new issue in the forum and close the issue here, Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "That apple forum didn't help (I already have the latest version of OS). I fairly don't know what to do and was trying to dig in but had to stop for other priorities this week. I will get back to this issue next week. ", "Let us know if you have any update as per the above comment. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "The apple forum where they discuss the problem is this: https://developer.apple.com/forums/thread/683780?login=true&page=1#698656022 \r\n\r\nBut it looks like the problem hasn't been solved yet. ", "This is more related to Apple. TF team doesn't have access to that hardware. However, TF Community is trying to build TF for Apple M1 [here](https://github.com/tensorflow/tensorflow/issues/52160). Please follow that thread for more discussion. Thanks! ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53026\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53026\">No</a>\n"]}, {"number": 53024, "title": "Error with higher batch size in stateful LSTM layer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.7.0-rc1-69-gc256c071bb2 2.7.0\r\n- Python version: 3.9.5\r\n- CUDA/cuDNN version: 11.2/8.1.1\r\n- GPU model and memory: GTX 1070 Ti\r\n\r\n**Describe the current behavior**\r\n\r\nWhen using a stateful LSTM layer with any batch size > 32 I get an error:\r\n```\r\nbatch_size = 33\r\ninput_tensor = tf.keras.Input(shape=[5], batch_size=batch_size)\r\nlstm_input = tf.keras.layers.Dense(16, activation=tf.keras.activations.elu)(input_tensor)\r\nlstm_input = tf.expand_dims(lstm_input, axis=1)\r\nlstm_layer = tf.keras.layers.LSTM(16, stateful=True, return_state=True)\r\nlstm_out, h, c = lstm_layer(lstm_input)\r\nout = tf.keras.layers.Dense(1, activation=tf.keras.activations.linear)(lstm_out)\r\n```\r\n```\r\n2021-11-10 19:32:33.397959: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at cudnn_rnn_ops.cc:1517 : INVALID_ARGUMENT: Invalid input_h shape: [1,33,16] [1,32,16]\r\n```\r\nAny batch size <= 32 works fine\r\nChanging the LSTM layer to stateful=False and the Input layer to batch_size=None makes the issue disappear (changing only stateful to False does not)\r\n\r\n**Describe the expected behavior**\r\nNo difference when changing the batch size.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\nbatch_size = 33\r\n\r\ninput_tensor = tf.keras.Input(shape=[5], batch_size=batch_size)\r\nlstm_input = tf.keras.layers.Dense(16, activation=tf.keras.activations.elu)(input_tensor)\r\nlstm_input = tf.expand_dims(lstm_input, axis=1)\r\nlstm_layer = tf.keras.layers.LSTM(16, stateful=True, return_state=True)\r\nlstm_out, h, c = lstm_layer(lstm_input)\r\nout = tf.keras.layers.Dense(1, activation=tf.keras.activations.linear)(lstm_out)\r\n\r\nmodel = tf.keras.Model(\r\n    inputs=[input_tensor],\r\n    outputs=[out, h, c])\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.MeanSquaredError())\r\nmodel.summary()\r\n\r\ninput_arr = np.random.random(size=(batch_size, 5))\r\nmodel.predict([input_arr])\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\nExecuting the above program with any batch size > 32 gives the following error:\r\n```\r\n2021-11-10 19:45:47.216169: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-11-10 19:45:47.636164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6615 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\r\nModel: \"model\"\r\n_________________________________________________________________\r\n Layer (type)                Output Shape              Param #   \r\n=================================================================\r\n input_1 (InputLayer)        [(33, 5)]                 0         \r\n                                                                 \r\n dense (Dense)               (33, 16)                  96        \r\n                                                                 \r\n tf.expand_dims (TFOpLambda)  (33, 1, 16)              0         \r\n                                                                 \r\n lstm (LSTM)                 [(33, 16),                2112      \r\n                              (33, 16),                          \r\n                              (33, 16)]                          \r\n                                                                 \r\n dense_1 (Dense)             (33, 1)                   17        \r\n                                                                 \r\n=================================================================\r\nTotal params: 2,225\r\nTrainable params: 2,225\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n2021-11-10 19:45:48.667605: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at cudnn_rnn_ops.cc:1517 : INVALID_ARGUMENT: Invalid input_h shape: [1,33,16] [1,32,16]\r\nTraceback (most recent call last):\r\n  File \"***\\PyCharmCE2021.2\\scratches\\scratch.py\", line 22, in <module>\r\n    model.predict([input_arr])\r\n  File \"***\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"***\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 58, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError:    Invalid input_h shape: [1,33,16] [1,32,16]\r\n\t [[{{node CudnnRNN}}]]\r\n\t [[model/lstm/PartitionedCall]] [Op:__inference_predict_function_1045]\r\n\r\nFunction call stack:\r\npredict_function -> predict_function -> predict_function\r\n\r\n\r\nProcess finished with exit code 1\r\n```\r\n", "comments": ["@Krzha,\r\n\r\nI modified the last line in your code from `model.predict([input_arr])` to `model.predict(input_arr,batch_size=batch_size)` and now it works fine for batch sizes more than 32. Please find the working [gist here](https://colab.sandbox.google.com/gist/sanatmpa1/b7f680b071db6f76702c68579630f202/53024.ipynb). Thanks!", "Indeed it does work\r\n\r\nI'm not sure why, but at least that solves the issue, thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53024\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53024\">No</a>\n"]}, {"number": 53023, "title": "Disabling wrong regex test.", "body": "The test failed for rocm in a weekly sync. Investigation under way but disabled it for rocm for now to keep community CI clean.\r\n@cheshire @chsigg @deven-amd @stevenireeves", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F53023) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent.", "> Why do we have a different error message for ROCm though?\r\n\r\nIt was indeed strange and still under investigation. Attached is the failing log.\r\n[tridiagonal_matmul_op_test_gpu_test.log](https://github.com/tensorflow/tensorflow/files/7514763/tridiagonal_matmul_op_test_gpu_test.log)\r\n", "@cheshire Can we get a re-approval on this PR? We had to do a minor change for pylint. "]}, {"number": 53022, "title": "Fix build of XLA unit tests on AARCH64", "body": "Fixes https://github.com/tensorflow/tensorflow/issues/53021", "comments": []}, {"number": 53021, "title": "unit test //tensorflow/compiler/xla/tests:local_client_aot_test fails to build on AARCH64", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: git HEAD\r\n- Python version: 3.6.9\r\n- Installed using virtualenv? pip? conda?: no\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 10.3.0\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nBuild fails with -\r\n2021-11-10 12:32:55.390296: F tensorflow/compiler/xla/tests/local_client_aot_test_helper.cc:81] unsupported TARGET_CPU: aarch64\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nbazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --remote_http_cache=\"\"  --remote_cache_proxy=\"\" --noremote_accept_cached --config=nonccl --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --copt=-ffp-contract=off --cxxopt=-ffp-contract=off --verbose_failures -- //tensorflow/compiler/xla/tests/...\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nERROR: /home/builder/1/tensorflow_build/tensorflow-git/tensorflow/compiler/xla/tests/BUILD:199:8: Executing genrule //tensorflow/compiler/xla/tests:local_client_aot_test_computation failed (Aborted): bash failed: error executing command \r\n(cd /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow && \\\r\n\r\nexec env - \\\r\n\r\nPATH=/home/builder/.cache/bazelisk/downloads/bazelbuild/bazel-3.7.2-linux-arm64/bin:/home/builder/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n\r\nPYTHON_BIN_PATH=/usr/bin/python3 \\\r\n\r\nPYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\\r\n\r\nTF2_BEHAVIOR=1 \\\r\n\r\n/bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/aarch64-opt/bin/tensorflow/compiler/xla/tests/local_client_aot_test_helper aarch64 > bazel-out/aarch64-opt/bin/tensorflow/compiler/xla/tests/local_client_aot_test_computation.o')\r\nExecution platform: @local_execution_config_platform//:platform\r\n2021-11-10 12:32:55.390296: F tensorflow/compiler/xla/tests/local_client_aot_test_helper.cc:81] unsupported TARGET_CPU: aarch64\r\n/bin/bash: line 1: 729298 Aborted                 (core dumped) bazel-out/aarch64-opt/bin/tensorflow/compiler/xla/tests/local_client_aot_test_helper aarch64 > bazel-out/aarch64-opt/bin/tensorflow/compiler/xla/tests/local_client_aot_test_computation.o\r\nINFO: Elapsed time: 579.863s, Critical Path: 253.00s\r\nINFO: 5482 processes: 2358 internal, 3124 local.\r\nFAILED: Build did NOT complete successfully", "comments": ["@cfRod @nSircombe ", "Hi @Saduf2019  ! Could you look at this issue ?A  Pull request has been created at  #53022 . ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53021\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53021\">No</a>\n"]}, {"number": 53020, "title": "Update jsoncpp to 1.9.5", "body": "This PR update jsoncpp from 1.9.4 to 1.9.5.\r\n\r\nThe biggest reason for update to 1.9.5, is that jsoncpp 1.9.5 now supports bazel build. So the tensorflow-maintained BUILD file can be dropped.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang Can you please resolve conflicts? Thanks!", "Thanks @mihaimaruseac @gbaned for the help. The PR has been rebased with conflict resolved."]}, {"number": 53019, "title": "Adding new op  to HLO and add tf to HLO lowering ", "body": "I want to lower tf.leakyrelu to the new HLO op mhlo.leaky_relu. For this,\r\nI created new op in mhlo for leakyrelu in iree/third_party/tensorflow/tensorflow/compiler/mlir/hlo/include/mlir-hlo/Dialect/mhlo/IR/hlo_ops.td\r\nThen in iree/third_party/tensorflow/tensorflow/compiler/mlir/xla/transforms/legalize_tf.cc updated the lowering for tf.leakyrelu to mhlo.leakyrelu \r\n\r\nBut when I run bazel build iree_tf_compiler:importer-binaries getting error as \r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/552a1865bc9b933b5dd3d8c3ed93b3cd/external/org_tensorflow/tensorflow/compiler/mlir/xla/BUILD:442:11: C++ compilation of rule '@org_tensorflow//tensorflow/compiler/mlir/xla:mlir_hlo_to_hlo' failed (Exit 1): clang failed: error executing command /usr/bin/clang -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 319 argument(s) skipped)\r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox clang failed: error executing command /usr/bin/clang -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 319 argument(s) skipped)\r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox\r\nIn file included from external/org_tensorflow/tensorflow/compiler/mlir/xla/mlir_hlo_to_hlo.cc:1274:\r\nbazel-out/k8-opt/bin/external/org_tensorflow/tensorflow/compiler/mlir/xla/operator_writers.inc:483:26: error: no member named 'LeakyRelu' in namespace 'xla'\r\n  auto xla_result = xla::LeakyRelu(Unwrap(xla_arg_0), Unwrap(xla_arg_1));\r\n                    ~~~~~^\r\n1 error generated.\r\nTarget //iree_tf_compiler:importer-binaries failed to build\r\n\r\n\r\nWhich are the files do I need to update if I want to create lowering to new hlo op.\r\n", "comments": ["I believe you have found a workaround/solution with reference to your same issue in another repo [here](https://github.com/google/iree/issues/7586). Can you please confirm the same and move the issue to closed, Thanks!", "Thank you"]}]