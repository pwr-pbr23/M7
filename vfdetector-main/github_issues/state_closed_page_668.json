[{"number": 33561, "title": " python 3.8 package", "body": "please release python 3.8 package.\r\n", "comments": ["duplicates\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/33374\r\nhttps://github.com/tensorflow/tensorflow/issues/33543", "Closing as duplicate. Let's follow this up after https://github.com/tensorflow/tensorflow/issues/33543", "I was able to successfully build TF on windows with python 3.8.\r\nHowever, grpcio still did not publish their python 3.8 package.\r\nTherefore, we are blocked on their release for our python 3.8 release", "@gunan [20616](https://github.com/grpc/grpc/pull/20616) is this the release that was blocking?", "https://pypi.org/project/grpcio/1.25.0/#files\r\n\r\nWe need a new release of grpcio uploaded to pypi (above) that has python 3.8 support for all operating systems TF supports.", "Hi @gunan ,\r\ncould you explain us how install TF - gpu with python 3.8 (unfortunatelly i use Windows10)?\r\ni become crazy, to fine a solution..\r\n\r\nthanks a lot! :)", "We still do not have the prebuilt packages. Unfortunately, we were not able to finish 2.1 and python 3.8 before christmas due to the year end rush. All our dependencies finally have support, but we did not have much time after they had releases with support.\r\n\r\nWe will push nightly packages for python 3.8 in January.\r\nUntil then, you can try building TF from sources."]}, {"number": 33560, "title": "BatchMatmul on GPU Wrong Result", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n-  Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): pip install tensorflow-gpu\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:Quadro P4000 8GB\r\n\r\n**Describe the current behavior**\r\n\r\nbatchmatmul on gpu gives wrong result.\r\n\r\n**Describe the expected behavior**\r\n\r\nbatchmatmul should give correct result.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nH, W = 480, 640\r\nxs = tf.random.normal((H, W, 3))\r\nm = tf.random.normal((3, 3))\r\nwith tf.device('/gpu:0'):\r\n  mvgpu = tf.linalg.matvec(m[None, None, None, :, : ], xs[None])\r\n\r\nwith tf.device('cpu:0'):\r\n  mvcpu = tf.linalg.matvec(m[None, None, None, :, : ], xs[None])\r\n\r\ndiff = mvgpu - mvcpu\r\nprint(diff) # a lot of large differences\r\nassert not np.allclose(diff, 0)\r\n```\r\n\r\n**Other info / logs**\r\npython diff.py \r\ntf.Tensor(\r\n[[[[ 0.          0.          0.        ]\r\n   [ 0.          0.          0.        ]\r\n   [ 0.          0.          0.        ]\r\n   ...\r\n   [ 0.          0.          0.        ]\r\n   [ 0.          0.          0.        ]\r\n   [ 0.          0.          0.        ]]\r\n\r\n  [[ 0.          0.          0.        ]\r\n   [ 0.          0.          0.        ]\r\n   [ 0.          0.          0.        ]\r\n   ...\r\n   [ 0.          0.          0.        ]\r\n   [ 0.          0.          0.        ]\r\n   [ 0.          0.          0.        ]]\r\n\r\n  [[ 0.          0.          0.        ]\r\n   [ 0.          0.          0.        ]\r\n   [ 0.          0.          0.        ]\r\n   ...\r\n   [ 0.          0.          0.        ]\r\n   [ 0.          0.          0.        ]\r\n   [ 0.          0.          0.        ]]\r\n\r\n  ...\r\n\r\n  [[ 2.7437444  -2.3577256   3.3067198 ]\r\n   [ 1.0874022  -0.7325933   2.4926019 ]\r\n   [-2.3672104   1.6497741  -2.1795664 ]\r\n   ...\r\n   [-1.4023315   0.68545413 -1.9975882 ]\r\n   [-0.44164103 -1.6766946  -0.6477224 ]\r\n   [ 2.5612988   0.7603723   4.286979  ]]\r\n\r\n  [[-0.9761815   1.5332515  -0.12404668]\r\n   [ 1.871354   -0.73664904 -1.0545558 ]\r\n   [-1.2774239   1.6571116  -2.36473   ]\r\n   ...\r\n   [ 2.4847538  -1.5389507   1.5169847 ]\r\n   [ 2.2117858  -3.3593345  -0.15468699]\r\n   [ 0.82464755 -2.0175495  -0.11506134]]\r\n\r\n  [[ 2.8697317   0.54362947 -0.10824746]\r\n   [ 1.6531861  -4.2324843   0.97695297]\r\n   [ 4.3966837  -3.0250916   1.8032213 ]\r\n   ...\r\n   [ 4.5854487  -2.0374475  -1.1027236 ]\r\n   [-2.7206469   0.864337   -1.3582373 ]\r\n   [-0.98220587  0.53226703 -4.277097  ]]]], shape=(1, 480, 640, 3), dtype=float32)\r\n\r\n", "comments": ["@kor01, Thanks for reporting this issue,\r\nI tried replicating the issue on colab with TF 2.0, it throws assertion error. Since the `diff` is `0`, `assert not` throws exception. Let me know if i misunderstood your issue. Please take a look at the [gist](https://colab.sandbox.google.com/gist/gadagashwini/24dfd6669d3be4e3a40c45e561195a11/untitled217.ipynb). Thanks!", "> @kor01, Thanks for reporting this issue,\r\n> I tried replicating the issue on colab with TF 2.0, it throws assertion error. Since the `diff` is `0`, `assert not` throws exception. Let me know if i misunderstood your issue. Please take a look at the [gist](https://colab.sandbox.google.com/gist/gadagashwini/24dfd6669d3be4e3a40c45e561195a11/untitled217.ipynb). Thanks!\r\n\r\nI run the script and replicate the error on my laptop (Quadro P4000 8GB) and my desktop computer( TITAN V  & Titan X Maxwell). However I could not replicate on K80 device in colab cloud. I have python 3.7 on my two computers whereas colab has python 3.6. It could be device or python version dependent. \r\n\r\nThe problem is also replicated in:\r\nhttps://github.com/tensorflow/tensorflow/issues/31166\r\n\r\nthe problem characteristics are the same with mine. A portion of computation is done correctly another portion is left with zeros.", "This is fixed in latest tf-nightly version '2.1.0-dev20200110' which uses cuda 10.1. (tested in google colab) Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33560\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33560\">No</a>\n"]}, {"number": 33559, "title": "Fix BUILD_WITH_MMAP error in TFLite Makefile", "body": "", "comments": ["@miaout17 Can you please take a look at this? ", "Thanks for sending the Pull Request. \r\n\r\nHowever it doesn't look right to disable mmap_allocation when `BUILD_WITH_MMAP` is true. \r\nCould you describe what issue are you running into?", "I'm closing this PR since it's incorrect to simply flipping the logic. \r\nPlease create a bug report if you're hitting some issues, with reproduce steps. \r\nThanks a lot!", "@miaout17 I think we should *INCLUDE* mmap_allocation.cc when BUILD_WITH_MMAP is true, not *EXCLUDE* it."]}, {"number": 33558, "title": "Dev-board Interpretere Runtime Error", "body": "I am working on the Coral dev board. I'm trying to deploy a segmentation model on it. When I'm running my deep lab segmentation model it is giving me the following error-\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"infer.py\", line 17, in <module>\r\n    interpreter.allocate_tensors()\r\n  File \"/home/mendel/.local/lib/python3.5/site-packages/tflite_runtime/interpreter.py\", line 244, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\n  File \"/home/mendel/.local/lib/python3.5/site-packages/tflite_runtime/interpreter_wrapper.py\", line 114, in AllocateTensors\r\n    return _interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\nRuntimeError: Internal: :71 tf_lite_type != kTfLiteUInt8 (9 != 3)Node number 79 (EdgeTpuDelegateForCustomOp) failed to prepare.\r\n```\r\nThe model and script are working fine if I don't make it TPU compatible using edgetpu_compiler.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nfrom tqdm import tqdm\r\nimport numpy as np\r\nfrom tflite_runtime.interpreter import Interpreter\r\nfrom tflite_runtime.interpreter import load_delegate\r\n\r\ntest_data = np.random.rand(480,480,3)\r\nimg = np.array([test_data], dtype=np.float32)\r\n\r\ninterpreter = Interpreter(\r\n      model_path=\"deep_lab_quant_edgetpu.tflite\",\r\n      experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\r\n\r\ninterpreter.allocate_tensors()\r\n\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\ninterpreter.set_tensor(input_details[0]['index'], img)\r\ninterpreter.invoke()\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\n```\r\n\r\n\r\n", "comments": ["I get the same error for my custom image processing model. I don't truly know the source of the error, but have found that when reducing the size of the model (number of channels in each layer) and/or the input tensor (height and width dimensions), the error goes away.\r\n\r\nHowever, the output of the edgetpu compiler suggests that even my \"full-size\" model easily fits on the TPU:\r\n```\r\nInput model: model.tflite\r\nInput size: 925.08KiB\r\nOutput model: model_edgetpu.tflite\r\nOutput size: 1.04MiB\r\nOn-chip memory available for caching model parameters: 5.78MiB\r\nOn-chip memory used for caching model parameters: 1019.75KiB\r\nOff-chip memory used for streaming uncached model parameters: 0.00B\r\nNumber of Edge TPU subgraphs: 1\r\nTotal number of operations: 54\r\nOperation log: model_edgetpu.log\r\n```\r\n\r\nHere are my system specs:\r\n```\r\nLinux kernel version:\r\n$ uname -r\r\n4.4.0-96-generic\r\n\r\nLinux release:\r\n$ lsb_release -a\r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 16.04.6 LTS\r\nRelease:\t16.04\r\nCodename:\txenial\r\n\r\nTensorflow python module version\r\n2.0.0-rc0\r\n\r\nEdge TPU python module version:\r\n2.12.1\r\n\r\nEdge TPU compiler version:\r\nEdge TPU Compiler version 2.0.267685300\r\n\r\nEdge TPU runtime version:\r\nBuildLabel(COMPILER=5.4.0 20160609,DATE=redacted,TIME=redacted,CL_NUMBER=267685300), RuntimeVersion(12)\r\n\r\nPaths of available Edge TPU devices, if any:\r\n('/sys/bus/usb/devices/4-3',)\r\n```", "@mattroos thanks for your suggestion. This did not solve my error though!\r\nBelow is the output log of edgetpu compilation of .tflite model. I'm using deeplab for segmentation. let me know if you are able to capture any error here.\r\nAlso, can you suggest me any other model which can be run on coral dev board?\r\nThanks. \r\n```\r\nEdge TPU Compiler version 2.0.267685300\r\nInput: deep_lab_quant.tflite\r\nOutput: deep_lab_quant_edgetpu.tflite\r\n\r\nOperator                       Count      Status\r\n\r\nQUANTIZE                       1          More than one subgraph is not supported\r\nQUANTIZE                       1          Operation is otherwise supported, but not mapped due to some unspecified limitation\r\nMUL                            10         More than one subgraph is not supported\r\nRESHAPE                        1          More than one subgraph is not supported\r\nRESIZE_BILINEAR                2          Image-interpolation layer won't run precisely enough on Edge TPU\r\nADD                            3          Mapped to Edge TPU\r\nADD                            7          More than one subgraph is not supported\r\nCONCATENATION                  1          More than one subgraph is not supported\r\nCONV_2D                        15         Mapped to Edge TPU\r\nCONV_2D                        23         More than one subgraph is not supported\r\nDEPTHWISE_CONV_2D              7          Mapped to Edge TPU\r\nDEPTHWISE_CONV_2D              10         Tensor has unsupported rank (up to 3 innermost dimensions mapped)\r\nBATCH_TO_SPACE_ND              10         Operation not supported\r\nDEQUANTIZE                     1          Operation is working on an unsupported data type\r\nSPACE_TO_BATCH_ND              10         Tensor has unsupported rank (up to 3 innermost dimensions mapped)\r\nMEAN                           1          More than one subgraph is not supported\r\n```", "See one of my EdgeTPU testing scripts, here: https://github.com/mattroos/EdgeTpuTesting/blob/master/test_edgetpu.ipynb\r\nIt builds, converts (to quantized INT8 model), compiles, and runs a very simple model (nothing but a max pooling layer). It works for me, and I've also successfully tried conv2d, relu, add, and separable conv2d layers. [Note that the output cells in the notebook don't really match the code in the cells, because I edited some of the code and saved the notebook before re-running it. But it does work for me, with layers \"Mapped to Edge TPU\" and giving meaningful output.]\r\n\r\nObviously this model doesn't do anything useful. It's just confirms that small, simple models can be run on the EdgeTPU.", "@adish333 @mattroos \r\nNam from Coral Team here, apologize for the issue. We currently have not supported models with tf2.0 yet. Could you guys run into the same issue if downgrading to tf1.5?\r\nEdit: tf1.15\r\n", "I'll give it a try. @Namburger, did you really mean tf1.5? Did you possibly mean tf1.15?", "@mattroos sorry, tf1.15 ", "@Namburger, I gave it a try with TF1.15, but can't get past the conversion of the model from the saved keras format to the tflite format. When I instantiate the converter I get warnings, but no errors:\r\n```\r\n>>> converter = tf.lite.TFLiteConverter.from_keras_model_file('model_keras') # TF1.15\r\n```\r\n```\r\nWARNING:tensorflow:From /home/mroos/python_envs/env_tensorflow1.15/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nWARNING:tensorflow:From /home/mroos/python_envs/env_tensorflow1.15/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nWARNING:tensorflow:From /home/mroos/python_envs/env_tensorflow1.15/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nWARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\r\nWARNING:tensorflow:From /home/mroos/python_envs/env_tensorflow1.15/lib/python3.6/site-packages/tensorflow_core/lite/python/util.py:249: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.convert_variables_to_constants`\r\nWARNING:tensorflow:From /home/mroos/python_envs/env_tensorflow1.15/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.extract_sub_graph`\r\nINFO:tensorflow:Froze 60 variables.\r\nINFO:tensorflow:Converted 60 variables to const ops.\r\n```\r\n\r\nThen when I call convert(), I get an error that I can't interpret:\r\n```\r\n>>> converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n>>> converter.representative_dataset = tf.lite.RepresentativeDataset(representative_dataset_gen)\r\n>>> converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] # For EdgeTPU, no float ops allowed\r\n>>> tflite_model = converter.convert()\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-8-7698de81c241> in <module>\r\n----> 1 tflite_model = converter.convert()\r\n      2 open('model.tflite', 'wb').write(tflite_model)\r\n\r\n~/python_envs/env_tensorflow1.15/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py in convert(self)\r\n    991     if self._is_calibration_quantize():\r\n    992       result = self._calibrate_quantize_model(result, inference_input_type,\r\n--> 993                                               inference_output_type)\r\n    994 \r\n    995     return result\r\n\r\n~/python_envs/env_tensorflow1.15/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py in _calibrate_quantize_model(self, result, inference_input_type, inference_output_type)\r\n    237     return calibrate_quantize.calibrate_and_quantize(\r\n    238         self.representative_dataset.input_gen, inference_input_type,\r\n--> 239         inference_output_type, allow_float)\r\n    240 \r\n    241   def _get_base_converter_args(self):\r\n\r\n~/python_envs/env_tensorflow1.15/lib/python3.6/site-packages/tensorflow_core/lite/python/optimize/calibrator.py in calibrate_and_quantize(self, dataset_gen, input_type, output_type, allow_float)\r\n     73     self._calibrator.Prepare()\r\n     74     for calibration_sample in dataset_gen():\r\n---> 75       self._calibrator.FeedTensor(calibration_sample)\r\n     76     return self._calibrator.QuantizeModel(\r\n     77         np.dtype(input_type.as_numpy_dtype()).num,\r\n\r\n~/python_envs/env_tensorflow1.15/lib/python3.6/site-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py in FeedTensor(self, input_value)\r\n    110 \r\n    111     def FeedTensor(self, input_value):\r\n--> 112         return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_FeedTensor(self, input_value)\r\n    113 \r\n    114     def QuantizeModel(self, input_py_type, output_py_type, allow_float):\r\n\r\nValueError: Failed to convert value into readable tensor.\r\n```\r\n", "@mattroos Thanks for trying, any chance you are converting the saved keras model that was created with tf2.0?\r\nIf it was created with tf1.15 and you are still having issue converting, then we might need to wait for a tensorflower to help out on this one :/", "@Namburger, I used a single script to create, save, and (attempt to) convert the model under TF1.15.", "@Namburger Thanks for the suggestion. Our code worked after downgrading to tf 1.15 and retraining the Keras model.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33558\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33558\">No</a>\n", "@adish333, do you mind sharing the lines of code you used for your working, TF1.15 conversion? E.g., did you set values for these attributes before calling convert()? Others?\r\n```\r\nconverter.inference_input_type = tf.float32\r\nconverter.inference_output_type = tf.float32\r\n```\r\n", "@mattroos I met the same issue when I tried to do post-training quantization:\r\n\r\n`Traceback (most recent call last):\r\n  File \"convert_tf.py\", line 183, in <module>\r\n    convert_pb_lite_quant()\r\n  File \"convert_tf.py\", line 80, in convert_pb_lite_quant\r\n    tflite_quant_model = converter.convert()\r\n  File \"/home/jiatian/.local/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py\", line 994, in convert\r\n    inference_output_type)\r\n  File \"/home/jiatian/.local/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py\", line 240, in _calibrate_quantize_model\r\n    inference_output_type, allow_float)\r\n  File \"/home/jiatian/.local/lib/python3.6/site-packages/tensorflow_core/lite/python/optimize/calibrator.py\", line 76, in calibrate_and_quantize\r\n    self._calibrator.FeedTensor(calibration_sample)\r\n  File \"/home/jiatian/.local/lib/python3.6/site-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py\", line 113, in FeedTensor\r\n    return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_FeedTensor(self, input_value)\r\nValueError: Failed to convert value into readable tensor.\r\n` \r\nDid you solve this?", "@JiatianWu, I think I had been using one of the nightly versions of tf1.15. After switching to tf1.15.0, that error went away.\r\n```\r\npip install tensorflow==1.15.0\r\n```\r\nI still get an error similar that of the original poster, however,\r\n```\r\nRuntimeError: Internal: :71 tf_lite_type != kTfLiteUInt8 (9 != 3)Node number 79 (EdgeTpuDelegateForCustomOp) failed to prepare.\r\n```\r\nwhen I build and run a model in which the input tensor is \"too large\", even with small models and an input tensor that isn't particularly large (e.g., (256, 256, 3) input into a model with one conv2d layer that has three input channels and 256 output channels).", "Same problem here with input tensors that are too large. @mattroos Did you solve your problem ?", "No, @alexanderfrey. I've been told by the coral support team that this a problem with the EdgeTPU runtime code, which is provided by their team of engineers (initial release was version 10, current release is version 12). They claimed it would be fixed in a future release but would not give a release date estimate. I'm not even sure it will be resolved in the next release, versus one further down the road. Hard to plan ahead on my projects with this kind of timeline-less problem.", "There is a new update from the coral team. Please have a look at it, you will find your solution. [January 2020 Updates ](https://coral.ai/news/updates-01-2020/)", "I haven't fully tested it, but at first pass, yes, the January updates resolved this. Thanks to the Google team!"]}, {"number": 33557, "title": "Support for Mediapipe tflite ops.", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0 \r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nMediapipe gives some custom implementation for few operators that can run only in Android GPU (Maxunpooling, Maxpoolingwithargmax2d, Convolution2dtransposebias). In tensorflow github repository, they have been mentioned as custom ops. But could not find out implementation for the same.Android GPU delegate does not run the model, and fails with error it is unresolved custom op.\r\n\r\n**Will this change the current api? How?**\r\nNo. It will be one another addition to the already existing operators in TFLite GPU delegate.\r\n\r\n**Who will benefit with this feature?**\r\nMediapipe, Tensorflow and Android researchers\r\n\r\n**Any Other info.**\r\nAttached with this is the sample hair segmentation TFLite, that they are using.\r\n\r\n[hair_segmentation.tflite.zip](https://github.com/tensorflow/tensorflow/files/3749221/hair_segmentation.tflite.zip)\r\n", "comments": ["Sorry for the late reply; I was out on a conference.\r\n\r\nWe have been trying to get those ops as part of builtin ops, but this never got prioritized.  Re-assigning to @jdduke ", "Which version of TensorFlow Lite are you using? In theory, it should now be possible (in the latest nightly) to apply the GPU delegate even if there are unresolved ops.", "How to build model with Convolution2DTransposeBias? I did not find any python implementation for the layer.", "I checked the latest benchmark, still I am not able to benchmark the hair segmentation model provided by Mediapipe.\r\n\r\nError log is attached below \r\n[benchmarking_issues.txt](https://github.com/tensorflow/tensorflow/files/3857883/benchmarking_issues.txt)\r\n", "@SanthoshRajendiran \r\n\r\nAh, thanks for sharing the log.  That makes it easier to understand.  Based on the logs, it looks like the op resolver doesn't know the custom ops.  Those need to be defined in the op resolver.  Take a look at these for examples:\r\n\r\nhttps://github.com/google/mediapipe/tree/master/mediapipe/util/tflite", "Just to be clear, you should still be able to benchmark this with benchmark_model by adding the `--use_gpu=true` flag. It looks like you're not enabling that in your command-line invocation.", "Actually, I tried that too.. That is also not working, attaching the error log herewith.\r\n[benchmarking_issues_gpu.txt](https://github.com/tensorflow/tensorflow/files/3878212/benchmarking_issues_gpu.txt)\r\n", "@impjdi Are you guys going to accept pull requests for this issue? I can help.", "@yxchng what exactly are you trying put in the PR?  implementations of those ops?", "@impjdi Yup. To make it possible to run MediaPipe models on PC.", "@yxchng Thank you for the offer, but that would directly create a conflict with what we have internally.  The best is if you convince TFLite team to make those ops (Maxunpooling, Maxpoolingwithargmax2d, Convolution2dtransposebias) a standard op (note that I'm not a member of the TFLite team directly).  Then, we can open source the three hidden ops.", "I was facing this issue but got a solution,\r\n\r\nThe initial code for Opresolver was \r\n```\r\ntflite::ops::builtin::BuiltinOpResolver resolver;\r\n```\r\nreplace it by \r\n```\r\ntflite::ops::builtin::BuiltinOpResolver builtins;\r\nbuiltins.AddCustom(\"MaxPoolingWithArgmax2D\", RegisterMaxPoolingWithArgmax2D());\r\nbuiltins.AddCustom(\"MaxUnpooling2D\", RegisterMaxUnpooling2D());\r\nbuiltins.AddCustom(\"Convolution2DTransposeBias\", RegisterConvolution2DTransposeBias());\r\nstd::unique_ptr<tflite::Interpreter> interpreter;\r\ntflite::InterpreterBuilder(*model.get(), builtins)(&interpreter);\r\n```\r\n\r\nAlso add the following code of the Custom Layers from the file \r\n<a href = \"https://github.com/google/mediapipe/blob/master/mediapipe/util/tflite/op_resolver.cc\">here</a>\r\n```\r\nTfLiteRegistration* RegisterMaxPoolingWithArgmax2D() {\r\n  static TfLiteRegistration reg = {\r\n      [](TfLiteContext*, const char*, size_t) -> void* {\r\n        return new TfLitePaddingValues();\r\n      },\r\n      [](TfLiteContext*, void* buffer) -> void {\r\n        delete reinterpret_cast<TfLitePaddingValues*>(buffer);\r\n      },\r\n      [](TfLiteContext* context, TfLiteNode* node) -> TfLiteStatus {\r\n        return kTfLiteOk;\r\n      },\r\n      [](TfLiteContext* context, TfLiteNode*) -> TfLiteStatus {\r\n        context->ReportError(\r\n            context, \"MaxPoolingWixthArgmax2D is only available on the GPU.\");\r\n        return kTfLiteError;\r\n      },\r\n  };\r\n  return &reg;\r\n}\r\n\r\nTfLiteRegistration* RegisterMaxUnpooling2D( ) {\r\n  static TfLiteRegistration reg = {nullptr, nullptr, nullptr, nullptr};\r\n  return &reg;\r\n}\r\n\r\nTfLiteRegistration* RegisterConvolution2DTransposeBias() {\r\n  static TfLiteRegistration reg = {nullptr, nullptr, nullptr, nullptr};\r\n  return &reg;\r\n}\r\n```\r\n\r\nAnd include the following headers\r\n```\r\n#include \"mediapipe/mediapipe/util/tflite/op_resolver.h\"\r\n#include \"tensorflow/lite/builtin_op_data.h\"\r\n```", "I have compiled Tensorflow lite with mediapipe custom ops \r\nhttps://github.com/kundanSingh11/TensorFlowLite-With-Mediapipe-Custom-Ops", "@SanthoshRajendiran,\r\nCan you please refer [this documentation](https://www.tensorflow.org/lite/examples/segmentation/overview) and let us know if this is what you are looking for? Also, please refer to the documentation for [Pose Estimation](https://www.tensorflow.org/lite/examples/pose_estimation/overview) and [Object Detection](https://www.tensorflow.org/lite/examples/object_detection/overview). \r\n\r\nHope this helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 33556, "title": "model.losses return null ?", "body": "I want to use regularization loss to improve model accuracy, so I follow the official tutorial, but model.losses return **[]**, and got \"**ValueError: inputs must be a list of at least one Tensor/IndexedSlices with the same dtype and shape**\", Is there something wrong and how to fix it?\r\n```\r\ndef train_step(images, labels):\r\n    with tf.GradientTape() as tape:\r\n        predictions = model(images, training=True)\r\n        pred_loss = loss_object(labels, predictions)\r\n        print(\"model.losses:\", model.losses)  \r\n        regularization_loss = tf.math.add_n(model.losses)\r\n        total_loss = pred_loss + regularization_loss\r\n    gradients = tape.gradient(total_loss, model.trainable_variables)\r\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n```\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. \r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "@Jessespace Did you set the `kernel_regularizer` arg in the respective layers while building the model?\r\nThe following code seems to work fine for me\r\n```\r\nfeatures = tf.keras.Input(shape=[32, 32, 3])\r\nx = tf.keras.layers.Conv2D(filters=8,\r\n                           kernel_size=3,\r\n                           activation='relu',\r\n                           kernel_regularizer=tf.keras.regularizers.l2(l=1e-4))(features)\r\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\r\nlogits = tf.keras.layers.Dense(units=4, activation=None)(x)\r\nmodel = tf.keras.Model(inputs=[features], outputs=[logits])\r\n\r\nx_train = tf.random.normal(shape=[1000, 32, 32, 3])\r\ny_train = tf.random.uniform(shape=[1000], dtype=tf.int32, maxval=4)\r\ndataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(8, drop_remainder=True).repeat().prefetch(1)\r\n\r\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\r\nfor images, labels in dataset:\r\n    with tf.GradientTape() as tape:\r\n        predictions = model(images, training=True)\r\n        pred_loss = loss_object(labels, predictions)\r\n        print(\"model.losses:\", model.losses)  \r\n        regularization_loss = tf.math.add_n(model.losses)\r\n        total_loss = pred_loss + regularization_loss\r\n    gradients = tape.gradient(total_loss, model.trainable_variables)\r\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n```\r\n\r\nhere is the stdout\r\n```\r\nmodel.losses: [<tf.Tensor: id=593, shape=(), dtype=float32, numpy=0.0004570447>]\r\nmodel.losses: [<tf.Tensor: id=831, shape=(), dtype=float32, numpy=0.00045690875>]\r\nmodel.losses: [<tf.Tensor: id=959, shape=(), dtype=float32, numpy=0.00045684364>]\r\nmodel.losses: [<tf.Tensor: id=1087, shape=(), dtype=float32, numpy=0.00045669058>]\r\nmodel.losses: [<tf.Tensor: id=1215, shape=(), dtype=float32, numpy=0.00045654297>]\r\nmodel.losses: [<tf.Tensor: id=1343, shape=(), dtype=float32, numpy=0.0004563856>]\r\nmodel.losses: [<tf.Tensor: id=1471, shape=(), dtype=float32, numpy=0.00045626235>]\r\nmodel.losses: [<tf.Tensor: id=1599, shape=(), dtype=float32, numpy=0.00045610484>]\r\nmodel.losses: [<tf.Tensor: id=1727, shape=(), dtype=float32, numpy=0.00045594215>]\r\nmodel.losses: [<tf.Tensor: id=1855, shape=(), dtype=float32, numpy=0.00045577507>]\r\nmodel.losses: [<tf.Tensor: id=1983, shape=(), dtype=float32, numpy=0.00045561302>]\r\nmodel.losses: [<tf.Tensor: id=2111, shape=(), dtype=float32, numpy=0.00045547922>]\r\nmodel.losses: [<tf.Tensor: id=2239, shape=(), dtype=float32, numpy=0.00045535524>]\r\nmodel.losses: [<tf.Tensor: id=2367, shape=(), dtype=float32, numpy=0.0004552529>]\r\n```", "> activation\r\n\r\nI used your method and can really solve the problem above.But still have 2 questions:\r\n1) When I use **tf.keras.applications.MobileNetV2**, how to add regularization loss?\r\n```python\r\nbase_model = tf.keras.applications.MobileNetV2(\r\n    input_shape=IMG_SHAPE,\r\n    include_top=False,\r\n    weights='imagenet'\r\n)\r\nbase_model.trainable = False\r\nmaxpool_layer = tf.keras.layers.GlobalMaxPooling2D()\r\nprediction_layer = tf.keras.layers.Dense(3, activation='sigmoid')\r\nmodel = tf.keras.Sequential([\r\n    base_model,\r\n    maxpool_layer,\r\n    prediction_layer\r\n])```\r\n2) Why is it defined in this form? I only need to add a regular term to the loss function. Why can't I call it as simple as pytorch or mxnet? It's so complicated and confusing.", "Closing the issue as it has been resolved by @srihari-humbarwadi's comment. You can raise new issue for other queries. Thanks!"]}, {"number": 33555, "title": "[TF 2.0] repeatdataset has no attribute 'make_one_shot_iterator'", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):window10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):2.0.0\r\n- Python version:3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:colab\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nwhen I tried to train with estimator object, i can see this error below.\r\nAttributeError: 'RepeatDataset' object has no attribute 'make_one_shot_iterator'\r\ndef mapping_fn(X,Y):\r\n  input, labels = {'x':X},Y\r\n  return input,labels\r\n\r\ndef train_input_fn():\r\n  dataset = tf.data.Dataset.from_tensor_slices((input_train, label_train))\r\n  dataset = dataset.shuffle(buffer_size = len(input_train))\r\n  dataset = dataset.batch(BATCH_SIZE)\r\n  dataset = dataset.map(mapping_fn)\r\n  dataset = dataset.repeat(count = NUM_EPOCHS)\r\n  iterator = dataset.make_one_shot_iterator()\r\n\r\n  return iterator.get_next()\r\n\r\ndef eval_input_fn():\r\n  dataset = tf.data.Dataset.from_tensor_slices((input_eval, label_eval))\r\n  dataset = dataset.shuffle(buffer_size = len(input_eval))\r\n  dataset = dataset.batch(16)\r\n  dataset = dataset.map(mapping_fn)\r\n  iterator =dataset.make_one_shot_iterator()\r\n\r\n  return iterator.get_next()\r\n\r\nwirh model_fn = CNN, RNN model \r\nest = tf.estimator.Estimator(model_fn, model_dir = \"/content/gdrive/checkpoint/cnn_model\")\r\nest.train(train_input_fn)\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["As per Release 2.0.0-alpha0, tf.data.Dataset.make_one_shot_iterator() has been deprecate in V1, removed from V2, and added to tf.compat.v1.data.make_one_shot_iterator(). \r\n\r\nFor more details you can refer to the following [issue](https://github.com/tensorflow/tensorflow/issues/29252)"]}, {"number": 33554, "title": "I have installed tensorflow 2.0 through Anaconda prompt, however the jupyternotebook can't find the tf2.0 module.", "body": "**System information**\r\n- Window 10 operation system\r\n- TensorFlow 2.0:\r\n- Python 3.6:\r\n- Installed using pip;\r\n- Downloaded from pypi.org.\r\n\r\n**Problem**\r\nI have installed TensorFlow 2.0 through Anaconda prompt, however, the jupyter notebook can't find the TensorFlow module. The detailed error is \r\n\r\n> ModuleNotFoundError: No module named 'tensorflow'.\r\n\r\nThrough the command 'pip list', I can find the TF 2.0.0 in my env. Meanwhile, when I using PyCharm in the env with TF 2.0.0, the PyCharm can find the TF 2.0.0 module.\r\n\r\n", "comments": ["**Try installing tensorflow this way:**\r\n\r\nCreate a virtual environment (recommended)\r\n\r\n```\r\n(base) C:\\Users\\XXXX>conda create -n venv pip python=3.6\r\n(base) C:\\Users\\XXXX>conda activate venv\r\n(venv) C:\\Users\\XXXX>pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-2.0.0-cp36-cp36m-win_amd64.whl\r\n```\r\n\r\n**Then test it in terminal:**\r\n\r\n```\r\n(venv) C:\\Users\\XXXX>python\r\nPython 3.6.8 |Anaconda, Inc.| (default, Feb 21 2019, 18:30:04) [MSC v.1916 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n```", "Thank you very much. My problem was solved with your method. @gowthamkpr ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33554\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33554\">No</a>\n"]}, {"number": 33553, "title": "fixed cond with grouped summaries in one branch", "body": "#24815 \r\nall credit goes to @ila96 ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33553) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33553) for more info**.\n\n<!-- ok -->", "Looks like there's test failure:\r\n\r\n======================================================================\r\nERROR: testCondWithGroupAndSummaries (__main__.CondTest)\r\ntestCondWithGroupAndSummaries (__main__.CondTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/control_flow_ops_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py\", line 37, in testPartExecutor\r\n    yield\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/control_flow_ops_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py\", line 162, in run\r\n    testMethod()\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/control_flow_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/control_flow_ops_test.py\", line 460, in testCondWithGroupAndSummaries\r\n    self.evaluate(summary_ops_v2.summary_writer_initializer_op())\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/control_flow_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/summary_ops_v2.py\", line 553, in summary_writer_initializer_op\r\n    \"tf.contrib.summary.summary_writer_initializer_op is only \"\r\nRuntimeError: tf.contrib.summary.summary_writer_initializer_op is only supported in graph mode.\r\n", "I noticed the error and I updated code with new commit. However, I do not know how to force the CI to run again. @googlebot  force-run", "@martinwicke  Could you help me force-run again? thank you", "`Ubuntu Sanity` failed because of indentation.\r\n```\r\n\r\nFAIL: Found 6 non-whitelisted pylint errors:\r\ntensorflow/python/ops/control_flow_ops_test.py:455: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 6\r\n\r\ntensorflow/python/ops/control_flow_ops_test.py:456: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 6\r\n\r\ntensorflow/python/ops/control_flow_ops_test.py:457: [W0311(bad-indentation), ] Bad indentation. Found 12 spaces, expected 8\r\n\r\ntensorflow/python/ops/control_flow_ops_test.py:462: [W0311(bad-indentation), ] Bad indentation. Found 12 spaces, expected 8\r\n\r\ntensorflow/python/ops/control_flow_ops_test.py:463: [W0311(bad-indentation), ] Bad indentation. Found 12 spaces, expected 8\r\n\r\ntensorflow/python/ops/control_flow_ops_test.py:464: [W0311(bad-indentation), ] Bad indentation. Found 12 spaces, expected 8\r\n```", "Hi @brianwa84,  I fixed the test, indentation and cleaned up commit history. Could you have a look at it? ", "@martinwicke  Hi, could you help me trigger CI again? thanks.", "The Android App failure is a flake unrelated to your change.", "Thanks Martin for clearing that up. @brianwa84 I've updated the code and CI is almost done. What should I do next to get the CI job to complete? ", "`Ubuntu CPU` failed because of `save_model` which is totally unrelated to the current branch.  And I rebased my branch on top of upstream master. Could you help me rerun the CI again. thanks. I think last time I had some similar issues and it has been resolved by rebasing.  @brianwa84 ", "`Ubuntu CC` option in CI is stuck, and there are no logs or anything. How can I fix this? ", "I saw one test failure creating the tb directory. Can you use\nself.create_tempdir('tb').full_path\n\nOn Tue, Oct 29, 2019, 6:11 AM Yusup <notifications@github.com> wrote:\n\n> Ubuntu CC option in CI is stuck, and there are no logs or anything. How\n> can I fix this?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/33553?email_source=notifications&email_token=AFJFSI4OGVLYOGLV4IRB7EDQRAD6DA5CNFSM4JCXCDO2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECP5K2Y#issuecomment-547345771>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AFJFSIZ4HK7R2YJJXC65W7LQRAD6DANCNFSM4JCXCDOQ>\n> .\n>\n", "Hi @brianwa84 ,  I've changed it to the  `temp_dir` ", " is `Ubuntu CC` in this PR somehow correlated to  #33798? @brianwa84 ", "@gbaned Hi,  Ubuntu CC is stuck and I cant see logs or anything. How can we move forward?"]}, {"number": 33552, "title": "Incorrect Arduino Person Detection .zip package", "body": "## URL(s) with the issue:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/person_detection\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/person_detection#obtain-and-import-the-library\r\n\r\n## Description of issue (what needs changing):\r\nUnder the heading \"Obtain and Import the Library\", the link incorrectly refers to the micro_speech.zip package instead of a person_detection example. \r\n\r\n", "comments": ["I am taking this problem!", "@smellslikeml  would you please elaborate on this issue...", "@Samsomyajit Certainly, both \"person_detection\" and the \"magic_wand\" Arduino demos under tensorflow/lite/experimental/micro/examples (under heading: Obtain and Import the Library) incorrectly link the micro_speech.zip and the hello_world.zip packages, respectively.", "True I get it..\n\nOn Mon, Oct 21, 2019 at 11:25 PM smellslikeml <notifications@github.com>\nwrote:\n\n> @Samsomyajit <https://github.com/Samsomyajit> Certainly, both\n> \"person_detection\" and the \"magic_wand\" Arduino demos under\n> tensorflow/lite/experimental/micro/examples (under heading: Obtain and\n> Import the Library) incorrectly link the micro_speech.zip and the\n> hello_world.zip packages, respectively.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33552?email_source=notifications&email_token=AJ45N5ZE55MBQKY3DNHE32TQPXUJBA5CNFSM4JCW77R2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEB3GEBI#issuecomment-544629253>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AJ45N55YVO2YN2ATYMFHCWTQPXUJBANCNFSM4JCW77RQ>\n> .\n>\n", "@smellslikeml That probaby deals with our problem here...I have suggested the necessary changes", "@Samsomyajit thanks! Look forward to the update.", "@Samsomyajit I think I found the package! \r\nhttps://storage.googleapis.com/tensorflow-nightly/github/tensorflow/tensorflow/lite/experimental/micro/tools/make/gen/arduino_x86_64/prj/person_detection/tensorflow_lite.zip\r\n\r\nIt's named tensorflow_lite.zip instead of person_detection.zip\r\n\r\nWe were able to flash the example correctly after renaming the zip to person_detection.zip, renaming the library name as it appears in ``` Arduino/libraries/ ``` to person_detection, as well as changing the name in the ``` library.properties``` file in the zip to ```name=TensorFlowLite:person_detection```", "Hey, I was looking for the link to the library.....thank you!\n\ud83d\ude0a\n\nOn Tue, Oct 22, 2019, 7:30 PM Salma Mayorquin <notifications@github.com>\nwrote:\n\n> @Samsomyajit <https://github.com/Samsomyajit> I think I found the package!\n>\n> https://storage.googleapis.com/tensorflow-nightly/github/tensorflow/tensorflow/lite/experimental/micro/tools/make/gen/arduino_x86_64/prj/person_detection/tensorflow_lite.zip\n>\n> It's named tensorflow_lite.zip instead of person_detection.zip\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33552?email_source=notifications&email_token=AJ45N52DZJRXZLIQVXBKTGLQP4BQTA5CNFSM4JCW77R2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEB5Z74A#issuecomment-544972784>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AJ45N53Y3G4QKEEAF42ITV3QP4BQTANCNFSM4JCW77RQ>\n> .\n>\n", "@mayorquinmachines   I have updated the link to the library. Thnx for the link", "@smellslikeml Can I close this issue as the update to the docs is being tracked [here](https://github.com/tensorflow/tensorflow/pull/33579). Thanks!", "@gowthamkpr Sure! Thank you :)"]}, {"number": 33551, "title": "Failure to build tensorflow in Ubuntu", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master 0f752f7368f88b61916a3d1700685d01aff835cc\r\n- Python version: Python 3.6.8\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 1.0.0\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nI am unable to rebuild the tensorflow library.\r\n\r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nCloned the repository as per instructions on https://www.tensorflow.org/lite/microcontrollers/library. When I tried to build using this command\r\n\r\nmake -f tensorflow/lite/experimental/micro/tools/make/Makefile test\r\n\r\nThe build fails:\r\n```\r\ng++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/kernels/logical.cc -o tensorflow/lite/experimental/micro/tools/make/gen/linux_x86_64/obj/tensorflow/lite/experimental/micro/kernels/logical.o\r\nIn file included from ./tensorflow/lite/kernels/internal/reference/binary_function.h:18:0,\r\n                 from tensorflow/lite/experimental/micro/kernels/logical.cc:16:\r\n./tensorflow/lite/kernels/internal/common.h:24:10: fatal error: fixedpoint/fixedpoint.h: No such file or directory\r\n #include \"fixedpoint/fixedpoint.h\"\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\ntensorflow/lite/experimental/micro/tools/make/Makefile:244: recipe for target 'tensorflow/lite/experimental/micro/tools/make/gen/linux_x86_64/obj/tensorflow/lite/experimental/micro/kernels/logical.o' failed\r\nmake: *** [tensorflow/lite/experimental/micro/tools/make/gen/linux_x86_64/obj/tensorflow/lite/experimental/micro/kernels/logical.o] Error 1\r\n```\r\n\r\n", "comments": ["@photogazer I noticed you are using Bazel 1.0.0. If I remember bazel 1.0.0 is still not supported yet. Could try with bazel 0.29.1 and see if the issue persists?", "> @photogazer I noticed you are using Bazel 1.0.0. If I remember bazel 1.0.0 is still not supported yet. Could try with bazel 0.29.1 and see if the issue persists?\r\n\r\nI am now on bazel 0.29.1, but I am seeing the same error.\r\n\r\n", "@rmothukuru This is a TF Lite build issue. Can you re-triage to one of their build experts, please? I'm not sure who owns the microcontroller builds here.", "This header file, fixedpoint.h, should have been downloaded by the make command to the tensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/fixedpoint/ directory.  Could you check if this gemmlowp directory has been successfully downloaded?  Possibly there was some issue with this download that scrolled by in your error log?\r\n\r\nAlso, the make command you executed is trying to build the tests for your host operating system, an x86 system.  If you want to compile code for a microcontroller platform, you'll need to compile for a specific target, set by adding TARGET=xxx on your commandline, as noted in that documentation you referenced.", "Yes, fixedpoint.h is located at the location you have indicated.\r\n\r\nStrangely, when I try to rerun the same command to build tonight, the build went through to completion: \r\n\r\n```\r\n~~~ALL TESTS PASSED~~~\r\n\r\ntensorflow/lite/experimental/micro/tools/make/gen/linux_x86_64/bin/kernel_add_test: PASS\r\n```\r\n\r\nI did not rebase. So perhaps there was a temporary issue connecting to a server last week. In any case I no longer see the build issue for now. Thanks for the help!", "Glad you got it figured out!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33551\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33551\">No</a>\n"]}, {"number": 33550, "title": "tf.keras.Model.fit ignores class_weight when passed tf.data.Dataset", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.6 or Colab\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0, 1.15.0-rc3\r\n- Python version: 3.7\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: None or Colab\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n`tf.keras.Model.fit` ignores `class_weight` when passed a `tf.data.Dataset`.  When passed an instance of `tf.data.Dataset` and a `class_weight` dictionary with nonsensical label keys, it runs without exception, whereas it correctly raises `ValueError` when passed a pair of `np.ndarray`.\r\n\r\n**Describe the expected behavior**\r\n\r\n`tf.keras.Model.fit` should apply `class_weight` when passed a `tf.data.Dataset`.  It should raise an exception for incorrect `class_weight` keys.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfeatures = np.array([[-1.], [1.]], dtype=np.float32)\r\nlabels = np.array([[0], [1]], dtype=np.int32)\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices((features, labels)).batch(1)\r\n\r\nmodel = tf.keras.Sequential([tf.keras.layers.Dense(1, activation='sigmoid')])\r\n\r\nmodel.compile(optimizer='sgd', loss='binary_crossentropy')\r\n\r\nclass_weight = {'bad_negative_label': 0.5, 'bad_positive_label': 0.5}\r\n\r\nfit_ops = (('tf.data.Dataset', lambda: model.fit(dataset, class_weight=class_weight, verbose=0)),\r\n           ('np.ndarray', lambda: model.fit(features, labels, class_weight=class_weight, verbose=0)))\r\n\r\nfor key, fit_op in fit_ops:\r\n    try:\r\n        print(f'fitting {key} with bad class_weight label')\r\n        fit_op()\r\n    except ValueError as e:\r\n        print('failed as it should have')\r\n        raise ValueError from e\r\n    else:\r\n        print('succeded but should have failed')\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nfitting tf.data.Dataset with bad class_weight label\r\nsucceded but should have failed\r\nfitting np.ndarray with bad class_weight label\r\nfailed as it should have\r\nTraceback (most recent call last):\r\n  File \"scratch.py\", line 22, in <module>\r\n    fit_op()\r\n  File \"scratch.py\", line 17, in <lambda>\r\n    ('np.ndarray', lambda: model.fit(features, labels, class_weight=class_weight, verbose=0)))\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 728, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 224, in fit\r\n    distribution_strategy=strategy)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 547, in _process_training_inputs\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 594, in _process_inputs\r\n    steps=steps)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 2530, in _standardize_user_data\r\n    feed_sample_weight_modes)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 2529, in <listcomp>\r\n    for (ref, sw, cw, mode) in zip(y, sample_weights, class_weights,\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\", line 946, in standardize_weights\r\n    '`class_weight`.' % (existing_classes - existing_class_weight))\r\nValueError: `class_weight` must contain all classes in the data. The classes {0, 1} exist in the data but not in `class_weight`.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"scratch.py\", line 25, in <module>\r\n    raise ValueError from e\r\nValueError\r\n\r\nProcess finished with exit code 1\r\n```\r\n", "comments": ["I could reproduce the issue on colab with TF 2.0.0.\r\nPlease see the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/12fbb6df6ff8c99dce74b1464b725141/untitled213.ipynb). Thanks", "Could it be that the call to `model._standardize_user_data` in training_v2.py:\r\nhttps://github.com/tensorflow/tensorflow/blob/8b324344adacdfd37587d1fa591aab0e6a23a6ac/tensorflow/python/keras/engine/training_v2.py#L641\r\nshould also be passed `extract_tensors_from_dataset=True`?", "p.s. Or, could it be that calling `train_on_batch` by running `strategy.experimental_run_v2` on `per_replica_function` in https://github.com/tensorflow/tensorflow/blob/8b324344adacdfd37587d1fa591aab0e6a23a6ac/tensorflow/python/keras/engine/training_v2_utils.py#L72 \r\nis failing to pass the `class_weight` to `train_on_batch`?  (Unless there is a partial binding of `class_weight` kwarg to `train_on_batch` somewhere, but I don't see it.).   \r\nThank you.", "We are having a similar problem. It would be nice if we could use class_weights in model.fit() instead of relying on adding a sample weight to the tf.data.Dataset (x, y, weight).", "It is very dangerous for the training behavior to completely change, in terms of applying vs. ignoring without any warning the `class_weight` kwarg, simply by going from `model.fit(x, y, class_weight=class_weight, ...)` to `model.fit(tf.data.Dataset.from_tensor_slices((x, y)).batch(...), class_weight=class_weight, ...)`.  \r\n\r\nConsider for example the [Classification on imbalanced data](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data) tutorial that specifically demos `class_weight`.  By simply replacing `train_features, test_features` with `tf.data.Dataset.from_tensor_slices((train_features, test_features)).batch(BATCH_SIZE)` (and similar for `validation_data`, and also removing the `batch_size` kwarg in `model.fit`), the tutorial example **fails** to apply `class_weight`, again, without any warning.", "Has this problem been resolved in tf2.1?", "@mmilosav Thanks for the issue!\r\n\r\nThis has been fixed in the latest tf-nightly: `pip install -U tf-nightly`", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33550\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33550\">No</a>\n"]}, {"number": 33549, "title": "I want to train my model on gpu. So i installed everything which given in tensorflow-gpu. But now i want to go back on cpu. So, how to come back on cpu ? ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Can you please go through the [link](https://www.tensorflow.org/guide/gpu) , #31135 and see if it helps you.Also, please let us know which TensorFlow version you are using ?.Thanks!", "tensorflow version 2.0.0", "Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 23:09:28) [MSC v.1916 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license()\" for more information.\r\n>>> from __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\n>>> from __future__ import absolute_import, division, print_function, unicode_literals\r\n>>> import tensorflow as tf\r\n>>> print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\nNum GPUs Available:  0\r\n>>> tf.debugging.set_log_device_placement(True)\r\n>>> a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\r\n>>> b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\r\n>>> c = tf.matmul(a, b)\r\n>>> print(c)\r\ntf.Tensor(\r\n[[22. 28.]\r\n [49. 64.]], shape=(2, 2), dtype=float32)\r\n\r\nNum GPUs Available:  0 .....here it's shown no gpu avilable. but i have NVIDIA Geforce 940MX. \r\n", "Since your system fails to recognize gpu it tells us that tf-gpu was not correctly configured.\r\nCan you please confirm following things;\r\nDid you install TF-GPU version? ```pip install tensorflow-gpu ```\r\nSee https://www.tensorflow.org/install/gpu\r\nDownloaded cuda toolkit and updated cuda, cupti, cudnn environment variables?\r\nSee https://www.tensorflow.org/install/gpu#software_requirements"]}, {"number": 33548, "title": "Getting error following steps in 'Building Standard TensorFlow ModelServer'", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/tfx/serving/serving_advanced\r\n\r\n## Description of issue (what needs changing):\r\n\r\nWhen I attempt to execute  the \r\n```\r\ntools/run_in_docker.sh python tensorflow_serving/example/mnist_saved_model.py \\\r\n  --training_iteration=100 --model_version=1 /tmp/mnist\r\n```\r\nstep in the in the tutorial I get the this error:\r\n\r\n```\r\nserving % tools/run_in_docker.sh python tensorflow_serving/example/mnist_saved_model.py \\\r\n  --training_iteration=100 --model_version=1 /tmp/mnist\r\n== Pulling docker image: tensorflow/serving:nightly-devel\r\nnightly-devel: Pulling from tensorflow/serving\r\n22e816666fd6: Already exists \r\n079b6d2a1e53: Already exists \r\n11048ebae908: Already exists \r\nc58094023a2e: Already exists \r\ne9d1145448f7: Pull complete \r\n3b2b266356de: Pull complete \r\n9f9b2b982b72: Pull complete \r\nede8854b3a01: Pull complete \r\n7bb55a638df9: Pull complete \r\nbdd9b510b8a7: Pull complete \r\n90a5454f6928: Pull complete \r\n1941316fdbd3: Pull complete \r\nc9c9a434ee49: Pull complete \r\nDigest: sha256:3b52152115c73a6be79a86cda94c4c94569df9b490a3e40c2530d5a9a007afac\r\nStatus: Downloaded newer image for tensorflow/serving:nightly-devel\r\ndocker.io/tensorflow/serving:nightly-devel\r\n== Running cmd: sh -c 'cd /Users/***REMOVED***/GitHub/serving; python tensorflow_serving/example/mnist_saved_model.py --training_iteration=100 --model_version=1 /tmp/mnist'\r\nTraceback (most recent call last):\r\n  File \"tensorflow_serving/example/mnist_saved_model.py\", line 39, in <module>\r\n    tf.app.flags.DEFINE_integer('training_iteration', 1000,\r\nAttributeError: 'module' object has no attribute 'app'\r\n```\r\nI am running this on Mac OS Catalina w/ 8GB RAM allocated to the Docker Engine.\r\n", "comments": ["You must have missed an import statement.\r\n\r\nCan you share some of your code which will help understanding the issue?", "@peter-mourfield,\r\nThis issue is related to Tensorflow Serving. Request you to raise an issue in [TF Serving Repository](https://github.com/tensorflow/serving/issues), as that Repo will be looked at by Experts in TF Serving.  Thanks!", "Thanks @rmothukuru! I've opened an issue in TF Serving. https://github.com/tensorflow/serving/issues/1468"]}, {"number": 33547, "title": "Optimize a data  member from map to vector for LoopInvariantNodeMotionOptimizer ", "body": "fix a TODO in LoopInvariantNodeMotionOptimizer,\r\n```\r\n// TODO(rmlarsen): Use vector instead of map, since frames ids are dense.\r\n```\r\n\r\nmodify modify frame_parent_ and frame_children_  from std::map to std::vector", "comments": ["@qqsun8819 Could you please address Ubuntu Sanity errors? Thanks!", "Sanity should be fixed now, retriggering."]}, {"number": 33546, "title": "Do not use memcpy in Go", "body": "Recently, Go added new cgocheck for doubtful usage of pointer.\r\n\r\nhttps://github.com/golang/go/commit/f9226454b9830dd7fe6405bdb2953a6747dce41b\r\n\r\n```\r\nfatal error: can't happen\r\n\r\ngoroutine 1 [running]:\r\nruntime.throw(0x640e64, 0xc)\r\n\tC:/dev/go/src/runtime/panic.go:774 +0x79 fp=0xc00007f970 sp=0xc00007f940 pc=0x430e49\r\nruntime.cgoCheckPointer(0x5eec00, 0xc0000c0000, 0xc000044460, 0x1)\r\n\tC:/dev/go/src/runtime/cgocall.go:448 +0x1b9 fp=0xc00007f9b8 sp=0xc00007f970 pc=0x404a79\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.(*Graph).ImportWithOptions.func8(0x3260cb0, 0xc00007fba0, 0xf86f60)\r\n\tC:/Users/mattn/go/src/github.com/tensorflow/tensorflow/tensorflow/go/graph.go:136 +0x10d fp=0xc00007fa10 sp=0xc00007f9b8 pc=0x5a3a3d\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.(*Graph).ImportWithOptions(0xc000006088, 0xc0000c0000, 0x1bd5509, 0x1bd5709, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0)\r\n\tC:/Users/mattn/go/src/github.com/tensorflow/tensorflow/tensorflow/go/graph.go:136 +0x1f3 fp=0xc00007fb98 sp=0xc00007fa10 pc=0x598563\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.(*Graph).Import(...)\r\n\tC:/Users/mattn/go/src/github.com/tensorflow/tensorflow/tensorflow/go/graph.go:153\r\nmain.main()\r\n\tC:/Users/mattn/go/src/github.com/mattn/go-object-detect-from-image/main.go:159 +0x577 fp=0xc00007ff60 sp=0xc00007fb98 pc=0x5ac4d7\r\nruntime.main()\r\n\tC:/dev/go/src/runtime/proc.go:203 +0x21e fp=0xc00007ffe0 sp=0xc00007ff60 pc=0x43281e\r\nruntime.goexit()\r\n\tC:/dev/go/src/runtime/asm_amd64.s:1375 +0x1 fp=0xc00007ffe8 sp=0xc00007ffe0 pc=0x45b4e1\r\n```\r\n\r\nThis pull-request fix to not use memcpy. cc: @mdempsky ", "comments": ["FYI, the original panic isn't because of the new unsafe.Pointer checking that I added for Go 1.14. I suspect it's simply because of a mismatch between your installed versions of cmd/cgo and package runtime (the ABI changed in golang/go@e85ffec7). If you're still getting `fatal error: can't happen` errors after running make.bash, please file an issue at golang.org/issue/new.\r\n\r\nThat said, the current revision of the PR looks correct to me, and I think is a worthwhile improvement/simplification anyway."]}, {"number": 33545, "title": "TypeError: Expected Operation, Variable, or Tensor, got 0", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):NONE\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NONE\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):2.0\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):NONE\r\n- GCC/Compiler version (if compiling from source):NONE\r\n- CUDA/cuDNN version:NONE\r\n- GPU model and memory:NONE\r\n\r\n**Describe the current behavior**\r\n```\r\nFile \"keras_to_tensorflow.py\", line 186, in <module>\r\n    app.run(main)\r\n  File \"/home/venv/lib/python3.6/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/venv/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"keras_to_tensorflow.py\", line 132, in main\r\n    model = load_model(FLAGS.input_model, FLAGS.input_model_json, FLAGS.input_model_yaml)\r\n  File \"keras_to_tensorflow.py\", line 65, in load_model\r\n    model = vgg_blstm_ctc.model(is_training=False, img_size=(256,32), num_classes=11, max_label_length=26)\r\n  File \"/home/keras_to_tensorflow/vgg_blstm_ctc.py\", line 70, in model\r\n    y = Bidirectional(LSTM(256, kernel_initializer=initializer, return_sequences=True), merge_mode='sum', name='LSTM_1')(rnn_input) # 32*512\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/layers/wrappers.py\", line 437, in __call__\r\n    return super(Bidirectional, self).__call__(inputs, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 75, in symbolic_fn_wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\", line 489, in __call__\r\n    output = self.call(inputs, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/layers/wrappers.py\", line 530, in call\r\n    initial_state=forward_state, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/layers/recurrent.py\", line 2247, in call\r\n    initial_state=initial_state)\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/layers/recurrent.py\", line 682, in call\r\n    input_length=timesteps)\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 3105, in rnn\r\n    targets=[last_output])\r\n  File \"/home/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/tf_utils.py\", line 134, in get_reachable_from_inputs\r\n    raise TypeError('Expected Operation, Variable, or Tensor, got ' + str(x))\r\nTypeError: Expected Operation, Variable, or Tensor, got 0\r\n```\r\n\r\n**Describe the expected behavior**\r\nwhen I convert the keras model to the tensorflow model, but I get an error above. anybody can help me?\r\n\r\n**Code to reproduce the issue**\r\n\r\nthe `keras_to_tensorflow.py` file is from [here](https://github.com/amir-abdi/keras_to_tensorflow),  the `vgg_blstm_ctc.py` file is from [here](https://github.com/DevilExileSu/BankCardOCR), the input model is from [here](https://github.com/DevilExileSu/BankCardOCR/tree/master/train/model).I has changed the `keras_to_tensorflow.py` file like this:\r\n\r\n```\r\ndef load_model(input_model_path, input_json_path=None, input_yaml_path=None):\r\n    if not Path(input_model_path).exists():\r\n        raise FileNotFoundError(\r\n            'Model file `{}` does not exist.'.format(input_model_path))\r\n    try:\r\n        #model = keras.models.load_model(input_model_path)\r\n        model = vgg_blstm_ctc.model(is_training=False, img_size=(256,32), num_classes=11, max_label_length=26)\r\n        return model \r\n    except FileNotFoundError as err:\r\n        logging.error('Input mode file (%s) does not exist.', FLAGS.input_model)\r\n        raise err\r\n```\r\n", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!\r\n\r\nPlease post it in Stackoverflow with the standalone code, we will resolve it there. Thanks!\r\n", "> This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!\r\n> \r\n> Please post it in Stackoverflow with the standalone code, we will resolve it there. Thanks!\r\n\r\nI have solved the problem, thx.", "> > This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!\r\n> > Please post it in Stackoverflow with the standalone code, we will resolve it there. Thanks!\r\n> \r\n> I have solved the problem, thx.\r\n\r\nI am facing same problem .Please could you help me for the same", "> > This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!\r\n> > Please post it in Stackoverflow with the standalone code, we will resolve it there. Thanks!\r\n> \r\n> I have solved the problem, thx.\r\n\r\nI am facing same problem .Please could you help me for this. It seems that when I use the keras.backend.set_learning_phase(0), than the exception will be throwed. What should I do when I convert the h5 to tensorflow pb.", "> > > This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!\r\n> > > Please post it in Stackoverflow with the standalone code, we will resolve it there. Thanks!\r\n> > \r\n> > \r\n> > I have solved the problem, thx.\r\n> \r\n> I am facing same problem .Please could you help me for the same\r\n\r\nIf you fix this problem, can you help me for this please. It seems that when I use the keras.backend.set_learning_phase(0), than the exception will be throwed. What should I do when I convert the h5 to tensorflow pb. ", "@jacpy Can you please provide the solution you for for the problem? I am running into the same issue passing a Tensor for an Embedding layer to an LSTM. That would help me a lot in my researches. Thank you", "> @jacpy Can you please provide the solution you for for the problem? I am running into the same issue passing a Tensor for an Embedding layer to an LSTM. That would help me a lot in my researches. Thank you\r\n\r\nSolved my issue. Some updates concerning Keras and Tensorflow. The best option so far is replacing:\r\n\r\n```import keras as K```\r\n\r\nto:\r\n\r\n```import tensorflow.python.keras as K```\r\n\r\nafter updating tensorflow and keras to their latest version.", "Use older versions of the libraries would solve the problem:\r\ntensorflow==1.13.2\r\nkeras==2.2.4", "> @jacpy Can you please provide the solution you for for the problem? I am running into the same issue passing a Tensor for an Embedding layer to an LSTM. That would help me a lot in my researches. Thank you\r\n> \r\n> Solved my issue. Some updates concerning Keras and Tensorflow. The best option so far is replacing:\r\n> import keras as K\r\n> to:\r\n> import tensorflow.python.keras as K\r\n> after updating tensorflow and keras to their latest version.\r\n\r\nthx!!! u'r ans helps me to solve the problem which disturbs me for  a long time", "> \r\n> \r\n> > @jacpy Can you please provide the solution you for for the problem? I am running into the same issue passing a Tensor for an Embedding layer to an LSTM. That would help me a lot in my researches. Thank you\r\n> \r\n> Solved my issue. Some updates concerning Keras and Tensorflow. The best option so far is replacing:\r\n> \r\n> `import keras as K`\r\n> \r\n> to:\r\n> \r\n> `import tensorflow.python.keras as K`\r\n> \r\n> after updating tensorflow and keras to their latest version.\r\n\r\nthanks bro!! was getting a similar error `TypeError: Expected Operation, Variable, or Tensor, got 1`", "> > > @jacpy Can you please provide the solution you for for the problem? I am running into the same issue passing a Tensor for an Embedding layer to an LSTM. That would help me a lot in my researches. Thank you\r\n> > \r\n> > \r\n> > Solved my issue. Some updates concerning Keras and Tensorflow. The best option so far is replacing:\r\n> > `import keras as K`\r\n> > to:\r\n> > `import tensorflow.python.keras as K`\r\n> > after updating tensorflow and keras to their latest version.\r\n> \r\n> thanks bro!! was getting a similar error `TypeError: Expected Operation, Variable, or Tensor, got 1`\r\n\r\n\r\n\r\n> > This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!\r\n> > Please post it in Stackoverflow with the standalone code, we will resolve it there. Thanks!\r\n> \r\n> I have solved the problem, thx.\r\n\r\nI tried replacing:\r\nimport keras as K\r\nto:\r\nimport tensorflow.python.keras as K\r\nBut still its is not working, My Both Keras and Tensorflow are also updated. Any idea what other issue could be there?", "> > > @jacpy Can you please provide the solution you for for the problem? I am running into the same issue passing a Tensor for an Embedding layer to an LSTM. That would help me a lot in my researches. Thank you\r\n> > \r\n> > \r\n> > Solved my issue. Some updates concerning Keras and Tensorflow. The best option so far is replacing:\r\n> > `import keras as K`\r\n> > to:\r\n> > `import tensorflow.python.keras as K`\r\n> > after updating tensorflow and keras to their latest version.\r\n> \r\n> thanks bro!! was getting a similar error `TypeError: Expected Operation, Variable, or Tensor, got 1`\r\n\r\nI tried replacing:\r\nimport keras as K\r\nto:\r\nimport tensorflow.python.keras as K\r\nBut still its is not working, My Both Keras and Tensorflow are also updated. Any idea what other issue could be there?"]}, {"number": 33543, "title": "[tf2.0.0] Fails to build on Python 3.8 - suggested fix (change nullptr to 0 in source)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0.0 with cuda and TensorRT\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: CUDA 10.0, CUDNN 7.6.4\r\n- GPU model and memory: NVidia RTX 2080 Ti\r\n\r\n**Describe the problem**\r\n\r\nWhile building from source:\r\nERROR: /home/daniel/tensorflow/tensorflow/python/BUILD:341:1: C++ compilation of rule '//tensorflow/python:ndarray_tensor_bridge' failed (Exit 1) tensorflow/python/lib/core/ndarray_tensor_bridge.cc:108:1: error: cannot convert \u2018std::nullptr_t\u2019 to \u2018Py_ssize_t {aka long int}\u2019 in initialization\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\ngit checkout r2.0\r\n\r\nbazel build --config=opt --config=cuda --config=v2 --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package\r\n\r\nFix:\r\n\r\nIn Python 3.8, the reserved \"tp_print\" slot was changed from a function pointer to a number, `Py_ssize_t tp_vectorcall_offset`.\r\nIn C, there is no \"nullptr\"; either a 0 or NULL casts automatically to both pointers and numbers.\r\nUse 0 instead of \"nullptr\" in the slot to be source-compatible both with Python 3.8 and previous versions.\r\n\r\nSearch for \"tp_print\" in the files listed below.  Change nullptr to 0 where tp_print is in the comment:\r\ni.e. \r\nchange:\r\n`    nullptr,                                      /* tp_print */     `\r\nto:\r\n`    0,                                      /* tp_print */     `\r\nList of files that need this change:\r\n/tensorflow/tensorflow/python/lib/core/ndarray_tensor_bridge.cc     ----> 1 change\r\n/tensorflow/tensorflow/python/lib/core/bfloat16.cc                -----> 1 change\r\n/tensorflow/tensorflow/python/eager/pywrap_tfe_src.cc      -------> 2 changes ( /* tp_print */ occurs twice)", "comments": ["Thanks @dbonner. I tested on Ubuntu 18.04 + python 3.8 and the build completed successfully. Created a PR #33575 to carry the fix.\r\n\r\nNote tensorflow depends on h5py which does not have python 3.8 support yet, as such I think a release version of tensorflow for python 3.8 may have to wait (for h5py).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33543\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33543\">No</a>\n", "Also confirming that we managed to compile Tensorfow 2.0 from master (62a81c36ea, #33575 being merged) using Python 3.8 and CUDA 10.1 on Windows 10 without any tweaks/hacks. Couldn't test with CUDA 10.0 as it's been problematic with Win10). Build env must have numpy==1.17.3 for Python 3.8 support. \r\n\r\nXLA disabled, AFAIK on W10 it doesn't work out of the box/requires heavy tweaking.", "@yongtang re h5py support for py 3.8, where are they tracking work for that? (I don't see an issue on their repo.) Also fwiw, i can pip install h5py on 3.8 just fine. Haven't tested it out, though. ", "@grisaitis From the PyPI.org https://pypi.org/project/h5py/2.10.0/#files it looks like h5py does not have out-of-box binary release for python 3.8 on Linux, though the source is available, so install h5py with python 3.8 might work on some distributions.", "@ahtik Hi, would you please describe further in details the procedure you have taken to do so ? I have spent a whole day and a half trying to build it from source on : \r\n- Windows 10 Enterprise 1709\r\n- Latest branch of Tensorflow 2.0 master\r\n- CUDA 10.1 (cuDNN 7.6 & TensorRT 6)\r\n- Python 3.8 with (abs-py 0.8.1, astor 0.8.0, h5py 2.10.0, keras-applications 1.0.8, kera-preprocessing 1.0.9, numpy 1.17.3, pip 19.3.1, setuptools 41.2.0)\r\n- Visual Studio 2019 (tried MSCV 2019 as well as MSVC 2017)\r\n- several versions of Bazel (0.26.0, 0.22.0, 0.29.1 even 1.1.0... well the latter did not work.. as expected though)\r\n- MSYS2 20190524\r\n- disabled XLA and ROCm\r\n\r\nIt is important to me as I would like to compile it to use AVX2, SSE4.x and FMA instructions set... Every time I run : `bazel build --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package` it process, raise WARNINGS on imports in Bazel BUILD and after a certain amount of time I get:\r\n\r\n> Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n> INFO: Elapsed time: 2914.437s, Critical Path: 504.14s\r\n> INFO: 2995 processes: 2995 local.\r\n> FAILED: Build did NOT complete successfully\r\n\r\nFYI : Bazel, MSYS, CUDA are perfectly working as standalones.\r\nThank you in advance for your time :)\r\n", "@eidalex Happy to.\r\nFirst make sure to have bazel 0.26.1 and MSVC 2017 installed.\r\nThen open the cmd.exe and set the env variables:\r\n```bash\r\nset BAZEL_VC=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\r\nset BAZEL_VS=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\r\nset CUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\r\nSET CUDA_TOOLKIT_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\r\n```\r\nCheck that your current python env is activated with python 3.8.0, my `pip freeze` outputs\r\n```h5py==2.10.0\r\nKeras-Applications==1.0.6\r\nKeras-Preprocessing==1.0.5\r\nnumpy==1.17.3\r\nsix==1.12.0\r\n```\r\n\r\nMSYS2 is just the latest from everything, `binutils` etc.\r\n\r\nTo build the package, I was using:\r\n`bazel build //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nPlease let me know if this fixes it, what might have been the issue.", "@ahtik You are my savior ! Thanks a lot !\r\n\r\nActually lets sumarize what happened there. Thank to you I have made the following modifications:\r\n- Uninstall Visual Studio 2019 and install Visual Studio 2017 (the latter already contains all the right versions of the compiler MSVC)\r\n- Delete the environment variable BAZEL_VC_FULL_VERSION which I had set (apparently it was optional so I don't think it made a real change)\r\n- Adding CUDA_TOOLKIT_PATH variable \r\n- Installing the TensorFlow 2.0 with the special commit (62a81c3, #33575 being merged)\r\n\r\nIt builds successfully with AVX2 ! \r\n\r\nNote: I tried the latest version of the master and it did not compiled because of build configuration errors.(it also did not work with the r2.0 branch version)\r\n\r\n> ERROR: error loading package '' : in C:/tensorflow/tensorflow/workspace.bzl : LABEL '//tensorflow/tools/def_file_filter_configure.bzl' crosses boundary of package 'tensorflow' (perhaps you want to put the column here: '//tensorflow:tools/def_file_filter/def_file_filter_configure.bzl')\r\n\r\nWell.. I tried to modify the wrorkspace and the BUILD but it did not work at all...", "@eidalex If you want to build from the latest master (https://github.com/tensorflow/tensorflow/commit/eda53c63dab8b364872ede8e423e4fed5d1686f7), it might be needed to clean the user home bazel dir in addition to `bazel clean` command. My build started to fail after https://github.com/tensorflow/tensorflow/commit/3cdc97306c0a72a939b6444e00bc880f98a06a14 and cleanup made it green again.\r\n\r\n```\r\nERROR: missing input file '@local_config_mlir//:tools/mlir-tblgen/EnumsGen.h'\r\nERROR: C:/users/ak/_bazel_ak/xv6zejqw/external/local_config_mlir/BUILD:1843:1: @local_config_mlir//:mlir-tblgen: missing input file '@local_config_mlir//:tools/mlir-tblgen/EnumsGen.h'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: C:/users/ak/_bazel_ak/xv6zejqw/external/local_config_mlir/BUILD:1843:1 1 input file(s) do not exist\r\n```", "I was able to successfully build TF on windows with python 3.8.\r\nHowever, grpcio still did not publish their python 3.8 package.\r\nTherefore, we are blocked on their release for our python 3.8 release", "[grpcio](https://pypi.org/project/grpcio/1.26.0/#files) and [h5py](https://pypi.org/project/h5py/2.10.0/#files) now provide binaries for python 3.8.", "See the below post for the plans.\r\nhttps://github.com/tensorflow/tensorflow/issues/33374#issuecomment-571074915", "is TensorFlow available for python 3.8? \r\nI was unable to install with python 3.8, do I need to downgrade into python 3.7?"]}, {"number": 33542, "title": "cleared unwanted else clauses and minor readability improvement", "body": "", "comments": ["Please fix failures", "@mihaimaruseac all required builds are succefull , can you merge this?", "Tests are not successful\r\n\r\n```\r\n//tensorflow/python/module:module_test                                   FAILED in 3 out of 3 in 39.5s\r\n  Stats over 3 runs: max = 39.5s, min = 17.3s, avg = 27.4s, dev = 9.2s\r\n  /tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/module/module_test/test.log\r\n  /tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/module/module_test/test_attempts/attempt_1.log\r\n  /tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/module/module_test/test_attempts/attempt_2.log\r\n```\r\n\r\n```\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/module/module_test.runfiles/org_tensorflow/tensorflow/python/module/module_test.py\", line 503, in test_with_path\r\n    mod._flatten(with_path=True, predicate=module._is_variable))\r\nTypeError: cannot convert dictionary update sequence element #1 to a sequence\r\n\r\n----------------------------------------------------------------------\r\n```", "@Navan0  Could you please check reviewer comments and keep us posted. Thanks!", "@gbaned sure", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@Navan0 did you fix tests? There is also a merge conflict that needs fixing.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 33541, "title": "Training freezes when adding tf.function", "body": "\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): `v2.0.0-rc2-26-g64c3d38 2.0.0`\r\n- Python version: 3.5.2\r\n- CUDA/cuDNN version: 9.2\r\n- GPU model and memory: GTX 1080 Ti\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nWhen adding the `tf.function` decorator to the `_epoch_train` (in the code below), it freezes.\r\n\r\n**Code to reproduce the issue**\r\nCode taken from this tutorial: https://www.tensorflow.org/tutorials/images/segmentation\r\nExcept that, instead of using `model.fit()`, I used the following custom class for the training loop:\r\n\r\n```\r\nimport tensorflow as tf\r\nclass Trainer(object):\r\n    def __init__(self, model, dataloaders, epochs=None,\r\n                 steps=-1, log_interval=1):\r\n        \"\"\"\r\n        dataloaders: {'train': train_loader, 'val': val_loader}\r\n        Here we assume that the model has been compiled, i.e. it contains\r\n            an optimizer and a loss function\r\n        \"\"\"\r\n        self.model = model\r\n        self.dataloaders = dataloaders\r\n        self.epochs = epochs\r\n        self.log_interval = log_interval\r\n    \r\n    def train(self):\r\n        for epoch in range(self.epochs):\r\n            message = '{}/{}:\\t'.format(epoch, self.epochs)\r\n\r\n            # Training phase\r\n            self._epoch_train()\r\n            # Tensorboard: https://www.tensorflow.org/tensorboard/get_started\r\n            # Display metrics at the end of each epoch.\r\n            for metric in self.model.metrics:\r\n                metric_value = float(metric.result())\r\n                message += 'train_{}: {}\\t'.format(metric.name, metric_value)\r\n                metric.reset_states()\r\n\r\n            # Validation phase\r\n            self._epoch_val()\r\n            for metric in self.model.metrics:\r\n                metric_value = float(metric.result())\r\n                message += 'val_{}: {}\\t'.format(metric.name, metric_value)\r\n                metric.reset_states()\r\n\r\n            print(message)\r\n\r\n    @tf.function\r\n    def _epoch_train(self):\r\n        \"\"\"\r\n        Perform one training epoch\r\n        \"\"\"\r\n        # Iterate over the batches of the dataset.\r\n        # Note: use ds.enumerate() instead of enumerate(ds)\r\n        # https://github.com/tensorflow/tensorflow/issues/30802\r\n        for step, (x_batch, y_batch) in self.dataloaders['train'].enumerate():\r\n            # Open a GradientTape to record the operations run\r\n            # during the forward pass, which enables autodifferentiation.\r\n            with tf.GradientTape() as tape:\r\n                # Run the forward pass of the layer.\r\n                # The operations that the layer applies\r\n                # to its inputs are going to be recorded\r\n                # on the GradientTape.\r\n                logits = self.model(x_batch)  # Logits for this minibatch\r\n\r\n                # Compute the loss value for this minibatch.\r\n                loss_value = self.model.loss(y_batch, logits)\r\n\r\n            # Use the gradient tape to automatically retrieve\r\n            # the gradients of the trainable variables with respect to the loss.\r\n            grads = tape.gradient(loss_value, self.model.trainable_weights)\r\n\r\n            # Run one step of gradient descent by updating\r\n            # the value of the variables to minimize the loss.\r\n            self.model.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\r\n\r\n            # Update metrics\r\n            for metric in self.model.metrics:\r\n                metric(y_batch, logits)\r\n\r\n    def _epoch_val(self):\r\n        \"\"\"\r\n        Perform one validation epoch\r\n        \"\"\"\r\n        # Run a validation loop at the end of each epoch.\r\n        for x_batch, y_batch in self.dataloaders['val']:\r\n            logits = self.model(x_batch)\r\n            # Update metrics\r\n            for metric in self.model.metrics:\r\n                metric(y_batch, logits)\r\n```\r\n\r\n", "comments": ["I guess the issue has something to do with iterating over the dataset (maybe similar to this issue: https://github.com/tensorflow/tensorflow/issues/30802), because if I take the training steps (together with `tf.function`) out of the training epochs then it works:\r\n\r\n```\r\n@tf.function\r\ndef _train_step(self, batch):\r\n    x_batch, y_batch = batch\r\n    with tf.GradientTape() as tape:\r\n        # Run the forward pass of the layer.\r\n        # The operations that the layer applies\r\n        # to its inputs are going to be recorded\r\n        # on the GradientTape.\r\n        logits = self.model(x_batch)  # Logits for this minibatch\r\n\r\n        # Compute the loss value for this minibatch.\r\n        loss = self.model.loss(y_batch, logits)\r\n\r\n    # Use the gradient tape to automatically retrieve\r\n    # the gradients of the trainable variables with respect to the loss.\r\n    grads = tape.gradient(loss, self.model.trainable_weights)\r\n\r\n    # Run one step of gradient descent by updating\r\n    # the value of the variables to minimize the loss.\r\n    self.model.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\r\n\r\n    # Update metrics\r\n    for metric in self.model.metrics:\r\n        metric(y_batch, logits)\r\n    \r\n    return loss\r\n\r\n@tf.function\r\ndef _val_step(self, batch):\r\n    x_batch, y_batch = batch\r\n    logits = self.model(x_batch)\r\n    for metric in self.model.metrics:\r\n        metric(y_batch, logits)\r\n```", "@netw0rkf10w \r\n\r\nLooks like issue is resolved. Please, confirm.Thanks!\r\n", "@ravikyram The code in my first post is supposed to work, isn't it? Maybe this is a bug?", "@netw0rkf10w \r\n\r\nCan you please help us with colab link  to reproduce the issue in our environment.It will be easy for localizing the issue faster.Thanks!", "@netw0rkf10w \r\n\r\nAny updates please. Thanks!", "@ravikyram My apologies for the delay.\r\nI tried to quickly reproduce the issue in [this notebook](https://colab.research.google.com/drive/11kDGNoNy2GbXMqpHEOTzCwM1-ZDR_28a), but failed (everything is working in the notebook).\r\n\r\nI observed the issue when working on a segmentation task, so maybe I will take some time to see if it's reproducible on that dataset. For now let me close this issue.\r\n\r\nThanks again!"]}, {"number": 33540, "title": "[Intel MKL] Improve eager performance for small batch sizes", "body": "Improved eager performance  for small batch size by 2X (Intel MKL.)\r\n\r\nModel Tested:\r\nresnet 50 model\r\n\r\nModified File:\r\nmkl_eager_op_rewrite.cc\r\n\r\nUnit test:(PR Already Merged with Master)\r\nhttps://github.com/tensorflow/tensorflow/pull/32935", "comments": ["> Thank you for the PR!\r\n> Why don't we just make mkl_eager_ops_ an unordered map in the first place?\r\n\r\n@penpornk Yes, there is a todo(line 44) for future further refactoring for the code by the main library owner. This is just a cleanup of the current PR as improvements are good.\r\nAll Changes done!", "> > Thank you for the PR!\r\n> > Why don't we just make mkl_eager_ops_ an unordered map in the first place?\r\n> \r\n> Yes, there is a todo for future further refactoring.\r\n\r\nWhy don't we do it now? It seems much simpler than keeping using std::vector and add another std::unordered_map for quick lookups on top of it.\r\n\r\nTo change `mkl_eager_ops_` to `unordered_map`, we only need to change these\r\nhttps://github.com/tensorflow/tensorflow/blob/1b5c66bc3a2b05b47f0e4247a4e7bf872d608984/tensorflow/core/common_runtime/eager/mkl_eager_op_rewrite.cc#L79-L87\r\nand these\r\nhttps://github.com/tensorflow/tensorflow/blob/1b5c66bc3a2b05b47f0e4247a4e7bf872d608984/tensorflow/core/common_runtime/eager/mkl_eager_op_rewrite.cc#L172-L181", "> Thank you for the PR!\r\n> Why don't we just make mkl_eager_ops_ an unordered map in the first place?\r\n> \r\n> Yes, there is a todo for future further refactoring.\r\n> \r\n> Why don't we do it now? It seems much simpler than keeping using std::vector and add another std::unordered_map for quick lookups on top of it.\r\n> To change mkl_eager_ops_ to unordered_map, we only need to change these\r\n> \r\n>   \r\n>     \r\n>       tensorflow/tensorflow/core/common_runtime/eager/mkl_eager_op_rewrite.cc\r\n>     \r\n>     \r\n>         Lines 79 to 87\r\n>       in\r\n>       1b5c66b\r\n>     \r\n>     \r\n>     \r\n>     \r\n> \r\n>         \r\n>           \r\n>            mkl_eager_ops_.push_back({\"BatchMatMul\", AlwaysRewrite, CreateGenericMklOp}); \r\n>         \r\n> \r\n>         \r\n>           \r\n>            mkl_eager_ops_.push_back( \r\n>         \r\n> \r\n>         \r\n>           \r\n>                {\"BatchMatMulV2\", AlwaysRewrite, CreateGenericMklOp}); \r\n>         \r\n> \r\n>         \r\n>           \r\n>            mkl_eager_ops_.push_back({\"Conv2D\", RewriteConv2D, CreateMklConv2DOp}); \r\n>         \r\n> \r\n>         \r\n>           \r\n>            mkl_eager_ops_.push_back( \r\n>         \r\n> \r\n>         \r\n>           \r\n>                {\"Conv2DBackpropInput\", RewriteConv2D, CreateMklConv2DOp}); \r\n>         \r\n> \r\n>         \r\n>           \r\n>            mkl_eager_ops_.push_back( \r\n>         \r\n> \r\n>         \r\n>           \r\n>                {\"Conv2DBackpropFilter\", RewriteConv2D, CreateMklConv2DOp}); \r\n>         \r\n> \r\n>         \r\n>           \r\n>            mkl_eager_ops_.push_back({\"MatMul\", AlwaysRewrite, CreateGenericMklOp}); \r\n>         \r\n>     \r\n>   \r\n> \r\n> \r\n> and these\r\n> \r\n>   \r\n>     \r\n>       tensorflow/tensorflow/core/common_runtime/eager/mkl_eager_op_rewrite.cc\r\n>     \r\n>     \r\n>         Lines 172 to 181\r\n>       in\r\n>       1b5c66b\r\n>     \r\n>     \r\n>     \r\n>     \r\n> \r\n>         \r\n>           \r\n>            *op_idx = -1; \r\n>         \r\n> \r\n>         \r\n>           \r\n>            // Find and call the op's rewrite rule that determines whether we need to \r\n>         \r\n> \r\n>         \r\n>           \r\n>            // rewrite this op or not. \r\n>         \r\n> \r\n>         \r\n>           \r\n>            for (auto it = mkl_eager_ops_.begin(); it != mkl_eager_ops_.end(); ++it) { \r\n>         \r\n> \r\n>         \r\n>           \r\n>              if (it->op_name.compare(op->Name()) == 0 && it->RewriteRule(op)) { \r\n>         \r\n> \r\n>         \r\n>           \r\n>                *op_idx = it - mkl_eager_ops_.begin(); \r\n>         \r\n> \r\n>         \r\n>           \r\n>                return true; \r\n>         \r\n> \r\n>         \r\n>           \r\n>              } \r\n>         \r\n> \r\n>         \r\n>           \r\n>            } \r\n>         \r\n> \r\n>         \r\n>           \r\n>            return false;\r\n\r\nThank you for suggesting where exactly code needs to change.\r\nThe only thing is even if this code is going to change it is unrelated to this PR and has to be submitted as a new PR as the reason for both the hash maps are different and both will exist. I have created an internal ticket to track this. ", "@penpornk you got it andThank you for the review. \r\nAlso, talking internally , looks like the ask for refactoring has been requested twice. I am planning to refactor the vector to hashmap as a new PR soon and will keep you posted.", "@nammbash Thank you very much! :)"]}, {"number": 33539, "title": "Not build 1.15 windows", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): 0.26.1\r\n- CUDA/cuDNN version:10.1.243_426.00\r\n- GPU model and memory:7.6.4.38\r\n\r\n> bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n\r\n```\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (392 packages loaded, 21941 targets configured).\r\nINFO: Found 1 target...\r\nINFO: Deleting stale sandbox base C:/users/rty/_bazel_rty/26orbg4z/sandbox\r\nINFO: From Compiling tensorflow/lite/kernels/internal/tensor_utils.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-O3'\r\nINFO: From Executing genrule //tensorflow/core:version_info_gen:\r\nfatal: Invalid path '/c/users/rty/_bazel_rty/26orbg4z/execroot/org_tensorflow/C:': No such file or directory\r\nERROR: missing input file '@local_config_mlir//:include/mlir/TableGen/OpInterfaces.h'\r\nERROR: C:/users/rty/_bazel_rty/26orbg4z/external/local_config_mlir/BUILD:1714:1: @local_config_mlir//:TableGen: missing input file '@local_config_mlir//:include/mlir/TableGen/OpInterfaces.h'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: C:/users/rty/_bazel_rty/26orbg4z/external/local_config_mlir/BUILD:1714:1 1 input file(s) do not exist\r\nINFO: Elapsed time: 219.352s, Critical Path: 26.02s\r\nINFO: 300 processes: 300 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["1.14 Build", "```\r\nfatal: Invalid path '/c/users/rty/_bazel_rty/26orbg4z/execroot/org_tensorflow/C:': No such file or directory\r\n```\r\n\r\nDoesn't seem to be a TF issue, but something related to your environment", "But version 1.41.1 is going fine", "@sonfiree, Please follow the instructions mentioned in [this link](https://www.tensorflow.org/install/source_windows). And also check the tested build configuration. Please let us know how it progresses. Thanks! ", "@sonfiree, Is this still issue!", "> @sonfiree, Is this still issue!\r\n\r\nYes", "Can you try building from master? In a separate clone of the repository? Please attach the full compilation log.", "Nothing has changed. Everything is like in the first post", "I wanted to see full logs of compilation process, including output of running configure.py to identify if error stems from another source.", "```\r\nD:\\tensorflow>python ./configure.py\r\nExtracting Bazel installation...\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.26.1 installed.\r\nPlease specify the location of python. [Default is C:\\anaconda3\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  C:\\anaconda3\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\anaconda3\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]:\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: Y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.1 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include\r\nFound cuDNN 7 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 6.1\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\r\nEigen strong inline overridden.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=gdr            # Build with GDR support.\r\n        --config=verbs          # Build with libverbs support.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=noignite       # Disable Apache Ignite support.\r\n        --config=nokafka        # Disable Apache Kafka support.\r\n```", "```\r\nINFO: From Compiling external/com_google_absl/absl/base/dynamic_annotations.cc:\r\ncl : Command line warning D9025 : overriding '/w' with '/W3'\r\nINFO: From Compiling tensorflow/python/util/tf_stack.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-fexceptions'\r\nINFO: From Linking tensorflow/python/_tf_stack.so:\r\n   Creating library bazel-out/x64_windows-opt/bin/tensorflow/python/lib_tf_stack.so.ifso and object bazel-out/x64_windows-opt/bin/tensorflow/python/lib_tf_stack.so.exp\r\nERROR: C:/users/user/_bazel_user/26orbg4z/external/local_config_mlir/BUILD:418:1: C++ compilation of rule '@local_config_mlir//:Support' failed (Exit 2): python.exe failed: error executing command\r\n  cd C:/users/user/_bazel_user/26orbg4z/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\WINDOWS\\Microsoft.NET\\Framework64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;;C:\\WINDOWS\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/anaconda3/python.exe\r\n    SET PYTHON_LIB_PATH=C:/anaconda3/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\user\\AppData\\Local\\Temp\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_NEED_CUDA=1\r\n    SET TMP=C:\\Users\\user\\AppData\\Local\\Temp\r\n  C:/anaconda3/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /Iexternal/local_config_mlir /Ibazel-out/x64_windows-opt/bin/external/local_config_mlir /Iexternal/llvm /Ibazel-out/x64_windows-opt/bin/external/llvm /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/local_config_mlir/include /Ibazel-out/x64_windows-opt/bin/external/local_config_mlir/include /Iexternal/llvm/include /Ibazel-out/x64_windows-opt/bin/external/llvm/include /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_CRT_NONSTDC_NO_DEPRECATE /D_CRT_NONSTDC_NO_WARNINGS /D_SCL_SECURE_NO_DEPRECATE /D_SCL_SECURE_NO_WARNINGS /DUNICODE /D_UNICODE /DLLVM_ENABLE_STATS /D__STDC_LIMIT_MACROS /D__STDC_CONSTANT_MACROS /D__STDC_FORMAT_MACROS /DLLVM_BUILD_GLOBAL_ISEL /showIncludes /MD /O2 /DNDEBUG -w -DWIN32_LEAN_AND_MEAN -DNOGDI /arch:AVX -std=c++14 /Fobazel-out/x64_windows-opt/bin/external/local_config_mlir/_objs/Support/StorageUniquer.o /c external/local_config_mlir/lib/Support/StorageUniquer.cpp\r\nExecution platform: @bazel_tools//platforms:host_platform\r\ncl : Command line warning D9002 : ignoring unknown option '-std=c++14'\r\nexternal/llvm/include\\llvm/Support/Compiler.h(79): fatal error C1189: #error:  LLVM requires at least MSVC 2017.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 568.313s, Critical Path: 55.14s\r\nINFO: 1186 processes: 1186 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`", "`fatal error C1189: #error:  LLVM requires at least MSVC 2017.` is a totally different error than the one posted at the beginning. And this one is actionable.", "\r\nupdated the branch from the master and this happened.", "Both of the errors posted so far are not TF errors but errors on your environment.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33539\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33539\">No</a>\n", "But TF 1.14 doesn't have this problem...\r\n@sonfiree Did you find the workaround for this issue?", "@AndreyPlotkinOr I`m used TF 1.14", "According to llvm\\Support\\Compiler.h there are several suitable VS:\r\n\r\n```\r\n/// \\macro LLVM_MSC_PREREQ\r\n/// Is the compiler MSVC of at least the specified version?\r\n/// The common \\param version values to check for are:\r\n/// * 1910: VS2017, version 15.1 & 15.2\r\n/// * 1911: VS2017, version 15.3 & 15.4\r\n/// * 1912: VS2017, version 15.5\r\n/// * 1913: VS2017, version 15.6\r\n/// * 1914: VS2017, version 15.7\r\n/// * 1915: VS2017, version 15.8\r\n/// * 1916: VS2017, version 15.9\r\n/// * 1920: VS2019, version 16.0\r\n/// * 1921: VS2019, version 16.1\r\n#ifdef _MSC_VER\r\n#define LLVM_MSC_PREREQ(version) (_MSC_VER >= (version))\r\n\r\n// We require at least MSVC 2017.\r\n#if !LLVM_MSC_PREREQ(1910)\r\n#error LLVM requires at least MSVC 2017.\r\n#endif\r\n```\r\n\r\nIt is not enough to have VS 2017, because minor version may not suit.\r\nWhen you open Command Prompt for VS, you will see something like this:\r\n\r\n```\r\n**********************************************************************\r\n** Visual Studio 2017 Developer Command Prompt v15.0\r\n** Copyright (c) 2017 Microsoft Corporation\r\n**********************************************************************\r\n```\r\n\r\nAs you may see, my version is 15.0, which is unsuitable.\r\n", "After updating VS 2017 to latest version, I got this message:\r\n```\r\n**********************************************************************\r\n** Visual Studio 2017 Developer Command Prompt v15.9.17\r\n** Copyright (c) 2017 Microsoft Corporation\r\n**********************************************************************\r\n```\r\n\r\nBut the problem didn't disappear.\r\n\r\nI referred this [link](https://stackoverflow.com/a/52089246/1100913) to resolve _MSC_VER and got 1916, which is suitable.\r\n\r\nThe problem is that bazel doesn't take into account environment from VS Command Prompt, and sets environment variables by himself.\r\n\r\n@sonfiree In your case it will be SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\WINDOWS\\Microsoft.NET\\Framework64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;;C:\\WINDOWS\\system32\r\n\r\nIn my environment there are 4 different visual studios.\r\nBazel chooses the wrong one.\r\n\r\nI found that VS may be specified manually by setting [BAZEL_VS and BAZEL_VC](https://docs.bazel.build/versions/master/windows.html#build-c-with-msvc)\r\n", "You may find list of all _MSC_VER sions [here](https://docs.microsoft.com/en-us/cpp/preprocessor/predefined-macros?view=vs-2019)", "I followed [this ](https://github.com/bazelbuild/bazel/issues/5148#issuecomment-386272023)proposal and it worked for me.\r\n\r\nJust a [ref ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/def_file_filter/def_file_filter_configure.bzl)of how BAZEL_VC internals work."]}, {"number": 33538, "title": "[Intel MKL] Improve Small batch size and eager performance", "body": "Improved Batch size and eager performance by 2X (Intel MKL.)\r\n\r\nModel Tested:\r\nresnet 50 model\r\n\r\nModified File:\r\nmkl_eager_op_rewrite.cc\r\n\r\nUnit test:(PR Already Merged with Master)\r\nhttps://github.com/tensorflow/tensorflow/pull/32935", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33538) for more info**.\n\n<!-- need_sender_cla -->", "closing due to cla error"]}, {"number": 33537, "title": "[Intel MKL] Improve Small batch size and eager performance", "body": "Improved Batch size and eager performance by 2X (Intel MKL.)\r\n\r\nModel Tested:\r\nresnet 50 model\r\n\r\nModified File: \r\nmkl_eager_op_rewrite.cc\r\n\r\nUnit test:(PR Already Merged with Master)\r\nhttps://github.com/tensorflow/tensorflow/pull/32935", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33537) for more info**.\n\n<!-- need_sender_cla -->", "Submitted from wrong account. Closing and resubmitting this with right cla account."]}, {"number": 33536, "title": "failed to query event: CUDA_ERROR_LAUNCH_FAILED", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10 Enterprise, 64bit (1903)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNA\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\ntensorflow-gpu-2.0\r\n- Python version:\r\n3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n10 / 7.6.0 (also have tried 7.6.1, 7.6.2, 7.6.3, 7.6.4)\r\n- GPU model and memory:\r\n2 * RTX 2080 8G\r\n\r\n**Describe the current behavior**\r\n```\r\n2019-10-20 01:32:26.104969: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2019-10-20 01:32:26.110674: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1\r\n```\r\n\r\nThis error randomly occurs in training. \r\nSome times occurs in the first epoch, some times after a few epochs.\r\n\r\nWhen showing this error, also pop-up \"python has stopped working\" message.\r\n\r\nI ran the same code on google cloab, it seems alright.\r\nI also re-install python, tensorflow, cuda, cudnn, and GPU driver, nothing help\r\n\r\n**Code to reproduce the issue**\r\nThere are 353 samples in my dataset, all samples are padded to the same length (about 100000).\r\nAnd it just a simple LSTM model\r\n\r\n```\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Activation, Masking, TimeDistributed, LSTM, Bidirectional\r\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\r\nfrom tensorflow.keras import backend as K\r\n\r\nDUMMY_VALUE = -1.0\r\n\r\nmodel = Sequential()\r\nmodel.add(Masking(mask_value=DUMMY_VALUE, input_shape=(None, 100)))\r\nmodel.add(Bidirectional(LSTM(100, return_sequences=True, implementation=1)))\r\nmodel.add(TimeDistributed(Dense(1, activation='sigmoid')))\r\nmodel.compile(optimizer='adam',\r\n              loss='binary_crossentropy',\r\n              metrics=['accuracy', K_precision, K_recall],\r\n              sample_weight_mode='temporal')\r\nmodel.summary()\r\n\r\nmodelName = 'test'\r\ncheckpoint = ModelCheckpoint(filepath='./model_checkpoints/{epoch:02d}-{val_loss:.4f}_' + modelName + '.h5', verbose=1, save_best_only=True, mode='min')\r\nes = EarlyStopping(monitor='val_loss', mode='min', patience=10, verbose=1)\r\n\r\nhistories = []\r\nhistories.append( model.fit(padding_x, padding_y, epochs=30, batch_size=2, validation_split=0.1, callbacks=[es, checkpoint], sample_weight=w) )\r\nmodel.save(modelName + '.h5')\r\n```\r\n", "comments": ["This issue can be related to drivers. can you try to reinstall the cuda and check?", "@CryMasK, Downgrade the CuDNN version to 7.4 and try again. Let us know if still issue persists. Thanks!", "> \r\n> \r\n> @CryMasK, Downgrade the CuDNN version to 7.4 and try again. Let us know if still issue persists. Thanks!\r\n\r\nTensorflow-gpu 2.0 can not build on CuDNN 7.4\r\n`2019-10-21 21:11:07.554798: E tensorflow/stream_executor/cuda/cuda_dnn.cc:319] Loaded runtime CuDNN library: 7.4.1 but source was compiled with: 7.6.0.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.`\r\n\r\n\r\nAfter a week of testing, I found the problem.\r\nI think there are some compatibility issues with Nvidia 436.** driver.\r\nI downgraded driver version to 431.60, it works well now.\r\n\r\nJust in case, there are other problems with newer driver:\r\n1. When I want to train a LSTM model with a little bigger input dimensions, it will throw error after specific number of epochs, even the memory of GPU is enough.\r\n\r\nlike this:\r\n`internalerror: [_derived_] failed to call thenrnnbackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 300, 300, 1, 2000, 2, 300 ...`\r\n\r\n2. If I load a trained LSTM model from .h5 file, and I want to use .predict() or .fit() function, it will cause OOM error.", "I also encountered this problem, this is caused by a mismatch between the graphics card driver version and the cuda version in my computer. Reinstall the cuda installation package and use the built-in driver version in the package, and the problem is gone.", "@CryMasK   Do you have more thoughts on this problem now? Can you answer my question at: https://stackoverflow.com/questions/65067397/error-polling-for-event-status-failed-to-query-event-cuda-error-launch-failed"]}, {"number": 33535, "title": "Using tf.data.Dataset.from_generator with tf.keras", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nI am referring to [this `tf.data` example](https://www.tensorflow.org/guide/data) where it has been shown how to wrap a `tf.keras.preprocessing.image.ImageDataGenerator` object with `tf.data.Dataset`. I am trying to supply this object to a Keras model and I am getting the following issue:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-12-f2c206e4a521> in <module>()\r\n      1 model.fit(ds, \r\n----> 2          steps_per_epoch=total_data//32)\r\n\r\n11 frames\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument:  TypeError: endswith first arg must be bytes or a tuple of bytes, not str\r\nTraceback (most recent call last):\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 464, in get_iterator\r\n    return self._iterators[iterator_id]\r\n\r\nKeyError: 2\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/script_ops.py\", line 221, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 585, in generator_py_func\r\n    values = next(generator_state.get_iterator(iterator_id))\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 466, in get_iterator\r\n    iterator = iter(self._generator(*self._args.pop(iterator_id)))\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\", line 540, in flow_from_directory\r\n    interpolation=interpolation\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/directory_iterator.py\", line 126, in __init__\r\n    classes, filenames = res.get()\r\n\r\n  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 644, in get\r\n    raise self._value\r\n\r\n  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\r\n    result = (True, func(*args, **kwds))\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\", line 216, in _list_valid_filenames_in_directory\r\n    for root, fname in valid_files:\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\", line 172, in _iter_valid_files\r\n    if fname.lower().endswith('.tiff'):\r\n\r\nTypeError: endswith first arg must be bytes or a tuple of bytes, not str\r\n\r\n\r\n\t [[{{node PyFunc}}]]\r\n\t [[IteratorGetNext]]\r\n\t [[IteratorGetNext/_4]]\r\n  (1) Invalid argument:  TypeError: endswith first arg must be bytes or a tuple of bytes, not str\r\nTraceback (most recent call last):\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 464, in get_iterator\r\n    return self._iterators[iterator_id]\r\n\r\nKeyError: 2\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/script_ops.py\", line 221, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 585, in generator_py_func\r\n    values = next(generator_state.get_iterator(iterator_id))\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 466, in get_iterator\r\n    iterator = iter(self._generator(*self._args.pop(iterator_id)))\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\", line 540, in flow_from_directory\r\n    interpolation=interpolation\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/directory_iterator.py\", line 126, in __init__\r\n    classes, filenames = res.get()\r\n\r\n  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 644, in get\r\n    raise self._value\r\n\r\n  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\r\n    result = (True, func(*args, **kwds))\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\", line 216, in _list_valid_filenames_in_directory\r\n    for root, fname in valid_files:\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\", line 172, in _iter_valid_files\r\n    if fname.lower().endswith('.tiff'):\r\n\r\nTypeError: endswith first arg must be bytes or a tuple of bytes, not str\r\n\r\n\r\n\t [[{{node PyFunc}}]]\r\n\t [[IteratorGetNext]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_distributed_function_1435]\r\n\r\nFunction call stack:\r\ndistributed_function -> distributed_function\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nHow to use this type of `tf.data.Dataset` objects along with Keras?\r\n\r\n**Code to reproduce the issue**\r\n[Link](https://colab.research.google.com/drive/1evx-qKG1vPSUE5tfsFLgG8ukAEczJSkN) to the Colab notebook.\r\n", "comments": ["Hi. The notebook is now updated with a worked example that:\r\n- Shows the usage of tf.data.Dataset.from_generator along with Keras ImageDataGenerator for Keras models.\r\n- Compares the performance between tf.data.Dataset.from_generator and ImageDtaGenerator.", "Hi,\r\n\r\nJust wanted to put this here that I had to use the lambda-trick referenced in the notebook with tf 2.1.0 to get the example at https://www.tensorflow.org/guide/data to work:\r\n\r\n```\r\n# Wrap the generator with tf.data\r\nds = tf.data.Dataset.from_generator(\r\n    lambda: img_gen.flow_from_directory(flowers,\r\n            target_size=(224, 224),\r\n            shuffle=True),\r\n    output_types=(tf.float32, tf.float32),\r\n    output_shapes = ([None,224,224,3],[None,5])\r\n)\r\n```", "Yes, of course. I made a presentation to put together some common utility things: http://bit.ly/MLWeekend19. ", "> Hi. The notebook is now updated with a worked example that:\r\n> \r\n>     * Shows the usage of tf.data.Dataset.from_generator along with Keras ImageDataGenerator for Keras models.\r\n> \r\n>     * Compares the performance between tf.data.Dataset.from_generator and ImageDtaGenerator.\r\n\r\nHey Thanks a lot. Your notebook did a great help to me!!", "Glad you liked it. Here's a presentation that accompanies that code: http://bit.ly/MLWeekend19. ", "Thanks Sayak for putting deck and code notebook here !!"]}, {"number": 33534, "title": "Finish making strided_slice equivalent to numpy's strided_slice", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.14\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently, Tensorflow uses a slicing syntax very similar to Numpy's: for example, x[...,:,0:-4] would work equivalently in both TF and Numpy. However, more advanced indexing does not work; for example, trying to use [this approach](https://matthewmcgonagle.github.io/blog/2017/10/13/SelectingSubsquaresWithNumpyIndexing) to get sub-squares from a Tensorflow tensor does not work, and in fact as far as I can tell, clever advanced indexing like this is currently impossible. As a researcher, I'm often trying to do somewhat non-standard things, and I've encountered these indexing issues several times. With TF reaching maturity, it would be good to have these advanced indexing features implemented so that slicing is fully on-par with Numpy.\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo, it will integrate seamlessly, simply providing more functionality.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nMost advanced users of Tensorflow (especially researchers) will get a lot of benefit from this.\r\n\r\n**Any Other info.**\r\n\r\nNumpy has already implemented this, so it shouldn't be too difficult to port that code to TF.", "comments": ["@jbuckman,\r\nCan you please let us know if [tf.slice](https://www.tensorflow.org/api_docs/python/tf/slice), [tf.gather](https://www.tensorflow.org/api_docs/python/tf/gather) and tf.[gather_nd](https://www.tensorflow.org/api_docs/python/tf/gather_nd) are the APIs that you are looking for? Thanks!"]}, {"number": 33533, "title": "Bcapak", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 33532, "title": "Operators not supported by Lite routine", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n# Copy and paste here\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Please fill in template, add the description of the issue in the message and use a much shorter title", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing as there was no further activity, template was not filled."]}, {"number": 33531, "title": "MultiWorkerMirroredStrategy does not work with Keras + accuracy metric", "body": "**System information**\r\nThe same environment as in https://github.com/tensorflow/tensorflow/issues/32654\r\nBut with 2 machines instead of 1 **and** Tensorflow 2.0 release from PyPi.\r\n\r\n**Describe the current behavior**\r\n\r\nI am training DenseNet121 on Imagenet with standard Keras code and custom dataset pipeline. `model.compile` is called with the only \"accuracy\" metric. I am using `MultiWorkerMirroredStrategy` as described in the tutorial. Here is the log. I had to erase ~7,000 warnings which are all the same: `2019-10-19 12:23:10.615259: W tensorflow/core/framework/op_kernel.cc:309] OpKernelContext is tracking allocations but they are not being consumed by the StepStatsCollector.`\r\n\r\n```\r\nCompiling with RMSprop\r\nFitting...\r\nWARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\n`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\n`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\n2019-10-19 03:57:05.813768: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:400] Cannot find shardable dataset, adding a shard node at the end of the dataset instead. This may have performance implications.\r\n2019-10-19 03:57:19.342401: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:400] Cannot find shardable dataset, adding a shard node at the end of the dataset instead. This may have performance implications.\r\n2019-10-19 03:59:48.236258: I tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:316] Abandoning ScopedAllocatorOptimizer because input FusedBatchNormGradV3_99 output 1 is already assigned to scope_id 132\r\n2019-10-19 03:59:48.236611: W tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:381] error: Internal: Abandoning ScopedAllocatorOptimizer because input FusedBatchNormGradV3_99 output 1 is already assigned to scope_id 132\r\n2019-10-19 03:59:48.236834: W tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:990] error: Internal: Abandoning ScopedAllocatorOptimizer because input FusedBatchNormGradV3_99 output 1 is already assigned to scope_id 132\r\n2019-10-19 03:59:48.237468: E tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:1007] ScopedAllocatorOptimizer: Internal: Abandoning ScopedAllocatorOptimizer because input FusedBatchNormGradV3_99 output 1 is already assigned to scope_id 132\r\n2019-10-19 03:59:48.237593: W tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:782] error: Internal: Abandoning ScopedAllocatorOptimizer because input FusedBatchNormGradV3_99 output 1 is already assigned to scope_id 132\r\n2019-10-19 03:59:48.299255: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] scoped_allocator_optimizer failed: Internal: Abandoning ScopedAllocatorOptimizer because input FusedBatchNormGradV3_99 output 1 is already assigned to scope_id 132\r\n2019-10-19 04:00:01.523007: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-10-19 04:00:11.506609: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-10-19 04:00:19.689077: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found\r\nRelying on driver to perform ptx compilation. This message will be only logged once.\r\n2019-10-19 04:00:29.848332: I tensorflow/core/profiler/lib/profiler_session.cc:184] Profiler session started.\r\n2019-10-19 04:00:29.848931: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcupti.so.10.0'; dlerror: libcupti.so.10.0: cannot open shared object file: No such file or directory\r\n2019-10-19 04:00:29.849025: W tensorflow/core/profiler/lib/profiler_session.cc:192] Encountered error while starting profiler: Unavailable: CUPTI error: CUPTI could not be loaded or symbol could not be found.\r\nTrain for 15974.0 steps\r\n\r\nEpoch 00001: LearningRateScheduler reducing learning rate to 0.0009375.\r\nEpoch 1/400\r\n2019-10-19 04:00:34.268294: I tensorflow/core/platform/default/device_tracer.cc:588] Collecting 0 kernel records, 0 memcpy records.\r\n2019-10-19 04:00:34.314465: E tensorflow/core/platform/default/device_tracer.cc:70] CUPTI error: CUPTI could not be loaded or symbol could not be found.\r\n15973/15974 [============================>.] - ETA: 1s - loss: 8.3656 - accuracy: 0.01342019-10-19 12:14:21.634609: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext_3}}]]\r\n\t [[GroupCrossDeviceControlEdges_1/metrics/accuracy/div_no_nan/_127]]\r\n2019-10-19 12:14:21.635077: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Out of range: [_Derived_]End of sequence\r\n\t [[{{node IteratorGetNext_3}}]]\r\n\t [[GroupCrossDeviceControlEdges_1/metrics/accuracy/div_no_nan/_127]]\r\n2019-10-19 12:14:21.635164: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Out of range: [_Derived_]End of sequence\r\n\t [[{{node IteratorGetNext_3}}]]\r\n\t [[GroupCrossDeviceControlEdges_1/metrics/accuracy/div_no_nan/_127]]\r\n2019-10-19 12:14:21.635253: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext_3}}]]\r\n\t [[replica_3/metrics/accuracy/AssignAddVariableOp_1/_55]]\r\n2019-10-19 12:14:21.635336: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Cancelled: [_Derived_]Cancelled\r\nAdditional GRPC error information:\r\n{\"created\":\"@1571487261.635191370\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Cancelled\",\"grpc_status\":1}\r\n2019-10-19 12:14:21.635412: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence\r\n\t [[{{node IteratorGetNext_3}}]]\r\n\t [[GroupCrossDeviceControlEdges_1/metrics/accuracy/div_no_nan/_127]]\r\n2019-10-19 12:14:21.635529: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence\r\n\t [[{{node IteratorGetNext_3}}]]\r\n\t [[GroupCrossDeviceControlEdges_1/metrics/accuracy/div_no_nan/_127]]\r\n2019-10-19 12:14:21.635680: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext_3}}]]\r\n\t [[replica_3/metrics/accuracy/AssignAddVariableOp_1/_43]]\r\n2019-10-19 12:14:21.635764: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Cancelled: [_Derived_]Cancelled\r\nAdditional GRPC error information:\r\n{\"created\":\"@1571487261.635191370\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Cancelled\",\"grpc_status\":1}\r\n\t [[{{node IteratorGetNext_3}}]]\r\n\t [[GroupCrossDeviceControlEdges_1/metrics/accuracy/div_no_nan/_127]]\r\n2019-10-19 12:14:21.635930: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext_3}}]]\r\n\t [[replica_1/metrics/accuracy/AssignAddVariableOp_1/_63]]\r\n\t [[{{node IteratorGetNext_3}}]]\r\n\t [[GroupCrossDeviceControlEdges_1/metrics/accuracy/div_no_nan/_127]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1571487261.635191370\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Cancelled\",\"grpc_status\":1}\r\n2019-10-19 12:14:21.636135: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext_3}}]]\r\n2019-10-19 12:14:23.196391: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Out of range: [_Derived_]End of sequence\r\n\t [[{{node IteratorGetNext_3}}]]\r\n\t [[GroupCrossDeviceControlEdges_1/metrics/accuracy/div_no_nan/_127]]\r\n2019-10-19 12:14:23.196583: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Out of range: [_Derived_]End of sequence\r\n\t [[{{node IteratorGetNext_3}}]]\r\n\t [[GroupCrossDeviceControlEdges_1/metrics/accuracy/div_no_nan/_127]]\r\n2019-10-19 12:14:23.196683: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence\r\n\t [[{{node IteratorGetNext_3}}]]\r\n\t [[GroupCrossDeviceControlEdges_1/metrics/accuracy/div_no_nan/_127]]\r\n2019-10-19 12:14:23.196964: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence\r\n\t [[{{node IteratorGetNext_3}}]]\r\n\t [[GroupCrossDeviceControlEdges_1/metrics/accuracy/div_no_nan/_127]]\r\n\t [[{{node IteratorGetNext_3}}]]\r\n\t [[GroupCrossDeviceControlEdges_1/metrics/accuracy/div_no_nan/_127]]\r\n\t [[{{node IteratorGetNext_3}}]]\r\n\t [[GroupCrossDeviceControlEdges_1/metrics/accuracy/div_no_nan/_127]]\r\n2019-10-19 12:14:23.197197: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Out of range: [_Derived_]End of sequence\r\n\t [[{{node IteratorGetNext_3}}]]\r\n\t [[GroupCrossDeviceControlEdges_1/metrics/accuracy/div_no_nan/_127]]\r\n2019-10-19 12:14:23.197232: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence\r\n\t [[{{node IteratorGetNext_3}}]]\r\n\t [[GroupCrossDeviceControlEdges_1/metrics/accuracy/div_no_nan/_127]]\r\n\t [[CollectiveReduce_3]]\r\n\t [[CollectiveReduce_1/_16]]\r\n2019-10-19 12:14:23.197283: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence\r\n\t [[{{node IteratorGetNext_3}}]]\r\n\t [[GroupCrossDeviceControlEdges_1/metrics/accuracy/div_no_nan/_127]]\r\n\t [[CollectiveReduce_2]]\r\n2019-10-19 12:14:23.197353: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence\r\n\t [[{{node IteratorGetNext_3}}]]\r\n\t [[GroupCrossDeviceControlEdges_1/metrics/accuracy/div_no_nan/_127]]\r\n\t [[CollectiveReduce_3]]\r\n\t [[CollectiveReduce/ReadVariableOp/_18]]\r\n2019-10-19 12:14:23.197395: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence\r\n\t [[{{node IteratorGetNext_3}}]]\r\n\t [[GroupCrossDeviceControlEdges_1/metrics/accuracy/div_no_nan/_127]]\r\n2019-10-19 12:14:23.197460: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Out of range: [_Derived_]End of sequence\r\n\t [[{{node IteratorGetNext_3}}]]\r\n\t [[GroupCrossDeviceControlEdges_1/metrics/accuracy/div_no_nan/_127]]\r\n2019-10-19 12:14:23.197507: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence\r\n\t [[{{node IteratorGetNext_3}}]]\r\n\t [[GroupCrossDeviceControlEdges_1/metrics/accuracy/div_no_nan/_127]]\r\n\t [[CollectiveReduce_3]]\r\n\t [[{{node IteratorGetNext_3}}]]\r\n\t [[GroupCrossDeviceControlEdges_1/metrics/accuracy/div_no_nan/_127]]\r\n2019-10-19 12:14:23.197742: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence\r\n\t [[{{node IteratorGetNext_3}}]]\r\n\t [[GroupCrossDeviceControlEdges_1/metrics/accuracy/div_no_nan/_127]]\r\n2019-10-19 12:14:23.197870: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence\r\n\t [[{{node IteratorGetNext_3}}]]\r\n\t [[GroupCrossDeviceControlEdges_1/metrics/accuracy/div_no_nan/_127]]\r\n\t [[CollectiveReduce_1]]\r\n\t [[{{node IteratorGetNext_3}}]]\r\n\t [[GroupCrossDeviceControlEdges_1/metrics/accuracy/div_no_nan/_127]]\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 668, in on_start\r\n    yield\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 372, in fit\r\n    prefix='val_')\r\n  File \"/usr/lib/python3.6/contextlib.py\", line 88, in __exit__\r\n    next(self.gen)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 685, in on_epoch\r\n    self.callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\", line 298, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\", line 963, in on_epoch_end\r\n    self._save_model(epoch=epoch, logs=logs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\", line 1001, in _save_model\r\n    self.model.save(filepath, overwrite=True)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py\", line 975, in save\r\n    signatures, options)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/save.py\", line 112, in save_model\r\n    model, filepath, overwrite, include_optimizer)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/hdf5_format.py\", line 109, in save_model_to_hdf5\r\n    save_weights_to_hdf5_group(model_weights_group, model_layers)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/hdf5_format.py\", line 627, in save_weights_to_hdf5_group\r\n    weight_values = K.batch_get_value(weights)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\", line 3296, in batch_get_value\r\n    return [x.numpy() for x in tensors]\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\", line 3296, in <listcomp>\r\n    return [x.numpy() for x in tensors]\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/values.py\", line 389, in __getattr__\r\n    return getattr(self.get(), name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/values.py\", line 322, in get\r\n    return self._get_cross_replica()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/values.py\", line 1237, in _get_cross_replica\r\n    self, axis=None)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 805, in reduce\r\n    return self._extended._reduce(reduce_op, value)  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 1436, in _reduce\r\n    device_util.current() or \"/device:CPU:0\"))[0]\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/collective_all_reduce_strategy.py\", line 490, in _reduce_to\r\n    reduce_op, value, destinations=destinations)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/cross_device_ops.py\", line 282, in reduce\r\n    destinations)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/cross_device_ops.py\", line 1025, in reduce_implementation\r\n    all_reduced = self._batch_all_reduce(reduce_op, [per_replica_value])[0]\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/cross_device_ops.py\", line 1091, in _batch_all_reduce\r\n    dense_results = self._do_batch_all_reduce_dense(reduce_op, dense_values)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/cross_device_ops.py\", line 1120, in _do_batch_all_reduce_dense\r\n    \"Id\")\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/cross_device_utils.py\", line 365, in build_collective_reduce\r\n    return collective_all_reduce()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\", line 526, in _call\r\n    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 1141, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 1224, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 511, in call\r\n    ctx=ctx)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.OutOfRangeError:  [_Derived_]End of sequence\r\n\t [[{{node IteratorGetNext_3}}]]\r\n\t [[GroupCrossDeviceControlEdges_1/metrics/accuracy/div_no_nan/_127]]\r\n\t [[CollectiveReduce_2]] [Op:__inference_collective_all_reduce_2894985]\r\n\r\nFunction call stack:\r\ncollective_all_reduce\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/user/vmarkovtsev/images/efficientoffice/efficientoffice/__main__.py\", line 5, in <module>\r\n    sys.exit(main())\r\n  File \"/user/vmarkovtsev/images/efficientoffice/efficientoffice/main.py\", line 221, in main\r\n    callbacks=[tensorboard_callback, checkpoint_callback, scheduler])\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\", line 728, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 789, in fit\r\n    *args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 776, in wrapper\r\n    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 360, in _run_single_worker\r\n    return worker_fn(strategy)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 771, in _worker_fn\r\n    return method(model, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 372, in fit\r\n    prefix='val_')\r\n  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 671, in on_start\r\n    self.callbacks._call_end_hook(mode)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\", line 258, in _call_end_hook\r\n    self.on_train_end()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\", line 375, in on_train_end\r\n    callback.on_train_end(logs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\", line 940, in on_train_end\r\n    self._training_state.delete_backup()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/distribute/multi_worker_training_state.py\", line 161, in delete_backup\r\n    tracking.AutoTrackable.__delattr__(self._model, CKPT_SAVED_EPOCH)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/tracking/tracking.py\", line 94, in __delattr__\r\n    super(AutoTrackable, self).__delattr__(name)\r\nAttributeError: _ckpt_saved_epoch\r\n\r\nEpoch 00001: loss improved from inf to 8.36576, saving model to model/DenseNet121-0001-8.366.hdf5\r\n2019-10-19 12:14:33.567096: W tensorflow/core/common_runtime/eager/context.cc:290] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe expected behavior is a successful epoch ending.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\n#!/usr/bin/env python3\r\nimport sys\r\nimport tensorflow as tf\r\n# Otherwise nothing works, and it really sucks, but is declared in the docs\r\nmulti_worker_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n\r\ndef main():\r\n    batch_size = 12\r\n    features_shape = 372, 558, 3\r\n    labels = 10\r\n    sample = tf.random.uniform(features_shape)\r\n\r\n    def with_shape(t, shape):\r\n        t = tf.squeeze(t)\r\n        t.set_shape(shape)\r\n        return t\r\n\r\n    ds_train = tf.data.Dataset.from_tensors([sample]).map(lambda s: (s, tf.ones((labels,)))) \\\r\n        .repeat().batch(batch_size).map(lambda s, l: (with_shape(s, (batch_size,) + features_shape),\r\n                                                      with_shape(l, (batch_size, labels))))\r\n    ds_val = tf.data.Dataset.from_tensors([sample]).map(lambda s: (s, tf.ones((labels,)))) \\\r\n        .repeat().batch(batch_size).take(10).map(\r\n        lambda s, l: (with_shape(s, (batch_size,) + features_shape), with_shape(l, (batch_size, labels))))\r\n    with multi_worker_strategy.scope():\r\n        model = tf.keras.applications.DenseNet121(\r\n            weights=None, input_shape=features_shape, classes=labels)\r\n        model.build((batch_size,) + features_shape)\r\n        model.summary()\r\n        optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\r\n        cross_entropy = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1)\r\n        model.compile(optimizer=optimizer, loss=cross_entropy, metrics=[\"accuracy\"])\r\n    model.fit(ds_train, validation_data=ds_val, epochs=1, steps_per_epoch=100)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    sys.exit(main())\r\n```", "comments": ["@vmarkovtsev,\r\nI tried reproducing the error with the code you provided but it resulted in no error. Here is the [Gist](https://colab.sandbox.google.com/gist/rmothukuru/0e20624aa912b7f41041c0772ad66ed0/33531.ipynb). \r\nCan you please help us in reproducing the error. Thanks!", "@rmothukuru You cannot reproduce it in Colab because it requires at least two physical nodes.", "Besides, you need to edit my snippet to proceed with the second epoch because the error happens during an epoch change.", "@vmarkovtsev, thanks for the report and apologies for the delay. I'm looking into this and will get back as soon as I find something. I was wondering how you set `TF_CONFIG`- is that before launching of this python program?", "As I looked into it I have not been able to repro using code attached (the only difference is I've set the TF_CONFIG on the two workers). That said we can add a check before deleting the attr.", "I can verify this. I independently reported https://github.com/tensorflow/tensorflow/issues/36153 which seems to be the same issue. I haven't seen an influence of the accuracy metric though and it does happen when using a single node and multiple GPUs too. It does NOT happen when using a single GPU only. It does happen when using 2 nodes with 1 GPU each.\r\n\r\nI tried the code posted here but get multiple warnings:\r\n\r\n> : W tensorflow/core/grappler/optimizers/data/auto_shard.cc:428] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Found an unshardable source dataset: name: \"TensorDataset/_1\"\r\n> Ignoring multi-device function optimization failure: Invalid argument: The graph couldn't be sorted in topological order.\r\n> 2020-01-23 17:43:32.377687: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Cancelled: [_Derived_]Cancelled\r\nAdditional GRPC error information:\r\n{\"created\":\"@1579797812.377570803\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Cancelled\",\"grpc_status\":1}\r\n2020-01-23 17:43:32.377719: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Cancelled: [_Derived_]Cancelled\r\nAdditional GRPC error information:\r\n{\"created\":\"@1579797812.377570803\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Cancelled\",\"grpc_status\":1}\r\n2020-01-23 17:43:32.377891: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at collective_ops.cc:253 : Cancelled: [_Derived_]Cancelled\r\nAdditional GRPC error information:\r\n{\"created\":\"@1579797812.377570803\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Cancelled\",\"grpc_status\":1}\r\n1\r\n\r\nAnd then a similar error as mine:\r\n\r\n```\r\nWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 200 batches). You may need to use the repeat() function when building your dataset.\r\nEpoch 2/2\r\nEpoch 2/2\r\nTraceback (most recent call last):\r\n  File \"git/tensorflow_tests/tf_issue_33531.py\", line 50, in <module>\r\n    sys.exit(main())\r\n  File \"git/tensorflow_tests/tf_issue_33531.py\", line 46, in main\r\n    model.fit(ds_train, validation_data=ds_val, epochs=2, steps_per_epoch=100)\r\n  File \"/scratch/ws/s3248973-EasyBuild/easybuild-haswell/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 819, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/scratch/ws/s3248973-EasyBuild/easybuild-haswell/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 790, in fit\r\n    *args, **kwargs)\r\n  File \"/scratch/ws/s3248973-EasyBuild/easybuild-haswell/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 777, in wrapper\r\n    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n  File \"/scratch/ws/s3248973-EasyBuild/easybuild-haswell/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \"/scratch/ws/s3248973-EasyBuild/easybuild-haswell/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 360, in _run_single_worker\r\n    return worker_fn(strategy)\r\n  File \"/scratch/ws/s3248973-EasyBuild/easybuild-haswell/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 772, in _worker_fn\r\n    return method(model, **kwargs)\r\n  File \"/scratch/ws/s3248973-EasyBuild/easybuild-haswell/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 342, in fit\r\n    total_epochs=epochs)\r\n  File \"/scratch/ws/s3248973-EasyBuild/easybuild-haswell/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 187, in run_one_epoch\r\n    aggregator.finalize()\r\n  File \"/scratch/ws/s3248973-EasyBuild/easybuild-haswell/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\", line 144, in finalize\r\n    raise ValueError('Empty training data.')\r\nValueError: Empty training data.\r\nTraceback (most recent call last):\r\n  File \"git/tensorflow_tests/tf_issue_33531.py\", line 50, in <module>\r\n    sys.exit(main())\r\n  File \"git/tensorflow_tests/tf_issue_33531.py\", line 46, in main\r\n    model.fit(ds_train, validation_data=ds_val, epochs=2, steps_per_epoch=100)\r\n  File \"/scratch/ws/s3248973-EasyBuild/easybuild-haswell/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 819, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/scratch/ws/s3248973-EasyBuild/easybuild-haswell/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 790, in fit\r\n    *args, **kwargs)\r\n  File \"/scratch/ws/s3248973-EasyBuild/easybuild-haswell/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 777, in wrapper\r\n    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n  File \"/scratch/ws/s3248973-EasyBuild/easybuild-haswell/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \"/scratch/ws/s3248973-EasyBuild/easybuild-haswell/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 360, in _run_single_worker\r\n    return worker_fn(strategy)\r\n  File \"/scratch/ws/s3248973-EasyBuild/easybuild-haswell/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 772, in _worker_fn\r\n    return method(model, **kwargs)\r\n  File \"/scratch/ws/s3248973-EasyBuild/easybuild-haswell/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 342, in fit\r\n    total_epochs=epochs)\r\n  File \"/scratch/ws/s3248973-EasyBuild/easybuild-haswell/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 128, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/scratch/ws/s3248973-EasyBuild/easybuild-haswell/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 98, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/scratch/ws/s3248973-EasyBuild/easybuild-haswell/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/scratch/ws/s3248973-EasyBuild/easybuild-haswell/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 599, in _call\r\n    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n  File \"/scratch/ws/s3248973-EasyBuild/easybuild-haswell/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2363, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/scratch/ws/s3248973-EasyBuild/easybuild-haswell/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1611, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/scratch/ws/s3248973-EasyBuild/easybuild-haswell/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1692, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/scratch/ws/s3248973-EasyBuild/easybuild-haswell/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 545, in call\r\n    ctx=ctx)\r\n  File \"/scratch/ws/s3248973-EasyBuild/easybuild-haswell/software/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.CancelledError: 2 root error(s) found.\r\n  (0) Cancelled:  RPC Request was cancelled\r\n\t [[node allreduce_1/CollectiveReduce (defined at git/tensorflow_tests/tf_issue_33531.py:46) ]]\r\n\t [[densenet121/conv3_block1_0_bn/ReadVariableOp/_835]]\r\n  (1) Cancelled:  RPC Request was cancelled\r\n\t [[node allreduce_1/CollectiveReduce (defined at git/tensorflow_tests/tf_issue_33531.py:46) ]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_distributed_function_41558]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node allreduce_1/CollectiveReduce:\r\n Cast_2 (defined at /scratch/ws/s3248973-EasyBuild/easybuild-haswell/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/threading.py:926)\r\n\r\nInput Source operations connected to node allreduce_1/CollectiveReduce:\r\n Cast_2 (defined at /scratch/ws/s3248973-EasyBuild/easybuild-haswell/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/threading.py:926)\r\n\r\nFunction call stack:\r\ndistributed_function -> distributed_function\r\n\r\n```", "So here's what I think is going on.  This is based on the same error message I saw during my runs.  I\r\n\r\nTL;DR your dataset size must be an even multiple of your \"total\" batch size.\r\n\r\n## Walking through what I saw:\r\nI'm using a dataset of size, let's say 3200.  My batch size is 128, and I'm using datasets.  I'm running _without_ strategy/dp.  It runs fine.\r\n\r\nI then switch over and start running two nodes in MWMS and same error:\r\n```text\r\ntensorflow.python.framework.errors_impl.OutOfRangeError:  [_Derived_]End of sequence\r\n\t [[{{node IteratorGetNext_3}}]]\r\n\t [[GroupCrossDeviceControlEdges_1/metrics/accuracy/div_no_nan/_127]]\r\n\t [[CollectiveReduce_2]] [Op:__inference_collective_all_reduce_2894985]\r\n```\r\nI realize that the true batch size is 128 * number of workers = 256.  Note that 3200 is evenly divisible by 128, yet not by 256.\r\n\r\nAgain, not sure if its the same problem, so buyer beware.", "The actual issue is 2 things (I might have explained that in #36153 ):\r\n\r\n- `MultiWorkerMirroredStrategy` **requires** `steps_per_epoch` being set (and the dataset to deliver that many full batches)\r\n- It does **not** work with multiple epochs unless you `.repeat()` the dataset because the iterator is not reset. I reported that in #36539\r\n\r\nUsing those 2 it works, but it's of course a pitfall with confusing error messages.", "Based on [this comment](https://github.com/tensorflow/tensorflow/issues/33531#issuecomment-620821740) multiworkermirroredstrategy can now handle partial batch size , and no error is raised with TF 2.3.0 release. \r\nI am closing this issue for now, @vmarkovtsev feel free to re-open if this is still not working for you. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33531\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33531\">No</a>\n", "Hey I have a hiccup with the multiworker srategy to include validation set during training just to have a sense of the model overfit. here is the error I am getting:\r\n\r\n 2020-09-01 13:17:58,695 WARNING (MainThread-32393) `eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\n2020-09-01 13:17:58,695 WARNING (MainThread-32393) `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\n2020-09-01 13:17:58,697 INFO (MainThread-32393) Using MirroredStrategy with devices ('/job:worker/task:71',)", "# Here is the code to reproduce this issue\r\n`def main_fun(args, ctx):\r\n    import tensorflow as tf\r\n    tf.compat.v1.enable_eager_execution()\r\n    from tensorflowonspark import compat\r\n\r\n    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n\r\n    BUFFER_SIZE = args.buffer_size\r\n    BATCH_SIZE = args.batch_size\r\n    NUM_WORKERS = args.cluster_size\r\n    total_days, n_days, n_features, n_sequence = 60, 56, 1019, 4\r\n    \r\n    def parse_tfos(example_proto):\r\n        num_features = 1019\r\n        \r\n        feature_def = {\"day_response\": tf.io.FixedLenFeature(n_sequence, tf.int64)\r\n                       ,\"days_features\": tf.io.FixedLenFeature(n_sequence*n_days*n_features, tf.int64)\r\n                      }\r\n        \r\n        features = tf.io.parse_single_example(example_proto, feature_def)\r\n        \r\n        \r\n        data= tf.cast(features['days_features'], tf.float64)\r\n\r\n        label = tf.cast(features['day_response'], tf.float64)\r\n        \r\n        #data_validation = tf.cast(features['days_features'][ (n_sequence-1) * n_days * n_features:], tf.float64)\r\n        \r\n        #label_validation = tf.cast(features['day_response'][ (n_sequence-1) * n_days * n_features:], tf.float64)\r\n\r\n\r\n        data= tf.reshape(data, (n_sequence, n_days, n_features))\r\n        label= tf.reshape(label, (n_sequence, 1))\r\n        \r\n        #data_validation = tf.reshape(data_validation, (n_sequence - (n_sequence-1),\r\n                                                       #n_days, \r\n                                                       #n_features\r\n                                                       #))\r\n        \r\n        #label_validation = tf.reshape(label_validation, (n_sequence - (n_sequence -1), \r\n                                                         #1))\r\n        \r\n        return (data, label)#, (data_validation, label_validation)\r\n    \r\n\r\n    week_pattern_train = ctx.absolute_path(args.week_week_outcome_train)\r\n    ds_train = tf.data.Dataset.list_files(week_pattern_train)\r\n    ds_train = ds_train.repeat(args.epochs).shuffle(BUFFER_SIZE)\r\n    ds_train = ds_train.interleave(tf.data.TFRecordDataset)\r\n\r\n\r\n    week_pattern_validate = ctx.absolute_path(args.week_week_outcome_validate)\r\n\r\n    ds_validate = tf.data.Dataset.list_files(week_pattern_validate)\r\n    ds_validate = ds_validate.repeat(args.epochs).shuffle(BUFFER_SIZE)\r\n    ds_validate = ds_validate.interleave(tf.data.TFRecordDataset)\r\n\r\n    \r\n    \r\n    \r\n    train_datasets_unbatched = ds_train.map(parse_tfos)\r\n    validation_datasets_unbatched = ds_validate.map(parse_tfos)\r\n    \r\n    def build_and_compile_lstm_model():\r\n        num_features = 1019\r\n        n_days = 56\r\n        model = tf.keras.Sequential([\r\n            tf.keras.layers.LSTM(num_features, input_shape=(n_days, num_features)),\r\n            tf.keras.layers.Dense(num_features, activation='relu'),\r\n            tf.keras.layers.Dropout(0.2),\r\n            tf.keras.layers.Dense(int(num_features*.5), activation='softplus'),\r\n            tf.keras.layers.Dropout(0.2),\r\n            tf.keras.layers.Dense(1),\r\n            ])\r\n        model.compile(loss='mean_squared_error', optimizer='adam')\r\n        return model\r\n\r\n    GLOBAL_BATCH_SIZE = BATCH_SIZE * NUM_WORKERS\r\n\r\n    from tensorflow.keras.callbacks import EarlyStopping\r\n    early_stop = EarlyStopping(monitor='val_loss',patience=5,restore_best_weights=True)\r\n\r\n    tf.io.gfile.makedirs(args.model_dir)\r\n    filepath = args.model_dir + \"/weights-{epoch:04d}\"\r\n    callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=filepath, verbose=1, save_weights_only=False) ,\r\n                 tf.keras.callbacks.TensorBoard(log_dir=args.model_dir)]\r\n#     steps_per_epoch = 200\r\n\r\n    with strategy.scope():\r\n        multi_worker_model = build_and_compile_lstm_model()\r\n        multi_worker_model.fit(x=train_datasets_unbatched, epochs=args.epochs, #steps_per_epoch=steps_per_epoch,\r\n                               callbacks=callbacks,\r\n                               validation_data = validation_datasets_unbatched)\r\n\r\n    from tensorflow_estimator.python.estimator.export import export_lib\r\n    export_dir = export_lib.get_timestamped_export_dir(args.export_dir)\r\n    compat.export_saved_model(multi_worker_model, export_dir, ctx.job_name == 'chief')\r\n`"]}]