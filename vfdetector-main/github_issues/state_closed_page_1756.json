[{"number": 206, "title": "Generalize slicing and slice assignment ops (including gather and scatter)", "body": "We should make our slicing and assignment ops more general to capture more of the functionality of numpy slicing, and add `__getitem__` sugar for all of it.  Specifically,\n1. We should have a 2.5 dimensional set of ops, with dimensions (1) get vs. set, (2) slice type, and for the assignment ops (3) the update op.  Currently we have `slice`, `assign_update`, `assign_add`, `assign_sub`, `gather`, `scatter_update`, `scatter_add`, `scatter_sub`.  We should also have `assign_slice_update`, `assign_slice_add`, `assign_slice_sub`.\n2. Both slicing and slice assignment should support strides, with no performance cost if strides aren't used.\n3. Ideally, the slice ops should support negative indexing a la Python.  Since the slice parameters are already CPU, this is implementable with near zero cost.   The unfortunate bit is that since we picked the wrong format for specifying ranges (start + length instead of start : end), negative indexing might be awkward.  Thus, it might be best left to a separate bug.\n4. Support numpy-like boolean indexing.\n5. Generalize `gather` and `scatter_*` to take an array of input index tensors, efficiently broadcast them, and do multidimensional indexing similar to numpy.\n6. Make `__getitem__` provide sugar for all of the above.  Ideally we'd have something idiomatically similar at least to `__setitem__`, but this is problematic since the returned assignment op is important to have, `__setitem__` does not return a value, and the nice range sugar is available only inside indexing / assignment calls.\n\n@ebrevdo: I'm assigning this to you for now since you might get to it first, but feel free to grab only the piece of it that you need for now.\n", "comments": ["Lasse requests the equivalent of numpy mixed indexing:\n\n```\nx[:, tensor]\n```\n\nwhich combines slicing with indexing-by-tensor.\n", "...where tensor can be either a scalar (which would select all the values in that column) or a vector which can select individual columns in the rows, so:\n\n```\nfoo = tf.constant([[1,2,3], [4,5,6]])\nfoo[:, 1] # [2, 5]\nindexes = tf.constant([1, 2])\nfoo[:, indexes] # [2, 6]\n```\n", "If I understand correctly the following code is exactly what is needed for cross-entropy loss function\nindexes = tf.constant([1, 2])\nfoo[:, indexes] # [2, 6]\n\nIf we will have this kind of indexing we can write:\ncost = -tf.reduce_sum(tf.log(network_output[:, targets]))\n\n, where targets is a vector of class's indexes instead of\n\ncost = -tf.reduce_sum(targets*tf.log(network_output))\n\n, where targets is a sparse matrix\n\nAm I correct?\n", "Numpy also has newaxis and \"Ellipsis\" objects. IE, to prepend an axis using numpy notation\na[np.newaxis,...]\n\nhttp://docs.scipy.org/doc/numpy-1.10.0/reference/arrays.indexing.html\n", "Yes, newaxis is essential. \n", "As part of this, we should make https://github.com/tensorflow/tensorflow/issues/418 work.\n", "It would also be helpful if gather supports an axis parameter: \n\ncurrent behavior:\ngather(v, indices)  --> output[i, ... ] = params[indices[i], ... ] \n\nwanted behavior: \ngather(v, indices, axis=1) --> output[:, i, :] = params[:, indices[i], :] \n\n(Please excuse, if this is covered by the list of requirements posted above already)\n", "@olange-google: I think we're unlikely to implement an axis parameter since it's beyond numpy features, but what you want is covered by the _combination_ of slice indexing and advanced indexing. \n", "Are you sure?  If in numpy I use:\n\narray[:, :, [1, 2, 3], :]\n\nthen that's equivalent to gather([1, 2, 3], axis=2)\n\nis it not?\n\nOn Wed, Feb 17, 2016 at 11:06 AM, Geoffrey Irving notifications@github.com\nwrote:\n\n> @olange-google https://github.com/olange-google: I think we're unlikely\n> to implement an axis parameter since it's beyond numpy features, but what\n> you want is covered by the _combination_ of slice indexing and advanced\n> indexing.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/206#issuecomment-185353303\n> .\n", "We're saying the same thing.  I'm not objecting to that functionality, just to exposing it via that sort of axis parameter rather than as a special case of the combination of slice indexing and advanced indexing.\n", "@ebrevdo What is the status on this issue? \nI am interested to implement this if no one is working on it. \n", "I plan to work on this but not until after next week. If you want to work\nin this, I suggest supporting GPU and CPU in your kernels. If you can't\nimplement that, you may want to wait for us to implement it.\nOn Feb 23, 2016 11:45 AM, \"Mohsen Hejrati\" notifications@github.com wrote:\n\n> @ebrevdo https://github.com/ebrevdo What is the status on this issue?\n> I am interested to implement this if no one is working on it.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/206#issuecomment-187869273\n> .\n", "Hi :) Could you please give an update on the status of this feature?\n\nThe `_SliceHelper` docstring says that `the \"stop\" of the slice must not be omitted`, however this case seem to be handled just fine in function implementation:\n\n```\nif s.stop is None or s.stop == sys.maxsize:\n        sizes.append(-1)\n```\n- or I'm getting something wrong?\n", "OK recently added the gather_nd op, which performs a special subset of the required functionality: given a tensor of indices, gather the requested values.\n\nAdvanced slicing is on the radar.\n", "@ebrevdo I tried using the gather_nd op to get the last relevant output from a variable length LSTM network. I'm passing sequence_length to the RNN, which means that the last few outputs of most examples are zeros, so I'm trying to read the last non-zero output. I'm getting this error, though, in the training phase:\n\n**NotImplementedError: Gradient for gather_nd is not implemented.**\n\n```\n  outputs, state = rnn.rnn(multi_rnn_cell, inputs, dtype=tf.float32, sequence_length=lengths_ph)\n\n  indicies = tf.concat(1, [\n      tf.expand_dims(lengths_ph - 1, 1),\n      tf.expand_dims(tf.range(tf.shape(vectors_ph)[0]), 1),\n      tf.expand_dims(tf.zeros_like(lengths_ph), 1),\n      ])\n  output_tensor = tf.pack(outputs)\n  relevant_output = tf.gather_nd(output_tensor, indicies)\n```\n", "Yeah - we haven't written the gradient implementation for gather_nd yet.\nIt's essentially a reshape followed by a call to sparse_to_dense; but\nsparse_to_dense doesn't have a GPU implementation (on my TODO) so I'm not\nusing it yet.\n\nOn Fri, Apr 8, 2016 at 7:24 PM, Waleed notifications@github.com wrote:\n\n> @ebrevdo https://github.com/ebrevdo I tried using the gather_nd op to\n> get the last relevant output from a variable length LSTM network. I'm\n> passing sequence_length to the RNN, which means that the last few outputs\n> of most examples are zeros, so I'm trying to read the last non-zero output.\n> I'm getting this error, though, in the training phase:\n> \n> _NotImplementedError: Gradient for gather_nd is not implemented._\n> \n>   outputs, state = rnn.rnn(multi_rnn_cell, inputs, dtype=tf.float32, sequence_length=lengths_ph)\n> \n>   indicies = tf.concat(1, [\n>       tf.expand_dims(lengths_ph - 1, 1),\n>       tf.expand_dims(tf.range(tf.shape(vectors_ph)[0]), 1),\n>       tf.expand_dims(tf.zeros_like(lengths_ph), 1),\n>       ])\n>   output_tensor = tf.pack(outputs)\n>   relevant_output = tf.gather_nd(output_tensor, indicies)\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/206#issuecomment-207684602\n", "hi @ebrevdo, do you have any timeline to implement gradient for that, as we are currently using that function. and it's quite useful function.\n", "While we wait for gather_nd for supporting gradients, this is a temporary solution:\n\n``` python\nx = tf.constant([[1, 2, 3],\n                 [4, 5, 6],\n                 [7, 8, 9]])\nidx = tf.constant([1, 0, 2])\nidx_flattened = tf.range(0, x.shape[0]) * x.shape[1] + idx\ny = tf.gather(tf.reshape(x, [-1]),  # flatten input\n              idx_flattened)  # use flattened indices\n\nwith tf.Session(''):\n  print y.eval()  # [2 4 9]\n```\n", "I will implement a gradient in the next week.  Keep in mind that it will be\nCPU-only for now.\n\nOn Wed, Apr 20, 2016 at 2:27 AM, Norman Casagrande <notifications@github.com\n\n> wrote:\n> \n> While we wait for gather_nd for supporting gradients, this is a temporary\n> solution:\n> \n> x = tf.constant([[1, 2, 3],\n>                  [4, 5, 6],\n>                  [7, 8, 9]])\n> idx = tf.constant([1, 0, 2])\n> idx_flattened = tf.range(0, x.shape[1]) \\* x.shape[0] + idx\n> y = tf.gather(tf.reshape(x, [-1]),  # flatten input\n>               idx_flattened)  # use flattened indices\n> with tf.Session(''):\n>   print y.eval()  # [2 4 9]\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/206#issuecomment-212347297\n", "@waleedka I adapted @ebrevdo's example to work with an additional dimension for the output neurons of an RNN. This should yield the last relevant output activations while preserving the shape information.\n\n``` python\ndef extract_last_relevant(outputs, length):\n    \"\"\"\n    Args:\n        outputs: [Tensor(batch_size, output_neurons)]: A list containing the output\n            activations of each in the batch for each time step as returned by\n            tensorflow.models.rnn.rnn.\n        length: Tensor(batch_size): The used sequence length of each example in the\n            batch with all later time steps being zeros. Should be of type tf.int32.\n\n    Returns:\n        Tensor(batch_size, output_neurons): The last relevant output activation for\n            each example in the batch.\n    \"\"\"\n    output = tf.transpose(tf.pack(outputs), perm=[1, 0, 2])\n    # Query shape.\n    batch_size = tf.shape(output)[0]\n    max_length = int(output.get_shape()[1])\n    num_neurons = int(output.get_shape()[2])\n    # Index into flattened array as a workaround.\n    index = tf.range(0, batch_size) * max_length + (length - 1)\n    flat = tf.reshape(output, [-1, num_neurons])\n    relevant = tf.gather(flat, index)\n    return relevant\n```\n", "@danijar this is a working solution, but when I tried it, I got the following warning from tensorflow:\n\nUserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n\nApparently, this is caused by the index slices returned by `tf.gather`. Does anyone know if there's a way to avoid this problem?\n", "@erickrf Would be glad to hear of a better solution as well. Of course you can hide the warning as any other Python warning though.\n", "> I will implement a gradient in the next week.  Keep in mind that it will be\n> CPU-only for now.\n\nIs this implemented yet by any chance @ebrevdo? \n\nFrom what I've seen, `tf.gather` is gpu gradient capable at least correct? Thanks @nova77  for the temporary solution!\n", "here is another implementation for 2d\n\n``` python\ndef gather_2d(params, indices):\n    # only for two dim now\n    shape = params.get_shape().as_list()\n    assert len(shape) == 2, 'only support 2d matrix'\n    flat = tf.reshape(params, [np.prod(shape)])\n    flat_idx = tf.slice(indices, [0,0], [shape[0],1]) * shape[1] + tf.slice(indices, [0,1], [shape[0],1])\n    flat_idx = tf.reshape(flat_idx, [flat_idx.get_shape().as_list()[0]])\n    return tf.gather(flat, flat_idx)\n```\n", "@ebrevdo I could try to help out with `__getitem__` / `__setitem__` if it would be useful. I think these ops are going to be the most intuitive way for people to interact with tensors especially since most are already comfortable with how numpy works. Would you like me to try or would it be better to just leave you to implement it?\n", "@keithshep PRs are most welcome.  I would start with adding negative indexing to the current getitem/setitem (the one that currently works on the outermost dimension).  This will require changes to both the kernel and the python wrappers, and updates to the shape inference code.\n", "Cc @aselle.\n", "@keithshep: Do you mind if we give this to @aselle as a starter project?  He just joined Brain. :)\n", "@girving Not at all. I was only going to get a chance to dig in this weekend so no problem. Oh, and congrats to @aselle \n", "For `__setitem__`: we can't use it directly because it doesn't return a value, and usually you need the return value of assignments (to make sure they execute).  However, we could make `x[i].assign(y)` work with a bit of class trickery.\n", "@girving I don't know if this is a reasonable path or not (I may be missing the point) but I wonder if borrowing ideas from SSA https://en.wikipedia.org/wiki/Static_single_assignment_form for manipulating the computation graph in cases where there is a side effect and no return value is a way to achieve this?\n", "@keithshep: Unfortunately I don't know what you mean.  We can't manipulate a computation graph until we have one to manipulate, and `__setitem__` discards the output of the assign.  Normally that return value needs to be used later on, typically as part of a training op which is then run.\n", "@girving my thought was to add some tensor version metadata similar to variable versioning used in SSA. This would mean that some internal tensor version bookkeeping would have to be performed whenever `__setitem__` is invoked, and that a tensor version would have to be recorded whenever a tensor is passed to an op since a single tensor python reference could refer to different versions depending on when it's used. Not sure if that makes sense, but as I think about it more it's probably more complicated than is worthwhile anyway.\n", "@keithshep: That approach would be reasonable, but unfortunately I think it's too big a jump from what we have currently.\n", "Just a note -- we _do not_ want to blindly copy NumPy vectorized indexing in `__getitem__`. Instead, we want so-called \"outer indexing\" (like MATLAB), which is much more intuitive.\n\nThis means that indexing with mixed slices and arrays _should_ differ in TensorFlow compared to NumPy.\n\nSee this NumPy PR for examples and a discussion of the relevant issues: https://github.com/numpy/numpy/pull/6256/files\n\nI'm happy to elaborate if that doesn't make sense.\n", "@shoyer: I don't think you need to elaborate.  Outer indexing makes sense, but it isn't what numpy does.  Numpy is also more flexible than outer indexing.  Finally, I like numpy indexing. :)  Thus, unless I see a bunch of support from other people for deviating from numpy, we'll stick with numpy.\n", "@shoyer What do the odds look like for that PR being accepted on the Numpy side?\n", "@girving I agree that NumPy's vectorized indexing is great. The problem is that NumPy's implementation includes a hacks to make it \"more intuitive\" when slices and arrays are mixed, which work a lot of the time but then result in some very bizarre edge cases.\n\nThe [canonical example](https://mail.scipy.org/pipermail/numpy-discussion/2015-April/072607.html) is indexing operations like `x[0, :, [0, 1, 2, 3]]`, which converts an array of shape `(2, 3, 4)` to `(4, 3)` rather than the \"obvious\" `(3, 4)`. This is point (2) under the motivation in proposal PR (https://github.com/numpy/numpy/pull/6256).\n\nOne way to avoid this issue entirely is to prohibit mixed slice/array indexing. But if you do allow mixed slice/array indexing (which is very tempting to handle cases like `x[:, [0, 1, 2]]`), then this edge case will inevitably arise. Hence the appeal of the alternative indexing operation `vindex` for vectorized indexing, which lets indexing like `x[:, [0, 1, 2]]` and `x[[0, 1, 2], :]` do the obvious \"outer indexing\" thing without any kludges.\n\n@yaroslavvb There are a few niggling details to figure out in the proposal, but I think it's extremely like that it (and the implementation https://github.com/numpy/numpy/pull/6075) will be merged at some point. There was a long discussion last year about just how unintuitive numpy's current indexing behavior is, and I think all the core developers agreed that this is the obvious path forward.\n", "@shoyer: Please get it merged on the numpy side and then file a separate issue.  I predict a 0% chance that numpy will remove the existing indexing support or make it not the default: that would break millions of lines of code and alienate huge numbers of numpy users.  If they add it as an optional feature, TensorFlow could easily do the same thing with the same syntax.\n", "Will the version @aselle is working on support slicing by start/end tensors? This would be very useful for handling batches of variable-length sequences.\n\n``` python\ndata = [[2, 2, 1, 3], [3, 1, 0, 0], [2, 1, 3, 0]]\nlength = [4, 2, 3]\ndata[:, 1: length - 1] == [[2, 1], [0, 0], [1, 0]]\n```\n", "When will the negative indices be supported? like a[:,-1]\n", "@danijar, @shampool: Yes on both counts.  \n", "@girving Okay, cool. I hate to spam the thread but is there a way to get the behavior from my example without the new syntax?\n", "@danijar: Let's keep this thread focused on the new feature.  Please ask side questions on StackOverflow. \n", "hi, I attempted to write a temporary workaround (that someone might possibly find useful)\nhttps://gist.github.com/MInner/8b0c0a0e528303b132bf02e277199996\n\nit's more or less clear (at least in simplest case) how to do\n`A[:, b, :] # => (10, 50, 30)` - transpose, gather, transpose back \nor \n`A[a, b, c] # => (50, )` - pack, gather_nd\n, but it's not clear to me how could one do `A[a, b, :] # => (50, 30)` without creating super-large index matrix for `gather_nd`, though it's clear how to do `(50, 50, 30)`\n\n`A is float [10, 20, 30] in (0..1); \na, b, c are ints [50,] in [0..10]`\n", "I'm working on a gather_nd extension to support inner slicing.. Note that\ngather_nd still has no gradient.\nOn Jun 8, 2016 4:51 PM, \"Ben Usman\" notifications@github.com wrote:\n\n> hi, I attempted to write a temporary workaround (that someone might\n> possibly find useful)\n> https://gist.github.com/MInner/8b0c0a0e528303b132bf02e277199996\n> \n> it's more or less clear (at least in simplest case) how to do\n> A[:, b, :] # => (10, 50, 30) - transpose, gather, transpose back\n> or\n> A[a, b, c] # => (50, ) - pack, gather_nd\n> , but it's not clear to me how could one do A[a, b, :] # => (50, 30)\n> without creating super-large index matrix for gather_nd, though it's\n> clear how to do (50, 50, 30)\n> \n> A is float [10, 20, 30] in (0..1);\n> a, b, c are ints [50,] in [0..10]\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/206#issuecomment-224765129,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/ABtim2OtDT1jm1ulE1a0NYpSTv0AzN95ks5qJ1VlgaJpZM4GiDCf\n> .\n", "A workaround for the special case of indexing across some dimension with a list of integers, which might be useful to some folks:\n\n```\n ind = [3, 5, 0]\n # y = x[:,ind,:]  # this doesn't work right now\n y = tf.concat(1, [tf.expand_dims(x[:, i, :], 1) for i in ind])\n```\n\nI emphasize that this only works if `ind` a list of integerts but does not cover the case when `ind` is a Tensor of integers.\n", "Inspired by @nova77, a workaround (until gather_nd has gradients), allowing to use gather_nd normally:\n\n```\ndef gather_nd(params, indices, name=None):\n  shape = params.get_shape().as_list()\n  rank = len(shape)\n  flat_params = tf.reshape(params, [-1])\n  multipliers = [reduce(lambda x, y: x*y, shape[i+1:], 1) for i in range(0, rank)]\n  indices_unpacked = tf.unpack(tf.transpose(indices, [rank - 1] + range(0, rank - 1), name))\n  flat_indices = sum([a*b for a,b in zip(multipliers, indices_unpacked)])\n  return tf.gather(flat_params, flat_indices, name=name)\n```\n", "- [x] Implement rvalue basic indexing\n- [x] Implement lvalue basic indexing\n- [ ] Implement advanced indexing\n- [ ] Implement bool mask indexing\n", "Here's another workaround for the case where your `params` variable is of unknown length. Hope it helps, and if there is a better way currently known please let me know! Not sure about differentiability because I'm using it for a policy gradient\n\n```\n# we want output[i] = params[i,indices[i]]\nindex_mask = tf.reshape(tf.one_hot(indices, 3), [-1,3])\noutput = tf.reduce_sum(params * index_mask,1)\n```\n", "I am looking to do a max_pool that doesn't reduce the input tensor but replaces the non-max elements with zeros. Currently using a clumsy and slow workaround using space_to_depth, argmax, onehot, select and several transposes/reshapes.\n\nImplementing numpy-like indexing for assignment will help simplify this, so I'd like to add my vote for this feature. If anyone has any other ideas on how I can go about this, let me know, Thanks.\n", "@Fenugreek: Please keep this thread focused on the new feature. Please ask side questions on StackOverflow.  Note that votes do not count as discussions of the feature.\n", "Will this also support multi-indexing such as:\n`matrix[row_indices, col_indices]` where `matrix` is a `mxn` tensor and `row_indices` and `col_indices` are int32 vectors of sizes `k` such that k is less than m and n respectively. The slice above returns a length `k` vector eventually i.e. vectorized operation of `matrix[i,j] for all i,j in row_indices, col_indices`\n", "Eventually we may add support for  that (it's what numpy calls advanced indexing), but this first version will just focus on making the so called numpy basic slicing fully supported.\n", "@aselle With what we have now, what's the optimal way of achieving this sort of indexing? I was hoping this would be solved in this issue.\n", "@tejaskhot: Please keep this thread focused on the new features. Please ask side questions on StackOverflow.\n", "Any ideas as to when gather_nd will support gradient? \n", "We have 3 rank tensor([9, 7, 11)], so  we changed into 2 rank tensor([9,11]) with tf.gather_nd(...)\nBut the problem is, I've noticed that tensorflow optimizer can't support tf.gater_nd(..)\nwe were already applied to our program in above some idea\nThat's why, we want to get any idea\nPlease, help us....\n", "Closing this issue. As of 0.10 improved basic indexing and sliced assignment is available. Advanced, mixed advanced and basic, as well as boolean slicing still need implementation. See #4638 and #4639.\n", "@aselle This issue was closed but the question of when will the gradient for gather_nd be added is still not answered.\n", "@sonalgupta You can use my workaround from a previous comment. \n", "@sonalgupta If I understand correctly, you don't need `tf.gather_nd()` anymore but can use slicing which has implemented gradients.\n", "Internally  we are working on tf.scatter which will be used to implement tf.gather_nd(). Currently the pythonic syntactic sugar cannot generate tf.gather's but we will add that once tf.scatter is done. We will also need some broadcasting code to generate indexes as well. The advanced slicing is now a separate issue.\n", "@danijar is it documented that they have gradients? I was trying to find where its stated before I started to use them.\n", "This example works for me:\n\n``` python\nimport numpy as np\nimport tensorflow as tf\n\nvariable = tf.Variable(np.ones((5, 5)))\ncost = tf.reduce_sum(variable[0, :])  # Do more fancy slicing if needed.\noptimize = tf.train.GradientDescentOptimizer(0.1).minimize(cost)\n\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\n\nprint('Cost (should be decreasing):')\nfor _ in range(5):\n    print(sess.run([cost, optimize])[0])\n```\n", "`tf.gather_nd` does not have gradients yet, but it will soon. `tf.gather_nd` is not used by pythonic `__getitem__` slicing e.g. you cannot yet do \n`x=[1,2];y=[1,1]; foo[x,y]`.l @danijar's example is only \"basic indexing\". Everything that Tensor.**getiem** supports does support Gradients.\n", "Does .10 now support negative indexing? as in X[:,-1] for the last column of a tensor?  Thanks\n", "0.11RC does (and also master).\n", "thanks!\n", "A little inconvenience I came across.\nWhen indexing with the Tensor I get a type error.\nI will cast 'a' to int32 in the following code, but it is a bit of a hack I believe.\n\n``` python\nimport tensorflow as tf\n\nA = tf.constant([0,2,3,1])\nB = tf.constant([0,1,2,3])\n\na = tf.argmax(A,1)\nB[a]\n```\n\nTypeError: Input 'strides' of 'StridedSlice' Op has type int32 that does not match type int64 of argument 'begin'.\n", "Hey all, I want to update a tensor at certain locations (along one dimension only). I understand I can do it via scatter_update however it appears it doesn't have a registered gradient. Is there any workaround this?  \n", "@ravigarg27 a dumb workaround (if your new values are not `Variables` you're going to compute gradients over) is to do something conceptually like `A - bool(update!=0)*A + update`, where `update` has same shape as `A` and all not updated entries are equal to 0; however, that is quite out of scope of this thread :)\n", "@MInner What I want is something on lines of A[indices, :] = B where A and B are Variables matrices\n", "@danijar , I wonder if your [`def extract_last_relevant()`](https://github.com/tensorflow/tensorflow/issues/206#issuecomment-214560963) snipped for extracting last non-zero outputs of `tensorflow.models.rnn.dynamic_rnn`s posted above is still required (downside: `sparse to dense` warning indicating that it might lead to huge matrix allocation) or one could use new (0.11rc) partial implementation of smart indexing in tensorflow to address this issue? (I'm still little confused regarding which parts of smart indexing work now; there was an announcement in 0.11rc changelog regarding improvements in indexing, but they does not seem to address this specific \"indexing via another tensor\" issue, aren't they?)", "@MInner I played around with. Unfortunately, it doesn't seem like the new indexing can simplify that.", "Hey guys.\r\n\r\nSorry for bugging.\r\nAny update on the gradient implementation for tf.gather_nd() ?", "Anyone who needs to `tf.gather()` along the second dimension:\r\n\r\n```python\r\ndef gather_along_second_axis(data, indices):\r\n  flat_indices = tf.tile(indices[None, :], [tf.shape(data)[0], 1])\r\n  batch_offset = tf.range(0, tf.shape(data)[0]) * tf.shape(data)[1]\r\n  flat_indices = tf.reshape(flat_indices + batch_offset[:, None], [-1])\r\n  flat_data = tf.reshape(data, tf.concat([[-1], tf.shape(data)[2:]], 0))\r\n  result_shape = tf.concat([[tf.shape(data)[0], -1], tf.shape(data)[2:]], 0)\r\n  result = tf.reshape(tf.gather(flat_data, flat_indices), result_shape)\r\n  shape = data.shape[:1].concatenate(indices.shape[:1])\r\n  result.set_shape(shape.concatenate(data.shape[2:]))\r\n  return result\r\n```", "@danijar I might have misunderstood what you are doing, but wouldn't `tf.transpose(tf.gather(tf.transpose(x, ..), ..), ..)` do a thing? Moreover, `gather_nd()` thankfully has gradients now.", "@warmspringwinds it is deployed.\r\n", "@MInner You're right, we can use transpose to to gather along any dimension:\r\n\r\n```python\r\ndef gather_along_axis(data, indices, axis=0):\r\n  if not axis:\r\n    return tf.gather(data, indices)\r\n  rank = data.shape.ndims\r\n  perm = [axis] + list(range(1, axis)) + [0] + list(range(axis + 1, rank))\r\n  return tf.transpose(tf.gather(tf.transpose(data, perm), indices), perm)\r\n```\r\n\r\nThis is slower than my snipped above though, as it needs to rotate the whole data array in memory instead of computing the correct indices into the original tensor.", "@danijar good to know! ", "Hi,\r\n\r\nI just saw @nova77 's code above,\r\n\r\n```python\r\nx = tf.constant([[1, 2, 3],\r\n                 [4, 5, 6],\r\n                 [7, 8, 9]])\r\nidx = tf.constant([1, 0, 2])\r\nidx_flattened = tf.range(0, x.shape[0]) * x.shape[1] + idx\r\ny = tf.gather(tf.reshape(x, [-1]),  # flatten input\r\n              idx_flattened)  # use flattened indices\r\n\r\nwith tf.Session(''):\r\n  print y.eval()  # [2 4 9]\r\n```\r\n\r\nDo we still need this workaround? Or is there already a solution (or an issue for me to track)? Thanks!", "Can anyone help me with this? Embedding Gather always giving Negative Index. How to solve this? Using Keras with Tensorflow backend\r\n\r\n```\r\nInvalidArgumentError (see above for traceback): indices[0,6] = -1 is not in [0, 15664)\r\n\t [[Node: embedding_1/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_1/embeddings/read, embedding_1/Cast)]]\r\n```\r\n"]}, {"number": 204, "title": "Using 3d Input for seq2seq Models -- Word Vector Input", "body": "Hey TF, great code.\n\nI was wondering for your seq2seq models, if it is possible to insert 3d input. The reason being is that you usually want to embed your words to lets say 128 vectors. Google's Word2vec tool does this very well.\n\nHowever, it looks that for all the seq2seq models, the input to the encoder and decoder are only 2d. This means that each word has to be converted to an integer correct? It would be nice if we could insert a 128 vector instead. Or perhaps I'm missing something? Thanks! \n", "comments": ["Hey @LeavesBreathe, can you please re-post this to the discussion [mailing list](https://groups.google.com/a/tensorflow.org/forum/#!forum/discuss) instead?  This is more of a discussion than a bug report.\n", "Will do thanks for the direction\n"]}, {"number": 203, "title": "Promote usage of docker above all other installation methods", "body": "Shouldn't we just focus on docker? At least on the main page, and then link to other methods elsewhere.\n\nWould just need a good docker development workflow (I personally got none yet).\n", "comments": ["Docker has tricky GPU support as well as CPU resource allocation that can lead to poor outcomes.\n", "Ouch. That's really good to know. But are you sure those are issues for TF?\n", "Well, I should caveat that my experiences has been on windows 10.  Perhaps it's better on linux/mac.  I'm having a hard time get a simple matrix_inverse to peg one of my cores to --cpu-shares=0  or even recognize more than one core.   The GPU issues are tricky though, they don't come by default.   Have you got it working with docker yet?\n", "Nope. I'm currently at https://codenewbie.slack.com/archives/general trying to work out a dev workflow first. Never done any with docker before. Here are a few trails I'm following:\n- http://stackoverflow.com/questions/30090007/whats-the-right-way-to-setup-a-development-environment-on-os-x-with-docker\n- http://discourse.codenewbie.org/t/docker-boot2docker-exposing-app-on-localhost/1148/3\n- https://www.airpair.com/node.js/posts/getting-started-with-docker-for-the-nodejs-dev\n", "Ah, ok, well fwiw, for docker I just installed it from http://docs.docker.com/v1.8/installation/ and ran the command they give on the tensorflow website docker run -it b.gcr.io/tensorflow/tensorflow.  It all worked perfectly fine.   Well, I had to open my computer's firewall to accept loopback connections, but I don't that's related to either docker or TF.\n", "It's related to boot2docker's vm. I'm at nearly that point as well, but not happy while I can't git everything up properly.\n", "Thank you for the suggestion.  In general, docker is the best approach for building a hermetic installation of TensorFlow and I encourage most heavy TensorFlow users to follow this approach.  In general, however, pip installs are easier for users who just want to play with TF.  And while I would encourage most such users to install in virtualenv, in practice it's always less friction to just gamble that there won't be any conflicts with system installed packages in a regular pip install.\n\nAll that to say is, [there's more than one way to skin a cat](http://www.phrases.org.uk/meanings/there-is-more-than-one-way-to-skin-a-cat.html).  Closing this for now since #149 is tracking building proper Docker images.\n\n@timshephard and @cauerego PTAL at [this script](https://github.com/tensorflow/tensorflow/blob/41930f0b81b52a34fb56d921c9bad65c36168323/tensorflow/tools/docker/docker_run_gpu.sh) for details on how to forward GPU support into the docker container.  This has worked for us.\n", "@ebrevdo [To me](http://talk.cregox.com/t/my-social-unfair-undervaluation-and-why-the-financial-system-is-broken/7751?u=cregox), you're just saying you will probably look at this back again in the future, but can't predict when! ;P\n\nIf you can confirm this, I will try and make docker installation simpler than pip. Would this work for you? If it is possible, I can probably and will eventually do it! :)\n\n(Just not with GPU support, of course, but that's beyond the point)\n\nPS: Never heard of [PTAL](http://www.acronymfinder.com/PTAL.html) before, google didn't help _much_. :)\n"]}, {"number": 202, "title": "Better error message for tf.assign", "body": "When I try to use tf.assign I'm always running into the error message\n\nTypeError: Input 'ref' of 'Assign' Op requires l-value input\n\nI have no idea what this means. It's a pretty unhelpful error message.\n\nSome context ...\n\n```\na = tf.random_normal([4,4]) # I tried this with tf.constant and tf.Variable and got the same result\n\ntf.assign(tf.slice(a, [1, 0], [-1, -1]), tf.slice(a, [0, 0], [3, -1]))\n\ntf.assign(a, 5)\n\ntf.assign(a, a)\n```\n\nAll three give the same error message.\n", "comments": ["http://tensorflow.org/get_started/basic_usage.md#variables  -- you might find that useful.\n\nYou can only assign to a tf.Variable -- that is the only mutable state you can use the assign ops with.\n\nWe'll see if we can come up with a better error message in the meantime.  Thanks for the report!\n", "@michaelisard: Assigning to you since presumably this will change as part of upcoming work. \n", "Switching to @prb12: Should we mark this as contributions welcome or is something more elaborate than an error message change warranted?\n", "It makes no sense to assign this to me?  \n", "This is pretty much the exact error that gcc gives when you try to write \nf(){ 3 = f; }\nOTOH, since we are embedded in another programming language the subtlety between a tensorflow variable and a python variable that is a handle to partial tensorflow graph is not obvious to the uninitiated.  Leaving open in case someone wants to update the error message.\n", "FWIW, I think vrv's explanation would make a very clear error message:\r\n\"TypeError: You can only tf.assign to a tf.Variable\"\r\nand the actual error message of \r\n\"TypeError: Input 'ref' of 'Assign' Op requires l-value input\" \r\nmakes no sense unless one knows what an \"I-value input\" is (which I don't, even after reading this thread). ", "I can work on this but I wanted to clarify something:\r\n\r\nThis error message [0] is only raised when the ref input needs to be of type `tf.Variable`, right? I can't find another example of it being raised. If so, this change is really just a matter of updating [0] and I can have a PR in no time :)\r\n\r\n[0] https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/op_def_library.py#L621", "@iRapha That sounds right."]}, {"number": 201, "title": "How To doc for tensor indexing and assigning ops", "body": "If you're coming from numpy you're used to be able to index and assign into arrays in the usual way.\n", "comments": ["Thanks for the note!  We're working on improving our suite of indexing / assignment ops, and we'll definitely want a good tutorial once they're in place.  There's a tracking bug for the necessary functionality here: https://github.com/tensorflow/tensorflow/issues/206\n", "Closing as duplicate of #206.\n"]}, {"number": 200, "title": "Broken link on beginner's tutorial page", "body": "At the bottom of http://www.tensorflow.org/tutorials/mnist/beginners/index.md is a link to \"the next tutorial\": http://www.tensorflow.org/tutorials/index.md\n\nFor some reason, that link redirects to the home page. Removing index.md from the end of the URL works.\n", "comments": ["Thanks for the report! Indeed, it should point to http://tensorflow.org/tutorials/mnist/pros/index.md\n\nWe'll check in that fix, and it will appear on the website when it's next updated.\n"]}, {"number": 199, "title": "a Makefile would be supremely helpful", "body": "It'd be really useful to have a Makefile to build the tree.  Bazel (or it is blaze?) isn't working for us out of the gate, and it's rather non-obvious what it will try to do given its config files.\n\nIf someone could provide a transcript of a full build (preferably with GPU), showing external commands run and simulated (e.g., if bazel does 'cp' itself, for example), I'll try to work on this.\n\nFor the record, here's the initial fail I see when trying to build, without GPU to keep things simple.  No doubt I'm making some novice error.  Nonetheless, this diagnostic is rather opaque compared to what one might expect from 'make'.  (Maybe something is missing?  The tree was grabbed with 'git clone --recurse-submodules' though.) \n\n$ bazel build --logging=6 -c opt //tensorflow/cc:tutorials_example_trainer  \nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nERROR: Loading of target '//tools/cpp:toolchain' failed; build aborted: no such package 'tools/cpp': BUILD file not found on package path.\nERROR: Loading failed; build aborted.\nINFO: Elapsed time: 0.877s\n", "comments": ["What version of bazel are you using?  Is it the one suggested in the getting started guide?  Do you have gcc/g++ installed on your machine?  If none of these help, try running the build with --verbose-failures and report back.\n", "The bazel version and g++ presence were fine.  The problem turned out to be a quirk in our environment.  We use environment modules and apparently that doesn't fit seamlessly with bazel's assumptions about its environment, necessitating a special bazelrc on every run, which I had left out.  That is, the error message was not about tensorflow, but rather bazel complaining that it couldn't find all of its parts.  (which didn't occur to me as 'make' simply cannot fail this way)\n\nThis got me farther, but unfortunately there are still showstopping build issues.  (We need to add our own include flag for a numpy header, and bazel fights hard to prevent this.)\n\nI'm sure bazel has advantages inside of Google, but for the purposes of just downloading and playing with this externally, it's all stick, unfortunately.  As before, a Makefile would be really helpful.\n\nP.S.  That's how that flag _should_ be spelled, but bazel cleverly invents its own novel option syntax as well (--verbose_failures).\n", "We're working on a way to automatically infer the location of the numpy headers.  The current locations are [here](https://github.com/tensorflow/tensorflow/blob/72a5a60dd4664a7caa4611344364ac7851464a60/tensorflow/python/BUILD#L698).\n", "I'm closing this for now unless you have any other issues we can help with.  I know the title is about having a Makefile, but the question seemed to be more related to bazel issues.  I don't think we have Makefile/CMakeFile creation on the horizon but feel free to reopen a new issue limiting the description to just that fact and we'll keep it in mind in future planning.\n", "It's your call, but to me the lack of a Makefile or some similar understandable and easy-to-adapt substitute is an issue.  I provided some supporting information just to illustrate that this isn't mere bikeshedding, but I hope that doesn't distract from the big picture.  I can't think of any open source project I've had as much trouble trying to build from scratch.  Cheers.\n", "> P.S. That's how that flag should be spelled, but bazel cleverly invents its own novel option syntax as well (--verbose_failures).\n\nFWIW, that is the syntax used by [gflags](https://gflags.github.io/gflags/).\n"]}, {"number": 198, "title": "cumulative longer epochs", "body": "A simple logistic regression (or any other function for that matter), incurs a hefty penalty over time. Code [gist here](https://gist.github.com/graphific/7f0a214350fd763275b8). \nEach training epoch takes increasingly longer, from 1.7s for th first epoch and already at 2.7 at 20 epochs:\n\n```\nEpoch: 0001 cost= 29.860479714\ntime this epoch= 1.749361\nEpoch: 0002 cost= 22.108508758\ntime this epoch= 1.873362\nEpoch: 0003 cost= 21.061937475\ntime this epoch= 2.053877\nEpoch: 0004 cost= 20.553687506\ntime this epoch= 2.401098\nEpoch: 0005 cost= 20.262585952\ntime this epoch= 2.563413\nEpoch: 0006 cost= 19.878952036\ntime this epoch= 2.675142\nEpoch: 0007 cost= 19.573502880\ntime this epoch= 2.638027\nEpoch: 0008 cost= 19.433302383\ntime this epoch= 2.632813\nEpoch: 0009 cost= 19.428594169\ntime this epoch= 2.616152\nEpoch: 0010 cost= 19.276754012\ntime this epoch= 2.553015\nEpoch: 0011 cost= 19.082279745\ntime this epoch= 2.571626\nEpoch: 0012 cost= 19.017995413\ntime this epoch= 2.657914\nEpoch: 0013 cost= 18.981637795\ntime this epoch= 2.684047\nEpoch: 0014 cost= 18.957392608\ntime this epoch= 2.626817\nEpoch: 0015 cost= 18.679365802\ntime this epoch= 2.629546\nEpoch: 0016 cost= 18.732087401\ntime this epoch= 2.633352\nEpoch: 0017 cost= 18.655737654\ntime this epoch= 2.662984\nEpoch: 0018 cost= 18.450943058\ntime this epoch= 2.765998\nEpoch: 0019 cost= 18.539302767\ntime this epoch= 2.774611\nEpoch: 0020 cost= 18.555477153\ntime this epoch= 2.79623\nEpoch: 0021 cost= 18.401914994\ntime this epoch= 2.81699\nEpoch: 0022 cost= 18.408855550\ntime this epoch= 2.864744\nEpoch: 0023 cost= 18.456620145\ntime this epoch= 2.852341\nEpoch: 0024 cost= 18.365951549\ntime this epoch= 2.787942\nEpoch: 0025 cost= 18.230793715\ntime this epoch= 2.748816\n```\n\nit might be something wrong in the code, or things to be fixed with tensorflow, any ideas?\n", "comments": ["I am not able to reproduce this unfortunately:\n\n```\nEpoch: 0001 cost= 29.860476766\ntime this epoch= 6.674506\nEpoch: 0002 cost= 22.030540740\ntime this epoch= 3.98675\nEpoch: 0003 cost= 21.065916606\n...\n\ntime this epoch= 6.122648\nEpoch: 0023 cost= 18.377196394\ntime this epoch= 6.448846\nEpoch: 0024 cost= 18.326277539\ntime this epoch= 6.469679\nEpoch: 0025 cost= 18.222360275\ntime this epoch= 6.436373\n\n```\n\nIt's a bit noisy, but it doesn't look like it's consistently increasing.\n", "@vrv k tnx for testing, ill run it over a few different machines as well, and see if I can narrow it down.\n", "(Clearing out our open issues list: feel free to reopen if you still think there's a problem)\n"]}, {"number": 197, "title": "Small typo in Beginners MNIST Tutorial", "body": "There is an extra 'as' in the following sentence, in the \"The MNIST Data\" section:\n\n> For the purposes of this tutorial, we're going to want our labels as as \"one-hot vectors\".\n\nhttp://tensorflow.org/tutorials/mnist/beginners/index.md\n", "comments": ["There's another one under the \"Training\" section as well:\n\n> Because TensorFlow know the entire graph of your computations...\n\nShould be:\n\n> Because TensorFlow knows the entire graph of your computations...\n", "Both of these are fixed in git, pending a push to the website, hopefully soon!\n"]}, {"number": 196, "title": "Padding type definition is swapped in the documentation.", "body": "In http://tensorflow.org/api_docs/python/nn.md#pooling and http://tensorflow.org/api_docs/python/nn.md#convolution\n\nThe definition of the padding types should be swapped.\n\nNow it is like this:\n\n> padding = 'SAME': Round down (only full size windows are considered).\n> padding = 'VALID': Round up (partial windows are included).\n\nBut it should be like this:\n\n> padding = 'SAME': Round up (partial windows are included).\n> padding = 'VALID': Round down (only full size windows are considered).\n\nI also think it should be made clear that the padding is already included in the shape(value) of the formula \"shape(output) = (shape(value) - ksize + 1) / strides\".\n\nOr maybe the formula should be changed to the following simpler one (taken from http://cs231n.github.io/convolutional-networks/#conv):\n\n> shape(output) = ((shape(value)+2*pad - ksize) / strides) + 1\n\nwhere pad = 0 if padding='VALID' or pad = floor(ksize/2) if padding='SAME'.\n", "comments": ["Thanks - the padding documentation definitely needs modification. The padding scheme is not as simple as you mentioned though - it follows some legacy reason which involved some non-trivial padding computations. I'll update the documentation to add the details.\n\nFor the time being, if you are interested in the the exact math, it is described here:\n\nhttps://github.com/Yangqing/caffe2/blob/master/caffe2/proto/caffe2_legacy.proto#L8\n", "Thanks again for reporting this. We have fixed this in our internal codebase and will be pushing it soon. Closing this for now.\n", "Propose to enrich the document of convolution. It is still quite confusing. \nhttp://tensorflow.org/api_docs/python/nn.md#convolution\n\nI think what **cesarsalgado** mentioned is correct. \n", "Could anyone help me to calculate each step output of cifar10 example of tensorflow?\nThe input image size is 24*24. But in max pooling step, the kernel size is 3 and stride is 2, so (24-3)/2+1 is not integer. What is wrong here? I am new in deep learning. Thank you for helping\n", "Hi,\n\nBeginner questions are best suited for Stack Overflow, not random semi-related bugs. Please post your question there instead.\n"]}, {"number": 195, "title": "Maybe the website in other languages?", "body": "We'll be able to translate the site for other idioms? \n", "comments": ["I'm going to close this as a duplicate of #73, we can discuss translating to other language there.\n"]}, {"number": 194, "title": "g3doc tutorial mnist.py tf.range missing argument", "body": "`indices = tf.expand_dims(tf.range(batch_size), 1)`\nin https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/tutorials/mnist/mnist.py#L94\n\nwhich should be `indices = tf.expand_dims(tf.range(0, batch_size, 1), 1)`\n\nThe [tutorial doc](http://www.tensorflow.org/tutorials/mnist/tf/index.md) shows the correct code.\n", "comments": ["Apologies for the inconsistency.  We fixed the code to allow one-argument range, but the release hasn't changed and that's probably what you're using.  Good to know about the synchronization issue; we'll keep it in mind in future. \n", "I'm going to fix this in git to be compatible back to 0.5.0.\n", "Fixed, will be part of the next git commit.\n"]}, {"number": 193, "title": "Building from source for BSD", "body": "Arch linux uses this line:\n\n```\nbazel build --jobs 2 -c opt //tensorflow/tools/pip_package:build_pip_package\n```\n\nI am on BSD, and I would rather build just executables, and not a pip package. How do I do this? Could you add build instructions?\n", "comments": ["Building from source is on our [installation page](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md), but we haven't tested it for BSD so it may not work out of the box.\n", "Can you give the build a try on BSD, and if it fails, add the flag --verbose_errors and paste them?  Would love to see what issues you have.  Please also provide us with details about your setup (BSD type and version, architecture, etc.)\n", "The only bazel command without \"pip\" on installation page is this:\n\n```\n$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\n```\n\nand it fails:\n\n```\nERROR: Loading of target '//tools/jdk:jdk' failed; build aborted: no such package 'tools/jdk': BUILD file not found on package path.\nERROR: Loading failed; build aborted.\nINFO: Elapsed time: 0.075s\n```\n\nWhat is this \"tools/jdk\"? I do have jdk installed.\n", "What version of jdk do you have installed?  How did you install bazel?  Can you run the build with the flag --verbose_failures?\n", "I have jdk-8.60.24. Bazel is from FreeBSD port.\n\n--verbose_failures doesn't change the output.\n", "xUbuntu use this command\n\n```\nbazel build --jobs 2 -c opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures\n```\n\nthe error is\n\n```\nINFO: From Compiling tensorflow/python/client/tf_session_helper.cc:\nIn file included from tensorflow/python/client/tf_session_helper.cc:1:0:\n./tensorflow/python/client/tf_session_helper.h:4:20: fatal error: Python.h: No such file or directory\n #include <Python.h>\n                    ^\ncompilation terminated.\nERROR: /home/aurora/workspace/tensorflow/tensorflow/tensorflow/python/BUILD:698:1: C++ compilation of rule '//tensorflow/python:tf_session_helper' failed: gcc failed: error executing command \n  (cd /home/aurora/.cache/bazel/_bazel_aurora/67e9d46b2d6eaad7004805288b920919/tensorflow && \\\n  exec env - \\\n    INTERCEPT_LOCALLY_EXECUTABLE=1 \\\n    PATH=/home/aurora/software/anaconda2/bin:/usr/local/cuda/bin:/home/aurora/software/jdk1.8.0_25/bin:/usr/local/cuda/bin:/home/aurora/software/jdk1.8.0_25/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \\\n  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -iquote . -iquote bazel-out/local_linux-opt/genfiles -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda/include -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -I/usr/include/python2.7 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/python/_objs/tf_session_helper/tensorflow/python/client/tf_session_helper.pic.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/python/_objs/tf_session_helper/tensorflow/python/client/tf_session_helper.pic.d -fPIC -c tensorflow/python/client/tf_session_helper.cc -o bazel-out/local_linux-opt/bin/tensorflow/python/_objs/tf_session_helper/tensorflow/python/client/tf_session_helper.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: gcc failed: error executing command \n  (cd /home/aurora/.cache/bazel/_bazel_aurora/67e9d46b2d6eaad7004805288b920919/tensorflow && \\\n  exec env - \\\n    INTERCEPT_LOCALLY_EXECUTABLE=1 \\\n    PATH=/home/aurora/software/anaconda2/bin:/usr/local/cuda/bin:/home/aurora/software/jdk1.8.0_25/bin:/usr/local/cuda/bin:/home/aurora/software/jdk1.8.0_25/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \\\n  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -iquote . -iquote bazel-out/local_linux-opt/genfiles -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda/include -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -I/usr/include/python2.7 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/python/_objs/tf_session_helper/tensorflow/python/client/tf_session_helper.pic.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/python/_objs/tf_session_helper/tensorflow/python/client/tf_session_helper.pic.d -fPIC -c tensorflow/python/client/tf_session_helper.cc -o bazel-out/local_linux-opt/bin/tensorflow/python/_objs/tf_session_helper/tensorflow/python/client/tf_session_helper.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\n```\n\nhow to solve this error\nthanks\n", "@auroua looks like you're missing the python-dev package, and your question is unrelated to the one above.  if you continue to have problems please open a new issue instead of commenting in this one.\n\n@yurivict sadly we don't have any FreeBSD boxes to play with so you're our guinea pig.  the problem stems from the \"jdk\" definition [here](https://github.com/bazelbuild/bazel/blob/687550660db90b007741e1f67a8092b08945f8e3/tools/jdk/BUILD) in the bazel code.  Do you have java/javac in your path?  If you do, can you try building bazel 0.1.0 on your machine and see if that solves the problem?\n", "```\n# which javac\n/usr/local/bin/javac\n```\n\nI did build the bazel port on my machine. In fact, I created the bazel port for FreeBSD -)\nDuring the bazel port build, tools/jdk/BUILD is present there. bazel builds into one solid executable and passes all tests that come with it. By looking at tools/jdk/BUILD I can't tell what is wrong with it.\n\nI am not sure where I go from here.\n", "I will ask this question on bazel page. My command fails like this:\n\n```\n# bazel build --jobs 2 -c opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures\nERROR: Loading of target '//tools/jdk:GenClass_deploy.jar' failed; build aborted: no such package 'tools/jdk': BUILD file not found on package path.\nERROR: Loading failed; build aborted.\nINFO: Elapsed time: 0.116s\n```\n", "It was an incomplete bazel installation.\n\nNow I am getting thee errors:\n\n```\nERROR: /usr/ports/math/tensorflow/work/tensorflow-0.5.0/tensorflow/python/BUILD:1: Extension file not found: 'google/protobuf/protobuf.bzl'.\nERROR: /usr/ports/math/tensorflow/work/tensorflow-0.5.0/tensorflow/tensorboard/BUILD:40:1: error loading package 'tensorflow/python': Extension file not found: 'google/protobuf/protobuf.bzl' and referenced by '//tensorflow/tensorboard:tensorboard'.\nERROR: Loading failed; build aborted.\n```\n\nAnother problem is that build process downloads from GitHub. Is there an easy way to switch to the the pre-downloaded GitHub projects folder? All BSDs will have problem with this, because it is unsafe to allow any downloads during build. So you should have the way to build it from pre-downloaded folder.\n", "@yurivict I meet the same error as yours. Have you solved it yet?\n", "@xubenben  Not yet.\n", "I am having the same problem in archlinux \n", "TensorFlow has a Git submodule containing protobuf. You would get the `Extension file not found: 'google/protobuf/protobuf.bzl'` error if you did not clone the tensorflow repo with the `--recursive` flag to pull in all the Git submodules. Can you try running `git submodule update --init` in your repo to pull in all submodules and try building again?\n\nPerhaps we should add a check to the tensorflow `configure` script to raise an error if the submodules have not been cloned? In the meantime, we will also work on making the Bazel error message more clear (see bazelbuild/bazel#605). \n", "Why do you need \"Git submodule\", and not the regular package dependency? I already have protobuf installed.\n", "We were using a version newer than the latest tagged version, which was extremely unlikely to be installed anywhere. To keep the installation simple, we packaged protobuf as a submodule. Assuming we find no more issues in protobuf that we have to fix at head, we can move to making protobuf a library dependency once all our fixes are in a reasonably common released version.\n", "Closing due to inactivity, related items are #1069, and possibly items that are bazel related.\n", "#2571 details how to compile TensorFlow on FreeBSD. Posting because this thread is near the top in search results.\n"]}, {"number": 192, "title": "ImportError: No module named copyreg", "body": "when I use OS X system to write first tensorflow program,I get some error message, who can tell me how to fix it??\n## here is error message\n\nbogon:~ ra$ python\nPython 2.7.10 (default, Jul 14 2015, 19:46:27) \n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.39)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n> > > import tensorflow as tf\n> > > Traceback (most recent call last):\n> > >   File \"<stdin>\", line 1, in <module>\n> > >   File \"/Library/Python/2.7/site-packages/tensorflow/**init**.py\", line 4, in <module>\n> > >     from tensorflow.python import *\n> > >   File \"/Library/Python/2.7/site-packages/tensorflow/python/**init**.py\", line 13, in <module>\n> > >     from tensorflow.core.framework.graph_pb2 import *\n> > >   File \"/Library/Python/2.7/site-packages/tensorflow/core/framework/graph_pb2.py\", line 8, in <module>\n> > >     from google.protobuf import reflection as _reflection\n> > >   File \"/Library/Python/2.7/site-packages/google/protobuf/reflection.py\", line 58, in <module>\n> > >     from google.protobuf.internal import python_message as message_impl\n> > >   File \"/Library/Python/2.7/site-packages/google/protobuf/internal/python_message.py\", line 59, in <module>\n> > >     import six.moves.copyreg as copyreg\n> > > ImportError: No module named copyreg\n", "comments": ["Search in issues. Someone already answered.\n", "Solutions described in our [install instructions](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md) at the bottom.  Let us know if that doesn't help!\n", "thanks\n"]}, {"number": 191, "title": "Unrecognized option: --data_dir", "body": "When I try to run the command \n\n```\nbazel run -c opt <...>/models/rnn/translate/translate.py\n  --data_dir [your_data_directory]\n```\n\nI get the error: \n`Unrecognized option: --data_dir`\n\nIs my syntax wrong?\n", "comments": ["you need to add a '--' after translate.py to distinguish options passed to the python script vs options passed to bazel:\n\n```\nbazel run -c opt <...>/models/rnn/translate/translate.py --\n  --data_dir [your_data_directory]\n```\n", "Yeah, bazel run syntax requires `--` to separate. \n", "When I try to run this command:\n\n`bazel run -c opt tensorflow/models/rnn/translate/translate.py -- --   data_dir ../data/translate/`\nI get the following error:\n\n```\n...................\nERROR: Cannot run target //tensorflow/models/rnn/translate:translate.py: Not   executable.\nINFO: Elapsed time: 1.537s\nERROR: Build failed. Not running target.\n```\n\nAny ideas how to resolve?\n", "Drop the .py from the end of the bazel run command.\nOn Nov 13, 2015 11:49 PM, \"mikeconnors909\" notifications@github.com wrote:\n\n> When I try to run this command:\n> \n> bazel run -c opt tensorflow/models/rnn/translate/translate.py -- --\n> data_dir ../data/translate/\n> I get the following error:\n> \n> ...................\n> ERROR: Cannot run target //tensorflow/models/rnn/translate:translate.py: Not   executable.\n> INFO: Elapsed time: 1.537s\n> ERROR: Build failed. Not running target.\n> \n> Any ideas how to resolve?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/191#issuecomment-156664411\n> .\n"]}, {"number": 190, "title": "when install from sources, I encounter ERROR: C++ compilation of rule '//google/protobuf:protobuf_lite' failed: crosstool_wrapper_driver_is_not_gcc failed", "body": "Do anyone compile the source of Tensor Flow?\nWhen I install Tensor Flow from sources, After I excute under command in terminal. \n\n``` bash\nbazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\n```\n\nI encounter the follow error.\n\n```\nERROR: ~/tensorflow/google/protobuf/BUILD:29:1: C++ compilation of rule \n'//google/protobuf:protobuf_lite' failed: crosstool_wrapper_driver_is_not_gcc failed: error \nexecuting command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -\nU_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-\nparameter -Wno-free-nonheap-object ... (remaining 40 argument(s) skipped).\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\n```\n\nMy OS is Ubuntu 14.04\uff0cGCC/G++ is 4.7, GPU is K40c, Python is 2.7,  cuda is 7.0, cudnn is 6.5v2, bazel is installed by compile source code.\n", "comments": ["Can you run the build with the flag --verbose_failures?  Perhaps this will give more info.\n", "Yes,no Problem.\nI run the build with the flag `--verbose_failures` by excute under code in terminal.\n\n```\nbazel build -c opt --config=cuda --verbose_failures //tensorflow/cc:tutorials_example_trainer\n```\n\nAnd, follow is my ERROR information\n\n```\n# command\nroot@optimal4:/home/share/TensorFlow/tensorflow# bazel build -c opt --config=cuda --\nverbose_failures //tensorflow/cc:tutorials_example_trainer\n\n# ERROR information\nINFO: Found 1 target...\nINFO: From Compiling google/protobuf/src/google/protobuf/compiler/objectivec/objectivec_oneof.cc \n[for host]:\nsrc/main/tools/namespace-sandbox.c:633: execvp(argv[0], argv): No such file or directory\nERROR: /home/share/TensorFlow/tensorflow/google/protobuf/BUILD:166:1: C++ compilation of rule \n'//google/protobuf:protoc_lib' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command \n  (cd /root/.cache/bazel/_bazel_root/def3d5a3fd5ec6e6da9184558cd2b5fe/tensorflow && \\\n  exec env - \\\n    PATH=.:/usr/local/jdk/bin:/usr/local/jdk/jre/bin:/home/share/TensorFlow/bazel/output:/home/xuezhisd/w\nork/caffe/build/tools:/usr/local/ffmpeg/bin:/usr/local/cuda/bin:usr/local/MATLAB/R2014b/bin:.:/usr/local/j\ndk/bin:/usr/local/jdk/jre/bin:/usr/local/cuda/bin:/usr/local/MATLAB/R2014b/bin:/usr/local/sbin:/usr/local/b\nin:/usr/sbin:/usr/bin:/sbin:/bin \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-\nD_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-\nnonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-\nstd=c++11' -iquote . -iquote bazel-out/host/genfiles -iquote external/bazel_tools -iquote bazel-\nout/host/genfiles/external/bazel_tools -isystem google/protobuf/src -isystem bazel-\nout/host/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -DHAVE_PTHREAD \n-Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare '-Wno-error=unused-function' -no-\ncanonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-\nD__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-\nout/host/bin/google/protobuf/_objs/protoc_lib/google/protobuf/src/google/protobuf/compiler/objectivec/\nobjectivec_oneof.o' -MD -MF bazel-\nout/host/bin/google/protobuf/_objs/protoc_lib/google/protobuf/src/google/protobuf/compiler/objectivec/\nobjectivec_oneof.d -c google/protobuf/src/google/protobuf/compiler/objectivec/objectivec_oneof.cc -o \nbazel-\nout/host/bin/google/protobuf/_objs/protoc_lib/google/protobuf/src/google/protobuf/compiler/objectivec/\nobjectivec_oneof.o): crosstool_wrapper_driver_is_not_gcc failed: error executing command \n  (cd /root/.cache/bazel/_bazel_root/def3d5a3fd5ec6e6da9184558cd2b5fe/tensorflow && \\\n  exec env - \\\n    PATH=.:/usr/local/jdk/bin:/usr/local/jdk/jre/bin:/home/share/TensorFlow/bazel/output:/home/xuezhisd/w\nork/caffe/build/tools:/usr/local/ffmpeg/bin:/usr/local/cuda/bin:usr/local/MATLAB/R2014b/bin:.:/usr/local/j\ndk/bin:/usr/local/jdk/jre/bin:/usr/local/cuda/bin:/usr/local/MATLAB/R2014b/bin:/usr/local/sbin:/usr/local/b\nin:/usr/sbin:/usr/bin:/sbin:/bin \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-\nD_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-\nnonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-\nstd=c++11' -iquote . -iquote bazel-out/host/genfiles -iquote external/bazel_tools -iquote bazel-\nout/host/genfiles/external/bazel_tools -isystem google/protobuf/src -isystem bazel-\nout/host/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -DHAVE_PTHREAD \n-Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare '-Wno-error=unused-function' -no-\ncanonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-\nD__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-\nout/host/bin/google/protobuf/_objs/protoc_lib/google/protobuf/src/google/protobuf/compiler/objectivec/\nobjectivec_oneof.o' -MD -MF bazel-\nout/host/bin/google/protobuf/_objs/protoc_lib/google/protobuf/src/google/protobuf/compiler/objectivec/\nobjectivec_oneof.d -c google/protobuf/src/google/protobuf/compiler/objectivec/objectivec_oneof.cc -o \nbazel-\nout/host/bin/google/protobuf/_objs/protoc_lib/google/protobuf/src/google/protobuf/compiler/objectivec/\nobjectivec_oneof.o).\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nINFO: Elapsed time: 2.362s, Critical Path: 1.03s.\n```\n", "This happened to me when I tried to use a different version of bazel than 0.1.0\n", "Can you say what version of bazel you are using?  And try @SWu's advice and see if it solves the problem?\n", "I used bazel-master.\n\nWhen I try to compile and install bazel 0.1.0, I also encounter some errors.\nI excute follow command,\n\n``` bash\n[root@/home/share/TensorFlow/bazel-0.1.0]#./compile.sh build --verbose_failures\n```\n\nand I got follow error information.\n\n```\nINFO: You can skip this first step by providing a path to the bazel binary as second argument:\nINFO:    ./compile.sh build /path/to/bazel\n\ud83c\udf43  Building Bazel from scratch............\n\ud83c\udf43  Building Bazel with Bazel.\n.Extracting Bazel installation...\nSending SIGTERM to previous Bazel server (pid=42474)... done.\n........\nINFO: Found 1 target...\nINFO: From Linking src/main/native/libunix.so:\n/usr/bin/ld: bazel-out/local_linux-\nfastbuild/bin/src/main/native/_objs/libunix.so/src/main/native/unix_jni.pic.o: relocation \nR_X86_64_PC32 against symbol `_Z13PostExceptionP7JNIEnv_iRKSs' can not be used\n when making a shared object; recompile with -fPIC\n/usr/bin/ld: final link failed: Bad value\ncollect2: error: ld returned 1 exit status\nERROR: /home/share/TensorFlow/bazel-0.1.0/src/main/native/BUILD:28:1: Linking of rule \n'//src/main/native:libunix.so' failed: gcc failed: error executing command /usr/bin/gcc \n-shared -o bazel-out/local_linux-fastbuild/bin/src/main/native/libunix.so -Wl,-whole-archive ... \n(remaining 16 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: \nProcess exited with status 1.\nERROR: Sandboxed execution failed: //src/java_tools/buildjar:skylark-deps..\nERROR: Sandboxed execution failed: //src/main/protobuf:proto_crosstool_config_srcjar..\nERROR: Sandboxed execution failed: //src:android_tools_repository_zip..\nERROR: Sandboxed execution failed: //src/main/java:shell-skylark..\nTarget //src:bazel failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 6.436s, Critical Path: 2.87s\n```\n", "Are you on a 32 bit system?\nOn Nov 13, 2015 5:40 PM, \"xuezhisd\" notifications@github.com wrote:\n\n> I used bazel-master.\n> \n> When I try to compile and install bazel 0.1.0, I also encounter some\n> errors.\n> I excute follow command,\n> \n> [root@/home/share/TensorFlow/bazel-0.1.0]#./compile.sh build --verbose_failures\n> \n> and I got follow error information.\n> \n> INFO: You can skip this first step by providing a path to the bazel binary as second argument:\n> INFO:    ./compile.sh build /path/to/bazel\n> \ud83c\udf43  Building Bazel from scratch............\n> \ud83c\udf43  Building Bazel with Bazel.\n> .Extracting Bazel installation...\n> Sending SIGTERM to previous Bazel server (pid=42474)... done.\n> ........\n> INFO: Found 1 target...\n> INFO: From Linking src/main/native/libunix.so:\n> /usr/bin/ld: bazel-out/local_linux-\n> fastbuild/bin/src/main/native/_objs/libunix.so/src/main/native/unix_jni.pic.o: relocation\n> R_X86_64_PC32 against symbol `_Z13PostExceptionP7JNIEnv_iRKSs' can not be used\n>  when making a shared object; recompile with -fPIC\n> /usr/bin/ld: final link failed: Bad value\n> collect2: error: ld returned 1 exit status\n> ERROR: /home/share/TensorFlow/bazel-0.1.0/src/main/native/BUILD:28:1: Linking of rule\n> '//src/main/native:libunix.so' failed: gcc failed: error executing command /usr/bin/gcc\n> -shared -o bazel-out/local_linux-fastbuild/bin/src/main/native/libunix.so -Wl,-whole-archive ...\n> (remaining 16 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException:\n> Process exited with status 1.\n> ERROR: Sandboxed execution failed: //src/java_tools/buildjar:skylark-deps..\n> ERROR: Sandboxed execution failed: //src/main/protobuf:proto_crosstool_config_srcjar..\n> ERROR: Sandboxed execution failed: //src:android_tools_repository_zip..\n> ERROR: Sandboxed execution failed: //src/main/java:shell-skylark..\n> Target //src:bazel failed to build\n> Use --verbose_failures to see the command lines of failed build steps.\n> INFO: Elapsed time: 6.436s, Critical Path: 2.87s\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/190#issuecomment-156605327\n> .\n", "no, my system is Ubuntu Server 14.04 64bit\n\n```\n[root@/home/share/backup/tensorflow]#uname -a\nLinux optimal4 3.16.0-49-generic #65~14.04.1-Ubuntu SMP Wed Sep 9 10:03:23 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\n```\n", "This issue is tracked by Bazel [here](https://github.com/bazelbuild/bazel/issues/359) where they offer two possible solutions.\n", "Hi @ebrevdo \n\n**Background**:\nI am installing tensorflow from source, since I am using a server where I don't have the root access.\nThe default python version on that server is python 2.6 (but I have my own virtual environment with python 2.7).\nThe default gcc is \"4.4.7\", but we also have a gcc \"4.9.1\" somewhere else.\nI have successfully installed bazel.\n\n**Problems:**\nThe bazel issue you mentioned has already been fixed in later version.\nBut I still have a similar bug and here is the error message \"third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc\".\n\nBesides, I have configured tensorflow to use python2.7 in tensorflow's ./configure\nWhy tf get \"  from argparse import ArgumentParser\nImportError: No module named argparse\"?\nClearly, tf is using the default python 2.6 on our server\n\n```\n$ ../bazel/output/bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer --verbose_failures\nINFO: Waiting for response from Bazel server (pid 6348)...\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nINFO: Found 1 target...\nERROR: /afs/inf.ed.ac.uk/user/s12/s1270921/.cache/bazel/_bazel_s1270921/84c5f700ae424d93a6550e3150808411/external/protobuf/BUILD:71:1: C++ compilation of rule '@protobuf//:protobuf_lite' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command \n  (cd /afs/inf.ed.ac.uk/user/s12/s1270921/.cache/bazel/_bazel_s1270921/84c5f700ae424d93a6550e3150808411/execroot/tensorflow && \\\n  exec env - \\\n    LD_LIBRARY_PATH=/afs/inf.ed.ac.uk/user/s12/s1270921/torch_cudnn/install/lib:/afs/inf.ed.ac.uk/user/s12/s1270921/torch/install/lib:/afs/inf.ed.ac.uk/user/s12/s1270921/torch_cudnn/install/lib:/afs/inf.ed.ac.uk/user/s12/s1270921/torch/install/lib::/afs/inf.ed.ac.uk/user/s12/s1270921/usr/kenlm/lib:/afs/inf.ed.ac.uk/group/project/img2txt/soft/usr/lib:/usr/lib64/atlas:/afs/inf.ed.ac.uk/user/s12/s1270921/usr/kenlm/lib:/afs/inf.ed.ac.uk/group/project/img2txt/soft/usr/lib:/usr/lib64/atlas \\\n    PATH=/afs/inf.ed.ac.uk/user/s12/s1270921/torch_cudnn/install/bin:/afs/inf.ed.ac.uk/user/s12/s1270921/torch/install/bin:/afs/inf.ed.ac.uk/user/s12/s1270921/torch_cudnn/install/bin:/afs/inf.ed.ac.uk/user/s12/s1270921/torch/install/bin:/home/s1270921/bin/:/usr/local/bin/:/opt/matlab-R2015a/bin/:/usr/local/sbin:/usr/bin:/bin:/afs/inf.ed.ac.uk/user/s12/s1270921/bin:/afs/inf.ed.ac.uk/group/project/img2txt/soft/usr/bin:/afs/inf.ed.ac.uk/user/s12/s1270921/irstlm/bin:/home/52nlp/mtworkdir/irstlm/bin://home/52nlp/mtworkdir/irstlm/bin/x86_64-redhat-linux-gnu:/afs/inf.ed.ac.uk/user/s12/s1270921/SRILM/bin:/afs/inf.ed.ac.uk/user/s12/s1270921/SRILM/bin/i686-m64:/afs/inf.ed.ac.uk/user/s12/s1270921/.local/bin:/afs/inf.ed.ac.uk/user/s12/s1270921/bin:/afs/inf.ed.ac.uk/group/project/img2txt/soft/usr/bin:/afs/inf.ed.ac.uk/user/s12/s1270921/irstlm/bin:/home/52nlp/mtworkdir/irstlm/bin://home/52nlp/mtworkdir/irstlm/bin/x86_64-redhat-linux-gnu:/afs/inf.ed.ac.uk/user/s12/s1270921/SRILM/bin:/afs/inf.ed.ac.uk/user/s12/s1270921/SRILM/bin/i686-m64:/afs/inf.ed.ac.uk/user/s12/s1270921/.local/bin \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-std=c++11' -MD -MF bazel-out/host/bin/external/protobuf/_objs/protobuf_lite/external/protobuf/src/google/protobuf/io/zero_copy_stream_impl_lite.d '-frandom-seed=bazel-out/host/bin/external/protobuf/_objs/protobuf_lite/external/protobuf/src/google/protobuf/io/zero_copy_stream_impl_lite.o' -iquote external/protobuf -iquote bazel-out/host/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -isystem external/protobuf/src -isystem bazel-out/host/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -DHAVE_PTHREAD -Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare '-Wno-error=unused-function' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers -c external/protobuf/src/google/protobuf/io/zero_copy_stream_impl_lite.cc -o bazel-out/host/bin/external/protobuf/_objs/protobuf_lite/external/protobuf/src/google/protobuf/io/zero_copy_stream_impl_lite.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nTraceback (most recent call last):\n  File \"third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc\", line 41, in <module>\n    from argparse import ArgumentParser\nImportError: No module named argparse\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nINFO: Elapsed time: 8.452s, Critical Path: 2.73s\n```\n", "same problem here. \n", "I'm in the same situation as well. Did you find a solution for getting it to use Python 2.7 instead of the server default version?\n", "@abewley \n\nNo, I didn't find a solution. In the end, I asked the administrator to upgrade our gcc, python and other related dependencies.\n", "same problem here.\n", "Same here, except with a different error:\n\n```\n.cache/bazel/_bazel_re2300/d217f35631206796f447d50c6f1d6243/external/protobuf/BUILD:331:1: Linking of rule '@protobuf//:protoc' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command \n  (cd /hmt/sirius1/skv0/u/4/r/re2300/.cache/bazel/_bazel_re2300/d217f35631206796f447d50c6f1d6243/execroot/tensorflow && \\\n  exec env - \\\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/host/bin/external/protobuf/protoc bazel-out/host/bin/external/protobuf/_objs/protoc/external/protobuf/src/google/protobuf/compiler/main.o bazel-out/host/bin/external/protobuf/libprotoc_lib.a bazel-out/host/bin/external/protobuf/libprotobuf.a bazel-out/host/bin/external/protobuf/libprotobuf_lite.a -lpthread -lstdc++ -B/usr/bin/ -pie -Wl,-z,relro,-z,now -no-canonical-prefixes -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,-S -Wl,--gc-sections): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n/usr/bin/ld: unrecognized option '-plugin'\n/usr/bin/ld: use the --help option for usage information\n```\n\nEDIT: I fixed this by adding the `fno-use-linker-plugin` flag to the compiler.\n\n```\nindex 20449a1..48a4e60 100755\n--- a/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl\n+++ b/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl\n@@ -309,6 +309,7 @@ def main():\n     # TODO(eliben): rename to a more descriptive name.\n     cpu_compiler_flags.append('-D__GCUDACC_HOST__')\n\n+  cpu_compiler_flags.append('-fno-use-linker-plugin')\n   return subprocess.call([CPU_COMPILER] + cpu_compiler_flags)\n\n if __name__ == '__main__':\n```\n", "I am getting this error\r\n\r\nERROR: C:/users/aditya/appdata/local/temp/_bazel_aditya/p979rsng/external/protobuf/BUILD:113:1: C++ compilation of rule '@protobuf//:protobuf' failed: clang failed: error executing command\r\n  cd C:/users/aditya/appdata/local/temp/_bazel_aditya/p979rsng/execroot/tensorflow\r\n  SET PATH=external/androidndk/ndk/toolchains/llvm/prebuilt/windows-x86_64/bin\r\n  external/androidndk/ndk/toolchains/llvm/prebuilt/windows-x86_64/bin/clang -gcc-toolchain external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/windows-x86_64 -fpic -ffunction-sections -funwind-tables -fstack-protector-strong -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -no-canonical-prefixes -fno-integrated-as -target armv7-none-linux-androideabi -march=armv7-a -mfloat-abi=softfp -mfpu=vfpv3-d16 -mthumb -Os -g -DNDEBUG -MD -MF bazel-out/arm-linux-androideabi-clang3.8-v7a-gnu-libstdcpp-opt/bin/external/protobuf/_objs/protobuf/external/protobuf/src/google/protobuf/util/internal/protostream_objectsource.d -frandom-seed=bazel-out/arm-linux-androideabi-clang3.8-v7a-gnu-libstdcpp-opt/bin/external/protobuf/_objs/protobuf/external/protobuf/src/google/protobuf/util/internal/protostream_objectsource.o -iquote external/protobuf -iquote bazel-out/arm-linux-androideabi-clang3.8-v7a-gnu-libstdcpp-opt/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/arm-linux-androideabi-clang3.8-v7a-gnu-libstdcpp-opt/genfiles/external/bazel_tools -isystem external/protobuf/src -isystem bazel-out/arm-linux-androideabi-clang3.8-v7a-gnu-libstdcpp-opt/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -DHAVE_PTHREAD -Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare -Wno-unused-function --sysroot=external/androidndk/ndk/platforms/android-14/arch-arm -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward -c external/protobuf/src/google/protobuf/util/internal/protostream_objectsource.cc -o bazel-out/arm-linux-androideabi-clang3.8-v7a-gnu-libstdcpp-opt/bin/external/protobuf/_objs/protobuf/external/protobuf/src/google/protobuf/util/internal/protostream_objectsource.o: com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nerror: unable to open output file 'C:\\Temp\\protostream_objectsource-60a699.s': 'No such file or directory'\r\n1 error generated.\r\nTarget //tensorflow/contrib/android:libtensorflow_inference.so failed to build", "@adityasingh1993 It appears you're trying to build Android TF on Windows. In this case to get TF to build you'll need to follow [this workaround](https://github.com/tensorflow/tensorflow/issues/6385#issuecomment-285208600) for installing Linux subsystem for Windows first.\r\n\r\nThough unless you need to make modifications to TF itself, I'd suggest just using the prebuilt AAR (instructions for doing so also found in the above-referenced thread).", "Same error:\r\nERROR: C:/users/zhangyunjin/_bazel_zhangyunjin/fmw3ujjq/external/farmhash_archive/BUILD.bazel:12:1: output 'external/farmhash_archive/_objs/farmhash/farmhash.o' was not created\r\nERROR: C:/users/zhangyunjin/_bazel_zhangyunjin/fmw3ujjq/external/nasm/BUILD.bazel:8:1: output 'external/nasm/_objs/nasm/outcoff.o' was not created"]}, {"number": 189, "title": "No model directory", "body": "I just installed tensorflow in a Virtualenv on Mac OS. There is no models directory. How do I get the models installed?\n", "comments": ["Figured it out! Cloned the repository and copied the model directory to the pip install. \n", "Let us know if you have any other issues.\n"]}, {"number": 188, "title": "Extra exp in softmax formula?", "body": "The Vector Representations of Words tutorial says:\nP(wt|h)=softmax(exp{score(wt,h)}) =exp{score(wt,h)} / \u2211exp{score(w\u2032,h)}.\n\nShouldn't that be ... softmax(score(wt,h)) ... since softmax applies an exp to its arguments?\n\nPage is http://www.tensorflow.org/tutorials/word2vec/index.md\n", "comments": ["Yes, thanks for reporting this. I sent out a fix; it should be updated pretty soon.\n", "Fixed and pushed in https://github.com/tensorflow/tensorflow/commit/a9ca5173b2252b0de5dd754147b275e85298e522 \n"]}, {"number": 187, "title": "Make Python/Numpy include paths configurable", "body": "The Mac OS X Numpy and Python include paths are hard-coded to `/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy/core/include` and `/usr/include/python2.7`. I am on OS X 10.11 and I use the Anaconda Python distribution (installed to `/Users/<user>/anaconda`).\n\nCurrently, Bazel doesn't find my headers:\n\n```\n./tensorflow/python/client/tf_session_helper.h:4:10: fatal error: 'Python.h' file not found\n#include <Python.h>\n         ^\n1 error generated.\n```\n\nIt would be great if Python include paths could be configured in `configure` along with the cuDNN and CUDA paths.\n", "comments": ["I just ran into the same problem. A configuration would be great. Just for now I solved this by editing https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/BUILD#L23 and add your Python / Numpy include paths, e.g. for OS X `\"-I/System/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7\"`\n", "https://github.com/tensorflow/tensorflow/commit/bf6b536bde7d8060c489b51fedb58968b8cbfd7c added the ability to configure the python paths -- as a consequence, we require users to now run ./configure before invoking bazel so they can specify the paths.\n\nLet us know if this works for you!\n"]}, {"number": 186, "title": "No documentation for Saver class", "body": "There are multiple references to **Saver** class in _api_docs/python/train.md_ and _tutorials/mnist/tf/index.md_, yet the documentation for the Saver class does not exist. \n\nIs there any way I can help with the documentation?\n", "comments": ["Hm, our doc links are sort of broken on the tensorflow.org website.  We'll fix it, but in the meantime: \n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/state_ops.md#Saver\n", "new website update fixes this: http://www.tensorflow.org/api_docs/python/state_ops.html#Saver\n"]}, {"number": 185, "title": "Incorrect matrix in beginner tutorial on www.tensorflow.org", "body": "The following image on the [Beginner's tutorial page](http://www.tensorflow.org/tutorials/mnist/beginners/index.md) is wrong:\n![softmax-regression-scalarequation](https://cloud.githubusercontent.com/assets/10536745/11130763/d030c652-89ac-11e5-8d19-186bf9af9b28.png)\n\nHere is a corrected version:\n![softmax-regression-scalarequationcorrected](https://cloud.githubusercontent.com/assets/10536745/11130892/910fafaa-89ad-11e5-88d0-083f3da5a3fe.png)\n", "comments": ["(This has been fixed already in our repo, pending a push to the website).  Thanks for reporting!\n"]}, {"number": 184, "title": "Dropout Loses Shape Inference Information", "body": "I was working on a simple conv net architecture (stack of convs followed by fully connected) and I came across the following issue. If you apply dropout to the output of a convolution and use a placeholder for p_keep on the dropout, you lose all of the shape information except for the last axis'. Below is a code snippet demonstrating this behavior\n\n```\ndef conv(input_, h, w, nfilt_in, nfilt_out, layer_number):\n    with tf.name_scope('conv_%d' % layer_number) as scope:\n        kernel = tf.Variable(tf.truncated_normal([h, w, nfilt_in, nfilt_out],\n                                                 dtype=tf.float32,\n                                                 stddev=1e-1), name='conv_kernel')\n        conv = tf.nn.conv2d(input_, kernel, [1, 1, 1, 1], padding='SAME')\n    return conv\n\n\ndef model(X, p_keep):\n\n    c = conv(X, 5, 5, 1, 8, 0)\n    print \"c_shape before dropout:\", c.get_shape()  # prints TensorShape([Dimension(None), Dimension(28), Dimension(28), Dimension(8)]) as expected\n    c = tf.nn.dropout(c, p_keep)\n    print \"c_shape after dropout:\", c.get_shape()  # prints TensorShape(None) which is strange\n    c2 = conv(c, 5, 5, 8, 16, 1)\n    print \"c2_shape:\", c2.get_shape()  # prints TensorShape([Dimension(None), Dimension(None), Dimension(None), Dimension(16)]) which is strange\n    return c2\n\nX = tf.placeholder(\"float\", [None, 28, 28, 1])\np_keep = tf.placeholder(\"float\")\ny_model = model(X, p_keep)\nsess = tf.Session()\ninit = tf.initialize_all_variables()\nsess.run(init)\n\n```\n\nAs an aside, if you explicitly pass in a value instead of using a placeholder (e.g. replace p_keep=tf.placeholder('float') with p_keep=0.5) this issue does not occur.\n", "comments": ["Thanks for pointing this out! The root cause is related to the p_keep placeholder not having any shape information. By digging into this, we found that passing an unshaped tensor for the optional noise_shape argument also loses shape information. We're preparing a fix.\n", "This should be fixed in the latest commit https://github.com/tensorflow/tensorflow/commit/6b12d081d54b89869e26d9d99828f13de381761e\n", "Awesome, thank you!\n"]}, {"number": 183, "title": "does tensorboard allow for building/editing models?", "body": "I was under the impression that tensorboard was an interactive UI that allows you to build/edit models.  Is this not the case?\n", "comments": ["Nope - at the moment TensorBoard only allows you to visualize and inspect existing models that were created with the Python API. Enabling the TensorBoard graph inspector to modify models would be extremely cool, and we've discussed the idea a bit, but it would also be extremely complicated to get it right, so we're not likely to prioritize it any time soon.\n"]}, {"number": 182, "title": "The icon is assymetric", "body": "The icon for Tensor Flow should be fixed. The horizontal bar of T is longer on one side of the structure but the shadow shows a perfect T. The part of the bar on the right side of the structure should be one more unit long.\n", "comments": ["Exercise for the reader: finding a configuration of light sources that actually produces that shadow :)\nTL;DR: we know, and you can't unsee it once you notice. I myself like to think that we're disciples of the [Wrong Theory](http://www.wired.com/2014/09/wrong-theory/) of design, but the reality is that it is this way because the asymmetric shadow or the symmetric T just don't look as good.\n", "This is exactly what I thought.. :D \n"]}, {"number": 181, "title": "Typo in tf.Graph.name_scope(name) docs", "body": "`c_1 = tf.constant(6.0, name=\"c\")` should be `c_1 = tf.constant(6.0, name=\"c_1\")`\n", "comments": []}, {"number": 180, "title": "'state' is not defined ", "body": "When I was trying to run the samples I got an error while running -> new_value = tf.add(state,one)\n\nError : Traceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'state' is not defined\n\n> > > import tensorflow as tf\n> > > var = tf.Variable(0, name= \"counter\")\n> > > one = tf.constant(1)\n> > > new_value = tf.add(state,one)\n> > > Traceback (most recent call last):\n> > >   File \"<stdin>\", line 1, in <module>\n> > > NameError: name 'state' is not defined\n", "comments": ["Our website has some incorrect code at the moment that we've already fixed in the repo -- if you can look at the corresponding docs in the git repository, it should be fixed.\n", "@shikhar1sharma The fix to this is just more or less renaming the variable 'var' to 'state'. You can find the updated code here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/basic_usage.md#variables-\n", "Thanks a lot. It worked \n"]}, {"number": 179, "title": "Typo in TensorBoard: Visualizing Learning docs", "body": "I believe there is a typo. In the code snippet in that section, currently it states:\n\n`summary_writer = tf.train.SummaryWriter('/tmp/mnist_logs', sess.graph)`\n\nBut `sess.graph` is a `Graph` object and `SummaryWriter` expects a `GraphDef` object instead, so something like:\n\n`summary_writer = tf.train.SummaryWriter('/tmp/mnist_logs', tf.Graph.as_graph_def(sess.graph))`\n", "comments": ["Yup, you are right -- should be sess.graph_def.  We'll try to fix this soon, thanks for the report!\n", "Fixed some time ago.\n"]}, {"number": 178, "title": "Installation error", "body": "While running the `sudo bazel build -c opt //tensorflow/tools/pip_package:build_pip_package`, I get the following error:\n\n```\n./tensorflow/python/client/tf_session_helper.h:6:10: fatal error: 'numpy/arrayobject.h' file not found\n#include \"numpy/arrayobject.h\"\n         ^\n1 error generated.\n\n```\n\nHowever, I installed numpy from the source listed and it worked perfectly. Any suggestions?\n", "comments": ["I forgot to add I am on Mac OS X Mavericks\n", "Edit:\n\nBy updating the file `./tensorflow/python/client/tf_session_helper.h` to include the actual path of the numpy include directory of the computer, I have managed to get a new error,\n\n```\nbazel-out/local_darwin-opt/bin/tensorflow/python/pywrap_tensorflow.cc:3609:10: fatal error: 'numpy/arrayobject.h' file not found\n#include \"numpy/arrayobject.h\"\n         ^\n1 warning and 1 error generated.\n```\n\nI have tried to edit this file the same way, but I cannot because it is just a symlink to a tmp direcotry, so it is regenerated each time.\n\nAny ideas?\n", "Possibly dumb question: is there a need to sudo bazel build?  what if you remove the sudo ?\n", "I removed the directory, reinstalled everything from scratch, then got the 1st error:\n\n```\nINFO: From Compiling tensorflow/python/client/tf_session_helper.cc:\nIn file included from tensorflow/python/client/tf_session_helper.cc:1:\n./tensorflow/python/client/tf_session_helper.h:6:10: fatal error: 'numpy/arrayobject.h' file not found\n#include \"numpy/arrayobject.h\"\n         ^\n1 error generated.\n```\n\nWhat next?\n", "Any chance this is related to https://github.com/tensorflow/tensorflow/issues/187 ? (are you using anaconda python?\n", "No, I installed numpy directly from source, and it is located in the right directory. The directory tree to the file the installer can't find is as follows:\n\n```\n/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/core/include/numpy/arrayobject.h\n\n```\n", "Hm, I'm not entirely sure what the answer is here.  https://github.com/BVLC/caffe/issues/1284 seems to suggest a similar problem, but the solution to 'reinstall numpy' doesn't really apply here.  Hopefully someone from the community can chime in and help.\n", "We look for the numpy headers here:\n\n/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy/core/include\n\nwhich is different from your setup.  Try changing [this line](https://github.com/tensorflow/tensorflow/blob/72a5a60dd4664a7caa4611344364ac7851464a60/tensorflow/python/BUILD#L23) accordingly.\n", "Works great for first step. Waiting for next two to execute. Thanks ebrevdo!\n", "Installation completely worked. Thanks!\n", "hey @ebrevdo, getting this issue from the `master` branch right now\r\n\r\nis there an analogous fix for it there or should I compile a different release?", "having the following issue on release branches 1.8 - 1.10\r\n```\r\n/tensorflow/third_party/python_runtime/BUILD:5:1: no such package '@local_config_python//': \r\n```\r\n\r\nand having `numpy` import errors on 1.11 and 1.12 and master"]}, {"number": 177, "title": "ImportError: /lib64/libc.so.6: version `GLIBC_2.17'", "body": "After the installation of tensorflow(cpu version) via pip command, I just used **import tensorflow as tf** which leads to the following error message\n\n> > > import tensorflow as tf\n> > > Traceback (most recent call last):\n> > >   File \"<stdin>\", line 1, in <module>\n> > >   File \"/home/hpc/pr63so/ga93yih2/anaconda/lib/python2.7/site-packages/tensorflow/**init**.py\", line 4, in <module>\n> > >     from tensorflow.python import *\n> > >   File \"/home/hpc/pr63so/ga93yih2/anaconda/lib/python2.7/site-packages/tensorflow/python/**init**.py\", line 22, in <module>\n> > >     from tensorflow.python.client.client_lib import *\n> > >   File \"/home/hpc/pr63so/ga93yih2/anaconda/lib/python2.7/site-packages/tensorflow/python/client/client_lib.py\", line 35, in <module>\n> > >     from tensorflow.python.client.session import InteractiveSession\n> > >   File \"/home/hpc/pr63so/ga93yih2/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 11, in <module>\n> > >     from tensorflow.python import pywrap_tensorflow as tf_session\n> > >   File \"/home/hpc/pr63so/ga93yih2/anaconda/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n> > >     _pywrap_tensorflow = swig_import_helper()\n> > >   File \"/home/hpc/pr63so/ga93yih2/anaconda/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n> > >     _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\n> > > ImportError: /lib64/libc.so.6: version `GLIBC_2.17' not found (required by /home/hpc/pr63so/ga93yih2/anaconda/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so)\n", "comments": ["see #53 \n", "De-duping with 53\n", "`conda install tensorflow` works ", "I install  with offline tensorflow conda package,  it still happens. ", "works for me!Thanks", "Hi,\r\n\r\nI recently got this problem by running tensorflow in a spark-submit. I downgraded **_protobuf_** in order to fix it.\r\n\r\nThis is the error I got \r\n\r\n```python\r\n\r\n\r\nFile \"/app/hadoop/yarn/local4/usercache/myapp/appcache/application_1645269118360_0591/container_e54_1645269118360_0591_01_000001/venv-myapp-poc/lib/python3.7/site-packages/tensorflow/__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"/app/hadoop/yarn/local4/usercache/myapp/appcache/application_1645269118360_0591/container_e54_1645269118360_0591_01_000001/venv-myapp-poc/lib/python3.7/site-packages/tensorflow/python/__init__.py\", line 53, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"/app/hadoop/yarn/local4/usercache/myapp/appcache/application_1645269118360_0591/container_e54_1645269118360_0591_01_000001/venv-myapp-poc/lib/python3.7/site-packages/tensorflow/core/framework/graph_pb2.py\", line 7, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"/app/hadoop/yarn/local4/usercache/myapp/appcache/application_1645269118360_0591/container_e54_1645269118360_0591_01_000001/venv-myapp-poc/lib/python3.7/site-packages/google/protobuf/descriptor.py\", line 47, in <module>\r\n    from google.protobuf.pyext import _message\r\nImportError: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /app/hadoop/yarn/local4/usercache/myapp/appcache/application_1645269118360_0591/container_e54_1645269118360_0591_01_000001/venv-myapp-poc/lib/python3.7/site-packages/google/protobuf/pyext/_message.cpython-37m-x86_64-linux-gnu.so)\r\n\r\n\r\n```\r\n"]}, {"number": 176, "title": "Split a tensor with Tensor 0-D int32 as num_split argument", "body": "Hi,\n\nAccording to the doc, th.split takes a 0-D int32 Tensor as num_split argument but executing :\n\nx = tf.placeholder(tf.float32, [5, None])\nnumsplit = tf.shape(x)[1]\nsplits = tf.split(1, numsplit, x)\n\nreturns :\n\n\"Expected int for argument 'num_split' not  < tensorflow.python.framework.ops.Tensor object at 0x7fcad834f810>\"\n\nIf the dimension is defined split also returns the same error :\n\nx = tf.placeholder(tf.float32, [5, 30])\nnumsplit = tf.shape(x)[1]\nsplits = tf.split(1, numsplit, x)\n\nSame thing with a constant as argument:\n\nx = tf.placeholder(tf.float32, [5, 30])\nnumsplit = tf.constant(30)\nsplits = tf.split(1, numsplit, x)\n\nAny ideas ?\n", "comments": ["Thinking about this twice, I can't imagine how split could populate the resulting list without having a \"hard\" number.\n", "Thanks, that's a documentation bug.  I'll fix it now.  As you point out, the number has to be known at graph construction time since it determines the topology.\n", "Fixed, will be pushed out in the next repo sync.  Thanks again for the catch!\n"]}]