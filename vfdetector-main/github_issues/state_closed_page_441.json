[{"number": 40628, "title": "Please enable ImageDataGenerator for tpu accelerator ", "body": "please enable ImageDataGenerator for tpu because create TFRecordDataset is the complex and worst method I ever have seen my entire deep-learning experience . or make a function that converts ImageDataGenerator to TFRecordDataset. \r\n\r\nIf converts ImageDataGenerator to TFRecordDataset gives me a reference.\r\n\r\nThank you", "comments": ["@imrankhan441 \r\n\r\nDo you have any use case that requires the feature you are interested in? Please feel free to submit a PR if you have use cases that supports that feature.Thanks!\r\n", "Hi @imrankhan441, if you haven't seen it already, it might be worth taking a look at issue #34346 which has some more information around the work involved in making ImageDataGenerator work with TPU.", "@ravikyram \r\n**Please Run This Code you will understand what I mean.**\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\nimport os\r\nimport zipfile\r\n\r\n\r\n\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver()\r\ntf.config.experimental_connect_to_cluster(resolver)\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\ntpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n\r\n\r\n\r\n\r\ndef cnn_model(Classes, input_shape = (128, 128, 3)):\r\n\r\n    model = tf.keras.models.Sequential()\r\n    model.add(tf.keras.layers.Conv2D(64, (5, 5), padding='same', input_shape = input_shape))\r\n    model.add(tf.keras.layers.MaxPooling2D(strides=(2,2)))\r\n    model.add(tf.keras.layers.Dropout(0.25))\r\n\r\n    model.add(tf.keras.layers.Conv2D(128, (5, 5), padding='same'))\r\n    model.add(tf.keras.layers.MaxPooling2D())\r\n    model.add(tf.keras.layers.Dropout(0.25))\r\n\r\n    model.add(tf.keras.layers.Conv2D(256, (5, 5), padding='same'))\r\n    model.add(tf.keras.layers.MaxPooling2D(strides=(2,2)))\r\n    model.add(tf.keras.layers.Dropout(0.25))\r\n\r\n    model.add(tf.keras.layers.Flatten())\r\n    model.add(tf.keras.layers.Dense(Classes, activation=\"softmax\"))\r\n\r\n\r\n    return model\r\n\r\n\r\n\r\n!wget --no-check-certificate \\\r\n    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\r\n    -O ./cats_and_dogs_filtered.zip\r\n\r\n\r\n\r\ndataset_path = \"./cats_and_dogs_filtered.zip\"\r\nzip_object = zipfile.ZipFile(file=dataset_path, mode=\"r\")\r\nzip_object.extractall(\"./\")\r\nzip_object.close()\r\n\r\ndataset_path_new = \"./cats_and_dogs_filtered/\"\r\ntrain_dir = os.path.join(dataset_path_new, \"train\")\r\nvalidation_dir = os.path.join(dataset_path_new, \"validation\")\r\n\r\nwith tpu_strategy.scope():\r\n    model = cnn_model(1)\r\n    model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\r\n\r\nbatch_size = 8 * 128 \r\n\r\ndata_gen_train = ImageDataGenerator(rescale=1/255.)\r\ndata_gen_valid = ImageDataGenerator(rescale=1/255.)\r\n\r\ntrain_generator = data_gen_train.flow_from_directory(train_dir, target_size=(128,128), batch_size=batch_size, class_mode=\"binary\")\r\nvalid_generator = data_gen_valid.flow_from_directory(validation_dir, target_size=(128,128), batch_size=batch_size, class_mode=\"binary\")\r\n\r\nmodel.fit_generator(train_generator, epochs=5, validation_data=valid_generator)\r\n\r\n```", "@imrankhan441 \r\n\r\nI have tried in colab with TF version 2.2.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/b7b0de3f9bf059d18f19a75f9b5b48b8/untitled60.ipynb#scrollTo=3IsxUejJUJ7v).Are you seeing the same behavior?.Thanks!", "@ravikyram \r\n\r\n**Yes exact same error Please fix this. This error didn't show in CPU and GPU. It shows tpu only.**\r\n\r\n\r\n```\r\nNotFoundError: {{function_node __inference_train_function_5687}} No registered 'PyFunc' OpKernel for 'CPU' devices compatible with node {{node PyFunc}}\r\n\t.  Registered:  <no registered kernels>\r\n\r\n\t [[PyFunc]]\r\n\t [[MultiDeviceIteratorGetNextFromShard]]\r\n\t [[RemoteCall]]\r\n\t [[IteratorGetNextAsOptional]]\r\n\r\n```", "I have the same issue as @imrankhan441 when i use ImageDataGenerator with TPU. please we need this fixed", "I use flow_from_dataframe, and I didn't find any other way to debug my code", "I'm closing this issue now because it is a duplicate of #34346. It is best to have one thread for tracking a particular feature. Any updates/progress will be posted there."]}, {"number": 40627, "title": "I want to implement the beam search", "body": "i am using this code : https://www.tensorflow.org/tutorials/text/nmt_with_attention\r\n\r\nI want to implement the beam search to this code in order to generate more than one sentence, but i don't know how can i do it. Can you please help me ?\r\n", "comments": ["@ousshml,\r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a TensorFlow bug or feature request. There is also a larger community that reads questions there. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40626, "title": "Fix error caused by incorrect rank assumption in tf.image.flip_left_right", "body": "This PR tries to address the issue raised in #40580 where an error\r\nwas thrown out when tf.image.flip_left_right process a tensor of unknown\r\nrank.\r\n\r\nThe reason was that tf.image.flip_left_right assumes rank == 3 in case\r\nof unknown rank.\r\n\r\nThis PR adjust to use array_ops.rank(image) to obtain the true rank\r\nwhen unknown rank is present.\r\n\r\nThis PR fixes #40580.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @mihaimaruseac for the review. The PR has been updated and ` next(iter(dataset))` has been converted to `get_single_element(dataset.take(1))`. This should resolve the graph mode failure I think. Please take a look and let me know if the issue still exists.", "Tests are still failing :(\r\n\r\nThis is `tensorflow/python:image_ops_test:FlipTransposeRotateTest.testFlipImageUnknownShape`\r\n```\r\nTraceback (most recent call last):\r\n  File \".../tensorflow/python/client/session.py\", line 1365, in _do_call\r\n    return fn(*args)\r\n  File \".../tensorflow/python/client/session.py\", line 1350, in _run_fn\r\n    target_list, run_metadata)\r\n  File \".../tensorflow/python/client/session.py\", line 1443, in _call_tf_sessionrun\r\n    run_metadata)\r\ngoogle3.third_party.tensorflow.python.framework.errors_impl.InvalidArgumentError: TypeError: `generator` yielded an element that could not be converted to the expected type. The expected type was int32, but the yielded element was Tensor(\"Const:0\", shape=(1, 2, 2, 3), dtype=int32).\r\nTraceback (most recent call last):\r\n\r\n  File \".../tensorflow/python/data/ops/dataset_ops.py\", line 843, in generator_py_func\r\n    ret, dtype=dtype.as_numpy_dtype))\r\n\r\n  File \".../tensorflow/python/ops/script_ops.py\", line 205, in _convert\r\n    result = np.asarray(value, dtype=dtype, order=\"C\")\r\n\r\n  File \".../numpy/core/numeric.py\", line 538, in asarray\r\n    return array(a, dtype, copy=False, order=order)\r\n\r\nTypeError: __array__() takes 1 positional argument but 2 were given\r\n```", "Thanks @mihaimaruseac for the help. The failure very much looks like related to shape is not passed in generator. I have added additional parameter to `from_generator`. Think this might solve the internal test issue. \r\n\r\nI have updated the PR. Please take a look and see if the issue still exist.", "Hmm, I am seeing the same error", "Let's try to resync copybara import to make sure all changes are in.", "Yes, same error with the latest commit included :(", "@yongtang, @mihaimaruseac Any update on this PR? Please. Thanks!\r\n", "@gbaned I will take a look, sorry for the late reply.", "@yongtang  Any update on this PR? Please. Thanks!\r\n", "@mihaimaruseac The PR has been updated by switch from usage of tf.constant to np.array inside the generator. This will avoid calling extra layer conversion between tf and numpy. I think this might resolve the issue. Can you give it a try? Sorry for the extra inconvenience."]}, {"number": 40625, "title": "update arg doc for sigmoid_cross_entropy", "body": "fixes #40593", "comments": []}, {"number": 40624, "title": "TF32 Support for NVIDIA A100", "body": "This is an update of https://github.com/tensorflow/tensorflow/pull/39764\r\n\r\nAttn: @reedwm ", "comments": ["@reedwm, PTAL", "Seems auto-merge is not happening but the changes are now committed so we can close this. Thank you for the PR."]}, {"number": 40623, "title": "ALBERT from TF Hub doesn't work with GradientTape", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 9.12\r\n- TensorFlow installed from (source or binary): Automatic installation for GCP deep learning vm\r\n- TensorFlow version (use command below): 2.1.0\r\n- Tensorflow Hub version: 0.8.0\r\n- Python version: 3.7.6\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Nvidia V100\r\n\r\n**Describe the current behavior**\r\nI get an error when trying to run ALBERT from TF-Hub inside a `tf.GradientTape()` context. The error occurs at the \"forward pass\" before calling `tape.gradient()` . The error does not occur when the model call is placed outside of the `tf.GradientTape()` context. \r\n\r\n**Describe the expected behavior**\r\nNo such error should occur and the model should run and gradients should be calculated properly. \r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\nfrom tensorflow import keras\r\nimport numpy as np\r\n\r\ndef get_model():\r\n    global max_seq_length\r\n    global batch_size\r\n    input_word_ids = keras.layers.Input(batch_shape=(batch_size, max_seq_length, ), \r\n                                           dtype=tf.int32,\r\n                                           name=\"input_word_ids\")\r\n    input_mask = keras.layers.Input(batch_shape=(batch_size, max_seq_length, ), \r\n                                       dtype=tf.int32,\r\n                                       name=\"input_mask\")\r\n    segment_ids = keras.layers.Input(batch_shape=(batch_size, max_seq_length, ), \r\n                                        dtype=tf.int32,\r\n                                        name=\"segment_ids\")\r\n    albert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/albert_en_base/1\",\r\n                                  trainable=True,\r\n                                  name='albert_layer')\r\n    pooled_output, sequence_output = albert_layer([input_word_ids, input_mask, segment_ids])\r\n    output = keras.layers.Dense(2)(sequence_output)\r\n\r\n    model = keras.Model(inputs=(input_word_ids, input_mask, segment_ids),\r\n                        outputs=output)\r\n    print(model.summary())\r\n    return model\r\n\r\n\r\nbatch_size = 4\r\nmax_seq_length = 16\r\nmodel = get_model()\r\n\r\ninput_ids = 5 * np.ones((4, 16), dtype=np.int32)\r\ninput_mask = np.ones((4, 16), dtype=np.int32)\r\nsegment_ids = np.zeros((4, 16), dtype=np.int32)\r\n\r\nwith tf.GradientTape(persistent=True) as tape:\r\n    logits = model({\r\n                  'input_word_ids' : input_ids,\r\n                  'input_mask' : input_mask,\r\n                  'segment_ids' : segment_ids\r\n    })\r\n    print(logits)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"albert_gradient_tape_test.py\", line 42, in <module>\r\n    'segment_ids' : segment_ids\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 822, in __call__\r\n    outputs = self.call(cast_inputs, *args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\", line 717, in call\r\n    convert_kwargs_to_constants=base_layer_utils.call_context().saving)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\", line 891, in _run_internal_graph\r\n    output_tensors = layer(computed_tensors, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 822, in __call__\r\n    outputs = self.call(cast_inputs, *args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py\", line 218, in call\r\n    lambda: f(training=False))\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/smart_cond.py\", line 56, in smart_cond\r\n    return false_fn()\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py\", line 218, in <lambda>\r\n    lambda: f(training=False))\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/saved_model/load.py\", line 438, in _call_attribute\r\n    return instance.__call__(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\nFile \"/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 606, in _call\r\n    results = self._stateful_fn(*args, **kwds)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2362, in __call__\r\n    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/saved_model/function_deserialization.py\", line 241, in restored_function_body\r\n    return _call_concrete_function(function, inputs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/saved_model/function_deserialization.py\", line 72, in _call_concrete_function\r\n    result = function._call_flat(tensor_inputs, function._captured_inputs)  # pylint: disable=protected-access\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/saved_model/load.py\", line 99, in _call_flat\r\n    cancellation_manager)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1697, in _call_flat\r\n    forward_function, args_with_tangents = forward_backward.forward()\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1423, in forward\r\n    self._inference_args, self._input_tangents)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1185, in forward\r\n    self._forward_and_backward_functions(inference_args, input_tangents))\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1379, in _forward_and_backward_functions\r\n    outputs, inference_args, input_tangents)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 882, in _build_functions_for_outputs\r\n    output)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/default_gradient.py\", line 45, in shape_and_dtype\r\n    \"of a variable without handle data:\\n%s\" % str(t))\r\nValueError: Internal error: Tried to take gradients (or similar) of a variable without handle data:\r\nTensor(\"StatefulPartitionedCall:949\", shape=(), dtype=resource)\r\n```\r\n", "comments": ["@Enumaris \r\nPlease post TF_Hub related issues in the [Hub Repo](https://github.com/tensorflow/hub/issues). Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40623\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40623\">No</a>\n"]}, {"number": 40622, "title": "tf.tpu.experimental.initialize_tpu_system fails to work on nightly builds", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colaboratory (Linux)\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tf-nightly_v2.3.0.dev20200619\r\n- Python version: v3.6.9\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n- TPU: Google Colab runtime with TPU accelerator\r\n\r\n**Describe the current behavior**\r\n`tf.tpu.experimental.initialize_tpu_system(tpu_cluster_resolver)` raises an unexpected error. The stack trace containing this error is provided underneath. \r\n\r\nThe sample notebook that was being used to train an `EfficientNetB0` model with `TPUStrategy` containing the error message is provided [here](https://colab.research.google.com/drive/1twKOkGxWO8NpYIE_SZ2qroVNEyGCxQrS?usp=sharing). \r\n\r\n**Describe the expected behavior**\r\nA ResNet50 model (as EfficientNetB0 is only present in TF-nightly) with similar code is able to run successfully with TPUStrategy and there are no such errors reported while calling `tf.tpu.experimental.initialize_tpu_system`.  A notebook with the corresponding training code for TFv2.2 can be found [here](https://colab.research.google.com/drive/1BPsM3Gh1d6-nXY0urp0AFuSm2WKlYorv?usp=sharing).\r\n\r\n**Standalone code to reproduce the issue**\r\nJust calling `tf.tpu.experimental.initialize_tpu_system` using the standard mechanism on a Colab runtime with TPU should suffice.\r\n\r\n```python\r\ntpu_url = 'grpc://' + os.environ['COLAB_TPU_ADDR']\r\ntpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_url)\r\n\r\ntf.config.experimental_connect_to_cluster(tpu_cluster_resolver)\r\ntf.tpu.experimental.initialize_tpu_system(tpu_cluster_resolver)\r\n\r\nstrategy = tf.distribute.experimental.TPUStrategy(tpu_cluster_resolver)\r\n```\r\n\r\n**Other info / logs** \r\n\r\n```python\r\nRunning on TPU  ['10.57.138.26:8470']\r\nINFO:tensorflow:Initializing the TPU system: grpc://10.57.138.26:8470\r\n\r\nINFO:tensorflow:Initializing the TPU system: grpc://10.57.138.26:8470\r\n\r\nINFO:tensorflow:Clearing out eager caches\r\n\r\nINFO:tensorflow:Clearing out eager caches\r\n\r\n---------------------------------------------------------------------------\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n\r\n<ipython-input-4-a42f01f7e70e> in <module>()\r\n      6 \r\n      7 tf.config.experimental_connect_to_cluster(tpu)\r\n----> 8 tf.tpu.experimental.initialize_tpu_system(tpu)\r\n      9 tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\r\n\r\n3 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/tpu/tpu_strategy_util.py in initialize_tpu_system(cluster_resolver)\r\n    101     context.context()._clear_caches()  # pylint: disable=protected-access\r\n    102 \r\n--> 103     serialized_topology = output.numpy()\r\n    104 \r\n    105     # TODO(b/134094971): Remove this when lazy tensor copy in multi-device\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in numpy(self)\r\n   1061     \"\"\"\r\n   1062     # TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\r\n-> 1063     maybe_arr = self._numpy()  # pylint: disable=protected-access\r\n   1064     return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr\r\n   1065 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _numpy(self)\r\n   1029       return self._numpy_internal()\r\n   1030     except core._NotOkStatusException as e:  # pylint: disable=protected-access\r\n-> 1031       six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access\r\n   1032 \r\n   1033   @property\r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: NodeDef expected inputs 'string' do not match 0 inputs specified; Op<name=_Send; signature=tensor:T -> ; attr=T:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>; NodeDef: {{node _Send}}\r\n```\r\n\r\nTypically, this bug is prevalent only on nightly builds (master branch) and not on TF v2.2 release.\r\n\r\n/cc: @tanzhenyu ", "comments": ["I have seen this issue on TF 2.2.0. I think there is something lingering on the TPU cores that `initialize_tpu_system` is unable to clear out.", "I have tried in colab with TF version 2.2 and i am not seeing any issue.However i am seeing this issue with nightly versions(`2.3.0-dev20200621').Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/f1c56122bf30a1bdadb7c72e7700e632/untitled54.ipynb).Thanks!", "Yes @ravikyram, thanks for acknowledging.", "It is due to a version mismatch between your client and TPU worker. Did you use nightly on both instances?", "Hi @rxsang, not sure if it's an error regarding the TPU version mismatch between the worker and the client. I had encountered the error on Colab. (Colab TPU runtimes do not provide for choice of versions for the TPU workers) I might need to try the same on Compute Engine Cloud TPUs for checking operability amongst different TPU worker versions.\r\n\r\nAFAIK client and worker versions are interoperable as previously I had used TPU workers running TFv1.15 with TFv2.x, so I think.\r\n\r\nThanks.", "@swghosh Yeah, Colab always defaults to the latest stable version of TPUs because of technical limitations. The current version of Colab TPU that runs is version 2.2, so Colab TPUs are only supported with TensorFlow v2.2 by default.\r\n\r\nIn general, we only support TPUs when the same version is used on both the user TF and on the TPUs. In practice, there is some scope for different versions working together if there are no changes in terms of protocol or op definitions (e.g. no new ops, ops didn't change signature, etc...) but it is not supported.", "@frankchn Thanks for the clarification. Much help. ", "> @swghosh Yeah, Colab always defaults to the latest stable version of TPUs because of technical limitations. The current version of Colab TPU that runs is version 2.2, so Colab TPUs are only supported with TensorFlow v2.2 by default.\r\n> \r\n> In general, we only support TPUs when the same version is used on both the user TF and on the TPUs. In practice, there is some scope for different versions working together if there are no changes in terms of protocol or op definitions (e.g. no new ops, ops didn't change signature, etc...) but it is not supported.\r\n\r\n@frankchn \r\nI found that tf 2.3 has been released, is there an update plan for the tf version on colab?", "The Colab roll out process lags the TensorFlow one by a few days, but it is being rolled out right now and TF 2.3 will be usable by the end of this week. When that happens, TF 2.2 will not be usable any more because of the same incompatibility between versions.", "Ok, thanks frank.", "still got following error with tf 2.3.0  tpu on colab.\r\n```\r\ntensorflow.python.framework.errors_impl.InternalError: RET_CHECK failure (learning/brain/google/xla/distributed_tpu_rewrite_pass.cc:1413) arg_shape.handle_type != DT_INVALID \r\n2020-08-06 01:47:30.383712: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 36311, Output num: 0\r\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:\r\n:{\"created\":\"@1596678450.383612323\",\"description\":\"Error received from peer ipv4:10.81.58.138:8470\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Unable to find the relevant tensor remote_handle: Op ID: 36311, Output num: 0\",\"grpc_status\":3}\r\n```\r\nPlease, find the [gist ](https://colab.research.google.com/gist/feiwofeifeixiaowo/a7ef19b4cb13517a18eec4c6786ac6e0/tpu-train-delf.ipynb#scrollTo=jmU5sTUR9Gxw)here.Thanks!", "From the trace, it seems you enabled tf summary inside TPU computation. Can you try calling `tf.config.set_soft_device_placement(True)` at the beginning, so outside compilation can be enabled?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40622\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40622\">No</a>\n", "Now, it seems to initialize well with 2.3.0 and nightly with Colab. However, whenever I run `gcloud ai-platform` training scripts, it always produces the same error whatever version of tf I use.. It is really frustrating. If it is all about the version mismatch, I can't find any single clue for solution anywhere.", "I'm not familiar with the `gcloud ai-platform` scripts, which TF and TPU version do that script use? @gagika Gigik, anyone from Cloud TPU team could help with that?", "For choosing matching Tensorflow version on TPU in colab and kaggle you can use the code below (see [here](https://stackoverflow.com/questions/63180202/invalidargumenterror-while-initializing-ttpu/63271669#63271669))\r\n\r\n```\r\n!pip install cloud-tpu-client\r\n\r\nimport tensorflow as tf\r\nfrom cloud_tpu_client import Client\r\nprint(tf.__version__)\r\n\r\nClient().configure_tpu_version(tf.__version__, restart_type='ifNeeded')\r\n```\r\n\r\nFor using training on TPUs on ai platform you can follow the steps [here](https://cloud.google.com/ai-platform/training/docs/using-tpus). At the moment AI supports TF versions `1.13, 1.14, 1.15, 2.1, and 2.2 `.", "> @swghosh Yeah, Colab always defaults to the latest stable version of TPUs because of technical limitations. The current version of Colab TPU that runs is version 2.2, so Colab TPUs are only supported with TensorFlow v2.2 by default.\r\n> \r\n> In general, we only support TPUs when the same version is used on both the user TF and on the TPUs. In practice, there is some scope for different versions working together if there are no changes in terms of protocol or op definitions (e.g. no new ops, ops didn't change signature, etc...) but it is not supported.\r\n\r\nYou don't really understand how frustrating it's becoming using some Tensorflow packages. I practice on Colab and do more work in Kaggle. In Kaggle the line `from tensorflow.keras.preprocessing import image_dataset_from_directory` only works in tf==2.3, while it works in default 2.2 in Colab.\r\n\r\nAlso in Kaggle initiating TPU fails in tf 2.3, but works in 2.2.... It's just a bunch of annoying upgrades and downgrades and a real mess to fully enjoy tensorflow packages.", "> > @swghosh Yeah, Colab always defaults to the latest stable version of TPUs because of technical limitations. The current version of Colab TPU that runs is version 2.2, so Colab TPUs are only supported with TensorFlow v2.2 by default.\r\n> > In general, we only support TPUs when the same version is used on both the user TF and on the TPUs. In practice, there is some scope for different versions working together if there are no changes in terms of protocol or op definitions (e.g. no new ops, ops didn't change signature, etc...) but it is not supported.\r\n> \r\n> You don't really understand how frustrating it's becoming using some Tensorflow packages. I practice on Colab and do more work in Kaggle. In Kaggle the line `from tensorflow.keras.preprocessing import image_dataset_from_directory` only works in tf==2.3, while it works in default 2.2 in Colab.\r\n> \r\n> Also in Kaggle initiating TPU fails in tf 2.3, but works in 2.2.... It's just a bunch of annoying upgrades and downgrades and a real mess to fully enjoy tensorflow packages.\r\n\r\nRight now Kaggle is using tf 2.3 and Colab tf 2.4, so the issue should be resolved on both.\r\nPlease let me know if it still there.", "> Now, it seems to initialize well with 2.3.0 and nightly with Colab. However, whenever I run `gcloud ai-platform` training scripts, it always produces the same error whatever version of tf I use.. It is really frustrating. If it is all about the version mismatch, I can't find any single clue for solution anywhere.\r\n\r\n`gcloud ai-platform` now [supports](https://cloud.google.com/ai-platform/training/docs/using-tpus#tpu-runtime-versions) TF 2.3, so the issue should be resolved.\r\n", "> > > @swghosh Yeah, Colab always defaults to the latest stable version of TPUs because of technical limitations. The current version of Colab TPU that runs is version 2.2, so Colab TPUs are only supported with TensorFlow v2.2 by default.\r\n> > > In general, we only support TPUs when the same version is used on both the user TF and on the TPUs. In practice, there is some scope for different versions working together if there are no changes in terms of protocol or op definitions (e.g. no new ops, ops didn't change signature, etc...) but it is not supported.\r\n> > \r\n> > \r\n> > You don't really understand how frustrating it's becoming using some Tensorflow packages. I practice on Colab and do more work in Kaggle. In Kaggle the line `from tensorflow.keras.preprocessing import image_dataset_from_directory` only works in tf==2.3, while it works in default 2.2 in Colab.\r\n> > Also in Kaggle initiating TPU fails in tf 2.3, but works in 2.2.... It's just a bunch of annoying upgrades and downgrades and a real mess to fully enjoy tensorflow packages.\r\n> \r\n> Right now Kaggle is using tf 2.3 and Colab tf 2.4, so the issue should be resolved on both.\r\n> Please let me know if it still there.\r\n\r\nSame trouble on Kaggle,\r\n\r\nCan't install or upgrade to TensorFlow 2.4\r\n\r\nSee details\r\n```\r\nInstalling tensorflow==2.4...\r\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\nosmnx 0.15.1 requires geopandas>=0.7, but you have geopandas 0.6.3 which is incompatible.\r\nkubernetes 10.1.0 requires pyyaml~=3.12, but you have pyyaml 5.3.1 which is incompatible.\r\nkmeans-smote 0.1.2 requires imbalanced-learn<0.5,>=0.4.0, but you have imbalanced-learn 0.7.0 which is incompatible.\r\nkmeans-smote 0.1.2 requires numpy<1.16,>=1.13, but you have numpy 1.19.5 which is incompatible.\r\nkmeans-smote 0.1.2 requires scikit-learn<0.21,>=0.19.0, but you have scikit-learn 0.23.1 which is incompatible.\r\njupyterlab-git 0.10.0 requires nbdime<2.0.0,>=1.1.0, but you have nbdime 2.0.0 which is incompatible.\r\nhypertools 0.6.2 requires scikit-learn<0.22,>=0.19.1, but you have scikit-learn 0.23.1 which is incompatible.\r\ngoogle-cloud-pubsub 1.4.3 requires google-api-core[grpc]<1.17.0,>=1.14.0, but you have google-api-core 1.17.0 which is incompatible.\r\nbokeh 2.1.1 requires tornado>=5.1, but you have tornado 5.0.2 which is incompatible.\r\nastroid 2.3.3 requires wrapt==1.11.*, but you have wrapt 1.12.1 which is incompatible.\r\n```\r\n\r\n", "The entire version mismatches surrounding Tensorflow is absolutely\nfrustrating! Can't te very many Google Engineers synchronise versions and\nmodules all at once?\n\n\n\n[image: Mailtrack]\n<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>\nSender\nnotified by\nMailtrack\n<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>\n01/22/21,\n01:01:55 PM\n\nOn Fri, Jan 22, 2021 at 2:19 AM Gagik Amirkhanyan <notifications@github.com>\nwrote:\n\n> Now, it seems to initialize well with 2.3.0 and nightly with Colab.\n> However, whenever I run gcloud ai-platform training scripts, it always\n> produces the same error whatever version of tf I use.. It is really\n> frustrating. If it is all about the version mismatch, I can't find any\n> single clue for solution anywhere.\n>\n> gcloud ai-platform now supports\n> <https://cloud.google.com/ai-platform/training/docs/using-tpus#tpu-runtime-versions>\n> TF 2.3, so the issue should be resolved.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/40622#issuecomment-765048728>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AJLJAUGQJWUERTB63FAC5D3S3DHC7ANCNFSM4OC66Z4A>\n> .\n>\n", "> @swghoshThe current version of Colab TPU that runs is version 2.2, so Colab TPUs are only supported with TensorFlow v2.2 by default.\r\n\r\nThis is not true. I've installed 2.2 via `!pip3 install tensorflow==2.2.0` on my Colab notebook, and restarted the runtime, and I still face the problem, it's super frustrating. \r\n\r\n*EDIT:* One has to install it via the code below, then it works.\r\n\r\n```\r\n!pip install cloud-tpu-client\r\n\r\nimport tensorflow as tf\r\nfrom cloud_tpu_client import Client\r\nprint(tf.__version__)\r\n\r\nClient().configure_tpu_version(tf.__version__, restart_type='ifNeeded')\r\n```\r\n", "> not\r\n\r\nNot working"]}, {"number": 40621, "title": "add script for updating keras application's efficientnet weights from ckpt", "body": "Adding a utility script for keras applications to update pretrained weight files based on checkpoints as those produced by https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet .  ", "comments": ["@yixingfu Can you please address Ubuntu Sanity errors? Thanks!", "@yixingfu Still, Ubuntu Sanity errors appearing, can you please fix those?. Thanks!", "@yixingfu could you please mention how the script should be used? What is its location? I am currently on TensorFlow 2.3.0 and I did not find it. I tried with `tf-nightly` too but still no luck. Could you shed some light? ", "> @yixingfu could you please mention how the script should be used? What is its location? I am currently on TensorFlow 2.3.0 and I did not find it. I tried with `tf-nightly` too but still no luck. Could you shed some light?\r\n\r\nThanks for asking. The script is in tensorflow/python/keras/applications/efficientnet_weight_update_util.py. You should find or separately download the script and call it. I assume you are trying to find it by some sort of import, which would not work as the script is not added as an api.", "@yixingfu I also did an `ls` in the `tensorflow/python/keras/applications/` directory but could not find it. I ended up downloading it from your [ref repository](https://github.com/yixingfu/tensorflow/tree/updateweights). ", "> @yixingfu I also did an `ls` in the `tensorflow/python/keras/applications/` directory but could not find it. I ended up downloading it from your [ref repository](https://github.com/yixingfu/tensorflow/tree/updateweights).\r\n\r\noh I see, it is not in the installed package. You can still find it in  the [master branch](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/applications/efficientnet_weight_update_util.py). But I'm glad that you find it in my repo!"]}, {"number": 40620, "title": "random in tf.data.Dataset.map is not random if not coming from tensorflow", "body": "**System information**\r\n== check os platform ===============================================\r\nos: Linux\r\nos kernel version: #53~18.04.1-Ubuntu SMP Thu Jun 4 14:58:26 UTC 2020\r\nos release version: 5.3.0-59-generic\r\nos platform: Linux-5.3.0-59-generic-x86_64-with-Ubuntu-18.04-bionic\r\nlinux distribution: ('Ubuntu', '18.04', 'bionic')\r\nlinux os distribution: ('Ubuntu', '18.04', 'bionic')\r\nmac version: ('', ('', '', ''), '')\r\n\r\n== check python ===================================================\r\npython version: 3.6.9\r\npython branch: \r\npython build version: ('default', 'Apr 18 2020 01:56:04')\r\npython compiler version: GCC 8.4.0\r\npython implementation: CPython\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\nCopyright (C) 2017 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n== check pips ===================================================\r\nnumpy                   1.18.4\r\nprotobuf                3.12.2\r\ntensorflow              2.1.1\r\ntensorflow-addons       0.10.0\r\ntensorflow-estimator    2.1.0\r\n\r\n== tensorflow import ============================================\r\ntf.version.VERSION = 2.1.1\r\ntf.version.GIT_VERSION = v2.1.0-33-g3ffdb91\r\ntf.version.COMPILER_VERSION = 7.3.1 20180303\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport random as rd\r\n\r\ndataset = (\r\n    tf.data.Dataset.from_tensor_slices((np.arange(5)))\r\n    .map(lambda annotation: (rd.random(), np.random.rand(), tf.random.uniform([])))\r\n)\r\n\r\nfor el in dataset:\r\n    print(el)\r\n```\r\n\r\n**Describe the current behavior**\r\nThe value for every element of the dataset is the same for python or numpy based random, but not for tensorflow based random.\r\n\r\n**Describe the expected behavior**\r\nThe value for tf, np, python (...)-based randomness is a new random value for every dataset element.\r\nOr a warning / error to avoid falling into this trap and debugging.\r\n", "comments": ["Was able to reproduce the issue with TF v2.1, [TF v.2.2](https://colab.research.google.com/gist/amahendrakar/7a8ad34fd0bceb98fdd35669f7f24e94/40620-2-2.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/3216528f7a297d5b932263356b452898/40620-tf-nightly.ipynb). Please find the attached gist. Thanks!", "This happens because functions passed to tf.data transformations are traced and converted to tensorflow graphs for execution in C++. During tracing, `rd.random()` and `np.random.rand()` get evaluated into constants (since tensorflow doesn't have hooks to detect these methods being called, like it does for tensorflow ops). The easiest way to work around this is to stick with tensorflow ops. Another option is to wrap non-tensorflow code in [tf.py_function](https://www.tensorflow.org/api_docs/python/tf/py_function), which executes arbitrary python code as a tensorflow op (with the caveat that this requires grabbing the GIL and generally results in worse performance compared to the equivalent tensorflow ops).\r\n\r\nI agree that it's too easy to get an unpleasant surprise from using non-tensorflow ops in tf.data functions. @Wirg, do you think there is anywhere that we could improve the documentation that would have helped you here? Some places that we could add discussion of tracing:\r\n- Top-level `tf.data.Dataset` doc: https://www.tensorflow.org/api_docs/python/tf/data/Dataset\r\n- Individual `tf.data.Dataset` transformations that take a function argument, e.g. https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map. The `map` doc does currently say that the function will be traced, but it is buried in the detailed section of the doc, and not mentioned in the short description of the function argument.\r\n- In the tf.data guide: https://www.tensorflow.org/guide/data", "Hi @aaudiber,\r\n\r\nI understand the inerant complexity. Thanks for the explanation.\r\n\r\nI think a warning the tf.data guide and in the map function dcos should be enough.\r\n\r\nSorry for the long time without answer.", "@Wirg Closing the issue as of now since we have provided the enough information on this. Please feel free to re-open the issue if you have any concern.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40620\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40620\">No</a>\n", "> \r\n@aaudiber\r\nIf I want to convert a numpy random op to a tensorflow op, but still need to use the result as a regular number or numpy int, how can I do this?  For example, I want to use the following random cropping code to crop an image and calculate the center shift.  It works with numpy, but of course if used in or as a map function it always generates the same shift which is unwanted behavior.  Trying to use the tensorflow equivalent op produces the desired different random number each time, but I can not get it to run without an error due to its use later when cropping the image using numpy slicing.  I can not simply use .numpy() as a method either as it will not run unless the code is executing eagerly.  \r\n\r\nNote: I already have the code running correctly using tf.py_function, but I am wondering if there is a way to use tensorflow random ops and therefore get the code to run without py_function and get some performance gains.  \r\n```\r\n\r\n`def random_crop_single(img, random_crop_size, img_real_size, is_valid = False):\r\n    # Note: image_data_format is 'channel_last'\r\n    #print(\"IMAGE SHAPE\" , img.shape)\r\n    assert img.shape[2] == 3\r\n    height, width = img.shape[0], img.shape[1]\r\n\r\n    dy, dx = random_crop_size\r\n    #Choose a random point in the valid area to be the new location, then crop\r\n    if not is_valid:\r\n        x = np.random.randint(0, width - dx + 1)\r\n        #y = tf.experimental.numpy.random.randint(0 , height - dy + 1, dtype=np.int)\r\n        #tf.print(\"y\", int(y), output_stream=sys.stderr)\r\n        y = np.random.randint(0, height - dy + 1)\r\n        #y = int(y)\r\n\r\n    else:\r\n        x = 0\r\n        y = 0\r\n    #Second and third return values are the x and y adjustments for labels (unscaled)\r\n    return img[ y:(y+dy), x:(x+dx), :], (float(x) / img.shape[1]) * img_real_size, (float(y) / img.shape[0]) * img_real_size`\r\n```"]}, {"number": 40619, "title": "TF2.x API Docs module.py example fix.", "body": "#37293 \r\n\r\nPer the thread under #37296 I renamed the args so that they are compatible with constructors of the relevant super classes. \r\n\r\nThe complete example code, extracted from docstring, now runs without issue:\r\n\r\n```\r\nclass Dense(tf.Module):\r\n    def __init__(self, input_dim, output_size, name=None):\r\n        super(Dense, self).__init__(name=name)\r\n        self.w = tf.Variable(\r\n            tf.random.normal([input_dim, output_size]), name='w')\r\n        self.b = tf.Variable(tf.zeros([output_size]), name='b')\r\n    def __call__(self, x):\r\n        y = tf.matmul(x, self.w) + self.b\r\n        return tf.nn.relu(y)\r\n\r\nclass MLP(tf.Module):\r\n    def __init__(self, input_size, sizes, name=None):\r\n      super(MLP, self).__init__(name=name)\r\n      self.layers = []\r\n      with self.name_scope:\r\n        for size in sizes:\r\n          self.layers.append(Dense(input_dim=input_size, output_size=size))\r\n          input_size = size\r\n\r\n    @tf.Module.with_name_scope\r\n    def __call__(self, x):\r\n      for layer in self.layers:\r\n        x = layer(x)\r\n      return x\r\n\r\nmlp = MLP(100, sizes=[30, 30])\r\n```\r\n", "comments": ["[example.zip](https://github.com/tensorflow/tensorflow/files/4898617/example.zip)\r\n\r\nI tried uploading the notebook but github didn't cooperate. So I zipped it instead. ", "Looks like the docstest failed because I forgot to put `>>> module.variables` line at the end.", "I fixed the issue causing failed doctest. However when I wanted to test it the tf_doctest.py failed.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"tf_doctest.py\", line 30, in <module>\r\n    import tensorflow.compat.v2 as tf\r\nModuleNotFoundError: No module named 'tensorflow.compat'\r\n```\r\n\r\n\r\nWhich is kind of  vexing because I do have all packages installed and it gives me the same message when I try to run it in devel docker image and directly on my machine. So I feel rather conflicted on this one.", "I tested your changes internally and it worked. Let's merge this.", "@gbaned Can you approve the copybara thing and send the CL to me internally?", "> I tested your changes internally and it worked. Let's merge this.\r\n\r\nThank you. "]}, {"number": 40618, "title": "Add gcs_filesystem_test", "body": "@mihaimaruseac \r\n- This PR add gcs_filesystem_test. This PR will fail to build if #40582 is merged because #40582 add a new dependency (`gcs_helper`) into `gcs_filesystem`. So I think you could review it but we don't merge it yet. After #40582 is merged, I will rebase and add the new dependency to this PR.\r\n- Should we add this test to Tensorflow CI ?", "comments": ["CI should pick it automatically after merging.", "@mihaimaruseac \r\nA small note for CI: If this test run outside Google, we have to set a env variable GOOGLE_APPLICATION_CREDENTIALS. In future PR, I will add another env variable TMPDIR to control the test directory as well", "@mihaimaruseac \r\nThe CI has a problem which I have just mentioned. It can not find GOOGLE_APPLICATION_CREDENTIALS", "If we cannot suply it automatically, let's make the test `manual`", "So I will add the new dependecy and `manual` tag in one commit after #40582 is merged", "@mihaimaruseac Rebase and add tag", "Will have to import manually. One of the failures is missing the license in the new `.cc` file, but it seems there are a few more. Will come back if there is stuff that you have to do", "Sorry, I forgot that one", "The other failure is from using `gunit` directly. Will take some time to convert that internally to the internal version. Can we instead use `//third_party/tensorflow/core/platform:test` as dependency and rewrite test to be similar to the `modular_filesystem_test`?", "ok I will use it"]}, {"number": 40617, "title": "keras.models.clone_model ignores input_tensors for functional models", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.7.7\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nFor functional models `tensorflow.keras.models.clone_model` ignores input_tensors if they are InputLayer objects. It keeps the original input tensors and does not build the model \"on top of new inputs tensors\".\r\n\r\n**Describe the expected behavior**\r\nThe cloned model should be built on provided \"input tensors or InputLayer objects\" as stated in the documentation and as it is for sequential models.\r\nThis would allow to change the input shape and dtype or clone a model on top of a model.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\n# Build a sequential and functional model\r\noriginal_input = tf.keras.Input(shape=(1,), dtype=tf.float32)\r\nsequential_model = tf.keras.Sequential([\r\n    original_input,\r\n    # tf.keras.layers.Lambda(lambda x: x),\r\n])\r\nfunctional_model = tf.keras.Model(sequential_model.inputs, sequential_model.outputs)\r\n\r\n# Predict some data\r\nprint(\"sequential_model:\", sequential_model.predict([[1.5]]))\r\nprint(\"functional_model:\", functional_model.predict([[1.5]]))\r\n\r\n# Clone the model\r\ncloned_sequential_model = tf.keras.models.clone_model(sequential_model)\r\ncloned_functional_model = tf.keras.models.clone_model(functional_model)\r\nassert id(cloned_sequential_model.input) != id(original_input)  # OK\r\nassert id(cloned_functional_model.input) != id(original_input)  # OK\r\n\r\n# Clone the model and make the input expect a different shape and dtype\r\nnew_input = tf.keras.Input(shape=(None,), dtype=tf.uint8)\r\ncloned_sequential_model = tf.keras.models.clone_model(sequential_model, input_tensors=[new_input])\r\ncloned_functional_model = tf.keras.models.clone_model(functional_model, input_tensors=[new_input])\r\n\r\nassert id(cloned_sequential_model.input) == id(new_input) # OK\r\nassert id(cloned_functional_model.input) == id(new_input) # FAILS (expected to pass)\r\nassert id(cloned_functional_model.input) != id(original_input)  # FAILS (expected to pass)\r\n\r\nprint(\"cloned_sequential_model.inputs:\", cloned_sequential_model.inputs)\r\nprint(\"cloned_functional_model.inputs:\", cloned_functional_model.inputs)\r\n\r\nassert(cloned_sequential_model.predict([[1.5]]) == cloned_functional_model.predict([[1.5]])) # FAILS (expected to pass)\r\nprint(\"cloned_sequential_model:\", cloned_sequential_model.predict([[1.5]]))\r\nprint(\"cloned_functional_model:\", cloned_functional_model.predict([[1.5]]))\r\n\r\n## Chain example\r\nx = tf.keras.Input(shape=(1,), dtype=tf.float32)\r\ny = tf.keras.layers.Lambda(lambda x: -x)(x)\r\nother_model = tf.keras.Model(x, y)\r\n\r\nchained_models = tf.keras.models.clone_model(functional_model, other_model.outputs)\r\nassert chained_models.predict([[1]]) == [[-1]] # FAILS (expected to pass)\r\nprint(\"chained_models:\", chained_models.predict([[1]]))\r\n\r\n```\r\n", "comments": ["I am able to replicate this issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/da1dd12adf48a54fc633fa2856bc3d2a/untitled233.ipynb)", "Was able to replicate the issue in TF 2.6.0-dev20210603,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/75bfd103e7cff4317dd6a21c44d6d93f/untitled200.ipynb#scrollTo=DJ0ccRYHA4CN)..Thanks !", "@tobiasmaier I tried to reproduce the issue on colab using TF v2.7.0 and faced different error .Please find the gist[ here](https://colab.research.google.com/gist/sushreebarsa/75bfd103e7cff4317dd6a21c44d6d93f/untitled200.ipynb#scrollTo=Sd_v1TUCA_0m) for reference.Thanks!", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40617\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40617\">No</a>\n"]}, {"number": 40616, "title": "RuntimeError: Mmap of '/home/pi/object-detector/model' failed.", "body": "Hi, \r\nI am using Raspberry pi 4 Model B, armv7l, with Raspbian stretch. I am having one 'mobilenet_v2_1.0_224_quant_edgetpu.tflite' model which I want to run with the google coral edge TPU. When I am trying to run the model it fails. \r\nI have used 658 images of two classes. I have also used same images for the training and validation. \r\nWhat can be the reason (not enough memory, TPU is damaged, the model is not prepared properly )?  How can I get to know the edge TPU is working or not? How can I fix it? \r\n\r\nHer is the traceback call: \r\n\r\n     pi@raspberrypi:~/object-detector $ python3 detect_object_video_edge.py --model  \r\n    /home/pi/object-mask-detector/model --labels /home/pi/object-detector/model/object_labels.txt\r\n    [INFO] parsing class labels...\r\n    [INFO] loading Coral model...\r\n    Traceback (most recent call last):\r\n      File \"detect_object_video_edge.py\", line 29, in <module>\r\n        model = DetectionEngine(args[\"model\"])\r\n      File \"/usr/lib/python3/dist-packages/edgetpu/detection/engine.py\", line 73, in __init__\r\n        super().__init__(model_path)\r\n      File \"/usr/lib/python3/dist-packages/edgetpu/basic/basic_engine.py\", line 92, in __init__\r\n        self._engine = BasicEnginePythonWrapper.CreateFromFile(model_path)\r\n    RuntimeError: Mmap of '/home/pi/object-detector/model' failed.\r\n", "comments": ["It got solved by adding the explicit path of the model rather than the directory where the model got saved. "]}, {"number": 40615, "title": " C++ interface session->Close() does not reclaim memory resources", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Utuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):1.5\r\n- Python version:\r\n- Bazel version (if compiling from source):0.24.1\r\n- GCC/Compiler version (if compiling from source):5.5.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I have executed session->closing(), the memory is not completely released.\r\nI found that after the destruction of the session object, only a part of the memory was released, not completely released.\r\n\r\nIn my scenario, the complete memory is 4.28G, after executing session->Close() and destructuring the object, the memory is 3G.\r\n\r\n**Describe the expected behavior**\r\n\r\nTheoretically\uff0cafter executed session->Close()\uff0c will Close the session to release the resources associated with.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nint main() {\r\n  {\r\n    tensorflow::SessionOptions sess_options_;\r\n    sess_options_.config.set_use_per_session_threads(true);\r\n    sess_options_.config.set_intra_op_parallelism_threads(1);\r\n    sess_options_.config.set_inter_op_parallelism_threads(1);\r\n\r\n    std::unique_ptr<tensorflow::Session> sess_;\r\n    sess_.reset(tensorflow::NewSession(sess_options_));\r\n\r\n    tensorflow::GraphDef graph_def;\r\n    auto default_env = tensorflow::Env::Default();\r\n    tensorflow::Status s = tensorflow::ReadBinaryProto(default_env, \r\n      pd_path, &graph_def);\r\n\r\n    sess_->Create(graph_def);\r\n\r\n    std::vector<tensorflow::Tensor> output;\r\n    std::vector<std::pair<std::string, tensorflow::Tensor>> tf_input;\r\n\r\n    tf_input.emplace_back(...);\r\n    auto status = sess_->Run(tf_input,\r\n                             std::vector<std::string>{\"output_node\"}, {},\r\n                             &output);\r\n    sess_->Close();\r\n  }\r\n}", "comments": ["Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 40613, "title": "TFLite GPU Delegate with OpenCL backend produces wrong result", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Oneplus 6 (Snapdragon 845)\r\n- TensorFlow installed from (source or binary): source (tag v2.1.1) and binary\r\n- TensorFlow version (use command below): v2.1.1\r\n- Python version: python 3.6.9\r\n- Bazel version (if compiling from source): bazel 0.29.1\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\n- CUDA/cuDNN version: \r\nfrom `nvidia-smi`:\r\nNVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2\r\nfrom `nvcc --version`:\r\nCuda compilation tools, release 10.1, V10.1.243\r\nno `cuDNN`\r\n- GPU model and memory:\r\nRTX 2080 Ti 11G memory\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nBuild environment information:\r\n[tf_env_bulid.txt](https://github.com/tensorflow/tensorflow/files/4803097/tf_env_bulid.txt)\r\n\r\nExecution evironment information:\r\n[tf_env_exec.txt](https://github.com/tensorflow/tensorflow/files/4803099/tf_env_exec.txt)\r\n\r\n\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n```\r\nv2.1.0-33-g3ffdb91 2.1.1\r\n```\r\n\r\n**Describe the current behavior**\r\nI used YOLOv3 (COCO dataset) TFLite model to detect obejcts,\r\nwhile TFLite OpenCL GPU Delegate produces error result compared to TFLite CPU backend.\r\n\r\nTFLite CPU:\r\n<img src=\"https://user-images.githubusercontent.com/3896992/85111276-aee63a00-b246-11ea-8723-f75cbf048fa4.jpg\" width=\"250\">\r\n\r\n\r\nTFLite OpenCL GPU Delegate:\r\n<img src=\"https://user-images.githubusercontent.com/3896992/85111294-b279c100-b246-11ea-84a3-3af63af3b977.jpg\" width=\"250\">\r\n\r\nNote the tie on Zidane is not detected with OpenCL GPU delegate.\r\n\r\nAt first, the detection problem is encountered on Oneplus 6T.\r\nTo ensure the problem is only with OpenCL GPU delegate and not with OpenGLES GPU delegate, I built both OpenCL and OpenGLES GPU delegate on Linux to double check the result.\r\nLinux OpenCL:\r\n<img src=\"https://user-images.githubusercontent.com/3896992/85112848-e48c2280-b248-11ea-831d-7f19c68b1f66.jpg\" width=\"400\">\r\n\r\nLinux OpenGLES:\r\n<img src=\"https://user-images.githubusercontent.com/3896992/85113251-9297cc80-b249-11ea-80ba-4e6610fad545.jpg\" width=\"400\">\r\n\r\n**Describe the expected behavior**\r\nTFLite OpenCL GPU Delegate output same result as TFLite CPU backend.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nOne can use https://github.com/zldrobit/onnx_tflite_yolov3/tree/reproduce-opencl to reproduce\r\n result from CPU, OpenCL and OpenGLES GPU delegate, with argument `--delegate \"\"`, \r\n `--delegate \"libtensorflowlite_gpu_delegate.so\"` and `--delegate \"libtensorflowlite_gpu_gl.so\"` respectively.\r\nThe model can be downloaded from https://drive.google.com/file/d/1iATViInSaPZWV7zH_YMAwMQK5XNDpdGl/view?usp=sharing\r\nA pre-bulit docker can be downloaded by\r\n```\r\ndocker pull zldrobit/cudagl:opencl\r\n```\r\nClone the git repo, download the model to `weights/yolov3_coco.fp32.tflite`, and run `python3 tflite_detect.py`. The results will be generated in the `output` folder.\r\n\r\nPS: I am unable to provide a Colab notebook, due to the configuration diffculty of OpenCL and OpenGL on Colab environment. `*.so` are based on Tensorflow tag v2.1.1, and I only add initilization code and clean code (Once I have time, I will publish this build process ASAP).\r\n\r\nPPS: I cannot run the OpenGLES GPU delegate on Oneplus 6T, the most critical error message may be\r\n```\r\nE/libEGL: call to OpenGL ES API with no current context (logged once per thread)\r\n\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Can you try building and running against the latest nightly/head version? I know we've resolved a number of accuracy issues over the last few months since the 2.1 release.\r\n\r\nAlso, it would be good to know which options you're enabling in the GPU delegate. Are you using relaxed precision? Are you using the Java or C APIs? Any other custom options? Thanks.", "I built against the nightly version, and now the prediction with TFLite GPU delegate is correct.\r\nPreviously, I use both relaxed and strict precisions, but the prediction with neither of them is as expected.\r\nI use Java API on Android, and Python API on Linux.\r\n\r\nThanks!", "Thanks for confirming!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40613\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40613\">No</a>\n", "@zldrobit Is it possible for you to share your openCL Implementation on Adnroid Studio for the GPU delegate. \r\n", "@dhruv-arcot , I just built tensorflow lite .so file with bazel in Ubuntu 18.04 using:\r\n```\r\nbazel build --copt -DMESA_EGL_NO_X11_HEADERS --copt -DEGL_NO_X11 tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so\r\n```\r\nBefore that, I have to install mesa libs:\r\n```\r\nsudo apt-get install mesa-common-dev libegl1-mesa-dev libgles2-mesa-dev\r\n```\r\nActually, the fastest way to get a OpenCL Implementation on Android is to use TFLite's AAR.\r\nOne can add\r\n```\r\nimplementation 'org.tensorflow:tensorflow-lite:2.3.0'\r\nimplementation 'org.tensorflow:tensorflow-lite-gpu:2.3.0'\r\n```\r\nin the Android gradle `dependency` to download TFLite's AAR.\r\nMy only reason to manually build OpenCL and OpenGLES implementations for TFLite GPU delegate on x86 is to debug TFLite output results.\r\n", "This is a question from a novice developer. I know that opengl or opencl can be delegated through gpu delegate in Android studio, how can I implement it?\r\nFor example in java api\r\n\r\n// NEW: Prepare GPU delegate.\r\nGpuDelegate delegate = new GpuDelegate();\r\nInterpreter.Options options = (new Interpreter.Options()).addDelegate(delegate);\r\n\r\n// Set up interpreter.\r\nInterpreter interpreter = new Interpreter(model, options);\r\n\r\n// Run inference.\r\nwriteToInputTensor(inputTensor);\r\ninterpreter.run(inputTensor, outputTensor);\r\nreadFromOutputTensor(outputTensor);\r\n\r\n// Clean up.\r\ndelegate.close();\r\n\r\nIs opengl used in this code? If correct, how should I implement opencl?", "@GwakEunSeong I don't know the details of the Java API, but IIRC, the logic whether the OpenCL backend is used or the OpenGL is hidden away from you.  If OpenCL is available, it'll try to use that; otherwise OpenGL.", "@impjdi Thanks for answering. If you can use opencl internally in the gpu delegate, you are saying that it is available with opencl, otherwise it uses opengl, right?? If so, is there anything I need to configure in order to use opencl??", "@GwakEunSeong it requires nothing to be configured in order to use opencl delegate.\r\n", "@zldrobit Thanks for answering!", "@zldrobit Thanks for the reply \r\n@GwakEunSeong That was actually going to be my follow up, thanks for that as well. \r\n"]}, {"number": 40612, "title": "Correct data type for tf serving warmup file: tf_serving_warmup_requests", "body": "```\r\n\"\"\"Generate Warmup requests.\"\"\"\r\nimport tensorflow as tf\r\nimport requests\r\nimport base64\r\nimport numpy as np\r\nimport requests\r\nfrom PIL import Image\r\n\r\nfrom tensorflow.python.framework import tensor_util\r\nfrom tensorflow_serving.apis import predict_pb2\r\nfrom tensorflow_serving.apis import prediction_log_pb2\r\n\r\nim_path = '/home/n/Documents/mask_rcnn/dataset/val/00.jpg'\r\nimg = np.array(Image.open(im_path))\r\nx = np.array(Image.open(im_path))\r\nimage_data = x.tolist()\r\nNUM_RECORDS = 2\r\n\r\n\r\ndef main():\r\n    \"\"\"Generate TFRecords for warming up.\"\"\"\r\n    with tf.io.TFRecordWriter(\"tf_serving_warmup_requests\") as writer:\r\n        predict_request = predict_pb2.PredictRequest()\r\n\r\n        predict_request.model_spec.name = 'a_model'\r\n\r\n        predict_request.model_spec.signature_name = 'serving_default'\r\n\r\n        predict_request.inputs['image:0'].CopyFrom(\r\n            tensor_util.make_tensor_proto(image_data))\r\n\r\n        log = prediction_log_pb2.PredictionLog(\r\n            predict_log=prediction_log_pb2.PredictLog(request=predict_request))\r\n\r\n        for r in range(NUM_RECORDS):\r\n            writer.write(log.SerializeToString())\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\nThis is file which generates tf_serving_warmup_requests,\r\nhowever im getting an error from the server:\r\n\r\n```\r\n2020-06-19 13:16:46.659681: E tensorflow_serving/util/retrier.cc:37] Loading servable: {name: a version: 1} failed: Invalid argument: Expects arg[0] to be float but half is provided\r\n\r\n```\r\n\r\nCould you help to set correct format for data input ?", "comments": ["@Adblu,\r\nTensorFlow Serving issues are tracked in the Serving repo. Could you please submit a new issue from [this link](https://github.com/tensorflow/serving/issues/new) and fill in the template, so that we can track the issue there. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Solved this by changing the image into the list:\r\n```\r\n            tensor_util.make_tensor_proto(image_data.tolist()))\r\n```\r\n\r\n"]}, {"number": 40611, "title": "[TF: 2.2] failing to load .pb protobuf properly", "body": "**System information**:\r\n\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n\r\n- TensorFlow installed source: source\r\n\r\n- TensorFlow version: 2.2.0 stable\r\n\r\n- Python version: python3\r\n\r\n- Bazel version: using Bazelisk with Bazel 2.0.0 version as required from tf-2.2.0\r\n\r\n- GCC/Compiler version (if compiling from source): GCC-8\r\n\r\n- CUDA/cuDNN version: No CUDA (for the moment)\r\n\r\n- Tensorflow compilation: `bazel --host_jvm_args=-Xmx30G build --jobs=8 --config=v2 --config=opt --copt=-O3 --copt=-m64 --copt=-march=native --verbose_failures //tensorflow:install_headers //tensorflow:tensorflow //tensorflow:tensorflow_cc //tensorflow:tensorflow_framework //tensorflow/tools/lib_package:libtensorflow`\r\n\r\n\r\n**What I have done**:\r\n\r\nI'm trying to load a protobuf graph saved with a python script to inference with C++ APIs.\r\n\r\nThe python script is:\r\n\r\n```\r\n#!/usr/bin/env python3\r\n\r\nfrom __future__ import print_function\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.datasets import cifar10\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\r\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\r\nimport os, sys\r\n\r\ntf.keras.backend.clear_session()\r\n\r\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\r\nfor dev in physical_devices:\r\n  try:\r\n    tf.config.experimental.set_memory_growth(dev, True)\r\n    print(dev, \"SET MEMORY GROWTH\")\r\n    #tf.config.set_logical_device_configuration(dev, [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])\r\n    print(tf.config.get_logical_device_configuration(dev))\r\n  except:\r\n    print(\"Device config error\")\r\n    sys.exit(1)\r\n\r\n\r\nbatch_size = 512\r\nnum_classes = 10\r\nepochs = 50\r\ndata_augmentation = True\r\n\r\n# The data, split between train and test sets:\r\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\nprint('x_train shape:', x_train.shape)\r\nprint(x_train.shape[0], 'train samples')\r\nprint(x_test.shape[0], 'test samples')\r\n\r\n# Convert class vectors to binary class matrices.\r\ny_train = tf.keras.utils.to_categorical(y_train, num_classes)\r\ny_test = tf.keras.utils.to_categorical(y_test, num_classes)\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv2D(32, (3, 3), padding='same',\r\n                 input_shape=x_train.shape[1:]))\r\nmodel.add(Activation('relu'))\r\nmodel.add(Conv2D(32, (3, 3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Dropout(0.25))\r\n\r\nmodel.add(Conv2D(64, (3, 3), padding='same'))\r\nmodel.add(Activation('relu'))\r\nmodel.add(Conv2D(64, (3, 3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Dropout(0.25))\r\n\r\nmodel.add(Flatten())\r\nmodel.add(Dense(1024))\r\nmodel.add(Activation('relu'))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(Dense(1024))\r\nmodel.add(Activation('relu'))\r\nmodel.add(Dropout(0.25))\r\nmodel.add(Dense(512))\r\nmodel.add(Activation('relu'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(num_classes))\r\nmodel.add(Activation('softmax'))\r\n\r\n# initiate RMSprop optimizer\r\nopt = tf.keras.optimizers.Adam(learning_rate=0.001)\r\n\r\n# Let's train the model using RMSprop\r\nmodel.compile(loss='categorical_crossentropy',\r\n              optimizer=opt,\r\n              metrics=['accuracy'])\r\n\r\nprint(model.summary())\r\n\r\nx_train = x_train.astype('float32')\r\nx_test = x_test.astype('float32')\r\nx_train /= 255.\r\nx_test /= 255.\r\n\r\nprint(x_train.shape[0])\r\n\r\nif not data_augmentation:\r\n    print('Not using data augmentation.')\r\n    model.fit(x_train, y_train,\r\n              steps_per_epoch=((x_train.shape[0] // batch_size)),\r\n              batch_size=batch_size,\r\n              epochs=epochs,\r\n              validation_data=(x_test, y_test),\r\n              validation_steps=((x_test.shape[0] // batch_size)),\r\n              shuffle=True)\r\nelse:\r\n    print('Using real-time data augmentation.')\r\n    # This will do preprocessing and realtime data augmentation:\r\n    datagen = ImageDataGenerator(\r\n        featurewise_center=False,  # set input mean to 0 over the dataset\r\n        samplewise_center=False,  # set each sample mean to 0\r\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\r\n        samplewise_std_normalization=False,  # divide each input by its std\r\n        zca_whitening=False,  # apply ZCA whitening\r\n        zca_epsilon=1e-06,  # epsilon for ZCA whitening\r\n        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\r\n        # randomly shift images horizontally (fraction of total width)\r\n        width_shift_range=0.1,\r\n        # randomly shift images vertically (fraction of total height)\r\n        height_shift_range=0.1,\r\n        shear_range=0.,  # set range for random shear\r\n        zoom_range=0.,  # set range for random zoom\r\n        channel_shift_range=0.,  # set range for random channel shifts\r\n        # set mode for filling points outside the input boundaries\r\n        fill_mode='nearest',\r\n        cval=0.,  # value used for fill_mode = \"constant\"\r\n        horizontal_flip=True,  # randomly flip images\r\n        vertical_flip=False,  # randomly flip images\r\n        # set rescaling factor (applied before any other transformation)\r\n        rescale=None,\r\n        # set function that will be applied on each input\r\n        preprocessing_function=None,\r\n        # image data format, either \"channels_first\" or \"channels_last\"\r\n        data_format=None,\r\n        # fraction of images reserved for validation (strictly between 0 and 1)\r\n        validation_split=0.0)\r\n\r\n    # Compute quantities required for feature-wise normalization\r\n    # (std, mean, and principal components if ZCA whitening is applied).\r\n    datagen.fit(x_train)\r\n\r\n    # Fit the model on the batches generated by datagen.flow().\r\n    model.fit(datagen.flow(x_train, y_train,\r\n                            batch_size=batch_size),\r\n                        steps_per_epoch=((x_train.shape[0] // batch_size)),\r\n                        epochs=epochs,\r\n                        validation_steps=((x_test.shape[0] // batch_size)),\r\n                        validation_data=(x_test, y_test),\r\n                        shuffle=True)\r\nprint(\"fitted\")\r\n\r\nmodel.save(save_format='tf', filepath='../../graphs/test0', include_optimizer=True)\r\nprint(\"saved\")\r\n\r\nmodel1 = tf.keras.models.load_model('../../graphs/test0')\r\nprint(model1.summary())\r\n\r\nprint(\"Done\")\r\n```\r\n\r\nThe C++ code is:\r\n\r\n```\r\n#include <stdlib.h>\r\n\r\n#include <fstream>\r\n#include <iostream>\r\n#include <string>\r\n#include <vector>\r\n\r\n#include \"class_name.h\"\r\n#include \"tensorflow/cc/ops/const_op.h\"\r\n#include \"tensorflow/cc/ops/standard_ops.h\"\r\n#include \"tensorflow/core/framework/tensor.h\"\r\n#include \"tensorflow/core/graph/default_device.h\"\r\n#include \"tensorflow/core/graph/graph_def_builder.h\"\r\n#include \"tensorflow/core/lib/core/errors.h\"\r\n#include \"tensorflow/core/lib/core/stringpiece.h\"\r\n#include \"tensorflow/core/lib/core/threadpool.h\"\r\n#include \"tensorflow/core/lib/io/path.h\"\r\n#include \"tensorflow/core/lib/strings/stringprintf.h\"\r\n#include \"tensorflow/core/platform/env.h\"\r\n#include \"tensorflow/core/platform/init_main.h\"\r\n#include \"tensorflow/core/platform/logging.h\"\r\n#include \"tensorflow/core/platform/types.h\"\r\n#include \"tensorflow/core/public/session.h\"\r\n#include \"tensorflow/core/util/command_line_flags.h\"\r\n\r\nusing namespace tensorflow;\r\nusing tensorflow::Flag;\r\nusing tensorflow::Status;\r\nusing tensorflow::string;\r\nusing tensorflow::Tensor;\r\n\r\n//Read the image file, apply appropriate decoding depending on type of image\r\nint TensorFromFile(string filename, const int i_height, const int i_width, std::vector<Tensor>* o_tensors) {\r\n  tensorflow::Status status;\r\n  auto root = tensorflow::Scope::NewRootScope();\r\n  using namespace ::tensorflow::ops;\r\n  std::unique_ptr<tensorflow::Session> session(tensorflow::NewSession({}));\r\n  tensorflow::GraphDef graph;\r\n\r\n  auto reader = tensorflow::ops::ReadFile(root.WithOpName(\"img_reader\"), filename);\r\n  const int channels = 1;\r\n  tensorflow::Output imgreader;\r\n\r\n  if (tensorflow::str_util::EndsWith(filename, \".png\")) {\r\n    imgreader = DecodePng(root.WithOpName(\"png_reader\"), reader, DecodePng::Channels(channels));\r\n  } else if (tensorflow::str_util::EndsWith(filename, \".gif\")) {\r\n    imgreader = DecodeGif(root.WithOpName(\"gif_reader\"), reader);\r\n  } else {\r\n    imgreader = DecodeJpeg(root.WithOpName(\"jpeg_reader\"), reader, DecodeJpeg::Channels(channels));\r\n  }\r\n\r\n  auto f_caster = Cast(root.WithOpName(\"float_caster\"), imgreader, tensorflow::DT_FLOAT);\r\n  ExpandDims(root.WithOpName(\"output\"), f_caster, 0);\r\n\r\n  status = root.ToGraphDef(&graph);\r\n  if (!status.ok()) {\r\n    LOG(ERROR) << status.ToString();\r\n    return -1;\r\n  }\r\n\r\n  status = session->Create(graph);\r\n  if (!status.ok()) {\r\n    LOG(ERROR) << status.ToString();\r\n    return -1;\r\n  }\r\n\r\n  status = session->Run({}, {\"output\"}, {}, o_tensors);\r\n  if (!status.ok()) {\r\n    LOG(ERROR) << status.ToString();\r\n    return -1;\r\n  }\r\n\r\n  return 0;\r\n}\r\n\r\nint main(int argc, char* argv[]) {\r\n  using namespace ::tensorflow::ops;\r\n  tensorflow::Status status;\r\n\r\n  std::string delimiter = \".\";\r\n  std::string ofilename;\r\n  std::vector<Tensor> inputs;\r\n  std::vector<Tensor> outputs;\r\n\r\n  std::string graph_path = \"../../graphs/test0\";\r\n  std::string image_path = \"../../graphs/test0.png\";\r\n\r\n  std::string mdlpath(graph_path);\r\n  std::string imgpath(image_path);\r\n  int32 inputdim = 32;\r\n\r\n  std::unique_ptr<tensorflow::Session> session(tensorflow::NewSession({}));\r\n  tensorflow::GraphDef graph;\r\n\r\n  LOG(INFO) << \"OK\";\r\n\r\n  //read model file\r\n  status = ReadBinaryProto(Env::Default(), mdlpath, &graph);\r\n  if (!status.ok()) {\r\n    std::cout << status.ToString() << \"\\n\";\r\n    return -1;\r\n  }\r\n\r\n  LOG(INFO) << \"STATUS: \" << status.ToString();\r\n  LOG(INFO) << \"OK\";\r\n\r\n  //add graph to scope\r\n  status = session->Create(graph);\r\n  if (!status.ok()) {\r\n    std::cout << status.ToString() << \"\\n\";\r\n    return -1;\r\n  }\r\n\r\n  LOG(INFO) << status.ToString();\r\n  LOG(INFO) << \"OK\";\r\n\r\n  //Read input image, assuming to be a sqaure image\r\n  if (TensorFromFile(imgpath, inputdim, inputdim, &inputs)) {\r\n    LOG(ERROR) << \"Image reading failed\"\r\n               << \"\\n\";\r\n    return -1;\r\n  }\r\n\r\n  LOG(INFO) << \"OK L1\";\r\n\r\n  std::cout << \"input dimension of the image: \" << inputs[0].DebugString() << std::endl;\r\n  std::cout << \"v: \" << graph.version() << std::endl;\r\n  std::cout << \"ns: \" << graph.node_size() << std::endl << std::endl;\r\n  auto shape = graph.node().Get(0).attr().at(\"shape\").shape();\r\n  for (int i = 0; i < shape.dim_size(); i++) {\r\n      std::cout << shape.dim(i).size()<<std::endl;\r\n  }\r\n\r\n  //get the appropriate input and out layer names from the graph/mode to execute\r\n  auto inputlayer = graph.node(0).name();\r\n  LOG(INFO) << \"OK A1\";\r\n  auto outputlayer = graph.node(graph.node_size() - 1).name();\r\n  LOG(INFO) << \"OK A2\";\r\n\r\n  status = session->Run({{inputlayer, inputs[0]}}, {outputlayer}, {}, &outputs);\r\n  if (!status.ok()) {\r\n    LOG(ERROR) << status.ToString();\r\n    return -1;\r\n  }\r\n\r\n  std::cout << \"Output dimension of the image\" << outputs[0].DebugString() << std::endl;\r\n\r\n  //create filename\r\n  ofilename.append(imgpath.substr(0, imgpath.find(delimiter)));\r\n  ofilename.append(\"_mask.png\");\r\n\r\n  std::cout << \"output filename: \" << ofilename << std::endl;\r\n\r\n  //Now write this to a image file\r\n  //if (TensorToFile(ofilename, outputs, threshold)) return -1;\r\n\r\n  session->Close();\r\n\r\n  return 0;\r\n}\r\n```\r\n\r\nAnd then I execute:\r\n\r\n`LD_LIBRARY_PATH=\"/opt/tf_cpp/lib\" ./test`\r\n\r\nWith output:\r\n\r\n```\r\n2020-06-19 14:13:51.120484: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3399905000 Hz\r\n2020-06-19 14:13:51.120742: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561cd4f5fe20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-19 14:13:51.120755: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-06-19 14:13:51.120974: I /home/niccolo/Documents/CodeBlocks/Prova/main.cpp:95] OK\r\n2020-06-19 14:13:51.121005: I /home/niccolo/Documents/CodeBlocks/Prova/main.cpp:104] STATUS: OK\r\n2020-06-19 14:13:51.121010: I /home/niccolo/Documents/CodeBlocks/Prova/main.cpp:105] OK\r\n2020-06-19 14:13:51.121016: I /home/niccolo/Documents/CodeBlocks/Prova/main.cpp:114] OK\r\n2020-06-19 14:13:51.121020: I /home/niccolo/Documents/CodeBlocks/Prova/main.cpp:115] OK\r\n2020-06-19 14:13:51.123531: I /home/niccolo/Documents/CodeBlocks/Prova/main.cpp:124] OK L1\r\ninput dimension of the image: Tensor<type: float shape: [1,32,32,1] values: [[[169][131][100]]]...>\r\nv: 0\r\nns: 0\r\n\r\n[libprotobuf FATAL /opt/tpt/tf_cpp/include/src/google/protobuf/repeated_field.h:1535] CHECK failed: (index) < (current_size_): \r\nterminate called after throwing an instance of 'google::protobuf::FatalException'\r\n  what():  CHECK failed: (index) < (current_size_): \r\nAborted (core dumped)\r\n```\r\n\r\nI followed these guides and tutorials:\r\n\r\n- https://medium.com/@hamedmp/exporting-trained-tensorflow-models-to-c-the-right-way-cf24b609d183\r\n\r\n- https://medium.com/analytics-vidhya/deploying-tensorflow-2-1-as-c-c-executable-1d090845055c\r\n\r\n- https://medium.com/@dibyajyoti_20397/building-an-inference-module-in-tensorflow-c-api-5cac2096c0ec\r\n\r\n(I didn't find a good complete guide)\r\n\r\nWhat could cause that error?\r\n\r\nThx in advance", "comments": ["It was my fault: \r\n\r\nwith this code:\r\n```\r\n  //get the appropriate input and out layer names from the graph/mode to execute\r\n  auto inputlayer = graph.node(0).name();\r\n  LOG(INFO) << \"OK A1\";\r\n  auto outputlayer = graph.node(graph.node_size() - 1).name();\r\n  LOG(INFO) << \"OK A2\";\r\n```\r\n\r\nI take first and last node of the graph but they are not the input and output layer of the network.\r\nI disver them using `saved_model_cli` as described here: https://www.tensorflow.org/guide/saved_model#details_of_the_savedmodel_command_line_interface"]}, {"number": 40610, "title": "Ensure there are test samples for imdb dataset, when maxlen is low", "body": "With the current imdb.load_data(), the following results are seen\r\nfor different values of maxlen.\r\n\r\n```\r\n\tload_data                (len(x_train), len(x_test))\r\n------------------------------------------------------------\r\nimdb.load_data(maxlen=50)    -->    (1035, 0)\r\nimdb.load_data(maxlen=100)   -->    (5736, 0)\r\nimdb.load_data(maxlen=200)   -->    (25000, 3913)\r\nimdb.load_data()             -->    (25000, 25000)\r\n```\r\n\r\nAnalysis: We can observe that when maxlen is low, the number\r\nof test samples can be 0. This is because the train and test data is\r\nconcatenated, then the samples with length > maxlen are removed, and\r\nthe first 25,000 are considered as training data.\r\n\r\nFix: This can be fixed when data can be filtered first to remove the\r\nones with length > maxlen, and then concatenate to process further.\r\nThe following are the results after the fix.\r\n\r\n```\r\n     fixed load_data              (len(x_train), len(x_test))\r\n------------------------------------------------------------\r\nimdb.load_data(maxlen=50)    -->    (477, 558)\r\nimdb.load_data(maxlen=100)   -->    (2773, 2963)\r\nimdb.load_data(maxlen=200)   -->    (14244, 14669)\r\nimdb.load_data()             -->    (25000, 25000)\r\n```\r\n\r\nFixes #40609 ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40610) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40610) for more info**.\n\n<!-- ok -->"]}, {"number": 40609, "title": "Test set is empty for imdb.load_data() with low maxlen value", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): custom\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\nWith the current imdb.load_data(), the following results are seen for different values of maxlen.\r\n\r\n```\r\nload_data                          (len(x_train), len(x_test))\r\n------------------------------------------------------------ \r\nimdb.load_data(maxlen=50)    -->    (1035, 0)\r\nimdb.load_data(maxlen=100)   -->    (5736, 0)\r\nimdb.load_data(maxlen=200)   -->    (25000, 3913)\r\nimdb.load_data()             -->    (25000, 25000)\r\n```\r\n\r\n<b>Analysis:</b> We can observe that when maxlen is low, the number of test samples can be 0. This is because the train and test data is concatenated, then the samples with length > maxlen are removed, and the first 25,000 are considered as training data.   \r\n\r\n**Describe the expected behavior**\r\n\r\nThe number of test samples should not be zero.\r\n\r\n<b>Fix:</b>This can be fixed when data can be filtered first to remove the ones with length > maxlen, and then concatenate to process further. The following are the results after the fix.\r\n\r\n```\r\nload_data                              (len(x_train), len(x_test))\r\n------------------------------------------------------------ \r\nimdb.load_data(maxlen=50)    -->    (477, 558)\r\nimdb.load_data(maxlen=100)   -->    (2773, 2963)\r\nimdb.load_data(maxlen=200)   -->    (14244, 14669)\r\nimdb.load_data()             -->    (25000, 25000)\r\n```\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nfrom tensorflow.keras.datasets import imdb\r\n(x_train, _), (x_test, _) = imdb.load_data(maxlen=50)\r\nprint((len(x_train), len(x_test)))  # This gives (1035, 0)\r\n\r\n(x_train, _), (x_test, _) = imdb.load_data(maxlen=100)\r\nprint((len(x_train), len(x_test)))  # This gives (5736, 0)\r\n\r\n(x_train, _), (x_test, _) = imdb.load_data(maxlen=200)\r\nprint((len(x_train), len(x_test)))  # This gives (25000, 3913)\r\n```", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40609\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40609\">No</a>\n"]}, {"number": 40608, "title": "tensorflow.keras.layer.add() causes exception in to_json()", "body": "\r\n**System information**\r\n- custom code (below)\r\n- OS: Windows 10\r\n- Tensorflow installed from binary\r\n- Tensorflow version v2.2.0-rc4-8-g2b96f3662b 2.2.0. \r\n- Keras version 2.3.0-tf\r\n- Python version 3.6\r\n- Run from CPU\r\n\r\n**Describe the current behavior**\r\nto_json() causes exception in nest.pack_sequence_as()\r\n\r\n**Describe the expected behavior**\r\nto_json() should work!\r\n\r\n**Standalone code to reproduce the issue**\r\n\"\"\"\r\n        Keras/Tensorflow bug.\r\n  Exception in to_json()/.../nest.pack_sequence_as().\r\n  Change \"KL.add\" to \"tf.add_n\" and there is no problem\r\n\r\n  Using Tensorflow 2.2.0\r\n        Keras 2.3.0-tf\r\n\"\"\" \r\nimport tensorflow.keras.layers as KL\r\nimport tensorflow.keras.models as KM\r\nimport tensorflow as tf\r\n\r\nInput=KL.Input(batch_shape=[2,2])\r\nsk=[]\r\no1=KL.Dense(17,name='d1')(Input)\r\nsk.append(o1)\r\no2=KL.Dense(17,name='d2')(o1)\r\nsk.append(o2)\r\nx1 = KL.add(sk,name='a1')\r\no3=KL.Dense(17,name='d3')(o2)\r\nsk.append(o3)\r\nx2 = KL.add(sk,name='a2')\r\n\r\nm=KM.Model(inputs=[Input],outputs=[x1,x2])\r\nm.summary()\r\njunk = m.to_json()\r\n", "comments": ["@EdmundButler,\r\nI was able to reproduce the error with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/11aa0e9bb6c11a86ed142395461dff56/40608.ipynb). However, the issue seems to be fixed with the latest [TF-nightly](https://colab.research.google.com/gist/amahendrakar/b88a2dafe177ef01458303e8659c1045/40608-tf-nightly.ipynb). Please check the linked gist. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40608\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40608\">No</a>\n"]}, {"number": 40607, "title": "tflite model failed to prepare in inference ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):  0.28.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: P100 16VRAM\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nAfter obtaining a .pb model with export_tflite_ssd_graph.py im using TOCO to create a detect.tflite model. I cant run inference with this model or compile it for the TPU.\r\n\r\nIm getting this error when running inference with detect.tflite \r\n\r\n> Traceback (most recent call last):\r\n  File \"test_tflite.py\", line 33, in <module>\r\n    main()\r\n  File \"test_tflite.py\", line 21, in main\r\n    interpreter.allocate_tensors() \r\n  File \"/usr/local/lib/python3.5/dist-packages/tflite_runtime/interpreter.py\", line 242, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tflite_runtime/interpreter_wrapper.py\", line 110, in AllocateTensors\r\n    return _interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\nRuntimeError: tensorflow/lite/kernels/kernel_util.cc:129 std::abs(input_product_scale - bias_scale) <= 1e-6 * std::min(input_product_scale, bias_scale) was not true.Node number 108 (CONV_2D) failed to prepare.\r\n\r\nI suspect it involves using SIGMOID as a score converter. When i use SOFTMAX the model works fine.\r\n\r\nI understand the difference between the two, but i suspect that the post-processing-op is not capable of handling num_classes > 2 when using sigmoid. \r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@NicholaiStaalung \r\nPlease share a simple stand alone code to replicate the issue faced or share a colab gist to analyse the error.", "> @NicholaiStaalung\r\n> Please share a simple stand alone code to replicate the issue faced or share a colab gist to analyse the error.\r\n\r\n[replicate_issue.tar.gz](https://github.com/tensorflow/tensorflow/files/4804424/replicate_issue.tar.gz)\r\n\r\nunpack and enter the folder\r\n`python3 test_tflite.py detect.tflite`\r\n", "Okay, i was able to compile the .tlifte model after a low number of training steps with SIGMOID and num_classes = 2. I kept it training over the weekend and now i got the same error as above. Kind of frustrating. So for now im switching feature extractor. I do find it really weird that this error occurs since i have been using the exact same workflow for almost a year. Please help me here!", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40607\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40607\">No</a>\n"]}, {"number": 40606, "title": "sparse_softmax_cross_entropy_with_logits wrong answer under XLA", "body": "Related to [https://github.com/tensorflow/probability/issues/975](https://github.com/tensorflow/probability/issues/975)\r\n\r\n## System spec\r\n\r\nTF version: 2.3.0-dev20200618 (`tf-nightly`)\r\nOS: Linux Mint 19.3, CUDA-10.1.\r\n\r\n## Current behaviour\r\n\r\nUnder XLA compilation, `tf.nn.sparse_softmax_cross_entropy_with_logits ` returns `nan` if one or more `logits` are `-inf`.  In regular graph mode, `-0.0` is returned.\r\n\r\n## Expected behaviour\r\nThe log_prob function above should return the cross entropy of obtaining labels `k` from logits `logits`, and be non-`nan` (`-inf` is okay) for valid values of the function parameters.  \r\n\r\n**NB** expected behaviour occurs under TF2.2.0 (stock Colab).  Only the current dev build is affected. \r\n\r\n## Minimal working example\r\n\r\n [colab here](https://colab.research.google.com/drive/1OUwqDF_KU0aUo-NHq-HKtxUgYv0Szy-u?usp=sharing)\r\n```python\r\nprobs = tf.constant([0, 1, 1], dtype=tf.int32)\r\nlogits = tf.math.log(probs)\r\n\r\n@tf.function(autograph=False, experimental_compile=True)\r\ndef log_prob(k):                                                                                   \r\n    lp = -tf.nn.sparse_softmax_cross_entropy_with_logits(labels=k, logits=logits)                                   \r\n    return lp  \r\nprint(log_prob(1))  # Expected: -0.6931472, Actual: nan\r\n```\r\n\r\n", "comments": ["@chrism0dwk \r\n\r\nI have tried in colab with TF nightly version(`2.3.0-dev20200619`) and was able to reproduce the issue.But i am not seeing any issue with TF 2.2 version.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/a216776610471b160d8f9f12e0d5be75/untitled49.ipynb).Thanks!", "Yup, confirmed that's it.  Thanks.", "@srvasude could you take a look at this?", "This is fixed by https://github.com/tensorflow/tensorflow/commit/e9b3d33ccfa4b670ca0a221db9bc7bc1281b0557", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40606\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40606\">No</a>\n"]}, {"number": 40604, "title": "Keras Checkpoint Callback's `mode` argument needs clearer documentation", "body": "In the documentation for the ModelCheckpoint class, the argument `mode` is described as such:\r\n> ... In `auto` mode, the direction is automatically inferred from the name of the monitored quantity.\r\n\r\nSuch a description leads to an overestimation of this module's inference capabilities! In my case, I was training a model with `monitor='val_auc'` and `mode='auto'`, believing that TF was setting smartly setting mode as `'max'` as one typically wants to maximize the AUC just like accuracy.\r\nHowever, I quickly realized that the model was only checkpointing when the AUC was decreasing. I looked through the source code to find out why:\r\nhttps://github.com/tensorflow/tensorflow/blob/c159f1599548428660c80dada924d69f269384a3/tensorflow/python/keras/callbacks.py#L1195\r\nThe algorithm sets mode as `'max'` only when the given metric string contained `'acc'` or when it started with `'fmeasure'` - everything else is set to `'min'`, including for `'val_auc'`.\r\n\r\nIn my opinion, this inference logic is extremely arbitrary. The string check of `'acc'`or `'fmeasure'` should be either directly mentioned in the documentation, or replaced all-together with a more capable algorithm. Another suggestion is to have a default optimization direction for each metric class and the ModelCheckpoint class can simply refer to that when `mode` is not set. Additionally, one could get rid of `'auto'` as an option since the optimization direction for every metric is clear and obvious from the practitioner's point of view.", "comments": ["I have also encountered the same issue @peterkim95 . ", "I like to work on this, Either I can change the documentation or modify the code not to support other metrics than 'acc' for `method =auto`   @jvishnuvardhan ", "The docs are now updated with commit [e82e381](https://github.com/tensorflow/tensorflow/commit/e82e381c5452e68918bef612ee324a4415108802). Thanks!"]}, {"number": 40603, "title": "cudaGetDevice() failed. Status: cudaGetErrorString symbol not found.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): tensorflow-gpu 2.0.0 from pip\r\n- TensorFlow version (use command below): tensorflow-gpu 2.0.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: 1650, 4 gb\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nwhen using tf.test.is_gpu_available()\r\nit shows error like this \r\ncudaGetDevice() failed. Status: cudaGetErrorString symbol not found.\r\n\r\n**Describe the expected behavior**\r\nTrue\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@piyushdavda007 \r\n\r\nTensorFlow supports CUDA 10.0 (TensorFlow >= 1.13.0).See [software requirements](https://www.tensorflow.org/install/gpu#software_requirements).Also see which TF binary suits your case https://www.tensorflow.org/install/pip?lang=python3#older-versions-of-tensorflow.\r\nPlease refer similar issue #30726,#34354,#34820,#32381 and see if it helps you.Also, try reinstalling CUDA once and see id the problem still persists.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40603\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40603\">No</a>\n"]}, {"number": 40602, "title": "add test case to keras model save with pathlike dir ", "body": "Added test case covering using pathlike objects such as pathlib.Path for path of saving model and saving weights in Keras. \r\nAlso make `parse_saved_model` compatible with pathlike objects. ", "comments": ["@yixingfu Can you please check build failures. Thanks!"]}, {"number": 40601, "title": "Add show_dtype support for plot_model, update related tests. ", "body": "In response to a feature request. ", "comments": []}, {"number": 40600, "title": "Non-GPU tensorflow:1.15.0 docker image fails when it can't find CUDA", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): *Official tensorflow docker image (`4.19.76-linuxkit`...?), running in Docker Desktop on Windows 10*\r\n- TensorFlow installed from (source or binary): *docker image `tensorflow/tensorflow:1.15.0`*\r\n- TensorFlow version: *1.15.0*\r\n- Python version: *2.7.15+*\r\n- Installed using virtualenv? pip? conda?: *docker*\r\n- CUDA/cuDNN version: *none*\r\n- GPU model and memory: *host has NVIDIA card, but not applicable to docker guest*\r\n\r\n**Describe the problem**\r\n\r\nI am attempting to run the CPU-version of tensorflow with the object_detection pets tutorial. I'm trying to do it using Docker Desktop for Windows, but using the CPU since nvidia-docker doesn't support windows. But when I run a simple example it gives me errors about failing to find CUDA. I'm not sure if these are fatal errors or ignorable warnings. The pets object detection job I'm trying to run stops shortly after these errors. I'm using the official tensorflow docker image `tensorflow/tensorflow:1.15.0`.\r\n\r\nI've also honed down a minimal example:\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nDockerfile:\r\n```\r\nFROM tensorflow/tensorflow:1.15.0\r\nCOPY test-tf.py /tensorflow/test-tf.py\r\nWORKDIR /tensorflow\r\n```\r\ntest-tf.py:\r\n```\r\n#!/usr/bin/env python\r\n\r\nimport tensorflow as tf\r\nhello = tf.constant('Hello, TensorFlow!')\r\nsess = tf.Session()\r\n```\r\nSteps:\r\n1. `docker build --tag detect-tf-generic .`\r\n2. `docker run --rm -it --privileged -p 6006:6006 detect-tf-generic`\r\n3. `./test-tf.py`\r\n\r\nOutput:\r\n```\r\nWARNING:tensorflow:From ./test-tf.py:5: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\r\n\r\n2020-06-19 00:48:56.905823: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2020-06-19 00:48:56.905858: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)\r\n2020-06-19 00:48:56.905987: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (c0e8a9783c35): /proc/driver/nvidia/version does not exist\r\n2020-06-19 00:48:56.906459: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-06-19 00:48:56.913014: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1992000000 Hz\r\n2020-06-19 00:48:56.913329: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5583d4015850 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-19 00:48:56.913358: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n```\r\nI can't tell if this is a fatal error that I need to fix or just a warning. As mentioned my object detection job completes early a few output lines after this. It doesn't seem like it should be looking for CUDA at all, but what do I know.", "comments": ["@xdhmoore Can you please check whether the version you are importing is `TF1.x` or `TF2.x`. After importing, can you print print(tf.__version__). It looks like it is importing `TF2.x` and complaining about `tf.Session`. Thanks!", "The following code gives me this output:\r\n\r\ntest-tf.py:\r\n```\r\n#!/usr/bin/env python\r\n\r\nimport tensorflow as tf\r\nprint(tf.version)\r\nprint(tf.__version__)\r\n```\r\n\r\nOutput:\r\n```\r\n<module 'tensorflow._api.v1.version' from '/usr/local/lib/python2.7/dist-packages/tensorflow_core/_api/v1/version/__init__.pyc'>\r\n1.15.0\r\n```", "I just rebuilt my docker image against `tensorflow/tensorflow:1.14.0` and did not get the CUDA error. Perhaps it's a bug specific to 1.15.0?", "Confirmed that I don't get the error in the 1.15.2 image.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40600\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40600\">No</a>\n"]}, {"number": 40599, "title": "Optimizer serialization casting, and dtype display support for plot_model.", "body": "Responding to two separate requests for small feature changes. ", "comments": []}, {"number": 40598, "title": "[Intel MKL] Fixing MklTanh compilation error with DNNL0", "body": "This PR fixes build error in DNNL0 config caused by recent PR to enable Tanh operation in MKL backend. Since optimized Tanh support exists in DNNL1 only, this PR makes this feature DNNL1 specific.", "comments": ["@penporn Required for TF2.3", "@penpornk This bug fix is required for TF2.3 . Friendly Ping."]}, {"number": 40597, "title": "StatusGroup fuzzers", "body": "Added StatusGroup fuzzers and build rules @mihaimaruseac ", "comments": []}]