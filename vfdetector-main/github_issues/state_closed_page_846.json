[{"number": 28139, "title": "'shape' attribute in strip_unused_lib.py", "body": "Added copying 'shape' attribute of graph node. Missing values caused unknown shapes for usages like TensorBoard or TensorRT.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28139) for more info**.\n\n<!-- need_sender_cla -->", "> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n> \r\n>  **Please visit https://cla.developers.google.com/ to sign.**\r\n> \r\n> Once you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\r\n> \r\n> #### What to do if you already signed the CLA\r\n> ##### Individual signers\r\n> * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n> ##### Corporate signers\r\n> * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\r\n> * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\r\n> \r\n>  **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28139) for more info**.\r\n\r\nI signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28139) for more info**.\n\n<!-- need_author_cla -->", "> to pass this check, please resolve this problem and have the pull request author add a\r\n\r\nadded email as alternate", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28139) for more info**.\n\n<!-- ok -->", "@MarkDaoust Could you PTAL and approve.", "@MarkDaoust can you please review this PR ? ", "Changes have been merged in to master by commit `2892a68` , so closing this PR."]}, {"number": 28138, "title": "Lite: Add Op Refatcored", "body": "1:> Optimized Macro usage to reduce code duplication.\r\n2:> Unnecessary Template usage removed.\r\n3:> Wrong error message corrected.\r\n\r\n", "comments": ["Added Jared to have a look. My only comment is that the old version is more consistent with other kernels. Any specific reason to change it\r\n\r\nThanks", "Yeah, this code is actually slightly less readable IMO. The EvalAddQuantized changes look fine though.", "@jdduke , @karimnosseir : Thanks for your valuable opinions. The reason being for this change is to reduce the duplicate codes, which i feel is acceptable, as the readability is not drastically reduced, Thanks!\r\n\r\nWaiting for your PoV!", "@jdduke : Improved the readability, please check, Thanks!", "Hmm, now the float path is inconsistent with the quantized path. I think for now it's best for us to just sit on this change, we may be doing a broader refactoring of kernels and we're trying to minimize churn."]}, {"number": 28137, "title": "Tensorflow lite conversion from h5 to tflite increases file size", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.13.1-0-g6612da8951' 1.13.1\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: RTX 2070 8GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nTflite conversion from h5 file increases in size from 141KB to 508KB\r\n**Describe the expected behavior**\r\nA reduction in size\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\r\n    tf.keras.layers.Dense(11, activation=tf.nn.softmax)\r\n])\r\n\r\nmodel.compile(optimizer='adam', \r\n          loss='sparse_categorical_crossentropy',\r\n          metrics=['accuracy'])\r\nmodel.fit(TrainingData, TrainingLabels, epochs=400)\r\n\r\ntf.keras.models.save_model(model, keras_file)\r\n\r\nconverter = tf.contrib.lite.TFLiteConverter.from_keras_model_file(keras_file)\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@kartikye Could you create a reproducible code and share the link to that gist? It will help us resolve the issue faster. Thanks!", "What should i include in the reproducible code?", "@kartikye If you can create a google colab gist or any other way that we can run to reproduce the issue   would be great. The above code is not complete to reproduce the issue. It will make issue resolutions faster. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28137\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28137\">No</a>\n"]}, {"number": 28136, "title": "Add kissfft license back and include it in the \"VALID_LIBS\" list in syslibs_configure.bzl.oss", "body": "", "comments": ["Replaced by https://github.com/tensorflow/tensorflow/pull/28159"]}, {"number": 28135, "title": "Remove kissfft license from the OSS licenses group as it is still causing build issues.", "body": "PiperOrigin-RevId: 245155471", "comments": ["Replaced by https://github.com/tensorflow/tensorflow/pull/28136"]}, {"number": 28134, "title": "Tensorflow 2.0 GPU error: Failed to load the native TensorFlow runtime.", "body": "\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64 bits\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): CONDA\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Geforce 910m\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nI just installed everything and got the error\r\n\r\n**Other info / logs**\r\n\r\n  File \"<ipython-input-1-d36b20bff3cb>\", line 1, in <module>\r\n    runfile('C:/Users/victor/Documents/s4y/labeled-surgical-tools/preparar_dados.py', wdir='C:/Users/victor/Documents/s4y/labeled-surgical-tools')\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 786, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 110, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"C:/Users/victor/Documents/s4y/labeled-surgical-tools/preparar_dados.py\", line 7, in <module>\r\n    import tensorflow as tf\r\n\r\n  File \"C:\\Users\\victor\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 27, in <module>\r\n    from tensorflow._api.v2 import audio\r\n\r\n  File \"C:\\Users\\victor\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\_api\\v2\\audio\\__init__.py\", line 8, in <module>\r\n    from tensorflow.python.ops.gen_audio_ops import decode_wav\r\n\r\n  File \"C:\\Users\\victor\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\n  File \"C:\\Users\\victor\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\victor\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\victor\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\victor\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: N\u00e3o foi poss\u00edvel encontrar o m\u00f3dulo especificado.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["@victorinno Could you provide the steps you followed to install TF2.0 using conda? There are some known issue with python3.7 installed through Anaconda. Could you downgrade to Python3.6.6 or 3.6.8 and install TF2.0. There are useful resource on GitHub and Stackoverflow. Here is one [resource](https://github.com/tensorflow/tensorflow/issues/26182) and [here](https://github.com/tensorflow/tensorflow/issues/27935). Please let us know how it progresses. Thanks! ", "@jvishnuvardhan I had to install it by Conda cmd, because it didn\u00b4t appeared in the environment option inside anacona navigator.\r\n\r\nBut I\u00b4ll try downgrading it, test and post the result here", "I\u00b4ve created another environment with python 3.6.\r\n\r\nInstead of installing it throw python navigator I had installed throw conda cmd with the following command: pip install tensorflow-gpu==2.0.0-alpha0\r\n\r\nThe error persists. Could it be the cuda version? I\u00b4m using 10.1\r\n\r\nHere follows the trace:\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-1-d36b20bff3cb>\", line 1, in <module>\r\n    runfile('C:/Users/victor/Documents/s4y/labeled-surgical-tools/preparar_dados.py', wdir='C:/Users/victor/Documents/s4y/labeled-surgical-tools')\r\n\r\n  File \"C:\\Users\\victor\\.conda\\envs\\base 3.6\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 827, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"C:\\Users\\victor\\.conda\\envs\\base 3.6\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 110, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"C:/Users/victor/Documents/s4y/labeled-surgical-tools/preparar_dados.py\", line 7, in <module>\r\n    import tensorflow as tf\r\n\r\n  File \"C:\\Users\\victor\\.conda\\envs\\base 3.6\\lib\\site-packages\\tensorflow\\__init__.py\", line 27, in <module>\r\n    from tensorflow._api.v2 import audio\r\n\r\n  File \"C:\\Users\\victor\\.conda\\envs\\base 3.6\\lib\\site-packages\\tensorflow\\_api\\v2\\audio\\__init__.py\", line 8, in <module>\r\n    from tensorflow.python.ops.gen_audio_ops import decode_wav\r\n\r\n  File \"C:\\Users\\victor\\.conda\\envs\\base 3.6\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\n  File \"C:\\Users\\victor\\.conda\\envs\\base 3.6\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\victor\\.conda\\envs\\base 3.6\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\victor\\.conda\\envs\\base 3.6\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\victor\\.conda\\envs\\base 3.6\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\victor\\.conda\\envs\\base 3.6\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\victor\\.conda\\envs\\base 3.6\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: N\u00e3o foi poss\u00edvel encontrar o m\u00f3dulo especificado.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "@victorinno TF2.0 and TF1.13.1 pip binaries are built with CUDA10.0. So you need to downgrade to CUDA10.0 if you use pip binaries to install TF. Another thing is you need to download NVDIA drivers, CUDA toolkit before running pip install command. I have listed detailed steps in install TF1.12-gpu [here](https://stackoverflow.com/questions/38896424/tensorflow-not-found-using-pip/55787868#55787868). Please follow the steps overall but use update drivers to support TF2.0. If you want to use Conda to install TF, then please follow the steps listed in resource#1 of my earlier response. \r\n\r\nFor installing, virtual env, please do this\r\n1. (base) C:\\Users\\XXXX>conda create -n venv pip python=3.6\r\n2. (base) C:\\Users\\XXXX>conda activate venv\r\n3. Then go to Anaconda Navigator, select venv \r\n4. Change 'Applications on' to the virtual environment you just created ('venv')\r\n5. Install a new spyder under 'venv' or you can go to command window, type conda install spyder --new-instance\r\n6. Run the new spyder and test\r\n\r\nThanks\r\n", "@jvishnuvardhan Do I need to instal the normal version of tensorflow 2.0?", "@victorinno It depends on your work. On day to day if you use multiple versions of TF, then it is better to use virtual environment and have and virtual env for each TF version. If you use single TF version, I would suggest to install NVIDIA drivers, CUDA, cuDNN, Visual studio, python, and tensorflow using pip. Some people uses Anaconda to install TF-gpu but as of now TF team doesn't support conda based installations.Thanks!\r\n", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!"]}, {"number": 28133, "title": "sketch_rnn algorithm returns error on loading environment", "body": "using the google sketch_rnn algorithm, i wanted to use its trained dataset to develop an AI that self draws, but when i used the code below it returned an error\r\n\r\n**System information**\r\n- OS Platform and Distribution (windows):\r\n- TensorFlow installed from (source):\r\n- TensorFlow version (1.13.1):\r\n- Python version (3.6):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n**INFO:tensorflow:Downloading http://github.com/hardmaru/sketch-rnn-datasets/raw/master/aaron_sheep/aaron_sheep.npz\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-8-bc699734c167> in <module>()\r\n----> 1 [train_set, valid_set, test_set, hps_model, eval_hps_model, sample_hps_model] = load_env_compatible(data_dir, model_dir)\r\n      2 \r\n      3 #with np.load(path, allow_pickle=True) as f:\r\n      4 #    x_train, labels_train = f['x_train'], f['y_train']\r\n      5 #    x_test, labels_test = f['x_test'], f['y_test']\r\n\r\n<ipython-input-7-1c7d550387ed> in load_env_compatible(data_dir, model_dir)\r\n      7         data[fix] = (data[fix] == 1)\r\n      8     model_params.parse_json(json.dumps(data))\r\n----> 9     return load_dataset(data_dir, model_params, inference_mode=True)\r\n     10 \r\n     11 def load_model_compatible(model_dir):\r\n\r\n~\\Anaconda3\\lib\\site-packages\\magenta\\models\\sketch_rnn\\sketch_rnn_train.py in load_dataset(data_dir, model_params, inference_mode)\r\n    139         data = np.load(data_filepath)\r\n    140     tf.logging.info('Loaded {}/{}/{} from {}'.format(\r\n--> 141         len(data['train']), len(data['valid']), len(data['test']),\r\n    142         dataset))\r\n    143     if train_strokes is None:\r\n\r\n~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py in __getitem__(self, key)\r\n    260                 return format.read_array(bytes,\r\n    261                                          allow_pickle=self.allow_pickle,\r\n--> 262                                          pickle_kwargs=self.pickle_kwargs)\r\n    263             else:\r\n    264                 return self.zip.read(key)\r\n\r\n~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\format.py in read_array(fp, allow_pickle, pickle_kwargs)\r\n    690         # The array contained Python objects. We need to unpickle the data.\r\n    691         if not allow_pickle:\r\n--> 692             raise ValueError(\"Object arrays cannot be loaded when \"\r\n    693                              \"allow_pickle=False\")\r\n    694         if pickle_kwargs is None:\r\n\r\nValueError: Object arrays cannot be loaded when allow_pickle=False**\r\n\r\n**INFO:tensorflow:Downloading http://github.com/hardmaru/sketch-rnn-datasets/raw/master/aaron_sheep/aaron_sheep.npz\r\nINFO:tensorflow:Loaded 7400/300/300 from aaron_sheep.npz\r\nINFO:tensorflow:Dataset combined: 8000 (7400/300/300), avg len 125\r\nINFO:tensorflow:model_params.max_seq_len 250.\r\ntotal images <= max_seq_len is 7400\r\ntotal images <= max_seq_len is 300\r\ntotal images <= max_seq_len is 300\r\nINFO:tensorflow:normalizing_scale_factor 18.5198.**\r\n\r\n**[train_set, valid_set, test_set, hps_model, eval_hps_model, sample_hps_model] = load_env_compatible(data_dir, model_dir)**\r\n\r\n\r\n**def load_env_compatible(data_dir, model_dir):\r\n    model_params = sketch_rnn_model.get_default_hparams()\r\n    with tf.gfile.Open(os.path.join(model_dir, 'model_config.json'), 'r') as f:\r\n        data = json.load(f)\r\n    fix_list = ['conditional', 'is_training', 'use_input_dropout', 'use_output_dropout', 'use_recurrent_dropout']\r\n    for fix in fix_list:\r\n        data[fix] = (data[fix] == 1)\r\n    model_params.parse_json(json.dumps(data))\r\n    return load_dataset(data_dir, model_params, inference_mode=True)\r\n\r\ndef load_model_compatible(model_dir):\r\n    model_params = sketch_rnn_model.get_default_hparams()\r\n    with tf.gfile.Open(os.path.join(model_dir, 'model_config.json'), 'r') as f:\r\n        data = json.load(f)\r\n    fix_list = ['conditional', 'is_training', 'use_input_dropout', 'use_output_dropout', 'use_recurrent_dropout']\r\n    for fix in fix_list:\r\n        data[fix] = (data[fix] == 1)\r\n    model_params.parse_json(json.dumps(data))\r\n    \r\n    model_params.batch_size = 1\r\n    eval_model_params = sketch_rnn_model.copy_hparams(model_params)\r\n    eval_model_params.use_input_dropout = 0\r\n    eval_model_params.use_recurrent_dropout = 0\r\n    eval_model_params.use_output_dropout = 0\r\n    eval_model_params.is_training = 0\r\n    sample_model_params = sketch_rnn_model.copy_hparams(eval_model_params)\r\n    sample_model_params.max_seq_len = 1\r\n    return [model_params, eval_model_params, sample_model_params]**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 28132, "title": "Cannot convert keras file to tensorflow lite", "body": "Here is the keras model:\r\n\r\n```\r\nkeras_file = \"keras_model.h5\"\r\n\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\r\n    tf.keras.layers.Dense(11, activation=tf.nn.softmax)\r\n])\r\n\r\nmodel.compile(optimizer='adam', \r\n          loss='sparse_categorical_crossentropy',\r\n          metrics=['accuracy'])\r\nmodel.fit(TrainingData, TrainingLabels, epochs=400)\r\n\r\ntf.keras.models.save_model(model, keras_file)\r\n\r\n```\r\n\r\nAnd then I am using this code to convert it to tflite:\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model_file(keras_file)\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\nand then i get this error:\r\n\r\n```\r\n      1 \r\n----> 2 converter = lite.TFLiteConverter.from_keras_model_file( 'keras_model.h5' )\r\n      3 model = converter.convert()\r\n      4 \r\n      5 file = open( 'model.tflite' , 'wb' )\r\n\r\n1 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in __init__(self, graph_def, input_tensors, output_tensors, input_arrays_with_shape, output_arrays)\r\n    192       if not input_arrays_with_shape or not output_arrays:\r\n    193         raise ValueError(\r\n--> 194             \"If input_tensors and output_tensors are None, both \"\r\n    195             \"input_arrays_with_shape and output_arrays must be defined.\")\r\n    196       self._input_arrays_with_shape = input_arrays_with_shape\r\n\r\nValueError: If input_tensors and output_tensors are None, both input_arrays_with_shape and output_arrays must be defined.\r\n```\r\n\r\n\r\nAm i doing anything wrong?", "comments": ["I have the same problem. Did you resolve your issue?"]}, {"number": 28131, "title": "\"import_meta_graph\" fails", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.5.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda 10.0 with cuDNN7\r\n- GPU model and memory: GTX1080 with 8GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nerror occurs when calling `import_meta_graph`\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.tensorrt as trt\r\n\r\nmeta_graph_path = \"bert-eng/model.ckpt-7300.meta\"\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n    with tf.Session() as sess:\r\n\r\n        saver = tf.train.import_meta_graph(meta_graph_path) // error occurs here\r\n```\r\nWhen I run the above code, I get error at line:\r\n```python\r\nsaver = tf.train.import_meta_graph(meta_graph_path) // error occurs here\r\n```\r\n\r\n**Other info / logs**\r\nstack trace:\r\n```\r\n2019-04-25 10:40:52.107550: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-04-25 10:40:52.119933: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2099880000 Hz\r\n2019-04-25 10:40:52.122488: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5431820 executing computations on platform Host. Devices:\r\n2019-04-25 10:40:52.122547: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\nTraceback (most recent call last):\r\n  File \"tf_to_trt.py\", line 15, in <module>\r\n    saver = tf.train.import_meta_graph(meta_graph_path)\r\n  File \"/home/msl/.virtualenvs/venv_waveglow/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1435, in import_meta_graph\r\n    meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]\r\n  File \"/home/msl/.virtualenvs/venv_waveglow/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1457, in _import_meta_graph_with_return_elements\r\n    **kwargs))\r\n  File \"/home/msl/.virtualenvs/venv_waveglow/lib/python3.5/site-packages/tensorflow/python/framework/meta_graph.py\", line 806, in import_scoped_meta_graph_with_return_elements\r\n    return_elements=return_elements)\r\n  File \"/home/msl/.virtualenvs/venv_waveglow/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/msl/.virtualenvs/venv_waveglow/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 399, in import_graph_def\r\n    _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)\r\n  File \"/home/msl/.virtualenvs/venv_waveglow/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 159, in _RemoveDefaultAttrs\r\n    op_def = op_dict[node.op]\r\nKeyError: 'ExperimentalFunctionBufferingResource'\r\n```", "comments": ["@ThisIsIsaac Could you create a reproducible code (a gist or colab) to reproduce the issue. The above code is not complete. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28131\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28131\">No</a>\n", "Similar Issues [#27752](https://github.com/tensorflow/tensorflow/issues/27752#issue-432019580)\r\n[#29751](https://github.com/tensorflow/tensorflow/issues/29751#issue-455874429)"]}, {"number": 28130, "title": "tf.data.Dataset Performance Issue", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0 / 7.4.1\r\n- GPU model and memory: NVIDIA Titan V\r\n\r\n- CPU Make & Model: 2x Intel Xeon E5-2620 v4 (8 cores/16 logical)\r\n- Data Drive: Samsung SSD 960 EVO 1 TB\r\n\r\n**Describe the current behavior**\r\nCurrently using the tf.data.Dataset API to load image pairs for super resolution. I believe based on the minimal examples I could find the method below is as optimized as I can get for my use-case. However, when I grew my path list from 550 to 7950 items it slows down over 3x. It doesn't seem like this part of the pipeline should scale so poorly since the batches themselves are the same size. And the process of mapping & batching (mostly IO) should be parallelized across the 32 CPU cores of the machine. \r\n\r\nAny ideas? Pertinent code below.\r\n\r\n\r\n    lr_paths, hr_paths = ... # Flat lists of paths to LR & HR images, respectively.\r\n\r\n    def load_image(path): return tf.image.decode_png(tf.read_file(path), 3)\r\n\r\n    # Create the dataset\r\n    dataset = (tf.data.Dataset.from_tensor_slices((lr_paths, hr_paths))\r\n        .apply(tf.data.experimental.shuffle_and_repeat(count, FLAGS.max_epochs))\r\n        .apply(tf.data.experimental.map_and_batch(lambda x, y: (load_image(x), load_image(y)), FLAGS.batch_size, num_parallel_batches=max(1, (cpu_count() - 1) // FLAGS.batch_size)))\r\n        .apply(tf.data.experimental.prefetch_to_device('/device:GPU:0')))\r\n\r\n    # Query for the iterator\r\n    iterator = dataset.make_one_shot_iterator()\r\n    im_LR_batch, im_HR_batch = iterator.get_next()\r\n\r\n    ... # Preprocess the image batches with crop, data augmentation, type conversion", "comments": ["Can you try the following uses the best practices from the [tf.data performance guide](https://www.tensorflow.org/alpha/guide/data_performance):\r\n\r\n```\r\ndataset = tf.data.Dataset.from_tensor_slices((lr_paths, hr_paths)\r\ndataset = dataset.interleave(lambda x, y: tf.data.Dataset.from_tensors((tf.read_file(x), tf.read_file(y))), num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\ndataset = dataset.shuffle(count)\r\ndataset = dataset.repeat(FLAGS.max_epochs)  # shuffle and repeat will be fused automatically\r\ndataset = dataset.map(lambda x, y: (tf.image.decode_png(x, 3), tf.image.decode_png(y, 3)), num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\ndataset = dataset.batch(FLAGS.batch_size)  # map and batch will be fused automatically\r\ndataset = dataset.apply(tf.data.experimental.prefetch_to_device('/device:GPU:0')))\r\n```\r\n\r\nIf you would like further help, please provide a reproducible example.", "Hey, just giving an update here. I created a performance test for four data pipeline configurations - https://gist.github.com/mhurliman/3a955c9253b9477fa1aa297647b3c063. \r\n\r\nYour demo-code of interleave didn't match the API specification, whose intended purpose seems to be splitting up a dataset into multiple datasets which can then be interleaved. This means the data in the original dataset needs to nested in some form to be properly processed by interleave. I leveraged it by chunking the filename list into N-equally sized lists of filenames. Hopefully this is acceptable to make full efficiency of it.\r\n\r\nI also decided to try an initializable iterator to avoid embedding a littany of string filename data into the graph, but it didn't seem to help.\r\n\r\nThe four configurations are a cross of non-chunked vs. chunked, and standard vs. experimental APIs. Performance was pretty comparable across the board, even when experimenting with the number of data splits and data count. I did see a huge divergence between two separate computers, which is strange since the seemingly dominant computer has about 1/3 perf as the other. \r\n\r\nStill trying to pin down exactly where the performance delta is being introduced, but unlike the actual production scenario it doesn't seem to be as affected by the list size in the test environment.", "@mhurliman thanks for the update.\r\n\r\n`interleave` is a generalized version of `flat_map` -- `flat_map` maps input elements into (finite) datasets and produces their elements as (flattened) output, `interleave` is similar except that its output interleaves the elements of multiple datasets. My example was missing `tf.data.Dataset.from_tensors(...)` which is needed as the result of the `interleave` function needs to be a dataset. With that modification, the example should work as is (avoiding the need for jumping through the the filename list chunking hoop).\r\n\r\nI am happy to test out (and profile) the example code as long as you provide me with (synthetic) data that to test the input pipeline with.", "Here's a script to synthesize junk super rez training pairs: https://gist.github.com/mhurliman/934fa8e13dca3586e9be8f7464b769fd\r\n\r\nI updated the previous tests (https://gist.github.com/mhurliman/3a955c9253b9477fa1aa297647b3c063) to implement your suggested usage pattern. I also expanded it to generate a number of randomized image crops at once, creating a dataset out of those with 'from_tensor_slices(...)'. This eliminates the need for the later 'map' function, though I suppose data augmentation steps could be moved there - haven't tested perf to see if that would matter.\r\n\r\nThe results are pretty significant. The experimental version of parallel_interleave seems to perform drastically better than the interleave in the standard API. Went from about 16 samples per second up to 600 samples per second. Not sure the initial problem is solved, but I'm satisfied with this performance. So probably stopping my investigations here.\r\n\r\nThanks for all your help @jsimsa!", "I'm also considering implementing jittered sampling (partitioned stochastic sampling) instead of completely randomized crops to get better coverage of samples, but this was easiest to test initially.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28130\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28130\">No</a>\n", "@mhurliman regarding the difference between `parallel_interleave` and `interleave`, did you set the `num_parallel_calls` argument of `interleave`? if so, could you please share the two versions of the input pipeline. The performance of the two transformations should be identical. If it's not, it's a bug that I would like to fix.", "Both versions of the input pipeline are available in the gist I linked above. Given references to the synthesized input directories it should work out of the box.", "Thanks!"]}, {"number": 28129, "title": "Build fails due to missing @kissfft//:LICENSE", "body": "\r\n**System information**\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version: r1.14 https://github.com/tensorflow/tensorflow/commit/70d8199abb0c548fe75568cebe6bd9e2b7e6c25e\r\n- Python version: 3.6.7\r\n- Installed using virtualenv? pip? conda?: from source\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n- CUDA/cuDNN version: 10.0/7.5\r\n- GPU model and memory: RTX 2060 6GB\r\n\r\n**Describe the problem**\r\n\r\nThe current master fails to build because it fails to find `@kissfft//:LICENSE`.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\ngit checkout r1.14\r\ngit pull --recurse-submodules.\r\nbazel clean --expunge\r\n./configure\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n**Any other info / logs**\r\nThis appears to be due to a recent check-in (https://github.com/tensorflow/tensorflow/commit/15209edfc99cf9ef44c81cd37977a326ccee31c3 ) expecting to find a missing file.\r\n\r\n", "comments": ["I got this exact same error:\r\nERROR: missing input file '@kissfft//:LICENSE'\r\n", "Fixed on master by 02badbcaba I think", "Should be fixed on r1.14 by #28136", "Builds on master are still failing. ", "This commit may have fixed it on master: https://github.com/tensorflow/tensorflow/commit/7b8594571921a4e1885372edbabd30ac36192257\r\n\r\n[The build hasn't failed yet at least]", "I'm sorry, the initial commit I posted was not fixing the build as I thought. The one you mentioned did.", "Build succeeded at April 25 evening, but not installing. \r\nWhile installing it gives this error:\r\n```\r\nCollecting kissfft<1.3.1,>=1.3.0 (from tensorflow==1.13.1)\r\n  Could not find a version that satisfies the requirement kissfft<1.3.1,>=1.3.0 (from tensorflow==1.13.1) (from versions: )\r\nNo matching distribution found for kissfft<1.3.1,>=1.3.0 (from tensorflow==1.13.1)\r\n```\r\n \r\nOS: Centos 7\r\nTensorflow: master branch. v1.13.1\r\nPython: 3.5.2\r\n\r\nAny solution?", "Sorry for the breakage - this should have been fixed by https://github.com/tensorflow/tensorflow/commit/b63f929cc71abe8e7ccb8bd452fc51ab2cdef3ee\r\n\r\nPlease re-open if you are still experiencing problems.", "I have to compile from sources as, while I have a modern GPU, my old xeon CPU lacks the SSE4 instruction set. \r\n\r\ngreg@salt:~/code/tensorflow$ git status\r\nOn branch master\r\nYour branch is up to date with 'origin/master'.\r\n\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\nsucceeds\r\n\r\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\nsucceeds\r\n\r\npip3 install /tmp/tensorflow_pkg/tensorflow-1.13.1-cp36-cp36m-linux_x86_64.whl \r\n\r\nCollecting kissfft<1.3.1,>=1.3.0 (from tensorflow==1.13.1)\r\nException:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3/dist-packages/pip/basecommand.py\", line 215, in main\r\n    status = self.run(options, args)\r\n  File \"/usr/lib/python3/dist-packages/pip/commands/install.py\", line 353, in run\r\n    wb.build(autobuilding=True)\r\n  File \"/usr/lib/python3/dist-packages/pip/wheel.py\", line 749, in build\r\n    self.requirement_set.prepare_files(self.finder)\r\n  File \"/usr/lib/python3/dist-packages/pip/req/req_set.py\", line 380, in prepare_files\r\n    ignore_dependencies=self.ignore_dependencies))\r\n  File \"/usr/lib/python3/dist-packages/pip/req/req_set.py\", line 554, in _prepare_file\r\n    require_hashes\r\n  File \"/usr/lib/python3/dist-packages/pip/req/req_install.py\", line 278, in populate_link\r\n    self.link = finder.find_requirement(self, upgrade)\r\n  File \"/usr/lib/python3/dist-packages/pip/index.py\", line 465, in find_requirement\r\n    all_candidates = self.find_all_candidates(req.name)\r\n  File \"/usr/lib/python3/dist-packages/pip/index.py\", line 423, in find_all_candidates\r\n    for page in self._get_pages(url_locations, project_name):\r\n  File \"/usr/lib/python3/dist-packages/pip/index.py\", line 568, in _get_pages\r\n    page = self._get_page(location)\r\n  File \"/usr/lib/python3/dist-packages/pip/index.py\", line 683, in _get_page\r\n    return HTMLPage.get_page(link, session=self.session)\r\n  File \"/usr/lib/python3/dist-packages/pip/index.py\", line 795, in get_page\r\n    resp.raise_for_status()\r\n  File \"/usr/share/python-wheels/requests-2.18.4-py2.py3-none-any.whl/requests/models.py\", line 935, in raise_for_status\r\n    raise HTTPError(http_error_msg, response=self)\r\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://pypi.org/simple/kissfft/\r\n\r\nUbuntu 18.10\r\nbazel 0.24.1\r\npython 3.6.8\r\ngcc-6 (others gcc-7, gcc-8 installed but configured to use 6)\r\ncuda 10.0 (downgraded from 10.1)\r\n\r\nI did compile kissfft from github and did a make install, but no help. \r\n\r\nIt looks like a configuration issue in the bazel workspace that can be changed, but not sure where to look. Any help would be appreciated. \r\n\r\n\r\n", "I guess I should read the instructions!\r\n\r\nbazel clean\r\ngit update\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package --nightly_flag /tmp/tensorflow_pkg\r\npip3 install /tmp/tensorflow_pkg/tensorflow-1.13.1-cp36-cp36m-linux_x86_64.whl \r\n\r\nThe inclusion of the --nightly_flag when building the wheel fixed it. \r\n\r\n"]}, {"number": 28128, "title": "Parse visible device list from ConfigProto", "body": "PiperOrigin-RevId: 244476157", "comments": []}, {"number": 28127, "title": "Add a BUILD.system file for kissfft so that the license can be used as a dependency", "body": "PiperOrigin-RevId: 245134696", "comments": []}, {"number": 28126, "title": "Adding exception handling code to TensorShape methods", "body": "Hi,\r\n\r\nThis PR adds exception handling code to the following TensorShape methods:\r\n\r\n1. merge_with\r\n2. concatenate\r\n3. assert_same_rank\r\n\r\nThanks.", "comments": ["@gbaned This PR is yet to be assigned to a reviewer.\r\nThanks", "@MarkDaoust Could you PTAL and approve.", "@MarkDaoust Made some minor updates to the code. ", "@MarkDaoust can you please review this PR ? ", "Output after changes:\r\n\r\n```\r\n>>> import tensorflow as tf\r\n>>> a = tf.TensorShape(['30'])\r\n>>> a.merge_with('30')\r\n(<class 'TypeError'>, '::', TypeError('A string has ambiguous TensorShape, please wrap in a list or convert to an int: 30'))\r\n```\r\n\r\nBefore changes:\r\n```\r\n>>> import tensorflow as tf\r\n>>> a = tf.TensorShape(['30'])\r\n>>> a.merge_with('30')\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/saishruthi.tn@ibm.com/anaconda3/envs/cp/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 841, in merge_with\r\n    other = as_shape(other)\r\n  File \"/Users/saishruthi.tn@ibm.com/anaconda3/envs/cp/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 1125, in as_shape\r\n    return TensorShape(shape)\r\n  File \"/Users/saishruthi.tn@ibm.com/anaconda3/envs/cp/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 670, in __init__\r\n    \"list or convert to an int: %s\" % dims)\r\nTypeError: A string has ambiguous TensorShape, please wrap in a list or convert to an int: 30\r\n```\r\nBasically, handling raised exception and displaying only the needed information as the message is clear. Got the reference for this PR from how it is being handled in other functions."]}, {"number": 28125, "title": "Prelu graph Bug fix and added TODO", "body": "Bug fix along with the TC.", "comments": ["@karimnosseir , this is kind of important PR for me, can you please review the PR.\r\n\r\nRegards\r\nAmit", "@karimnosseir thanks for spending time on the PR, i also felt this is getting more complex, so i have removed the TODO part and for now i have only given the BUG fix and TC added to verify the bug fix. Once you review , approve and hopefully this gets merged , will upstream the TODO part as well.\r\n\r\nRegards\r\nAmit", "@karimnosseir can you please review this PR ? ", "@amitsrivastava78 could you please resolve the conflicts? Thanks!", "Closing the PR since another one is already existing"]}, {"number": 28124, "title": "Exclude device_tracer for CPU build", "body": "The file tensorflow/core/platform/default/device_tracer.cc is actually empty during the CPU build because GOOGLE_CUDA is not defined. So the cc library for device_tracer is empty, and when force linking (/WHOLEARCHIVE is used for linking shared library) an empty cc library with MSVC, we hit this compiler bug:\r\nhttps://support.microsoft.com/en-hk/help/4020481/fix-link-exe-crashes-with-a-fatal-lnk1000-error-when-you-use-wholearch\r\n\r\nSince device tracer is only for GPU build, the solution is to exclude it from CPU build.\r\n\r\nPiperOrigin-RevId: 244872492", "comments": []}, {"number": 28123, "title": "Fix missing dependencies in pip package", "body": "Also adds the license for the third party dependency \"kissfft\".\r\n\r\nPiperOrigin-RevId: 245098837", "comments": []}, {"number": 28122, "title": "Cherrypick fix for missing dependencies in pip package", "body": "", "comments": []}, {"number": 28121, "title": "1.14.0-rc0 cherry-pick request: Merge pull request #27069 from yongtang:tf.math.floordiv-floormod", "body": "PiperOrigin-RevId: 245101673\r\n\r\nCherrypick https://github.com/tensorflow/tensorflow/pull/27069 since it removes some endpoints in 2.0. We want all API removals to be checked in by 1.14 release.", "comments": []}, {"number": 28120, "title": "Pin tf-estimator-nightly to 1.14.0.dev2019042301", "body": "", "comments": []}, {"number": 28119, "title": "[ROCm] Correcting license filename for rocprim", "body": "The actual filename in the rocprim archive is LICENSE.txt (lowecase extension).\r\nUpdating the TF BUILD files to correct the name.\r\n\r\nThis change is (one of two) needed to fix the broken `--config=rocm` build (post PR 26722 merge). See PR #28116 for the other part of the fix", "comments": ["@gunan, fyi."]}, {"number": 28118, "title": "How to specify Tensorflow Estimator Version when building tensorflow from source?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: tf-nightly==1.13.1\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: 1080ti\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nHow to specify Tensorflow Estimator Version when building tensorflow from source?\r\n\r\nI get the source code from commit id: d5a1417612a396dc8f5b8be08d3428367fa3771e\r\nI find that Tensorflow Estimator is changed when I install it.\r\n\r\n- `tf-estimator-nightly    1.14.0.dev2019042301` # new one\r\n- `tf-estimator-nightly   1.14.0.dev2019031401` # previous one\r\n\r\nThe new one is not compatible with the d5a1417612a396dc8f5b8be08d3428367fa3771e code.\r\nSo, how to specify Tensorflow Estimator Version to `1.14.0.dev2019031401`?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n```bash \r\n...\r\n  File \"/home/wxf/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/__init__.py\", line 25, in <module>\r\n    import tensorflow_estimator.python.estimator.estimator_lib\r\n  File \"/home/wxf/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator_lib.py\", line 22, in <module>\r\n    from tensorflow_estimator.python.estimator.canned.baseline import BaselineClassifier\r\n  File \"/home/wxf/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/canned/baseline.py\", line 65, in <module>\r\n    from tensorflow_estimator.python.estimator import estimator\r\n  File \"/home/wxf/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1708, in <module>\r\n    class EstimatorV2(Estimator):\r\n  File \"/home/wxf/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1711, in EstimatorV2\r\n    export_savedmodel = deprecation.HIDDEN_ATTRIBUTE\r\nAttributeError: module 'tensorflow.python.util.deprecation' has no attribute 'HIDDEN_ATTRIBUTE'\r\n```", "comments": ["Can you try `pip install tf-estimator-nightly==1.14.0.dev2019031401` just before building?\r\n\r\nThe change of nightly should have been only on the r1.14 branch", "@mihaimaruseac Yes, it works. ", "Perfect. Sometime in the future the two projects won't be that tightly coupled.\r\n\r\nClosing now as this has been fixed / has workaround"]}, {"number": 28117, "title": "pylint fix to remove range(len) with enumerate", "body": "", "comments": ["@miaout17 Can you please review this PR? Thanks!"]}, {"number": 28116, "title": "[ROCm] Fixes for broken `--config=rocm` build, post PR #26722", "body": "This is a follow up to PR #26722 .\r\n\r\nThere were a couple of changes introduced in that PR that break the `--config=rocm` build, and this PR has fixes for the same.\r\n\r\n1. changing `if_cuda` to `if_cuda_is_configured` in cases where a \"duplicate dependency\" is introduced, due to a common dependency in the CUDA + ROCm paths\r\n\r\n2. correcting a filename typo for `@rocprim_archive//:LICENSE.txt` that was leading to compile error.\r\n\r\n@tatianashp , just FYI.\r\n\r\n@gunan , @chsigg, @whchung \r\n\r\nLink to discussion regarding `if_cuda` vs `if_cuda_is_configured` from another PR : https://github.com/tensorflow/tensorflow/pull/26753#discussion_r267022934\r\n\r\n\r\n", "comments": ["@chsigg.  thank you for getting the workaround merged...it will help us make progress with upstreaming our code.  Let me know if / when you want us to try out the actual solution once it becomes available. There are quite a few uses of `if_rocm_is_configured` and I would like to remove them all, given that the `_is_configured` variant will be deprecated sooner or later.\r\n\r\ndon't know why this PR is still showing as \"Open\". Assuming you will take care of that part.\r\n\r\nthanks again\r\n\r\ndeven\r\n", "Thanks Deven, I'm working on simplifying the if_cuda/rocm* situation. My current plan:\r\n\r\n- `if_*_is_configured()` should refer to whether the gpu libraries are available. Searching for CUDA libraries will be enabled through `--config=using_cuda`, which has the effect of `--define=using_cuda=true` and `--action_env=TF_NEED_CUDA=1`. The former is the standard way to pick among `config_settings`, and I would like `if_cuda_is_configured()` to return a select statement (to match the internal implementation, which returns a select distinguishing between platforms where CUDA is available). The latter is the only way to affect a repository_rule. ROCm should have an equivalent setup, and I want to add an `if_gpu_is_configured(if_cuda, if_rocm, otherwise)` macro. \r\n\r\n- `if_*()` should refer to whether we build op kernels gpus (this excludes XLA, which should only rely on the mechanics above). CUDA op kernels are enabled through `--config=cuda`, which again triggers a define (`--define=using_cuda_nvcc=true` or `--define=using_cuda_clang=true`) which triggers a `config_setting` which is used in a `select` statement. This is already the case today. Again, same for ROCm and add an `if_gpu(if_cuda, if_rocm, otherwise)`. \r\n\r\nNot sure why the PR still shows as open, I'm closing it manually."]}, {"number": 28115, "title": "compute_output_shape may return incorrect results due to broken cache implementation", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nDocker image tensorflow/tensorflow:1.12.0-gpu-py3 running under Host on Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nDocker image tensorflow/tensorflow:1.12.0-gpu-py3\r\n- TensorFlow version (use command below):\r\ntensorflow-gpu      1.12.0   \r\n- Python version:\r\n3.5.2\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n9.0 / 7\r\n- GPU model and memory:\r\nGTX 1080\r\n\r\n**Describe the current behavior**\r\nWhen called repeatedly, compute_output_shape may return incorrect results due to a faulty caching implementation in keras.Network\r\n\r\n**Describe the expected behavior**\r\ncompute_output_shape should always return correct results\r\n\r\n**Code to reproduce the issue**\r\nThe issue is traced to [this](https://github.com/tensorflow/tensorflow/blob/d7cd03f1398cd33fc8579ce336b602350ac99259/tensorflow/python/keras/engine/network.py#L902) line, where the function generic_utils.object_list_uid is used to generate a cache key. This function just generates a unique identifier for a tuple by taking the object ids of all the components, converting them to strings and concatenating. Firstly, this will not generate cache hits if the function is called twice with the same parameter values, because the object ids will not be the same even though their values are. Far more seriously though, it is possible for the function to sporadically generate  false hits, since object ids are only guaranteed to be unique for objects existing at the same time. \r\n\r\nThe bug is hidden when all dimensions are 256 or lower, because of the [special way](https://docs.python.org/3.5/c-api/long.html) that Python treats integer objects between -5 and 256. \r\n\r\nThe code below causes this to happen reliably on my machine. \r\n\r\nIf the maintainers agree with my diagnosis, I'll happily provide a pull request to fix it. Personally I think the cache should just be removed as it won't save a significant amount of computation in any sane code and has unbounded memory requirements in broken code, but if there are good reasons to retain it we can just key on the values of the tuple components, rather than their object ids.\r\n```\r\nimport tensorflow as tf\r\n\r\ninput = tf.keras.layers.Input(shape = [None,None,3])\r\noutput = tf.keras.layers.Conv2D(filters=1,kernel_size=[1,1])(input)\r\nmodel = tf.keras.Model(inputs=input,outputs=output)\r\n# The model consists of a single 1x1 convolution. Therefore the spatial extent of the output is always trivially equal to\r\n# the spatial extent of the input.\r\n\r\n# We must start from 257 because, when creating ints between -5 and 256 Python returns references to singletons.\r\n# Therefore there is a one-to-one mapping between int value and object-id in this range and the cache works as\r\n# intended. Outside this range, no such guarantee is possible and the cache fails.\r\nfor x in range(257,300):\r\n    shape=model.compute_output_shape([[1,x,x,3]])\r\n    assert (shape[1]==x)\r\n```", "comments": ["I can reproduce the issue with TF1.12.0 and TF1.13.1\r\nHere is the error log . Thanks!\r\n\r\n```\r\n257\r\n258\r\n259\r\n260\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-2-ade9f8af85b4> in <module>()\r\n     13   print(x)\r\n     14   shape=model.compute_output_shape([[1,x,x,3]])\r\n---> 15   assert (shape[1]==x)\r\n\r\nAssertionError: \r\n```", "Are there any updates on this issue?\r\nI am experiencing it on Windows with TF1.14.1 and it happens with even small number of well!\r\n\r\nExample:\r\n```\r\nfor x in range(300):\r\n    print(x)\r\n    shape=model.compute_output_shape([[1,x,x,3]])\r\n    assert (shape[1]==x)\r\n    \r\n0\r\n1\r\nTraceback (most recent call last):\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-10-6b87c5a54df0>\", line 4, in <module>\r\n    assert (shape[1]==x)\r\nAssertionError\r\n\r\n```\r\n\r\nEvery help would be highly appreciated! Thanks!", "@jjnaude I can reproduce the issue with TF1.x but I don't think there will be any code changes in TF1.x unless there is any security related updates.\r\n\r\nI cannot reproduce the issue in recent `tf-nightly`. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/50647092d6edb5cc729882d23c16c92f/untitled25.ipynb). Thanks!\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28115\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28115\">No</a>\n"]}, {"number": 28114, "title": "For ConcatV2, `axis` really require int64 support?", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04, 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary):  source\r\n- TensorFlow version (use command below): 0.12.0\r\n- Python version: 2.7.15rc1\r\n- Bazel version (if compiling from source): 0.15.0\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\nThe `testConcatAxisType` test in `concat_op_test` is failing on an assertion for s390x:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/87c06bad5aec2f21ec96b7253de551e8/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/kernel_tests/concat_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/concat_op_test.py\", line 621, in testConcatAxisType\r\n    self.assertEqual([2, 6], c.get_shape().as_list())\r\nAssertionError: Lists differ: [2, 6] != [4, 3]\r\n```\r\n\r\n**Describe the expected behavior**\r\nAssert values should match.\r\n\r\n**Code to reproduce the issue**\r\nOn Intel:\r\n```\r\n>>> from __future__ import absolute_import\r\n>>> from __future__ import division\r\n>>> from __future__ import print_function\r\n>>> import numpy as np\r\n>>> import tensorflow as tf\r\n>>> from tensorflow.python.framework import constant_op\r\n>>> from tensorflow.python.framework import dtypes\r\n>>> from tensorflow.python.ops import gen_array_ops\r\n>>>\r\n>>> t1 = [[1, 2, 3], [4, 5, 6]]\r\n>>> t2 = [[7, 8, 9], [10, 11, 12]]\r\n>>>\r\n>>> c = constant_op.constant(1, dtype=dtypes.int32)\r\n>>> z = gen_array_ops.concat_v2([t1, t2], c)\r\n>>> z\r\n<tf.Tensor 'ConcatV2_2:0' shape=(2, 6) dtype=int32>\r\n>>>\r\n>>> c = constant_op.constant(1, dtype=dtypes.int64)\r\n>>> z = gen_array_ops.concat_v2([t1, t2], c)\r\n>>> z\r\n<tf.Tensor 'ConcatV2_3:0' shape=(2, 6) dtype=int32>\r\n>>>\r\n```\r\n\r\nOn s390x:\r\n```\r\n>>> from __future__ import absolute_import\r\n>>> from __future__ import division\r\n>>> from __future__ import print_function\r\n>>> import numpy as np\r\n>>> import tensorflow as tf\r\n>>> from tensorflow.python.framework import constant_op\r\n>>> from tensorflow.python.framework import dtypes\r\n>>> from tensorflow.python.ops import gen_array_ops\r\n>>>\r\n>>> t1 = [[1, 2, 3], [4, 5, 6]]\r\n>>> t2 = [[7, 8, 9], [10, 11, 12]]\r\n>>>\r\n>>> c = constant_op.constant(1, dtype=dtypes.int32)\r\n>>> z = gen_array_ops.concat_v2([t1, t2], c)\r\n>>> z\r\n<tf.Tensor 'ConcatV2_2:0' shape=(2, 6) dtype=int32>\r\n>>>\r\n>>> c = constant_op.constant(1, dtype=dtypes.int64)\r\n>>> z = gen_array_ops.concat_v2([t1, t2], c)\r\n>>> z\r\n<tf.Tensor 'ConcatV2_3:0' shape=(4, 3) dtype=int32>\r\n>>>\r\n```\r\n\r\n**Other info / logs**\r\nFrom above logs we can see that when we pass a constant (axis) of int64 type to `ConcatV2`, we get different results on s390x which causes assertion failure. \r\n\r\nAfter searching the historical records, I found this PR #14251 which has introduced this test case. However when I scanned the TensorFlow code for checking how the `concat` operations work, I found that it deals mostly with int32 type.\r\n\r\nGetting different results on s390x for 32 Vs 64 bit data types is nothing new. We have handled couple of cases in issue #11431 and #14017 and it happens because of NumPy's different behavior on Intel Vs Z (PR #12963) . As my code understanding with `concat` is different, I am curious to understand if we really need to process int64 type of axis (is it a real time use case?) or its just added to fill the gap between `ConcatV2` supported types Vs its kernel implementation?\r\nI might be wrong but want to be sure that I proceed to find fix for the right thing. Please check and confirm.", "comments": ["@mrry @aselle do you have any ideas?", "I think this was fixed by the change to concat_op.cc in https://github.com/tensorflow/tensorflow/commit/2d0531d72c7dcbb0e149cafdd3a16ee8c3ff357a, which added [these lines](https://github.com/tensorflow/tensorflow/blame/f4815551dc19b2149a70f20dba5a6c043e627a5b/tensorflow/core/kernels/concat_op.cc#L65-L87). That change isn't part of TensorFlow 0.12.0, but it looks like it became available with 1.8.0.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28114\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28114\">No</a>\n", "@mrry The concat_op_test is failing for the architecture I am working on and we are hitting the same numpy bug which we encountered in [#12963](https://github.com/tensorflow/tensorflow/pull/12963). In this issue I just wanted to check if we can skip int64 support for my architecture if its not that critical.\r\n\r\nIf I change the dtype to int32 in [make_tensor_proto](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/tensor_util.py#L376), the test passes for us. I need to add following code after [line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/tensor_util.py#L485):\r\n```\r\n  if nparray.dtype == np.int64:\r\n    nparray = np.array(values, dtype=np.int32)\r\n    dtype = dtypes.int32\r\n```\r\nPlease suggest if you are aware of any better approach to achieve this. Thanks in advance :)"]}, {"number": 28113, "title": "TF 2.0.0-alpha0 doesn't build on FreeBSD: error in hwloc", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): FreeBSD 12.0\r\n- Bazel version (if compiling from source): 0.23.0\r\n- GCC/Compiler version (if compiling from source): Clang 8.0.0\r\n\r\nI'n trying to build TensorFlow from source on FreeBSD 12. During build the following problem shows up:\r\n\r\n```\r\nERROR: /usr/home/arr/.cache/bazel/_bazel_arr/5d34786d346adb404a81c63780107a71/external/hwloc/BUILD.bazel:212:1: C++ compilation of rule '@hwloc//:hwloc' failed (Exit 1)\r\nexternal/hwloc/hwloc/topology-linux.c:39:10: fatal error: 'mntent.h' file not found\r\n#include <mntent.h>\r\n         ^~~~~~~~~~\r\n1 error generated.\r\n```\r\n\r\nThis is because `topology-linux.c` is being compiled instead of `topology-freebsd.c`. I have zero knowledge in Bazel, so I wasn't able to include on of these files based on the condition.\r\n\r\nThis should be easy to fix for anyone experienced with Bazel.", "comments": ["An easy fix is to skip building `hwloc`. With https://github.com/tensorflow/tensorflow/pull/25471 and some minor additional changes (https://github.com/freedomtan/tensorflow/commit/ffd86f605bfdbe6f209c1fa707089d6010f0b719), I can build tensorflow master on FreeBSD.", "Ping @gunan; this might not on your radar but in case we have contributors you could offer some advice.", "I managed to build TF 2.0.0-alpha with hwloc enabled, however I had to apply a [handful of hacks](http://arrowd.name/tf.patch). All of them a pretty simple, it would be great if someone push them upstream.", "@arrowd clean up your patches and send pull request then. But I think some of the hacks are in #25471, so you may wanna check it first.", "@arrowd : Did you get the chance to look into @freedomtan's suggestion ? Is this still an issue. Please let us know. Thanks!", "@freedomtan 's branch contains namy changes that I also have. I'll merge them with my patches and create a new pull request.", "That's great. Shall be looking forward to it. Thanks!", "I got blocked by TF not supporting Bazel 0.25. Is there plans to allow building using new Bazel version?", "It is funny that I get both these errors:\r\n\r\n>You have bazel 0.23.0 installed.\r\n>Please upgrade your bazel installation to version 0.24.1 or higher to build TensorFlow!\r\n\r\n>You have bazel 0.25.0 installed.\r\n>Please downgrade your bazel installation to version 0.24.1 or lower to build TensorFlow!", "I need some help with Bazel. How do I implement following changes without braking Linux case?\r\n\r\n```\r\n--- third_party/hwloc/BUILD.bazel.orig\t2019-04-25 07:56:49 UTC\r\n+++ third_party/hwloc/BUILD.bazel\r\n@@ -36,7 +36,6 @@ template_rule(\r\n         \"#undef HWLOC_VERSION_RELEASE\": \"#define HWLOC_VERSION_RELEASE 3\",\r\n         \"#undef HWLOC_VERSION_GREEK\": \"#define HWLOC_VERSION_GREEK \\\"\\\"\",\r\n         \"#undef HWLOC_VERSION\": \"#define HWLOC_VERSION \\\"2.0.3\\\"\",\r\n-        \"#undef HWLOC_LINUX_SYS\": \"#define HWLOC_LINUX_SYS 1\",\r\n         \"#undef hwloc_pid_t\": \"#define hwloc_pid_t pid_t\",\r\n         \"#undef hwloc_thread_t\": \"#define hwloc_thread_t pthread_t\",\r\n         \"#  undef HWLOC_HAVE_STDINT_H\": \"#  define HWLOC_HAVE_STDINT_H 1 \",\r\n@@ -86,7 +85,6 @@ _INCLUDE_PRIVATE_HWLOC_AUTOIGEN_CONFIG_H_COMMON_SUBS =\r\n     \"#undef HAVE_NL_LANGINFO\": \"#define HAVE_NL_LANGINFO 1\",\r\n     \"#undef HAVE_OPENAT\": \"#define HAVE_OPENAT 1\",\r\n     \"#undef HAVE_POSIX_MEMALIGN\": \"#define HAVE_POSIX_MEMALIGN 1\",\r\n-    \"#undef HAVE_PROGRAM_INVOCATION_NAME\": \"#define HAVE_PROGRAM_INVOCATION_NAME 1\",\r\n     \"#undef HAVE_PTHREAD_T\": \"#define HAVE_PTHREAD_T 1\",\r\n     \"#undef HAVE_PUTWC\": \"#define HAVE_PUTWC 1\",\r\n     \"#undef HAVE_SETLOCALE\": \"#define HAVE_SETLOCALE 1\",\r\n@@ -149,7 +147,6 @@ _INCLUDE_PRIVATE_HWLOC_AUTOIGEN_CONFIG_H_COMMON_SUBS =\r\n     \"#undef HWLOC_HAVE_SYSCALL\": \"#define HWLOC_HAVE_SYSCALL 1\",\r\n     \"#undef HWLOC_HAVE_X11_KEYSYM\": \"#define HWLOC_HAVE_X11_KEYSYM 1\",\r\n     \"#undef HWLOC_HAVE_X86_CPUID\": \"#define HWLOC_HAVE_X86_CPUID 1\",\r\n-    \"#undef HWLOC_LINUX_SYS\": \"#define HWLOC_LINUX_SYS 1\",\r\n     \"#undef HWLOC_SIZEOF_UNSIGNED_INT\": \"#define HWLOC_SIZEOF_UNSIGNED_INT 4\",\r\n     \"#undef HWLOC_SIZEOF_UNSIGNED_LONG\": \"#define HWLOC_SIZEOF_UNSIGNED_LONG 8\",\r\n     \"#undef HWLOC_SYM_PREFIX\": \"#define HWLOC_SYM_PREFIX hwloc_\",\r\n@@ -224,13 +221,12 @@ cc_library(\r\n         \"hwloc/static-components.h\",\r\n         \"hwloc/topology.c\",\r\n         \"hwloc/topology-hardwired.c\",\r\n-        \"hwloc/topology-linux.c\",\r\n+        \"hwloc/topology-freebsd.c\",\r\n         \"hwloc/topology-noos.c\",\r\n         \"hwloc/topology-synthetic.c\",\r\n         \"hwloc/topology-xml.c\",\r\n         \"hwloc/topology-xml-nolibxml.c\",\r\n         \"hwloc/traversal.c\",\r\n-        \"include/hwloc/linux.h\",\r\n         \"include/hwloc/plugins.h\",\r\n         \"include/hwloc/shmem.h\",\r\n         \"include/private/autogen/config.h\",\r\n```", "@arrowd : Can we open a new issue (bug/feature) if it is not related to the issue at hand. We would be happy to help on that and also it would be very easy for us to track. Meanwhile can we close this since it is resolved. Let us know. Thanks!   ", "@arrowd if you haven't figured out how to do it yet, check bazel's [select](https://docs.bazel.build/versions/master/be/functions.html#select). See TensorFlow's [BUILD.bazel](https://github.com/tensorflow/tensorflow/blob/master/third_party/aws/BUILD.bazel) of AWS SDK for example usage of `select`.", "@arrowd :  Let us know if that helps. Thanks!", "This is mostly fixed by https://github.com/tensorflow/tensorflow/pull/28689"]}, {"number": 28112, "title": "TF 2.0 can't import anymore on Windows TypeError", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 1809\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.0.0.dev20190420\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: rtx 2060 6gb\r\n\r\n**Describe the current behavior**\r\nI get the error below when I try to import tensorflow on Windows 10. Also, I don't know if it is related but I upgraded my gpu. Tf 2.0 was working fine when I last tried 3 weeks ago with my old gpu.\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n`import tensorflow as tf`\r\n\r\n**Other info / logs**\r\nFile \"d:/pythonapps/next/test.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"D:\\pythonapps\\next\\env\\lib\\site-packages\\tensorflow\\__init__.py\", line 42, in <module>\r\n    from tensorflow._api.v2 import compat\r\n  File \"D:\\pythonapps\\next\\env\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\__init__.py\", line 21, in <module>\r\n    from tensorflow._api.v2.compat import v1\r\n  File \"D:\\pythonapps\\next\\env\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\__init__.py\", line 643, in <module>\r\n    'tensorflow_estimator.python.estimator.api._v1.estimator'))\r\n  File \"D:\\pythonapps\\next\\env\\lib\\site-packages\\tensorflow\\python\\tools\\component_api_helper.py\", line 56, in package_hook\r\n    child_pkg = importlib.import_module(child_package_str)\r\n  File \"C:\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"D:\\pythonapps\\next\\env\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\api\\_v1\\estimator\\__init__.py\", line 8, in <module>\r\n    from tensorflow_estimator.python.estimator.api._v1.estimator import experimental\r\n  File \"D:\\pythonapps\\next\\env\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\api\\_v1\\estimator\\experimental\\__init__.py\", line 29, in <module>\r\n    _sys.modules[__name__], \"estimator.experimental\")\r\nTypeError: __init__() missing 1 required positional argument: 'deprecated_to_canonical'", "comments": ["I was on an old preview build, upgraded to alpha and problem solved."]}, {"number": 28111, "title": "Issue for tf.keras.layers.DenseFeatures when model.build is called", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, my own example code.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary, from tensorflow/tensorflow:2.0.0a0-gpu-py3 docker image.\r\n- TensorFlow version (use command below): 2.0.0a0\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\ntf.keras.layers.DenseFeatures is a new keras layer added in tf2.0a0. This is the first layer for tf.keras.Model that accepts dict of tensors as the input.\r\n\r\nIf tf.keras.layers.DenseFeatures is used as the first layer in a keras model created by subclass tf.keras.Model,  model.build(input_shape) will fail with any type of input_shape.\r\n\r\n**Describe the expected behavior**\r\nmodel.build(input_shape) will succeed, and corresponding model variables are created.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nfeature_columns = [tf.feature_column.numeric_column(header) for header in ['c1', 'c2']]\r\n\r\nclass TestModel(tf.keras.Model):\r\n    def __init__(self, feature_columns):\r\n        super(TestModel, self).__init__()\r\n        self.feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\r\n        self.dense_layer = tf.keras.layers.Dense(8)\r\n\r\n    def call(self, inputs):\r\n        x = self.feature_layer(inputs)\r\n        return self.dense_layer(x)\r\n\r\nmodel = TestModel(feature_columns=feature_columns)\r\nshape = {'c1': (1,1), 'c2': (1,1)}\r\n# change to:\r\n# shape = [(1,1), (1, 1)]\r\n# also fails.\r\nmodel.build(shape)\r\nprint(model.trainable_variables)\r\n```\r\n\r\n**Other info / logs**\r\nhttps://github.com/skydoorkai/tests_and_issues/tree/master/tensorflow/densefeature\r\nprovides with the repro Dockerfile, and a possible solution to it: Network.build(input_shape) accepts dict as the input.", "comments": ["Seems like this is closely related to #27416 where the issue boils down to not being able to use DenseFeatures in a functional style? ", "@skydoorkai The approach referenced above looks good to me. Do you want to make a PR? If not I will submit a fix", "This issue exists with latest tf 2.0 nightly version '2.0.0-dev20190906'", "@skydoorkai On the other hand, why do you want to build it? An alternative way is to call the model:\r\nmodel(some dictionary of numpy inputs or keras.Input)", "@skydoorkai As mentioned by @tanzhenyu calling the model works. Please check the gist [here](https://colab.sandbox.google.com/gist/jvishnuvardhan/d7f73dae8cd44241cb7e83f478f0096f/untitled853.ipynb). Thanks!\r\n\r\nPlease close the issue if this was resolved for you. Thanks!", "I am closing the issue as it was resolved. Please feel free to reopen if the issue persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28111\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28111\">No</a>\n"]}, {"number": 28110, "title": "[TF 2.0] TF 2.0 consumes twice as much memory as TF 1.x or CNTK.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64 1809\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip install tensorflow-gpu==2.0.0-alpha0\r\n- TensorFlow version (use command below): v1.12.0-9492-g2c319fb415 2.0.0-alpha0\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 10.0, cudnn-10.0-windows10-x64-v7.5.0.56\r\n- GPU model and memory: GeForce GTX 1070 8GB\r\n\r\n**Describe the current behavior**\r\nEvaluating TF 2.0 keras model allocates twice as much memory as TF 1.x or CNTK.\r\n\r\n**Describe the expected behavior**\r\nMemory usage of TF 2.0 should be same or similar to other libraries, not double.\r\n\r\n**Code to reproduce the issue**\r\nFor TF 2.0 or 1.x\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# tf.config.gpu.set_per_process_memory_growth(True)\r\n\r\nsize = 28000\r\n\r\ninputs = tf.keras.Input((size,), dtype='float32')\r\noutputs = tf.keras.layers.Dense(size)(inputs)\r\nmodel = tf.keras.models.Model(inputs=inputs, outputs=outputs)\r\n\r\nmodel.predict(np.ones((1, size,), dtype=np.float32))\r\n\r\nprint('complete')\r\n\r\nwhile True:\r\n    pass\r\n```\r\nFor TF 1.x or CNTK with keras\r\n```\r\nimport keras\r\nimport numpy as np\r\n\r\nsize = 28000\r\n\r\ninputs = keras.Input((size,), dtype='float32')\r\noutputs = keras.layers.Dense(size)(inputs)\r\nmodel = keras.models.Model(inputs=inputs, outputs=outputs)\r\n\r\nmodel.predict(np.ones((1, size,), dtype=np.float32))\r\n\r\nprint('complete')\r\n\r\nwhile True:\r\n    pass\r\n```\r\nWith 8GB VRAM GPU, TF 1.x and CNTK works successfully, and TF 2.0 code are failed due to Resource exhausted exception.", "comments": ["TL;DR It's actually a somewhat niche case (a single weight is `O(gpu memory size)`), but it's also a pretty easy fix.\r\n\r\nIn your example, the model is failing when it builds the kernel for the dense layer; or more specifically, when it calls the initializer. If you consider a much simpler example:\r\n\r\n```\r\na = tf.ones((size, size))\r\nb = a + 1\r\nc = b + 1\r\n```\r\nIn graph mode when you evaluate `c` (which is a symbolic tensor), TensorFlow's memory allocator will reuse blocks of memory when it detects that they are no longer being used, so by the time `c` is computed the buffer that backed `a` is long gone. By contrast, in eager a handle to all three tensors co-exist in the body, so the runtime is not allowed to free them until they go out of scope on the python side. (Since a user could presumably add other computations later, unlike the graph case where all computations are known.)\r\n\r\nFortunately, wrapping the initializer call in a `tf.function` fixes the issue. In general I suspect more and more of these utility functions are going to get function'd for precisely this sort of reason.", "@robieta Thanks for investigation. Can you provide sample code of tf.function usage? What is the initializer?\r\n\r\nAlso, my actual code is big CNN network with TF 2.0 keras API and has manual training loop by using tf.data.Dataset and GradientTape, samely suffer from this memory and performance issue. If you provide sample code of the usage of tf.function for keras model and manual training loop, it will be very helpful.", "For the case I described:\r\n```\r\nsize = 40000  # Slightly larger b/c I was testing on a GPU w/ 16 GB\r\ninit_fn = tf.keras.initializers.glorot_uniform()\r\ninit_fn = tf.function(init_fn, autograph=False) # <== This is what prevents the OOM (Comment it out to test)\r\nlayer = tf.keras.layers.Dense(size, kernel_initializer=init_fn)\r\nlayer.build((size,))\r\n```\r\n\r\nAnd my plan is to make builtin initializers do the tf.function wrap automatically.\r\n\r\nIn general functions are documented in https://www.tensorflow.org/alpha/tutorials/eager/tf_function, https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/function", "Thanks for sample code! But I have one more question, how about \"gamma_initializer\" for BatchNormailization layer? its initializer is a Class (Ones or Zeros), not method so I can't wrap it with tf.function.", "glorot_uniform is also a class. (When you trace through it's just the `init_ops.GlorotUniform` class). That's why we call it on the second line. So tf.function is actually wrapping `__call__` of that particular class instance. So Ones or Zeros should work the same.", "Oh, I mistaked that using \"ones\" instead of \"Ones\". Using Ones() works for gamma_initializer.", "> And my plan is to make builtin initializers do the tf.function wrap automatically.\r\n\r\n@robieta You mean that this temporary fix will not be needed in official TF 2.0 release?", "Also, I found that TF 2.0 requires more memory than CNTK for training. In example, completly same model (same structure, same # of trainable_variable) with same PC, cntk can train the model with x2 batch size. It may not be related this issue but disturb migrating from cntk to TF 2.0.", "Using GradientTape for manual training loop requires double GPU ram. I found that using Model.train_on_batch() instead of GradientTape is more fast and comsume less GPU ram. Wrapping entire train loop by using tf.function makes GradientTape to be fast as train_on_batch(), but memory usage does not changed. \r\n\r\nAlso, train_on_batch() has another issue that allocate CPU memory infinity without tf.keras.backend.set_learning_phase(1). I'll open as new issue for this when I found minimul reproducible code.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28110\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28110\">No</a>\n", "> > And my plan is to make builtin initializers do the tf.function wrap automatically.\r\n> \r\n> @robieta You mean that this temporary fix will not be needed in official TF 2.0 release?\r\n\r\nI'm not sure if it will make it into the next release, but yes that's the plan.", "I don't train. only loading the trained model takes more than twice memory.\r\n\r\nI have 2 environments. tf 1.14 installed for python 3.5 and tf 2 for python 3.7.\r\n\r\nI run the same code and load the same keras model in both environments. in py3.5, after model loaded, it takes 560MB memory. while in py3.7, it takes 1.2GB.\r\n\r\nis there any way to reduce the memory for tf 2 ? ", "Same problem here. Loading the model it's consuming a lot of memory. Some solution was found? "]}]