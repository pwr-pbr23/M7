[{"number": 27464, "title": "[tflite] add GPU Delegate to label_image on Android", "body": "`label_image -g 1 -m your_floating_point_model` will be delegated to GL GPU delegate on Android", "comments": ["@freedomtan Any update please ?", "@gbaned what kind of updates?", "> @gbaned what kind of updates?\r\n\r\n@freedomtan my apologies.", "@miaout17 Hi, Could you PTAL and approve.", "@freedomtan could you please resolve the conflicts? Thanks!", "@gbaned rebased.", "Nagging Reviewer @miaout17, @jianlijianli: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "Sorry for the delay. Load-balancing to @srjoglekar246 -- could you review delegate-related changes?", "addressing last comments, fixed non-Android platform problem, rebased.", "merge to resolve conflicts with https://github.com/tensorflow/tensorflow/pull/30090"]}, {"number": 27463, "title": "[TF 2.0 API Docs] tf.keras.Sequential", "body": "**System information**\r\n- TensorFlow version: 2.0 alpha\r\n- Doc Link:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Sequential\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/sequential.py\r\n\r\n**Describe the documentation issue**\r\n\r\n**- Description:**\r\n\r\nThe order of _Properties_ and _Methods_ is alphabetical, probably by TF doc design. With a large doc for an arguably popular module such as `tf.keras.Sequential` navigating around it may be confusing to the user. Perhaps we should start with the most important ones e.g. `compile` and `fit` under Methods. Since `tf.keras.Sequential`, like many other modules/classes, expands on `keras.Sequential` it would still be logical to list certain Properties, Methods etc in the beginning (at the top of the page) in order of importance for better UX. See: https://keras.io/models/sequential/ as a good example where these are not in alphabetical order.\r\n\r\nAlso, in the beginning after a short intro (\"inherits from `Model`... a linear stack of layers\"), a link to \"Build a Simple Model\" (with `tf.keras.Sequential`) in TensorFlow would be cool for those who are new to `(tf.)Keras`/TF - https://www.tensorflow.org/alpha/guide/keras/overview#sequential_model. Also, the official Keras.io docs include a URL in https://keras.io/models/sequential/ to \"Getting started with the Keras Sequential model\": https://keras.io/getting-started/sequential-model-guide.\r\n\r\n**- Examples:**\r\n\r\nThere already is an example right in the beginning. Maybe adding adding a separate link to or copying an example from this Sequential notebook would be cool too - https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/quickstart/beginner.ipynb (source: https://www.tensorflow.org/alpha/guide/keras/overview#sequential_model)\r\n\r\n**- Parameters**\r\n\r\nInconsistent. Some reformatting may be needed e.g. input_shape -> `input_shape`. \r\nAlso, both _Arguments_ and _Args_ are used throughout the doc.\r\n\r\n**- Returns, Raises:**\r\n\r\nSometimes exist, sometimes not - creates a UX issue because of inconsistency.\r\n\r\n**- Visuals:**\r\n\r\nA simple one similar to https://www.tensorflow.org/alpha/guide/keras/functional would be appreciated.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\nFor sure.", "comments": ["I will love to work on the issue", "We are linking off to guides now: https://www.tensorflow.org/api_docs/python/tf/keras/Sequential?version=nightly\r\n\r\nRegarding the ordering, this is being tracked internally and being worked on. Closing this for now. Please reopen if something is not fixed :)"]}, {"number": 27462, "title": "[tflite] make metal delegate code build", "body": "changes to build libtflite_gpu_metal.so and libmetal_delegate.a.\r\n\r\n```\r\nbazel build --apple_platform_type ios --cpu=ios_arm64 --cxxopt=-std=c++14 \\\r\ntensorflow/lite/delegates/gpu:libtflite_gpu_metal.so\r\n```\r\nand\r\n```\r\nbazel build --apple_platform_type ios --cpu=ios_arm64 --cxxopt=-std=c++14 \\\r\ntensorflow/lite/delegates/gpu:metal_delegate\r\n```\r\nbuild.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/27325", "comments": ["rebased because `LOG(INFO)` in `tensorflow/lite/delegates/gpu/metal/api.cc` is no longer there.", "@freedomtan \r\n\r\nThanks for the PR, but I just learned that this must be solved at a different level.  I will decline this PR and fix it internally.  I will update this issue once it's pushed out.", "Closing this PR as this will be handled internally. @impjdi please update this issue once it is done #27325", "it seems it's fixed in 088e9ef4b6b23766cde2e9e20fa1a007dbdcc211", "@freedomtan \r\n\r\nYupp, it was committed earlier, before I headed home ;)  I reconfigured our publishing tools to watch out for objc files depending on ABSL.  Thanks for bringing that to our attention!", "@impjdi any plan to enable Metal backend on macOS? None of my Mac machines have CDUA, so the Metal backend could help a bit. Actually, with some hacks, I managed to modify some existing command line programs, e.g., `label_image` and `benchmark_model`, to use Metal on macOS. ", "@freedomtan\r\n\r\nFrom our metal engineer, I know it can be easily extended to be run on MacOS, but we are pretty short staffed and can't prioritize that at the moment.  It's not just getting the plumbing done but also the testing that cannot be easily automated in our current infrastructure.  In near future, we will focus on improving performance, adding ops, supporting batching etc. \r\n\r\nMaybe in some future? ;)", "> @freedomtan\r\n> \r\n> From our metal engineer, I know it can be easily extended to be run on MacOS, but we are pretty short staffed and can't prioritize that at the moment. It's not just getting the plumbing done but also the testing that cannot be easily automated in our current infrastructure. In near future, we will focus on improving performance, adding ops, supporting batching etc.\r\n> \r\n> Maybe in some future? ;)\r\n\r\nAre we in the future yet? \r\n\r\nThe idea of running TFLite on Metal on macOS could solve an issue i have\r\n\r\nSam"]}, {"number": 27461, "title": "[TF 2.0 API Docs] tf.keras.Model", "body": "**System information**\r\n- TensorFlow version: 2.0 alpha\r\n- Doc Link: \r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training.py\r\n\r\n**Describe the documentation issue**\r\n\r\n**- Links:**\r\n\r\nA link to **_Keras Functional API in TensorFlow_** would be appreciated for those who are new to `(tf.)Keras` and TensorFlow 1.x and 2.0 - https://www.tensorflow.org/alpha/guide/keras/functional. E.g. See this Keras Model class API doc: https://keras.io/models/model/#model-class-api.\r\n\r\n**- Description:**\r\n\r\nTaking into account this is a 'heavy' module it may not be easy to write good docs for `tf.keras.Model`. Since `tf.keras` is a crucial API to TF 2.0, so let's make the docs a delight to read. The current documentation for `tf.keras.Model` is a bit incomprehensible for a novice or experienced user.\r\n\r\nIn terms of user experience for someone who is new or not new to TF, maybe the description should be improved and follow the keras.io docs more closely. For instance, arguments under `compile` were apparently copy-pasted from `keras.Model` docs (https://keras.io/models/model/). However, you have to scroll all the way down to see them here: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model#compile (_update: maybe because Methods etc are listed in alphabetical order which is not intuitive_).\r\n\r\nThe description at keras.io is neater and more organized imo. It starts with a short description and jumps to args from `compile`. I'd suggest we follow the same structure.\r\n\r\nAlso, as in https://keras.io/models/model/, we should include a link to the guide to **_Keras Functional API in TensorFlow_** - https://www.tensorflow.org/alpha/guide/keras/functional - which is quite well written.\r\n\r\n**- Examples:**\r\n\r\nNot enough examples - they are mentioned here and there. See UX issues above under Description. Recommend to rewrite it to follow the original `keras.Model` module - https://keras.io/models/model/#model-class-api - along with examples. \r\n\r\n**- Parameters, Returns, Raises:**\r\n\r\nUX issues - see Description above.\r\n\r\n**- Visuals:**\r\n\r\nRecommend to add visuals from https://www.tensorflow.org/alpha/guide/keras/functional\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\nYes, let's make `tf.keras` docs awesome.", "comments": ["@8bitmp3 I want to work on this can you assign me this??", "@8bitmp3 I searched out into the docs repository but did'nt the file for tf.keras documentation can be please help me in finding the appropriate files.", "It's a known issue. This is the original: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras. But the `__init__` URL is broken - https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/api/_v2/keras/__init__.py lands you on a 404. @margaretmz has raised it here https://github.com/tensorflow/tensorflow/issues/26197\r\n\r\n> @8bitmp3 I searched out into the docs repository but did'nt the file for tf.keras documentation can be please help me in finding the appropriate files.\r\n\r\n", "@8bitmp3 I am working on it and will keep you updated\r\n", "@8bitmp3 Thanks for bringing this to our notice. We will take a look and resolve it. Thanks!", "@8bitmp3 I found out that the API docs will be generated with the help of api_generator we have to create a build_doc API with the help of api_generator API to create docs! but I didn't get how this would be done!", "Is this issue still valid?\r\nWe now automatically links in the 2.0 ref docs to places it's used in the guide and tutorials. See the second section here: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model", "> Is this issue still valid?\r\n> We now automatically links in the 2.0 ref docs to places it's used in the guide and tutorials. See the second section here: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model\r\n\r\n@lamberta Just noticed the new layout and links\u2014I think it looks awesome \ud83d\ude4c Thanks a bunch \ud83d\udc4d I shall close the issue"]}, {"number": 27460, "title": "Reloading trained model gives random predictions every time (tf.keras)", "body": "tensorflow-version: 1.13.1\r\nI have trained model using **tf.keras**. It has custom layers and is doing multi tasking. Below is code for same:\r\n\r\n```\r\ntrainable_model = keras.models.load_model(\r\n            path, custom_objects={'Attention': Attention, 'MultiTaskLoss': MultiTaskLoss}, compile=True) # tried with compile=False\r\n        trainable_model.summary()\r\n        input = trainable_model.inputs[0]\r\n        i_output = trainable_model.get_layer('i_output').output\r\n        t_output = trainable_model.get_layer('t_output').output\r\n        testable_model = keras.models.Model(\r\n            inputs=input, outputs=[i_output, t_output])\r\n       # Tried without below for loop.\r\n        for l in testable_model.layers:\r\n            l_name = l.name\r\n            l.set_weights(trainable_model.get_layer(name=l_name).get_weights())\r\n        testable_model.compile('adam', loss={\r\n                               'i_output': 'categorical_crossentropy', 't_output': 'categorical_crossentropy'})\r\n        return testable_model\r\n```\r\nBut when I predict using testable_model, predictions are different and random everytime, as if weights are totally different from trained model.\r\nModel summary is as follows:\r\n**Trainable model**:\r\n![trainable_model](https://user-images.githubusercontent.com/25479695/55475976-f200c100-5632-11e9-9819-2d0bee121744.png)\r\n\r\n**Testable_model**:\r\n![testable_model](https://user-images.githubusercontent.com/25479695/55475996-047afa80-5633-11e9-8723-2c03665411b8.png)\r\n\r\n\r\n\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n", "[Link to stack overflow](https://stackoverflow.com/questions/55494908/reloading-trained-model-gives-random-predictions-every-time-tf-keras)"]}, {"number": 27459, "title": "ImportError: /usr/lib/x86_64-linux-gnu/libstdc++", "body": "During handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"asrserver.py\", line 10, in <module>\r\n    import keras\r\n  File \"/usr/local/lib/python3.7/site-packages/keras/__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"/usr/local/lib/python3.7/site-packages/keras/utils/__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n  File \"/usr/local/lib/python3.7/site-packages/keras/utils/conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n  File \"/usr/local/lib/python3.7/site-packages/keras/backend/__init__.py\", line 89, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/local/lib/python3.7/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/local/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /usr/local/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["Please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. What were the steps you followed during installation?. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 27458, "title": "Not able to decode", "body": "Hi,\r\nI am trying to fit my own model in this app. When I speak into the app, it extracts & prints the MFCC features, but crashes afterwards giving the following error:\r\n\r\n04-03 12:56:16.754 24654-24815/org.tensorflow.demo E/TensorFlowInferenceInterface: Failed to run TensorFlow inference with inputs:[SeqLen], outputs:[SparseToDense]\r\n\r\n--------- beginning of crash\r\n04-03 12:56:16.755 24654-24815/org.tensorflow.demo E/AndroidRuntime: FATAL EXCEPTION: Thread-7556\r\nProcess: org.tensorflow.demo, PID: 24654\r\njava.lang.IllegalArgumentException: Expects arg[0] to be int32 but float is provided\r\nat org.tensorflow.Session.run(Native Method)\r\nat org.tensorflow.Session.access$100(Session.java:48)\r\nat org.tensorflow.Session$Runner.runHelper(Session.java:314)\r\nat org.tensorflow.Session$Runner.run(Session.java:264)\r\nat org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:228)\r\nat org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:197)\r\nat org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:187)\r\nat org.tensorflow.demo.SpeechActivity.recognize(SpeechActivity.java:229)\r\nat org.tensorflow.demo.SpeechActivity.access$100(SpeechActivity.java:48)\r\nat org.tensorflow.demo.SpeechActivity$3.run(SpeechActivity.java:193)\r\nat java.lang.Thread.run(Thread.java:818)\r\n\r\nCan someone please help me fix this? I'm not able to understand where that arg[0] is pointing too.\r\nIf anyone has a solution, please do share. Thanks in advance.", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n"]}, {"number": 27456, "title": "LazyAdamOptimizer  is not faster than AdamOptimizer ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux redhat\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nsource\r\n- TensorFlow version (use command below):\r\n1.12\r\n- Python version:\r\nPython 3.6.8 |Anaconda,\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nThere are 2.7 hundred million variables in my embedding files.And instread of adamOptimer,i used lazyadam.But it cost the same time as adamOptimizer\r\n**Describe the expected behavior**\r\nit should be faster than adamOptimizer when sparse update.\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nreturn tf.estimator.DNNClassifier(\r\n        model_dir=FLAGS.model_dir,\r\n        feature_columns=deep_columns,\r\n        #optimizer=tf.train.AdamOptimizer(learning_rate=0.001),\r\n        optimizer =  tf.contrib.opt.LazyAdamOptimizer(learning_rate = 0.001),\r\n        hidden_units=hidden_units,\r\n        config=my_checkpointing_config\r\n```\r\nand it cost 200s for every 100 steps ,the same time as AdamOptimizer\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I would like to work on this issue.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 27455, "title": "TF2.0 gradient problem of using tf.nn.relu in tf.keras.Model.", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.0.0-alpha0\r\n- Python version: 3.6.8\r\n\r\n**Describe the current behavior**\r\nI built a keras model with only a `tf.nn.relu`, but the gradient seems to be `None` after being decorated by `@tf.function`\r\n\r\n**Code to reproduce the issue**\r\n1. `tf.nn.relu` + `tf.keras.Model` + `@tf.function` (this is the only case that produce `None` gradient)\r\n```python\r\nimport tensorflow as tf\r\n\r\nz = tf.keras.Input(())\r\nh = tf.nn.relu(z)\r\nm = tf.keras.Model(z, h)\r\n\r\n@tf.function\r\ndef f(x):  # with @tf.function\r\n    with tf.GradientTape() as t:\r\n        t.watch(x)\r\n        z = m(x ** 2)\r\n    return t.gradient(z, x)\r\n\r\nprint(f(tf.convert_to_tensor(10.0)))\r\n\r\n>>> None\r\n```\r\n\r\n1.2 `tf.nn.relu` + `tf.keras.Model` without `@tf.function`\r\n```python\r\ndef f(x):  # without @tf.function\r\n    with tf.GradientTape() as t:\r\n        t.watch(x)\r\n        z = m(x ** 2)\r\n    return t.gradient(z, x)\r\n\r\nprint(f(tf.convert_to_tensor(10.0)))\r\n\r\n>>> tf.Tensor(20.0, shape=(), dtype=float32)\r\n```\r\n\r\n2. `tf.keras.layers.ReLU()` + `tf.keras.Model` + `@tf.function`\r\n```python\r\nimport tensorflow as tf\r\n\r\nz = tf.keras.Input(())\r\nh = tf.keras.layers.ReLU()(z)\r\nm = tf.keras.Model(z, h)\r\n\r\n@tf.function\r\ndef f(x):  # with @tf.function\r\n    with tf.GradientTape() as t:\r\n        t.watch(x)\r\n        z = m(x ** 2)\r\n    return t.gradient(z, x)\r\n\r\nprint(f(tf.convert_to_tensor(10.0)))\r\n\r\n>>> tf.Tensor(20.0, shape=(), dtype=float32)\r\n```\r\n\r\n2.2 `tf.keras.layers.ReLU()` + `tf.keras.Model` without `@tf.function`\r\n```python\r\ndef f(x):  # without @tf.function\r\n    with tf.GradientTape() as t:\r\n        t.watch(x)\r\n        z = m(x ** 2)\r\n    return t.gradient(z, x)\r\n\r\nprint(f(tf.convert_to_tensor(10.0)))\r\n\r\n>>> tf.Tensor(20.0, shape=(), dtype=float32)\r\n```\r\n\r\n3. only `tf.nn.relu`\r\n```python\r\nimport tensorflow as tf\r\nm = tf.nn.relu\r\n\r\n@tf.function\r\ndef f(x):  # with @tf.function\r\n    with tf.GradientTape() as t:\r\n        t.watch(x)\r\n        z = m(x ** 2)\r\n    return t.gradient(z, x)\r\n\r\nprint(f(tf.convert_to_tensor(10.0)))\r\n\r\n>>> tf.Tensor(20.0, shape=(), dtype=float32)\r\n```\r\n\r\nSo, I think its the problem between `tf.nn.relu` and `tf.keras.Model`? Besides, `tf.nn.tanh` has the same problem.", "comments": ["@tomerk I think something is broken with the keras graph here since the tape isn't seeing it.\r\n\r\nCan you take a look, or help triage to the right person?", "I have a fix for this that will be submitted soon.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27455\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27455\">No</a>\n"]}, {"number": 27454, "title": "grpc+seastar: add grpc+seastar protocol which uses Seastar as RPC for WorkerService", "body": "This PR serves as a placeholder for contribution from @liutongxuan and his colleagues in Alibaba. \r\n\r\nSince TF is going to have yet another (hopefully last) release before 2.0 (r1.14 to be cut on April 15), I am not sure if we have enough time (or incentive) to push this feature into the main repo. \r\n\r\n[tensorflow/networking](https://github.com/tensorflow/networking) should serve as its final target after 2.0. As we are still transitioning and @annarev is working on the networking C API, I would still like to submit this PR against the main repo, mainly for the convenience of reviewing. When we feel like ready, we shall have another PR in https://github.com/tensorflow/networking for refactoring it as a standalone plugin.\r\n\r\nPing @jbedorf @poxvoculi; if you have time feel free to join the reviewing process.", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27454) for more info**.\n\n<!-- need_author_consent -->", "CLA Agree", "CLA Agree", "@byronyi Please sign CLA in order to proceed with this PR. Thank you.", "@gbaned Look closer; I did signed CLA and the commit author did, too. You need to manually confirm the CLA status though, as we have coauthors for this commit who commented above.", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27454) for more info**.\n\n<!-- cla_yes -->", "CLA Agree", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27454) for more info**.\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27454) for more info**.\n\n<!-- cla_yes -->", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27454) for more info**.\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27454) for more info**.\n\n<!-- cla_yes -->", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27454) for more info**.\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27454) for more info**.\n\n<!-- cla_yes -->", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27454) for more info**.\n\n<!-- need_author_consent -->", "@byronyi please resolve conflicts", "Let me know if this change is ready to take a look again.", "> @byronyi please resolve conflicts\r\n\r\nconflicts fixed.", "grpc+seastar protocol can not be identified.\r\n```\r\nTraceback (most recent call last):\r\n  File \"tf.py\", line 55, in <module>\r\n    server = tf.train.Server(cluster, protocol=\"grpc+seastar\", job_name=job_name, task_index=task_index, config=config)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/server_lib.py\", line 149, in __init__\r\n    self._server = c_api.TF_NewServer(self._server_def.SerializeToString())\r\ntensorflow.python.framework.errors_impl.NotFoundError: No server factory registered for the given ServerDef: cluster {\r\n  job {\r\n    name: \"ps\"\r\n    tasks {\r\n      key: 0\r\n      value: \"node1:31152\"\r\n    }\r\n    tasks {\r\n      key: 1\r\n      value: \"node1:26427\"\r\n    }\r\n  }\r\n  job {\r\n    name: \"worker\"\r\n    tasks {\r\n      key: 0\r\n      value: \"node2:8122\"\r\n    }\r\n    tasks {\r\n      key: 1\r\n      value: \"node2:27213\"\r\n    }\r\n  }\r\n}\r\njob_name: \"worker\"\r\ndefault_session_config {\r\n  intra_op_parallelism_threads: 40\r\n  inter_op_parallelism_threads: 40\r\n}\r\nprotocol: \"grpc+seastar\"\r\n\r\nThe available server factories are: [ GRPC_SERVER ]\r\n```", "> grpc+seastar protocol can not be identified.\r\n> \r\n> ```\r\n> Traceback (most recent call last):\r\n>   File \"tf.py\", line 55, in <module>\r\n>     server = tf.train.Server(cluster, protocol=\"grpc+seastar\", job_name=job_name, task_index=task_index, config=config)\r\n>   File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/server_lib.py\", line 149, in __init__\r\n>     self._server = c_api.TF_NewServer(self._server_def.SerializeToString())\r\n> tensorflow.python.framework.errors_impl.NotFoundError: No server factory registered for the given ServerDef: cluster {\r\n>   job {\r\n>     name: \"ps\"\r\n>     tasks {\r\n>       key: 0\r\n>       value: \"node1:31152\"\r\n>     }\r\n>     tasks {\r\n>       key: 1\r\n>       value: \"node1:26427\"\r\n>     }\r\n>   }\r\n>   job {\r\n>     name: \"worker\"\r\n>     tasks {\r\n>       key: 0\r\n>       value: \"node2:8122\"\r\n>     }\r\n>     tasks {\r\n>       key: 1\r\n>       value: \"node2:27213\"\r\n>     }\r\n>   }\r\n> }\r\n> job_name: \"worker\"\r\n> default_session_config {\r\n>   intra_op_parallelism_threads: 40\r\n>   inter_op_parallelism_threads: 40\r\n> }\r\n> protocol: \"grpc+seastar\"\r\n> \r\n> The available server factories are: [ GRPC_SERVER ]\r\n> ```\r\n\r\nPlease manually add  \"build:seastar --define with_seastar_support=true\" in .tf_configure.bazelrc then build command \"bazel build --config=opt --config=seastar //tensorflow/tools/pip_package:build_pip_package\", then you could succefully build up tensorflow with seastar. \r\nThere's left conflicts are that seastar and tensorflow both use hwloc which conflict with each other.", "@gunan added `@hwloc` third-party package and @liutongxuan you could seek help from him.", "Load endpoint map file failed.\r\n```\r\n    std::ifstream fin(kEndpointMapFile, std::ios::in);\r\n    if (!fin.good()) {\r\n      LOG(FATAL) << \"Load endpoint map file failed.\";\r\n    }\r\n```\r\nShould we add .endpoint_map file manually ?", "> Our code hangs using this patch. Need we modify some configuration ? @liutongxua\r\n\r\n> Load endpoint map file failed.\r\n> \r\n> ```\r\n>     std::ifstream fin(kEndpointMapFile, std::ios::in);\r\n>     if (!fin.good()) {\r\n>       LOG(FATAL) << \"Load endpoint map file failed.\";\r\n>     }\r\n> ```\r\n> Should we add .endpoint_map file manually ?\r\n\r\nI'll refactor the code by adding a new service API to pass Seastar's ports later. Follow the similar solution as rdma's ports. \r\n\".endpoint_map\" format like follow:\r\n42353, 42354 is grpc's ports, 46068, 47079 is related seastar's ports.\r\n```\r\n127.0.0.1:42353=127.0.0.1:46068\r\n127.0.0.1:42354=127.0.0.1:47079\r\n```", "> On docker, we start a container using `docker run -it --cap-add=SYS_PTRACE --cpuset-cpus=\"0-55\" --cpus=56 --security-opt seccomp=unconfined --net=host tensorflow:grpc-seastar bash`.\r\n> \r\n> Then another error we got :\r\n> \r\n> ```\r\n> 2019-04-18 15:05:18.210504: I tensorflow/contrib/seastar/seastar_server_lib.cc:355] SeastarWorkerCacheFactory, name_prefix:/job:ps/replica:0/task:0\r\n> 2019-04-18 15:05:18.210562: I tensorflow/contrib/seastar/seastar_server_lib.cc:370] Started server with target: grpc://localhost:31054\r\n> terminate called after throwing an instance of 'std::runtime_error'\r\n>   what():  insufficient processing units\r\n> Aborted (core dumped)\r\n> ```\r\n> In tf, the seastar code 'unsigned available_procs = hwloc_get_nbobjs_by_type(topology, HWLOC_OBJ_PU)' get 1. However, if we run it in a standalone test.cc file, it get 28.\r\n> \r\n> ```\r\n> #include <hwloc.h>\r\n> int main(int argc,char **argv)\r\n> {\r\n>     int nPhysicalProcessorCount = 0;\r\n>     hwloc_topology_t topology;\r\n>     hwloc_topology_init(&topology);\r\n>     hwloc_topology_load(topology);\r\n>     unsigned available_procs = hwloc_get_nbobjs_by_type(topology, HWLOC_OBJ_PU);\r\n>     printf(\"%d\", available_procs); // 28\r\n> }\r\n> ```\r\n> Our machine contains 28 cores.\r\n> \r\n> After we export SEASTAR_CORE_NUMBER=1, 'Segmentation fault (core dumped)' arise without core dump file.\r\n> \r\n> What's more, why core_number_ is equals to server_number?\r\n> `core_number_ = GetCoreNumber(server_number);`\r\n> \r\n> @liutongxuan\r\n\r\nSeastar's hwloc is conflict with TensorFlow's.  I temporary disable seastar's bind core, i'll fix this conflict in seastar side. (Disable seastar's bind core could affect performance, but compare with Grpc, still could 2 times faster in our embedding variable case.)\r\n\r\nPlease pull latest code. I tested several cases passed. If any questions let me know.\r\n`core_number_ = GetCoreNumber(server_number);` because don't want more core than connections polling which could affect other threads. For example, if only one ps and one worker, we only need one connection which means we just need to launch one seastar thread. If there's 4 threads polling by default, would waste cpu resources.\r\n", "> @gunan added `@hwloc` third-party package and @liutongxuan you could seek help from him.\r\nthx, @gunan @byronyi ", "> Let me know if this change is ready to take a look again.\r\n\r\nCurrently code could run now. I'm not sure the design of endpoint_map in seastar_server_lib.cc is ok or not for you guys?  User have to configure a .endpoint_map to record seastar's ports. @byronyi @poxvoculi @annarev", "> > Let me know if this change is ready to take a look again.\r\n> \r\n> Currently code could run now. I'm not sure the design of endpoint_map in seastar_server_lib.cc is ok or not for you guys? User have to configure a .endpoint_map to record seastar's ports. @byronyi @poxvoculi @annarev\r\n\r\nThere seems to be a file conflict in `tensorflow/python/training/server_lib.py`. ", "> > > Let me know if this change is ready to take a look again.\r\n> > \r\n> > \r\n> > Currently code could run now. I'm not sure the design of endpoint_map in seastar_server_lib.cc is ok or not for you guys? User have to configure a .endpoint_map to record seastar's ports. @byronyi @poxvoculi @annarev\r\n> \r\n> There seems to be a file conflict in `tensorflow/python/training/server_lib.py`.\r\n\r\nfixed, thx @byronyi ", "@liutongxuan Thanks! Have a wonderful Sunday night :)", "Can you make sure \"/root/.cache/bazel/_bazel_root/***/external/seastar/core/channel.hh\",   \r\nIf the is_init is not defined like follows, please cleanup the bazel cache then build again.\r\n`const std::atomic_bool& is_init();\r\n`\r\nAnother concern is the port which you use, any other process use the same port?", "is_init is defined like this 'bool is_init();', we will try to clean and build again. thx!\r\nWhat's more, should we modify the channel.hh in seastar ? \r\nAfter rebuilding, is_init is still defined as 'bool is_init()' .\r\n\r\n\r\n@liutongxuan ", "> is_init is defined like this 'bool is_init();', we will try to clean and build again. thx!\r\n> What's more, should we modify the channel.hh in seastar ?\r\n> After rebuilding, is_init is still defined as 'bool is_init()' .\r\n> \r\n> @liutongxuan\r\n\r\nYes, i missed to update seastar repo, please fetch the pr again, cleanup bazel cache, try again. Sorry for the inconvenient.\r\n", "> I added some nit comments.\r\n> \r\n> I was also checking RFC for `tf.contrib` deprecation: https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md. Looks like the plan is to delete contrib directory after we release 2.0.0rc0. `tf.contrib` will only be accessible at older commits. So, might be better to submit this to https://github.com/tensorflow/networking.\r\n> \r\n> Basically, having code under tensorflow/contrib might not be useful in a very short time since you won't be able to make changes (because the code will only be available at older commits).\r\n\r\nYes, we'll submit another PR based on tensorflow/networking repo. Thanks Anna.", "> Basically, having code under tensorflow/contrib might not be useful in a very short time since you won't be able to make changes (because the code will only be available at older commits).\r\n\r\n@annarev We did not expect another release in 1.x series after 1.14. Martin said we are likely going to have a 1.15 release though. In that case, it might be worthwhile to push this plugin into r1.15. The networking project has not shipped a release yet. Either way, we are trying to make this plugin usable while reviewing the commits, so people could download and try it out themselves. ", "> > Basically, having code under tensorflow/contrib might not be useful in a very short time since you won't be able to make changes (because the code will only be available at older commits).\r\n> \r\n> @annarev We did not expect another release in 1.x series after 1.1r. Martin said we are likely going to have a 1.15 release though. In that case, it might be worthwhile to push this plugin into r1.15. The networking project has not shipped a release yet. Either way, we are trying to make this plugin usable while reviewing the commits, so people could download and try it out themselves.\r\n\r\n@byronyi  Sounds good", "@gbaned Mind to verify CLA status and kickoff CI bot? We could do another review pass after CI passes. ", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27454) for more info**.\n\n<!-- cla_yes -->", "> > is_init is defined like this 'bool is_init();', we will try to clean and build again. thx!\r\n> > What's more, should we modify the channel.hh in seastar ?\r\n> > After rebuilding, is_init is still defined as 'bool is_init()' .\r\n> > @liutongxuan\r\n> \r\n> Yes, i missed to update seastar repo, please fetch the pr again, cleanup bazel cache, try again. Sorry for the inconvenient.\r\n\r\nUnder your guidance, it works finally. Thank you very much!! @liutongxuan\r\nWhat's more, what is the Recommended value of SEASTAR_CORE_NUMBER env? ", "> > > is_init is defined like this 'bool is_init();', we will try to clean and build again. thx!\r\n> > > What's more, should we modify the channel.hh in seastar ?\r\n> > > After rebuilding, is_init is still defined as 'bool is_init()' .\r\n> > > @liutongxuan\r\n> > \r\n> > \r\n> > Yes, i missed to update seastar repo, please fetch the pr again, cleanup bazel cache, try again. Sorry for the inconvenient.\r\n> \r\n> Under your guidance, it works finally. Thank you very much!! @liutongxuan\r\n> What's more, what is the Recommended value of SEASTAR_CORE_NUMBER env?\r\n\r\nIt depends on the model's pattern (how long per step, each step how many packets recv/send, what's the size of packets, computation & communication's proportion in one step). Besides depends on the machine, how many cores.\r\n\r\nNormally, we don't suggest set it too much, Seastar threads would be polling there which affect computation.  I have to say you'd better test to get a best number. \r\n\r\nYou can setup different SEASTAR_CORE_NUM for ps/worker. In our job, we have thousands of worker and tens of ps, so worker setup 1 or 2 core, ps set 8 core. (64 core machine).", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27454) for more info**.\n\n<!-- need_author_consent -->", "Seastar server and client connection does not have  retry mechanism. If a connection is failed, such as 'connection reset or address in use', it can not recover.\r\n\r\nSeastar server:\r\n```\r\n  seastar::keep_doing([this, tag_factory] {\r\n    return _listener->accept()\r\n    .then([this, tag_factory] (seastar::connected_socket fd,\r\n                               seastar::socket_address addr) mutable {\r\n      auto conn = new Connection(std::move(fd), tag_factory, addr);\r\n      seastar::do_until([conn] {return conn->_read_buf.eof(); }, [conn] {\r\n        return conn->Read();\r\n      }).then_wrapped([this, conn] (auto&& f) {\r\n        try {\r\n          f.get();\r\n          LOG(INFO) << \"Remote close the connection:  addr = \" << conn->_addr;\r\n        } catch (std::exception& ex) {\r\n          LOG(INFO) << \"Read got an exception: \"\r\n                    << ex << \", addr = \" << conn->_addr;\r\n        }\r\n      });\r\n    });\r\n  }).or_terminate();\r\n```\r\n\r\nSeastar client:\r\n```\r\n  seastar::engine().net().connect(seastar::make_ipv4_address(server_addr),\r\n                                  local,\r\n                                  seastar::transport::TCP).then(\r\n    [this, chan, s, server_addr, tag_factory] (seastar::connected_socket fd) {\r\n    auto conn = new Connection(std::move(fd),\r\n                               chan,\r\n                               tag_factory,\r\n                               seastar::socket_address(server_addr));\r\n\r\n    seastar::do_until([conn] {return conn->_read_buf.eof(); }, [conn] {\r\n      return conn->Read();\r\n    }).then_wrapped([this, conn, s, chan] (auto&& f) {\r\n      try {\r\n        f.get();\r\n        LOG(WARNING) << \"Remote closed the connection: addr = \" << s;\r\n      } catch(std::exception& ex) {\r\n        LOG(WARNING) << \"Read got an exception: \" << ex << \", addr = \" << s;\r\n      }\r\n      chan->set_channel_broken();\r\n    });\r\n    return seastar::make_ready_future();\r\n  }).handle_exception([this, chan, server_addr, s, tag_factory](auto ep) {\r\n    LOG(WARNING) << \"Failed to connect \" << server_addr << \". Get exception: \" << ep;\r\n    using namespace std::chrono_literals;\r\n    LOG(INFO) << \"SeastarClient::Connect throws exception:\" << ep;\r\n    return seastar::sleep(1s)\r\n        .then([this, chan, server_addr, s, tag_factory] {\r\n      this->Connect(server_addr, s, chan, tag_factory);\r\n    });\r\n  });\r\n\r\n```", "> Seastar server and client connection does not have retry mechanism. If a connection is failed, such as 'connection reset or address in use', it can not recover.\r\n> \r\n> Seastar server:\r\n> \r\n> ```\r\n>   seastar::keep_doing([this, tag_factory] {\r\n>     return _listener->accept()\r\n>     .then([this, tag_factory] (seastar::connected_socket fd,\r\n>                                seastar::socket_address addr) mutable {\r\n>       auto conn = new Connection(std::move(fd), tag_factory, addr);\r\n>       seastar::do_until([conn] {return conn->_read_buf.eof(); }, [conn] {\r\n>         return conn->Read();\r\n>       }).then_wrapped([this, conn] (auto&& f) {\r\n>         try {\r\n>           f.get();\r\n>           LOG(INFO) << \"Remote close the connection:  addr = \" << conn->_addr;\r\n>         } catch (std::exception& ex) {\r\n>           LOG(INFO) << \"Read got an exception: \"\r\n>                     << ex << \", addr = \" << conn->_addr;\r\n>         }\r\n>       });\r\n>     });\r\n>   }).or_terminate();\r\n> ```\r\n> Seastar client:\r\n> \r\n> ```\r\n>   seastar::engine().net().connect(seastar::make_ipv4_address(server_addr),\r\n>                                   local,\r\n>                                   seastar::transport::TCP).then(\r\n>     [this, chan, s, server_addr, tag_factory] (seastar::connected_socket fd) {\r\n>     auto conn = new Connection(std::move(fd),\r\n>                                chan,\r\n>                                tag_factory,\r\n>                                seastar::socket_address(server_addr));\r\n> \r\n>     seastar::do_until([conn] {return conn->_read_buf.eof(); }, [conn] {\r\n>       return conn->Read();\r\n>     }).then_wrapped([this, conn, s, chan] (auto&& f) {\r\n>       try {\r\n>         f.get();\r\n>         LOG(WARNING) << \"Remote closed the connection: addr = \" << s;\r\n>       } catch(std::exception& ex) {\r\n>         LOG(WARNING) << \"Read got an exception: \" << ex << \", addr = \" << s;\r\n>       }\r\n>       chan->set_channel_broken();\r\n>     });\r\n>     return seastar::make_ready_future();\r\n>   }).handle_exception([this, chan, server_addr, s, tag_factory](auto ep) {\r\n>     LOG(WARNING) << \"Failed to connect \" << server_addr << \". Get exception: \" << ep;\r\n>     using namespace std::chrono_literals;\r\n>     LOG(INFO) << \"SeastarClient::Connect throws exception:\" << ep;\r\n>     return seastar::sleep(1s)\r\n>         .then([this, chan, server_addr, s, tag_factory] {\r\n>       this->Connect(server_addr, s, chan, tag_factory);\r\n>     });\r\n>   });\r\n> ```\r\n\r\nYes, we need a resolver. We had design a resolver aim at failover (ps failover and worker failover, and solve your cases), i removed it before the code merge to simplify the PR. We'll add it back after this PR done.", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27454) for more info**.\n\n<!-- cla_yes -->", "> > Seastar server and client connection does not have retry mechanism. If a connection is failed, such as 'connection reset or address in use', it can not recover.\r\n> > Seastar server:\r\n> > ```\r\n> >   seastar::keep_doing([this, tag_factory] {\r\n> >     return _listener->accept()\r\n> >     .then([this, tag_factory] (seastar::connected_socket fd,\r\n> >                                seastar::socket_address addr) mutable {\r\n> >       auto conn = new Connection(std::move(fd), tag_factory, addr);\r\n> >       seastar::do_until([conn] {return conn->_read_buf.eof(); }, [conn] {\r\n> >         return conn->Read();\r\n> >       }).then_wrapped([this, conn] (auto&& f) {\r\n> >         try {\r\n> >           f.get();\r\n> >           LOG(INFO) << \"Remote close the connection:  addr = \" << conn->_addr;\r\n> >         } catch (std::exception& ex) {\r\n> >           LOG(INFO) << \"Read got an exception: \"\r\n> >                     << ex << \", addr = \" << conn->_addr;\r\n> >         }\r\n> >       });\r\n> >     });\r\n> >   }).or_terminate();\r\n> > ```\r\n> > Seastar client:\r\n> > ```\r\n> >   seastar::engine().net().connect(seastar::make_ipv4_address(server_addr),\r\n> >                                   local,\r\n> >                                   seastar::transport::TCP).then(\r\n> >     [this, chan, s, server_addr, tag_factory] (seastar::connected_socket fd) {\r\n> >     auto conn = new Connection(std::move(fd),\r\n> >                                chan,\r\n> >                                tag_factory,\r\n> >                                seastar::socket_address(server_addr));\r\n> > \r\n> >     seastar::do_until([conn] {return conn->_read_buf.eof(); }, [conn] {\r\n> >       return conn->Read();\r\n> >     }).then_wrapped([this, conn, s, chan] (auto&& f) {\r\n> >       try {\r\n> >         f.get();\r\n> >         LOG(WARNING) << \"Remote closed the connection: addr = \" << s;\r\n> >       } catch(std::exception& ex) {\r\n> >         LOG(WARNING) << \"Read got an exception: \" << ex << \", addr = \" << s;\r\n> >       }\r\n> >       chan->set_channel_broken();\r\n> >     });\r\n> >     return seastar::make_ready_future();\r\n> >   }).handle_exception([this, chan, server_addr, s, tag_factory](auto ep) {\r\n> >     LOG(WARNING) << \"Failed to connect \" << server_addr << \". Get exception: \" << ep;\r\n> >     using namespace std::chrono_literals;\r\n> >     LOG(INFO) << \"SeastarClient::Connect throws exception:\" << ep;\r\n> >     return seastar::sleep(1s)\r\n> >         .then([this, chan, server_addr, s, tag_factory] {\r\n> >       this->Connect(server_addr, s, chan, tag_factory);\r\n> >     });\r\n> >   });\r\n> > ```\r\n> \r\n> Yes, we need a resolver. We had design a resolver aim at failover (ps failover and worker failover, and solve your cases), i removed it before the code merge to simplify the PR. We'll add it back after this PR done.\r\n\r\nWithout retry mechanism, it is hard to use this PR, especially there are hundreds of workers. Looking forward to your PR.", "@rthadur Mind to kick off Kokoro? We could have another review pass before May.", "> @rthadur Mind to kick off Kokoro? We could have another review pass before May.\r\n\r\ndone", "> Sorry for delay, I added a few small comments. My main concern now is over fetching entire boost library just for a single call. I wonder if there is a workaround without using boost.\r\n\r\nThanks comments Anna, boost is not referenced by only code in here. Seastar highly depends on boost, so have to reference it.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27454) for more info**.\n\n<!-- need_author_consent -->", "I've met an error building with `--config=seastar`. Installing `libaio-dev` fixes this error.\r\n\r\n```\r\nERROR: /home/byronyi/.cache/bazel/_bazel_byronyi/294c5cbab5ae82f218d48ba2056277d9/external/seastar/BUILD.bazel:3:1: C++ compilation of rule '@seastar//:seastar' failed (Exit 1)\r\nIn file included from external/seastar/net/net.hh:25:0,\r\n                 from external/seastar/net/net.cc:25:\r\nexternal/seastar/core/reactor.hh:33:20: fatal error: libaio.h: No such file or directory\r\n #include <libaio.h>\r\n                    ^\r\ncompilation terminated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 406.776s, Critical Path: 65.08s\r\nINFO: 3939 processes: 3939 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n\r\n@tongxuan would you mind adding a README for building TF with seastar support?", "> I've met an error building with `--config=seastar`. Installing `libaio-dev` fixes this error.\r\n> \r\n> ```\r\n> ERROR: /home/byronyi/.cache/bazel/_bazel_byronyi/294c5cbab5ae82f218d48ba2056277d9/external/seastar/BUILD.bazel:3:1: C++ compilation of rule '@seastar//:seastar' failed (Exit 1)\r\n> In file included from external/seastar/net/net.hh:25:0,\r\n>                  from external/seastar/net/net.cc:25:\r\n> external/seastar/core/reactor.hh:33:20: fatal error: libaio.h: No such file or directory\r\n>  #include <libaio.h>\r\n>                     ^\r\n> compilation terminated.\r\n> Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n> Use --verbose_failures to see the command lines of failed build steps.\r\n> INFO: Elapsed time: 406.776s, Critical Path: 65.08s\r\n> INFO: 3939 processes: 3939 local.\r\n> FAILED: Build did NOT complete successfully\r\n> ```\r\n> \r\n> @tongxuan would you mind adding a README for building TF with seastar support?\r\n\r\nThanks, bairen, i'll add a README as soon as possible. For the error you met, because of libaio library (https://www.archlinux.org/packages/core/x86_64/libaio/)  is not installed in tensorflow developer docker which seastar depends on. I'll check whether i could remove the dependency of libaio.", "@liutongxuan It's okay to have a necessary third-party dependency, it is just better to document it so people know how to build or run that without trivial errors. @perfinion Jason might know how to add `libaio-dev` (and also others required by existing networking plugins, such as `librdmacm-dev` and `libibverbs-dev`) into a `systemlib` bazel dependency.\r\n\r\nA list of dependencies missing from my side:\r\n\r\n* libaio-dev\r\n* libunwind-dev\r\n* libsctp-dev\r\n* xfslibs-dev\r\n* systemtap-sdt-dev", "I am able to run tensorflow/benchmarks using an `.endpoint_map` file. Thanks @liutongxuan!", "@liutongxuan I think it'd be good to copy the content of the seastar RFC to your README, which also addresses your team's contribution to the users.", "> @liutongxuan I think it'd be good to copy the content of the seastar RFC to your README, which also addresses your team's contribution to the users.\r\n\r\nYes, i'll append it.", "@annarev @byronyi can we start another round of review?", "Looks good to me at a high level, but I want to find someone internally who knows more about networking than I do to take a look as well.\r\nUnfortunately, looks like everyone I can think of is on vacation right now and can take a look is beginning of June. When I find someone, I will add that person as a reviewer. Sorry for delay!", "Is there a performance pressure report compared to native GRPC?", "> Is there a performance pressure report compared to native GRPC?\r\n\r\nHey, @zhaiyuyong we have tested many models such as W&DL (internal model in Alibaba which could not open the source) which show significantly improvement. (in tensorflow/contrib/seastar/README there's a link to RFC with performance result).  We haven't test it on open source models, welcome to provide models for the comparison.", "> > Is there a performance pressure report compared to native GRPC?\r\n> \r\n> Hey, @zhaiyuyong we have tested many models such as W&DL (internal model in Alibaba which could not open the source) which show significantly improvement. (in tensorflow/contrib/seastar/README there's a link to RFC with performance result). We haven't test it on open source models, welcome to provide models for the comparison.\r\n\r\nI think it'd be nice to have some externally reproducible benchmarks, especially when we'd like to do a blog post for this. Are you aware of any well-recognized wide and deep models with dataset available? Maybe we could seek for some help from the researchers. @ewilderj appreciate if you could give some pointers to people we could talk to.", "> > Is there a performance pressure report compared to native GRPC?\r\n> \r\n> Hey, @zhaiyuyong we have tested many models such as W&DL (internal model in Alibaba which could not open the source) which show significantly improvement. (in tensorflow/contrib/seastar/README there's a link to RFC with performance result). We haven't test it on open source models, welcome to provide models for the comparison.\r\n\r\n@liutongxuan  do you think seastar has an advantage over BRPC? we are now doing a comparison of GRPC/BRPC/seastar three performance work\r\n", "> > > Is there a performance pressure report compared to native GRPC?\r\n> > \r\n> > \r\n> > Hey, @zhaiyuyong we have tested many models such as W&DL (internal model in Alibaba which could not open the source) which show significantly improvement. (in tensorflow/contrib/seastar/README there's a link to RFC with performance result). We haven't test it on open source models, welcome to provide models for the comparison.\r\n> \r\n> @liutongxuan do you think seastar has an advantage over BRPC? we are now doing a comparison of GRPC/BRPC/seastar three performance work\r\n\r\nSeastar is best in clean environment (no matter big/small data). You can use my Seastar repo to test which have some optimization on big data transmit (truly zero copy when data bigger than 8KB, this feature not open in current PR, will submit a separate PR later). Brpc have some interesting features such as connection pool (multiple connection between two node).", "Whether to consider using seastar native network stack? As far as doc of seastar, it is several times faster than the posix stack. ", "> Whether to consider using seastar native network stack? As far as doc of seastar, it is several times faster than the posix stack.\r\n\r\nYes, it's would be a configuration of grpc+seastar protocol.", "> > Whether to consider using seastar native network stack? As far as doc of seastar, it is several times faster than the posix stack.\r\n> \r\n> Yes, it's would be a configuration of grpc+seastar protocol.\r\n\r\nThat would be great. We are now working on enable the native network stack, while got into troubles with building.  DPDK could be built, and .h and .a files are generated. Is there any hint for us that how could these files link into TF project? Or should we build DPDK by bazel?", "> > > Whether to consider using seastar native network stack? As far as doc of seastar, it is several times faster than the posix stack.\r\n> > \r\n> > \r\n> > Yes, it's would be a configuration of grpc+seastar protocol.\r\n> \r\n> That would be great. We are now working on enable the native network stack, while got into troubles with building. DPDK could be built, and .h and .a files are generated. Is there any hint for us that how could these files link into TF project? Or should we build DPDK by bazel?\r\n\r\nBe careful with seastar native network stack. I have ever fix some native stack bugs. But I am not sure about the stability of the latest version. So what the trouble with u about the native stack.", "@liutongxuan Hi Tongxuan, impressive work! I am having difficulty compiling seastar with HAVE_DPDK option, and your point of view on this matter is well appreciated. Basically, I have 3 questions in my mind. 1. Which version or release of DPDK is it in your cached-dpdk folder? 2. Are you able to compile the cached-dpdk files? 3. Are you able to build your tensorflow with seaster with HAVE_DPDK on? If yes, were you using the cached-dpdk as the source for DPDK or a branch from a different release of DKDP (can you share which version of DPDK it is) ?  Please share instructions on how to build with HAVE_DPDK enabled. Also, should I include net/dpdk.cc when building seastar? Thank you so much.", "> > On docker, we start a container using `docker run -it --cap-add=SYS_PTRACE --cpuset-cpus=\"0-55\" --cpus=56 --security-opt seccomp=unconfined --net=host tensorflow:grpc-seastar bash`.\r\n> > Then another error we got :\r\n> > ```\r\n> > 2019-04-18 15:05:18.210504: I tensorflow/contrib/seastar/seastar_server_lib.cc:355] SeastarWorkerCacheFactory, name_prefix:/job:ps/replica:0/task:0\r\n> > 2019-04-18 15:05:18.210562: I tensorflow/contrib/seastar/seastar_server_lib.cc:370] Started server with target: grpc://localhost:31054\r\n> > terminate called after throwing an instance of 'std::runtime_error'\r\n> >   what():  insufficient processing units\r\n> > Aborted (core dumped)\r\n> > ```\r\n> > \r\n> > \r\n> > In tf, the seastar code 'unsigned available_procs = hwloc_get_nbobjs_by_type(topology, HWLOC_OBJ_PU)' get 1. However, if we run it in a standalone test.cc file, it get 28.\r\n> > ```\r\n> > #include <hwloc.h>\r\n> > int main(int argc,char **argv)\r\n> > {\r\n> >     int nPhysicalProcessorCount = 0;\r\n> >     hwloc_topology_t topology;\r\n> >     hwloc_topology_init(&topology);\r\n> >     hwloc_topology_load(topology);\r\n> >     unsigned available_procs = hwloc_get_nbobjs_by_type(topology, HWLOC_OBJ_PU);\r\n> >     printf(\"%d\", available_procs); // 28\r\n> > }\r\n> > ```\r\n> > \r\n> > \r\n> > Our machine contains 28 cores.\r\n> > After we export SEASTAR_CORE_NUMBER=1, 'Segmentation fault (core dumped)' arise without core dump file.\r\n> > What's more, why core_number_ is equals to server_number?\r\n> > `core_number_ = GetCoreNumber(server_number);`\r\n> > @liutongxuan\r\n> \r\n> Seastar's hwloc is conflict with TensorFlow's. I temporary disable seastar's bind core, i'll fix this conflict in seastar side. (Disable seastar's bind core could affect performance, but compare with Grpc, still could 2 times faster in our embedding variable case.)\r\n> \r\n> Please pull latest code. I tested several cases passed. If any questions let me know.\r\n> `core_number_ = GetCoreNumber(server_number);` because don't want more core than connections polling which could affect other threads. For example, if only one ps and one worker, we only need one connection which means we just need to launch one seastar thread. If there's 4 threads polling by default, would waste cpu resources.\r\n\r\nIn the RFC: Seastar-based RPC for TF Worker Service, is shows grpc+seastar performs more than 3 times faster than GRPC. However we only get 1 times faster in our weed-and-deep or ctr models within 200 workers. If worker's number increases to 400, it performs bad than 200 workers. We wonder if hwloc conflict affects this. And Have you fixed hwloc conflict ? Thx! @liutongxuan ", "> > > On docker, we start a container using `docker run -it --cap-add=SYS_PTRACE --cpuset-cpus=\"0-55\" --cpus=56 --security-opt seccomp=unconfined --net=host tensorflow:grpc-seastar bash`.\r\n> > > Then another error we got :\r\n> > > ```\r\n> > > 2019-04-18 15:05:18.210504: I tensorflow/contrib/seastar/seastar_server_lib.cc:355] SeastarWorkerCacheFactory, name_prefix:/job:ps/replica:0/task:0\r\n> > > 2019-04-18 15:05:18.210562: I tensorflow/contrib/seastar/seastar_server_lib.cc:370] Started server with target: grpc://localhost:31054\r\n> > > terminate called after throwing an instance of 'std::runtime_error'\r\n> > >   what():  insufficient processing units\r\n> > > Aborted (core dumped)\r\n> > > ```\r\n> > > \r\n> > > \r\n> > > In tf, the seastar code 'unsigned available_procs = hwloc_get_nbobjs_by_type(topology, HWLOC_OBJ_PU)' get 1. However, if we run it in a standalone test.cc file, it get 28.\r\n> > > ```\r\n> > > #include <hwloc.h>\r\n> > > int main(int argc,char **argv)\r\n> > > {\r\n> > >     int nPhysicalProcessorCount = 0;\r\n> > >     hwloc_topology_t topology;\r\n> > >     hwloc_topology_init(&topology);\r\n> > >     hwloc_topology_load(topology);\r\n> > >     unsigned available_procs = hwloc_get_nbobjs_by_type(topology, HWLOC_OBJ_PU);\r\n> > >     printf(\"%d\", available_procs); // 28\r\n> > > }\r\n> > > ```\r\n> > > \r\n> > > \r\n> > > Our machine contains 28 cores.\r\n> > > After we export SEASTAR_CORE_NUMBER=1, 'Segmentation fault (core dumped)' arise without core dump file.\r\n> > > What's more, why core_number_ is equals to server_number?\r\n> > > `core_number_ = GetCoreNumber(server_number);`\r\n> > > @liutongxuan\r\n> > \r\n> > \r\n> > Seastar's hwloc is conflict with TensorFlow's. I temporary disable seastar's bind core, i'll fix this conflict in seastar side. (Disable seastar's bind core could affect performance, but compare with Grpc, still could 2 times faster in our embedding variable case.)\r\n> > Please pull latest code. I tested several cases passed. If any questions let me know.\r\n> > `core_number_ = GetCoreNumber(server_number);` because don't want more core than connections polling which could affect other threads. For example, if only one ps and one worker, we only need one connection which means we just need to launch one seastar thread. If there's 4 threads polling by default, would waste cpu resources.\r\n> \r\n> In the RFC: Seastar-based RPC for TF Worker Service, is shows grpc+seastar performs more than 3 times faster than GRPC. However we only get 1 times faster in our weed-and-deep or ctr models within 200 workers. If worker's number increases to 400, it performs bad than 200 workers. We wonder if hwloc conflict affects this. And Have you fixed hwloc conflict ? Thx! @liutongxuan\r\n\r\nHWLOC does not affect performance if you don't use NUMA.  Can you share the config parameters here, which like thread-num, core-num, bin-core and so on. Some fine-tuning may be needed here.", "> @liutongxuan Hi Tongxuan, impressive work! I am having difficulty compiling seastar with HAVE_DPDK option, and your point of view on this matter is well appreciated. Basically, I have 3 questions in my mind. 1. Which version or release of DPDK is it in your cached-dpdk folder? 2. Are you able to compile the cached-dpdk files? 3. Are you able to build your tensorflow with seaster with HAVE_DPDK on? If yes, were you using the cached-dpdk as the source for DPDK or a branch from a different release of DKDP (can you share which version of DPDK it is) ? Please share instructions on how to build with HAVE_DPDK enabled. Also, should I include net/dpdk.cc when building seastar? Thank you so much.\r\n\r\nPlease wait for the next PR about add option to build with DPDK.", "> > > On docker, we start a container using `docker run -it --cap-add=SYS_PTRACE --cpuset-cpus=\"0-55\" --cpus=56 --security-opt seccomp=unconfined --net=host tensorflow:grpc-seastar bash`.\r\n> > > Then another error we got :\r\n> > > ```\r\n> > > 2019-04-18 15:05:18.210504: I tensorflow/contrib/seastar/seastar_server_lib.cc:355] SeastarWorkerCacheFactory, name_prefix:/job:ps/replica:0/task:0\r\n> > > 2019-04-18 15:05:18.210562: I tensorflow/contrib/seastar/seastar_server_lib.cc:370] Started server with target: grpc://localhost:31054\r\n> > > terminate called after throwing an instance of 'std::runtime_error'\r\n> > >   what():  insufficient processing units\r\n> > > Aborted (core dumped)\r\n> > > ```\r\n> > > \r\n> > > \r\n> > > In tf, the seastar code 'unsigned available_procs = hwloc_get_nbobjs_by_type(topology, HWLOC_OBJ_PU)' get 1. However, if we run it in a standalone test.cc file, it get 28.\r\n> > > ```\r\n> > > #include <hwloc.h>\r\n> > > int main(int argc,char **argv)\r\n> > > {\r\n> > >     int nPhysicalProcessorCount = 0;\r\n> > >     hwloc_topology_t topology;\r\n> > >     hwloc_topology_init(&topology);\r\n> > >     hwloc_topology_load(topology);\r\n> > >     unsigned available_procs = hwloc_get_nbobjs_by_type(topology, HWLOC_OBJ_PU);\r\n> > >     printf(\"%d\", available_procs); // 28\r\n> > > }\r\n> > > ```\r\n> > > \r\n> > > \r\n> > > Our machine contains 28 cores.\r\n> > > After we export SEASTAR_CORE_NUMBER=1, 'Segmentation fault (core dumped)' arise without core dump file.\r\n> > > What's more, why core_number_ is equals to server_number?\r\n> > > `core_number_ = GetCoreNumber(server_number);`\r\n> > > @liutongxuan\r\n> > \r\n> > \r\n> > Seastar's hwloc is conflict with TensorFlow's. I temporary disable seastar's bind core, i'll fix this conflict in seastar side. (Disable seastar's bind core could affect performance, but compare with Grpc, still could 2 times faster in our embedding variable case.)\r\n> > Please pull latest code. I tested several cases passed. If any questions let me know.\r\n> > `core_number_ = GetCoreNumber(server_number);` because don't want more core than connections polling which could affect other threads. For example, if only one ps and one worker, we only need one connection which means we just need to launch one seastar thread. If there's 4 threads polling by default, would waste cpu resources.\r\n> \r\n> In the RFC: Seastar-based RPC for TF Worker Service, is shows grpc+seastar performs more than 3 times faster than GRPC. However we only get 1 times faster in our weed-and-deep or ctr models within 200 workers. If worker's number increases to 400, it performs bad than 200 workers. We wonder if hwloc conflict affects this. And Have you fixed hwloc conflict ? Thx! @liutongxuan\r\n\r\nHow many PS do you use? We strongly suggest not use too many PS. And also as @shanshanpt said, please carefully tune the Seastar thread number.  In our case, there's tens of PS and thousands of Worker, PS is configured 8 threads, Worker is configured 2 threads. Besides, there's zero copy & fuse recv feature is not included in the PR, we'll submit later.", "> > > > On docker, we start a container using `docker run -it --cap-add=SYS_PTRACE --cpuset-cpus=\"0-55\" --cpus=56 --security-opt seccomp=unconfined --net=host tensorflow:grpc-seastar bash`.\r\n> > > > Then another error we got :\r\n> > > > ```\r\n> > > > 2019-04-18 15:05:18.210504: I tensorflow/contrib/seastar/seastar_server_lib.cc:355] SeastarWorkerCacheFactory, name_prefix:/job:ps/replica:0/task:0\r\n> > > > 2019-04-18 15:05:18.210562: I tensorflow/contrib/seastar/seastar_server_lib.cc:370] Started server with target: grpc://localhost:31054\r\n> > > > terminate called after throwing an instance of 'std::runtime_error'\r\n> > > >   what():  insufficient processing units\r\n> > > > Aborted (core dumped)\r\n> > > > ```\r\n> > > > \r\n> > > > \r\n> > > > In tf, the seastar code 'unsigned available_procs = hwloc_get_nbobjs_by_type(topology, HWLOC_OBJ_PU)' get 1. However, if we run it in a standalone test.cc file, it get 28.\r\n> > > > ```\r\n> > > > #include <hwloc.h>\r\n> > > > int main(int argc,char **argv)\r\n> > > > {\r\n> > > >     int nPhysicalProcessorCount = 0;\r\n> > > >     hwloc_topology_t topology;\r\n> > > >     hwloc_topology_init(&topology);\r\n> > > >     hwloc_topology_load(topology);\r\n> > > >     unsigned available_procs = hwloc_get_nbobjs_by_type(topology, HWLOC_OBJ_PU);\r\n> > > >     printf(\"%d\", available_procs); // 28\r\n> > > > }\r\n> > > > ```\r\n> > > > \r\n> > > > \r\n> > > > Our machine contains 28 cores.\r\n> > > > After we export SEASTAR_CORE_NUMBER=1, 'Segmentation fault (core dumped)' arise without core dump file.\r\n> > > > What's more, why core_number_ is equals to server_number?\r\n> > > > `core_number_ = GetCoreNumber(server_number);`\r\n> > > > @liutongxuan\r\n> > > \r\n> > > \r\n> > > Seastar's hwloc is conflict with TensorFlow's. I temporary disable seastar's bind core, i'll fix this conflict in seastar side. (Disable seastar's bind core could affect performance, but compare with Grpc, still could 2 times faster in our embedding variable case.)\r\n> > > Please pull latest code. I tested several cases passed. If any questions let me know.\r\n> > > `core_number_ = GetCoreNumber(server_number);` because don't want more core than connections polling which could affect other threads. For example, if only one ps and one worker, we only need one connection which means we just need to launch one seastar thread. If there's 4 threads polling by default, would waste cpu resources.\r\n> > \r\n> > \r\n> > In the RFC: Seastar-based RPC for TF Worker Service, is shows grpc+seastar performs more than 3 times faster than GRPC. However we only get 1 times faster in our weed-and-deep or ctr models within 200 workers. If worker's number increases to 400, it performs bad than 200 workers. We wonder if hwloc conflict affects this. And Have you fixed hwloc conflict ? Thx! @liutongxuan\r\n> \r\n> HWLOC does not affect performance if you don't use NUMA. Can you share the config parameters here, which like thread-num, core-num, bin-core and so on. Some fine-tuning may be needed here.\r\n\r\nWith 200 workers and 20 parameter servers, inter and intra thread pools are set to 80 (each node have 80 cores), and workers' seastar core numbers set to 6, and  parameter servers' seastar core numbers set to 10 (this configuration get better performance). We increase parameter server number to 26 or decrease it to 12, global_steps/sec  remains little change.  What's more, without zero copy & fuse recv feature, does this PR can perform more than 3 times faster than GRPC? Thx!  @shanshanpt @liutongxuan ", "> > > > > On docker, we start a container using `docker run -it --cap-add=SYS_PTRACE --cpuset-cpus=\"0-55\" --cpus=56 --security-opt seccomp=unconfined --net=host tensorflow:grpc-seastar bash`.\r\n> > > > > Then another error we got :\r\n> > > > > ```\r\n> > > > > 2019-04-18 15:05:18.210504: I tensorflow/contrib/seastar/seastar_server_lib.cc:355] SeastarWorkerCacheFactory, name_prefix:/job:ps/replica:0/task:0\r\n> > > > > 2019-04-18 15:05:18.210562: I tensorflow/contrib/seastar/seastar_server_lib.cc:370] Started server with target: grpc://localhost:31054\r\n> > > > > terminate called after throwing an instance of 'std::runtime_error'\r\n> > > > >   what():  insufficient processing units\r\n> > > > > Aborted (core dumped)\r\n> > > > > ```\r\n> > > > > \r\n> > > > > \r\n> > > > > In tf, the seastar code 'unsigned available_procs = hwloc_get_nbobjs_by_type(topology, HWLOC_OBJ_PU)' get 1. However, if we run it in a standalone test.cc file, it get 28.\r\n> > > > > ```\r\n> > > > > #include <hwloc.h>\r\n> > > > > int main(int argc,char **argv)\r\n> > > > > {\r\n> > > > >     int nPhysicalProcessorCount = 0;\r\n> > > > >     hwloc_topology_t topology;\r\n> > > > >     hwloc_topology_init(&topology);\r\n> > > > >     hwloc_topology_load(topology);\r\n> > > > >     unsigned available_procs = hwloc_get_nbobjs_by_type(topology, HWLOC_OBJ_PU);\r\n> > > > >     printf(\"%d\", available_procs); // 28\r\n> > > > > }\r\n> > > > > ```\r\n> > > > > \r\n> > > > > \r\n> > > > > Our machine contains 28 cores.\r\n> > > > > After we export SEASTAR_CORE_NUMBER=1, 'Segmentation fault (core dumped)' arise without core dump file.\r\n> > > > > What's more, why core_number_ is equals to server_number?\r\n> > > > > `core_number_ = GetCoreNumber(server_number);`\r\n> > > > > @liutongxuan\r\n> > > > \r\n> > > > \r\n> > > > Seastar's hwloc is conflict with TensorFlow's. I temporary disable seastar's bind core, i'll fix this conflict in seastar side. (Disable seastar's bind core could affect performance, but compare with Grpc, still could 2 times faster in our embedding variable case.)\r\n> > > > Please pull latest code. I tested several cases passed. If any questions let me know.\r\n> > > > `core_number_ = GetCoreNumber(server_number);` because don't want more core than connections polling which could affect other threads. For example, if only one ps and one worker, we only need one connection which means we just need to launch one seastar thread. If there's 4 threads polling by default, would waste cpu resources.\r\n> > > \r\n> > > \r\n> > > In the RFC: Seastar-based RPC for TF Worker Service, is shows grpc+seastar performs more than 3 times faster than GRPC. However we only get 1 times faster in our weed-and-deep or ctr models within 200 workers. If worker's number increases to 400, it performs bad than 200 workers. We wonder if hwloc conflict affects this. And Have you fixed hwloc conflict ? Thx! @liutongxuan\r\n> > \r\n> > \r\n> > HWLOC does not affect performance if you don't use NUMA. Can you share the config parameters here, which like thread-num, core-num, bin-core and so on. Some fine-tuning may be needed here.\r\n> \r\n> With 200 workers and 20 parameter servers, inter and intra thread pools are set to 80 (each node have 80 cores), and workers' seastar core numbers set to 6, and parameter servers' seastar core numbers set to 10 (this configuration get better performance). We increase parameter server number to 26 or decrease it to 12, global_steps/sec remains little change. What's more, without zero copy & fuse recv feature, does this PR can perform more than 3 times faster than GRPC? Thx! @shanshanpt @liutongxuan\r\n\r\nI'm not sure only this PR could meet your requirement, because the performance data in RFC includes all features of grpc+seastar which i mentioned. And also W&DL's workload would highly affected by how many feature columns in it. And also your inter/intra would affect the Seastar thread, reduce the intra/inter to physical core number not logic core number, make sure there's not too much cpu-migration. Besides, why 80 cores how many physical cores, how many NUMA nodes?  ", "> > > > > > On docker, we start a container using `docker run -it --cap-add=SYS_PTRACE --cpuset-cpus=\"0-55\" --cpus=56 --security-opt seccomp=unconfined --net=host tensorflow:grpc-seastar bash`.\r\n> > > > > > Then another error we got :\r\n> > > > > > ```\r\n> > > > > > 2019-04-18 15:05:18.210504: I tensorflow/contrib/seastar/seastar_server_lib.cc:355] SeastarWorkerCacheFactory, name_prefix:/job:ps/replica:0/task:0\r\n> > > > > > 2019-04-18 15:05:18.210562: I tensorflow/contrib/seastar/seastar_server_lib.cc:370] Started server with target: grpc://localhost:31054\r\n> > > > > > terminate called after throwing an instance of 'std::runtime_error'\r\n> > > > > >   what():  insufficient processing units\r\n> > > > > > Aborted (core dumped)\r\n> > > > > > ```\r\n> > > > > > \r\n> > > > > > \r\n> > > > > > In tf, the seastar code 'unsigned available_procs = hwloc_get_nbobjs_by_type(topology, HWLOC_OBJ_PU)' get 1. However, if we run it in a standalone test.cc file, it get 28.\r\n> > > > > > ```\r\n> > > > > > #include <hwloc.h>\r\n> > > > > > int main(int argc,char **argv)\r\n> > > > > > {\r\n> > > > > >     int nPhysicalProcessorCount = 0;\r\n> > > > > >     hwloc_topology_t topology;\r\n> > > > > >     hwloc_topology_init(&topology);\r\n> > > > > >     hwloc_topology_load(topology);\r\n> > > > > >     unsigned available_procs = hwloc_get_nbobjs_by_type(topology, HWLOC_OBJ_PU);\r\n> > > > > >     printf(\"%d\", available_procs); // 28\r\n> > > > > > }\r\n> > > > > > ```\r\n> > > > > > \r\n> > > > > > \r\n> > > > > > Our machine contains 28 cores.\r\n> > > > > > After we export SEASTAR_CORE_NUMBER=1, 'Segmentation fault (core dumped)' arise without core dump file.\r\n> > > > > > What's more, why core_number_ is equals to server_number?\r\n> > > > > > `core_number_ = GetCoreNumber(server_number);`\r\n> > > > > > @liutongxuan\r\n> > > > > \r\n> > > > > \r\n> > > > > Seastar's hwloc is conflict with TensorFlow's. I temporary disable seastar's bind core, i'll fix this conflict in seastar side. (Disable seastar's bind core could affect performance, but compare with Grpc, still could 2 times faster in our embedding variable case.)\r\n> > > > > Please pull latest code. I tested several cases passed. If any questions let me know.\r\n> > > > > `core_number_ = GetCoreNumber(server_number);` because don't want more core than connections polling which could affect other threads. For example, if only one ps and one worker, we only need one connection which means we just need to launch one seastar thread. If there's 4 threads polling by default, would waste cpu resources.\r\n> > > > \r\n> > > > \r\n> > > > In the RFC: Seastar-based RPC for TF Worker Service, is shows grpc+seastar performs more than 3 times faster than GRPC. However we only get 1 times faster in our weed-and-deep or ctr models within 200 workers. If worker's number increases to 400, it performs bad than 200 workers. We wonder if hwloc conflict affects this. And Have you fixed hwloc conflict ? Thx! @liutongxuan\r\n> > > \r\n> > > \r\n> > > HWLOC does not affect performance if you don't use NUMA. Can you share the config parameters here, which like thread-num, core-num, bin-core and so on. Some fine-tuning may be needed here.\r\n> > \r\n> > \r\n> > With 200 workers and 20 parameter servers, inter and intra thread pools are set to 80 (each node have 80 cores), and workers' seastar core numbers set to 6, and parameter servers' seastar core numbers set to 10 (this configuration get better performance). We increase parameter server number to 26 or decrease it to 12, global_steps/sec remains little change. What's more, without zero copy & fuse recv feature, does this PR can perform more than 3 times faster than GRPC? Thx! @shanshanpt @liutongxuan\r\n> \r\n> I'm not sure only this PR could meet your requirement, because the performance data in RFC includes all features of grpc+seastar which i mentioned. And also W&DL's workload would highly affected by how many feature columns in it. And also your inter/intra would affect the Seastar thread, reduce the intra/inter to physical core number not logic core number, make sure there's not too much cpu-migration. Besides, why 80 cores how many physical cores, how many NUMA nodes?\r\n\r\nThere are 40 physical cores and 2 NUMA nodes, we have set inter/intra  to 40, and it has little affect.", "> > > > > > > On docker, we start a container using `docker run -it --cap-add=SYS_PTRACE --cpuset-cpus=\"0-55\" --cpus=56 --security-opt seccomp=unconfined --net=host tensorflow:grpc-seastar bash`.\r\n> > > > > > > Then another error we got :\r\n> > > > > > > ```\r\n> > > > > > > 2019-04-18 15:05:18.210504: I tensorflow/contrib/seastar/seastar_server_lib.cc:355] SeastarWorkerCacheFactory, name_prefix:/job:ps/replica:0/task:0\r\n> > > > > > > 2019-04-18 15:05:18.210562: I tensorflow/contrib/seastar/seastar_server_lib.cc:370] Started server with target: grpc://localhost:31054\r\n> > > > > > > terminate called after throwing an instance of 'std::runtime_error'\r\n> > > > > > >   what():  insufficient processing units\r\n> > > > > > > Aborted (core dumped)\r\n> > > > > > > ```\r\n> > > > > > > \r\n> > > > > > > \r\n> > > > > > > In tf, the seastar code 'unsigned available_procs = hwloc_get_nbobjs_by_type(topology, HWLOC_OBJ_PU)' get 1. However, if we run it in a standalone test.cc file, it get 28.\r\n> > > > > > > ```\r\n> > > > > > > #include <hwloc.h>\r\n> > > > > > > int main(int argc,char **argv)\r\n> > > > > > > {\r\n> > > > > > >     int nPhysicalProcessorCount = 0;\r\n> > > > > > >     hwloc_topology_t topology;\r\n> > > > > > >     hwloc_topology_init(&topology);\r\n> > > > > > >     hwloc_topology_load(topology);\r\n> > > > > > >     unsigned available_procs = hwloc_get_nbobjs_by_type(topology, HWLOC_OBJ_PU);\r\n> > > > > > >     printf(\"%d\", available_procs); // 28\r\n> > > > > > > }\r\n> > > > > > > ```\r\n> > > > > > > \r\n> > > > > > > \r\n> > > > > > > Our machine contains 28 cores.\r\n> > > > > > > After we export SEASTAR_CORE_NUMBER=1, 'Segmentation fault (core dumped)' arise without core dump file.\r\n> > > > > > > What's more, why core_number_ is equals to server_number?\r\n> > > > > > > `core_number_ = GetCoreNumber(server_number);`\r\n> > > > > > > @liutongxuan\r\n> > > > > > \r\n> > > > > > \r\n> > > > > > Seastar's hwloc is conflict with TensorFlow's. I temporary disable seastar's bind core, i'll fix this conflict in seastar side. (Disable seastar's bind core could affect performance, but compare with Grpc, still could 2 times faster in our embedding variable case.)\r\n> > > > > > Please pull latest code. I tested several cases passed. If any questions let me know.\r\n> > > > > > `core_number_ = GetCoreNumber(server_number);` because don't want more core than connections polling which could affect other threads. For example, if only one ps and one worker, we only need one connection which means we just need to launch one seastar thread. If there's 4 threads polling by default, would waste cpu resources.\r\n> > > > > \r\n> > > > > \r\n> > > > > In the RFC: Seastar-based RPC for TF Worker Service, is shows grpc+seastar performs more than 3 times faster than GRPC. However we only get 1 times faster in our weed-and-deep or ctr models within 200 workers. If worker's number increases to 400, it performs bad than 200 workers. We wonder if hwloc conflict affects this. And Have you fixed hwloc conflict ? Thx! @liutongxuan\r\n> > > > \r\n> > > > \r\n> > > > HWLOC does not affect performance if you don't use NUMA. Can you share the config parameters here, which like thread-num, core-num, bin-core and so on. Some fine-tuning may be needed here.\r\n> > > \r\n> > > \r\n> > > With 200 workers and 20 parameter servers, inter and intra thread pools are set to 80 (each node have 80 cores), and workers' seastar core numbers set to 6, and parameter servers' seastar core numbers set to 10 (this configuration get better performance). We increase parameter server number to 26 or decrease it to 12, global_steps/sec remains little change. What's more, without zero copy & fuse recv feature, does this PR can perform more than 3 times faster than GRPC? Thx! @shanshanpt @liutongxuan\r\n> > \r\n> > \r\n> > I'm not sure only this PR could meet your requirement, because the performance data in RFC includes all features of grpc+seastar which i mentioned. And also W&DL's workload would highly affected by how many feature columns in it. And also your inter/intra would affect the Seastar thread, reduce the intra/inter to physical core number not logic core number, make sure there's not too much cpu-migration. Besides, why 80 cores how many physical cores, how many NUMA nodes?\r\n> \r\n> There are 40 physical cores and 2 NUMA nodes, we have set inter/intra to 40, and it has little affect.\r\n\r\nWhat's the workload of the machine? docker is configured cpushare or cpuset? What's the cpu usage in worker/ps. Besides what's the hot spots?", "> cpushare\r\n\r\n\r\n\r\n> > > > > > > > On docker, we start a container using `docker run -it --cap-add=SYS_PTRACE --cpuset-cpus=\"0-55\" --cpus=56 --security-opt seccomp=unconfined --net=host tensorflow:grpc-seastar bash`.\r\n> > > > > > > > Then another error we got :\r\n> > > > > > > > ```\r\n> > > > > > > > 2019-04-18 15:05:18.210504: I tensorflow/contrib/seastar/seastar_server_lib.cc:355] SeastarWorkerCacheFactory, name_prefix:/job:ps/replica:0/task:0\r\n> > > > > > > > 2019-04-18 15:05:18.210562: I tensorflow/contrib/seastar/seastar_server_lib.cc:370] Started server with target: grpc://localhost:31054\r\n> > > > > > > > terminate called after throwing an instance of 'std::runtime_error'\r\n> > > > > > > >   what():  insufficient processing units\r\n> > > > > > > > Aborted (core dumped)\r\n> > > > > > > > ```\r\n> > > > > > > > \r\n> > > > > > > > \r\n> > > > > > > > In tf, the seastar code 'unsigned available_procs = hwloc_get_nbobjs_by_type(topology, HWLOC_OBJ_PU)' get 1. However, if we run it in a standalone test.cc file, it get 28.\r\n> > > > > > > > ```\r\n> > > > > > > > #include <hwloc.h>\r\n> > > > > > > > int main(int argc,char **argv)\r\n> > > > > > > > {\r\n> > > > > > > >     int nPhysicalProcessorCount = 0;\r\n> > > > > > > >     hwloc_topology_t topology;\r\n> > > > > > > >     hwloc_topology_init(&topology);\r\n> > > > > > > >     hwloc_topology_load(topology);\r\n> > > > > > > >     unsigned available_procs = hwloc_get_nbobjs_by_type(topology, HWLOC_OBJ_PU);\r\n> > > > > > > >     printf(\"%d\", available_procs); // 28\r\n> > > > > > > > }\r\n> > > > > > > > ```\r\n> > > > > > > > \r\n> > > > > > > > \r\n> > > > > > > > Our machine contains 28 cores.\r\n> > > > > > > > After we export SEASTAR_CORE_NUMBER=1, 'Segmentation fault (core dumped)' arise without core dump file.\r\n> > > > > > > > What's more, why core_number_ is equals to server_number?\r\n> > > > > > > > `core_number_ = GetCoreNumber(server_number);`\r\n> > > > > > > > @liutongxuan\r\n> > > > > > > \r\n> > > > > > > \r\n> > > > > > > Seastar's hwloc is conflict with TensorFlow's. I temporary disable seastar's bind core, i'll fix this conflict in seastar side. (Disable seastar's bind core could affect performance, but compare with Grpc, still could 2 times faster in our embedding variable case.)\r\n> > > > > > > Please pull latest code. I tested several cases passed. If any questions let me know.\r\n> > > > > > > `core_number_ = GetCoreNumber(server_number);` because don't want more core than connections polling which could affect other threads. For example, if only one ps and one worker, we only need one connection which means we just need to launch one seastar thread. If there's 4 threads polling by default, would waste cpu resources.\r\n> > > > > > \r\n> > > > > > \r\n> > > > > > In the RFC: Seastar-based RPC for TF Worker Service, is shows grpc+seastar performs more than 3 times faster than GRPC. However we only get 1 times faster in our weed-and-deep or ctr models within 200 workers. If worker's number increases to 400, it performs bad than 200 workers. We wonder if hwloc conflict affects this. And Have you fixed hwloc conflict ? Thx! @liutongxuan\r\n> > > > > \r\n> > > > > \r\n> > > > > HWLOC does not affect performance if you don't use NUMA. Can you share the config parameters here, which like thread-num, core-num, bin-core and so on. Some fine-tuning may be needed here.\r\n> > > > \r\n> > > > \r\n> > > > With 200 workers and 20 parameter servers, inter and intra thread pools are set to 80 (each node have 80 cores), and workers' seastar core numbers set to 6, and parameter servers' seastar core numbers set to 10 (this configuration get better performance). We increase parameter server number to 26 or decrease it to 12, global_steps/sec remains little change. What's more, without zero copy & fuse recv feature, does this PR can perform more than 3 times faster than GRPC? Thx! @shanshanpt @liutongxuan\r\n> > > \r\n> > > \r\n> > > I'm not sure only this PR could meet your requirement, because the performance data in RFC includes all features of grpc+seastar which i mentioned. And also W&DL's workload would highly affected by how many feature columns in it. And also your inter/intra would affect the Seastar thread, reduce the intra/inter to physical core number not logic core number, make sure there's not too much cpu-migration. Besides, why 80 cores how many physical cores, how many NUMA nodes?\r\n> > \r\n> > \r\n> > There are 40 physical cores and 2 NUMA nodes, we have set inter/intra to 40, and it has little affect.\r\n> \r\n> What's the workload of the machine? docker is configured cpushare or cpuset? What's the cpu usage in worker/ps. Besides what's the hot spots?\r\n\r\nFor ps, one machine only has one ps, and the bandwidth utilization is about 75%, the cpu utilization is about 50%. For workers, one machine only has two or three workers, and the bandwidth utilization is about 20%, the cpu utilization is about 15%. \r\n\r\nCpushare or cpuset is not configured in docker.\r\n\r\n", "> > > > > > > On docker, we start a container using `docker run -it --cap-add=SYS_PTRACE --cpuset-cpus=\"0-55\" --cpus=56 --security-opt seccomp=unconfined --net=host tensorflow:grpc-seastar bash`.\r\n> > > > > > > Then another error we got :\r\n> > > > > > > ```\r\n> > > > > > > 2019-04-18 15:05:18.210504: I tensorflow/contrib/seastar/seastar_server_lib.cc:355] SeastarWorkerCacheFactory, name_prefix:/job:ps/replica:0/task:0\r\n> > > > > > > 2019-04-18 15:05:18.210562: I tensorflow/contrib/seastar/seastar_server_lib.cc:370] Started server with target: grpc://localhost:31054\r\n> > > > > > > terminate called after throwing an instance of 'std::runtime_error'\r\n> > > > > > >   what():  insufficient processing units\r\n> > > > > > > Aborted (core dumped)\r\n> > > > > > > ```\r\n> > > > > > > \r\n> > > > > > > \r\n> > > > > > > In tf, the seastar code 'unsigned available_procs = hwloc_get_nbobjs_by_type(topology, HWLOC_OBJ_PU)' get 1. However, if we run it in a standalone test.cc file, it get 28.\r\n> > > > > > > ```\r\n> > > > > > > #include <hwloc.h>\r\n> > > > > > > int main(int argc,char **argv)\r\n> > > > > > > {\r\n> > > > > > >     int nPhysicalProcessorCount = 0;\r\n> > > > > > >     hwloc_topology_t topology;\r\n> > > > > > >     hwloc_topology_init(&topology);\r\n> > > > > > >     hwloc_topology_load(topology);\r\n> > > > > > >     unsigned available_procs = hwloc_get_nbobjs_by_type(topology, HWLOC_OBJ_PU);\r\n> > > > > > >     printf(\"%d\", available_procs); // 28\r\n> > > > > > > }\r\n> > > > > > > ```\r\n> > > > > > > \r\n> > > > > > > \r\n> > > > > > > Our machine contains 28 cores.\r\n> > > > > > > After we export SEASTAR_CORE_NUMBER=1, 'Segmentation fault (core dumped)' arise without core dump file.\r\n> > > > > > > What's more, why core_number_ is equals to server_number?\r\n> > > > > > > `core_number_ = GetCoreNumber(server_number);`\r\n> > > > > > > @liutongxuan\r\n> > > > > > \r\n> > > > > > \r\n> > > > > > Seastar's hwloc is conflict with TensorFlow's. I temporary disable seastar's bind core, i'll fix this conflict in seastar side. (Disable seastar's bind core could affect performance, but compare with Grpc, still could 2 times faster in our embedding variable case.)\r\n> > > > > > Please pull latest code. I tested several cases passed. If any questions let me know.\r\n> > > > > > `core_number_ = GetCoreNumber(server_number);` because don't want more core than connections polling which could affect other threads. For example, if only one ps and one worker, we only need one connection which means we just need to launch one seastar thread. If there's 4 threads polling by default, would waste cpu resources.\r\n> > > > > \r\n> > > > > \r\n> > > > > In the RFC: Seastar-based RPC for TF Worker Service, is shows grpc+seastar performs more than 3 times faster than GRPC. However we only get 1 times faster in our weed-and-deep or ctr models within 200 workers. If worker's number increases to 400, it performs bad than 200 workers. We wonder if hwloc conflict affects this. And Have you fixed hwloc conflict ? Thx! @liutongxuan\r\n> > > > \r\n> > > > \r\n> > > > HWLOC does not affect performance if you don't use NUMA. Can you share the config parameters here, which like thread-num, core-num, bin-core and so on. Some fine-tuning may be needed here.\r\n> > > \r\n> > > \r\n> > > With 200 workers and 20 parameter servers, inter and intra thread pools are set to 80 (each node have 80 cores), and workers' seastar core numbers set to 6, and parameter servers' seastar core numbers set to 10 (this configuration get better performance). We increase parameter server number to 26 or decrease it to 12, global_steps/sec remains little change. What's more, without zero copy & fuse recv feature, does this PR can perform more than 3 times faster than GRPC? Thx! @shanshanpt @liutongxuan\r\n> > \r\n> > \r\n> > I'm not sure only this PR could meet your requirement, because the performance data in RFC includes all features of grpc+seastar which i mentioned. And also W&DL's workload would highly affected by how many feature columns in it. And also your inter/intra would affect the Seastar thread, reduce the intra/inter to physical core number not logic core number, make sure there's not too much cpu-migration. Besides, why 80 cores how many physical cores, how many NUMA nodes?\r\n> \r\n> There are 40 physical cores and 2 NUMA nodes, we have set inter/intra to 40, and it has little affect.\r\n\r\nYou'd better provide a timeline here. Grpc++ has different effects on different experimental performance.", "Adding +swpnlptl to take a look as well. He is our new networking expert.", "> > > > > > > > On docker, we start a container using `docker run -it --cap-add=SYS_PTRACE --cpuset-cpus=\"0-55\" --cpus=56 --security-opt seccomp=unconfined --net=host tensorflow:grpc-seastar bash`.\r\n> > > > > > > > Then another error we got :\r\n> > > > > > > > ```\r\n> > > > > > > > 2019-04-18 15:05:18.210504: I tensorflow/contrib/seastar/seastar_server_lib.cc:355] SeastarWorkerCacheFactory, name_prefix:/job:ps/replica:0/task:0\r\n> > > > > > > > 2019-04-18 15:05:18.210562: I tensorflow/contrib/seastar/seastar_server_lib.cc:370] Started server with target: grpc://localhost:31054\r\n> > > > > > > > terminate called after throwing an instance of 'std::runtime_error'\r\n> > > > > > > >   what():  insufficient processing units\r\n> > > > > > > > Aborted (core dumped)\r\n> > > > > > > > ```\r\n> > > > > > > > \r\n> > > > > > > > \r\n> > > > > > > > In tf, the seastar code 'unsigned available_procs = hwloc_get_nbobjs_by_type(topology, HWLOC_OBJ_PU)' get 1. However, if we run it in a standalone test.cc file, it get 28.\r\n> > > > > > > > ```\r\n> > > > > > > > #include <hwloc.h>\r\n> > > > > > > > int main(int argc,char **argv)\r\n> > > > > > > > {\r\n> > > > > > > >     int nPhysicalProcessorCount = 0;\r\n> > > > > > > >     hwloc_topology_t topology;\r\n> > > > > > > >     hwloc_topology_init(&topology);\r\n> > > > > > > >     hwloc_topology_load(topology);\r\n> > > > > > > >     unsigned available_procs = hwloc_get_nbobjs_by_type(topology, HWLOC_OBJ_PU);\r\n> > > > > > > >     printf(\"%d\", available_procs); // 28\r\n> > > > > > > > }\r\n> > > > > > > > ```\r\n> > > > > > > > \r\n> > > > > > > > \r\n> > > > > > > > Our machine contains 28 cores.\r\n> > > > > > > > After we export SEASTAR_CORE_NUMBER=1, 'Segmentation fault (core dumped)' arise without core dump file.\r\n> > > > > > > > What's more, why core_number_ is equals to server_number?\r\n> > > > > > > > `core_number_ = GetCoreNumber(server_number);`\r\n> > > > > > > > @liutongxuan\r\n> > > > > > > \r\n> > > > > > > \r\n> > > > > > > Seastar's hwloc is conflict with TensorFlow's. I temporary disable seastar's bind core, i'll fix this conflict in seastar side. (Disable seastar's bind core could affect performance, but compare with Grpc, still could 2 times faster in our embedding variable case.)\r\n> > > > > > > Please pull latest code. I tested several cases passed. If any questions let me know.\r\n> > > > > > > `core_number_ = GetCoreNumber(server_number);` because don't want more core than connections polling which could affect other threads. For example, if only one ps and one worker, we only need one connection which means we just need to launch one seastar thread. If there's 4 threads polling by default, would waste cpu resources.\r\n> > > > > > \r\n> > > > > > \r\n> > > > > > In the RFC: Seastar-based RPC for TF Worker Service, is shows grpc+seastar performs more than 3 times faster than GRPC. However we only get 1 times faster in our weed-and-deep or ctr models within 200 workers. If worker's number increases to 400, it performs bad than 200 workers. We wonder if hwloc conflict affects this. And Have you fixed hwloc conflict ? Thx! @liutongxuan\r\n> > > > > \r\n> > > > > \r\n> > > > > HWLOC does not affect performance if you don't use NUMA. Can you share the config parameters here, which like thread-num, core-num, bin-core and so on. Some fine-tuning may be needed here.\r\n> > > > \r\n> > > > \r\n> > > > With 200 workers and 20 parameter servers, inter and intra thread pools are set to 80 (each node have 80 cores), and workers' seastar core numbers set to 6, and parameter servers' seastar core numbers set to 10 (this configuration get better performance). We increase parameter server number to 26 or decrease it to 12, global_steps/sec remains little change. What's more, without zero copy & fuse recv feature, does this PR can perform more than 3 times faster than GRPC? Thx! @shanshanpt @liutongxuan\r\n> > > \r\n> > > \r\n> > > I'm not sure only this PR could meet your requirement, because the performance data in RFC includes all features of grpc+seastar which i mentioned. And also W&DL's workload would highly affected by how many feature columns in it. And also your inter/intra would affect the Seastar thread, reduce the intra/inter to physical core number not logic core number, make sure there's not too much cpu-migration. Besides, why 80 cores how many physical cores, how many NUMA nodes?\r\n> > \r\n> > \r\n> > There are 40 physical cores and 2 NUMA nodes, we have set inter/intra to 40, and it has little affect.\r\n> \r\n> You'd better provide a timeline here. Grpc++ has different effects on different experimental performance.\r\n\r\nWe add logger_->RecordRecvTensor() in RecvTensorAsync() method of seastar_remote_worker.cc file.\r\nFrom timeline we find that some RecvTensor take about 50ms, and the timeline total time is about 120ms. @shanshanpt ", "> > > > > > > > > On docker, we start a container using `docker run -it --cap-add=SYS_PTRACE --cpuset-cpus=\"0-55\" --cpus=56 --security-opt seccomp=unconfined --net=host tensorflow:grpc-seastar bash`.\r\n> > > > > > > > > Then another error we got :\r\n> > > > > > > > > ```\r\n> > > > > > > > > 2019-04-18 15:05:18.210504: I tensorflow/contrib/seastar/seastar_server_lib.cc:355] SeastarWorkerCacheFactory, name_prefix:/job:ps/replica:0/task:0\r\n> > > > > > > > > 2019-04-18 15:05:18.210562: I tensorflow/contrib/seastar/seastar_server_lib.cc:370] Started server with target: grpc://localhost:31054\r\n> > > > > > > > > terminate called after throwing an instance of 'std::runtime_error'\r\n> > > > > > > > >   what():  insufficient processing units\r\n> > > > > > > > > Aborted (core dumped)\r\n> > > > > > > > > ```\r\n> > > > > > > > > \r\n> > > > > > > > > \r\n> > > > > > > > > In tf, the seastar code 'unsigned available_procs = hwloc_get_nbobjs_by_type(topology, HWLOC_OBJ_PU)' get 1. However, if we run it in a standalone test.cc file, it get 28.\r\n> > > > > > > > > ```\r\n> > > > > > > > > #include <hwloc.h>\r\n> > > > > > > > > int main(int argc,char **argv)\r\n> > > > > > > > > {\r\n> > > > > > > > >     int nPhysicalProcessorCount = 0;\r\n> > > > > > > > >     hwloc_topology_t topology;\r\n> > > > > > > > >     hwloc_topology_init(&topology);\r\n> > > > > > > > >     hwloc_topology_load(topology);\r\n> > > > > > > > >     unsigned available_procs = hwloc_get_nbobjs_by_type(topology, HWLOC_OBJ_PU);\r\n> > > > > > > > >     printf(\"%d\", available_procs); // 28\r\n> > > > > > > > > }\r\n> > > > > > > > > ```\r\n> > > > > > > > > \r\n> > > > > > > > > \r\n> > > > > > > > > Our machine contains 28 cores.\r\n> > > > > > > > > After we export SEASTAR_CORE_NUMBER=1, 'Segmentation fault (core dumped)' arise without core dump file.\r\n> > > > > > > > > What's more, why core_number_ is equals to server_number?\r\n> > > > > > > > > `core_number_ = GetCoreNumber(server_number);`\r\n> > > > > > > > > @liutongxuan\r\n> > > > > > > > \r\n> > > > > > > > \r\n> > > > > > > > Seastar's hwloc is conflict with TensorFlow's. I temporary disable seastar's bind core, i'll fix this conflict in seastar side. (Disable seastar's bind core could affect performance, but compare with Grpc, still could 2 times faster in our embedding variable case.)\r\n> > > > > > > > Please pull latest code. I tested several cases passed. If any questions let me know.\r\n> > > > > > > > `core_number_ = GetCoreNumber(server_number);` because don't want more core than connections polling which could affect other threads. For example, if only one ps and one worker, we only need one connection which means we just need to launch one seastar thread. If there's 4 threads polling by default, would waste cpu resources.\r\n> > > > > > > \r\n> > > > > > > \r\n> > > > > > > In the RFC: Seastar-based RPC for TF Worker Service, is shows grpc+seastar performs more than 3 times faster than GRPC. However we only get 1 times faster in our weed-and-deep or ctr models within 200 workers. If worker's number increases to 400, it performs bad than 200 workers. We wonder if hwloc conflict affects this. And Have you fixed hwloc conflict ? Thx! @liutongxuan\r\n> > > > > > \r\n> > > > > > \r\n> > > > > > HWLOC does not affect performance if you don't use NUMA. Can you share the config parameters here, which like thread-num, core-num, bin-core and so on. Some fine-tuning may be needed here.\r\n> > > > > \r\n> > > > > \r\n> > > > > With 200 workers and 20 parameter servers, inter and intra thread pools are set to 80 (each node have 80 cores), and workers' seastar core numbers set to 6, and parameter servers' seastar core numbers set to 10 (this configuration get better performance). We increase parameter server number to 26 or decrease it to 12, global_steps/sec remains little change. What's more, without zero copy & fuse recv feature, does this PR can perform more than 3 times faster than GRPC? Thx! @shanshanpt @liutongxuan\r\n> > > > \r\n> > > > \r\n> > > > I'm not sure only this PR could meet your requirement, because the performance data in RFC includes all features of grpc+seastar which i mentioned. And also W&DL's workload would highly affected by how many feature columns in it. And also your inter/intra would affect the Seastar thread, reduce the intra/inter to physical core number not logic core number, make sure there's not too much cpu-migration. Besides, why 80 cores how many physical cores, how many NUMA nodes?\r\n> > > \r\n> > > \r\n> > > There are 40 physical cores and 2 NUMA nodes, we have set inter/intra to 40, and it has little affect.\r\n> > \r\n> > \r\n> > You'd better provide a timeline here. Grpc++ has different effects on different experimental performance.\r\n> \r\n> We add logger_->RecordRecvTensor() in RecvTensorAsync() method of seastar_remote_worker.cc file.\r\n> From timeline we find that some RecvTensor take about 50ms, and the timeline total time is about 120ms. @shanshanpt\r\n\r\nForward or backward's RecvTensor? Have u analyze all RecvTensor's transmission size, and what's the distribution of them?", "> transmission size\r\n\r\nps's RecvTensor cost 50ms, so it is the backward's RecvTensor ? \r\nAll variables are partitioned by partitioner=tf.min_max_variable_partitioner(ps_numbers), so all variables are averaged. After partition, top 2 variables' size are 240M and 80M. But they are sparse tensor, we do not known is sparse tensor optimized when transmission ?\r\n\r\nWe sample server workers timeline, the RecvTensor's duration as follows:\r\n```\r\n| worker number | ps number | RecvTensor's duration | total duration |\r\n-----------------------------------------------------------------------------\r\n|     worker 0    |    ps 3     |        50ms         |        80ms        |\r\n-----------------------------------------------------------------------------\r\n|     worker 1    |   ps 1      |        50ms         |      140ms        |\r\n|     worker 1    |    ps 12   |        51ms         |      140ms        |\r\n|     worker 1    |   ps 14    |        59ms         |      140ms        |\r\n-----------------------------------------------------------------------------\r\n|     worker 10  |    ps 10   |        54ms         |        80ms        |\r\n|     worker 10  |    ps 11   |        54ms         |        80ms        |\r\n|     worker 10  |    ps 12   |        54ms         |        80ms        |\r\n-----------------------------------------------------------------------------\r\n|     worker 11  |    ps 2     |        54ms         |        80ms        |\r\n|     worker 11  |    ps 4     |        48ms         |        80ms        |\r\n|     worker 11  |    ps 9     |        52ms         |        80ms        |\r\n-----------------------------------------------------------------------------\r\n|   worker 100  |    ps 15   |        62ms         |       100ms       |\r\n|   worker 100  |    ps 3     |        22ms         |       100ms       |\r\n-----------------------------------------------------------------------------\r\n```\r\nHere, we have 16 pss and 200 workers. \r\n\r\nThe code view profiler as follows:\r\n\r\n```\r\n=========================Options=============================\r\n-max_depth                  1000\r\n-min_bytes                  0\r\n-min_peak_bytes             0\r\n-min_residual_bytes         0\r\n-min_output_bytes           0\r\n-min_micros                 1000\r\n-min_accelerator_micros     0\r\n-min_cpu_micros             0\r\n-min_params                 0\r\n-min_float_ops              0\r\n-min_occurrence             0\r\n-step                       -1\r\n-order_by                   micros\r\n-account_type_regexes       .*\r\n-start_name_regexes         .*\r\n-trim_name_regexes          \r\n-show_name_regexes          .*\r\n-hide_name_regexes          \r\n-account_displayed_op_only  false\r\n-select                     micros\r\n-output                     stdout:\r\n\r\n==================Model Analysis Report======================\r\n\r\nDoc:\r\ncode: When python trace is available, the nodes are python lines and their are organized by the python call stack.\r\ntotal execution time: Sum of accelerator execution time and cpu execution time.\r\ncpu execution time: The time from the start to the end of the operation. It's the sum of actual cpu run time plus the time that it spends waiting if part of computation is launched asynchronously.\r\naccelerator execution time: Time spent executing on the accelerator. This is normally measured by the actual hardware library.\r\n\r\nProfile:\r\nnode name | total execution time | accelerator execution time | cpu execution time\r\n_TFProfRoot (--/37.18ms, --/0us, --/37.18ms)\r\n  tf_test.py:214:<module> (0us/19.17ms, 0us/0us, 0us/19.17ms)\r\n    optimizer.py:410:minimize (0us/19.16ms, 0us/0us, 0us/19.16ms)\r\n      optimizer.py:610:apply_gradients (0us/18.16ms, 0us/0us, 0us/18.16ms)\r\n        optimizer.py:128:update_op (0us/17.50ms, 0us/0us, 0us/17.50ms)\r\n          optimizer.py:1014:_apply_sparse_dup... (0us/10.80ms, 0us/0us, 0us/10.80ms)\r\n            optimizer.py:73:_deduplicate_inde... (0us/5.60ms, 0us/0us, 0us/5.60ms)\r\n              array_ops.py:1257:unique (0us/5.60ms, 0us/0us, 0us/5.60ms)\r\n                gen_array_ops.py:9011:unique (0us/5.60ms, 0us/0us, 0us/5.60ms)\r\n                  op_def_library.py:787:_apply_op_helper (0us/5.60ms, 0us/0us, 0us/5.60ms)\r\n                    deprecation.py:488:new_func (0us/5.60ms, 0us/0us, 0us/5.60ms)\r\n                      ops.py:3274:create_op (0us/5.60ms, 0us/0us, 0us/5.60ms)\r\n                        ops.py:1770:__init__ (5.60ms/5.60ms, 0us/0us, 5.60ms/5.60ms)\r\n            optimizer.py:76:_deduplicate_inde... (0us/5.20ms, 0us/0us, 0us/5.20ms)\r\n              gen_math_ops.py:9037:unsorted_segment_... (0us/2.69ms, 0us/0us, 0us/2.69ms)\r\n                op_def_library.py:787:_apply_op_helper (0us/2.69ms, 0us/0us, 0us/2.69ms)\r\n                  deprecation.py:488:new_func (0us/2.69ms, 0us/0us, 0us/2.69ms)\r\n                    ops.py:3274:create_op (0us/2.69ms, 0us/0us, 0us/2.69ms)\r\n                      ops.py:1770:__init__ (2.69ms/2.69ms, 0us/0us, 2.69ms/2.69ms)\r\n              array_ops.py:500:_slice_helper (0us/1.02ms, 0us/0us, 0us/1.02ms)\r\n                array_ops.py:863:stack (0us/1.02ms, 0us/0us, 0us/1.02ms)\r\n                  ops.py:1050:convert_to_tensor (0us/1.02ms, 0us/0us, 0us/1.02ms)\r\n                    ops.py:1146:internal_convert_... (0us/1.02ms, 0us/0us, 0us/1.02ms)\r\n                      constant_op.py:229:_constant_tensor_... (0us/1.02ms, 0us/0us, 0us/1.02ms)\r\n                        constant_op.py:214:constant (0us/1.02ms, 0us/0us, 0us/1.02ms)\r\n                          deprecation.py:488:new_func (0us/1.02ms, 0us/0us, 0us/1.02ms)\r\n                            ops.py:3274:create_op (0us/1.02ms, 0us/0us, 0us/1.02ms)\r\n                              ops.py:1770:__init__ (1.02ms/1.02ms, 0us/0us, 1.02ms/1.02ms)\r\n          optimizer.py:1019:_apply_sparse_dup... (0us/6.70ms, 0us/0us, 0us/6.70ms)\r\n            adagrad.py:122:_apply_sparse (0us/6.70ms, 0us/0us, 0us/6.70ms)\r\n              gen_training_ops.py:3110:sparse_apply_adag... (0us/6.70ms, 0us/0us, 0us/6.70ms)\r\n                op_def_library.py:787:_apply_op_helper (0us/6.70ms, 0us/0us, 0us/6.70ms)\r\n                  deprecation.py:488:new_func (0us/6.70ms, 0us/0us, 0us/6.70ms)\r\n                    ops.py:3274:create_op (0us/6.70ms, 0us/0us, 0us/6.70ms)\r\n                      ops.py:1770:__init__ (6.70ms/6.70ms, 0us/0us, 6.70ms/6.70ms)\r\n\r\n======================End of Report==========================\r\n```\r\n\r\n@liutongxuan \r\n", "@liutongxuan , is your performance test based on FuseRecv feature or not? Does enabling FuseRecv bring performance enhancement and how much performance gain does it bring? Thank you for comments on this matter.", "> @liutongxuan , is your performance test based on FuseRecv feature or not? Does enabling FuseRecv bring performance enhancement and how much performance gain does it bring? Thank you for comments on this matter.\r\n\r\nFuseRecv RFC: https://docs.google.com/document/d/1tQc0vGk-6_SjxdcHkIsFzd-8djtWyBazedZcTn3tce4/edit#     FuseRecv would bring 40% improvement.\r\n\r\nThe grpc+seastar doesn't include FuseRecv.", "> > @liutongxuan , is your performance test based on FuseRecv feature or not? Does enabling FuseRecv bring performance enhancement and how much performance gain does it bring? Thank you for comments on this matter.\r\n> \r\n> FuseRecv RFC: https://docs.google.com/document/d/1tQc0vGk-6_SjxdcHkIsFzd-8djtWyBazedZcTn3tce4/edit# FuseRecv would bring 40% improvement.\r\n> \r\n> The grpc+seastar doesn't include FuseRecv.\r\n@liutongxuan Thanks for your response. This is very interesting idea. However I could not find any code changes regarding the FuseRecv Op and graph topology mentioned in the RFC above. Would you mind creating a branch for the Fuse related changes or send a version of it to my email weidai932019@gmail.com?  ", "@swpnlptl hey, any update of the PR?", "There are a couple of issues standing, which I will address in the next week:\r\n\r\n1. Sync with latest TF master with rebase\r\n2. Instead of using @liutongxuan's fork of seastar and boost, use upstream directly with patchset\r\n\r\nBoost one seems easier, it's just plain 1.63.0. Seastar is complex though; the upstream is dated back to 2017, which makes the plugin hard to upgrade and maintain later on.\r\n", "> There are a couple of issues standing, which I will address in the next week:\r\n> \r\n> 1. Sync with latest TF master with rebase\r\n> 2. Instead of using @liutongxuan's fork of seastar and boost, use upstream directly with patchset\r\n> \r\n> Boost one seems easier, it's just plain 1.63.0. Seastar is complex though; the upstream is dated back to 2017, which makes the plugin hard to upgrade and maintain later on.\r\n\r\nHey Bairen, i could move the Seastar repo into Alibaba-PAI group, is that ok?", "That\u2019s completely fine. You could submit a new PR from PAI; when you are done, let me know and I will close this one.", "> That\u2019s completely fine. You could submit a new PR from PAI; when you are done, let me know and I will close this one.\r\n\r\nNot this PR, i mean i could move the Seastar repo from my own to PAI group.", "@swpnlptl I agree that \"boost/asio/ip/address_v4.hpp\" could be removed from this patchset. However, Seastar uses boost quite heavily, so we might need to add it as a transitive dependency after all.", "A patch to remove CUDA dependencies all together, and also fixes the breaks for CUDA build.\r\n\r\n```\r\ndiff --git a/tensorflow/contrib/seastar/BUILD b/tensorflow/contrib/seastar/BUILD\r\nindex d90da244d6..0d2b3cb269 100644\r\n--- a/tensorflow/contrib/seastar/BUILD\r\n+++ b/tensorflow/contrib/seastar/BUILD\r\n@@ -24,11 +24,8 @@ filegroup(\r\n\r\n load(\r\n     \"//tensorflow:tensorflow.bzl\",\r\n-    \"tf_cuda_library\",\r\n     \"tf_cc_test\",\r\n )\r\n-load(\"//tensorflow:tensorflow.bzl\", \"tf_cuda_cc_test\")\r\n-load(\"//tensorflow:tensorflow.bzl\", \"tf_cuda_cc_tests\")\r\n load(\"//tensorflow:tensorflow.bzl\", \"tf_cc_binary\")\r\n\r\n # For platform specific build config\r\n@@ -36,10 +33,6 @@ load(\r\n     \"//tensorflow/core:platform/default/build_config.bzl\",\r\n     \"tf_kernel_tests_linkstatic\",\r\n )\r\n-load(\r\n-    \"//tensorflow/core:platform/default/build_config_root.bzl\",\r\n-    \"tf_cuda_tests_tags\",\r\n-)\r\n\r\n package(default_visibility = [\r\n     \"//tensorflow:internal\",\r\n@@ -59,7 +52,7 @@ cc_library(\r\n     ],\r\n )\r\n\r\n-tf_cuda_library(\r\n+cc_library(\r\n     name = \"seastar_worker_service\",\r\n     srcs = select({\"//tensorflow:with_seastar_support\": [\"seastar_worker_service.cc\",\r\n                                                          \"seastar_client_tag.cc\",\r\n@@ -86,7 +79,6 @@ tf_cuda_library(\r\n                                                          \":seastar_tensor_coding\"],\r\n                    \"//conditions:default\": []})\r\n     + [\r\n-        \"//tensorflow/contrib/verbs:verbs_util\",\r\n         \"//tensorflow/core:lib\",\r\n         \"//tensorflow/core:master_proto_cc\",\r\n         \"//tensorflow/core:worker_proto_cc\",\r\ndiff --git a/tensorflow/contrib/seastar/seastar_client_tag.cc b/tensorflow/contrib/seastar/seastar_client_tag.cc\r\nindex 107b3893a9..55f8586d9b 100644\r\n--- a/tensorflow/contrib/seastar/seastar_client_tag.cc\r\n+++ b/tensorflow/contrib/seastar/seastar_client_tag.cc\r\n@@ -1,8 +1,4 @@\r\n #include \"tensorflow/core/common_runtime/dma_helper.h\"\r\n-#if GOOGLE_CUDA\r\n-#include \"tensorflow/core/common_runtime/gpu/gpu_util.h\"\r\n-#include \"tensorflow/core/common_runtime/gpu/process_state.h\"\r\n-#endif // GOOGLE_CUDA\r\n #include \"tensorflow/core/distributed_runtime/worker_env.h\"\r\n #include \"tensorflow/core/lib/core/threadpool.h\"\r\n #include \"tensorflow/core/platform/logging.h\"\r\n@@ -98,8 +94,10 @@ void InitSeastarClientTag(protobuf::Message* request,\r\n     if (can_memcpy) {\r\n       if (response->GetDevice()->tensorflow_gpu_device_info() &&\r\n           (!response->GetOnHost())) {\r\n- #if GOOGLE_CUDA\r\n-        Allocator* alloc = ProcessState::singleton()->GetCUDAHostAllocator(0);\r\n+        AllocatorAttributes alloc_attrs;\r\n+        alloc_attrs.set_gpu_compatible(true);\r\n+        alloc_attrs.set_on_host(true);\r\n+        Allocator* alloc = response->GetDevice()->GetAllocator(alloc_attrs);\r\n         Tensor cpu_copy(alloc, sm.data_type_, sm.tensor_shape_);\r\n\r\n         tag->resp_tensor_buf_.data_\r\n@@ -108,9 +106,6 @@ void InitSeastarClientTag(protobuf::Message* request,\r\n         tag->resp_tensor_buf_.owned_ = false;\r\n\r\n         response->SetTensor(cpu_copy);\r\n-#else\r\n-        return errors::Internal(\"No GPU device in process\");\r\n-#endif\r\n\r\n       } else {\r\n         Tensor val(response->GetAlloc(), sm.data_type_, sm.tensor_shape_);\r\n@@ -145,14 +140,11 @@ void InitSeastarClientTag(protobuf::Message* request,\r\n     if (can_memcpy) {\r\n       if (response->GetDevice()->tensorflow_gpu_device_info() &&\r\n           (!response->GetOnHost())) {\r\n-#if GOOGLE_CUDA\r\n         Tensor* gpu_copy = new Tensor(response->GetAlloc(),\r\n                                       response->GetTensor().dtype(),\r\n                                       response->GetTensor().shape());\r\n-        GPUUtil::CopyCPUTensorToGPU(&response->GetTensor(),\r\n-                                    response->GetDevice()->tensorflow_gpu_device_info()->default_context,\r\n-                                    response->GetDevice(),\r\n-                                    gpu_copy,\r\n+        DeviceContext* recv_dev_context = response->GetDevice()->tensorflow_gpu_device_info()->default_context;\r\n+        recv_dev_context->CopyCPUTensorToDevice(&response->GetTensor(), response->GetDevice(), gpu_copy,\r\n                                     [gpu_copy, response, done, tag](const Status& s) {\r\n                                       CHECK(s.ok()) << \"copy tensor to gpu sync\";\r\n                                       response->SetTensor(*gpu_copy);\r\n@@ -160,10 +152,6 @@ void InitSeastarClientTag(protobuf::Message* request,\r\n                                       delete gpu_copy;\r\n                                       delete tag;\r\n                                     });\r\n-#else\r\n-        done(errors::Internal(\"No GPU device in process\"));\r\n-        delete tag;\r\n-#endif\r\n       } else {\r\n         done(s);\r\n         delete tag;\r\ndiff --git a/tensorflow/contrib/seastar/seastar_rendezvous_mgr.cc b/tensorflow/contrib/seastar/seastar_rendezvous_mgr.cc\r\nindex 868f7d4f9c..c642159b67 100644\r\n--- a/tensorflow/contrib/seastar/seastar_rendezvous_mgr.cc\r\n+++ b/tensorflow/contrib/seastar/seastar_rendezvous_mgr.cc\r\n@@ -200,7 +200,7 @@ void SeastarRemoteRendezvous::RecvFromRemoteAsync(\r\n                          \" is invalid remote source device.\");\r\n   }\r\n   WorkerSession* sess = session();\r\n-  WorkerInterface* rwi = sess->worker_cache->CreateWorker(call->src_worker_);\r\n+  WorkerInterface* rwi = sess->worker_cache->GetOrCreateWorker(call->src_worker_);\r\n   if (s.ok() && rwi == nullptr) {\r\n     s = errors::Internal(\"No worker known as \", call->src_worker_);\r\n   }\r\ndiff --git a/tensorflow/contrib/seastar/seastar_worker_cache.cc b/tensorflow/contrib/seastar/seastar_worker_cache.cc\r\nindex 246ae646d0..597baf6c21 100644\r\n--- a/tensorflow/contrib/seastar/seastar_worker_cache.cc\r\n+++ b/tensorflow/contrib/seastar/seastar_worker_cache.cc\r\n@@ -30,7 +30,7 @@ public:\r\n     channel_cache_->ListWorkersInJob(job_name, workers);\r\n   }\r\n\r\n-  WorkerInterface* CreateWorker(const string& target) override {\r\n+  WorkerInterface* GetOrCreateWorker(const string& target) override {\r\n     if (target == local_target_) {\r\n       return local_worker_;\r\n     } else {\r\ndiff --git a/tensorflow/contrib/seastar/seastar_worker_service.cc b/tensorflow/contrib/seastar/seastar_worker_service.cc\r\nindex 52b187149e..77f4d9b827 100644\r\n--- a/tensorflow/contrib/seastar/seastar_worker_service.cc\r\n+++ b/tensorflow/contrib/seastar/seastar_worker_service.cc\r\n@@ -2,7 +2,6 @@\r\n #include \"tensorflow/contrib/seastar/seastar_tag_factory.h\"\r\n #include \"tensorflow/contrib/seastar/seastar_tensor_coding.h\"\r\n #include \"tensorflow/contrib/seastar/seastar_worker_service.h\"\r\n-#include \"tensorflow/contrib/verbs/verbs_util.h\"\r\n #include \"tensorflow/core/common_runtime/device_factory.h\"\r\n #include \"tensorflow/core/distributed_runtime/call_options.h\"\r\n #include \"tensorflow/core/distributed_runtime/rendezvous_mgr_interface.h\"\r\n@@ -11,11 +10,6 @@\r\n #include \"tensorflow/core/platform/logging.h\"\r\n #include \"tensorflow/core/public/session_options.h\"\r\n\r\n-#if GOOGLE_CUDA\r\n-#include \"tensorflow/core/common_runtime/gpu/gpu_util.h\"\r\n-#include \"tensorflow/core/common_runtime/gpu/process_state.h\"\r\n-#endif  // GOOGLE_CUDA\r\n-\r\n namespace tensorflow {\r\n namespace {\r\n\r\n@@ -285,43 +279,25 @@ void SeastarWorker::RecvTensorAsync(CallOptions* opts,\r\n\r\n           if (src_dev->tensorflow_gpu_device_info() &&\r\n               (!send_args.alloc_attrs.on_host())) {\r\n-#if GOOGLE_CUDA\r\n             CHECK(send_args.device_context)\r\n               << \"send dev name: \" << src_dev->name()\r\n               << \" gpu_info: \" << src_dev->tensorflow_gpu_device_info();\r\n\r\n-            if (can_memcpy) {\r\n-              Allocator* alloc =\r\n-                ProcessState::singleton()->GetCUDAHostAllocator(0);\r\n-              Tensor* cpu_copy =\r\n-                new Tensor(alloc, val.dtype(), val.shape());\r\n-\r\n-              GPUUtil::CopyGPUTensorToCPU(\r\n-                  src_dev, send_args.device_context, &val, cpu_copy,\r\n-                  [response, cpu_copy, done](const Status& s) {\r\n-                    CHECK(s.ok()) << \"copy tensor from gpu sync\";\r\n-                    response->SetTensor(*cpu_copy);\r\n-                    delete cpu_copy;\r\n-                    done(s);\r\n-                  });\r\n-            } else {\r\n-              // NOTE(rangeng.llb): Should not be executed currrently.\r\n-              Tensor* copy = new Tensor(val);\r\n-              GPUUtil::SetProtoFromGPU(*copy,\r\n-                  src_dev,\r\n-                  send_args.device_context,\r\n-                  &response->GetTensorProto(),\r\n-                  is_dead,\r\n-                  [response, copy, done] (const Status& s) {\r\n-                    CHECK(s.ok()) << \"copy proto from gpu sync\";\r\n-                    response->SetTensor(*copy);\r\n-                    delete copy;\r\n-                    done(s);\r\n-                  });\r\n-            }\r\n-#else\r\n-            done(errors::Internal(\"No GPU device in process\"));\r\n-#endif\r\n+            AllocatorAttributes alloc_attrs;\r\n+            alloc_attrs.set_gpu_compatible(true);\r\n+            alloc_attrs.set_on_host(true);\r\n+            Allocator* alloc = src_dev->GetAllocator(alloc_attrs);\r\n+            Tensor* cpu_copy =\r\n+              new Tensor(alloc, val.dtype(), val.shape());\r\n+\r\n+            send_args.device_context->CopyDeviceTensorToCPU(\r\n+                &val, request->rendezvous_key(), src_dev, cpu_copy,\r\n+                [response, cpu_copy, done](const Status& s) {\r\n+                  CHECK(s.ok()) << \"copy tensor from gpu sync\";\r\n+                  response->SetTensor(*cpu_copy);\r\n+                  delete cpu_copy;\r\n+                  done(s);\r\n+                });\r\n           } else {\r\n             // tensor is in CPU memory.\r\n             response->SetTensor(val);\r\n```", "Another patch for 2019-06-25 nightly.\r\n\r\n```\r\ndiff --git a/tensorflow/contrib/seastar/seastar_worker_cache.cc b/tensorflow/contrib/seastar/seastar_worker_cache.cc\r\nindex 597baf6c21..f065a0ef70 100644\r\n--- a/tensorflow/contrib/seastar/seastar_worker_cache.cc\r\n+++ b/tensorflow/contrib/seastar/seastar_worker_cache.cc\r\n@@ -49,6 +49,12 @@ public:\r\n     }\r\n   }\r\n\r\n+   Status GetEagerClientCache(\r\n+       std::unique_ptr<eager::EagerClientCache>* eager_client_cache) override {\r\n+     return errors::Unimplemented(\r\n+         \"Eager client not yet implemented for this protocol\");\r\n+   }\r\n+\r\n   void SetLogging(bool v) override { logger_.SetLogging(v); }\r\n   void ClearLogs() override { logger_.ClearLogs(); }\r\n   bool RetrieveLogs(int64 step_id, StepStats* ss) override {\r\ndiff --git a/tensorflow/core/distributed_runtime/rpc/grpc_server_lib.h b/tensorflow/core/distributed_runtime/rpc/grpc_server_lib.h\r\nindex 2bb5b032b8..6f3bdd2cb5 100644\r\n--- a/tensorflow/core/distributed_runtime/rpc/grpc_server_lib.h\r\n+++ b/tensorflow/core/distributed_runtime/rpc/grpc_server_lib.h\r\n@@ -122,9 +122,6 @@ class GrpcServer : public ServerInterface {\r\n   const ServerDef& server_def() const { return server_def_; }\r\n   GrpcWorker* worker_impl() const { return worker_impl_.get(); }\r\n\r\n-  void set_channel_cache(GrpcChannelCache* channel_cache) {\r\n-    channel_cache_.reset(channel_cache);\r\n-  }\r\n\r\n  private:\r\n   // The overall server configuration.\r\n```", "Another patch that addresses @swpnlptl's review comments:\r\n\r\n```\r\ndiff --git a/tensorflow/contrib/seastar/seastar_server.cc b/tensorflow/contrib/seastar/seastar_server.cc\r\nindex 0845398b7c..c66fed59fa 100644\r\n--- a/tensorflow/contrib/seastar/seastar_server.cc\r\n+++ b/tensorflow/contrib/seastar/seastar_server.cc\r\n@@ -1,4 +1,5 @@\r\n-#include \"boost/asio/ip/address_v4.hpp\"\r\n+#include <arpa/inet.h>\r\n+\r\n #include \"core/reactor.hh\"\r\n #include \"core/channel.hh\"\r\n\r\n@@ -43,8 +44,8 @@ SeastarServer::Connection::Connection(seastar::connected_socket&& fd,\r\n   : tag_factory_(tag_factory),\r\n     addr_(addr) {\r\n   seastar::ipv4_addr ip_addr(addr);\r\n-  boost::asio::ip::address_v4 addr_v4(ip_addr.ip);\r\n-  string addr_str = addr_v4.to_string() + \":\" + std::to_string(ip_addr.port);\r\n+  struct in_addr addr_v4 { .s_addr = ip_addr.ip };\r\n+  string addr_str = string(inet_ntoa(addr_v4)) + \":\" + std::to_string(ip_addr.port);\r\n   channel_ = new seastar::channel(addr_str);\r\n   fd_ = std::move(fd);\r\n   fd_.set_nodelay(true);\r\n```", "@liutongxuan You may incorporate the patches above into this PR. Hope them helps!", "Oops...merging from the master branch is not good. Let me do a rebase."]}, {"number": 27453, "title": "no module namedtensorflow.python tensorflow is not a package", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["Please provide following information. Thanks!\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 27452, "title": "Import tensorflow not working on RHEL6", "body": "**System information**\r\n- Red Hat Enterprise Linux Server release 6.8 (Santiago)\r\n- Install using Virtualenv : https://www.tensorflow.org/install/pip\r\n- TensorFlow version: latest\r\n- Python version: 3.6.4\r\n- Installed using virtualenv\r\n- Bazel version (if compiling from source): Not compiling from source\r\n- GCC/Compiler version (if compiling from source): gcc (GCC) 4.4.7 20120313 (Red Hat 4.4.7-18)\r\n\r\n**Describe the problem**\r\n\r\nWe have installed Tensorflow using Virtualenv.\r\nBelow steps which we followed from our end:\r\n\r\nvirtualenv --system-site-packages -p python3 venv\r\n[root@abi venv]$ ls\r\nbin  include  lib  lib64  pip-selfcheck.json\r\nsource ~/tensorflow/venv/bin/activate\r\npip3 install --upgrade tensorflow // in venv environment\r\n \r\nAfter installation we tried to import \"tensorflow\" in python like \"import tensorflow\".\r\nFacing below issue:\r\n\r\n`(venv) [root@abinaya-simics-r6 ~]$ python\r\nPython 3.6.4 (default, Oct  8 2018, 06:53:21)\r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-4)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"/root/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/root/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/root/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/root/tensorflow/venv/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/root/tensorflow/venv/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /lib64/libc.so.6: version `GLIBC_2.16' not found (required by /root/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/root/tensorflow/venv/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/root/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/root/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/root/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/root/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/root/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/root/tensorflow/venv/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/root/tensorflow/venv/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /lib64/libc.so.6: version `GLIBC_2.16' not found (required by /root/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>>\r\n`\r\n\r\nAs per the issue we could understand that \"/root/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\" need \"GLIBC 2.14 atleast.\r\n\r\nBut available GLIBC version in RHEL6 is:\r\n\r\n`[root@abinaya-simics-r6 ~]$ strings /usr/lib64/libstdc++.so.6 | grep GLIBC_\r\nGLIBC_2.2.5\r\nGLIBC_2.3\r\nGLIBC_2.4\r\nGLIBC_2.3.2\r\n`\r\nCould you please confirm us whether the tensorflow really need latest version of GLIBC.\r\nProvide some Doc on this case for better understand.", "comments": ["@Abinayasandhiya Could you follow the instructions listed in TF [website](https://www.tensorflow.org/install/pip). \r\n$virtualenv --system-site-packages -p python3 ./venv\r\n$source ./venv/bin/activate  # sh, bash, ksh, or zsh\r\n(venv) $pip install --upgrade tensorflow\r\n(venv) $python\r\n(venv) $import tensorflow as tf.\r\n\r\nI tried the steps you followed and this (source ~/tensorflow/venv/bin/activate) step didn't work for me.\r\nPlease follow the instructions from TF website and let me know how it progresses. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 27451, "title": "ImportError: DLL load failed: Access is denied.", "body": "\r\n\r\n**System information**  \r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64bit  \r\n- TensorFlow installed from (source or binary): pip  \r\n- TensorFlow version: 1.12.0  \r\n- Python version: 3.6.7 64bit  \r\n- Installed using virtualenv? pip? conda?: No virtualenv. Direct install with pip. No conda. Virtualenv tried, but result is the same.  \r\n- CUDA/cuDNN version: cuda_9.0.176_win10, cudnn-9.0-windows10-x64-v7.5.0.56  \r\n- GPU model and memory: RTX2070 8G  \r\n- CPU model: intel i5-9400f (CPU without integrated GPU)  \r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nTo be clear, this is NOT a issue about the dll files that can not be found. Files are there, but something is wrong with the previlige.\r\n\r\nI recently built a new windows 10 PC and installed tensorflow on it. The installation was successful, but I had to run the code with Administrator. Otherwise a simple import code like \"import tensorflow as tf\" would result in \"ImportError: DLL load failed: Access is denied\" if I try to run it without \"Run as Administrator\". The code did work flawlessly if I choose to run as admin though. But on my old notebook I could execute without admin previlige.\r\n\r\nThe installations on my new and old computer were the same, so the software version and system environment variables should not be the problem. I am wondering why this is happening and is there any way I can get rid of the \"run as admin\" process which is rather danger and annoying for every time I want to run the code.\r\n\r\n\r\n\r\n**Any other info / logs**\r\n\r\n```\r\n3.6.7 (v3.6.7:6ec5cf24b7, Oct 20 2018, 13:35:33) [MSC v.1900 64 bit (AMD64)]\r\nTraceback (most recent call last):\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Access is denied.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"hello tensorflow.py\", line 12, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Access is denied.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```", "comments": ["duplicate #26059", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27451\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27451\">No</a>\n", "I just find the way to reproduce the error. Create another user on Windows 10, and you will find that when you try to execute any tensorflow related code the \"DLL load failed: Access is denied\" would come out.\r\n\r\nI think there is a problem with multiple users on Windows 10 for tensorflow. The only user who can correct start tensorflow is the first one install your system with. Any later created user would counter this kind of problem.", "Did you install tensorflow as an admin? This seems like standard OS security mechanism than an issue with tensorflow. You may want to install it as the user you want to run the code as. Or check pip documentation for user specific install: https://pip.pypa.io/en/stable/user_guide/#user-installs", "One thing worked for me is to turn off the firewall on your PC. Mine was called 360 Fire Wall. After I turned it off, everything worked well"]}, {"number": 27450, "title": "[TF 2.0 Build] Bazel version checking bug when compiling from source", "body": "**System information**\r\n- OS: macOSMojave 10.14.4 Beta\r\n- Device: MacBook Pro\r\n- TensorFlow version: 2.0.0 alpha0\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source): 0.24.0\r\n- GCC/Compiler version (if compiling from source): Apple LLVM 10.0\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: Intel HD640\r\n\r\n\r\n**Problem Description**\r\n\r\n**Bazel 0.24.0 is correctly installed in my system but tensorflow failed to recognize it and logged the following when compiling from source:**\r\n`\r\nYou have bazel 0.24.0 installed.\r\nPlease downgrade your bazel installation to version 0.24 or lower to build TensorFlow! To downgrade: download the installer for the old version (from https://github.com/bazelbuild/bazel/releases) then run the installer.\r\n`\r\n\r\nThis can be reproduced by running` ./configure` when following the official **'Build From Source'** instructions. I have investigated the issue and realize that it is caused by incorrect conversion of bazel version info to integer in **'configure.py'**.\r\n\r\nAfter adding print statements at line 472, 474, 476 of **'configure.py'**, such bug is easy to see:\r\n```python3\r\nmin_version_int = convert_version_to_int(min_version)\r\nprint('min version required:', min_version_int)\r\ncurr_version_int = convert_version_to_int(curr_version)\r\nprint('currently installed version:', curr_version_int)\r\nmax_version_int = convert_version_to_int(max_version)\r\nprint('maximum version allowed:', max_version_int)\r\n```\r\n**Then  running** `./configure` **again produces:**\r\n```\r\nmin version required: 19000\r\ncurrently installed version: 24000\r\nmaximum version allowed: 24\r\nYou have bazel 0.24.0 installed.\r\nPlease downgrade your bazel installation to version 0.24 or lower to build TensorFlow! To downgrade: download the installer for the old version (from https://github.com/bazelbuild/bazel/releases) then run the installer.\r\n```\r\nThe version checking logic in **'configure.py'** relies on such conversion.\r\nBelow is the version checking logic from line 475 to 493 in  **'configure.py'**:\r\n```python3\r\n# Check if current bazel version can be detected properly.\r\n  if not curr_version_int:\r\n    print('WARNING: current bazel installation is not a release version.')\r\n    print('Make sure you are running at least bazel %s' % min_version)\r\n    return curr_version\r\n\r\n  print('You have bazel %s installed.' % curr_version)\r\n\r\n  if curr_version_int < min_version_int:\r\n    print('Please upgrade your bazel installation to version %s or higher to '\r\n          'build TensorFlow!' % min_version)\r\n    sys.exit(1)\r\n  if (curr_version_int > max_version_int and\r\n      'TF_IGNORE_MAX_BAZEL_VERSION' not in os.environ):\r\n    print('Please downgrade your bazel installation to version %s or lower to '\r\n          'build TensorFlow! To downgrade: download the installer for the old '\r\n          'version (from https://github.com/bazelbuild/bazel/releases) then '\r\n          'run the installer.' % max_version)\r\n    sys.exit(1)\r\n```\r\n\r\nI am very willing to contribute to a fix to this issue. I will create a pull request soon after I fully tested my solution on this.\r\nThank you very much.\r\n\r\n", "comments": ["Thanks for catching this and raising new [issue](https://github.com/tensorflow/tensorflow/issues/27575) to address this problem. I will close this issue now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27450\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27450\">No</a>\n"]}, {"number": 27448, "title": "How can I use Queue with tf 2.0?", "body": "How can I use Queue with tf 2.0?\r\n\r\n~~~\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\n\r\nimport time\r\n\r\n_placeholders = [\r\n            tf.keras.backend.placeholder([None, None], dtype=tf.int32, name='inputs'),\r\n            tf.keras.backend.placeholder([None], dtype=tf.int32, name='input_lengths'),\r\n            tf.keras.backend.placeholder([None], dtype=tf.float32, name='loss_coeff'),\r\n            tf.keras.backend.placeholder([None, None,10], dtype=tf.float32, name='mel_targets'),\r\n            tf.keras.backend.placeholder([None, None, 10], dtype=tf.float32, name='linear_targets'),\r\n        ]\r\ndtypes = [tf.int32, tf.int32, tf.float32, tf.float32, tf.float32]\r\n\r\nqueue = tf.queue.FIFOQueue(capacity=10, dtypes=dtypes)\r\nenque = queue.enqueue(_placeholders)\r\n~~~", "comments": []}, {"number": 27447, "title": "How can I use Queue with tf 2.0?", "body": "How can I use Queue with tf 2.0?\r\n\r\n~~~\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\n\r\nimport time\r\n\r\n_placeholders = [\r\n            tf.keras.backend.placeholder([None, None], dtype=tf.int32, name='inputs'),\r\n            tf.keras.backend.placeholder([None], dtype=tf.int32, name='input_lengths'),\r\n            tf.keras.backend.placeholder([None], dtype=tf.float32, name='loss_coeff'),\r\n            tf.keras.backend.placeholder([None, None,10], dtype=tf.float32, name='mel_targets'),\r\n            tf.keras.backend.placeholder([None, None, 10], dtype=tf.float32, name='linear_targets'),\r\n        ]\r\ndtypes = [tf.int32, tf.int32, tf.float32, tf.float32, tf.float32]\r\n\r\nqueue = tf.queue.FIFOQueue(capacity=10, dtypes=dtypes)\r\nenque = queue.enqueue(_placeholders)\r\n~~~", "comments": []}, {"number": 27446, "title": "Compute F1 score multilabel classifier #27171", "body": "#27171 Tried again", "comments": ["@alextp please review it", "Basically in the issue related it was asked to add minor and major function in f1_score calculation similar to scipy inbuilt function. It's my first time for adding any functionality to tensorflow can you please guide me with where can I add this piece of code to affect Tf API, further I will add tests and examples.", "> It's my first time for adding any functionality to tensorflow can you please guide me with where can I add this piece of code to affect Tf API, further I will add tests and examples.\r\n\r\n@shashvatshahi1998 Thank you for offering to contribute, and for submitting this PR. To understand how this code could impact the broader TensorFlow API, I strongly suggest that you take a look at the [Contribution Guide](https://www.tensorflow.org/community/contribute), especially the [**Code Review**](https://www.tensorflow.org/community/contribute/code#code_review) and **Testing** sections. We require documentation and tests for any feature added to TensorFlow organization projects.\r\n\r\n[`scikit-learn` also has an excellent step-by-step process](https://scikit-learn.org/stable/developers/contributing.html#contributing-pull-requests) for submitting open-source PRs. Many of the steps would also apply to TensorFlow organization projects.", "There are no files changed, so I'll close this PR.", "@alextp Can you please guide me regarding this issue?\r\n", "@shashvatshahi1998 what guidance do you need? If you want to send a pull request you need to be able to send a valid pull request, which changes some files in TF. I can't teach you how to use github or how to program.", "@alextp I just wanted to know where f1 score is defined in tf.keras .I have also raised an issue regarding this so that I can contribute there as I am not getting it anywhere in tf.keras.", "I don't think tf.keras defines f1 score anywhere.\n\nOn Mon, Apr 29, 2019 at 9:31 AM Shashvat Chand Shahi <\nnotifications@github.com> wrote:\n\n> @alextp <https://github.com/alextp> I just wanted to know where f1 score\n> is defined in tf.keras .I have also raised an issue regarding this so that\n> I can contribute there as I am not getting it anywhere in tf.keras.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/27446#issuecomment-487647166>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRM2WJ27KIFQCFQ2WILPS4PGBANCNFSM4HDGMZHQ>\n> .\n>\n\n\n-- \n - Alex\n", "You asked me once to contribute this patch in tf.keras as TF2.0 is focusing more on keras  that's why I thought that it may be defined there also. "]}, {"number": 27445, "title": "Custom RNN cell's internal layers are not built properly", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMaxOSX 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nBinary\r\n- TensorFlow version (use command below):\r\n`tf.version.VERSION = '2.0.0-dev20190402'`\r\n`tf.version.GIT_VERSION = 'v1.12.0-11462-gf6732b0261'`\r\n- Python version:\r\n3.6.8\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nWhen creating a custom RNN cell containing a few layers (created in the constructor), these internal layers are not properly built the first time the cell is used.\r\n\r\n**Describe the expected behavior**\r\nI expect custom cells to behave like custom layers: the first time the cell is used, all of its internal layers should be automatically built.\r\n\r\n**Code to reproduce the issue**\r\nThe following code fails because the internal `SimpleRNNCell` is not built before it is used. If you uncomment the two lines that define the `build()` method, then everything works fine. But this should not be required (just like it is not required for custom layers):\r\n\r\n```python\r\nimport numpy as np\r\nfrom tensorflow import keras\r\n\r\nclass MyCell(keras.layers.Layer):\r\n    def __init__(self, units, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.units = units\r\n        self.state_size = units\r\n        self.simple_rnn_cell = keras.layers.SimpleRNNCell(self.units)\r\n#    def build(self, step_input_shape):\r\n#        self.simple_rnn_cell.build(step_input_shape)\r\n    def call(self, inputs, states):\r\n        outputs, new_states = self.simple_rnn_cell.call(inputs, states)\r\n        return outputs, new_states\r\n\r\nmodel = keras.models.Sequential([\r\n    keras.layers.RNN(MyCell(20), return_sequences=True),\r\n    keras.layers.RNN(MyCell(20), return_sequences=True),\r\n    keras.layers.TimeDistributed(keras.layers.Dense(1))\r\n])\r\n\r\nX = np.random.randn(1000, 30, 10)\r\nY = np.random.randn(1000, 30, 1)\r\nmodel.compile(loss=\"mse\", optimizer=\"adam\")\r\nhistory = model.fit(X, Y) # <= AttributeError: 'SimpleRNNCell' object has no attribute 'kernel'\r\n```\r\n\r\n**Other info / logs**\r\nHere is the full stacktrace:\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \".../tensorflow/python/keras/engine/training.py\", line 807, in fit\r\n    shuffle=shuffle)\r\n  File \".../tensorflow/python/keras/engine/training.py\", line 2417, in _standardize_user_data\r\n    self._set_inputs(cast_inputs)\r\n  File \".../tensorflow/python/keras/engine/training.py\", line 2646, in _set_inputs\r\n    outputs = self(inputs, **kwargs)\r\n  File \".../tensorflow/python/keras/engine/base_layer.py\", line 601, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \".../tensorflow/python/keras/engine/sequential.py\", line 256, in call\r\n    outputs = layer(inputs, **kwargs)\r\n  File \".../tensorflow/python/keras/layers/recurrent.py\", line 639, in __call__\r\n    return super(RNN, self).__call__(inputs, **kwargs)\r\n  File \".../tensorflow/python/keras/engine/base_layer.py\", line 601, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \".../tensorflow/python/keras/layers/recurrent.py\", line 764, in call\r\n    zero_output_for_mask=self.zero_output_for_mask)\r\n  File \".../tensorflow/python/keras/backend.py\", line 3527, in rnn\r\n    initial_states + constants)\r\n  File \".../tensorflow/python/keras/layers/recurrent.py\", line 749, in step\r\n    output, new_states = self.cell.call(inputs, states, **kwargs)\r\n  File \"<stdin>\", line 10, in call\r\n  File \".../tensorflow/python/keras/layers/recurrent.py\", line 1241, in call\r\n    h = K.dot(inputs, self.kernel)\r\nAttributeError: 'SimpleRNNCell' object has no attribute 'kernel'\r\n```", "comments": ["Oops, my bad, I should have used:\r\n\r\n```python\r\noutputs, new_states = self.simple_rnn_cell(inputs, states)\r\n```\r\n\r\ninstead of\r\n\r\n```python\r\noutputs, new_states = self.simple_rnn_cell.call(inputs, states)\r\n```\r\n\r\nNote to self: get more sleep. ;-)"]}, {"number": 27444, "title": "TF 2.0 / `SequenceFeatures` changes order of columns.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): \r\n- Python version: 2.0 alpha\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): None\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n\r\nThe order of input data is changed.\r\nFor example:\r\n```\r\ninputs = {'A': [1,2,3,4], 'B':[7,8,9,10]}\r\nA = fc.sequence_numeric_column('A')\r\nB = fc.sequence_numeric_column('B')\r\ncolumns = [A,B]\r\nseq_feature_layer = keras.experimental.SequenceFeatures(columns)\r\nseq_input, seq_len = seq_feature_layer(inputs)\r\n```\r\nAnd It prints like\r\n```\r\nprint(seq_input)\r\n> [[7, 1], [8, 2], [9, 3], [10,4]]\r\n```", "comments": ["I found where it changes the order from the below function:\r\n```\r\ndef _normalize_feature_columns(feature_columns):\r\n  \"\"\"Normalizes the `feature_columns` input.\r\n  This method converts the `feature_columns` to list type as best as it can. In\r\n  addition, verifies the type and other parts of feature_columns, required by\r\n  downstream library.\r\n  Args:\r\n    feature_columns: The raw feature columns, usually passed by users.\r\n  Returns:\r\n    The normalized feature column list.\r\n  Raises:\r\n    ValueError: for any invalid inputs, such as empty, duplicated names, etc.\r\n  \"\"\"\r\n  if isinstance(feature_columns, FeatureColumn):\r\n    feature_columns = [feature_columns]\r\n\r\n  if isinstance(feature_columns, collections.Iterator):\r\n    feature_columns = list(feature_columns)\r\n\r\n  if isinstance(feature_columns, dict):\r\n    raise ValueError('Expected feature_columns to be iterable, found dict.')\r\n\r\n  for column in feature_columns:\r\n    if not isinstance(column, FeatureColumn):\r\n      raise ValueError('Items of feature_columns must be a FeatureColumn. '\r\n                       'Given (type {}): {}.'.format(type(column), column))\r\n  if not feature_columns:\r\n    raise ValueError('feature_columns must not be empty.')\r\n  name_to_column = {}\r\n  for column in feature_columns:\r\n    if column.name in name_to_column:\r\n      raise ValueError('Duplicate feature column name found for columns: {} '\r\n                       'and {}. This usually means that these columns refer to '\r\n                       'same base feature. Either one must be discarded or a '\r\n                       'duplicated but renamed item must be inserted in '\r\n                       'features dict.'.format(column,\r\n                                               name_to_column[column.name]))\r\n    name_to_column[column.name] = column\r\n\r\n  return sorted(feature_columns, key=lambda x: x.name)\r\n```", "Refer the below links:\r\n1. https://github.com/tensorflow/tensorflow/issues/21751\r\n2. https://github.com/tensorflow/tensorflow/issues/14768"]}, {"number": 27443, "title": "Tensorflow lite model inference time increases when adding JNI on Android", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS10.14.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: HW P9, Mi8, Oneplus 5, VIVOX9, HuaWei Honor V9....\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):  1.13.0.dev20190126\r\n- Python version: 2.7/3.7(both tryed)\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\nI use tflite model (quantization-aware training and fully quantized with toco) and deploy on Android for segmentation task. I got correct output of the model, and the inference time is 30ms(1 thread), 17ms(2 threads), 15ms(3 threads). However, when i add some post-processing with C++(JNI), the inference time (only the function `Interpreter.run(input, output)`  run time)increases to 47ms(1 thread), 34ms(2 threads) , 30ms(3 threads), respectively, and I got the same inference time even though I didn't use the post-processing (just put the jni code in my project). \r\nWhen I use post-processing with java code, the inference time is 30ms(1 thread), 17ms(2 threads), 15ms(3 threads) again, so I guess JNI would influence the inference time.\r\n\r\n**Describe the expected behavior**\r\nGet same inference time with JNI on Android.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\njust add some jni code to the official demo would got the same issue.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@buaadf Could you provide a code to reproduce bug? what are the commands used to convert model?\r\nCould you try TF1.13.1 (master) and/or TF2.0.0.alpha, or tf-nightly to check whether the bug is persistent with recent TF versions. Thanks!", "I have tried with 1.13.1 and nightly, the bug is pesistent..", "I finally found the reason, the demo with and  without jni use armeabi_v7 and arm64_v8a , respectively.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27443\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27443\">No</a>\n"]}, {"number": 27442, "title": "TF 2.0 / `SequenceFeatures` shows an error when I put same sequence length of features.", "body": "`keras.experimental.SequenceFeatures` requires `SparseTensor` but it measures the sequence length without `0` values.\r\n\r\nFor example, sequence data [[0,0,1,2], [0,1,2,3]] has `4` sequence lengths but when it converted to `SparseTensor`, it has a `2` sequence length and a `3` sequence length.\r\n\r\nIt shows an error as below:\r\n```\r\nTraceback (most recent call last):\r\n  File \"pipeline2.py\", line 280, in <module>\r\n    seq_input, seq_len = seq_feature_layer(inputs)\r\n  File \"/Users/seungbaeji/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 660, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/Users/seungbaeji/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/feature_column/sequence_feature_column.py\", line 140, in call\r\n    sequence_length = _assert_all_equal_and_return(sequence_lengths)\r\n  File \"/Users/seungbaeji/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/feature_column/sequence_feature_column.py\", line 489, in _assert_all_equal_and_return\r\n    assert_equal_ops.append(check_ops.assert_equal(tensors[0], t))\r\n  File \"/Users/seungbaeji/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py\", line 496, in assert_equal\r\n    (message or '', index_and_values_str, summary_msg)))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: \r\nCondition x == y did not hold.\r\nIndices of first 3 different values:\r\n[[0]\r\n [1]\r\n [2]]\r\nCorresponding x values:\r\n[95 94 93]\r\nCorresponding y values:\r\n[96 96 96]\r\nFirst 3 elements of x:\r\n[95 94 93]\r\nFirst 3 elements of y:\r\n[96 96 96]\r\n```", "comments": ["`tf.feature_columns.utils.sequence_length_from_sparse_tensor` doesn't count `0` value as an element of the sequence. Therefore, if you have a `0` in your sequence data, you have to find indices of `0`."]}, {"number": 27441, "title": "Failing on a in tensorflow_cc.so on Windows 7 on Quadro R5000 16Gb with v1.12 and CUDA 10.0.130 and CUDNN 7.4.2.24 OK under Windows 10 Quadro P5000 and GTX 1060 6Gb", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nI have linked against the //tensorflow:libtensorflow_cc.so and //tensorflow:libtensorflow_framework.so targets using other libs, abseil-cpp, libprotobuf etc\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10 (build) and Window 7 (deployment)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v1.12.0\r\n(tensorflow-cuda10) C:\\Users\\user\\dev\\tensorflow-cuda10\\tensorflow\\tensorflow\\core\\common_runtime\\gpu>python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nb'v1.12.0-0-ga6d8ffae09' 1.12.0\r\n- Python version: 3.6 (N/A)\r\n- Bazel version (if compiling from source): 0.19.2\r\n- GCC/Compiler version (if compiling from source):MSVC 14.0\r\n- CUDA/cuDNN version: 10.0.130, 7.4.2.24\r\n- GPU model and memory: GTX 1060 6Gb and Quadro R5000 16Gb\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n(tensorflow-cuda10) C:\\Users\\user\\dev\\tensorflow-cuda10\\tensorflow\\tensorflow\\core\\common_runtime\\gpu>python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nb'v1.12.0-0-ga6d8ffae09' 1.12.0\r\n\r\n**Describe the current behavior**\r\nThe application is currently crashed when initialising the session on the Quadro card on the client's computer running Windows 7 with the error messsage:\r\n\r\n2019-04-02 11:30:18.871580: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:274] Unexpected Event status: 1\r\n\r\nhere is the code for that file, where LOG FATAL is line 274\r\n\r\n```\r\n// This function must be called periodically to check whether pending\r\n// events have recorded, and then retire them.  Initial observations\r\n// suggest that typical behavior in a TensorFlow program is to have\r\n// 0-3 events pending most of the time, but there are occasionally\r\n// spikes of up to several hundred outstanding.\r\n//\r\n// NOTE: If all events are on the same stream, no later event will\r\n// complete before an earlier event, except possibly if the earlier\r\n// event transitions to an error state, so there's no advantage in\r\n// looking past the first kPending event.  However, if we're using\r\n// multiple streams there may be some gain in looking deeper.\r\n// As a compromise, PollEvent() calls that are triggered by the queueing\r\n// of a single event never look past the first kPending event.  Calls\r\n// coming from the dedicated polling thread always sweep the full queue.\r\n//\r\n// Note that allowing the queue to grow very long could cause overall\r\n// GPU memory use to spike needlessly.  An alternative strategy would\r\n// be to throttle new Op execution until the pending event queue\r\n// clears.\r\nvoid EventMgr::PollEvents(bool is_dedicated_poller,\r\n                          gtl::InlinedVector<InUse, 4>* to_free) {\r\n  VLOG(2) << \"PollEvents  free_events_ \" << free_events_.size()\r\n          << \" used_events_ \" << used_events_.size();\r\n  // Sweep the remaining events in order.  If this is the dedicated\r\n  // polling thread, check the entire set.  Otherwise, just sweep up to\r\n  // the first non-complete record that is still pending.\r\n  for (auto& iu : used_events_) {\r\n    if (iu.event == nullptr) continue;\r\n    se::Event::Status s = iu.event->PollForStatus();\r\n    switch (s) {\r\n      case se::Event::Status::kUnknown:\r\n      case se::Event::Status::kError:\r\n        // We don't expect to see these.  Someday maybe propagate\r\n        // a Status error, but for now fail hard.\r\n        LOG(FATAL) << \"Unexpected Event status: \" << static_cast<int>(s);\r\n        break;\r\n      case se::Event::Status::kPending:\r\n        if (!is_dedicated_poller) return;  // quit processing queue\r\n        break;\r\n      case se::Event::Status::kComplete:\r\n        // Make a copy of the InUse record so we can free it after releasing\r\n        // the lock\r\n        to_free->push_back(iu);\r\n        free_events_.push_back(iu.event);\r\n        // Mark this InUse record as completed.\r\n        iu.event = nullptr;\r\n    }\r\n  }\r\n  // Then clear any completed InUse records from the front of the queue.\r\n  while (!used_events_.empty()) {\r\n    InUse& iu = used_events_.front();\r\n    if (iu.event == nullptr) {\r\n      used_events_.pop_front();\r\n    } else {\r\n      break;\r\n    }\r\n  }\r\n}\r\n\r\n}  // namespace tensorflow\r\n```\r\n\r\n**Describe the expected behavior**\r\nI would expect the software to load the graph into a fresh session and compute\r\n\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\t        tensorflow::SessionOptions options;\r\n\t        tensorflow::ConfigProto* config = &options.config;\r\n                options.config.mutable_gpu_options()->set_per_process_gpu_memory_fraction(0.9);\r\n\t\tdevice_count->insert({ \"GPU\",1});\r\n\t}\r\n\t        device_count->insert({ \"CPU\", 1 });\r\n                //bytes is read from graph_file_name\r\n                graph_def->ParseFromArray(bytes.data(), (int)bytes.size()))\r\n\t\tsession>reset(tensorflow::NewSession(options);\r\n\t\tstd::cout << \"Rotobot: Swapping to model: \" << graph_file_name << \" using a single model per render is more efficent\" << std::endl;\r\n                //crashes after here\r\n\t\tauto status = (*session)->Create(graph_def);\r\n                auto status2 = (*session)->Run(Input_Tensors);\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nYou can download the built software from:\r\nhttps://kognat.com/product/rotobot-openfx-plugin-windows-64-gpu-v1-2-0-rc2-cuda-10/\r\n\r\n\r\nYou will just need an OpenFX host like Natron\r\nhttps://natrongithub.github.io/\r\n\r\nThis tutorial will give you reproduction steps\r\nhttps://kognat.com/2019/03/28/rotobot-srgb/", "comments": ["OK walking backwards through the call stack I can see this could have been raised from\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.12.0/tensorflow/core/common_runtime/gpu/gpu_device.cc#L543\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.12.0/tensorflow/core/common_runtime/gpu/gpu_util.cc#L166\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.12.0/tensorflow/core/common_runtime/gpu/gpu_util.cc#L238\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.12.0/tensorflow/core/common_runtime/gpu/gpu_util.cc#L288\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.12.0/tensorflow/core/common_runtime/gpu/gpu_util.cc#L334\r\n\r\nActually a grep is easier\r\n\r\n```\r\ntensorflow/core/common_runtime/gpu/gpu_event_mgr.h:42:// The callback provided to EventMgr::ThenExecute must not block or take a long\r\ntensorflow/core/common_runtime/gpu/gpu_event_mgr.h:99:  inline void ThenExecute(se::Stream* stream, std::function<void()> func) {\r\ntensorflow/core/common_runtime/gpu/gpu_event_mgr_test.cc:259:  em.ThenExecute(stream.get(), [&hit, &note]() {\r\ntensorflow/core/common_runtime/gpu/gpu_util.cc:166:  dev_info->event_mgr->ThenExecute(\r\ntensorflow/core/common_runtime/gpu/gpu_util.cc:238:  dev_info->event_mgr->ThenExecute(\r\ntensorflow/core/common_runtime/gpu/gpu_util.cc:288:  dev_info->event_mgr->ThenExecute(\r\ntensorflow/core/common_runtime/gpu/gpu_util.cc:334:  dev_info->event_mgr->ThenExecute(\r\ntensorflow/core/common_runtime/gpu/gpu_util_platform_specific.cc:40:Status GPUDeviceContext::ThenExecute(Device* device, se::Stream* stream,\r\ntensorflow/core/common_runtime/gpu/gpu_util_platform_specific.cc:44:  gpu_info->event_mgr->ThenExecute(stream, func);\r\ntensorflow/core/common_runtime/gpu_device_context.h:63:  Status ThenExecute(Device* device, se::Stream* stream,\r\ntensorflow/core/common_runtime/ring_reducer.cc:580:    Status s = gpu_info->default_context->ThenExecute(\r\ntensorflow/core/common_runtime/ring_reducer.cc:587:          errors::Internal(\"Failed to dispatch ThenExecute in RingReducer\");\r\ntensorflow/core/framework/device_base.h:98:  virtual Status ThenExecute(Device* device, stream_executor::Stream* stream,\r\ntensorflow/core/framework/device_base.h:100:    return errors::Internal(\"ThenExecute not supported by device\");\r\ntensorflow/core/kernels/check_numerics_op.cc:208:    context->device()->tensorflow_gpu_device_info()->event_mgr->ThenExecute(\r\ntensorflow/core/kernels/crop_and_resize_op.cc:823:  context->device()->tensorflow_gpu_device_info()->event_mgr->ThenExecute(\r\ntensorflow/core/kernels/cuda_device_array.h:89:    context_->device()->tensorflow_gpu_device_info()->event_mgr->ThenExecute(\r\ntensorflow/core/kernels/cuda_solvers.cc:247:      ->event_mgr->ThenExecute(stream, std::move(cb));\r\ntensorflow/core/kernels/dynamic_partition_op_gpu.cu.cc:318:    c->device()->tensorflow_gpu_device_info()->event_mgr->ThenExecute(\r\ntensorflow/core/kernels/segment_reduction_ops.cc:292:    context->device()->tensorflow_gpu_device_info()->event_mgr->ThenExecute(\r\ntensorflow/core/kernels/where_op.cc:358:    context->device()->tensorflow_gpu_device_info()->event_mgr->ThenExecute(\r\n```\r\n\r\nI am surprised I am not getting any error messages out of\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.12.0/tensorflow/stream_executor/cuda/cuda_driver.cc\r\n\r\nThe verbosity of the application is turned down to error only, maybe I can supply the client with a build with maximum verbosity and see if that helps trace the error.", "OK my action plan for now is to build a version with maximum debug and see what is happening and what is not happening.\r\n\r\nThanks for listening.\r\n\r\nsam", "I got a new error report \r\n\r\nWith error level 3 only\r\n\r\ntfSession->Run failed: Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n[[{{node xception_65/entry_flow/conv1_1/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 2, 2], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](xception_65/entry_flow/conv1_1/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, xception_65/entry_flow/conv1_1/weights)]]\r\n[[{{node SemanticPredictions/_45}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2428_SemanticPredictions\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n", "Everything points to a runtime environment problem https://github.com/tensorflow/tensorflow/issues/24828#issuecomment-457425190\r\n\r\n\r\nIs it possible to write a diagnostic tool which prints the path and versions of CuDNN and CUDA in the client\u2019s runtime environment?\r\n\r\nSam", "It all leads back to cudnn64_7.dll not having an overly specific name see https://en.wikipedia.org/wiki/DLL_Hell", "Could be related to this. https://github.com/tensorflow/tensorflow/issues/24496", "The suggested solution on that ticket is to allow growth\r\n\r\nhttps://www.tensorflow.org/guide/using_gpu#allowing_gpu_memory_growth\r\n\r\nBut this is a performance decision ", "Just confirmed this code:\r\n```\r\noptions.config.mutable_gpu_options()->set_allow_growth(true);\r\noptions.config.mutable_gpu_options()->set_per_process_gpu_memory_fraction(fraction);\r\n```\r\n\r\nThis results in a properly calculated graph\r\n```\r\n\\\\options.config.mutable_gpu_options()->set_allow_growth(true);\r\noptions.config.mutable_gpu_options()->set_per_process_gpu_memory_fraction(fraction);\r\n```\r\n\r\n", "Here is the log without set allow growth\r\n\r\n```\r\nRotobot: Model Decrypting Started... Decrypting Ended!\r\nRotobot: Calculating with the follow CUDA enabled GPU\r\nRotobot: Device Number: 0\r\nRotobot:   Device name: GeForce GTX 1060 6GB\r\nRotobot:   Using VRAM percentage 80.4%\r\n2019-04-07 09:56:42.668345: W tensorflow/stream_executor/cuda/cuda_driver.cc:416] A non-primary context 0000022A3A10C490 for device 0 exists before initializing the StreamExecutor. The primary context is now 0000022A765700B0. We haven't verified StreamExecutor works with that.\r\n2019-04-07 09:56:42.699831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:\r\nname: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085\r\npciBusID: 0000:0f:00.0\r\ntotalMemory: 6.00GiB freeMemory: 4.77GiB\r\n2019-04-07 09:56:42.725273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n2019-04-07 09:56:44.840990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-04-07 09:56:44.860442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0\r\n2019-04-07 09:56:44.870580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N\r\n2019-04-07 09:56:44.879572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4938 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:0f:00.0, compute capability: 6.1)\r\nRotobot: Swapping to model: C:\\Program Files (x86)\\Kognat/rotobot_segmentation.pb using a single model per render is more efficent\r\n2019-04-07 09:57:05.076919: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 4.82G (5178684160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n```", "\r\n\r\n\r\n\r\nWith allow growth enabled, uses Shared GPU memory as well as Dedicated GPU memory.\r\n\r\n```\r\nRotobot: Model Decrypting Started... Decrypting Ended!\r\nRotobot: Calculating with the follow CUDA enabled GPU\r\nRotobot: Device Number: 0\r\nRotobot:   Device name: GeForce GTX 1060 6GB\r\nRotobot:   Using VRAM percentage 80.6%\r\n2019-04-07 10:20:07.363840: W tensorflow/stream_executor/cuda/cuda_driver.cc:416] A non-primary context 00000218A6A37420 for device 0 exists before initializing the StreamExecutor. The primary context is now 00000218E5E1DD60. We haven't verified StreamExecutor works with that.\r\n2019-04-07 10:20:07.391016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:\r\nname: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085\r\npciBusID: 0000:0f:00.0\r\ntotalMemory: 6.00GiB freeMemory: 4.74GiB\r\n2019-04-07 10:20:07.413503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n2019-04-07 10:20:08.383296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-04-07 10:20:08.399261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0\r\n2019-04-07 10:20:08.408422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N\r\n2019-04-07 10:20:08.417466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4954 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:0f:00.0, compute capability: 6.1)\r\nRotobot: Swapping to model: C:\\Program Files (x86)\\Kognat/rotobot_segmentation.pb using a single model per render is more efficent\r\n2019-04-07 10:20:28.264662: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 4.84G (5195658240 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-04-07 10:20:39.033128: E tensorflow/stream_executor/cuda/cuda_dnn.cc:373] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n2019-04-07 10:20:39.055123: E tensorflow/stream_executor/cuda/cuda_dnn.cc:373] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\nRotobot: tfSession->Run failed: Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[{{node xception_65/entry_flow/conv1_1/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 2, 2], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](xception_65/entry_flow/conv1_1/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, xception_65/entry_flow/conv1_1/weights)]]\r\n         [[{{node SemanticPredictions/_45}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2428_SemanticPredictions\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n```", "Looks like this tool\r\n\r\nhttp://docs.nvidia.com/cuda/cuda-memcheck/index.html\r\n\r\nCan be useful if I can instruct the clients about how to use it", "With allow growth turned on there is nothing insightful from cuda-memtool\r\n\r\nsee\r\n\r\n```\r\nC:\\WINDOWS\\system32>\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin\\cuda-memcheck.exe\" --report-api-errors all \"C:\\Program Files\\Nuke11.2v5\\Nuke11.2.exe\"\r\n========= CUDA-MEMCHECK\r\nNuke 11.2v5, 64 bit, built Nov 20 2018.\r\nCopyright (c) 2018 The Foundry Visionmongers Ltd.  All Rights Reserved.\r\nLicence expires on: 2019/5/23\r\nA QuickTime install could not be detected. Reading and writing of QuickTime files will be limited.\r\nDisk cache C:/Users/user/AppData/Local/Temp/nuke\\ViewerCache/??: 8036MB (79% of 10240MB) used in 1412 files.\r\nRotobot: Model Decrypting Started... Decrypting Ended!\r\nRotobot: Calculating with the follow CUDA enabled GPU\r\nRotobot: Device Number: 0\r\nRotobot:   Device name: GeForce GTX 1060 6GB\r\nRotobot: Swapping to model: C:\\Program Files (x86)\\Kognat/rotobot_segmentation.pb using a single model per render is more efficent\r\nRotobot: tfSession->Run failed: Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[{{node xception_65/entry_flow/conv1_1/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 2, 2], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](xception_65/entry_flow/conv1_1/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, xception_65/entry_flow/conv1_1/weights)]]\r\n         [[{{node SemanticPredictions/_45}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2428_SemanticPredictions\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n========= Error: process didn't terminate successfully\r\n========= No CUDA-MEMCHECK results found\r\n```", "Without memory growth on I am finally able to get a trace from Natron\r\n\r\n```\r\nC:\\Users\\user>\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin\\cuda-memcheck.exe\" \"C:\\Program Files\\INRIA\\Natron-2.3.14\\bin\\Natron.exe\"\r\n========= CUDA-MEMCHECK\r\nNatron Version 2.3.14\r\nCopyright (C) 2013-2018 INRIA and Alexandre Gauthier-Foichat\r\n>>>Use the --help or -h option to print usage.<<<\r\nInfo: init.py script not loaded (this is not an error)\r\nInfo: initGui.py script not loaded (this is not an error)\r\nRotobot: Model Decrypting Started... Decrypting Ended!\r\nRotobot: Calculating with the follow CUDA enabled GPU\r\nRotobot: Device Number: 0\r\nRotobot:   Device name: GeForce GTX 1060 6GB\r\nRotobot:   Using VRAM percentage 83.1%\r\n2019-04-08 09:33:09.663952: W tensorflow/stream_executor/cuda/cuda_driver.cc:416] A non-primary context 0000000044ED9EA0 for device 0 exists before initializing the StreamExecutor. The primary context is now 0000000049662B00. We haven't verified StreamExecutor works with that.\r\n2019-04-08 09:33:09.691736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:\r\nname: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085\r\npciBusID: 0000:0f:00.0\r\ntotalMemory: 6.00GiB freeMemory: 4.97GiB\r\n2019-04-08 09:33:09.727184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n2019-04-08 09:33:15.074830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-04-08 09:33:15.089958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0\r\n2019-04-08 09:33:15.104339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N\r\n2019-04-08 09:33:15.118188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5107 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:0f:00.0, compute capability: 6.1)\r\nRotobot: Swapping to model: C:\\Program Files (x86)\\Kognat/rotobot_segmentation.pb using a single model per render is more efficent\r\n2019-04-08 09:33:33.417588: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 4.99G (5355410176 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-04-08 09:34:28.049345: E tensorflow/stream_executor/cuda/cuda_dnn.cc:373] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-04-08 09:34:28.067807: E tensorflow/stream_executor/cuda/cuda_dnn.cc:373] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nRotobot: tfSession->Run failed: Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[{{node xception_65/entry_flow/conv1_1/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 2, 2], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](xception_65/entry_flow/conv1_1/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, xception_65/entry_flow/conv1_1/weights)]]\r\n         [[{{node SemanticPredictions/_45}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2428_SemanticPredictions\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n========= Error: process didn't terminate successfully\r\n========= Program hit CUDA_ERROR_OUT_OF_MEMORY (error 2) due to \"out of memory\" on CUDA API call to cuMemAlloc_v2.\r\n=========     Saved host backtrace up to driver entry point at error\r\n=========     Host Frame:C:\\WINDOWS\\SYSTEM32\\nvcuda.dll (cuD3D9UnmapVertexBuffer + 0x1b1147) [0x1bf482]\r\n=========     Host Frame:C:\\Program Files (x86)\\Kognat\\shared_libraries\\tensorflow_cc.dll (icu_62::MessagePattern::getPatternString + 0x45b6) [0x32eedd6]\r\n=========     Host Frame:C:\\Program Files (x86)\\Kognat\\shared_libraries\\tensorflow_cc.dll (icu_62::MessagePattern::getPatternString + 0x63780) [0x334dfa0]\r\n=========     Host Frame:C:\\Program Files (x86)\\Kognat\\shared_libraries\\tensorflow_cc.dll (icu_62::UCharsTrieBuilder::matchNodesCanHaveValues + 0x2e6de3) [0x31f68d3]\r\n=========     Host Frame:C:\\Program Files (x86)\\Kognat\\shared_libraries\\tensorflow_cc.dll (icu_62::ResourceTable::getSize + 0x382dc) [0x32aa36c]\r\n=========     Host Frame:C:\\Program Files (x86)\\Kognat\\shared_libraries\\tensorflow_cc.dll (icu_62::ResourceTable::getSize + 0x36bad) [0x32a8c3d]\r\n=========     Host Frame:C:\\Program Files (x86)\\Kognat\\shared_libraries\\tensorflow_cc.dll (icu_62::ResourceTable::getSize + 0x36880) [0x32a8910]\r\n=========     Host Frame:C:\\Program Files (x86)\\Kognat\\shared_libraries\\tensorflow_cc.dll (icu_62::ResourceTable::getSize + 0x36a4a) [0x32a8ada]\r\n=========     Host Frame:C:\\Program Files (x86)\\Kognat\\shared_libraries\\tensorflow_cc.dll (tensorflow::TensorShapeBase<tensorflow::TensorShape>::dim_size + 0xb0db) [0x33fd62b]\r\n=========     Host Frame:C:\\Program Files (x86)\\Kognat\\shared_libraries\\tensorflow_cc.dll (tensorflow::TensorShapeBase<tensorflow::TensorShape>::dim_size + 0xf942) [0x3401e92]\r\n=========     Host Frame:C:\\Program Files (x86)\\Kognat\\shared_libraries\\tensorflow_cc.dll (icu_62::UCharsTrieBuilder::matchNodesCanHaveValues + 0x2f54be) [0x3204fae]\r\n=========     Host Frame:C:\\Program Files (x86)\\Kognat\\shared_libraries\\tensorflow_cc.dll (icu_62::UCharsTrieBuilder::matchNodesCanHaveValues + 0x2f5215) [0x3204d05]\r\n=========     Host Frame:C:\\Program Files (x86)\\Kognat\\shared_libraries\\tensorflow_cc.dll (icu_62::CharString::length + 0xe9f4b) [0x650b4b]\r\n=========     Host Frame:C:\\Program Files (x86)\\Kognat\\shared_libraries\\tensorflow_cc.dll (icu_62::CharString::length + 0xe8361) [0x64ef61]\r\n=========     Host Frame:C:\\Program Files (x86)\\Kognat\\shared_libraries\\tensorflow_cc.dll (icu_62::ReorderingBuffer::getStart + 0x1438a) [0x341e2ba]\r\n=========     Host Frame:C:\\Program Files (x86)\\Kognat\\shared_libraries\\tensorflow_cc.dll (icu_62::ResourceTable::getSize + 0x612b) [0x32781bb]\r\n=========     Host Frame:C:\\Program Files (x86)\\Kognat\\shared_libraries\\tensorflow_cc.dll (tensorflow::NewSession + 0x33c17) [0x3269827]\r\n=========     Host Frame:C:\\Program Files (x86)\\Kognat\\shared_libraries\\tensorflow_cc.dll (tensorflow::NewSession + 0x342df) [0x3269eef]\r\n=========     Host Frame:C:\\Program Files (x86)\\Kognat\\shared_libraries\\tensorflow_cc.dll (icu_62::UCharsTrieBuilder::matchNodesCanHaveValues + 0x1ca132) [0x30d9c22]\r\n=========     Host Frame:C:\\Program Files (x86)\\Kognat\\shared_libraries\\tensorflow_cc.dll (icu_62::UCharsTrieBuilder::matchNodesCanHaveValues + 0x1c5162) [0x30d4c52]\r\n=========     Host Frame:C:\\Program Files (x86)\\Kognat\\shared_libraries\\tensorflow_cc.dll (icu_62::UCharsTrieBuilder::matchNodesCanHaveValues + 0x1c7786) [0x30d7276]\r\n=========     Host Frame:C:\\Program Files (x86)\\Kognat\\shared_libraries\\tensorflow_cc.dll (icu_62::UCharsTrieBuilder::matchNodesCanHaveValues + 0x1c6a34) [0x30d6524]\r\n=========     Host Frame:C:\\Program Files (x86)\\Kognat\\shared_libraries\\tensorflow_cc.dll (icu_62::UCharsTrieBuilder::matchNodesCanHaveValues + 0x1d5821) [0x30e5311]\r\n=========     Host Frame:C:\\Program Files (x86)\\Kognat\\shared_libraries\\tensorflow_cc.dll (icu_62::ICUServiceKey::getID + 0x7c550) [0x134d040]\r\n=========     Host Frame:C:\\Program Files (x86)\\Kognat\\shared_libraries\\tensorflow_cc.dll (icu_62::ResourceTable::getSize + 0xa069) [0x327c0f9]\r\n=========     Host Frame:C:\\Program Files (x86)\\Kognat\\shared_libraries\\tensorflow_cc.dll (icu_62::ResourceTable::getSize + 0xbba4) [0x327dc34]\r\n=========     Host Frame:C:\\Program Files (x86)\\Kognat\\shared_libraries\\tensorflow_cc.dll (icu_62::ResourceTable::getSize + 0xba41) [0x327dad1]\r\n=========     Host Frame:C:\\Program Files (x86)\\Kognat\\shared_libraries\\tensorflow_cc.dll (icu_62::ResourceTable::getSize + 0x900) [0x3272990]\r\n=========     Host Frame:C:\\Program Files (x86)\\Kognat\\shared_libraries\\tensorflow_cc.dll (icu_62::UCharsTrieBuilder::matchNodesCanHaveValues + 0x1cc884) [0x30dc374]\r\n=========     Host Frame:C:\\Program Files (x86)\\Kognat\\shared_libraries\\tensorflow_cc.dll (icu_62::UCharsTrieBuilder::matchNodesCanHaveValues + 0x1cf508) [0x30deff8]\r\n=========     Host Frame:C:\\Program Files (x86)\\Kognat\\shared_libraries\\tensorflow_cc.dll (icu_62::UCharsTrieBuilder::matchNodesCanHaveValues + 0x1d2603) [0x30e20f3]\r\n=========     Host Frame:C:\\Program Files (x86)\\Kognat\\shared_libraries\\tensorflow_cc.dll (icu_62::UCharsTrieBuilder::matchNodesCanHaveValues + 0x1d2299) [0x30e1d89]\r\n=========     Host Frame:C:\\Program Files\\Common Files\\OFX\\Plugins\\rotobot.ofx.bundle\\Contents\\win64\\rotobot.ofx (drawMasksDP + 0x5c0) [0xa7bf0]\r\n=========     Host Frame:C:\\Program Files\\Common Files\\OFX\\Plugins\\rotobot.ofx.bundle\\Contents\\win64\\rotobot.ofx (rotobotSegmentationPlugin::computeMask + 0x959) [0xa46a9]\r\n=========     Host Frame:C:\\Program Files\\Common Files\\OFX\\Plugins\\rotobot.ofx.bundle\\Contents\\win64\\rotobot.ofx (rotobotSegmentationPlugin::setupAndProcess + 0x1c7) [0xafe27]\r\n=========     Host Frame:C:\\Program Files\\Common Files\\OFX\\Plugins\\rotobot.ofx.bundle\\Contents\\win64\\rotobot.ofx (rotobotSegmentationPlugin::render + 0xc3) [0xaef93]\r\n=========     Host Frame:C:\\Program Files\\Common Files\\OFX\\Plugins\\rotobot.ofx.bundle\\Contents\\win64\\rotobot.ofx (OFX::Private::renderAction + 0x6c) [0xc495c]\r\n=========     Host Frame:C:\\Program Files\\Common Files\\OFX\\Plugins\\rotobot.ofx.bundle\\Contents\\win64\\rotobot.ofx (OFX::Private::mainEntryStr + 0xb2a) [0xc2dda]\r\n=========     Host Frame:C:\\Program Files\\Common Files\\OFX\\Plugins\\rotobot.ofx.bundle\\Contents\\win64\\rotobot.ofx (OFX::FactoryMainEntryHelper<rotobotSegmentationPluginFactory>::mainEntry + 0x45) [0xa9cb5]\r\n=========     Host Frame:C:\\Program Files\\INRIA\\Natron-2.3.14\\bin\\Natron.exe (initNatronEngine + 0x418122) [0x71f782]\r\n=========     Host Frame:C:\\Program Files\\INRIA\\Natron-2.3.14\\bin\\Natron.exe (initNatronEngine + 0x3794ce) [0x680b2e]\r\n=========     Host Frame:C:\\Program Files\\INRIA\\Natron-2.3.14\\bin\\Natron.exe (initNatronEngine + 0x42080b) [0x727e6b]\r\n=========     Host Frame:C:\\Program Files\\INRIA\\Natron-2.3.14\\bin\\Natron.exe (initNatronEngine + 0x17c8cf) [0x483f2f]\r\n=========     Host Frame:C:\\Program Files\\INRIA\\Natron-2.3.14\\bin\\Natron.exe (initNatronEngine + 0x284736) [0x58bd96]\r\n=========     Host Frame:C:\\Program Files\\INRIA\\Natron-2.3.14\\bin\\Natron.exe (initNatronEngine + 0x28e572) [0x595bd2]\r\n=========     Host Frame:C:\\Program Files\\INRIA\\Natron-2.3.14\\bin\\Natron.exe (initNatronEngine + 0x29158c) [0x598bec]\r\n=========     Host Frame:C:\\Program Files\\INRIA\\Natron-2.3.14\\bin\\Natron.exe (initNatronEngine + 0x272663) [0x579cc3]\r\n=========     Host Frame:C:\\Program Files\\INRIA\\Natron-2.3.14\\bin\\Natron.exe (initNatronEngine + 0x277c85) [0x57f2e5]\r\n=========     Host Frame:C:\\Program Files\\INRIA\\Natron-2.3.14\\bin\\Natron.exe (initNatronEngine + 0x5a577) [0x361bd7]\r\n=========     Host Frame:C:\\Program Files\\INRIA\\Natron-2.3.14\\bin\\Natron.exe (initNatronEngine + 0x5fe57) [0x3674b7]\r\n=========     Host Frame:C:\\Program Files\\INRIA\\Natron-2.3.14\\bin\\Natron.exe (ZN5boost7archive6detail11oserializerINS0_15binary_oarchiveESt6vectorINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEESaISA_EEEC2Ev + 0xbd821) [0x99b521]\r\n=========     Host Frame:C:\\Program Files\\INRIA\\Natron-2.3.14\\bin\\QtCore4.dll (ZN17QThreadPoolThread3runEv + 0x18f) [0xf12f]\r\n=========     Host Frame:C:\\Program Files\\INRIA\\Natron-2.3.14\\bin\\QtCore4.dll (ZN7QThread21setTerminationEnabledEb + 0x268) [0x1a318]\r\n=========     Host Frame:C:\\WINDOWS\\System32\\msvcrt.dll (beginthreadex + 0x126) [0x3aa96]\r\n=========     Host Frame:C:\\WINDOWS\\System32\\msvcrt.dll (endthreadex + 0xac) [0x3ab6c]\r\n=========\r\n========= No CUDA-MEMCHECK results found\r\n```", "Interestingly running without cuda-memcheck.exe doesnt result in a crash.\r\n\r\nbut the program is in a zombie state after closing\r\n\r\n```\r\nC:\\Users\\user>\"C:\\Program Files\\INRIA\\Natron-2.3.14\\bin\\Natron.exe\"\r\nNatron Version 2.3.14\r\nCopyright (C) 2013-2018 INRIA and Alexandre Gauthier-Foichat\r\n>>>Use the --help or -h option to print usage.<<<\r\nInfo: init.py script not loaded (this is not an error)\r\nInfo: initGui.py script not loaded (this is not an error)\r\nRotobot: Model Decrypting Started... Decrypting Ended!\r\nRotobot: Calculating with the follow CUDA enabled GPU\r\nRotobot: Device Number: 0\r\nRotobot:   Device name: GeForce GTX 1060 6GB\r\nRotobot:   Using VRAM percentage 82.4%\r\n2019-04-08 09:39:09.238191: W tensorflow/stream_executor/cuda/cuda_driver.cc:416] A non-primary context 00000000422B89B0 for device 0 exists before initializing the StreamExecutor. The primary context is now 0000000046202EF0. We haven't verified StreamExecutor works with that.\r\n2019-04-08 09:39:09.261324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:\r\nname: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085\r\npciBusID: 0000:0f:00.0\r\ntotalMemory: 6.00GiB freeMemory: 4.89GiB\r\n2019-04-08 09:39:09.281762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n2019-04-08 09:39:10.424100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-04-08 09:39:10.435463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0\r\n2019-04-08 09:39:10.443461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N\r\n2019-04-08 09:39:10.453030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5064 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:0f:00.0, compute capability: 6.1)\r\nRotobot: Swapping to model: C:\\Program Files (x86)\\Kognat/rotobot_segmentation.pb using a single model per render is more efficent\r\n2019-04-08 09:39:30.327633: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 4.95G (5310059520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n```\r\n\r\nSorry the process does complete after being in a suspended state for a few seconds.", "I made this for the clients to help with debugging on their systems so I may as well leave it here\r\n\r\nThe result will be watermarked \r\n\r\nthis might be informative\r\n\r\nI made a video to help you get better debug information\r\n\r\nhttps://www.youtube.com/watch?v=oWULIoJlrto \r\n\r\nDownload and install Natron\r\nhttps://sourceforge.net/projects/natron/files/Windows/64/releases/Natron-2.3.14-Windows-x86_64bit-setup.exe/download\r\n\r\nso we have a known quantity with OpenFX hosts.\r\n\r\nClose all other applications\r\n\r\nThen follow the tutorial about how to open Natron from the command prompt\r\n\r\nThen playback a few frames and see if it crashes.\r\n\r\nThe footage used in the clip is here:\r\nhttp://bit.ly/IMG_6463-MOV\r\n\r\nThe debug installer is here:\r\nhttp://bit.ly/Kognat-1-2-0-RC2-cuda10-debug-windows-installer\r\n\r\nIf or when it does crash if you can say what you did before it crashing and provided the information from the command prompt, just highlight and use Ctrl-C and Ctrl-V to put it into a text document\r\n\r\nThere are subtitles on the YouTube video, the audio isnt very good.\r\n", "the trace says before it that allow growth was turned off.\n\nI cannot see your comment on GitHub.\n\nOn Thu, Apr 11, 2019 at 6:19 AM Daniel Bryce Evans <notifications@github.com>\nwrote:\n\n> @samhodge <https://github.com/samhodge> You were able to get a trace from\n> cuda-memcheck with allow_growth on or off?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27441#issuecomment-481855155>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAFTooTPVAwS_C2e6GzlzYD2OW-t7-nkks5vfk5lgaJpZM4cZdwc>\n> .\n>\n\n\n-- \nSam Hodge\nDirector\n\nKognat Proprietary Limited\nACN 623 943 304\nMobile : +61417801006\nsam@kognat.com\n", "Memory management in TF is the greatest cause of bugs in my application, any help would be useful for the entire community", "@samhodge There are several [tutorials](https://www.tensorflow.org/tutorials/) listed on TF website to deal about the memory management. Please take a look at them. There are articles on best practices of Tensorflow [lite](https://www.tensorflow.org/lite/performance/best_practices). You could search for similar articles on the internet.\r\n\r\nThis is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "The only relevant article is this https://www.tensorflow.org/guide/using_gpu#allowing_gpu_memory_growth which is not in the tutorials.\r\n\r\nI have set the value to 0.95 percentage of memory available for allocation by testing using CUDA device API.\r\n\r\nWhen other applications get into this GPU memory space it results in a segfault.\r\n\r\nThis can be done easily by opening a browser or similar.\r\n\r\nThis makes my customers upset.\r\n\r\nThere is no way to deallocate TF memory apart from session->reset() which doesn't actually work.\r\n\r\n\r\nsee\r\nhttps://github.com/tensorflow/tensorflow/issues/20387\r\nhttps://github.com/tensorflow/tensorflow/issues/1578\r\nhttps://github.com/tensorflow/tensorflow/issues/5302\r\n\r\nIf memory management is so well documented why are capable C++ coders having issues with it?\r\n", "The use case is as follows, there are several models that are run in the one C++ software application.\r\n\r\nto create the next session a singleton is used for the session and a new model is allocated but only the first allocation will set aside the VRAM to be used.\r\n\r\nSo in 5.2Gb of VRAM is allocated and one model is running and no other application uses that VRAM everything is OK.\r\n\r\nThen you switch a new model, there is some memory fragmentation and the other application on the host machine allocate some VRAM (watching a video on YouTube for instance) while waiting for the TF model to execute.\r\n\r\nThen the application switches to another model and allocates a new session, the memory allocated by TF no longer has access to all of the 5.2Gb that it originally had, and you end up with an OOM condition.\r\n\r\nWhere is the tutorial about this use case?", "@jvishnuvardhan Thank you for the articles on quantization, I am looking into this and I am also looking into use of TFLite on LInux and macOS, I am not sure how useful it is on windows.", "@jvishnuvardhan as for using Stack Overflow\r\n\r\nsee this https://stackoverflow.com/questions/52683649/libtensorflow-cc-so-initialised-a-second-time-causes-segfault this cannot be fixed by anybody but the TF devs, it was ignored and as a result I cannot get my OFX plugin to run in Autodesk Flame 2020 which would be a sizable product user base, I reported in October 2018 Autodesk Flame was released in April 2019. There was no useful response from the Stack Overflow community or the TF devs.", "@jvishnuvardhan \r\n\r\nhere is the TF team's response https://github.com/tensorflow/tensorflow/issues/22810", "https://github.com/miglopst/cs263_spring2018/wiki/Memory-management-for-tensorflow Is the most detailed reference ", "@samhodge  Thanks for sharing the resource. Thanks!", "Here is another error report \r\n\u2018\u2019\u2019\r\nRotobot: tfSession->Run failed: Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[{{node xception_65/entry_flow/conv1_1/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 2, 2], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](xception_65/entry_flow/conv1_1/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, xception_65/entry_flow/conv1_1/weights)]]\r\n         [[{{node SemanticPredictions/_45}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2428_SemanticPredictions\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\u2018\u2019\u2019", "@samhodge Exactly the same situation. Unless I am using a RTX2080Ti under windows 10. However, if I set this `options.config.mutable_gpu_options()->set_per_process_gpu_memory_fraction(fraction);`  in my code. It gives me the error \r\n```\r\nError\tLNK2019\tunresolved external symbol \"private: static class tensorflow::GPUOptions * __cdecl google::protobuf::Arena::CreateMaybeMessage(class google::protobuf::Arena *)\" (??$CreateMaybeMessage@VGPUOptions@tensorflow@@$$V@Arena@protobuf@google@@CAPEAVGPUOptions@tensorflow@@PEAV012@@z) referenced in function \"protected: static class tensorflow::GPUOptions * __cdecl google::protobuf::MessageLite::CreateMaybeMessage(class google::protobuf::Arena *)\" (??$CreateMaybeMessage@VGPUOptions@tensorflow@@@MessageLite@protobuf@google@@KAPEAVGPUOptions@tensorflow@@PEAVArena@12@@z)\ttftest\r\n```\r\n Any idea on this? I compiled from source.", "Expose those symbols in your script of all the symbols you are exposing.\r\n\r\nWhat project are you working towards?", "@samhodge Hi Sam, I just replied you in another issue. Thank you for your reply.", "@samhodge We see that you are using old version of tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Hence moving this to closed status.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27441\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27441\">No</a>\n"]}, {"number": 27440, "title": "add example for bitwise and, or, xor, invert, left_shift, right_shift", "body": "add an example for bitwise and, or, xor, invert, left_shift, right_shift #27166", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27440) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27440) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27440) for more info**.\n\n<!-- ok -->", "@hksonngan can you please resolve conflicts and push changes again.", "@rthadur done."]}, {"number": 27439, "title": "TFTRT: Add converter for DepthToSpace and SpaceToDepth", "body": "Also adds unit tests", "comments": ["@aaroey @smit-hinsu Could one of you please review this so it can get into 1.14? Thanks!"]}, {"number": 27438, "title": "DLL Loading Failed & Using An AMD GPU", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro x64\r\n- TensorFlow installed from (source or binary): Unknown\r\n- TensorFlow version: Unknown\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: 4GB ATI Radeon RX 580 (MSI)\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nUnable to import a DLL & Getting Tensorflow to work on an AMD GPU\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nimport tensorflow as tf\r\n\r\n**Any other info / logs**\r\n`Traceback (most recent call last):\r\n  File \"F:\\Python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"F:\\Python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"F:\\Python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"F:\\Python\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"F:\\Python\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"F:\\Python\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"F:\\Python\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"F:\\Python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"F:\\Python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"F:\\Python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"F:\\Python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"F:\\Python\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"F:\\Python\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.`\r\n", "comments": ["TensorFlow GPU requires [NVIDIA hardware](https://www.tensorflow.org/install/gpu#hardware_requirements). With current configuration you have to install TensorFlow cpu version.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27438\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27438\">No</a>\n"]}, {"number": 27436, "title": "[Intel MKL] Enabling BFloat16 element-wise ops using MKLDNN API - Rou\u2026", "body": "\u2026nd 2\r\n\r\nThis PR enables support for BFloat16 versions of element-wise arithmetic ops\r\nand activation functions using MKL-DNN API.", "comments": ["Thanks @shahzadlone for approval! Would you mind taking a look at first part of this PR #27300? ", "Thanks for approval @penpornk!"]}, {"number": 27435, "title": "AttributeError: module 'tensorflow' has no attribute 'enable_eager_execution' in Verify installation", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: not applicable\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: tensorflow-gpu 2.0.0a0\r\n- Python version: 3.6.7\r\n- Installed using virtualenv? pip? conda?: virtualenv + pip\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): - \r\n- CUDA/cuDNN version: 10.0/7.5.0.56\r\n- GPU model and memory: NVIDIA GTX 1080 Max-Q\r\n\r\n\r\n\r\n**Describe the problem**\r\nI've just followed the installation guides for tensorflow 2.0 (see above) from:\r\n- https://www.tensorflow.org/install/pip\r\n- https://www.tensorflow.org/install/gpu#software_requirements\r\nThe final step is to \"Verify the install\" (see the pip link above) which gives me the following output:\r\n```\r\n$ python -c \"import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute 'enable_eager_execution'\r\n```\r\n\r\nWhich I did not expect/hope to see.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n- Follow the guides mentioned above.\r\n- Create all the environment variables. \r\n-- Path within venv then contains: \r\n```\r\nD:\\dev\\project\\project-venv\\\r\nD:\\dev\\tools\\cuda\\cudnn-10.0-windows10-x64-v7.5.0.56\\cuda\\bin;\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\extras\\CUPTI\\libx64;\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\include;\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;\r\nC:\\ProgramData\\DockerDesktop\\version-bin;\r\nC:\\Program Files\\Docker\\Docker\\Resources\\bin;\r\nD:\\dev\\tools\\oraclexe\\oraclexe\\app\\oracle\\product\\11.2.0\\server\\bin;\r\nC:\\Program Files\\Python36\\Scripts\\;\r\nC:\\Program Files\\Python36\\;\r\nC:\\Program Files\\Microsoft MPI\\Bin\\;\r\nC:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;\r\nC:\\ProgramData\\Oracle\\Java\\javapath;\r\nC:\\WINDOWS\\system32;\r\nC:\\WINDOWS;\r\nC:\\WINDOWS\\System32\\Wbem;\r\nC:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;\r\nC:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;\r\nC:\\Program Files\\dotnet\\;\r\nC:\\Program Files\\Microsoft SQL Server\\130\\Tools\\Binn\\;\r\nD:\\dev\\tools\\putty\\;\r\nC:\\Program Files\\TortoiseSVN\\bin;\r\nC:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;\r\nC:\\Program Files\\nodejs\\;\r\nC:\\WINDOWS\\system32;\r\nC:\\WINDOWS;\r\nC:\\WINDOWS\\System32\\Wbem;\r\nC:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;\r\nC:\\WINDOWS\\System32\\OpenSSH\\;\r\nC:\\dev\\tools\\python\\python3.6.7\\Scripts\\;\r\nC:\\dev\\tools\\python\\python3.6.7\\;\r\nC:\\Program Files\\Python36\\Scripts\\;\r\nC:\\Program Files\\Python36\\;\r\nC:\\Users\\tve21314\\AppData\\Local\\Microsoft\\WindowsApps;\r\nD:\\dev\\tools\\maven\\apache-maven-3.5.3\\bin;\r\nC:\\Program Files\\Java\\jdk1.8.0_201\\bin;\r\nC:\\Program Files\\Java\\jre1.8.0_201\\bin;\r\nC:\\Program Files\\Git\\bin;\r\nC:\\Users\\tve21314\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;\r\nC:\\Users\\tve21314\\AppData\\Roaming\\npm;\r\n```\r\n- Finally run from within the venv:\r\n```python -c \"import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))\"```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n- The traceback again:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute 'enable_eager_execution'\r\n```\r\n\r\n- If I use ```python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random_normal([1000, 1000])))\"``` the error becomes:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute 'random_normal'\r\n```", "comments": ["Hi, I have the same issue.\r\n\r\nI install tensorflow on my MacBook pro based on the page: https://www.tensorflow.org/install,  \r\n$pip3 install tensorflow==2.0.0-alpha0\r\nIt works.\r\n\r\nI install tensorflow with pip3 step by step too: https://www.tensorflow.org/install/pip. It almost works, but the last step 'verify the install' has one Error, please refer to the sentence below: \r\n$python -c \"import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))\"\r\nTraceback (most recent call last):\r\n  File \"\\<string\\>\", line 1, in \\<module\\>\r\n**AttributeError: module 'tensorflow' has no attribute 'enable_eager_execution'**\r\n\r\nPython 3.7.3\r\nmacOS Mojave 10.14.4\r\nThank you.\r\n", "Same issue here.\r\n\r\nUbuntu 18.04\r\nCUDA 10.0\r\nvirtualenv + pip\r\nNVIDIA GTX 1060\r\nPython 3.6.7\r\n\r\nAttribute errors for both eager execution & random_normal, but installs successfully.", "@timoveldt @p9i @lynscott  Thanks for reporting this issue. We have to update our docs to address TF 2.0 installation verification.\r\n\r\n```AttributeError: module 'tensorflow' has no attribute 'enable_eager_execution'```\r\nEager execution is enabled by default in TF 2.0 Therefore we need not enable it explicitly.\r\n\r\n```AttributeError: module 'tensorflow' has no attribute 'random_normal'```\r\nTF 2.0 comes with new [aliases](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/random/normal#aliases) for random_normal\r\nUsing ```tf.random.normal``` should execute successfully.\r\n", "@ymodak That fixed it!\r\n\r\nNice to see that I was on the right track by removing the eager execution. I never would've guest that the next step was in the aliases. \r\n\r\nI'm trying to contribute the actual fix, but I've got some hoops to jump through :)", "Glad it worked. Thanks for submitting the PR. I will close this issue now. "]}, {"number": 27434, "title": "tensorflow-gpu installation failing ", "body": "Please find the error below.\r\n\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\priya\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\priya\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\priya\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\priya\\Anaconda3\\envs\\tensorflow\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\priya\\Anaconda3\\envs\\tensorflow\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\priya\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\priya\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\priya\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\priya\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\priya\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\priya\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\priya\\Anaconda3\\envs\\tensorflow\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\priya\\Anaconda3\\envs\\tensorflow\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\n", "comments": ["@priyak1917  Hey! sorry to hear this, can you tell me the version of tensorflow you are installing plus the version of CUDA and CUDNN you have installed in your system!\r\n\r\n", "Hi Aman,\n\nI'm a beginner and tried to install Tensorflow. I understood the problem, i\ndon't have NVIDIA Graphics card in my System.\n\nI want to uninstall the Tensowflow-GPU and continue to use Tensorflow-Cpu,\nis it possible? Can you please guide?\n\n\nRegards\n\nOn Wed, Apr 3, 2019 at 1:57 AM Aman Patel <notifications@github.com> wrote:\n\n> @priyak1917 <https://github.com/priyak1917> Hey! sorry to hear this, can\n> you tell me the version of tensorflow you are installing plus the version\n> of CUDA and CUDNN you have installed in your system!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27434#issuecomment-479183550>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/Aayicn1anjxLCdMxafW0L935bgeca8N4ks5vc71IgaJpZM4cY0Kv>\n> .\n>\n\n\n-- \nMs.Priyanka Kolli\nBTech-Dept of BioTechnology\nNational Institute of Technology, Warangal\nWarangal-506 021\nAndhra Pradesh\nIndia.\nContact no: +919886032005\n", "@priyak1917 yeah sure I can help you but it will be much easier if you tell me that how you installed tensorflow-gpu??\r\nthrough anaconda distribution or through pip install manager??", "https://www.youtube.com/watch?v=SNdQqYpfCV4\n\nI followed exactly above mentioned Video procedure.\n\n\nRegards\n\nOn Wed, Apr 3, 2019 at 6:32 PM Aman Patel <notifications@github.com> wrote:\n\n> @priyak1917 <https://github.com/priyak1917> yeah sure I can help you but\n> it will be much easier if you tell me that how you installed\n> tensorflow-gpu??\n> through anaconda distribution or through pip install manager??\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27434#issuecomment-479477417>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AayicnAi-YaMqQ2-0vppZT61Xt1VXxDVks5vdKZOgaJpZM4cY0Kv>\n> .\n>\n\n\n-- \nMs.Priyanka Kolli\nBTech-Dept of BioTechnology\nNational Institute of Technology, Warangal\nWarangal-506 021\nAndhra Pradesh\nIndia.\nContact no: +919886032005\n", "@priyak1917 just activate your virtual environment and type conda remove tensorflow-gpu this will remove your gpu version of tensorflow!\r\n\r\nDo this and let me know if it works!", "Hi aman,\n\nI had activated the tensorflow and gave the command \"conda remove\ntensorflow-gpu\".\n\n[image: image.png]\n\nIt actually got failed.\n\n[image: image.png]\n\nCan you tell me how to uninstall entire tensorflow? I will re install only\nCPU version?\n\nOn Wed, Apr 3, 2019 at 11:44 PM Aman Patel <notifications@github.com> wrote:\n\n> @priyak1917 <https://github.com/priyak1917> just activate your virtual\n> environment and type conda remove tensorflow-gpu this will remove your gpu\n> version of tensorflow!\n>\n> Do this and let me know if it works!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27434#issuecomment-479598789>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AayicrbrsHz6ycSfdNQ8idjwA91piyx5ks5vdO91gaJpZM4cY0Kv>\n> .\n>\n\n\n-- \nMs.Priyanka Kolli\nBTech-Dept of BioTechnology\nNational Institute of Technology, Warangal\nWarangal-506 021\nAndhra Pradesh\nIndia.\nContact no: +919886032005\n", "@priyak1917 If you have installed in Virtual environment, you can deactivate that env and create another virtual env and install tensorflow. Installing tf_cpu is very easy. Just open a terminal and type \"pip install tensorflow\". Thanks! ", "@vishu \nThank you so much I will try and let you know. \n\nSent from my iPhone\n\n> On 05-Apr-2019, at 2:50 AM, Vishnuvardhan Janapati <notifications@github.com> wrote:\n> \n> @priyak1917 If you have installed in Virtual environment, you can deactivate that env and create another virtual env and install tensorflow. Installing tf_cpu is very easy. Just open a terminal and type \"pip install tensorflow\". Thanks!\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27434\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27434\">No</a>\n"]}, {"number": 27433, "title": "Clearing Tensorflow-Keras GPU memory", "body": "\r\nI'm fitting a model in a for loop,but i'm getting an error that my gpu's memory is full. I'm using Keras in Anaconda spyder ide.My gpu is asus gtx 1060 6gb.\r\n\r\nI've also used codes like : K.clear_session() , gc.collect() , tf.reset_default_graph() , del custom_model but none of them worked. Gpu properties say's 98% of memory is full. enter image description here\r\n\r\nNothing flush gpu memory except numba.cuda.close() but won't allow me to use my gpu again. The only way to clear it is restarting kernel and rerun my code.\r\n\r\nI'm looking for any script code to add my code allow me to use my code in for loop and clear gpu in every loop.\r\n\r\n\r\nPart of my code :\r\n\r\nimage_input = Input(shape=(224, 224, 3))\r\nbase_model = Xception(input_tensor=image_input, include_top=False,weights='imagenet')\r\nbase_model.compile(loss='categorical_crossentropy',optimizer='adadelta',metrics=['accuracy'])\r\nhist = base_model.fit(X,Y,epochs=2)\r\n\r\n**System information**\r\n- Have I written custom code :\r\n- Windows 10 64-bit\r\n- TensorFlow installed from conda install tensorflow-gpu\r\n- TensorFlow version: 1.3\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.2\r\n- GPU model and memory: Asus GTX 1060 6gb\r\n\r\n", "comments": [" @sepehrghafari \r\n\r\nTry using: \r\n\r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.333)\r\nsess = tf.Session(config = tf.ConfigProto(gpu_options = gpu_options)\r\n\r\nThis command will only use 33.3% of your GPU memory and will prevent the following error. You can play with the number here and can use any number of your choice!\r\n\r\nUpdate me if it worked!", "@amanp592 \r\nThank you for replying ,\r\nI got error when i use this cause when i load imagenet weights,it needs memory and this code slows running time. i don't have out of memory issue in single run. I need a script to clear GPU memory in every loop.", "@sepehrghafari In order to expedite the troubleshooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "@muddham I wrote my code in comment. I have no problem with code, my issue is i want to compile and run and train simple training session with Keras with any Pre-trained models like Xception ,VGG or Resnet, multiple times is a single run, in order to do this i put my whole code in for. but after loading Pre-trained models, GPU memory gets about 80-90% full. In second loop of for i got OOM error.\r\nI tried K.clear_session() , gc.collect() , tf.reset_default_graph() , del model but none of them doesn't flush the GPU memory.", "This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. If you think this is a bug, please provide a short code and error trace so that we can reproduce the bug and resolve it. Thanks!", "I found solition in this article :\r\nhttps://github.com/keras-team/keras/issues/12625\r\n\r\nSo i close this title, thanks everyone who participated.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27433\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27433\">No</a>\n"]}, {"number": 27432, "title": "Errors with tensorflow-gpu 2.0 alpha0", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: tensorflow-gpu 2.0 alpha0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cudatoolkit 10.0.130, and cudnn 7.5\r\n- GPU model and memory: NVIDIA GTX 1070/6GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nI have installed tensorflow-gpu 2.0alpha0, cudatoolkit 10.0.130, and cudnn 7.5 in a windows 10.\r\n\r\nTensorFlow 2.0 works well with CPU, but encounters errors while running with GPU.\r\n\r\nErrors messages are as bellow:\r\n```\r\n2019-04-02 23:47:38.646661: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-04-02 23:47:38.666653: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'nvcuda.dll'; dlerror: nvcuda.dll not found\r\n2019-04-02 23:47:38.666842: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)\r\n```\r\n\r\nI think the main issue is \"Could not dlopen library 'nvcuda.dll'\".\r\n\r\nHowever, I have installed the latest NVIDIA driver (version 419.67), and 'nvcuda.dll' can be found in C:\\Windows\\System32\\nvcuda.dll.\r\n\r\nThe issue is also posted in [stackoverflow](https://stackoverflow.com/questions/55478968/errors-with-tensorflow-gpu-2-0alpha0).\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@loveunk Could you check this [issue](https://github.com/tensorflow/tensorflow/issues/19266) which is similar to your issue but for older TF version. If those solutions doesn't work, could you uninstall and reinstall CUDA and cuDNN? Please let us know how it progresses. Also, try to uninstall and reinstall tensorflow-gpu. Thanks!", "@jvishnuvardhan It does not work. The same error exists.", "@loveunk  Could you uninstall and reinstall CUDA and cuDNN? Can you try some solutions that were listed in some of GitHub issues and Stackoverflow? This is more related to NVIDIA's CUDA so they might have solutions in their GitHub repo. Thanks!", "@jvishnuvardhan I have tried several different penitential solutions, including reinstalling CUDA Toolkit and cuDNN. Unfortunately, none of them work.\r\n\r\nCurrently, it's hard to judge whether this issue is more related to CUDA than tensorflow. \r\n\r\nI have write some test win32 application with C++ to load 'nvcuda.dll',  it can be found and loaded to the program successfully. ", "YMMV, but we had the same issue with tensorflow-gpu 2.0.0-beta0. Here's how we got around it:\r\n\r\nconda create -n tf_beta python=3.6 cudnn cudatoolkit cudatoolkit-dev pip; conda activate tf_beta; pip install tensorflow-gpu==2.0.0-beta0\r\n\r\nFor alpha0, this seems to work on our 2080ti rig:\r\n\r\nconda create -n tf_alpha python=3.6 cudnn cudatoolkit cudatoolkit-dev pip; conda activate tf_alpha; pip install tensorflow-gpu==2.0.0-alpha0\r\n\r\n\r\nSystem information\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Xubuntu 18.04\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version: tensorflow-gpu 2.0 alpha0 & beta0\r\nPython version: 3.6\r\nInstalled using virtualenv? pip? conda?: conda with pip\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version: cudatoolkit 10.0.130, and cudnn 7.5\r\nGPU model and memory: 2x NVIDIA RTX 2080 TI / 11G x 2 (NVLink 22G)", "@loveunk Is this resolved. If yes, please close the issue. Thanks!", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!"]}]