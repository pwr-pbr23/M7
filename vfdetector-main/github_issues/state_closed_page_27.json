[{"number": 54641, "title": "Migrates all `tf_python_pybind_extension` targets to the `cc_shared_library` implementation.", "body": "Migrates all `tf_python_pybind_extension` targets to the `cc_shared_library` implementation.\n", "comments": []}, {"number": 54640, "title": "Migrates `tensorflow_cc` to `cc_shared_library`.", "body": "Migrates `tensorflow_cc` to `cc_shared_library`.\n\nImplements `//tensorflow:tensorflow_cc` using `cc_shared_library`. `dynamic_deps` is not used.\n", "comments": []}, {"number": 54639, "title": "Migrates `tensorflow_framework` to `cc_shared_library`.", "body": "Migrates `tensorflow_framework` to `cc_shared_library`.\n\nImplements `//tensorflow:libtensorflow_framework.so` using `tf_cc_shared_library`. `dynamic_deps` is not used.\n", "comments": []}, {"number": 54638, "title": "Migrates `libtensorflow.so` to `cc_shared_library`.", "body": "Migrates `libtensorflow.so` to `cc_shared_library`.\n\nImplements `//tensorflow:libtensorflow.so` with `cc_shared_library`. `dynamic_deps` is not used. It will be added in a separate change.\n", "comments": []}, {"number": 54637, "title": "Where is  \"from tensorflow.python.keras.layers.normalization import LayerNormalization\" in ther repository", "body": "It seems that tensorflow/python/keras/layers/__init__.py does not have a line \"from tensorflow.python.keras.layers.normalization import LayerNormalization\", but why the error\r\n\r\n```\r\ntensorflow/python/keras/layers/__init__.py](https://localhost:8080/#) in <module>()\r\n    144 \r\n    145 # Normalization layers.\r\n--> 146 from tensorflow.python.keras.layers.normalization import LayerNormalization\r\n    147 from tensorflow.python.keras.layers.normalization_v2 import SyncBatchNormalization\r\n    148 \r\n\r\nImportError: cannot import name 'LayerNormalization' from 'tensorflow.python.keras.layers.normalization'\r\n```\r\n\r\noccur? Thanks.", "comments": ["Hi @Mather10 !\r\nCould you please update the template too as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks!", "@Mather10 - You may want to use \r\n```\r\nfrom tensorflow.keras.layers import LayerNormalization\r\n```", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54637\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54637\">No</a>\n"]}, {"number": 54636, "title": "[XLA:DYNAMIC_PADDER] Do not pad init values into non-reduced dimensions", "body": "[XLA:DYNAMIC_PADDER] Do not pad init values into non-reduced dimensions\n", "comments": []}, {"number": 54635, "title": "Fix flatbuffer import by fixing output signature conversion to mlir.", "body": "Fix flatbuffer import by fixing output signature conversion to mlir.\n\nMultiple signature outputs can refer to the same tensor. Avoid setting signature output attribute at the same index by maintaining a set.\n", "comments": []}, {"number": 54634, "title": "ssertionError: The decorated function's signature must exactly match the signature of the overridden op", "body": "How to fix the error below? Thanks.\r\n\r\n```\r\ndist-packages/tensorflow/python/util/dispatch.py](https://localhost:8080/#) in decorator(func)\r\n    179   \"\"\"\r\n    180 \r\n--> 181   def __init__(self, override_func, types):\r\n    182     self._types = types\r\n    183     self._override_func = override_func\r\n\r\nAssertionError: The decorated function's signature must exactly match the signature of the overridden op.\r\n```\r\n", "comments": ["@Mather10 \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),\r\nThanks!", "I have the same problem any suggestion ?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54634\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54634\">No</a>\n"]}, {"number": 54633, "title": "Implement XLA JIT cache persistence so that compiled cache entries can be saved and later loaded.", "body": "Implement XLA JIT cache persistence so that compiled cache entries can be saved and later loaded.\n", "comments": []}, {"number": 54632, "title": "Support replica_group of XLA Allreduce with CollectiveReduceV2.", "body": "Support replica_group of XLA Allreduce with CollectiveReduceV2.\n\nIntroduces a new op CollectiveAssignGroupV2. Behavior:\n\n- On non-XLA path (TF Ops Kernel): computes group_key based on the group assignment input, such that participants in the same group in group assignment share the same group key; the logic is based on a similar logic in DTensor.\n\n- On XLA path (MLIR tf2xla bridge): Op is merged with the following CollectiveReduceV2 to emit a XLAAllReduce Op with the group assignment, which is then lowered as an XLAAllReduce. This is an optimization for GPU targets, though might be a requirement on TPU targets. I am slightly concerned about this detail leaking back to TF, though I think the benefit of supporting group-assignment in both TF Kernel path and XLA path outweights the possibility of leaking on the XLA path.\n\n- This Op is not supported in the legacy bridge. Although (I think) we do need to add a registration in the legacy bridge in order use it in the MLIR bridge.\n\nWith these update semantics in place, DTensor's CollectiveLowering logic can emit same Collective ops on all *PU platforms. The bridge handles the platform dependence. I will work on that move in a child CL.\n", "comments": []}, {"number": 54631, "title": "Fixes https://github.com/tensorflow/tensorflow/issues/51502", "body": "Fixes https://github.com/tensorflow/tensorflow/issues/51502\r\n\r\nCurrently error is long and confusing in some cases. This CL tries to create a simple and clear error description as shown below.\r\n\r\nOp name         : Max\r\nRoot-cause      : Value for attr 'T' of complex128 is not in the list of allowed values\r\nSupported types : float, double, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64, qint8, quint8, qint32, qint16, quint16\r\n<img width=\"1279\" alt=\"Screen Shot 2022-02-25 at 5 14 40 PM\" src=\"https://user-images.githubusercontent.com/46058173/155822913-f54350e7-6072-4996-b4b9-2eb173ac26d7.png\">\r\n\r\n\r\n", "comments": []}, {"number": 54630, "title": "Enable TensorFlow quantization tests in the OSS builds", "body": "Enable TensorFlow quantization tests in the OSS builds\n", "comments": []}, {"number": 54629, "title": "[TFRT:XLIR] Enhance TFRT to support GPU infeed/outfeed through BEF_THUNK", "body": "[TFRT:XLIR] Enhance TFRT to support GPU infeed/outfeed through BEF_THUNK\nexecution.\n\nAdd XLIR instructions for infeed and outfeed.\nAdd rewrite patterns to convert lmhlo.infeed/outfeed to xlir.infeed/outfeed.\nImplement the BEF_THUNK kernels for xlir.infeed/outfeed.\n\nAdd MLIR tests for rewriting lmhlo.infeed/outfeed to xlir.infeed/outfeed.\n\nAdd relevant xla/gpu tests to bef_thunk_tests and bef_executable_tests.\n", "comments": []}, {"number": 54628, "title": "[tf.data service] Fix issue with dynamic sharding of GroupByWindow.", "body": "[tf.data service] Fix issue with dynamic sharding of GroupByWindow.\n\nPreviously, we incorrectly propagated split providers into iterators created for the nested reduce_fn datasets in GroupByWindow. This could cause errors if a GroupByWindow uses a reduce_fn that produces a splittable dataset.\n\nThis CL also fixes a segmentation fault that previously happened in split_utils when CreateInputIteratorContexts received a context with too few split providers.\n", "comments": []}, {"number": 54627, "title": "Add a new API for DeviceAssigment to return a map from physical device to logical device ID.", "body": "Add a new API for DeviceAssigment to return a map from physical device to logical device ID.\n", "comments": []}, {"number": 54626, "title": "Fix the OSS build errors on the quantize model wrapper", "body": "Fix the OSS build errors on the quantize model wrapper\n", "comments": []}, {"number": 54625, "title": "[tf:tfrt] Add more lit test for the transpose codegen", "body": "[tf:tfrt] Add more lit test for the transpose codegen\n\nNow that the code generation strategy is more stable, these tests will\nmake sure that we are aware of any changes impacting the transpose transformations.\n", "comments": []}, {"number": 54624, "title": "[tf:tfrt] Improve vectorization strategy of transpose ops", "body": "[tf:tfrt] Improve vectorization strategy of transpose ops\n\nThis patch adds some smartness to the way vector dimensions are picked for transpose operations.\nSpecifically, we pick the dimensions that would lead to contiguous memory accesses for both the\ninput and output of the generic op implementing the transpose operation. This leads to a much\nmore efficient memory access pattern for all the transpose cases and a higher benefit when those\ncases fall into the AVX2 lowering patterns.\n", "comments": []}, {"number": 54623, "title": "[tf:tfrt] Disable lowering of shape_cast in transpose codegen", "body": "[tf:tfrt] Disable lowering of shape_cast in transpose codegen\n\nShape cast operations should be optimized away during the lowering of the\nvector transfer operations. If we lower them too early, they will generate\na set of insert/extract operations that won't be optimized away, even by LLVM.\n", "comments": []}, {"number": 54622, "title": "Better coalescing in parallel loop generation", "body": "Better coalescing in parallel loop generation\n", "comments": []}, {"number": 54621, "title": "Pack BF16 values and element indices together into single 32-bit values to", "body": "Pack BF16 values and element indices together into single 32-bit values to\nimprove top-k speed on TPUs.\n", "comments": []}, {"number": 54620, "title": "Fix and re-enable tf_doctest.", "body": "Fix and re-enable tf_doctest.\n\n* Fix failing tests.\n* Shorten the one really long (90s) doctest.\n* Add sharding support.\n* Re-enable the test.\n", "comments": []}, {"number": 54619, "title": "Profiler need to know if op is eagerly executed.", "body": "Profiler need to know if op is eagerly executed.\nCurrent definition is eager = run under EagerKernelExecute but !run under ExecutorState::Process (graph execution)\n\nwith the unification of eager op path => eager function path this no longer works.\n", "comments": []}, {"number": 54618, "title": "Internal change with an external component for testing new CI changes.", "body": "Internal change with an external component for testing new CI changes.\n", "comments": []}, {"number": 54617, "title": "[tf.data] Fixes issue where compute_batch_size fails to handle tensor specs with no output shape.", "body": "[tf.data] Fixes issue where compute_batch_size fails to handle tensor specs with no output shape.\n", "comments": []}, {"number": 54616, "title": "Add some complicated unit tests in merge_control_flow pass.", "body": "Add some complicated unit tests in merge_control_flow pass.\n", "comments": []}, {"number": 54615, "title": "Include kernel launch dimensions in the string representation of KernelThunk", "body": "Include kernel launch dimensions in the string representation of KernelThunk\n", "comments": []}, {"number": 54614, "title": "Tighten access controls in HloDataflowAnalysis", "body": "Tighten access controls in HloDataflowAnalysis\n", "comments": []}, {"number": 54613, "title": "Implement _without_tensor_names for tf_probability TypeSpecs", "body": "Implement _without_tensor_names for tf_probability TypeSpecs\n", "comments": []}, {"number": 54612, "title": "TensorFlow doesn't recognize RTX 3080 ti Laptop GPU", "body": "**System information**\r\n- OS Linux Ubuntu 21.10\r\n- TensorFlow installed with pip install tensorflow==2.7.0\r\n- Python 3.9.7:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version 9.4:\r\n- CUDA 11.2 / cuDNN 8.1:\r\n- GPU RXT 3080 ti Laptop\r\n\r\n**I installed CUDA and Cudnn but TensorFlow doesn't recognize GPU**\r\n\r\n**I have 510 Nvidia driver but saw that TensorFlow support  CUDA 11.2 and Cudnn 8.1. That mean I have to download 460 Nvidia driver. Unfortunately, my GPU does not work with 460 Nvidia driver. I tried to continue with CUDA 11.2 and CUDNN 8.1 but I got mistake as below. Can anyone help, please?**\r\n\r\n\r\n**Python Code**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.client import device_lib\r\n\r\nprint(tf.__version__)\r\nprint(device_lib.list_local_devices())\r\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\r\n```\r\n\r\n**OUTPUT**\r\n2022-02-25 21:03:35.125701: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2022-02-25 21:03:36.870592: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported display driver / cuda driver combination\r\n2022-02-25 21:03:36.870651: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ak-pc\r\n2022-02-25 21:03:36.870654: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ak-pc\r\n2022-02-25 21:03:36.870702: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.103.1\r\n2022-02-25 21:03:36.870721: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 510.47.3\r\n2022-02-25 21:03:36.870724: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 510.47.3 does not match DSO version 470.103.1 -- cannot find working devices in this configuration\r\n[name: \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 16144767737604269050\r\nxla_global_id: -1\r\n]\r\nNum GPUs Available:  0\r\n\r\n", "comments": ["@serdarakyol - Did you downgrade the CUDA to 11.2?  Looking at Nvidia docs it looks like the display driver and cuda driver do not match - https://docs.nvidia.com/deploy/cuda-compatibility/#check-for-compatibility-support\r\n\r\nI think TensorFlow should work for 11.2+ so you may not need to downgrade the driver or Cuda version.  I have not verified this, but believe 11.2+ are compatible with 11.2.", "Thanks for letting me know @sampathweb\r\nI had the problem when I installed via ```Additional Drivers```. I installed drivers via ppa package and now it's working. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54612\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54612\">No</a>\n"]}]