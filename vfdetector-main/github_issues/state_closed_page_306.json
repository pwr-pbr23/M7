[{"number": 45077, "title": "Model saving using the tf format uses the wrong TensorSpec for convolutions", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Yes\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not on mobile device\r\n- TensorFlow installed from (source or binary): Colab\r\n- TensorFlow version (use command below): 2.3\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): Not compiling from source\r\n- GCC/Compiler version (if compiling from source): Not compiling from source\r\n- CUDA/cuDNN version: No GPU\r\n- GPU model and memory: No GPU\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nWhen saving a subclassed model, using the SavedModel format, the inputs specifications for a convolution layer are not correctly infered.\r\n\r\nThe error is the following:\r\n```\r\nValueError: Python inputs incompatible with input_signature:\r\n  inputs: (\r\n    tf.Tensor(\r\n[[[[1.]\r\n   [1.]\r\n   [1.]\r\n   ...\r\n   [1.]\r\n   [1.]\r\n   [1.]]\r\n\r\n  [[1.]\r\n   [1.]\r\n   [1.]\r\n   ...\r\n   [1.]\r\n   [1.]\r\n   [1.]]\r\n\r\n  [[1.]\r\n   [1.]\r\n   [1.]\r\n   ...\r\n   [1.]\r\n   [1.]\r\n   [1.]]\r\n\r\n  ...\r\n\r\n  [[1.]\r\n   [1.]\r\n   [1.]\r\n   ...\r\n   [1.]\r\n   [1.]\r\n   [1.]]\r\n\r\n  [[1.]\r\n   [1.]\r\n   [1.]\r\n   ...\r\n   [1.]\r\n   [1.]\r\n   [1.]]\r\n\r\n  [[1.]\r\n   [1.]\r\n   [1.]\r\n   ...\r\n   [1.]\r\n   [1.]\r\n   [1.]]]], shape=(1, 32, 32, 1), dtype=float32))\r\n  input_signature: (\r\n    TensorSpec(shape=(None, 96, 96, 1), dtype=tf.float32, name='input_1'))\r\n```\r\n\r\nUsing the solution from [the documentation on how to specify the signature during export](https://www.tensorflow.org/guide/saved_model#specifying_signatures_during_export) didn't work.\r\n\r\nThe error I got was the following:\r\n```\r\nAttributeError: 'function' object has no attribute 'get_concrete_function'\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nI would like to be able to save my convolutional model and be able to reuse it on inputs with arbitrary batch size (already the case), width and height (not the case).\r\n\r\n**Standalone code to reproduce the issue**\r\nThis is [the colab](https://colab.research.google.com/drive/1YBSOmfNMFkmA_t1hyjCA0dG9vUiYxQNt?usp=sharing) I did to illustrate this bug.\r\n\r\n**Other info / logs** \r\n\r\nThe full stack trace for the inputs spec mismatch is the following:\r\n\r\n```\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-9-bba6984211c7> in <module>()\r\n----> 1 other_model(tf.ones([1, 32, 32, 1]))\r\n\r\n5 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n    983 \r\n    984         with ops.enable_auto_cast_variables(self._compute_dtype_object):\r\n--> 985           outputs = call_fn(inputs, *args, **kwargs)\r\n    986 \r\n    987         if self._activity_regularizer:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saved_model/utils.py in return_outputs_and_add_losses(*args, **kwargs)\r\n     69     inputs = args[inputs_arg_index]\r\n     70     args = args[inputs_arg_index + 1:]\r\n---> 71     outputs, losses = fn(inputs, *args, **kwargs)\r\n     72     layer.add_loss(losses, inputs=inputs)\r\n     73 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    778       else:\r\n    779         compiler = \"nonXla\"\r\n--> 780         result = self._call(*args, **kwds)\r\n    781 \r\n    782       new_tracing_count = self._get_tracing_count()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    842       canon_args, canon_kwds = \\\r\n    843           self._stateful_fn._function_spec.canonicalize_function_inputs(  # pylint: disable=protected-access\r\n--> 844               *args, **kwds)\r\n    845       # If we did not create any variables the trace we have is good enough.\r\n    846       return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in canonicalize_function_inputs(self, *args, **kwargs)\r\n   2620           inputs,\r\n   2621           self._input_signature,\r\n-> 2622           self._flat_input_signature)\r\n   2623       return inputs, {}\r\n   2624 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _convert_inputs_to_signature(inputs, input_signature, flat_input_signature)\r\n   2711       flatten_inputs)):\r\n   2712     raise ValueError(\"Python inputs incompatible with input_signature:\\n%s\" %\r\n-> 2713                      format_error_message(inputs, input_signature))\r\n   2714 \r\n   2715   if need_packing:\r\n\r\nValueError: Python inputs incompatible with input_signature:\r\n  inputs: (\r\n    tf.Tensor(\r\n[[[[1.]\r\n   [1.]\r\n   [1.]\r\n   ...\r\n   [1.]\r\n   [1.]\r\n   [1.]]\r\n\r\n  [[1.]\r\n   [1.]\r\n   [1.]\r\n   ...\r\n   [1.]\r\n   [1.]\r\n   [1.]]\r\n\r\n  [[1.]\r\n   [1.]\r\n   [1.]\r\n   ...\r\n   [1.]\r\n   [1.]\r\n   [1.]]\r\n\r\n  ...\r\n\r\n  [[1.]\r\n   [1.]\r\n   [1.]\r\n   ...\r\n   [1.]\r\n   [1.]\r\n   [1.]]\r\n\r\n  [[1.]\r\n   [1.]\r\n   [1.]\r\n   ...\r\n   [1.]\r\n   [1.]\r\n   [1.]]\r\n\r\n  [[1.]\r\n   [1.]\r\n   [1.]\r\n   ...\r\n   [1.]\r\n   [1.]\r\n   [1.]]]], shape=(1, 32, 32, 1), dtype=float32))\r\n  input_signature: (\r\n    TensorSpec(shape=(None, 96, 96, 1), dtype=tf.float32, name='input_1'))\r\n```\r\n\r\nThe full stack trace for the signature specification error is the following:\r\n```\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-12-f5c9e67f3a76> in <module>()\r\n----> 1 call = model.__call__.get_concrete_function(tf.TensorSpec([None, None, None, 1], tf.float32))\r\n\r\nAttributeError: 'function' object has no attribute 'get_concrete_function'\r\n```", "comments": ["Small note, the same problem happens when using the `load` function from the `saved_model` module, i.e. `other_model = tf.saved_model.load('my_model')`.", "I am able to replicate the issue reported, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/91bff313ec70d32c6b691f4fdb1b0130/untitled471.ipynb)", "@zaccharieramzi I loaded your model and checked signature as shown below.\r\n\r\n```\r\nloaded = tf.keras.models.load_model('my_model')\r\ninfer = loaded.signatures[\"serving_default\"]\r\nprint(infer.structured_outputs)\r\n\r\n# output\r\n{'output_1': TensorSpec(shape=(None, 96, 96, 1), dtype=tf.float32, name='output_1')}\r\n```\r\n\r\nSo, the loaded model was expecting a signature of (None, 96, 96, 1), while you are providing (None, 32, 32, 1), so the code was throwing an error `ValueError: Python inputs incompatible with input_signature:`.\r\n\r\nWhen I changed inputs to (1, 96, 96, 1) or (2, 96, 96, 1) or any batch_size, it is working as expected. \r\n\r\nPlease check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/149165f921c9a907f570b5555d7e060e/untitled471.ipynb).\r\n\r\nPlease let me know what you think. Thanks!", "Yes but that's exactly the problem : it shouldn't have this signature because it uses only convolutions.\r\nI should be able to use convolutions on images of arbitrary width and height no matter what I used it on in an arbitrary run (first run or not).\r\nTo be explicit I should be able to use the saved model and on both 32\u00d732 and 96\u00d796 images. There is nothing in the convolution operation (both mathematically and in impl\u00e9mentation) preventing me from that and it works perfectly when not using the TF format to save the model.", "Hi Zaccharie, subclassed models automatically save the inputs shapes the first time the model is called. For greater control over the shapes, you should make `call` a `tf.function` with an `input_signature`:\r\n\r\n```\r\nclass MyModel(tf.keras.models.Model):\r\n  def __init__(self, **kwargs):\r\n    super().__init__(**kwargs)\r\n    self.conv = tf.keras.layers.Conv2D(\r\n        1,\r\n        3,\r\n        padding='same',\r\n    )\r\n\r\n  @tf.function(input_signature=[tf.TensorSpec([None, None, None, 1])])\r\n  def call(self, inputs):\r\n    return self.conv(inputs)\r\n``` ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45077\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45077\">No</a>\n", "@k-w-w hmm that seems like a weird behaviour, but fair enough. Also, your fix does work in this case, thanks!\r\nWhat I don't understand though is why this error only happens once saved though? Because before I save it, the model works perfectly fine on different image sizes."]}, {"number": 45076, "title": "Fixed multiple mistakes in XNNPACK delegate tensor type checking code", "body": "Some XNNPACK delegate methods intended for visiting nodes (`VisitConv2DNode`, `VisitFullyConnectedNode` and `VisitMediaPipeDeconvolutionNode`) had typos in their tensor type checking code, which leaded to double checking of `filter_tensor` type and `bias_tensor` type not being checked at all.", "comments": []}, {"number": 45075, "title": "Fix wrong image_data_format() usage", "body": "", "comments": []}, {"number": 45073, "title": "Quantization differences from TensorFlow 1.9 to 2.3", "body": "An issue I found is in regards to INT8 or UINT8 weights on Conv / Dense (trainable) layers during quantization.  Namely, there does not seem to be any way to support UINT8 weights on these layers.  With version 1.9 this was supported as I have implemented my own runtime.  Currently, it seems to only support INT8, Float32, and Float16.  My runtime has to change as a result of this, and it would be nice to know if I am correct in my findings so far on this issue.  I would prefer UINT8 weights on the Conv2D / Dense layers but as mentioned, I cannot see any feasible solution to this problem.  If I am wrong please point me to the correct API usage.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OS X\r\n- TensorFlow installed from (source or binary): 2.3.0 GPU (Docker image)\r\n\r\n```\r\n\r\n# Example code taken from Post-Training Quantization documentation\r\n# https://www.tensorflow.org/lite/performance/post_training_quantization\r\n\r\nimport tensorflow as tf\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ndef representative_dataset_gen():\r\n  for _ in range(num_calibration_steps):\r\n    # Get sample input data as a numpy array in a method of your choosing.\r\n    yield [input]\r\nconverter.representative_dataset = representative_dataset_gen\r\n# there is no such tf.lite.OpsSet.TFLITE_BUILTINS_UINT8\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8  # or tf.int8 ( note this has zero effect on the tensors produced for Conv2D operations -- all of which include signed int8 unless you were to output the model as float16/32 )\r\nconverter.inference_output_type = tf.uint8  # or tf.int8\r\ntflite_quant_model = converter.convert()\r\n```\r\n\r\n", "comments": ["@krworks,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!", "the dataset is simple images where the values range from -1.0 to 1.0 in shape [24, 32, 1]\r\nEDIT: A change from 0.0 to 1.0 had no effect (and it wouldn't because UINT supports this sort of scaling with the offset)", "Something that seems relevant here is TFLite implementation details, and the additional quantization capabilities related to INT8/16.  Currently, the quantization of INT8/INT16 supports N quantization parameters for each convolution op where N = filter depth.  The TFLite code states that this is not implemented on UINT8, and it doesn't get into saying if it's actually planned or not.  Since the quantization process is scaling and offsetting the products differently at each filter depth it would not be theoretically as accurate in UINT8 mode with a singular set of scale and offset.\r\n\r\nThere must be reasons why it's not implemented in legacy mode. Perhaps just the existence of UINT8 weights could be used as a legacy switch to not utilize a newer data structure in-format.  At any rate, it would be nice if legacy output could be supported with TFLite export.  It certainly looks as if it's parsed properly with TFLite library.  With my data, I don't see any true performance benefit with the far more complicated quantization process, but perhaps for large models, this is very beneficial.\r\n\r\nI am implementing this none-the-less, so I can get beyond this without rolling back.  Any possible way we could support the more simpleton UINT8 format [as well] here?  For the sake of comparison, this would be great without rolling back to TF 1.9.\r\n\r\n**source from kernel_util.cc:81 in TFLite / TensorFlow r2.3 branch**\r\n```\r\n  const float input_scale = input->params.scale;\r\n  const float output_scale = output->params.scale;\r\n  const float* filter_scales = affine_quantization->scale->data;\r\n  for (int i = 0; i < num_channels; ++i) {\r\n    // If per-tensor quantization parameter is specified, broadcast it along the\r\n    // quantization dimension (channels_out).\r\n    const float scale = is_per_channel ? filter_scales[i] : filter_scales[0];\r\n    const double filter_scale = static_cast<double>(scale);\r\n    const double effective_output_scale = static_cast<double>(input_scale) *\r\n                                          filter_scale /\r\n                                          static_cast<double>(output_scale);\r\n    int32_t significand;\r\n    int channel_shift;\r\n    QuantizeMultiplier(effective_output_scale, &significand, &channel_shift);\r\n    per_channel_multiplier[i] = significand;\r\n    per_channel_shift[i] = channel_shift;\r\n  }\r\n```\r\n\r\nThen, it's effectively used in a **gemm kernel: (kernel_common.h:135)** (i = 0 to filter depth)\r\nThe ApplyMultiplier is applying a different scale based on i.  This is not compatible with UINT8.\r\n```\r\n        if (mul_params.bias()) {\r\n          accum += mul_params.bias()[i];\r\n        }\r\n        if (lhs.zero_point) {\r\n          accum -= lhs.zero_point * rhs.sums[j];\r\n        }\r\n        if (rhs.zero_point) {\r\n          accum -= rhs.zero_point * lhs.sums[i];\r\n        }\r\n        if (lhs.zero_point && rhs.zero_point) {\r\n          accum += lhs.zero_point * rhs.zero_point * depth;\r\n        }\r\n        ApplyMultiplier(mul_params, i, &accum);\r\n        accum += dst->zero_point;\r\n        accum = std::min<AccumScalar>(accum, mul_params.clamp_max());\r\n        accum = std::max<AccumScalar>(accum, mul_params.clamp_min());\r\n        *ElementPtr(dst, i, j) = static_cast<DstScalar>(accum);\r\n```\r\n\r\n", "@krworks \r\nCould you please let us know if this is still an issue in latest stable TF v2.6.0 ?Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45073\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45073\">No</a>\n"]}, {"number": 45072, "title": "Physical devices not shown when I do have one", "body": "I first import TensorFlow and try to find a GPU configure but it shows nothing.\r\n`physical_devices = tf.config.list_physical_devices(\"GPU\")`\r\n![image](https://user-images.githubusercontent.com/68514251/99892785-b91dc180-2c46-11eb-9ad1-0d2173704169.png)\r\n", "comments": ["@CalendulaED \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]\r\n\r\nAs per the screen shot, you need to find which directory these dlls are in, and then update your PATH to include that directory. Thanks!\r\n\r\nPlease refer to these issues with same error: #36111, #40804, [link](https://stackoverflow.com/questions/59823283/could-not-load-dynamic-library-cudart64-101-dll-on-tensorflow-cpu-only-install), [link2](https://github.com/tensorflow/tensorflow/issues/44885#issuecomment-727914704)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45072\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45072\">No</a>\n"]}, {"number": 45071, "title": "Missing DLL when importing (but which one?)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10 20H2\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version: 2 (latest as of Nov 22 2020)\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: Installed via pip in a conda env\r\n- GPU model and memory: GeForce 1050Ti 4GB\r\n\r\n\r\n\r\n**I run into an issue where I cannot import Tensorflow when using Jupyter from a Conda environment*\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n-Fresh Anaconda install\r\n-Fresh C++ redist install\r\n-Create new Conda environment with Python 3.8\r\n-Activate said environment\r\n-Install TF via pip (important, as I need 2.3.0+)\r\n- Install Jupyter notebook\r\n-Open Jupyter notebook\r\n-Try to import TF with `import tensorflow as tf`\r\n-Get error saying I'm missing a DLL, not sure which one\r\n\r\nWhat I noticed and is curious that my _pywrap_tensorflow_internal.py file is completely empty even after multiple reinstalls, and it doesn't matter if I use a brand-new Conda environment or just install on Windows \"manually\" via pip. Both options make the file empty and I run into this issue.\r\n\r\nTrace:\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n~\\.conda\\envs\\wtf\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     63   try:\r\n---> 64     from tensorflow.python._pywrap_tensorflow_internal import *\r\n     65   # This try catch logic is because there is no bazel equivalent for py_extension.\r\n\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-3-6dadb603fc7c> in <module>\r\n      2 import datetime\r\n      3 \r\n----> 4 import tensorflow as tf\r\n      5 import IPython\r\n      6 import IPython.display\r\n\r\n~\\.conda\\envs\\wtf\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     39 import sys as _sys\r\n     40 \r\n---> 41 from tensorflow.python.tools import module_util as _module_util\r\n     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader\r\n     43 \r\n\r\n~\\.conda\\envs\\wtf\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     38 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\r\n     39 \r\n---> 40 from tensorflow.python.eager import context\r\n     41 \r\n     42 # pylint: enable=wildcard-import\r\n\r\n~\\.conda\\envs\\wtf\\lib\\site-packages\\tensorflow\\python\\eager\\context.py in <module>\r\n     33 from tensorflow.core.protobuf import config_pb2\r\n     34 from tensorflow.core.protobuf import rewriter_config_pb2\r\n---> 35 from tensorflow.python import pywrap_tfe\r\n     36 from tensorflow.python import tf2\r\n     37 from tensorflow.python.client import pywrap_tf_session\r\n\r\n~\\.conda\\envs\\wtf\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py in <module>\r\n     26 \r\n     27 # pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import\r\n---> 28 from tensorflow.python import pywrap_tensorflow\r\n     29 from tensorflow.python._pywrap_tfe import *\r\n\r\n~\\.conda\\envs\\wtf\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     81 for some common reasons and solutions.  Include the entire stack trace\r\n     82 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 83   raise ImportError(msg)\r\n     84 \r\n     85 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\csana\\.conda\\envs\\wtf\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["@csanadpoda\r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here.](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)\r\n.Also, please follow the instructions from to install from [Tensorflow website.](https://www.tensorflow.org/install/source_windows)\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167\r\n\r\nIf the issue still persists please post it on Anaconda repo as we do not support conda environment.\r\nThis issue is more suitable on Continuum [Anaconda repo](https://github.com/ContinuumIO/anaconda-issues/issues) since its related to TF installation with Anaconda.\r\nPlease post it on [Continuum Anaconda](https://github.com/ContinuumIO/anaconda-issues/issues).\r\nThanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45071\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45071\">No</a>\n"]}, {"number": 45070, "title": "v2.0.4rc2 CUBLAS_STATUS_NOT_INITIALIZED", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): v2.4.0-rc2-0-g0b06f2927b 2.4.0-rc2\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): bazel 3.7.0\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\n- CUDA/cuDNN version: CUDA 11.1 - 8.0.5\r\n- GPU model and memory: RTX 3080 10016MiB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nCUBLAS_STATUS_NOT_INITIALIZED\r\n\r\n**Describe the expected behavior**\r\n\r\nCublas to be initialized\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\npip install ai-benchmark\r\n```\r\n\r\n```python\r\n from ai_benchmark import AIBenchmark\r\nresults = AIBenchmark().run() \r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\n2020-11-22 00:03:17.085732: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-11-22 00:03:17.086649: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2020-11-22 00:03:17.125644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-22 00:03:17.126133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:08:00.0 name: GeForce RTX 3080 computeCapability: 8.6\r\ncoreClock: 1.71GHz coreCount: 68 deviceMemorySize: 9.78GiB deviceMemoryBandwidth: 707.88GiB/s\r\n2020-11-22 00:03:17.126162: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-11-22 00:03:17.127890: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-11-22 00:03:17.127926: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2020-11-22 00:03:17.128494: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2020-11-22 00:03:17.128616: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2020-11-22 00:03:17.130269: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\r\n2020-11-22 00:03:17.130675: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2020-11-22 00:03:17.130775: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-11-22 00:03:17.130855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-22 00:03:17.131377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-22 00:03:17.131924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-11-22 00:03:17.131946: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-11-22 00:03:17.478652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-11-22 00:03:17.478689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2020-11-22 00:03:17.478696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2020-11-22 00:03:17.478871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-22 00:03:17.479349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-22 00:03:17.479796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-22 00:03:17.480226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/device:GPU:0 with 8754 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3080, pci bus id: 0000:08:00.0, compute capability: 8.6)\r\n2020-11-22 00:03:17.480578: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-11-22 00:03:17.480629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-22 00:03:17.481050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:08:00.0 name: GeForce RTX 3080 computeCapability: 8.6\r\ncoreClock: 1.71GHz coreCount: 68 deviceMemorySize: 9.78GiB deviceMemoryBandwidth: 707.88GiB/s\r\n2020-11-22 00:03:17.481073: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-11-22 00:03:17.481090: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-11-22 00:03:17.481099: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2020-11-22 00:03:17.481107: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2020-11-22 00:03:17.481116: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2020-11-22 00:03:17.481124: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\r\n2020-11-22 00:03:17.481132: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2020-11-22 00:03:17.481141: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-11-22 00:03:17.481176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-22 00:03:17.481614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-22 00:03:17.482021: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-11-22 00:03:17.482036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-11-22 00:03:17.482041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2020-11-22 00:03:17.482048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2020-11-22 00:03:17.482103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-22 00:03:17.482553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-22 00:03:17.482975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/device:GPU:0 with 8754 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3080, pci bus id: 0000:08:00.0, compute capability: 8.6)\r\n2020-11-22 00:03:17.483019: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-11-22 00:03:17.483063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-22 00:03:17.483473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:08:00.0 name: GeForce RTX 3080 computeCapability: 8.6\r\ncoreClock: 1.71GHz coreCount: 68 deviceMemorySize: 9.78GiB deviceMemoryBandwidth: 707.88GiB/s\r\n2020-11-22 00:03:17.483487: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-11-22 00:03:17.483496: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-11-22 00:03:17.483504: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2020-11-22 00:03:17.483512: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2020-11-22 00:03:17.483520: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2020-11-22 00:03:17.483527: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\r\n2020-11-22 00:03:17.483536: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2020-11-22 00:03:17.483543: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-11-22 00:03:17.483578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-22 00:03:17.484013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-22 00:03:17.484420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-11-22 00:03:17.484433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-11-22 00:03:17.484439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2020-11-22 00:03:17.484443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2020-11-22 00:03:17.484496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-22 00:03:17.484935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-22 00:03:17.485350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/device:GPU:0 with 8754 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3080, pci bus id: 0000:08:00.0, compute capability: 8.6)\r\n*  TF Version: 2.4.0-rc2\r\n*  Platform: Linux-5.4.0-54-generic-x86_64-with-glibc2.29\r\n*  CPU: N/A\r\n*  CPU RAM: 16 GB\r\n*  GPU/0: GeForce RTX 3080\r\n*  GPU RAM: 8.5 GB\r\n*  CUDA Version: 11.1\r\n*  CUDA Build: V11.1.105\r\n\r\nThe benchmark is running...\r\nThe tests might take up to 20 minutes\r\nPlease don't interrupt the script\r\n\r\n1/19. MobileNet-V2\r\n\r\n2020-11-22 00:03:24.434737: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-11-22 00:03:24.434977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-22 00:03:24.436277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:08:00.0 name: GeForce RTX 3080 computeCapability: 8.6\r\ncoreClock: 1.71GHz coreCount: 68 deviceMemorySize: 9.78GiB deviceMemoryBandwidth: 707.88GiB/s\r\n2020-11-22 00:03:24.436333: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-11-22 00:03:24.436375: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-11-22 00:03:24.436400: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2020-11-22 00:03:24.436426: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2020-11-22 00:03:24.436451: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2020-11-22 00:03:24.436476: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\r\n2020-11-22 00:03:24.436501: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2020-11-22 00:03:24.436525: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-11-22 00:03:24.436619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-22 00:03:24.437700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-22 00:03:24.438732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-11-22 00:03:24.439178: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-11-22 00:03:24.439311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-22 00:03:24.440332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:08:00.0 name: GeForce RTX 3080 computeCapability: 8.6\r\ncoreClock: 1.71GHz coreCount: 68 deviceMemorySize: 9.78GiB deviceMemoryBandwidth: 707.88GiB/s\r\n2020-11-22 00:03:24.440369: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-11-22 00:03:24.440397: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-11-22 00:03:24.440419: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2020-11-22 00:03:24.440441: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2020-11-22 00:03:24.440463: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2020-11-22 00:03:24.440486: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\r\n2020-11-22 00:03:24.440507: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2020-11-22 00:03:24.440530: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-11-22 00:03:24.440621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-22 00:03:24.441695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-22 00:03:24.442723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-11-22 00:03:24.442781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-11-22 00:03:24.442797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2020-11-22 00:03:24.442810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2020-11-22 00:03:24.442957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-22 00:03:24.443970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-22 00:03:24.444408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8754 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3080, pci bus id: 0000:08:00.0, compute capability: 8.6)\r\n2020-11-22 00:03:24.730375: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\r\n2020-11-22 00:03:24.782653: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3193620000 Hz\r\n2020-11-22 00:03:25.752569: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-11-22 00:03:26.979335: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-11-22 00:03:26.985894: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n```", "comments": ["@Noobzik,\r\nPlease try limiting the GPU memory growth as specified in [this guide](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and let us know if it helps.\r\n\r\nAlso, take a look at issue [#9489](https://github.com/tensorflow/tensorflow/issues/9489#issuecomment-562394257) with a similar error for reference. Thanks!", "Add this at the start of your code just after importing everything:\r\n`physical_devices = tf.config.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)`", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45070\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45070\">No</a>\n"]}, {"number": 45069, "title": "AttributeError: module 'tensorflow.python.framework.ops' has no attribute 'register_tensor_conversion_function'", "body": "AttributeError: module 'tensorflow.python.framework.ops' has no attribute 'register_tensor_conversion_function'\r\n\r\nI got this error when I run the file in cmd. But when I run in spyder, I did not get the error\r\n**Code**\r\n```\r\nfrom tensorflow.keras.models import load_model\r\nmodel=load_model('tf_model.h5')\r\nprint(model.summary())\r\n```\r\n\r\n**Packages**\r\nabsl-py==0.11.0\r\nalabaster==0.7.12\r\nappdirs==1.4.4\r\nastor==0.8.1\r\nastroid==2.4.2\r\nastunparse==1.6.3\r\nasync-generator==1.10\r\natomicwrites==1.4.0\r\nattrs==20.3.0\r\nautopep8==1.5.4\r\nBabel==2.9.0\r\nbackcall==0.2.0\r\nbcrypt==3.2.0\r\nblack==20.8b1\r\nbleach==3.2.1\r\ncached-property==1.5.2\r\ncachetools==4.1.1\r\ncertifi==2020.11.8\r\ncffi==1.14.3\r\nchardet==3.0.4\r\nclick==7.1.2\r\ncloudpickle==1.6.0\r\ncolorama==0.4.4\r\ncryptography==3.2.1\r\ndecorator==4.4.2\r\ndefusedxml==0.6.0\r\ndiff-match-patch==20200713\r\ndocutils==0.16\r\nentrypoints==0.3\r\nflake8==3.8.4\r\ngast==0.3.3\r\ngoogle-auth==1.23.0\r\ngoogle-auth-oauthlib==0.4.2\r\ngoogle-pasta==0.2.0\r\ngrpcio==1.33.2\r\nh5py==2.10.0\r\nhelpdev==0.7.1\r\nidna==2.10\r\nimagesize==1.2.0\r\nimportlib-metadata==2.0.0\r\nintervaltree==3.1.0\r\nipykernel==5.3.4\r\nipython==7.19.0\r\nipython-genutils==0.2.0\r\nisort==5.6.4\r\njedi==0.17.2\r\nJinja2==2.11.2\r\njoblib==0.17.0\r\njsonschema==3.2.0\r\njupyter-client==6.1.7\r\njupyter-core==4.7.0\r\njupyterlab-pygments==0.1.2\r\nKeras-Applications==1.0.8\r\nKeras-Preprocessing==1.1.2\r\nkeyring==21.5.0\r\nlazy-object-proxy==1.4.3\r\nMarkdown==3.3.3\r\nMarkupSafe==1.1.1\r\nmccabe==0.6.1\r\nmistune==0.8.4\r\nmypy-extensions==0.4.3\r\nnbclient==0.5.1\r\nnbconvert==6.0.7\r\nnbformat==5.0.8\r\nnest-asyncio==1.4.3\r\nnumpy==1.19.4\r\nnumpydoc==1.1.0\r\noauthlib==3.1.0\r\nopt-einsum==3.3.0\r\npackaging==20.4\r\npandas==1.1.4\r\npandocfilters==1.4.3\r\nparamiko==2.7.2\r\nparso==0.7.0\r\npathspec==0.8.1\r\npathtools==0.1.2\r\npexpect==4.8.0\r\npickleshare==0.7.5\r\nPillow==8.0.1\r\npluggy==0.13.1\r\nprompt-toolkit==3.0.8\r\nprotobuf==3.14.0\r\npsutil==5.7.3\r\nptyprocess==0.6.0\r\npyasn1==0.4.8\r\npyasn1-modules==0.2.8\r\npycodestyle==2.6.0\r\npycparser==2.20\r\npydocstyle==5.1.1\r\npyflakes==2.2.0\r\nPygments==2.7.2\r\npylint==2.6.0\r\npyls-black==0.4.6\r\npyls-spyder==0.1.1\r\nPyNaCl==1.4.0\r\npyparsing==2.4.7\r\nPyQt5==5.12.3\r\nPyQt5-sip==12.8.1\r\nPyQtWebEngine==5.12.1\r\npyrsistent==0.17.3\r\npython-dateutil==2.8.1\r\npython-jsonrpc-server==0.4.0\r\npython-language-server==0.36.1\r\npytz==2020.4\r\npywin32==300\r\npywin32-ctypes==0.2.0\r\npyzmq==20.0.0\r\nQDarkStyle==2.8.1\r\nQtAwesome==1.0.1\r\nqtconsole==4.7.7\r\nQtPy==1.9.0\r\nregex==2020.11.13\r\nrequests==2.25.0\r\nrequests-oauthlib==1.3.0\r\nrope==0.18.0\r\nrsa==4.6\r\nscikit-learn==0.23.2\r\nscipy==1.4.1\r\nsix==1.15.0\r\nsnowballstemmer==2.0.0\r\nsortedcontainers==2.3.0\r\nSphinx==3.3.1\r\nsphinxcontrib-applehelp==1.0.2\r\nsphinxcontrib-devhelp==1.0.2\r\nsphinxcontrib-htmlhelp==1.0.3\r\nsphinxcontrib-jsmath==1.0.1\r\nsphinxcontrib-qthelp==1.0.3\r\nsphinxcontrib-serializinghtml==1.1.4\r\nspyder==4.2.0\r\nspyder-kernels==1.10.0\r\ntensorboard==2.2.2\r\ntensorboard-plugin-wit==1.7.0\r\ntensorflow-cpu==2.2.0\r\ntensorflow-estimator==2.2.0\r\ntermcolor==1.1.0\r\ntestpath==0.4.4\r\nthreaded==4.1.0\r\nthreadpoolctl==2.1.0\r\nthree-merge==0.1.1\r\ntoml==0.10.2\r\ntornado==6.1\r\ntraitlets==5.0.5\r\ntyped-ast==1.4.1\r\ntyping-extensions==3.7.4.3\r\nujson==4.0.1\r\nurllib3==1.26.2\r\nwatchdog==0.10.4\r\nwcwidth==0.2.5\r\nwebencodings==0.5.1\r\nWerkzeug==1.0.1\r\nwrapt==1.12.1\r\nyapf==0.30.0\r\nzipp==3.4.0\r\n\r\n\r\n**Error**\r\n```\r\nTraceback (most recent call last):\r\n  File \"code3.py\", line 77, in <module>\r\n    from tensorflow.keras.models import load_model\r\n  File \"D:\\fiverr\\traffic\\gui\\venv\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"D:\\fiverr\\traffic\\gui\\venv\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 64, in <module>\r\n    from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin\r\n  File \"D:\\fiverr\\traffic\\gui\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\framework_lib.py\", line 25, in <module>\r\n    from tensorflow.python.framework.ops import Graph\r\n  File \"D:\\fiverr\\traffic\\gui\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 64, in <module>\r\n    from tensorflow.python.platform import app\r\n  File \"D:\\fiverr\\traffic\\gui\\venv\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 23, in <module>\r\n    from absl.app import run as _run\r\n  File \"D:\\fiverr\\traffic\\gui\\venv\\lib\\site-packages\\absl\\app.py\", line 35, in <module>\r\n    import pdb\r\n  File \"C:\\Anaconda3\\lib\\pdb.py\", line 76, in <module>\r\n    import code\r\n  File \"D:\\fiverr\\traffic\\gui\\code.py\", line 107, in <module>\r\n    from tensorflow.keras.models import load_model\r\n  File \"D:\\fiverr\\traffic\\gui\\venv\\lib\\site-packages\\tensorflow\\keras\\__init__.py\", line 14, in <module>\r\n    from . import activations\r\n  File \"D:\\fiverr\\traffic\\gui\\venv\\lib\\site-packages\\tensorflow\\keras\\activations\\__init__.py\", line 10, in <module>\r\n    from tensorflow.python.keras.activations import deserialize\r\n  File \"D:\\fiverr\\traffic\\gui\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\__init__.py\", line 27, in <module>\r\n    from tensorflow.python.keras import models\r\n  File \"D:\\fiverr\\traffic\\gui\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\models.py\", line 23, in <module>\r\n    from tensorflow.python.keras import backend as K\r\n  File \"D:\\fiverr\\traffic\\gui\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 36, in <module>\r\n    from tensorflow.python.client import session as session_module\r\n  File \"D:\\fiverr\\traffic\\gui\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 38, in <module>\r\n    from tensorflow.python.framework import sparse_tensor\r\n  File \"D:\\fiverr\\traffic\\gui\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\sparse_tensor.py\", line 29, in <module>\r\n    from tensorflow.python.framework import constant_op\r\n  File \"D:\\fiverr\\traffic\\gui\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 324, in <module>\r\n    ops.register_tensor_conversion_function(\r\nAttributeError: module 'tensorflow.python.framework.ops' h\r\n```as no attribute 'register_tensor_conversion_function'", "comments": ["@talhaanwarch \r\nCould you please share simple stand alone indented code to replicate the issue or if possible share a colab gist with the error reported.\r\nCan you try on an updated tf version if the issue exists.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45069\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45069\">No</a>\n"]}, {"number": 45068, "title": "2.4.0rc2 RTX3080 cuda 11.1 OP_REQUIRES failed at conv_ops_fused_impl.h:697 : Not found: No algorithm worked!", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): v2.4.0-rc2-0-g0b06f2927b 2.4.0-rc2\r\n- Python version:Python 3.8.5\r\n- Bazel version (if compiling from source): bazel 3.7.0\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\n- CUDA/cuDNN version: 11.1 / 8.0.5\r\n- GPU model and memory: RTX 3080 10016MiB\r\n\r\n**Describe the current behavior**\r\n\r\nThrows an error : \r\nOP_REQUIRES failed at conv_ops_fused_impl.h:697 : Not found: No algorithm worked!\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras import datasets, layers, models\r\nimport matplotlib.pyplot as plt\r\n\r\n(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\r\n\r\n# Normalize pixel values to be between 0 and 1\r\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\r\n\r\nmodel = models.Sequential()\r\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\r\nmodel.add(layers.MaxPooling2D((2, 2)))\r\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(layers.MaxPooling2D((2, 2)))\r\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\n\r\nmodel.add(layers.Flatten())\r\nmodel.add(layers.Dense(64, activation='relu'))\r\nmodel.add(layers.Dense(10))\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\n\r\nhistory = model.fit(train_images, train_labels, epochs=10,\r\n                    validation_data=(test_images, test_labels))\r\n```\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\n/usr/bin/python3.8 /home/noobzik/Documents/deep_learning_day2/hello_conv_nets.py\r\n2020-11-21 23:53:23.135241: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-11-21 23:53:25.040004: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-11-21 23:53:25.040526: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2020-11-21 23:53:25.075627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-21 23:53:25.076103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:08:00.0 name: GeForce RTX 3080 computeCapability: 8.6\r\ncoreClock: 1.71GHz coreCount: 68 deviceMemorySize: 9.78GiB deviceMemoryBandwidth: 707.88GiB/s\r\n2020-11-21 23:53:25.076117: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-11-21 23:53:25.077747: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-11-21 23:53:25.077778: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2020-11-21 23:53:25.078373: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2020-11-21 23:53:25.078508: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2020-11-21 23:53:25.080174: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\r\n2020-11-21 23:53:25.080608: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2020-11-21 23:53:25.080689: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-11-21 23:53:25.080776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-21 23:53:25.081276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-21 23:53:25.081708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-11-21 23:53:25.082535: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-11-21 23:53:25.082603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-21 23:53:25.083044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:08:00.0 name: GeForce RTX 3080 computeCapability: 8.6\r\ncoreClock: 1.71GHz coreCount: 68 deviceMemorySize: 9.78GiB deviceMemoryBandwidth: 707.88GiB/s\r\n2020-11-21 23:53:25.083059: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-11-21 23:53:25.083071: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-11-21 23:53:25.083078: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2020-11-21 23:53:25.083086: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2020-11-21 23:53:25.083093: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2020-11-21 23:53:25.083101: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\r\n2020-11-21 23:53:25.083107: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2020-11-21 23:53:25.083115: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-11-21 23:53:25.083149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-21 23:53:25.083611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-21 23:53:25.084038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-11-21 23:53:25.084060: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-11-21 23:53:25.474941: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-11-21 23:53:25.474969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2020-11-21 23:53:25.474975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2020-11-21 23:53:25.475140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-21 23:53:25.475614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-21 23:53:25.476058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-21 23:53:25.476486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8680 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3080, pci bus id: 0000:08:00.0, compute capability: 8.6)\r\n2020-11-21 23:53:26.015884: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 614400000 exceeds 10% of free system memory.\r\n2020-11-21 23:53:26.199466: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2020-11-21 23:53:26.218389: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3193620000 Hz\r\nEpoch 1/10\r\n2020-11-21 23:53:26.594301: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-11-21 23:53:27.057102: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2020-11-21 23:53:27.073390: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-11-21 23:53:27.767718: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at conv_ops_fused_impl.h:697 : Not found: No algorithm worked!\r\nTraceback (most recent call last):\r\n  File \"/home/noobzik/Documents/deep_learning_day2/hello_conv_nets.py\", line 26, in <module>\r\n    history = model.fit(train_images, train_labels, epochs=10,\r\n  File \"/home/noobzik/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 1100, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n  File \"/home/noobzik/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/noobzik/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 888, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/home/noobzik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2942, in __call__\r\n    return graph_function._call_flat(\r\n  File \"/home/noobzik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1918, in _call_flat\r\n    return self._build_call_outputs(self._inference_function.call(\r\n  File \"/home/noobzik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 555, in call\r\n    outputs = execute.execute(\r\n  File \"/home/noobzik/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.NotFoundError:  No algorithm worked!\r\n\t [[node sequential/conv2d/Relu (defined at /Documents/deep_learning_day2/hello_conv_nets.py:26) ]] [Op:__inference_train_function_800]\r\n\r\nFunction call stack:\r\ntrain_function\r\n\r\n\r\nProcess finished with exit code 1\r\n```", "comments": ["Add this at the start of your code just after importing everything:\r\n```python\r\nphysical_devices = tf.config.list_physical_devices('GPU') \r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n```", "> W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 614400000 exceeds 10% of free system memory.\r\n\r\nYou may try [limiting gpu memory growth](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) as mentioned in the above comment.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45068\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45068\">No</a>\n", "Use gpustat to check  if there's someone using GPU at the same time.\r\nI also met this problem, the reason turned out to be too much memory allocated in GPU. After I killed other process which is allocating memory of GPU, this problem disappered.", "This error seems to be related to some kind of OOM on the GPU. In my case, the error is raised when I'm trying to fit a model that exceeds the memory of my RTX 2080 Super (Driver Version: 450.119.03, CUDA 11.0, cuDNN 8.0.4). It would be nice to output a more concise error when some kind of OOM occurs.", "workaround: I changed my batch size from 100 to 50 :-/"]}, {"number": 45067, "title": "Trying to perform feature engineering and data processing in tf.data pipeline", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nHi Team, \r\nHere, I have taken an example from tensorflow.org. I have built my model as similar as below example, but then as I am dealing with\r\na bit enormous data. I thought I can perform preprocessing and feature engineer task inside tf.data pipeline. Unfortunately, I haven't\r\nfound any examples yet. Hence why, I reached out you, and apologies for disturbing during the weekend. \r\n\r\n\r\n### example model I have built now similar to this ###\r\nimport pathlib\r\nimport numpy as np\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nimport logging\r\ntf.get_logger().setLevel(logging.ERROR)\r\nfrom tensorflow import feature_column\r\nfrom tensorflow.keras import layers\r\nfrom sklearn.model_selection import train_test_split\r\n\r\ndataset_url = 'http://storage.googleapis.com/download.tensorflow.org/data/petfinder-mini.zip'\r\ncsv_file = 'datasets/petfinder-mini/petfinder-mini.csv'\r\n\r\ntf.keras.utils.get_file('petfinder_mini.zip', dataset_url,\r\n                        extract=True, cache_dir='.')\r\ndataframe = pd.read_csv(csv_file)\r\n# In the original dataset \"4\" indicates the pet was not adopted.\r\ndataframe['target'] = np.where(dataframe['AdoptionSpeed']==4, 0, 1)\r\n\r\n# Drop un-used columns.\r\ndataframe = dataframe.drop(columns=['AdoptionSpeed', 'Description'])\r\n\r\ntrain, test = train_test_split(dataframe, test_size=0.2)\r\ntrain, val = train_test_split(train, test_size=0.2)\r\nprint(len(train), 'train examples')\r\nprint(len(val), 'validation examples')\r\nprint(len(test), 'test examples')\r\n\r\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32):\r\n  dataframe = dataframe.copy()\r\n  labels = dataframe.pop('target')\r\n  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\r\n  if shuffle:\r\n    ds = ds.shuffle(buffer_size=len(dataframe))\r\n  ds = ds.batch(batch_size)\r\n  ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\r\n  return ds\r\n\r\nbatch_size = 32\r\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\r\nval_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\r\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\r\n\r\nfeature_columns = []\r\n\r\n# numeric cols\r\nfor header in ['PhotoAmt', 'Fee', 'Age']:\r\n  feature_columns.append(feature_column.numeric_column(header))\r\n# bucketized cols\r\nage = feature_column.numeric_column('Age')\r\nage_buckets = feature_column.bucketized_column(age, boundaries=[1, 2, 3, 4, 5])\r\nfeature_columns.append(age_buckets)\r\n\r\n# indicator_columns\r\nindicator_column_names = ['Type', 'Color1', 'Color2', 'Gender', 'MaturitySize',\r\n                          'FurLength', 'Vaccinated', 'Sterilized', 'Health']\r\nfor col_name in indicator_column_names:\r\n  categorical_column = feature_column.categorical_column_with_vocabulary_list(\r\n      col_name, dataframe[col_name].unique())\r\n  indicator_column = feature_column.indicator_column(categorical_column)\r\n  feature_columns.append(indicator_column)\r\n\r\n# embedding columns\r\nbreed1 = feature_column.categorical_column_with_vocabulary_list(\r\n      'Breed1', dataframe.Breed1.unique())\r\nbreed1_embedding = feature_column.embedding_column(breed1, dimension=8)\r\nfeature_columns.append(breed1_embedding)\r\n\r\nfeature_layer = tf.keras.layers.DenseFeatures(feature_columns)\r\n\r\nmodel = tf.keras.Sequential([\r\n  feature_layer,\r\n  layers.Dense(128, activation='relu'),\r\n  layers.Dense(128, activation='relu'),\r\n  layers.Dropout(.1),\r\n  layers.Dense(1)\r\n])\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(train_ds,\r\n          validation_data=val_ds,\r\n          epochs=10)\r\nThis is my current code. This methods works perfectly.\r\n\r\n**Describe the expected behavior**\r\n\r\nExpected way:\r\n\r\nHere, I am trying to perform feature engineer and data pre-processing inside tf.data pipeline. So I would require your guidance to perform these actions. I have tried scikit learn for feature engineer, but I want to completely perform every single step using TensorFlow 2\r\n\r\n### trying to perform data preprocessing and feature engineering inside tf.data pipeline ###\r\nimport pathlib\r\nimport numpy as np\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nimport logging\r\ntf.get_logger().setLevel(logging.ERROR)\r\nfrom tensorflow import feature_column\r\nfrom tensorflow.keras import layers\r\nfrom sklearn.model_selection import train_test_split\r\n\r\ndataset_url = 'http://storage.googleapis.com/download.tensorflow.org/data/petfinder-mini.zip'\r\ncsv_file = 'datasets/petfinder-mini/petfinder-mini.csv'\r\n\r\ntf.keras.utils.get_file('petfinder_mini.zip', dataset_url,\r\n                        extract=True, cache_dir='.')\r\ndataframe = pd.read_csv(csv_file)\r\n# In the original dataset \"4\" indicates the pet was not adopted.\r\ndataframe['target'] = np.where(dataframe['AdoptionSpeed']==4, 0, 1)\r\n\r\n# Drop un-used columns.\r\ndataframe = dataframe.drop(columns=['AdoptionSpeed', 'Description'])\r\n\r\ntrain, test = train_test_split(dataframe, test_size=0.2)\r\ntrain, val = train_test_split(train, test_size=0.2)\r\nprint(len(train), 'train examples')\r\nprint(len(val), 'validation examples')\r\nprint(len(test), 'test examples')\r\n\r\n### here I have a doubt to performe preprocessing and feature engineer ### not sure how to output the data after these process.\r\ndef preprocessing_and_feat_engg(x):\r\n    feature_columns = []\r\n\r\n    # numeric cols\r\n    for header in ['PhotoAmt', 'Fee', 'Age']:\r\n    feature_columns.append(feature_column.numeric_column(header))\r\n    # bucketized cols\r\n    age = feature_column.numeric_column('Age')\r\n    age_buckets = feature_column.bucketized_column(age, boundaries=[1, 2, 3, 4, 5])\r\n    feature_columns.append(age_buckets)\r\n\r\n    # indicator_columns\r\n    indicator_column_names = ['Type', 'Color1', 'Color2', 'Gender', 'MaturitySize',\r\n                            'FurLength', 'Vaccinated', 'Sterilized', 'Health']\r\n    for col_name in indicator_column_names:\r\n    categorical_column = feature_column.categorical_column_with_vocabulary_list(\r\n        col_name, dataframe[col_name].unique())\r\n    indicator_column = feature_column.indicator_column(categorical_column)\r\n    feature_columns.append(indicator_column)\r\n\r\n    # embedding columns\r\n    breed1 = feature_column.categorical_column_with_vocabulary_list(\r\n        'Breed1', dataframe.Breed1.unique())\r\n    breed1_embedding = feature_column.embedding_column(breed1, dimension=8)\r\n    feature_columns.append(breed1_embedding)\r\n    ### not sure how to return the value from function.\r\n    return x\r\n\r\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32):\r\n  dataframe = dataframe.copy()\r\n  labels = dataframe.pop('target')\r\n  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\r\n  ds = ds.map(preprocessing_and_feat_engg,num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n  if shuffle:\r\n    ds = ds.shuffle(buffer_size=len(dataframe))\r\n  ds = ds.batch(batch_size)\r\n  ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\r\n  return ds\r\n\r\nbatch_size = 32\r\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\r\nval_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\r\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\r\n\r\n\r\nfeature_layer = tf.keras.layers.DenseFeatures(feature_columns)\r\n\r\nmodel = tf.keras.Sequential([\r\n  feature_layer,\r\n  layers.Dense(128, activation='relu'),\r\n  layers.Dense(128, activation='relu'),\r\n  layers.Dropout(.1),\r\n  layers.Dense(1)\r\n])\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(train_ds,\r\n          validation_data=val_ds,\r\n          epochs=10)\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@IamExperimenting,\r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a TensorFlow bug or feature request. There is also a larger community that reads questions there. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45067\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45067\">No</a>\n"]}, {"number": 45066, "title": "Numpy and dataframe converter", "body": "Addresses issue #43487, coded with @nelsonlin2708968\r\n", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45066) for more info**.\n\n<!-- need_author_cla -->", "@nelsonlin2708968 to do cla\r\n", "I fixed it\n\nOn Sat, Nov 21, 2020 at 11:55 AM google-cla[bot] <notifications@github.com>\nwrote:\n\n> We found a Contributor License Agreement for you (the sender of this pull\n> request), but were unable to find agreements for all the commit author(s)\n> or Co-authors. If you authored these, maybe you used a different email\n> address in the git commits than was used to sign the CLA (login here\n> <https://cla.developers.google.com/> to double check)? If these were\n> authored by someone else, then they will need to sign a CLA as well, and\n> confirm that they're okay with these being contributed to Google.\n> In order to pass this check, please resolve this problem and then comment @googlebot\n> I fixed it.. If the bot doesn't comment, it means it doesn't think\n> anything has changed.\n>\n> \u2139\ufe0f *Googlers: Go here\n> <https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45066>\n> for more info*.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/45066#issuecomment-731613071>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AD2U3X45FDJF4BTI2KYHA5LSQ75JRANCNFSM4T55RO3Q>\n> .\n>\n\n\n-- \nNelson Lin\nMS Mechanical Engineering Candidate\nColumbia University in the City of New York\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45066) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it\n\nOn Sat, Nov 21, 2020 at 12:00 PM google-cla[bot] <notifications@github.com>\nwrote:\n\n> We found a Contributor License Agreement for you (the sender of this pull\n> request), but were unable to find agreements for all the commit author(s)\n> or Co-authors. If you authored these, maybe you used a different email\n> address in the git commits than was used to sign the CLA (login here\n> <https://cla.developers.google.com/> to double check)? If these were\n> authored by someone else, then they will need to sign a CLA as well, and\n> confirm that they're okay with these being contributed to Google.\n> In order to pass this check, please resolve this problem and then comment @googlebot\n> I fixed it.. If the bot doesn't comment, it means it doesn't think\n> anything has changed.\n>\n> \u2139\ufe0f *Googlers: Go here\n> <https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45066>\n> for more info*.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/45066#issuecomment-731613837>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AD2U3XYLGLHRCVQWRZZZTZDSQ755NANCNFSM4T55RO3Q>\n> .\n>\n\n\n-- \nNelson Lin\nMS Mechanical Engineering Candidate\nColumbia University in the City of New York\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45066) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45066) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45066) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45066) for more info**.\n\n<!-- need_author_cla -->", "@rd16395p  Can you please sign CLA. Thanks!", "@gbaned It is showing I did when I check the link. ", "@googlebot I fixed it.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45066) for more info**.\n\n<!-- need_author_cla -->", "@gbaned I check and the CLA has also been signed for me using my github email", "@gbaned could it be that the last commit is showing as Nelson Lin and the google bot can't pick it up?", "@rd16395p I think it's because I had left it on the origin branch and not your origin/master. Can we try again?\r\n", "@nelsonlin2708968 I think it's fine. If you look at this commit, it does not show your account, maybe that is it? \r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/45066/commits/ab8e02528146ae68d506d42ab2c26742e08efca7\r\n\r\n@gbaned What do you think?", "@rd16395p, @nelsonlin2708968  I have validated your github id and added cla:yes manually. Thanks!", "@gband Thank you!", "@aaudiber Got it, I will push these changes soon, and mirror what is currently in tensorflow_datasets. Thank you!", "Addressed feedback from @aaudiber , thank you for the suggestions!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45066) for more info**.\n\n<!-- need_author_cla -->", "Can we strip this down to the minimum amount of code needed to implement this feature? A lot of the added files don't seem necessary", "@aaudiber I hate to ask, but could you provide more details? I made an effort to omit testing files, and trace most of the code back to the files that were needed. Thank you for your help. ", "@rd16395p It won't work to copy over entire files because the supported Python versions and coding style used by TFDS is different from TensorFlow. The code in TFDS is useful as inspiration for how to implement `as_dataframe`, but can't be copied directly.", "@aaudiber Thanks for expanding the feedback, I will get working on it soon. ", "@rd16395p  Any update on this PR? Please. Thanks!", "@rd16395p Any update on this PR? Please. Thanks!", "@rd16395p Any update on this PR? Please. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!", "@gbaned Weird, I did not get any notification of your comments, we are working on it at the moment still - can you re open it please?"]}, {"number": 45065, "title": "importing tensorflow", "body": "---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     63   try:\r\n---> 64     from tensorflow.python._pywrap_tensorflow_internal import *\r\n     65   # This try catch logic is because there is no bazel equivalent for py_extension.\r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-2-64156d691fe5> in <module>\r\n----> 1 import tensorflow as tf\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py in <module>\r\n     39 import sys as _sys\r\n     40 \r\n---> 41 from tensorflow.python.tools import module_util as _module_util\r\n     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader\r\n     43 \r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     38 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\r\n     39 \r\n---> 40 from tensorflow.python.eager import context\r\n     41 \r\n     42 # pylint: enable=wildcard-import\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\context.py in <module>\r\n     33 from tensorflow.core.protobuf import config_pb2\r\n     34 from tensorflow.core.protobuf import rewriter_config_pb2\r\n---> 35 from tensorflow.python import pywrap_tfe\r\n     36 from tensorflow.python import tf2\r\n     37 from tensorflow.python.client import pywrap_tf_session\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tfe.py in <module>\r\n     26 \r\n     27 # pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import\r\n---> 28 from tensorflow.python import pywrap_tensorflow\r\n     29 from tensorflow.python._pywrap_tfe import *\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     81 for some common reasons and solutions.  Include the entire stack trace\r\n     82 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 83   raise ImportError(msg)\r\n     84 \r\n     85 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Customer\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: The specified module could not be found.", "comments": ["@bimbolamide this issue seems to have been submitted twice by you, the other being #45064. Could you close the other and continue here?", "Looking at the DLL load fail error, It might help to just do a fresh reinstall of TF @bimbolamide ", "@bimbolamide,\r\nYou might be facing this issue because of the following reasons\r\n\r\n- You are running 32-bit Python or 32-bit OS\r\n- You have not installed the [Microsoft Visual C++ Redistributable](https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads) package\r\n- Your CPU does not support AVX instructions. \r\n\r\nPlease take a look at the [system requirements](https://www.tensorflow.org/install/pip#system-requirements) and check if you have the correct dependencies installed.\r\n\r\nAlso, check these similar duplicate issues: #36167 #36138 and let us know if you are still facing the same error. Thanks!", "Thank you all for the support!\r\n\r\nI have been able to resolve it using those tips. I do not possess the right Microsoft Visual C++ Redistributable to enable a smooth installation.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45065\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45065\">No</a>\n"]}, {"number": 45064, "title": "importing tensorflow", "body": "---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     63   try:\r\n---> 64     from tensorflow.python._pywrap_tensorflow_internal import *\r\n     65   # This try catch logic is because there is no bazel equivalent for py_extension.\r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-2-64156d691fe5> in <module>\r\n----> 1 import tensorflow as tf\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py in <module>\r\n     39 import sys as _sys\r\n     40 \r\n---> 41 from tensorflow.python.tools import module_util as _module_util\r\n     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader\r\n     43 \r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     38 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\r\n     39 \r\n---> 40 from tensorflow.python.eager import context\r\n     41 \r\n     42 # pylint: enable=wildcard-import\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\context.py in <module>\r\n     33 from tensorflow.core.protobuf import config_pb2\r\n     34 from tensorflow.core.protobuf import rewriter_config_pb2\r\n---> 35 from tensorflow.python import pywrap_tfe\r\n     36 from tensorflow.python import tf2\r\n     37 from tensorflow.python.client import pywrap_tf_session\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tfe.py in <module>\r\n     26 \r\n     27 # pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import\r\n---> 28 from tensorflow.python import pywrap_tensorflow\r\n     29 from tensorflow.python._pywrap_tfe import *\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     81 for some common reasons and solutions.  Include the entire stack trace\r\n     82 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 83   raise ImportError(msg)\r\n     84 \r\n     85 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Customer\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: The specified module could not be found.", "comments": ["@bimbolamide \r\n\r\nYou might be facing this issue because of the following reasons\r\n\r\nYou are running 32-bit Python or 32-bit OS\r\nYou have not installed the [Microsoft Visual C++ Redistributable](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads) package\r\nYour CPU does not support AVX instructions.\r\nPlease take a look at the [system requirements](https://www.tensorflow.org/install/pip#system-requirements) and check if you have the correct dependencies installed.\r\n\r\nAlso, check these similar duplicate issues: #36167 #36138 #42367 and let us know if you are still facing the same error. Thanks!", "Thanks Tensorflow/Tensorflow,\n\nI have been able to rectify it using the tips sent.\n\nThe specific Microsoft Visual C++ Redistributable needed for installation\nseems to absent.\n\nThank you for your time.\n\nBest,\nBimbolamide\n\n\n\nOn Mon, Nov 23, 2020 at 9:57 AM Saduf2019 <notifications@github.com> wrote:\n\n> @bimbolamide <https://github.com/bimbolamide>\n>\n> You might be facing this issue because of the following reasons\n>\n> You are running 32-bit Python or 32-bit OS\n> You have not installed the Microsoft Visual C++ Redistributable\n> <https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads>\n> package\n> Your CPU does not support AVX instructions.\n> Please take a look at the system requirements\n> <https://www.tensorflow.org/install/pip#system-requirements> and check if\n> you have the correct dependencies installed.\n>\n> Also, check these similar duplicate issues: #36167\n> <https://github.com/tensorflow/tensorflow/issues/36167> #36138\n> <https://github.com/tensorflow/tensorflow/issues/36138> #42367\n> <https://github.com/tensorflow/tensorflow/issues/42367> and let us know\n> if you are still facing the same error. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/45064#issuecomment-732021695>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AGE44U6ONMZGK3OFUPWMRDTSRIPX7ANCNFSM4T53OEIQ>\n> .\n>\n", "Closing as resolved (via comments from duplicate)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45064\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45064\">No</a>\n"]}, {"number": 45062, "title": "Not working GeForce GTX 1070 on Tensorflow 2.3.1", "body": "System characteristics:\r\n\r\n```\r\nUbuntu 20.04\r\nNVIDIA Corporation GP104BM [GeForce GTX 1070 Mobile]\r\nIntel\u00ae Core\u2122 i7-7700HQ CPU @ 2.80GHz \u00d7 8\r\nRAM: 31,3 GiB\r\nCuda Driver Version: 450.80.02 \r\ntensorflow-2.3.1\r\nCUDA Version: 11.0\r\ncudnn-11.0\r\n```\r\n\r\nuser@Linux:~$ nvidia-smi\r\n\r\n```\r\nSat Nov 21 15:52:38 2020       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1070    Off  | 00000000:01:00.0  On |                  N/A |\r\n| N/A   50C    P8     9W /  N/A |    297MiB /  8111MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A       932      G   /usr/lib/xorg/Xorg                 92MiB |\r\n|    0   N/A  N/A      1295      G   /usr/bin/gnome-shell              111MiB |\r\n|    0   N/A  N/A      3277      G   /usr/lib/firefox/firefox            1MiB |\r\n|    0   N/A  N/A      3828      C   /usr/bin/python3                   87MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n```\r\nuser@Linux:~$ nvcc -V\r\n\r\n```\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2019 NVIDIA Corporation\r\nBuilt on Sun_Jul_28_19:07:16_PDT_2019\r\nCuda compilation tools, release 10.1, V10.1.243\r\n```\r\n\r\nfrom tensorflow.python.client import device_lib print(device_lib.list_local_devices())\r\n\r\n\r\n```\r\n[name: \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 11676041203334616666\r\n, name: \"/device:XLA_CPU:0\"\r\ndevice_type: \"XLA_CPU\"\r\nmemory_limit: 17179869184\r\nlocality {\r\n}\r\nincarnation: 9716226955421748203\r\nphysical_device_desc: \"device: XLA_CPU device\"\r\n, name: \"/device:XLA_GPU:0\"\r\ndevice_type: \"XLA_GPU\"\r\nmemory_limit: 17179869184\r\nlocality {\r\n}\r\nincarnation: 1948208650029266498\r\nphysical_device_desc: \"device: XLA_GPU device\"\r\n]\r\n\r\n```\r\nWhen I run the code, I get an error:\r\n\r\n```\r\nwith tf.device('/device:XLA_GPU:0'):\r\n    history=model.fit(train_generator,\r\n                      epochs=10,\r\n                      validation_data=valid_generator,\r\n                      validation_steps=test.shape[0]//batch_size,\r\n                      steps_per_epoch=train.shape[0]//batch_size\r\n                        )\r\n\r\nEpoch 1/10\r\n\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-15-cc03ad3613b3> in <module>\r\n      1 with tf.device('/device:XLA_GPU:0'):\r\n----> 2     history=model.fit(train_generator,\r\n      3                       epochs=10,\r\n      4                       validation_data=valid_generator,\r\n      5                       validation_steps=test.shape[0]//batch_size,\r\n...\r\n\r\nEpoch 1/10\r\n\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-15-cc03ad3613b3> in <module>\r\n      1 with tf.device('/device:XLA_GPU:0'):\r\n----> 2     history=model.fit(train_generator,\r\n      3                       epochs=10,\r\n      4                       validation_data=valid_generator,\r\n      5                       validation_steps=test.shape[0]//batch_size,\r\n```\r\n\r\nwithout code: with tf.device('/device:XLA_GPU:0') everything works well, but for a very long time (no GPU work...)\r\n", "comments": ["IIRC, Tensorflow v2.3.1 only supports the CUDA Toolkit 10.1, not 11.\r\nIt is also possible that your tensorflow version wasn't built for CUDA compute capability 6.1 (i.e. your GTX 1070).\r\nI had the same problem and had to build from source to enable compute capability 6.1 via the configuration to support my GTX 1070.\r\n\r\nYou can test whether your GPU is available:\r\n```python\r\nimport tensorflow as tf\r\n\r\ngpu_devices = tf.config.list_physical_devices('GPU')\r\nif len(gpu_devices) > 0:\r\n  print(f'{len(gpu_devices)} GPU(s) found and ready for use\ud83d\udc4d')\r\nelse:\r\n  print('No GPU support\u2639')\r\n```\r\n\r\nEdit: In case you need to build from source, too, here's a list with [CUDA compute capability by GPU](https://developer.nvidia.com/cuda-gpus) (click `GeForce and TITAN Products` to find your GPU)", "@MrKribel,\r\nEvery TensorFlow release is compatible with certain CUDA and cuDNN version. TensorFlow v2.3 is compatible with CUDA 10.1 and cuDNN 7.6. For more information, please take a look at the [tested build configurations](https://www.tensorflow.org/install/source#gpu).\r\n\r\nCould you please install CUDA 10.1 and cuDNN 7.6, and let us know if you are facing the same issue. Thanks!\r\n\r\n", "> @MrKribel,\r\n> Every TensorFlow release is compatible with certain CUDA and cuDNN version. TensorFlow v2.3 is compatible with CUDA 10.1 and cuDNN 7.6. For more information, please take a look at the [tested build configurations](https://www.tensorflow.org/install/source#gpu).\r\n> \r\n> Could you please install CUDA 10.1 and cuDNN 7.6, and let us know if you are facing the same issue. Thanks!\r\n\r\nThank you very much for your answer, it gave me hope...I was installing from these links: https://medium.com/@cwbernards/tensorflow-2-3-on-ubuntu-20-04-lts-with-cuda-11-0-and-cudnn-8-0-fb136a829e7f and https://gist.github.com/kmhofmann/e368a2ebba05f807fa1a90b3bf9a1e03 but nothing worked. \r\nafter 20 minutes when i start compiling tensorflow it gives an error asking to downgrade the compiler version to 8. after changing the compiler version to 8.5, it compiles for a long time, and then dumps the error stack onto the libraries.\r\nI used the following versions: \r\n1) TF 2.3.0\r\n2) cuda 10.1\r\n3) cudnn 7.6.5 for 10.01\r\n4) gcc-9/gcc-8/gcc-7 \r\n\r\n:'(\r\n\r\n\r\n", "@MrKribel,\r\nCould you please uninstall all the packages and check if a fresh installation works. \r\n\r\nAlso, please follow [this official guide](https://www.tensorflow.org/install/gpu#ubuntu_1804_cuda_101) while installing TensorFlow. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45062\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45062\">No</a>\n"]}, {"number": 45061, "title": "Updated XNNPACK delegate readme", "body": "Added some clarification on XNNPACK delegate to avoid confusion cause by XNNPACK engine being single-threaded by default.\r\nFurther details are available in the description of #42277 issue.", "comments": []}, {"number": 45060, "title": "Add unit tests to losses utils and ensure RaggedTensor squeeze works.", "body": "While working on PR #45015 I bumped into issues with the function remove_squeezable_dimensions. This PR adds unit test coverage for this function and ensures that squeeze works as expected with RaggedTensors.", "comments": ["@gbaned @edloper Please advise as to whether you believe this additional unit test is useful or whether I should abandon the PR.", "Window CI build failure:\r\n```\r\n.\\tensorflow/compiler/mlir/lite/transforms/prepare_quantize_helper.h(555): error C7555: use of designated initializers requires at least '/std:c++latest'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n\r\nThis is unrelated to my commit. I've rebased the PR branch from master. @gbaned or @edloper Can you please re-trigger the CI build ? thanks.", "Windows CI errors:\r\n```\r\nFAIL: //py_test_dir/tensorflow/python/util:tf_stack_test (see T:/tmp/bigvaudl/execroot/org_tensorflow/bazel-out/x64_windows-opt/testlogs/py_test_dir/tensorflow/python/util/tf_stack_test/test_attempts/attempt_1.log)\r\nFAIL: //py_test_dir/tensorflow/python/util:tf_stack_test (see T:/tmp/bigvaudl/execroot/org_tensorflow/bazel-out/x64_windows-opt/testlogs/py_test_dir/tensorflow/python/util/tf_stack_test/test_attempts/attempt_2.log)\r\nFAIL: //py_test_dir/tensorflow/python/util:tf_stack_test (see T:/tmp/bigvaudl/execroot/org_tensorflow/bazel-out/x64_windows-opt/testlogs/py_test_dir/tensorflow/python/util/tf_stack_test/test.log)\r\n\r\nFAILED: //py_test_dir/tensorflow/python/util:tf_stack_test (Summary)\r\n      T:/tmp/bigvaudl/execroot/org_tensorflow/bazel-out/x64_windows-opt/testlogs/py_test_dir/tensorflow/python/util/tf_stack_test/test.log\r\n      T:/tmp/bigvaudl/execroot/org_tensorflow/bazel-out/x64_windows-opt/testlogs/py_test_dir/tensorflow/python/util/tf_stack_test/test_attempts/attempt_1.log\r\n      T:/tmp/bigvaudl/execroot/org_tensorflow/bazel-out/x64_windows-opt/testlogs/py_test_dir/tensorflow/python/util/tf_stack_test/test_attempts/attempt_2.log\r\nINFO: From Testing //py_test_dir/tensorflow/python/util:tf_stack_test:\r\n==================== Test output for //py_test_dir/tensorflow/python/util:tf_stack_test:\r\nRunning tests under Python 3.7.4: C:\\Python37\\python.exe\r\n[ RUN      ] TFStackTest.testFormatStackSelfConsistency\r\nINFO:tensorflow:time(__main__.TFStackTest.testFormatStackSelfConsistency): 0.01s\r\nI0115 17:41:43.657681 20736 test_util.py:2071] time(__main__.TFStackTest.testFormatStackSelfConsistency): 0.01s\r\n[       OK ] TFStackTest.testFormatStackSelfConsistency\r\n[ RUN      ] TFStackTest.testFrameSummaryEquality\r\nINFO:tensorflow:time(__main__.TFStackTest.testFrameSummaryEquality): 0.0s\r\nI0115 17:41:43.657681 20736 test_util.py:2071] time(__main__.TFStackTest.testFrameSummaryEquality): 0.0s\r\n[       OK ] TFStackTest.testFrameSummaryEquality\r\n[ RUN      ] TFStackTest.testFrameSummaryEqualityAndHash\r\nINFO:tensorflow:time(__main__.TFStackTest.testFrameSummaryEqualityAndHash): 0.0s\r\nI0115 17:41:43.659635 20736 test_util.py:2071] time(__main__.TFStackTest.testFrameSummaryEqualityAndHash): 0.0s\r\n[       OK ] TFStackTest.testFrameSummaryEqualityAndHash\r\n[ RUN      ] TFStackTest.testLastUserFrame\r\nINFO:tensorflow:time(__main__.TFStackTest.testLastUserFrame): 0.0s\r\nI0115 17:41:43.660615 20736 test_util.py:2071] time(__main__.TFStackTest.testLastUserFrame): 0.0s\r\n[  FAILED  ] TFStackTest.testLastUserFrame\r\n[ RUN      ] TFStackTest.test_session\r\n[  SKIPPED ] TFStackTest.test_session\r\n======================================================================\r\nFAIL: testLastUserFrame (__main__.TFStackTest)\r\nTFStackTest.testLastUserFrame\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\T:\\tmp\\Bazel.runfiles_s9zl8mw2\\runfiles\\org_tensorflow\\py_test_dir\\tensorflow\\python\\util\\tf_stack_test.py\", line 57, in testLastUserFrame\r\n    self.assertRegex(frame.line, \"# COMMENT\")\r\nAssertionError: Regex didn't match: '# COMMENT' not found in '_source_filter_stacks[thread_key][-1].internal_set)'\r\n```\r\n\r\nThis is unrelated to my CL. Looking at the code, it seems that commit 73a6839fb3b61bd307630b612261292bc562f527 touched this area recently. It actually dropped windows file patterns from consideration.\r\n```\"tensorflow\\\\python\"``` is no longer considered as a pattern for IsInternalFrameForFilename. It is unclear to me whether this change is correct.\r\nThis was subsequently addressed in commit 569a996c095980c42fe6f43f5e483a848a549609. Unfortunately the CI run was done with the tree synched to an intermediate point between this CLs. I've resynched to master. @edloper: Can you please trigger the CI again ?\r\n", "@tomerk The CI errors are unrelated to the PR. Previously, the issue typically requires me to sync my tree and request a new CI run which typically takes 1 week. By the time the CI is re-run it tends to fail again. Is there any mechanism by which I can trigger the CI runs ? If not, can someone in the google team take over and apply the patch internally ? The patch isn't particularly high value for me... I was just hoping to avoid that the loss utils with break again unintentionally."]}, {"number": 45059, "title": "build tensorflow_cc failed with cycle dependency graph", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.3.1\r\n\r\n- Python version: 3.7.7\r\n- bazelisk version: 1.7.4\r\n- Bazel version (if compiling from source): 3.1.0\r\n- CUDA/cuDNN version: 10.1/7.6.5\r\n- GPU model and memory: rtx 2080ti\r\n\r\n\r\n\r\n**Describe the problem**\r\nin order to build tensorflow_cc.dll with tensorrt support on win10,  I add **--config=tensorrt** option on build command,  it failed with cycle dependency graph.\r\ndid I miss any environment variables? or is TF-TRT currently not supported by windows?\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nno **--define=with_xla_support=false**:  \r\n```\r\nbazel build --config=opt --config=v2 --config=tensorrt //tensorflow:tensorflow_cc //tensorflow:tensorflow_cc_dll_import_lib //tensorflow:install_headers\r\n```\r\n\r\nwith  **--define=with_xla_support=false**:  \r\n```\r\nbazel build --config=opt --config=v2 --config=tensorrt --define=with_xla_support=false //tensorflow:tensorflow_cc //tensorflow:tensorflow_cc_dll_import_lib //tensorflow:install_headers\r\n```\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nno **--define=with_xla_support=false**:  \r\n\r\n```log\r\nD:\\Project\\cpp\\tf\\tensorflow-2.3.1>bazel build --config=opt --config=v2 --config=tensorrt //tensorflow:tensorflow_cc //tensorflow:tensorflow_cc_dll_import_lib //tensorflow:install_headers\r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [v2]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=125\r\nINFO: Reading rc options for 'build' from d:\\project\\cpp\\tf\\tensorflow-2.3.1\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/ihual/AppData/Local/Microsoft/WindowsApps/python.exe\r\nINFO: Reading rc options for 'build' from d:\\project\\cpp\\tf\\tensorflow-2.3.1\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from d:\\project\\cpp\\tf\\tensorflow-2.3.1\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=D:/SoftInstall/Anaconda3/envs/tf2_p37/python.exe --action_env PYTHON_LIB_PATH=D:/SoftInstall/Anaconda3/envs/tf2_p37/lib/site-packages --python_path=D:/SoftInstall/Anaconda3/envs/tf2_p37/python.exe --config=xla --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=7.5 --config=cuda --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file d:\\project\\cpp\\tf\\tensorflow-2.3.1\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file d:\\project\\cpp\\tf\\tensorflow-2.3.1\\.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:cuda in file d:\\project\\cpp\\tf\\tensorflow-2.3.1\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file d:\\project\\cpp\\tf\\tensorflow-2.3.1\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nINFO: Found applicable config definition build:opt in file d:\\project\\cpp\\tf\\tensorflow-2.3.1\\.tf_configure.bazelrc: --copt=/arch:AVX2 --define with_default_optimizations=true\r\nINFO: Found applicable config definition build:v2 in file d:\\project\\cpp\\tf\\tensorflow-2.3.1\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:tensorrt in file d:\\project\\cpp\\tf\\tensorflow-2.3.1\\.bazelrc: --action_env TF_NEED_TENSORRT=1\r\nINFO: Found applicable config definition build:windows in file d:\\project\\cpp\\tf\\tensorflow-2.3.1\\.bazelrc: --copt=/w --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file d:\\project\\cpp\\tf\\tensorflow-2.3.1\\.bazelrc: --define framework_shared_object=false\r\nWARNING: D:/project/cpp/tf/tensorflow-2.3.1/tensorflow/core/BUILD:1840:1: in linkstatic attribute of cc_library rule //tensorflow/core:lib_internal: setting 'linkstatic=1' is recommended if there are no object files. Since this rule was created by the macro 'cc_library', the error might have been caused by the macro implementation\r\nWARNING: D:/project/cpp/tf/tensorflow-2.3.1/tensorflow/core/BUILD:2252:1: in linkstatic attribute of cc_library rule //tensorflow/core:framework_internal: setting 'linkstatic=1' is recommended if there are no object files. Since this rule was created by the macro 'tf_cuda_library', the error might have been caused by the macro implementation\r\nERROR: D:/project/cpp/tf/tensorflow-2.3.1/tensorflow/core/BUILD:1110:1: in cc_library rule //tensorflow/core:all_kernels: cycle in dependency graph:\r\n    //tensorflow:tensorflow_cc\r\n    //tensorflow:tensorflow_cc.dll\r\n    //tensorflow/cc/profiler:profiler\r\n    //tensorflow/core/profiler/internal:tfprof_stats\r\n    //tensorflow/core/profiler/internal:tfprof_code\r\n    //tensorflow/c:c_api\r\n    //tensorflow/compiler/jit:jit\r\n    //tensorflow/compiler/jit:xla_cpu_jit\r\n    //tensorflow/compiler/jit:xla_kernel_creator\r\n    //tensorflow/compiler/jit:jit_compilation_passes\r\n    //tensorflow/compiler/jit:compilation_passes\r\n.-> //tensorflow/core:all_kernels\r\n|   //tensorflow/core:all_kernels_impl\r\n|   //tensorflow/compiler/tf2tensorrt:trt_engine_resource_op_kernels\r\n|   //tensorflow/python:pywrap_tensorflow_import_lib\r\n|   //tensorflow/python:pywrap_tensorflow_import_lib_file\r\n|   //tensorflow/python:get_pywrap_tensorflow_import_lib_file\r\n|   //tensorflow/python:_pywrap_tensorflow_internal.so\r\n|   //tensorflow/core/distributed_runtime/rpc:grpc_server_lib\r\n|   //tensorflow/core/distributed_runtime:local_master\r\n|   //tensorflow/core/distributed_runtime:master\r\n|   //tensorflow/core/distributed_runtime:master_session\r\n|   //tensorflow/core/distributed_runtime:scheduler\r\n|   //tensorflow/core:tensorflow_opensource\r\n`-- //tensorflow/core:all_kernels\r\nThis cycle occurred because of a configuration option\r\nERROR: Analysis of target '//tensorflow:tensorflow_cc' failed; build aborted\r\nINFO: Elapsed time: 8.539s\r\nINFO: 0 processes.\r\n```\r\n\r\n\r\nwith  **--define=with_xla_support=false**:  \r\n\r\n```log\r\nD:\\Project\\cpp\\tf\\tensorflow-2.3.1>bazel build --config=opt --config=v2 --config=tensorrt --define=with_xla_support=false //tensorflow:tensorflow_cc //tensorflow:tensorflow_cc_dll_import_lib //tensorflow:install_headers\r\nWARNING: The following configs were expanded more than once: [v2]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=125\r\nINFO: Reading rc options for 'build' from d:\\project\\cpp\\tf\\tensorflow-2.3.1\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/ihual/AppData/Local/Microsoft/WindowsApps/python.exe\r\nINFO: Reading rc options for 'build' from d:\\project\\cpp\\tf\\tensorflow-2.3.1\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from d:\\project\\cpp\\tf\\tensorflow-2.3.1\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=D:/SoftInstall/Anaconda3/envs/tf2_p37/python.exe --action_env PYTHON_LIB_PATH=D:/SoftInstall/Anaconda3/envs/tf2_p37/lib/site-packages --python_path=D:/SoftInstall/Anaconda3/envs/tf2_p37/python.exe --config=xla --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=7.5 --config=cuda --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file d:\\project\\cpp\\tf\\tensorflow-2.3.1\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file d:\\project\\cpp\\tf\\tensorflow-2.3.1\\.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:cuda in file d:\\project\\cpp\\tf\\tensorflow-2.3.1\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file d:\\project\\cpp\\tf\\tensorflow-2.3.1\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nINFO: Found applicable config definition build:opt in file d:\\project\\cpp\\tf\\tensorflow-2.3.1\\.tf_configure.bazelrc: --copt=/arch:AVX2 --define with_default_optimizations=true\r\nINFO: Found applicable config definition build:v2 in file d:\\project\\cpp\\tf\\tensorflow-2.3.1\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:tensorrt in file d:\\project\\cpp\\tf\\tensorflow-2.3.1\\.bazelrc: --action_env TF_NEED_TENSORRT=1\r\nINFO: Found applicable config definition build:windows in file d:\\project\\cpp\\tf\\tensorflow-2.3.1\\.bazelrc: --copt=/w --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file d:\\project\\cpp\\tf\\tensorflow-2.3.1\\.bazelrc: --define framework_shared_object=false\r\nINFO: Build option --define has changed, discarding analysis cache.\r\nWARNING: D:/project/cpp/tf/tensorflow-2.3.1/tensorflow/core/BUILD:1840:1: in linkstatic attribute of cc_library rule //tensorflow/core:lib_internal: setting 'linkstatic=1' is recommended if there are no object files. Since this rule was created by the macro 'cc_library', the error might have been caused by the macro implementation\r\nWARNING: D:/project/cpp/tf/tensorflow-2.3.1/tensorflow/core/BUILD:2252:1: in linkstatic attribute of cc_library rule //tensorflow/core:framework_internal: setting 'linkstatic=1' is recommended if there are no object files. Since this rule was created by the macro 'tf_cuda_library', the error might have been caused by the macro implementation\r\nERROR: D:/project/cpp/tf/tensorflow-2.3.1/tensorflow/compiler/tf2tensorrt/BUILD:91:1: in cc_library rule //tensorflow/compiler/tf2tensorrt:trt_op_kernels: cycle in dependency graph:\r\n    //tensorflow:tensorflow_cc\r\n    //tensorflow:tensorflow_cc.dll\r\n    //tensorflow/core:tensorflow\r\n    //tensorflow/core:tensorflow_opensource_wrap_all_kernels\r\n.-> //tensorflow/compiler/tf2tensorrt:trt_op_kernels\r\n|   //tensorflow/compiler/tf2tensorrt:trt_conversion\r\n|   //tensorflow/python:pywrap_tensorflow_import_lib\r\n|   //tensorflow/python:pywrap_tensorflow_import_lib_file\r\n|   //tensorflow/python:get_pywrap_tensorflow_import_lib_file\r\n|   //tensorflow/python:_pywrap_tensorflow_internal.so\r\n|   //tensorflow/python:safe_ptr\r\n|   //tensorflow/c/eager:c_api\r\n|   //tensorflow/core/distributed_runtime/rpc:grpc_server_lib\r\n|   //tensorflow/core/distributed_runtime:master_session\r\n|   //tensorflow/core/distributed_runtime:scheduler\r\n|   //tensorflow/core:tensorflow_opensource\r\n|   //tensorflow/core:all_kernels\r\n|   //tensorflow/core:all_kernels_impl\r\n`-- //tensorflow/compiler/tf2tensorrt:trt_op_kernels\r\nThis cycle occurred because of a configuration option\r\nERROR: Analysis of target '//tensorflow:tensorflow_cc' failed; build aborted\r\nINFO: Elapsed time: 2.397s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 19577 targets configured)\r\n```\r\n", "comments": ["**bazel build --config=opt --config=v2 //tensorflow:tensorflow_cc //tensorflow:tensorflow_cc_dll_import_lib //tensorflow:install_headers** woks fun for me\uff0c it seems that the circular dependency is caused by adding **--config=tensorrt**.\r\n\r\nbuild successed on ubuntu 18.04\r\n\r\nalso tried on 2.4.0-rc2 on windows, the same is a circular dependency, but the type of error is different\r\n\r\n```\r\n D:\\Project\\cpp\\tf\\tensorflow-2.4.0-rc2>bazel build --config=opt --config=v2 --config=tensorrt //tensorflow:tensorflow_cc //tensorflow:tensorflow_cc_dll_import_lib //tensorflow:install_headers\r\nWARNING: The following configs were expanded more than once: [v2]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=125\r\nINFO: Reading rc options for 'build' from d:\\project\\cpp\\tf\\tensorflow-2.4.0-rc2\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=D:/SoftInstall/Anaconda3/envs/tf2_p37/python.exe\r\nINFO: Reading rc options for 'build' from d:\\project\\cpp\\tf\\tensorflow-2.4.0-rc2\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from d:\\project\\cpp\\tf\\tensorflow-2.4.0-rc2\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=D:/SoftInstall/Anaconda3/envs/tf2_p37/python.exe --action_env PYTHON_LIB_PATH=D:/SoftInstall/Anaconda3/envs/tf2_p37/lib/site-packages --python_path=D:/SoftInstall/Anaconda3/envs/tf2_p37/python.exe --config=xla --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=7.5 --config=cuda --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file d:\\project\\cpp\\tf\\tensorflow-2.4.0-rc2\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file d:\\project\\cpp\\tf\\tensorflow-2.4.0-rc2\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file d:\\project\\cpp\\tf\\tensorflow-2.4.0-rc2\\.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:cuda in file d:\\project\\cpp\\tf\\tensorflow-2.4.0-rc2\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file d:\\project\\cpp\\tf\\tensorflow-2.4.0-rc2\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\nINFO: Found applicable config definition build:opt in file d:\\project\\cpp\\tf\\tensorflow-2.4.0-rc2\\.tf_configure.bazelrc: --copt=/arch:AVX2 --define with_default_optimizations=true\r\nINFO: Found applicable config definition build:v2 in file d:\\project\\cpp\\tf\\tensorflow-2.4.0-rc2\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:tensorrt in file d:\\project\\cpp\\tf\\tensorflow-2.4.0-rc2\\.bazelrc: --action_env TF_NEED_TENSORRT=1\r\nINFO: Found applicable config definition build:windows in file d:\\project\\cpp\\tf\\tensorflow-2.4.0-rc2\\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file d:\\project\\cpp\\tf\\tensorflow-2.4.0-rc2\\.bazelrc: --define framework_shared_object=false\r\nDEBUG: Rule 'io_bazel_rules_go' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1557349968 -0400\"\r\nDEBUG: Repository io_bazel_rules_go instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule git_repository defined at:\r\n  C:/users/ihual/_bazel_ihual/leceuitt/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>\r\nERROR: D:/project/cpp/tf/tensorflow-2.4.0-rc2/tensorflow/c/eager/BUILD:34:1: in cc_library rule //tensorflow/c/eager:c_api: cycle in dependency graph:\r\n    //tensorflow:tensorflow_cc\r\n    //tensorflow:tensorflow_cc.dll\r\n.-> //tensorflow/c/eager:c_api\r\n|   //tensorflow/core/distributed_runtime/rpc:grpc_server_lib\r\n|   //tensorflow/core/distributed_runtime/rpc:grpc_master_service\r\n|   //tensorflow/core/distributed_runtime:master\r\n|   //tensorflow/core/distributed_runtime:master_session\r\n|   //tensorflow/core/distributed_runtime:scheduler\r\n|   //tensorflow/core:tensorflow_opensource\r\n|   //tensorflow/core:all_kernels\r\n|   //tensorflow/core:all_kernels_impl\r\n|   //tensorflow/compiler/tf2tensorrt:trt_op_kernels\r\n|   //tensorflow/python:pywrap_tensorflow_import_lib\r\n|   //tensorflow/python:pywrap_tensorflow_import_lib_file\r\n|   //tensorflow/python:get_pywrap_tensorflow_import_lib_file\r\n|   //tensorflow/python:_pywrap_tensorflow_internal.so\r\n|   //tensorflow/compiler/aot:tfcompile_lib\r\n|   //tensorflow/compiler/tf2xla:mlir_tf2xla\r\n|   //tensorflow/compiler/mlir/tensorflow:compile_mlir_util\r\n|   //tensorflow/compiler/mlir/tensorflow:tf_dialect_passes\r\n`-- //tensorflow/c/eager:c_api\r\nThis cycle occurred because of a configuration option\r\nERROR: Analysis of target '//tensorflow:tensorflow_cc' failed; build aborted\r\nINFO: Elapsed time: 1.719s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)\r\n```", "@ihuale Could you please try to use latest **TF v2.6.0** and let us know if it helps?Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45059\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45059\">No</a>\n", "@sushreebarsa ok, I will try it later"]}, {"number": 45058, "title": "Training model with VGGloss  on TPU gives tcmalloc warning and training time significantly increases.", "body": "I am using colab tpu with tensorflow version 2.3.0\r\n![image](https://user-images.githubusercontent.com/18486587/99874097-02e8b680-2c0b-11eb-9f2b-f04f4799c798.png)\r\n\r\nCode for VGG loss is:\r\n\r\n```\r\ndef vgg_layers(layer_names):\r\n\tvgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\r\n\toutputs = [vgg.get_layer(name).output for name in layer_names]\r\n\tmodel = tf.keras.Model([vgg.input], outputs)\r\n\treturn model\r\n\r\nclass VGGloss(Model):\r\n\tdef __init__(self):\r\n\t\tsuper(VGGloss, self).__init__()\r\n\t\tlayers = [f'block{i+1}_conv1' for i in range(5)]\r\n\t\tself.layerweights = [1./32, 1./16, 1./8, 1./4, 1.]\r\n\t\tself.vgg = vgg_layers(layers)\r\n\t\tself.vgg.trainable = False\r\n\t\tself.l1_loss = tf.keras.losses.MeanAbsoluteError('auto')\r\n\r\n\tdef call(self, x, y):\r\n\t\tx_vgg, y_vgg = self.vgg(x), self.vgg(y), \r\n\t\tloss = 0\r\n\t\tfor w, xi, yi in zip(self.layerweights, x_vgg, y_vgg):\r\n\t\t\tloss += w*self.l1_loss(xi, yi)\r\n\t\treturn loss\r\n```\r\n", "comments": ["@coreqode \r\n\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 45057, "title": "Improve grappler layout optimizer to support 5D tensors [Resubmit]", "body": "This is for resubmitting #44270.\r\n\r\n@nluehr @andyly ", "comments": []}, {"number": 45056, "title": " Custom loop training of tacotron2 with MultiWorkerMirroredStrategy - Exceptions and failures when use MultiWorkerMirroredStrategy -", "body": "When I use tf.distribute.experimental.MultiWorkerMirroredStrategy to run training on multiple machines (custom loop) I face following errors. Please advise when other necessary changes are needed. I am trying to run the training for tacotron2 (https://github.com/TensorSpeech/TensorFlowTTS/blob/master/examples/tacotron2/train_tacotron2.py).\r\n\r\nErrors and exceptions are:\r\n\r\n2020-11-16 12:03:50,968 (cross_device_ops:1130) INFO: Collective batch_all_reduce for IndexedSlices: 1 all-reduces, group_size = 2\r\n2020-11-16 12:03:56.443402: W tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:439] error: Internal: Complete shape not known for AdamWeightDecay/allreduce/CollectiveReduce_23\r\n2020-11-16 12:03:56.443474: W tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:1121] error: Internal: Complete shape not known for AdamWeightDecay/allreduce/CollectiveReduce_23\r\n2020-11-16 12:03:56.443606: E tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:1138] ScopedAllocatorOptimizer: Internal: Complete shape not known for AdamWeightDecay/allreduce/CollectiveReduce_23", "comments": ["@ma-siddiqui,\r\nOn running the Python script you have linked, I am facing an error stating `ModuleNotFoundError: No module named 'examples.tacotron2'\r\n`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/85225c4d31a51589c68711d13a45a20f/45056.ipynb). \r\n\r\nIn order to expedite the trouble-shooting process, please provide a minimal code snippet to reproduce the issue reported here and the TensorFlow version you are using. Thanks!\r\n", "Also, please take a look at issue [#40277](https://github.com/tensorflow/tensorflow/issues/40277#issuecomment-657824045) which has a similar error log and let us know if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hi, any update?", "> On running the Python script you have linked, I am facing an error stating `ModuleNotFoundError: No module named 'examples.tacotron2' `. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/85225c4d31a51589c68711d13a45a20f/45056.ipynb).\r\n> \r\n> In order to expedite the trouble-shooting process, please provide a minimal code snippet to reproduce the issue reported here and the TensorFlow version you are using. Thanks!\r\n\r\n@ma-siddiqui,\r\nAs mentioned earlier, I am facing an error while running the code. Could you please share the Python script/notebook you are running, so that we can reproduce the issue on our end. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hi,\r\n\r\nI am using TensorflowTTS code.\r\n\r\n\r\nThanks,", "> I am using TensorflowTTS code.\r\n\r\n@ma-siddiqui,\r\nCould you please share the Python script/notebook you are running, so that we can reproduce the issue on our end? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45056\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45056\">No</a>\n"]}, {"number": 45055, "title": " W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution Windows 10 x64\r\n- TensorFlow installed using pip\r\n- TensorFlow version 2.5.0-dev20201028\r\n- Python version: Python  3.8.6 (tags/v3.8.6:db45529, Sep 23 2020, 15:52:53) [MSC v.1927 64 bit (AMD64)] on win32\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: cuda 11.1/cudnn 80.04\r\n- GPU model and memory: NVIDIA RTX3080\r\n\r\n\r\nHi,\r\nI got this issues when I am trying to import tensorflow from  command line.\r\nCurrently I am using NVIDIA RTX3080, with cuda 11.1, cudnn 8.04, python 64 bit, and nvidia driver 456.43. \r\nI follow the instruction on https://www.tensorflow.org/install, and install tf-nightly 2.5.0.dev and tf-nightly-gpu 2.5.0.dev. \r\nI also have try the solution on this discussion but it didn't work me https://github.com/tensorflow/tensorflow/issues/43193. \r\nBut I am still getting this error\r\n![error](https://user-images.githubusercontent.com/12694360/99868899-bea4e880-2c01-11eb-90d9-c8d1e80638f6.PNG)\r\n\r\n![program_file](https://user-images.githubusercontent.com/12694360/99868905-d1b7b880-2c01-11eb-86c5-4ff7affa7886.PNG)\r\n![path](https://user-images.githubusercontent.com/12694360/99868808-d465de00-2c00-11eb-82f8-4a1532bf2618.PNG)\r\n![nvcc](https://user-images.githubusercontent.com/12694360/99868809-d7f96500-2c00-11eb-91db-1af79086e982.PNG)\r\n![python](https://user-images.githubusercontent.com/12694360/99868829-0e36e480-2c01-11eb-81b1-8635d52a9790.PNG)\r\n", "comments": ["I think You should check if you have added all the cuda enviroment path variable. Type this in you command prompt \r\n`PATH`", "@marshallia \r\nplease refer to these issues with same error and let us know: [link](https://stackoverflow.com/questions/59823283/could-not-load-dynamic-library-cudart64-101-dll-on-tensorflow-cpu-only-install), [link1](https://github.com/tensorflow/tensorflow/issues/45072#issuecomment-731783830), #43193\r\n\r\nHave you set in PATH\r\nSET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11\\bin;%PATH%\r\n", "> @marshallia\r\n> please refer to these issues with same error and let us know: [link](https://stackoverflow.com/questions/59823283/could-not-load-dynamic-library-cudart64-101-dll-on-tensorflow-cpu-only-install), [link1](https://github.com/tensorflow/tensorflow/issues/45072#issuecomment-731783830), #43193\r\n> \r\n> Have you set in PATH\r\n> SET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11\\bin;%PATH%\r\n\r\nHi, Saduf. \r\nI have give up on installing tensorflow on windows. Instead of using Windows I use Ubuntu and install cuda 11.1 package that contains nvidia driver inside it. Then install tensorflow nightly. And it is work for me\r\n\r\nCuda 11.1\r\nNvidia driver 455.32\r\nTensorflow 2.5.0-dev\r\npython 3.8.5\r\nUbuntu 20.04", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45055\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45055\">No</a>\n", "I have the same problem and the solution is:\r\n\r\n1) Uninstall all drivers that says \"nvidia\" from Uninstall programs (ALL, Cuda and gpu driver)\r\n2) Install last nvidia drivers for your GPU https://uk.download.nvidia.com/GFE/GFEClient/3.21.0.36/GeForce_Experience_v3.21.0.36.exe\r\n3) Install cuda 11.2 https://developer.download.nvidia.com/compute/cuda/11.2.1/local_installers/cuda_11.2.1_461.09_win10.exe\r\n2) Install Anaconda https://repo.anaconda.com/archive/Anaconda3-2020.11-Windows-x86_64.exe\r\n3) conda create -n cudaenv python=3.6\r\n4) conda activate cudaenv\r\n5) pip install tensorflow python-dotenv\r\n6) Create a.py with this: import tensorflow as tf\r\n7) run it: python a.py \r\nOutput:\r\nxxx: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll", "@CostanzoPablo, thank you very much for the above post.  I finally got tf 2.4.1 to install in a conda-forge environment with GPU support (CUDA 11.3, Python 3.8.8)!  Where did you find out that \"python-dotenv\" should be installed?  I haven't seen that package mentioned anywhere on tf website, or anywhere else, with regard to tf install.  \r\n\r\nHowever, the above post leaves out many important steps that are required to get tf working with GPU support.", "@CostanzoPablo  That worked for me! Thanks for posting this solution. "]}, {"number": 45054, "title": "Concatenating sparse keras input layers results in None shape, preventing use in Dense layer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Colab\r\n- TensorFlow version (use command below): 2.3\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): None\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n**Describe the current behavior**\r\n\r\nUsing the Keras functional API, when concatenating together multiple instances of `tensorflow.keras.layers.Input` with `sparse=True`, the resulting `SparseTensor` has a `None` shape (even when `batch_size` is specified). Feeding the merged layer with shape `None` into a dense layer results in an error:\r\n```\r\nValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nConcatenating *non-sparse* instances of `tensorflow.keras.layers.Input` (i.e. `sparse=False`) results in a tensor which has a defined shape (if `batch_size` is specified) and can be fed into a dense layer successfully. Additionally a single sparse input layer has a shape and can be fed into a dense layer. I would expect concatenated sparse input layers to act similarly.\r\n\r\nThe dense shape of the merged sparse tensor should already be known as we set the shapes of the individual input layers.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nsp_a = tf.keras.layers.Input(shape=(1,), batch_size=2, sparse=True)\r\nsp_b = tf.keras.layers.Input(shape=(2,), batch_size=2, sparse=True)\r\n\r\nsp_merged = tf.keras.layers.concatenate(inputs=[sp_a, sp_b], axis=1)\r\n\r\nprint(sp_merged.shape) # prints \"(None, None)\"\r\n\r\nsp_result = tf.keras.layers.Dense(4, activation='relu', name='dense1')(sp_merged) # fails with above ValueError\r\n```\r\n\r\nAlso tried concatenating with `tf.keras.layers.Concatenate` and `tf.sparse.concat` instead. Same result.\r\n\r\nReproduced success (non-sparse input) and failure (sparse input) cases in Colab:\r\nhttps://colab.research.google.com/drive/1g9a9lzpw28NqIPI1Z3-fYLJke1fa10Af?usp=sharing\r\n", "comments": ["I have tried in colab with TF version 2.3, nightly version(`2.5.0-dev20201122`) and was able to reproduce the issue.Please, find the gist [here.](https://colab.research.google.com/gist/ravikyram/d735c0ed788454efba238a30af594edb/untitled532.ipynb) Thanks!", "Thanks for the report; looks like a bug.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45054\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45054\">No</a>\n"]}, {"number": 45053, "title": "Add support for ragged tensors to tf.keras.layers.experimental.preprocessing.Resizing", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.4.0rc2\r\n- Are you willing to contribute it (Yes/No): Yes, though I am not sure of the best way to implement this\r\n\r\n**Describe the feature and the current behavior/state.** tf.keras includes [experimental layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing) for bringing preprocessing operations, such as resizing, inside the model. Users may want to use this functionality in place of resizing in a `tf.data` pipeline, enabling them to bring in a batch of multiple differently sized images and make them a uniform size. However, `tf.keras.layers.experimental.preprocessing.Resizing` does not currently support RaggedTensors, and as such all inputs within a batch must be the same dimensions.\r\n\r\n\r\nAttempting to pass in a ragged tensor (as shown below) gives an error. Adding support for ragged tensors would be helpful as it would allow for a batch to contain images of different sizes, thus reducing the need for external preprocessing code.\r\n\r\n**Will this change the current api? How?**\r\nYes, API changes will be required as one or more functions will need to be updated with support for ragged tensors.\r\n\r\nThe API for `tf.keras.layers.experimental.preprocessing.Resizing` would change, though all existing code using it should continue to work.\r\n\r\nAdditionally, if the feature is implemented by updating `tf.image.resize` to accept `images` as a ragged tensor instead of only a 4-D Tensor of shape [batch, height, width, channels], this will result in an API change, though it shouldn't be breaking.\r\n\r\n**Who will benefit with this feature?**\r\nUsers looking to move more of their preprocessing inside their model that want to resize multiple differently sized images in one batch.\r\n\r\n**Any Other info.**\r\n\r\nIdeal flow (currently failing):\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\na = np.random.rand(512, 512, 3)\r\nb = np.random.rand(1080, 1920, 3)\r\n\r\nvalues = tf.ragged.stack([a, b])\r\nlabels = np.asarray([0, 1])\r\n\r\nprint(values.shape)\r\n# NHWC format\r\n# TensorShape([2, None, None, 3])\r\n\r\nprint(labels.shape)\r\n# (2,)\r\n\r\n# Highly simplified image model\r\nmodel = tf.keras.models.Sequential()\r\n# The output of this layer will always be (Batch, 224, 224, 3), so adding support for\r\n# ragged tensors shouldn't require updates for downstream ops\r\nmodel.add(\r\n  tf.keras.layers.experimental.preprocessing.Resizing(\r\n    224,\r\n    224,\r\n    input_shape=(None, None, 3), \r\n    name=\"resize\"))\r\nmodel.add(tf.keras.layers.Conv2D(kernel_size=3, filters=24, name=\"kernel\"))\r\nmodel.add(tf.keras.layers.GlobalMaxPool2D(name=\"pool\"))\r\nmodel.add(tf.keras.layers.Dense(1, name=\"dense_second\"))\r\n\r\nmodel.compile()\r\n\r\nmodel.fit(values, labels)\r\n```\r\n\r\nError message available in [this gist](https://gist.github.com/TylerADavis/ab693b3d10d618612186f9553c919fbc).\r\n", "comments": ["Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/27257bb7af0d50b9696000cf0228c5df/45053.ipynb). Thanks!", "cc @ymodak ", "@TylerADavis Looks like this was resolved in recent `tf-nightly`. [Here](https://colab.research.google.com/gist/jvishnuvardhan/7009ba0e1039aa3d3a36dbf6134de279/untitled1133.ipynb) is a gist for reference.\r\n\r\nCan you please verify once and close the issue if this was resolved for you.\r\n\r\nIf this was not resolved, can you please open the issue in  [keras-team/keras repo](https://github.com/keras-team/keras/issues) repo as keras development moved to that repo to focus mainly on Keras. Thanks!\r\n\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Thanks! I'll take a look ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Looks like it works, thanks @jvishnuvardhan !"]}, {"number": 45052, "title": "ifdef out known failing test cases for xtensa for svdf.", "body": "Manually confirmed that the following command passes:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifimini XTENSA_CORE=mini1m1m_RG test_kernel_svdf_test\r\n```\r\n\r\nhttp://b/170332589\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "tagging @pnikam-cad @nyadla-sys @kpraving"]}, {"number": 45051, "title": "ifdef out the known failing test cases for FullyConnected for hifimini.", "body": "Manually confirmed that the following command passes:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifimini XTENSA_CORE=mini1m1m_RG test_kernel_fully_connected_test\r\n```\r\n\r\nhttp://b/170503075\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "tagging @pnikam-cad @nyadla-sys @kpraving"]}, {"number": 45050, "title": "ifdef out known failing test cases for xtensa for depthwise_conv", "body": "Manually confirmed that the following command passes:\n```\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifimini XTENSA_CORE=mini1m1m_RG test_kernel_depthwise_conv_test\n```\n\nhttp://b/170322965\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "tagging @pnikam-cad @nyadla-sys @kpraving"]}, {"number": 45049, "title": "docs(tflite): remove '--linkopt' flag", "body": "In the comments above these two TFLite GPU delegate build configurations, the suggested command threw an error for me on both OSX and Ubuntu with various gcc/clang versions.\r\n\r\nSee issue https://github.com/tensorflow/tensorflow/issues/44926", "comments": []}, {"number": 45048, "title": "ifdef out known failing test cases for Conv2D for hifimini.", "body": "With this change it should become clearer what test cases currently pass for the optimized xtensa conv kernel.\r\n\r\nManually confirmed that the following command passes:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifimini XTENSA_CORE=mini1m1m_RG test_kernel_conv_test\r\n```\r\n\r\nAddresses http://b/170321206\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "tagging @pnikam-cad @nyadla-sys @kpraving"]}, {"number": 45047, "title": "Use batch_size without modulo", "body": "Please check #44933", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45047) for more info**.\n\n<!-- need_author_consent -->", "@aaudiber Mhh.. we are going to need to diverge some tests like this between eager and graph mode. \r\nSo how much extra work do you think is needed to expand this patch over graph mode to maintain the tests unified?\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/063940bae365534a74d27421c7ef37736042cb3a/tensorflow/python/data/experimental/kernel_tests/rebatch_dataset_test.py#L184-L191", "I found a small free time slot for an initial refactoring to try to cover graph mode. Not all tests are passing.", "I've tried a quick hack with `tf.keras.backend.eval` for the failing distribution tests but what we could use for a for a replica safe evaluation?", "> @aaudiber Mhh.. we are going to need to diverge some tests like this between eager and graph mode.\r\n> So how much extra work do you think is needed to expand this patch over graph mode to maintain the tests unified?\r\n> \r\n> https://github.com/tensorflow/tensorflow/blob/063940bae365534a74d27421c7ef37736042cb3a/tensorflow/python/data/experimental/kernel_tests/rebatch_dataset_test.py#L184-L191\r\n\r\nIt would be a lot of work since all cardinality logic is currently in C++. Given that we would need to diverge a lot of tests between eager and graph mode, I think it's better to wait until the logic is migrated to Python. ", "I have no more special testing needs and I suppose it could work in graph mode.\r\n\r\nThe issue now is how to safely evaluate `evenly_divisible`.\r\n", "@aaudiber Can you re-check now? I've not diverged the test between edge and graph mode.\r\nNow is rollback on an early formulation when the only failing test is `testMultipleBatches` in graph_mode. ", "The single test is failing cause if we extract a simple example:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import tensor_util\r\nfrom tensorflow.python.client import session as session_lib\r\n\r\nds = tf.data.Dataset.range(16)\r\nprint(\"Card eager. \", ds.cardinality().numpy())\r\ntf.compat.v1.disable_eager_execution()\r\nds = tf.data.Dataset.range(16)\r\nprint(\"Card const: \", tensor_util.constant_value(ds.cardinality()))\r\nwith session_lib.Session() as session:\r\n    print(\"Card eval: \", ds.cardinality().eval())\r\n```\r\n\r\n```\r\nCard eager.  16\r\nCard const:  None\r\nCard eval:  16\r\n```\r\n\r\nIt cannot extract the value from `constant_value`. How can I safely evaluate?", "@bhack this looks like a fundamental issue in graph mode. We can't get the result of `cardinality` because it requires evaluating the cardinality op, which can't be done eagerly when in graph mode. The only way I can think of is to implement cardinality for each type of dataset as a Python method, so that the method can produce the right cardinality in both graph mode and eager mode. ", "> @bhack this looks like a fundamental issue in graph mode. We can't get the result of `cardinality` because it requires evaluating the cardinality op, which can't be done eagerly when in graph mode. The only way I can think of is to implement cardinality for each type of dataset as a Python method, so that the method can produce the right cardinality in both graph mode and eager mode.\r\n\r\nWhy it  happens only for a single test? All the other tests are passing. Is it only a problem for `RangeDataset`?", "@aaudiber I made just an extra commit to show that all the test are passing.\r\n\r\nIs it possible that the problem of cardinality occurs in a single test?\r\nIs it a coincidence?\r\n", "I've fixed also some GPU and distributed test It was a little bit slow cause currently I don't have a spare GPU for executing this tests so I needed to use the public CI. .\r\nNow all the tests are passing also on GPU.\r\n\r\nI'am only diverging a single `def` test for legacy rebatch for graph mode.\r\n\r\nIt will need a refactor for sure but lets give a quick review pass.", "I've reverted the last refactoring commit https://github.com/tensorflow/tensorflow/pull/45047/commits/f8466c83fa6f5c77fc9203b7dbd4042f1461d08e cause it was creating some strange error on function wrapping in edge mode. Tell me if you have any feedback as the old build is still available", "Let me know.", "/cc @jsimsa if interested"]}, {"number": 45046, "title": "add BiasAddGradient", "body": "@saxenasaurabh \r\n\r\nThis draft PR shows you how I want to split the test. In summary, we have some several files for testing right now. \r\n- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/eager/gradients_test.cc ( for Math gradients )\r\n- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/eager/mnist_gradients_test.cc\r\n- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/eager/gradient_checker_test.cc ( for NN gradients )\r\n\r\nAnd utils:\r\n- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/eager/gradients_util.h\r\n- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/eager/unified_api_testutil.h\r\n- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/eager/mnist_gradients_testutil.h\r\n\r\nSince the gradients are classed by `math` and `nn`, but the tests aren't, it causes some overlap. Like `AddGradModel` is written twice.\r\nhttps://github.com/tensorflow/tensorflow/blob/6c2616c1ea7d580546d9ac818a70e6edf40c56f4/tensorflow/c/eager/mnist_gradients_testutil.h#L40\r\nhttps://github.com/tensorflow/tensorflow/blob/6c2616c1ea7d580546d9ac818a70e6edf40c56f4/tensorflow/c/eager/gradients_test.cc#L77\r\n\r\n\r\nFurthermore, `gradients_util.h` and `unified_api_testutil.h` has some overlaps too\r\nhttps://github.com/tensorflow/tensorflow/blob/6c2616c1ea7d580546d9ac818a70e6edf40c56f4/tensorflow/c/eager/gradients_util.h#L85\r\nhttps://github.com/tensorflow/tensorflow/blob/6c2616c1ea7d580546d9ac818a70e6edf40c56f4/tensorflow/c/eager/unified_api_testutil.h#L49\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/6c2616c1ea7d580546d9ac818a70e6edf40c56f4/tensorflow/c/eager/gradients_util.h#L43-L46\r\nhttps://github.com/tensorflow/tensorflow/blob/6c2616c1ea7d580546d9ac818a70e6edf40c56f4/tensorflow/c/eager/unified_api_testutil.h#L56-L58\r\n\r\n---\r\n\r\nI propose that we should:\r\n- `*GradModel` should be in `math_grad_utils.h` and `nn_grad_utils.h`\r\n- `Test*Grad` should be in `math_grad_test.h` and `nn_grad_test.h`\r\n- Merge `gradients_util.h` and `unified_api_testutil.h`\r\n- Have an unified process for testing the gradient, because currently, some gradients are tested with `gradient_checker` and some are tested by writing the result manually.\r\n\r\nPart of #42668", "comments": ["@saxenasaurabh \r\nCould you take a look at this PR ? Thank you !", "In e5cf403, I add \r\n- `grad_test_helper` which is a helper function to automatically test the model against `gradient_checker`. \r\n\r\nOne thing I don't understand, why `gradient_checker` modifies the `inputs`. I think it should not touch the `inputs` at all. I added a TODO for revisiting this point. We could merge this PR as-is, and I will send some follow-up PRs for clean up the test.\r\n\r\nIn d369986, I add\r\n- `TF_MODEL_FACTORY` and `TF_GRAD_MODEL_FACTORY` will expand to models that are needed for testing. ( more detail in the comment ).\r\n- Move `RegisterGradients` and `BuildImmediateExecutionContext` to `Setup`.", "@saxenasaurabh \r\nCould you take a look at this PR ? Thank you!", "> One thing I don't understand, why `gradient_checker` modifies the `inputs`.\r\n\r\nFixed in 68b0bd3", "One more thing, should we move gradient-related files into `tensorflow/c/experimental/gradients` ?", "FYI, The tests failed mainly because of `llvm`.", "> FYI, The tests failed mainly because of `llvm`.\r\n\r\nYep llvm broke windows. A fix has been submitted I think. \r\nThere is a pending change to fix the macos build as well.\r\n", "> Yep llvm broke windows. A fix has been submitted I think.\r\nThere is a pending change to fix the macos build as well.\r\n\r\nI think we could rebase master if you need.", "Looks like we need to rebase.", "> Looks like we need to rebase.\r\n\r\nDone", "The internal checks failed again \ud83d\ude22 ", "> The internal checks failed again \ud83d\ude22\r\n\r\nLooks like a memory leak in nn_grad_test. Are you able to run your test with `-fsanitize=address`? I will patch your change and run internally as well.", "Everything seems good this time \ud83c\udf89 ", "> Everything seems good this time \ud83c\udf89\r\n\r\nLet's maybe start tracking what kind of things pass in opensource but fail internally. Ideally there should be no difference.", "> Let's maybe start tracking what kind of things pass in opensource but fail internally. Ideally there should be no difference.\r\n\r\nI got this problem a lot. I think mainly because the internal CI is somewhat stricter. Tracking is a good idea but I don't have any experience. Do you have any suggestions ?"]}]