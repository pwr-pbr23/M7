[{"number": 45842, "title": "\"InternalError: cudaGetDevice() failed. Status: initialization error\"  while using Sequential() and other", "body": "**System information**\r\n- I wrote a very simple code.\r\n- OS Platform and Distribution: ManjaroLinux 20.2\r\n- CPU: ryzen7 3750H \r\n- TensorFlow installed from (source or binary): pip install tensorflow-gpu\r\n- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n- Python version: 3.8.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA : V11.1.105 / cuDNN : cudnn-8.0.5.39-1   \r\n- GPU model and memory: GTX 1660ti Max-Q 6 GB\r\n\r\n```\r\n\r\n== check python ===================================================\r\npython version: 3.8.6\r\npython branch: \r\npython build version: ('default', 'Sep 30 2020 04:00:38')\r\npython compiler version: GCC 10.2.0\r\npython implementation: CPython\r\n\r\n\r\n== check os platform ===============================================\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (GCC) 10.2.0\r\nCopyright (C) 2020 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== check pips ===================================================\r\nnumpy                        1.19.2\r\nprotobuf                     3.14.0\r\ntensorflow-estimator         2.4.0\r\ntensorflow-gpu               2.4.0\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH :/opt/cuda/lib64\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nFri Dec 18 17:15:55 2020       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.100      Driver Version: 440.100      CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 166...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n| N/A   56C    P8    10W /  N/A |     27MiB /  5944MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0       936      G   /usr/lib/Xorg                                 14MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n\r\n== tensorflow installed from info ==================\r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(3, 8, 6, 'final', 0)\r\n\r\n== bazel version  ===============================================\r\n```\r\n\" tensorflow import\" part is too long so I uploaded here [https://justpaste.it/9olm4](https://justpaste.it/9olm4)\r\n\r\n\r\nThis is my code\r\n```\r\nimport tensorflow as tf\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense\r\nfrom keras.layers import LSTM\r\nfrom keras.layers import Dropout\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nprint(gpus)\r\nif gpus:\r\n    try:\r\n        for gpu in gpus:\r\n            tf.config.experimental.set_memory_growth(gpu, True)\r\n    except RuntimeError as e:\r\n        print(e)\r\nmodel = Sequential()\r\n```\r\nOutput\r\n```\r\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\n\r\n---------------------------------------------------------------------------\r\nInternalError                             Traceback (most recent call last)\r\n<ipython-input-3-65dd765202ab> in <module>\r\n     12     except RuntimeError as e:\r\n     13         print(e)\r\n---> 14 model = Sequential()\r\n\r\n~/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    515     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    516     try:\r\n--> 517       result = method(self, *args, **kwargs)\r\n    518     finally:\r\n    519       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py in __init__(self, layers, name)\r\n    115     \"\"\"\r\n    116     # Skip the init in FunctionalModel since model doesn't have input/output yet\r\n--> 117     super(functional.Functional, self).__init__(  # pylint: disable=bad-super-call\r\n    118         name=name, autocast=False)\r\n    119     base_layer.keras_api_gauge.get_cell('Sequential').set(True)\r\n\r\n~/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    515     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    516     try:\r\n--> 517       result = method(self, *args, **kwargs)\r\n    518     finally:\r\n    519       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in __init__(self, *args, **kwargs)\r\n    291     self._steps_per_execution = None\r\n    292 \r\n--> 293     self._init_batch_counters()\r\n    294     self._base_model_initialized = True\r\n    295 \r\n\r\n~/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    515     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    516     try:\r\n--> 517       result = method(self, *args, **kwargs)\r\n    518     finally:\r\n    519       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in _init_batch_counters(self)\r\n    299     # `evaluate`, and `predict`.\r\n    300     agg = variables.VariableAggregationV2.ONLY_FIRST_REPLICA\r\n--> 301     self._train_counter = variables.Variable(0, dtype='int64', aggregation=agg)\r\n    302     self._test_counter = variables.Variable(0, dtype='int64', aggregation=agg)\r\n    303     self._predict_counter = variables.Variable(\r\n\r\n~/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)\r\n    260       return cls._variable_v1_call(*args, **kwargs)\r\n    261     elif cls is Variable:\r\n--> 262       return cls._variable_v2_call(*args, **kwargs)\r\n    263     else:\r\n    264       return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n\r\n~/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py in _variable_v2_call(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)\r\n    242     if aggregation is None:\r\n    243       aggregation = VariableAggregation.NONE\r\n--> 244     return previous_getter(\r\n    245         initial_value=initial_value,\r\n    246         trainable=trainable,\r\n\r\n~/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py in <lambda>(**kws)\r\n    235                         shape=None):\r\n    236     \"\"\"Call on Variable class. Useful to force the signature.\"\"\"\r\n--> 237     previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\r\n    238     for _, getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access\r\n    239       previous_getter = _make_getter(getter, previous_getter)\r\n\r\n~/.local/lib/python3.8/site-packages/tensorflow/python/ops/variable_scope.py in default_variable_creator_v2(next_creator, **kwargs)\r\n   2652   shape = kwargs.get(\"shape\", None)\r\n   2653 \r\n-> 2654   return resource_variable_ops.ResourceVariable(\r\n   2655       initial_value=initial_value,\r\n   2656       trainable=trainable,\r\n\r\n~/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)\r\n    262       return cls._variable_v2_call(*args, **kwargs)\r\n    263     else:\r\n--> 264       return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n    265 \r\n    266 \r\n\r\n~/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\r\n   1572       self._init_from_proto(variable_def, import_scope=import_scope)\r\n   1573     else:\r\n-> 1574       self._init_from_args(\r\n   1575           initial_value=initial_value,\r\n   1576           trainable=trainable,\r\n\r\n~/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\r\n   1715               self._update_uid = initial_value.checkpoint_position.restore_uid\r\n   1716               initial_value = initial_value.wrapped_value\r\n-> 1717             initial_value = ops.convert_to_tensor(initial_value,\r\n   1718                                                   name=\"initial_value\",\r\n   1719                                                   dtype=dtype)\r\n\r\n~/.local/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py in wrapped(*args, **kwargs)\r\n    161         with Trace(trace_name, **trace_kwargs):\r\n    162           return func(*args, **kwargs)\r\n--> 163       return func(*args, **kwargs)\r\n    164 \r\n    165     return wrapped\r\n\r\n~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\r\n   1538 \r\n   1539     if ret is None:\r\n-> 1540       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1541 \r\n   1542     if ret is NotImplemented:\r\n\r\n~/.local/lib/python3.8/site-packages/tensorflow/python/framework/tensor_conversion_registry.py in _default_conversion_function(***failed resolving arguments***)\r\n     50 def _default_conversion_function(value, dtype, name, as_ref):\r\n     51   del as_ref  # Unused.\r\n---> 52   return constant_op.constant(value, dtype, name=name)\r\n     53 \r\n     54 \r\n\r\n~/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)\r\n    262     ValueError: if called on a symbolic tensor.\r\n    263   \"\"\"\r\n--> 264   return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n    265                         allow_broadcast=True)\r\n    266 \r\n\r\n~/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\r\n    274       with trace.Trace(\"tf.constant\"):\r\n    275         return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n--> 276     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n    277 \r\n    278   g = ops.get_default_graph()\r\n\r\n~/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py in _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n    299 def _constant_eager_impl(ctx, value, dtype, shape, verify_shape):\r\n    300   \"\"\"Implementation of eager constant.\"\"\"\r\n--> 301   t = convert_to_eager_tensor(value, ctx, dtype)\r\n    302   if shape is None:\r\n    303     return t\r\n\r\n~/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)\r\n     95     except AttributeError:\r\n     96       dtype = dtypes.as_dtype(dtype).as_datatype_enum\r\n---> 97   ctx.ensure_initialized()\r\n     98   return ops.EagerTensor(value, ctx.device_name, dtype)\r\n     99 \r\n\r\n~/.local/lib/python3.8/site-packages/tensorflow/python/eager/context.py in ensure_initialized(self)\r\n    524         if self._use_tfrt is not None:\r\n    525           pywrap_tfe.TFE_ContextOptionsSetTfrt(opts, self._use_tfrt)\r\n--> 526         context_handle = pywrap_tfe.TFE_NewContext(opts)\r\n    527       finally:\r\n    528         pywrap_tfe.TFE_DeleteContextOptions(opts)\r\n\r\nInternalError: cudaGetDevice() failed. Status: initialization error\r\n```\r\n\r\nJupyter Terminal Output\r\n```\r\n2020-12-18 17:52:13.476509: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-12-18 17:52:13.476810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-12-18 17:52:13.478279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1660 Ti with Max-Q Design computeCapability: 7.5\r\ncoreClock: 1.335GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s\r\n2020-12-18 17:52:13.478341: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-12-18 17:52:13.478429: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-12-18 17:52:13.478465: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2020-12-18 17:52:13.478496: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2020-12-18 17:52:13.478525: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2020-12-18 17:52:13.478553: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2020-12-18 17:52:13.478582: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2020-12-18 17:52:13.478610: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-12-18 17:52:13.478836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-12-18 17:52:13.480513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-12-18 17:52:13.481882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n\r\n```", "comments": ["@chalermporn17personal,\r\nTensorFlow v2.4 is built and tested against CUDA 11 (not 11.1) and cuDNN 8. For more information please take a look at the [tested build configurations](https://www.tensorflow.org/install/source#gpu). \r\n\r\nCould you please install CUDA 11 with cuDNN 8 and check if you are facing the same issue. Thanks!", "> @chalermporn17personal,\r\n> TensorFlow v2.4 is built and tested against CUDA 11 (not 11.1) and cuDNN 8. For more information please take a look at the [tested build configurations](https://www.tensorflow.org/install/source#gpu).\r\n> \r\n> Could you please install CUDA 11 with cuDNN 8 and check if you are facing the same issue. Thanks!\r\n\r\nI downgrade to CUDA 11 and cuDNN 8 and it still not work for me.\r\nAfter that I see in nvidia-smi that it support only CUDA 10.2\r\nSo i install CUDA 10.1 and cuDNN 7 with tensorflow2.3\r\nIt work very well now.\r\n\r\nThank  you very much for answer me.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45842\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45842\">No</a>\n"]}, {"number": 45841, "title": "fix softnms bug when set iou threshold", "body": "cc @mingxingtan ", "comments": ["I have verified this PR passed all internal tests."]}, {"number": 45840, "title": "Fix: tf.contrib.distribute.CollectiveAllReduceStrategy can't load model from checkpoint #45839", "body": "About [issue 45839](https://github.com/tensorflow/tensorflow/issues/45839)\r\n\r\nI analyzed and debug the source code and found the following problems:\r\n\r\n1.  The value of `worker_context.should_checkpoint` will change the basic path of checkpoint. But the restore action is related to\r\n\r\n the base path of the checkpoint.\r\n```\r\nvenv/lib/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py line: 341\r\ndef _create_monitored_session_with_worker_context(...)\r\n  ...\r\n  if (((save_checkpoint_secs and save_checkpoint_secs > 0) or\r\n       (save_checkpoint_steps and save_checkpoint_steps > 0)) and\r\n      checkpoint_dir):\r\n    if worker_context.should_checkpoint:\r\n      all_hooks.append(\r\n          basic_session_run_hooks.CheckpointSaverHook(\r\n              checkpoint_dir,\r\n              save_steps=save_checkpoint_steps,\r\n              save_secs=save_checkpoint_secs,\r\n              scaffold=scaffold))\r\n    elif tmpdir:\r\n      all_hooks.append(\r\n          basic_session_run_hooks.CheckpointSaverHook(\r\n              os.path.join(checkpoint_dir, tmpdir),\r\n              save_steps=save_checkpoint_steps,\r\n              save_secs=save_checkpoint_secs,\r\n              scaffold=scaffold))\r\n  ...\r\n```\r\n\r\n2. The value of `worker_context.should_checkpoint` depends on the _is_chief attribute of Class `Collective AllReduce Extended`\r\n```\r\nvenv/lib/python3.6/sitepackages/tensorflow_core/python/distribute/collective_all_reduce_strategy.py line 231\r\n\r\ndef _initialize_multi_worker(self, cluster_resolver):\r\n    \"\"\"Initializes the object for multi-worker training.\"\"\"\r\n    ....\r\n    self._is_chief = multi_worker_util.is_chief(\r\n        cluster_spec, task_type,task_id)\r\n```\r\n3. `multi_worker_util.py is_chief()` is the key to the problem \r\n```\r\n/venv/lib/python3.6/site-packages/tensorflow_core/python/distribute/multi_worker_util.py\r\nline 93: is_chief\r\ndef is_chief(cluster_spec=None, task_type=None, task_id=None):\r\n  ...\r\n  if task_type == \"chief\" or task_type == \"evaluator\":\r\n    return True\r\n\r\n  if (\"chief\" not in cluster_spec and task_type == \"worker\" and task_id == 0):\r\n    return True\r\n  return False\r\n  ...\r\n```\r\n\r\nMy sulotion as follow:\r\n```\r\ndef is_chief_for_collective_all_reduce_strategy(cluster_spec=None, task_type=None, task_id=None):\r\n  # Fix the bug that the model fails to load from checkpoint when using collective_all_reduce_strategy\r\n  # if task_type == \"chief\" or task_type == \"worker\" or task_type == \"evaluator\":\r\n  if task_type == \"chief\" or task_type == \"evaluator\":\r\n    return True\r\n  return False\r\n```\r\n\r\nI will use this solution in kubeflow in production environment.  Is there any risk in this?\r\n\r\n\r\nHope the PR can pass, if any questions, please give me some advice.\r\n\r\nThanks a lot!!!", "comments": ["This should be opened against master branch?", "@mihaimaruseac \r\n> This should be opened against master branch?\r\n\r\nYes. I read the code of master branch and this problem also exists. \r\n\r\nI ran the code on Tensorflow v2.3 and encountered the same problem.\r\n\r\nShould I need to resubmit a new PR to the master branch?", "Let's try to rebase this against master branch or close this and open one on the master branch.", "Thanks! I have resubmitted a PR against the master branch on [#45839](https://github.com/tensorflow/tensorflow/pull/45898), may I close this PR?"]}, {"number": 45839, "title": "tf.contrib.distribute.CollectiveAllReduceStrategy can't load model from checkpoint", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.15.4\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nI am working on distributed learning in tensorflow through estimators API using below simple code template:\r\n```\r\ndef main(argv):\r\n\r\n    # Init, set model dir| export dir | log dir.\r\n    model_dir, export_dir = init()\r\n    # Loading dataset\r\n    download_dataset()\r\n\r\n    # Select distribute strategy, such as sync, async, allReduce, etc.\r\n    # CollectiveAllReduceStrategy for allReduce\r\n    dist_strategy = tf.contrib.distribute.CollectiveAllReduceStrategy(num_gpus_per_worker=FLAGS.num_gpus_per_worker)\r\n    # Set run config, including checkpoint saving strategy, maximum number of checkpoints saved, etc.\r\n    run_config = tf.estimator.RunConfig(train_distribute=dist_strategy,\r\n                                        eval_distribute=dist_strategy,\r\n                                        # save_checkpoints_secs=10,\r\n                                        save_checkpoints_steps=FLAGS.save_checkpoints_steps,\r\n                                        keep_checkpoint_max=FLAGS.keep_checkpoint_max)\r\n\r\n    # Feature columns describe how to use the input.\r\n    my_feature_columns = get_feature_columns()\r\n\r\n    # Model\r\n    # Build 2 hidden layer DNN with 10, 10 units respectively.\r\n    classifier = Net(model_dir, my_feature_columns, run_config).net\r\n\r\n    # TrainSpec for training\r\n    train_spec = tf.estimator.TrainSpec(\r\n        input_fn=lambda: csv_input_fn(TRAIN_PATH, FLAGS.batch_size, True),\r\n        max_steps=FLAGS.train_steps,\r\n        hooks=[])\r\n    # EvalSpec for test\r\n    eval_spec = tf.estimator.EvalSpec(\r\n        input_fn=lambda: csv_input_fn(TEST_PATH, FLAGS.batch_size, True))\r\n    print(\"---training and testing---\")\r\n    tf.estimator.train_and_evaluate(classifier, train_spec, eval_spec)\r\n    print(\"---training finished---\")\r\n\r\n    # All role are workers, pick the task_index with 0 to save model\r\n    if FLAGS.task_index == 0:\r\n        classifier.export_saved_model(export_dir, serving_input_receiver_fn)\r\n        # classifier.export_savedmodel(export_dir, serving_input_receiver_fn, strip_default_attrs=True)\r\n    print(\"finish...\")\r\n```\r\nFirstly, I train for 1000 rounds(train_stpes=1000) and save the checkpoint, it works normally.\r\n\r\nThen I set the train_steps to 2000, only the is_chief role can restore the model from the checkpoint without any error.\r\n\r\nChief output as follow:\r\n```\r\nINFO:tensorflow:Graph was finalized.\r\nI1218 15:24:28.721170 139824828032832 monitored_session.py:240] Graph was finalized.\r\nINFO:tensorflow:Restoring parameters from /tmp/iris/iris-chief-0/checkpoint/model.ckpt-1000\r\nI1218 15:24:28.722573 139824828032832 saver.py:1284] Restoring parameters from /tmp/iris/iris-chief-0/checkpoint/model.ckpt-1000\r\nWARNING:tensorflow:From /root/PycharmProjects/multi_gpu_demo/venv/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file utilities to get mtimes.\r\nW1218 15:24:28.753480 139824828032832 deprecation.py:323] From /root/PycharmProjects/multi_gpu_demo/venv/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file utilities to get mtimes.\r\nINFO:tensorflow:Running local_init_op.\r\nI1218 15:24:28.782264 139824828032832 session_manager.py:500] Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nI1218 15:24:28.938534 139824828032832 session_manager.py:502] Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 1000 into /tmp/iris/iris-chief-0/checkpoint/model.ckpt.\r\nI1218 15:24:29.215120 139824828032832 basic_session_run_hooks.py:606] Saving checkpoints for 1000 into /tmp/iris/iris-chief-0/checkpoint/model.ckpt.\r\n```\r\n\r\nWorker output as follow:\r\n```\r\nINFO:tensorflow:Graph was finalized.\r\nI1218 15:24:59.388847 140629246773056 monitored_session.py:240] Graph was finalized.\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect that when I increase train_steps\uff08from 1000 to 2000\uff09, no matter which roles should be able to restore the model from \r\n\r\ncheckpoint and continue training. Because I think each role only saves a part of the parameters of the model on allReduce mode, \r\n\r\nso each role should participate in restoring the model from checkpoint.\r\n\r\nBut, I analyzed and debug the source code and found only roles with chief or worker index=0 can restore model parameters \r\n\r\nfrom checkpoint\r\n```\r\n/venv/lib/python3.6/site-packages/tensorflow_core/python/distribute/multi_worker_util.py\r\nline 93: is_chief\r\ndef is_chief(cluster_spec=None, task_type=None, task_id=None):\r\n  ...\r\n  if task_type == \"chief\" or task_type == \"evaluator\":\r\n    return True\r\n\r\n  if (\"chief\" not in cluster_spec and task_type == \"worker\" and task_id == 0):\r\n    return True\r\n  return False\r\n  ...\r\n```\r\n", "comments": ["@UrmsOne,\r\nTensorFlow 1.x is not actively supported. Could you please update TensorFlow to v2.4 and check if you are facing the same issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45839\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45839\">No</a>\n"]}, {"number": 45837, "title": "Op type not registered 'StatefulPartitionedCall' in binary running", "body": "\u4f7f\u7528spark-2.3\uff0c\u7136\u540e\u901a\u8fc7maven\u4e0b\u8f7d\u4f9d\u8d56\u5305\uff0clibtensorflow-1.9.0.jar,libtensorflow_jni-1.9.0.jar\uff1b\r\n\u5728spark-submit\u63d0\u4ea4\uff0c\u8bfb\u53d6\u6a21\u578b\uff0c\u62a5\u5982\u4e0b\u9519\uff1a\r\n\r\norg.tensorflow.TensorFlowException: Op type not registered 'StatefulPartitionedCall' in binary running on [\u673a\u5668\u540d]. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\r\n", "comments": ["@chlyzzo,\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. Thanks!\r\n", "Also, looks like you are using TensorFlow 1.x which is not actively supported. Please install TensorFlow v2.4 and check if you are facing the same issue. Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 45836, "title": "Fix: NoneType' object has no attribute 'UnimplementedError", "body": "At shutdown, ` tensorflow.python.framework.errors` may have been garbage collected.", "comments": ["Should this be open against master branch? Is there a bug?", "This bug seems to have been fixed in the master branch.", "In this case, closing this PR."]}, {"number": 45835, "title": "v1.15.4 NoneType' object has no attribute 'UnimplementedError", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.15.4\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version:  N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n**Describe the current behavior**\r\nI am working on distributed learning in tensorflow through estimators API using below simple code template:\r\n\r\n```\r\n# TrainSpec for training\r\n    train_spec = tf.estimator.TrainSpec(\r\n        input_fn=lambda: csv_input_fn(TRAIN_PATH, FLAGS.batch_size, True),\r\n        max_steps=FLAGS.train_steps,\r\n        hooks=[])\r\n    # EvalSpec for test\r\n    eval_spec = tf.estimator.EvalSpec(\r\n        input_fn=lambda: csv_input_fn(TEST_PATH, FLAGS.batch_size, True))\r\n    print(\"---training and testing---\")\r\n    tf.estimator.train_and_evaluate(classifier, train_spec, eval_spec)\r\n    print(\"---training finished---\")\r\n\r\n    # All role are workers, pick the task_index with 0 to save model\r\n    if FLAGS.task_index == 0:\r\n        classifier.export_saved_model(export_dir, serving_input_receiver_fn)\r\n        # classifier.export_savedmodel(export_dir, serving_input_receiver_fn, strip_default_attrs=True)\r\n    print(\"finish...\")\r\n```\r\n\r\nI encountered the following error during training.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/server_lib.py\", line 158, in __del__\r\nAttributeError: 'NoneType' object has no attribute 'UnimplementedError'\r\n```\r\n\r\nI analyzed the source code and found that the errors object may be garbage collected.\r\n\r\nI will submit a PR later, please pass it. Because I will use v1.15.4(v1.15.4 version on our production environment) \r\n\r\nfor distributed training on kubeflow, \r\n\r\nI don\u2019t want to modify the code there every time.\r\n\r\nThanks!!!\r\n", "comments": ["@UrmsOne,\r\nTensorFlow 1.x is not actively supported. Could you please update TensorFlow to v2.4 and check if you are facing the same issue? Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45835\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45835\">No</a>\n", "same issue here, any updates?", "@power1628 \r\n\r\nModify the source code to solve the problem by myself. You could try it, too.\r\n\r\nReference:\r\n\r\n[https://github.com/tensorflow/tensorflow/pull/45836/](https://github.com/tensorflow/tensorflow/pull/45836/commits/7bb1dbc3991854c92b1aa03fd7a7159e8bc4dfe5)"]}, {"number": 45834, "title": "Tensorflow 2.4 Throwing Errors ", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10 v20H2 \r\n- TensorFlow installed from (source or binary): Through pip command\r\n- TensorFlow version (use command below): v2.4.0\r\n- Python version: v3.8.5\r\n- CUDA/cuDNN version: v11.0 (update 1)/v8.0.4\r\n- GPU model and memory: Nvidia GTX 1650 4GB GDDR5\r\n\r\n-tf.version.GIT_VERSION: library cudart64_110.dll\r\n-tf.version.VERSION: v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n\r\n**Describe the current behavior**\r\nUnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node sequential/conv2d/Conv2D (defined at <ipython-input-4-f922f88cbe61>:19) ]] [Op:__inference_train_function_753]\r\nFunction call stack:\r\ntrain_function\r\n\r\n**Describe the expected behavior**\r\nIt should be training the network with no issues. The same code worked on tensorflow v2.3.0.\r\n\r\n**Standalone code to reproduce the issue**\r\nNotebook at:\r\nhttps://colab.research.google.com/drive/14vdfsSkqGn375E54vd11--yXy0WKemWm?usp=sharing\r\nIt was working fine on v2.3.0 at my end but after upgrading to 2.4.0 its throwing error.  Also I tried one more CNN network with different architecture which is working fine with no errors. I have also renamed a dll file in order to make tensorflow open CUPTI library.\r\n\r\n**Other info / logs** Logs are included.\r\n[Expected_log.txt](https://github.com/tensorflow/tensorflow/files/5713830/Expected_log.txt)\r\n[Current_log.txt](https://github.com/tensorflow/tensorflow/files/5713831/Current_log.txt)\r\n\r\n\r\n", "comments": ["@milind-prajapat,\r\nPlease take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/45779#issuecomment-747403789) from a similar issue #45779 and check if it helps. Thanks!", "Yes, It worked. Thank you. But its wired to add them each time. Any\npermanent solution?\n\nOn Fri, 18 Dec 2020 at 18:15, Abhilash Mahendrakar <notifications@github.com>\nwrote:\n\n> @milind-prajapat <https://github.com/milind-prajapat>,\n> Please take a look at this comment\n> <https://github.com/tensorflow/tensorflow/issues/45779#issuecomment-747403789>\n> from a similar issue #45779\n> <https://github.com/tensorflow/tensorflow/issues/45779> and check if it\n> helps. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/45834#issuecomment-748065802>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AODGNKGNJWN3TNPPZMOQEYDSVNFIBANCNFSM4VAVYICQ>\n> .\n>\n", "> Yes, It worked.\r\n\r\n@milind-prajapat,\r\nThank you for the update. You'll have to set the GPU memory limit every time.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45834\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45834\">No</a>\n", "Yes, it is working.\n\nOn Thu, 7 Jan, 2021, 12:19 am tensorflow-butler[bot], <\nnotifications@github.com> wrote:\n\n> Are you satisfied with the resolution of your issue?\n> Yes\n> <https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45834>\n> No\n> <https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45834>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/45834#issuecomment-755507333>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AODGNKCFEXEPXLWYSGCQPA3SYSWCFANCNFSM4VAVYICQ>\n> .\n>\n"]}, {"number": 45830, "title": "[INTEL MKL] Replace dead Switch nodes with Identity", "body": "If the \"pred\" input of a Switch node is constant, and it is determined to be a dead switch, we can replace it with Identity. In other pass, Identity gets removed and it enables more fusions resulting in performance improvements.", "comments": ["@ezhulenev - Hi Eugene, can you please provide me details for this 1 check failed \"\r\nfeedback/copybara \u2014 Google internal checks FAILED for runs with create time 2020-12-29T13:31:47.826852083Z.\"\r\nI am unable to see the details", "Some of the tests are failing with this PR:\r\n\r\n```\r\n//third_party/tensorflow/python/kernel_tests:control_flow_ops_py_test_xla_gpu\tFAILED\r\n//third_party/tensorflow/python/kernel_tests:control_flow_ops_py_test\tFAILED\r\n//third_party/tensorflow/python/kernel_tests:control_flow_ops_py_test\tFAILED\r\n```\r\n\r\nExample:\r\n\r\n```\r\n[ RUN      ] ControlFlowContextCheckTest.testInvalidContextInCond\r\nI1229 05:37:04.054435    2628 control_flow_util.py:363] Cannot use 'while/Const_1' as input to 'cond/Add' because 'while/Const_1' is in a while loop.\r\n\r\ncond/Add while context: None\r\nwhile/Const_1 while context: while/while_context\r\n```", "@gaurides  Can you please check @ezhulenev's comments and keep us posted ? Thanks!", "Hi @gbaned  @ezhulenev - ControlFlowTest.testWhileCondWithControl_1 is the failing test case. I have a work around, but working on the right fix. will keep you updated.", "@gbaned @ezhulenev  - i updated the PR with changes to fix the failing test", "Hi @ezhulenev , @gbaned  , did you get a chance to review the changes? Please let me know if looks ok or needs more changes. Thanks.", "@ezhulenev  @gbaned  : Can you please share what tests failed if you need me to fix them. How can I reproduce these issues on my end? Thanks!", "Seems to be a transient build infra failure, not sure to to force pull one more time, @gbaned can you please take a look.", "> Seems to be a transient build infra failure, not sure to to force pull one more time, @gbaned can you please take a look.\r\n\r\n@ezhulenev  Sure. It is completed successfully now. Thank you!", "Hi @gbaned : I wanted to check on this Ubuntu CPU build, if there's something I need to fix. Thanks.\r\n\"Ubuntu CPU \u2014 Internal CI infrastructure error. Please apply label 'kokoro:force-run' to rerun the build.\"", "Hi @gbaned : can you please provide me inputs on what failed in this build \"feedback/copybara \u2014 Google internal checks FAILED \"? and if possible how to reproduce?\r\nAlso, the 2 Windows builds failure don't seem to be related to my changes here. \r\nThanks!", "@ezhulenev  Can you please take a look on above comments from @gaurides. Thanks!", "@ezhulenev Any update on this PR? Please. Thanks!"]}, {"number": 45824, "title": "micro: port op SPACE_TO_DEPTH from lite", "body": "@tensorflow/micro\r\n\r\nThis issue tracks my work porting operator SPACE_TO_DEPTH from lite to micro. The port will be submitted in a number of PRs. Here's a rough flight plan in the style of #45306:\r\n\r\n* PR 1: Extract the code for parsing the op from a flatbuffer out of ParseOpDataTfLite in tensorflow/lite/core/api/flatbuffer_conversions.cc into a standalone function that can be called from micro's op resolver\r\n* PR 2: Extract the reference implementation out of tensorflow/lite/kernels/internal/reference/reference_ops.h into its own header which can be included without dragging in reference_ops.h's dependences\r\n* PR 3: Copy operator from lite to micro without making any changes or including in the build\r\n* PR 4: Delete extra code from the micro copy of the operator\r\n* PR 5: Port micro copy of operator as necessary and add a corresponding test\r\n", "comments": ["This issue has been migrated to the tflite-micro repository as tensorflow/tflite-micro#113.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45824\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45824\">No</a>\n"]}, {"number": 45823, "title": "Official Sample C++ Test Custom Op Build Fails With TF 2.4 Using VS2019", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): Both\r\n- TensorFlow version (use command below): 2.4\r\n- Python version: 3.8\r\n- GCC/Compiler version (if compiling from source): VS2019 16.8.3\r\n- CUDA/cuDNN version: 11.0 / 8.0.5\r\n- GPU model and memory: 2070 MaxQ\r\n\r\n**Describe the current behavior**\r\n\r\nWhen attempting to build the test custom c++ op as described on https://www.tensorflow.org/guide/create_op , fails to build with the errors listed below. This same code / VS project builds perfectly fine with TF 2.3 and before.\r\n\r\n```\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n#include \"tensorflow/core/framework/op.h\"\r\n#include \"tensorflow/core/framework/shape_inference.h\"\r\n\r\nusing namespace tensorflow;\r\n\r\nREGISTER_OP(\"ZeroOut\")\r\n.Input(\"to_zero: int32\")\r\n.Output(\"zeroed: int32\")\r\n.SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\r\n    c->set_output(0, c->input(0));\r\n    return Status::OK();\r\n});\r\n\r\n\r\nusing namespace tensorflow;\r\n\r\nclass ZeroOutOp : public OpKernel {\r\npublic:\r\n    explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}\r\n\r\n    void Compute(OpKernelContext* context) override {\r\n        // Grab the input tensor\r\n        const Tensor& input_tensor = context->input(0);\r\n        auto input = input_tensor.flat<int32>();\r\n\r\n        // Create an output tensor\r\n        Tensor* output_tensor = NULL;\r\n        OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),\r\n            &output_tensor));\r\n        auto output_flat = output_tensor->flat<int32>();\r\n\r\n        // Set all but the first element of the output tensor to 0.\r\n        const int N = input.size();\r\n        for (int i = 1; i < N; i++) {\r\n            output_flat(i) = 0;\r\n        }\r\n\r\n        // Preserve the first input value if possible.\r\n        if (N > 0) output_flat(0) = input(0);\r\n    }\r\n};\r\n\r\nREGISTER_KERNEL_BUILDER(Name(\"ZeroOut\").Device(DEVICE_CPU), ZeroOutOp);\r\n```\r\n\r\nError Message:\r\n\r\n```\r\n1>zero_test.cpp\r\n1>C:\\sdks\\tensorflow\\bazel-tensorflow\\external\\eigen_archive\\unsupported\\Eigen\\CXX11\\src\\Tensor\\Tensor.h(76,1): warning C4554: '&': check operator precedence for possible error; use parentheses to clarify precedence\r\n1>C:\\sdks\\tensorflow\\bazel-tensorflow\\external\\eigen_archive\\unsupported\\Eigen\\CXX11\\src\\Tensor\\TensorMap.h(33): message : see reference to class template instantiation 'Eigen::Tensor<T,1,1,int>' being compiled\r\n1>        with\r\n1>        [\r\n1>            T=float\r\n1>        ]\r\n1>C:\\sdks\\tensorflow\\tensorflow\\core\\framework\\tensor_types.h(105): message : see reference to class template instantiation 'Eigen::TensorMap<Eigen::Tensor<T,1,1,int>,16,Eigen::MakePointer>' being compiled\r\n1>        with\r\n1>        [\r\n1>            T=float\r\n1>        ]\r\n1>C:\\sdks\\tensorflow\\bazel-bin\\tensorflow\\core\\framework\\attr_value.pb.h(687,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>C:\\sdks\\tensorflow\\bazel-bin\\tensorflow\\core\\framework\\node_def.pb.h(97,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>C:\\sdks\\tensorflow\\bazel-bin\\tensorflow\\core\\framework\\function.pb.h(300,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>C:\\sdks\\tensorflow\\bazel-bin\\tensorflow\\core\\framework\\function.pb.h(332,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>C:\\sdks\\tensorflow\\bazel-bin\\tensorflow\\core\\framework\\function.pb.h(587,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>C:\\sdks\\tensorflow\\bazel-bin\\tensorflow\\core\\framework\\function.pb.h(590,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>C:\\sdks\\tensorflow\\bazel-bin\\tensorflow\\core\\framework\\function.pb.h(621,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>C:\\sdks\\tensorflow\\bazel-bin\\tensorflow\\core\\framework\\function.pb.h(624,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>C:\\sdks\\tensorflow\\tensorflow\\core\\util\\tensor_format.h(502,79): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>C:\\sdks\\tensorflow\\tensorflow\\core\\util\\tensor_format.h(524,71): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>C:\\sdks\\tensorflow\\tensorflow\\core\\util\\tensor_format.h(558,77): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>C:\\sdks\\tensorflow\\tensorflow\\core\\framework\\node_def_util.h(147,20): warning C4267: 'return': conversion from 'size_t' to 'int', possible loss of data\r\n1>C:\\sdks\\tensorflow\\bazel-bin\\tensorflow\\core\\framework\\step_stats.pb.h(1180,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>C:\\sdks\\tensorflow\\bazel-bin\\tensorflow\\core\\protobuf\\cluster.pb.h(97,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>C:\\sdks\\tensorflow\\bazel-bin\\tensorflow\\core\\protobuf\\rewriter_config.pb.h(543,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>C:\\sdks\\tensorflow\\bazel-bin\\tensorflow\\core\\protobuf\\config.pb.h(1949,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>C:\\sdks\\tensorflow\\bazel-bin\\tensorflow\\core\\protobuf\\config.pb.h(3784,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>C:\\sdks\\tensorflow\\bazel-bin\\tensorflow\\core\\protobuf\\config.pb.h(3787,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>C:\\sdks\\tensorflow\\bazel-bin\\tensorflow\\core\\protobuf\\config.pb.h(3818,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>C:\\sdks\\tensorflow\\bazel-bin\\tensorflow\\core\\protobuf\\config.pb.h(3821,1): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data\r\n1>C:\\sdks\\tensorflow\\tensorflow\\core\\framework\\op_kernel.h(158,59): warning C4244: 'return': conversion from 'unsigned __int64' to 'int', possible loss of data\r\n1>C:\\sdks\\tensorflow\\tensorflow\\core\\framework\\op_kernel.h(165,61): warning C4244: 'return': conversion from 'unsigned __int64' to 'int', possible loss of data\r\n1>C:\\sdks\\tensorflow\\tensorflow\\core\\framework\\op_kernel.h(306,59): warning C4244: 'return': conversion from 'unsigned __int64' to 'int', possible loss of data\r\n1>C:\\sdks\\tensorflow\\tensorflow\\core\\framework\\op_kernel.h(314,61): warning C4244: 'return': conversion from 'unsigned __int64' to 'int', possible loss of data\r\n1>C:\\sdks\\tensorflow\\tensorflow\\core\\framework\\op_kernel.h(722,56): warning C4244: 'return': conversion from 'unsigned __int64' to 'int', possible loss of data\r\n1>C:\\sdks\\tensorflow\\tensorflow\\core\\framework\\op_kernel.h(727,49): warning C4244: 'return': conversion from 'unsigned __int64' to 'int', possible loss of data\r\n1>C:\\sdks\\tensorflow\\tensorflow\\core\\framework\\shape_inference.h(265,26): warning C4267: 'return': conversion from 'size_t' to 'int', possible loss of data\r\n1>C:\\sdks\\tensorflow\\tensorflow\\core\\framework\\shape_inference.h(320,27): warning C4267: 'return': conversion from 'size_t' to 'int', possible loss of data\r\n1>C:\\sdks\\tensorflow\\tensorflow\\core\\framework\\shape_inference.h(785,39): warning C4267: 'initializing': conversion from 'size_t' to 'const tensorflow::int32', possible loss of data\r\n1>C:\\Users\\Adam\\Documents\\Visual Studio 2019\\Projects\\OSD_Custom_Op\\ZeroTest\\zero_test.cpp(7,1): warning C4003: not enough arguments for function-like macro invocation 'REGISTER_OP_IMPL'\r\n1>C:\\Users\\Adam\\Documents\\Visual Studio 2019\\Projects\\OSD_Custom_Op\\ZeroTest\\zero_test.cpp(7,1): warning C4002: too many arguments for function-like macro invocation 'SHOULD_REGISTER_OP'\r\n1>C:\\Users\\Adam\\Documents\\Visual Studio 2019\\Projects\\OSD_Custom_Op\\ZeroTest\\zero_test.cpp(7,1): error C2059: syntax error: '||'\r\n1>C:\\Users\\Adam\\Documents\\Visual Studio 2019\\Projects\\OSD_Custom_Op\\ZeroTest\\zero_test.cpp(7,1): error C2039: 'value': is not a member of '`global namespace''\r\n1>C:\\Users\\Adam\\Documents\\Visual Studio 2019\\Projects\\OSD_Custom_Op\\ZeroTest\\zero_test.cpp(7,1): error C2146: syntax error: missing ')' before identifier 'value'\r\n1>C:\\Users\\Adam\\Documents\\Visual Studio 2019\\Projects\\OSD_Custom_Op\\ZeroTest\\zero_test.cpp(7,1): error C2065: 'value': undeclared identifier\r\n1>C:\\Users\\Adam\\Documents\\Visual Studio 2019\\Projects\\OSD_Custom_Op\\ZeroTest\\zero_test.cpp(7,1): error C2059: syntax error: ')'\r\n1>C:\\Users\\Adam\\Documents\\Visual Studio 2019\\Projects\\OSD_Custom_Op\\ZeroTest\\zero_test.cpp(7,1): error C2440: '<function-style-cast>': cannot convert from 'initializer list' to 'tensorflow::register_op::OpDefBuilderWrapper'\r\n1>C:\\Users\\Adam\\Documents\\Visual Studio 2019\\Projects\\OSD_Custom_Op\\ZeroTest\\zero_test.cpp(7,1): message : No constructor could take the source type, or constructor overload resolution was ambiguous\r\n1>C:\\Users\\Adam\\Documents\\Visual Studio 2019\\Projects\\OSD_Custom_Op\\ZeroTest\\zero_test.cpp(34,33): warning C4244: 'initializing': conversion from '__int64' to 'int', possible loss of data\r\n1>C:\\Users\\Adam\\Documents\\Visual Studio 2019\\Projects\\OSD_Custom_Op\\ZeroTest\\zero_test.cpp(34,21): warning C4244: 'initializing': conversion from '__int64' to 'const int', possible loss of data\r\n1>C:\\Users\\Adam\\Documents\\Visual Studio 2019\\Projects\\OSD_Custom_Op\\ZeroTest\\zero_test.cpp(44,1): warning C4003: not enough arguments for function-like macro invocation 'REGISTER_KERNEL_BUILDER_IMPL_2'\r\n1>C:\\Users\\Adam\\Documents\\Visual Studio 2019\\Projects\\OSD_Custom_Op\\ZeroTest\\zero_test.cpp(44,1): warning C4003: not enough arguments for function-like macro invocation 'REGISTER_KERNEL_BUILDER_IMPL_3'\r\n1>C:\\Users\\Adam\\Documents\\Visual Studio 2019\\Projects\\OSD_Custom_Op\\ZeroTest\\zero_test.cpp(44,1): warning C4003: not enough arguments for function-like macro invocation 'SHOULD_REGISTER_OP_KERNEL'\r\n1>C:\\Users\\Adam\\Documents\\Visual Studio 2019\\Projects\\OSD_Custom_Op\\ZeroTest\\zero_test.cpp(44,1): warning C4002: too many arguments for function-like macro invocation 'SHOULD_REGISTER_OP'\r\n1>C:\\Users\\Adam\\Documents\\Visual Studio 2019\\Projects\\OSD_Custom_Op\\ZeroTest\\zero_test.cpp(44,1): error C2059: syntax error: '||'\r\n1>C:\\Users\\Adam\\Documents\\Visual Studio 2019\\Projects\\OSD_Custom_Op\\ZeroTest\\zero_test.cpp(44,1): error C2039: 'value': is not a member of '`global namespace''\r\n1>C:\\Users\\Adam\\Documents\\Visual Studio 2019\\Projects\\OSD_Custom_Op\\ZeroTest\\zero_test.cpp(44,1): error C2146: syntax error: missing ')' before identifier 'value'\r\n1>C:\\Users\\Adam\\Documents\\Visual Studio 2019\\Projects\\OSD_Custom_Op\\ZeroTest\\zero_test.cpp(44,1): error C2065: 'value': undeclared identifier\r\n1>C:\\Users\\Adam\\Documents\\Visual Studio 2019\\Projects\\OSD_Custom_Op\\ZeroTest\\zero_test.cpp(44,1): error C2059: syntax error: ')'\r\n1>C:\\Users\\Adam\\Documents\\Visual Studio 2019\\Projects\\OSD_Custom_Op\\ZeroTest\\zero_test.cpp(44,1): error C2059: syntax error: ','\r\n1>C:\\Users\\Adam\\Documents\\Visual Studio 2019\\Projects\\OSD_Custom_Op\\ZeroTest\\zero_test.cpp(44,1): error C2062: type 'void' unexpected\r\n1>C:\\Users\\Adam\\Documents\\Visual Studio 2019\\Projects\\OSD_Custom_Op\\ZeroTest\\zero_test.cpp(44,1): error C2059: syntax error: 'return'\r\n1>C:\\Users\\Adam\\Documents\\Visual Studio 2019\\Projects\\OSD_Custom_Op\\ZeroTest\\zero_test.cpp(44,1): error C2059: syntax error: '}'\r\n1>C:\\Users\\Adam\\Documents\\Visual Studio 2019\\Projects\\OSD_Custom_Op\\ZeroTest\\zero_test.cpp(44,1): error C2143: syntax error: missing ')' before '.'\r\n1>C:\\Users\\Adam\\Documents\\Visual Studio 2019\\Projects\\OSD_Custom_Op\\ZeroTest\\zero_test.cpp(44,1): error C2064: term does not evaluate to a function taking 0 arguments\r\n1>C:\\Users\\Adam\\Documents\\Visual Studio 2019\\Projects\\OSD_Custom_Op\\ZeroTest\\zero_test.cpp(44,1): message : class does not define an 'operator()' or a user defined conversion operator to a pointer-to-function or reference-to-function that takes appropriate number of arguments\r\n```\r\n", "comments": ["Got the same problem trying to build a custom op after upgrading to Tensorflow 2.4 on Windows.\r\n\r\nThis appears to be related to the way MSVC handles \\_\\_VA_ARGS\\_\\_ in the nested variadic macro expansions under REGISTER_OP and REGISTER_KERNEL_BUILDER. \r\n\r\nUsing the workaround from https://stackoverflow.com/questions/5134523/msvc-doesnt-expand-va-args-correctly, this can be solved by forcing a double expansion in the macros **TF_NEW_ID_FOR_INIT_2** and **TF_EXTRACT_KERNEL_NAME_IMPL** the following way:\r\n\r\n    #define EXPAND(x) x\r\n\r\n    #define TF_NEW_ID_FOR_INIT_2(m, c, ...) EXPAND(m(c, __VA_ARGS__)) // L145 selective_registration.h\r\n    #define TF_EXTRACT_KERNEL_NAME_IMPL(m, ...) EXPAND(m(__VA_ARGS__)) // L1431 op_kernel.h\r\n\r\nA workaround is to add these definitions in your own code somewhere after including op_kernel.h\r\n\r\n", "@oracle3001  Can you please test with latest release 2.5 and validate if the issue still persists? Thanks!", "Unfortunately I am writing a paper this week and so don't have time to test this. I will try and take a look in a week or so. ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This works with TF 2.5", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45823\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45823\">No</a>\n"]}, {"number": 45821, "title": "I think I only have cudart64_110.dll", "body": "I think I only have cudart64_110.dll\r\n(The one that I highlighted)\r\n![image](https://user-images.githubusercontent.com/68514251/93034064-d0b54a00-f606-11ea-991e-429bb7713d58.png)\r\n\r\n_Originally posted by @CalendulaED in https://github.com/tensorflow/tensorflow/issues/43193#issuecomment-691760016_\r\n\r\nplease pass me this file cudart64_110.dll its misssing here, i dont know what else i can do\r\n", "comments": []}, {"number": 45819, "title": "created pizza.txt", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45819) for more info**.\n\n<!-- need_sender_cla -->", "Closing this PR since no changes in file.  Thanks!\r\ncc @mihaimaruseac \r\n", "Please don't spam @saisravan1390 "]}, {"number": 45818, "title": "Add GPU kernels for SparseApply[Proximal]Adagrad", "body": "Also applies to Resource and V2 versions of the ops.\r\n\r\nThis is a follow-up to https://github.com/tensorflow/tensorflow/pull/44919\r\n\r\ncc @nluehr ", "comments": []}, {"number": 45817, "title": "[INTEL MKL] DNN 0.x code cleanup - Fused Matmul op", "body": "This PR replaces the old one https://github.com/tensorflow/tensorflow/pull/45564 (which is not approved/merged), \r\n\r\n\r\nDNN 0.x cleanup of Fused Matmul op:\r\n\r\n(1) Remove all DNN 0.x related code;\r\n\r\n(2) Replace all DNN 1.x macro usages\r\n\r\nAnd minor change in quantized op (macro replacement in a couple of places)", "comments": ["@gzmkl Can you please resolve conflicts? Thanks!", "@gbaned  I have addressed the merge conflict issues. thank you!"]}, {"number": 45816, "title": "Copy TFL kernel EXP and exp_test into TFLM kernel without changes", "body": "Issue #45415 PR3: copy the reference kernel from lite to micro without making any changes. At this point the kernel is in micro but it is not part of the build", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "Thanks. I changed TODOs to OLDTODOs.\r\n-Robby\r\n\r\nFrom: Rajeshwar Reddy T <notifications@github.com>\r\nSent: Monday, January 11, 2021 3:24 PM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: Robby Sun <rsun@bdti.com>; State change <state_change@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] Copy TFL kernel EXP and exp_test into TFLM kernel without changes (#45816)\r\n\r\n\r\n@rthadur requested changes on this pull request.\r\n\r\n________________________________\r\n\r\nIn tensorflow/lite/micro/kernels/exp.cc<https://github.com/tensorflow/tensorflow/pull/45816#discussion_r555402686>:\r\n\r\n> +  ExpContext op_context(context, node);\r\n\r\n+  TfLiteIntArray* output_dims = TfLiteIntArrayCopy(op_context.input->dims);\r\n\r\n+  op_context.output->type = op_context.input->type;\r\n\r\n+  return context->ResizeTensor(context, op_context.output, output_dims);\r\n\r\n+}\r\n\r\n+\r\n\r\n+template <KernelType kernel_type>\r\n\r\n+TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\r\n\r\n+  ExpContext op_context(context, node);\r\n\r\n+\r\n\r\n+#define TF_LITE_EXP(kernel_type, data_type)                               \\\r\n\r\n+  kernel_type::Exp<data_type>(GetTensorData<data_type>(op_context.input), \\\r\n\r\n+                              NumElements(op_context.input),              \\\r\n\r\n+                              GetTensorData<data_type>(op_context.output))\r\n\r\n+\r\n\r\n+  // TODO(kanlig): supports half, bfloat16, float64, complex64, and complex128.\r\n\r\nplease remove all TODOs , this is a blocker for internal merge , thank you\r\n\r\n________________________________\r\n\r\nIn tensorflow/lite/micro/kernels/exp.cc<https://github.com/tensorflow/tensorflow/pull/45816#discussion_r555402778>:\r\n\r\n> +        return kTfLiteError;\r\n\r\n+    }\r\n\r\n+  }\r\n\r\n+#undef TF_LITE_EXP\r\n\r\n+  return kTfLiteOk;\r\n\r\n+}\r\n\r\n+\r\n\r\n+}  // namespace exp\r\n\r\n+\r\n\r\n+TfLiteRegistration* Register_EXP_REF() {\r\n\r\n+  static TfLiteRegistration r = {nullptr, nullptr, exp::Prepare,\r\n\r\n+                                 exp::Eval<exp::kReference>};\r\n\r\n+  return &r;\r\n\r\n+}\r\n\r\n+\r\n\r\n+// TODO(kanlig): add optimized implementation of Exp.\r\n\r\nsame as above\r\n\r\n\u2014\r\nYou are receiving this because you modified the open/close state.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/pull/45816#pullrequestreview-565816328>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AQSCHARP3ZBSZW56O5N4SZLSZOB7NANCNFSM4VAMQ53A>.\r\n", "With https://github.com/tensorflow/tensorflow/commit/6cf83851ced6a8326cb48821a2c2ee2bef81f1f4, a previous version of this PR has been merged (without https://github.com/tensorflow/tensorflow/pull/45816/commits/9dcbe61107f9fce517298a7e522654d0dad66479). Closing the PR"]}, {"number": 45815, "title": "Copy TFL kernel CAST and cast_test into TFLM kernel without changes", "body": "Issue #45608 PR3: Create a TFLite Micro copy of the TFLite CAST operator and its test code", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 45814, "title": "Refactor Lite reference op CAST from reference_ops.h into cast.h", "body": "Issue #45608 PR2: Refactor reference implementation from lite/kernels/internal/reference/reference_ops.h into its own header cast.h without making any changes", "comments": ["the internal checks are failing because of a missing license in cast.h.\r\n\r\nPR https://github.com/tensorflow/tensorflow/pull/46182 will make the error visible externally as well so that it will be caught much earlier in the review process. ", "@rsun-bdti  Can you please resolve conflicts? Thanks!", "Merged code is not failing. I am digging into this.", "Seems auto-merge is not happening but the changes are merged into master now, so we can close this. Thank you for the PR."]}, {"number": 45812, "title": "Add renode downloads to the Makefile.", "body": "Since the first integration with the portable renode, we have converged\r\non adding synchronous calls to download scripts as part of the Makefile\r\nand this change integrates the renode download with that flow.\r\n\r\nAddresses http://b/172939049", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 45811, "title": "Fix bad cherrypick", "body": "", "comments": []}, {"number": 45810, "title": "Fix bad cherrypick", "body": "", "comments": []}, {"number": 45809, "title": "Fix bad cherrypick", "body": "", "comments": []}, {"number": 45808, "title": "[INTEL MKL] cherry-pick peg docker version to '<=4.3.0' for partials", "body": "Cherry picking fixes in #45807", "comments": ["@angerson please help review and merge this into current release branch.\r\n\r\nThanks.", "This will have to wait until the 2.4 patch release, since we need to also track what gets merged on the branch at that time.", "Sounds good.\r\n\r\nFor now users who want to build `r2.4` need to manually check the file, rebuild the `tf-tools:latest` image and resume with `asm_dockerfiles` and `asm_images` from there."]}, {"number": 45807, "title": "[INTEL MKL] Peg docker version to '<=4.3.0' for partials", "body": "Looks like with the most recent version of `docker` on `PyPi` we get:\r\n\r\n```\r\nasm_dockerfiles --release dockerfiles --construct_dockerfiles\r\n> Skipping release nightly\r\n> Skipping release nightly\r\n> Skipping release versioned\r\n> Skipping release onednn\r\n> Skipping release onednn\r\n> Skipping release onednn\r\n> Skipping release onednn\r\n> Skipping release onednn\r\n> Skipping release onednn\r\n> Skipping release onednn\r\n> Skipping release onednn\r\n> Skipping release onednn\r\n> Skipping release onednn\r\n> Skipping release onednn\r\n> Skipping release onednn\r\n> Skipping release onednn\r\n> Emptying Dockerfile dir \"./dockerfiles\"\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/urllib3/connectionpool.py\", line 706, in urlopen\r\n    chunked=chunked,\r\n  File \"/usr/local/lib/python3.5/dist-packages/urllib3/connectionpool.py\", line 394, in _make_request\r\n    conn.request(method, url, **httplib_request_kw)\r\n  File \"/usr/lib/python3.5/http/client.py\", line 1151, in request\r\n    self._send_request(method, url, body, headers)\r\n  File \"/usr/lib/python3.5/http/client.py\", line 1196, in _send_request\r\n    self.endheaders(body)\r\n  File \"/usr/lib/python3.5/http/client.py\", line 1147, in endheaders\r\n    self._send_output(message_body)\r\n  File \"/usr/lib/python3.5/http/client.py\", line 950, in _send_output\r\n    self.send(msg)\r\n  File \"/usr/lib/python3.5/http/client.py\", line 893, in send\r\n    self.connect()\r\n  File \"/usr/local/lib/python3.5/dist-packages/docker/transport/unixconn.py\", line 43, in connect\r\n    sock.connect(self.unix_socket)\r\nFileNotFoundError: [Errno 2] No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/requests/adapters.py\", line 449, in send\r\n    timeout=timeout\r\n  File \"/usr/local/lib/python3.5/dist-packages/urllib3/connectionpool.py\", line 756, in urlopen\r\n    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\r\n  File \"/usr/local/lib/python3.5/dist-packages/urllib3/util/retry.py\", line 531, in increment\r\n    raise six.reraise(type(error), error, _stacktrace)\r\n  File \"/usr/local/lib/python3.5/dist-packages/urllib3/packages/six.py\", line 734, in reraise\r\n    raise value.with_traceback(tb)\r\n  File \"/usr/local/lib/python3.5/dist-packages/urllib3/connectionpool.py\", line 706, in urlopen\r\n    chunked=chunked,\r\n  File \"/usr/local/lib/python3.5/dist-packages/urllib3/connectionpool.py\", line 394, in _make_request\r\n    conn.request(method, url, **httplib_request_kw)\r\n  File \"/usr/lib/python3.5/http/client.py\", line 1151, in request\r\n    self._send_request(method, url, body, headers)\r\n  File \"/usr/lib/python3.5/http/client.py\", line 1196, in _send_request\r\n    self.endheaders(body)\r\n  File \"/usr/lib/python3.5/http/client.py\", line 1147, in endheaders\r\n    self._send_output(message_body)\r\n  File \"/usr/lib/python3.5/http/client.py\", line 950, in _send_output\r\n    self.send(msg)\r\n  File \"/usr/lib/python3.5/http/client.py\", line 893, in send\r\n    self.connect()\r\n  File \"/usr/local/lib/python3.5/dist-packages/docker/transport/unixconn.py\", line 43, in connect\r\n    sock.connect(self.unix_socket)\r\nurllib3.exceptions.ProtocolError: ('Connection aborted.', FileNotFoundError(2, 'No such file or directory'))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/docker/api/client.py\", line 214, in _retrieve_server_version\r\n    return self.version(api_version=False)[\"ApiVersion\"]\r\n  File \"/usr/local/lib/python3.5/dist-packages/docker/api/daemon.py\", line 181, in version\r\n    return self._result(self._get(url), json=True)\r\n  File \"/usr/local/lib/python3.5/dist-packages/docker/utils/decorators.py\", line 46, in inner\r\n    return f(self, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/docker/api/client.py\", line 237, in _get\r\n    return self.get(url, **self._set_request_timeout(kwargs))\r\n  File \"/usr/local/lib/python3.5/dist-packages/requests/sessions.py\", line 555, in get\r\n    return self.request('GET', url, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/requests/sessions.py\", line 542, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/requests/sessions.py\", line 655, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/requests/adapters.py\", line 498, in send\r\n    raise ConnectionError(err, request=request)\r\nrequests.exceptions.ConnectionError: ('Connection aborted.', FileNotFoundError(2, 'No such file or directory'))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"assembler.py\", line 719, in <module>\r\n    app.run(main)\r\n  File \"/usr/local/lib/python3.5/dist-packages/absl/app.py\", line 303, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.5/dist-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"assembler.py\", line 504, in main\r\n    dock = docker.from_env()\r\n  File \"/usr/local/lib/python3.5/dist-packages/docker/client.py\", line 101, in from_env\r\n    **kwargs_from_env(**kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/docker/client.py\", line 45, in __init__\r\n    self.api = APIClient(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/docker/api/client.py\", line 197, in __init__\r\n    self._version = self._retrieve_server_version()\r\n  File \"/usr/local/lib/python3.5/dist-packages/docker/api/client.py\", line 222, in _retrieve_server_version\r\n    'Error while fetching server API version: {0}'.format(e)\r\ndocker.errors.DockerException: Error while fetching server API version: ('Connection aborted.', FileNotFoundError(2, 'No such file or directory'))\r\n```", "comments": ["@angerson this one is also needed for both main and `r2.4` branches.\r\n\r\nThe cherry-pick is here: #45808 "]}, {"number": 45806, "title": "[INTEL MKL] cherry-pick fixes for 'Horovod>0.19.5' installation", "body": "This is basically just a cherry-pick for this PR: #45800 ", "comments": ["@angerson this is needed for `r2.4`", "This will have to wait until the 2.4 patch release, since we need to also track what gets merged on the branch at that time.", "That should be ok.\r\n\r\nThanks."]}, {"number": 45805, "title": "Refactor flatbuffer_conversions.cc/h to port TFL kernel CAST to TFLM", "body": "Issue #45608 PR1: refactor flatbuffer_conversions parsing function", "comments": []}, {"number": 45804, "title": "Fix import path", "body": "", "comments": []}, {"number": 45803, "title": "Fix import path", "body": "", "comments": []}, {"number": 45802, "title": "Fix import path", "body": "", "comments": []}, {"number": 45801, "title": "Said in the docs to add the Softmax Activation function but never did in code.", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/tutorials/images/cnn#add_dense_layers_on_top\r\n\r\n## Description of issue (what needs changing):\r\nIn the last sentence \"CIFAR has 10 output classes, so you use a final Dense layer **with 10 outputs and a softmax activation.\"** \r\n\"and a softmax activation\" **but I guess some one forgot to add the softmax activation at the end.**\r\n**OLD CODE:**\r\n```\r\nmodel.add(layers.Flatten())\r\nmodel.add(layers.Dense(64, activation='relu'))\r\nmodel.add(layers.Dense(10))\r\n```\r\n\r\n**NEW CODE:**\r\n```\r\nmodel.add(layers.Flatten())\r\nmodel.add(layers.Dense(64, activation='relu'))\r\nmodel.add(layers.Dense(10))\r\nmodel.add(layers.Activation('softmax'))\r\n```\r\n\r\n", "comments": ["Closing this issue since PR has been merged. Thank you."]}]