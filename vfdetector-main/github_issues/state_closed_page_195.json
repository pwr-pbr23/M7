[{"number": 48867, "title": "Issues serializing model (resource conversion)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: T4 16GB\r\n\r\n**Describe the current behavior**\r\n\r\nWhen attempting to serialize my model using model.save(), I get the following warnings/errors:\r\n\r\n```\r\nWARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe87653dc50>, because it is not built.\r\nWARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe87653dc50>, because it is not built.\r\nWARNING:absl:Found untraced functions such as concatenate_layer_call_and_return_conditional_losses, concatenate_layer_call_fn, resnet_block_layer_call_and_return_conditional_losses, resnet_block_layer_call_fn, resnet_block_1_layer_call_and_return_conditional_losses while saving (showing 5 of 365). These functions will not be directly callable after loading.\r\nWARNING:absl:Found untraced functions such as concatenate_layer_call_and_return_conditional_losses, concatenate_layer_call_fn, resnet_block_layer_call_and_return_conditional_losses, resnet_block_layer_call_fn, resnet_block_1_layer_call_and_return_conditional_losses while saving (showing 5 of 365). These functions will not be directly callable after loading.\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(op_type_name, name, **keywords)\r\n    521                 as_ref=input_arg.is_ref,\r\n--> 522                 preferred_dtype=default_dtype)\r\n    523         except TypeError as err:\r\n\r\n27 frames\r\nValueError: Tensor conversion requested dtype resource for Tensor with dtype float32: <tf.Tensor 'Adam/beta_1/Read/ReadVariableOp:0' shape=() dtype=float32>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(op_type_name, name, **keywords)\r\n    543           if input_arg.type != types_pb2.DT_INVALID:\r\n    544             raise TypeError(\"%s expected type of %s.\" %\r\n--> 545                             (prefix, dtypes.as_dtype(input_arg.type).name))\r\n    546           else:\r\n    547             # Update the maps with the default, if needed.\r\n\r\nTypeError: Input 'resource' of 'AssignVariableOp' Op has type float32 that does not match expected type of resource.\r\n```\r\n\r\nI'm not sure what the warning about the batchnorm layer means and if it's relevant to the error. From #47479 it the warnings about untraced functions can be safely ignored. It seems odd though that it would error out on the beta parameter of the adam optimizer. Can someone clarify what this float32 -> resource conversion is and why it could fail? I'm unfamiliar with the concept of a \"resource\" and documentation is rather open ended.\r\n\r\n**Describe the expected behavior**\r\nThe layer should properly serialize\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\ntboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs)\r\nmodel = build_model_functional()\r\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.002, epsilon=1e-8)\r\nmodel.compile(optimizer=optimizer, loss={'classes': ClassLoss()})\r\nmodel.fit(final_ds, epochs=4, steps_per_epoch=4, callbacks = [tboard_callback])\r\n```\r\n\r\nWorking on creating a minimal example but having difficulty pinpointing the exact element causing this issue. Worst case I can post my full project colab notebook but I'd rather be more general if possible. I'm using a pretty standard functional api model that has some internal custom layers that make use of the subclassing API.\r\n", "comments": ["Update: I found the issue, it's adding \r\n\r\n```\r\n tf.debugging.experimental.enable_dump_debug_info(\r\n     logs,\r\n     tensor_debug_mode=\"FULL_HEALTH\",\r\n     circular_buffer_size=-1)\r\n```\r\n\r\nThat causes the issue. This is related to #36833. I will disable the when saving models, but this issue should either be fixed or noted in the documentation for model.save() or tf.debugging", "@atyshka ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48867\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48867\">No</a>\n", "> Update: I found the issue, it's adding\r\n> \r\n> ```\r\n>  tf.debugging.experimental.enable_dump_debug_info(\r\n>      logs,\r\n>      tensor_debug_mode=\"FULL_HEALTH\",\r\n>      circular_buffer_size=-1)\r\n> ```\r\n> \r\n> That causes the issue. This is related to #36833. I will disable the when saving models, but this issue should either be fixed or noted in the documentation for model.save() or tf.debugging\r\n\r\nHi @atyshka,\r\n\r\nI have a similar problem with type conversion in model.save(), could you please indicate how you disabled this piece of code for the workaround to work before I start a new issue related to this. I can't find the input parameter related to this.\r\n\r\nThanks in advance!"]}, {"number": 48866, "title": "undefined symbol: _ZN10tensorflow....", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r2.5\r\n- Python version: 3.9.4\r\n- Installed using virtualenv? pip? conda?: N/A, building from source\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): gcc 10.2.0 (Ubuntu 10.2.0-5ubuntu1~20.04)\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: No GPU\r\n\r\n\r\n\r\n**Describe the problem**\r\nI encounter an undefined symbol error when it tried to import something on target 17076/32005, about 30k seconds in.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n``git clone https://github.com/tensorflow/tensorflow.git``\r\n``cd /root/tensorflow/``\r\n``git checkout r2.5``\r\n``./configure``\r\n(answered no to everything, copts set to -march=native and -Os)\r\n``/root/bazelisk-linux-amd64 build --config=opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures --config=v2 --config=noaws --config=nogcp --config=nohdfs --config=nonccl --config=libc++ --config=mkl --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --local_ram_resources=4096``\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\nERROR: /root/tensorflow/tensorflow/python/keras/api/BUILD:138:19: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Exit 1): bash failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CXXFLAGS='-stdlib=libc++' \\\r\n    PATH=/root/.cache/bazelisk/downloads/bazelbuild/bazel-3.7.2-linux-x86_64/bin:/root/.autojump/bin:/root/.pyenv/shims:/root/.pyenv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n    PYTHON_BIN_PATH=/root/.pyenv/versions/3.9.4/bin/python3 \\\r\n    PYTHON_LIB_PATH=/root/.pyenv/versions/3.9.4/lib/python3.9/site-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2  --apidir=bazel-out/k8-opt/bin/tensorflow/python/keras/api_v2/ --apiname=keras --apiversion=2  --loading=default --packages=tensorflow.python,tensorflow.python.keras,tensorflow.python.keras.activations,tensorflow.python.keras.applications.densenet,tensorflow.python.keras.applications.efficientnet,tensorflow.python.keras.applications.imagenet_utils,tensorflow.python.keras.applications.inception_resnet_v2,tensorflow.python.keras.applications.inception_v3,tensorflow.python.keras.applications.mobilenet,tensorflow.python.keras.applications.mobilenet_v2,tensorflow.python.keras.applications.mobilenet_v3,tensorflow.python.keras.applications.nasnet,tensorflow.python.keras.applications.resnet,tensorflow.python.keras.applications.resnet_v2,tensorflow.python.keras.applications.vgg16,tensorflow.python.keras.applications.vgg19,tensorflow.python.keras.applications.xception,tensorflow.python.keras.backend,tensorflow.python.keras.backend_config,tensorflow.python.keras.callbacks,tensorflow.python.keras.callbacks_v1,tensorflow.python.keras.constraints,tensorflow.python.keras.datasets.boston_housing,tensorflow.python.keras.datasets.cifar10,tensorflow.python.keras.datasets.cifar100,tensorflow.python.keras.datasets.fashion_mnist,tensorflow.python.keras.datasets.imdb,tensorflow.python.keras.datasets.mnist,tensorflow.python.keras.datasets.reuters,tensorflow.python.keras.engine.base_layer,tensorflow.python.keras.engine.data_adapter,tensorflow.python.keras.engine.input_layer,tensorflow.python.keras.engine.input_spec,tensorflow.python.keras.engine.sequential,tensorflow.python.keras.engine.training,tensorflow.python.keras.estimator,tensorflow.python.keras.feature_column.sequence_feature_column,tensorflow.python.keras.initializers,tensorflow.python.keras.initializers.initializers_v1,tensorflow.python.keras.initializers.initializers_v2,tensorflow.python.keras.layers.advanced_activations,tensorflow.python.keras.layers.convolutional,tensorflow.python.keras.layers.convolutional_recurrent,tensorflow.python.keras.layers.core,tensorflow.python.keras.layers.cudnn_recurrent,tensorflow.python.keras.layers.dense_attention,tensorflow.python.keras.layers.embeddings,tensorflow.python.keras.layers.local,tensorflow.python.keras.layers.merge,tensorflow.python.keras.layers.noise,tensorflow.python.keras.layers.normalization,tensorflow.python.keras.layers.normalization_v2,tensorflow.python.keras.layers.preprocessing,tensorflow.python.keras.layers.pooling,tensorflow.python.keras.layers.recurrent,tensorflow.python.keras.layers.recurrent_v2,tensorflow.python.keras.layers.serialization,tensorflow.python.keras.layers.wrappers,tensorflow.python.keras.losses,tensorflow.python.keras.metrics,tensorflow.python.keras.mixed_precision.get_layer_policy,tensorflow.python.keras.mixed_precision.loss_scale_optimizer,tensorflow.python.keras.mixed_precision.policy,tensorflow.python.keras.models,tensorflow.python.keras.optimizer_v2.adadelta,tensorflow.python.keras.optimizer_v2.adagrad,tensorflow.python.keras.optimizer_v2.adam,tensorflow.python.keras.optimizer_v2.adamax,tensorflow.python.keras.optimizer_v2.ftrl,tensorflow.python.keras.optimizer_v2.gradient_descent,tensorflow.python.keras.optimizer_v2.learning_rate_schedule,tensorflow.python.keras.optimizer_v2.nadam,tensorflow.python.keras.optimizer_v2.optimizer_v2,tensorflow.python.keras.optimizer_v2.rmsprop,tensorflow.python.keras.optimizers,tensorflow.python.keras.premade.linear,tensorflow.python.keras.premade.wide_deep,tensorflow.python.keras.preprocessing.image,tensorflow.python.keras.preprocessing.sequence,tensorflow.python.keras.preprocessing.text,tensorflow.python.keras.regularizers,tensorflow.python.keras.saving.model_config,tensorflow.python.keras.saving.save,tensorflow.python.keras.saving.saved_model_experimental,tensorflow.python.keras.utils.data_utils,tensorflow.python.keras.utils.generic_utils,tensorflow.python.keras.utils.io_utils,tensorflow.python.keras.utils.layer_utils,tensorflow.python.keras.utils.losses_utils,tensorflow.python.keras.utils.multi_gpu_utils,tensorflow.python.keras.utils.np_utils,tensorflow.python.keras.utils.vis_utils,tensorflow.python.keras.wrappers.scikit_learn --output_package=tensorflow.python.keras.api._v2 --use_relative_imports=True bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/activations/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/densenet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/efficientnet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/imagenet_utils/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/inception_resnet_v2/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/inception_v3/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/mobilenet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/mobilenet_v2/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/mobilenet_v3/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/nasnet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/resnet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/resnet_v2/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/resnet50/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/vgg16/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/vgg19/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/xception/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/backend/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/callbacks/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/callbacks/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/constraints/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/boston_housing/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/cifar10/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/cifar100/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/fashion_mnist/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/imdb/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/mnist/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/reuters/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/estimator/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/initializers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/layers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/layers/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/layers/experimental/preprocessing/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/losses/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/metrics/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/mixed_precision/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/mixed_precision/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/premade/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/models/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/optimizers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/optimizers/schedules/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/image/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/sequence/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/text/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/regularizers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/utils/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/utils/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/wrappers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/wrappers/scikit_learn/__init__.py')\r\nExecution platform: @local_execution_config_platform//:platform\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow6StatusC1ENS_5error4CodeEN4absl14lts_2020_09_2311string_viewEOSt6vectorINS_10StackFrameESaIS7_EE\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 26, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/eager/context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow6StatusC1ENS_5error4CodeEN4absl14lts_2020_09_2311string_viewEOSt6vectorINS_10StackFrameESaIS7_EE\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```", "comments": ["Also tried on gcc 7.5.0, didn't work. I can't figure out how to install gcc-7.3.1 on ubuntu 20.04.", "Fixed. For anyone who comes across this later, I fixed it by deleting /root/.cache/bazel to force it to rebuild from scratch, and I used gcc 7.5.0. The problem seems to have been that even though I had aliased gcc to gcc 7.5.0, it was still using the old cached files from when I had previously built with gcc 10, and so it continued to encounter the errors from gcc 10.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48866\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48866\">No</a>\n"]}, {"number": 48865, "title": "Performance Issue: GTX 1080 Tensorflow training is stuck indefinitely | GPU Load 0%", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): _**Yes**_\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): _**Windows 10 Version 2004 Build 19041.928**_\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): _**source**_\r\n- TensorFlow version (use command below): _**Tensorflow = 2.4.0 / Keras = 2.4.3**_\r\n- Python version: _**3.6.5 64-bit (virtualenv)**_\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: _**CUDA = 11.0.194 / CuDNN = 8.0.4**_  | **Nvidia Driver = 460.79**\r\n- GPU model and memory: **_Nvidia GTX 1080 / 8GB Memory / 320.3 GBs Bandwitdh_**\r\n- Other System Specs _**: Intel i5-3570K @4.1GHz | 16GB DDR3 RAM @1360Mhz | 5400RPM HDD (where the code is executed from)**_\r\n- IDE:  **_Visual Studio Code 1.55.2_**\r\n\r\n\r\n**Describe the current behavior**\r\n_Running a custom code to train a BERT transformer on sentimenent analysis traning dataset.\r\nBatch Size = 8\r\nSequence Length = 128\r\nDense Layers = 1 with 512 neurons_\r\n\r\n_The GPU VRAM fills up to around 7.5GB without any OOM, however the GPU Load is 0~1%, and a CPU load is fluctuating between 60% ~ 100%\r\n_The 1st training Epoch is stuck there indefinitely, without any progress in the training progress bar._\r\n_If I trained using the CPU, it is slow, but there is a progress and it is not stuck.(CPU Load 100% all the time)_\r\n\r\n**Describe the expected behavior**\r\n\r\n_Running another non-custom code brought from github to test if the GPU is working or not,\r\nAfter executing it, the GPU VRAM is filled at around 7.5GB with a GPU load of 22% to 27%.\r\nI am expecting a similar behavior with the custom code._\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n\r\n\r\n- BERT.ipynb (The BERT Transformer) <-- The code that exhibits the current behaviour.\r\n- cnn_tutorial.ipynb <-- A code that exhibits an expected behaviour to ensure TF is running on GPU.\r\n- train.tsv <-- Training dataset for the custom code.\r\n[Attachments.zip](https://github.com/tensorflow/tensorflow/files/6409674/Attachments.zip)\r\n\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached. **_(None)_**\r\n@bhack ", "comments": ["@MuSamiNaf \r\nI am unable to access the code shared, can you please share a colab gist with the issue reported, also please try on tf 2.5rc1 and let us know if you face the same issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 48864, "title": "tf.compat.v1.flags.EnumClassSerializer throws error on Windows", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): TensorFlow 2.4.1\r\n- Python version: 3.7.5\r\n- CUDA/cuDNN version: N/a\r\n- GPU model and memory: N/a\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\ntf.compat.v1.flags.EnumClassSerializer\r\n```\r\n**Outputs:**\r\n```\r\nAttributeError: module 'tensorflow.python.platform.flags' has no attribute 'EnumClassSerializer'\r\n```\r\n\r\n**Describe the current behavior**\r\nThrows AttributeError. The same problem occurs when I try to use the following APIs:\r\n - tf.compat.v1.app.flags.EnumClassListSerializer\r\n - tf.compat.v1.app.flags.EnumClassSerializer\r\n - tf.compat.v1.flags.EnumClassListSerializer\r\n\r\n**Describe the expected behavior**\r\nI can use these APIs on Linux. Expect them to work on Windows as well.\r\n", "comments": ["@lugalUrim ,\r\n\r\nI was able to run the given code without any errors.Please find the [gist](https://colab.research.google.com/gist/tilakrayal/60efd73f0e5bd11e7ba4e35e74e56932/48864.ipynb) here. Could you please try to execute the code in colab from your side and let us know if you are facing the issue again.Also, please refer the [link](https://www.tensorflow.org/api_docs/python/tf/compat/v1/flags/EnumClassSerializer) for more info.\r\n\r\nThanks!", "@lugalUrim Can you check if is it solved on your Windows system with TensorFlow 2.5.0.rc2? \r\nIt seems running without problem under Wine:\r\n\r\n```\r\ndocker run -it --rm tobix/pywine bash -c \"wine python -m pip install tensorflow-cpu==2.5.0.rc2 && wine python -c 'import tensorflow as tf; print(tf.__version__); tf.compat.v1.flags.EnumClassSerializer'\"\r\n```", "/cc @nikitamaia ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48864\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48864\">No</a>\n"]}, {"number": 48863, "title": "Stateful ConvLSTM2D reset_states() throws error", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Windows 10 (10.0.19042)\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: v2.4.0-49-g85c8b2a817f, 2.4.1\r\n- Python version: 3.7.7\r\n- CUDA/cuDNN version: 11.0/8.0\r\n- GPU model and memory: GeForce GTX 1080 Ti\r\n\r\n**Describe the current behavior**\r\n\r\nThe `reset_states()` method of the `ConvolutionalLSTM2D` layer throws a TypeError if called with the default argument.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe `reset_states()` method of the `ConvolutionalLSTM2D` layer should reset its states if the method is called with the default argument.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nl = tf.keras.layers.ConvLSTM2D(32, kernel_size=3, strides=1, padding='same', stateful=True, batch_size=32)\r\nl.reset_states()\r\n```\r\n\r\n**Other info / logs** \r\n\r\nRelevant part of the stack trace: \r\n```\r\nFile \"...\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional_recurrent.py\", line 357, in reset_states\r\n    state_shape = self.compute_output_shape(input_shape)\r\n  File \"...\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\", line 272, in wrapper\r\n    output_shape = fn(instance, input_shape)\r\n  File \"...\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional_recurrent.py\", line 193, in compute_output_shape\r\n    rows = input_shape[2]\r\nTypeError: 'NoneType' object is not subscriptable\r\n\r\nProcess finished with exit code 1\r\n```\r\n\r\n", "comments": ["Was able to reproduce the issue in TF 2.4 and Nightly versions.Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/bdf24c8618cfd44fcfe0f18e0cda8ae4/-48863.ipynb#scrollTo=k-pQruNuQ-Hp).", "Was able to replicate the issue with TF 2.6.0-dev20210606,please find the gist [here ](https://colab.research.google.com/gist/sushreebarsa/4a7347ca07a84514bd322a5dd728d1c3/untitled237.ipynb)..Thanks !", "Was able to replicate the issue with TF 2.6, please find the gist [**`here`**](https://colab.research.google.com/gist/kumariko/8bb2b739f5caf9a2fd2fe7351d95dd80/untitled237.ipynb#scrollTo=S9uBhSV1fLvo) . Thanks !", "@rschiewer Could you please refer to this [thread](https://stackoverflow.com/questions/42763928/how-to-use-model-reset-states-in-keras) and let us know if it helps?\r\nThanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48863\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48863\">No</a>\n"]}, {"number": 48862, "title": "Optimizer that is wrapped with two different LossScaleOptimizer throws a error.", "body": "**System information**\r\n- OS: colab (Ubuntu 18.04.5)\r\n- Where TensorFlow installed from : pre-installed in colab\r\n- TensorFlow version: 2.4.1\r\n- Python version: 3.7.10\r\n- GPU model and memory: Tesla K80, 12GB\r\n\r\n**The current behavior**\r\n\r\nWhen using tf.keras.Optimizer that is wrapped with two different `tf.keras.mixed_precision.LossScaleOptimizer`, the optimizer raises a error below.\r\n\r\n```\r\nValueError: Called Trackable._track_trackable() with name='loss_scale', but a Trackable with this name is already declared as a dependency. Names must be unique (or overwrite=True).\r\n```\r\n\r\n**The expected behavior**\r\n\r\nI believe that the error should be NOT occurred.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n\u2022 The code snippet to reproduce the error.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\noptimizer = tf.optimizers.RMSprop()\r\nwrapped_optimizer_1 = tf.keras.mixed_precision.LossScaleOptimizer(optimizer) # There is no error.\r\nwrapped_optimizer_2 = tf.keras.mixed_precision.LossScaleOptimizer(optimizer) # The error occurred.\r\n```\r\n\r\n\u2022 The error\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-1-efa101fb525d> in <module>()\r\n      3 optimizer = tf.optimizers.RMSprop()\r\n      4 wrapped_optimizer_1 = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\r\n----> 5 wrapped_optimizer_2 = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\r\n\r\n2 frames\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale_optimizer.py in __init__(self, inner_optimizer, dynamic, initial_scale, dynamic_growth_steps)\r\n    545       self._loss_scale = _DynamicLossScaleState(\r\n    546           initial_scale, dynamic_growth_steps, multiplier=2)\r\n--> 547       self._track_trackable(self._loss_scale, 'loss_scale')\r\n    548     else:\r\n    549       if initial_scale is None:\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale_optimizer.py in _track_trackable(self, trackable, name, overwrite)\r\n    154 \r\n    155   def _track_trackable(self, trackable, name, overwrite=False):  # pylint: disable=redefined-outer-name\r\n--> 156     return self._trackable._track_trackable(trackable, name, overwrite)\r\n    157 \r\n    158   def _handle_deferred_dependencies(self, name, trackable):  # pylint: disable=redefined-outer-name\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py in _track_trackable(self, trackable, name, overwrite)\r\n    897             (\"Called Trackable._track_trackable() with name='%s', \"\r\n    898              \"but a Trackable with this name is already declared as a \"\r\n--> 899              \"dependency. Names must be unique (or overwrite=True).\") % (name,))\r\n    900       # This is a weird thing to do, but we're not going to stop people from\r\n    901       # using __setattr__.\r\n\r\nValueError: Called Trackable._track_trackable() with name='loss_scale', but a Trackable with this name is already declared as a dependency. Names must be unique (or overwrite=True).\r\n```\r\n\r\n**Other info / logs**\r\n\r\nN/A\r\n\r\nThanks!", "comments": ["@keisen \r\nAs per the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/LossScaleOptimizer?version=nightly#hyperparameters) Object (saved)name should be unique for more info please find the [link](https://colab.research.google.com/gist/UsharaniPagadala/ed3328ee33ac81c36048ace6593af3cd/-48862.ipynb) .Thanks", "@UsharaniPagadala \r\n\r\n> As per the documentation\r\n\r\nWhere does it say that?\r\nWould you be more specific about where it is?\r\n\r\n> As per the documentation Object (saved)name should be UNIQUE for more info please find the LINK \r\n\r\nWhat is `Object` you said?\r\nIf you said `Object` is `tf.keras.optimizers.RMSprop` instance, why there are no error when we use multiple RMSprop instances (whose name are all default value 'RMSprop') in model training?\r\n\r\n> ValueError: Called Trackable._track_trackable() with name='loss_scale', but a Trackable with this name is already declared as a dependency. Names must be unique (or overwrite=True).\r\n\r\nWhat named it 'loss_scale'?\r\nI opened this issue because I thought the naming logic in `LossScaleOptimizer` is wrong.\r\n\r\n\r\nThanks.", "@keisen,\r\nCan you please let us know why do you want to wrap the **`an Optimizer`** with two **`tf.keras.mixed_precision.LossScaleOptimizer`** instances? Thanks!", "> Can you please let us know why do you want to wrap the an Optimizer with two tf.keras.mixed_precision.LossScaleOptimizer instances?\r\n\r\n@rmccorm4 , Thank you for your response.\r\n\r\nI know it's only natural but, we do NOT want to meaninglessly wrap the one with multiple LossScaleOptimizers.\r\nWe want to resolve this issue for our library users.\r\nWe've been developing [a library](https://github.com/keisen/tf-keras-vis) that analyze the model, the code below is its I/F.\r\n\r\nhttps://github.com/keisen/tf-keras-vis/blob/ddd951396f16e7f5b7a0e8619f43f99c599628fb/tf_keras_vis/activation_maximization.py#L12-L20\r\n\r\nAs above, our library's I/F accepts an optimizer instance as necessary, that's, when our user does NOT specify it, the `optimizer` will be default value (`tf.optimizers.RMSprop(1., 0.95)`).\r\nSo users will face the error in two situation as follow:\r\n\r\n1. Call the API twice without specifying `optimizer`.\r\n\r\n```\r\nlibrary_instance = ActivationMaximization(...)\r\nresults = library_instance(...)\r\n# Do something\r\nresults = library_instance(...) # The error occurred!\r\n```\r\n\r\n2. Call the API twice with an optimizer instance which is created by user, such below:\r\n\r\n```\r\noptimizer = tf.optimizers.RMSprop()\r\n\r\nlibrary_instance = ActivationMaximization(...)\r\nresults = library_instance(..., optimizer=optimizer)\r\n# Do something\r\nresults = library_instance(..., optimizer=optimizer) # The error occurred!\r\n```\r\n\r\nWe want to avoid facing the error.\r\nLet me get this straight.\r\n\r\n1. Although we've used the I/F above ever since Tensorflow1.x, we've never faced an error about the uniqueness of optimizer .\r\n1. This problem affects various users that deal with optimizers wrapped by LossScaleOptimizers, like above.\r\n\r\nFor the above reasons, \u00a0I suspect that this behavior is a specification bug of LossScaleOptimizer.\r\nBut even if it were an official\u00a0specification ...\r\n\r\n1. I could NOT find the explanation that makes us understand this phenomenon clearly in Tensorflow documentations.\r\n1. It is too hard to identify the cause of it ,at least, from the error message below. Actually, I took a long time to find it.\r\n\r\n```\r\nValueError: Called Trackable._track_trackable() with name='loss_scale', but a Trackable with this name is already declared as a dependency. Names must be unique (or overwrite=True).\r\n```\r\n\r\nSo I'd like to request an appropriate error message and enough documentation.\r\n\r\nThanks!", "I believe @rmothukuru was supposed to be pinged above instead.", "@rmothukuru , @rmccorm4 , I sincerely apologize for the mistake!", "The error message was improved in 57caac46d0f036754f8f093fd40099eaaf6c06c2 (which will be in TF 2.6) but the issue remains unfixed. \r\n\r\nCan you clarify what the use case is? I'm assuming \"I/F\" means \"interface\". Is the issue that `library_instance` wraps the optimizer with a `LossScaleOptimizer`, which causes this error when `library_instance` is called twice??", "Hi, @reedwm .\r\n\r\n> The error message was improved in 57caac4 (which will be in TF 2.6) but the issue remains unfixed.\r\n\r\nThank you so much for the improvement!\r\n\r\n\r\n> Can you clarify what the use case is? I'm assuming \"I/F\" means \"interface\". Is the issue that library_instance wraps the optimizer with a LossScaleOptimizer, which causes this error when library_instance is called twice??\r\n\r\nI have something to tell you first.\r\nWhen I created this issue, I thought there are scenarios many more that has this problem.\r\nHowever, now, I notice that almost of them can be avoided by redesigning our library's interface. That's, We should NOT define the interface that defalult value is a optimizer instance.\r\n\r\nAs a result, the scenario that needs to reuse the same optimizer instance is only on when our users use them on jupyter notebook or colab interactively , such below.\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tf_keras_vis.activation_maximization import ActivationMaximization as OurLibrary\r\n\r\n# Setting mixed-precisoin\r\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\r\n\r\n# Create library_instance\r\nmodel = tf.keras.applications.vgg16.VGG16(weights='imagenet', include_top=True)\r\nlibrary_instance = OurLibrary(model)\r\n\r\n# Create optimizer instance\r\noptimizer = tf.keras.optimizers.RMSprop()\r\n\r\n# First call\r\nactivations = library_instance(score=lambda x: x[:, 20],\r\n                               seed_input=None,\r\n                               optimizer=optimizer)\r\n\r\n# To do thing...\r\n# Check results, but it was not calculated enough.\r\n# So we decided to continue the calculation with the result of first call as seed_input value.\r\n\r\n# But the error occurred!\r\nactivations = library_instance(score=lambda x: x[:, 20],\r\n                               seed_input=activations,\r\n                               optimizer=optimizer)\r\n```\r\n\r\n\r\nI'm sorry for the late reply.\r\nThanks!", "Unfortunately, supporting this is very difficult, so this will probably not get fixed. I will update the documentation.\r\n\r\nI highly recommend having the user pass a `LossScaleOptimizer` to `library_instance` instead of a normal optimizer. In general, it's a good idea to wrap the optimizer with a `LossScaleOptimizer` shortly after creating it. Alternatively, if you want `library_instance` to wrap with a LossScaleOptimizer and allow it to be called multiple times, you can maintain a `dict` (or `WeakKeyDictionary`) from optimizer to LossScaleOptimizer, and reuse the LossScaleOptimizer if the same optimizer is passed multiple times.\r\n\r\nTechnical details ahead (no need to read this): The reason this is so difficult to implement is because LossScaleOptimizer emulates the checkpoint format of the inner optimizer. This means if a checkpoint is saved with a LossScaleOptimizer, the checkpoint format is exactly the same as if it were saved with the inner optimizer, except the optimizer has a dependency on the loss scale. However, if two LossScaleOptimizers wrap the same inner_optimizer, then each will try to create a trackable dependency from the inner_optimizer to the loss scale (with name \"loss_scale\"), giving an error since an object cannot two dependencies with the same name.", "@reedwm , Thank you for your reply!\r\n\r\n\r\n> Unfortunately, supporting this is very difficult, so this will probably not get fixed. I will update the documentation.\r\n\r\n>if two LossScaleOptimizers wrap the same inner_optimizer, then each will try to create a trackable dependency from the inner_optimizer to the loss scale (with name \"loss_scale\"), giving an error since an object cannot two dependencies with the same name.\r\n\r\nThat's too bad but I understand.\r\n\r\n\r\n> I highly recommend having the user pass a LossScaleOptimizer to library_instance instead of a normal optimizer. In general, it's a good idea to wrap the optimizer with a LossScaleOptimizer shortly after creating it.\r\n\r\nYou're right. However we don't want our users to have check whether the visualization target model has a low-precision variable or not. In the scenario below, UserB doesn't know that he needs to wrap a optimizer with LSO if UserA does NOT tell him that the model was saved in a mixed-precision environment or not.\r\n\r\n1. UserA implemented and trained a model, and then saved it into xxx.h5 file.\r\n2. UserA publishes it, and UserB finds and downloads it.\r\n3. UserB loads the model and visualizes it with `library_instance`.\r\n\r\nBecause the `library_instance` uses the outputs of intermediate layer of the model, to know it, I believe that our users have to do like below:\r\n\r\n```python\r\nfor layer in model.layers:\r\n\u00a0 \u00a0 if (layer.variable_dtype != layer.compute_dtype) and\r\n\u00a0 \u00a0 \u00a0 \u00a0 (layer.compute_dtype in [tf.float16, tf.bfloat16]):\r\n\u00a0 \u00a0 \u00a0 \u00a0 # TODO wrap a optimizer with LSO\r\n\u00a0 \u00a0 \u00a0 \u00a0 break\r\n```\r\n\r\nSo if we're able to use LSO instance regardless if the model contains lower-precision variables or not, we're so happy to adopt the way you recommended. (That's, we will re-define the specification that our users need to pass the optimizer wrapped with LSO when they aren't confident whether the model is float32-precision.)\r\nIf not, I believe, it's too verbose to have our users implement it in every using `library_instance`.\r\n\r\nSo, I have two questions below:\r\n1. Is there no problem when we apply LSO to calculation of gradients of the float32-precision model?\r\n2. If not, is it possible to fix it ?\r\n\r\n\r\nThanks!", "> Is there no problem when we apply LSO to calculation of gradients of the float32-precision model?\r\n\r\nThis won't cause issues, although I still don't recommend it. LSO is typically only used for mixed precision models, and may cause confusion is used for float32 models. If an LSO is used, the dynamic loss scale will be raised to a very high value, which won't cause issues but will cause confusion if a user queries the loss scale.\r\n\r\nAlso, LSO is not needed for bfloat16, and there is no need to check if the variable dtype and compute dtype differ, as the variable dtype should always be float32 if the compute dtype is float16. Therefore, the check can be simplified to be `layer.compute_dtype == tf.float16` for each layer. Granted, I understand you still don't want users to have to write that check. Perhaps you could have a `model_uses_mixed_precision` or `maybe_wrap_with_lso` function that does the check for them. Or, in `library_instance`, you can cache the LSO in a `dict` mapping from inner_optimizer to LSO, as I suggested in my previous comment.\r\n\r\n> If not, is it possible to fix it ?\r\n\r\nI think this is theoretically possible but would add a lot of complexity, so I don't think it is worth implementing.\r\n\r\nTechnical details follow where I describe how this could theoretically be implemented (no need to read this): Currently, the inner_optimizer has a trackable dependency named \"loss_scale\" on the loss scale. Since the inner_optimizer cannot have multiple dependencies with the same name, we cannot have multiple LSOs wrap the inner optimizer. However, we could instead have the inner_optimizer have a dependency on a list of loss_scales, one per LSO that wraps it. Each LSO would have an index into this list (but this index would not be saved in the checkpoint).\r\n\r\nThis approach would be difficult to implement and add a lot of complexity. I'm also not sure if it's possible, as there might be something I didn't consider that prevents this from working. So I don't think it is worth implementing this.\r\n", "@reedwm , Thank you for your reply and kind instruction!\r\n\r\n\r\n> LSO is not needed for bfloat16, and there is no need to check if the variable dtype and compute dtype differ, as the variable dtype should always be float32 if the compute dtype is float16.\r\n\r\nThat's good news for us!\r\nAs you said, we will provide a function to check model's variables.\r\n\r\n> This approach would be difficult to implement and add a lot of complexity. I'm also not sure if it's possible, as there might be something I didn't consider that prevents this from working. So I don't think it is worth implementing this.\r\n\r\nI understood why the approach should NOT be implemented. I agree with your opinion.\r\n\r\n\r\nThank you so much!\r\nBest regards.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48862\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48862\">No</a>\n"]}, {"number": 48861, "title": "@YonitZall", "body": "@YonitZall \r\ndid you get any solutions? I am facing the same problem\r\n\r\n_Originally posted by @apramanik62 in https://github.com/tensorflow/tensorflow/issues/29078#issuecomment-830577011_", "comments": ["@apramanik62 ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the The tesnorflow version,complete code and dataset to reproduce the issue.\r\n\r\nThanks!", "@tilakrayal \r\nThanks for your kind response . can you share me your email address for sharing necessary files?", "@apramanik62 ,\r\n\r\nPlease, share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "@tilakrayal \r\nbelow is the link \r\nhttps://1drv.ms/u/s!At88VZrdRtnbhHhyJor1fXyD_4Nc?e=rK2FJT\r\nkindly give your comments for the same.\r\ntensorflow version 1.15", "@apramanik62 ,\r\n\r\nLooks like this issue is related to tensorflow/models repo and similar issue is being tracked [here](https://github.com/tensorflow/models/issues/7636).\r\n\r\n", "@tilakrayal \r\nThanks for your response. I will look at the given link.\r\nCan you tell me how did figure out that? is there any wrong file in TensorFlow 1.15 \r\n", "@apramanik62 ,\r\n\r\nTensorFlow 1.x is not actively supported. Could you please update TensorFlow v2.x and also as mentioned please follow similar issue [here](https://github.com/tensorflow/models/issues/7636).\r\n\r\nThanks!", "ok I will check on TF 2.x\r\nThanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48861\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48861\">No</a>\n"]}, {"number": 48860, "title": "[Mixed_precision] Model#__call__() behavior is different when using tf.Tensor input and when using tf.Variable input.", "body": "**System information**\r\n- OS: colab (Ubuntu 18.04.5)\r\n- Where TensorFlow installed from : pre-installed in colab\r\n- TensorFlow version: 2.4.1\r\n- Python version: 3.7.10\r\n- GPU model and memory: Tesla K80, 12GB\r\n\r\n**The current behavior**\r\n\r\nOn default (`float32`) policy environment, I have to load and use a keras model that is loaded from a `h5` file that was created with `mixed_float16` policy.\r\nWhen using `tf.Variable` instead of `tf.Tensor` as input value to model, a error bellow is occurred.\r\n\r\n```\r\nValueError: Incompatible type conversion requested to type 'float32' for AutoCastVariable which is casted to type 'float16'\r\n```\r\n\r\nI want to calculate the gradient with respect to the input value and update the input value itself iteratively.\r\nTo do so, the input value needs to be `tf.Variable`.\r\n\r\n**The expected behavior**\r\n\r\nI believe that the behavior of both (tf.Tensor and tf.Variable) should be same, that's, the error should be NOT occurred.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nThe steps for reproducing:\r\n\r\n1. Set mixed_float16 policy\r\n1. Create a model file\r\n1. Set float32 (default) policy\r\n1. Load model and When using tf.Tensor for input of model that is loaded from h5 file, Model#__call__() will NOT throw the error.\r\n1. Load model and When using tf.Valiable for input of model that is loaded from h5 file,Model#__call__() will throw the error.\r\n\r\nI've uploaded a notebook to reproduce the error:\r\n\r\nhttps://gist.github.com/keisen/087713c2e547b05a6867506ea5402b93\r\n\r\n**Other info / logs**\r\n\r\nN/A\r\n\r\n\r\nThanks!\r\n\r\n", "comments": ["Was able to reproduce the issue in TF 2.4 and Nightly versions. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/c42138712e61aa3fcfb8c6253f372681/error-when-calling-model-with-tf_variables.ipynb). Thanks!", "@keisen,\r\nAFAIK, this behavior is expected because **`Tensorflow Variables`** are used when the values are not constant and if they change continuously, like the **`Weights`** in a **`Model`**. They are not used for **`Inputs`**  to the **`Model`** because **`Inputs`** to a **`Model`** is a **`Tensor`**, not a **`Variable`**.\r\nWhat do you think? ", "@rmothukuru , Thank you for your response!\r\n\r\nI've been implementing a library ([tf-keras-vis](https://github.com/keisen/tf-keras-vis)) that provide [Model Feature Visualization](https://distill.pub/2017/feature-visualization/#:~:text=Feature%20visualization%20answers%20questions%20about,looking%20for%20by%20generating%20examples.&text=As%20a%20young%20field%2C%20neural,not%20yet%20have%20standardized%20terminology). \r\nThis algorism need to calculate the gradient with respect to the input value and to update the input value itself iteratively.\r\nI believe, to do so, the input value needs to be `tf.Variable`, not `tf.constant`.\r\n\r\nSince version 1.x, Tensorflow has helped realize this algorism!\r\nSo I expect Tensorflow will help realize it even if it adopts mixed-precision mechanism.\r\n\r\nThanks!", "@keisen The idea still applies that the inputs to keras layers needs to be constant\r\nThere are multiple ways to achieve this, but the one that involves fewer codes is to have tf.Variable in that layer to track the inputs.\r\ni.e.\r\n```python\r\ndef call(self, inputs):\r\n        self.my_tf_var.assign(tf.cast(inputs,self.variable_dtype))\r\n        ...\r\n```\r\nAnd then, you can take the gradient with respect to this my_tf_var and applies updates to the appropriate tf.Variable that suits you purpose, maybe the variables in other layers. \r\n\r\nPlease see if this would help", "@laplacericky , Thank you for your reply.\r\n\r\nI didn't understand what you said. \r\nOur library receives user's model instance, and then calculates the features of it with gradients.\r\n\r\n> There are multiple ways to achieve this, but the one that involves fewer codes is to have tf.Variable in that layer to track the inputs.\r\n\r\nDo you mean that our library have to re-build our user's model and replace target layer to be containing a variable instance?, or we have to define Input layer class that have a tf.Variable attribute?\r\n\r\nIn any cast, I don't think it's a good idea.\r\nCould you explain that in a little more detail?\r\n\r\nThanks!", "@keisen I am just replying to your comment,\r\n\r\n> This algorism need to calculate the gradient with respect to the input value and to update the input value itself iteratively.\r\nI believe, to do so, the input value needs to be tf.Variable, not tf.constant.\r\n\r\nAnd actually tensorflow can get gradient with any possible tensors and I was just suggesting ways to achieve it. Please forget that if you find confusing. \r\n\r\nAfter viewing your gist, I guess this is what you want to do.\r\n```python\r\ninput_value = tf.Variable(tf.random.uniform((1, 224, 224, 3), dtype=tf.float32))\r\nmodel = tf.keras.models.load_model('model-with-mixed_float16-policy.h5')\r\nwith tf.GradientTape() as g:\r\n   Loss=objective_function(model(input_value.value()))\r\n\r\ndL_dinput_value=g.gradient(Loss,input_value)\r\n```\r\nThis is how you can get gradient with respect to the input_value while still enjoying mixed_precision speed up of the model, and, without error message. Then you can take the `dL_dinput_value` and apply update back to `input_value` which is a variable.", "@laplacericky , Thank you for your great reply!\r\n\r\n> This is how you can get gradient with respect to the input_value while still enjoying mixed_precision speed up of the model, and, without error message. Then you can take the dL_dinput_value and apply update back to input_value which is a variable.\r\n\r\nI have tried the way you suggested, and tested for  various situations, and then understood this is the best for us.\r\nTo be honest, at first, I thought that it was pretty tricky. However it has had no problem.\r\nThank you so much!\r\n\r\nBest regards.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48860\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48860\">No</a>\n", "@laplacericky , I reopened this because of facing a issue.\r\nIt's that the result of the method you proposed and the result of the method adopted by our library are different.\r\nPlease see the notebook below for details.\r\n\r\nhttps://gist.github.com/keisen/63724e83f843bc8585ec1c97d68b824e\r\n\r\nAssuming one of them is the wrong value, which it is?\r\nWe're investigating the cause, but we don't know that for now.\r\n\r\nThanks!", "@keisen It is because `tf.keras.optimizers.Adam()` is stateful by default. If a new instance of `tf.Variable` is passed into it every time, it cannot apply the correct stateful effect on the variable like moment estimate, etc, because it would be treated as if it was a separate and new variable. Therefore, the same instance of variable has to be passed into it. You can use `tf.keras.optimizers.SGD()` with zero momentum and the two versions of codes will give the same results.", "Thank you for your quick response!\r\nI didn't know that. You're amazing!\r\n\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48860\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48860\">No</a>\n"]}, {"number": 48859, "title": "[Intel MKL] Einsum threadpool fix", "body": "bugfix for Mkl_einsum_op.cc to compile without errors.", "comments": ["@penpornk  thank you! \r\n@gbaned, wondering when this be merged?\r\n"]}, {"number": 48858, "title": "Keras bidirectional lstm incorrect predictions", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution Linux Ubuntu 20.04):\r\n- TensorFlow installed from (source or binary): Binary & Source\r\n- TensorFlow version (use command below): 2.4 & 2.3.1\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): 3.1\r\n- CUDA/cuDNN version: 11 / 8\r\n- GPU model and memory: RTX Quadro 8000 / 48GB\r\n\r\nYou can also obtain the TensorFlow version with:\r\nI tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\r\nunknown 2.3.1\r\n\r\n**Describe the current behavior**\r\n\r\nI have a previous model written with the old Keras API before it was integrated into TensorFlow. We've been using this model, code and build setup for 2 years now no issues whatsoever. We decided to finally upgrade to TensorFlow 2. After upgrading we tried training. \r\n\r\nDuring training the model achieves the expected validation accuracy, but if we test the model after training it just spits out garbage values. For a sanity check we tested the model trained with the old version of Keras in TensorFlow 2 and it produces the correct output. Absolutely nothing has changed with the data or process. Only difference is the version of Keras and TensorFlow. We are stumped as to what the issue could be. The model is very simple, stacked bi-directional LSTMs. It also achieves the expected validation accuracy in training. \r\n\r\nInterestingly, this model is destined for CoreML, but exporting the model trained in TensorFlow 2 generates a larger model with significantly more layers than the previous. \r\n\r\n**Describe the expected behavior**\r\n\r\nThe trained model should produce the correct output.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nThis is the model.\r\n\r\n`\r\n\r\n        x = Input(shape=(length , feature_size,))\r\n        branch_1 = Bidirectional(LSTM(256, return_sequences=True))(x)\r\n        branch_1 = Bidirectional(LSTM(128, return_sequences=True))(branch_1)\r\n        branch_1 = Bidirectional(LSTM(64, return_sequences=True))(branch_1)\r\n        sequence = TimeDistributed(Dense(10, activation='softmax'), name='out')(branch_1)\r\n        temporal_model = Model([x], [sequence])`\r\n\r\n\r\n", "comments": ["@teaglin \r\n\r\nIn order to reproduce the issue reported here, could you please provide the complete code/colab gist along with the dependencies and the dataset you are using.Thanks!\r\n\r\n\r\n\r\n", "Looks like it was a hardware failure issue. Closing issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48858\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48858\">No</a>\n"]}, {"number": 48857, "title": "java.lang.UnsatisfiedLinkError: Failed to load native TensorFlow Lite methods.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy s5 min\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version: 'org.tensorflow:tensorflow-lite:2.4.0' + org.tensorflow:tensorflow-android:1.13.1'\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?: \r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nhello little question for a computer project in computer license I have to refactor an old application and make it modular. But here's when I build my generate signed bundle and I launch a feature on demand who use tensorflow on my android I have this error:\r\n\r\nFATAL EXCEPTION: Thread-9\r\nProcess: descartes.info.l3ak2.eyetrek, PID: 7275\r\njava.lang.UnsatisfiedLinkError: Failed to load native TensorFlow Lite methods. Check that the correct native libraries are present, and, if using a custom native library, have been properly loaded via System.loadLibrary():\r\njava.lang.UnsatisfiedLinkError: dalvik.system.PathClassLoader[DexPathList[[zip file \"/system/framework/android.test.runner.jar\", zip file \"/data/app/androidx.test.tools.crawler-gvh38WebyoKL_IpGVoIEuw==/base.apk\", zip file \"/data/app/descartes.info.l3ak2.eyetrek-h1-hReoVf26OAguR-8QZHw==/base.apk\", zip file \"/data/app/descartes.info.l3ak2.eyetrek-h1-hReoVf26OAguR-8QZHw==/split_config.xxhdpi.apk\", zip file \"/data/app/descartes.info.l3ak2.eyetrek-h1-hReoVf26OAguR-8QZHw==/split_moduleastronomie.apk\", zip file \"/data/app/descartes.info.l3ak2.eyetrek-h1-hReoVf26OAguR-8QZHw==/split_moduleastronomie.config.xxhdpi.apk\", zip file \"/data/app/descartes.info.l3ak2.eyetrek-h1-hReoVf26OAguR-8QZHw==/split_modulereconnaissancechantsoiseaux.apk\", zip file \"/data/app/descartes.info.l3ak2.eyetrek-h1-hReoVf26OAguR-8QZHw==/split_modulereconnaissancechantsoiseaux.config.xxhdpi.apk\", zip file \"/data/app/descartes.info.l3ak2.eyetrek-h1-hReoVf26OAguR-8QZHw==/split_modulereconnaissancefeuille.apk\", zip file \"/data/app/descartes.info.l3ak2.eyetrek-h1-hReoVf26OAguR-8QZHw==/split_modulereconnaissancefeuille.config.xxhdpi.apk\"],nativeLibraryDirectories=[/data/app/androidx.test.tools.crawler-gvh38WebyoKL_IpGVoIEuw==/lib/arm64, /data/app/descartes.info.l3ak2.eyetrek-h1-hReoVf26OAguR-8QZHw==/lib/arm64, /system/lib64, /vendor/lib64]]] couldn't find \"libtensorflowlite_jni.so\"\r\nat org.tensorflow.lite.TensorFlowLite.init(TensorFlowLite.java:80)\r\nat org.tensorflow.lite.NativeInterpreterWrapper.(NativeInterpreterWrapper.java:52)\r\nat org.tensorflow.lite.Interpreter.(Interpreter.java:277)\r\nat org.tensorflow.lite.Interpreter.(Interpreter.java:262)\r\nat descartes.info.l3ak2.eyetrek.module_reconnaissance_feuille.tensorflow.ImageClassifierFpasF.(ImageClassifierFpasF.java:102)\r\nat descartes.info.l3ak2.eyetrek.module_reconnaissance_feuille.fragment.FragmentScanFeuille$6.run(FragmentScanFeuille.java:1306)\r\nat java.lang.Thread.run(Thread.java:764)\r\n\r\nhow can I resolve this ?\r\n", "comments": ["@Duskley \r\nwould you share the below deatils to help you out the above problem,\r\n1- build script which use to compile android app?\r\n2- your handset cpu abi version(adb shell getprop ro.product.cpu.abi)?\r\n3- sdk and ndk version which you configure with tensorflow?", "> @Duskley\r\n> would you share the below deatils to help you out the above problem,\r\n> 1- build script which use to compile android app?\r\n> 2- your handset cpu abi version(adb shell getprop ro.product.cpu.abi)?\r\n> 3- sdk and ndk version which you configure with tensorflow?\r\n\r\nOf course, is that enough for you?\r\n\r\napply plugin: 'com.android.application'\r\n\r\nandroid {\r\n    compileSdkVersion 30\r\n    defaultConfig {\r\n        applicationId \"descartes.info.l3ak2.eyetrek\"\r\n        minSdkVersion 23\r\n        targetSdkVersion 30\r\n        versionCode 63\r\n        versionName \"6.0\"\r\n        testInstrumentationRunner 'androidx.test.runner.AndroidJUnitRunner'\r\n        renderscriptTargetApi 28 //must match target sdk and build tools\r\n        renderscriptSupportModeEnabled true\r\n\r\n        ndk {\r\n            abiFilters 'arm64-v8a,armeabi-v7a'\r\n        }\r\n         \r\n    }\r\n    lintOptions {\r\n        abortOnError false\r\n        checkReleaseBuilds false\r\n    }\r\n    buildTypes {\r\n        release {\r\n            minifyEnabled false\r\n            proguardFiles getDefaultProguardFile('proguard-android.txt'), 'proguard-rules.pro'\r\n        }\r\n        intermediaire {\r\n        }\r\n    }\r\n    aaptOptions {\r\n        noCompress \"tflite\"\r\n        noCompress \"lite\"\r\n    }\r\n    compileOptions {\r\n        sourceCompatibility JavaVersion.VERSION_1_8\r\n        targetCompatibility JavaVersion.VERSION_1_8\r\n    }\r\n    dataBinding {\r\n        enabled true\r\n    }\r\n\r\n    splits\r\n    {\r\n        density\r\n         {\r\n            enable false\r\n            exclude \"ldpi\", \"xxhdpi\", \"xxxhdpi\"\r\n            compatibleScreens 'small', 'normal', 'large', 'xlarge'\r\n         }\r\n    }\r\n\r\n    sourceSets { main { jni.srcDirs = ['src/main/jni', 'src/main/jniLibs/'] } }\r\n\r\n    buildToolsVersion = '28.0.3'\r\n\r\n    dynamicFeatures = [':features:moduleastronomie',\r\n                       ':features:modulereconnaissancechantsoiseaux',\r\n                       ':features:modulereconnaissancefeuille']\r\n}\r\n\r\ndependencies {\r\n    implementation fileTree(dir: \"libs\", include: [\"*.jar\"])\r\n\r\n    api 'org.tensorflow:tensorflow-lite:2.4.0'\r\n    api 'org.tensorflow:tensorflow-android:1.13.1'\r\n    api 'org.tensorflow:tensorflow-lite:2.4.0'\r\n    api 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly'\r\n    api project(':openCVLibrary3413')\r\n\r\n    implementation 'androidx.annotation:annotation:1.1.0'\r\n    api \"androidx.appcompat:appcompat:${versions.appcompat}\"\r\n    api 'androidx.constraintlayout:constraintlayout:2.0.4'\r\n    api 'com.google.android.material:material:1.3.0'\r\n    api \"com.google.android.play:core:${versions.playcore}\"\r\n    androidTestImplementation 'androidx.test.ext:junit:1.1.2'\r\n\r\n    implementation 'pl.droidsonroids.gif:android-gif-drawable:1.2.23'\r\n    implementation 'androidx.appcompat:appcompat:1.2.0'\r\n    implementation 'androidx.constraintlayout:constraintlayout:2.0.4'\r\n    implementation 'com.google.android.material:material:1.3.0'\r\n    implementation 'com.github.mmin18:realtimeblurview:1.2.1'\r\n    testImplementation 'junit:junit:4.13.2'\r\n    androidTestImplementation 'androidx.test.ext:junit:1.1.2'\r\n    androidTestImplementation 'androidx.test.espresso:espresso-core:3.3.0'\r\n    implementation 'com.squareup.retrofit2:retrofit:2.3.0'\r\n    implementation 'com.squareup.retrofit2:converter-gson:2.3.0'\r\n    implementation 'com.getkeepsafe.taptargetview:taptargetview:1.11.0'\r\n    implementation 'de.hdodenhof:circleimageview:1.2.1'\r\n    implementation 'net.danlew:android.joda:2.9.9.1'\r\n    implementation 'androidx.palette:palette:1.0.0'\r\n    implementation('com.github.bumptech.glide:glide:4.6.1')\r\n    {\r\n        exclude group: \"com.android.support\"\r\n    }\r\n    implementation 'androidx.fragment:fragment:1.3.1'\r\n    implementation 'life.knowledge4:k4l-video-trimmer:1.0'\r\n    implementation 'androidx.recyclerview:recyclerview:1.1.0'\r\n    implementation 'androidx.cardview:cardview:1.0.0'\r\n    implementation 'com.android.volley:volley:1.2.0'\r\n    implementation 'com.github.wendykierp:JTransforms:3.1'\r\n    implementation 'pl.droidsonroids.gif:android-gif-drawable:1.2.19'\r\n    implementation 'jp.wasabeef:blurry:3.0.0'\r\n}", "Could you also share your handset cpu abi version(adb shell getprop ro.product.cpu.abi)?", "> Could you also share your handset cpu abi version(adb shell getprop ro.product.cpu.abi)?\r\n\r\nI do not know enough Android studio how to do?\r\n", "Fyi, https://support.google.com/android/answer/7680439?hl=en", "> Fyi, https://support.google.com/android/answer/7680439?hl=en\r\n\r\nVersion android : 6.01\r\nSecurity patch level : ASKS v1.2 Release 161011 | SMR aug-2017 Release Q\r\nmodel number : SM-G800F\r\n", "> > Fyi, https://support.google.com/android/answer/7680439?hl=en\r\n> \r\n> Version android : 6.01\r\n> Security patch level : ASKS v1.2 Release 161011\r\n\r\nI specify I changed the software of my phone so that I can do the tests in android 6.0. The application must work at least in 6.0 (it is recorded on the specifications ^^) ", "@thaink could you take a look?", "I am still waiting for your answer. have you found a solution to my problem ^^ ?", "Instead of using the mixed version like the below one;\r\n\r\n```\r\napi 'org.tensorflow:tensorflow-lite:2.4.0'\r\napi 'org.tensorflow:tensorflow-android:1.13.1'\r\napi 'org.tensorflow:tensorflow-lite:2.4.0'\r\napi 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly'\r\n```\r\n\r\nUse the same version:\r\n\r\n```\r\n   // Import tflite dependencies\r\n    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly-SNAPSHOT'\r\n    // The GPU delegate library is optional. Depend on it as needed.\r\n    implementation 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly-SNAPSHOT'\r\n```", "> Instead of using the mixed version like the below one;\r\n> \r\n> ```\r\n> api 'org.tensorflow:tensorflow-lite:2.4.0'\r\n> api 'org.tensorflow:tensorflow-android:1.13.1'\r\n> api 'org.tensorflow:tensorflow-lite:2.4.0'\r\n> api 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly'\r\n> ```\r\n> \r\n> Use the same version:\r\n> \r\n> ```\r\n>    // Import tflite dependencies\r\n>     implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly-SNAPSHOT'\r\n>     // The GPU delegate library is optional. Depend on it as needed.\r\n>     implementation 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly-SNAPSHOT'\r\n> ```\r\n\r\nok i will try I'll let you know ^^\r\n", "> Instead of using the mixed version like the below one;\r\n> \r\n> ```\r\n> api 'org.tensorflow:tensorflow-lite:2.4.0'\r\n> api 'org.tensorflow:tensorflow-android:1.13.1'\r\n> api 'org.tensorflow:tensorflow-lite:2.4.0'\r\n> api 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly'\r\n> ```\r\n> \r\n> Use the same version:\r\n> \r\n> ```\r\n>    // Import tflite dependencies\r\n>     implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly-SNAPSHOT'\r\n>     // The GPU delegate library is optional. Depend on it as needed.\r\n>     implementation 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly-SNAPSHOT'\r\n> ```\r\n\r\nthe same error yet \r\n\r\ni try to put alone api 'org.tensorflow:tensorflow-lite:0.0.0-nightly-SNAPSHOT' in my app graddle (no work) and the same for api 'org.tensorflow:tensorflow-lite:2.4.0'\r\n\r\n", "@Duskley Does it work on emulator with the same Android version?\r\n", "> @Duskley Does it work on emulator with the same Android version?\r\n\r\nHello, yes it works very well. To switch from an aab format to an apk format I have to change the ndk filters. I'm going from arm64-v8a, armeabi-v7a to armeabi-v7a (My phone only supports armeabi-v7a). Does this crash have something to do with ndk filters?\r\n\r\nPS: I do not use an emulator because my pc cannot run an emulator without a bug. I use the Android option which allows me to launch an application directly on my phone when it is connected to my pc ", "Not sure but I suspect that or `sourceSets` might be the problem.\r\nCould you check if the .so file does exists in the apk and tests several option on these fields?", "> Not sure but I suspect that or `sourceSets` might be the problem.\r\n> Could you check if the .so file does exists in the apk and tests several option on these fields?\r\n\r\nHello, sorry for the late reply. I have checked in my apk and the libtensorflowlite.so file is present but in the aab file it cannot be found, so I think my problem is due to the libtensorflowlite.so file which is nonexistent in my aab file. The question now is \"is there a step to include .so files in the aab?\". I specify there are normally other .so which are present in my apk but which are not found in the aab \r\n\r\n**apk .so file** : \r\n\r\n![image](https://user-images.githubusercontent.com/65198558/117069352-a991c100-ad2c-11eb-8518-fc0698101a47.png)\r\n\r\nPS : in the aab file there is no libs folder is this a mistake?\r\nPS 2: when I put only one ndk filter the lib folder appears well with the .so files in it. How do I make the libs folder \"appear\" when there is more than one ndk filter? ", "I am not familiar with aab bundling. Hope this link could help: https://developer.android.com/studio/build/configure-apk-splits#configure-split", "> I am not familiar with aab bundling. Hope this link could help: https://developer.android.com/studio/build/configure-apk-splits#configure-split\r\n\r\nok no worries thank you very much i will make do with what you sent now that i know where the error is. I close the topic ^^ ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48857\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48857\">No</a>\n"]}, {"number": 48856, "title": "Issue 47269: Disallow inline optimizer to optimize away function when it has been decorated with 'experimental_implements'", "body": "Reference issue: https://github.com/tensorflow/tensorflow/issues/47269\r\n\r\nThe cause of the issue (i.e. that the 'experimental_implements' flag did not cause the PrepareCompositeFunctionsPass::ConvertTFImplements function to be called) was that the inline optimizer optimized away the function entirely, and thereby removing said flag.\r\nUnless I misunderstand the intended functionality, when a function has been decorated with that flag, it should remain entirely untouched, since it is intended to be transformed by ConvertTFImplements.\r\n\r\nTfhe fix in the PR checks for the  \"_implements\"  attribute in the function (which is what 'experimental_implements' gets transformed into) and returns as un-optimizable.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48856) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "Please add a regression test for the behavior you're fixing. Probably fine if it's more of an integration test involving the Lite converter if that's easier; potentially someone involved with the converter should review if so.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48856) for more info**.\n\n<!-- need_author_cla -->", "@allenlavoie I added the regression test. Note that I also added checks for when a function has been declared with experimental_implements, but it was not picked up by anything in the ConvertTFAPIImplements function(s). I don't see how that shouldn't throw an error.\r\nThe regression test exercises the string version of experimental_implements, and replaces a dummy function with embedding_matmul and checks for correct output.", "Neat. The change looks good to me, but I have limited familiarity with Lite. @abattery is probably a better reviewer, and may have more insight on the ConvertTFAPIImplements question.", "@allenlavoie we don't want to error out when there is no matching function with the attributes since we can just use the functions without fusing to run and no available fused function is not an error.", "@threebrooks Can you please check @abattery's comments and keep us posted ? Thanks!", "@gbaned @abattery I apologize if I misunderstood the current conversation, I was actually hoping for @abattery to give me further guidance for the wording used in those warning messages (see https://github.com/tensorflow/tensorflow/pull/48856#discussion_r632675166 ). I think once I have that wording I can amend the PR and we would be able to go on.", "@gbaned @abattery @allenlavoie Is there still interest in merging this PR? ", "@abattery, @allenlavoie Can you please assist on above comments from @threebrooks. Thanks!", "@abattery, @allenlavoie  Any update on this PR? Please. Thanks!", "@abattery, @allenlavoie  Any update on this PR? Please. Thanks!", "Closing the PR from my side."]}, {"number": 48854, "title": "add go_package to proto definition files", "body": "In TF 2.5.0, protoc-gen-go errors on a select set of proto definition files: `unable to determine import path`.  This PR fixes this issue by declaring the go_package in these definitions.  See also #17262. ", "comments": []}, {"number": 48853, "title": "Recommenders item vs item.", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue: https://www.tensorflow.org/recommenders/examples/basic_ranking\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing): Right now everything is pretty much user to item, I was wondering how can I use tensorflow in order to get an item vs item while accomodating the user behavior? Right now we are passing in the user to predict what are the recommended item for the user, Is there a way I can pass in an item and recommend me similar items?\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful? Content-based filtering.\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly? Yes\r\n\r\n### Returns defined\r\n\r\nAre return values defined? Yes\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\nNo\r\n\r\n### Usage example\r\n\r\nIs there a usage example? In - https://www.tensorflow.org/recommenders/examples/basic_retrieval#making_predictions\r\nHoping to do \r\n_, titles = index(tf.constant([\"Bridges of Madison County, The (1995)\"]))\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@navalta3030 \r\nIs this still an issue, could you please check on the latest tf version and let us know.", "This isn't really a bug.  this is more like a question for stack-overflow question.\r\n\r\n>  Is there a way I can pass in an item and recommend me similar items?\r\n\r\nYes, you can do it, give it a shot. But we can't maintain a tutorial for every use case."]}, {"number": 48852, "title": "Use of TensorFlow trademark in product name", "body": "After reading the licensing doc and the TM usage guidelines I am still unclear: can the TensorFlow TM be used in a product name, provided that the attributions, as described in the TM guide, are used in the product description.  As an example:\r\n\r\n\"xxxxx for Machine Learning - Includes Installed TensorFlow Distribution\"\r\n\r\nThanks in advance.\r\n", "comments": ["Gotta follow the brand guidelines... \r\nhttps://www.tensorflow.org/extras/tensorflow_brand_guidelines.pdf", "That's the doc I referenced initially.  Unfortunately, it seems to be more of a style guide; we'd like to make sure our use is legal/acceptable.", "@77blackbird ,\r\n\r\nCan you please provide more information/details to understand the issue.Thanks!", "@tilakrayal Hi and thank you for the follow-up.\r\n\r\nOur proposed use is packaging this as part of a kit that would include a number of components, including a SD Card or SSD pre-loaded with TensorFlow (we'd also include the TPU co-processor as part of the package).  We do not intend to use the logo, or other branding, only the TM \"TensorFlow\" (we will use branding, if so required).  TensorFlow would be part of the product name, as indicated in the OP, and referenced in the short product description.\r\n\r\nThanks again!", "@77blackbird please try https://github.com/tensorflow/tensorflow/issues/48413#issuecomment-817941455", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 48851, "title": "tf.function traces twice when creating Variables in the first call?", "body": "I have struggled to understand this strange behavior when using `@tf.function`. I have the following code:\r\n```python\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # This prevents tensorflow from print WARNING messages\r\nimport tensorflow as tf\r\n\r\n\r\nclass Count:\r\n    def __init__(self):\r\n        self.count = tf.Variable(0)\r\n\r\n    @tf.function\r\n    def increment(self):\r\n        print(\"This function is tracing !\")\r\n        tf.print(\"This function is executing !\")\r\n        return self.count.assign_add(1)\r\n\r\ncounter = Count()\r\ncounter.increment()\r\n```\r\nAccording to [this guide](https://www.tensorflow.org/guide/function#tracing), the above code will first build a `tf. Graph` (**tracing**) and then execute the built `tf. Graph`. When running the above code, I get the expected result as below:\r\n```\r\nThis function is tracing!\r\nThis function is executing!\r\n```\r\nHowever, if I try to create the variable `self. count` inside the function decorated with `@tf.function` (`increment` in this case), the function gets traced twice:\r\n\r\n*Code:*\r\n```python\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # This prevents tensorflow from print WARNING messages\r\nimport tensorflow as tf\r\n\r\n\r\nclass Count:\r\n    def __init__(self):\r\n        self.count = None\r\n\r\n    @tf.function\r\n    def __call__(self):\r\n        print(\"This function is tracing !\")\r\n        tf.print(\"This function is executing !\")\r\n        if self.count is None:\r\n            self.count = tf.Variable(0)\r\n        return self.count.assign_add(1)\r\n\r\ncounter = Count()\r\ncounter()\r\n```\r\n*Output:*\r\n```\r\nThis function is tracing!\r\nThis function is tracing!\r\nThis function is executing!\r\n```\r\n\r\nThere is another closely related question, which is [this](https://stackoverflow.com/questions/65323657/why-tf-function-traces-layers-twice). But none of the answers give a clear explanation to my question. I am currently using Python 3.8.5 and TensorFlow 2.4.1\r\n\r\n\r\n", "comments": ["Can you try with\r\n\r\n```python\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # This prevents tensorflow from print WARNING messages\r\nimport tensorflow as tf\r\n\r\n\r\nclass Count:\r\n    def __init__(self):\r\n        self.count = None\r\n\r\n    @tf.function\r\n    def __call__(self):\r\n        print(\"This function is tracing !\")\r\n        tf.print(\"This function is executing !\")\r\n        if self.count is None:\r\n            self.count = 0\r\n        self.count += 1\r\n        return self.count\r\n\r\ncounter = Count()\r\nprint(counter().numpy())\r\nprint(counter.__call__.pretty_printed_concrete_signatures())\r\n\r\n```\r\n\r\nThis will trace only once but a second counter() call will print always 1.\r\nThis is a known behavior/limit. You can see more details in a similar example at https://www.tensorflow.org/guide/function#depending_on_python_objects", "For your example it will need to re-valuate `self.count` when you assign. You can check yourself run with commented and uncommented `#self.count = tf.Variable(0)`\r\n\r\n```python\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # This prevents tensorflow from print WARNING messages\r\nimport tensorflow as tf\r\n\r\n\r\nclass Count:\r\n    def __init__(self):\r\n        self.count = None\r\n\r\n    @tf.function\r\n    def __call__(self):\r\n        print(\"This function is tracing !\")\r\n        tf.print(\"This function is executing !\")\r\n        if self.count is None:\r\n            print(\"True\")\r\n            #self.count = tf.Variable(0)\r\n        else:\r\n          print(\"False\")\r\n        return 1\r\n\r\ncounter = Count()\r\ncounter()\r\n```", "> This will trace only once but a second counter() call will print always 1.\r\n> This is a known behavior/limit. You can see more details in a similar example at > https://www.tensorflow.org/guide/function#depending_on_python_objects\r\n\r\nI have read this guide and I understand why second counter() always print 1, but that's not my question. My question is **why are there 2 tracings with only one call to `__call__`?**, not **why is there only one tracing with 2 calls to `__call__`?**. Your answer seems to be for the latter question.\r\n\r\n> For your example it will need to re-valuate `self.count` when you assign.\r\n\r\nSince I am asking for **tracing** behavior of `@tf.function`, I do not get what you mean by this explanation. It seems irrelevant.\r\n\r\nI ask this question because I had a problem when training my Keras model. Class like `tf.keras.layers.Dense` defers the creation of Variables until the first call to it. And as I mentioned, the creation of Variables inside a function decorated with `@tf.function` cause that function to trace twice. Hence, my Keras model always traces twice in the first training step, which consumes a lot of time when the model is large. Below code is an example code:\r\n\r\n**Code**\r\n\r\n```python\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # This prevents tensorflow from print WARNING messages\r\nimport tensorflow as tf\r\n\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self, **kwargs):\r\n        super(MyModel, self).__init__(**kwargs)\r\n        self.dense = tf.keras.layers.Dense(10)\r\n\r\n    def call(self, inputs):\r\n        return self.dense(inputs)\r\n\r\nmodel = MyModel()\r\n\r\n@tf.function\r\ndef train_step(sample):\r\n    print(\"This function is tracing !\")\r\n    model(sample)\r\n\r\ntensor = tf.random.normal([128])\r\ndataset = tf.data.Dataset.from_tensors(tensor).repeat(16).batch(4)\r\n\r\nsample = next(iter(dataset))\r\ntrain_step(sample)\r\n\r\nprint(\"-----------------------------------------------\")\r\nprint(train_step.pretty_printed_concrete_signatures())\r\n```\r\n**Output**\r\n```\r\nThis function is tracing !\r\nThis function is tracing !\r\n-----------------------------------------------\r\ntrain_step(sample)\r\n  Args:\r\n    sample: float32 Tensor, shape=(4, 128)\r\n  Returns:\r\n    NoneTensorSpec()\r\n\r\ntrain_step(sample)\r\n  Args:\r\n    sample: float32 Tensor, shape=(4, 128)\r\n  Returns:\r\n    NoneTensorSpec()\r\n```\r\n\r\nAs you can see, the two-tracings behavior of `@tf.function` results in two `ConcreteFunction` with exactly same input signatures. According to my understand, it should have only one `ConcreteFunction` (one computation graph) for each input signature. Is this behavior expected or not? If it is expected, what is its purpose?", "I think this example is different and probably you can explicit build the model or not?\r\n```python\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # This prevents tensorflow from print WARNING messages\r\nimport tensorflow as tf\r\n\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self, **kwargs):\r\n        super(MyModel, self).__init__(**kwargs)\r\n        self.dense = tf.keras.layers.Dense(10)\r\n\r\n    def call(self, inputs):\r\n        return self.dense(inputs)\r\n\r\nmodel = MyModel()\r\n\r\n@tf.function\r\ndef train_step(sample):\r\n    print(\"This function is tracing !\")\r\n    model(sample)\r\n\r\ntensor = tf.random.normal([128])\r\ndataset = tf.data.Dataset.from_tensors(tensor).repeat(16).batch(4)\r\nmodel.build(input_shape=(4, 128))\r\nsample = next(iter(dataset))\r\ntrain_step(sample)\r\n\r\nprint(\"-----------------------------------------------\")\r\nprint(train_step.pretty_printed_concrete_signatures())\r\n```\r\n\r\n```\r\nThis function is tracing !\r\n-----------------------------------------------\r\ntrain_step(sample)\r\n  Args:\r\n    sample: float32 Tensor, shape=(4, 128)\r\n  Returns:\r\n    NoneTensorSpec()\r\n```", "Must I always build the model's weights in advance to avoid tracing 2 times? Not only the model but also the optimizer creates some Variables when applying gradients for the first time. Explicitly build the model and optimizer looks not good. Is there a way other than building the model and optimizer explicitly?\r\n\r\nAgain, my question is **why**. I want to understand why the exactly same computation graph is built 2 times, which does not make sense to me. Explicitly build the model and optimizer can solve the problem, but another way without doing this seems to be better. ", "When you create a `tf.varaible` inside a `tf.function` it will re-trigger a retrace. \r\nIt is also why [Variables may only be created once](https://www.tensorflow.org/api_docs/python/tf/function?version=nightly#variables_may_only_be_created_once) and in the same doc section\r\n> In general, it is recommended to create tf.Variables outside of tf.function\r\n\r\nSo calling `model(sample)` on an not built model it will go to create model variable inside your `tf.function` and this will trigger the retrace.\r\n\r\nThe minimal example is:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef train_step():\r\n    #the next line will trigger a retrace inside a tf.function\r\n    tf.Variable(0)\r\ntrain_step()\r\n```\r\n\r\n ", "Okay, I got it. Thank you @bhack ", "When should the `build()` method be called then? Where is the documentation for that method?\r\n\r\nIs it okay to call build() for every batch (only the first call would have an effect) ? Or should I keep track of (un)built models?\r\n\r\n~Additionally, how should the `build()` method be called when they are multiple inputs to the model?~ Answer in the code."]}, {"number": 48848, "title": "[MLIR] Add lowering for TF::SeluOp and TF::SeluGradOp", "body": "This PR adds the lowering for TF::SeluOp and TF::SeluGradOp to mhlo. Relevant test cases are also added.\r\n\r\nThis PR also adds TF to TF lowering for the same.\r\n\r\nSigned-Off-By: Prateek Gupta <prateek@polymagelabs.com>", "comments": ["Hi @smit-hinsu can you please review this? Thanks!", "> Hi @smit-hinsu can you please review this? Thanks!\r\n\r\nGentle ping, @smit-hinsu .", "Gentle ping, @joker-eph @sanjoy.", "> Let's remove these ops from this list so that this patterns are used in all contexts:\r\n> https://github.com/tensorflow/tensorflow/blob/c83cdf85c5937730af51a384c0cc605bfb8f9214/tensorflow/compiler/mlir/xla/transforms/legalize_tf_with_tf2xla.cc#L226\r\n> \r\n> I am also curious about your usecase that the above didn't work.\r\n> \r\n> Sorry for the delay on this. Thanks for the contributions.\r\n\r\nPlease re-review @smit-hinsu. I have used DRRs now for the lowering. I am not sure about the code you have pointed. Can you elaborate on this?", "> > Let's remove these ops from this list so that this patterns are used in all contexts:\r\n> > https://github.com/tensorflow/tensorflow/blob/c83cdf85c5937730af51a384c0cc605bfb8f9214/tensorflow/compiler/mlir/xla/transforms/legalize_tf_with_tf2xla.cc#L226\r\n> > \r\n> > I am also curious about your usecase that the above didn't work.\r\n> > Sorry for the delay on this. Thanks for the contributions.\r\n> \r\n> Please re-review @smit-hinsu. I have used DRRs now for the lowering. I am not sure about the code you have pointed. Can you elaborate on this?\r\n\r\nHow do you use the legalize pass? Do you have xla-legalize-tf-with-tf2xla pass in the pass pipeline?\r\n\r\nxla-legalize-tf-with-tf2xla pass in that file is used to legalize ops mentioned in the list. That pass can already legalize Selu op but now that we will have native lowering after this change, we can drop it from the list to keep only one implementation. So, just remove Selu op from the list.", "@pr4tgpt  Any update on this PR? Please. Thanks!", "@gbaned I am currently working on the changes mentioned by @smit-hinsu in the ``lower_tf.cc``. Will soon this update this PR.", "Hi @smit-hinsu please re-review. I have added changes as you suggested. Thanks!", "Gentle ping @smit-hinsu.", "There were holidays in the US so will get to this on Tuesday or Wednesday.", "> There were holidays in the US so will get to this on Tuesday or Wednesday.\r\n\r\nGentle Ping @smit-hinsu . ", "@pr4tgpt Can you please check @smit-hinsu's comments and keep us posted ? Thanks!", "> @pr4tgpt Can you please check @smit-hinsu's comments and keep us posted ? Thanks!\r\n\r\nSure. I am working on this. Will update the PR soon.", "@smit-hinsu Please re-review.", "Hi @smit-hinsu, Thank you for your valuable comments. Addressed all. Please re-review. Thanks!", "Gentle Ping @smit-hinsu .", "Rebased and pushed. ", "Gentle ping @smit-hinsu.", "Hi @smit-hinsu,  I am not sure why tests are failing, can you tell where it is failing? Thanks!", "@pr4tgpt  Can you please resolve conflicts? Thanks!", "Look at the failure log in this: https://tensorflow-ci.intel.com/job/tensorflow-mkl-linux-cpu-pr/10251/", "Hi @smit-hinsu, rebased and merged. ", "Hi @smit-hinsu can you look into these failing tests. I dont think it is related to my code."]}, {"number": 48847, "title": "[mlir-hlo] Added Userange Analysis for Buffers.", "body": "This PR introduces a new analysis, the `UserangeAnalysis`. This analysis works similar to a SSA `Liveness` analysis, but differs in a way that a buffer is not live as soon as it is allocated. The general idea is that the userange starts with the first use of a buffer and ends with its last use. The use case for this analysis is to write a pass that can reuse buffers depending on the userange. Later on, we want to use this analysis as a cornerstone to reuse buffers in order to reduce memory consumption.\r\n```\r\nExample:\t\t\t\tResult:\r\n1:             %0 = alloc()             Userange %0:\r\n2:             %1 = alloc()             (4, 4)\r\n3:             cond_br                  Userange %1:\r\n               /       \\                (5, 5)\r\n4:         use(%0)\r\n5:                   use(%1)\r\n               \\       /\t\t\r\n6:              return\r\n```\r\nAssuming the example above: The userange of %0 are the operations inside the left block in between the first and last use of %0. %1 has a similar userange compared to %0, but in the right block. The useranges of both values do not interfere, because the userange of %0 reaches from operation 5 to 5 and %1 reaches from 6 to 6.\r\n\r\nAs a special case, gaps can appear inside useranges, which is not possible in the `Liveness` analysis.\r\n```\r\nExample:\t\t\t\tResult:\r\n1:             %0 = alloc()             Userange %0:\r\n2:             %1 = alloc()             (5, 5)\r\n3:             use(%1)                  Userange %1:\r\n4:             cond_br                  (3, 4), (6, 6)\r\n               /       \\\r\n5:         use(%0)\r\n6:                   use(%1)\r\n               \\       /\t\t\r\n7:              return\r\n```\r\nEach line represents an operation ID, which is stored inside this analysis class using a Map.\r\nThe userange of %1 starts in with operation ID 3 and ends with ID 4, which is the last operation in that block. Furthermore, the userange continues with ID 6. Operation ID 5 is not considered for the userange, since there is no usage of %1.\r\n\r\nThe first step in this analysis is to collect all operations where a given buffer is in use. The second step is to include all operations between the uses. Finally, an interval is created depending on the operation IDs.\r\n\r\nConceptionally each operation is stored using two IDs. This allows us to track fine-grained userange information per operation. Consider an elementwise sample operation that uses three buffers. The first two buffers have read effects and the third has a write effect. Thus the userange of the first two buffers do not exceed the third buffer. In other words, the userange of the first two buffers end before the third buffer.", "comments": ["@dfki-albo  Can you please check @sherhut's comments and keep us posted ? Thanks!", "Based on some comments in the follow-up PR([#48883](https://github.com/tensorflow/tensorflow/pull/48883)), I will change this PR to a draft and work on a better way to endcode the \"double IDs\".", "@dfki-albo This PR is in draft, any update on this? Please. Thanks!"]}, {"number": 48846, "title": "Is tensorflow going to support amd gpu with ROCm \uff1f ", "body": "**Describe the problem**\r\nI notice that ```pytorch``` supports amd gpu with ROCm in version 1.8.1 ([https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)) . I want to know whether ```tensorflow``` is going to support amd gpu or not ...", "comments": ["Isn't that a question for AMD?", "@DachuanZhao  Please go through the community supported build section [here](https://github.com/ROCmSoftwarePlatform/tensorflow-upstream#community-supported-builds)  and you will get more information on this. \r\n\r\nAlso, Please go through this  SO [Link](https://stackoverflow.com/questions/37892784/using-keras-tensorflow-with-amd-gpu) for detailed information. Thanks!\r\n", "@DachuanZhao Seems Tensorflow is now supporting AMD GPU with ROCM. Please go through this [document](https://medium.com/analytics-vidhya/install-tensorflow-2-for-amd-gpus-87e8d7aeb812) for Step by step installation. Thanks!", "> @DachuanZhao Seems Tensorflow is now supporting AMD GPU with ROCM. Please go through this [document](https://medium.com/analytics-vidhya/install-tensorflow-2-for-amd-gpus-87e8d7aeb812) for Step by step installation. Thanks!\r\n\r\nWhy not add the installation to the [offical tensorflow document](https://www.tensorflow.org/install/gpu)  ~~~", "I would also appreciate having this inside the official documentation too. I found the [AMD documentation](https://rocmdocs.amd.com/en/latest/Deep_learning/Deep-learning.html) to be quite helpful otherwise.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 48844, "title": "TensorFlow 2 slow on small numbers of batches", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 and Colab (observed such performance issue on both platform)\r\n- TensorFlow installed from (source or binary): pip / provided in colab\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.8\r\n- CUDA/cuDNN version: CUDA 11.0, cuDNN 8.0 / provided in colab\r\n- GPU model and memory: GTX 1660Ti / provided in colab\r\n\r\nI have already asked this question on StackOverflow, but nobody answered, so considering it is a performance issue, I opened an issue, sorry if this is not appropriate to be asked in issues.\r\n\r\nWhen training a small number of batches (not batch size), TensorFlow 2 is significantly slower than PyTorch.\r\n(for example, train 30 batches, note that different but still relatively large ratios still apply for a smaller number of batches like training a single batch, different models with a few more layers/params, and/or different batch size)\r\nbelow are approximations of the performance difference I observed in colab CPU:\r\n\r\nwith low-level gradient tape training, TensorFlow is 9 times slower than equivalent model/data on PyTorch\r\nwith low-level gradient tape + @tf.function train step, TensorFlow is 18 times slower than equivalent model/data on PyTorch\r\nwith high-level fit(), TensorFlow is 19 times slower than equivalent model/data on PyTorch\r\nwith high-level train_on_batch(), TensorFlow is 22 times slower than equivalent model/data on PyTorch\r\n\r\n**A reproducible code** example can be found here, which shows a simple model (can run on both CPU or GPU depending on whether GPU is turned on in Colab) and generated random data being trained with all functions/API discussed above: https://colab.research.google.com/drive/1VHYLEZ79etKSkK0iWIqHkt7OG8nbU9uU?usp=sharing \r\n\r\nabove code is partially modified from the official tutorial https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch\r\nand partially written on my own.\r\n\r\ntraining single batch or a few batches are widely used in Deep Reinforcement Learning and some special cases of Deep Learning. \r\nThe code above is just for demonstration of my observation. I also noticed a large train time gap between TF and PyTorch when I work on DQN locally on my Windows machine, since DQN Experience replay trains on one batch each time.\r\n\r\nAm I using TF incorrectly? if yes, what is the best way to train a small number of batches?\r\nEven if I am not using the best way. \r\nWhy is @tf.function that is meant to accelerate slowing down training?\r\nWhy is train_on_batch that seems to be designed for a single batch the slowest of all?\r\n\r\nAny help would be great! Thank you.\r\n", "comments": ["@jvishnuvardhan ,\r\n\r\nI was able to reproduce the code.Please find the [gist](https://colab.research.google.com/gist/tilakrayal/74e54faafe4e743a8f2b4538481c6db9/48844.ipynb) here", "Your findings are mostly expected, I will include some explanations below. At a glance, I don't see any obvious issues with the way you've been constructing the code.\r\n\r\n1. TF eager\r\n\r\nTF eager is know to be slower compared to PyTorch. Your measurement of about an order of magnitude seems in the ballpark. The rule of thumb is to use eager execution for debugging, and production code to use tf.function (see below for its performance considerations)\r\n\r\n2. Keras\r\n\r\nI'm actually not sure why the Keras versions which use model.fit and model.train_on_batch are not faster, even with a warm-up call. Internally, they should use `tf.function`, but then again, they may perform additional work (like summaries or whatnot).\r\ncc @tomerk @fchollet for additional insight\r\n\r\n3. tf.function\r\n\r\nThe most important thing to know about tf.function is that it compiles just-in-time, that is, it's actually compiled the first time you call it (traced in TF parlance, see [here](https://www.tensorflow.org/guide/intro_to_graphs) and [here](https://www.tensorflow.org/guide/function) for some more high-level info). Tracing is considerably slow compared to executing. However, once traced, subsequent executions will be much faster.\r\n\r\nLong story short: the initial call of tf.function is usually much slower, as your test revealed. So if you have a workflow that repeatedly builds a function and calls it only a small number of times, it will not be faster than eager. Usually, you'd notice this in unit tests. But if you have a way to reuse the same function for multiple runs, then it should quickly pay off.\r\n\r\nNow, if we modify our benchmark to account for that by running a warm-up call:\r\n\r\n```\r\ntrain_step(inputs, labels)  # Do a warm-up run for tracing\r\n\r\nt = time.time()\r\nfor _ in range(30):\r\n  loss_v = train_step(inputs, labels)\r\nprint(time.time() - t)\r\n```\r\n\r\nThen we get speed roughly on par with PyTorch. It's slightly faster, but in the same order of magnitude.\r\n\r\nSo the main caveat is the function tracing, which should be a one-time cost.\r\n\r\nMore on `tf.function`: to get the state-of-the art speed, you should consider enabling XLA compilation, by specifying `@tf.function(experimental_compile=True)` or `@tf.function(jit_compile=True)` in newer versions. That is restricted to a subset of TensorFlow (it works in our case), but gives you the best available performance.\r\n\r\nEven more on `tf.function`: a good way to get even greater performance is to run several training steps in the same graph:\r\n\r\n```\r\n@tf.function\r\ndef train_loop(n_steps):\r\n  for _ in tf.range(n_steps):\r\n    loss_v = train_step(inputs, labels)\r\n  return loss_v\r\n\r\ntrain_loop(tf.constant(1))  # Again, a warm-up for tracing\r\n\r\n# Note: use tf.constant(1) and tf.constant(30), not 1 and 30, to avoid retracing\r\n\r\nt = time.time()\r\ntrain_loop(tf.constant(30))\r\nprint(time.time() - t)\r\n```\r\n\r\nThis gave the best speed-up in my tests."]}, {"number": 48843, "title": "Can't detect GPU with Tensorflow-GPU", "body": "- NVIDIA GTX 1050 Ti (4GB)\r\n- Window 10\r\n- Python 3.9.0\r\n- Cuda V11.2.152\r\n\r\nI have `tensorflow-gpu` installed and I run this code to verify if tensorflow detected my GPU\r\n\r\n```\r\nfrom tensorflow.python.client import device_lib\r\nprint(device_lib.list_local_devices())\r\n```\r\n\r\nTensorflow could only detect my CPU\r\n\r\n```\r\n[name: \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 10182600208144693220\r\n]\r\n```\r\n\r\nSo I uninstalled `tensorflow-gpu` and installed `tf-nightly-gpu`, then I run the same code again.\r\n\r\n```\r\n[name: \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 761424825523677949\r\n, name: \"/device:GPU:0\"\r\ndevice_type: \"GPU\"\r\nmemory_limit: 2912380519\r\nlocality {\r\n  bus_id: 1\r\n  links {\r\n  }\r\n}\r\nincarnation: 4915443179038048022\r\nphysical_device_desc: \"device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\"\r\n]\r\n```\r\n\r\nThis time no problem. But one probelm with nigtly is that it doesn't support addons. Is there a bug in `tensorflow-gpu` or something?\r\n\r\n", "comments": ["@LogicNg  Please use  the below command  to list  the number of visible GPUs.\r\n\r\n```\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\nprint(\"Num GPUs:\", len(physical_devices))\r\n```\r\n\r\n", "`Num GPUs: 0`", "Finally figure out the problem, TensorFlow could not load dynamic library 'cusolver_10.dll', so I move to  `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2\\bin` and rename `cusolver64_11.dll`  To  `cusolver64_10.dll`.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48843\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48843\">No</a>\n", "@LogicNg  Glad to hear that issue has been resolved now. Thanks!"]}, {"number": 48842, "title": "removing inconsistencies in micro_speech_test.cc", "body": "PR to fix issue mentioned in #48841. \r\n\r\nI ran \r\n\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile test_micro_speech_test -j8\r\n```\r\n\r\nall tests passed on macOS", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 48841, "title": "Inconsistencies in micro_speech example", "body": "This may be related to https://github.com/tensorflow/tensorflow/issues/48752. The `micro_speech` model is int8 quantized for both input and output. The test for the example has some lines with `data.int8` and `data.uint8` when querying the output data.\r\n\r\nFor example:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/173b50a525e0f88fa25674980f1243ada460114e/tensorflow/lite/micro/examples/micro_speech/micro_speech_test.cc#L103\r\n\r\nand:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/173b50a525e0f88fa25674980f1243ada460114e/tensorflow/lite/micro/examples/micro_speech/micro_speech_test.cc#L105\r\n\r\n\r\n<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): master branch\r\n- Python version: n/a\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\nTFLM example micro_speech has inconsistencies in the test file. It treats some outputs as `int8` and some as `uint8`.\r\n\r\n**Describe the expected behavior**\r\nTreat all outputs from the model as the same as they are all `int8`.\r\n\r\n**Standalone code to reproduce the issue**\r\nAffected lines can be seen above. Does not raise an error but it does not make sense.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.", "comments": ["@henriwoodcock \r\n\r\nCould you please fill the template and elaborate your Query with clear details. What is your error and what are the steps followed to reproduce it. Thanks\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "@UsharaniPagadala I've added the template", "@henriwoodcock \r\nApologize for the late reply.Could you please reopen the issue if it is still persist.Thanks"]}, {"number": 48840, "title": "Minor fix for implicit-const-int-float-conversion warning", "body": "Fixing: #48839\r\n", "comments": []}, {"number": 48839, "title": "Can't build TFL micro build with armclang toolchain", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): N/A\r\n- Tensorflow version (commit SHA if source): 9000e360d95b9d2c3925672ba100579bd459ee30\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Any target\r\n\r\n**Describe the problem**\r\nWhen using the armclang toolchain to build the microlite lib, the warning `implicit-const-int-float-conversion` is hit. Warnings are treated as errors, hence build fails.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nFor example:\r\n`make -j -f tensorflow/lite/micro/tools/make/Makefile OPTIMIZED_KERNEL_DIR=ethos_u TOOLCHAIN=armclang TARGET=cortex_m_generic TARGET_ARCH=cortex-m55 microlite\r\n`\r\n\r\nOutput with error:\r\n```\r\narmclang -std=c11 -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -DTF_LITE_DISABLE_X86_NEON -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter -DCORTEX_M_GENERIC -DETHOS_U --target=arm-arm-none-eabi -mcpu=cortex-m55 -DTF_LITE_MCU_DEBUG_LOG -mthumb -mfloat-abi=hard -funsigned-char -mlittle-endian -Wno-implicit-fallthrough -Wno-strict-aliasing -Wno-unused-variable -Wno-type-limits -Wno-unused-private-field -fomit-frame-pointer -MD -DCPU_M55=1 -D__DSP_PRESENT=1 -D__FPU_PRESENT=1 -I. -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/ethos_u_core_driver/include -Itensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/Core/Include -c tensorflow/lite/micro/tools/make/downloads/ethos_u_core_driver/src/ethosu_driver.c -o tensorflow/lite/micro/tools/make/gen/cortex_m_generic_cortex-m55_default/obj/tensorflow/lite/micro/tools/make/downloads/ethos_u_core_driver/src/ethosu_driver.o\r\ntensorflow/lite/kernels/kernel_util.cc:342:15: error: implicit conversion from 'std::numeric_limits<int>::type' (aka 'int') to 'float' changes value from 2147483647 to 2147483648 [-Werror,-Wimplicit-const-int-float-conversion]\r\n        tmp <= std::numeric_limits<int32_t>::max());\r\n```", "comments": ["@freddan80,\r\nThank you for submitting the fix. As the PR #48840 has been merged, can you please let us know if we can close this issue? \r\nThanks!", "Absolutely, you can close it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48839\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48839\">No</a>\n"]}, {"number": 48838, "title": "LSTM and GRU layers do not use cuDNN acceleration when processing RaggedTensors", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian stable\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.0 / 8.0 (the ones on Colab were used)\r\n- GPU model and memory: Colab\r\n\r\n**Describe the current behavior**\r\n\r\nWhen LSTM/GRU layers are executed on RaggedTensors, they do not use cuDNN acceleration on GPU and are much slower. The attached Colab notebook shows more than 5-times slowdown compared to manual conversion before/after the call.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe LSTM/GRU layers executed on RaggedTensors should use cuDNN acceleration on GPU.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nhttps://colab.research.google.com/drive/1eR3ggwp-5sNKQjJld7XiTjet3BT740F1?usp=sharing\r\n\r\nNote that when converting the RaggedTensors before/after the RNN call manually like this:\r\n```python\r\noutputs = rnn(inputs.to_tensor(), mask=tf.sequence_mask(inputs.row_lengths()))\r\noutputs = tf.RaggedTensor.from_tensor(outputs, inputs.row_lengths())\r\n```\r\nallows using cuDNN implementation.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nThe problem is described directly in the sources: GRU here: https://github.com/tensorflow/tensorflow/blob/9a2545aecc498e7a5e8bf5d63c47af59e1550111/tensorflow/python/keras/layers/recurrent_v2.py#L447-L449 and LSTM here: https://github.com/tensorflow/tensorflow/blob/9a2545aecc498e7a5e8bf5d63c47af59e1550111/tensorflow/python/keras/layers/recurrent_v2.py#L1165-L1167\r\n\r\nNote that same code is still in current head: GRU here: https://github.com/tensorflow/tensorflow/blob/9000e360d95b9d2c3925672ba100579bd459ee30/tensorflow/python/keras/layers/recurrent_v2.py#L434-L436 LSTM here: https://github.com/tensorflow/tensorflow/blob/9000e360d95b9d2c3925672ba100579bd459ee30/tensorflow/python/keras/layers/recurrent_v2.py#L1161-L1163", "comments": ["@foxik  I can see there is a performance issue with TF version 2.4. When i tried to execute your code on Nightly version,  I didn't see any performance issue. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/d8cb1b310aa880a9dda1bbf56d34fc5c/raggedtensorlstm.ipynb).Thanks!", "@saikumarchalla Please note that tf-nightly does not use GPU on Colab (it requires too new CUDA), which you can verify by adding for example\r\n```python\r\nprint(tf.config.get_visible_devices())\r\n```\r\nwhich shows\r\n```\r\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\r\n```\r\n\r\nThe issue is not fixed in nightly, as visible for example in the following source:\r\nhttps://github.com/tensorflow/tensorflow/blob/baaa9b23f76e307eb9e1dd2773ee5956f677dccc/tensorflow/python/keras/layers/recurrent_v2.py#L434-L438\r\nso please remove the `Fixed in Nightly` tag - thanks :-)", "@foxik  As mentioned in the Ragged Tensors [document](https://www.tensorflow.org/guide/ragged_tensor#tensorflow_apis), if you use Ragged Tensors or `ragged=True` in your layer you would get a warning like `WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU`\r\n", "@sachinprasadhs Yes, that is true -- the warning is caused by the source snippets above, which are marked with `TODO(b/156447398)`, which I assume is some internal bug tracking number.\r\n\r\nThe current code path converts the ragged tensors to dense tensors (inside the LSTM/GRU) anyway, but then it does not use the cuDNN path. But if the conversion is done manually outside (using the same RaggedTensor conversion functions), cuDNN implementation is used -- so it is definitely possible to take the cuDNN path even with RaggedTensors -- which is why I filed this issue.", "@foxik Yes, you are right, there is already a internal bug(b/156447398) created for the same issue, till the issue is fixed the cuDNN GPU is disabled for the ragged inputs to avoid the input shape error due to the error in conversion of ragged tensors to dense tensors.", "@sachinprasadhs It seemed like the issue is being tracked internally, but I did not find any mention about the problem here, so I created this issue to make it publicly visible.\r\n\r\nIt is however surprising that performing the conversion manually outside of the LSTM/GRU layer works without any problems (while I really encountered an issue when I tried to remove the workaround in the sources)...", "Manually conversion works fine, but as I have mentioned above the issue seems to be with the RNN code converts the ragged tensor to dense tensor before feeding it to cuDNN kernel, so the error might be in the conversion code.", "I created a pull request in the keras repository https://github.com/keras-team/keras/pull/15756 .", "Hi, Could you please move this issue to closed since the PR is successfully merged. Thanks!", "Definitely, thanks for the remainder! Cheers."]}, {"number": 48837, "title": "BatchNormalization layers in Tensorflow 2.4.1 give constant validation accuracy", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.2 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Anaconda (conda 4.10.1 using conda install)\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: Python 3.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: GeForce GTX 1080, 7845MB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI was building a simple CNN for a binary image classification task and found something confusing regarding the use of the `BatchNormalization` layer in Tensorflow. There are 320 images in the training set with evenly divided negative and positive cases. There are 80 images in the validation set with evenly divided negative and positive cases. I set the `batch_size` for both training and validation set to 32.\r\n\r\nHere is the architecture of my original model.\r\n```\r\nfrom tensorflow.keras import Sequential\r\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dropout, Dense\r\n        \r\nmodel = Sequential([\r\n    Input(shape=(256, 256, 3]),\r\n    Conv2D(64, (3, 3), padding=\u201csame\u201d, activation=\u201crelu\u201d),\r\n    Conv2D(64, (3, 3), padding=\u201csame\u201d, activation=\u201crelu\u201d),\r\n    MaxPooling2D((2, 2)),\r\n    Conv2D(128, (3, 3), padding=\u201csame\u201d, activation=\u201crelu\u201d),\r\n    Conv2D(128, (3, 3), padding=\u201csame\u201d, activation=\u201crelu\u201d),\r\n    MaxPooling2D((2, 2)),\r\n    Conv2D(256, (3, 3), padding=\u201csame\u201d, activation=\u201crelu\u201d),\r\n    Conv2D(256, (3, 3), padding=\u201csame\u201d, activation=\u201crelu\u201d),\r\n    Conv2D(256, (3, 3), padding=\u201csame\u201d, activation=\u201crelu\u201d),\r\n    MaxPooling2D((2, 2)),\r\n    Flatten(),\r\n    Dense(128, activation=\u201crelu\u201d),\r\n    Dropout(0.5),\r\n    Dense(64, activation=\u201crelu\u201d),\r\n    Dense(1, activation=\u201csigmoid\u201d)\r\n])\r\n\r\n```\r\nBoth training and validation accuracies increased from somewhere between 0.5 and 0.6 and eventually converged somewhere between 0.95, which is good. However, with everything else unchanged, if I introduced `BatchNormalization` layers between `Conv2D` layers like the following:\r\n```\r\n\r\nmodel = Sequential([\r\n    Input(shape=(256, 256, 3]),\r\n    Conv2D(64, (3, 3), padding=\u201csame\u201d),\r\n    BatchNormalization(),\r\n    ReLU(),\r\n    Conv2D(64, (3, 3), padding=\u201csame\u201d, activation=\u201crelu\u201d),\r\n    MaxPooling2D((2, 2)),\r\n    Conv2D(128, (3, 3), padding=\u201csame\u201d),\r\n    BatchNormalization(),\r\n    ReLU(),\r\n    Conv2D(128, (3, 3), padding=\u201csame\u201d, activation=\u201crelu\u201d),\r\n    MaxPooling2D((2, 2)),\r\n    Conv2D(256, (3, 3), padding=\u201csame\u201d),\r\n    BatchNormalization(),\r\n    ReLU(),\r\n    Conv2D(256, (3, 3), padding=\u201csame\u201d),\r\n    BatchNormalization(),\r\n    ReLU(),\r\n    Conv2D(256, (3, 3), padding=\u201csame\u201d, activation=\u201crelu\u201d),\r\n    MaxPooling2D((2, 2)),\r\n    Flatten(),\r\n    Dense(128, activation=\u201crelu\u201d),\r\n    Dropout(0.5),\r\n    Dense(64, activation=\u201crelu\u201d),\r\n    Dense(1, activation=\u201csigmoid\u201d)\r\n])\r\n\r\n```\r\nThe training accuracy still behaved the same way as before but validation accuracy remained constant at 0.5. I also tried several other implementations of `BatchNormalization` layers and had the same observation.\r\n```\r\n\r\nmodel = Sequential([\r\n    Input(shape=(256, 256, 3]),\r\n    Conv2D(64, (3, 3), padding=\u201csame\u201d, activation=\u201crelu\u201d),\r\n    BatchNormalization(),\r\n    Conv2D(64, (3, 3), padding=\u201csame\u201d, activation=\u201crelu\u201d),\r\n    MaxPooling2D((2, 2)),\r\n    Conv2D(128, (3, 3), padding=\u201csame\u201d, activation=\u201crelu\u201d),\r\n    BatchNormalization(),\r\n    Conv2D(128, (3, 3), padding=\u201csame\u201d, activation=\u201crelu\u201d),\r\n    MaxPooling2D((2, 2)),\r\n    Conv2D(256, (3, 3), padding=\u201csame\u201d, activation=\u201crelu\u201d),\r\n    BatchNormalization(),\r\n    Conv2D(256, (3, 3), padding=\u201csame\u201d, activation=\u201crelu\u201d),\r\n    BatchNormalization(),\r\n    Conv2D(256, (3, 3), padding=\u201csame\u201d, activation=\u201crelu\u201d),\r\n    MaxPooling2D((2, 2)),\r\n    Flatten(),\r\n    Dense(128, activation=\u201crelu\u201d),\r\n    Dropout(0.5),\r\n    Dense(64, activation=\u201crelu\u201d),\r\n    Dense(1, activation=\u201csigmoid\u201d)\r\n])\r\n\r\n```\r\nThere are people online saying that there are bugs in Keras\u2019s implementation of `BatchNormalization`. I had tried to reduce the layers of `BatchNormalization` and found that with even a single layer of `BatchNormalization` introduced, the validation accuracy will be constant at 0.5. Do you have any ideas why?\r\n\r\n**Describe the expected behavior**\r\n\r\n I am rather new with deep learning and from what I have read recently on different tutorials about `BatchNormalization`, it should usually (though not always) be a performance booster. However, from what I have observed, `BatchNormalization` was ruining my validation performance. \r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48837\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48837\">No</a>\n", "@TianruiZhang \r\nApologies for the delayed response, is this still an issue, please share a colab gist with the issue reported, can you try on tf 2.5 and let us know.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48837\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48837\">No</a>\n"]}, {"number": 48836, "title": "CMake build option TFLITE_ENABLE_RUY=OFF does not work", "body": "**System information**\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version: 2.5.0-rc2\r\nPython version: 3.6.8 (but probably n/a)\r\nInstalled using virtualenv? pip? conda?: n/a\r\nBazel version (if compiling from source): n/a\r\nGCC/Compiler version (if compiling from source): Apple clang version 12.0.0\r\nCUDA/cuDNN version: n/a\r\nGPU model and memory: n/a\r\n\r\n**Describe the problem**\r\n\r\nFollow the instructions at https://www.tensorflow.org/lite/guide/build_cmake to build `libtensorflow-lite.a`, using the default value of TFLITE_ENABLE_RUY=OFF. Then use `nm` to look at the library and see that there are many uses of ruy (including undefined symbols) that make this library unusable.\r\n\r\nFrom a casual inspection, it appear that there are lots of references to ruy in the source code (eg #includes in optimized_ops.h) that may not be easily removed; if ruy is truly required to build TFLite, there shouldn't be an option to disable it. (Alternately, if it's not required, the code should actually allow excluding it.)\r\n", "comments": ["@terryheo could you take a look?", "Plz check https://github.com/tensorflow/tensorflow/issues/47862 for the detail.\r\n\r\n", "Thanks for the link. If current versions of TFLite really do require Ruy, that's fine and dandy, but it doesn't really make sense to offer an option that claims to disable Ruy but does not -- surely the option should be removed in that case?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48836\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48836\">No</a>\n"]}, {"number": 48835, "title": "CMake build rule for C api is incomplete when building with TFLITE_C_BUILD_SHARED_LIBS=OFF", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.5.0-rc2\r\n- Python version:  3.6.8 (but probably n/a)\r\n- Installed using virtualenv? pip? conda?: n/a\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): Apple clang version 12.0.0\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the problem**\r\n\r\nDocumentation at https://www.tensorflow.org/lite/guide/build_cmake describes how to use the (experimental) CMake build files to build TFLite, and also the TFLite C API. If you build the C API library as-is using the instructions on that page, you do indeed get `libtensorflowlite_c.so` as expected, which is usable as-is. \r\n\r\nHowever, if you use the option `TFLITE_C_BUILD_SHARED_LIBS=OFF` when building, the resulting `libtensorflowlite_c.a` doesn't include any of tflite itself... just the C API wrappers for it. This is unexpected and suboptimal. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nn/a", "comments": ["BTW: it appears that `libtensorflow-lite.a` is produced as a byproduct if you look in the `tensorflow-lite/` subdir of the build folder, so this problem may be possible to work around by linking that in as well; however, that led me to find another issue (https://github.com/tensorflow/tensorflow/issues/48836) that made that unusable as well.\r\n\r\nIdeally the two libraries should be combined into one, to mirror the use case of the shared-library form. Failing that, this weirdness/workaround should at least be documented somewhere (e.g., even just inside the CMake build file) as known behavior.\r\n\r\n", "If you want to use static library, you'd better have a project in CMake so the make tool can handle all the dependencies.\r\nhttps://www.tensorflow.org/lite/guide/build_cmake#create_a_cmake_project_which_uses_tensorflow_lite", "Hmm, ok, so if I understand correctly, this CMake support is intended *only* for use when included inside another CMake build? That's not unreasonable, but it is unusual enough that I think the documentation needs to call this out much more specifically. \r\n\r\nI can probably make this fit my needs (by creating a local wrapper build), but it seems unnecessarily limited and counterintuitive that one supported configuration (shared) works fine as-is, but another (static) does not.\r\n\r\n", "We have kind of documentation on this.\r\n\r\nhttps://www.tensorflow.org/lite/guide/build_arm\r\n\r\nFor C++ libraries, similar with CMake, you need to have a Bazel project to use Bazel generated binaries.\r\n\r\nThere are two issues on using them with different build system.\r\n1. There is no easy way of copying necessary header files\r\n2. The generated TFLite library doesn't contain 3rd party libraries so you need to link it manually\r\n\r\nFor C shared library, you can just copy and use the output with few C header files.\r\n", "@steven-johnson Could you please try as per the above [comment](https://github.com/tensorflow/tensorflow/issues/48835#issuecomment-830563725) and let us know if it helps?Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48835\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48835\">No</a>\n"]}, {"number": 48834, "title": "Add GPU support of AsString + StringToHashBucket (for Float Dtype)", "body": "This PR is an extension of https://github.com/tensorflow/tensorflow/pull/47936 for adding float dtype support.\r\n\r\ncc. @nluehr @benbarsdell", "comments": ["@kaixih  Can you please resolve conflicts? Thanks!", "@kaixih  Any update on this PR? Please. Thanks!", "Landed as b6f0f312d68231c9143533823093b146c3c74633."]}]