[{"number": 15680, "title": "Improve doc of TFRecordDataset, shuffle ahead of map", "body": "In the origin document, the code to demonstrate TFRecordDataset do `dataset.map(parser)` then do `dataset.shuffle(10000)`.\r\nThis code use a high number of buffer size (10000), and since `map` do ahead of `shuffle`, means when the first time this dataset yield one result it will need to run `map` over 10000 items and this can take a lot of time.\r\nSo, instead we can do `shuffle` ahead of `map`, since the item of `TFRecordDataset` is one `Example` raw data, `shuffle` ahead will not compromise the randomness and then the `map(parser)` only need to process one batch of items at a time. Which results much faster startup.\r\n", "comments": ["Can one of the admins verify this patch?", "Thanks for the contribution. I don't think this is a strict improvement, so I'm going to stick with the status quo and close the change. In particular, placing map before shuffle can be a better move if the map *reduces the size* of a record, and hence reduces the overall memory pressure of the shuffle buffer.\r\n\r\nIt might be worth making a follow-up change to the [`tf.data` performance guide](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/datasets_performance.md) that makes a similar point to the one you intended, and we'd welcome a PR for that."]}, {"number": 15679, "title": "My validate loss is unchanged when training, what's the reason may it be?", "body": "Dears,\r\n        Lately, I train a Inception-v3 model in RAP_dataset. BUT, entropy loss  on Validate data is always unchanged when training. The loss shows as follows:\r\n\r\n- sorry, I do not find the port attaching images. ........\r\n\r\nAnd, yesterday, I add tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)) in code, it still not work. What reasons may it be? Sincerely hope your replies, and BEST WISHES TO YOU!", "comments": []}, {"number": 15678, "title": "De-Bazel pip_smoke_test sanity check", "body": "See https://github.com/tensorflow/tensorflow/pull/15368#issuecomment-354156560 for reference.\r\n@gunan @martinwicke /cc", "comments": ["Can one of the admins verify this patch?"]}, {"number": 15677, "title": "De-Bazel check_load_py_test sanity check", "body": "See https://github.com/tensorflow/tensorflow/pull/15368#issuecomment-354156560 for reference.\r\n@gunan @martinwicke Friendly ping.", "comments": ["Can one of the admins verify this patch?", "@gunan do you see a reason why this would not work? This should be completely independent of the makefile build, right?", "This one is independent from the makefile build. This just makes sure we use our own py_test rule rather than the native py_test rule.", "@martinwicke is currently on vacation due to the holidays, I would like to leave the final call to him on these changes.\r\nHe should be back next week.", "Redirecting this to @yifeif who wrote the check.", "Resolved merge conflict.", "Thank you!"]}, {"number": 15676, "title": "Enabling tests to pass with python3.6. Updating dependencies for dock\u2026", "body": "\u2026er tests.", "comments": ["Jenkins, test this please."]}, {"number": 15675, "title": "Branch 180224227", "body": "Need a push for the release.", "comments": ["@tensorflow-jenkins test this please"]}, {"number": 15674, "title": "Remove third_party/ prefixes", "body": "", "comments": ["Can one of the admins verify this patch?", "Not sure if we want to do this internally. Thanks for pointing out.", "@drpngx What you mean? The community guidelines should not have the prefix. Not a single `BUILD` file in this repository does.", "Sorry, I meant I'm not sure if we want to push this internally first. This is tied to our exporting scripts. I feel that it should be easier to fix it internally and push forward instead of merging this in. @yifeif would know.", "Thanks @Androbin! I'm going to fix this internally for the reason @drpngx mentioned."]}, {"number": 15673, "title": "[README] Link to build history", "body": "Completes #15666", "comments": ["Can one of the admins verify this patch?"]}, {"number": 15672, "title": "matmul back-propagation quadratic memory consumption", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 3.6.3\r\n- **CUDA/cuDNN version**: using CPU only\r\n\r\n### Describe the problem\r\ntf.matmul in back-propagation has memory consumption quadratic in the number of records of data. Test case and discussion here:\r\nhttps://stackoverflow.com/questions/47991747/back-propagation-exhibiting-quadratic-memory-consumption\r\n\r\nOne answer at time of writing suggests a workaround using mini-batches, appears to confirm quadratic memory consumption is not expected. I've also tried using tf.tensordot(a,b,1) which seems to be a synonym of tf.matmul(a,b) and does not use as much memory. This despite the existence of another discussion indicating tf.matmul should be more efficient:\r\nhttps://stackoverflow.com/questions/43100679/tensorflow-einsum-vs-matmul-vs-tensordot\r\n", "comments": ["It turns out to be again the problem of shape. Using matmul the way I did there, generates output of shape (n,1). Using that in a context where shape (n,) was expected, silently generates quadratic blowup. This is arguably at least a usability bug, but that has already been posted elsewhere, so this one can be closed."]}, {"number": 15671, "title": "Turn check_futures_test into a sanity check", "body": "See https://github.com/tensorflow/tensorflow/pull/15368#issuecomment-354156560 for reference.\r\n@gunan @martinwicke /cc", "comments": ["Can one of the admins verify this patch?", "Could you check your script?\r\n\r\n```\r\n=== Sanity check step 3 of 11: do_check_futures_test (Check that python files have certain __future__ imports) ===\r\n\r\nTraceback (most recent call last):\r\n  File \"check_futures_test.py\", line 107, in <module>\r\n    main()\r\n  File \"check_futures_test.py\", line 90, in main\r\n    BASE_DIR)\r\nAssertionError: BASE_DIR = '../..' doesn't end with tensorflow\r\n```", "Interesting, `os.path.normpath` may fail depending on the working directory.\r\nSubstituting with `os.path.abspath` should do the trick.", "Jenkins, test this please.", "Yes, the build is stuck today. We'll have to wait a bit.", "Jenkins, test this please.", "Oh, I think it's good. Ubuntu CC moved to kokoro, I think, so the build is good. Waiting for review.", "Looks good to me. @gunan unless you object violently and soon.", "@Androbin Please resolve conflicts.", "@rmlarsen Conflicts resolved.", "Almost there! Thanks!"]}, {"number": 15670, "title": "[CMake] Add sanity tests for python file lists", "body": "Replaces #15166\r\nSee discussion in #15368\r\n@gunan @martinwicke Friendly ping.", "comments": ["Can one of the admins verify this patch?", "+ @mrry for CMAKE-fu to check whether this is testing what it should be.\r\n\r\n@yifeif @gunan with the aim of making all_opensource_files redundant, I think this is great. But I'm not sure about the specifics. ", "Why should we add the new syntax? Why not make the test slightly more flexible, and just comment out the lines we use the special syntax instead?\r\n\r\nMy concern is the syntax is specific to this, and will be alien to anyone adding new modules. So they will just \"hack\" the test instead of trying to write the special syntax.", "> Why not make the test slightly more flexible, and just comment out the lines we use the special syntax instead?", "This can be as simple as \"If some folder is left out from this list, we do not care about them\" and \"if something is commented, we know these should exist in our python packages, but there are bugs with these packages, and they will be added to the pip package as soon as possible.", "The CMake code looks fine to me, but I'll defer to @martinwicke or @gunan about what we'd like to support in the build.", "* Only lines starting with `tensorflow/` will be whitelisted\r\n* Behavior is documented in `python_modules.txt` and `python_sanity_test.py`\r\n* Descriptions of problem and solution are printed respectively", "Great. The only question left is runtime, but I don't expect problems for this."]}, {"number": 15669, "title": "Session::Run() allocates a lot of memory after the first call", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS Sierra 10.12.6\r\n- **TensorFlow installed from (source or binary)**: Binary ([CPU C API](https://www.tensorflow.org/install/install_c))\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\n\r\nI am working on an application that deploys several TensorFlow models using TensorFlow's C API. I have noticed that there seems to be a behavior where TensorFlow takes a long time to run the first time `TF_SessionRun()` is called, and it allocates a lot of extra memory that hangs around until the session is destroyed. In my case, my program's memory footprint is ~85MB after all of the models are loaded (which makes sense, that's about how large the model .pb files are on disk) but after the first call to `TF_SessionRun()` it jumps to ~250MB. After profiling my code it appears that TensorFlow is the culprit, and I've observed similar behavior on Android as well.\r\n\r\nTensorFlow seems to be doing some lazy initialization, but there doesn't appear to be much documentation or discussion about this. Could someone shed some light on what is happening here? Why does it require so much memory? Is this a bug or expected behavior?\r\n\r\n### Source code / logs\r\n\r\nHere's a memory call tree from Xcode showing *persistent* memory allocations after the first call to `TF_SessionRun()` for one of my models:\r\n\r\n<img width=\"996\" alt=\"screen shot 2017-12-27 at 1 14 59 pm\" src=\"https://user-images.githubusercontent.com/3229244/34394457-1af8625c-eb0e-11e7-8cd8-ff61737dcb50.png\">\r\n\r\nLet me know if there is any more information that I can provide. I'm curious what's going on here.", "comments": ["This is kind of expected since tensorflow also runs graph rewriting code to optimize the graph.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Not sure if waiting for a response from me \u2013 but if it runs a graph optimization step why is the memory usage so large? Does it do a deep copy of the old graph+weights but persists the unoptimized graph somewhere?\r\n\r\nWe are trying to use TF in a somewhat memory-sensitive environment, so we currently delete the TF session when we don't need it. This is less than ideal though, since the next time we need it we have to re-load the graph from file and let TF do this graph optimization process again.", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "@mrry, can you comment? It looks like the constant op is taking some memory from the memory call tree.", "> Does it do a deep copy of the old graph+weights but persists the unoptimized graph somewhere?\r\n\r\nYes, it does. There's currently no API to release the unoptimized graph, which must be kept around because the Session API allows the user to execute node in the graph. The current `ConstantOp` implementation also keeps two copies of the constant data in memory, in the `NodeDef` and in a `Tensor` object.\r\n\r\nTo avoid this blowup in memory consumption, you might be better off using the [`ConvertConstantsToImmutable()` utility](https://github.com/tensorflow/tensorflow/blob/4ab2d8531c461169cd6a33bc0fef1129b419e9df/tensorflow/contrib/util/convert_graphdef_memmapped_format_lib.h#L28), which removes large constants from the graph and replaces them with a mmappable form.\r\n\r\n/cc @gunan @aselle ", "I'm closing this given mrry@'s at response. Please reopen if you think this is something that a community member could work on.", "FWIW, fc09f65a5d283baa9af182536e3e3652c7a41dd7 should halve the amount of memory that is consumed by each `tensorflow::ConstantOp` object. The first `Run()` call will probably still cause a spike in memory consumption, since there are other copies of the `ConstantOp` data that will be retained, but it should be a little less tall now.", "@mrry  I am facing similar issue. \r\nI am using a SCNN model, model size is 90MB.\r\nIn Windows, during the first call to the session->Run, the working set memory increases 200MB and the peak working set increases by 600MB. \r\n\r\nIs this still a expected behavior ?"]}, {"number": 15668, "title": "MNIST dataset - gzip: train-images-idx3-ubyte.gz: not in gzip format", "body": "initiated from tensorflow-discuss From: greina@eng.ucsd.edu \r\n\r\n> \r\n> \r\n> I'm trying to use:\r\n> \r\n> from tensorflow.examples.tutorials.mnist import input_data\r\n> mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)\r\n> \r\n> but I am getting the error that the downloaded file is not in GZip format.\r\n> \r\n> There are several bug reports on this, but none of them seem to solve the issue. \r\n> \r\n> I found train-images-idx3-ubyte.gz in the local directory MNIST_data, but even trying `gunzip` fails:\r\n> \r\n> ```(tf) [bduser@param03 MNIST_data]$ gunzip\r\n> gzip: compressed data not read from a terminal. Use -f to force decompression.\r\n> For help, type: gzip -h\r\n> (tf) [bduser@param03 MNIST_data]$ gunzip train-images-idx3-ubyte.gz\r\n> \r\n> gzip: train-images-idx3-ubyte.gz: not in gzip format\r\n> ```\r\n> \r\n> I'm thinking that this is a problem with the original datafile. Maybe the GZip format has changed or is in conflict?? Or perhaps my OS' version of GZip can't understand the GZip file??\r\n> \r\n> I am using TF 1.4 on CentOS Linux release 7.4.1708 (Core)\r\n> \r\n> Thanks.\r\n> -Tony\r\n> \r\n> \r\n> \r\n> ```\r\n> >>> from tensorflow.examples.tutorials.mnist import input_data\r\n> >>> mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)\r\n> Successfully downloaded train-images-idx3-ubyte.gz 727 bytes.\r\n> Extracting MNIST_data/train-images-idx3-ubyte.gz\r\n> Traceback (most recent call last):\r\n>   File \"<stdin>\", line 1, in <module>\r\n>   File \"/home/bduser/miniconda2/envs/tf/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py\", line 242, in read_data_sets\r\n>     train_images = extract_images(f)\r\n>   File \"/home/bduser/miniconda2/envs/tf/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py\", line 56, in extract_images\r\n>     magic = _read32(bytestream)\r\n>   File \"/home/bduser/miniconda2/envs/tf/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py\", line 38, in _read32\r\n>     return numpy.frombuffer(bytestream.read(4), dtype=dt)[0]\r\n>   File \"/home/bduser/miniconda2/envs/tf/lib/python2.7/gzip.py\", line 268, in read\r\n>     self._read(readsize)\r\n>   File \"/home/bduser/miniconda2/envs/tf/lib/python2.7/gzip.py\", line 303, in _read\r\n>     self._read_gzip_header()\r\n>   File \"/home/bduser/miniconda2/envs/tf/lib/python2.7/gzip.py\", line 197, in _read_gzip_header\r\n>     raise IOError, 'Not a gzipped file'\r\n> IOError: Not a gzipped file\r\n> >>> exit()\r\n> ```\r\n> ", "comments": ["It doesn't look like the downloaded file worked properly. Please cat the file and look what those 700bytes are. I'm sure they are a text error message saying why the download fails. See, my MNIST_data/train-images-idx3-ubyte.gz is *9912422 bytes* and yours is *727 bytes*. Catting the file will probably tell you why it wasn't downloaded properly...\r\n\r\n```\r\n>>> from tensorflow.examples.tutorials.mnist import input_data\r\n>>> mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)\r\nSuccessfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\r\nExtracting MNIST_data/train-images-idx3-ubyte.gz\r\nSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\r\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\r\nSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\r\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\r\nSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\r\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\r\n```\r\n", "\r\nYes. It looks like my corporate firewall is preventing the service from downloading the file. \r\n\r\nI thought the proxy settings were correct on this machine, but apparently not.\r\n\r\nThanks for the help.\r\n\r\n> (tf) [bduser@param03 MNIST_data]$ cat train-images-idx3-ubyte.gz\r\n> <HTML><HEAD>\r\n> <TITLE>Request Error</TITLE>\r\n> </HEAD>\r\n> <BODY>\r\n> <FONT face=\"Helvetica\">\r\n> <big><strong></strong></big><BR>\r\n> </FONT>\r\n> <blockquote>\r\n> <TABLE border=0 cellPadding=1 width=\"80%\">\r\n> <TR><TD>\r\n> <FONT face=\"Helvetica\">\r\n> <big>Request Error (invalid_request)</big>\r\n> <BR>\r\n> <BR>\r\n> </FONT>\r\n> </TD></TR>\r\n> <TR><TD>\r\n> <FONT face=\"Helvetica\">\r\n> Your request could not be processed. Request could not be handled\r\n> </FONT>\r\n> </TD></TR>\r\n> <TR><TD>\r\n> <FONT face=\"Helvetica\">\r\n> This could be caused by a misconfiguration, or possibly a malformed request.\r\n> </FONT>\r\n> </TD></TR>\r\n> <TR><TD>\r\n> <FONT face=\"Helvetica\" SIZE=2>\r\n> <BR>\r\n> For assistance, contact your network support team.\r\n> </FONT>\r\n> </TD></TR>\r\n> </TABLE>\r\n> </blockquote>\r\n> </FONT>\r\n> </BODY></HTML>", "Glad you figured out the problem. Closing for now.\r\n"]}, {"number": 15667, "title": "minor wording fix in (slim) readme", "body": "\"we want to restore a model from a checkpoint\r\nwhose variables have different names **to** those in the current graph.\"  (add the **to** in the line)", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Flaky test, retrying."]}, {"number": 15666, "title": "updated Readme.md", "body": "tf-nightly whl files download links added for Linux of python 3.6 version", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 15665, "title": " Fix NadamOptimizer to work with sparse gradients properly", "body": "Fix for #15035 and  #13980 ", "comments": ["Can one of the admins verify this patch?", "Nagging Reviewer @strategist333: It has been 112 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @strategist333: It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @strategist333: It has been 22 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 15664, "title": "Fix small typo in docstring which causes the API doc to render incorr\u2026", "body": "\u2026ectly\r\nSee\r\n![screen shot 2017-12-27 at 19 53 06](https://user-images.githubusercontent.com/11613312/34390366-adc111e8-eb3f-11e7-8d31-bdb0c24c32a9.png)\r\non https://www.tensorflow.org/api_docs/python/tf/train/piecewise_constant", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please."]}, {"number": 15663, "title": "Revert \"Fix a bug: bfloat16 is unsigned on Windows (#15302)\"", "body": "This reverts commit fdf34a88bec9645473f10ba2d52df4cfcb80d582.", "comments": []}, {"number": 15662, "title": "S3 Support does not work for private bucket", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: -\r\n- **GPU model and memory**: - \r\n- **Exact command to reproduce**: -\r\n\r\n\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nWhen trying to access a file on one of my private S3 buckets, I get an error message that the object does not exist (see below for source code and traceback). Using the AWS CLI downloading the object works fine(aka I'm sure I have the access rights to access the bucket). Also accessing the object in a public S3 bucket (like the one in issue #15159) works fine. I tried looking into the source code [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/s3/s3_file_system.cc#L404) to see if it's maybe an issue with missing or wrong environment variables concerning the AWS credentials, but I couldn't find any code checking for any credentials at all. Am I missing something simple?\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nWhen running the following code\r\n\r\n```python\r\nfrom tensorflow.python.lib.io import file_io\r\nfile_io.stat('s3://myprivatebucket/filethatexists')\r\n```\r\n\r\nI get the following error\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 2, in <module>\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py\", line 98, in size\r\n    return stat(self.__name).length\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py\", line 540, in stat\r\n    return file_statistics\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: Object s3://myprivatebucket/filethatexists does not exist\r\n```", "comments": ["Could you try building locally @yongtang's [branch which contains more logging](https://github.com/yongtang/tensorflow/tree/15159-s3-logging) to see if there's anything more useful logged?\r\n", "Hmm, I tried doing that but unfortunately don't get any more detailed logging. Is there a command I could try that should trigger a more detailed error message with @yongtang's AWS logging - just to double-check that at least that is working correctly?", "@yongtang should confirm, but it looks like it is obeying the standard environment variable `TF_CPP_MIN_LOG_LEVEL` - if you have this unset or set to 0 it should dump the most amount of logging. If that's already your case, maybe you can add a logging in `AWSLogSystem::InitializeAWSLogging` in `tensorflow/core/platform/s3/aws_logging.cc` to note that it has been spun up. ", "@timonbimon The following is the expected output with AWS logging enabled (with all default value without customerization). Please see if you could get something similar.\r\n```\r\nubuntu@ubuntu:~$ python -c \"from tensorflow.python.lib.io import file_io; file_io.stat('s3://datasets.elasticmapreduce/ngrams/books/20090715/eng-1M/1gram/data')\"\r\n2017-12-28 09:49:22.726196: I tensorflow/core/platform/s3/aws_logging.cc:53] Initializing Curl library\r\n2017-12-28 09:49:23.134517: I tensorflow/core/platform/s3/aws_logging.cc:53] Initializing config loader against fileName /home/ubuntu//.aws/config and using profilePrefix = 1\r\n2017-12-28 09:49:23.134559: I tensorflow/core/platform/s3/aws_logging.cc:53] Initializing config loader against fileName /home/ubuntu//.aws/credentials and using profilePrefix = 0\r\n2017-12-28 09:49:23.134570: I tensorflow/core/platform/s3/aws_logging.cc:53] Setting provider to read credentials from /home/ubuntu//.aws/credentials for credentials file and /home/ubuntu//.aws/config for the config file , for use with profile default\r\n2017-12-28 09:49:23.134579: I tensorflow/core/platform/s3/aws_logging.cc:53] Creating HttpClient with max connections2 and scheme http\r\n2017-12-28 09:49:23.134596: I tensorflow/core/platform/s3/aws_logging.cc:53] Initializing CurlHandleContainer with size 2\r\n2017-12-28 09:49:23.134607: I tensorflow/core/platform/s3/aws_logging.cc:53] Creating Instance with default EC2MetadataClient and refresh rate 900000\r\n2017-12-28 09:49:23.134653: I tensorflow/core/platform/s3/aws_logging.cc:53] Successfully reloaded configuration.\r\n2017-12-28 09:49:23.134702: I tensorflow/core/platform/s3/aws_logging.cc:53] Initializing CurlHandleContainer with size 25\r\n2017-12-28 09:49:23.134896: I tensorflow/core/platform/s3/aws_logging.cc:53] Pool grown by 2\r\n2017-12-28 09:49:23.134914: I tensorflow/core/platform/s3/aws_logging.cc:53] Connection has been released. Continuing.\r\n2017-12-28 09:49:23.419656: I tensorflow/core/platform/s3/aws_logging.cc:53] Connection has been released. Continuing.\r\n2017-12-28 09:49:23.491859: I tensorflow/core/platform/s3/aws_logging.cc:53] Cleaning up CurlHandleContainer.\r\n2017-12-28 09:49:23.491891: I tensorflow/core/platform/s3/aws_logging.cc:53] Cleaning up CurlHandleContainer.\r\nubuntu@ubuntu:~$ \r\n```", "ah that was my bad. I git-cloned @yongtang's repository and then forgot to switch the branch.\r\nThe logging works (great stuff @yongtang \ud83d\udc4d) and the error was \r\n`The authorization header is malformed; the region 'us-east-1' is wrong; expecting 'eu-west-1'`\r\n\r\nThe fix that worked for me was to explicitly do an `export AWS_REGION=eu-west-1`.\r\n\r\nHowever the region set in my ~/.aws/config was already 'eu-west-1' so there seems to be an error that this value wasn't taken by default (or it was overwritten later on when the request was made).", "Thanks for digging into this @quaeler @yongtang, glad @timonbimon could find a solution.\r\n\r\n@jhseu FYI\r\n\r\nFrom a cursory reading of https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/core/platform/s3/s3_file_system.cc#L40 - it seems that only the environment variables are used and the configuration in the filesystem is not accessed. But I could be wrong.\r\n\r\nContributions to address this are welcome!", "The AWS C++ SDK does not use the config file by default though it is possible to load the config file explicitly. Added a PR #15723 for the fix. Please take a look if interested.", "Automatically closing this out since I understand it to be resolved by the PR #15723 (merged already), but please let me know if I'm mistaken.Thanks!"]}, {"number": 15661, "title": "Fix typo 'updaye' in audio_recognition.md", "body": "Fix typo 'updaye' to 'update' in audio_recognition.md", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "I can't find your CLA in the database. Could you double check?", "I just checked my existing CLA data, there is one agreement. Is the email case sensitive? Or does the name in CLA need to match the one in the commit?", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please."]}, {"number": 15660, "title": "Distributed training fault tolerance", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04.5 LTS\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1\r\n- **Python version**: Python 3.6.3\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: release 8.0, V8.0.44/ libcudnn v6.0.21\r\n- **GPU model and memory**: Titan X (Pascal) 12 GB\r\n- **Exact command to reproduce**: In the description\r\n\r\n### Describe the problem\r\nI am using the parameter server/master/worker paradigm to run model training in distributed mode. The master node does the training and evaluation, while the worker nodes only do the training (on their shard of the data). I should also note that I am using the `tf.contrib.learn.Experiment` interface.\r\n\r\nThis model of distributed training works as expected, however, sometimes one or more of the worker nodes fail. While they have failed, the master node and other worker nodes continue the training. The problem is that when this occurs (even when only one of the worker nodes have failed), the loss suddenly becomes zero, and as a result gradients as well become zero, while the metrics suddenly change to the value of a random model, as can be seen in the figures below.\r\n\r\n* Loss curve. As can be seen one or more of the workers have failed three times during the training.\r\n![loss_fail](https://user-images.githubusercontent.com/1921165/34386146-3ed3d652-eaf5-11e7-99de-7923862089e6.jpg)\r\n\r\n* Gradient norm curve\r\n![gradient_fail](https://user-images.githubusercontent.com/1921165/34386155-48760dba-eaf5-11e7-86f9-d653cda03a3e.PNG)\r\n\r\n* Accuracy (on the validation set)\r\n![accuracy_fail](https://user-images.githubusercontent.com/1921165/34386161-51b1f114-eaf5-11e7-8a6c-303d8972b2b8.PNG)\r\n\r\n\r\n**Is there a way to prevent this behavior, either by stopping the training when one of the workers fails, or pausing the training until the failed worker comes back online again (like in the beginning of the training when training only starts when all of the workers come online)?**\r\n\r\n### Source code / logs\r\nAs indicated above, I use the experiment interface. This is the configuration for distributed training:\r\n```python\r\ndist_config = {}\r\ndist_config['cluster'] = {\r\n    'master': ['127.0.0.1:{}'.format(dist_start_port + 1)],\r\n    'ps': ['127.0.0.1:{}'.format(dist_start_port)],\r\n    'worker': ['127.0.0.1:{}'.format(dist_start_port + 2 + i)\r\n            for i in range(worker_count)]\r\n}\r\ndist_config['task'] = {\r\n    'type': dist_type,\r\n    'index': (worker_index if args.dist_type == 'worker' else 0)\r\n}\r\ndist_config['environment'] = 'cloud'\r\nos.environ['TF_CONFIG'] = json.dumps(dist_config)\r\n```\r\nAnd this is how I start each node:\r\n```python\r\nif dist_type == 'master':\r\n    experiment.train_and_evaluate()\r\nelif dist_type == 'ps':\r\n    experiment.run_std_server()\r\nelse:\r\n    experiment.train()\r\n```\r\n", "comments": ["@xiejw @martinwicke : Mind taking a look?", "This should not happen. \r\n\r\nBut in order to understand the problem, I would like to ask\r\n- I assume the curves are grabbed on Tensorboard, right? \r\n- When you observed this problem, how many workers did you deploy in the cluster?\r\n- I observed a periodic up-and-down on the graph, e.g., the loss curve dives almost every 4.5K steps. Is this expected?\r\n- If possible, do you have a small self-contained repro script? \r\n\r\nThanks in advance. \r\n", "* Yes, I grabbed them from Tensorboard.\r\n* I had 5 workers, and when it happened one of them failed due to out of memory error.\r\n* No that is not expected, the dives happen when the failure happens, i.e. when one of the workers stops working.\r\n* I will try to create a small repro script asap.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Originally, this error happened when training a large recurrent network. I tried reproducing it with a small convolutional network but I wasn't successful.\r\nAs a result I am closing this issue for now, but I will reopen after I have reproduced it."]}, {"number": 15659, "title": "Document Bazel-Tensorflow-Cuda interdependencies", "body": "Since each version of Tensorflow appears to require some specific release of bazel, it would be helpful to have documentation like [this](https://www.tensorflow.org/install/install_sources#tested_source_configurations) also for people who are stuck on an older version of CUDA. For instance, I cannot upgrade to CUDA 8 on the machine I am using and am now left with the exercise of finding a working config in a space of 5 dimensions (python version, bazel version, tf version, cuda version, cudnn version).\r\nI had it once working with CUDA 7.0 and python 3.5 (and I think bazel 0.3), but cannot reproduce now.\r\nIn this concrete case, I am trying to build `r0.11` with python 3.6, cuda 7.5, cudnn 6 and bazel 0.3/0.4/0.8 and nothing's working.", "comments": ["Unfortunately we don't support cuda 7 any more. The document link you provided is more like \"Supported\" source configurations rather than tested source configurations.\r\n", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 15658, "title": "add a function to add extra logging handler", "body": "I want to separate different level of logs. So error message will not be buried by info/debug logs. In r1.4.  I use following code to do the job.\r\n\r\n```\r\n  file_hanlder = logging.FileHandler(\"running_error.log\", mode='w', encoding=None, delay=False)\r\n  file_hanlder.setLevel(logging.WARNING)\r\n  tf.logging._logger.addHandler(file_hanlder)\r\n```\r\nbut, when I change to r1.5, it don't work any more. because tensorflow rewrite tf_logging.py and _logger  is not exposed.  So I think add a function to add extra handler should be useful.", "comments": ["Can one of the admins verify this patch?", "any comments on this PR?", "I believe there is a plan underway to move to [ABSL libraries for logging](https://github.com/abseil/abseil-py), so adding to the existing logging APIs may not make sense.\r\n\r\n@martinwicke @yilei would have more information on the timeline there.", "I'm not actively working on the `logging` library switch (only did for `tf.flags`).\r\n\r\nAs for the original request, why not adding your handler to the root logger?\r\n\r\n```python\r\nfile_hanlder = logging.FileHandler(\"running_error.log\", mode='w', encoding=None, delay=False)\r\nfile_hanlder.setLevel(logging.WARNING)\r\nlogging.getLogger().addHandler(file_hanlder)\r\n```", "my intention is to capture warning/error level of tensorflow log. I think I can do it by this. Now I think a dedicated API is redundant.\r\n\r\n```\r\ntf_logger = logging.getLogger('tensorflow')\r\ntf_logger.addHandler(file_hanlder)\r\n```"]}, {"number": 15657, "title": "Removed misplaced quote char", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 15656, "title": "CUDA 9.1 and TensorFlow", "body": "I am using NVIDIA GeForce GTX 1050 and installed NVIDIA 387.26. I installed cuDNN 7.0.5 and CUDA 9.1. As of my understanding, I know that, tensorflow is not supported in CUDA 9.1. My question is when I can expect the next build/release of TF to support CUDA 9.1. For the time being, shall I make a link from CUDA 9.0 to CUDA 9.1 and expect to work? Or is there any better way to solve the problem?", "comments": ["@gunan @av8ramit @case540 may have some information, especially relevant to the upcoming 1.5 release.", "will be 1.6, will be out in about 2 months.\r\n\r\n1.5 is building with 9.0. However if you build from sources, I am sure it should just work.", "tf-nightly-gpu latest should also work with CUDA 9.0", "@av8ramit Is it a branch or tag name? I couldn't find it.", "@jonghyunc r1.5 is the branch name. The release tag hasn't been released yet as we are still creating rc0.\r\n  \r\ntf-nightly-gpu is a pypi package that builds every night and is indexed by date\r\nhttps://pypi.python.org/pypi/tf-nightly-gpu", "@av8ramit Thank you! Will try r1.5 for running tf on cuda 9.1\r\n\r\n(updated)\r\nGot it. Will try the tf-nightly-gpu  ", "@av8ramit tf-nightly-gpu is built only on cuda 9.0 not 9.1. Do you have a version that is built with cuda 9.1?\r\n  ", "We are still working kinks out with cuda 9.1 Once that goes into master tf-nightly-gpu will automatically be built with 9.1 @gunan might have more information", "Thank you @av8ramit for the update! @gunan Do you have a rough idea when the version with cuda 9.1 is going to be ready?", "It wont be ready until we release 1.5 final. That would be at least a month away.\r\nBut building from sources with cuda 9.1 should work just fine.", "@gunan Got it! Thanks!", "Try this:  https://drive.google.com/open?id=1WdlN-NRlXQWIJrDAlw9aum7ooxI_-A4L\r\nCuda9.1 wheel with tensorflow v1.4.1\r\n", "@jonghyunc @gunan I can confirm that building TF 1.5 from source with CUDA 9.1 and cuDNN 7.0.5 works fine on Ubuntu 16.04. Works fine for both python 2.7.12 and 3.5.2. Just built today from TF master (latest). A few weird warnings here and there during building and when I run a program, but runs fine. Haven't done any benchmarking yet.", "I have not built myself but @reedwm or @tfboyd may be able to confirm.\r\nSince 9.1 is a minor release, I would expect anything that works with 9.0 to work with 9.1.", "@samuelBB I've successfully built TF 1.5 from source (branch r1.5, a week ago, before they release the 1.5 in the master branch) with CUDA 9.1 and cuDNN 7.0.5 on Ubuntu 16.04; can train a network and performs inference. Tested on Titan V, Titan Xp, and Titan X and had no problem.", "@gunan @jonghyunc Ah, I think I was unclear in my comment. I meant to say that *I can confirm...* as in I was able to successfully do the things described in my comment. I edited my above comment. ", "Hi noob here, how would I build from source? is there a guide? I don't quite understand what it means to built from source and I've just spent entire day finding this post as I'm stuck on this specific issue. ... thanks \ud83d\ude04 Also I'm on windows 8.1 which is quite painful... I might just go re download the cuda 9.0", "I found this to be a very good guide when I was building from source: http://www.python36.com/install-tensorflow141-gpu/\r\nThe guide is for TF1.4 and CUDA9.1 but equally applies with TF1.5 or other combinations; just make sure to specify the correct files and version numbers. Cheers.", "@zhang-yf maybe [this](https://www.tensorflow.org/install/install_sources) will also help?", "Could someone tell me how to find the exact version of the dependencies used in the binaries? I assumed 9.1 was fine for 1.5 RC0, but it didn't work (in Windows; cf. #16014), so I had to spend a while un- and re-installing stuff. ", "Thanks @samuelBB  and @av8ramit, I see now I just needed to specify the Cuda sdk version to be 9.1 while building from source. That seems straightforward. I was wondering would it be possible to write a script so that while installing through  pip it will just grab the 9.x version since it makes no difference the minor release version in this case. Also @navneethc  you have the same problem which lead me here, if you check the log when importing tensorflow it says it tries to find the 9.0 and it couldnt find the cudart64_90.dll because the cuda 9.1 have the cudart64_91.dll instead. Which is why im going to re install cuda 9.0 \ud83d\ude06 .. because windows.", "@zhang-yf Yes, thankfully I\u2019m past all that and running TF (that\u2019s where the the uninstalls and reinstalls happened:) ). I\u2019m now requesting a reference for the future. :D ", "You can find couda older version(CUDA 9) here:\r\nhttps://developer.nvidia.com/cuda-toolkit-archive", "Just to update, after discussing with nvidia, and our team we decided to stay with cuda 9.0 for 1.6 release.", "Will there be a cuda 9.1-compatible version?  :(\r\n\r\nAnd why did you avoid cuda 9.1? Technical issues?", "As indicated just one comment above, this is a decision after we discussed with NVIDIA.\r\nThe answer to why is driver issues in the ones required by 9.1, not many new features we need in cuda 9.1, and a few more minor issues. 1.7 is also building right now with CUDA 9.0", "Ok thanks for replying.\n\nOn Mon, 12 Mar 2018 at 03:42, Gunhan Gulsoy <notifications@github.com>\nwrote:\n\n> As indicated just one comment above, this is a decision after we discussed\n> with NVIDIA.\n> The answer to why is driver issues in the ones required by 9.1, not many\n> new features we need in cuda 9.1, and a few more minor issues. 1.7 is also\n> building right now with CUDA 9.0\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15656#issuecomment-372207184>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABn8RWXD4nZGIhP9wwYezVZ-3IgCPWgeks5tdhjygaJpZM4RNhrd>\n> .\n>\n", "As far as I know Volta optimizations are only in 9.1, like CuBLAS for Volta etc, and the use of the tensor cores.  I guess nobody is using Tesla V100 or the Titan V with tensorflow so its not important...", "I am using cuda 9.1 and cudnn 7.0. Could we use tensorflow-1.6 build from pip? If not, how about build from source?", "Followed the instructions in this link to build tensorflow with cuda 9.1 (cuDNN 7.1). Now everything works fine for me. http://www.python36.com/install-tensorflow-gpu-windows/", "Tensorflow 1.7RC1 is out, but the CUDA version is not specified. Is CUDA 9.1 now supported?", "@michaelklachko have you tried installing with 9.1 yet? If so, could you please post the results?", "Building from source with 9.1 works fine for me.", "@hitvoice  Which version of tensorflow did you build from source? Also, which operating system do you use? ", "@erolrecep 1.7.0, on Ubuntu Server 16.04", "@hitvoice @erolrecep Can you post the whl files?", "@hitvoice Me too,can you give a download page for  the whl files?", "@boyangwm @Oldpan @erolrecep Hope it helps.\r\n\r\n- [tensorflow-1.7.0-cp36-cp36m-linux_x86_64.whl](https://drive.google.com/open?id=15iGFiagWcwTSzJMpOxYvRad-yKGjTB9J)\r\n- [tensorflow-1.8.0rc0-cp36-cp36m-linux_x86_64.whl](https://drive.google.com/open?id=1xqHxMrsGQLsZhT0XMBfmJsccjFpJtXa5)\r\n\r\nEnvironment:\r\n- python 3.6\r\n- Ubuntu Server 16.04\r\n- CUDA 9.1, CUDNN 7.1, both in `usr/local/cuda`\r\n- for tf1.8.0rc0, NCCL2.1 is in `/usr/local/nccl_2.1.15-1+cuda9.1_x86_64` (NCCL 2.1.15 O/S agnostic)", "@hitvoice Thanks, man!", "BTW, when I did the search online yesterday, I found this repository is very helpful.\r\nhttps://github.com/mind/wheels\r\n\r\nI can use a whl file from the repo, and the problem was solved. Refer this link to folks who have the same problem with running Tensorflow in CUDA 9.1", "@hitvoice \r\nDude,I try your whl file 'tensorflow-1.7.0-cp36-cp36m-linux_x86_64.whl' just now but I still get a wrong message: \"libcublas.so.9.1: cannot open shared object file: No such file or directory\".And I have no idea.\r\nI built a same whl before and the build process is right and then I got a same problem.So I tried yours.But it still doesn't work.Maybe some configuration in my system doesn't fit?\r\n\r\nMy environment:\r\n- Cuda compilation tools, release 9.1, V9.1.85\r\n- cudnn:7.1.2\r\n- nvidia-driver:390.48\r\n- card:1080ti\r\n- ubuntu:16.04\r\n\r\nThanks again~", "@boyangwm I'll take a look at this repository,hope this will save my time.", "@Oldpan Does the file `/usr/local/cuda/lib64/libcublas.so.9.1` exist on your machine?", "@hitvoice \r\nThanks!\r\nI figured out yesterday. It's a subtle mistake that I didn't read erroe message carefully. There are many files missing in /usr/lib, so I do a link:\r\n```\r\nsudo ln -s /usr/local/cuda-9.1/lib64/libcublas.so.9.1 /usr/lib/libcublas.so.9.1\r\nsudo ln -s /usr/local/cuda-9.1/lib64/libcusolver.so.9.1 /usr/lib/libcusolver.so.9.1\r\nsudo ln -s /usr/local/cuda-9.1/lib64/libcudart.so.9.1 /usr/lib/libcudart.so.9.1\r\nsudo ln -s /usr/local/cuda-9.1/lib64/libcudnn.so.7 /usr/lib/libcudnn.so.7\r\nsudo ln -s /usr/local/cuda-9.1/lib64/libcufft.so.9.1 /usr/lib/libcufft.so.9.1\r\nsudo ln -s /usr/local/cuda-9.1/lib64/libcurand.so.9.1 /usr/lib/libcurand.so.9.1\r\n```\r\nAnd it works....", "Just wanted to remind you guys that it's been a while and CUDA 9.1 still isn't supported, even by the nightly build.\r\n\r\nThis means I still can't use TensorFlow because my system requires CUDA 9.1 for another purpose. I hope this can be fixed soon.", "@StevenGann I am running TensorFlow on a system that has CUDA 8, 9.0 and 9.1 installed, with TensorFlow using 9.0. It works. So you can have CUDA 9.1 installed \"for another purpose\" and still use TensorFlow.", "@bersbersbers This doesn't seem to be the case. When I attempt to install CUDA 9.0, the installation runs but it only updates 9.1's installation. The Nvidia installer does not permit me to install 9.0 as long as 9.1 is installed.\r\n\r\nAlso, it seems fairly lazy to stick with an outdated library version and require people to uninstall and reinstall drivers, especially when the newer version was supposed to be supported months ago. Is there a technical reason why TensorFlow's developers can't figure out how to use 9.1?", "@StevenGann I am almost certain that I installed 9.0 after 9.1 (I think I did 9.1, 8.0, 9.0 until I got it working). That was in Windows 10. Anyway, uninstalling 9.1 and installing 9.0, followed by 9.1 looks like it should work?", "@StevenGann if you build from sources, I believe you can use CUDA 9.1 as it asks for the CUDA version you want to build with.", "Just seconding what a pain this is. The CUDA link on the page at https://www.tensorflow.org/install/install_windows will end you up at https://developer.nvidia.com/cuda-downloads . This ONLY shows 9.1 now. You need to go into the \"Legacy releases\" area to get 9.0. I spent forever waiting for that GB install of 9.1 only to then hit this problem.\r\n\r\nI resolved it by installing using the \"network\" install from https://developer.nvidia.com/cuda-90-download-archive , and doing a custom install of only the runtime libraries, then ensuring `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin` was on my path. That got me working in a few mins and is fine side-by-side with 9.1).", "Are there plans for tensorflow to upgrade to CUDA v9.1?", "While waiting for the official release of TensorFlow with CUDA 9.1, [here is another attempt](http://tech.amikelive.com/node-746/guide-installing-tensor-flow-1-8-with-gpu-support-against-cuda-9-1-and-cudnn-7-1-on-ubuntu-16-04/) to describe how TensorFlow 1.8 can be built against CUDA 9.1.\r\n\r\nThe final system configuration will be:\r\n\r\n- OS: Ubuntu 16.04\r\n- CUDA version: 9.1\r\n- cuDNN version: 7.1.3\r\n- NCCL version: 2.1.15\r\n- Python version: 2.7.12\r\n- Python install method: virtualenv\r\n- TensorFlow version: 1.8.0\r\n\r\nTroubleshooting for common errors is also added in [the article content](http://tech.amikelive.com/node-746/guide-installing-tensor-flow-1-8-with-gpu-support-against-cuda-9-1-and-cudnn-7-1-on-ubuntu-16-04/). I found out that the build process was smooth like sailing a calm ocean (alternative meaning: it can be drowsy to wait for the build to complete).", "win7 Service Pack1+Anaconda4.3.30+python3.6.1+tf1.8.0-gpu+CUDA9.0+cuDNN7.1  works!\r\nMy GPU is GF940mx", "Which cuda version can tf-nightly-1.9.0 work with? \r\nMy system configuration:\r\n- OS:Ubuntu 16.04\r\n- Python version 3.6\r\n", "sorry, i am working under win7.\n\n\n\n\n\n\nAt 2018-06-14 22:20:23, \"Teng Li\" <notifications@github.com> wrote:\n\n\nWhich cuda version can tf-nightly-1.9.0 work with?\nMy system configuration:\n\nOS:Ubuntu 16.04\nPython version 3.6\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.", "Still waiting on actual support because not everyone can build from sources. I, for example, need it in a conda environment which is created in Docker (and therefore can't be activated to build then switched back to the root environment for the rest of the build tasks). \r\n\r\nGiven the feedback here in this comment chain, it shouldn't be a lot of work to ship a 9.1 compatible version. Hell give me a week with access to the relevant parts of your deployment chain and I'll do it! I just need this to be a thing.", "> Still waiting on actual support because not everyone can build from sources. I, for example, need it in a conda environment which is created in Docker (and therefore can't be activated to build then switched back to the root environment for the rest of the build tasks).\r\n> \r\n> Given the feedback here in this comment chain, it shouldn't be a lot of work to ship a 9.1 compatible version. Hell give me a week with access to the relevant parts of your deployment chain and I'll do it! I just need this to be a thing.\r\n\r\nIf it helps, Anaconda offers its own packaged version of Tensorflow compiled with CUDA 9.2. And supposedly it's faster than pip-installed TF.\r\n\r\nhttps://www.anaconda.com/tensorflow-in-anaconda/", "> > Still waiting on actual support because not everyone can build from sources. I, for example, need it in a conda environment which is created in Docker (and therefore can't be activated to build then switched back to the root environment for the rest of the build tasks).\r\n> > Given the feedback here in this comment chain, it shouldn't be a lot of work to ship a 9.1 compatible version. Hell give me a week with access to the relevant parts of your deployment chain and I'll do it! I just need this to be a thing.\r\n> \r\n> If it helps, Anaconda offers its own packaged version of Tensorflow compiled with CUDA 9.2. And supposedly it's faster than pip-installed TF.\r\n> \r\n> https://www.anaconda.com/tensorflow-in-anaconda/\r\n\r\nUnfortunately I'm specifically stuck on 9.1 due to the host system(s) that I'd be running on. And the fact that bloating my images with further CUDA installations is not an attractive solution.. Otherwise that would be great, yeah"]}, {"number": 15655, "title": "tf.layers.conv3d with \"channels_first\" does not accept batch dimension to be None", "body": "code to reproduce:\r\n\r\n```python\r\nimport tensorflow as tf\r\nx = tf.placeholder(dtype=tf.float32, shape=[None, 1, 32, 32, 32])\r\ny = tf.layers.conv3d(x, 32, 9, data_format='channels_first')\r\n```\r\n\r\ntraceback\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\", line 468, in make_tensor_proto\r\n    str_values = [compat.as_bytes(x) for x in proto_values]\r\n  File \"/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\", line 468, in <listcomp>\r\n    str_values = [compat.as_bytes(x) for x in proto_values]\r\n  File \"/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/util/compat.py\", line 65, in as_bytes\r\n    (bytes_or_text,))\r\nTypeError: Expected binary or unicode string, got None\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py\", line 809, in conv3d\r\n    return layer.apply(inputs)\r\n  File \"/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 671, in apply\r\n    return self.__call__(inputs, *args, **kwargs)\r\n  File \"/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 575, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py\", line 185, in call\r\n    outputs_shape[4]])\r\n  File \"/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3938, in reshape\r\n    \"Reshape\", tensor=tensor, shape=shape, name=name)\r\n  File \"/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 513, in _apply_op_helper\r\n    raise err\r\n  File \"/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 510, in _apply_op_helper\r\n    preferred_dtype=default_dtype)\r\n  File \"/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 926, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 229, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 208, in constant\r\n    value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"/usr/local/Cellar/pyenv/1.2.0/versions/3.6.3/envs/mrtoct/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\", line 472, in make_tensor_proto\r\n    \"supported type.\" % (type(values), values))\r\nTypeError: Failed to convert object of type <class 'list'> to Tensor. Contents: [None, 32, 576, 24]. Consider casting elements to a supported type.\r\n```\r\n\r\nThe error source appears [here][1] and can be simply fixed by adding\r\n\r\n```python\r\nif outputs_shape[0] is None:\r\n  outputs_shape[0] = -1\r\n```\r\n\r\nhowever you might suggest a deeper fix?\r\n\r\n\r\n[1]: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/convolutional.py#L181-L185", "comments": ["Thank you, @bodokaiser . I think the issue might have been resolved in  #14120, could you test it on tf-nightly?", "issue still occurs with tf_nightly-1.5.0.dev20171222-cp36-cp36m-macosx_10_12_x86_64.whl ", "@bodokaiser Yes, it seems that batch_size=None will raise the exception here.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/996ae8e750e8abd18166b40559043a88d7bdc1ce/tensorflow/python/layers/convolutional.py#L181-L187\r\n\r\nI agree that casting `None` to `-1` could solve the problem, however it would be better if we could find a general solution, say, to use dynamic shape? Sorry that I have no good idea, let's wait for reply from tensorflowers.", "@martinwicke", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "I think casting to -1 is fine. @fchollet is there a reason that's a bad idea?", "This is fixed by #16307", "The issue is **not** fixed apparently. Only Conv3DTranposed seems to be fixed, but the last nightly build still provides the same error with tf.layers.conv3d.\r\n\r\nCan someone confirm the bug and reopen the issue?\r\n\r\nThe trick in the initial post solves the issue. ", "You are correct, sorry for closing. The code sample in the first post still has an error.\r\n\r\n@fchollet can you fix this?", "Please submit a PR.", "Added a PR #18027 for the fix."]}, {"number": 15654, "title": "Enable `axis` support for `tf.unique`", "body": "The `axis` support for `Unique` has been Added in PR #12952 (defined in `UniqueV2` ops). The support for `axis` in python version of the `tf.unique` was not enabled yet, due to the API workflow porcess (3 weeks). This fix adds the support for `axis` with `tf.unique` by adds `Unique` to `hidden.txt`, and adds a python wrapper of `tf.unique` to pointing to `UniqueV2`.\r\n\r\nThis fix addresses part of the issue #15644.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @drpngx. The PR tries to handle two cases. One is `axis = nil` and another is axis is not nil (`0, 1, 2, etc`).\r\n\r\nHowever, the `Input` field does not have a way to handle `not exist` case. (Ref https://stackoverflow.com/questions/42754965/marking-input-as-optional-in-tensorflow).\r\n\r\nSo this PR tries to use `axis = []` to represent `axis = nil` and use `axis =[0]` (one-element array) to represent `axis = 0`.\r\n\r\nI think alternatively there are some other ways to simplify:\r\n1. We could use `Attr` for `axis` then it might be possible to have a `not exist` case.\r\n2. We could create two Ops, one for `UniqueOp` and another for `UniqueWithAxisOp` so that axis = None could be routed to `UniqueOp` and axis = 0 could be routed to  `UniqueWithAxisOp`.\r\n\r\nNot sure which way is the best. Maybe we could use `Attr` instead. \r\n", "@drpngx I updated the PR. Now `unique` will be routed to `gen_array_ops._unique` if axis = None and `gen_array_ops._unique_v2` otherwise. `UniqueV2` will take a scalar axis now. That should make the `def unique()` simpler. Please take a look.", "So `axis=None` is not `axis = []`? Basically an empty tensor.", "@drpngx In case `None` is passed an ValueError as TensorFlow will try to convert to a Tensor:\r\n```\r\nValueError: Tried to convert 'axis' to a tensor and failed. Error: None values not supported.\r\n```", "I see. Can the wrapper do that conversion?", "@drpngx An easy way is to pass `[]` if axis is `None`, then `[]` will be converted to a 1-D tensor. An explicit conversion is also possible I think.", "OK, sounds good.", "@drpngx The PR has been updated. Please take a look.", "Jenkins, test this please.", "Thanks @drpngx. The previous Jenkins error was caused by `//tensorflow/tools/api/tests:api_compatibility_test`. This is expected as we switch `tf.unique` from Unique to UniqueV2.\r\n\r\nI have run API golden and updated the PR:\r\n```\r\nbazel-bin/tensorflow/tools/api/tests/api_compatibility_test\r\n           --update_goldens True\r\n```\r\n\r\nI think the Jenkins should pass now.", "@vrv does the hiding trigger a compatibility issue?", "Jenkins, test this please.", "I doubt it, since the python API contract is the same, but you can check with @annarev maybe?", "@drpngx @vrv The PR switched `tf.unique` in python from Unique to UniqueV2 so there is one extra `axis` for `tf.unique`.\r\n\r\nIt also hides `Unique` and `UniqueV2`.\r\n\r\nPreviously `Unique` was not hidden and was used by `tf.unique` implicitly (`gen_array_ops.unique`). \r\n\r\nThe `UniqueV2` probably should have been hidden at the first place though it will not have an impact on user I think.", "Yep, it shouldn't cause a compatibility issue.", "Thanks for the PR. A few things from API review:\r\n\r\n- The `UniqueV2` operation takes an `int64` axis. It should take an `int32` consistent with other operations that take an axis argument. It should be fine to change `UniqueV2` in place since there are no users of it. (And then we'll have to wait another 3 weeks for the Python change)\r\n\r\n- Documentation of the `UniqueV2` op in C++ seems incorrect (for example, starts with \"1-D tensor \" :)\r\n\r\n- Update the documentation for `tf.unique` to include examples with the `axis` argument.", "@asimshankar Thanks. The PR has been updated and v2 switching has been removed. Now this PR does:\r\n1. Change axis from int64 to int32.\r\n2. Update docstring in array_ops.cc\r\n3. Hide both Unique and UniqueV2\r\n4. Create a thin python wrapper to point `tf.unique` to `gen_array_ops._unique`. (Will switch to v2 3 weeks after this PR is merged).\r\n\r\nPlease take a look and let me know if there are any issues.", "Actually, I think it makes sense to support both int32 and int64 axis. The PR has been updated for that. Please take a look.", "(good for API)\r\n\r\nBut could you add an example to the documentation that shows what happens when axis is set?", "Thanks @martinwicke. The PR has been pushed with docstring update. Please take a look.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Argh -- I think while this happened, @annarev merged the change which moves the docs to APIDef, creating a nontrivial conflict. ", "@martinwicke Let me take a look and fix the conflict.", "@yongtang and @martinwicke to fix it, you can just remove newly added/updated .Doc() calls.\r\napi_def_*.pbtxt are now the location where docs are stored.", "Thanks @martinwicke @annarev. The PR has been rebased and pushed. Please take a look.", "Jenkins, test this please.", "@martinwicke @drpngx All tests passed except `cla/google` which seems to stuck. Maybe it requires an override?", "Yes, since I added a commit, it got stuck."]}, {"number": 15653, "title": "Clarify batch_norm documentation to highlight that the dimensions for\u2026", "body": "... normalization depend on the shape of the tensor.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "Yes I used a different e-mail address for the commit. I cannot add this address to the google account where I signed the contract.", "Could you change the email address on the commit?", "Ok I changed the e-mail address in my commit. However this is not a standard procedure with git and requires to rewrite all commits. As far as I can see, this pull-request just contains my change, so I think it should work now.", "OK, maybe let's wait until the next refresh.", "@Netzeband Still broken. I don'y anything pushed. You can amend the commit with `--author` and push again.", "I think this will go into the wrong direction, I will close the request and setup a new one, where not the whole commit history have been rewritten (due to the e-mail address change).", "I created a new request: https://github.com/tensorflow/tensorflow/pull/15728\r\n"]}, {"number": 15652, "title": "fix variable name", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks!\r\n\r\nJenkins, test this please.", "Grrr, `math_grad_test` and `cloud_file_block_cache` flakiness."]}, {"number": 15651, "title": "Not found: FeedInputs: unable to find feed output Mul", "body": "@satok16 I have already set up and was able to run `hexagon_graph_execution` on my hvx board, however, when I tried to use my own [inception-v3 pre-trained model](https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models) that I froze using [this quantization method](https://www.tensorflow.org/performance/quantization), I am receiving this error:\r\n\r\n```\r\n[ RUN      ] GraphTransferer.RunInceptionV3OnHexagonExampleWithTfRuntime\r\nnative : hexagon_graph_execution_test.cc:519 Fuse and run inception v3 on hexagon with tf runtime\r\nnative : hexagon_graph_execution_test.cc:94 Hexagon controller version is 90\r\nnative : hexagon_graph_execution_test.cc:142 Read /data/local/tmp/img_299x299.bmp, size = 269156bytes\r\nnative : hexagon_graph_execution_test.cc:148 header size = 54\r\nnative : hexagon_graph_execution_test.cc:150 image size = 40\r\nnative : hexagon_graph_execution_test.cc:152 width = 299\r\nnative : hexagon_graph_execution_test.cc:154 height = -299\r\nnative : hexagon_graph_execution_test.cc:533 Ioading image finished.\r\nt1(loading image time)=0.026770\r\nnative : hexagon_graph_execution_test.cc:546 Build fused graph\r\nnative : remote_fused_graph_execute_utils.cc:259 Error during inference: Not found: FeedInputs: unable to find feed output Mul\r\nnative : graph_transfer_utils.cc:110 Check failed: status.ok()\r\nAborted\r\n```\r\nDo you know if the issue is because of an incorrect input argument here: \r\n\r\n```\r\ncurl -L \"https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz\" |\r\n  tar -C tensorflow/examples/label_image/data -xz\r\nbazel build tensorflow/tools/graph_transforms:transform_graph\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n  --in_graph=tensorflow/examples/label_image/data/inception_v3_2016_08_28_frozen.pb \\\r\n  --out_graph=/tmp/quantized_graph.pb \\\r\n  **--inputs=input \\**\r\n  --outputs=InceptionV3/Predictions/Reshape_1 \\\r\n  --transforms='add_default_attributes strip_unused_nodes(type=float, shape=\"1,299,299,3\")\r\n    remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true)\r\n    fold_batch_norms fold_old_batch_norms quantize_weights quantize_nodes\r\n    strip_unused_nodes sort_by_execution_order'\r\n```\r\n\r\nI found on [this] (https://stackoverflow.com/questions/43022516/tensorflow-inception-feedinputs-unable-to-find-feed-output-input) and also [this][https://github.com/tensorflow/tensorflow/issues/2883#issuecomment-226591095](url) posts that we might have to use `Mul` instead. Tried that with no success. Interestingly, when I test my frozen_quantized graph with:\r\n\r\n`bazel-bin/tensorflow/examples/label_image/label_image --graph=/tmp/my_inception_quantized_graph_hvx.pb`\r\n\r\nI receive similar results compared to a non-quantized version, so it shows that my frozen_quantized is not faulty. Can you verify the issue here? \r\nWas the file `https://storage.googleapis.com/download.tensorflow.org/models/tensorflow_inception_v3_stripped_optimized_quantized.pb` used in the original hvx hexgon_graph_execution produced differently?", "comments": ["Hi @amirjamez, sorry for the delay. Is this still an issue or has it been addressed in a newer release?", "I am trying to build the more recent `libhexagon_nn_skel.so` version mentioned in [TF-HVX](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/hvx/), however, I am having a compilation error: \r\n\r\n```\r\n\r\nmake[1]: Entering directory `/home/aashouri/Qualcomm/Hexagon_SDK/3.0/examples/common/nnlib'\r\nERROR: Could not open file (/prj/dsp/qdsp6/arch/cnn/setup/inceptionv3_uint8in.c)\r\nmake[1]: *** [android_Release/inceptionv3_uint8in.o] Error 255\r\n\r\n```\r\n\r\nWorking around the issue by defining a dummy input data as mentioned by @satok16 [here](https://github.com/tensorflow/tensorflow/issues/9428) did not solve my issue either. Unless\r\n\r\n ```\r\nmake tree VERBOSE=1 V=hexagon_Release_dynamic_toolv72_v60\r\n\r\n```\r\nwill suffice to complete the toolchain.\r\n\r\nThere are many combinations of versions between `Qualcomm SDK (3.0, 3.1, 3.2 and 3.3.3)` and `libbnn` available. A clarification on the compatible versions with and also to Tensorflow build system would be appreciated. ", "@suharshs are you able to comment on this? Thanks.", "We have focused quantization efforts to https://www.tensorflow.org/performance/quantization with support in [TensorFlow Lite](https://www.tensorflow.org/mobile/tflite/).\r\n\r\nThat being said, your current issue seems specific to contrib/hvx. @satok16 @petewarden @andrewharp do you have any ideas what could be the issue here?", "I'm also encountering these errors in trying to build a demo model to run to the hexagon DSP chip. In TF Light I see good work on quantization (though a little sparse in terms of references to exactly what technique is being applied there), but I don't see any reference in TF Light to running those quantized models on the Hexagon DSP. Has there been more recent work in TF Light to support platforms like the Hexagon DSP or mobile GPUs? Is `contrib/hvx` still the right place place to look to work with the Hexagon DSP?\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/hvx\r\n", "@satok16 is the only one I know that has really worked on this, and it doesn't appear that he has been involved in it for some time.  TFLite's NNAPI delegate supports the Hexagon DSP and mobile GPU's indirectly through the vendor's NNAPI HAL drivers.  Have you tried this acceleration path? ", "I see, thanks for the reply. I'll try to set up a demo using Tensorflow Lite and the android NNAPI and see how it goes. I'm currious where XLA is going then? I was under the impression that this hvx contrib effort was a start at generic accelerated device support via XLA/LLVM.", "Closing this, since the direct HVX support in TensorFlow is deprecated in favor of the NN API through TF Lite."]}]