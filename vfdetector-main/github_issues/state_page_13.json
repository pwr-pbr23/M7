[{"number": 53615, "title": "SelfAdjointEigV2 GPU operation takes a lot of temporary memory.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.9.0\r\n- Python version: 3.8.10\r\n- CUDA/cuDNN version: 11.5\r\n- GPU model and memory: GTX 1660 Ti\r\n\r\n**Describe the current behavior**\r\nA single call to `tf.linalg.eigh()` takes a linear amount of memory in batch size, despite being processed matrix by matrix. I think the ScratchSpace is only freed in the end of the call after ALL matrices in the batch are processed, instead of on the fly matrix, per matrix. This I conclude from the enormous amount of allocations reported by the allocator during the OOM, and looking at the code.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): maybe\r\n- Briefly describe your candidate solution(if contributing):\r\n  - **Best solution:** reuse the ScratchSpace for every matrix.  \r\n  - **Next best solution:** free the ScratchSpace after every matrix in the batch.\r\n\r\n**Standalone code to reproduce the issue**\r\n```py\r\nimport tensorflow as tf\r\ntensor = tf.random.uniform((1024 * 256, 4, 4)) # just make sure the batch size is big enough\r\ntensor = tf.matmul(tensor, tensor, transpose_b=True)\r\nt = tf.linalg.eigvalsh(tensor)\r\n```\r\n\r\n**Other info / logs**\r\nSee below the summary of the allocator:\r\n```\r\n2022-01-03 16:18:32.407928: W tensorflow/core/common_runtime/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 215.4KiB (rounded to 220672)requested by op SelfAdjointEigV2\r\n\r\n2022-01-03 16:18:32.518897: I tensorflow/core/common_runtime/bfc_allocator.cc:1071]      Summary of in-use Chunks by size: \r\n2022-01-03 16:18:32.518906: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 256 totalling 256B\r\n2022-01-03 16:18:32.518916: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1280 totalling 1.2KiB\r\n2022-01-03 16:18:32.518924: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 65536 totalling 64.0KiB\r\n2022-01-03 16:18:32.518930: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 11841 Chunks of size 220672 totalling 2.43GiB\r\n2022-01-03 16:18:32.518937: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 251648 totalling 245.8KiB\r\n2022-01-03 16:18:32.518944: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 410880 totalling 401.2KiB\r\n2022-01-03 16:18:32.518951: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 4194304 totalling 4.00MiB\r\n2022-01-03 16:18:32.518958: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 268435456 totalling 512.00MiB\r\n2022-01-03 16:18:32.518964: I tensorflow/core/common_runtime/bfc_allocator.cc:1078] Sum Total of in-use chunks: 2.94GiB\r\n2022-01-03 16:18:32.518970: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] total_region_allocated_bytes_: 3154771968 memory_limit_: 3154771968 available bytes: 0 curr_region_allocation_bytes_: 6309543936\r\n2022-01-03 16:18:32.518982: I tensorflow/core/common_runtime/bfc_allocator.cc:1086] Stats: \r\nLimit:                      3154771968\r\nInUse:                      3154771968\r\nMaxInUse:                   3154771968\r\nNumAllocs:                       11850\r\nMaxAllocSize:                268435456\r\nReserved:                            0\r\nPeakReserved:                        0\r\nLargestFreeBlock:                    0\r\n\r\n2022-01-03 16:18:32.519244: W tensorflow/core/common_runtime/bfc_allocator.cc:474] ****************************************************************************************************\r\n2022-01-03 16:18:32.519294: F ./tensorflow/core/util/gpu_solvers.h:533] Non-OK-status: context->allocate_temp(DataTypeToEnum<Scalar>::value, shape, &scratch_tensor_, alloc_attr) status: RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[55137] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n```\r\n\r\nEspecially this line:  \r\n```\r\n...r.cc:1074] 11841 Chunks of size 220672 totalling 2.43GiB\r\n```\r\nSo here, I conclude that it crashes when it's trying to process matrix number 11842 out of 16384.\r\n\r\nPointers:\r\n - ScratchSpace request in the `HeevdImpl` call (at line 635): https://github.com/tensorflow/tensorflow/blob/322cba072c9313689aeb2fc3174f18fce57194d7/tensorflow/core/util/cuda_solvers.cc#L620-L643\r\n - Batch processing matrix by matrix: https://github.com/tensorflow/tensorflow/blob/322cba072c9313689aeb2fc3174f18fce57194d7/tensorflow/core/kernels/linalg/self_adjoint_eig_v2_op_gpu.cc#L130-L140", "comments": ["@sachinprasadhs ,\r\nI was able to execute the code in tf v2.7 and nightly.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/afc5e6dd5ab8a140d83d3373141be7d1/untitled177.ipynb).", "@tilakrayal @sachinprasadhs The Google Cloud collab thing has more memory than me. Changing it to\r\n\r\n```py\r\ntensor = tf.random.uniform((1024 * 256, 4, 4)) # just make sure the batch size is big enough\r\n```\r\n\r\ndemonstrates the issue (note that the total number of floats is even less than before: the batch size is what matters here)."]}, {"number": 53613, "title": "Eager loading of datasets", "body": "**System information**\r\n- TensorFlow version (you are using): 2.6.0, 2.7.0\r\n- Are you willing to contribute it (Yes/No): No (I don't have the required knowledge)\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nIn #53292, I mentioned the inability to load datasets from temporary files and mentioned eager dataset load as a potential solution. This led to the discovery and correction of a bug in `Dataset.cache`.\r\n\r\nI still believe that eager dataset load is desirable, since calling `cache` then consuming the dataset to achieve the same result feels at best like a makeshift hack. Hence I'm making this new issue as a more explicit feature request, because I do believe that this feature would be beneficial in some cases.\r\n\r\n**Will this change the current api? How?**\r\n\r\nThe feature I propose is to allow callers of `tf.data.experimental.load` to specify whether they want a lazily-loaded dataset, or an eagerly-loaded dataset (that reads the files only once when `load` is called, and stores the data in memory).\r\n\r\nThe way I see it, this evolution would add a new optional parameter to `tf.data.experimental.load`. Its default value needs to correspond to lazy loading for backward compatibility.\r\n\r\nThis would not pose a big issue since 1. it will be backward-compatible and 2. only the experimental API will be modified (and the documentation explicitly says the experimental API may evolve at any time).\r\n\r\n**Who will benefit from this feature?**\r\n\r\nAnyone who currently needs to cache a loaded dataset to memory, be it because the dataset comes from temporary files or because it is accessed very frequently\r\n\r\n**Any Other info.**\r\n\r\nAs in the previous issue:\r\n\r\nThe fact that the datasets are lazy-loaded is not documented, it would be nice to add this to the current documentation of `tf.data.experimental.load`.\r\n\r\nAlso on the subject of documentation, once (if) the eager load option is added, the documentation should warn the user about the potentially high memory usage when eager-loading large datasets.", "comments": []}, {"number": 53612, "title": "INVALID_ARGUMENT: transpose expects a vector of size 0 (when GPU units are more than 1)", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 20.04.2 LTS\r\n- TensorFlow installed from (source or binary): Yes\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.8.5\r\n- CUDA/cuDNN version &  GPU model and memory:\r\n\r\n![image](https://user-images.githubusercontent.com/9564095/147903851-129bf763-dcb2-4f47-b136-14f31ad67553.png)\r\n\r\n\r\nI am using a pre-trained model to train an image classifier. Below Code is running fine on CPU and single unit GPU (i.e. when #GPU=1)\r\n\r\n```\r\nclass Metrics(tf.keras.callbacks.Callback):\r\n    def __init__(self, train_tf_data, val_tf_data, CLASSES, logs={}, **kwargs):\r\n        super().__init__(**kwargs)\r\n        # self.keras_metric = tf.keras.metrics.Mean(\"val_f1_after_epoch\")\r\n        self.train_tf_data = train_tf_data\r\n        self.val_tf_data = val_tf_data\r\n        # self.model = model\r\n        self.CLASSES = CLASSES\r\n\r\n    def on_epoch_end(self, epoch, logs={}):\r\n        # self.keras_metric.reset_state()\r\n        # for train data\r\n        self.train_reports = test_model(model=self.model, data=self.train_tf_data, CLASSES=self.CLASSES)\r\n        self.train_f1_after_epoch = self.train_reports['f1_score']\r\n        self.train_recall_after_epoch = self.train_reports['recall']\r\n        self.train_prec_after_epoch = self.train_reports['precision']\r\n\r\n        # for val data\r\n        self.val_reports = test_model(model=self.model, data=self.val_tf_data, CLASSES=self.CLASSES)\r\n        self.val_f1_after_epoch = self.val_reports['f1_score']\r\n        self.val_recall_after_epoch = self.val_reports['recall']\r\n        self.val_prec_after_epoch = self.val_reports['precision']\r\n\r\n        # saving train results to log dir\r\n        logs[\"f1_after_epoch\"]=self.train_f1_after_epoch\r\n        logs['precision_after_epoch'] = self.train_prec_after_epoch\r\n        logs['recall_after_epoch'] = self.train_recall_after_epoch\r\n        \r\n        # saving val results to log dir\r\n        logs['val_f1_after_epoch'] = self.val_f1_after_epoch\r\n        logs['val_precision_after_epoch'] = self.val_prec_after_epoch\r\n        logs['val_recall_after_epoch'] = self.val_recall_after_epoch\r\n        # self.keras_metric.update_state(self.val_f1_after_epoch)\r\n\r\n        print('reports_after_epoch', self.train_reports)\r\n        print('val_reports_after_epoch', self.val_reports)\r\n        \r\n\r\n\r\n\r\nwith strategy.scope():\r\n    pretrained_model = tf.keras.applications.MobileNetV2(\r\n                                                    weights='imagenet',\r\n                                                    include_top=False,\r\n                                                    input_shape=[*IMAGE_SIZE, IMG_CHANNELS])\r\n    pretrained_model.trainable = True #fine tuning\r\n    q_aware_pretrained_model = tf.keras.models.clone_model(pretrained_model,\r\n                                                          clone_function=apply_quantization_to_dense,)\r\n    base_model = tf.keras.Sequential([\r\n                            tf.keras.layers.Lambda(# Convert image from int[0, 255] to the format expect by this base_model\r\n                            lambda data:tf.keras.applications.mobilenet.preprocess_input(\r\n                                tf.cast(data, tf.float32)), input_shape=[*IMAGE_SIZE, 3]),\r\n                            q_aware_pretrained_model,\r\n                            tf.keras.layers.GlobalAveragePooling2D()])\r\n    base_model.layers[1]._name = 'custom_mnet_trainable'\r\n    base_model.add(tf.keras.layers.Dense(64, name='object_dense',kernel_regularizer=tf.keras.regularizers.l2(l2=0.1)))\r\n    base_model.add(tf.keras.layers.BatchNormalization(scale=False, center = False))\r\n    base_model.add(tf.keras.layers.Activation('relu', name='relu_dense_64'))\r\n    base_model.add(tf.keras.layers.Dropout(rate=0.5, name='dropout_dense_64'))\r\n    base_model.add(tf.keras.layers.Dense(32, name='object_dense_2',kernel_regularizer=tf.keras.regularizers.l2(l2=0.1)))\r\n    base_model.add(tf.keras.layers.BatchNormalization(scale=False, center = False))\r\n    base_model.add(tf.keras.layers.Activation('relu', name='relu_dense_32'))\r\n    base_model.add(tf.keras.layers.Dropout(rate=0.4, name='dropout_dense_32'))\r\n    base_model.add(tf.keras.layers.Dense(16, name='object_dense_16', kernel_regularizer=tf.keras.regularizers.l2(l2=0.1)))\r\n    base_model.add(tf.keras.layers.Dense(len(CLASS_NAMES), activation='softmax', name='object_prob'))\r\n    m1 = tf.keras.metrics.CategoricalAccuracy()\r\n    m2 = tf.keras.metrics.Recall()\r\n    m3 = tf.keras.metrics.Precision()\r\n\r\n    m4 = Metrics(train_tf_data=train_data, val_tf_data=test_data, CLASSES=CLASS_NAMES)\r\n\r\n\r\n    optimizers = [\r\n        tfa.optimizers.AdamW(learning_rate=lr * .001 , weight_decay=wd),\r\n        tfa.optimizers.AdamW(learning_rate=lr, weight_decay=wd)\r\n            ]\r\n\r\n    optimizers_and_layers = [(optimizers[0], base_model.layers[0]), (optimizers[1], base_model.layers[1:])]\r\n\r\n    optimizer = tfa.optimizers.MultiOptimizer(optimizers_and_layers)\r\n\r\n    annotated_model = tf.keras.models.clone_model(\r\n        base_model,\r\n        clone_function=apply_quantization_to_dense,\r\n    )\r\n\r\n\r\n    model = tfmot.quantization.keras.quantize_apply(annotated_model)\r\n    model.compile(\r\n        optimizer= optimizer, loss=tfa.losses.SigmoidFocalCrossEntropy(reduction=tf.keras.losses.Reduction.AUTO),\r\n        metrics=[m1, m2, m3],\r\n        )\r\n\r\ntensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\r\n\r\ncheckpoint_name = os.getcwd() + os.sep + CUSTOM_MODEL_PATH + os.sep + \"training_chkpts/cp-{epoch:04d}-{val_f1_after_epoch:.2f}.ckpt\"\r\ncheckpoint_dir_path  = os.getcwd() + os.sep + CUSTOM_MODEL_PATH + os.sep+ \"training_chkpts\"\r\ncheckpoint_cb = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_name, \r\n                                                    monitor = 'val_f1_after_epoch',\r\n                                                    save_best_only=True,\r\n                                                    save_weights_only=True,\r\n                                                    mode='max',\r\n                                                    save_freq='epoch',\r\n                                                    verbose=1)\r\n\r\ncheckpoint_cb._supports_tf_logs = False\r\ncurrent_dir = os.getcwd()\r\nhistory = model.fit(train_data, validation_data=test_data, \r\n                    epochs=N_EPOCHS,\r\n                    callbacks=[m4, checkpoint_cb, tensorboard_cb])\r\n```\r\nBut If I use a system when the number of GPU > 1 then it is throwing the below error.\r\n\r\nEpoch 1/2 6/Unknown - 44s 150ms/step - loss: 19.2255 - categorical_accuracy: 0.0625 - recall: 0.0000e+00 - precision: 0.0000e+00\r\n\r\n/bwz_venv/lib/python3.8/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument. layer_config = serialize_layer_fn(layer) 288/Unknown - 84s 141ms/step - loss: 13.7873 - categorical_accuracy: 0.1788 - recall: 0.0080 - precision: 0.77082021-12-30 15:08:31.404434: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at transpose_op.cc:142 : INVALID_ARGUMENT: transpose expects a vector of size 0. But input(1) is a vector of size 4\r\n\r\nTraceback (most recent call last): File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main return _run_code(code, main_globals, None, File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code exec(code, run_globals) File \"/ssd/custom_mnet_v2.py\", line 536, in history = model.fit(train_data, validation_data=test_data, File \"bwz_venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler raise e.with_traceback(filtered_tb) from None File \"/bwz_venv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 58, in quick_execute tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name, tensorflow.python.framework.errors_impl.InvalidArgumentError: 3 root error(s) found.\r\n\r\n(0) INVALID_ARGUMENT: transpose expects a vector of size 0. But input(1) is a vector of size 4 [[{{node gradient_tape/replica_1/sequential/custom_mnet_trainable/Conv1/Conv2D/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer}}]] [[div_no_nan_3/ReadVariableOp/_558]]\r\n\r\n(1) INVALID_ARGUMENT: transpose expects a vector of size 0. But input(1) is a vector of size 4 [[{{node gradient_tape/replica_1/sequential/custom_mnet_trainable/Conv1/Conv2D/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer}}]] [[assert_less_equal/Assert/AssertGuard/else/_4049/assert_less_equal/Assert/AssertGuard/Assert/data_4/_546]]\r\n\r\n(2) INVALID_ARGUMENT: transpose expects a vector of size 0. But input(1) is a vector of size 4 [[{{node gradient_tape/replica_1/sequential/custom_mnet_trainable/Conv1/Conv2D/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer}}]] 0 successful operations. 0 derived errors ignored. [Op:__inference_train_function_1079980]\r\n\r\nFunction call stack: train_function -> train_function -> train_function\r\n\r\nFew things that I have already tested\r\n\r\n1) Tried out different metrics (categorical_accuracy) to check whether the issue is related to the custom monitoring metrics or not. Working fine\r\n2) Run the code in CPU and single GPU environment and it is working perfectly fine\r\n\r\n[Here](https://colab.research.google.com/drive/1n9MmaLKZORa0vQvluQcY3t-VuX-yiSxf?usp=sharing) is the link to the Google Colab Notebook to reproduce the error(please set #GPU>1)", "comments": ["@Saduf2019 ,\r\nI was able to execute the code without any issues in tf v2.7 and nightly(gpu and cpu).Please find the gist here.[1](https://colab.research.google.com/gist/tilakrayal/c40002e343f97b6a9627541577924615/53612.ipynb),[2](https://colab.research.google.com/gist/tilakrayal/53b7bd04e6ce54f2ebcd981e3e1e9691/53612-cpu.ipynb),[3](https://colab.research.google.com/gist/tilakrayal/1560d87c9a169ba64c64da48143bbcc0/53612-nightly-gpu.ipynb),[4](https://colab.research.google.com/gist/tilakrayal/dcebd254bb6272b68f8d056000517bd9/53612-cpunightly.ipynb)", "@tilakrayal  Have you checked for more than one GPU? Also, can you please run it for more than one epochs? From my obs, it is breaking after the execution of First Epochs. \r\n\r\nAlso in the main code, I forgot to add then I am using the below strategery for TPU|GPU. Apology for the miss from my end.  I have added the below line of code in the notebook. Can you please try it with this also?\r\n```\r\nNUM_TRAINING_IMAGES = count_data_items(train_data)\r\nNUM_TEST_IMAGES = count_data_items(test_data)\r\n\r\n#TPU or GPU detection\r\ntry:\r\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\r\n    strategy = tf.distribute.TPUStrategy(tpu)\r\n    BUFFER_SIZE = NUM_TRAINING_IMAGES\r\n    BATCH_SIZE_PER_REPLICA = BATCH_SIZE / strategy.num_replicas_in_sync\r\n    GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\r\n    STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\r\n    VALIDATION_STEPS = -(-NUM_TEST_IMAGES // BATCH_SIZE) # The \"-(-//)\" trick rounds up instead of down :-)\r\n\r\nexcept ValueError:\r\n    strategy = tf.distribute.MirroredStrategy()\r\n    # Defining tf_distribute strategy\r\n    BUFFER_SIZE = NUM_TRAINING_IMAGES\r\n    BATCH_SIZE_PER_REPLICA = BATCH_SIZE / strategy.num_replicas_in_sync\r\n    GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\r\n    STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\r\n    VALIDATION_STEPS = -(-NUM_TEST_IMAGES // BATCH_SIZE) # The \"-(-//)\" trick rounds up instead of down :-)\r\n    STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE_PER_REPLICA\r\n    VALIDATION_STEPS = -(-NUM_TEST_IMAGES // BATCH_SIZE_PER_REPLICA) # The \"-(-//)\" trick rounds up instead of down :-)\r\n    print('Dataset: training images, {} test images {}'.format(NUM_TRAINING_IMAGES, NUM_TEST_IMAGES))\r\n\r\n\r\n```\r\n\r\n\r\n\r\n\r\n"]}, {"number": 53597, "title": "error LNK2001 when build on windows with --config=dynamic_kernels", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Windows 10 19044.1415\r\n- TensorFlow installed from: source\r\n- TensorFlow version: master\r\n- Python version: 3.9.7\r\n- Installed using: conda\r\n- Bazel version: 4.2.2\r\n- GCC/Compiler version: msvc 19.29.30038.1\r\n- CUDA/cuDNN version: cuda 11.1, cudnn 8.2.1\r\n- GPU model and memory: GTX 1080TI 11G\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI'm trying to compile tensorflow with \"--config=dynamic_kernels\" so I can use custom op. What I did:\r\n* Following https://www.tensorflow.org/install/source_windows?hl=zh-cn except using anaconda python\r\n* Edit .bazelrc and replace \"build:windows --config=monolithic\" with \"build:windows --config=dynamic_kernels\"\r\n* run \"bazel build //tensorflow/tools/pip_package:build_pip_package --config=avx2_win\"\r\nWhen building goes into link stage I got a lot of  LNK2001 error.\r\n\r\n\r\n**Any other info / logs**\r\n[build-log.txt](https://github.com/tensorflow/tensorflow/files/7796223/build-log.txt)\r\n\r\n", "comments": ["The document which you are following has some old versions, please follow [this](https://www.tensorflow.org/install/source_windows) document which has the steps with the latest version and let us know if the problem still persists. Thanks!", "> \r\n\r\nOk, so the only difference i found is the step \"Install Python and the TensorFlow package dependencies\"\r\nI followed the newest document (removed keras_application and upgraded keras_preprocessing to newest) and I still got same errors.", "@maz-1,\r\nCan you provide complete error log/Traceback to investigate the issue. ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 53584, "title": "Please add `tf.random.stateless_shuffle`", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.7\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nwe only have `tf.random.shuffle` which is generally annoying to work with and bug-prune when reproducible results are needed\r\n\r\n(same reasoning for all 'tf.random.stateless_*`)\r\n\r\n**Will this change the current api? How?**A\r\nonly API addition\r\n\r\n**Who will benefit with this feature?**\r\npeople that want to reproduce random function rexecution\r\n\r\n\r\n**Any Other info.**\r\nno, thank you", "comments": ["Hi @Saduf2019 ! Could you please look at this feature request?", "@amitport \r\nPlease feel free to create a PR or share where this change has to be made.", "@Saduf2019 \r\nI don't have the capacity to implement this. Just thought you guys would like to keep track of things that probably should be on the roadmap.", "Could you please look at the available stateless distribution [here](https://www.tensorflow.org/api_docs/python/tf/random) like `stateless_binomial`, `stateless_normal`, `stateless_uniform` etc and let us know if this is what you are looking for or you need something specific. Thanks!", "@sachinprasadhs \r\n\r\nNo. stateless_shuffle is not covered by the other functions as is", "There is already an implementation available on nightly to run ops deterministically which is [enable_op_determinism](https://www.tensorflow.org/api_docs/python/tf/config/experimental/enable_op_determinism) you can start using that in nightly version as of now and it will be available in future stable releases. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", " non determinism does not replace all stateless ops AFAIKT.\r\nI want do be able to supply shuffle with a specific seed and get the same result in every context....", "We are working on adding `stateless_shuffle`.\r\n\r\nBTW there is a new op [`index_shuffle`](https://www.tensorflow.org/api_docs/python/tf/random/experimental/index_shuffle) which can be used to implement `stateless_shuffle` (though the native `stateless_shuffle` we are adding will probably be faster).", "@wangpengmit I just want to add that I specifically implemented a 'select random K without replacement' op.\r\nFor which [`index_shuffle`](https://www.tensorflow.org/api_docs/python/tf/random/experimental/index_shuffle) seems like a better alternative than the `stateless_shuffle` :)."]}, {"number": 53582, "title": "Description of Build Order and bootstrapped builds", "body": "I would like to request documentation on the ideal build order for tensorflow and both its required dependencies as well as suggested dependencies:\r\n\r\nAs described in: https://github.com/tensorflow/tensorflow/blob/v2.7.0/tensorflow/tools/pip_package/setup.py#L104\r\nthe package `tensorflow` depends on 4 dependencies that must be moved in step:\r\n```\r\n    'tensorboard ~= 2.6',\r\n    'tensorflow_estimator ~= 2.7.0rc0, < 2.8',\r\n    # Keras release is not backward compatible with old tf release, and we have\r\n    # to make the version aligned between TF and Keras.\r\n    'keras >= 2.7.0rc0, < 2.8',\r\n    'tensorflow-io-gcs-filesystem >= 0.21.0',\r\n```\r\n\r\nHowever, it is my understanding that in order to build `estimator` that `tensorflow` must be importable by python.\r\n\r\nIs that really the case?\r\n\r\nIf not, is there somewhere where we can read up on the ideal build order. Ideally we would have:\r\n1. Build `tensorflow`. This will create an importable package, but without many extra features.\r\n     * It might be that this package is called `tensorflow-base` or something else to your liking\r\n2. Build `estimator`....\r\n3. Build `keras` ....\r\n4. Assemble all dependencies into a single dependency called `tensorflow`\r\n\r\nIdeally this would form some directed acyclic graph so that we can bootstrap the builds without having an \"older version of tensorflow\" already compiled.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Any\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.7.0\r\n- Python version: 3.7-3.10\r\n- Installed using virtualenv? pip? conda?: conda with conda-forge\r\n- Bazel version (if compiling from source): 4.XX\r\n- GCC/Compiler version (if compiling from source): 9\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: Many\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nIt is best to look a the file `recipe/build.sh`  that builds the wheel. Finally the file `recipe/build_pkg.sh` installs the wheel.\r\nhttps://github.com/conda-forge/tensorflow-feedstock/pull/189\r\n\r\n**Any other info / logs**\r\nThe best logs can be found on the conda-forge recipe. Here is the PR upgrading the system to 2.7.0. You can see a few patches strip out certain dependencies.\r\n\r\nhttps://github.com/conda-forge/tensorflow-feedstock/pull/189\r\n\r\n", "comments": ["@hmaarrfk ,\r\nPlease feel free to submit a PR for the requested change or share the link where requested change is to be made", "Thank you for your support. I will look into how best to document things. Hopefully it won't require any changes to the source.", "@hmaarrfk ,\r\nAs suggested Please feel free to submit a PR for the requested change.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "I'm not sure why this is stale. Can we mark it as not stale? This kind of documentation is not a trivial one liner.\r\n\r\nIn my mind, the solution is one of two things:\r\n1. Documentation including \"patches\" to apply to `tensorflow` and dependencies such as `tensorflow-io` to get the system to compile from a boostrapped environment.\r\n2. Creating new packages that break the circular dependency between `tensorflow` and `tensorflow-io`.\r\n\r\nFor example, see how tensorflow depends on `tensorflow-io-gcs-filesystem`:\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.7.0/tensorflow/tools/pip_package/setup.py#L109\r\n\r\nAnd how `tensorflow-io-gcs-filesystem` depends on tensorflow:\r\nhttps://github.com/tensorflow/io/blob/v0.23.1/setup.py#L171\r\nhttps://github.com/tensorflow/io/blob/v0.23.1/tensorflow_io/python/ops/version_ops.py#L18\r\n\r\nTypically, these circular dependencies are broken by having a \"core\" package. The DAG might look like this\r\n```\r\ntensorflow ---------------------------------------------> tensorflow-core\r\n          \\                                              ^\r\n           \\                                            /\r\n            \\--------tensorflow-io-gcs-filesystem -----/\r\n```\r\nHaving such a DAG allows people to have a clear starting point of where compilation should start, and what packages need to be built sequentially.\r\n\r\nAt conda-forge, we are getting close to doing option `1.` but, it is rather unmaintainable (and not yet fully documented)\r\n\r\nOption 2, in my mind, is in the interest of `tensorflow` developers as untangling the dependencies will help them maintain more architectures.\r\n\r\nOption #2 is more than what I can provide. and why I think it is an important issue to raise for tensorflow at large.", "Our `BUILD` files ensure that there is no actual cyclic dependency between the projects, except at Python import layer, as modules/plugins.\r\n\r\nHence, you can build TF, Estimator (deprecated, will be dropped in the future), the GCS filesystem plugin in this order and then at import time in TF it all should work.\r\n\r\nWe do this when building the standard release:\r\n\r\n- cut branch in TF, cut branch in Keras and Estimator at commits that corresponds to the same point in time (`PiperOrigin-RevId` number is close to the one in the TF's commit but not greater)\r\n- build TF RC0 against estimator nightly wheel (we can build it without any estimator dependency but then we won't be able to run all tests or we would need a separate script for this)\r\n- built Estimator RC0 against TF RC0\r\n- Keras RC0 can be build in parallel, it does not depend on Estimator and C++ parts of TF\r\n- update bounds in TF's `setup.py` to use these RC0s\r\n- do TF RC1, RC2, etc and similar for the others if needed, against latest TF RC\r\n- release final Estimator against latest TF RC\r\n- release final Keras\r\n- update bounds in TF's `setup.py` to use the latest versions\r\n- release final TF\r\n\r\nWe had a proposal 2 years ago to modularize all this and have a real DAG of dependencies but there has been significant pushback as that would break the ability to do cross-cutting changes that some C++ teams preferred to do. Then, people that proposed the modularization left the team, the team halved, the proposal dropped.", "Thank you for explination of the build process. I think this should help us better put things together.\r\n\r\nIn your description, does \"GCS filesystem\" get built in parallel with estimator?", "The GCS filesystem is built by SIG IO community. It can be build in parallel with any other TF process, it doesn't need TF afaik.\r\n\r\nThe filesystem is a shared object (`.so`, `.dylib`, `.dll`, depending on operating system) that is then loaded by TF on import,if present."]}, {"number": 53581, "title": "tf.ragged.stack_dynamic_partitions silently output wrong result with negative `partitions`", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: Cuda 11.4\r\n- GPU model and memory: n/a\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\ndata=['a', 'b', 'c', 'd', 'e']\r\npartitions=[3, -2, 2, -1, 2]\r\nnum_partitions=5\r\nt1 = tf.ragged.stack_dynamic_partitions(data, partitions, num_partitions) # Succeed\r\nprint(t1)\r\nt2 = tf.ragged.stack(tf.dynamic_partition(data, partitions, num_partitions)) # Raise InvalidArgumentError\r\nprint(t2)\r\n```\r\nOutputs\r\n```\r\n<tf.RaggedTensor [[], [], [b'b', b'd'], [b'c'], []]>\r\nInvalidArgumentError: partitions[1] = -2 is not in [0, 5) [Op:DynamicPartition]\r\n```\r\n\r\n**Describe the current behavior**\r\nFor `tf.ragged.stack_dynamic_partitions`, values in `partitions` should be greater or equal to zero. (as the document points out)\r\n\r\n**Describe the expected behavior**\r\nIn the document it says \"if `num_partitions` is an int (not a Tensor), then `tf.ragged.stack_dynamic_partitions(data, partitions, num_partitions)` is equivalent to `tf.ragged.stack(tf.dynamic_partition(data, partitions, num_partitions))`\". So similar to the latter case, an `InvalidArgumentError` is expected to be raised when calling `tf.ragged.stack_dynamic_partitions` with `partitions` containing negative values.\r\n", "comments": ["Hi @ArrowIntoTheSky ! It is raising invalid argument error in first case only   . According to this [document](https://www.tensorflow.org/api_docs/python/tf/ragged/stack_dynamic_partitions#args) , Values in Partitions must be greater than or equal to zero, and less than num_partitions. Attaching [Gist](https://colab.sandbox.google.com/gist/mohantym/69d7ecbc4924ce8b37c71356b4df2285/github_53581.ipynb) for reference.", "Thank you @mohantym ! I checked your Gist, and indeed on CPU it is raising invalid argument error. However, if you enable GPU it is not raising any error.", "Hi @Saduf2019! Could you  please look at this issue . This issue is replicating in GPU mode. Attaching gist in[ 2.6](https://colab.sandbox.google.com/gist/mohantym/1917c86e14f83915194adba8d4d7f034/github_53581.ipynb#scrollTo=cfSJsLFEB3Km) and [2.7 ](https://colab.sandbox.google.com/gist/mohantym/69d7ecbc4924ce8b37c71356b4df2285/github_53581.ipynb#scrollTo=fUA6f6FO_2oX)for reference."]}, {"number": 53576, "title": "[tensorflow/lite/schema/upgrade_schema.py] Move `(see :schema.fbs).` up to `operator_type` arg descriptor", "body": "Closes #53566\r\n\r\nI have custom docstring parsers that is reading in your codebase to make changes down the track (like automatically inferring types and adding them as annotations)\u2026 but this line of your codebase it's hiccuping on because it's on a new line and has a colon in it and lower indentation then the line above (indicating same scope as that arg); but doesn't actually refer to a new argument. This PR moves it to the same line as above; so it refers to that arg.", "comments": []}, {"number": 53574, "title": "Update post_training_quantization.md", "body": "Fix Markdown formatting for the bullet points of use cases", "comments": ["I've just signed the CLA:\r\n\r\n![image](https://user-images.githubusercontent.com/3373514/147671504-186b8e95-ef43-4924-8bf8-2cf68e5de57e.png)\r\n\r\n@gbaned Can you update my permissions, please? Thanks.", "> I've just signed the CLA:\r\n> \r\n> ![image](https://user-images.githubusercontent.com/3373514/147671504-186b8e95-ef43-4924-8bf8-2cf68e5de57e.png)\r\n> \r\n> @gbaned Can you update my permissions, please? Thanks.\r\n\r\n@mathemage  Can you please comment like this  **@googlebot I consent**  ? Thanks!", "@googlebot I consent", "> @googlebot I consent\r\n\r\n@gbaned Have I done this correctly? Is there any other issue or blocker for this?", "> @googlebot I consent\r\n\r\n@miaout17  Have I done this correctly? Is there any other issue or blocker for this?", "@googlebot I consent", "@mathemage Sorry for the late response. CLA looks good now. This PR is waiting for review.  Thanks!", "Hi @gbaned - any new progress?\r\n\r\nAnything from my side to do?", "> Hi @gbaned - any new progress?\r\n> \r\n> Anything from my side to do?\r\n\r\n@mathemage Sorry for the delay. This PR is waiting for the review. Nothing pending from your end.  Thank you. ", "Hi @gbaned ! Is there any problem with CLA from my side?\r\n\r\nLet me know if you need something for me to do. Thanks!", "> Hi @gbaned ! Is there any problem with CLA from my side?\r\n> \r\n> Let me know if you need something for me to do. Thanks!\r\n\r\nHi @mathemage  CLA looks good, nothing pending from your side at this moment. This PR is processing internally, we will let you know if anything required from you. Thank you!"]}, {"number": 53572, "title": "tf.data.Dataset .map().batch() pattern is not matched to use fused implementation.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v1.12.1-69264-g0cdf35562dc 2.9.0\r\n- Python version: 3.8.10\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.5 / 8.3\r\n- GPU model and memory: GTX1660 Ti\r\n\r\n**Describe the current behavior**\r\ncombining `tf.data.Dataset.map()` with `.batch()` does not use the fused BatchAndMap implementation.\r\n\r\n**Describe the expected behavior**\r\nIt does use the fused implementation. Currently, it's only possible to use the fused implementation when using the deprecated `experimental.map_and_batch()` transformation.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```py\r\nimport os\r\nimport datetime\r\nfrom tqdm import tqdm\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\nprint('TF version', tf.__version__)\r\n\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n    try:\r\n        # Currently, memory growth needs to be the same across GPUs\r\n        for gpu in gpus:\r\n            tf.config.experimental.set_memory_growth(gpu, True)\r\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n        print(len(gpus), 'Physical GPUs,', len(logical_gpus), 'Logical GPUs')\r\n    except RuntimeError as e:\r\n        # Memory growth must be set before GPUs have been initialized\r\n        print(e)\r\n\r\n\r\n@tf.function\r\ndef do_stuff(wmat, tf_var):\r\n    with tf.device(\"/gpu:0\"):\r\n        S = tf.constant(0.0)\r\n        for i in tf.range(4):\r\n            fi = tf.cast(i, dtype=tf.float32)\r\n            A = tf.math.lgamma(tf.tanh(tf.matmul(wmat + fi, tf.transpose(wmat - fi, [0, 2, 1]))))\r\n            S += tf.reduce_sum(A)\r\n        error = tf.reduce_mean(tf_var)\r\n        return error, S\r\n\r\nexp_uuid = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\n\r\nn_batches = 512\r\n\r\n\r\ndef gen():\r\n    for i in range(n_batches):\r\n        with tf.device(\"/cpu:0\"): # Make sure it comes from CPU\r\n            r = tf.ones((400,800))\r\n        yield r\r\n\r\noption_names = ['map().batch()', 'map_and_batch()']\r\nfor option in range(2):\r\n\r\n    with tf.device(\"/cpu:0\"):\r\n        dataset = tf.data.Dataset.from_generator(gen, output_types=tf.float32)\r\n\r\n        def my_identity(x):\r\n            with tf.device(\"/cpu:0\"):\r\n                print(\"my_identity input:\", x, x.device)\r\n                y = tf.identity(x)\r\n                print(\"my_identity output:\", y, y.device)\r\n                return y\r\n\r\n        if option == 0:\r\n            ## Option 0: map().batch()\r\n            dataset = dataset.map(my_identity).batch(16)\r\n\r\n        elif option == 1:\r\n            ## Option 1: deprecated map_and_batch()\r\n            dataset = dataset.apply(tf.data.experimental.map_and_batch(my_identity, 16))\r\n\r\n    gpu_transform = tf.data.experimental.prefetch_to_device('/gpu:0', buffer_size=4)\r\n    dataset = dataset.apply(gpu_transform)\r\n\r\n\r\n    tf_var = tf.Variable(tf.zeros(3))\r\n    adam = tf.keras.optimizers.Adam(1e-4)\r\n    logpath = os.path.join('data', 'logs', 'pa_' + exp_uuid + '_' + option_names[option])\r\n\r\n    tf.profiler.experimental.start(logpath)\r\n    start = datetime.datetime.now()\r\n    for b, wmat in tqdm(enumerate(dataset)):\r\n        with tf.GradientTape() as tape:\r\n\r\n            if b == 0:\r\n                print('\\n\\n dataset element device', wmat.device)\r\n                print('\\n')\r\n\r\n            # Do some calculations\r\n            result = do_stuff(wmat, tf_var)\r\n\r\n        grads = tape.gradient(result[0], [tf_var])\r\n        adam.apply_gradients(zip(grads, [tf_var]))\r\n    stop = datetime.datetime.now()\r\n    tf.profiler.experimental.stop()\r\n\r\n    print(f'\\n\\nOption {option_names[option]}\\n===========================\\n')\r\n    print(logpath)\r\n    print('Time lapsed=', stop - start)\r\n    print(\"\\n\\n\")\r\n```\r\n\r\n**Other info / logs**\r\n\r\n**Option 1**:\r\n![image](https://user-images.githubusercontent.com/845012/147661299-a7f72017-00ff-47b6-bb71-8812bd5163d3.png)\r\nSymptoms:\r\n - See the blocks `Iterator::FlapMap` and `Iterator::BatchV2` stacked on top of each other.\r\n - The MemcpyH2D (selected, see the details panel) is comping from pagable memory, instead of pinned memory (which is what MapAndBatch does). Because of the source being pagable memory, it can't overlap with kernel computations.\r\n \r\n**Option 2**:\r\n![image](https://user-images.githubusercontent.com/845012/147661558-9f316201-a4e5-4df1-a77b-032272537321.png)\r\nEvidence:\r\n - The MapAndBatch block is used.\r\n - The MemcopyH2D comes from pinned memory (see details pane) and overlaps with kernel computations.\r\n\r\nThe whole deal about pinned memory is to allow parallel data upload and kernel computations. So the dataset needs to be produced into pinned host memory, which then can be uploaded asynchronously by the driver without an extra copy. See https://github.com/tensorflow/tensorflow/issues/43905#issuecomment-823675760 and https://github.com/tensorflow/tensorflow/issues/43905#issuecomment-824145184 and:\r\nhttps://github.com/tensorflow/tensorflow/blob/40e9b534962989af7486bc6567ca472d71eb5049/tensorflow/core/kernels/data/experimental/map_and_batch_dataset_op.cc#L522\r\n\r\nThis is a follow up on https://github.com/tensorflow/tensorflow/issues/43905.\r\n", "comments": ["@mcourteaux ,\r\nWe see that you are using tf version 1.12, 1.x is not actively supported, please update to latest stable tf v2.7 and let us know if you are using same issue.", "No, I'm most definitely not. This was a fresh build from master branch from yesterday. Idk why the script that gives the TF version gives 1.12. It's most definitely wrong. I moved to TF 2 years ago. Note that that is the GIT_VERSION. Instead, `tf.version.VERSION` gives 2.9.0.", "@mcourteaux ,\r\n I was able to execute the code without any issues.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/d0b954e177f03ab8e85b6fa087612ba6/untitled171.ipynb).Please provide the error log and also confirm if anything is missing here.Thanks!", "*First a little frustration:* @tilakrayal I get the sensation you are not paying attention. How does Google hope to get contributions to a project if all of the useful feedback is dismissed as being either wrong or nobody paying attention? It's frustrating that I lost around 2 hours identifying this problem, and then one more hour making a nice MWE that demonstrates the problem cleanly. Compare my three hour effort, to flow of this issue... I'll tag people who know what's going on: @jsimsa, @aaudiber.\r\n\r\n---\r\nYou somehow managed to make a notebook with the code from the linked issue, not mine. There is no error message, with my code. I showed you how the `MapAndBatchDataset` implementation is only used when using `tf.data.experimental.map_and_batch()`, and not with `map().batch()` as is actually promised. To see this, you look at the performance trace in TensorBoard, not in a log or error message.\r\n\r\nHappy new year!", "Hi @mcourteaux, thank you for the detailed repro and sorry for the initial response. I will have someone on the tf.data team take a closer look.", "@wilsingosti is taking a look", "The `my_identity` function is a no op. When `option = 1`, the `Map` is eliminated by the tf.data `noop_elimination` optimization. What is left is just `Batch`. If you replace the `my_identify` function with another that is not a no op, you should see the `MapAndBatch` implementation.", "IIUC, the `map` in the repro above is introduced to make sure the memory allocated for the batch is [pinned](https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/). From that perspective, the `map` is intended to be a no-op. To keep the `map` is and still trigger the fusion, it should be sufficient to disable the no-op elimination using tf.data [options](https://www.tensorflow.org/api_docs/python/tf/data/Options) as follows:\r\n\r\n```\r\ndataset = ... # your dataset\r\n\r\noptions = tf.data.Options()\r\noptions.experimental_optimization.noop_elimination = False\r\n\r\ndataset = dataset.with_options(dataset)\r\n```", "@wilsingosti Thanks for checking this. I'm wondering why not most Dataset implementations use the `gpu_compatible` memory when allocating stuff. Wouldn't that be useful in general, and then have us not rely on this `MapAndBatch` being selected to just copy the data to pinned memory. ", "Yes, it would be useful in general. AFAIK, it has just not been prioritized so far. I will try to do this for `Batch` dataset."]}, {"number": 53566, "title": "tensorflow/lite/schema/upgrade_schema.py has a docstring arg out-of-line", "body": "## URL(s) with the issue:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/87462bf/tensorflow/lite/schema/upgrade_schema.py#L234\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nThe `(see :schema.fbs).` is an odd syntax, which my custom docstring parser picks up as an argument. What function does it have? - Can it be moved to the line above it?\r\n\r\n```python\r\n    def RemapOperatorType(operator_type):\r\n      \"\"\"Remap operator structs from old names to new names.\r\n      Args:\r\n        operator_type: String representing the builtin operator data type\r\n          string.\r\n        (see :schema.fbs).\r\n      Raises:\r\n        ValueError: When the model has consistency problems.\r\n      Returns:\r\n        Upgraded builtin operator data type as a string.\r\n      \"\"\"\r\n```\r\n\r\n### Parameters defined\r\n\r\nNo there is a parameter that is defined that doesn't exist, namely `(see :schema.fbs).`.\r\n\r\n### Submit a pull request?\r\n\r\nYes I can send a PR", "comments": ["@SamuelMarks ,\r\nPlease feel free to submit a PR for the requested change or share the link where requested change is to be made", "Opened PR", "@SamuelMarks \r\nThe related PR has been assigned for reviewing and once it is merged this issue will move to closed status."]}, {"number": 53554, "title": "Typing tensorflow.data.Dataset", "body": "**System information**\r\n- TensorFlow version (you are using): 2.5\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nI have some functions that accept instances of tensorflow.data.Dataset. I would like to type those functions so that the caller knows what is expected. However, doing `data: tensorflow.data.Dataset`, is not good enough, because people wouldn't know what kind of structure is produced by the object (shape of tensors, whether is tuple or dictionary, etc).\r\n\r\nThere is another object, `tensorflow.data.DatasetSpec`, which can be used to document the objects that will be produces by the tensorflow.data.Dataset. However, I am not sure how to put those things together for a typing annotation, and I am left to describe the structure in docstring, which I do not think is ideal.\r\n\r\nI would love to see something like a generic, where you pass the type it accepts, like `Dataset[Spec]`(like in lists we do `List[int]`), but I am happy to learn some other ways to achieve more descriptive typing than just `data: tensorflow.data.Dataset`.\r\n\r\n**Will this change the current api? How?**\r\nOnly for typing purposes\r\n\r\n**Who will benefit with this feature?**\r\nA lot of people from the Machine Learning community using Tensorflow to build production systems. \r\n\r\n**Any Other info.**\r\nI could help if provided with guidance. ", "comments": ["Could you provide some more details about this feature request with example use case. ", "Thanks @sachinprasadhs . The use case is to have strong type guarantees when used with static type checkers like `mypy`. Such type annotations and type checking is very useful for maintaining production code. ", "ccing @mdanatg who is more familiar with typing in TF", "I think this would be blocked by `Spec` (as in `Dataset[Spec]`) being itself a type, which in turn is gated by Tensor being a parametric type, so you can write things like `Dataset[Tuple[Tensor[Int32], etc.]]` (see https://github.com/tensorflow/tensorflow/issues/12345). There has been an effort to do that, and got blocked by making `Tensor` a subtype of `typing.Generic` - a PR actually got submitted (https://github.com/tensorflow/tensorflow/pull/40921), but unfortunately some CPython-specific optimizations of EagerTensor broke and forced us to roll it back.\r\n\r\nLong story short, this would become one step closer to reality once the PR can be resubmitted so that `Tensor` is a `Generic`."]}, {"number": 53552, "title": "DSP Overflow - high pixel values are being clamped when running on DSP", "body": "Hi,\r\nI trained a keras model to extract gray-level segmentation maps. \r\nI converted the model to TFLite and quantized the model.\r\nThe quantized model produces similar results on CPU and DSP HWs, if the values of the pixels are not very high (<<1).\r\nIf the maps produce pixel predictions with high values that are closer to 1, it seems that the values are being clamped when running on DSP, unlike in CPU which produces reasonable maps.\r\n\r\nI repeated the tests on versions 2.2, 2.4, and 2.7 and all demonstrated the same results.\r\n\r\nWhat can I do to change this?\r\nShould I change the dynamic range of the maps?\r\n", "comments": ["@aviaisr ,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.", "1. I saved the kears model + tflite file + quantized tflite file here: https://drive.google.com/drive/folders/1ZIsPCiBPCzDmFcTbVAokjM7CtzLJO8y7?usp=sharing\r\n\r\n2. Conversion code\r\n_# Quntization script\r\n\r\n# read an hdf5 keras model and convert to tflite with quantization\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport cv2\r\nimport sys\r\nimport os\r\n\r\ndef rep_data_gen0():\r\n    a = []\r\n    for i in range(128):\r\n        img = 5*np.fromfile('norm_images/image_'+str(i)+\".bin\", dtype=np.float32)\r\n        img = img.reshape(224, 224, 3)\r\n        a.append(img)\r\n\r\n    a = np.array(a)\r\n    print(a.shape)\r\n    img = tf.data.Dataset.from_tensor_slices(a).batch(1)\r\n    for i in img.take(128):\r\n        yield [i]\r\n\r\ndef convert_and_quantize(model):\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.representative_dataset = rep_data_gen0\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n\r\n    print(\"Converting to TfLite using full integer quantization\")\r\n    quant_model = converter.convert()\r\n    return quant_model\r\n\r\ndef main_func():\r\n    print (f'Reading keras model from {sys.argv[1]}')\r\n    keras_model = tf.keras.models.load_model(sys.argv[1], compile = False)\r\n    input_name = keras_model.input_names[0]\r\n    index = keras_model.input_names.index(input_name)\r\n    keras_model.inputs[index].set_shape([1, 224, 224, 3])\r\n    keras_model.trainable = False\r\n    quant_model = convert_and_quantize(keras_model)\r\n\r\n    print (f'Writing quantized tflite model to {sys.argv[2]}')\r\n    f = open(sys.argv[2],'wb')\r\n    f.write(quant_model)\r\n    f.close()\r\n    \r\n\r\nif __name__ == \"__main__\":\r\n    print(tf.__version__)\r\n    if len(sys.argv) != 3:\r\n        print('Wrong command line arguments')\r\n        sys.exit(1)\r\n    main_func()\r\n    print(\"Done\")_\r\n\r\n3. Example dataset (I train on 5M images, so I just uploaded 400 images as example). My task is a segmentation task and the trained maps are binary maps.\r\nhttps://drive.google.com/drive/folders/1Dn0olSmZ9HFGwAIViq1cfl9kfuvC5Qs_?usp=sharing\r\n\r\n4. representative dataset as bins saved in \r\nhttps://drive.google.com/drive/folders/1Q0iqM1qHdWFdW_PyA0BEQtzde9dmU7S5?usp=sharing\r\n\r\n5. Main training code details:\r\n\r\n_trainDataset = tf.data.Dataset.from_tensor_slices(dataPaths['train']) #same for validation\r\nbatchSize = 32\r\n\r\nmodel = Unet('mobilenetv2', input_shape=inputShape, encoder_weights='imagenet', encoder_freeze=False,\r\n              decoder_block_type='transpose', decoder_filters=decoderFilters)  # Deconvolution\r\n              \r\nmodel = utils.set_regularization(model, kernel_regularizer=keras.regularizers.l2(0.001),\r\n                                  bias_regularizer=keras.regularizers.l2(0.001))\r\n\r\nlr = keras.callbacks.LearningRateScheduler(scheduler)\r\n\r\nloss = losses.binary_focal_dice_loss\r\n\r\ncheckpoint = keras.callbacks.ModelCheckpoint(weightsPath, monitor='val_loss', verbose=0,\r\n                                              save_best_only=True, save_weights_only=False,\r\n                                              mode='auto', save_freq='epoch')\r\n\r\ncallbacks_list = [lr, checkpoint, historyLogger]\r\n\r\n# compile\r\nadam = keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-07, decay=0.0, amsgrad=False)\r\n\r\nmodel.compile(loss=loss, optimizer=adam)\r\n\r\nhistory = model.fit(x=trainDataset,\r\n                    batch_size=batchSize,\r\n                    epochs=numEpochs,\r\n                    verbose=1,  # show progress bar\r\n                    validation_data=valDataset,\r\n                    initial_epoch=initialEpoch,\r\n                    steps_per_epoch=stepsPerEpoch,\r\n                    validation_steps=validationSteps,\r\n                    callbacks=callbacks_list)_"]}, {"number": 53533, "title": "Tensorflow conv1d computed incorrectly on GPU", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\n- TensorFlow installed from (source or binary):  binary\r\n- TensorFlow version (use command below): 2.7\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: Cuda-11.2\r\n- GPU model and memory: Tesla K80\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nComputing a single filter 1d convolution on GPU with kernel_size=15, padding='valid' and strides=1 on a (1, 64, 32) input tensor (1 sample X 64 coordinates X 32 input channels) results in data from the end of the input tensor effecting the beginning of the output vector. \r\n\r\nThis behavior only happens when performing the calculation on GPU (specifically nvidia Telsa K80). \r\nOn CPU results are fine.\r\n\r\n**Describe the expected behavior**\r\nReceptive field should be limited to the kernel size. \r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nIssue reproduces on colab with GPU:\r\nhttps://colab.research.google.com/drive/1pK1tNxWVtC9qilcX0u5sbpGmQEZiCDEX?usp=sharing\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n[Tensorflow-bug.ipynb - Colaboratory.pdf](https://github.com/tensorflow/tensorflow/files/7769397/Tensorflow-bug.ipynb.-.Colaboratory.pdf)\r\n\r\n", "comments": ["Hi @chunduriv ! Could you please look at this issue ?Attaching Gist in  [CPU](https://colab.sandbox.google.com/gist/mohantym/d4faa2bdc668b85261732fc18d8015e5/tensorflow-bug.ipynb#scrollTo=ZyXg6Bdgv1OP) , [GPU](https://colab.sandbox.google.com/gist/mohantym/d9330230394869e3e2c19c6287c9cb52/tensorflow-bug.ipynb#scrollTo=cnL4334XwalE)  and original [code](https://colab.sandbox.google.com/gist/mohantym/fec5233cc88b49ef5ffcb88ba3ee0afa/tensorflow-bug.ipynb#scrollTo=L79VLeg0rNcV) for reference. Thanks!", "@edanh88, \r\n\r\nDid you found the same using different GPU cards?  Can you brief `On CPU results are fine`? Thanks!", "Thanks for looking at this @chunduriv \r\n\r\nI didn't try different GPU cards. I've tried multiple instances of Nvidia Tesla K80 because that is the only card available to me at the moment.\r\n\r\nwhen running on CPU (either by setting CUDA_VISIBLE_DEVICES=\"\" ) or by changing runtime environment in Colab, or by using \"with tf.device(\"CPU:0\")\" as can be seen in the Colab example there is no error in the calculation. \r\n\r\nWhen run on GPU, I simply get the wrong results (basically I'm convolving a kernel of tf.ones with inputs of tf.ones so I expect the resulting tensor to have the value of KERNEL_WIDTH*INPUT_CHANNELS - and this is **not** what I get on GPU).\r\nMoreover, one can see that values outside of the convolution's receptive field affect the results of the convolution. \r\n\r\nThis does not happen on CPU. \r\nOn CPU the results are exactly as expected and do not depend on values outside of the receptive field.\r\n\r\nThe 7'th cell in the colab example contains a few print lines - they should all print 0 if there is no error. And indeed when run without GPU (by setting runtime environment) they do.\r\n\r\nThanks", "Hi @sanjoy @sachinprasadhs,\r\nSorry to bother you.\r\nJust wondering if this is being addressed in some way? \r\nThanks,\r\nEdan"]}, {"number": 53529, "title": "TRT Converter not working in 2.7.0 version of official image", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): docker image `tensorflow/tensorflow:2.7.0-gpu`\r\n- TensorFlow version (use command below): v2.7.0-rc1-69-gc256c071bb2 2.7.0\r\n- Python version: Python 3.8.10\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: Nvidia GeForce 1050\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI tried to optimize a Tensorflow SavedModel using trt_converter. It works in official `tensorflow/tensorflow:2.3.0-gpu`, `tensorflow/tensorflow:2.4.0-gpu` yet fails in `tensorflow/tensorflow:2.7.0-gpu`\r\n**Describe the expected behavior**\r\n`trt_convert.TrtGraphConverterV2` should work out of box as in previous versions of official builds.\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\nfrom tensorflow.python.compiler.tensorrt import trt_convert as trt\r\n\r\ninput_saved_model_dir = \"directory/of/tensorflow/saved_model\"\r\noutput_saved_model_dir = \"tensorrt-test\"\r\nconverter = trt.TrtGraphConverterV2(input_saved_model_dir=input_saved_model_dir)\r\ntrt_graph = converter.convert()\r\nconverter.save(output_saved_model_dir);print(\"====saved====\")\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\n2021-12-23 09:10:04.726658: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such fil\r\ne or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n2021-12-23 09:10:04.726892: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object fil\r\ne: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n2021-12-23 09:10:04.726909: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:35] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the \r\nmissing libraries mentioned above are installed properly.\r\nERROR:tensorflow:Tensorflow needs to be built with TensorRT support enabled to allow TF-TRT to operate.\r\nTraceback (most recent call last):\r\n  File \"trt.py\", line 10, in <module>\r\n    converter = trt.TrtGraphConverterV2(input_saved_model_dir=input_saved_model_dir)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 1009, in __init__\r\n    _check_trt_version_compatibility()\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 221, in _check_trt_version_compatibility\r\n    raise RuntimeError(\"Tensorflow has not been built with TensorRT support.\")\r\nRuntimeError: Tensorflow has not been built with TensorRT support.\r\n```", "comments": ["Hi @BorisPolonsky ! Have you checked these threads yet for TensorRT installation in your environment yet? . Ref [1](https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html) ,[2 ](https://github.com/prratadiya/tensorrt-installation-colab),[ 3](https://www.code-helper.com/answers/install-tensorrt-on-colab) . Thank you!", "> Hi @BorisPolonsky ! Have you checked these threads yet for TensorRT installation in your environment yet? . Ref [1](https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html) ,[2 ](https://github.com/prratadiya/tensorrt-installation-colab),[ 3](https://www.code-helper.com/answers/install-tensorrt-on-colab) . Thank you!\r\n\r\nI've checked the \"verify your installation\" part in ref [1](https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html). Here's what I got in `tensorflow/tensorflow:2.7.0-gpu`:\r\n```\r\ndpkg -l | grep TensorRT\r\nii  libnvinfer-plugin8            8.0.0-1+cuda11.0                  amd64        TensorRT plugin libraries\r\nii  libnvinfer8                   8.0.0-1+cuda11.0                  amd64        TensorRT runtime libraries\r\n```\r\nAs comparison, here's what I got in `tensorflow/tensorflow:2.4.0-gpu`, a version wheere `trt.TrtGraphConverterV2` works out of box:\r\n```\r\ndpkg -l | grep TensorRT\r\nii  libnvinfer-plugin7           7.1.3-1+cuda11.0                    amd64        TensorRT plugin libraries\r\nii  libnvinfer7                  7.1.3-1+cuda11.0                    amd64        TensorRT runtime libraries\r\n```\r\nI don't see any problem with TensorRT installation here. Maybe there's something wrong with `_check_trt_version_compatibility` in tf2.7?", "Hi @Saduf2019! Could you please look at this issue?", "@BorisPolonsky \r\nIs this failing on 2.6 as well for you?", "> @BorisPolonsky Is this failing on 2.6 as well for you?\r\n\r\nUnfortunately it's **not** working on tf2.6 either. The conversion process end up with `Aborted (core dumped)` message in `tensorflow/tensorflow:2.6.0-gpu`.\r\n```\r\n2021-12-30 23:19:23.161255: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvrtc.so.11.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n2021-12-30 23:19:23.161278: F tensorflow/compiler/tf2tensorrt/stub/nvinfer_stub.cc:49] getInferLibVersion symbol not found.\r\nAborted (core dumped)\r\n```\r\nHere's verification step of TensorRT installation in the container.\r\n```\r\nii  libnvinfer-plugin7           7.2.2-1+cuda11.1                    amd64        TensorRT plugin libraries\r\nii  libnvinfer7                  7.2.2-1+cuda11.1                    amd64        TensorRT runtime libraries\r\n```", "Boris, Tensorflow 2.7.0 (from [pypi](https://pypi.org/project/tensorflow/)) was built with TensorRT 7.2.2 (not 8.x) - it needs `libnvinfer.so.7` and `libnvinfer_plugin.so.7` but you have `libnvinfer8` installed.\r\n\r\nTensorRT does not support forward compatibility\r\n\r\nTo test that Tensorflow can find TensorRT libraries you can do the following:\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.compiler as tf_cc\r\ntf_cc.tf2tensorrt._pywrap_py_utils.is_tensorrt_enabled()\r\n\r\nprint(\"loaded trt ver:\", tf_cc.tf2tensorrt._pywrap_py_utils.get_loaded_tensorrt_version())\r\nprint(\"linked trt ver:\", tf_cc.tf2tensorrt._pywrap_py_utils.get_linked_tensorrt_version())\r\n```\r\nif you do not have `libnvinfer.so.7` and `libnvinfer_plugin.so.7` then it will output\r\n```\r\n>>> cc.tf2tensorrt._pywrap_py_utils.is_tensorrt_enabled()\r\n2022-01-06 05:37:08.526925: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\r\n2022-01-06 05:37:08.527000: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\r\n2022-01-06 05:37:08.527013: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:35] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\nFalse\r\n```\r\n\r\nSo, either install TRT 7.2.2 (or 7.2.3) or build TF with TRT 8.x support from source.\r\n\r\nHow to build TF with TRT support from source is described here https://apivovarov.medium.com/run-tensorflow-2-object-detection-models-with-tensorrt-on-jetson-xavier-using-tf-c-api-e34548818ac6", "> Boris, Tensorflow 2.7.0 (from [pypi](https://pypi.org/project/tensorflow/)) was built with TensorRT 7.2.2 (not 8.x) - it needs `libnvinfer.so.7` and `libnvinfer_plugin.so.7` but you have `libnvinfer8` installed.\r\n> \r\n> TensorRT does not support forward compatibility\r\n> \r\n> To test that Tensorflow can find TensorRT libraries you can do the following:\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> import tensorflow.compiler as tf_cc\r\n> tf_cc.tf2tensorrt._pywrap_py_utils.is_tensorrt_enabled()\r\n> \r\n> print(\"loaded trt ver:\", tf_cc.tf2tensorrt._pywrap_py_utils.get_loaded_tensorrt_version())\r\n> print(\"linked trt ver:\", tf_cc.tf2tensorrt._pywrap_py_utils.get_linked_tensorrt_version())\r\n> ```\r\n> \r\n> if you do not have `libnvinfer.so.7` and `libnvinfer_plugin.so.7` then it will output\r\n> \r\n> ```\r\n> >>> cc.tf2tensorrt._pywrap_py_utils.is_tensorrt_enabled()\r\n> 2022-01-06 05:37:08.526925: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\r\n> 2022-01-06 05:37:08.527000: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\r\n> 2022-01-06 05:37:08.527013: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:35] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n> False\r\n> ```\r\n> \r\n> So, either install TRT 7.2.2 (or 7.2.3) or build TF with TRT 8.x support from source.\r\n> \r\n> How to build TF with TRT support from source is described here https://apivovarov.medium.com/run-tensorflow-2-object-detection-models-with-tensorrt-on-jetson-xavier-using-tf-c-api-e34548818ac6\r\n\r\nThanks for clarifying. Looks like the official image of tf2.7 switched to a base docker image with TRT 8.x installed. Will the next official release enforce compatibility between tensorflow and TRT version in the base image so that Tensorflow works with TensorRT out of box?", "Setup tensorflow/tensorflow:2.7.0-gpu container\r\n```\r\n$ sudo docker run -it --name tf270 --gpus all tensorflow/tensorflow:2.7.0-gpu\r\n```\r\nTest TF/TRT\r\n```\r\n$ apt list --installed | grep infer\r\n\r\nlibnvinfer-plugin8/unknown,now 8.0.0-1+cuda11.0 amd64 [installed,upgradable to: 8.2.0-1+cuda11.4]\r\nlibnvinfer8/unknown,now 8.0.0-1+cuda11.0 amd64 [installed,upgradable to: 8.2.0-1+cuda11.4]\r\n\r\n\r\nimport tensorflow as tf\r\nimport tensorflow.compiler as tf_cc\r\ntf_cc.tf2tensorrt._pywrap_py_utils.get_linked_tensorrt_version()\r\n\r\n(7, 2, 2)\r\n```\r\nok, installed TRT is 8.0.0 but Tensorflow was built and linked with TRT 7.2.2. Looks like nobody tested TensorRT with tensorflow/tensorflow:2.7.0-gpu image\r\n", "To run TF 2.7.0 with TRT-7.2.3 we can use TF 2.6.1-gpu image (because 2.6.1 image has correct TRT version - 7.2.2)\r\n```\r\nsudo docker run -it --name tf270 --gpus all tensorflow/tensorflow:2.6.1-gpu\r\napt update\r\napt upgrade\r\n\r\npython3 -m pip install -U pip\r\npython3 -m pip install -U tensorflow==2.7.0\r\n\r\napt install vim\r\n# fix minor bug in trt_convert.py log message\r\nvi /usr/local/lib/python3.8/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\r\n# go to line 266 and replace `trt_utils.versionTupleToString(loaded_version)` with just `loaded_version`. \r\n# Do the same for `linked_version` on line 267\r\n\r\nexport LD_LIBRARY_PATH=/usr/local/cuda-11.1/targets/x86_64-linux/lib\r\npython3\r\n```\r\nTest Tensorflow/TRT converter\r\n```\r\nimport tensorflow as tf\r\ntf.test.is_gpu_available()\r\nimport tensorflow.compiler as tf_cc\r\ntf_cc.tf2tensorrt._pywrap_py_utils.is_tensorrt_enabled()\r\n\r\nfrom tensorflow.python.compiler.tensorrt import trt_convert as trt\r\nconversion_params = trt.TrtConversionParams(precision_mode=trt.TrtPrecisionMode.FP16)\r\n\r\nconverter = trt.TrtGraphConverterV2(\r\n    input_saved_model_dir=\"saved_model\",\r\n    conversion_params=conversion_params)\r\n\r\nconverter.convert()\r\n```", "@apivovarov @BorisPolonsky any workaround for working this out on Google Colab or Kaggle Kernels? This [official Keras example](https://keras.io/examples/vision/near_dup_search/) is failing because of this. ", "> @apivovarov @BorisPolonsky any workaround for working this out on Google Colab or Kaggle Kernels? This [official Keras example](https://keras.io/examples/vision/near_dup_search/) is failing because of this.\r\n\r\nI use `tensorflow 2.4.0` for this purpose for now. ", "I see. Do you use it on Colab?", "> I see. Do you use it on Colab?\r\n\r\nI run containers from official docker images.", "Yeah as I mentioned in the comment, yes. ", "The workaround can be:\r\n- use Docker image `tensorflow/tensorflow:2.6.1-gpu`\r\n- install `tensorflow==2.7.1` using pip", "But won't be possible on Colab and Kaggle I guess. ", "Do you have a link to a sample Colab?\r\nI tried [this one](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/quickstart/beginner.ipynb) - it does not have TensorRT at all\r\n```\r\nimport tensorflow as tf\r\nprint(\"TensorFlow version:\", tf.__version__)\r\nimport tensorflow.compiler as tf_cc\r\nlinked_trt_ver=tf_cc.tf2tensorrt._pywrap_py_utils.get_linked_tensorrt_version()\r\nprint(f\"Linked TRT ver: {linked_trt_ver}\")\r\n```\r\n```\r\nTensorFlow version: 2.7.0\r\nLinked TRT ver: (0, 0, 0)\r\n```\r\n", "Yeah, that is what. I was asking if there's a workaround for that.", "[This Colab notebook](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/quickstart/beginner.ipynb) does not have GPU at all.\r\n```\r\nimport tensorflow as tf\r\nprint(\"TensorFlow version:\", tf.__version__)\r\ntf.test.is_gpu_available()\r\n```\r\n```\r\nTensorFlow version: 2.7.0\r\nFalse\r\n```", "You need to change the runtime:\r\n\r\n![image](https://user-images.githubusercontent.com/22957388/153375816-7350069f-336e-4675-9604-835b205c0ac3.png)\r\n\r\n![image](https://user-images.githubusercontent.com/22957388/153375888-3f4e602c-adce-4622-8eba-56774f1d0cdf.png)\r\n", "Got it, Thank you!\r\nLooks like Tensorflow 2.7.0 binary which is used in this notebook was build with Cuda support, but not with TensorRT support.\r\n```\r\nimport tensorflow as tf\r\nprint(\"TensorFlow version:\", tf.__version__)\r\nprint(\"is_gpu_available:\", tf.test.is_gpu_available())\r\nimport tensorflow.compiler as tf_cc\r\nprint(\"is TensorRT enabled:\", tf_cc.tf2tensorrt._pywrap_py_utils.is_tensorrt_enabled())\r\nprint(\"loaded trt ver:\", tf_cc.tf2tensorrt._pywrap_py_utils.get_loaded_tensorrt_version())\r\nprint(\"linked trt ver:\", tf_cc.tf2tensorrt._pywrap_py_utils.get_linked_tensorrt_version())\r\n```\r\n```\r\nTensorFlow version: 2.7.0\r\nis_gpu_available: True\r\nis TensorRT enabled: False\r\nloaded trt ver: (0, 0, 0)\r\nlinked trt ver: (0, 0, 0)\r\n```\r\nThis github issue is about Docker image `tensorflow/tensorflow:2.7.0-gpu` (which uses pypi Tensorflow 2.7.0)\r\nThe Colab notebook is most probably not using this Docker image and definitely not using  pypi Tensorflow 2.7.0.\r\n```\r\ntf.sysconfig.get_build_info()\r\n\r\nTensorFlow version: 2.7.0\r\nOrderedDict([('cpu_compiler', '/usr/bin/x86_64-linux-gnu-gcc-7'),\r\n             ('cuda_compute_capabilities',\r\n              ['compute_37',\r\n               'compute_60',\r\n               'compute_61',\r\n               'compute_70',\r\n               'compute_75',\r\n               'compute_80']),\r\n             ('cuda_version', '11.1'),\r\n             ('cudnn_version', '8'),\r\n             ('is_cuda_build', True),\r\n             ('is_rocm_build', False),\r\n             ('is_tensorrt_build', False)])\r\n```", "Seems like there isn't a workaround for this yet? \r\n\r\nI did have success with the earlier versions of TensorFlow installed on Colab. "]}, {"number": 53525, "title": "Type 'INT8' is not supported by tile.Node number 92 (TILE) failed to invoke", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution: MacOS 10.15.7\r\n- TensorFlow installation (pip package or built from source): pip3\r\n- TensorFlow library (version): 2.7.0\r\n\r\n### 2. Code\r\n\r\nCode to run the inference on the int8 model (change `np.int8` to `np.float32` in you want to try float16 model):\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# Load the TFLite model and allocate tensors.\r\ngenerator_interpreter = tf.lite.Interpreter(model_path=\"kp_detector.tflite\")\r\ngenerator_interpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = generator_interpreter.get_input_details()\r\noutput_details = generator_interpreter.get_output_details()\r\n\r\nfor i in range(0, len(input_details)):\r\n    # Test the model on random input data.\r\n    input_shape = input_details[i]['shape']\r\n    input_data = np.array(np.random.random_sample(input_shape), dtype=np.int8)\r\n    generator_interpreter.set_tensor(input_details[i]['index'], input_data)\r\n\r\ngenerator_interpreter.invoke()\r\n\r\n# The function `get_tensor()` returns a copy of the tensor data.\r\n# Use `tensor()` in order to get a pointer to the tensor.\r\noutput_data = generator_interpreter.get_tensor(output_details[0]['index'])\r\nprint(output_data)\r\n```\r\n\r\n#### 3. Failure after conversion\r\nI have converted a `.pb` model to `.tflite` with float16 and int8 conversions. The inference is correct on float16, but not the int8. Let me know if you want me to provide the models.  \r\n", "comments": ["@pkarimib \r\nIn order to reproduce the issue reported here, could you please provide the complete code(models) that you are using. Thanks!", "@sushreebarsa \r\nThanks for your response. The code is in [this link](https://drive.google.com/file/d/1ZpWAyFw-Zk8b5CwGy4-kX7_DOidme60K/view?usp=sharing). \r\n\r\nRun the following to test the quantized model:\r\n```\r\npython3 test_quantized_model.py \r\n```\r\n\r\nTo build the int8 module, please run:\r\n```\r\npython3 build.py --int8 --module generator \r\n```\r\n\r\nPlease let me know if you need anything else.", "What is the difference in accuracy you are getting from float16 to int8?.\r\nAlso, check the model performance using the tools [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks#accuracy--correctness).", "I have not been successful in running the model so I could measure the accuracy. The code exits with `failed to invoke` error. "]}, {"number": 53521, "title": "[Go] Document use of tensorflow.ConfigProto protocol message to set session options", "body": "PR documents the use of tensorflow.ConfigProto to set session options (e.g., for use in calls to `LoadSavedModel` or `NewSession`), adding a test and two examples.\r\n\r\n@mattn - I also wonder if this is suitable to close #22926?  It looks like that issue was created before the bindings depended on compiled core protos.", "comments": []}, {"number": 53517, "title": "Can All_gather of collective_ops be used in deep learning model building and reverse optimization of the model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux CentOS 7.6.1810\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):1.15.4\r\n- Python version:3.6.8\r\n- Bazel version (if compiling from source):None\r\n- GCC/Compiler version (if compiling from source):None\r\n- CUDA/cuDNN version:None\r\n- GPU model and memory:None\r\n\r\n**Describe the current behavior**\r\nI want to implement a data exchange function between workers,So I use Send/Recv of collective_ops while building a model.But errors occured when computing gradients.\r\n> No gradient defined for operation 'CollectiveGather_1' (op type: CollectiveGather)\r\n\r\n**Describe the expected behavior**\r\nNormal training.\r\n\r\n**Code to reproduce the issue**\r\n``` python\r\n import tensorflow as tf\r\n from tensorflow.python.ops import collective_ops\r\n FLAGS = tf.app.flags.FLAGS\r\n worker_replicas=2\r\n def shuffle(tensor):\r\n    batch_size = tf.shape(tensor)[0]\r\n    rank = FLAGS.task_index\r\n    with tf.device('/cpu:0'):\r\n        all_idx = tf.range(worker_replicas * batch_size)\r\n        shuffle_idx = tf.random.shuffle(all_idx)\r\n        if FLAGS.task_index == 0:\r\n            index_broadcast = collective_ops.broadcast_send(shuffle_idx, shape=shuffle_idx.shape,\r\n                                                            dtype=shuffle_idx.dtype,\r\n                                                            group_size=worker_replicas, group_key=3, instance_key=100)\r\n        else:\r\n            index_broadcast = collective_ops.broadcast_recv(shape=shuffle_idx.shape, dtype=shuffle_idx.dtype,\r\n                                                            group_size=worker_replicas, group_key=3, instance_key=100)\r\n        my_idxs = tf.slice(index_broadcast, [rank * batch_size], [batch_size])\r\n        all_tensor = collective_ops.all_gather(\r\n            tensor, worker_replicas, worker_replicas, worker_replicas)\r\n\r\n    return tf.gather(all_tensor, my_idxs), shuffle_idx\r\n\r\n\r\n```\r\n\r\n**Other info / logs**\r\nI insert the code snippet while model building,it shuffles tensors between workers,but the log as follows.\r\n```\r\nFile \"/home/shangyw/git_rep/test_model/deploy/model_deploy.py\", line 435, in _optimize_clone\r\n    clone_grad = optimizer.compute_gradients(sum_loss, **kwargs)\r\n  File \"/home/shangyw/virtualenv/p3_tf115/lib/python3.6/site-packages/tensorflow_core/python/training/optimizer.py\", line 512, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/home/shangyw/virtualenv/p3_tf115/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_impl.py\", line 158, in gradients\r\n    unconnected_gradients)\r\n  File \"/home/shangyw/virtualenv/p3_tf115/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 637, in _GradientsHelper\r\n    (op.name, op.type))\r\n2021-12-22 11:04:07,317 - DEBUG - No gradient defined for operation 'CollectiveGather_1' (op type: CollectiveGather)\r\n```\r\n\r\n\r\n\r\n", "comments": ["@upwindflys ,\r\nWe see that you are using tf version 1.15, 1.x is not actively supported, please update to v2.7 and let us know if you are using same issue.", "@tilakrayal Thank you for your reply.But there is no available 2.7.x tf version, so I tried tf version 2.6,the same error occured.\r\n\r\n```\r\n(p3_tf2) root@147tf$ pip install tensorflow-gpu==2.7\r\nERROR: Could not find a version that satisfies the requirement tensorflow-gpu==2.7 (from versions: 0.12.1, 1.0.0, 1.0.1, 1.1.0, 1.2.0, 1.2.1, 1.3.0, 1.4.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.9.0, 1.10.0, 1.10.1, 1.11.0, 1.12.0, 1.12.2, 1.12.3, 1.13.1, 1.13.2, 1.14.0, 1.15.0, 1.15.2, 1.15.3, 1.15.4, 1.15.5, 2.0.0, 2.0.1, 2.0.2, 2.0.3, 2.0.4, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.1.4, 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.4.0, 2.4.1, 2.4.2, 2.4.3, 2.4.4, 2.5.0, 2.5.1, 2.5.2, 2.6.0, 2.6.1, 2.6.2)\r\nERROR: No matching distribution found for tensorflow-gpu==2.7\r\n```\r\n```\r\nTraceback (most recent call last):\r\n  File \"all_gather_gradient.py\", line 98, in <module>\r\n    start_process()\r\n  File \"all_gather_gradient.py\", line 94, in start_process\r\n    process_fn(worker_hosts,INDEX)\r\n  File \"all_gather_gradient.py\", line 63, in process_fn\r\n    grad = tf.gradients(loss,params)\r\n  File \"/home/shangyw/virtualenv/p3_tf2/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 172, in gradients\r\n    unconnected_gradients)\r\n  File \"/home/shangyw/virtualenv/p3_tf2/lib/python3.6/site-packages/tensorflow/python/ops/gradients_util.py\", line 636, in _GradientsHelper\r\n    (op.name, op.type))\r\nLookupError: No gradient defined for operation 'worker0/CollectiveGather' (op type: CollectiveGather)\r\n\r\n```\r\n", "@tilakrayal \r\nI tried the tf version 2.7.0 by using docker image tensorflow/tensorflow:2.7.0-gpu. But the same error occured.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gradients_util.py\", line 598, in _GradientsHelper\r\n    grad_fn = ops.get_gradient_function(op)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\", line 2788, in get_gradient_function\r\n    return gradient_registry.lookup(op_type)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/registry.py\", line 99, in lookup\r\n    raise LookupError(\r\nLookupError: gradient registry has no entry for: CollectiveGather\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"all_gather_gradient.py\", line 98, in <module>\r\n    start_process()\r\n  File \"all_gather_gradient.py\", line 94, in start_process\r\n    process_fn(worker_hosts,INDEX)\r\n  File \"all_gather_gradient.py\", line 63, in process_fn\r\n    grad = tf.gradients(loss,params)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 169, in gradients\r\n    return gradients_util._GradientsHelper(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gradients_util.py\", line 626, in _GradientsHelper\r\n    raise LookupError(\r\nLookupError: No gradient defined for operation'worker0/CollectiveGather' (op type: CollectiveGather). In general every operation must have an associated `@tf.RegisterGradient` for correct autodiff, which this op is lacking. If you want to pretend this operation is a constant in your program, you may insert `tf.stop_gradient`. This can be useful to silence the error in cases where you know gradients are not needed, e.g. the forward pass of tf.custom_gradient. Please see more details in https://www.tensorflow.org/api_docs/python/tf/custom_gradient.\r\n", "@upwindflys ,\r\nWhile trying to execute the given code i was facing different error.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/d1bfa5103ba3720b64632f01e97f9497/untitled154.ipynb) and provide the latest code to reproduce the issue.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@tilakrayal Hi,\r\n### Code to reproduce the issue.\r\n```python\r\n\"\"\"File all_gather.py\"\"\"\r\n\r\nimport numpy as np\r\nimport sys\r\nNUM_PROCESSES = 2\r\nNUM_TOWERS = 1\r\n\r\nINDEX = int(sys.argv[1])\r\nprint ('Index: {}'.format(INDEX))\r\ndef process_fn(worker_hosts, task_index):\r\n    \"\"\"allgather process\"\"\"\r\n\r\n    import time\r\n    import tensorflow.compat.v1 as tf\r\n    tf.disable_v2_behavior()\r\n    from tensorflow.python.ops import collective_ops\r\n\r\n    cluster_spec = tf.train.ClusterSpec({'worker': worker_hosts})\r\n\r\n    # unconfigured collective_group_leader make each worker the leader\r\n    # '/replica:0' is necessary in the configuration.\r\n    config = tf.ConfigProto(device_count={'CPU': NUM_TOWERS})\r\n    config.experimental.collective_group_leader = '/job:worker/replica:0/task:0'\r\n    server = tf.train.Server(cluster_spec, config=config,\r\n                             job_name='worker', task_index=task_index)\r\n    with tf.Graph().as_default():\r\n        # create weight\r\n        all_weights = list()\r\n        for worker_index in range(NUM_PROCESSES):\r\n            worker_weights = list()\r\n            with tf.variable_scope('worker{}'.format(worker_index)):\r\n                for tower_index in range(NUM_TOWERS):\r\n                    device = '/job:worker/replica:0/task:{}/device:CPU:{}'.format(\r\n                        worker_index, tower_index)\r\n                    with tf.device(device):\r\n                        x_data = np.array([1,2])\r\n                        y_data = x_data*0.1+0.3\r\n                        weight = tf.get_variable(\r\n                            'weight{}'.format(tower_index), shape=[1])\r\n                        worker_weights.append(weight)\r\n                        all_idx = tf.range(2)\r\n                        shuffle_idx = tf.random.shuffle(all_idx)\r\n                        if task_index == 0:\r\n                            tensor = collective_ops.broadcast_send(shuffle_idx, shape=shuffle_idx.shape,\r\n                                                                   dtype=shuffle_idx.dtype, group_size=NUM_PROCESSES,\r\n                                                                   group_key=3, instance_key=100)\r\n                        else:\r\n                            tensor = collective_ops.broadcast_recv(shape=shuffle_idx.shape, dtype=shuffle_idx.dtype,\r\n                                                                   group_size=NUM_PROCESSES, group_key=3,\r\n                                                                   instance_key=100)\r\n                        my_idxs = tf.slice(tensor, [INDEX * 1], [1])\r\n                        inter_reduced = collective_ops.all_gather(\r\n                            weight, NUM_PROCESSES, NUM_PROCESSES, NUM_PROCESSES)\r\n                        dst = tf.gather(inter_reduced, my_idxs, axis=0)\r\n                        biases = tf.Variable(tf.zeros([1]))\r\n                        y = dst*x_data+biases\r\n                        loss = tf.reduce_mean(tf.square(y-y_data\r\n                                                        ))\r\n                        params = [weight,biases]\r\n                        grad = tf.gradients(loss,params)\r\n                        grads_param = zip(grad,params)\r\n                        optimizer = tf.train.GradientDescentOptimizer(0.01)\r\n                        train_op = optimizer.apply_gradients(grads_param)\r\n\r\n            all_weights.append(worker_weights)\r\n\r\n\r\n        inter_reduced = None\r\n\r\n        session_creator = tf.train.ChiefSessionCreator(\r\n            master=server.target)\r\n        with tf.train.MonitoredSession(session_creator=session_creator) \\\r\n                as mon_sess:\r\n            result_all = mon_sess.run(all_weights)\r\n            print('task {} sense {}'.format(task_index, result_all))\r\n            result_inter,my_idxs_np,result_slice,_ = mon_sess.run([inter_reduced,my_idxs,dst,tensor])\r\n            print('task {} \\n inter_reduce {} \\n my_idxs {} idx {} \\n slice_tensor {}'.format(task_index, result_inter,my_idxs_np, _, result_slice))\r\n            mon_sess.run(train_op)\r\n            time.sleep(1)\r\n\r\ndef start_process():\r\n    \"\"\"start process\"\"\"\r\n    port = 60000\r\n    host_fmt = 'localhost:{}'\r\n    worker_hosts = list()\r\n    for process_index in range(NUM_PROCESSES):\r\n        worker_hosts.append(host_fmt.format(port + process_index))\r\n    process_fn(worker_hosts,INDEX)\r\n\r\n\r\nif __name__ == '__main__':\r\n    start_process()\r\n\r\n```\r\n### Command\r\n`woker0: python all_gather.py 0   \r\nworker1: python all_gather.py 1 `", "@upwindflys,\r\nI was facing different error while trying to execute the code.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/19f7fa69150660ed4ce44b172120e4a4/untitled178.ipynb).", "@tilakrayal I modified the code receiving external incoming variables.\r\n### Code to reproduce the issue.\r\n```python\r\n\"\"\"Illustrate AllGather\"\"\"\r\nimport multiprocessing as mp\r\n\r\nMP_METHOD = 'fork'  # 'fork' (UNIX), 'spawn' (WINDOWS);\r\nimport numpy as np\r\nNUM_PROCESSES = 2\r\nNUM_TOWERS = 1\r\n\r\ndef process_fn(worker_hosts, task_index):\r\n    \"\"\"allgather process\"\"\"\r\n\r\n    import time\r\n    import tensorflow.compat.v1 as tf\r\n    tf.disable_v2_behavior()\r\n    from tensorflow.python.ops import collective_ops\r\n\r\n    cluster_spec = tf.train.ClusterSpec({'worker': worker_hosts})\r\n\r\n    # unconfigured collective_group_leader make each worker the leader\r\n    # '/replica:0' is necessary in the configuration.\r\n    config = tf.ConfigProto(device_count={'CPU': NUM_TOWERS})\r\n    config.experimental.collective_group_leader = '/job:worker/replica:0/task:0'\r\n    server = tf.train.Server(cluster_spec, config=config,\r\n                             job_name='worker', task_index=task_index)\r\n    with tf.Graph().as_default():\r\n        # create weight\r\n        all_weights = list()\r\n        # for worker_index in range(NUM_PROCESSES):\r\n        worker_weights = list()\r\n        with tf.variable_scope('worker{}'.format(task_index)):\r\n            for tower_index in range(NUM_TOWERS):\r\n                device = '/job:worker/replica:0/task:{}/device:CPU:{}'.format(\r\n                    task_index, tower_index)\r\n                with tf.device(device):\r\n                    x_data = np.array([1,2])\r\n                    y_data = x_data*0.1+0.3\r\n                    weight = tf.get_variable(\r\n                        'weight{}'.format(tower_index), shape=[1])\r\n\r\n\r\n                    worker_weights.append(weight)\r\n                    all_idx = tf.range(2)\r\n                    shuffle_idx = tf.random.shuffle(all_idx)\r\n                    if task_index == 0:\r\n                        tensor = collective_ops.broadcast_send(shuffle_idx, shape=shuffle_idx.shape,\r\n                                                               dtype=shuffle_idx.dtype, group_size=NUM_PROCESSES,\r\n                                                               group_key=3, instance_key=100)\r\n                    else:\r\n                        tensor = collective_ops.broadcast_recv(shape=shuffle_idx.shape, dtype=shuffle_idx.dtype,\r\n                                                               group_size=NUM_PROCESSES, group_key=3,\r\n                                                               instance_key=100)\r\n                    my_idxs = tf.slice(tensor, [task_index * 1], [1])\r\n                    inter_reduced = collective_ops.all_gather(\r\n                        weight, NUM_PROCESSES, NUM_PROCESSES, NUM_PROCESSES)\r\n                    dst = tf.gather(inter_reduced, my_idxs, axis=0)\r\n\r\n                    biases = tf.Variable(tf.zeros([1]))\r\n                    y = dst*x_data+biases\r\n                    # y = weight * x_data + biases\r\n                    loss = tf.reduce_mean(tf.square(y-y_data\r\n                                                    ))\r\n                    params = [weight,biases]\r\n                    grad = tf.gradients(loss,params)\r\n                    grads_param = zip(grad,params)\r\n                    optimizer = tf.train.GradientDescentOptimizer(0.01)\r\n                    train_op = optimizer.apply_gradients(grads_param)\r\n\r\n\r\n        all_weights = worker_weights\r\n        inter_reduced = None\r\n\r\n        session_creator = tf.train.ChiefSessionCreator(\r\n            master=server.target)\r\n        with tf.train.MonitoredSession(session_creator=session_creator) \\\r\n                as mon_sess:\r\n            result_all = mon_sess.run(all_weights)\r\n            print('task {} sense {}'.format(task_index, result_all))\r\n            result_inter,my_idxs_np,result_slice,_ = mon_sess.run([inter_reduced,my_idxs,dst,tensor])\r\n            print('task {} \\n inter_reduce {} \\n my_idxs {} idx {} \\n slice_tensor {}'.format(task_index, result_inter,my_idxs_np, _, result_slice))\r\n            mon_sess.run(train_op)\r\n            time.sleep(1)\r\n\r\ndef start_process():\r\n    \"\"\"start process\"\"\"\r\n\r\n    port = 60000\r\n    host_fmt = 'localhost:{}'\r\n    worker_hosts = list()\r\n    for process_index in range(NUM_PROCESSES):\r\n        worker_hosts.append(host_fmt.format(port + process_index))\r\n    mp_ctx = mp.get_context(MP_METHOD)\r\n    processes = list()\r\n    for process_index in range(NUM_PROCESSES):\r\n        process = mp_ctx.Process(target=process_fn,\r\n                                 args=(worker_hosts, process_index,))\r\n        processes.append(process)\r\n        process.start()\r\n    for process in processes:\r\n        process.join()\r\n\r\n\r\n\r\nif __name__ == '__main__':\r\n    start_process()\r\n\r\n\r\n\r\n\r\n```", "@sachinprasadhs ,\r\nI was able to reproduce the issue in tf v2.5, v2.7 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/d9d6490bb0f5ac3c9ac51bb31d0c3f65/untitled181.ipynb).", "I could not find the source for `collective_ops` which is registered under gradient function(`@ops.RegisterGradient`).\r\nMeanwhile you can check [this](https://www.tensorflow.org/guide/create_op#implement_the_gradient_in_python) to implement the gradient in python for the custom ops.", "I have the same problem. Could you please solve this problem? You don't provide gradient functions for these collective_ops operations, which makes TensorFlow looks like a work-in-progress. A lot of my friends have switched to Pytorch and it looks like I have to do the same.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53517\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53517\">No</a>\n"]}, {"number": 53501, "title": "Batches handled differently in the int reference fully connected kernel", "body": "**System information**\r\n-- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): SHA1: 403b4b19f9d143b4868d8a822c325acb6fdd8704\r\n\r\n**Describe the current behavior**\r\nAlmost all fully connected kernels handle batches differently than the int8 reference fully connected kernel. Is this really intentional or a bug?\r\n\r\nWe have seen models (with keep_num_dims=True for fully_connected) where batches can be different in fully connected when a model has been converted from float to int8.\r\n\r\nfind . -name \"fully_connected.h\" |xargs grep \"const int batches\"|grep -v \"//\"\r\n./lite/kernels/internal/reference/fully_connected.h:  const int batches = FlatSizeSkipDim(output_shape, output_dims_count - 1);\r\n./lite/kernels/internal/reference/fully_connected.h:  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);\r\n./lite/kernels/internal/reference/fully_connected.h:  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);\r\n./lite/kernels/internal/reference/fully_connected.h:  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);\r\n./lite/kernels/internal/reference/fully_connected.h:  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);\r\n./lite/kernels/internal/reference/integer_ops/fully_connected.h:  const int batches = output_shape.Dims(0);\r\n./lite/kernels/internal/reference/integer_ops/fully_connected.h:  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);\r\n./lite/kernels/internal/optimized/sparse_ops/fully_connected.h:  const int batches = FlatSizeSkipDim(output_shape, output_dims_count - 1);\r\n./lite/kernels/internal/optimized/sparse_ops/fully_connected.h:  const int batches = thread_end - thread_start;\r\n./lite/kernels/internal/optimized/sparse_ops/fully_connected.h:  const int batches =\r\n./lite/kernels/internal/optimized/integer_ops/fully_connected.h:  const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1);\r\n\r\nThe typical case is const int batches = FlatSizeSkipDim(output_shape, output_dim_count - 1)\r\nIn the int8 ref case: const int batches = output_shape.Dims(0);\r\n\r\n**Describe the expected behavior**\r\nBatches are handled the same way for int and float fc kernels.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): yes\r\n- Briefly describe your candidate solution(if contributing):\r\nChange const int batches = output_shape.Dims(0);\r\nTo\r\nconst int output_dims_count = output_shape.DimensionsCount();\r\nconst int batches = FlatSizeSkipDim(output_shape, output_dims_count - 1);\r\n\r\n", "comments": ["We are also seeing this issue. As a workaround, conv layers with 1x1 filter kernels can substitute for fully connected layers. Note that for quantized models this will result in per-channel quantization being applied, which may impact your model size and output.\r\n\r\nThere's a related issue here tracking fixing the problem for optimized kernels on Arm embedded devices that use TensorFlow Lite for Microcontrollers: https://github.com/ARM-software/CMSIS_5/issues/1398", "With https://github.com/tensorflow/tensorflow/pull/54223 merged the problem should now be solved. The fix was also imported in tflite-micro  (see commit https://github.com/tensorflow/tflite-micro/commit/1501b574b74fd7877aba30aa9d8b667f41b139c3).", "@mansnils , Could you please try it in Nightly version and let us know if the problem is fixed as per the above comment. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 53491, "title": "Error occur: undefined reference when trying to ndk-build to run tflite example", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 14.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: RK3399 embedded board\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r2.7\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): NDK r21\r\n\r\n---\r\n\r\n**Describe the problem**\r\n\r\nI'm trying to NDK build simple tflite example code.\r\n\r\nWhen I build, this error is occured.\r\n\r\n```error: undefined reference to 'tflite::InterpreterBuilder::operator()(std::__1::unique_ptr<tflite::Interpreter, std::__1::default_delete<tflite::Interpreter> >*)'```\r\n\r\n---\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nFirst, I build tflite. I type command ```bazel build --config=elinux_aarch64 -c opt //tensorflow/lite:libtensorflowlite.so``` and libtensorflow.so file is created.\r\n\r\nAnd Secondely, I prepare tflite code.\r\n```C\r\n  // Load model\r\n  std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(model_file.c_str());\r\n\r\n  // Build the interpreter\r\n  tflite::ops::builtin::BuiltinOpResolver resolver;\r\n  std::unique_ptr<tflite::Interpreter> interpreter;\r\n  tflite::InterpreterBuilder(*model, resolver)(&interpreter);    <<<<<<----- This is where the error occurs. \r\n\r\n  interpreter->SetNumThreads(num_thread);\r\n  interpreter->AllocateTensors();\r\n\r\n  // Set input\r\n  float* input = interpreter->typed_input_tensor<float>(0);   \r\n  for(int y=0; y<height; y++)\r\n  {\r\n    for(int x=0; x<width; x++)\r\n    {\r\n      *(input + y*width + x) = ((float)img.data[3*y*width + 3*x]);\r\n      *(input + y*width + x + 1 * height*width) = ((float)img.data[3*y*width + 3*x + 1]);\r\n      *(input + y*width + x + 2 * height*width) = ((float)img.data[3*y*width + 3*x + 2]);\r\n    }\r\n  }\r\n\r\n  TfLiteTensor* output_tensor = nullptr;\r\n  interpreter->Invoke();\r\n  output_tensor = interpreter->tensor(interpreter->outputs()[0]);\r\n  printf(\"Elapsed: %f\\n\\n\",sec.count()*1000);\r\n```\r\n\r\nand then, I link the libtensorflow.so file.\r\n\r\nI add two line in my Android.mk\r\n\r\n```\r\nTFLITE_INSTALL_PATH := packages/apps/3rdparty/tflite\r\nLOCAL_LDLIBS:= $(TFLITE_INSTALL_PATH)/lib_android/libtensorflowlite.so\r\n```\r\n\r\n---\r\n\r\nWhat I tried to solve the error is followed\r\n\r\n1) change ndk version\r\nNow I'm using ndk ver 21, and I try another version(13, 15, 18) too.\r\n\r\n2) re-build tensorflow lite\r\nI add option such as --config=monolithic, --config=dynamic_kernels and --cxxopt=--std=c++14 And I also change option --config=elinux_aarch64 to --config=android_arm64\r\n\r\nIn this situation, Is there anything to check? I don't know what to do anymore. Is this problem of Android.mk setting? or build bazel setting?\r\n\r\n---\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@woojinn8 Could you please have a look at the similar [issue1](https://github.com/tensorflow/tensorflow/issues/39118) , [issue2 ](https://stackoverflow.com/questions/64805224/undefined-references-when-trying-to-use-tensorflow-lite-c-api-built-for-arm64-w)and let us know if it helps?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@sushreebarsa Thank you for reply. Unfortunately both issue are not useful. I'd better to check my configuration again.", "@woojinn8 Did you finally figure this issues out? I got in the same trouble.", "@woojinn8 hi\uff0cI finally figured this out "]}, {"number": 53469, "title": "[Go] Improve tests: fix test issues, add cases and locate assets ", "body": "PR makes improvements to tensorflow/go tests:\r\n- Fixes several missing checks on returned errors\r\n- Adds test cases for LoadSavedModel (half plus two) and improves consistency of variable naming with funcs and documentation for api\r\n- Locates a minimal set of required test assets inside the module; this addresses issues caused by making relative references elsewhere in the repository and outside of the Go module\r\n- Makes a few grammatical corrections", "comments": []}, {"number": 53464, "title": "TensorFlow Lite Model Maker has version conflict during install on RPi02W", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/lite/guide/model_maker\r\n\r\nPlease provide a link to the documentation entry, for example:\r\n[Model Maker Object Detection](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/)\r\n\r\n## Description of issue (what needs changing):\r\n\r\nCannot install **tflite_model_maker** and **tflite_support** on Raspberry Pi Zero 2 W\r\n\r\n` uname -a\r\nLinux raspbari17 5.10.63-v7+ #1488 SMP Thu Nov 18 16:14:44 GMT 2021 armv7l GNU/Linux\r\n`\r\n### Clear description\r\n\r\nTo fix this you could try to:\r\n1. _loosen_ the range of package versions you've specified\r\n2. _remove_ package versions to allow pip attempt to solve the dependency conflict\r\n\r\nERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/user_guide/#fixing-conflicting-dependencies\r\n\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct? n/a\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly? Yes\r\n\r\n### Returns defined\r\n\r\nAre return values defined? n/a\r\n\r\n### Raises listed and defined\r\n\r\nERROR: Cannot install tflite-model-maker==0.1.0, tflite-model-maker==0.1.1, tflite-model-maker==0.1.2, tflite-model-maker==0.2.0, tflite-model-maker==0.2.1, tflite-model-maker==0.2.2, tflite-model-maker==0.2.3, tflite-model-maker==0.2.4, tflite-model-maker==0.2.5, tflite-model-maker==0.3.0, tflite-model-maker==0.3.1, tflite-model-maker==0.3.2, tflite-model-maker==0.3.3 and tflite-model-maker==0.3.4 because these package versions have conflicting dependencies.\r\n\r\nThe conflict is caused by:\r\n    tflite-model-maker 0.3.4 depends on tensorflow-addons>=0.11.2\r\n    tflite-model-maker 0.3.3 depends on tensorflow-addons>=0.11.2\r\n    tflite-model-maker 0.3.2 depends on tensorflow-addons>=0.11.2\r\n    tflite-model-maker 0.3.1 depends on tensorflow-addons>=0.11.2\r\n    tflite-model-maker 0.3.0 depends on tensorflow-addons>=0.11.2\r\n    tflite-model-maker 0.2.5 depends on tflite-support==0.1.0rc4\r\n    tflite-model-maker 0.2.4 depends on tflite-support==0.1.0rc4\r\n    tflite-model-maker 0.2.3 depends on tflite-support==0.1.0rc3.dev2\r\n    tflite-model-maker 0.2.2 depends on tflite-support==0.1.0rc3.dev2\r\n    tflite-model-maker 0.2.1 depends on tf-nightly==2.4.0.dev20200811\r\n    tflite-model-maker 0.2.0 depends on tf-nightly==2.4.0.dev20200810\r\n    tflite-model-maker 0.1.2 depends on tf-nightly\r\n    tflite-model-maker 0.1.1 depends on tf-nightly\r\n    tflite-model-maker 0.1.0 depends on tf-nightly\r\n\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\n`sudo python3 -m pip install tflite-model-maker`\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? No but terminal output entered above.\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? No, beyond my job grade. Sorry.\r\n", "comments": ["I've been getting the same error trying to install `tflite-model-maker` on an M1 Mac for a few months. I've tried to install:\r\n\r\n* In a Miniforge3 environment;\r\n* In a Python Docker image;\r\n* In a Miniconda3 x86 environment, using a version of Terminal running through Rosetta.\r\n\r\n```\r\nconda create --name test python\r\nconda activate test\r\npip install tflite-model-maker\r\n```\r\n\r\nI was, however, able to install it using a Miniconda environment or a Python Docker image on Windows via WSL2.", "@baqwas,\r\n\r\nCould you let us know what is the version of `tflite-model-maker` and `tensorflow` installed? \r\n\r\nPlease refer [requirements.txt](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/lite/model_maker/requirements.txt) for dependent libraries and let us know if it helps you? Thanks!\r\n\r\n", "Hello @chunduriv,\r\n\r\nSorry, I couldn't locate any _applicable_ `requirements.txt` file (related to TFL) except for Pete Warden's famous _Magic Wand_ example. My installation steps are:\r\n\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow.git tensorflow\r\ncd tensorflow && ./tensorflow/lite/tools/make/download_dependencies.sh\r\n./tensorflow/lite/tools/make/build_rpi_lib.sh\r\n\r\n```\r\n\r\nI don't have the knowledge to loosen the version requirements. I tried the `-Wno-error=class-memaccess` option but that did not eliminate that specific error. The Flatbuffers team claim to have fixed the issue at their end several versions ago. Here is the advice from Rients at Q-Engineering (who success in leveraging RPi0 I was trying to emulate with RPi02W):\r\n\r\n```\r\nThey repaired the issue on May 29, 2020. The next fixed version will be v2.0.0 released on May 10, 2021.\r\nLooking at the download script of TF 2.6 and 2.7 you see they use v1.12.0. That is why it errors. (BTW, only with gcc 10.2.1 found in Bullseye).\r\nTF has deprecate the script building of lite, leaving with the hint to use Bazel.\r\nTip: try replacing the v.1.12.0 with the v2.0.0.tar.gz in the download_dependencies.sh (tensorflow/lite/tools/make/)\r\n\r\n```\r\n\r\nI apologize for not being able to reply more precisely to your request for information (so that you can provide further guidance).\r\n\r\nThe hardware platform information is:\r\n```\r\n\r\ncat /proc/cpuinfo\r\n...\r\nprocessor\t: 3\r\nmodel name\t: ARMv7 Processor rev 4 (v7l)\r\nBogoMIPS\t: 38.40\r\nFeatures\t: half thumb fastmult vfp edsp neon vfpv3 tls vfpv4 idiva idivt vfpd32 lpae evtstrm crc32 \r\nCPU implementer\t: 0x41\r\nCPU architecture: 7\r\nCPU variant\t: 0x0\r\nCPU part\t: 0xd03\r\nCPU revision\t: 4\r\n\r\nHardware\t: BCM2835\r\nRevision\t: 902120\r\nModel\t\t: Raspberry Pi Zero 2 Rev 1.0\r\n\r\ncat /proc/version\r\nLinux version 5.10.63-v7+ (dom@buildbot) (arm-linux-gnueabihf-gcc-8 (Ubuntu/Linaro 8.4.0-3ubuntu1) 8.4.0, GNU ld (GNU Binutils for Ubuntu) 2.34) #1488 SMP Thu Nov 18 16:14:44 GMT 2021\r\n\r\ncat /home/pi/projects/tensorflow/tensorflow/lite/micro/examples/magic_wand/train/requirements.txt\r\nnumpy==1.16.2\r\ntensorflow==2.5.0\r\n\r\n```\r\n\r\nWe're all doing TFL modeling on our desktops and downloading to our TinyML devices but being to do _one stop shopping_ using RPi02W would be more productive, versatile and ubiquitous. Thanks for your help and understanding.\r\n\r\nKind regards."]}, {"number": 53449, "title": "Fix XNNPACK build failure with when -mcpu compiler switch is set", "body": "Some build systems set the --mcpu compiler switch, which is incompatible\r\nwith XNNPACK build mechanism. This patch removes this switch for XNNPACK\r\ncompilation.", "comments": ["Reviewers this is https://github.com/google/XNNPACK/issues/1559 is related to the PR. ", "@Maratyszcza could you take a look at this?", "The patch looks a bit hacky to me.\r\n@robert-kalmar could you share what kind of error do you have in which compiler? ", "@terryheo , imagine this situation when the cross compilation SDK set up the target processor with (`--mcpu` switch in GNU Toolchain) and one  cross-compiles the TensorFlow Lite with CMake, using this SDK. \r\n\r\nFor instance the Yocto Project (an embedded Linux distribution generator) cross compilation SDK set up the cross compilation environmental variables like this:\r\n```\r\nCC=aarch64-poky-linux-gcc  -mcpu=cortex-a53 -march=armv8-a+crc+crypto -fstack-protector-strong  -O2 -D_FORTIFY_SOURCE=2 -Wformat -Wformat-security -Werror=format-security --sysroot=...\r\nCFLAGS= -O2 -pipe -g -feliminate-unused-debug-types \r\nCXX=aarch64-poky-linux-g++  -mcpu=cortex-a53 -march=armv8-a+crc+crypto -fstack-protector-strong  -O2 -D_FORTIFY_SOURCE=2 -Wformat -Wformat-security -Werror=format-security --sysroot=...\r\nCXXFLAGS= -O2 -pipe -g -feliminate-unused-debug-types\r\n```\r\nHere the SDK set up the target processor using the combination of  --mcpu and --march compiler options. This is OK for most of the packages, and even beneficial, as the compiler can adjust the optimizations for best performance on the target processor. \r\nHowever XNNPACK has a different deployment strategy: It compiles and links together compute kernels optimized for various AARCH64 architectures (armv8, 8.1, 8.2) and at runtime it executes the best suited for CPU the XNNPACK runs on. \r\nAs the XNNPACK set/rewrites the `--march` only, the `--mcpu`  is set by the SDK. Some kernels compilation fails as using instructions not available on particular CPU. For instance the kernels using the arm-8.2a instructions fails to build for Cortex-A53 as it is arm-8.0a, consequently the whole XNNPACK build fails. \r\n\r\nThe problem is not with specific compiler, but with specific compiler options. ", "Hi guys!\r\n\r\nI'm facing the [same XNNPACK issue](https://github.com/google/XNNPACK/issues/1559) while building TFLite for Yocto (dunfell) BSP. And this solution is not working for me. \r\n\r\nIn my case Yocto TFLite package config chooses assembler as:\r\n`Found assembler: {build_dir}/tmp-glibc/work/aarch64-oe-linux/tensorflow-lite/2.7.0-r0/recipe-sysroot-native/usr/bin/aarch64-oe-linux/aarch64-oe-linux-gcc`\r\nBut CMAKE_ASM_FLAGS remains the same because CMAKE_ASM_COMPILER_ID is not set to GNU. It is not set at all.\r\nOriginal CMAKE_ASM_FLAGS (with -mcpu=cortex-a57) is used for XNNPACK compilation that leads to build failure.\r\n\r\nMy assumption is that `-mcpu=cortex-a57` should be removed from all CMAKE_*_FLAGS in XNNPACK config (not only for GNU id) if they are not empty to avoid any compiler configuration issues.", "@terryheo  Can you please review this PR ? Thank you!"]}, {"number": 53446, "title": "[PluggableDevice] Cross-device copies support", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v1.12.1-69032-gfb972503145 2.8.0-dev20211220\r\n- Python version: 3.9\r\n- Bazel version (if compiling from source): 3.7.2\r\n\r\n**Describe the current behavior**\r\n\r\nWhen using a PluggableDevice plugin with 2 physical graphics cards, the `memcpy_dtod` function is invoked. This function, however, only has a single `SP_Device` as a parameter, even when the `src` and `dst` memory regions are from different devices.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe `memcpy_dtod` function should have 2 `SP_Device` parameters in case the copy is done cross-adapter to give the plugin a chance to handle cross-device copies.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): yes\r\n- Briefly describe your candidate solution(if contributing): Adding a `memcpy_dtod_cross_adapter` function for cross-device memory copies, or adding a second `SP_Device` parameter to `memcpy_dtod` (although the latter wouldn't be backward compatible).\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.compat.v1.disable_eager_execution()\r\n\r\na = tf.compat.v1.placeholder(dtype=tf.float32, shape=[3,1])\r\na_values = [[1],[2],[3]]\r\n\r\nb = tf.compat.v1.placeholder(dtype=tf.float32, shape=[3,1])\r\nb_values = [[4],[6],[11]]\r\n\r\nc = tf.compat.v1.placeholder(dtype=tf.float32, shape=[1,3])\r\nc_values = [[2,4,5]]\r\n\r\nwith tf.device(\"/device:CUSTOM:0\"):\r\n    y1 = tf.raw_ops.AddN(inputs=[a,b], name=\"MyAdd\")\r\nwith tf.device(\"/device:CUSTOM:1\"):\r\n    y2 = tf.raw_ops.MatMul(a=y1, b=c, name=\"MyMultiply\")\r\n\r\nwith tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True)) as s:\r\n    print(s.run(y2, feed_dict={a:a_values, b:b_values, c:c_values}))\r\n```", "comments": ["@PatriceVignola \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),\r\nThanks!", "@sushreebarsa Thank you, I updated the issue with the template.", "@PatriceVignola Thank you for the update! \r\nWe see that you are using `TF v1.12` which is not actively supported ,could you please upgrade to `TF v2.4. or later`(FYI Latest stable TF version is 2.7.0 )?  \r\nThis` tf.compat.v1.Session` API was designed for TensorFlow v1. Please check this [details](https://www.tensorflow.org/api_docs/python/tf/compat/v1/Session) on how to migrate from this API to a native TensorFlow v2 equivalent. See the TensorFlow v1 to TensorFlow v2 [migration guide](https://www.tensorflow.org/guide/migrate) for instructions on how to migrate the rest of your code.Please have a look at the  [thread](https://stackoverflow.com/questions/56820327/the-name-tf-session-is-deprecated-please-use-tf-compat-v1-session-instead)\r\nAs `TensorFlow version 1.x `is out of support window so for any further queries  please post this issue in [TF Forum](https://discuss.tensorflow.org/) where there is a larger community to help .Thank you!", "I'm not using 1.12, I'm using 2.8.0-dev20211220 (I was also using 2.7.0 and having the same problem). This is a PluggableDevice specific issue, which is a TF 2 feature."]}, {"number": 53434, "title": "Documentation for unit testing is not clear", "body": "## URL(s) with the issue:\r\n\r\nOfficial TF tests documentation page: https://www.tensorflow.org/community/contribute/tests\r\n\r\n## Description of issue (what needs changing):\r\n\r\nIt is not at all clear how one should run unit tests. In fact, some unit tests are _very_ hard to execute singularly.\r\n\r\nFor example, I have been working on a PR that requires some testing on `//tensorflow/core/lib/io` libraries, and I have spent half an hour understanding how to run the unit test [`zlib_buffers_test.cc`](https://github.com/tensorflow/tensorflow/blob/610d01a46ca531231986c75d053a0fe81e87f101/tensorflow/core/lib/io/BUILD#L382).\r\n\r\nThis is how the exploration went:\r\n\r\n`zlib_buffers_test.cc` is included in the `filegroup` with name `legacy_lib_io_all_tests`. This in turn is included in [`//tensorflow/core:low_level_library_tests`](https://github.com/tensorflow/tensorflow/blob/610d01a46ca531231986c75d053a0fe81e87f101/tensorflow/core/BUILD#L1856).\r\n\r\nThis set of tests however is defined using `tf_cc_tests` (mind the final \"s\" here), which behavior is declared in [`//tensorflow/tensorflow.bzl`](https://github.com/tensorflow/tensorflow/blob/610d01a46ca531231986c75d053a0fe81e87f101/tensorflow/tensorflow.bzl#L1520), which basically allows me to run the tests under `legacy_lib_io_all_tests` like this:\r\n\r\n```\r\nbazel test //tensorflow/core:__tensorflow_core_lib_io_legacy_lib_io_all_tests\r\n```\r\n\r\nwhich is, at least for me, not really intuitive. And I still haven't understood how to effectively just test the `zlib_buffers_test.cc` unit test, if not by adding the flag `--test_filter=<filter>`.\r\n\r\nI couldn't find any resource online, nor on any issue here, that mentions the behavior of unit tests or a guideline on how they should be structured and tested for using `bazel`.\r\n\r\nI found, however, [a guy with my same issue](https://www.cxybb.com/article/weixin_44351244/116571161), so at least I know I am not alone \ud83d\ude05\r\n\r\n### Clear description\r\n\r\n> For example, why should someone use this method? How is it useful?\r\n\r\nAnytime someone wants to make improvements on a unit test, it should be relatively easy to execute it, and relevant documentation about how that process work can really speed up development and contributions to existing parts of the project.\r\n\r\n### Submit a pull request?\r\n\r\nI am very happy to help! If there is some existing documentation about the unit testing process, I would happily refer to that one. If it doesn't exist, but the testing process is \"known\" somehow, I would gladly add a piece of documentation to the existing page mentioning how to generally run unit tests.", "comments": ["Hello, \r\n\r\nIn any complex system there's some archaeology necessary to figure out what's going on. There's no way without at least learning the basics of bazel, and struggling is how we all learn.\r\n\r\nIf we want to provide better docs for people with this sort of problem, then this is probably the place to add them:\r\n\r\nhttps://github.com/tensorflow/docs/blob/master/site/en/community/contribute/tests.md\r\n\r\nIt sounds like what you're looking for is a \"finding and running tests\" section. That sounds like a reasonable addition. If we can make the content clear and generic enough to be helpful in general.\r\n"]}, {"number": 53433, "title": "Experimental jax natural allocation emit", "body": "**This code is only experimental and should never be used in production!**\r\n\r\nIt could be use to dump a JAX generated mlir module with xla.\r\n\r\n1. Steps: Donwload JAX on github git clone https://github.com/google/jax.\r\n2. Find the Workspace file. Switch to a local tensorflow with this modified mhlo_to_lhlo_with_xla pass.\r\n3. Pick a name of a mlir module and change the string in the modified mhlo_to_lhlo_with_xla.cc.\r\n\r\n> - If you do not know the module names dump all modules with the command of step 5.\r\n\r\n4. Build JAX with  (see [how to build Jax](https://jax.readthedocs.io/en/latest/developer.html) ) \r\n\r\n> - python build/build.py --enable cuda\r\n> - pip install dist/*.whl --force-reinstall \r\n\r\n5. Use this command to generate mlir modues:\r\n> - XLA_FLAGS=\"--xla_dump_to=folder/to/dump\" python3 ./examples/your_example.py\r\n6. JAX will through a exception but the picked module will be dumped with natural allocs.", "comments": ["@dfki-thsc This PR is in draft, any update on this? Please. Thank you!"]}, {"number": 53427, "title": "Is it possible to report warning when plugin device cannot load instead of error?", "body": "Hi,\r\nI found TF would crash when load plugin device fail, Is it possible to change it with warning instead of crash and do not enable plugin device? Just like the GPU device behavior.  \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/api_template.__init__.py#L150", "comments": ["Hi @guizili0 ! \r\nCould you please update the other details in this [template ](https://github.com/tensorflow/tensorflow/issues/new/choose)too as it helps us analyse the issue .Thanks!", "@mohantym my issue is belong to others issue, in others issue, I found this \r\n\r\n> For high-level discussions about TensorFlow, please post to discuss@tensorflow.org\r\n\r\nDo I need submit a topic here? https://discuss.tensorflow.org/\r\n\r\nBasically I want to plugin device load fail with warning instead of error.\r\nthe python code for the plugin device is here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/api_template.__init__.py#L150\r\nand the error raise is here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/tf_session_wrapper.cc#L714\r\nI hope for plugin device load, here just show warning instead of raise error. Thanks!\r\n\r\n", "Hi @Saduf2019! Could you please look at this issue?"]}, {"number": 53424, "title": "Enabling tf.debugging.set_log_device_placement(True) does not print output", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): `pip install tensorflow` / wheel\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: NVIDIA TU102 [GeForce RTX 2080 Ti Rev. A]\r\n\r\n**Describe the current behavior**\r\n\r\nI expect to see physical device placement output when running the examples in the [GPU documentation](https://tensorflow.google.cn/guide/gpu?hl=en).\r\n\r\n**Describe the expected behavior**\r\n\r\nI do not see any additional output when enabling the logging for device placement.\r\n\r\n- Do you want to contribute a PR? (yes/no): unsure\r\n- Briefly describe your candidate solution(if contributing): n/a\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.debugging.set_log_device_placement(True)\r\nprint(tf.__version__)\r\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\r\n\r\n# Place tensors on the CPU\r\nwith tf.device('/CPU:0'):\r\n  a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\r\n  b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\r\n  c = tf.matmul(a, b)\r\nprint(c)\r\n\r\n# Place tensors on the GPU\r\nwith tf.device('/GPU:0'):\r\n  a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\r\n  b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\r\n  c = tf.matmul(a, b)\r\nprint(c)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nRunning the above gives me the following output\r\n\r\n```\r\n2.7.0\r\nNum GPUs Available:  1\r\ntf.Tensor(\r\n[[22. 28.]\r\n [49. 64.]], shape=(2, 2), dtype=float32)\r\ntf.Tensor(\r\n[[22. 28.]\r\n [49. 64.]], shape=(2, 2), dtype=float32)\r\n```\r\n\r\ncudnn is 8.2.\r\n\r\n```\r\n$ cat /usr/include/cudnn_version.h  | grep -e '\\(PATCHLEVEL\\|MAJOR\\|MINOR\\)'\r\n#define CUDNN_MAJOR 8\r\n#define CUDNN_MINOR 2\r\n#define CUDNN_PATCHLEVEL 0\r\n#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)\r\n```\r\n\r\nand `nvcc`\r\n\r\n```\r\n$ nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2021 NVIDIA Corporation\r\nBuilt on Sun_Feb_14_21:12:58_PST_2021\r\nCuda compilation tools, release 11.2, V11.2.152\r\nBuild cuda_11.2.r11.2/compiler.29618528_0\r\n```", "comments": ["@sanatmpa1 ,\r\nI was able to get the same result as mentioned in doc for tf v2.7, v2.5 and tf-nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/f446baf19b99009b7c150c30b9a02af2/53424.ipynb).", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "> This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\r\n\r\nIt's a bug tho. You don't close issues of actual bugs do you?", "@kratsg ,\r\nPlease feel free to submit a PR for the requested change or share the link where requested change is to be made", "> Please feel free to submit a PR for the requested change or share the link where requested change is to be made\r\n\r\nI don't know what caused the regression.", "@kratsg ,\r\nAs mentioned in gist we are able to get the same result as provided in doc.Please feel free to submit a PR for the requested change.It helps to analyse and debug the bug.Thanks!", "@tilakrayal I don't understand why I'm only able to get those messages showing up in google colab. Is there some logging settings one needs to enable? I've tried on four different GPU machines I have handy with various OSes and I am unable to reproduce the colab. So I must be missing something. As an example of what I'm seeing\r\n\r\n```\r\n(venv) kratsg@slugpu:~$ python tflow-53424.py \r\n2.7.0\r\nNum GPUs Available:  1\r\ntf.Tensor(\r\n[[22. 28.]\r\n [49. 64.]], shape=(2, 2), dtype=float32)\r\ntf.Tensor(\r\n[[22. 28.]\r\n [49. 64.]], shape=(2, 2), dtype=float32)\r\n```\r\n\r\nand interactively\r\n\r\n```\r\n(venv) kratsg@slugpu:~$ python\r\nPython 3.8.5 (default, Jul 28 2020, 12:59:40) \r\n[GCC 9.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.debugging.set_log_device_placement(True)\r\n>>> print(tf.__version__)\r\n2.7.0\r\n>>> print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\r\nNum GPUs Available:  1\r\n>>> with tf.device('/CPU:0'):\r\n...   a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\r\n...   b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\r\n...   c = tf.matmul(a, b)\r\n... \r\n>>> print(c)\r\ntf.Tensor(\r\n[[22. 28.]\r\n [49. 64.]], shape=(2, 2), dtype=float32)\r\n>>> exit()\r\n```\r\n\r\nand I'm unable to get any messages coming out. I've also tried this on my Mac with a GPU and I cannot see any additional logging. However, when I launch this in a jupyter notebook instead -- I see the messages. Is this something that only shows up in a notebook?"]}, {"number": 53419, "title": "Use the same variable", "body": null, "comments": ["What is the purpose of this change? `input` and `input_param` are the same at that point in code, so using `input` instead of `input_param` makes no difference.", "In my opinion, using the same variable can make the program easier to understand. Or, why use it alternatively?"]}, {"number": 53417, "title": "TFLite GPU failed to delegate with ssd_mobilenet_v2_fpnlite_320 model", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):  2.6.0\r\n- Python version: 3.9.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: GeForce GTX 1050Ti 4G\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI downloaded SSD MobileNet V2 FPNLite 320x320 pretrained model from [TF Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md) , then used export_tflite_graph_tf2.py script to convert checkpoints to saved model.\r\n\r\n```\r\npython export_tflite_graph_tf2.py \\\r\n    --pipeline_config_path ssd_mobilenet_v2_fpnlite_320/pipeline.config \\\r\n    --trained_checkpoint_dir ssd_mobilenet_v2_fpnlite_320/checkpoint \\\r\n    --output_directory ssd_mobilenet_v2_fpnlite_320 \\\r\n    --config_override \" \\\r\n            model{ \\\r\n            ssd{ \\\r\n              post_processing { \\\r\n                batch_non_max_suppression { \\\r\n                        score_threshold: 0.0 \\\r\n                        iou_threshold: 0.5 \\\r\n                } \\\r\n             } \\\r\n          } \\\r\n       } \\\r\n       \"\r\n```\r\n\r\nThen convert saved model to tflite\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(ssd_mobilenet_v2_fpnlite_320/saved_model)\r\nconverter.allow_custom_ops = True\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.target_spec.supported_types = [tf.float16]\r\nconverter.experimental_new_converter = True\r\ntflite_model = converter.convert()\r\nopen(output_name, \"wb\").write(tflite_model)\r\n```\r\n\r\nHowever, when use NNAPI to delegate this model, it's failed with error :\r\n```\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nERROR: Following operations are not supported by GPU delegate:\r\nCUSTOM TFLite_Detection_PostProcess: Operation is not supported.\r\nDEQUANTIZE: \r\n327 operations will run on the GPU, and the remaining 2 operations will run on the CPU.\r\nERROR: TfLiteGpuDelegate Init: PACK: Tensor \"ssd_mobile_net_v2fpn_keras_feature_extractor/FeatureMaps/top_down/nearest_neighbor_upsampling/nearest_neighbor_upsampling/w_stack\" has bad input dims size: 5.\r\nINFO: Created 0 GPU delegate kernels.\r\nERROR: TfLiteGpuDelegate Prepare: delegate is not initialized\r\nERROR: Node number 329 (TfLiteGpuDelegateV2) failed to prepare.\r\n```\r\n\r\n**Describe the expected behavior**\r\nTFLite NNAPI could delegate ssd_mobilenet_v2_fpnlite_320 model.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Hey @michaelnguyen11 , can you try this process with the non-FPN SSD version? \r\n\r\nAlso, you seem to be using the GPU delegate (as shown by your logs), and not the NNAPI delegate - and for float16 models, GPU is indeed the correct delegate - just wanted to confirm.", "Hi @srjoglekar246 ,\r\n\r\nI'm using the SSD Mobilenet V2 and it's working well (used TF1 to convert to TFLite). And now I want to use FPN SSD versison.\r\n\r\nYes I'm using GPU delegate. Both float16 and float32 have the same issue :\r\n```\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nERROR: Following operations are not supported by GPU delegate:\r\nCUSTOM TFLite_Detection_PostProcess: Operation is not supported.\r\n154 operations will run on the GPU, and the remaining 1 operations will run on the CPU.\r\nERROR: TfLiteGpuDelegate Init: PACK: Tensor \"ssd_mobile_net_v2fpn_keras_feature_extractor/FeatureMaps/top_down/nearest_neighbor_upsampling/nearest_neighbor_upsampling/w_stack\" has bad input dims size: 5.\r\nINFO: Created 0 GPU delegate kernels.\r\nERROR: TfLiteGpuDelegate Prepare: delegate is not initialized\r\nERROR: Node number 155 (TfLiteGpuDelegateV2) failed to prepare.\r\n\r\nERROR: Restored original execution plan after delegate application failure.\r\n```\r\n", "Is CUSTOM TFLite_Detection_PostProcess supported by GPU delegate?"]}]