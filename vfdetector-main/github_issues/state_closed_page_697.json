[{"number": 32677, "title": "gcc: error: unrecognized command line option '-std=c++14'", "body": "I'm trying to build on CentOS, but it fails:\r\n```\r\nINFO: Analyzed 4 targets (305 packages loaded, 27773 targets configured).\r\nINFO: Found 4 targets...\r\nINFO: Deleting stale sandbox base /home/xx/.cache/bazel/_bazel_xx/aee6fd7a0bb70a6e710bf5cf6db40de3/sandbox\r\nERROR: /home/xx/tensorflow-build/tensorflow/core/BUILD:386:1: C++ compilation of rule '//tensorflow/core:util_port' failed (Exit 1)\r\ngcc: error: unrecognized command line option '-std=c++14'\r\nINFO: Elapsed time: 13.712s, Critical Path: 0.21s\r\nINFO: 17 processes: 17 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nWhy is the C++ option ```-std=c++14``` passed to gcc?\r\n\r\nrev.e627965\r\ngcc-9.2.0\r\nCentOS 7.6.1810", "comments": ["@yurivict \r\nPlease, let us know TensorFlow version you are using and also request you to provide\r\nthe exact sequence of commands / steps that you executed before running into the problem.Thanks!", "I'm having the same issue. Any thoughts? \r\n\r\nTensorflow 1.15.0\r\nPython 3.6.6\r\nBazel 0.24.1\r\nCuda 10.0\r\nCudnn 7.4\r\ngcc version 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC)\r\nCentOS 7.6.1810\r\nPython virtual env\r\npip install six numpy wheel setuptools mock 'future>=0.17.1' keras_applications==1.0.6 --no-deps  keras_preprocessing==1.0.5 --no-deps\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ngit checkout r1.15\r\n./configure \r\ncat .tf_configure.bazelrc\r\npost config .tf_configure.bazelrc shown below:\r\nuser> cat .tf_configure.bazelrc                \r\nbuild --host_force_python=PY2\r\nbuild --action_env PYTHON_BIN_PATH=\"/home/VirtualEnvironments/3.6.6.build.tf1.15/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/home/VirtualEnvironments/3.6.6.build.tf1.15/lib/python2.7/site-packages\"\r\nbuild --python_path=\"/home/VirtualEnvironments/3.6.6.build.tf1.15/bin/python\"\r\nbuild:xla --define with_xla_support=true\r\nbuild --action_env TF_CUDA_VERSION=\"10\"\r\nbuild --action_env TF_CUDNN_VERSION=\"7\"\r\nbuild --action_env TF_NCCL_VERSION=\"\"\r\nbuild --action_env TF_CUDA_PATHS=\"/utils/Cuda/10.0\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/utils/Cuda/10.0\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1\"\r\nbuild --action_env LD_LIBRARY_PATH=\"/utils/SVN/1.9.7/lib:/utils/JDK-Server/64/jdk1.8.0_144/lib:/utils/PYTHON/3.6.6/lib:/utils/PHP/5.6.35/lib:/utils/OpenCV/4.0.0/lib64:/utils/JDK/64/jdk1.8.0_60/lib:/utils/CMAKE/3.6.3/lib:/utils/Cuda/10.0/lib64:/utils/Cuda/10.0/extras/CUPTI/lib64:/usr/lib:/utils/lib\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc\"\r\nbuild --config=cuda\r\nbuild:opt --copt=-march=native\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\nbuild:v2 --define=tf_api_version=2\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest --test_tag_filters=-benchmark-test,-no_oss,-oss_serial\r\ntest --build_tag_filters=-benchmark-test,-no_oss\r\ntest --test_tag_filters=-gpu\r\ntest --build_tag_filters=-gpu\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n\r\nuser> bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\nThanks,\r\nJamie ", "This seems to fix my immediate error with respect to C++14:\r\n https://www.softwarecollections.org/en/scls/rhscl/devtoolset-7/\r\nJamie", "@yurivict \r\n\r\nPlease, let us know was this issue resolved by following @jamieaware suggestion?", "This issue was resolved by installing https://www.softwarecollections.org/en/scls/rhscl/devtoolset-7/\r\n\r\n\r\nFrom: ravikyram <notifications@github.com>\r\nSent: Thursday, September 26, 2019 7:57 AM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: Jamie Rogers <jrogers@aware.com>; Mention <mention@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] gcc: error: unrecognized command line option '-std=c++14' (#32677)\r\n\r\n\r\n@yurivict<https://github.com/yurivict>\r\n\r\nPlease, let us know was this issue resolved by following @jamieaware<https://github.com/jamieaware> suggestion?\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/32677?email_source=notifications&email_token=AJIPCWZJQCWVEXUJC6DRQC3QLSPPZA5CNFSM4IYRN3F2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD7VJMUA#issuecomment-535467600>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AJIPCW76I7ERONO43BIBF23QLSPPZANCNFSM4IYRN3FQ>.\r\n", "@yurivict \r\nCan we close the issue since the query is been resolved. Let us know. Thanks!", "Closed.\r\n\r\nFrom: ravikyram <notifications@github.com>\r\nSent: Thursday, October 3, 2019 9:50 AM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: Jamie Rogers <jrogers@aware.com>; Mention <mention@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] gcc: error: unrecognized command line option '-std=c++14' (#32677)\r\n\r\n\r\n@yurivict<https://github.com/yurivict>\r\nCan we close the issue since the query is been resolved. Let us know. Thanks!\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/32677?email_source=notifications&email_token=AJIPCWYOSRIBTTHSOJ64UDTQMX2BRA5CNFSM4IYRN3F2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEAIHYCI#issuecomment-537951241>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AJIPCW2Y5KID4DZAA7BG2ILQMX2BRANCNFSM4IYRN3FQ>.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32677\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32677\">No</a>\n", "https://www.tensorflow.org/install/source states\r\n\r\n\"The official TensorFlow packages are built with GCC 4 and use the older ABI.\"\r\n\r\nHow do you build tensorflow 2.0 with GCC 4 if it requires c++14 ?", "> https://www.tensorflow.org/install/source states\r\n> \r\n> \"The official TensorFlow packages are built with GCC 4 and use the older ABI.\"\r\n> \r\n> How do you build tensorflow 2.0 with GCC 4 if it requires c++14 ?\r\n\r\nI can build Tensorflow 2.0 on CentOS 7.6 with gcc 4.8.5 without any issue, But yes I get the same error about c++14 when I was building Tensorflow 1.15.0. I do not have sudo access to install the suggestion provided :(", "i followed the instructions in this url(https://www.softwarecollections.org/en/scls/rhscl/devtoolset-7/) to install the developer tools.\nThen enabled created a python 3.6.4 virtual environment, installed the pre-reqs, installed the latest bazel, enabled the development environment so I was now using the gcc in /opt/rh/devtoolset-7/root/usr/bin/gcc. followed the instructions for configure and bazel build.\n\nThat worked.\njamie\n\n\n\nFrom: Henry Leung [notifications@github.com]\nSent: Wednesday, October 16, 2019 5:02 PM\nTo: tensorflow/tensorflow\nCc: Jamie Rogers; Mention\nSubject: Re: [tensorflow/tensorflow] gcc: error: unrecognized command line option '-std=c++14' (#32677)\n\n\nhttps://www.tensorflow.org/install/source states\n\n\"The official TensorFlow packages are built with GCC 4 and use the older ABI.\"\n\nHow do you build tensorflow 2.0 with GCC 4 if it requires c++14 ?\n\nI can build Tensorflow 2.0 on CentOS 7.6 with gcc 4.8.5 without any issue, But yes I get the same error about c++14 when I was building Tensorflow 1.15.0\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/32677?email_source=notifications&email_token=AJIPCW7BXADVD6U7KQVIT6DQO56NRA5CNFSM4IYRN3F2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEBN46KA#issuecomment-542887720>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AJIPCWYBG6PI36NHKIC2DWDQO56NRANCNFSM4IYRN3FQ>.\n", "> i followed the instructions in this url(https://www.softwarecollections.org/en/scls/rhscl/devtoolset-7/) to install the developer tools. Then enabled created a python 3.6.4 virtual environment, installed the pre-reqs, installed the latest bazel, enabled the development environment so I was now using the gcc in /opt/rh/devtoolset-7/root/usr/bin/gcc. followed the instructions for configure and bazel build. That worked. jamie From: Henry Leung [notifications@github.com] Sent: Wednesday, October 16, 2019 5:02 PM To: tensorflow/tensorflow Cc: Jamie Rogers; Mention Subject: Re: [tensorflow/tensorflow] gcc: error: unrecognized command line option '-std=c++14' (#32677) https://www.tensorflow.org/install/source states \"The official TensorFlow packages are built with GCC 4 and use the older ABI.\" How do you build tensorflow 2.0 with GCC 4 if it requires c++14 ? I can build Tensorflow 2.0 on CentOS 7.6 with gcc 4.8.5 without any issue, But yes I get the same error about c++14 when I was building Tensorflow 1.15.0 \u2014 You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<#32677?email_source=notifications&email_token=AJIPCW7BXADVD6U7KQVIT6DQO56NRA5CNFSM4IYRN3F2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEBN46KA#issuecomment-542887720>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AJIPCWYBG6PI36NHKIC2DWDQO56NRANCNFSM4IYRN3FQ>.\r\n\r\nUnfortunately I need to compile it on a shared computing server which I do not have root access :(, so I cannot really install that to fix the problem. The ideal solution for me is to modify the code so I can compile it. Since Tensorflow 2.0 can be compiled and work without c++14 for me so I guess Tensorflow 1.15 can be too.", "> I can build Tensorflow 2.0 on CentOS 7.6 with gcc 4.8.5 without any issue\r\n\r\nFYI, The latest version of gcc is 9.2.", "About my OP: I abandoned the attempts to build on CentOS. CentOS is a system with very outdated and lacking packaging system. No idea how/why can it be used for development. I develop on FreeBSD.\r\n", "@henrysky  Hi, Have you solved the problem, build Tensorflow 2.0 on CentOS 7.6 with gcc 4.8.5?", "Compiled latest master 2.4 successfully on CentOS 7.8 after having this error (and another one about gcc and libstdc++.so.6).\r\n\r\nAll steps I've done (relevant one specific to this ticket is highlighted):\r\n\r\n1. Upgrade to latest GCC by compiling from source. [This guide](https://linuxhostsupport.com/blog/how-to-install-gcc-on-centos-7/) is nice, but remember to reload PATH after install before checking the new version\r\n2. Check `sudo find / -name \"libstdc++.so.6*\"` and point `/lib64/libstdc++.so.6` to the latest version, in my case ([ref](https://stackoverflow.com/questions/49797965/updating-libstdc-so-6-to-glibcxx-3-4-21-and-cxxabi-1-3-9-after-updating-to-gcc))\r\n`cp /usr/local/lib64/libstdc++.so.6.0.24 /usr/lib64/`\r\n`mv /usr/lib64/libstdc++.so.6 /usr/lib64/libstdc++.so.6.OLD`\r\n`ln -s /usr/lib64/libstdc++.so.6.0.24 /usr/lib64/libstdc++.so.6`\r\n3. **Edit `.bazelrc`, line 300,301 ([ref](https://stackoverflow.com/questions/36245428/c-error-unrecognized-command-line-option-std-c14)) :**\r\n`build:linux --cxxopt=-std=c++1y`\r\n`build:linux --host_cxxopt=-std=c++1y`\r\n", "I found a predefined rpm in https://github.com/harshad16/tensorflow-model-serving/tree/master/tensorflow-model-server-rpm. Its version is 1.14."]}, {"number": 32676, "title": "Refactor {Filter, FixedLengthRecord, FlatMap}DatasetOpTest", "body": "This PR refactors `FilterDatasetOpTest`, `FixedLengthRecordDatasetOpTest`, and `FlatMapDatasetOpTest`.", "comments": ["@feihugis Can you please resolve conflicts? Thanks!", "> @feihugis Can you please resolve conflicts? Thanks!\r\n\r\n@gbaned Thanks for the reminder! The conflicts with (https://github.com/tensorflow/tensorflow/commit/ee8e382cfbb0ec568e1799a14e0628d1fc97ca23) are resolved now.\r\n\r\n@jsimsa Could you please take another look at the changes (https://github.com/tensorflow/tensorflow/pull/32676/commits/55f9e9a5cec6ac31020b894be1886dc6baf75d1d)? The failures in `Ubuntu Sanity` and `Ubuntu CPU` seem to be unrelated.", "@aaudiber `Predict` is updated to `Predicate` now. Please take a look at the change (https://github.com/tensorflow/tensorflow/pull/32676/commits/f2c91e3223dccfed4e09f1cb0acd7ac9d901f834)!", "@gbaned An error happened while migrating the change to the internal checks. Could you please re-trigger the internal tests?", "@feihugis need to resolve conflicts after merging https://github.com/tensorflow/tensorflow/pull/32750", "> @feihugis need to resolve conflicts after merging #32750\r\n\r\n@aaudiber Thanks for the reminder! The conflicts are resolved here (https://github.com/tensorflow/tensorflow/pull/32676/commits/3d69ca21f12d3b96ff2156329162c23972cb367f). Could you please take a look?", "@aaudiber Due to the change here (https://github.com/tensorflow/tensorflow/commit/d9f05892a72db8a9dca1682642cbad33486b9f81), one more commit (https://github.com/tensorflow/tensorflow/pull/32676/commits/086e8678bf79406419b450c7c0b550cc8068660c) is added to rename `GetInputPlaceholder(...)` to `GetInputNames()`. Could you please take another look? "]}, {"number": 32675, "title": "Equivalent of Torch EmbeddingBag", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.14\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently Tensorflow has only tf.nn.embedding_lookup (which, I believe in the backend uses a tf.gather operation). Torch has a torch.nn.EmbeddingBag class which gathers and combines embeddings in specified ways. It allows for a weighted summation of embeddings which they say is much more efficient than getting the embeddings, multiplying (broadcasting) a set of weights across them, and then reduce-summing them. \r\n\r\nI am trying to implement product-key memory layers (https://arxiv.org/pdf/1907.05242.pdf) in tensorflow but they are running very slowly for me - roughly 50% slower than an equivalent BERT-Base sized model without product-key memory layers. Looking at the debugging results, I find that the operations of tf.nn.embedding_lookup, the broadcasted multiplication between scores and values, and the tf.math.reduce_sum operation are taking up a lot of time. A more efficient implementation that performs these operations in one go should reduce the time it takes to run these layers. All product-key memory layers which have a weighted summation of some memory values should benefit from this optimization. \r\n\r\nFor a concrete example pseudocode in tensorflow I have to do these operations:\r\n\r\nquery = tf.Tensor()\r\nkey = tf.Tensor()\r\nvalues = tf.get_variable()\r\nscores, indices = get_scores_and_indices(query,key)\r\nattended_values = tf.nn.embedding_lookup(values,indices)\r\noutput = tf.math.reduce_sum(scores*attended_values)  \r\n\r\nOn torch, you could do\r\nquery = torch.Tensor()\r\nkey = torch.Tensor()\r\nvalues = torch.nn.EmbeddingBag()\r\nscores, indices = get_scores_and_indices(query,key)\r\noutput = values(indices, per_sample_weights=scores)\r\n\r\nTorch claims that using an EmbeddingBag is much more efficient than using a simple Embedding+weighted sum operation. And this does indeed appear to be the case. \r\n \r\n**Will this change the current api? How?**\r\nIt will add new functionality in a tf.nn.embedding_bag_lookup() method\r\n\r\n**Who will benefit with this feature?**\r\nAnybody who wants to implement query-key memory layers based on values saved in embeddings. \r\n\r\n**Any Other info.**\r\nThe closest thing I could find to this was tf.nn.embedding_lookup_sparse, but that method requires the indices and scores to be sparse tensors and then just uses a tf.nn.embedding_lookup with some broadcast multiplication and reduce_sum in the backend anyhow. ", "comments": ["Same questions, currently my solution same with you ~\r\n`\r\n  scores, indices = get_scores_and_indices(query,key)  \r\n`\r\n`\r\n  output = tf.nn.embedding_lookup(values, indices) # [bs, heads*knn, v_dim]\r\n`\r\n`\r\n  scores = tf.expand_dims(scores, axis=-1) #[bs, heads*knn, 1]\r\n`\r\n`\r\n  output = tf.reduce_sum(output * scors)   #[bs, v_dim]\r\n`\r\nany difference ? @Enumaris \r\n", "@kobenaxie we are doing the same thing. I didn't explicitly write out the `scores = tf.expand_dims(scores, axis=-1)`  line of my code and I use a `reshape` instead of an `expand_dims` but otherwise we are doing identical operations. ", "@Enumaris actually identical, sorry for not specifying the axis in reduce_sum\r\n`output = tf.reduce_sum(output * scors, axis=1) #[bs, v_dim]`", "@kobenaxie Right, we are performing identical operations. After some debugging, it appears that the `tf.math.reduce_sum(output*scores, axis=1)` line takes up all the time actually. These are quite large tensors that need to be broadcasted and multiplied together then summed. But it is beyond my scope of knowledge to figure out how to optimize these operations. ", "Could any of you reproduce the product-key memory layer in tf? I get here looking for the same, coming from the same place. Did you found a solution/implementation to this that works reasonably fast?", "Hey everyone, I have a public reimplementation of the layer in TF at https://github.com/Rocketknight1/tf-pkm-layer . I've verified that output is identical to the PyTorch layer, and speed seems okay. My replacement for `EmbeddingBag` was\r\n\r\n```\r\nvalues = tf.gather(self.values, indices)\r\noutput = tf.einsum('ijk, ij -> ik', values, scores)  # (bs,v_dim)\r\n``` \r\n\r\nThe second line is equivalent to `tf.reduce_sum(values * tf.expand_dims(scores, -1))`. Performance seems reasonably good for me on GPU, but I haven't done extensive testing and integration into a bigger model yet.", "Update: I tried benchmarking my implementation and when compiled with XLA, Tensorflow's performance for the PKM layers is only about 20% slower than PyTorch's, even though it doesn't have a native implementation of EmbeddingBag. Given that PKM layers are going to be a small fraction of the total runtime compared to other layers like self-attention, I'd guess full model performance should be almost identical between the two frameworks.", "Did you try to reproduce the paper results? Did you gain some performance adding the pkm layer? Thanks in advance for the work!", "@gaceladri I haven't tried retraining a full transformer model with it yet. In testing I realized that memory usage during training (but not inference) was very high, and the reason is that when you need a backward pass Tensorflow retains the very large tensor containing all the slices from the memory, before they are weighted and summed.\r\n\r\nIn PyTorch, this isn't a problem because all of that step happens in the native EmbeddingBag layer. I'm in the middle of writing a custom backward pass for my Tensorflow EmbeddingBag equivalent, so hopefully that fixes everything!\r\n\r\nIf I get it working, I'll push it to the repo and tag you here.", "Update: I haven't been able to perfectly match the memory usage of Torch on this. The closest I got was using a tf.while_loop with a custom backward pass, to ensure that the whole pre-sum gathered tensor wasn't materialized all at the same time. Even with this method, though, we go OOM on batch sizes about half of the ones Torch can manage. I think doing better would require me diving into CUDA and adding a custom op, but I don't understand CUDA or the TF codebase nearly well enough to do that.\r\n\r\nI'll tidy everything up and push the while loop implementation to my repo soon.", "Thanks for the update. I directly changed my project to Pytorch... ", "Hey, I think I have a solution that has similar memory usage to Pytorch, at least up to about 500m network parameters. Beyond that point, the sheer size of the values and values gradient tensors gets really big, and PyTorch does a bit better for some reason.\r\n\r\nThe solution is very similar to my einsum solution above, but with some reshaping and use of map_fn with limited parallel steps and control_dependencies to keep memory usage down during the embeddingbag portion. I'm still working on performance - XLA ignores a lot of dependency control, so it blows out my memory again, but when I can use XLA performance is comparable to PyTorch.", "I'll do some performance testing and then finally push everything soon.", "Update: I was very wrong about that solution being a good one, but I've implemented a custom op in CUDA and am trying to push it into tf/addons right now.", "See https://github.com/tensorflow/addons/issues/2201", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 32674, "title": "Tensorflow Java API and @tf.function signatures", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): 25.1\r\n- GCC/Compiler version (if compiling from source): 7.3\r\n\r\n**Describe the current behavior**\r\n\r\nThe current Tensorflow Java API (1.14 or 2.0) doesn't seem to be able to do inference on Tensorflow nodes created within a `@tf.function` signature. \r\n\r\nOn a related note, https://github.com/tensorflow/community/blob/master/rfcs/20180918-functions-not-sessions-20.md mentions that Java is planning to stay `Session` centric. Ideally, a signature inference centric API would be a good way to solve this issue and would be a good addition to the existing Java API.\r\n\r\nConcretely, here is the Python snippet defining a `request` and `response` (full Python code attached at the bottom of this ticket):\r\n```\r\n  @tf.function(input_signature=[tf.TensorSpec(shape=[None, None], name=\"serving\")])\r\n  def serve(self, request):\r\n    features = tf.identity(self.input_receiver(request), name='request')\r\n    output = self.call(features)\r\n    response = tf.identity(self.response_receiver(output), name='response')\r\n    return response\r\n```\r\nAfter exporting my model to a `SavedModelBundle` and  trying to do some prediction in Java via the `Session.runner()` API, I am getting:\r\n```\r\njava.lang.IllegalArgumentException: No Operation named [request] in the Graph\r\n  at org.tensorflow.Session$Runner.operationByName(Session.java:380)\r\n  at org.tensorflow.Session$Runner.parseOutput(Session.java:394)\r\n  at org.tensorflow.Session$Runner.feed(Session.java:131)\r\n  ... 34 elided\r\n```\r\nwhere `request` is defined within a `@tf.function`.\r\n\r\n**Describe the expected behavior**\r\n\r\nI would expect to not have a runtime failure at inference time on the JVM: either `operationByName` would recognize the `request:0` node or another Java API would exist to fulfill my requirements.\r\n\r\n**Code to reproduce the issue**\r\n\r\nPython code (model creation):\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\n\r\n\r\nclass MnistModel(tf.keras.Model):\r\n  def __init__(self):\r\n    super().__init__()\r\n    self.first_dense = layers.Dense(64, input_shape=(784,), activation='relu', name='dense_1')\r\n    self.out = layers.Dense(10, activation='softmax', name='predictions')\r\n\r\n  def call(self, inp):\r\n    f_dense = self.first_dense(inp)\r\n    s_dense = self.out(f_dense)\r\n    return s_dense\r\n\r\n  def input_receiver(self, inp):\r\n    return inp\r\n\r\n  def response_receiver(self, output):\r\n    return output\r\n\r\n  @tf.function(input_signature=[tf.TensorSpec(shape=[None, None], name=\"serving\")])\r\n  def serve(self, request):\r\n    features = tf.identity(self.input_receiver(request), name='request')\r\n    output = self.call(features)\r\n    response = tf.identity(self.response_receiver(output), name='response')\r\n    return response\r\n\r\nmodel = MnistModel()\r\n\r\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\nx_train = x_train.reshape(60000, 784).astype('float32') / 255\r\nx_test = x_test.reshape(10000, 784).astype('float32') / 255\r\n\r\nmodel.compile(loss='sparse_categorical_crossentropy',\r\n              optimizer=keras.optimizers.RMSprop())\r\nhistory = model.fit(x_train, y_train,\r\n                    batch_size=64,\r\n                    epochs=1)\r\n\r\nkeras.experimental.export_saved_model(model, 'local_path', serving_only=True)\r\n```\r\n\r\nJVM / Scala code (inference code):\r\n```\r\nimport org.tensorflow.{SavedModelBundle, Session, Tensor}\r\nimport java.nio.ByteBuffer\r\nimport java.lang.{Float => JFloat}\r\n\r\nval savedModelBundle = SavedModelBundle.load(\"local_path\", \"serve\")\r\n\r\nval session = savedModelBundle.session()\r\n\r\nval byteBuffer = ByteBuffer.allocate(784*4)\r\n\r\nval tensor = Tensor.create(\r\n    classOf[JFloat],\r\n    Array(784),\r\n    byteBuffer)\r\n\r\nsession.runner()\r\n  .feed(\"request:0\", tensor)\r\n  .fetch(\"output:0\")\r\n  .run()\r\n\r\ntensor.close()\r\nbyteBuffer.close()\r\n```\r\n\r\nI am getting the following error:\r\n```\r\njava.lang.IllegalArgumentException: No Operation named [request] in the Graph\r\n  at org.tensorflow.Session$Runner.operationByName(Session.java:380)\r\n  at org.tensorflow.Session$Runner.parseOutput(Session.java:394)\r\n  at org.tensorflow.Session$Runner.feed(Session.java:131)\r\n  ... 34 elided\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@bqm Can you try TF1.15.0rc1 and let us know whether the issue persists or not? Thanks!", "Hello @jvishnuvardhan, thank you for your answer.\r\n\r\nUsing the 1.15.0rc1 JAR provided in https://storage.googleapis.com/tensorflow/, I was able to confirm that the issue also exists on TF1.15.0rc1.", "You can find an example of the created SavedModel at https://gist.github.com/bqm/88c62df65d76832108de2ec0f9b853cc.\r\n\r\nThe `request:0` and `response:0` nodes are stored in the `Library`\r\nprotobuf object (which starts line 1306). My guess is that the current Tensorflow API ignores that Library object. Thoughts?\r\n\r\nDefinition of the Protobuf Library object below:\r\nhttps://github.com/tensorflow/tensorflow/blob/9590c4c32dd4346ea5c35673336f5912c6072bf2/tensorflow/core/framework/graph.proto#L55\r\n", "@saxenasaurabh - https://groups.google.com/a/tensorflow.org/forum/#!topic/jvm/685y-ou__PU mentions that you might be working on a C API relating to this issue. Can you let us know if that's the case? And if so, what's the best way to track the progress?", "Feeding values and fetching values of tensors defined inside tf.function is not allowed.\r\ncc: @k-w-w @allenlavoie ", "@saxenasaurabh Thank you for your answer! I have a bunch of follow-up questions, some of which I am planning to investigate more next week but would love your input if you know the answers off-hand (no worries if you have answers only to a few, that would still be really helpful!):\r\n* 'Not allowed' - is this a design decision / would it make sense to change it? (would love if you know of a link / documentation explaining this)\r\n* If you are not planning to change it, what would be an alternative? Assuming the `serving:0` is accessible to the JVM API (input node of tf.function), what would be the right output node to use?\r\n* This doesn't seem to be specific to the JVM API, is that correct? For instance, do you know how TF Serving would behave? (I will dive more into this one but if you know off-hand, response appreciated :))\r\n", "After more investigations, it seems that using `tf.saved_model.save` for exporting with `signatures` provided seem to add the missing input and output tensors **for the outer tensors**.\r\n\r\nSo the export call becomes:\r\n```\r\ntf.saved_model.save(model, 'local_path', signatures={\"serve\": model.serve})\r\n```\r\nThis will have the effect of adding new TF tensors which can be feed and retrieved via the Session JVM API:\r\n```\r\n!saved_model_cli show --dir local_path --all\r\n\r\nsignature_def['__saved_model_init_op']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['__saved_model_init_op'] tensor_info:\r\n        dtype: DT_INVALID\r\n        shape: unknown_rank\r\n        name: NoOp\r\n  Method name is:\r\n\r\nsignature_def['serve']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n    inputs['serve'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: unknown_rank\r\n        name: serve_serve:0\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['output_0'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: unknown_rank\r\n        name: PartitionedCall:0\r\n  Method name is: tensorflow/serving/predict\r\n```\r\nThe right JVM tensor to feed is `serve_serve:0` for the input and `PartitionedCall:0` for the output in that example.\r\n\r\nThis setup doesn't give me access to the `request:0` and `response:0` tensors but I think that's fine - one can create new `@tf.function` and add the relevant signatures for that.\r\n\r\nThe user experience is not great because the user doesn't have control on the tensor names (`serve_serve:0` and `PartitionedCall:0` in my previous example) but it does the job.\r\n\r\nMy proposal would be to close this issue based on the findings above. I can reopen a new github issue in https://github.com/tensorflow/java directly to improve the user experience (for instance via providing a signature API on the JVM).\r\n\r\n@sjamesr any thoughts?\r\n\r\n", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32674\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32674\">No</a>\n"]}, {"number": 32673, "title": "[XLA:GPU][ROCm] Enabling amdgpu backend in XLA unit tests", "body": "This PR enables amdgpu XLA backend for unit tests, in preparation for setting up ROCm XLA community support builds.", "comments": ["@jerryyin Could you please check failed build errors? Thanks!", "@gbaned Judging from the build failures: `external/local_config_mlir/include/mlir/Dialect/QuantOps/UniformSupport.h:148:14: error: 'clamp' is not a member of 'std'`\r\n\r\nI don't think the failures have anything to do with this PR. `std::clamp` is a C++ 17 addition and shouldn't be used in that header file.\r\n\r\nBTW in my local cached result the previous run looks as follows:\r\n![image](https://user-images.githubusercontent.com/3454501/65440477-66e8d280-ddee-11e9-80c0-0cb522e73dd5.png)\r\n\r\nWhat has triggered a re-run after the previous run?\r\n\r\n\r\n------\r\n\r\n@gbaned Mind adding the `ready to pull` tag back?"]}, {"number": 32672, "title": "A keras model containing a tf.tile op layer with a tensor in the `multiples` arg fails when saving to hdf5", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.0.0-rc0-101-gd2d2566eef 2.0.0-rc1\r\n- Python version: Python 3.6.8 :: Anaconda, Inc.\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nA keras model containing a tf.tile op layer with a tensor in the `multiples` arg throws an exception when saving to hdf5.\r\n\r\n(I'm using tf.tile because RepeatVector(n) doesn't accept a tensor for n. The goal is to stack a 2d feature batch so it can be concatenated to a variable length 3d batch of sequence features.)\r\n\r\n**Describe the expected behavior**\r\n\r\nModel.save() should save the model.\r\n\r\n**Code to reproduce the issue**\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Input\r\nfrom tensorflow.keras import Model\r\n\r\na = Input(shape=(10,))\r\nout = tf.tile(a, (1, tf.shape(a)[0]))\r\nmodel = Model(a, out)\r\n\r\nx = np.zeros((50,10), dtype=np.float32)\r\nprint(model(x).numpy())\r\n\r\nmodel.save('my_model.h5')\r\n\r\n**Other info / logs**\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-2-9b1429243599> in <module>\r\n     11 print(model(x).numpy())\r\n     12 \r\n---> 13 model.save(model_dir + '/my_model.h5')\r\n\r\n~/miniconda3/envs/tf20/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py in save(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\r\n   1187     \"\"\"\r\n   1188     saving.save_model(self, filepath, overwrite, include_optimizer, save_format,\r\n-> 1189                       signatures, options)\r\n   1190 \r\n   1191   def save_weights(self, filepath, overwrite=True, save_format=None):\r\n\r\n~/miniconda3/envs/tf20/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/save.py in save_model(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\r\n    110           'or using `save_weights`.')\r\n    111     hdf5_format.save_model_to_hdf5(\r\n--> 112         model, filepath, overwrite, include_optimizer)\r\n    113   else:\r\n    114     saved_model_save.save(model, filepath, overwrite, include_optimizer,\r\n\r\n~/miniconda3/envs/tf20/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py in save_model_to_hdf5(model, filepath, overwrite, include_optimizer)\r\n    107     model_weights_group = f.create_group('model_weights')\r\n    108     model_layers = model.layers\r\n--> 109     save_weights_to_hdf5_group(model_weights_group, model_layers)\r\n    110 \r\n    111     # TODO(b/128683857): Add integration tests between tf.keras and external\r\n\r\n~/miniconda3/envs/tf20/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py in save_weights_to_hdf5_group(f, layers)\r\n    623 \r\n    624   for layer in layers:\r\n--> 625     g = f.create_group(layer.name)\r\n    626     weights = _legacy_weights(layer)\r\n    627     weight_values = K.batch_get_value(weights)\r\n\r\n~/miniconda3/envs/tf20/lib/python3.6/site-packages/h5py/_hl/group.py in create_group(self, name, track_order)\r\n     66             name, lcpl = self._e(name, lcpl=True)\r\n     67             gcpl = Group._gcpl_crt_order if track_order else None\r\n---> 68             gid = h5g.create(self.id, name, lcpl=lcpl, gcpl=gcpl)\r\n     69             return Group(gid)\r\n     70 \r\n\r\nh5py/_objects.pyx in h5py._objects.with_phil.wrapper()\r\n\r\nh5py/_objects.pyx in h5py._objects.with_phil.wrapper()\r\n\r\nh5py/h5g.pyx in h5py.h5g.create()\r\n\r\nValueError: Unable to create group (name already exists)\r\n", "comments": ["I was able to replicate the issue with given code for TF-2.0rc1, please find the [gist](https://colab.sandbox.google.com/gist/oanush/8ee3a61919408dadf12499d2d95792ca/32672.ipynb) of colab.Thanks!", "@currivan I Don't see any issue if you remove the \".h5\" from the `model.save`. I changed only last line from your code as follows.\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Input\r\nfrom tensorflow.keras import Model\r\n\r\na = Input(shape=(10,))\r\nout = tf.tile(a, (1, tf.shape(a)[0]))\r\nmodel = Model(a, out)\r\n\r\nx = np.zeros((50,10), dtype=np.float32)\r\nprint(model(x).numpy())\r\n\r\nmodel.save('./my_model')\r\n```\r\nPlease check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/9348e139288b969517ed527334e5fb57/32672.ipynb). Thanks!\r\n\r\nI am closing this issue as it was resolved. Please feel free to open it if the issue persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32672\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32672\">No</a>\n", "This should not have been closed. Throwing an exception here is a bug, and the solution is unacceptable. Saving this type of model with h5 takes a second, and the proposed way it takes over a minute. Loading is also unacceptably slow. ", "@currivan In case If i need to use an op like `tf.tile` as in your case, I will call it with a lambda layer. So the code is as follows\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Input, Lambda\r\nfrom tensorflow.keras import Model\r\n\r\ndef my_fun(a):\r\n  out = tf.tile(a, (1, tf.shape(a)[0]))\r\n  return out\r\n\r\na = Input(shape=(10,))\r\n#out = tf.tile(a, (1, tf.shape(a)[0]))\r\nout = Lambda(lambda x : my_fun(x))(a)\r\nmodel = Model(a, out)\r\n\r\nx = np.zeros((50,10), dtype=np.float32)\r\nprint(model(x).numpy())\r\n\r\nmodel.save('my_model.h5')\r\n\r\n#load the model\r\nnew_model=tf.keras.models.load_model(\"my_model.h5\")\r\n\r\n```\r\nPlease let me know what you think? Please check the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/56a985172d3b0702b1eb0b0b2d263761/tf32672.ipynb). Thanks!\r\n", "@jvishnuvardhan thanks, I was also able to create a custom layer. In general I feel if a model can be executed, it should be savable by all standard methods without throwing an opaque exception. Someone should still fix the bug with saving op layers. \"ValueError: Unable to create group (name already exists)\" isn't the right way to handle this even if it's unsupported.", "@currivan I agree. May be we need to update the error description. Please note that fixing this is not simple.\r\n\r\n@k-w-w Could you please take a look at this issue?", "I believe that this is the same issue as #12195.  When adding a `tf.tile` operation the keras model gets a `tf_op_layer_Title/multiples` layer added _before_ a `tf_op_layer_Tile` layer which I believe is what causes the problem. ", "@ELind77 : #12195 seems totally unrelated to this. Did you mean to link to a different issue?\r\n\r\n@currivan : One problem with using Lambda is that your layer won't have a proper shape defined in case following layers need that.", "@currivan I think this was resolved recently in `tf-nightly`. I ran it with `tf-nightly` without any issue. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/15f06ff402a678a77d0c47592a290701/32672.ipynb). Thanks!\r\n\r\nI am closing this issue as it was resolved. Please feel free to reopen. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32672\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32672\">No</a>\n", "I have a similar problem. I'm not using tf.tile, I have a custom layer which uses tf.concat, tf.map_fn, and tf.cast. The latest nightly (tf-nightly-2.2.0.dev20200226) seems to crash on my machine so I'm not able to test it fully.\r\n\r\nThe error message is the same (I found this issue by googling it).", "@jsilter Can you please create  new issue with a standalone code to reproduce the issue? Thanks!", "This issue is happening for me with tf.clip_by_value(). I haven't tested the nightly build."]}, {"number": 32671, "title": "[XLA:GPU][ROCm] Rename cudnn conv padding legalization to gpu conv padding legalization", "body": "This is the one PR to rename the series of components in XLA, per @cheshire request. The aim of this series is such that all components shared by cuda and rocm have the prefix gpu instead of cudnn.\r\n\r\nTo make the PR easy to review, I make following decisions:\r\n\r\n- Each PR only rename one component\r\n- Renaming is done by: $ find . -type f -exec sed -i \"s/foo/bar/g\" {} \\\r\n- Each commit handles the renaming of one symbol. I highly recommend reviewing by commit.\r\n   - 1st commit: File renaming\r\n   - 2nd commit: Update include guard and `CudnnConvPaddingLegalization` symbol\r\n   - All commits verified by `convolution_test_gpu`\r\n\r\nThis PR is relatively straightforward as its public functions does not include `cudnn` in it.", "comments": []}, {"number": 32670, "title": "[r1.15-CherryPick] [tf.data] Avoid double conversion to a tensor during input normalizat\u2026", "body": "\u2026ion.\r\n\r\nPiperOrigin-RevId: 270046393", "comments": []}, {"number": 32669, "title": "[r2.0-CherryPick]:[tf.data] Avoid double conversion to a tensor during input normalizat\u2026", "body": "\u2026ion.\r\n\r\nPiperOrigin-RevId: 270046393", "comments": []}, {"number": 32668, "title": "Updated doc for all symbols in tf.keras.activations", "body": "Updated Descriptions, Examples, Arguments, Returns and Raises lists.\r\n\r\n\r\nFixes #25828", "comments": ["> Thanks,\r\n> \r\n> Mostly this looks good.\r\n> \r\n> At a few places you've deleted blank lines, and those can be important. The linters will complain if there isn't a blank line after the first-line summary. It's best to keep a blank line above a codeblock.\r\n> \r\n> The examples look helpful. It is worth noting that we've enabled [doctest](https://docs.python.org/3/library/doctest.html) for TensorFlow, and are in the process of converting the docstrings. So it would be cool if you could make these examples tested by switching them to use the doctest format `>>>` instead of the code fences ```.\r\n\r\nThank you for the suggestions. I have added the blank lines wherever required, and have converted the examples to the doctest format.", "@asmitapoddar Could you please check failed build errors? Thanks!", "@gbaned @MarkDaoust I fixed some issues with the file to resolve the failed build errors. Does it look fine now?", "Though the commit had passed the `import/copybara` check yesterday, I'm not sure why it shows `import/copybara \u2014 An error happened while migrating the change` now.\r\n", "I think everything's okay on your side.\r\n\r\nI'm fixing this internally. This should be merged soon."]}, {"number": 32667, "title": "[XLA:GPU][ROCm] Renaming cudnn conv rewriter to gpu conv rewriter", "body": "This is the one PR to rename the series of components in XLA, per @cheshire request. The aim of this series is such that all components shared by cuda and rocm have the prefix `gpu` instead of `cudnn`.\r\n\r\nTo make the PR easy to review, I make following decisions:\r\n\r\n- Each PR only rename one component\r\n- Renaming is done by: `$ find . -type f -exec sed -i \"s/foo/bar/g\" {} \\`\r\n- Each commit handles the renaming of one symbol. I highly recommend reviewing by commit.\r\n   - 1st commit: File renaming only, no symbol change\r\n   - 2nd commit: Rename only include guard\r\n   - 3rd commit: Rename only `CudnnConvRewriterTest` to `GpuConvRewriterTest`\r\n   - 4th commit: Rename only `CudnnConvRewriter` to `GpuConvRewriter`\r\n   - 5th commit: Misc renames\r\n- Each commit verified by `gpu_conv_rewriter_test`\r\n\r\nNote: This approach does not guarantees completely removing `cudnn` symbol from the source code, does not seek for perfectionism, but will purge the current code base in a gradual manner. The whole purpose of doing it is to make sure reviewer can review it easily, and that each PR can be merged as fast as it can, since renaming touch interface and runs into conflict often. I want to minimize the overhead of spinning after PR is submitted.", "comments": []}, {"number": 32666, "title": "cloudpickle cannot unpickle tf.keras in 1.14: KeyError: 'tensorflow.keras'", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS, Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Anaconda binary\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n$ python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nunknown 1.14.0\r\n\r\n**Describe the current behavior**\r\n\r\ncloudpickle cannot unpickle `tf.keras` because the `deprecation_wrapper` introduced in 1.14.\r\n\r\nIn one python session:\r\n\r\n~~~python \r\nimport cloudpickle\r\nimport tensorflow.keras as K\r\nwith open(\"/tmp/K.pkl\", \"wb\") as f:\r\n  cloudpickle.dump(K, f)\r\n~~~\r\n\r\nThen start another python session:\r\n\r\n~~~python\r\nimport cloudpickle\r\nwith open(\"/tmp/K.pkl\", \"rb\") as f:\r\n  cloudpickle.load(f)\r\n~~~\r\n\r\nError:\r\n\r\n~~~\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 2, in <module>\r\n  File \"/Users/meng/conda/envs/tf-1.14/lib/python3.7/site-packages/tensorflow/python/util/deprecation_wrapper.py\", line 148, in __setstate__\r\n    sys.modules[d]._dw_wrapped_module,\r\nKeyError: 'tensorflow.keras'\r\n~~~\r\n\r\n**Describe the expected behavior**\r\n\r\nThe same code worked in TensorFlow 1.13. In TensorFlow 1.13, `K.__module__` does not exist, while in TensorFlow 1.14, `K.__module__` is \r\n\r\n~~~\r\n'tensorflow.python.util.deprecation_wrapper'\r\n~~~\r\n\r\nIt makes the `tensorflow.keras` module not loaded during unpickling.\r\n\r\nThere are multiple applications use pickle to ship TensorFlow code to a remote machine to unpickle and execute, e.g., Spark, horovod.spark. So the behavior change would fail the jobs.\r\n\r\nBtw, one workaround is to do `from tensorflow import keras as K` instead of `import tensorflow.keras as K`. The diff is that `K.__name__` is `'tensorflow.python.keras.api._v1.keras'` in the first case and `tensorflow.keras` in the second. And pickle uses `__name__` to get global names: https://github.com/python/cpython/blob/3.7/Lib/pickle.py#L952\r\n\r\ncc: @alsrgv @hanyucui @annarev\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nSee above.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "comments": ["Duplicate of #32159. There is a workaround there and justification for why we have this at the moment.", "@mihaimaruseac The issue reported in #32159 does not happen on 1.14. Use TensorFlow with Spark for distributed batch inference is a very common use case. I don't quite understand your comment at https://github.com/tensorflow/tensorflow/issues/32159#issuecomment-528044061. It seems the `DeprecationWrapper` can detect if the module name exists in `sys.modules` and dynamically load it if not at https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/util/deprecation_wrapper.py#L132. I don't know why we have to give up Python 2 support for the fix.", "Try to use `tensorflow.python.keras` should work."]}, {"number": 32665, "title": "[XLA] Thunk to cudnn fp16 kernels for F16 batchnormalization and enable cudnn for batchnormalization by default", "body": "When using cuDNN for batchnormalization, xla currently thunks fp16 batchnorms to fp32 kernels by casting fp16 inputs and gradients to fp32 in the tf2xla bridge. This is non-optimal since cuDNN 7.6 has highly efficient `Half` kernels that can be leveraged. This PR modifies `cudnn_batchnorm_rewriter` and `cudnn_batchnorm_thunk` such that the converts introduced in the bridge get eliminated and the corresponding fp16 cudnn APIs get called. \r\nNote that instead of eliminating convert ops, I add converts which then get eliminated by `algebraic_simplifier`. [PR](https://github.com/tensorflow/tensorflow/pull/32435) adding capability to eliminate of convert pairs was merged earlier.\r\n\r\nWhile investigating performance of mobilenet, it was found that enabling cuDNN for Batchnorm improves performance by around 10-12%. Running the numbers for a set of convolutional networks in addition to mobilenet with cuDNN turned on for Batchnorm, showed that enabling cuDNN, either improves performance or keeps it the same. This PR enables cudnn for batchnorm by default  based on the above-mentioned performance enhancements.", "comments": ["I'll create 2 separate PRs and address the comments.", "Pushed another commit incorporating most of the comments. Figured it would be convenient to address most of the comments here before migrating to the 2-PR submission. Will address the refactoring to use templates in the subsequent PR.   ", "> Pushed another commit incorporating most of the comments. Figured it would be convenient to address most of the comments here before migrating to the 2-PR submission. Will address the refactoring to use templates in the subsequent PR.\r\n\r\nThanks Ayan, LMK when this is ready to review.\r\n\r\nBtw, do you have a microbenchmark showing the performance difference between XLA's batchnorm and cudnn's batchnorm?", "Sorry for the delayed response. Using `tf_cnn_benchmarks` in https://github.com/tensorflow/benchmarks for mobilenet_v2, the most recent numbers that I have are as follows: \r\nWith cudnn BN: ~4500 images/sec\r\nWith Fused kernel BN: ~3200 images/sec\r\nThis is on a GV100. Note that this is based on fairly new repo but not the latest. The latest tensorflow has some refactors that causes the benchmark script to fail (tensorflow contrib has been refactored somehow). The benchmark scripts haven't been updated to reflect those changes. A few weeks back, I had the numbers looking like this:\r\nWith cudnn BN: ~4500 images/sec\r\nWith Fused kernel BN: ~3800 images/sec\r\nThe numbers seems dynamic but it is clear that at this point cudnn batchnorm performs better than the kernel fusion. \r\nI actually investigated the kernel fusion for batchnorm and found that the difference is predominantly due to `GTE` thwarting fusion of multioutput fused kernels. Infact there are other cases where see that FusionFusion merger (i.e, fusion across `GTE` ) is the bottleneck.", "@sanjoy I have pushed the PR to thunk to cudnn (https://github.com/tensorflow/tensorflow/pull/32887). Please take a look. I will create a new PR with the `cudnn_batchnorm_rewriter` changes once this PR is merged. The rewriter changes are dependent on https://github.com/tensorflow/tensorflow/pull/32887 getting merged.", "@AyanmoI Can you please resolve conflicts? Thanks!", "@sanjoy Please take a look at https://github.com/tensorflow/tensorflow/pull/33259 which handles fp16 batchnorms in `cudnn_batchnorm_rewriter`. Please note that https://github.com/tensorflow/tensorflow/pull/33259  addresses all the comments you made in this PR.", "> @AyanmoI Can you please resolve conflicts? Thanks!\r\n\r\n@gbaned This PR has been split into 2 parts actually. Part 1 https://github.com/tensorflow/tensorflow/pull/32887 has already been merged. Part 2 https://github.com/tensorflow/tensorflow/pull/33259 is up for review. I will close this PR eventually. I had just kept this around as a reference."]}, {"number": 32664, "title": "Broken link in https://www.tensorflow.org/guide/eager#top_of_page", "body": "https://www.tensorflow.org/guide/eager#top_of_page\r\nsays\r\n\"For a collection of examples running in eager execution, see: tensorflow/contrib/eager/python/examples.\"\r\n\r\nThe link to `https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples` returns a 404 error.", "comments": ["@rodrigob TensorFlow website is getting updated for `TF2.0`. Please wait for couple of days to update the links. For now you can find that file at this [location](https://github.com/tensorflow/tensorflow/tree/r1.15/tensorflow/contrib/eager/python/examples). Thanks!\r\n\r\n", "why are we deploying websites with broken links ?\r\nin 2019 automated links checking should be part of the deployment/testing process.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32664\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32664\">No</a>\n"]}, {"number": 32663, "title": "[XLA:GPU][ROCm] Renaming cudnn conv runner to gpu conv runner", "body": "This is the first PR to rename the series of components in XLA, per @cheshire request [here](https://github.com/tensorflow/tensorflow/pull/32364#issuecomment-530908570). The aim of this series is such that all components shared by cuda and rocm have the prefix `gpu` instead of `cudnn`. \r\n\r\nTo make the PR easy to review, I make following decisions:\r\n\r\n- Each PR only rename one component\r\n- Renaming is done by: `$ find . -type f -exec sed -i \"s/foo/bar/g\" {} \\`\r\n- Each commit handles the renaming of one symbol. I highly recommend reviewing by commit.\r\n   - 1st commit: File renaming only, no symbol change\r\n   - 2nd commit: Rename only `CudnnConvParams` to `GpuConvParams`\r\n   - 3rd commit: Rename only `RunCudnnConv` to `RunGpuConv`\r\n   - 4th commit: Rename only include guard\r\n- Each commit verified by `convlution_test_gpu`\r\n\r\nNote: This approach does not guarantees completely removing `cudnn` symbol from the source code, does not seek for perfectionism, but will purge the current code base in a gradual manner. The whole purpose of doing it is to make sure reviewer can review it easily, and that each PR can be merged as fast as it can, since renaming touch interface and runs into conflict often. I want to minimize the overhead of spinning after PR is submitted.", "comments": []}, {"number": 32662, "title": "Attention layer not serializable because it takes init args but doesn't implement get_config", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.5 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.0.0-rc0-101-gd2d2566eef 2.0.0-rc1\r\n- Python version: Python 3.6.8 :: Anaconda, Inc.\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nTrying to save a model with a tensorflow.keras.layers.Attention layer throws because it doesn't implement get_config().\r\n\r\nNotImplementedError: Layers with arguments in `__init__` must override `get_config`.\r\n\r\n**Describe the expected behavior**\r\n\r\nModel.save() should save the model.\r\n\r\n**Code to reproduce the issue**\r\n\r\na = Input(shape=(None, 10))\r\nattn = Attention()([a,a])\r\nmodel = Model(a, attn)\r\nmodel(np.zeros((50,10), dtype=np.float32))\r\nmodel.save('my_model.h5')\r\n\r\n**Other info / logs**\r\n\r\n~/miniconda3/envs/tf20/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in get_config(self)\r\n    571     # or that `get_config` has been overridden:\r\n    572     if len(extra_args) > 1 and hasattr(self.get_config, '_is_default'):\r\n--> 573       raise NotImplementedError('Layers with arguments in `__init__` must '\r\n    574                                 'override `get_config`.')\r\n    575     # TODO(reedwm): Handle serializing self._dtype_policy.\r\n\r\nNotImplementedError: Layers with arguments in `__init__` must override `get_config`.", "comments": ["Was able to reproduce the issue. Please find the github gist [here](https://colab.sandbox.google.com/gist/gowthamkpr/d79de99944192c0f8195b975718809b5/untitled147.ipynb). In the mean while, I was able to save the weights and load them. You can find it in the gist above.", "The issue was recently fixed in https://github.com/tensorflow/tensorflow/commit/498e815097e74aff7fefdbbae69ba9daf6e9c023, but we haven't cherrypick it into 2.0 rc yet. I verified the code against nightly and it works.\r\n\r\nCould you try to save it as tf format as a workaround?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32662\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32662\">No</a>\n", "same problem in tensorflow 2.0 right now. Actually I don't know what triggers this issue.  "]}, {"number": 32661, "title": "Documentation about XLA backend is incomplete and inaccurate", "body": "Looking at ```Scenario 3``` in https://github.com/tensorflow/tensorflow/issues/new?labels=type%3Adocs&template=20-documentation-issue.md\r\n\r\nIt has a link to the class ```StreamExecutor``` but the link leads to the empty class with comments suggesting that it has been removed (?)\r\n", "comments": ["@yurivict,\r\nPlease provide the correct link of documentation where the problem persists. Thanks!", "I fixed the link, sorry for  the mishap.", "@yurivict,\r\nGood to know that its fixed. \r\nI am closing the issue now. Please feel free to reopen if the issue still persists. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32661\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32661\">No</a>\n"]}, {"number": 32660, "title": "Performance of tf.io.read_file in graph mode", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.0.0rc1\r\n- Python version: 3.6.0\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Current behavior**\r\nI observed surprising performances of the different ways to read a file, especially when comparing graph mode (by `tf.function`) versus eager mode. I compared two ways to read a file:\r\n`tf.io.gfile.GFile(filepath).read(-1)` versus  `tf.io.read_file(filepath)`.\r\nIf the two have similar performances in eager mode, the first option gains a close to 50% speedup when using it in graph mode (wrapped into a `tf.function` call), while the latter one do not show any speed improvement, rather a slowdown.\r\n\r\n**Expected behavior**\r\nI do not  understand why such a speed up is observed on `tf.io.gfile.GFile` while `tf.io.read_file` does not benefit any. This is crucial because my data pipeline yields filepaths through `tf.data.Dataset.list_files()`, which are further read and decoded, but tf.io.gfile.GFile does not accept string tensors as input. Hence I am forced to use `tf.io.read_file` (see  [issue](https://github.com/tensorflow/tensorflow/issues/32620))\r\n\r\n**Code to reproduce the issue**\r\nSee below a MVCE:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport timeit\r\nimport os\r\n\r\nfilepath = '/tmp/array.npy'\r\nnp.save(filepath, np.ones(shape=(400, ) * 3))\r\n\r\ndef read_Gfile(path):\r\n    f = tf.io.gfile.GFile(name=path, mode='rb')\r\n    s = f.read(-1)\r\n    f.close()\r\n    return s\r\n\r\nread_io = lambda x: tf.io.read_file(x)\r\n\r\ngraph_read_io = tf.function(read_io)\r\ngraph_read_Gfile = tf.function(read_Gfile)\r\n\r\nprint('Reading with numpy: %.3f ms' % (timeit.Timer(lambda: np.load(filepath)).timeit(number=10) * 1000 / 10))\r\nprint('Reading in eager mode with GFile: %.3f ms' % (timeit.Timer(lambda: read_Gfile(filepath)).timeit(number=10) * 1000 / 10))\r\nprint('Reading in eager mode with io.read_file: %.3f ms' % (timeit.Timer(lambda: read_io(filepath)).timeit(number=10) * 1000 / 10))\r\nprint('Reading in graph mode with GFile: %.3f ms' % (timeit.Timer(lambda: graph_read_Gfile(filepath)).timeit(number=10) * 1000 / 10))\r\nprint('Reading in graph mode with io.read_file: %.3f ms' % (timeit.Timer(lambda: graph_read_io(filepath)).timeit(number=10) * 1000/ 10))\r\n\r\nos.remove(filepath)\r\n```\r\nThis code produces the following performances:\r\n```\r\nReading with numpy: 175.189 ms\r\nReading in eager mode with GFile: 826.163 ms\r\nReading in eager mode with io.read_file: 767.376 ms\r\nReading in graph mode with GFile: 509.069 ms\r\nReading in graph mode with io.read_file: 864.643 ms\r\n```\r\n", "comments": ["@remydubois, Will it be possible to share the .npy file that you are using. Thanks!", "Thanks for looking into that,\r\n\r\nthe np array is created and saved at the beginning of the MVCE `np.save(filepath, np.ones(shape=(400, ) * 3))`. It creates a ~200MB file on disk, used later for benchmarking.\r\nIt is as well deleted at the end of the script, that might be the reason why you don't find it once the MVCE has run.", "@remydubois,\r\nWhen i tried executing the code on colab i got the following error. Please see the colab [gist](https://colab.sandbox.google.com/gist/gadagashwini/f336c6860e2884b5a281dc7ff9ea3c7f/untitled.ipynb). \r\n`NameError: name 'path' is not defined.`\r\nThanks!", "My bad, MVCE updated, there was a small typo.\r\nThanks for reporting, it should be running fine now, I updated the _timeit_s as well", "@remydubois, \r\nThanks for the update. Please take a look at the colab [gist](https://colab.sandbox.google.com/gist/gadagashwini/f336c6860e2884b5a281dc7ff9ea3c7f/untitled.ipynb#scrollTo=Ira0XqRn-IJ1). Thanks! \r\n", "@remydubois, Did you get chance to look at the colab gist. Thanks! ", "Thanks for looking into that.\r\n\r\nI looked at your benchmarks, it seems that the speed evaluations are highly non-reproducible, depending on the machine.\r\nWithout any further insights, I think we can consider this issue closed, because the problem is hard to identify.\r\n\r\nFeel free to reopen it if you think it necessary", "@remydubois, Closing the issue, feel free to comment/reopen the issue if issue still persists. Thanks! "]}, {"number": 32659, "title": "Potential bug in _compute_sampled_logits when `num_true` > 1", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave (10.14.6)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.6.4\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen logits are computed in `_compute_sampled_logits` for the evaluation of `tf.nn.nce_loss`, the output related to the true labels from `out_labels` might be wrong. \r\n\r\nLet me provide an example. Assume `batch_size = 2`, `num_true = 1000`, `num_sampled = 20`, then `out_labels` will be a Tensor of size `2 x 1020` (1020 is the addition of `num_true` and `num_sampled`).\r\nAccording to the following line:\r\n`array_ops.ones_like(true_logits) / num_true` (one of the last lines of `_compute_sampled_logits` function),\r\nthe first 1000 elements in each row of `out_labels` are: 1./1000 = 0.001, while the last 20 are 0.\r\n\r\nThe potential problem is that the output of `_compute_sampled_logits` is then fed to a sigmoid cross entropy (**not** softmax cross entropy which requires the labels to sum to 1 in each row). As a result, labels of an example are independent to each other. This means we can have as many labels as possible for each example, and `labels` do not necessarily have to sum to 1 (for each example).\r\n\r\nBy dividing `array_ops.ones_like(true_logits)` by `num_true`, the weights for the first 1000 (positive) labels of each example would try to produce a `y_hat` that would be close to 0.001 instead of 1. Predictions, which are generated via a sigmoid function, represent the probability that a given label is positive (for this example). Let's call this prediction, `y_hat` for simplicity. Due to the above formulation, `y_hat` would try to be as close to value `1/num_true` (instead of 1) for labels that appeared with the corresponding x feature.\r\n\r\n**Describe the expected behavior**\r\nThe expected behavior would be that all positive labels in each example be assigned value 1 instead of `1/num_true`. This way, the sigmoid which applies independently to each label will try to produce a `y_hat` for each (positive) label close to 1 instead of `1/num_true`.\r\n\r\nThe solution would be:\r\n`    out_labels = array_ops.concat([array_ops.ones_like(true_logits), array_ops.zeros_like(sampled_logits)], 1)\r\n`\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport pandas as pd\r\nimport numpy as np\r\nimport math\r\n\r\nbatch_size = 4\r\nnum_classes = 2000\r\ndim = 16\r\nnum_sampled = 20\r\nnum_true = 1000\r\nsampled_values = None\r\nremove_accidental_hits = False\r\npartition_strategy = 'div'\r\n\r\nweights = tf.get_variable('weights', initializer=tf.truncated_normal([num_classes, dim], stddev=1.0 / math.sqrt(dim)), trainable=True)\r\nbiases = tf.get_variable('biases', initializer=tf.zeros([num_classes]), trainable=True)\r\ninputs = tf.random_normal([batch_size, dim])\r\ndf = pd.DataFrame({'labels': [np.array([0, 2], np.int32), np.array([1], np.int32),\r\n                              np.array([6, 4, 1, 0, 3, 2], np.int32), np.array([5, 0, 2, 1, 6, 3, 9, 4], np.int32)]})\r\nlabels = tf.keras.preprocessing.sequence.pad_sequences(df['labels'].values, num_true, dtype=np.int64, value=0)\r\nlabels = tf.constant(labels)\r\n\r\nlogits, labels = _compute_sampled_logits(\r\n    weights=weights,\r\n    biases=biases,\r\n    labels=labels,\r\n    inputs=inputs,\r\n    num_sampled=num_sampled,\r\n    num_classes=num_classes,\r\n    num_true=num_true,\r\n    sampled_values=sampled_values,\r\n    subtract_log_q=True,\r\n    remove_accidental_hits=remove_accidental_hits,\r\n    partition_strategy=partition_strategy)\r\n```\r\n\r\nHere, `_compute_sampled_logits` is from `tensorflow.python.ops.nn_impl`.\r\n", "comments": ["Looks like code is incomplete. In order to expedite the trouble-shooting process, please provide a minimal standalone code to reproduce the issue reported here. Thanks!", "Hi @ravikyram thank you very much for your response! Thinking about it a bit more carefully, the code might probably be correct if you output predictions via a softmax function.\r\n\r\nTo extend a little bit more on what I meant above, let's assume `inputs` is a `batch_size` x `1024` matrix, `nce_weights` is `num_classes` x `1024` and `nce_biases` is `num_classes` x `1`.\r\n\r\nThen, `all_logits = tf.nn.bias_add(tf.matmul(inputs, nce_weights, transpose_b=True), nce_biases)`\r\nis a `batch_size` x `num_classes` matrix. An element `all_logits[i,j]` represents the logit for label `j` (of example `i`).\r\n\r\nFor each row of `all_logits` we select `num_true` columns that correspond to the true labels of this example (row). If, for example, the first row (example) is comprised of `num_true = 3` labels, which are 1, 4, 6, we will select the 1st, 4th, 6th column from the first row of `all_logits` (we assume here that columns start indexed at zero). These are the `true_logits`. In addition, we select `num_sampled` columns from each row of `all_logits` representing the negative label (0). These are the `sampled_logits`.\r\n\r\n`out_logits`, which is concatenation of `true_logits` and `sampled_logits`, is a `batch_size` x (`num_true` + `num_sampled`) matrix. These `out_logits` are aligned with `out_labels` as follows:\r\n```\r\nout_labels = [[1./num_true  1./num_true  1./num_true ... 1./num_true 0 0 ... 0],\r\n              [1./num_true  1./num_true  1./num_true ... 1./num_true 0 0 ... 0],\r\n               ...\r\n              [1./num_true  1./num_true  1./num_true ... 1./num_true 0 0 ... 0]]\r\n```\r\nAn element of `out_logits` corresponding to a true label would try to match `1./num_true` via the sigmoid: `1./(1 + exp(-out_logits)) \u2248 1./num_true`. \r\n\r\nNow, if we used a **sigmoid** function to retrieve predictions later on (and assuming the NCE loss did a good job bringing `out_logits` close to `out_labels`), the result of the prediction might have been deceptive. The predicted value for a label would be `1./(1 + exp(-out_logits))` and its maximum value (in places of a true label) would have been close to `1./num_true` (that is, far from 1). However, if we retrieve predictions via a **softmax** function instead, then the max value (in places of a true label) would be something close to `1./num_true`, which is ok since softmax imposes a valid distribution over the `num_classes` labels."]}, {"number": 32658, "title": "Why doesn't tf.matmul work with transposed tensor?", "body": "Why doesn't `tf.matmul` work with transposed tensor?\r\n\r\n`transpose_b=True` is ok, but not `tf.transpose(inp)`.\r\n\r\nThis screenshot was made in Colab with `tensorflow-gpu==2.0.0-rc1`:\r\n\r\n![image](https://user-images.githubusercontent.com/35609308/65253964-f2bfdd80-db14-11e9-859f-ca6a38498d20.png)", "comments": ["Hi @qo4on, it's because you are trying to transpose 3D tensor with `tf.transpose`. The `perm` argument defaults to `None` and will transpose the first and last axis only. You have to let `perm=[0, 2, 1]` so that the shape is compatible with batch matrix multiplication.\r\n\r\nThe workable example is here:\r\nhttps://colab.research.google.com/drive/1JMEpncNz23mQ4EHOj1QUak27sH1Iw-lg", "Hi, WindQAQ. Thank you.", "Closing this issue since its resolved. Thanks!"]}, {"number": 32657, "title": "Broken link in https://www.tensorflow.org/versions/r2.0/api_docs", "body": "In\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs\r\nthe `Swift (Early Release)`  link points to `https://www.tensorflow.org/versions/r2.0/swift` which shows a 404 page.\r\n", "comments": ["@rodrigob Thanks for pointing that \"404\" error. TensorFlow website is getting updated for TF2.0. Please wait for couple of days to update the links. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32657\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32657\">No</a>\n"]}, {"number": 32656, "title": "TF2rc Simple model subclass with Gradient tape error when transformed with autograph", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): https://colab.research.google.com/github/tensorflow/docs/blob/r2.0rc/site/en/r2/tutorials/quickstart/advanced.ipynb\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): TF2RC binary\r\n- TensorFlow version (use command below): TF2RC\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nA simple toy example using Gradient tape and a subclass-model cannot be transformed into a graph with Autograph.\r\nError: _**could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4**_\r\n\r\n**Describe the expected behavior**\r\nThe use of `@tf.function` should correctly create Autograph\r\n\r\n**Code to reproduce the issue**\r\nPlease, run \r\nhttps://colab.research.google.com/github/tensorflow/docs/blob/r2.0rc/site/en/r2/tutorials/quickstart/advanced.ipynb\r\n\r\n> \r\n> try:\r\n>   # %tensorflow_version only exists in Colab.\r\n>   %tensorflow_version 2.x\r\n> except Exception:\r\n>   pass\r\n> from __future__ import absolute_import, division, print_function, unicode_literals\r\n> \r\n> import tensorflow as tf\r\n> \r\n> from tensorflow.keras.layers import Dense, Flatten, Conv2D\r\n> from tensorflow.keras import Model\r\n> mnist = tf.keras.datasets.mnist\r\n> \r\n> (x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n> x_train, x_test = x_train / 255.0, x_test / 255.0\r\n> \r\n> # Add a channels dimension\r\n> x_train = x_train[..., tf.newaxis]\r\n> x_test = x_test[..., tf.newaxis]\r\n> train_ds = tf.data.Dataset.from_tensor_slices(\r\n>     (x_train, y_train)).shuffle(10000).batch(32)\r\n> \r\n> test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\r\n> class MyModel(Model):\r\n>   def __init__(self):\r\n>     super(MyModel, self).__init__()\r\n>     self.conv1 = Conv2D(32, 3, activation='relu')\r\n>     self.flatten = Flatten()\r\n>     self.d1 = Dense(128, activation='relu')\r\n>     self.d2 = Dense(10, activation='softmax')\r\n> \r\n>   def call(self, x):\r\n>     x = self.conv1(x)\r\n>     x = self.flatten(x)\r\n>     x = self.d1(x)\r\n>     return self.d2(x)\r\n> \r\n> # Create an instance of the model\r\n> model = MyModel()\r\n> loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\r\n> \r\n> optimizer = tf.keras.optimizers.Adam()\r\n> train_loss = tf.keras.metrics.Mean(name='train_loss')\r\n> train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\r\n> \r\n> test_loss = tf.keras.metrics.Mean(name='test_loss')\r\n> test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\r\n> @tf.function\r\n> def train_step(images, labels):\r\n>   with tf.GradientTape() as tape:\r\n>     predictions = model(images)\r\n>     loss = loss_object(labels, predictions)\r\n>   gradients = tape.gradient(loss, model.trainable_variables)\r\n>   optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n> \r\n>   train_loss(loss)\r\n>   train_accuracy(labels, predictions)\r\n> @tf.function\r\n> def test_step(images, labels):\r\n>   predictions = model(images)\r\n>   t_loss = loss_object(labels, predictions)\r\n> \r\n>   test_loss(t_loss)\r\n>   test_accuracy(labels, predictions)\r\n> EPOCHS = 5\r\n> \r\n> for epoch in range(EPOCHS):\r\n>   for images, labels in train_ds:\r\n>     train_step(images, labels)\r\n> \r\n>   for test_images, test_labels in test_ds:\r\n>     test_step(test_images, test_labels)\r\n> \r\n>   template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\r\n>   print(template.format(epoch+1,\r\n>                         train_loss.result(),\r\n>                         train_accuracy.result()*100,\r\n>                         test_loss.result(),\r\n>                         test_accuracy.result()*100))\r\n> \r\n>   # Reset the metrics for the next epoch\r\n>   train_loss.reset_states()\r\n>   train_accuracy.reset_states()\r\n>   test_loss.reset_states()\r\n>   test_accuracy.reset_states()\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@fgr1986 ,\r\nGiven code worked with TF-2.0rc1 without any issue's, please take a look at the [gist](https://github.com/tensorflow/tensorflow/issues/32651) of colab. Try running the code in TF-2.0rc1 and let us know.Thanks!", "Hi @oanush , just reproduced the problem with ' 2.0.0-rc1'. Please, note that the error is a warning, (it continues execution but it does not correctly pass the autograph function.\r\n\r\nYou can see the issue putting ` print(tf.__version__)` in https://colab.research.google.com/github/tensorflow/docs/blob/r2.0rc/site/en/r2/tutorials/quickstart/advanced.ipynb", "@fgr1986 ,\r\nI tried again and I did not face and error for TF-2.0rc1. Let us know exactly what warnings are received.", "Hi @oanush .\r\nI do not know where the problem to reproduce the issue may be.\r\n1) Right now, I just opened: https://colab.research.google.com/github/tensorflow/docs/blob/r2.0rc/site/en/r2/tutorials/quickstart/advanced.ipynb\r\nAdded `import tensorflow as tf\r\nprint(tf.__version__)`\r\nWhich reports version `2.0.0-rc1`\r\n\r\n2) Changed the runtime to GPU\r\n\r\n3) Run all, and get the following warnings:\r\n```\r\nWARNING:tensorflow:Entity <function train_step at 0x7f47c59bb268> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\r\nWARNING: Entity <function train_step at 0x7f47c59bb268> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\r\nWARNING:tensorflow:Layer my_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\r\n\r\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\r\n\r\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\r\n\r\nWARNING:tensorflow:Entity <bound method MyModel.call of <__main__.MyModel object at 0x7f47c59f7390>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\r\nWARNING: Entity <bound method MyModel.call of <__main__.MyModel object at 0x7f47c59f7390>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f47c4994c80> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\nWARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f47c4994c80> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <bound method MyModel.call of <__main__.MyModel object at 0x7f47c59f7390>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\r\nWARNING: Entity <bound method MyModel.call of <__main__.MyModel object at 0x7f47c59f7390>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <function test_step at 0x7f47c59bb9d8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\r\nWARNING: Entity <function test_step at 0x7f47c59bb9d8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\r\nWARNING:tensorflow:Entity <bound method MyModel.call of <__main__.MyModel object at 0x7f47c59f7390>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\r\nWARNING: Entity <bound method MyModel.call of <__main__.MyModel object at 0x7f47c59f7390>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method MyModel.call of <__main__.MyModel object at 0x7f47c59f7390>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\r\nWARNING: Entity <bound method MyModel.call of <__main__.MyModel object at 0x7f47c59f7390>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\r\n\r\n```\r\n\r\nThanks", "@fgr1986 I ran it in `2.0.0rc2` with GPU (as mentioned by you) and it runs without any issue. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/a2ee547861e319f66974c23e9c288927/advanced.ipynb). \r\n\r\nHere is the error trace\r\n```\r\nWARNING:tensorflow:Layer my_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\r\n\r\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\r\n\r\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\r\n\r\nEpoch 1, Loss: 0.13170050084590912, Accuracy: 95.97166442871094, Test Loss: 0.06416505575180054, Test Accuracy: 97.98999786376953\r\nEpoch 2, Loss: 0.04071308672428131, Accuracy: 98.74166870117188, Test Loss: 0.05933062732219696, Test Accuracy: 98.15999603271484\r\nEpoch 3, Loss: 0.022074595093727112, Accuracy: 99.30333709716797, Test Loss: 0.053413353860378265, Test Accuracy: 98.33999633789062\r\nEpoch 4, Loss: 0.013294064439833164, Accuracy: 99.55500030517578, Test Loss: 0.05290050432085991, Test Accuracy: 98.40999603271484\r\nEpoch 5, Loss: 0.008136439137160778, Accuracy: 99.73500061035156, Test Loss: 0.06418339163064957, Test Accuracy: 98.2699966430664\r\n```\r\n\r\nIf you set the default `dtype`  as `tf.keras.backend.set_floatx('float64')`, then the output is \r\n\r\n```\r\nEpoch 1, Loss: 0.14186638195651272, Accuracy: 95.70166666666667, Test Loss: 0.05724755003185461, Test Accuracy: 98.11999999999999\r\nEpoch 2, Loss: 0.043617788805440066, Accuracy: 98.65666666666667, Test Loss: 0.047525364846504776, Test Accuracy: 98.49\r\nEpoch 3, Loss: 0.022846894193099192, Accuracy: 99.23833333333333, Test Loss: 0.0562137230045946, Test Accuracy: 98.22\r\nEpoch 4, Loss: 0.01501299387764108, Accuracy: 99.515, Test Loss: 0.05169376732314508, Test Accuracy: 98.38\r\nEpoch 5, Loss: 0.008679648822207916, Accuracy: 99.715, Test Loss: 0.062491611017893994, Test Accuracy: 98.33\r\n```\r\n\r\nPlease close the issue if it was resolved by `2.0.0rc2`. Thanks!", "Yeap, rc2 worked nicely. \r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32656\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32656\">No</a>\n"]}, {"number": 32655, "title": "Unable to load tensorflow model in Java that was originally created in python (file not found exception)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): _Yes_\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): _Windows 10 Version 1903_\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: _/_\r\n- TensorFlow installed from (source or binary): _installed via pip in virtualenv_\r\n- TensorFlow version (use command below): _1.14_\r\n- Python version: _3.7_\r\n- Bazel version (if compiling from source): _/_\r\n- GCC/Compiler version (if compiling from source): _/_\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: Nvidia GTX 1050 Ti (3 GB)\r\n\r\n**Describe the current behavior**\r\nI train a neural network. After training, I want to export my model in order to use it in Java for predictions. Unfortunately, it always displays an error while trying to load the model in Java, saying the file cannot be found (see error message below). I checked the path a hundred times and made sure via the built-in Java methods that the files exist and I have the rights to read them.\r\n\r\n**Describe the expected behavior**\r\nI expect the model to be loaded in order to use it for predictions within my native Java code.\r\n\r\n**Code to reproduce the issue**\r\nBelow this you can see the Python code to export the model.\r\n\r\n```\r\ntf.saved_model.simple_save(sess,\r\n                           path_graph_model_export_dir,\r\n                           inputs=model_inputs, outputs={'sim': similarity})\r\n```\r\n\r\nThis is the structure of the exported directory.\r\n\r\n```\r\nsaved_model\r\n---- saved_model.pb\r\n---- variables\r\n-------- variables.data-00000-of-00001\r\n-------- variables.index\r\n```\r\n\r\nAnd the code in Java to load the model..\r\n\r\n```\r\nSavedModelBundle model = SavedModelBundle.load(modelPath, \"serve\")\r\n```\r\n\r\nresults in this exception.\r\n\r\n```\r\n2019-09-17 10:59:03.905538: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: saved_model\r\n2019-09-17 10:59:03.905997: I tensorflow/cc/saved_model/loader.cc:311] SavedModel load for tags { serve }; Status: fail. Took 457 microseconds.\r\norg.tensorflow.TensorFlowException: Could not find SavedModel .pb or .pbtxt at supplied export directory path: saved_model\r\n```\r\n\r\n**Other info / logs**\r\nI use a tensorflow graph built with Sonnet (version 1.34) but I don't think this is an issue as Sonnet only assists in building more complex graphs.\r\nTensorboard model log: [tensorboard.zip](https://github.com/tensorflow/tensorflow/files/3631054/tensorboard.zip)\r\nSaved model folder: [saved_model.zip](https://github.com/tensorflow/tensorflow/files/3631055/saved_model.zip)\r\n", "comments": ["@maxx2803 One question. Can you confirm whether `path_graph_model_export_dir` and `modelPath` are same? Can you try TF1.15.0rc1 and let us know whether the issue persists.\r\n\r\nCan you also share a simple standalone code to reproduce the issue. You can use public data.  Thanks!", "@jvishnuvardhan These two paths definitely link to the same folder. I checked it multiple times.\r\nI also tested the new version 1.15.0-rc1 but the issue persists.\r\n\r\nI put together some code to reproduce the issue.\r\n\r\nFirst a maven repository that simply loads the model saved with python ([maven_repo.zip](https://github.com/tensorflow/tensorflow/files/3657870/maven_repo.zip)). You can run the class after first importing all dependencies included in the pom.xml and building the project (I use IntelliJ IDEA for this). For simplicity reasons, I included an exported model in this repository. This is the model that was exported by this python file ([python_code.zip](https://github.com/tensorflow/tensorflow/files/3657884/python_code.zip)). \r\n\r\nI use this python code together with the requirements to train a network and export the model. Unfortunately, I was not able to provide a shorter example. The code is taken from this repository (https://github.com/deepmind/deepmind-research/tree/master/graph_matching_networks) and my original code (that I cannot share here due to privacy restrictions) uses this code as well.\r\n\r\nYou can run the python file to export your own model folder that can then be inserted into the maven repository to be imported by Java. For me, this leads to the exception shown in my first post. I hope this helps in reproducing the issues. Thanks a lot!", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32655\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32655\">No</a>\n", "> @jvishnuvardhan These two paths definitely link to the same folder. I checked it multiple times. I also tested the new version 1.15.0-rc1 but the issue persists.\r\n> \r\n> I put together some code to reproduce the issue.\r\n> \r\n> First a maven repository that simply loads the model saved with python ([maven_repo.zip](https://github.com/tensorflow/tensorflow/files/3657870/maven_repo.zip)). You can run the class after first importing all dependencies included in the pom.xml and building the project (I use IntelliJ IDEA for this). For simplicity reasons, I included an exported model in this repository. This is the model that was exported by this python file ([python_code.zip](https://github.com/tensorflow/tensorflow/files/3657884/python_code.zip)).\r\n> \r\n> I use this python code together with the requirements to train a network and export the model. Unfortunately, I was not able to provide a shorter example. The code is taken from this repository (https://github.com/deepmind/deepmind-research/tree/master/graph_matching_networks) and my original code (that I cannot share here due to privacy restrictions) uses this code as well.\r\n> \r\n> You can run the python file to export your own model folder that can then be inserted into the maven repository to be imported by Java. For me, this leads to the exception shown in my first post. I hope this helps in reproducing the issues. Thanks a lot!\r\n\r\nhi, did you solve the problem now? I meet the same error, and i don't know how to solve it.", "Eventually, it worked for me. I am not entirely sure why, but I think my problem was caused by the paths that reference my models. I use Windows and I found that it makes a difference if you try to load a model at the path `C:\\Users\\testuser\\model` or at `\\C:\\Users\\testuser\\model`. Apparently, this is a difference for TF. Also remember that the saved_model format always is a directory. So you have to load this directory as a model."]}, {"number": 32654, "title": "tf.distribute.MirroredStrategy leads to an infinite polling cycle with 4 GPUs", "body": "### System information\r\nA physical tower with 4 GPUs running Ubuntu 18.04 over Kubernetes\r\n\r\n- 256 GB of RAM\r\n- TensorFlow: tested on `tf-nightly-gpu-2.0-preview==2.0.0.dev20190902` to `tf-nightly-gpu-2.0-preview==2.0.0.dev20190918`\r\n- Python 3.6.8\r\n- CUDA 10.0, cuDNN 7.6.3.30 (also tested with cuDNN 7.5.0.56)\r\n- NVIDIA GTX 1080\r\n\r\n<details>\r\n<summary>nvidia-smi</summary>\r\n<pre>\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 410.78       Driver Version: 410.78       CUDA Version: 10.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:02:00.0 Off |                  N/A |\r\n| 53%   70C    P2    79W / 250W |  10889MiB / 11178MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 108...  Off  | 00000000:03:00.0 Off |                  N/A |\r\n| 52%   69C    P2    76W / 250W |  10893MiB / 11178MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce GTX 108...  Off  | 00000000:82:00.0 Off |                  N/A |\r\n| 48%   65C    P2    78W / 250W |  10889MiB / 11178MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  GeForce GTX 108...  Off  | 00000000:83:00.0 Off |                  N/A |\r\n| 45%   62C    P2    76W / 250W |  10893MiB / 11178MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n</pre>\r\n</details>\r\n\r\n### Problem\r\n\r\nI run the following sample code:\r\n\r\n```python\r\n#!/usr/bin/env python3\r\nimport sys\r\nimport tensorflow as tf\r\n\r\n\r\ndef main():\r\n    batch_size = 12\r\n    features_shape = 372, 558, 3\r\n    labels = 10\r\n    sample = tf.random.uniform(features_shape)\r\n\r\n    def with_shape(t, shape):\r\n        t = tf.squeeze(t)\r\n        t.set_shape(shape)\r\n        return t\r\n\r\n    ds_train = tf.data.Dataset.from_tensors([sample]).map(lambda s: (s, tf.ones((labels,)))) \\\r\n        .repeat().batch(batch_size).map(lambda s, l: (with_shape(s, (batch_size,) + features_shape),\r\n                                                      with_shape(l, (batch_size, labels))))\r\n    ds_val = tf.data.Dataset.from_tensors([sample]).map(lambda s: (s, tf.ones((labels,)))) \\\r\n        .repeat().batch(batch_size).take(10).map(\r\n        lambda s, l: (with_shape(s, (batch_size,) + features_shape), with_shape(l, (batch_size, labels))))\r\n    with tf.distribute.MirroredStrategy().scope():\r\n        model = tf.keras.applications.DenseNet121(\r\n            weights=None, input_shape=features_shape, classes=labels)\r\n        model.build((batch_size,) + features_shape)\r\n        model.summary()\r\n        optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\r\n        cross_entropy = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1)\r\n        model.compile(optimizer=optimizer, loss=cross_entropy, metrics=[\"accuracy\"])\r\n    model.fit(ds_train, validation_data=ds_val, epochs=1, steps_per_epoch=100)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    sys.exit(main())\r\n```\r\n\r\nIt outputs the following log and hangs for at least 9 hours (I killed it after):\r\n\r\n<details>\r\n<summary>log</summary>\r\n<pre>\r\n2019-09-19 11:22:16.548532: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2019-09-19 11:22:16.553080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1632] Found device 0 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:02:00.0\r\n2019-09-19 11:22:16.554064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1632] Found device 1 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:03:00.0\r\n2019-09-19 11:22:16.555051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1632] Found device 2 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:82:00.0\r\n2019-09-19 11:22:16.555890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1632] Found device 3 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:83:00.0\r\n2019-09-19 11:22:16.556021: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcudart.so.10.0\r\n2019-09-19 11:22:16.556046: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcublas.so.10.0\r\n2019-09-19 11:22:16.556062: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcufft.so.10.0\r\n2019-09-19 11:22:16.556079: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcurand.so.10.0\r\n2019-09-19 11:22:16.556095: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcusolver.so.10.0\r\n2019-09-19 11:22:16.556111: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcusparse.so.10.0\r\n2019-09-19 11:22:16.556127: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcudnn.so.7\r\n2019-09-19 11:22:16.562745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Adding visible gpu devices: 0, 1, 2, 3\r\n2019-09-19 11:22:16.562815: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcudart.so.10.0\r\n2019-09-19 11:22:16.566634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1173] Device interconnect StreamExecutorwith strength 1 edge matrix:\r\n2019-09-19 11:22:16.566650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1179]      0 1 2 3\r\n2019-09-19 11:22:16.566657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1192] 0:   N Y N N\r\n2019-09-19 11:22:16.566661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1192] 1:   Y N N N\r\n2019-09-19 11:22:16.566666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1192] 2:   N N N Y\r\n2019-09-19 11:22:16.566670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1192] 3:   N N Y N\r\n2019-09-19 11:22:16.571630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1318] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n2019-09-19 11:22:16.573706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1318] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10470 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1)\r\n2019-09-19 11:22:16.575382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1318] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10470 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:82:00.0, compute capability: 6.1)\r\n2019-09-19 11:22:16.576566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1318] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 10470 MB memory) -> physical GPU (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:83:00.0, compute capability: 6.1)\r\nWARNING:tensorflow:Entity <function main.<locals>.<lambda> at 0x7fe776f021e0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\r\n2019-09-19 11:22:17.393146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1632] Found device 0 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:02:00.0\r\n2019-09-19 11:22:17.394380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1632] Found device 1 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:03:00.0\r\n2019-09-19 11:22:17.395221: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1632] Found device 2 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:82:00.0\r\n2019-09-19 11:22:17.396088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1632] Found device 3 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:83:00.0\r\n2019-09-19 11:22:17.396168: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcudart.so.10.0\r\n2019-09-19 11:22:17.396202: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcublas.so.10.0\r\n2019-09-19 11:22:17.396218: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcufft.so.10.0\r\n2019-09-19 11:22:17.396233: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcurand.so.10.0\r\n2019-09-19 11:22:17.396263: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcusolver.so.10.0\r\n2019-09-19 11:22:17.396278: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcusparse.so.10.0\r\n2019-09-19 11:22:17.396293: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcudnn.so.7\r\n2019-09-19 11:22:17.402450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Adding visible gpu devices: 0, 1, 2, 3\r\n2019-09-19 11:22:17.402599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1173] Device interconnect StreamExecutorwith strength 1 edge matrix:\r\n2019-09-19 11:22:17.402611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1179]      0 1 2 3\r\n2019-09-19 11:22:17.402619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1192] 0:   N Y N N\r\n2019-09-19 11:22:17.402625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1192] 1:   Y N N N\r\n2019-09-19 11:22:17.402631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1192] 2:   N N N Y\r\n2019-09-19 11:22:17.402637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1192] 3:   N N Y N\r\n2019-09-19 11:22:17.407338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1318] Created TensorFlow device (/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n2019-09-19 11:22:17.408425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1318] Created TensorFlow device (/device:GPU:1 with 10470 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1)\r\n2019-09-19 11:22:17.409430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1318] Created TensorFlow device (/device:GPU:2 with 10470 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:82:00.0, compute capability: 6.1)\r\n2019-09-19 11:22:17.410293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1318] Created TensorFlow device (/device:GPU:3 with 10470 MB memory) -> physical GPU (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:83:00.0, compute capability: 6.1)\r\nModel: \"densenet121\"\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to\r\n==================================================================================================\r\ninput_1 (InputLayer)            [(None, 372, 558, 3) 0\r\n__________________________________________________________________________________________________\r\nzero_padding2d (ZeroPadding2D)  (None, 378, 564, 3)  0           input_1[0][0]\r\n__________________________________________________________________________________________________\r\nconv1/conv (Conv2D)             (None, 186, 279, 64) 9408        zero_padding2d[0][0]\r\n__________________________________________________________________________________________________\r\nconv1/bn (BatchNormalization)   (None, 186, 279, 64) 256         conv1/conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv1/relu (Activation)         (None, 186, 279, 64) 0           conv1/bn[0][0]\r\n__________________________________________________________________________________________________\r\nzero_padding2d_1 (ZeroPadding2D (None, 188, 281, 64) 0           conv1/relu[0][0]\r\n__________________________________________________________________________________________________\r\npool1 (MaxPooling2D)            (None, 93, 140, 64)  0           zero_padding2d_1[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block1_0_bn (BatchNormali (None, 93, 140, 64)  256         pool1[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block1_0_relu (Activation (None, 93, 140, 64)  0           conv2_block1_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block1_1_conv (Conv2D)    (None, 93, 140, 128) 8192        conv2_block1_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block1_1_bn (BatchNormali (None, 93, 140, 128) 512         conv2_block1_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block1_1_relu (Activation (None, 93, 140, 128) 0           conv2_block1_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block1_2_conv (Conv2D)    (None, 93, 140, 32)  36864       conv2_block1_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block1_concat (Concatenat (None, 93, 140, 96)  0           pool1[0][0]\r\n                                                                 conv2_block1_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block2_0_bn (BatchNormali (None, 93, 140, 96)  384         conv2_block1_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block2_0_relu (Activation (None, 93, 140, 96)  0           conv2_block2_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block2_1_conv (Conv2D)    (None, 93, 140, 128) 12288       conv2_block2_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block2_1_bn (BatchNormali (None, 93, 140, 128) 512         conv2_block2_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block2_1_relu (Activation (None, 93, 140, 128) 0           conv2_block2_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block2_2_conv (Conv2D)    (None, 93, 140, 32)  36864       conv2_block2_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block2_concat (Concatenat (None, 93, 140, 128) 0           conv2_block1_concat[0][0]\r\n                                                                 conv2_block2_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block3_0_bn (BatchNormali (None, 93, 140, 128) 512         conv2_block2_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block3_0_relu (Activation (None, 93, 140, 128) 0           conv2_block3_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block3_1_conv (Conv2D)    (None, 93, 140, 128) 16384       conv2_block3_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block3_1_bn (BatchNormali (None, 93, 140, 128) 512         conv2_block3_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block3_1_relu (Activation (None, 93, 140, 128) 0           conv2_block3_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block3_2_conv (Conv2D)    (None, 93, 140, 32)  36864       conv2_block3_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block3_concat (Concatenat (None, 93, 140, 160) 0           conv2_block2_concat[0][0]\r\n                                                                 conv2_block3_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block4_0_bn (BatchNormali (None, 93, 140, 160) 640         conv2_block3_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block4_0_relu (Activation (None, 93, 140, 160) 0           conv2_block4_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block4_1_conv (Conv2D)    (None, 93, 140, 128) 20480       conv2_block4_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block4_1_bn (BatchNormali (None, 93, 140, 128) 512         conv2_block4_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block4_1_relu (Activation (None, 93, 140, 128) 0           conv2_block4_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block4_2_conv (Conv2D)    (None, 93, 140, 32)  36864       conv2_block4_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block4_concat (Concatenat (None, 93, 140, 192) 0           conv2_block3_concat[0][0]\r\n                                                                 conv2_block4_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block5_0_bn (BatchNormali (None, 93, 140, 192) 768         conv2_block4_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block5_0_relu (Activation (None, 93, 140, 192) 0           conv2_block5_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block5_1_conv (Conv2D)    (None, 93, 140, 128) 24576       conv2_block5_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block5_1_bn (BatchNormali (None, 93, 140, 128) 512         conv2_block5_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block5_1_relu (Activation (None, 93, 140, 128) 0           conv2_block5_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block5_2_conv (Conv2D)    (None, 93, 140, 32)  36864       conv2_block5_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block5_concat (Concatenat (None, 93, 140, 224) 0           conv2_block4_concat[0][0]\r\n                                                                 conv2_block5_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block6_0_bn (BatchNormali (None, 93, 140, 224) 896         conv2_block5_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block6_0_relu (Activation (None, 93, 140, 224) 0           conv2_block6_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block6_1_conv (Conv2D)    (None, 93, 140, 128) 28672       conv2_block6_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block6_1_bn (BatchNormali (None, 93, 140, 128) 512         conv2_block6_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block6_1_relu (Activation (None, 93, 140, 128) 0           conv2_block6_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block6_2_conv (Conv2D)    (None, 93, 140, 32)  36864       conv2_block6_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv2_block6_concat (Concatenat (None, 93, 140, 256) 0           conv2_block5_concat[0][0]\r\n                                                                 conv2_block6_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\npool2_bn (BatchNormalization)   (None, 93, 140, 256) 1024        conv2_block6_concat[0][0]\r\n__________________________________________________________________________________________________\r\npool2_relu (Activation)         (None, 93, 140, 256) 0           pool2_bn[0][0]\r\n__________________________________________________________________________________________________\r\npool2_conv (Conv2D)             (None, 93, 140, 128) 32768       pool2_relu[0][0]\r\n__________________________________________________________________________________________________\r\npool2_pool (AveragePooling2D)   (None, 46, 70, 128)  0           pool2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block1_0_bn (BatchNormali (None, 46, 70, 128)  512         pool2_pool[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block1_0_relu (Activation (None, 46, 70, 128)  0           conv3_block1_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block1_1_conv (Conv2D)    (None, 46, 70, 128)  16384       conv3_block1_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block1_1_bn (BatchNormali (None, 46, 70, 128)  512         conv3_block1_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block1_1_relu (Activation (None, 46, 70, 128)  0           conv3_block1_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block1_2_conv (Conv2D)    (None, 46, 70, 32)   36864       conv3_block1_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block1_concat (Concatenat (None, 46, 70, 160)  0           pool2_pool[0][0]\r\n                                                                 conv3_block1_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block2_0_bn (BatchNormali (None, 46, 70, 160)  640         conv3_block1_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block2_0_relu (Activation (None, 46, 70, 160)  0           conv3_block2_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block2_1_conv (Conv2D)    (None, 46, 70, 128)  20480       conv3_block2_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block2_1_bn (BatchNormali (None, 46, 70, 128)  512         conv3_block2_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block2_1_relu (Activation (None, 46, 70, 128)  0           conv3_block2_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block2_2_conv (Conv2D)    (None, 46, 70, 32)   36864       conv3_block2_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block2_concat (Concatenat (None, 46, 70, 192)  0           conv3_block1_concat[0][0]\r\n                                                                 conv3_block2_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block3_0_bn (BatchNormali (None, 46, 70, 192)  768         conv3_block2_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block3_0_relu (Activation (None, 46, 70, 192)  0           conv3_block3_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block3_1_conv (Conv2D)    (None, 46, 70, 128)  24576       conv3_block3_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block3_1_bn (BatchNormali (None, 46, 70, 128)  512         conv3_block3_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block3_1_relu (Activation (None, 46, 70, 128)  0           conv3_block3_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block3_2_conv (Conv2D)    (None, 46, 70, 32)   36864       conv3_block3_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block3_concat (Concatenat (None, 46, 70, 224)  0           conv3_block2_concat[0][0]\r\n                                                                 conv3_block3_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block4_0_bn (BatchNormali (None, 46, 70, 224)  896         conv3_block3_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block4_0_relu (Activation (None, 46, 70, 224)  0           conv3_block4_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block4_1_conv (Conv2D)    (None, 46, 70, 128)  28672       conv3_block4_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block4_1_bn (BatchNormali (None, 46, 70, 128)  512         conv3_block4_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block4_1_relu (Activation (None, 46, 70, 128)  0           conv3_block4_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block4_2_conv (Conv2D)    (None, 46, 70, 32)   36864       conv3_block4_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block4_concat (Concatenat (None, 46, 70, 256)  0           conv3_block3_concat[0][0]\r\n                                                                 conv3_block4_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block5_0_bn (BatchNormali (None, 46, 70, 256)  1024        conv3_block4_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block5_0_relu (Activation (None, 46, 70, 256)  0           conv3_block5_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block5_1_conv (Conv2D)    (None, 46, 70, 128)  32768       conv3_block5_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block5_1_bn (BatchNormali (None, 46, 70, 128)  512         conv3_block5_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block5_1_relu (Activation (None, 46, 70, 128)  0           conv3_block5_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block5_2_conv (Conv2D)    (None, 46, 70, 32)   36864       conv3_block5_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block5_concat (Concatenat (None, 46, 70, 288)  0           conv3_block4_concat[0][0]\r\n                                                                 conv3_block5_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block6_0_bn (BatchNormali (None, 46, 70, 288)  1152        conv3_block5_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block6_0_relu (Activation (None, 46, 70, 288)  0           conv3_block6_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block6_1_conv (Conv2D)    (None, 46, 70, 128)  36864       conv3_block6_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block6_1_bn (BatchNormali (None, 46, 70, 128)  512         conv3_block6_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block6_1_relu (Activation (None, 46, 70, 128)  0           conv3_block6_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block6_2_conv (Conv2D)    (None, 46, 70, 32)   36864       conv3_block6_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block6_concat (Concatenat (None, 46, 70, 320)  0           conv3_block5_concat[0][0]\r\n                                                                 conv3_block6_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block7_0_bn (BatchNormali (None, 46, 70, 320)  1280        conv3_block6_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block7_0_relu (Activation (None, 46, 70, 320)  0           conv3_block7_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block7_1_conv (Conv2D)    (None, 46, 70, 128)  40960       conv3_block7_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block7_1_bn (BatchNormali (None, 46, 70, 128)  512         conv3_block7_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block7_1_relu (Activation (None, 46, 70, 128)  0           conv3_block7_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block7_2_conv (Conv2D)    (None, 46, 70, 32)   36864       conv3_block7_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block7_concat (Concatenat (None, 46, 70, 352)  0           conv3_block6_concat[0][0]\r\n                                                                 conv3_block7_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block8_0_bn (BatchNormali (None, 46, 70, 352)  1408        conv3_block7_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block8_0_relu (Activation (None, 46, 70, 352)  0           conv3_block8_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block8_1_conv (Conv2D)    (None, 46, 70, 128)  45056       conv3_block8_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block8_1_bn (BatchNormali (None, 46, 70, 128)  512         conv3_block8_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block8_1_relu (Activation (None, 46, 70, 128)  0           conv3_block8_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block8_2_conv (Conv2D)    (None, 46, 70, 32)   36864       conv3_block8_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block8_concat (Concatenat (None, 46, 70, 384)  0           conv3_block7_concat[0][0]\r\n                                                                 conv3_block8_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block9_0_bn (BatchNormali (None, 46, 70, 384)  1536        conv3_block8_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block9_0_relu (Activation (None, 46, 70, 384)  0           conv3_block9_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block9_1_conv (Conv2D)    (None, 46, 70, 128)  49152       conv3_block9_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block9_1_bn (BatchNormali (None, 46, 70, 128)  512         conv3_block9_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block9_1_relu (Activation (None, 46, 70, 128)  0           conv3_block9_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block9_2_conv (Conv2D)    (None, 46, 70, 32)   36864       conv3_block9_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block9_concat (Concatenat (None, 46, 70, 416)  0           conv3_block8_concat[0][0]\r\n                                                                 conv3_block9_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block10_0_bn (BatchNormal (None, 46, 70, 416)  1664        conv3_block9_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block10_0_relu (Activatio (None, 46, 70, 416)  0           conv3_block10_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block10_1_conv (Conv2D)   (None, 46, 70, 128)  53248       conv3_block10_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block10_1_bn (BatchNormal (None, 46, 70, 128)  512         conv3_block10_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block10_1_relu (Activatio (None, 46, 70, 128)  0           conv3_block10_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block10_2_conv (Conv2D)   (None, 46, 70, 32)   36864       conv3_block10_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block10_concat (Concatena (None, 46, 70, 448)  0           conv3_block9_concat[0][0]\r\n                                                                 conv3_block10_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block11_0_bn (BatchNormal (None, 46, 70, 448)  1792        conv3_block10_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block11_0_relu (Activatio (None, 46, 70, 448)  0           conv3_block11_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block11_1_conv (Conv2D)   (None, 46, 70, 128)  57344       conv3_block11_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block11_1_bn (BatchNormal (None, 46, 70, 128)  512         conv3_block11_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block11_1_relu (Activatio (None, 46, 70, 128)  0           conv3_block11_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block11_2_conv (Conv2D)   (None, 46, 70, 32)   36864       conv3_block11_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block11_concat (Concatena (None, 46, 70, 480)  0           conv3_block10_concat[0][0]\r\n                                                                 conv3_block11_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block12_0_bn (BatchNormal (None, 46, 70, 480)  1920        conv3_block11_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block12_0_relu (Activatio (None, 46, 70, 480)  0           conv3_block12_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block12_1_conv (Conv2D)   (None, 46, 70, 128)  61440       conv3_block12_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block12_1_bn (BatchNormal (None, 46, 70, 128)  512         conv3_block12_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block12_1_relu (Activatio (None, 46, 70, 128)  0           conv3_block12_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block12_2_conv (Conv2D)   (None, 46, 70, 32)   36864       conv3_block12_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv3_block12_concat (Concatena (None, 46, 70, 512)  0           conv3_block11_concat[0][0]\r\n                                                                 conv3_block12_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\npool3_bn (BatchNormalization)   (None, 46, 70, 512)  2048        conv3_block12_concat[0][0]\r\n__________________________________________________________________________________________________\r\npool3_relu (Activation)         (None, 46, 70, 512)  0           pool3_bn[0][0]\r\n__________________________________________________________________________________________________\r\npool3_conv (Conv2D)             (None, 46, 70, 256)  131072      pool3_relu[0][0]\r\n__________________________________________________________________________________________________\r\npool3_pool (AveragePooling2D)   (None, 23, 35, 256)  0           pool3_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block1_0_bn (BatchNormali (None, 23, 35, 256)  1024        pool3_pool[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block1_0_relu (Activation (None, 23, 35, 256)  0           conv4_block1_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block1_1_conv (Conv2D)    (None, 23, 35, 128)  32768       conv4_block1_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block1_1_bn (BatchNormali (None, 23, 35, 128)  512         conv4_block1_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block1_1_relu (Activation (None, 23, 35, 128)  0           conv4_block1_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block1_2_conv (Conv2D)    (None, 23, 35, 32)   36864       conv4_block1_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block1_concat (Concatenat (None, 23, 35, 288)  0           pool3_pool[0][0]\r\n                                                                 conv4_block1_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block2_0_bn (BatchNormali (None, 23, 35, 288)  1152        conv4_block1_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block2_0_relu (Activation (None, 23, 35, 288)  0           conv4_block2_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block2_1_conv (Conv2D)    (None, 23, 35, 128)  36864       conv4_block2_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block2_1_bn (BatchNormali (None, 23, 35, 128)  512         conv4_block2_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block2_1_relu (Activation (None, 23, 35, 128)  0           conv4_block2_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block2_2_conv (Conv2D)    (None, 23, 35, 32)   36864       conv4_block2_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block2_concat (Concatenat (None, 23, 35, 320)  0           conv4_block1_concat[0][0]\r\n                                                                 conv4_block2_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block3_0_bn (BatchNormali (None, 23, 35, 320)  1280        conv4_block2_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block3_0_relu (Activation (None, 23, 35, 320)  0           conv4_block3_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block3_1_conv (Conv2D)    (None, 23, 35, 128)  40960       conv4_block3_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block3_1_bn (BatchNormali (None, 23, 35, 128)  512         conv4_block3_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block3_1_relu (Activation (None, 23, 35, 128)  0           conv4_block3_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block3_2_conv (Conv2D)    (None, 23, 35, 32)   36864       conv4_block3_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block3_concat (Concatenat (None, 23, 35, 352)  0           conv4_block2_concat[0][0]\r\n                                                                 conv4_block3_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block4_0_bn (BatchNormali (None, 23, 35, 352)  1408        conv4_block3_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block4_0_relu (Activation (None, 23, 35, 352)  0           conv4_block4_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block4_1_conv (Conv2D)    (None, 23, 35, 128)  45056       conv4_block4_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block4_1_bn (BatchNormali (None, 23, 35, 128)  512         conv4_block4_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block4_1_relu (Activation (None, 23, 35, 128)  0           conv4_block4_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block4_2_conv (Conv2D)    (None, 23, 35, 32)   36864       conv4_block4_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block4_concat (Concatenat (None, 23, 35, 384)  0           conv4_block3_concat[0][0]\r\n                                                                 conv4_block4_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block5_0_bn (BatchNormali (None, 23, 35, 384)  1536        conv4_block4_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block5_0_relu (Activation (None, 23, 35, 384)  0           conv4_block5_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block5_1_conv (Conv2D)    (None, 23, 35, 128)  49152       conv4_block5_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block5_1_bn (BatchNormali (None, 23, 35, 128)  512         conv4_block5_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block5_1_relu (Activation (None, 23, 35, 128)  0           conv4_block5_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block5_2_conv (Conv2D)    (None, 23, 35, 32)   36864       conv4_block5_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block5_concat (Concatenat (None, 23, 35, 416)  0           conv4_block4_concat[0][0]\r\n                                                                 conv4_block5_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block6_0_bn (BatchNormali (None, 23, 35, 416)  1664        conv4_block5_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block6_0_relu (Activation (None, 23, 35, 416)  0           conv4_block6_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block6_1_conv (Conv2D)    (None, 23, 35, 128)  53248       conv4_block6_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block6_1_bn (BatchNormali (None, 23, 35, 128)  512         conv4_block6_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block6_1_relu (Activation (None, 23, 35, 128)  0           conv4_block6_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block6_2_conv (Conv2D)    (None, 23, 35, 32)   36864       conv4_block6_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block6_concat (Concatenat (None, 23, 35, 448)  0           conv4_block5_concat[0][0]\r\n                                                                 conv4_block6_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block7_0_bn (BatchNormali (None, 23, 35, 448)  1792        conv4_block6_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block7_0_relu (Activation (None, 23, 35, 448)  0           conv4_block7_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block7_1_conv (Conv2D)    (None, 23, 35, 128)  57344       conv4_block7_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block7_1_bn (BatchNormali (None, 23, 35, 128)  512         conv4_block7_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block7_1_relu (Activation (None, 23, 35, 128)  0           conv4_block7_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block7_2_conv (Conv2D)    (None, 23, 35, 32)   36864       conv4_block7_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block7_concat (Concatenat (None, 23, 35, 480)  0           conv4_block6_concat[0][0]\r\n                                                                 conv4_block7_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block8_0_bn (BatchNormali (None, 23, 35, 480)  1920        conv4_block7_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block8_0_relu (Activation (None, 23, 35, 480)  0           conv4_block8_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block8_1_conv (Conv2D)    (None, 23, 35, 128)  61440       conv4_block8_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block8_1_bn (BatchNormali (None, 23, 35, 128)  512         conv4_block8_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block8_1_relu (Activation (None, 23, 35, 128)  0           conv4_block8_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block8_2_conv (Conv2D)    (None, 23, 35, 32)   36864       conv4_block8_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block8_concat (Concatenat (None, 23, 35, 512)  0           conv4_block7_concat[0][0]\r\n                                                                 conv4_block8_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block9_0_bn (BatchNormali (None, 23, 35, 512)  2048        conv4_block8_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block9_0_relu (Activation (None, 23, 35, 512)  0           conv4_block9_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block9_1_conv (Conv2D)    (None, 23, 35, 128)  65536       conv4_block9_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block9_1_bn (BatchNormali (None, 23, 35, 128)  512         conv4_block9_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block9_1_relu (Activation (None, 23, 35, 128)  0           conv4_block9_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block9_2_conv (Conv2D)    (None, 23, 35, 32)   36864       conv4_block9_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block9_concat (Concatenat (None, 23, 35, 544)  0           conv4_block8_concat[0][0]\r\n                                                                 conv4_block9_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block10_0_bn (BatchNormal (None, 23, 35, 544)  2176        conv4_block9_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block10_0_relu (Activatio (None, 23, 35, 544)  0           conv4_block10_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block10_1_conv (Conv2D)   (None, 23, 35, 128)  69632       conv4_block10_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block10_1_bn (BatchNormal (None, 23, 35, 128)  512         conv4_block10_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block10_1_relu (Activatio (None, 23, 35, 128)  0           conv4_block10_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block10_2_conv (Conv2D)   (None, 23, 35, 32)   36864       conv4_block10_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block10_concat (Concatena (None, 23, 35, 576)  0           conv4_block9_concat[0][0]\r\n                                                                 conv4_block10_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block11_0_bn (BatchNormal (None, 23, 35, 576)  2304        conv4_block10_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block11_0_relu (Activatio (None, 23, 35, 576)  0           conv4_block11_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block11_1_conv (Conv2D)   (None, 23, 35, 128)  73728       conv4_block11_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block11_1_bn (BatchNormal (None, 23, 35, 128)  512         conv4_block11_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block11_1_relu (Activatio (None, 23, 35, 128)  0           conv4_block11_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block11_2_conv (Conv2D)   (None, 23, 35, 32)   36864       conv4_block11_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block11_concat (Concatena (None, 23, 35, 608)  0           conv4_block10_concat[0][0]\r\n                                                                 conv4_block11_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block12_0_bn (BatchNormal (None, 23, 35, 608)  2432        conv4_block11_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block12_0_relu (Activatio (None, 23, 35, 608)  0           conv4_block12_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block12_1_conv (Conv2D)   (None, 23, 35, 128)  77824       conv4_block12_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block12_1_bn (BatchNormal (None, 23, 35, 128)  512         conv4_block12_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block12_1_relu (Activatio (None, 23, 35, 128)  0           conv4_block12_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block12_2_conv (Conv2D)   (None, 23, 35, 32)   36864       conv4_block12_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block12_concat (Concatena (None, 23, 35, 640)  0           conv4_block11_concat[0][0]\r\n                                                                 conv4_block12_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block13_0_bn (BatchNormal (None, 23, 35, 640)  2560        conv4_block12_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block13_0_relu (Activatio (None, 23, 35, 640)  0           conv4_block13_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block13_1_conv (Conv2D)   (None, 23, 35, 128)  81920       conv4_block13_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block13_1_bn (BatchNormal (None, 23, 35, 128)  512         conv4_block13_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block13_1_relu (Activatio (None, 23, 35, 128)  0           conv4_block13_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block13_2_conv (Conv2D)   (None, 23, 35, 32)   36864       conv4_block13_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block13_concat (Concatena (None, 23, 35, 672)  0           conv4_block12_concat[0][0]\r\n                                                                 conv4_block13_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block14_0_bn (BatchNormal (None, 23, 35, 672)  2688        conv4_block13_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block14_0_relu (Activatio (None, 23, 35, 672)  0           conv4_block14_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block14_1_conv (Conv2D)   (None, 23, 35, 128)  86016       conv4_block14_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block14_1_bn (BatchNormal (None, 23, 35, 128)  512         conv4_block14_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block14_1_relu (Activatio (None, 23, 35, 128)  0           conv4_block14_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block14_2_conv (Conv2D)   (None, 23, 35, 32)   36864       conv4_block14_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block14_concat (Concatena (None, 23, 35, 704)  0           conv4_block13_concat[0][0]\r\n                                                                 conv4_block14_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block15_0_bn (BatchNormal (None, 23, 35, 704)  2816        conv4_block14_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block15_0_relu (Activatio (None, 23, 35, 704)  0           conv4_block15_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block15_1_conv (Conv2D)   (None, 23, 35, 128)  90112       conv4_block15_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block15_1_bn (BatchNormal (None, 23, 35, 128)  512         conv4_block15_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block15_1_relu (Activatio (None, 23, 35, 128)  0           conv4_block15_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block15_2_conv (Conv2D)   (None, 23, 35, 32)   36864       conv4_block15_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block15_concat (Concatena (None, 23, 35, 736)  0           conv4_block14_concat[0][0]\r\n                                                                 conv4_block15_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block16_0_bn (BatchNormal (None, 23, 35, 736)  2944        conv4_block15_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block16_0_relu (Activatio (None, 23, 35, 736)  0           conv4_block16_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block16_1_conv (Conv2D)   (None, 23, 35, 128)  94208       conv4_block16_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block16_1_bn (BatchNormal (None, 23, 35, 128)  512         conv4_block16_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block16_1_relu (Activatio (None, 23, 35, 128)  0           conv4_block16_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block16_2_conv (Conv2D)   (None, 23, 35, 32)   36864       conv4_block16_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block16_concat (Concatena (None, 23, 35, 768)  0           conv4_block15_concat[0][0]\r\n                                                                 conv4_block16_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block17_0_bn (BatchNormal (None, 23, 35, 768)  3072        conv4_block16_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block17_0_relu (Activatio (None, 23, 35, 768)  0           conv4_block17_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block17_1_conv (Conv2D)   (None, 23, 35, 128)  98304       conv4_block17_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block17_1_bn (BatchNormal (None, 23, 35, 128)  512         conv4_block17_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block17_1_relu (Activatio (None, 23, 35, 128)  0           conv4_block17_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block17_2_conv (Conv2D)   (None, 23, 35, 32)   36864       conv4_block17_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block17_concat (Concatena (None, 23, 35, 800)  0           conv4_block16_concat[0][0]\r\n                                                                 conv4_block17_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block18_0_bn (BatchNormal (None, 23, 35, 800)  3200        conv4_block17_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block18_0_relu (Activatio (None, 23, 35, 800)  0           conv4_block18_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block18_1_conv (Conv2D)   (None, 23, 35, 128)  102400      conv4_block18_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block18_1_bn (BatchNormal (None, 23, 35, 128)  512         conv4_block18_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block18_1_relu (Activatio (None, 23, 35, 128)  0           conv4_block18_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block18_2_conv (Conv2D)   (None, 23, 35, 32)   36864       conv4_block18_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block18_concat (Concatena (None, 23, 35, 832)  0           conv4_block17_concat[0][0]\r\n                                                                 conv4_block18_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block19_0_bn (BatchNormal (None, 23, 35, 832)  3328        conv4_block18_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block19_0_relu (Activatio (None, 23, 35, 832)  0           conv4_block19_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block19_1_conv (Conv2D)   (None, 23, 35, 128)  106496      conv4_block19_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block19_1_bn (BatchNormal (None, 23, 35, 128)  512         conv4_block19_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block19_1_relu (Activatio (None, 23, 35, 128)  0           conv4_block19_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block19_2_conv (Conv2D)   (None, 23, 35, 32)   36864       conv4_block19_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block19_concat (Concatena (None, 23, 35, 864)  0           conv4_block18_concat[0][0]\r\n                                                                 conv4_block19_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block20_0_bn (BatchNormal (None, 23, 35, 864)  3456        conv4_block19_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block20_0_relu (Activatio (None, 23, 35, 864)  0           conv4_block20_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block20_1_conv (Conv2D)   (None, 23, 35, 128)  110592      conv4_block20_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block20_1_bn (BatchNormal (None, 23, 35, 128)  512         conv4_block20_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block20_1_relu (Activatio (None, 23, 35, 128)  0           conv4_block20_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block20_2_conv (Conv2D)   (None, 23, 35, 32)   36864       conv4_block20_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block20_concat (Concatena (None, 23, 35, 896)  0           conv4_block19_concat[0][0]\r\n                                                                 conv4_block20_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block21_0_bn (BatchNormal (None, 23, 35, 896)  3584        conv4_block20_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block21_0_relu (Activatio (None, 23, 35, 896)  0           conv4_block21_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block21_1_conv (Conv2D)   (None, 23, 35, 128)  114688      conv4_block21_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block21_1_bn (BatchNormal (None, 23, 35, 128)  512         conv4_block21_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block21_1_relu (Activatio (None, 23, 35, 128)  0           conv4_block21_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block21_2_conv (Conv2D)   (None, 23, 35, 32)   36864       conv4_block21_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block21_concat (Concatena (None, 23, 35, 928)  0           conv4_block20_concat[0][0]\r\n                                                                 conv4_block21_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block22_0_bn (BatchNormal (None, 23, 35, 928)  3712        conv4_block21_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block22_0_relu (Activatio (None, 23, 35, 928)  0           conv4_block22_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block22_1_conv (Conv2D)   (None, 23, 35, 128)  118784      conv4_block22_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block22_1_bn (BatchNormal (None, 23, 35, 128)  512         conv4_block22_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block22_1_relu (Activatio (None, 23, 35, 128)  0           conv4_block22_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block22_2_conv (Conv2D)   (None, 23, 35, 32)   36864       conv4_block22_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block22_concat (Concatena (None, 23, 35, 960)  0           conv4_block21_concat[0][0]\r\n                                                                 conv4_block22_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block23_0_bn (BatchNormal (None, 23, 35, 960)  3840        conv4_block22_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block23_0_relu (Activatio (None, 23, 35, 960)  0           conv4_block23_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block23_1_conv (Conv2D)   (None, 23, 35, 128)  122880      conv4_block23_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block23_1_bn (BatchNormal (None, 23, 35, 128)  512         conv4_block23_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block23_1_relu (Activatio (None, 23, 35, 128)  0           conv4_block23_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block23_2_conv (Conv2D)   (None, 23, 35, 32)   36864       conv4_block23_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block23_concat (Concatena (None, 23, 35, 992)  0           conv4_block22_concat[0][0]\r\n                                                                 conv4_block23_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block24_0_bn (BatchNormal (None, 23, 35, 992)  3968        conv4_block23_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block24_0_relu (Activatio (None, 23, 35, 992)  0           conv4_block24_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block24_1_conv (Conv2D)   (None, 23, 35, 128)  126976      conv4_block24_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block24_1_bn (BatchNormal (None, 23, 35, 128)  512         conv4_block24_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block24_1_relu (Activatio (None, 23, 35, 128)  0           conv4_block24_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block24_2_conv (Conv2D)   (None, 23, 35, 32)   36864       conv4_block24_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv4_block24_concat (Concatena (None, 23, 35, 1024) 0           conv4_block23_concat[0][0]\r\n                                                                 conv4_block24_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\npool4_bn (BatchNormalization)   (None, 23, 35, 1024) 4096        conv4_block24_concat[0][0]\r\n__________________________________________________________________________________________________\r\npool4_relu (Activation)         (None, 23, 35, 1024) 0           pool4_bn[0][0]\r\n__________________________________________________________________________________________________\r\npool4_conv (Conv2D)             (None, 23, 35, 512)  524288      pool4_relu[0][0]\r\n__________________________________________________________________________________________________\r\npool4_pool (AveragePooling2D)   (None, 11, 17, 512)  0           pool4_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block1_0_bn (BatchNormali (None, 11, 17, 512)  2048        pool4_pool[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block1_0_relu (Activation (None, 11, 17, 512)  0           conv5_block1_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block1_1_conv (Conv2D)    (None, 11, 17, 128)  65536       conv5_block1_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block1_1_bn (BatchNormali (None, 11, 17, 128)  512         conv5_block1_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block1_1_relu (Activation (None, 11, 17, 128)  0           conv5_block1_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block1_2_conv (Conv2D)    (None, 11, 17, 32)   36864       conv5_block1_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block1_concat (Concatenat (None, 11, 17, 544)  0           pool4_pool[0][0]\r\n                                                                 conv5_block1_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block2_0_bn (BatchNormali (None, 11, 17, 544)  2176        conv5_block1_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block2_0_relu (Activation (None, 11, 17, 544)  0           conv5_block2_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block2_1_conv (Conv2D)    (None, 11, 17, 128)  69632       conv5_block2_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block2_1_bn (BatchNormali (None, 11, 17, 128)  512         conv5_block2_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block2_1_relu (Activation (None, 11, 17, 128)  0           conv5_block2_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block2_2_conv (Conv2D)    (None, 11, 17, 32)   36864       conv5_block2_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block2_concat (Concatenat (None, 11, 17, 576)  0           conv5_block1_concat[0][0]\r\n                                                                 conv5_block2_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block3_0_bn (BatchNormali (None, 11, 17, 576)  2304        conv5_block2_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block3_0_relu (Activation (None, 11, 17, 576)  0           conv5_block3_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block3_1_conv (Conv2D)    (None, 11, 17, 128)  73728       conv5_block3_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block3_1_bn (BatchNormali (None, 11, 17, 128)  512         conv5_block3_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block3_1_relu (Activation (None, 11, 17, 128)  0           conv5_block3_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block3_2_conv (Conv2D)    (None, 11, 17, 32)   36864       conv5_block3_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block3_concat (Concatenat (None, 11, 17, 608)  0           conv5_block2_concat[0][0]\r\n                                                                 conv5_block3_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block4_0_bn (BatchNormali (None, 11, 17, 608)  2432        conv5_block3_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block4_0_relu (Activation (None, 11, 17, 608)  0           conv5_block4_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block4_1_conv (Conv2D)    (None, 11, 17, 128)  77824       conv5_block4_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block4_1_bn (BatchNormali (None, 11, 17, 128)  512         conv5_block4_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block4_1_relu (Activation (None, 11, 17, 128)  0           conv5_block4_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block4_2_conv (Conv2D)    (None, 11, 17, 32)   36864       conv5_block4_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block4_concat (Concatenat (None, 11, 17, 640)  0           conv5_block3_concat[0][0]\r\n                                                                 conv5_block4_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block5_0_bn (BatchNormali (None, 11, 17, 640)  2560        conv5_block4_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block5_0_relu (Activation (None, 11, 17, 640)  0           conv5_block5_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block5_1_conv (Conv2D)    (None, 11, 17, 128)  81920       conv5_block5_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block5_1_bn (BatchNormali (None, 11, 17, 128)  512         conv5_block5_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block5_1_relu (Activation (None, 11, 17, 128)  0           conv5_block5_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block5_2_conv (Conv2D)    (None, 11, 17, 32)   36864       conv5_block5_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block5_concat (Concatenat (None, 11, 17, 672)  0           conv5_block4_concat[0][0]\r\n                                                                 conv5_block5_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block6_0_bn (BatchNormali (None, 11, 17, 672)  2688        conv5_block5_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block6_0_relu (Activation (None, 11, 17, 672)  0           conv5_block6_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block6_1_conv (Conv2D)    (None, 11, 17, 128)  86016       conv5_block6_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block6_1_bn (BatchNormali (None, 11, 17, 128)  512         conv5_block6_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block6_1_relu (Activation (None, 11, 17, 128)  0           conv5_block6_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block6_2_conv (Conv2D)    (None, 11, 17, 32)   36864       conv5_block6_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block6_concat (Concatenat (None, 11, 17, 704)  0           conv5_block5_concat[0][0]\r\n                                                                 conv5_block6_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block7_0_bn (BatchNormali (None, 11, 17, 704)  2816        conv5_block6_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block7_0_relu (Activation (None, 11, 17, 704)  0           conv5_block7_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block7_1_conv (Conv2D)    (None, 11, 17, 128)  90112       conv5_block7_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block7_1_bn (BatchNormali (None, 11, 17, 128)  512         conv5_block7_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block7_1_relu (Activation (None, 11, 17, 128)  0           conv5_block7_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block7_2_conv (Conv2D)    (None, 11, 17, 32)   36864       conv5_block7_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block7_concat (Concatenat (None, 11, 17, 736)  0           conv5_block6_concat[0][0]\r\n                                                                 conv5_block7_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block8_0_bn (BatchNormali (None, 11, 17, 736)  2944        conv5_block7_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block8_0_relu (Activation (None, 11, 17, 736)  0           conv5_block8_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block8_1_conv (Conv2D)    (None, 11, 17, 128)  94208       conv5_block8_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block8_1_bn (BatchNormali (None, 11, 17, 128)  512         conv5_block8_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block8_1_relu (Activation (None, 11, 17, 128)  0           conv5_block8_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block8_2_conv (Conv2D)    (None, 11, 17, 32)   36864       conv5_block8_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block8_concat (Concatenat (None, 11, 17, 768)  0           conv5_block7_concat[0][0]\r\n                                                                 conv5_block8_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block9_0_bn (BatchNormali (None, 11, 17, 768)  3072        conv5_block8_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block9_0_relu (Activation (None, 11, 17, 768)  0           conv5_block9_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block9_1_conv (Conv2D)    (None, 11, 17, 128)  98304       conv5_block9_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block9_1_bn (BatchNormali (None, 11, 17, 128)  512         conv5_block9_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block9_1_relu (Activation (None, 11, 17, 128)  0           conv5_block9_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block9_2_conv (Conv2D)    (None, 11, 17, 32)   36864       conv5_block9_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block9_concat (Concatenat (None, 11, 17, 800)  0           conv5_block8_concat[0][0]\r\n                                                                 conv5_block9_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block10_0_bn (BatchNormal (None, 11, 17, 800)  3200        conv5_block9_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block10_0_relu (Activatio (None, 11, 17, 800)  0           conv5_block10_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block10_1_conv (Conv2D)   (None, 11, 17, 128)  102400      conv5_block10_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block10_1_bn (BatchNormal (None, 11, 17, 128)  512         conv5_block10_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block10_1_relu (Activatio (None, 11, 17, 128)  0           conv5_block10_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block10_2_conv (Conv2D)   (None, 11, 17, 32)   36864       conv5_block10_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block10_concat (Concatena (None, 11, 17, 832)  0           conv5_block9_concat[0][0]\r\n                                                                 conv5_block10_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block11_0_bn (BatchNormal (None, 11, 17, 832)  3328        conv5_block10_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block11_0_relu (Activatio (None, 11, 17, 832)  0           conv5_block11_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block11_1_conv (Conv2D)   (None, 11, 17, 128)  106496      conv5_block11_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block11_1_bn (BatchNormal (None, 11, 17, 128)  512         conv5_block11_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block11_1_relu (Activatio (None, 11, 17, 128)  0           conv5_block11_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block11_2_conv (Conv2D)   (None, 11, 17, 32)   36864       conv5_block11_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block11_concat (Concatena (None, 11, 17, 864)  0           conv5_block10_concat[0][0]\r\n                                                                 conv5_block11_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block12_0_bn (BatchNormal (None, 11, 17, 864)  3456        conv5_block11_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block12_0_relu (Activatio (None, 11, 17, 864)  0           conv5_block12_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block12_1_conv (Conv2D)   (None, 11, 17, 128)  110592      conv5_block12_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block12_1_bn (BatchNormal (None, 11, 17, 128)  512         conv5_block12_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block12_1_relu (Activatio (None, 11, 17, 128)  0           conv5_block12_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block12_2_conv (Conv2D)   (None, 11, 17, 32)   36864       conv5_block12_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block12_concat (Concatena (None, 11, 17, 896)  0           conv5_block11_concat[0][0]\r\n                                                                 conv5_block12_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block13_0_bn (BatchNormal (None, 11, 17, 896)  3584        conv5_block12_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block13_0_relu (Activatio (None, 11, 17, 896)  0           conv5_block13_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block13_1_conv (Conv2D)   (None, 11, 17, 128)  114688      conv5_block13_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block13_1_bn (BatchNormal (None, 11, 17, 128)  512         conv5_block13_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block13_1_relu (Activatio (None, 11, 17, 128)  0           conv5_block13_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block13_2_conv (Conv2D)   (None, 11, 17, 32)   36864       conv5_block13_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block13_concat (Concatena (None, 11, 17, 928)  0           conv5_block12_concat[0][0]\r\n                                                                 conv5_block13_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block14_0_bn (BatchNormal (None, 11, 17, 928)  3712        conv5_block13_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block14_0_relu (Activatio (None, 11, 17, 928)  0           conv5_block14_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block14_1_conv (Conv2D)   (None, 11, 17, 128)  118784      conv5_block14_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block14_1_bn (BatchNormal (None, 11, 17, 128)  512         conv5_block14_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block14_1_relu (Activatio (None, 11, 17, 128)  0           conv5_block14_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block14_2_conv (Conv2D)   (None, 11, 17, 32)   36864       conv5_block14_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block14_concat (Concatena (None, 11, 17, 960)  0           conv5_block13_concat[0][0]\r\n                                                                 conv5_block14_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block15_0_bn (BatchNormal (None, 11, 17, 960)  3840        conv5_block14_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block15_0_relu (Activatio (None, 11, 17, 960)  0           conv5_block15_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block15_1_conv (Conv2D)   (None, 11, 17, 128)  122880      conv5_block15_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block15_1_bn (BatchNormal (None, 11, 17, 128)  512         conv5_block15_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block15_1_relu (Activatio (None, 11, 17, 128)  0           conv5_block15_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block15_2_conv (Conv2D)   (None, 11, 17, 32)   36864       conv5_block15_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block15_concat (Concatena (None, 11, 17, 992)  0           conv5_block14_concat[0][0]\r\n                                                                 conv5_block15_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block16_0_bn (BatchNormal (None, 11, 17, 992)  3968        conv5_block15_concat[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block16_0_relu (Activatio (None, 11, 17, 992)  0           conv5_block16_0_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block16_1_conv (Conv2D)   (None, 11, 17, 128)  126976      conv5_block16_0_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block16_1_bn (BatchNormal (None, 11, 17, 128)  512         conv5_block16_1_conv[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block16_1_relu (Activatio (None, 11, 17, 128)  0           conv5_block16_1_bn[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block16_2_conv (Conv2D)   (None, 11, 17, 32)   36864       conv5_block16_1_relu[0][0]\r\n__________________________________________________________________________________________________\r\nconv5_block16_concat (Concatena (None, 11, 17, 1024) 0           conv5_block15_concat[0][0]\r\n                                                                 conv5_block16_2_conv[0][0]\r\n__________________________________________________________________________________________________\r\nbn (BatchNormalization)         (None, 11, 17, 1024) 4096        conv5_block16_concat[0][0]\r\n__________________________________________________________________________________________________\r\nrelu (Activation)               (None, 11, 17, 1024) 0           bn[0][0]\r\n__________________________________________________________________________________________________\r\navg_pool (GlobalAveragePooling2 (None, 1024)         0           relu[0][0]\r\n__________________________________________________________________________________________________\r\nfc1000 (Dense)                  (None, 10)           10250       avg_pool[0][0]\r\n==================================================================================================\r\nTotal params: 7,047,754\r\nTrainable params: 6,964,106\r\nNon-trainable params: 83,648\r\n__________________________________________________________________________________________________\r\nTrain for 100 steps, validate for 10 steps\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/normalization.py:477: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\n2019-09-19 11:25:34.482086: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcublas.so.10.0\r\n2019-09-19 11:25:34.711640: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcudnn.so.7\r\n2019-09-19 11:25:35.685779: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Not found: ./bin/ptxas not found\r\nRelying on driver to perform ptx compilation. This message will be only logged once.\r\n</pre>\r\n</details>\r\n\r\nIf I remove the `MirroredStrategy` scope, the code runs successfully and does not hang (doing meaningless training).\r\n\r\n### Investigation\r\n\r\n<details>\r\n<summary><code>top</code></summary>\r\n<pre>\r\n 3161 root      20   0  0.112t 0.013t 948384 S  24.0  5.3 181:17.23 python3\r\n</pre>\r\n</details>\r\n\r\n`nvidia-smi`'s output is the same that I used in the \"System information\": all the GPUs are constantly 100% busy.\r\n\r\n<details>\r\n<summary><code>top -H -p 3161</code> - threads of the running process</summary>\r\n<pre>\r\n Threads: 155 total,   0 running, 155 sleeping,   0 stopped,   0 zombie\r\n%Cpu(s):  0.9 us,  0.8 sy,  0.0 ni, 97.8 id,  0.0 wa,  0.3 hi,  0.2 si,  0.0 st\r\nKiB Mem : 26408952+total, 99229216 free, 21207464 used, 14365283+buff/cache\r\nKiB Swap:        0 total,        0 free,        0 used. 20145740+avail Mem\r\n\r\n  PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND\r\n 3261 root      20   0  0.112t 0.013t 948360 S  6.3  5.3  42:18.36 python3\r\n 3255 root      20   0  0.112t 0.013t 948360 S  6.0  5.3  41:49.75 python3\r\n 3259 root      20   0  0.112t 0.013t 948360 S  6.0  5.3  42:09.41 python3\r\n 3257 root      20   0  0.112t 0.013t 948360 S  5.6  5.3  42:10.03 python3\r\n 3161 root      20   0  0.112t 0.013t 948360 S  0.0  5.3   2:11.62 python3\r\n 3165 root      20   0  0.112t 0.013t 948360 S  0.0  5.3   0:00.00 python3\r\n 3166 root      20   0  0.112t 0.013t 948360 S  0.0  5.3   0:15.45 python3\r\n...\r\n</pre>\r\n</details>\r\n\r\n<details>\r\n<summary><code>bt</code> in <code>gdb --pid 3161</code> - trace of the main thread</summary>\r\n<pre>\r\n#0  0x00007f26924c5839 in syscall () from /lib/x86_64-linux-gnu/libc.so.6\r\n#1  0x00007f264b30e53b in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#2  0x00007f264b30db59 in nsync::nsync_sem_wait_with_cancel_(nsync::waiter*, timespec, nsync::nsync_note_s_*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#3  0x00007f264b30b11b in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#4  0x00007f264b30b5f3 in nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*) () from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#5  0x00007f264344f60c in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, absl::InlinedVector<tensorflow::TensorValue, 4ul, std::allocator<tensorflow::TensorValue> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::NodeExecStats*, tensorflow::StepStats*, tensorflow::GraphCollector*, tensorflow::CancellationManager*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#6  0x00007f264344fa06 in tensorflow::KernelAndDeviceFunc::Run(absl::InlinedVector<tensorflow::TensorValue, 4ul, std::allocator<tensorflow::TensorValue> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::NodeExecStats*, tensorflow::StepStats*, tensorflow::GraphCollector*, tensorflow::CancellationManager*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#7  0x00007f26434313f6 in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*> > const&, std::unique_ptr<tensorflow::KernelAndDevice, tensorflow::core::RefCountDeleter> const&, tensorflow::NodeExecStats*, tensorflow::StepStats*, tensorflow::GraphCollector*, tensorflow::CancellationManager*, absl::Span<tensorflow::TensorHandle*>) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#8  0x00007f2643431aed in tensorflow::ExecuteNode::Run() ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#9  0x00007f264346ca85 in tensorflow::EagerExecutor::RunItem(std::unique_ptr<tensorflow::EagerExecutor::NodeItem, tensorflow::core::RefCountDeleter>) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#10 0x00007f264346d18d in tensorflow::EagerExecutor::AddOrExecute(std::unique_ptr<tensorflow::EagerNode, std::default_delete<tensorflow::EagerNode> >) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#11 0x00007f264342cd86 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#12 0x00007f264342ed00 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) ()\r\n---Type <return> to continue, or q <return> to quit---\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#13 0x00007f26432bc05d in TFE_Execute ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#14 0x00007f264324640c in TFE_Py_ExecuteCancelable(TFE_Context*, char const*, char const*, absl::InlinedVector<TFE_TensorHandle*, 4ul, std::allocator<TFE_TensorHandle*> >*, _object*, TFE_CancellationManager*, absl::InlinedVector<TFE_TensorHandle*, 2ul, std::allocator<TFE_TensorHandle*> >*, TF_Status*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#15 0x00007f2643246941 in TFE_Py_Execute(TFE_Context*, char const*, char const*, absl::InlinedVector<TFE_TensorHandle*,4ul, std::allocator<TFE_TensorHandle*> >*, _object*, absl::InlinedVector<TFE_TensorHandle*, 2ul, std::allocator<TFE_TensorHandle*> >*, TF_Status*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#16 0x00007f2642ddeb34 in _wrap_TFE_Py_Execute ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#17 0x00000000005097cf in _PyCFunction_FastCallDict (kwargs=<optimized out>, nargs=<optimized out>,\r\n    args=<optimized out>, func_obj=<built-in method TFE_Py_Execute of module object at remote 0x7f26805d2778>)\r\n    at ../Objects/methodobject.c:234\r\n#18 _PyCFunction_FastCallKeywords (kwnames=<optimized out>, nargs=<optimized out>, stack=<optimized out>,\r\n    func=<optimized out>) at ../Objects/methodobject.c:294\r\n#19 call_function.lto_priv () at ../Python/ceval.c:4851\r\n#20 0x000000000050b4a9 in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3335\r\n#21 0x0000000000507125 in PyEval_EvalFrameEx (throwflag=0, f=\r\n    Frame 0x62d109a8, for file /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py, line 61,in quick_execute (op_name='__inference_distributed_function_164755', num_outputs=3, inputs=[<tensorflow.python.framework.ops.EagerTensor at remote 0x7f256431f198>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7f256431f2e8>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7f25642d2c18>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7f263badc6d8>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7f260c506cc0>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7f260c50f8d0>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7f260c506780>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7f260c49d2e8>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7f260c50fc18>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7f260c420d68>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7f260c420630>, <tensorflow.python.frame...(truncated)) at ../Python/ceval.c:754\r\n#22 _PyEval_EvalCodeWithName.lto_priv.1821 () at ../Python/ceval.c:4166\r\n#23 0x0000000000508fa0 in fast_function.lto_priv () at ../Python/ceval.c:4992\r\n#24 0x000000000050999d in call_function.lto_priv () at ../Python/ceval.c:4872\r\n#25 0x000000000050c36e in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3351\r\n#26 0x0000000000507125 in PyEval_EvalFrameEx (throwflag=0,\r\n    f=Frame 0x71ccbef8, for file /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py, line 4---Type <return> to continue, or q <return> to quit---\r\n95, in call (self=<_EagerDefinedFunction(name=b'__inference_distributed_function_164755', _function_deleter=<_EagerDefinedFunctionDeleter(name=b'__inference_distributed_function_164755') at remote 0x7f1e0e0df438>, _registered_on_context=True, definition=<FunctionDef at remote 0x7f24bc06bfa8>, signature=<OpDef at remote 0x7f24bc06bef8>, _num_outputs=3, _output_types=[9, 1, 1], _output_shapes=[<TensorShape(_dims=[]) at remote 0x7f2384537a90>, <TensorShape(_dims=[]) at remote 0x7f2384537518>, <TensorShape(_dims=[]) at remote 0x7f2384537e80>], _control_captures=set(), _func_graph_outputs=[<Tensor(_op=<Operation(_graph=<FuncGraph(_lock=<_thread.RLock at remote 0x7f25642c78d0>, _group_lock=<GroupLock(_ready=<Condition(_lock=<_thread.lock at remote 0x7f24c4746288>, acquire=<built-in method acquire of _thread.lock object at remote 0x7f24c4746288>, release=<built-in method release of _thread.lock object at...(truncated)) at ../Python/ceval.c:754\r\n#27 _PyEval_EvalCodeWithName.lto_priv.1821 () at ../Python/ceval.c:4166\r\n#28 0x0000000000508fa0 in fast_function.lto_priv () at ../Python/ceval.c:4992\r\n#29 0x000000000050999d in call_function.lto_priv () at ../Python/ceval.c:4872\r\n#30 0x000000000050c36e in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3351\r\n#31 0x0000000000507125 in PyEval_EvalFrameEx (throwflag=0,\r\n    f=Frame 0x71ccb5b8, for file /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py, line 1600, in _call_flat (self=<ConcreteFunction(_arg_keywords=None, _num_positional_args=None, _func_graph=<FuncGraph(_lock=<_thread.RLock at remote 0x7f25642c78d0>, _group_lock=<GroupLock(_ready=<Condition(_lock=<_thread.lock at remote 0x7f24c4746288>, acquire=<built-in method acquire of _thread.lock object at remote 0x7f24c4746288>, release=<built-in method release of _thread.lock object at remote 0x7f24c4746288>, _waiters=<collections.deque at remote 0x7f24e44428d0>) at remote0x7f2384537f60>, _num_groups=2, _group_member_counts=[0, 0]) at remote 0x7f2384537c88>, _nodes_by_id={1: <Operation(_graph=<...>, _inputs_val=(), _id_value=1, _original_op=None, _traceback=<tensorflow_core.python._tf_stack.StackSummary at remote 0x7f23844c6fb8>, _device_code_locations=[<TraceableObject(obj='/job:localhost/replica:0/task:0/device:GPU:0', filename='/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/fr...(truncated)) at ../Python/ceval.c:754\r\n#32 _PyEval_EvalCodeWithName.lto_priv.1821 () at ../Python/ceval.c:4166\r\n#33 0x0000000000508fa0 in fast_function.lto_priv () at ../Python/ceval.c:4992\r\n#34 0x000000000050999d in call_function.lto_priv () at ../Python/ceval.c:4872\r\n#35 0x000000000050b4a9 in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3335\r\n#36 0x0000000000508c69 in PyEval_EvalFrameEx (throwflag=0,\r\n    f=Frame 0x7f18b8000b38, for file /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py, line 1515, in _filtered_call (self=<ConcreteFunction(_arg_keywords=None, _num_positional_args=None, _func_graph=<FuncGraph(_lock=<_thread.RLock at remote 0x7f25642c78d0>, _group_lock=<GroupLock(_ready=<Condition(_lock=<_thread.lock at remote0x7f24c4746288>, acquire=<built-in method acquire of _thread.lock object at remote 0x7f24c4746288>, release=<built-in method release of _thread.lock object at remote 0x7f24c4746288>, _waiters=<collections.deque at remote 0x7f24e44428d0>) at remote 0x7f2384537f60>, _num_groups=2, _group_member_counts=[0, 0]) at remote 0x7f2384537c88>, _nodes_by_id={1: <Operation(_graph=<...>, _inputs_val=(), _id_value=1, _original_op=None, _traceback=<tensorflow_core.python._tf_stack.StackSummary at remote 0x7f23844c6fb8>, _device_code_locations=[<TraceableObject(obj='/job:localhost/replica:0/task:0/device:GPU:0', filename='/usr/local/lib/python3.6/dist-packages/tensorflow_core/p...(truncated)) at ../Python/ceval.c:754\r\n---Type <return> to continue, or q <return> to quit---\r\n#37 _PyFunction_FastCall (globals=<optimized out>, nargs=139744142953272, args=<optimized out>, co=<optimized out>)\r\n    at ../Python/ceval.c:4933\r\n#38 fast_function.lto_priv () at ../Python/ceval.c:4968\r\n#39 0x000000000050999d in call_function.lto_priv () at ../Python/ceval.c:4872\r\n#40 0x000000000050b4a9 in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3335\r\n#41 0x0000000000507125 in PyEval_EvalFrameEx (throwflag=0,\r\n    f=Frame 0x1d37bb48, for file /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py, line 2237, in __call__ (self=<Function(_python_function=<function at remote 0x7f2635ff3a60>, _function_spec=<FunctionSpec(_fullargspec=<FullArgSpec at remote 0x7f24942b4eb8>, _is_method=False, _default_values=None, _args_to_indices={'input_iterator': 0}, arg_names=['input_iterator'], vararg_name=None, _arg_indices_to_default_values={}, _input_signature=None) at remote 0x7f25642e3630>, _name='distributed_function', _autograph=False, _autograph_options=None, _experimental_relax_shapes=False, _function_cache=<FunctionCache(missed={<CacheKey at remote 0x7f244a21be28>}, primary={<CacheKey at remote 0x7f244a21bd68>: <ConcreteFunction(_arg_keywords=None, _num_positional_args=None, _func_graph=<FuncGraph(_lock=<_thread.RLock at remote 0x7f25642c78d0>, _group_lock=<GroupLock(_ready=<Condition(_lock=<_thread.lock at remote 0x7f24c4746288>, acquire=<built-in method acquire of _thread.lock object at remote 0x7f24c4...(truncated)) at ../Python/ceval.c:754\r\n#42 _PyEval_EvalCodeWithName.lto_priv.1821 () at ../Python/ceval.c:4166\r\n#43 0x0000000000508794 in _PyFunction_FastCallDict () at ../Python/ceval.c:5084\r\n#44 0x00000000005940d1 in _PyObject_FastCallDict (kwargs={}, nargs=2, args=0x7ffcaa451a50,\r\n    func=<function at remote 0x7f263bd949d8>) at ../Objects/abstract.c:2310\r\n#45 _PyObject_Call_Prepend (kwargs={}, args=<optimized out>, obj=<optimized out>,\r\n    func=<function at remote 0x7f263bd949d8>) at ../Objects/abstract.c:2373\r\n#46 method_call.lto_priv () at ../Objects/classobject.c:314\r\n#47 0x0000000000549f41 in PyObject_Call (kwargs={},\r\n    args=(<DistributedIterator(_enable_get_next_as_optional=False, _iterators=[<_SingleWorkerDatasetIterator(_dataset=<_AutoShardDataset(_input_dataset=<_OptionsDataset(_input_dataset=<_OptionsDataset(_input_dataset=<PrefetchDataset(_input_dataset=<_RebatchDataset(_input_dataset=<MapDataset(_input_dataset=<BatchDataset(_input_dataset=<RepeatDataset(_input_dataset=<MapDataset(_input_dataset=<TensorDataset(_structure=<TensorSpec at remote 0x7f26295ffe10>, _tensors=[<tensorflow.python.framework.ops.EagerTensor at remote 0x7f263d514438>], _variant_tensor_attr=<tensorflow.python.framework.ops.EagerTensor at remote 0x7f263d5148d0>, _self_setattr_tracking=True, _self_unconditional_checkpoint_dependencies=[<TrackableReference at remote 0x7f26295ffd80>], _self_unconditional_dependency_names={'_variant_tracker': <_VariantTracker(_resource_handle=<...>, _resource_device='CPU', _resource_deleter=<CapturableResourceDeleter(_destroy_resource=None) at remote 0x7f263afb4400>, _create_resource=<function at remote 0x7f263bb23620>, _sel...(truncated),\r\n    func=<method at remote 0x7f25643a5d88>) at ../Objects/abstract.c:2261\r\n#48 slot_tp_call () at ../Objects/typeobject.c:6207\r\n#49 0x000000000059f50e in PyObject_Call () at ../Objects/abstract.c:2261\r\n#50 0x000000000050c854 in do_call_core (kwdict={},\r\n---Type <return> to continue, or q <return> to quit---\r\n    callargs=(<DistributedIterator(_enable_get_next_as_optional=False, _iterators=[<_SingleWorkerDatasetIterator(_dataset=<_AutoShardDataset(_input_dataset=<_OptionsDataset(_input_dataset=<_OptionsDataset(_input_dataset=<PrefetchDataset(_input_dataset=<_RebatchDataset(_input_dataset=<MapDataset(_input_dataset=<BatchDataset(_input_dataset=<RepeatDataset(_input_dataset=<MapDataset(_input_dataset=<TensorDataset(_structure=<TensorSpec at remote 0x7f26295ffe10>, _tensors=[<tensorflow.python.framework.ops.EagerTensor at remote 0x7f263d514438>], _variant_tensor_attr=<tensorflow.python.framework.ops.EagerTensor at remote 0x7f263d5148d0>, _self_setattr_tracking=True, _self_unconditional_checkpoint_dependencies=[<TrackableReference at remote 0x7f26295ffd80>], _self_unconditional_dependency_names={'_variant_tracker': <_VariantTracker(_resource_handle=<...>, _resource_device='CPU', _resource_deleter=<CapturableResourceDeleter(_destroy_resource=None) at remote 0x7f263afb4400>, _create_resource=<function at remote 0x7f263bb23620>, _sel...(truncated),\r\n    func=<Function(_python_function=<function at remote 0x7f2635ff3a60>, _function_spec=<FunctionSpec(_fullargspec=<FullArgSpec at remote 0x7f24942b4eb8>, _is_method=False, _default_values=None, _args_to_indices={'input_iterator': 0}, arg_names=['input_iterator'], vararg_name=None, _arg_indices_to_default_values={}, _input_signature=None) at remote 0x7f25642e3630>, _name='distributed_function', _autograph=False, _autograph_options=None, _experimental_relax_shapes=False, _function_cache=<FunctionCache(missed={<CacheKey at remote 0x7f244a21be28>}, primary={<CacheKey at remote 0x7f244a21bd68>: <ConcreteFunction(_arg_keywords=None, _num_positional_args=None, _func_graph=<FuncGraph(_lock=<_thread.RLock at remote 0x7f25642c78d0>, _group_lock=<GroupLock(_ready=<Condition(_lock=<_thread.lock at remote 0x7f24c4746288>, acquire=<built-inmethod acquire of _thread.lock object at remote 0x7f24c4746288>, release=<built-in method release of _thread.lock object at remote 0x7f24c4746288>, _waiters=<collections.deque at remote 0x7f24e...(truncated)) at ../Python/ceval.c:5120\r\n#51 _PyEval_EvalFrameDefault () at ../Python/ceval.c:3404\r\n#52 0x0000000000507125 in PyEval_EvalFrameEx (throwflag=0,\r\n    f=Frame 0x68702018, for file /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py, line 543, in _call (args=(<DistributedIterator(_enable_get_next_as_optional=False, _iterators=[<_SingleWorkerDatasetIterator(_dataset=<_AutoShardDataset(_input_dataset=<_OptionsDataset(_input_dataset=<_OptionsDataset(_input_dataset=<PrefetchDataset(_input_dataset=<_RebatchDataset(_input_dataset=<MapDataset(_input_dataset=<BatchDataset(_input_dataset=<RepeatDataset(_input_dataset=<MapDataset(_input_dataset=<TensorDataset(_structure=<TensorSpec at remote 0x7f26295ffe10>, _tensors=[<tensorflow.python.framework.ops.EagerTensor at remote 0x7f263d514438>], _variant_tensor_attr=<tensorflow.python.framework.ops.EagerTensor at remote 0x7f263d5148d0>, _self_setattr_tracking=True, _self_unconditional_checkpoint_dependencies=[<TrackableReference at remote 0x7f26295ffd80>], _self_unconditional_dependency_names={'_variant_tracker': <_VariantTracker(_resource_handle=<...>, _resource_device='CPU', _resource_deleter...(truncated)) at ../Python/ceval.c:754\r\n#53 _PyEval_EvalCodeWithName.lto_priv.1821 () at ../Python/ceval.c:4166\r\n#54 0x0000000000508794 in _PyFunction_FastCallDict () at ../Python/ceval.c:5084\r\n#55 0x00000000005940d1 in _PyObject_FastCallDict (kwargs={}, nargs=2, args=0x7ffcaa451e10,\r\n    func=<function at remote 0x7f263bdae048>) at ../Objects/abstract.c:2310\r\n#56 _PyObject_Call_Prepend (kwargs={}, args=<optimized out>, obj=<optimized out>,\r\n    func=<function at remote 0x7f263bdae048>) at ../Objects/abstract.c:2373\r\n#57 method_call.lto_priv () at ../Objects/classobject.c:314\r\n---Type <return> to continue, or q <return> to quit---\r\n#58 0x000000000059f50e in PyObject_Call () at ../Objects/abstract.c:2261\r\n#59 0x000000000050c854 in do_call_core (kwdict={},\r\n    callargs=(<DistributedIterator(_enable_get_next_as_optional=False, _iterators=[<_SingleWorkerDatasetIterator(_dataset=<_AutoShardDataset(_input_dataset=<_OptionsDataset(_input_dataset=<_OptionsDataset(_input_dataset=<PrefetchDataset(_input_dataset=<_RebatchDataset(_input_dataset=<MapDataset(_input_dataset=<BatchDataset(_input_dataset=<RepeatDataset(_input_dataset=<MapDataset(_input_dataset=<TensorDataset(_structure=<TensorSpec at remote 0x7f26295ffe10>, _tensors=[<tensorflow.python.framework.ops.EagerTensor at remote 0x7f263d514438>], _variant_tensor_attr=<tensorflow.python.framework.ops.EagerTensor at remote 0x7f263d5148d0>, _self_setattr_tracking=True, _self_unconditional_checkpoint_dependencies=[<TrackableReference at remote 0x7f26295ffd80>], _self_unconditional_dependency_names={'_variant_tracker': <_VariantTracker(_resource_handle=<...>, _resource_device='CPU', _resource_deleter=<CapturableResourceDeleter(_destroy_resource=None) at remote 0x7f263afb4400>, _create_resource=<function at remote 0x7f263bb23620>, _sel...(truncated),\r\n    func=<method at remote 0x7f25b05c7f88>) at ../Python/ceval.c:5120\r\n#60 _PyEval_EvalFrameDefault () at ../Python/ceval.c:3404\r\n#61 0x0000000000507125 in PyEval_EvalFrameEx (throwflag=0,\r\n    f=Frame 0x7f2564359dd8, for file /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py, line 480, in __call__ (self=<Function(_lock=<_thread.lock at remote 0x7f2564374df0>, _python_function=<function at remote 0x7f2564495f28>, _function_spec=<FunctionSpec(_fullargspec=<FullArgSpec at remote 0x7f25644326d8>, _is_method=False, _default_values=None, _args_to_indices={'input_iterator': 0}, arg_names=['input_iterator'], vararg_name=None, _arg_indices_to_default_values={}, _input_signature=None) at remote 0x7f256435b400>, _autograph=False, _experimental_autograph_options=None, experimental_relax_shapes=False, _experimental_compile=None, _created_variables=[<weakref at remote 0x7f256418ea48>, <weakref at remote 0x7f256418eae8>, <weakref at remote 0x7f256418ebd8>, <weakref at remote 0x7f256418ed18>, <weakref at remote 0x7f256418ed68>, <weakref at remote 0x7f256418eef8>, <weakref at remote 0x7f252832d098>, <weakref at remote 0x7f252832d188>, <weakref at remote 0x7f252832d228>, <weakref at r...(truncated)) at ../Python/ceval.c:754\r\n#62 _PyEval_EvalCodeWithName.lto_priv.1821 () at ../Python/ceval.c:4166\r\n#63 0x0000000000508537 in _PyFunction_FastCallDict () at ../Python/ceval.c:5075\r\n#64 0x00000000005940d1 in _PyObject_FastCallDict (kwargs=0x0, nargs=2, args=0x7ffcaa452190,\r\n    func=<function at remote 0x7f263bdbef28>) at ../Objects/abstract.c:2310\r\n#65 _PyObject_Call_Prepend (kwargs=0x0, args=<optimized out>, obj=<optimized out>,\r\n    func=<function at remote 0x7f263bdbef28>) at ../Objects/abstract.c:2373\r\n#66 method_call.lto_priv () at ../Objects/classobject.c:314\r\n#67 0x0000000000549f41 in PyObject_Call (kwargs=0x0,\r\n    args=(<DistributedIterator(_enable_get_next_as_optional=False, _iterators=[<_SingleWorkerDatasetIterator(_dataset=<_AutoShardDataset(_input_dataset=<_OptionsDataset(_input_dataset=<_OptionsDataset(_input_dataset=<PrefetchDataset(_input_dataset=<_RebatchDataset(_input_dataset=<MapDataset(_input_dataset=<BatchDataset(_input_dataset=<RepeatDataset(_input_dataset=<MapDataset(_input_dataset=<TensorDataset(_structure=<TensorSpec at remote 0x7f26295ffe10>, _tensors=[<tensorflow.python.framework.ops.EagerTensor at remote 0x7f263d514438>], _variant_tensor_attr=<tensorflow.python.framework.ops.Eager---Type <return> to continue, or q <return> to quit---\r\nTensor at remote 0x7f263d5148d0>, _self_setattr_tracking=True, _self_unconditional_checkpoint_dependencies=[<TrackableReference at remote 0x7f26295ffd80>], _self_unconditional_dependency_names={'_variant_tracker': <_VariantTracker(_resource_handle=<...>, _resource_device='CPU', _resource_deleter=<CapturableResourceDeleter(_destroy_resource=None) at remote 0x7f263afb4400>, _create_resource=<function at remote 0x7f263bb23620>, _sel...(truncated),\r\n    func=<method at remote 0x7f26914e20c8>) at ../Objects/abstract.c:2261\r\n#68 slot_tp_call () at ../Objects/typeobject.c:6207\r\n#69 0x00000000005a95fc in _PyObject_FastCallDict (kwargs=<optimized out>, nargs=1, args=0x7f25642fdc98,\r\n    func=<Function(_lock=<_thread.lock at remote 0x7f2564374df0>, _python_function=<function at remote 0x7f2564495f28>,_function_spec=<FunctionSpec(_fullargspec=<FullArgSpec at remote 0x7f25644326d8>, _is_method=False, _default_values=None, _args_to_indices={'input_iterator': 0}, arg_names=['input_iterator'], vararg_name=None, _arg_indices_to_default_values={}, _input_signature=None) at remote 0x7f256435b400>, _autograph=False, _experimental_autograph_options=None, experimental_relax_shapes=False, _experimental_compile=None, _created_variables=[<weakref at remote 0x7f256418ea48>, <weakref atremote 0x7f256418eae8>, <weakref at remote 0x7f256418ebd8>, <weakref at remote 0x7f256418ed18>, <weakref at remote 0x7f256418ed68>, <weakref at remote 0x7f256418eef8>, <weakref at remote 0x7f252832d098>, <weakref at remote 0x7f252832d188>,<weakref at remote 0x7f252832d228>, <weakref at remote 0x7f252832d278>, <weakref at remote 0x7f252832d1d8>, <weakref atremote 0x7f252832d318>, <weakref at remote 0x7f252832d4a8>, <weakref at r...(truncated))\r\n    at ../Objects/tupleobject.c:131\r\n#70 _PyObject_FastCallKeywords () at ../Objects/abstract.c:2496\r\n#71 0x0000000000509ad3 in call_function.lto_priv () at ../Python/ceval.c:4875\r\n#72 0x000000000050b4a9 in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3335\r\n#73 0x0000000000507125 in PyEval_EvalFrameEx (throwflag=0,\r\n    f=Frame 0x7f25642fdaf8, for file /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py, line 86, in execution_function (input_fn=<DistributedIterator(_enable_get_next_as_optional=False, _iterators=[<_SingleWorkerDatasetIterator(_dataset=<_AutoShardDataset(_input_dataset=<_OptionsDataset(_input_dataset=<_OptionsDataset(_input_dataset=<PrefetchDataset(_input_dataset=<_RebatchDataset(_input_dataset=<MapDataset(_input_dataset=<BatchDataset(_input_dataset=<RepeatDataset(_input_dataset=<MapDataset(_input_dataset=<TensorDataset(_structure=<TensorSpec at remote 0x7f26295ffe10>, _tensors=[<tensorflow.python.framework.ops.EagerTensor at remote 0x7f263d514438>], _variant_tensor_attr=<tensorflow.python.framework.ops.EagerTensor at remote 0x7f263d5148d0>, _self_setattr_tracking=True, _self_unconditional_checkpoint_dependencies=[<TrackableReference at remote 0x7f26295ffd80>], _self_unconditional_dependency_names={'_variant_tracker': <_VariantTracker(_resource_handle=<...>, _resource_...(truncated)) at ../Python/ceval.c:754\r\n#74 _PyEval_EvalCodeWithName.lto_priv.1821 () at ../Python/ceval.c:4166\r\n#75 0x0000000000508fa0 in fast_function.lto_priv () at ../Python/ceval.c:4992\r\n#76 0x000000000050999d in call_function.lto_priv () at ../Python/ceval.c:4872\r\n#77 0x000000000050b4a9 in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3335\r\n#78 0x0000000000507125 in PyEval_EvalFrameEx (throwflag=0,\r\n    f=Frame 0x689353d8, for file /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.---Type <return> to continue, or q <return> to quit---\r\npy, line 123, in run_one_epoch (model=<Model(_self_setattr_tracking=True, _nested_outputs=<Tensor(_op=<Operation(_graph=<FuncGraph(_lock=<_thread.RLock at remote 0x7f262967f690>, _group_lock=<GroupLock(_ready=<Condition(_lock=<_thread.lockat remote 0x7f260c4a7f30>, acquire=<built-in method acquire of _thread.lock object at remote 0x7f260c4a7f30>, release=<built-in method release of _thread.lock object at remote 0x7f260c4a7f30>, _waiters=<collections.deque at remote 0x7f260c594730>) at remote 0x7f260c5101d0>, _num_groups=2, _group_member_counts=[0, 0]) at remote 0x7f260c510160>, _nodes_by_id={1: <Operation(_graph=<...>, _inputs_val=None, _id_value=1, _original_op=None, _traceback=<tensorflow_core.python._tf_stack.StackSummary at remote 0x7f260c510f48>, _device_code_locations=[<TraceableObject(obj='', filename='/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py', ...(truncated)) at ../Python/ceval.c:754\r\n#79 _PyEval_EvalCodeWithName.lto_priv.1821 () at ../Python/ceval.c:4166\r\n#80 0x0000000000508fa0 in fast_function.lto_priv () at ../Python/ceval.c:4992\r\n#81 0x000000000050999d in call_function.lto_priv () at ../Python/ceval.c:4872\r\n#82 0x000000000050c36e in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3351\r\n#83 0x0000000000507125 in PyEval_EvalFrameEx (throwflag=0,\r\n    f=Frame 0x68693178, for file /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py, line 331, in fit (self=<Loop at remote 0x7f260c5102b0>, model=<Model(_self_setattr_tracking=True, _nested_outputs=<Tensor(_op=<Operation(_graph=<FuncGraph(_lock=<_thread.RLock at remote 0x7f262967f690>, _group_lock=<GroupLock(_ready=<Condition(_lock=<_thread.lock at remote 0x7f260c4a7f30>, acquire=<built-in method acquire of _thread.lock object at remote 0x7f260c4a7f30>, release=<built-in method release of _thread.lock object at remote 0x7f260c4a7f30>, _waiters=<collections.deque at remote 0x7f260c594730>) at remote 0x7f260c5101d0>, _num_groups=2, _group_member_counts=[0, 0]) at remote 0x7f260c510160>, _nodes_by_id={1: <Operation(_graph=<...>, _inputs_val=None, _id_value=1, _original_op=None, _traceback=<tensorflow_core.python._tf_stack.StackSummary at remote 0x7f260c510f48>, _device_code_locations=[<TraceableObject(obj='',filename='/usr/local/lib/python3.6/dist-packages/tensorflow_core/pytho...(truncated)) at ../Python/ceval.c:754\r\n#84 _PyEval_EvalCodeWithName.lto_priv.1821 () at ../Python/ceval.c:4166\r\n#85 0x0000000000508fa0 in fast_function.lto_priv () at ../Python/ceval.c:4992\r\n#86 0x000000000050999d in call_function.lto_priv () at ../Python/ceval.c:4872\r\n#87 0x000000000050c36e in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3351\r\n#88 0x0000000000507125 in PyEval_EvalFrameEx (throwflag=0,\r\n    f=Frame 0x7f20bc0086b8, for file /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py, line 766, in fit (self=<Model(_self_setattr_tracking=True, _nested_outputs=<Tensor(_op=<Operation(_graph=<FuncGraph(_lock=<_thread.RLock at remote 0x7f262967f690>, _group_lock=<GroupLock(_ready=<Condition(_lock=<_thread.lock at remote0x7f260c4a7f30>, acquire=<built-in method acquire of _thread.lock object at remote 0x7f260c4a7f30>, release=<built-in method release of _thread.lock object at remote 0x7f260c4a7f30>, _waiters=<collections.deque at remote 0x7f260c594730>) at remote 0x7f260c5101d0>, _num_groups=2, _group_member_counts=[0, 0]) at remote 0x7f260c510160>, _nodes_by_id={1: <Operation(_graph=<...>, _inputs_val=None, _id_value=1, _original_op=None, _traceback=<tensorflow_core.python._tf_stack.StackSummary at remote 0x7f260c510f48>, _device_code_locations=[<TraceableObject(obj='', filename='/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py', lineno=390...(truncated)) at ../Python/ceval.c:754\r\n---Type <return> to continue, or q <return> to quit---\r\n#89 _PyEval_EvalCodeWithName.lto_priv.1821 () at ../Python/ceval.c:4166\r\n#90 0x0000000000508fa0 in fast_function.lto_priv () at ../Python/ceval.c:4992\r\n#91 0x000000000050999d in call_function.lto_priv () at ../Python/ceval.c:4872\r\n#92 0x000000000050c36e in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3351\r\n#93 0x0000000000507125 in PyEval_EvalFrameEx (throwflag=0,\r\n    f=Frame 0x52a7658, for file /user/vmarkovtsev/images/hang.py, line 31, in main (sample=<tensorflow.python.framework.ops.EagerTensor at remote 0x7f26295f78d0>, ds_train=<MapDataset(_input_dataset=<BatchDataset(_input_dataset=<RepeatDataset(_input_dataset=<MapDataset(_input_dataset=<TensorDataset(_structure=<TensorSpec at remote 0x7f26295ffe10>, _tensors=[<tensorflow.python.framework.ops.EagerTensor at remote 0x7f263d514438>], _variant_tensor_attr=<tensorflow.python.framework.ops.EagerTensor at remote 0x7f263d5148d0>, _self_setattr_tracking=True, _self_unconditional_checkpoint_dependencies=[<TrackableReference at remote 0x7f26295ffd80>], _self_unconditional_dependency_names={'_variant_tracker': <_VariantTracker(_resource_handle=<...>, _resource_device='CPU', _resource_deleter=<CapturableResourceDeleter(_destroy_resource=None)at remote 0x7f263afb4400>, _create_resource=<function at remote 0x7f263bb23620>, _self_setattr_tracking=True, _self_unconditional_checkpoint_dependencies=[], _self_unconditional_dependency_n...(truncated)) at ../Python/ceval.c:754\r\n#94 _PyEval_EvalCodeWithName.lto_priv.1821 () at ../Python/ceval.c:4166\r\n#95 0x0000000000508fa0 in fast_function.lto_priv () at ../Python/ceval.c:4992\r\n#96 0x000000000050999d in call_function.lto_priv () at ../Python/ceval.c:4872\r\n#97 0x000000000050b4a9 in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3335\r\n#98 0x0000000000507125 in PyEval_EvalFrameEx (throwflag=0,\r\n    f=Frame 0x20509a8, for file /user/vmarkovtsev/images/hang.py, line 35, in <module> ()) at ../Python/ceval.c:754\r\n#99 _PyEval_EvalCodeWithName.lto_priv.1821 () at ../Python/ceval.c:4166\r\n#100 0x000000000050a3b3 in PyEval_EvalCodeEx (closure=0x0, kwdefs=0x0, defcount=0, defs=0x0, kwcount=0, kws=0x0,\r\n    argcount=0, args=0x0, locals=<optimized out>, globals=<optimized out>, _co=<optimized out>)\r\n    at ../Python/ceval.c:4187\r\n#101 PyEval_EvalCode (co=<optimized out>, globals=<optimized out>, locals=<optimized out>) at ../Python/ceval.c:731\r\n#102 0x00000000006349e2 in run_mod () at ../Python/pythonrun.c:1025\r\n#103 0x0000000000634a97 in PyRun_FileExFlags () at ../Python/pythonrun.c:978\r\n#104 0x000000000063824f in PyRun_SimpleFileExFlags () at ../Python/pythonrun.c:419\r\n#105 0x0000000000638425 in PyRun_AnyFileExFlags () at ../Python/pythonrun.c:81\r\n#106 0x0000000000638df1 in run_file (p_cf=0x7ffcaa45361c, filename=<optimized out>, fp=<optimized out>)\r\n    at ../Modules/main.c:340\r\n#107 Py_Main () at ../Modules/main.c:810\r\n#108 0x00000000004b0de0 in main (argc=2, argv=0x7ffcaa453818) at ../Programs/python.c:69\r\n</pre>\r\n</details>\r\n\r\n<details>\r\n<summary><code>bt</code> of each of the 4 running threads</summary>\r\n<pre>\r\n#0  0x00007fa23e7989d0 in nanosleep () from /lib/x86_64-linux-gnu/libc.so.6\r\n#1  0x00007fa1ec03cffd in tensorflow::(anonymous namespace)::PosixEnv::SleepForMicroseconds(long long) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2\r\n#2  0x00007fa1f5d2dcd5 in tensorflow::EventMgr::PollLoop() ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#3  0x00007fa1ec0528d1 in Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2\r\n#4  0x00007fa1ec04feb8 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2\r\n#5  0x00007fa1ec6a58df in std::execute_native_thread_routine (__p=0x6360ed0)\r\n    at /dt7-src/libstdc++-v3/src/nonshared11/../c++11/thread.cc:83\r\n#6  0x00007fa23e49c6db in start_thread () from /lib/x86_64-linux-gnu/libpthread.so.0\r\n#7  0x00007fa23e7d588f in clone () from /lib/x86_64-linux-gnu/libc.so.6\r\n</pre>\r\n</details>\r\n\r\n### Speculation\r\n\r\nAs we see, there are 4 threads - I guess one for each of my GPUs - which are polling something. They make 25-30% CPU load together. There are more than a hundred other threads, so I don't know which ones I should `bt` additionally. I tried with different batch sizes, which ofc influences the memory consumption, but does not change anything with the hang.\r\n\r\nI can provide the access to the hardware or execute arbitrary commands if needed.", "comments": ["Thank you for the bug report. \r\nIf you're able to compile and run from source, can you re-run with setting env variable TF_CPP_VMODULE=\"nccl_manager=2\" ? This will give us more logging wrt nccl which maybe be a potential place this could be hanging. ", "@guptapriya Can you please build me a wheel that I can install? Compiling TF from source with some additional hacks is so error-prone and hard for me. BTW why are the nightlies not suitable?", "@guptapriya This is the log I get on `tf-nightly-gpu-2.0-preview==2.0.0.dev20190927`:\r\n\r\n```\r\n2019-09-27 11:12:56.480481: I tensorflow/core/nccl/nccl_manager.cc:213] New NcclManager 0x7fe01001c190\r\n2019-09-27 11:12:57.411454: I tensorflow/core/nccl/nccl_manager.cc:602] RunCollective rank 0 global_rank -1 root_rank -1\r\n2019-09-27 11:12:57.411490: I tensorflow/core/nccl/nccl_manager.cc:602] RunCollective rank 1 global_rank -1 root_rank -1\r\n2019-09-27 11:12:57.411499: I tensorflow/core/nccl/nccl_manager.cc:602] RunCollective rank 2 global_rank -1 root_rank -1\r\n2019-09-27 11:12:57.411524: I tensorflow/core/nccl/nccl_manager.cc:602] RunCollective rank 3 global_rank -1 root_rank -1\r\n2019-09-27 11:12:57.411627: I tensorflow/core/nccl/nccl_manager.cc:679] call NcclAllReduce collective_key c1;-3597338873254438932;0:0 participant 0 sendbuff 0x7fe378d2d600 recvbuff 0x7fe378d2d600 nccl_comm 0x7fdf54001610 comm_stream 0x7fdf7c58a5c0 cuda_stream 0x7fdf7c58c030\r\n2019-09-27 11:12:57.411635: I tensorflow/core/nccl/nccl_manager.cc:679] call NcclAllReduce collective_key c1;-3597338873254438932;0:0 participant 2 sendbuff 0x7fe8f9d78e00 recvbuff 0x7fe8f9d78e00 nccl_comm 0x7fdf44000fd0 comm_stream 0x7fe01000eb20 cuda_stream 0x7feb58000e70\r\n2019-09-27 11:12:57.411638: I tensorflow/core/nccl/nccl_manager.cc:679] call NcclAllReduce collective_key c1;-3597338873254438932;0:0 participant 3 sendbuff 0x7fe0e980e200 recvbuff 0x7fe0e980e200 nccl_comm 0x7fdf48001d20 comm_stream 0x7fdf7c58dc80 cuda_stream 0x7fdf7c588c90\r\n2019-09-27 11:12:57.411632: I tensorflow/core/nccl/nccl_manager.cc:679] call NcclAllReduce collective_key c1;-3597338873254438932;0:0 participant 1 sendbuff 0x7fe6371f5400 recvbuff 0x7fe6371f5400 nccl_comm 0x7fdf58000fd0 comm_stream 0x7fe010018a30 cuda_stream 0x7fdf7c8e8830\r\n```\r\n\r\n\\<nothing is printed to the terminal after that\\>", "`bt` of the threads which have \"nccl\" in `bt`\r\n\r\n```\r\nThread 155 (Thread 0x7f1cc4b7a700 (LWP 7307)):\r\n#0  0x00007f2b209eb839 in syscall () from /lib/x86_64-linux-gnu/libc.so.6\r\n#1  0x00007f2ace5db1cb in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2\r\n#2  0x00007f2ace5da7e9 in nsync::nsync_sem_wait_with_cancel_(nsync::waiter*, timespec, nsync::nsync_note_s_*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2\r\n#3  0x00007f2ace5d7dab in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2\r\n#4  0x00007f2ace5d8283 in nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*) () from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2\r\n#5  0x00007f2ad52ef649 in tensorflow::NcclManager::LoopKernelLaunches(tensorflow::NcclManager::NcclStream*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#6  0x00007f2ad52f0678 in std::_Function_handler<void (), tensorflow::NcclManager::GetCommunicator(tensorflow::NcclManager::Collective*, tensorflow::NcclManager::Communicator**)::{lambda()#2}>::_M_invoke(std::_Any_data const&) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#7  0x00007f2ace81a3af in std::execute_native_thread_routine (__p=0x7f1c687b4fd0)\r\n    at /dt7-src/libstdc++-v3/src/nonshared11/../c++11/thread.cc:83\r\n#8  0x00007f2b206b86db in start_thread () from /lib/x86_64-linux-gnu/libpthread.so.0\r\n#9  0x00007f2b209f188f in clone () from /lib/x86_64-linux-gnu/libc.so.6\r\n\r\nThread 154 (Thread 0x7f1d30b78700 (LWP 7306)):\r\n#0  0x00007f2b209eb839 in syscall () from /lib/x86_64-linux-gnu/libc.so.6\r\n#1  0x00007f2ace5db1cb in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2\r\n#2  0x00007f2ace5da7e9 in nsync::nsync_sem_wait_with_cancel_(nsync::waiter*, timespec, nsync::nsync_note_s_*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2\r\n#3  0x00007f2ace5d7dab in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2\r\n#4  0x00007f2ace5d8283 in nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*) () from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2\r\n#5  0x00007f2ad52ef649 in tensorflow::NcclManager::LoopKernelLaunches(tensorflow::NcclManager::NcclStream*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#6  0x00007f2ad52f0678 in std::_Function_handler<void (), tensorflow::NcclManager::GetCommunicator(tensorflow::NcclManager::Collective*, tensorflow::NcclManager::Communicator**)::{lambda()#2}>::_M_invoke(std::_Any_data const&) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#7  0x00007f2ace81a3af in std::execute_native_thread_routine (__p=0x7f1c687b6c50)\r\n    at /dt7-src/libstdc++-v3/src/nonshared11/../c++11/thread.cc:83\r\n#8  0x00007f2b206b86db in start_thread () from /lib/x86_64-linux-gnu/libpthread.so.0\r\n#9  0x00007f2b209f188f in clone () from /lib/x86_64-linux-gnu/libc.so.6\r\n\r\nThread 153 (Thread 0x7f1d31379700 (LWP 7305)):\r\n#0  0x00007f2b209eb839 in syscall () from /lib/x86_64-linux-gnu/libc.so.6\r\n#1  0x00007f2ace5db1cb in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2\r\n#2  0x00007f2ace5da7e9 in nsync::nsync_sem_wait_with_cancel_(nsync::waiter*, timespec, nsync::nsync_note_s_*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2\r\n#3  0x00007f2ace5d7dab in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2\r\n#4  0x00007f2ace5d8283 in nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*) () from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2\r\n#5  0x00007f2ad52ef649 in tensorflow::NcclManager::LoopKernelLaunches(tensorflow::NcclManager::NcclStream*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#6  0x00007f2ad52f0678 in std::_Function_handler<void (), tensorflow::NcclManager::GetCommunicator(tensorflow::NcclManager::Collective*, tensorflow::NcclManager::Communicator**)::{lambda()#2}>::_M_invoke(std::_Any_data const&) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#7  0x00007f2ace81a3af in std::execute_native_thread_routine (__p=0x7f1c687a1f70)\r\n    at /dt7-src/libstdc++-v3/src/nonshared11/../c++11/thread.cc:83\r\n#8  0x00007f2b206b86db in start_thread () from /lib/x86_64-linux-gnu/libpthread.so.0\r\n#9  0x00007f2b209f188f in clone () from /lib/x86_64-linux-gnu/libc.so.6\r\n\r\nThread 152 (Thread 0x7f1d31b7a700 (LWP 7304)):\r\n#0  0x00007f2b209eb839 in syscall () from /lib/x86_64-linux-gnu/libc.so.6\r\n#1  0x00007f2ace5db1cb in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2\r\n#2  0x00007f2ace5da7e9 in nsync::nsync_sem_wait_with_cancel_(nsync::waiter*, timespec, nsync::nsync_note_s_*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2\r\n#3  0x00007f2ace5d7dab in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2\r\n#4  0x00007f2ace5d8283 in nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*) () from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2\r\n#5  0x00007f2ad52ef649 in tensorflow::NcclManager::LoopKernelLaunches(tensorflow::NcclManager::NcclStream*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#6  0x00007f2ad52f0678 in std::_Function_handler<void (), tensorflow::NcclManager::GetCommunicator(tensorflow::NcclManager::Collective*, tensorflow::NcclManager::Communicator**)::{lambda()#2}>::_M_invoke(std::_Any_data const&) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#7  0x00007f2ace81a3af in std::execute_native_thread_routine (__p=0x7f1c687a3300)\r\n    at /dt7-src/libstdc++-v3/src/nonshared11/../c++11/thread.cc:83\r\n#8  0x00007f2b206b86db in start_thread () from /lib/x86_64-linux-gnu/libpthread.so.0\r\n#9  0x00007f2b209f188f in clone () from /lib/x86_64-linux-gnu/libc.so.6", "Ah yes, I later learned that the env variable works with pip packages as well. \r\nThank you for the logs. It seems like it could have something to do with NCCL. Can you re-run with NCCL_DEBUG=INFO (another env variable) which will give us more info? ", "cc @dubey ", "@guptapriya sure, this is what I see:\r\n\r\n```\r\njupyter-vmarkovtsev:7580:7706 [0] NCCL INFO NET/Socket : Using [0]eth0:10.2.3.32<0>\r\njupyter-vmarkovtsev:7580:7706 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\n\r\njupyter-vmarkovtsev:7580:7706 [0] external/nccl_archive/src/misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\r\nNCCL version 2.4.7+cudaCUDA_MAJOR.CUDA_MINOR\r\njupyter-vmarkovtsev:7580:7765 [0] NCCL INFO Setting affinity for GPU 0 to ff00ff\r\njupyter-vmarkovtsev:7580:7764 [2] NCCL INFO Setting affinity for GPU 2 to ff00ff00\r\njupyter-vmarkovtsev:7580:7763 [3] NCCL INFO Setting affinity for GPU 3 to ff00ff00\r\njupyter-vmarkovtsev:7580:7762 [1] NCCL INFO Setting affinity for GPU 1 to ff00ff\r\njupyter-vmarkovtsev:7580:7762 [1] NCCL INFO Channel 00 :    0   3   1   2\r\njupyter-vmarkovtsev:7580:7765 [0] NCCL INFO Ring 00 : 3[0] -> 1[3] via direct shared memory\r\njupyter-vmarkovtsev:7580:7763 [3] NCCL INFO Ring 00 : 1[3] -> 2[2] via P2P/direct pointer\r\njupyter-vmarkovtsev:7580:7764 [2] NCCL INFO Ring 00 : 2[2] -> 0[1] via direct shared memory\r\njupyter-vmarkovtsev:7580:7762 [1] NCCL INFO Ring 00 : 0[1] -> 3[0] via P2P/direct pointer\r\njupyter-vmarkovtsev:7580:7762 [1] NCCL INFO Using 256 threads, Min Comp Cap 6, Trees disabled\r\njupyter-vmarkovtsev:7580:7764 [2] NCCL INFO comm 0x7f1dbc001b20 rank 2 nranks 4 cudaDev 2 nvmlDev 2 - Init COMPLETE\r\njupyter-vmarkovtsev:7580:7762 [1] NCCL INFO comm 0x7f1dd0001610 rank 0 nranks 4 cudaDev 1 nvmlDev 1 - Init COMPLETE\r\njupyter-vmarkovtsev:7580:7765 [0] NCCL INFO comm 0x7f1db4001810 rank 3 nranks 4 cudaDev 0 nvmlDev 0 - Init COMPLETE\r\njupyter-vmarkovtsev:7580:7763 [3] NCCL INFO comm 0x7f1dc4000fd0 rank 1 nranks 4 cudaDev 3 nvmlDev 3 - Init COMPLETE\r\njupyter-vmarkovtsev:7580:7757 [1] NCCL INFO Launch mode Group/CGMD\r\n```\r\n\\<hangs\\>\r\n\r\n<details>\r\n<summary>ifconfig</summary>\r\n<pre>\r\neth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1450\r\n        inet 10.2.3.32  netmask 255.255.255.255  broadcast 0.0.0.0\r\n        ether ae:72:33:81:21:71  txqueuelen 0  (Ethernet)\r\n        RX packets 2336309  bytes 18475645041 (18.4 GB)\r\n        RX errors 0  dropped 0  overruns 0  frame 0\r\n        TX packets 3641498  bytes 414040978 (414.0 MB)\r\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\r\n</pre>\r\n</details>", "`NCCL_DEBUG_SUBSYS=COLL` also gives this right before `Launch mode Group/CGMD`:\r\n\r\n```\r\njupyter-vmarkovtsev:7959:8138 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f2485d3ea00 recvbuff 0x7f2485d3ea00 count 6964106 datatype 7 op 0 root 0 comm 0x7f1adc000fd0 [nranks=4] stream 0x7f1b1eff6520\r\njupyter-vmarkovtsev:7959:8137 [3] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f1c75849600 recvbuff 0x7f1c75849600 count 6964106 datatype 7 op 0 root 0 comm 0x7f1ae8001d20 [nranks=4] stream 0x7f1b1efe3180\r\njupyter-vmarkovtsev:7959:8136 [1] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f21c1ee1800 recvbuff 0x7f21c1ee1800 count 6964106 datatype 7 op 0 root 0 comm 0x7f1ae4001610 [nranks=4] stream 0x7f1ba4013210\r\njupyter-vmarkovtsev:7959:8139 [2] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f1f09abcc00 recvbuff 0x7f1f09abcc00 count 6964106 datatype 7 op 0 root 0 comm 0x7f1ae0000fd0 [nranks=4] stream 0x7f1b1efe4460\r\n```", "AllReduce tests from https://github.com/nvidia/nccl-tests hang for me\r\n\r\n```\r\n./build/all_reduce_perf -b 8 -e 256M -f 2 -g 4\r\n```\r\n\r\nSo this is probably not a problem with Tensorflow itself... Reporting this upstream.\r\n", "Thanks, please reopen if needed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32654\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32654\">No</a>\n", "My root problem was malfunctioning peer to peer GPU access. I saw something like this in `dmesg`:\r\n\r\n```\r\n[1478401.486621] DMAR: DRHD: handling fault status reg 502\r\n[1478401.486981] DMAR: [DMA Write] Request device [02:00.0] fault addr cd139000 [fault reason 05] PTE Write access is not set\r\n[1478401.487694] DMAR: DRHD: handling fault status reg 2\r\n[1478401.488053] DMAR: [DMA Write] Request device [82:00.0] fault addr f8139000 [fault reason 05] PTE Write access is not set\r\n[1478401.716106] DMAR: DRHD: handling fault status reg 602\r\n[1478401.716534] DMAR: [DMA Write] Request device [02:00.0] fault addr cd139000 [fault reason 05] PTE Write access is not set\r\n[1478401.719859] DMAR: DRHD: handling fault status reg 102\r\n[1478401.720267] DMAR: [DMA Write] Request device [82:00.0] fault addr f8139000 [fault reason 05] PTE Write access is not set\r\n[1478419.000793] dmar_fault: 32 callbacks suppressed\r\n[1478419.000795] DMAR: DRHD: handling fault status reg 702\r\n[1478419.001500] DMAR: [DMA Write] Request device [02:00.0] fault addr cd139000 [fault reason 05] PTE Write access is not set\r\n[1478421.063012] DMAR: DRHD: handling fault status reg 202\r\n[1478421.063361] DMAR: [DMA Write] Request device [82:00.0] fault addr f8139000 [fault reason 05] PTE Write access is not set\r\n```\r\n\r\nMy workaround is `export NCCL_P2P_DISABLE=1`.", "Thanks for posting the update!  It may help others who run into a similar issue.", "Although `NCCL_P2P_DISABLE=1` fixed the original problem, I've got an even worse one. I saw the Keras' progress bar, then it hanged again. More DMAR errors in `dmesg`, the process consumes 100% CPU (1 core) and is immortal: even `kill -9` does not help. Disabling VT-x in BIOS did not help. I will try booting the kernel with `intel_iommu=off` to see if it helps.", "I confirm that booting the kernel with `intel_iommu=off` fixed all the hangs and I am finally enjoying the multi-gpu training."]}, {"number": 32653, "title": "Keras model.save() is extremely slow under MirroredStrategy context when keras.layers.BatchNormalization is in use.", "body": "**System information**\r\n- Have I written custom code: yes, the training script is written from scratch but the network resnet_v1.5-50 is copied from official example tensorflow/models\r\n- OS Platform and Distribution: Linux Debian 9\r\n- TensorFlow installed from (source or binary): yes\r\n- GCC version for compiling: gcc 6.3 git commit 1eea9f905c4b4ac4844855eb64e2f9e415babe56\r\n- TensorFlow version (use command below): 1.15\r\n- Python version: python 3.5\r\n- CUDA/cuDNN version: cuda 10.0 cudnn 7.6.2.24\r\n- GPU model and memory: V100 32GB\r\n\r\n**Describe the current behavior**\r\nI try to use keras API for image classification model training and I'm testing the performance of `MirroredStrategy`. Everything works ordinarily except for keras model saving, which is extremely slow. Typically saving a model with resnet_v1.5-50 as backbone can elapsed hundreds of seconds.\r\n\r\nHere's the saving time for each layers in resnet_v1.5-50, I found that saving `keras.layers.BatchNormalization` is the bottleneck:\r\n```\r\nLayer res5c_branch2b K.batch_get_value() for weight aggregation elapsed 0.00420689582824707s\r\nSave layer res5c_branch2b weights elapsed 0.010241508483886719s\r\nLayer bn5c_branch2b K.batch_get_value() for weight aggregation elapsed 6.323298692703247s\r\nSave layer bn5c_branch2b weights elapsed 6.325744867324829s\r\nLayer activation_47 K.batch_get_value() for weight aggregation elapsed 3.337860107421875e-05s\r\nSave layer activation_47 weights elapsed 0.002166748046875s\r\n```\r\nAfter checking source codes of `BatchNormalization` and `MirroredStrategy`, I guess this bottleneck may be caused by synchronization between multiple replicas for parameters `moving_mean` and `moving_variance`. These two parameters use `SyncOnReadVariable` with `aggregation=VariableAggregation.MEAN` under `MirroredStrategy` context, and saving these two parameters needs synchronization, which is slow in my runtime environment.\r\n\r\nThe source codes which may be related to this problem:\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/python/keras/layers/normalization.py#L393\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/python/distribute/values.py#L1230\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/python/distribute/values.py#L1151\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/python/distribute/mirrored_strategy.py#L524\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/python/distribute/mirrored_strategy.py#L734\r\n\r\nrelated issue: https://github.com/keras-team/keras/issues/9298\r\n\r\nI didn't test tensorflow 2.0 version, but I found no changes in terms of `moving_mean`, `moving_variance` and `SyncOnReadVariable`, so I guess this problem may still exists in 2.0 version.\r\n", "comments": ["A trick to bypass this problem is that, we can save only the first replica of parameters `moving_mean` and `moving_variance`. Currently horovod does not support synchronization of these two parameters, saving model under horovod context works perfectly (as fast as expected). So I tried to add a monkey-patch to change attribute `aggregation` from `VariableAggregation.MEAN` to `VariableAggregation.ONLY_FIRST_REPLICA` to avoid synchronization during model saving.\r\n```\r\nimport ast\r\n\r\ndef _replace_variable_aggregation(tree):\r\n\r\n  class RewriteVariableAggregation(ast.NodeTransformer):\r\n\r\n    def visit_Call(self, node):\r\n      \"\"\"replace if this node is calling `add_weight` method.\"\"\"\r\n      # get function name\r\n      if isinstance(node.func, ast.Attribute):\r\n        function_name = node.func.attr\r\n      else:\r\n        function_name = node.func.id\r\n      if function_name != 'add_weight':\r\n        # do nothing\r\n        return node\r\n      is_sync_on_read = False\r\n      for key_idx, keyword in enumerate(node.keywords):\r\n        # check if keyword `synchronization` is presented and its value\r\n        if keyword.arg == 'synchronization' and getattr(keyword.value, 'attr', None) == 'ON_READ':\r\n          # this variable use `VariableSynchronization.ON_READ` as synchronization\r\n          is_sync_on_read = True\r\n          break\r\n      if not is_sync_on_read:\r\n        # do nothing\r\n        return node\r\n      for key_idx, keyword in enumerate(node.keywords):\r\n        # find keyword `aggregation`\r\n        if keyword.arg == 'aggregation' and keyword.value.attr == 'MEAN':\r\n          # replace value attr `MEAN` by `ONLY_FIRST_REPLICA`\r\n          keyword.value.attr = 'ONLY_FIRST_REPLICA'\r\n      return node\r\n\r\n  tree = RewriteVariableAggregation().visit(tree)\r\n  return tree\r\n\r\n\r\ndef _monkey_patch_batch_normalization():\r\n  import inspect\r\n  from tensorflow.python.keras import layers as python_layers\r\n  from tensorflow.python.keras.layers import normalization, normalization_v2\r\n\r\n  # get AST for class `BatchNormalizationBase`\r\n  bn_base_cls_ast = ast.parse(inspect.getsource(normalization.BatchNormalizationBase))\r\n\r\n  def _find_function_build(tree):\r\n    # find AST for method `BatchNormalizationBase.build`\r\n    bn_build_ast = None\r\n    bn_build_ast_idx = 0\r\n    for idx, func_body in enumerate(tree.body[0].body):\r\n      if not isinstance(func_body, ast.FunctionDef):\r\n        continue\r\n      if func_body.name == 'build':\r\n        bn_build_ast = func_body\r\n        bn_build_ast_idx = idx\r\n        break\r\n    return bn_build_ast, bn_build_ast_idx\r\n\r\n  # recursively replace `VariableAggregation.MEAN` with `VariableAggregation.ONLY_FIRST_REPLICA` for all nodes in bn_build_ast\r\n  bn_build_ast, bn_build_ast_idx = _find_function_build(bn_base_cls_ast)\r\n  bn_build_ast = _replace_variable_aggregation(bn_build_ast)\r\n  bn_base_cls_ast.body[0].body[bn_build_ast_idx] = bn_build_ast\r\n  ast.fix_missing_locations(bn_base_cls_ast)\r\n\r\n  # apply new bn code\r\n  exec(compile(bn_base_cls_ast, '<string>', 'exec'), normalization.__dict__)\r\n\r\n  # recompile code for keras.layers.BatchNormalization v1 and v2 since its parant class is changed\r\n  bn_cls_v1_ast = ast.parse(inspect.getsource(normalization.BatchNormalization))\r\n  exec(compile(bn_cls_v1_ast, '<string>', 'exec'), normalization.__dict__)\r\n  bn_cls_v2_ast = ast.parse(inspect.getsource(normalization_v2.BatchNormalization))\r\n  exec(compile(bn_cls_v2_ast, '<string>', 'exec'), normalization_v2.__dict__)\r\n\r\n  # handle keras_export decorator and change keras API access\r\n  from tensorflow.python.util.tf_export import keras_export\r\n  keras_export(v1=['keras.layers.BatchNormalization'])(normalization.BatchNormalization)\r\n  python_layers.BatchNormalization = normalization.BatchNormalization\r\n  tf.compat.v1.keras.layers.BatchNormalization = normalization.BatchNormalization\r\n  keras_export('keras.layers.BatchNormalization', v1=[])(normalization_v2.BatchNormalization)\r\n  python_layers.BatchNormalizationV2 = normalization_v2.BatchNormalization\r\n  # for keras v2 API\r\n  tf_major_version = int(tf.__version__.split('.')[0])\r\n  if tf_major_version >= 2:\r\n    import tensorflow.keras as keras_api\r\n    keras_api.layers.BatchNormalization = normalization_v2.BatchNormalization\r\n    tf.compat.v2.keras.layers.BatchNormalization = normalization_v2.BatchNormalization\r\n\r\n  tf.compat.v1.logging.info(\r\n        ('Using monkey-patched version of keras.layers.BatchNormalization. '\r\n        'Disable synchronization of moving variables during keras model saving.'))\r\n```\r\n\r\nIn eager-execution mode this patch works, and the saving time is indeed reduced compared to `aggregation=VariableAggregation.MEAN`, but in tf 1.x static graph mode it fails with the error below:\r\n```\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py\", line 1211, in save\r\n    saving.save_model(self, filepath, overwrite, include_optimizer, save_format)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/saving/save.py\", line 113, in save_model\r\n    model, filepath, overwrite, include_optimizer)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 105, in save_model_to_hdf5\r\n    save_weights_to_hdf5_group(model_weights_group, model_layers)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 687, in save_weights_to_hdf5_group\r\n    weight_values = K.batch_get_value(weights)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py\", line 3010, in batch_get_value\r\n    return get_session(tensors).run(tensors)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 950, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1158, in _run\r\n    self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 474, in __init__\r\n    self._fetch_mapper = _FetchMapper.for_fetch(fetches)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 264, in for_fetch\r\n    return _ListFetchMapper(fetch)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 373, in __init__\r\n    self._mappers = [_FetchMapper.for_fetch(fetch) for fetch in fetches]\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 373, in <listcomp>\r\n    self._mappers = [_FetchMapper.for_fetch(fetch) for fetch in fetches]\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 274, in for_fetch\r\n    return _ElementFetchMapper(fetches, contraction_fn)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 307, in __init__\r\n    (fetch, type(fetch), str(e)))\r\nTypeError: Fetch argument SyncOnReadVariable:{\r\n  0 /replica:0/task:0/device:GPU:0: <tf.Variable 'bn_conv1/moving_mean:0' shape=(64,) dtype=float32>,\r\n  1 /replica:0/task:0/device:GPU:1: <tf.Variable 'bn_conv1/moving_mean/replica_1:0' shape=(64,) dtype=float32>,\r\n  2 /replica:0/task:0/device:GPU:2: <tf.Variable 'bn_conv1/moving_mean/replica_2:0' shape=(64,) dtype=float32>,\r\n  3 /replica:0/task:0/device:GPU:3: <tf.Variable 'bn_conv1/moving_mean/replica_3:0' shape=(64,) dtype=float32>,\r\n  4 /replica:0/task:0/device:GPU:4: <tf.Variable 'bn_conv1/moving_mean/replica_4:0' shape=(64,) dtype=float32>,\r\n  5 /replica:0/task:0/device:GPU:5: <tf.Variable 'bn_conv1/moving_mean/replica_5:0' shape=(64,) dtype=float32>,\r\n  6 /replica:0/task:0/device:GPU:6: <tf.Variable 'bn_conv1/moving_mean/replica_6:0' shape=(64,) dtype=float32>,\r\n  7 /replica:0/task:0/device:GPU:7: <tf.Variable 'bn_conv1/moving_mean/replica_7:0' shape=(64,) dtype=float32>\r\n} has invalid type <class 'tensorflow.python.distribute.values.SyncOnReadVariable'>, must be a string or Tensor. (Can not convert a ResourceVariable into a Tensor or Operation.)\r\n```\r\nSo I wonder what is this error, and is there a way to at least bypass this saving problem in static graph mode?", "@yuefengz @guptapriya mind to take a look? Seems it's an issue on BatchNorm with MirroredStrategy.", "Hi, It is expected for the batch norm layer to trigger syncing on the variables before checkpointing. It should not take very long, certainly not any longer than how long it takes to sync gradients at each step. \r\nCan you provide the following so we debug this further:\r\n- your training code. you said you used the model from official but the training code is yours. Can you share it so we can see what might be different than the official training code.\r\n- Can you test with TF 2.0 as well? (batch norm code doesn't change there, but there are a number of other changes in keras and saving etc) \r\n- Can you share your setup? How many GPUs do you have? what type of connection is available between your GPUs? Are you using NCCL? Or would the all-reduce be happening via CPU? \r\n", "@guptapriya Thank you for your response! Unfortunately I can't share my code as it's a part of our product, but I can provide you later a short snippet to reproduce this problem. I'm about to test TF 2.0 to see if this bug still exists. Currently I have 8 V100 32G gpus and I use MirroredStrategy with NCCL, and I observed no difference between NCCL mode and AUTO mode in terms of this issue.", "Btw, I found another solution to temporarily bypass this problem by only saving the first replica of model. Although saving checkpoint without synchronization is undesirable, but as long as we have enough batch size in a single GPU and we do data shuffling, the estimation of moving mean and variance in a single replica will be very close to the values after synchronization.\r\n\r\nI post my solution below for others who may also encounter this problem:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\n\r\n_TENSORFLOW_MAJOR_VERSION = int(tf.__version__.split('.')[0])\r\n_TENSORFLOW_MINOR_VERSION = int(tf.__version__.split('.')[1])\r\n\r\n# modification of keras.callbacks.ModelCheckpoint._save_model() method\r\n\r\ndef _save_model(self, epoch, logs):\r\n    \"\"\"Saves the model with include_optimizer flag.\"\"\"\r\n    def _save_model(model, filepath, include_optimizer=True, save_weights_only=False):\r\n      if save_weights_only:\r\n        if K.in_multi_worker_mode():\r\n          model._ckpt_saved_epoch = epoch\r\n        model.save_weights(filepath, overwrite=True)\r\n      else:\r\n        model.save(filepath, overwrite=True, include_optimizer=include_optimizer)\r\n\r\n    def _save_first_replica_only(model, filepath, include_optimizer=True, save_weights_only=False):\r\n      \"\"\"only execute _save_model in first replica.\"\"\"\r\n      context = tf.distribute.get_replica_context()\r\n      if context is None:\r\n        raise ValueError('_save_first_replica_only() must be called in a replica context.')\r\n      replica_id = context.replica_id_in_sync_group\r\n      if replica_id == 0:\r\n        # this is the first replica, save this replica\r\n        _save_model(model,\r\n                    filepath,\r\n                    include_optimizer=include_optimizer,\r\n                    save_weights_only=save_weights_only)\r\n\r\n    logs = logs or {}\r\n\r\n    if isinstance(self.save_freq,\r\n                  int) or self.epochs_since_last_save >= self.period:\r\n      self.epochs_since_last_save = 0\r\n      # API changed starting from tf version 1.15\r\n      # See https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/python/keras/callbacks.py#L987\r\n      if _TENSORFLOW_MAJOR_VERSION >= 2 or (_TENSORFLOW_MAJOR_VERSION == 1 and _TENSORFLOW_MINOR_VERSION >= 15):\r\n        filepath = self._get_file_path(epoch, logs)\r\n      else:\r\n        file_handle, filepath = self._get_file_handle_and_path(epoch, logs)\r\n\r\n      if self.save_best_only:\r\n        current = logs.get(self.monitor)\r\n        if current is None:\r\n          logging.warning('Can save best model only with %s available, '\r\n                          'skipping.', self.monitor)\r\n        else:\r\n          if self.monitor_op(current, self.best):\r\n            if self.verbose > 0:\r\n              print('\\nEpoch %05d: %s improved from %0.5f to %0.5f,'\r\n                    ' saving model to %s' % (epoch + 1, self.monitor, self.best,\r\n                                             current, filepath))\r\n            self.best = current\r\n            if self._save_first_replica_only and self.model._distribute_strategy is not None:\r\n              self.model._distribute_strategy.experimental_run_v2(\r\n                  _save_first_replica_only,\r\n                  args=(self.model, filepath),\r\n                  kwargs={\r\n                      'save_weights_only': self.save_weights_only,\r\n                  })\r\n            else:\r\n              _save_model(self.model,\r\n                          filepath,\r\n                          save_weights_only=self.save_weights_only)\r\n          else:\r\n            if self.verbose > 0:\r\n              print('\\nEpoch %05d: %s did not improve from %0.5f' %\r\n                    (epoch + 1, self.monitor, self.best))\r\n      else:\r\n        if self.verbose > 0:\r\n          print('\\nEpoch %05d: saving model to %s' % (epoch + 1, filepath))\r\n        if self._save_first_replica_only and self.model._distribute_strategy is not None:\r\n          self.model._distribute_strategy.experimental_run_v2(\r\n              _save_first_replica_only,\r\n              args=(self.model, filepath),\r\n              kwargs={\r\n                  'save_weights_only': self.save_weights_only,\r\n              })\r\n        else:\r\n          _save_model(self.model,\r\n                      filepath,\r\n                      save_weights_only=self.save_weights_only)\r\n\r\n      # API changed starting from tf version 1.15\r\n      # See https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/python/keras/callbacks.py#L1017\r\n      if _TENSORFLOW_MAJOR_VERSION >= 2 or (_TENSORFLOW_MAJOR_VERSION == 1 and _TENSORFLOW_MINOR_VERSION >= 15):\r\n        self._maybe_remove_file()\r\n      else:\r\n        self._maybe_remove_file(file_handle, filepath)\r\n```\r\n\r\n```", "Updates: I've tested my codes in TF 2.0 version. The saving time reduced from at least 10 mins to about 3 mins, which is acceptable but still has room for improvement. I'll update a minimal reproducible example later.", "Thank you for the update, good to know it's better with 2.0. Can you compare the numbers with and without batch norm? 3 mins might still be too much if it's primarily coming from batch norm. If you have NCCL then it should not be taking that long (how much is your step time? There is typically gradient aggregation happening at each step so I suspect that would be slow as well, if the sync at the end is slow). \r\n\r\nCode to repro would be very helpful, thanks.", "> Thank you for the update, good to know it's better with 2.0. Can you compare the numbers with and without batch norm? 3 mins might still be too much if it's primarily coming from batch norm. If you have NCCL then it should not be taking that long (how much is your step time? There is typically gradient aggregation happening at each step so I suspect that would be slow as well, if the sync at the end is slow).\r\n> \r\n> Code to repro would be very helpful, thanks.\r\n\r\nSince the code path uses auto mixed precision, I suspect this has been fixed by cc9938e891ea387939c85238c9316f0c0dbb6e1a. Ping @anj-s @guptapriya; I will double check with the latest nightly.\r\n\r\nSee https://github.com/tensorflow/tensorflow/pull/32245#issuecomment-536239014 for the context.", "> > Thank you for the update, good to know it's better with 2.0. Can you compare the numbers with and without batch norm? 3 mins might still be too much if it's primarily coming from batch norm. If you have NCCL then it should not be taking that long (how much is your step time? There is typically gradient aggregation happening at each step so I suspect that would be slow as well, if the sync at the end is slow).\r\n> > Code to repro would be very helpful, thanks.\r\n> \r\n> Since the code path uses auto mixed precision, I suspect this has been fixed by [cc9938e](https://github.com/tensorflow/tensorflow/commit/cc9938e891ea387939c85238c9316f0c0dbb6e1a). Ping @anj-s @guptapriya; I will double check with the latest nightly.\r\n> \r\n> See [#32245 (comment)](https://github.com/tensorflow/tensorflow/pull/32245#issuecomment-536239014) for the context.\r\n\r\nTested with this nightly version in both fp32 and fp16 setup, but no improvement in terms of saving time:\r\n\r\nfp32: ~ 3 mins\r\nwith \"mixed_float16\" policy: ~ 6 mins", "Are you able to provide code for repro? \r\nAlso, how much is saving time without batch norm layer?", "Also can you generate a profile for the saving part? Some documentation for profiling: https://www.tensorflow.org/tensorboard/r2/tensorboard_profiling_keras#profiler_apis", "@bearsroom based on what you said `I found another solution to temporarily bypass this problem by only saving the first replica of model.`, looks like the allreduces of the variables caused the slowness. I think it may be worthwhile to get a profile to see why these allreduces are slow.", "i have the same issue with tensorflow 1.12", "I observe this issue too. Below is my colab profiling showing that a model with BatchNormalization (ResNet50V2) is taking much longer to save when MirroredStrategy is used.\r\n\r\nhttps://gist.github.com/kyamagu/695c127f7f457b5a5cee4aaaa80b6336", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 32652, "title": "fix typo", "body": "a typo", "comments": []}, {"number": 32651, "title": "[TF1.14][TPU]Can not use custom TFrecord dataset on Colab using TPU", "body": "I have created a TFRecord dataset file consisting elements and their corresponding labels. I want to use it for training model on Colab using free TPU. I can load the TFRecord file and even run an iterator just to see the contents however, before the beginning of the epoch it throws following error- \r\n````\r\nUnimplementedError: From /job:worker/replica:0/task:0:\r\nFile system scheme '[local]' not implemented (file: '/content/gdrive/My Drive/data/encodeddata_inGZIP.tfrecord')\r\n\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\r\n\t [[RemoteCall]]\r\n\t [[IteratorGetNextAsOptional_1]]\r\n````\r\nIn my understanding, it wants the TFRecord file on the TPU bucket, I don't know how to do that on Colab. How can one use a TFRecord file directly on Colab TPU?", "comments": ["@rishabhsahrawat ,\r\nThanks for reporting the issue, Can you please provide a simple standalone code to reproduce the issue  or share us your colab gist. Also mention the TF version being used.Thanks!", "HELLO @oanush , I am using TF1.14, TPU, Python 2. Here is the Colab file [link](https://drive.google.com/open?id=1nQi-DkqpaXj4NCNR3_EBbx_kNJr1BLV2) for reproducing the error. In the code, it requires a `.tfrecord` data file, which is available [here](https://drive.google.com/open?id=126IjSpvmYEE7ooG7436sgAI9MaH-i7kN) for download. Please have a look and help me as soon as possible I request you. Thank you!", "Issue replicating for TF-1.14.", "Hello @ymodak @oanush , any updates on this?. It is a major issue but no one is responding. Please  help me with this issue or just share a time window for which I must wait.\r\nThank you!", "Hello! Yeah, we don't allow local filesystem access on TPUs right now, and we recommend that you read and write checkpoints and other files on GCS. See our [MNIST on TPU colab](https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/keras_mnist_tpu.ipynb#scrollTo=Hd5zB1G7Y9-7) for an example.\r\n\r\nWith future TensorFlow and TPU releases (2.1+), you will be able to use the `ram://` file system to write temporary files that you don't need to access.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32651\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32651\">No</a>\n", "Hello @frankchn , thank you for your response. In your example link for MNIST data, it uses GCS bucket for storing image and label files, but I do not have a GCS bucket since I am using Google Colab free TPU or, maybe there is a way for free users to access and load the data to GCS.\r\nAlso, you talked about Checkpoints, checkpoints will be created once the model starts training which will require the dataset first.\r\nSo, my question is if there is a way I can use my TFRecord data directly to train a model on Colab using TPU?", "@rishabhsahrawat Unfortunately you have to use GCS to store data going into and out of TPUs. This is a current design limitation for the platform that we are working to resolve, but we don't have anything to announce at the moment.\r\n\r\nYou can sign up for a Google Cloud Platform account with 5GB of free storage and $300 in credits at https://cloud.google.com/free/, so that should be able to provide you with enough credits to get started.", "@frankchn alright, thank you for your answer."]}, {"number": 32650, "title": "Converting unsupported operation ??", "body": "- OS Platform and Distribution ( Linux Ubuntu 18.04):\r\n- TensorFlow installed from (source):\r\n- TensorFlow version (https://github.com/tensorflow/tensorflow):\r\n\r\n\r\n`tensorflow-master$ bazel-bin/tensorflow/lite/toco/toco --input_file=../my_freeze12xshell.pb --output_file=../my12.tflite --output_format=TFLITE --input_shapes=2,4,513 --input_arrays=x_mixed --output_arrays=y_out1,y_out2 --inference_type=QUANTIZED_UINT8 --inference_input_type=FLOAT --std_dev_values=1 --mean_values=0`\r\n\r\nmy pb file is here\r\n\r\n[my_freeze12xshell.zip](https://github.com/tensorflow/tensorflow/files/3630369/my_freeze12xshell.zip)\r\n\r\nany one could help me ?\r\n", "comments": ["the errors are here, so many\r\n\r\n```\r\n2019-09-19 17:17:52.344665: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3\r\n2019-09-19 17:17:52.352375: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-09-19 17:17:52.352443: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3\r\n2019-09-19 17:17:52.352458: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-09-19 17:17:52.352514: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3\r\n2019-09-19 17:17:52.352538: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-09-19 17:17:52.352550: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-09-19 17:17:52.352560: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-09-19 17:17:52.352570: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-09-19 17:17:52.352580: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-09-19 17:17:52.352589: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-09-19 17:17:52.352608: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-09-19 17:17:52.352620: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-09-19 17:17:52.352631: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: LoopCond\r\n2019-09-19 17:17:52.352663: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3\r\n2019-09-19 17:17:52.352674: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-09-19 17:17:52.352682: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-09-19 17:17:52.352690: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-09-19 17:17:52.355377: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-09-19 17:17:52.355402: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-09-19 17:17:52.355431: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-09-19 17:17:52.355443: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-09-19 17:17:52.356399: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-09-19 17:17:52.356428: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-09-19 17:17:52.356457: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-09-19 17:17:52.356469: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-09-19 17:17:52.357420: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-09-19 17:17:52.357441: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-09-19 17:17:52.357468: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-09-19 17:17:52.357480: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-09-19 17:17:52.357501: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3\r\n2019-09-19 17:17:52.357516: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-09-19 17:17:52.357525: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-09-19 17:17:52.357545: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit\r\n2019-09-19 17:17:52.357556: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3\r\n2019-09-19 17:17:52.357577: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3\r\n2019-09-19 17:17:52.358208: F tensorflow/lite/toco/tooling_util.cc:1469] Check failed: batch == 1 (2 vs. 1)\r\nAborted (core dumped)\r\n```", "\"unsupported operation\" is simply an info log.\r\nThe failure is:\r\n\r\n```\r\nTensorArrayGatherV3 2019-09-19 17:17:52.358208: F tensorflow/lite/toco/tooling_util.cc:1469] Check failed: batch == 1 (2 vs. 1) Aborted (core dumped)\r\n```", "From the error message, i see there are v1 control flows and tensorarray ops in your graph, unfortunately those are not (and will not be supported) by TOCO. But we are currently working on a new MLIR-based converter which could address those. Please stay tuned! Thanks.", "> \"unsupported operation\" is simply an info log.\r\n> The failure is:\r\n> \r\n> ```\r\n> TensorArrayGatherV3 2019-09-19 17:17:52.358208: F tensorflow/lite/toco/tooling_util.cc:1469] Check failed: batch == 1 (2 vs. 1) Aborted (core dumped)\r\n> ```\r\n\r\nbatch only can be 1 ??I don't think so !", "\r\n\r\n\r\n> From the error message, i see there are v1 control flows and tensorarray ops in your graph, unfortunately those are not (and will not be supported) by TOCO. But we are currently working on a new MLIR-based converter which could address those. Please stay tuned! Thanks.\r\n\r\nLooking forward to your great project.\r\n", "> \"unsupported operation\" is simply an info log.\r\n> The failure is:\r\n> \r\n> ```\r\n> TensorArrayGatherV3 2019-09-19 17:17:52.358208: F tensorflow/lite/toco/tooling_util.cc:1469] Check failed: batch == 1 (2 vs. 1) Aborted (core dumped)\r\n> ```\r\n\r\nI have changed the batch ,but also have the errors below\r\n\r\n`2019-10-08 11:32:35.873302: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3\r\n2019-10-08 11:32:35.881045: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-10-08 11:32:35.881104: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3\r\n2019-10-08 11:32:35.881119: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-10-08 11:32:35.881177: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3\r\n2019-10-08 11:32:35.881200: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-10-08 11:32:35.881212: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-10-08 11:32:35.881223: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-10-08 11:32:35.881233: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-10-08 11:32:35.881243: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-10-08 11:32:35.881252: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-10-08 11:32:35.881272: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-10-08 11:32:35.881283: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-10-08 11:32:35.881294: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: LoopCond\r\n2019-10-08 11:32:35.881327: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3\r\n2019-10-08 11:32:35.881338: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-10-08 11:32:35.881347: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-10-08 11:32:35.881355: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-10-08 11:32:35.884055: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-10-08 11:32:35.884081: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-10-08 11:32:35.884112: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-10-08 11:32:35.884125: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-10-08 11:32:35.885106: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-10-08 11:32:35.885130: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-10-08 11:32:35.885159: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-10-08 11:32:35.885172: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-10-08 11:32:35.886138: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-10-08 11:32:35.886160: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-10-08 11:32:35.886189: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-10-08 11:32:35.886202: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-10-08 11:32:35.886223: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3\r\n2019-10-08 11:32:35.886240: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter\r\n2019-10-08 11:32:35.886250: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-10-08 11:32:35.886270: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit\r\n2019-10-08 11:32:35.886280: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3\r\n2019-10-08 11:32:35.886302: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3\r\n2019-10-08 11:32:35.888354: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 155 operators, 252 arrays (0 quantized)\r\n2019-10-08 11:32:35.890071: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 155 operators, 252 arrays (0 quantized)\r\n2019-10-08 11:32:35.892621: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 119 operators, 187 arrays (1 quantized)\r\n2019-10-08 11:32:35.894755: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 117 operators, 185 arrays (1 quantized)\r\n2019-10-08 11:32:35.896502: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 117 operators, 185 arrays (1 quantized)\r\n2019-10-08 11:32:35.897481: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 117 operators, 185 arrays (1 quantized)\r\n2019-10-08 11:32:35.898753: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 117 operators, 185 arrays (1 quantized)\r\n2019-10-08 11:32:35.900011: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before default min-max range propagation graph transformations: 117 operators, 185 arrays (1 quantized)\r\n2019-10-08 11:32:35.901080: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After default min-max range propagation graph transformations pass 1: 117 operators, 185 arrays (1 quantized)\r\n2019-10-08 11:32:35.902385: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 117 operators, 185 arrays (1 quantized)\r\nUnimplemented: this graph contains an operator of type (Unsupported TensorFlow op: TensorArrayV3) for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).`\r\n\r\nSo how to deal with it ?\r\nThx", "and the command is here\r\n`tensorflow-master$ bazel-bin/tensorflow/lite/toco/toco --input_file=../my_freeze5.pb --output_file=../my5.tflite --output_format=TFLITE --input_shapes=1,5,513 --input_arrays=x_mixed --output_arrays=y_out1,y_out2 --inference_type=QUANTIZED_UINT8 --inference_input_type=QUANTIZED_UINT8 --std_dev_values=1 --mean_values=0 --default_ranges_min=-1 --default_ranges_max=1`", "Now I have some new errors\r\nhow to deal with it ?\r\n`2019-10-09 20:23:58.705608: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorListFromTensor\r\n2019-10-09 20:23:58.705645: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-10-09 20:23:58.705670: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: While\r\n2019-10-09 20:23:58.705693: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-10-09 20:23:58.705699: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-10-09 20:23:58.705714: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorListStack\r\n2019-10-09 20:23:58.705934: F tensorflow/lite/toco/tooling_util.cc:1041] Check failed: array->has_shape() \r\nFatal Python error: Aborted`\r\n\r\nwho could help me will be appreciated.\r\n\r\n", "that is\r\n`Converting unsupported operation: TensorListFromTensor`\r\n`Unsupported data type in placeholder op: 21`\r\n`Converting unsupported operation: While`\r\n`Converting unsupported operation: TensorListStack`\r\n`Check failed: array->has_shape() `\r\n\r\nhow to solve ?\r\n", "@ucasiggcas I am also getting same type of error. \r\nWere you able to fix this?", "Can you try the new TF Lite converter by setting:\r\nconverter.experimental_new_converter = True\r\n\r\nTry it with tf-nightly pip package.", "@ucasiggcas I was getting the same error but this code worked for me:\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                                                tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n\r\nI hope this helps.", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32650\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32650\">No</a>\n"]}, {"number": 32649, "title": "Add load error", "body": "Let developers know the failure reason.", "comments": []}, {"number": 32648, "title": "[Intel MKL] Transpose + Maxpool3D + Transpose fusion", "body": "Another pattern for transpose fusion.\r\n\r\nRelated PRs:\r\n\r\n1. https://github.com/tensorflow/tensorflow/pull/23152\r\n2. https://github.com/tensorflow/tensorflow/pull/29431", "comments": ["@wenxizhu Could you please check reviewer comments and keep us posted. Thanks!", "@gbaned I was told in PR https://github.com/tensorflow/tensorflow/pull/32541 that @wenxizhu won't be working on this anymore. Let's wait until someone else from Intel takes over.\r\n@guizili0 Just FYI, this PR is also by @wenxizhu.", "Hi @penpornk , I'll take Wenxi's work. ", "@Zantares Thank you!", "Hi @pengchongjin , sorry for the delay because I spent sometime to understand Wenxi's work. now I have updated it according to your review, please take a look again, thanks!", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32648) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32648) for more info**.\n\n<!-- ok -->"]}]