[{"number": 4912, "title": "\"no module named tensorflow\" when I \"import tensorflow\" after I install tensorflow on windows successfully", "body": "I followed ..\\tensorflow\\contrib\\cmake\\README to install tensorflow on windows. Everything went well and I installed tensorflow successfully. But after I \"activate tensorflow\", and tried to \"import tensorflow\" using python, it went wrong, saying \"no module named tensorflow\". How can I fix this problem? Many thanks!\n", "comments": ["@mrry might have an answer, but we do not officially have support for windows yet.\nTherefore, pip packages built might not be working on windows.\n", "Can you please share the exact commands that you ran between \"activating tensorflow\" and \"importing tensorflow\"? \n", "@mrry \nI followed the README: \"Invoke MSBuild to build TensorFlow\", run \n'MSBuild /p:Configuration=Release tf_python_build_pip_package.vcxproj on cmd.exe'\nAnd it It compiled.\n\nThen I \"activate tensorflow\", and open the python.exe.\nAnd I \"import tensorflow\", it said \"no module named tensorflow\".\n\nIs there anything that I have missed?  Thanks\n", "What command do you run to \"activate tensorflow\"? Is this an Anaconda command?\n\nIt sounds like there's a step missing where I'd expect you to install the PIP package. Something like the following (where `%TF_PIP_PACKAGE_FILENAME%` is replaced with the path to the PIP package that you just built):\n\n```\nC:\\...\\> pip install %TF_PIP_PACKAGE_FILENAME%\n```\n", "@mrry \nExcuse me, after pip being successfully built, what the next step shall I do? Yes, of course, I use Anaconda.\n\nThanks for your generous help!\n", "After you run the above `pip install` command, you should be able to run `python` and `import tensorflow as tf` should succeed.\n", "Probably, documentation should be temporarily be changed to \"Docker flow\" for Windows - at least until `pip` is fully working on Windows.\n", "hope tensorflow officially supports Windows soon~\n", "Did you try to `pip install` the built wheel file, as I suggested [here](https://github.com/tensorflow/tensorflow/issues/4912#issuecomment-256356781)?\n", "You can follow #17 for more updates on windows support. I will close this issue to merge all TF on Windows conversation.\n", "Hi, I have laid down step-by-step instructions to successfully install Tensorflow on Windows.\r\n\r\nhttps://github.com/bhavsarpratik/install_Tensorflow_GPU_windows", "Use python 3.5 or less. If Anaconda, you can use one of the three ways here. I used the second one. https://docs.anaconda.com/anaconda/faq#how-do-i-get-the-latest-anaconda-with-python-3-5"]}, {"number": 4911, "title": "Greenish image when using convertYUV420ToARGB8888", "body": "I'm experimenting the [TensorFlow Android Camera Demo](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android). but it seems that the [ImageUtils.convertYUV420ToARGB8888](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowImageListener.java#L164) used to convert images from the camera input to RGB  bytes array is not handling very well the [YUV_420_888 format](https://developer.android.com/reference/android/graphics/ImageFormat.html).\n\nHere is the output of `ImageUtils.saveBitmap(rgbFrameBitmap);`\n\n![](http://i.imgur.com/bsL1czy.png)\n", "comments": ["Interesting, I noticed that the resulting bmp is not just greenish, it's completely green -- all R and B values are 0.\n\nI'd theorize that either your device is not providing a 420sp camera frame or is not providing an actual argb8888 output bitmap, as I'm unable to replicate this locally. Have you made any other modifications to the code? What sort of device are you using?\n\nTo get a little bit more detail, could you add some logging in YUV2RGB() in [yuv2rgb.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/jni/yuv2rgb.cc), and see what both the input and output values look like?\n", ">  your device is not providing a 420sp camera frame or is not providing an actual argb8888 output bitmap\n\n`LOGGER.i(\"IMAGE FORMAT: \" + String.valueOf(image.getFormat()));` Outputs `IMAGE FORMAT: 35`. refering to [this page](https://developer.android.com/reference/android/graphics/ImageFormat.html), it is the YUV_420_888 format.\n\nAll [this code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowImageListener.java#L122-L186) is exactly the same. I've just added a debug line after seeing that my model's output is incoh\u00e9rente.\n\n`ImageUtils.saveBitmap(rgbFrameBitmap);`\n\nI've added some logging to the YUV2RGB() function.\n\n``` cpp\nstatic inline uint32 YUV2RGB(int nY, int nU, int nV) {\n\n  LOG(INFO) << \"nY: \" << nY << \", \"\n            << \"nU: \" << nU << \", \"\n            << \"nV: \" << nV << \"\\n\";\n  nY -= 16;\n  nU -= 128;\n  nV -= 128;\n  if (nY < 0) nY = 0;\n\n  // This is the floating point equivalent. We do the conversion in integer\n  // because some Android devices do not have floating point in hardware.\n  // nR = (int)(1.164 * nY + 2.018 * nU);\n  // nG = (int)(1.164 * nY - 0.813 * nV - 0.391 * nU);\n  // nB = (int)(1.164 * nY + 1.596 * nV);\n\n  int nR = (int)(1192 * nY + 1634 * nV);\n  int nG = (int)(1192 * nY - 833 * nV - 400 * nU);\n  int nB = (int)(1192 * nY + 2066 * nU);\n\n  LOG(INFO) << \"nR: \" << nR << \", \"\n            << \"nG: \" << nG << \", \"\n            << \"nB: \" << nB << \"\\n\";\n\n\n  nR = MIN(kMaxChannelValue, MAX(0, nR));\n  nG = MIN(kMaxChannelValue, MAX(0, nG));\n  nB = MIN(kMaxChannelValue, MAX(0, nB));\n\n  nR = (nR >> 10) & 0xff;\n  nG = (nG >> 10) & 0xff;\n  nB = (nB >> 10) & 0xff;\n\n  LOG(INFO) << \"nR: \" << nR << \", \"\n            << \"nG: \" << nG << \", \"\n            << \"nB: \" << nB << \"\\n\\n\";\n\n  return 0xff000000 | (nR << 16) | (nG << 8) | nB;\n}\n\n```\n\nHere is the output\n\n```\n...\n10-14 10:31:59.406 28676-28716/? I/native: yuv2rgb.cc:33 nY: 69, nU: 0, nV: 0\n10-14 10:31:59.406 28676-28716/? I/native: yuv2rgb.cc:51 nR: -145976, nG: 221000, nB: -201272\n10-14 10:31:59.406 28676-28716/? I/native: yuv2rgb.cc:64 nR: 0, nG: 215, nB: 0\n10-14 10:31:59.406 28676-28716/? I/native: yuv2rgb.cc:33 nY: 71, nU: 0, nV: 0\n10-14 10:31:59.406 28676-28716/? I/native: yuv2rgb.cc:51 nR: -143592, nG: 223384, nB: -198888\n10-14 10:31:59.406 28676-28716/? I/native: yuv2rgb.cc:64 nR: 0, nG: 218, nB: 0\n10-14 10:31:59.406 28676-28716/? I/native: yuv2rgb.cc:33 nY: 75, nU: 0, nV: 0\n10-14 10:31:59.406 28676-28716/? I/native: yuv2rgb.cc:51 nR: -138824, nG: 228152, nB: -194120\n10-14 10:31:59.406 28676-28716/? I/native: yuv2rgb.cc:64 nR: 0, nG: 222, nB: 0\n10-14 10:31:59.406 28676-28716/? I/native: yuv2rgb.cc:33 nY: 78, nU: 0, nV: 0\n10-14 10:31:59.406 28676-28716/? I/native: yuv2rgb.cc:51 nR: -135248, nG: 231728, nB: -190544\n10-14 10:31:59.406 28676-28716/? I/native: yuv2rgb.cc:64 nR: 0, nG: 226, nB: 0\n10-14 10:31:59.406 28676-28716/? I/native: yuv2rgb.cc:33 nY: 80, nU: 0, nV: 0\n10-14 10:31:59.406 28676-28716/? I/native: yuv2rgb.cc:51 nR: -132864, nG: 234112, nB: -188160\n10-14 10:31:59.411 28676-28716/? I/native: yuv2rgb.cc:64 nR: 0, nG: 228, nB: 0\n10-14 10:31:59.411 28676-28716/? I/native: yuv2rgb.cc:33 nY: 80, nU: 0, nV: 0\n10-14 10:31:59.411 28676-28716/? I/native: yuv2rgb.cc:51 nR: -132864, nG: 234112, nB: -188160\n10-14 10:31:59.411 28676-28716/? I/native: yuv2rgb.cc:64 nR: 0, nG: 228, nB: 0\n10-14 10:31:59.411 28676-28716/? I/native: yuv2rgb.cc:33 nY: 80, nU: 0, nV: 0\n10-14 10:31:59.411 28676-28716/? I/native: yuv2rgb.cc:51 nR: -132864, nG: 234112, nB: -188160\n10-14 10:31:59.411 28676-28716/? I/native: yuv2rgb.cc:64 nR: 0, nG: 228, nB: 0\n10-14 10:31:59.411 28676-28716/? I/native: yuv2rgb.cc:33 nY: 78, nU: 0, nV: 0\n10-14 10:31:59.411 28676-28716/? I/native: yuv2rgb.cc:51 nR: -135248, nG: 231728, nB: -190544\n10-14 10:31:59.411 28676-28716/? I/native: yuv2rgb.cc:64 nR: 0, nG: 226, nB: 0\n...\n```\n", "A dump of `yuvBytes` just before [`ImageUtils.convertYUV420ToARGB8888()`](https://github.com/tensorflow/tensorflow/blob/754048a0453a04a761e112ae5d99c149eb9910dd/tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowImageListener.java#L164) call\n\n```\n10-14 10:57:41.551 12667-12709/org.tensorflow.demo I/tensorflow: TensorFlowImageListener: yuvBytes[0]: [16, 76, 76, 76, 76, 76, 75, 73, 70, 64, 65, 70, 74, 76, 76, 77, 77, 77, 77, 78, ...]\n10-14 10:57:41.571 12667-12709/org.tensorflow.demo I/tensorflow: TensorFlowImageListener: yuvBytes[1]: [-128, -125, -124, -124, -125, -125, -125, -125, -125, -125, -125, -126, -126, ...]\n10-14 10:57:41.586 12667-12709/org.tensorflow.demo I/tensorflow: TensorFlowImageListener: yuvBytes[2]: [-128, 123, 123, 122, 122, 123, 123, 124, 124, 124, 124, 123, 124, 125, 125, 125, ...]\n```\n", "I think that this issue is related to this one [#306](https://github.com/tensorflow/tensorflow/issues/306).\n\nIt works when I updated to 6.0.1.\n\nThe version I have before was not really old (5.0.2)\n"]}, {"number": 4910, "title": "Ubuntu 16.04 + CUDA8.0, GPU build from source: C++ compilation fails (crosstool_wrapper_driver_is_not_gcc failed)", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\n[Issue 190](https://github.com/tensorflow/tensorflow/issues/190) is the same, but was closed & pointed to a [bazel issue that has been fixed](https://github.com/bazelbuild/bazel/issues/359).\n### Environment info\n\nOperating System: **Ubuntu 16.04**\n\nInstalled version of CUDA and cuDNN: **8.0.44 + 5.1.5**\n\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n-rw-r--r-- 1 root root   558720 Okt 11 19:24 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Okt 11 19:24 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root       19 Okt 11 19:24 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\n-rwxr-xr-x 1 root root   415432 Okt 11 19:24 /usr/local/cuda/lib64/libcudart.so.8.0.44\n-rw-r--r-- 1 root root   775162 Okt 11 19:24 /usr/local/cuda/lib64/libcudart_static.a\nlrwxrwxrwx 1 root root       13 Okt 11 19:47 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5\nlrwxrwxrwx 1 root root       17 Okt 11 19:47 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5\n-rwxr-xr-x 1 root root 79337624 Okt 11 19:47 /usr/local/cuda/lib64/libcudnn.so.5.1.5\n-rw-r--r-- 1 root root 69756172 Okt 11 19:47 /usr/local/cuda/lib64/libcudnn_static.a\n```\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`): **1975cd1e9d539e75a1b85b56f16448c91ef88d90**\n2. The output of `bazel version`: **0.3.2**\n\n```\nBuild label: 0.3.2\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Oct 7 17:25:10 2016 (1475861110)\nBuild timestamp: 1475861110\nBuild timestamp as int: 1475861110\n```\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nIt occurred when running both training example & pip build (same error message):\n\n```\nbazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n```\n### What other attempted solutions have you tried?\n\nI have followed both [this tutorial](http://www.computervisionbytecnalia.com/en/2016/06/deep-learning-development-setup-for-ubuntu-16-04-xenial/) and [this one](https://alliseesolutions.wordpress.com/2016/09/08/install-gpu-tensorflow-from-sources-w-ubuntu-16-04-and-cuda-8-0-rc/).\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n\nLast message before the error was:\n\n```\nINFO: From Compiling tensorflow/core/kernels/string_split_op.cc:\n```\n\nThe full error message, using `--verbose_failures`:\n\n```\nERROR: /home/sebastien/tensorflow/tensorflow/core/kernels/BUILD:1199:1: C++ compilation of rule '//tensorflow/core/kernels:determinant_op' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command \n  (cd /home/sebastien/.cache/bazel/_bazel_sebastien/f91199c4da2d428eb9d05b40a2d00b4e/execroot/tensorflow && \\\n  exec env - \\\n    LD_LIBRARY_PATH=:/usr/local/cuda/lib64:/usr/progtools/caffe-nv/distribute/lib:/usr/local/cuda/lib64 \\\n    PATH=/usr/local/cuda/bin:/home/sebastien/bin:/home/sebastien/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin \\\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -D_FORCE_INLINES -D_MWAITXINTRIN_H_INCLUDED '-std=c++11' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/determinant_op/tensorflow/core/kernels/determinant_op.d '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/determinant_op/tensorflow/core/kernels/determinant_op.o' -DEIGEN_MPL2_ONLY -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/protobuf -iquote bazel-out/local_linux-opt/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/com_googlesource_code_re2 -iquote bazel-out/local_linux-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/local_linux-opt/genfiles/external/farmhash_archive -iquote external/gif_archive -iquote bazel-out/local_linux-opt/genfiles/external/gif_archive -iquote external/highwayhash -iquote bazel-out/local_linux-opt/genfiles/external/highwayhash -iquote external/jpeg_archive -iquote bazel-out/local_linux-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/local_linux-opt/genfiles/external/zlib_archive -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -iquote external/local_config_cuda -iquote bazel-out/local_linux-opt/genfiles/external/local_config_cuda -isystem external/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/farmhash_archive -isystem bazel-out/local_linux-opt/genfiles/external/farmhash_archive -isystem external/gif_archive -isystem bazel-out/local_linux-opt/genfiles/external/gif_archive -isystem external/highwayhash -isystem bazel-out/local_linux-opt/genfiles/external/highwayhash -isystem external/jpeg_archive -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive -isystem external/png_archive -isystem bazel-out/local_linux-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/local_linux-opt/genfiles/external/zlib_archive -isystem external/eigen_archive -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem external/local_config_cuda/cuda/include -isystem bazel-out/local_linux-opt/genfiles/external/local_config_cuda/cuda/include -isystem external/local_config_cuda/cuda -isystem bazel-out/local_linux-opt/genfiles/external/local_config_cuda/cuda -fno-exceptions -DEIGEN_AVOID_STL_ARRAY '-DGOOGLE_CUDA=1' -pthread '-DGOOGLE_CUDA=1' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers -c tensorflow/core/kernels/determinant_op.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/determinant_op/tensorflow/core/kernels/determinant_op.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nvirtual memory exhausted: Cannot allocate memory\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\n```\n\n[build_error_msg.txt](https://github.com/tensorflow/tensorflow/files/524061/build_error_msg.txt)\n", "comments": ["Thank you for taking the time to send us a report. We don't support CUDA 8.0 yet. It will be supported soon. Right now we only officially support 7.0 - 7.5. Please see #2559 which is tracking this. There's also #4895 which encountered this issue on the same version of Ubuntu.\n"]}, {"number": 4909, "title": "Empty array as loss function causes unhandled exception", "body": "If the loss fed to an optimizer is an empty array and attempt is made to train on a GPU, an unhandled exception occurs that kills the python kernel. The logs reveal the following error message\n\n```\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:03:00.0\nTotal memory: 12.00GiB\nFree memory: 4.89GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0)\nF tensorflow/stream_executor/cuda/cuda_dnn.cc:423] could not set cudnn tensor descriptor: CUDNN_STATUS_BAD_PARAM\n```\n\nCode to reproduce the problem:\n\n``` python\nimport tensorflow as tf\nimport numpy as np\n\n# Define a useless network\nwith tf.Graph().as_default() as graph:\n    placeholder = tf.placeholder(tf.float32)\n    filter = tf.Variable(np.random.gamma(1, 1, (10, 10, 1, 1)).astype(np.float32))\n    loss = tf.nn.conv2d(placeholder, filter, [1, 1, 1, 1], 'VALID')\n    optimizer = tf.train.AdamOptimizer()\n    train_op = optimizer.minimize(loss)\n    init_op = tf.initialize_all_variables()\n\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3)\nsession = tf.Session(graph=graph, config=tf.ConfigProto(gpu_options=gpu_options))\nsession.run(init_op)\n\n\ndef run(x):\n    print(\"loss\", session.run(loss, {placeholder: x}))\n    session.run(train_op, {placeholder: x})\n    print(\"executed one training step\")\n\n\n# This succeeds\nrun(np.ones((1, 10, 10, 1)))\n\n# This kills the kernel because the 'VALID' padding in the convolutional layer\n# leads to an empty array which the optimizer cannot handle\nrun(np.ones((1, 10, 9, 1)))\n```\n", "comments": ["Which version of CUDA are you using?\n", "I am using 7.5.17.\n", "In the past thirty days @oakkas, @vguizilini, and @tongda indicated in #2033 that they ran into this same error. @jimfleming also ran into this error in #3088 but said it was due to an error on his part.\n\n@jimfleming could you please share with us what you did to fix this problem?\n", "Hmm, it's been a while but if I remember correctly the problem was invalid values in the tensor. At some point, my feed tensors were getting corrupted due to image augmentation.\n", "I'm going to mark this as contributions welcome in case anyone wants to add a precondition that will show a more helpful error message in situations like this.\n", "My situation mentioned in #2033 was similar to @jimfleming. It was caused by an accidental operation that feed an empty array to the trainer. It took me some time to find out the root cause. Hope there was some more inspiring message at first.\n", "I've a similar issue when I'm using Keras with TensorFlow. I get this when I call `train_on_batch` twice. \nhttps://github.com/fchollet/keras/issues/4110\n", "This issue is quite old and hasn't had recent activity. If it is still not working in the latest version of TensorFlow, and please create a new bug. Thank you."]}, {"number": 4908, "title": "Revise wrong name in RELEASE.md", "body": "I fixed wrong nickname in RELEASE.md.\n", "comments": ["Can one of the admins verify this patch?\n", "@beopst, thanks for your PR! By analyzing the history of the files in this pull request, we identified @gunan, @vrv and @tensorflower-gardener to be potential reviewers.\n", "@tensorflow-jenkins test this please\n", "Changes should not affect any tests. Should be safe to merge.\n"]}, {"number": 4907, "title": "DeepDream tutorial and TFSlim", "body": "What do you think to switch the DeepDream tutorial  notebook to MetaGraph? I see that  fine tuning and new inception models are more oriented on tfslim and slim models use the new meta+check point.\n", "comments": ["@vrv What do you think of this? I can also try to make a PR but I need some support on how to handle this..\n", "It sounds reasonable.  In addition, there's also support coming for the even more general \"SavedModel\" which builds on MetaGraph, the details of which I'm not familiar.  Assigning this to Sherry who can comment!\n\ncc @sherrym @nfiedel \n", "I see also that models are too inhomogeneous between examples. The InceptionV3 in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/image_retraining is not the same of DeepDream tutorial that it is different from slims models and its finetune instructions.  \n", "I think that we can restore a tfslim model meta and checkpoint using:\n\n```\nnew_saver = tf.train.import_meta_graph(<meta_graph>)\nnew_saver.restore(sess, <checkpoint>)\n```\n\nBut after notebook uses \n\n```\ntf.import_graph_def(graph_def, {'input':t_preprocessed})\n```\n\nI've tryied to substitute with\n\n```\ntf.import_graph_def(graph.as_graph_def(), {'distort_image/ExpandDims':t_preprocessed})\n```\n\nto maintain the next cells code and change only layers names in the notebook but I think that it is not the right way to do it.\n", "Assigning to Pete, since I believe he set up the inception5h.zip which has the raw GraphDef rather than the MetaGraph.  I don't know how to regenerate the MetaGraph for inception easily.  Presumably someone could take an inception model from github.com/tensorflow/models.git and do this for us as a learning exercise, if Pete or someone else doesn't get to it.\n", "@vrv Following [Slim Readme](https://github.com/tensorflow/models/blob/master/slim/README.md) for finetuning we have Meta+Checkpoint. Input tensor it is not named \"input\" like in inception5h and layers name changes. Probably for DeepDream with finetuning on user data we need to propose a larger trainable_scope than \"InceptionV3/Logits,InceptionV3/AuxLogits\".\n", "Also https://github.com/tensorflow/models/blob/master/inception/README.md has cross reference with https://github.com/tensorflow/models/blob/master/slim/README.md but models are different and create a little bit of confusion.\n", "Could be partially related to https://github.com/tensorflow/tensorflow/issues/5036\n", "I am really fighting with getting deepdream running with slim inception model....\r\nWhile it was running with a pre-trained model as is in the tutorial, I trained a model from models/slim/nets/inception_v3, and simply can't get it running at all..\r\n\r\nwhile I got to passing the model with the instructions from bhack:\r\n```\r\nnew_saver = tf.train.import_meta_graph(<meta_graph>)\r\nnew_saver.restore(sess, <checkpoint>)\r\ntf.import_graph_def(graph.as_graph_def(), {'distort_image/ExpandDims':t_preprocessed})\r\n\r\n```\r\nI simply can't get past that point while getting this error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"tensorflowdream_simple.py\", line 165, in <module>\r\n    render_naive('outputs/experiments/naive',T(layer)[:,:,:,channel], img_noise, 40)\r\n  File \"tensorflowdream_simple.py\", line 157, in render_naive\r\n    g, score = sess.run([t_grad, t_score], {t_input:img})\r\n  File \"/home/vilem/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 778, in run\r\n    run_metadata_ptr)\r\n  File \"/home/vilem/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 969, in _run\r\n    fetch_handler = _FetchHandler(self._graph, fetches, feed_dict_string)\r\n  File \"/home/vilem/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 408, in __init__\r\n    self._fetch_mapper = _FetchMapper.for_fetch(fetches)\r\n  File \"/home/vilem/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 230, in for_fetch\r\n    return _ListFetchMapper(fetch)\r\n  File \"/home/vilem/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 337, in __init__\r\n    self._mappers = [_FetchMapper.for_fetch(fetch) for fetch in fetches]\r\n  File \"/home/vilem/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 227, in for_fetch\r\n    (fetch, type(fetch)))\r\nTypeError: Fetch argument None has invalid type <type 'NoneType'>\r\n```\r\n\r\n\r\n\r\nSincerely, when I started with tf, I hoped it would be simple as it can be with such a complex thing as neural networks, and everything looked that way until the point where I wanted to go my own way.The networks aren't hard at all in the end, compared with learning how to read, write a graph, use it, or create own dataset.... ", "@vilemduha from the error I'm guessing `t_grad` and/or `t_score` are `None` instead of being tensors", "yaroslavvb - \r\nExactly...I found that allready by placing a print of the fetches in the session py file.\r\n but I don't get why?\r\nthe code is otherwise actually the same as in tutorial. \r\n\r\nthis is start:\r\n```\r\ncheckpoint_folder = '/media/vilem/RYCHLEJ/nntrain/train/scan3d/all/'\r\n\r\n\r\n    \r\n# We retrieve our checkpoint fullpath\r\ncheckpoint = tf.train.get_checkpoint_state(checkpoint_folder)\r\ninput_checkpoint = checkpoint.model_checkpoint_path\r\n\r\n# We clear devices to allow TensorFlow to control on which device it will load operations\r\nclear_devices = True\r\n\r\n# We import the meta graph and retrieve a Saver\r\nsaver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)\r\n\r\n# We retrieve the protobuf graph definition\r\n#graph = tf.get_default_graph()\r\n#graph_def = graph.as_graph_def()\r\n\r\n# We start a session and restore the graph weights\r\nsess = tf.InteractiveSession()\r\nsaver.restore(sess, input_checkpoint)  \r\ngraph = sess.graph\r\ngraph_def = sess.graph_def\r\n    \r\nt_input = tf.placeholder(np.float32, name='input') # define the input tensor\r\nimagenet_mean = 117.0\r\nt_preprocessed = tf.expand_dims(t_input-imagenet_mean, 0)\r\n\r\n\r\ntf.import_graph_def(graph_def, {'distort_image/ExpandDims':t_preprocessed})\r\n```\r\n\r\nand this is a function call later:\r\n```\r\n\r\nimg_noise = np.random.uniform(size=(299,299,3)) + 100.0\r\ndef render_naive(fname, t_obj, img0=img_noise, iter_n=20, step=1.0):\r\n    t_score = tf.reduce_mean(t_obj) # defining the optimization objective\r\n    t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!\r\n    \r\n    \r\n   \r\n    \r\n    img = img0.copy()\r\n    for i in range(iter_n):\r\n        g, score = sess.run([t_grad, t_score], {t_input:img0})\r\n        # normalizing the gradient, so the same step size should work \r\n        g /= g.std()+1e-8         # for different layers and networks\r\n        img += g*step\r\n        print(score, end = ' ')\r\n    #clear_output()\r\n    showarray(visstd(img), fname)\r\n\r\nrender_naive('outputs/experiments/naive',T(layer)[:,:,:,channel], img_noise, 40)\r\n```", "I forgot to append - \r\nthe printout from the 'fetches' is \r\n[None, <tf.Tensor 'Mean:0' shape=() dtype=float32>]\r\nso t_grad is None.", "@vilemduha Did you ever get the DeepDream tutorial working with the meta graphs?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Closing due to lack of recent activity, but please let me know if I'm mistaken. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.", "Closing this issue, as TF-Slim is no longer officially supported and will be [sunset in TensorFlow 2.0](https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md). Please reopen if you have any questions. Thanks!"]}, {"number": 4906, "title": "The \"cos\" merge mode when using tf as backend cann't get the expect result", "body": "I have two batch of sentences as inputs, like:\n\n```\na = np.array([[0, 1, 2, 3, 4], [0,1,0,0,0]], dtype='int32')\nb = np.array([[0, 1, 2, 3, 4], [0,0,2,0,0]], dtype='int32')\nvec = model.predict([a, b])\n```\n\nIn my model, I trying to get the cos score between two vector, so I use 'cos' model in merge\n`cos_score = merge([vec_a, vec_b], mode='cos', dot_axes=1)`\nWhen I use theano as backend, it works fine, but when I use tf as backend, the output_shape=[batch_size, batch_size]\n\nThe result by 'tf' backend:\n\n```\n[[ 0.99999994  0.97052568]\n [ 1.0303694   0.80238068]]\n```\n\nThe result by 'theano' backend:\n\n```\n[[ 1.        ]\n [ 0.80238068]]\n```\n\nThis is caused by [tf.batch_matmul](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard5/tf.batch_matmul.md), \n\n> The input tensors x and y are 3-D or higher\n\nBut I just input 2-D vectors. I think it have to be expanded when the input shape is 2-D\n\nBTW, My solution is:\n\n```\nif (keras.backend.backend() == \"theano\"):\n    cos_score = merge([vec_a, vec_b], mode='cos', dot_axes=1)\nelse:\n    vec_a = RepeatVector(1)(vec_a)\n    vec_b = RepeatVector(1)(vec_b)\n    cos_score = merge([vec_a, vec_b], mode='cos', dot_axes=2)\n    cos_score = Flatten()(cos_score)\n```\n\nSorry about my English \ud83d\ude03 \n", "comments": ["Sorry I upload the wrong address.. This is the issue about keras\n"]}, {"number": 4905, "title": "Fix Mac CUDNN library path issue in configure script", "body": "", "comments": ["@caisq, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @keveman and @davidzchen to be potential reviewers.\n", "Thanks for fixing this!\n"]}, {"number": 4904, "title": "is the documentation about sampled_softmax_loss correct?", "body": "Hi, \n\"tf.nn.sampled_softmax_loss(weights, biases, inputs, labels, num_sampled, num_classes, num_true=1, sampled_values=None, remove_accidental_hits=True, partition_strategy='mod', name='sampled_softmax_loss')\"\nthe document says:\n\"At inference time, you can compute full softmax probabilities with the expression tf.nn.softmax(tf.matmul(inputs, tf.transpose(weights)) + biases).\"\n\nI am not sure if this describes the behavior correctly. First, the labels are not used in the softmax computation; Second, the weights matrix is merely a lookup table that maps each word to a vector (word2vec), the above matmul operation is not meaningful.\n\nHere is my understanding of the code:\nGiven any word w, we have a vector encoding V1 from inputs, and then with label, we look up the weight matrix to find a vector encoding V2. Then we compute the softmax loss (cross-entropy loss) between V1 and V2. Finally, we sum up the loss for the sampled words.\n", "comments": ["@gouwsmeister : Could you comment on this?\n", "Hi @lpxz,\r\n\r\nRegarding your first question: `labels` are used both in the softmax computation and in the sampled softmax computation. Here is another more intuitive explanation that might be useful: \r\n\r\nLet each batch of the training data consist of (inputs, labels) examples. Here, inputs may be a (vector) of integer word ids (the \"contexts\"), and labels is a vector of batch_size target-class integer ids (the \"target words\"). \r\n\r\nWe now assume your model has a network which maps the `input`s into fixed-length `dim`-dimensional vectors. This [batch_size, dim] matrix is the `inputs` argument to tf.nn.sampled_softmax_loss. For each of the num_classes classes, we furthermore have a (learned) `dim`-dimensional embedding vector. This [num_classes, dim] matrix is the `weights` argument (same with full softmax). Finally, we also pass in the vector of observed target-class ids described above, i.e. `labels`.\r\n\r\nSo far it's the same for both the normal softmax and the sampled softmax. The only difference is then that the sampled softmax only samples `num_sampled` so-called \"negative\" (i.e. not observed) classes for each observed class in `labels`, and then computes the cross-entropy loss between the observed (labels) and not observed (sampled) classes, whereas the full softmax uses all the actual classes for the \"negative\" part.\r\n\r\nRegarding your second question: You are correct that `weights` is simply a lookup table (as defined above). The matmul computes the dot-product between each `dim`-dimensional row in `inputs`, and each `dim`-dimensional column (\"output embedding\") in `weights`. In each example, this dot product, together with the biases, represents the input \"logit\" to the softmax function (i.e. the z_j in the first equation in https://en.wikipedia.org/wiki/Softmax_function).\r\n\r\nSo I think the explanations are correct as is and I'll mark this closed for now. Please let me know if you have any other questions.\r\n\r\nStephan\r\n\r\n", "hey @gouwsmeister \r\n\r\nThanks for your great explanation above.  I have a further question. I'm not sure if it's a good place to ask this. If not, please let me know and I will just delete it. thanks!\r\n\r\n**Is tf.nn.sampled_softmax_loss() an implementation of negative sampling?**\r\n\r\nI always think they are different. Sampled softmax loss is a fast way to compute softmax (because the denominator of the softmax is huge) and negative sampling is ... emm ... conceptually \"randomly select just a small number of \u201cnegative\u201d words (let\u2019s say 5) to update the weights for\", i don't really know how to implement it. It's like a big black box to me. And just now some friend mentioned that we can use sampled softmax loss to implement negative sampling. I was shocked and did a little research about it.\r\n\r\nIt seems like they are from different paper and [What is candidate sampling](https://www.tensorflow.org/extras/candidate_sampling.pdf) describes that NCE is a generalized version of subsampled softmax and NEG is simplifying case of the NCE... this is just quite confusing. So I guess this is not the same but they are closely related and comparable?", "> hey @gouwsmeister\r\n> \r\n> Thanks for your great explanation above. I have a further question. I'm not sure if it's a good place to ask this. If not, please let me know and I will just delete it. thanks!\r\n> \r\n> **Is tf.nn.sampled_softmax_loss() an implementation of negative sampling?**\r\n> \r\n> I always think they are different. Sampled softmax loss is a fast way to compute softmax (because the denominator of the softmax is huge) and negative sampling is ... emm ... conceptually \"randomly select just a small number of \u201cnegative\u201d words (let\u2019s say 5) to update the weights for\", i don't really know how to implement it. It's like a big black box to me. And just now some friend mentioned that we can use sampled softmax loss to implement negative sampling. I was shocked and did a little research about it.\r\n> \r\n> It seems like they are from different paper and [What is candidate sampling](https://www.tensorflow.org/extras/candidate_sampling.pdf) describes that NCE is a generalized version of subsampled softmax and NEG is simplifying case of the NCE... this is just quite confusing. So I guess this is not the same but they are closely related and comparable?\r\n\r\nWould love to hear more details as well", "Hi @gouwsmeister \r\nI have noticed that the \"num_true\" in sampled_softmax_loss, which are the number of targets class, is fixed for each training instance. But in some situation, we have the different number for this, and I find here are an implementation for a dynamic \"number true\". Hope this can add in tensorflow.\r\n\r\nTensorFlow op for the dynamic number of true classes per instance.\r\nhttps://github.com/bxshi/dynamic_sampled_softmax_loss", "how to test the results trained by the sampled_softmax_loss,\r\nis there any metrics?\r\nsuch as acc, precision, recall, auc ?\r\nwith the  sampled_softmax_loss function ,I could not get the metrics, \r\ncould u pls help me ?\r\nif you can read Chinese , also see the [blog](https://iggcas.blog.csdn.net/article/details/106147091)\r\n\r\nthx\r\n"]}, {"number": 4903, "title": "tensorboard command broken", "body": "Running tensorboard results in this:\n\n```\n    logdir = os.path.expanduser(FLAGS.logdir)\nAttributeError: 'NoneType' object has no attribute 'logdir'\n```\n\nBest practice in this case is arg(opt)parse.\n", "comments": ["I had this issue, had to run the `tensorboard.py` script directly instead of through `bin/tensorboard`. `FLAGS` is set in the new if-main block.\n", "@elibixby could you please take a look? It seems this was caused by https://github.com/tensorflow/tensorflow/commit/8018346e12f9fef76cdc7accc248de17514f6d38\n", "How are you providing args to the script in the first place if you're not executing it as main? If you aren't providing args, why is the expected behavior for FLAGS to be not be `None`? \n", "I'm running `tensorboard --logdir='/tmp/tflearn_logs'`\nAnyway, expected behaviour is 'logdir flag is missing', not crash.\n", "Hmm I don't see any reason for this in the source. FYI tensorboard does now use argparse. In fact this is the change (from tf.flags to argparse) that was responsible for this regression. I'm going to try to independently verify this behavior. \n\nEDIT: for discoverability I would suggest changing the title to something like: \"tensorboard.FLAGS is None-type even when run as script\"\n\nEDIT2: Famous last words, there is in fact an obvious reason for this, pointed out by the above poster. I suspect after build bin/tensorboard actually calls out to: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/__main__.py which obviously wouldn't execute the main block of tensorboard.py... Argparse switch was made in bulk by a script which is why this wasn't caught. I'll get working on a fix. \n", "@elibixby When pip installs TensorFlow it creates a `tensorboard` .sh file. See the setup.py.\n\n``` py\nCONSOLE_SCRIPTS = [\n    'tensorboard = tensorflow.tensorboard.tensorboard:main',\n]\n```\n\nWhich creates a script like this:\n\n``` sh\n#!/Users/jart/tensorflow/bin/python\n\n# -*- coding: utf-8 -*-\nimport re\nimport sys\n\nfrom tensorflow.tensorboard.tensorboard import main\n\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n    sys.exit(main())\n```\n", "Even after rollback https://github.com/tensorflow/tensorflow/commit/39d0932c8796802a15663d3b59f559ab36244ece this issue most likely still exists @elibixby, due to the fact that the if name == main doesn't get evaluated when the pip generated script calls it.\n", "I'm on the end of the master branch and am having the same problem reported by @ror6ax. I think the changes made in https://github.com/tensorflow/tensorflow/commit/39d0932c8796802a15663d3b59f559ab36244ece were undone by the automated rollback in https://github.com/tensorflow/tensorflow/commit/7c23870e57c70f0c3da1dee93d083447e0fa3f56.\n", "Yes. Still seeing this problem.\n", "Re-opening this issue.\n", "@gunan you should be aware this issue exists.\n", "meet same issue, use `python /home/zido/anaconda2/lib/python2.7/site-packages/tensorflow/tensorboard/tensorboard.py --logdir=xxxxxx` is ok.\n", "Same issue here, directly using \n`(tensorboard)$ python tensorboard.py --help` is working,\nalso note that\n`(tensorflow)$ /_python_build/tensorflow/tensorboard$ ./tensorboard --help` is working\nbut\n`$ tensorboard --help` is not working\n\n>   File \"/usr/local/bin/tensorboard\", line 11, in <module>\n>     load_entry_point('tensorflow', 'console_scripts', 'tensorboard')()\n>   File \"/home/zeng/essential/tensorflow/_python_build/tensorflow/tensorboard/tensorboard.py\", line 44, in main\n>     logdir = os.path.expanduser(FLAGS.logdir)\n> AttributeError: 'NoneType' object has no attribute 'logdir'\n\nlooks like it's the symbolic link doesn't read arguments.\n\nnote that in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/setup.py#L47\nit directly run main function which doesn't run the argument parser in if **name**==\"**main**\"\n\nsystem: ubuntu14.04\ninstallment: master, build from source\n", "@willdzeng 's pull-request should fix. The command-line `tensorboard` call the `def main()` so all the code of initialization should be in that function.\n\nThanks for contribution.\n", "Please see my comment on the pull request. I'm going to revert the argparse changes to tensorboard, and have opened an internal issue to fix the underlying problems to allow us to move cleanly to argparse, while supporting both internal and external users.\n", "The rollback has been made internally, and should be pushed out shortly. \n", "Clarification: The push is manual, so the soonest the fix will be available here is Monday, sometime in the afternoon if there are no failures with the push.\nThanks for the fix!\n", "Thanks for this!\n"]}, {"number": 4902, "title": "Consistently unable to run camera example iOS app", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nClosest match: https://github.com/tensorflow/tensorflow/issues/4640\n### Environment info\n\nOperating System: iOS 10.0.1\nXcode 8 + macOS Sierra\n1. The commit hash (`git rev-parse HEAD`) - 781603968c60bb14a40cf00653f9b31be7826f20\n2. The output of `bazel version` - bazel command not found\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\ntensorflow/contrib/ios_examples/camera in Xcode\n### Logs or other output that would be helpful\n\nException Type:  EXC_CRASH (SIGABRT)\nException Codes: 0x0000000000000000, 0x0000000000000000\nException Note:  EXC_CORPSE_NOTIFY\nTriggered by Thread:  0\n\nApplication Specific Information:\nabort() called\n\nFiltered syslog:\nNone found\n\nThread 0 name:  Dispatch queue: com.apple.main-thread\nThread 0 Crashed:\n0   libsystem_kernel.dylib          0x000000018127e014 **pthread_kill + 8\n1   libsystem_pthread.dylib         0x0000000181345460 pthread_kill + 112\n2   libsystem_c.dylib               0x00000001811f23f4 abort + 140\n3   CameraExample                   0x00000001005d8a64 tensorflow::internal::LogMessageFatal::~LogMessageFatal() + 0\n4   CameraExample                   0x00000001005d8a8c tensorflow::internal::LogMessageFatal::~LogMessageFatal() + 40\n5   CameraExample                   0x000000010129fecc -[CameraExampleViewController viewDidLoad](CameraExampleViewController.mm:392)\n6   UIKit                           0x00000001880f85c8 -[UIViewController loadViewIfRequired] + 1056\n7   UIKit                           0x00000001880f8190 -[UIViewController view] + 28\n8   UIKit                           0x00000001880fe93c -[UIWindow addRootViewControllerViewIfPossible] + 76\n9   UIKit                           0x00000001880fbddc -[UIWindow _setHidden:forced:] + 272\n10  UIKit                           0x000000018816e604 -[UIWindow makeKeyAndVisible] + 48\n11  CameraExample                   0x00000001012a38ac -[CameraExampleAppDelegate application:didFinishLaunchingWithOptions:](CameraExampleAppDelegate.m:23)\n12  UIKit                           0x000000018816a61c -[UIApplication _handleDelegateCallbacksWithOptions:isSuspended:restoreState:] + 400\n13  UIKit                           0x000000018837ad60 -[UIApplication _callInitializationDelegatesForMainScene:transitionContext:] + 3524\n14  UIKit                           0x0000000188380ad0 -[UIApplication _runWithMainScene:transitionContext:completion:] + 1656\n15  UIKit                           0x0000000188395270 __84-[UIApplication _handleApplicationActivationWithScene:transitionContext:completion:]_block_invoke.3134 + 48\n16  UIKit                           0x000000018837dab4 -[UIApplication workspaceDidEndTransaction:] + 168\n17  FrontBoardServices              0x0000000183e51904 __FBSSERIALQUEUE_IS_CALLING_OUT_TO_A_BLOCK** + 36\n18  FrontBoardServices              0x0000000183e51770 -[FBSSerialQueue _performNext] + 176\n19  FrontBoardServices              0x0000000183e51b18 -[FBSSerialQueue _performNextFromRunLoopSource] + 56\n20  CoreFoundation                  0x000000018225e278 __CFRUNLOOP_IS_CALLING_OUT_TO_A_SOURCE0_PERFORM_FUNCTION__ + 24\n21  CoreFoundation                  0x000000018225dbc0 __CFRunLoopDoSources0 + 524\n22  CoreFoundation                  0x000000018225b7c0 __CFRunLoopRun + 804\n23  CoreFoundation                  0x000000018218a048 CFRunLoopRunSpecific + 444\n24  UIKit                           0x00000001881637cc -[UIApplication _run] + 608\n25  UIKit                           0x000000018815e550 UIApplicationMain + 208\n26  CameraExample                   0x000000010129bce0 main (main.mm:23)\n27  libdyld.dylib                   0x000000018116c5b8 start + 4\n", "comments": ["@martinwicke do you have any idea about this cryptic error?\n", "No. @petewarden or @andrewharp, any idea?\n", "It's dying with a LOG(FATAL) message, so the output in the debugger console should give more information about what went wrong. From the call stack, it looks like it's failing to load either the model or the labels file, so you should make sure they've been added to the project correctly. \n", "Closing due to inactivity. Please comment with new information and I will reopen.\n", "Deleting my local repo and cloning the latest source code seems to have fixed the issue. I am able to build and deploy the camera example now. Thank you for the help.\n"]}, {"number": 4901, "title": "Added support for new fused Winograd algorithms in cudnn 5.1", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@benbarsdell, thanks for your PR! By analyzing the history of the files in this pull request, we identified @zheng-xq, @keveman and @vrv to be potential reviewers.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "LGTM. \n\n@tensorflow-jenkins, test this please. \n", "@benbarsdell, conv_ops_test is failing on jenkins with this CL. Could you confirm that it runs well for you locally with Cuda 7.5 + Cudnn 5? Thanks. \n\nThe log is here: \nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-gpu/2119/consoleFull\n\nIf you cannot see it, this is the relevant portion: \n\n## FAIL: testInceptionBackFilter_4 (**main**.Conv2DTest)\n\nTraceback (most recent call last):\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-gpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local_linux-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/conv_ops_test.py\", line 1272, in Test\n    strides, padding)\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-gpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local_linux-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/conv_ops_test.py\", line 569, in _CompareBackFilter\n    self.assertAllClose(values[0], values[i], rtol=1e-4, atol=1e-4)\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-gpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local_linux-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 449, in assertAllClose\n    np.testing.assert_allclose(a, b, rtol=rtol, atol=atol)\n  File \"/usr/local/lib/python2.7/dist-packages/numpy/testing/utils.py\", line 1391, in assert_allclose\n    verbose=verbose, header=header)\n  File \"/usr/local/lib/python2.7/dist-packages/numpy/testing/utils.py\", line 733, in assert_array_compare\n    raise AssertionError(msg)\nAssertionError: \nNot equal to tolerance rtol=0.0001, atol=0.0001\n\n(mismatch 5.26315789474%)\n x: array([[[[ 46.046501,  45.859997,  56.073101, ...,  45.907337,  51.214821,\n           44.84082 ],\n         [ 47.817616,  42.558422,  52.609974, ...,  45.89362 ,  46.778015,...\n y: array([[[[ 33.582195,  36.529572,  56.073128, ...,  45.90733 ,  51.214813,\n           44.840816],\n         [ 35.047573,  32.791656,  52.609985, ...,  45.893631,  46.778004,...\n", "Sorry for the delay, I'm in the process of looking into this failure.\n", "SG, if you're interested in merging this at some point, please make sure you follow the instructions to sign the CLA.\n", "@benbarsdell: if you have signed the CLA under your nvidia address, maybe git amend your commits to that?  It's currently set to your gmail account.\n", "(clearing this out of our open PRs -- ping me or this thread when you want me to reopen after the CLA issues are resolved)\n"]}, {"number": 4900, "title": "Fix a the bug in the doc of a tutorial for training linear model", "body": "In [this tutorial](https://www.tensorflow.org/versions/r0.11/tutorials/wide/index.html#tensorflow-linear-model-tutorial), the column `gender` should have two category: \"Female/Male\" not \"female/male\". See the source of datasets [https://archive.ics.uci.edu/ml/machine-learning-databases/adult/](https://archive.ics.uci.edu/ml/machine-learning-databases/adult/).\n\nAlthough the linear model in the tutorial works well, for some other methods like SVM, the existence of column `gender` in `feature_columns` will probably terminate the program.\n", "comments": ["Can one of the admins verify this patch?\n", "@LiamHe, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener and @tiagonj to be potential reviewers.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "@googlebot I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "I only changed two markdown files in branch r0.11. It is weird that two tests fails.\n", "@tensorflow-jenkins test this please\n", "You may need to change the actual code here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/wide_n_deep_tutorial.py\n"]}, {"number": 4899, "title": "Branch 135828145", "body": "", "comments": ["@rohan100jain, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @keveman and @yuanbyu to be potential reviewers.\n", "@tensorflow-jenkins test this please\n", "I just clone and tested this branch. For new batch_norm layer, when `fused` is `True` and `is_training` is a symbolic placeholder, gradients computation failed (shapes are not compatible). Pull request #4892 addresses a little bit of this, but that won't be merged due to overlaps with this one I guess. \n", "Hi thuyen, thank you very much for your contribution and the help on testing. Unfortunately we had similar internal changes at about the same time. I noticed you added is_training to the gradient op in nn_ops.cc. Is it to solve this \"shape not compatible\" problem? There is no gradient computation for inference (is_training = False), in other words, is_training is not applicable to the gradient op. So adding it is a bit confusing to me. Could you explain why it is needed (or how it solved the shape incompatible issue)? Thanks!\n", "Hi thuyen, could you also point out which test could reproduce this error? Does this test cover this case? https://github.com/tensorflow/tensorflow/pull/4899/commits/df75f38eb7e1aacfbb85c637c72ccdfffc1ada78#diff-fa0c7294c4b5300fddcdc8753433406aR1616\n\nThanks.\n", "Hi Yao, \n\nYou should see the error with this snippet:\n\n```\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\nimport numpy as np\nis_training = tf.placeholder(tf.bool)\ninputs = tf.constant(np.zeros((10, 3, 100, 100)).astype('float32'))\noutputs = slim.batch_norm(inputs, is_training=is_training, fused=True, data_format='NCHW')\nloss = tf.reduce_sum(outputs)\ngrad = tf.gradients(loss, tf.trainable_variables())\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\np = sess.run(grad, feed_dict={is_training:True})\n```\n\nIt's like: \n\n```\nFile \"..../tensorflow/python/ops/gradients.py\", line 491, in gradients\nin_grad.set_shape(t_in.get_shape())\nFile \"..../tensorflow/python/framework/ops.py\", line 408, in set_shape\nself._shape = self._shape.merge_with(shape)\nFile \"..../tensorflow/python/framework/tensor_shape.py\", line 583, in merge_with\n(self, other))\nValueError: Shapes (0,) and (3,) are not compatible\n```\n\nNotice in the fused layer:\n\n```\ndef _fused_batch_norm_training():\n  return nn.fused_batch_norm(\n      inputs, gamma, beta, epsilon=epsilon, data_format=data_format)\ndef _fused_batch_norm_inference():\n  return nn.fused_batch_norm(\n      inputs,\n      gamma,\n      beta,\n      mean=moving_mean,\n      variance=moving_variance,\n      epsilon=epsilon,\n      is_training=False,\n      data_format=data_format)\noutputs, mean, variance = utils.smart_cond(is_training,\n                                           _fused_batch_norm_training,\n                                           _fused_batch_norm_inference)\n\n```\n\nSo when when `is_training` is a placeholder, we need both branches of the condition for the `outputs`. The same is true for gradients. So the tf.gradients function will actually go into the branch `is_training=False`, and perform gradient computation symbolically (even though at run time we never call the gradient op when `is_training=False`). Since we are not calling that branch at run time we shouldn't worry too much about it. But right now the shapes for  `moving_mean` and `moving_variance` gradients are set incorrectly when `is_training=False` (they are all `0` now, should be `0` for `is_training=True` only) and the tf.gradients catches that. I modified shape inference for gradient op in `nn_ops.cc` for that reason. \n\nRight now the `_fused_batch_norm` layer also has quite some redundant ops (the `mean` and `variance` at the end for example) and it doesn't support `2D` tensor (mine does). \n", "Hi thuyen, thank you for catching this subtle issue, as well as the detailed explanation! After the merge, I think we can still have your changes on the fix (adding is_training to the gradient op) and the data_format support for bias_add in your pull request #4892\n", "Yao, let me know if this is something you want to approve. After that I could merge this in and we can fix the other issues that have been brought up (thanks @thuyen !) through the other PR.\n", "Hi Rohan, yes, this sounds good to me.\n"]}, {"number": 4898, "title": "Dequeue op loses TensorShape", "body": "I am not able to find related issues online.\n\nOperating System:\nUbuntu, Mac OSX\n\nInstalled version of CUDA and cuDNN: \nCUDA 7.5, cuDNN v5.1\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\n`TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc0-cp27-none-linux_x86_64.whl`\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n`I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally`\n`I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally`\n`I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally`\n`I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally`\n`I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally`\n`0.11.0rc0`\n\nTo reproduce the problem:\n\n`a = tf.placeholder(dtype=tf.int32, shape=[None, 10])`\n`print a.get_shape()`\n`queue = tf.FIFOQueue(20, dtypes=[tf.int32])`\n`queue.enqueue([a])`\n`b = queue.dequeue()`\n`print b.get_shape()`\n\nThe first print output <?, 10>, which is OK, and there is a TensorShape for a.\nBut the second print output is <unknown>, and there is no TensorShape for b.\n\nTensorShape information is lost after dequeue operation.\n\nAnd this TensorShape is required in LSTM cell class.\n\nCan we add this TensorShape information back to the dequeued tensors?\n\nThanks.\n", "comments": ["Have you tried tf.train.batch?\n", "@ebrevdo Thanks for the reply. I plan to use this FIFOqueue for the input in my evaluation graph. And I don't want to specify batch_size for my evaluation graph.\n\nAnd yes, I have tf.train.batch in my training graph. And it works fine.\n\nThanks.\n", "If you're reading one element at a time, why are you using a queue?\n\nOn Oct 11, 2016 1:36 PM, \"Yandi Xia\" notifications@github.com wrote:\n\n@ebrevdo https://github.com/ebrevdo Thanks for the reply. I plan to use\nthis FIFOqueue for the input in my evaluation graph. And I don't want to\nspecify batch_size for my evaluation graph.\n\nAnd yes, I have tf.train.batch in my training graph. And it works fine.\n\nThanks.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/tensorflow/tensorflow/issues/4898#issuecomment-253037713,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtim7o5Bv_qnSiMksafZ0BjTK_A02tYks5qy_NXgaJpZM4KT_tb\n.\n", "@ebrevdo \nI don't specify batch size in the placeholder.\nI enqueue a batch of instances into the queue (variable size of batches) each time.\nAnd each dequeue will return a batch of data.\n\nThanks.\n", "Isn't that equivalent to passing enqueue_many=True to tf.train.batch?\n\nOn Oct 11, 2016 1:50 PM, \"Yandi Xia\" notifications@github.com wrote:\n\n> @ebrevdo https://github.com/ebrevdo\n> I don't specify batch size in the placeholder.\n> I enqueue a batch of instances into the queue (variable size of batches)\n> each time.\n> And each dequeue will return a batch of data.\n> \n> Thanks.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4898#issuecomment-253041735,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim2H0qptO2KDqp6BJ-xqu2nKrbUgQks5qy_amgaJpZM4KT_tb\n> .\n", "@ebrevdo \nI don't think so. \n\neven if using tf.train.batch with enqueue_many=True, I think a fixed batch_size need to be set, right? I really need a variable batch_size here.\n\nFurther more, I use this input method for evaluation graph. And the graph will take in-memory input, rather than TFRecord or CSV files. Therefore, I don't think tf.train.batch can solve this problem.\n\nThank you very much!\n", "The batch_size is the batch size for the output; and it can be a scalar\nTensor; i.e., you can use a Placeholder for it.  tf.train.batch really just\nuses a Queue underneath, it's agnostic to how you feed data.\n\nEither way, if you're taking in-memory input, I still don't understand how\nyou're planning to use a queue.  Do you plan to feed input directly in one\npython thread, and have the eval running in a separate thread?\n\nOn Tue, Oct 11, 2016 at 4:27 PM, Yandi Xia notifications@github.com wrote:\n\n> @ebrevdo https://github.com/ebrevdo\n> I don't think so.\n> \n> even if using tf.train.batch with enqueue_many=True, I think a fixed\n> batch_size need to be set, right? I really need a variable batch_size here.\n> \n> Further more, I use this input method for evaluation graph. And the graph\n> will take in-memory input, rather than TFRecord or CSV files. Therefore, I\n> don't think tf.train.batch can solve this problem.\n> \n> Thank you very much!\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4898#issuecomment-253076539,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim7f5RBqbwxzN7u-G-6QmMzladFZJks5qzBtegaJpZM4KT_tb\n> .\n", "@ebrevdo \n\nIt is good to know that tf.train.batch can take placeholder as input. Thanks.\n\n\"Do you plan to feed input directly in one\npython thread, and have the eval running in a separate thread?\"\nYes, I create a thread for enqueue, and the input of the evaluation graph is the result of dequeue. After all the data in test set is enqueued, the thread halts and waits for the evaluation to be done.\nThis is how TensorFlow FIFOQueue works, right? The producer/consumer style?\n\nThanks\n", "Yep it is.\n\nOn Oct 11, 2016 8:18 PM, \"Yandi Xia\" notifications@github.com wrote:\n\n> @ebrevdo https://github.com/ebrevdo\n> \n> It is good to know that tf.train.batch can take placeholder as input.\n> Thanks.\n> \n> \"Do you plan to feed input directly in one\n> python thread, and have the eval running in a separate thread?\"\n> Yes, I create a thread for enqueue, and the input of the evaluation graph\n> is the result of dequeue. After all the data in test set is enqueued, the\n> thread halts and waits for the evaluation to be done.\n> This is how TensorFlow FIFOQueue works, right? The producer/consumer style?\n> \n> Thanks\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4898#issuecomment-253108262,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtimxS9b4uqRC5MTh7qsaBaTUpMH90pks5qzFFwgaJpZM4KT_tb\n> .\n", "I found the solution. Actually there is a shapes argument in tf.FIFOQueue(). Just specify the shapes there, and the TensorShape info will be kept in the tensor objects.\n\nThanks @ebrevdo for your help and discussion!\n", "No, actually tf.FIFOQueue() can't take shapes like [None, 10]. Every dimension needs to be specified. \n\nIs there any way that I can use FIFOQueue with variable batch_size without losing TensorShape information?\n\nThanks\n", "When you use FIFOQueue, you don't pass in the batch shape, only the\nremainder.  For example, if you pass shapes like [None, 10] to\nqueue.enqueue_many(), you only use shape value [10] for that component.\n\nOn Wed, Oct 12, 2016 at 12:57 PM, Yandi Xia notifications@github.com\nwrote:\n\n> No, actually tf.FIFOQueue() can't take shapes like [None, 10]. Every\n> dimension needs to be specified.\n> \n> Is there any way that I can use FIFOQueue with variable batch_size without\n> losing TensorShape information?\n> \n> Thanks\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4898#issuecomment-253320531,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim4uBrFLMA1skRM4BcprjAMocyTM8ks5qzTudgaJpZM4KT_tb\n> .\n", "Thanks @ebrevdo!\n\nI just found a work around to this problem. Just using set_shape() to set shape of the dequeued tensor back.\nFor the above example, just use: \n`b.set_shape([None, 10])`\n\nAnd then b will have a known TensorShape attribute.\n"]}, {"number": 4897, "title": "Provide unaggregated gradients tensors", "body": "As described here, TF is inflexible when it comes to access to the gradients:\nhttp://stackoverflow.com/questions/35731506/unaggregated-gradients-gradients-per-example-in-tensorflow?rq=1\n\nPlease provide a method where the user can retrieve the raw gradients, not the averaged gradients. Requiring the user to compute their own gradients is impractical -- the framework should work for the user, not the other way around.\n\nUse case: this is needed for reinforcement learning, where the gradients of one net needs to be backpropagated through another net (in separate steps).\n", "comments": ["TensorFlow is just a regular automatic differentiation system, it gives the gradient that you ask for -- so if you ask for a gradient of the mean loss over all examples, the gradient is aggregated, but if you ask for the gradient of loss over a single example, it gives you \"unaggregated\" gradient for that particular datapoint. Naively, if you have a batch of size k, you could have \"k\" `tf.gradient` calls, and that would give you gradient for each of the k examples. You can make this more efficient by reformulating your task, here's a [post](https://plus.google.com/+IanGoodfellow/posts/Re2QnrNBvJm) from @goodfeli  For networks with conv layers it's more tricky\n", "I saw the article. This isn't what I'm looking for since I'd have to do all the computations by hand. It's difficult and error prone. Doing \"k\" tf.gradients() call is slow.\n\nObtaining the batch of gradients is required for many algorithms. It really should be something that's easy to do.\n", "`[tf.gradients(cost....) for cost in example_costs]` is the way to get gradients for multiple costs. If that's too slow, it might be useful to provide timelines to see where the slowness happens\n", "If I follow what you're saying, example_costs would be defined at graph instantiation time, right? So that forces a fixed batch size.\n", "`example_costs` in my example is a Python list of costs you want to differentiate with respect to, and you can change this list during runtime. For efficiency, you may want small number of per-example-cost gradient graphs and reuse them during runtime.\n\nGenerally though, \"retrieve raw gradient\" request is ill-specified -- there's no place in TensorFlow where \"per-example gradients\" are added together since there's no notion of \"example\" when differentiating. IE, you can encode your examples as rows of data matrix, or as columns, or in some other way, and that would give you the same gradient, but different sets of \"per-example gradients\". The AD system of TensorFlow gives you freedom to choose example encoding and does not need to know about your choice, which I think is what's needed from a general computation framework.\n\nThat said, it would be useful to see implementations of methods like Ian's or others that give ways of computing per-example gradients efficiently\n", "Thanks for the answer. Yes, some examples of implementations would come handy.\n\nI don't understand how TF is providing freedom here. The implicit assumption that the last matrix rank corresponds to the batch instances seems to be built-in the design. For instance, how could you pack your data in rows in a matrix and pass that through a convolutional layer? The convolutional layer imposes semantics on the content of your tensors.\n\nThe notion of a batch of gradients is well-defined in general (not speaking about TF specifically). You have a batch of inputs. The output of the network is a vector containing the loss value for each input. The batch of gradients is a tensor containing one entry per (input, loss) pair.\n\nIn TF, in terms of semantics, I think all you'd have to do is to allow the user to specify a vector loss instead of a scalar loss in tf.gradients(), and then retrieve the requested batch of gradients.\n\nIn practice TensorFlow is a deep learning framework, and the AD system is subservient to that goal. If it's hard to retrieve something so fundamental as a batch gradients, it seems to me that there's something missing in the API.\n", "On the contrary, it's because it's an AD framework rather than a DL framework that it is somewhat difficult to get the batch gradients. The gradient of an individual example is a deep learning concept. As a pure math engine, tensorflow just exposes the concept of the gradient with respect to a specific tensor. Because there's no tensor representing the parameters as used on a single example, the AD engine doesn't have a concept for the DL value you want.\n\nThis actually does have a reasonably efficient solution that doesn't require n tf.gradients calls. Do n calls to convolution, wrapping the same parameters in a different identity op each time. Then do one tf.gradients call getting the gradient with respect to each of the identity copies.\n", "Alright, thanks for the help.\n", "Baking in the NN assumptions too deeply is why DistBelief ended up so inflexible. In TensorFlow you can minimize the loss which is `sum(XWX')`, where X is a single matrix-valued example, so that's your per-example gradient. Allowing `tf.gradients` to differentiate with respect to vector valued `vec` seems like a good idea, that would make it simpler to compute Hessians as well as per-example gradients\n", "Closing this due to inactivity (and it seems that there was a healthy discussion). Feel free to re-open if you think something needs to change here.\n", "When I ask tensorflow for the gradient of a function that returns a scalar for every example, tf just gives me one gradient instead of a gradient per example. My \"loss\" function is not a sum or average so I can't understand this behaviour. Simple example:\n\n``` python\nx = tf.placeholder(tf.float32, [None,3] , name=\"input\")\nW1 = tf.get_variable(\"W1\", shape=[3, 1],\n           initializer=tf.contrib.layers.xavier_initializer())\noutput = tf.nn.relu(tf.matmul(x,W1))\ngrads = tf.gradients(output,[W1])\nsess = tf.InteractiveSession()\ninit = tf.initialize_all_variables()\nsess.run(init)\nprint sess.run(grads,feed_dict={x:np.random.rand(10,3)})\n>>[array([[ 0.83393538],\n       [ 0.16146532],\n       [ 0.43589821]], dtype=float32)]\n```\n", "I agree that it would be useful for various algorithms to be able to get each individual gradient per batch and it doesn't seem easy to do this. As jpiabrantes commented above, the issue is **not** limited to asking for the gradient of a tensor that does a a batch reduction. In fact, it's clear that the gradient on an aggregation operation should too be aggregated. The problem is tf.gradients also aggregates even when the output of the tensor by which you differentiating doesn't aggregate.\r\n\r\nI understand why it is the way it is for mathematical generality, but the very fact that tf.gradients has a flag for how aggregation is done suggests that we should be able to specify a batch mode where it does *not* aggregate the gradients and instead returns an NxMx... tensor where n is the number gradients over which it would normally aggregate and Mx... is the shape of the variable.", "This is only pseudocode, but basic idea is:\n\nexamples = tf.split(batch)\nweight_copies = [tf.identity(weights) for x in examples]\noutput = tf.stack(f(x, w) in zip(examples, weight_copies))\ncost = cost_function(output)\nper_example_gradients = tf.gradients(cost, weight_copies)\n\nOn Sat, Apr 1, 2017 at 1:47 PM, jjough <notifications@github.com> wrote:\n\n> Do n calls to convolution, wrapping the same parameters in a different\n> identity op each time. Then do one tf.gradients call getting the gradient\n> with respect to each of the identity copies.\n>\n> @goodfeli <https://github.com/goodfeli> could you please explain this\n> further? Would love to see the trick, but not really sure what you mean.\n>\n> I'm manually calculating a full Jacobian by making a bunch of calls to\n> tf.gradients, and it's very slow.\n>\n>\n> def tf_jacobian(tensor2, tensor1, feed_dict):\n>     \"\"\"\n>     Computes the tensor d(tensor2)/d(tensor1) recursively.\n>     \"\"\"\n>     shape = list(sess.run(tf.shape(tensor2), feed_dict))\n>     if shape:\n>         return tf.stack([tf_jacobian(tf.squeeze(M, squeeze_dims = 0), tensor1, feed_dict) for M in tf.split(split_dim = 0, num_split = shape[0], value = tensor2)])\n>     else:\n>         grad = tf.gradients(tensor2, tensor1)\n>         if grad[0] != None:\n>             return tf.squeeze(grad, squeeze_dims = [0])\n>         else:\n>             return tf.zeros_like(tensor1)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/4897#issuecomment-290946084>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAXrGqYwDyn590JO8VdyFECt-I8LZwiIks5rrrfQgaJpZM4KT_Op>\n> .\n>\n", "I've used it and it was fast\n\nOn Sun, Apr 2, 2017 at 10:44 AM, jjough <notifications@github.com> wrote:\n\n> Thanks Ian - this makes sense, trying it out now. Is it obvious that this\n> would be faster than the naive method?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/4897#issuecomment-291001872>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAXrGkyMYBIVaI3qNIxpBvv5AUF8zMWcks5rr951gaJpZM4KT_Op>\n> .\n>\n", "Thank you @goodfeli !\r\n\r\nIt would be very useful to support this in tensorflow for calculating second-order gradients (e.g. fisher information, etc). \r\n\r\nAny hope for official support or is this the recommended solution?\r\n\r\ncc @datang1992", "I think this is the recommended solution for now\n\nOn Tue, Apr 25, 2017 at 5:47 PM, Jaan Altosaar <notifications@github.com>\nwrote:\n\n> Thank you @goodfeli <https://github.com/goodfeli> !\n>\n> It would be very useful to support this in tensorflow for calculating\n> second-order gradients (e.g. fisher information, etc).\n>\n> Any hope for official support or is this the recommended solution?\n>\n> cc @datang1992 <https://github.com/datang1992>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/4897#issuecomment-297073506>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAXrGg-Q5lr6Ew5YWeIpmMV_mN700WOmks5rzhWJgaJpZM4KT_Op>\n> .\n>\n", "@altosaar I recently had to compute per-example gradients, and ended up with implementation [here](https://github.com/yaroslavvb/stuff/blob/master/natural_gradient_multilayer.py) . To make things fast I had to drop `tf.gradients` and compute things manually. There's two orders of magnitude speed-up if you use specialized structures, specifically, use \"khatri-rao\" product which gives you a batch of matmul gradients given a batch of activations + batch of backprops. It can be computed efficiently through [einsum](https://github.com/yaroslavvb/stuff/blob/2c716e7abde25018ccfeeeff96c6397fef4f33ba/whitening_util.py#L515)", "Awesome, thank you @goodfeli. @yaroslavvb that implementation is extremely helpful - exactly what we're trying to do \ud83d\udc4d \ud83d\udc4d \ud83d\udc4d \r\n\r\nAlso cc @ebrevdo based on past discussions!", "See https://github.com/tensorflow/tensorflow/issues/675#issuecomment-299669705 for a proposal for `tf.jacobian`.\r\n\r\nI'm not sure this is exactly the issue discussed here, but I'll just note how I solved my use-case for \"per example gradients\" using the helper function in the linked comment. With scalar input/output `jacobian(outputs, inputs)` calculates the right result. For batched inputs/outputs, `jacobian(tf.reduce_sum(outputs, 0), inputs)` gives me the result I'm looking for, because each inputs example only effects the corresponding entry in outputs, so the other entries get dropped out when differentiating the sum.", "@goodfeli , how would the code given above change if I wish to get the gradient between in a specific layer? (that is, between the output of the layer and the weights at the input of the layer)\r\n\r\n> examples = tf.split(batch)\r\nweight_copies = [tf.identity(weights) for x in examples]\r\noutput = tf.stack(f(x, w) in zip(examples, weight_copies))\r\ncost = cost_function(output)\r\nper_example_gradients = tf.gradients(cost, weight_copies)", "The mathematical concept of a gradient is defined only when the output is a single scalar. (If you give TensorFlow an output that isn't a scalar, it will give you the gradient of the sum). You probably want a Jacobian. So you'd need to make a loop that computes the gradient once per entry in the output.", "Thank you for the reply.\r\n I think my followup question is more relevant in stack overflow so I posted there. If you get a chance I will really appreciate the help since no one answered it in a while. (The basic idea is that I wish to use the gradients of one layer as inputs to another layer - like Fisher Vectors but really can't) \r\nHere is the [link ](https://stackoverflow.com/questions/44745535/how-to-use-gradients-as-inputs-in-tensorflow) to my question there.\r\n", "Hi @goodfeli  @yaroslavvb , I have a problem that is very similar to per sample gradient. The current tf.gradients() outputs the sum of gradients given the batch, is this possible to output the sum of element-wise power of gradients, i.e., sum over batch (dy/dx)^2, instead of sum over batch (dy/dx) currently", "@goodfeli As far as I understand, one needs to split up the input variable into 'batch_size' number of tensors. However, one would need to know the batch_size (as an integer) a priori when constructing this using tf.split. Is there an easy way to use your pseudocode since we usually dont have the batch_size a priori?\r\n\r\n", "@sitzikbs Did you end up using something similar to the pseudocode that @goodfeli suggested? If so, was this fast, and does it work on batch level ops like batch norm?", "> @sitzikbs Did you end up using something similar to the pseudocode that @goodfeli suggested? If so, was this fast, and does it work on batch level ops like batch norm?\r\n\r\n@pkadambi I ended up computing the gradients manually in an external function, its in the 3dmfv function in the  tf_utils file at this repo:  [link](https://github.com/sitzikbs/3DmFV-Net)", "> \r\n> This is only pseudocode, but basic idea is: examples = tf.split(batch) weight_copies = [tf.identity(weights) for x in examples] output = tf.stack(f(x, w) in zip(examples, weight_copies)) cost = cost_function(output) per_example_gradients = tf.gradients(cost, weight_copies)\r\n\r\nCan somebody please elaborate the pseudo code given by @goodfeli a little bit more. A small example for a tiny MLP would be very nice!", "is there an efficient tflow way to compute the following covariance matrix M_{xy} of derivatives:\r\n\r\nM_{xy} = Cov(\\partial_x f(x,y);  \\partial_y g(x,y)) =  \r\n= <\\partial_x f(x,y) \\partial_y g(x,y)> - <\\partial_x f(x,y)><\\partial_y g(x,y)>\r\n\r\nwhere f(x,y), g(x,y) are differentiable functions of some vectors x and y, \\partial_x is the gradient along x, and <> is the minibatch average. \r\n\r\nThe second part of the expression,  <\\partial_x f(x,y)>*<\\partial_y g(x,y)>, can be readily computed using `tf.gradients`. However, the first part, <\\partial_x f(x,y) \\partial_y g(x,y)>, represents one of those instances discussed above where taking the partial derivatives does not commute with the minibatch average.\r\n\r\nFrom the discussion above it sounds like this type of operations is what tensorflow should be able to do.\r\n", "@mgbukov using tf.gradients for this in TensorFlow will be inefficient, more generally, autograd in all major neural network frameworks is specialized to \"single output summed over examples\" case. When I needed access to individual gradients I had to derive the formulas by hand in terms of einsum ops. [Jax](https://github.com/google/jax) has a better automatic way of doing it through vmap.", "tf.vectorized_map can be used to efficiently compute per-example gradients.", "Just to add a remark: our team at Owkin has studied this and done some benchmarks, please see report on [arXiv](https://arxiv.org/abs/1912.06015) and sample code at [this repo on Github](https://github.com/owkin/grad-cnns). Our analysis is focused on CNNs; we propose a new approach which combines Goodfellow's trick to a a PyTorch peculiarity (the `groups` argument in the convolution), and show that it is sometimes faster than simply recreating pointers to the original model (the approach proposed by @goodfeli above, and what `tf.vectorized_map` is effectivelly doing). Unfortunately, it is unclear how much of it applies to Tensorflow, as there's no `groups` argument here! Please let us know if you have any feedback.", "It is not obvious to me if \"multi\" in your paper is the same as what tf.vectorized_map based graph rewrite would translate to.  So it will be worth benchmarking that  as well. It has been tested on complex networks and provides order of magnitude speedups due to vectorization based rewrites.\r\n\r\nThe problem, as you hint at, is that we have not implemented an efficient vectorization of Conv2DBackpropFilter, which is what is required for your case to run fast. See\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/parallel_for/pfor.py#L1768\r\nwhich falls back to a while loop. \r\n\r\nCreating a new kernel for this, (or perhaps mapping this to an existing kernel), should do the trick, and might be akin to adding the group argument you mentioned.", "Thanks for the comments @agarwal-ashish! \r\n\r\n> It is not obvious to me if \"multi\" in your paper is the same as what tf.vectorized_map based graph rewrite would translate to. So it will be worth benchmarking that as well.\r\n\r\nThat's a very good point, I imagine there are some optimizations in `tf.vectorized_map` since it's graph based. We did all our benchmarks in PyTorch, but it is definitely a good idea to do cross PyTorch/Tensorflow benchmarks at some point.\r\n\r\n> The problem, as you hint at, is that we have not implemented an efficient vectorization of Conv2DBackpropFilter, which is what is required for your case to run fast.\r\n\r\nIndeed that is the main issue in porting it to Tensorflow. We might look into implementing a custom kernel for this, thanks for the tip :)"]}, {"number": 4896, "title": "[bug]unmatched quote causes big problem", "body": "An unmatched quote causes thousands lines typesetting error. Look at the api docs, from [here](https://www.tensorflow.org/versions/r0.11/api_docs/python/state_ops.html#constant_initializer) to [here](https://www.tensorflow.org/versions/r0.11/api_docs/python/state_ops.html#IndexedSlices) has the problem, which influence almost 30 functions api docs from `constant_initializer` to `IndexedSlices`. \n\nWe'd better fix it ASAP\n", "comments": ["Can one of the admins verify this patch?\n", "@DjangoPeng, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @keveman and @vrv to be potential reviewers.\n", "@rohan100jain Gotcha~ I updated the `init_ops.constant_initializer()`, and I think it would work when you generate the doc next time.\n"]}, {"number": 4895, "title": "Tensorflow fails to find GPU device (CUDA 8.0)", "body": "- Operating System: Ubuntu 16.04\n- Installed version of CUDA and cuDNN: CUDA 8.0.27, cuDNN 5.1.5\n\n```\n$ ls ~/bin/cuda-8.0/lib64/libcud*\n/home/maxim/bin/cuda-8.0/lib64/libcudadevrt.a    /home/maxim/bin/cuda-8.0/lib64/libcudart.so.8.0.27  /home/maxim/bin/cuda-8.0/lib64/libcudnn.so.5\n/home/maxim/bin/cuda-8.0/lib64/libcudart.so      /home/maxim/bin/cuda-8.0/lib64/libcudart_static.a   /home/maxim/bin/cuda-8.0/lib64/libcudnn.so.5.1.5\n/home/maxim/bin/cuda-8.0/lib64/libcudart.so.8.0  /home/maxim/bin/cuda-8.0/lib64/libcudnn.so          /home/maxim/bin/cuda-8.0/lib64/libcudnn_static.a\n```\n\n`nvidia-smi` shows the GPU.\n- Environment variables:\n\n```\ndeclare -x CUDA_HOME=\"/home/maxim/bin/cuda-8.0\"\ndeclare -x PATH=\"/home/maxim/bin/anaconda2/bin:/home/maxim/bin/cuda-8.0/bin:/home/maxim/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\"\ndeclare -x LD_LIBRARY_PATH=\"/home/maxim/bin/cuda-8.0/lib64:\"\n```\n- Tensorflow version: 0.10.0rc0 (installed by Anaconda)\n- Note that on the same machine Theano _works perfectly_:\n\n```\n$ python theano_check1.py \nUsing gpu device 0: GeForce GTX 960M (CNMeM is disabled, cuDNN 5105)\n```\n\nBut Tensorflow does not:\n\n```\nCould not satisfy explicit device specification '/device:GPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0\n```\n", "comments": ["How did you install TensorFlow? Official binaries (11rc0 and before) don't support CUDA 8.0, only CUDA 7.5\n", "> How did you install TensorFlow?\n\n`conda install tensorflow`\nIt looks like 0.10.0rc0 is the latest version in Anaconda.\nI have also tried 0.11.0rc0 from `pip`, nothing's changed.\n\n> Official binaries (11rc0 and before) don't support CUDA 8.0, only CUDA 7.5\n\nInteresting. The doc says \"The GPU version works best with Cuda Toolkit 7.5 and cuDNN v5\"\n\nBTW Nvidia lists CUDA Toolkit 7.5 and below under \"archived\":\nhttps://developer.nvidia.com/cuda-toolkit-archive\nAnd CUDA Toolkit 8.0 is the only version supported for Ubuntu 16.04, so I guess I don't have a workaround to downgrade CUDA.\n", "@maxim5 yeah, I had to downgrade my Ubuntu down from 16.04 in order to get CUDA working\n", "In case anyone has the same problem: tensorflow supports CUDA 8.0, if you build it from sources (and configure accordingly).\nThe following post worked for me: https://alliseesolutions.wordpress.com/2016/09/08/install-gpu-tensorflow-from-sources-w-ubuntu-16-04-and-cuda-8-0-rc/\n\nThe issue is therefore: support CUDA 8.0 out of the box, because it is the primary CUDA version now.\n", "CUDA 8.0 will be supported soon. Right now we only officially support 7.0 - 7.5. Please see #2559 which is tracking this.\n"]}, {"number": 4894, "title": "virtualenv pip3 install fails", "body": "I am trying to install TensorFlow in virtualenv using Python 3.4.\n\nI did:\n\n```\n$ pyvenv-3.4 --system-site-packages tensorflow\n$ source tensorflow/bin/activate\n$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc0-cp34-cp34m-linux_x86_64.whl\n$ pip3 install --upgrade $TF_BINARY_URL\n```\n\nThe pip3 install ends in this error:\n\n```\nInstalling collected packages: tensorflow, protobuf, wheel, numpy, six, setuptools                                                                                                                                                                                        \u2502\nCleaning up...                                                                                                                                                                                                                                                            \u2502\n  Removing temporary dir /tmp/pip_build_ashwin...                                                                                                                                                                                                                         \u2502\nException:                                                                                                                                                                                                                                                                \u2502\nTraceback (most recent call last):                                                                                                                                                                                                                                        \u2502\n  File \"/usr/lib/python3/dist-packages/pip/basecommand.py\", line 122, in main                                                                                                                                                                                             \u2502\n    status = self.run(options, args)                                                                                                                                                                                                                                      \u2502\n  File \"/usr/lib/python3/dist-packages/pip/commands/install.py\", line 283, in run                                                                                                                                                                                         \u2502\n    requirement_set.install(install_options, global_options, root=options.root_path)                                                                                                                                                                                      \u2502\n  File \"/usr/lib/python3/dist-packages/pip/req.py\", line 1436, in install                                                                                                                                                                                                 \u2502\n    requirement.install(install_options, global_options, *args, **kwargs)                                                                                                                                                                                                 \u2502\n  File \"/usr/lib/python3/dist-packages/pip/req.py\", line 672, in install                                                                                                                                                                                                  \u2502\n    self.move_wheel_files(self.source_dir, root=root)                                                                                                                                                                                                                     \u2502\n  File \"/usr/lib/python3/dist-packages/pip/req.py\", line 902, in move_wheel_files                                                                                                                                                                                         \u2502\n    pycompile=self.pycompile,                                                                                                                                                                                                                                             \u2502\n  File \"/usr/lib/python3/dist-packages/pip/wheel.py\", line 206, in move_wheel_files                                                                                                                                                                                       \u2502\n    clobber(source, lib_dir, True)                                                                                                                                                                                                                                        \u2502\n  File \"/usr/lib/python3/dist-packages/pip/wheel.py\", line 175, in clobber                                                                                                                                                                                                \u2502\n    os.makedirs(dest)                                                                                                                                                                                                                                                     \u2502\n  File \"/usr/lib/python3.4/os.py\", line 237, in makedirs                                                                                                                                                                                                                  \u2502\n    mkdir(name, mode)                                                                                                                                                                                                                                                     \u2502\nPermissionError: [Errno 13] Permission denied: '/usr/lib/python3.4/site-packages' \n```\n\nWhy is pip3 install inside a virtual environment trying to write in `/usr/lib/python3.4`? How to make this install work?\n\nOperating System: Ubuntu 14.04\nInstalled version of CUDA and cuDNN: CUDA 7.5 and cuDNN 5\n\n```\nlrwxrwxrwx 1 root root   16 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcublas.so -> libcublas.so.7.5                                                                                                                                                                       \u2502\nlrwxrwxrwx 1 root root   19 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcublas.so.7.5 -> libcublas.so.7.5.18                                                                                                                                                                \u2502\n-rwxr-xr-x 1 root root  23M Aug 15  2015 /usr/local/cuda-7.5/lib64/libcublas.so.7.5.18                                                                                                                                                                                    \u2502\n-rw-r--r-- 1 root root  28M Aug 16  2015 /usr/local/cuda-7.5/lib64/libcublas_device.a                                                                                                                                                                                     \u2502\n-rw-r--r-- 1 root root  27M Aug 16  2015 /usr/local/cuda-7.5/lib64/libcublas_static.a                                                                                                                                                                                     \u2502\n-rw-r--r-- 1 root root 316K Aug 16  2015 /usr/local/cuda-7.5/lib64/libcudadevrt.a                                                                                                                                                                                         \u2502\nlrwxrwxrwx 1 root root   16 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so -> libcudart.so.7.5                                                                                                                                                                       \u2502\nlrwxrwxrwx 1 root root   19 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18                                                                                                                                                                \u2502\n-rwxr-xr-x 1 root root 375K Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so.7.5.18                                                                                                                                                                                    \u2502\n-rw-r--r-- 1 root root 704K Aug 16  2015 /usr/local/cuda-7.5/lib64/libcudart_static.a                                                                                                                                                                                     \u2502\nlrwxrwxrwx 1 root root   15 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcufft.so -> libcufft.so.7.5                                                                                                                                                                         \u2502\nlrwxrwxrwx 1 root root   16 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcufftw.so -> libcufftw.so.7.5                                                                                                                                                                       \u2502\nlrwxrwxrwx 1 root root   19 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcufftw.so.7.5 -> libcufftw.so.7.5.18                                                                                                                                                                \u2502\n-rwxr-xr-x 1 root root 438K Aug 15  2015 /usr/local/cuda-7.5/lib64/libcufftw.so.7.5.18                                                                                                                                                                                    \u2502\n-rw-r--r-- 1 root root  42K Aug 16  2015 /usr/local/cuda-7.5/lib64/libcufftw_static.a                                                                                                                                                                                     \u2502\nlrwxrwxrwx 1 root root   18 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcufft.so.7.5 -> libcufft.so.7.5.18                                                                                                                                                                  \u2502\n-rwxr-xr-x 1 root root 107M Aug 15  2015 /usr/local/cuda-7.5/lib64/libcufft.so.7.5.18                                                                                                                                                                                     \u2502\n-rw-r--r-- 1 root root 110M Aug 16  2015 /usr/local/cuda-7.5/lib64/libcufft_static.a                                                                                                                                                                                      \u2502\nlrwxrwxrwx 1 root root   17 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcuinj64.so -> libcuinj64.so.7.5                                                                                                                                                                     \u2502\nlrwxrwxrwx 1 root root   20 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcuinj64.so.7.5 -> libcuinj64.so.7.5.18                                                                                                                                                              \u2502\n-rwxr-xr-x 1 root root 5.5M Aug 15  2015 /usr/local/cuda-7.5/lib64/libcuinj64.so.7.5.18                                                                                                                                                                                   \u2502\n-rw-r--r-- 1 root root 1.6M Aug 16  2015 /usr/local/cuda-7.5/lib64/libculibos.a                                                                                                                                                                                           \u2502\nlrwxrwxrwx 1 root root   16 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcurand.so -> libcurand.so.7.5                                                                                                                                                                       \u2502\nlrwxrwxrwx 1 root root   19 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcurand.so.7.5 -> libcurand.so.7.5.18                                                                                                                                                                \u2502\n-rwxr-xr-x 1 root root  50M Aug 15  2015 /usr/local/cuda-7.5/lib64/libcurand.so.7.5.18                                                                                                                                                                                    \u2502\n-rw-r--r-- 1 root root  50M Aug 16  2015 /usr/local/cuda-7.5/lib64/libcurand_static.a                                                                                                                                                                                     \u2502\nlrwxrwxrwx 1 root root   18 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcusolver.so -> libcusolver.so.7.5                                                                                                                                                                   \u2502\nlrwxrwxrwx 1 root root   21 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcusolver.so.7.5 -> libcusolver.so.7.5.18                                                                                                                                                            \u2502\n-rwxr-xr-x 1 root root  36M Aug 15  2015 /usr/local/cuda-7.5/lib64/libcusolver.so.7.5.18                                                                                                                                                                                  \u2502\n-rw-r--r-- 1 root root  16M Aug 16  2015 /usr/local/cuda-7.5/lib64/libcusolver_static.a                                                                                                                                                                                   \u2502\nlrwxrwxrwx 1 root root   18 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcusparse.so -> libcusparse.so.7.5                                                                                                                                                                   \u2502\nlrwxrwxrwx 1 root root   21 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcusparse.so.7.5 -> libcusparse.so.7.5.18                                                                                                                                                            \u2502\n-rwxr-xr-x 1 root root  36M Aug 15  2015 /usr/local/cuda-7.5/lib64/libcusparse.so.7.5.18                                                                                                                                                                                  \u2502\n-rw-r--r-- 1 root root  43M Aug 16  2015 /usr/local/cuda-7.5/lib64/libcusparse_static.a\n```\n", "comments": ["Try putting the word `sudo` before that last command and it should work. If not, please let us know.\n", "@jart Using `sudo` would install the files outside the virtual environment in the root filesystem. Wouldn't that invalidate the reason of using virtualenv in the first place?\n", "I failed to notice the pyvenv-3.4 command. In that case, this is probably a bug in pip. The backtrace seems to trace only into pip, which seems to be trying to create a folder in your system directory, even though the virtualenv is activated.\n", "My pip3 is v1.5.4 on Ubuntu 14.04. Since this is a LTS version of Ubuntu, a large number of folks might be having this same error. Is the error reproducible on your computer?\n", "I just installed TensorFlow using the commands you provided above on my Goobuntu 14.04 LTS workstation with pip3 v1.5.4 with python3.4 for both CPU and GPU and it worked. I'm sorry I wish I could help but I can't reproduce this.\n", "The same problem here!", "I tried this and finished without errors, I don't know yet if it works https://stackoverflow.com/a/38088879/224255\r\n\r\ntl;dr add `--user` parameter to the `install` command: `pip3 install tensorflow --user`", "@alexeieleusis your suggestion works for me. Thanks"]}, {"number": 4893, "title": "How to create `input_fn` using `read_batch_examples` with `num_epochs` set?", "body": "TF Version: 0.10.0rc\n## Update\n\nSolved the issue, see [StackOverflow post here for solution](http://stackoverflow.com/q/39877710/6557588).\n## Original non-Issue...\n\nI have a basic `input_fn` that can be used with Tensorflow Estimators below. It works flawlessly without setting the `num_epochs` parameter; the obtained tensor has a discrete shape. Pass in `num_epochs` as anything other than `None` results in an unknown shape. My issue lies with constructing sparse tensors whilst using `num_epochs`; I cannot figure out how to generically create said tensors without knowing the shape of the input tensor.\n\nCan anyone think of a solution to this problem? I'd like to be able to pass `num_epochs=1` to be able to evaluate only 1 time over the data set, as well as to pass to `predict` to yield a set of predictions the size of the data set, no more no less.\n\n``` python\n    def input_fn(batch_size):\n        examples_op = tf.contrib.learn.read_batch_examples(\n            FILE_NAMES,\n            batch_size=batch_size,\n            reader=tf.TextLineReader,\n            num_epochs=1,\n            parse_fn=lambda x: tf.decode_csv(x, [tf.constant([''], dtype=tf.string)] * len(HEADERS)))\n\n        examples_dict = {}\n        for i, header in enumerate(HEADERS):\n            examples_dict[header] = examples_op[:, i]\n\n        continuous_cols = {k: tf.string_to_number(examples_dict[k], out_type=tf.float32)\n                           for k in CONTINUOUS_FEATURES}\n\n        # Problems lay here while creating sparse categorical tensors\n        categorical_cols = {\n            k: tf.SparseTensor(\n                indices=[[i, 0] for i in range(examples_dict[k].get_shape()[0])],\n                values=examples_dict[k],\n                shape=[int(examples_dict[k].get_shape()[0]), 1])\n            for k in CATEGORICAL_FEATURES}\n\n        feature_cols = dict(continuous_cols)\n        feature_cols.update(categorical_cols)\n        label = tf.string_to_number(examples_dict[LABEL], out_type=tf.int32)\n\n        return feature_cols, label\n```\n", "comments": ["We recommend [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow) for community support. We try to keep this issue tracker tightly focused on bugs and feature requests.\n"]}, {"number": 4892, "title": "Add fused_batch_norm layer", "body": "Add data_format option for convolution2d, bias_add, max_pool2d, and avg_pool2d as well. This helps to fully utilize the cudnn speed for normal convnets with 'NCHW' data layout.\n", "comments": ["@thuyen, thanks for your PR! By analyzing the history of the files in this pull request, we identified @jhseu, @tensorflower-gardener and @drpngx to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "Close due to #4919 and #4899 \n"]}, {"number": 4891, "title": "Add fused_batch_norm layer", "body": "Add data_format option for convolution2d, bias_add, max_pool2d, and avg_pool2d as well. This helps to fully utilize the cudnn speed for normal convnets with 'NCHW' data layout. \n", "comments": ["Can one of the admins verify this patch?\n", "@thuyen, thanks for your PR! By analyzing the history of the files in this pull request, we identified @jhseu, @tensorflower-gardener and @drpngx to be potential reviewers.\n"]}, {"number": 4890, "title": " TypeError: __init__() got an unexpected keyword argument 'state_is_tuple'", "body": "Please help me resolved the error:\n\n```\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:717] Creating TensorFlow device (/gpu:3) -> (device: 3, name: GeForce GTX TITAN X, pci bus id: 0000:05:00.0)\n..\n======================================================================\nERROR: testAttentionDecoderStateIsTuple (__main__.Seq2SeqTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py\", line 308, in testAttentionDecoderStateIsTuple\n    cell = tf.nn.rnn_cell.BasicLSTMCell(2, state_is_tuple=True)\nTypeError: __init__() got an unexpected keyword argument 'state_is_tuple'\n\n======================================================================\nERROR: testDynamicAttentionDecoder1 (__main__.Seq2SeqTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py\", line 271, in testDynamicAttentionDecoder1\n    enc_outputs, enc_state = tf.nn.dynamic_rnn(cell, inp, dtype=tf.float32)\nTypeError: dynamic_rnn() takes at least 3 arguments (3 given)\n\n======================================================================\nERROR: testDynamicAttentionDecoder2 (__main__.Seq2SeqTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py\", line 290, in testDynamicAttentionDecoder2\n    enc_outputs, enc_state = tf.nn.dynamic_rnn(cell, inp, dtype=tf.float32)\nTypeError: dynamic_rnn() takes at least 3 arguments (3 given)\n\n======================================================================\nERROR: testEmbeddingAttentionDecoder (__main__.Seq2SeqTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py\", line 368, in testEmbeddingAttentionDecoder\n    embedding_size=2, output_size=3)\nTypeError: embedding_attention_decoder() got an unexpected keyword argument 'embedding_size'\n\n======================================================================\nERROR: testEmbeddingAttentionSeq2Seq (__main__.Seq2SeqTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py\", line 382, in testEmbeddingAttentionSeq2Seq\n    cell = tf.nn.rnn_cell.BasicLSTMCell(2, state_is_tuple=True)\nTypeError: __init__() got an unexpected keyword argument 'state_is_tuple'\n\n======================================================================\nERROR: testEmbeddingRNNDecoder (__main__.Seq2SeqTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py\", line 85, in testEmbeddingRNNDecoder\n    cell = tf.nn.rnn_cell.BasicLSTMCell(2, state_is_tuple=True)\nTypeError: __init__() got an unexpected keyword argument 'state_is_tuple'\n\n======================================================================\nERROR: testEmbeddingRNNSeq2Seq (__main__.Seq2SeqTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py\", line 105, in testEmbeddingRNNSeq2Seq\n    cell = tf.nn.rnn_cell.BasicLSTMCell(2, state_is_tuple=True)\nTypeError: __init__() got an unexpected keyword argument 'state_is_tuple'\n\n======================================================================\nERROR: testEmbeddingTiedRNNSeq2Seq (__main__.Seq2SeqTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py\", line 170, in testEmbeddingTiedRNNSeq2Seq\n    cell = tf.nn.rnn_cell.BasicLSTMCell(2, state_is_tuple=True)\nTypeError: __init__() got an unexpected keyword argument 'state_is_tuple'\n\n======================================================================\nERROR: testModelWithBooleanFeedPrevious (__main__.Seq2SeqTest)\nTest the model behavior when feed_previous is True.\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py\", line 764, in testModelWithBooleanFeedPrevious\n    TestModel(model)\n  File \"tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py\", line 695, in TestModel\n    enc_inp, dec_inp_fp_true, feed_previous=True)\n  File \"tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py\", line 685, in ForwardBackward\n    dec_op, _ = seq2seq(enc_inp, dec_inp, feed_previous=feed_previous)\n  File \"tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py\", line 726, in EmbeddingRNNSeq2SeqF\n    cell = tf.nn.rnn_cell.BasicLSTMCell(2, state_is_tuple=True)\nTypeError: __init__() got an unexpected keyword argument 'state_is_tuple'\n\n======================================================================\nERROR: testModelWithBuckets (__main__.Seq2SeqTest)\nLarger tests that does full sequence-to-sequence model training.\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py\", line 617, in testModelWithBuckets\n    _, losses = SampleGRUSeq2Seq(inp, out, weights)\n  File \"tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py\", line 609, in SampleGRUSeq2Seq\n    softmax_loss_function=SampledLoss)\n  File \"/usr/local/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/seq2seq.py\", line 926, in model_with_buckets\n    decoder_inputs[:bucket[1]])\n  File \"tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py\", line 598, in GRUSeq2Seq\n    state_is_tuple=True)\nTypeError: __init__() got an unexpected keyword argument 'state_is_tuple'\n\n======================================================================\nERROR: testModelWithBucketsScopeAndLoss (__main__.Seq2SeqTest)\nTest that variable scope reuse is not reset after model_with_buckets.\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py\", line 568, in testModelWithBucketsScopeAndLoss\n    _, losses1 = SampleGRUSeq2Seq(inp, out, weights, per_example_loss=False)\n  File \"tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py\", line 561, in SampleGRUSeq2Seq\n    per_example_loss=per_example_loss)\n  File \"/usr/local/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/seq2seq.py\", line 926, in model_with_buckets\n    decoder_inputs[:bucket[1]])\n  File \"tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py\", line 554, in GRUSeq2Seq\n    state_is_tuple=True)\nTypeError: __init__() got an unexpected keyword argument 'state_is_tuple'\n\n======================================================================\nERROR: testOne2ManyRNNSeq2Seq (__main__.Seq2SeqTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"tensorflow/tensorflow/python/kernel_tests/seq2seq_test.py\", line 452, in testOne2ManyRNNSeq2Seq\n    cell = tf.nn.rnn_cell.BasicLSTMCell(2, state_is_tuple=True)\nTypeError: __init__() got an unexpected keyword argument 'state_is_tuple'\n\n----------------------------------------------------------------------\nRan 20 tests in 1.935s\n```\n", "comments": ["How are you invoking these tests? What version are you using?\n", "Closing automatically due to lack of recent activity. Please reopen when further information becomes available. Thank you.\n"]}, {"number": 4889, "title": "Switch stream_executor to use portable dlopen()", "body": "Use the functions from `port::Env` rather than `dlfcn.h`, to make it easier to port stream_executor to Windows.\n", "comments": ["@mrry, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @zheng-xq and @vrv to be potential reviewers.\n", "@vrv This is a step towards making stream_executor build on Windows. I haven't edited code in here before... is there any special process for modifying stream_executor code?\n"]}, {"number": 4888, "title": "\"src: warning: directory does not exist.\u201c when I build syntaxnet", "body": "when I build syntaxnet, a problem occurs that.\nERROR: /home/zwg/.cache/bazel/_bazel_zwg/532f17a1037f0f671972f45f175b09ce/external/protobuf/BUILD:560:1: null failed: protoc failed: error executing command bazel-out/host/bin/external/protobuf/protoc '--python_out=bazel-out/local-opt/genfiles/src' -Isrc external/protobuf/src/google/protobuf/any.proto external/protobuf/src/google/protobuf/api.proto ... (remaining 10 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nsrc: warning: directory does not exist.\nexternal/protobuf/src/google/protobuf/any.proto: File does not reside within any path specified using --proto_path (or -I).  You must specify a --proto_path which encompasses this file.  Note that the proto_path must be an exact prefix of the .proto file names -- protoc is too dumb to figure out when two paths (e.g. absolute and relative) are equivalent (it's harder than you think).\n\n Build Syntaxnet on my machine with the following configuration:\nOS:\n[zwg@localhost example]$ cat /proc/version\nLinux version 2.6.32-358.el6.x86_64 (mockbuild@c6b8.bsys.dev.centos.org) (gcc version 4.4.7 20120313 (Red Hat 4.4.7-3) (GCC) ) #1 SMP Fri Feb 22 00:31:26 UTC 2013\n\nPython:\n[zwg@localhost example]$ python\nPython 2.7.10 (default, Oct 10 2016, 02:48:27) \n[GCC 4.4.7 20120313 (Red Hat 4.4.7-3)] on linux2\n\nGCC:\n[zwg@localhost example]$ gcc -v\nUsing built-in specs.\nCOLLECT_GCC=gcc\nCOLLECT_LTO_WRAPPER=/opt/rh/devtoolset-2/root/usr/libexec/gcc/x86_64-redhat-linux/4.8.2/lto-wrapper\nTarget: x86_64-redhat-linux\nConfigured with: ../configure --prefix=/opt/rh/devtoolset-2/root/usr --mandir=/opt/rh/devtoolset-2/root/usr/share/man --infodir=/opt/rh/devtoolset-2/root/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-bootstrap --enable-shared --enable-threads=posix --enable-checking=release --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --enable-languages=c,c++,fortran,lto --enable-plugin --with-linker-hash-style=gnu --enable-initfini-array --disable-libgcj --with-isl=/dev/shm/home/centos/rpm/BUILD/gcc-4.8.2-20140120/obj-x86_64-redhat-linux/isl-install --with-cloog=/dev/shm/home/centos/rpm/BUILD/gcc-4.8.2-20140120/obj-x86_64-redhat-linux/cloog-install --with-mpc=/dev/shm/home/centos/rpm/BUILD/gcc-4.8.2-20140120/obj-x86_64-redhat-linux/mpc-install --with-tune=generic --with-arch_32=i686 --build=x86_64-redhat-linux\nThread model: posix\ngcc version 4.8.2 20140120 (Red Hat 4.8.2-15) (GCC)\n\nJAVA:\n[zwg@localhost example]$ java -version\njava version \"1.8.0_101\"\nJava(TM) SE Runtime Environment (build 1.8.0_101-b13)\nJava HotSpot(TM) 64-Bit Server VM (build 25.101-b13, mixed mode)\n\nBAZEL:\n[zwg@localhost example]$ bazel version\nBuild label: 0.2.2b- (@non-git)\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Mon Oct 10 13:48:34 2016 (1476107314)\nBuild timestamp: 1476107314\nBuild timestamp as int: 1476107314\n", "comments": ["What are the commands you're using to build this?\n", "Closing automatically due to lack of recent activity. Please reopen when further information becomes available. Thank you.\n"]}, {"number": 4887, "title": "resnet model in tf slim does not take is_training param", "body": "Compare the [resnet_v2 model](https://github.com/tensorflow/tensorflow/blob/8e48ec6ea0492e2cb9fd19c0a2ccf41afc7b4dc6/tensorflow/contrib/slim/python/slim/nets/resnet_v2.py) to the [vgg model](https://github.com/tensorflow/tensorflow/blob/8e48ec6ea0492e2cb9fd19c0a2ccf41afc7b4dc6/tensorflow/contrib/slim/python/slim/nets/vgg.py). The vgg model takes `is_training` whereas the resnet model does not. This param should be taken and passed to the `batch_norm` layers.\n\nThe docs for the v1 model do reference `is_training`, but I don't see it used.\n", "comments": ["I guess you do something like this:\n\n```\nwith slim.arg_scope(resnet_v2.resnet_arg_scope(is_training)):\n  ....\n```\n\nNot sure if docs can be improved here?\n", "Tangential: I'm surprised you can reference `tf.contrib.framework.arg_scope` as `tf.contrib.slim.arg_scope`. We're going to have to do something about that. I filed #4950.\n\nMaybe the `is_training` param should be deprecated on the vgg model and display a warning to use `tf.contrib.framework.arg_scope` instead. I'm going to mark this as contributions welcome if anyone wants to send us a PR with that cleanup. CC: @sguada\n", "I am going to send a PR shortly.\n", "See https://github.com/tensorflow/models/blob/master/slim/nets/resnet_v2.py which is going to replace this soon.\n", "Passing `is_training=False` during inference messes up the output for the resnet_v1 model. Tried doing \r\n`with slim.arg_scope(resnet_v2.resnet_arg_scope(is_training)):`  but getting\r\n\r\n\r\n` with slim.arg_scope(resnet_arg_scope(is_training)):\r\nNameError: name 'is_training' is not defined`\r\n\r\nHow exactly does one go about passing the `is_training` parameter to the model ? BTW, will a pre-trained checkpoint be released for resnet_v2? \r\n", "you need to define is_training\r\n\r\n`with slim.arg_scope(resnet_arg_scope(is_training=False)):`", "@sguada what's the status on this? Can we close this or is there still something being requested?", "I also have an experience similar to @dhaneshr : Passing `is_training=False` messes up the output (resnet_v2 with TFv1.2.0). That means the feed-forward pass diverges to large numbers, and sometimes even to NaN. I wonder if that's a problem of `slim` or `layers`. \r\n\r\nI train my own ResNet model, which successfully converges after a few epochs. However, after I save the model and then recover it for inference, everything seems wrong, even with images used during training. If I do `is_training=True` the outpus are mostly random (caused by BatchNormalization). When I recover the model for additional training epochs, it works just fine, and keeps the accuracy improving. I checked the weights of the network, and they're alright.\r\n\r\nIt seems like the flag `is_training=False` is not being passed correctly somewhere, and the BatchNormalization layers fail to keep the activations bounded. Does anybody have a suggestion for fixing this? Or maybe I'm missing something?", "@jccaicedo I'm suffering the same problem, have you got any solution?", " I'm suffering the same problem", "I am meeting the same problem. I have tried to decrease my loss in training, however,in testing I find the outout is all messed!   @jccaicedo  Have you found any way out?", "It is most likely somethings (e.g. batchnorm) are not set properly for is_training=False. However, since this is a departure from our official models, I would recommend using the official resnet models (https://github.com/tensorflow/models).\r\n\r\nI am closing this bug now. If you have similar issues for the official models, please feel free to filea separate bug."]}, {"number": 4886, "title": "Gradient of tf.reduce_max and tf.nn.max_pool don't agree with each other, and theano", "body": "``` python\na = tf.placeholder(dtype=tf.float32, name='a', shape=[4])\nb = tf.reduce_max(a)\nb2 = tf.nn.max_pool(tf.reshape(a, [1, 2, 2, 1]),\n        [1,2,2,1], [1,1,1,1], 'VALID')[0,0,0,0]\nc = tf.gradients([b], [a])[0]\nc2 = tf.gradients([b2], [a])[0]\n\nwith tf.Session() as sess:\n    v = np.asarray([1, 2, 2, 2], dtype='float32')\n    print sess.run(c, feed_dict={a:v})  # 0, 0.3, 0.3, 0.3\n    print sess.run(c2, feed_dict={a:v})  # 0, 1, 0, 0\n\nimport theano.tensor as T\nimport theano.tensor.signal.downsample as D\na = T.fvector('a')\nb = T.max(a)\nb2 = T.reshape(a, [1,2,2])\nb2 = D.max_pool_2d(b2, [2,2])[0,0,0]\nc = T.grad(b, a)\nc2 = T.grad(b2, a)\nprint c.eval({a: v})    # 0, 1, 1, 1\nprint c2.eval({a: v})    # 0, 1, 1, 1\n```\n\nBug or feature? \ud83d\ude26 Although as subgradient they all seem to make sense, but is there a reason to justify the difference of reduce_max and max_pool?\n\nI saw the [comment](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_grad.py#L71) for reduce_max and it seems like the first one is feature. \n", "comments": ["@girving Another gradients question.\n", "@ebrevdo can you take a look on this since @girving is not available right now.\n", "Is this on GPU or not?  Are you using cudnn?\n", "Looks like cudnn maxpool returns `[0, 1, 0, 0]`.  xq; is there a flag that controls this?\n", "I think both tf.reduce_max and tf.maxpool are correct in their own sense. But the Theano's answer is not. \n\nWhen a backprop comes back into a max operation, and if there are multiple winners, the credit can only be shared among them. There is no constraint on which one should get the credit, and by how much. But they shouldn't all get the full credit. \n\nConsider this:\na = max(x, x, x). The correct answer is d(a)/d(x)=1, but Theano's answer leads to d(a)/d(x) = 3,  following the chain rule. \n\nBoth tf.reduce_max and tf.maxpool gives somewhat self-consistent answers. One equally distributed it among all windows, while the other gives it to the first one. Both arbitrary choices, but nevertheless correct in their own sense. \n\nSince we pick the maxpool implementation from Cudnn, it's unclear that we can get all implementations consistent across all different libraries. So my perspective is that each implementation has to be self-consistent in that the total weight of a backprop value sums to 1. It might be a bit over-constrained if we require all operations to make the same arbitrary choice.\n", "Makes sense to me. Thanks!\n"]}, {"number": 4885, "title": "After some iterations my accuracy has decrease to 0, but the loss and cross entropy are not nan.", "body": "Hello\nI can't understand this question:\nI can trained my net in a correct result in beginning but after about 4K iterations ,the accuracy suddenly decrease to zero, and the loss and cross entropy suddenly increased to a high value, it is like this curves:\ntrain accuracy:\n![image](https://cloud.githubusercontent.com/assets/5306116/19257677/2f14d1ec-8fa4-11e6-9f31-50255f2cef0e.png)\nval accuracy:\n![image](https://cloud.githubusercontent.com/assets/5306116/19257685/3c3b26f0-8fa4-11e6-854f-3d5b459158f6.png)\ncross entropy:\n![image](https://cloud.githubusercontent.com/assets/5306116/19257709/54f385de-8fa4-11e6-88c7-d5e90af88f94.png)\nregular loss:\n![image](https://cloud.githubusercontent.com/assets/5306116/19257716/676f5f1c-8fa4-11e6-8b87-79d92f35202d.png)\n\nI think you can reproduce it in this link(3D Convolution, UCF101):\nhttps://github.com/hx173149/C3D-tensorflow\n\nthanks~\n", "comments": ["Thanks for reaching out @hx173149. We rely on the community to provide support so this question is probably more appropriate for [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow). We try to keep this issue tracker focused on bugs and feature requests.\n"]}, {"number": 4884, "title": "Issue 1702", "body": "Sorry I make a mistake. I should create a branch in my repo in order to contribute.\nwould you please close https://github.com/tensorflow/tensorflow/pull/4873\n", "comments": ["Can one of the admins verify this patch?\n", "@guotong1988, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @keveman and @benoitsteiner to be potential reviewers.\n", "OK , I will read it and edit the format.\n", "@keveman Do you have a python code format doc? Thank you .\n", "@guotong1988 moved this to #5026.\n"]}, {"number": 4883, "title": "Ability to directly set the gradient of a node for use in backpropagation.", "body": "Imagine a tiny network defined as follows, where linear is a typical helper function defining TensorFlow variables for a weight matrix and activation function:\n\n`final_layer = linear(linear(_input,10,tf.nn.tanh),20)`\n\nNormally this would be optimized via gradient descent on a loss:\n\n`loss = tf.reduce_sum(tf.square(final_layer - _target))\ntrain_step = tf.train.AdamOptimizer().minimmize(loss)`\n\nBut assume I'm getting the derivatives of the loss w.r.t. final_layer from an external  source (e.g. a tf.placeholder named _deriv). I would like to be able to use this gradient information with one of the builtin optimizers to backpropagate and update the network parameters, but this appears to be currently impossible.\n\nThe workaround I'm currently using is to construct an artificial loss consisting of the inner product between _deriv and final_layer (since the derivatives of this loss w.r.t. final_layer will be equal to _deriv). \n\n`loss = tf.reduce_sum(final_layer*_deriv)\ntrain_step = tf.train.AdamOptimizer().minimmize(loss)`\n\nThis is very wasteful though, as it needs to do this unnecessary inner product and calculate its derivative for every training step even though I already know this information.\n\nFor those thinking this an odd thing to need to do, it is necessary for implementing [synthetic gradients](https://arxiv.org/abs/1608.05343).\n", "comments": ["You can use `stop_gradient` trick to replace gradient of any node with\nsomething else\nhttp://stackoverflow.com/questions/36456436/how-can-i-define-only-the-gradient-for-a-tensorflow-subgraph/36480182#36480182\n\nOn Mon, Oct 10, 2016 at 3:03 PM, zergylord notifications@github.com wrote:\n\n> Imagine a tiny network defined as follows, where linear is a typical\n> helper function defining TensorFlow variables for a weight matrix and\n> activation function:\n> \n> final_layer = linear(linear(_input,10,tf.nn.tanh),20)\n> \n> Normally this would be optimized via gradient descent on a loss:\n> \n> loss = tf.reduce_sum(tf.square(final_layer - _target))\n> train_step = tf.train.AdamOptimizer().minimmize(loss)\n> \n> But assume I'm getting the derivatives of the loss w.r.t. final_layer from\n> an external source (e.g. a tf.placeholder named _deriv). I would like to be\n> able to use this gradient information with one of the builtin optimizers to\n> backpropagate and update the network parameters, but this appears to be\n> currently impossible.\n> \n> The workaround I'm currently using is to construct an artificial loss\n> consisting of the inner product between _deriv and final_layer (since the\n> derivatives of this loss w.r.t. final_layer will be equal to _deriv).\n> \n> loss = tf.reduce_sum(final_layer*_deriv)\n> train_step = tf.train.AdamOptimizer().minimmize(loss)\n> \n> This is very wasteful though, as it needs to do this unnecessary inner\n> product and calculate its derivative for every training step even though I\n> already know this information.\n> \n> For those thinking this an odd thing to need to do, it is necessary for\n> implementing synthetic gradients https://arxiv.org/abs/1608.05343.\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4883, or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHISF59xfOeRfoUw6a33uH1Yk_I-7ks5qyrZEgaJpZM4KTCSQ\n> .\n", "Thank you for answering this one @yaroslavvb.\n"]}]