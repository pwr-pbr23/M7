[{"number": 52189, "title": "Mark fail test as nopip..merging to r2.7", "body": null, "comments": []}, {"number": 52188, "title": "[TF:TRT] bilinear resize refactor + workaround", "body": "Refactors the `ConvertBilinear` TF-TRT converter:\r\n- Use the new `OpConverterBase` class to implement the converter.\r\n- Move the converter class to `tf2tensorrt/convert/ops/resize.cc`.\r\n\r\nAdds a workaround for conversion of 1bilinear_resize1 when `align_corners=false`:\r\n- When using TensorRT < 8, a workaround consisting of a sequence of transposed convolutions and slice operations is used.\r\n- When using TensorRT > 8, a workaround is not needed. A function is added to use the new TensorRT `SetCoordinateTransform` API for the `IResizeLayer`.\r\n- Adds additional tests on the Python side to test the workaround. Use of the workaround code path can be enforced through an environment variable `TF_TRT_FORCE_BILINEAR_RESIZE_WAR`, which allows for testing the code path with TRT8.", "comments": ["I'm going to factor out the common parts of this and #52248 into another PR", "@bixia1 Can you please review this PR ? Thanks!", "@christopherbate  would you please rebase the tree?", "@christopherbate Can you please check @bixia1's comments and resolve conflicts?. Thanks!", "I've rebased this, and then I need to look at tf2xla to verify correctness, since they `tf2xla` uses the same approach to convert bilienar resize.", "@christopherbate I understand you are going to check correctness, can you please also address the comments.", "@bixia1 It turned out my workaround strategy for 2x bilinear upsample was a special case of the strategy TF2XLA uses for their bilinear resize implementation. \r\n\r\nI built a more comprehensive test fixture in Python to test the TF2XLA strategy, and found it to be performant, so I reworked the workaround strategy here. It is implemented in the final commit. \r\n\r\nIn summary:\r\n\r\nBilinear resize is implemented by using TensorRT's bilinear resize layer. The only exception is the special case for TensorRT version < 8.0 where the node parameters are \"half_pixel_center = false\" and \"align_corners = false\". In this case, we use the following strategy: we implement a bilinear resize subroutine using a sequence of (TransposeConv, Slice). This is efficient in typical use cases (2x upsample, for example). However, when GCD(in_size[dim], out_size[dim]) is small for any spatial dimension, this strategy can have a relatively high memory cost. Thus, we follow the dynamic programming strategy used in TF2XLA for building a table that computes optimum sequnce of resize operations from input size to output size. The only difference is that we treat cost on a path (i -> j -> k) as the maximum of memory cost of kernels (i -> j) and (j ->k) rather than adding the costs.\r\n\r\n\r\n", "Working on making some additional corrections, adding Python tests", "Comments were addressed and I'm done doing fixup/cleanup.", "squashed", "PR description updated.", "I'm investigating the test failure", "This test is failing inside google\r\nResizeTestSizeConst.testTfTrt_OfflineConversion_DynamicEngine_FP16_NoCalibration_ImplicitBatch\r\n 0:00:02.700\r\nTraceback (most recent call last):\r\n  File \"/build/work/dcd43c46567fcc8befcaa49fe2411cb743e1/google3/runfiles/google3/third_party/tensorflow/python/framework/test_util.py\", line 3468, in assertDictEqual\r\n    super().assertDictEqual(a, b, msg)\r\n  File \"/build/work/dcd43c46567fcc8befcaa49fe2411cb743e1/google3/runfiles/google3/third_party/py/absl/testing/absltest.py\", line 1780, in assertDictEqual\r\n    raise self.failureException('\\n'.join(message))\r\nAssertionError: {'Const': set(), 'ResizeBilinear': {'Const', 'input_0'}, 'input_0': set(), 'output_0': {'ResizeBilinear'}} != {'TRTEngineOp_0': {'input_0'}, 'input_0': set(), 'output_0': {'TRTEngineOp_0'}} (\r\nexpected:\r\n[('Const', set()), ('ResizeBilinear', {'Const', 'input_0'}), ('input_0', set()), ('output_0', {'ResizeBilinear'})]\r\nvs actual:\r\n[('TRTEngineOp_0', {'input_0'}), ('input_0', set()), ('output_0', {'TRTEngineOp_0'})])\r\nUnexpected, but present entries:\r\n'TRTEngineOp_0': {'input_0'}\r\n\r\nrepr() of differing entries:\r\n'output_0': {'ResizeBilinear'} != {'TRTEngineOp_0'}\r\n\r\nMissing entries:\r\n'Const': set()\r\n'ResizeBilinear': {'Const', 'input_0'}", "Not sure what that means, I can't reproduce. What is the TRT version?", "Is that a TF1 style test?", "It is tensorflow/python/compiler/tensorrt/test:gpu_resize_test. We use TensorRT 7.1.3.", "@christopherbate Can you please check @bixia1's comments and keep us posted ? Thanks!", "Currently testing fixes for the issues I was alerted too.\r\n\r\n@bixia1 I know you were importing the previous commits into your system already, so it would be better for me to push a separate commit still? Or are you going to start over?", "rebased", "tensorflow/python/compiler/tensorrt/test:gpu_resize_test still fails.\r\n\r\nResizeTestSizeConst.testTfTrt_OfflineConversion_DynamicEngine_FP16_NoCalibration_ImplicitBatchcontent_copy\r\nTraceback (most recent call last):  File \"/build/work/7b81de36195d65ce29801716c0aa93be1569/google3/runfiles/google3/third_party/tensorflow/python/framework/test_util.py\", line 3468, in assertDictEqual    super().assertDictEqual(a, b, msg)  File \"/build/work/7b81de36195d65ce29801716c0aa93be1569/google3/runfiles/google3/third_party/py/absl/testing/absltest.py\", line 1780, in assertDictEqual    raise self.failureException('\\n'.join(message))AssertionError: {'Const': set(), 'ResizeBilinear': {'Const', 'input_0'}, 'input_0': set(), 'output_0': {'ResizeBilinear'}} != {'TRTEngineOp_0': {'input_0'}, 'input_0': set(), 'output_0': {'TRTEngineOp_0'}} (expected:[('Const', set()), ('ResizeBilinear', {'Const', 'input_0'}), ('input_0', set()), ('output_0', {'ResizeBilinear'})]vs actual:[('TRTEngineOp_0', {'input_0'}), ('input_0', set()), ('output_0', {'TRTEngineOp_0'})])Unexpected, but present entries:'TRTEngineOp_0': {'input_0'}repr() of differing entries:'output_0': {'ResizeBilinear'} != {'TRTEngineOp_0'}\r\nMissing entries:'Const': set()'ResizeBilinear': {'Const', 'input_0'}\r\n\r\n", "@bixia1 thanks for the updates. I am not sure where the discrepancy is between your system and ours.", "Can you give the more complete log from test run?", "@christopherbate Can you please resolve conflicts? Thank you!", "Closing in favor of reopening with smaller change"]}, {"number": 52187, "title": "Fix tfl-to-tosa quantization intermediate type", "body": "Intermediate quantized type should be an I32 under all circumstances however\r\nthe lowering assumed the intermediate type was the same as the output type.\r\nThis recently appeared due to some changes in the linalg compilation.", "comments": []}, {"number": 52186, "title": "[TF:TRT] Create execution context with device memory if shape output is present in TRT 7", "body": "Workaround for TRT 7 to handle shape outputs.\r\n\r\nPreviously, we use createExecutionContextWithoutDeviceMemory to create IExecutionContext, so that TF-TRT can manage the needed device memory to support the execution using the IExecutionContext. This triggers a bug in TRT 7. To workaround the bug, we switch to use createExecutionContext for creating IExecutionContext when the TRTEngine has int32 output.\r\n\r\nAdd two test cases.\r\n\r\nTracker #45481\r\n\r\nTagging @bixia1 for review.", "comments": ["Looks good to me. I modified the PR description with more details. Please check. ", "Based on the code then it's not the fact that there is a shape tensor, but that it is int32 data type. That's not the same thing, right? Because shape tensor has additional qualifications of being 0-d or 1-d.\r\n\r\nDoes this same behavior occur when index output of \"topK\" operation is set as output?\r\n\r\nIs there no op that we can insert at the output to force TRT into correct behavior? ", "Thanks @christopherbate for your comment! You are right, we could and should be more specific when checking the output tensor. I have added checks to ensure that it is 0-d or 1-d.\r\n\r\n> Does this same behavior occur when index output of \"topK\" operation is set as output?\r\n\r\nThe error happens at an assertion of a shape tensor size: \"Assertion failed: size == shapeTensorVolume(out.extent)\"\r\nI expect that the problem would only appear with int32 outputs that are shape tensors.\r\n\r\n> Is there no op that we can insert at the output to force TRT into correct behavior?\r\n\r\nI am not aware of any.\r\n\r\n\r\n\r\n", "I am not sure whether we can/should add check for 0-d or 1-d to filter the cases. It depends on the situation of the bug. For example, if we have this in the network:\r\n     shapeOp(4-d) -> reshape(2x2)\r\nDo we need to allocate context with GPU memory to make this work?\r\n", "@bixia1, you are right. I have checked it, and the error happens if I insert the reshape. I will revert the change, which filters for the 0 and 1d cases. \r\n\r\nI will also add two unit tests that would trigger the bug. To add the tests, I will need to rebase this PR to #52181.", "Rebased to avoid conflicts. Ready to proceed @bixia1"]}, {"number": 52185, "title": "Removing potentially dead-code (stream-executor ThenMatMul* calls)", "body": "It looks like there are no calls to the stream executor `ThenMatMul*` APIs. Then APIs seem to have been added in the initial TF commit, and do not seem to have been touched since. This PR/commit removes them, assuming they are indeeded no longer required / dead-code.", "comments": ["/cc @cheshire @chsigg ", "@timshen91 thoughts? To me this seems like a good idea.", "LGTM", "This code is used internally. Unfortunately we need to keep it in the stream_executor interface.", "Sorry, I forgot to close this PR. This code is needed internally."]}, {"number": 52184, "title": "AttributeError: 'Conv2D' object has no attribute 'shape' and ValueError: You are trying to load a weight file containing 1 layers into a model with 19 layers.", "body": "TENSORFLOW VERSION: 2.1.0\r\nHey there i am trying to load a model like the following - \r\nBelow is my model architecture - \r\n```py\r\ndef down_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1, input = False):\r\n    if input == False:\r\n        c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\r\n    else:\r\n        c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\", input_shape=(HEIGHT, WIDTH, 3))\r\n    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\r\n    p = MaxPool2D((2, 2), (2, 2))(c)\r\n    return c, p\r\n\r\ndef up_block(x, skip, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\r\n    us = UpSampling2D((2, 2))(x)\r\n    concat = Concatenate()([us, skip])\r\n    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(concat)\r\n    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\r\n    return c\r\n\r\ndef bottleneck(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\r\n    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\r\n    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\r\n    return c\r\n\r\ndef UNet():\r\n    f = [16, 32, 64, 128, 256]\r\n \r\n    \r\n\r\n    c1, p1 = down_block(None, f[0], input = True) #128 -> 64\r\n    c2, p2 = down_block(p1, f[1]) #64 -> 32\r\n    c3, p3 = down_block(p2, f[2]) #32 -> 16\r\n    c4, p4 = down_block(p3, f[3]) #16->8\r\n    \r\n    bn = bottleneck(p4, f[4])\r\n    \r\n    u1 = up_block(bn, c4, f[3]) #8 -> 16\r\n    u2 = up_block(u1, c3, f[2]) #16 -> 32\r\n    u3 = up_block(u2, c2, f[1]) #32 -> 64\r\n    u4 = up_block(u3, c1, f[0]) #64 -> 128\r\n    \r\n    outputs = Conv2D(13, (1, 1), padding=\"same\", activation=\"sigmoid\")(u4)\r\n    model = Model(inputs, outputs)\r\n    return model\r\n```\r\nThen i make a variable named unet and pass in the model architecture - \r\n```py\r\nunet = UNet()\r\nunet.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"acc\"])\r\nunet.summary()\r\n```\r\n\r\nI get the following error - \r\n```py\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-26-33ba33cd9519> in <module>\r\n----> 1 unet = UNet()\r\n      2 unet.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"acc\"])\r\n      3 unet.summary()\r\n\r\n<ipython-input-25-6aea0c9181eb> in UNet()\r\n     25 \r\n     26 \r\n---> 27     c1, p1 = down_block(None, f[0], input = True) #128 -> 64\r\n     28     c2, p2 = down_block(p1, f[1]) #64 -> 32\r\n     29     c3, p3 = down_block(p2, f[2]) #32 -> 16\r\n\r\nAttributeError: 'Conv2D' object has no attribute 'shape'\r\n```\r\n\r\nFull traceback below -\r\nhttps://pastebin.com/mkTSg4bS\r\n\r\nby the way originally i started out with the following code the thing is the actual code was -\r\n```py\r\ndef down_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\r\n    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\r\n    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\r\n    p = MaxPool2D((2, 2), (2, 2))(c)\r\n    return c, p\r\n\r\ndef up_block(x, skip, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\r\n    us = UpSampling2D((2, 2))(x)\r\n    concat = Concatenate()([us, skip])\r\n    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(concat)\r\n    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\r\n    return c\r\n\r\ndef bottleneck(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\r\n    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\r\n    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\r\n    return c\r\n\r\ndef UNet():\r\n    f = [16, 32, 64, 128, 256]\r\n    inputs = Input((HEIGHT,WIDTH,3))\r\n    \r\n    p0 = inputs\r\n    c1, p1 = down_block(p0, f[0]) #128 -> 64\r\n    c2, p2 = down_block(p1, f[1]) #64 -> 32\r\n    c3, p3 = down_block(p2, f[2]) #32 -> 16\r\n    c4, p4 = down_block(p3, f[3]) #16->8\r\n    \r\n    bn = bottleneck(p4, f[4])\r\n    \r\n    u1 = up_block(bn, c4, f[3]) #8 -> 16\r\n    u2 = up_block(u1, c3, f[2]) #16 -> 32\r\n    u3 = up_block(u2, c2, f[1]) #32 -> 64\r\n    u4 = up_block(u3, c1, f[0]) #64 -> 128\r\n    \r\n    outputs = Conv2D(13, (1, 1), padding=\"same\", activation=\"sigmoid\")(u4)\r\n    model = Model(inputs, outputs)\r\n    return model\r\n```\r\nWhen i would try to load my model using this code- \r\n```py\r\nunet = UNet()\r\nunet.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"acc\"])\r\nunet.summary()\r\n\r\n# unet.load_weights('unet1m.h5')\r\nunet.load_weights('unet.h5')\r\n```\r\n i would get the following  errro -\r\n```\r\nValueError: You are trying to load a weight file containing 1 layers into a model with 19 layers.\r\n```\r\nThen i found this issue on github [ISSUE](https://github.com/keras-team/keras/issues/10417#issuecomment-415905336).\r\n It suggested not to use an input layer instead use input_shape in conv2d \r\nso i edited the code as you see in the original post\r\ni did not want to modify my whole code so came up with a messy solution\r\ni added an argument named input\r\nthen in the function i wrote that if input = False just do the bussiness it was doing in the original code\r\nif  it was input = true we add that extra input_shape argument in Conv2D \r\nNow for not to remove the x parameter i just simply passed x as none in the input block\r\nThen i found this issue on github. It suggested not to use an input layer instead use input_shape in conv2d \r\nso i edited the code as you see at the top\r\n\r\n**FOR EXPERIMENTING WITH THE CODE **\r\nThe model -\r\n[MODEL](https://drive.google.com/file/d/1RaRn9eI40ZDXAX-ajnCT0oWJclR8w9RF/view)\r\nThe script - \r\n[SCRIPT](https://gist.github.com/Space-Fighter/53d7a25dd50278c9106103dc6bb5afbd)\r\nThe training script - \r\n[TRAINING SCRIPT](https://github.com/srihari-humbarwadi/cityscapes-segmentation-with-Unet/blob/master/batch_training.py)\r\nThe repositry from where i have taken the model and code - \r\n[REPOSITRY](https://github.com/srihari-humbarwadi/cityscapes-segmentation-with-Unet)", "comments": ["```\r\n   if input == False:\r\n        c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\r\n    else:\r\n        c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\", input_shape=(HEIGHT, WIDTH, 3))\r\n  \r\n```\r\nI think that you forgot the (x) in \r\n       c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\", input_shape=(HEIGHT, WIDTH, 3))", "Hi @Space-Fighter!  \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks! ", "> Hi @Space-Fighter! We see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks!\r\n\r\nHey there i have added the tensorflow version rest everything was already there like steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].", "@vulkomilev as i have mentioned in my post - \r\n\r\noriginally i started out with the following code -\r\n```py\r\ndef down_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\r\n    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\r\n    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\r\n    p = MaxPool2D((2, 2), (2, 2))(c)\r\n    return c, p\r\n\r\ndef up_block(x, skip, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\r\n    us = UpSampling2D((2, 2))(x)\r\n    concat = Concatenate()([us, skip])\r\n    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(concat)\r\n    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\r\n    return c\r\n\r\ndef bottleneck(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\r\n    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\r\n    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\r\n    return c\r\n\r\ndef UNet():\r\n    f = [16, 32, 64, 128, 256]\r\n    inputs = Input((HEIGHT,WIDTH,3))\r\n    \r\n    p0 = inputs\r\n    c1, p1 = down_block(p0, f[0]) #128 -> 64\r\n    c2, p2 = down_block(p1, f[1]) #64 -> 32\r\n    c3, p3 = down_block(p2, f[2]) #32 -> 16\r\n    c4, p4 = down_block(p3, f[3]) #16->8\r\n    \r\n    bn = bottleneck(p4, f[4])\r\n    \r\n    u1 = up_block(bn, c4, f[3]) #8 -> 16\r\n    u2 = up_block(u1, c3, f[2]) #16 -> 32\r\n    u3 = up_block(u2, c2, f[1]) #32 -> 64\r\n    u4 = up_block(u3, c1, f[0]) #64 -> 128\r\n    \r\n    outputs = Conv2D(13, (1, 1), padding=\"same\", activation=\"sigmoid\")(u4)\r\n    model = Model(inputs, outputs)\r\n    return model\r\n```\r\nWhen i would try to load my model using this code- \r\n```py\r\nunet = UNet()\r\nunet.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"acc\"])\r\nunet.summary()\r\n\r\n# unet.load_weights('unet1m.h5')\r\nunet.load_weights('unet.h5')\r\n```\r\n i would get the following  errro -\r\n```\r\nValueError: You are trying to load a weight file containing 1 layers into a model with 19 layers.\r\n```\r\nThen i found this issue on github. It suggested not to use an input layer instead use input_shape in conv2d \r\nso i edited the code as you see in the original post\r\ni did not want to modify my whole code so came up with a messy solution\r\ni added an argument named input\r\nthen in the function i wrote that if input = False just do the bussiness it was doing in the original code\r\nif  it was input = true we add that extra input_shape argument in Conv2D \r\nNow for not to remove the x parameter i just simply passed x as none in the input block\r\nThen i found this issue on github. It suggested not to use an input layer instead use input_shape in conv2d \r\nso i edited the code as you see at the top", "Hi @sanatmpa1! ,Could you please look at this issue , was getting different Error in TF [2.5 ](https://colab.sandbox.google.com/drive/1iXaogg_FOcSZTsckaJkKq0-UbbqwMZ6t?resourcekey=0-sNtR27kcDZSZ-KRioS-ceA#scrollTo=FsoDIPU4861i),[2.6](https://colab.research.google.com/gist/mohantym/2c0d4715bcd8038e3c0f11b7fdd230d8/github_52184.ipynb#scrollTo=BQVBCqlB8uH4) and [nightly](https://colab.research.google.com/gist/mohantym/a971b6a86fe1362c41df58039a285700/github_52184.ipynb#scrollTo=BQVBCqlB8uH4).", "@Space-Fighter \r\n\r\nAs per the [script](https://pastebin.com/kCr8DDLk) that you provided, You have provided input `x` as `None` in this line `c1, p1 = down_block(None, f[0], input = True) #128 -> 64` and so while trying to reproduce your code, I am getting the error `TypeError: Inputs to a layer should be tensors. Got: None` [gist here](https://colab.research.google.com/gist/sanatmpa1/e92bbc22183fc6d32e8fa8344ef0bb4e/52184.ipynb). Can you provide a github gist to reproduce the issue reported?", "> @Space-Fighter\r\n> \r\n> As per the [script](https://pastebin.com/kCr8DDLk) that you provided, You have provided input `x` as `None` in this line `c1, p1 = down_block(None, f[0], input = True) #128 -> 64` and so while trying to reproduce your code, I am getting the error `TypeError: Inputs to a layer should be tensors. Got: None` [gist here](https://colab.research.google.com/gist/sanatmpa1/e92bbc22183fc6d32e8fa8344ef0bb4e/52184.ipynb). Can you provide a github gist to reproduce the issue reported?\r\n\r\n@sanatmpa1 Minor correction in the post i mentioned the following line - \r\n![image](https://user-images.githubusercontent.com/63576756/136048511-c37f4e93-a6a9-41b0-aced-7834b057de79.png)\r\nbut i mistakenly sent my old script - \r\n![image](https://user-images.githubusercontent.com/63576756/136048696-6c9ebf1d-17e2-417c-afdf-54323d328824.png)\r\n\r\n\r\nHere is the script - \r\n[SCRIPT](https://gist.github.com/Space-Fighter/53d7a25dd50278c9106103dc6bb5afbd)\r\n\r\nI have updated this in the post too\r\n\r\nRegarding the following error - \r\n```TypeError: Inputs to a layer should be tensors. Got: None```\r\n\r\nAfter doing the correction as mentioned above in the google colab you sent me I get a new error - \r\n\r\n![image](https://user-images.githubusercontent.com/63576756/136051306-3e2979a7-4939-45e1-9ceb-7c2efe95730e.png)\r\n\r\nBut in my local machine i get the same error as i have specified in my post  -\r\n\r\n![image](https://user-images.githubusercontent.com/63576756/136051842-25c9dbe0-7f2e-4f02-a50e-06da9489e833.png)\r\n\r\n", "@Space-Fighter,\r\n\r\nAs per the updated script, you don't have proper input passed to convolution layer in line 11 i.e `c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\", input_shape=(HEIGHT, WIDTH, 3))`, with which the error `Inputs to a layer should be tensors. Got: <keras.layers.convolutional.Conv2D object at 0x7f811955afd0>` is intended.\r\n\r\n> Then i found this issue on github. It suggested not to use an input layer instead use input_shape in conv2d\r\n\r\nRegarding above line in this [comment](https://github.com/tensorflow/tensorflow/issues/52184#issuecomment-931181988), Can you provide the issue that you have mentioned there?", "> @Space-Fighter,\r\n> \r\n> As per the updated script, you don't have proper input passed to convolution layer in line 11 i.e `c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\", input_shape=(HEIGHT, WIDTH, 3))`, with which the error `Inputs to a layer should be tensors. Got: <keras.layers.convolutional.Conv2D object at 0x7f811955afd0>` is intended.\r\n> \r\n> > Then i found this issue on github. It suggested not to use an input layer instead use input_shape in conv2d\r\n> \r\n> Regarding above line in this [comment](https://github.com/tensorflow/tensorflow/issues/52184#issuecomment-931181988), Can you provide the issue that you have mentioned there?\r\n\r\n@sanatmpa1  Here is the issue (updated in post too)\r\n[ISSUE](https://github.com/keras-team/keras/issues/10417#issuecomment-415905336)", "@Space-Fighter,\r\n\r\nThe [issue](https://github.com/keras-team/keras/issues/10417#issuecomment-415905336) mentioned above speaks about not using `Input layer`, whereas the two reasons for error that I noted in your [script](https://gist.github.com/Space-Fighter/53d7a25dd50278c9106103dc6bb5afbd) is,\r\n\r\n1.)  In line 11, you haven't provided any inputs to the conv layer and so it throws `Inputs to a layer should be tensors. Got: <keras.layers.convolutional.Conv2D object at 0x7fb209e6ec50>`, you should add input x to the layer like below, which is the ideal way of writing keras functional API.you can refer [here](https://keras.io/guides/functional_api/).\r\n\r\nc = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\", input_shape=(HEIGHT, WIDTH, 3))**(x)**\r\n\r\n2.) In line 33, you have passed input as `None` and so even after correcting the above line, we got the error `TypeError: Inputs to a layer should be tensors. Got: None`. So you should pass some input to your models and test if its working and not a None value.\r\n\r\nSo far, I feel that this is not a bug from TF end and its mostly an issue with the code that you have written. ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@sanatmpa1 Trying out the first suggestion i get the following error - \r\n```py\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-7-33ba33cd9519> in <module>\r\n----> 1 unet = UNet()\r\n      2 unet.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"acc\"])\r\n      3 unet.summary()\r\n\r\n<ipython-input-6-6627c0447d90> in UNet()\r\n     28 \r\n     29 \r\n---> 30     c1, p1 = down_block(None, f[0], input = True) #128 -> 64\r\n     31     c2, p2 = down_block(p1, f[1]) #64 -> 32\r\n     32     c3, p3 = down_block(p2, f[2]) #32 -> 16\r\n\r\n<ipython-input-6-6627c0447d90> in down_block(x, filters, kernel_size, padding, strides, input)\r\n      5         c = tf.keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\r\n      6     else:\r\n----> 7         c = tf.keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\", input_shape=(512,512, 3))(x)\r\n      8         c = tf.keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\r\n      9         p = tf.keras.layers.MaxPool2D((2, 2), (2, 2))(c)\r\nAttributeError: 'NoneType' object has no attribute 'shape'\r\n```\r\n[FULL TRACEBACK](https://gist.github.com/Space-Fighter/847755b29adb561d1fe65f0fa316db46)\r\n\r\nTrying out the second suggestion my code becomes - \r\n\r\n[CODE](https://gist.github.com/Space-Fighter/7f926edf96260758f901b06ae5a72157)\r\n\r\nError i get - \r\n\r\n[ERROR](https://gist.github.com/Space-Fighter/05cbf8a19172ceab33520c3197d87003)", "@Space-Fighter,\r\n\r\nI passed a random image and I am getting a different error when running [this code](https://gist.github.com/Space-Fighter/7f926edf96260758f901b06ae5a72157), `ValueError: Input tensors to a Functional must come from 'tf.keras.Input'` . Please find the [gist here](https://colab.sandbox.google.com/gist/sanatmpa1/0869b83260679fe1e824b1ebfbc032b6/github_52184.ipynb).\r\n\r\nInstead of sharing the code in a seperate link, Can you share a colab gist which can expedite the trouble shooting process?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "> > @Space-Fighter\r\n> > As per the [script](https://pastebin.com/kCr8DDLk) that you provided, You have provided input `x` as `None` in this line `c1, p1 = down_block(None, f[0], input = True) #128 -> 64` and so while trying to reproduce your code, I am getting the error `TypeError: Inputs to a layer should be tensors. Got: None` [gist here](https://colab.research.google.com/gist/sanatmpa1/e92bbc22183fc6d32e8fa8344ef0bb4e/52184.ipynb). Can you provide a github gist to reproduce the issue reported?\r\n> \r\n> @sanatmpa1 Minor correction in the post i mentioned the following line - ![image](https://user-images.githubusercontent.com/63576756/136048511-c37f4e93-a6a9-41b0-aced-7834b057de79.png) but i mistakenly sent my old script - ![image](https://user-images.githubusercontent.com/63576756/136048696-6c9ebf1d-17e2-417c-afdf-54323d328824.png)\r\n> \r\n> Here is the script - [SCRIPT](https://gist.github.com/Space-Fighter/53d7a25dd50278c9106103dc6bb5afbd)\r\n> \r\n> I have updated this in the post too\r\n> \r\n> Regarding the following error - `TypeError: Inputs to a layer should be tensors. Got: None`\r\n> \r\n> After doing the correction as mentioned above in the google colab you sent me I get a new error -\r\n> \r\n> ![image](https://user-images.githubusercontent.com/63576756/136051306-3e2979a7-4939-45e1-9ceb-7c2efe95730e.png)\r\n> \r\n> But in my local machine i get the same error as i have specified in my post -\r\n> \r\n> ![image](https://user-images.githubusercontent.com/63576756/136051842-25c9dbe0-7f2e-4f02-a50e-06da9489e833.png)\r\n\r\n@sanatmpa1 As i see in the post quoted above i seem to get a different error in my local machine and a diffrent one in colab. \r\n\r\nAs i see the version of tensorflow in colab is - \r\n![image](https://user-images.githubusercontent.com/63576756/139585230-ba01ba09-85b5-48b9-bb23-74b6424ad44b.png)\r\n\r\nand in my machine it is - \r\n\r\n![image](https://user-images.githubusercontent.com/63576756/139585385-66cdae7b-431a-44e2-951c-bc51ac2fcbd4.png)\r\n\r\n", "@Space-Fighter,\r\n\r\nCan you try updating TF to latest stable version i.e 2.6.0 in your local machine as well? as 2.1.0 is an older version and there are many different problems fixed and features added in `2.6.0`", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52184\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52184\">No</a>\n"]}, {"number": 52183, "title": "Object Detection Android App Crash ", "body": "I am trying to convert my custom  `MobileNet Single Shot Detector (v2)` to `tflite` using Roboflow tutorial colab (https://colab.research.google.com/drive/1qXn9q6m5ug7EWJsJov6mHaotHhCUY-wG?usp=sharing). After Conversion I deploy it on Tensorflow android app demo (https://github.com/tensorflow/examples.git). When I run the app it always crashes after lunch immediately showing the following error:\r\n\r\nE/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: org.tensorflow.lite.examples.detection, PID: 10743\r\n    java.lang.AssertionError: Error occurred when initializing ObjectDetector: Input tensor has type kTfLiteFloat32: it requires specifying NormalizationOptions metadata to preprocess input images.\r\n        at org.tensorflow.lite.task.vision.detector.ObjectDetector.initJniWithByteBuffer(Native Method)\r\n        at org.tensorflow.lite.task.vision.detector.ObjectDetector.access$100(ObjectDetector.java:86)\r\n        at org.tensorflow.lite.task.vision.detector.ObjectDetector$3.createHandle(ObjectDetector.java:211)\r\n        at org.tensorflow.lite.task.core.TaskJniUtils.createHandleFromLibrary(TaskJniUtils.java:91)\r\n        at org.tensorflow.lite.task.vision.detector.ObjectDetector.createFromBufferAndOptions(ObjectDetector.java:207)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.<init>(TFLiteObjectDetectionAPIModel.java:87)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:81)\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:99)\r\n        at org.tensorflow.lite.examples.detection.CameraActivity$7.onPreviewSizeChosen(CameraActivity.java:446)\r\n        at org.tensorflow.lite.examples.detection.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:357)\r\n        at org.tensorflow.lite.examples.detection.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:362)\r\n        at org.tensorflow.lite.examples.detection.CameraConnectionFragment.access$300(CameraConnectionFragment.java:66)\r\n        at org.tensorflow.lite.examples.detection.CameraConnectionFragment$3.onSurfaceTextureAvailable(CameraConnectionFragment.java:171)\r\n        at android.view.TextureView.getTextureLayer(TextureView.java:415)\r\n        at android.view.TextureView.draw(TextureView.java:360)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:21389)\r\n        at android.view.View.draw(View.java:22254)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4541)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:21380)\r\n        at android.view.View.draw(View.java:22254)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4541)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:21380)\r\n        at android.view.View.draw(View.java:22254)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4541)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)\r\n        at android.view.View.draw(View.java:22538)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:21389)\r\n        at android.view.View.draw(View.java:22254)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4541)\r\n        at androidx.coordinatorlayout.widget.CoordinatorLayout.drawChild(CoordinatorLayout.java:1277)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)\r\n        at android.view.View.draw(View.java:22538)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:21389)\r\n        at android.view.View.draw(View.java:22254)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4541)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:21380)\r\n        at android.view.View.draw(View.java:22254)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4541)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:21380)\r\n        at android.view.View.draw(View.java:22254)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4541)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:21380)\r\n        at android.view.View.draw(View.java:22254)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4541)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:21380)\r\n        at android.view.View.draw(View.java:22254)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4541)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)\r\n        at android.view.View.draw(View.java:22538)\r\n        at com.android.internal.policy.DecorView.draw(DecorView.java:848)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:21389)\r\n        at android.view.ThreadedRenderer.updateViewTreeDisplayList(ThreadedRenderer.java:559)\r\n        at android.view.ThreadedRenderer.updateRootDisplayList(ThreadedRenderer.java:565)\r\n        at android.view.ThreadedRenderer.draw(ThreadedRenderer.java:647)\r\n        at android.view.ViewRootImpl.draw(ViewRootImpl.java:4417)\r\n        at android.view.ViewRootImpl.performDraw(ViewRootImpl.java:4144)\r\n        at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:3391)\r\n        at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:2182)\r\n        at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:8730)\r\n        at android.view.Choreographer$CallbackRecord.run(Choreographer.java:1352)\r\n        at android.view.Choreographer.doCallbacks(Choreographer.java:1149)\r\n        at android.view.Choreographer.doFrame(Choreographer.java:1049)\r\n        at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:1333)\r\n        at android.os.Handler.handleCallback(Handler.java:938)\r\n        at android.os.Handler.dispatchMessage(Handler.java:99)\r\n        at android.os.Looper.loop(Looper.java:233)\r\n        at android.app.ActivityThread.main(ActivityThread.java:8010)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:631)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:978)\r\nI/Process: Sending signal. PID: 10743 SIG: 9", "comments": ["@sushreebarsa  Could you provide assistance?", "java.lang.AssertionError: Error occurred when initializing ObjectDetector: Input tensor has type kTfLiteFloat32: it requires specifying NormalizationOptions metadata to preprocess input images.\r\n\r\nDid you specify this?\r\nAre you sure the two programs are compatible ", "@myasser63 In order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),Thanks!", "I have added the new issue on Tensorflow lite converter", "> java.lang.AssertionError: Error occurred when initializing ObjectDetector: Input tensor has type kTfLiteFloat32: it requires specifying NormalizationOptions metadata to preprocess input images.\r\n> \r\n> Did you specify this? Are you sure the two programs are compatible\r\n\r\nHow can I specify the normalization options?", "@myasser63 We see that new[ issue ](https://github.com/tensorflow/tensorflow/issues/52194) has been created , Could you please confirm if we can close this ticket as we will track the other one ?Thanks!", "Sure, you can close this issue\r\n"]}, {"number": 52182, "title": "Didn't find op for builtin opcode 'QUANTIZE' version '2'", "body": "### 1. System information\r\n\r\n- TensorFlow installation (pip package or built from source): 2.5.0\r\n\r\n#### 2. Description \r\n**I have tried to import a TensorFlow lite model using int8 quantization into a Nucleo l496zg and I received the following error.**  \r\n\r\n\r\n### 3. Error\r\n\"Didn't find op for builtin opcode 'QUANTIZE' version '2'. An older version of this builtin might be supported. Are you using an old TFLite binary with a newer model?\"\r\n\r\nFailed to get registration from op code QUANTIZE", "comments": ["The quantize version 2 was added in 2019 and 2.5.0 is release in 2021. In theory this shouldn't happen. \r\nCould you follow the [bug issue template](https://github.com/tensorflow/tensorflow/issues/new?assignees=&labels=type%3Abug&template=00-bug-issue.md) to fill out more information? \r\nAre you making custom build? Could you provide instructions to reproduce this issue?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52182\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52182\">No</a>\n"]}, {"number": 52181, "title": "[TF:TRT] Always set binding dimension", "body": "This PR fixes a bug related to shape value tensor handling and input shape specification: we need to set binding dimensions even if tensor is not an execution tensor\r\n\r\nBackground. According to nvinfer1::ITensor::isExecutionTensor API description:\r\n\r\n> A tensor with isShapeTensor() == false and isExecutionTensor() == false can still show up as an input to the engine if its dimensions are required.\r\n\r\nThis PR fixes optimization profile definition for that case, by always calling execution_context->setBindingDimension().\r\n\r\nA unit test is added to for such a case.\r\n\r\nTracker #45481\r\n\r\nTagging @bixia1 for review.", "comments": []}, {"number": 52180, "title": "Tensorflow GPU 2.4 from source - fatal error in compilation", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): W10 Pro 21H1\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.4\r\n- Python version: 3.7.8 64-bits\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.0/8\r\n- GPU model and memory: Nvidia Quadro K4200\r\n\r\n\r\n**Describe the problem**\r\n\r\nI try to compile Tensorflow 2.4 from source for GPU compute capability 3.0 compatibility. I run into a weird fail (log below) I don't understand and for wich I don't find any ressource online. It seems to be related to Cython maybe.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nFrom https://www.tensorflow.org/install/source_windows\r\n\r\n```git clone https://github.com/tensorflow/tensorflow.git\r\ngit checkout r2.4\r\n.\\configure.py\r\nbazel build --config=opt --copt=-nvcc_options=disable-warnings --copt=-DTF_EXTRA_CUDA_CAPABILITIES=3.0 --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n**Any other info / logs**\r\n\r\n.configure.py log\r\n```\r\nPS C:\\Users\\*username*\\tensorflow> C:\\Users\\*username*\\AppData\\Local\\Programs\\Python\\Python37\\python.exe .\\configure.py\r\nYou have bazel 3.1.0 installed.\r\nPlease specify the location of python. [Default is C:\\Users\\*username*\\AppData\\Local\\Programs\\Python\\Python37\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  C:\\Users\\*username*\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\*username*\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nFound CUDA 11.0 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/include\r\nFound cuDNN 8 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 3.0\r\n\r\n\r\nWARNING: XLA does not support CUDA compute capabilities lower than 3.5. Disable XLA when running on older GPUs.\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\r\nEigen strong inline overridden.\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]:\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=mkl_aarch64    # Build with oneDNN support for Aarch64.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n```\r\n\r\nBuild log with the error\r\n```\r\nPS C:\\Users\\*username*\\tensorflow> bazel build --config=opt --copt=-nvcc_options=disable-warnings --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: The following configs were expanded more than once: [cuda, using_cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=209\r\nINFO: Reading rc options for 'build' from c:\\users\\*username*\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/*username*/AppData/Local/Programs/Python/Python39/python.exe\r\nINFO: Reading rc options for 'build' from c:\\users\\*username*\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from c:\\users\\*username*\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/*username*/AppData/Local/Programs/Python/Python37/python.exe --action_env PYTHON_LIB_PATH=C:/Users/*username*/AppData/Local/Programs/Python/Python37/lib/site-packages --python_path=C:/Users/*username*/AppData/Local/Programs/Python/Python37/python.exe --config=xla --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.0 --config=cuda --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file c:\\users\\*username*\\tensorflow\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file c:\\users\\*username*\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file c:\\users\\*username*\\tensorflow\\.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:cuda in file c:\\users\\*username*\\tensorflow\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file c:\\users\\*username*\\tensorflow\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\nINFO: Found applicable config definition build:opt in file c:\\users\\*username*\\tensorflow\\.tf_configure.bazelrc: --copt=/arch:AVX --host_copt=/arch:AVX --define with_default_optimizations=true\r\nINFO: Found applicable config definition build:cuda in file c:\\users\\*username*\\tensorflow\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file c:\\users\\*username*\\tensorflow\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\nINFO: Found applicable config definition build:windows in file c:\\users\\*username*\\tensorflow\\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\users\\*username*\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Build option --copt has changed, discarding analysis cache.\r\nINFO: Repository cython instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule tf_http_archive defined at:\r\n  C:/users/*username*/tensorflow/third_party/repo.bzl:131:19: in <toplevel>\r\nINFO: Repository 'cython' used the following cache hits instead of downloading the corresponding file.\r\n * Hash 'bccc9aa050ea02595b2440188813b936eaf345e85fb9692790cecfe095cf91aa' for https://storage.googleapis.com/mirror.tensorflow.org/github.com/cython/cython/archive/0.28.4.tar.gz\r\nIf the definition of 'cython' was updated, verify that the hashes were also updated.\r\nERROR: An error occurred during the fetch of repository 'cython':\r\n   Traceback (most recent call last):\r\n        File \"C:/users/*username*/tensorflow/third_party/repo.bzl\", line 105\r\n                _apply_delete(ctx, <1 more arguments>)\r\n        File \"C:/users/*username*/tensorflow/third_party/repo.bzl\", line 74, in _apply_delete\r\n                _execute_and_check_ret_code(ctx, <1 more arguments>)\r\n        File \"C:/users/*username*/tensorflow/third_party/repo.bzl\", line 52, in _execute_and_check_ret_code\r\n                fail(<1 more arguments>)\r\nNon-zero return code(256) when executing 'C:\\msys64\\usr\\bin\\bash.exe -l -c \"rm\" \"-rf\" \"C:/cygwin64/home/*username*/_bazel_*username*/feynteyc/external/cython/BUILD.bazel\"':\r\nStdout:\r\nStderr:       1 [main] bash (14200) C:\\msys64\\usr\\bin\\bash.exe: *** fatal error - add_item (\"\\??\\C:\\msys64\", \"/\", ...) failed, errno 1\r\nStack trace:\r\nFrame        Function    Args\r\n000FFFF8630  00180063085 (00180297142, 00180272E41, 0000000003F, 000FFFF8B10)\r\n000FFFF8B60  001800488F2 (000FFFFFFFF, 00180020010, 000FFFFABCA, 000FFFF9BB0)\r\n000FFFF9B70  00180048931 (000FFFF9BB0, 00000000001, 0000000003F, 00000000001)\r\n000FFFF9C00  001800E658D (00000000000, 00140000024, 00000000000, 000FFFFCC50)\r\n000FFFFCC70  00180136D95 (00000000000, 00000000000, 00000000000, 00000000000)\r\n000FFFFCCE0  00180048F75 (00000000000, 00000000000, 00000000000, 00000000000)\r\n000FFFFCDA0  0018004794A (00000000000, 00000000000, 00000000000, 00000000000)\r\n000FFFFCE50  00180047A0C (00000000000, 00000000000, 00000000000, 00000000000)\r\nEnd of stack trace\r\nERROR: C:/users/*username*/tensorflow/tensorflow/python/BUILD:7748:1: //tensorflow/python:framework/fast_tensor_util.pyx_cython_translation depends on @cython//:cython_binary in repository @cython which failed to fetch. no such package '@cython//': Traceback (most recent call last):\r\n        File \"C:/users/*username*/tensorflow/third_party/repo.bzl\", line 105\r\n                _apply_delete(ctx, <1 more arguments>)\r\n        File \"C:/users/*username*/tensorflow/third_party/repo.bzl\", line 74, in _apply_delete\r\n                _execute_and_check_ret_code(ctx, <1 more arguments>)\r\n        File \"C:/users/*username*/tensorflow/third_party/repo.bzl\", line 52, in _execute_and_check_ret_code\r\n                fail(<1 more arguments>)\r\nNon-zero return code(256) when executing 'C:\\msys64\\usr\\bin\\bash.exe -l -c \"rm\" \"-rf\" \"C:/cygwin64/home/*username*/_bazel_*username*/feynteyc/external/cython/BUILD.bazel\"':\r\nStdout:\r\nStderr:       1 [main] bash (14200) C:\\msys64\\usr\\bin\\bash.exe: *** fatal error - add_item (\"\\??\\C:\\msys64\", \"/\", ...) failed, errno 1\r\nStack trace:\r\nFrame        Function    Args\r\n000FFFF8630  00180063085 (00180297142, 00180272E41, 0000000003F, 000FFFF8B10)\r\n000FFFF8B60  001800488F2 (000FFFFFFFF, 00180020010, 000FFFFABCA, 000FFFF9BB0)\r\n000FFFF9B70  00180048931 (000FFFF9BB0, 00000000001, 0000000003F, 00000000001)\r\n000FFFF9C00  001800E658D (00000000000, 00140000024, 00000000000, 000FFFFCC50)\r\n000FFFFCC70  00180136D95 (00000000000, 00000000000, 00000000000, 00000000000)\r\n000FFFFCCE0  00180048F75 (00000000000, 00000000000, 00000000000, 00000000000)\r\n000FFFFCDA0  0018004794A (00000000000, 00000000000, 00000000000, 00000000000)\r\n000FFFFCE50  00180047A0C (00000000000, 00000000000, 00000000000, 00000000000)\r\nEnd of stack trace\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@cython//': Traceback (most recent call last):\r\n        File \"C:/users/*username*/tensorflow/third_party/repo.bzl\", line 105\r\n                _apply_delete(ctx, <1 more arguments>)\r\n        File \"C:/users/*username*/tensorflow/third_party/repo.bzl\", line 74, in _apply_delete\r\n                _execute_and_check_ret_code(ctx, <1 more arguments>)\r\n        File \"C:/users/*username*/tensorflow/third_party/repo.bzl\", line 52, in _execute_and_check_ret_code\r\n                fail(<1 more arguments>)\r\nNon-zero return code(256) when executing 'C:\\msys64\\usr\\bin\\bash.exe -l -c \"rm\" \"-rf\" \"C:/cygwin64/home/*username*/_bazel_*username*/feynteyc/external/cython/BUILD.bazel\"':\r\nStdout:\r\nStderr:       1 [main] bash (14200) C:\\msys64\\usr\\bin\\bash.exe: *** fatal error - add_item (\"\\??\\C:\\msys64\", \"/\", ...) failed, errno 1\r\nStack trace:\r\nFrame        Function    Args\r\n000FFFF8630  00180063085 (00180297142, 00180272E41, 0000000003F, 000FFFF8B10)\r\n000FFFF8B60  001800488F2 (000FFFFFFFF, 00180020010, 000FFFFABCA, 000FFFF9BB0)\r\n000FFFF9B70  00180048931 (000FFFF9BB0, 00000000001, 0000000003F, 00000000001)\r\n000FFFF9C00  001800E658D (00000000000, 00140000024, 00000000000, 000FFFFCC50)\r\n000FFFFCC70  00180136D95 (00000000000, 00000000000, 00000000000, 00000000000)\r\n000FFFFCCE0  00180048F75 (00000000000, 00000000000, 00000000000, 00000000000)\r\n000FFFFCDA0  0018004794A (00000000000, 00000000000, 00000000000, 00000000000)\r\n000FFFFCE50  00180047A0C (00000000000, 00000000000, 00000000000, 00000000000)\r\nEnd of stack trace\r\nINFO: Elapsed time: 26.147s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (2 packages loaded, 15684 targets configured)\r\n    Fetching @llvm-project; fetching 24s\r\n    Fetching @local_config_git; fetching 24s\r\n    Fetching ...me/*username*/_bazel_*username*/feynteyc/external/llvm-project; Extracting C:/cygwin64/home/*username*/_bazel_*username*/feynteyc/external/llvm-project/f402e682d0ef5598eeffc9a21a691b03e602ff58.tar.gz 22s\r\n    Fetching @aws; fetching 22s\r\n    Fetching C:/cygwin64/home/*username*/_bazel_*username*/feynteyc/external/aws; Extracting C:/cygwin64/home/*username*/_bazel_*username*/feynteyc/external/aws/1.7.336.tar.gz 22s\r\n``", "comments": ["Hi @louim-lbs! Could you please look at this  [similar issue](https://github.com/tensorflow/tensorflow/issues/4279).", "Hi @mohantym! Thanks for your answer.\r\nI've looked at this issue, and tried to configure.py again before to build, but it changed nothing...", "Hi @Saduf2019! Could you please look at this issue!", "@louim-lbs \r\nPlease refer to [this comment](https://github.com/tensorflow/tensorflow/issues/49263#issuecomment-857868501) and let us know.\r\nCan you please upgrade to higher versions and let us know if it is still an issue.", "I need this specific version (<= 2.4) for compatibility issue with other packages. So I won't try newer ones, although it surely might work.\r\n\r\nBut I find an other way: using tensorflow windows wheel provided by @fo40225 at [https://github.com/fo40225/tensorflow-windows-wheel.git](url).", "@louim-lbs \r\nCan you please confirm if the above solutions helps resolve the issue faced by you.", "No, the solution do not work in order to compile <= 2.4 versions, obviously.\r\n\r\nI'll try with next versions of tensorflow if I have time, but it is out the field of my specific issue, since I need <= 2.4.\r\n\r\nThanks for your help!", "@louim-lbs \r\nSure please update us after upgrading tf version.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52180\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52180\">No</a>\n"]}, {"number": 52179, "title": "FloorMod on gpu", "body": "```\r\n^f41959ccb2 (Manjunath Kudlur       2015-11-06 16:27:58 -0800 20) namespace tensorflow {\r\n^f41959ccb2 (Manjunath Kudlur       2015-11-06 16:27:58 -0800 21) namespace functor {\r\nbdae9c62caa (Andrew Selle           2016-10-18 16:08:51 -0800 22) // TODO(b/32239807) No GPU ops for mod yet.\r\n^f41959ccb2 (Manjunath Kudlur       2015-11-06 16:27:58 -0800 23) }  // namespace functor\r\n^f41959ccb2 (Manjunath Kudlur       2015-11-06 16:27:58 -0800 24) }  // namespace tensorflow\r\n^f41959ccb2 (Manjunath Kudlur       2015-11-06 16:27:58 -0800 25)\r\n```\r\nTODO(b/32239807) on **2016-10-18**, and now **2021-9-23**\r\nWhen will we develop this feature?\r\n", "comments": ["@zhaozheng09 \r\nPlease feel free to submit a PR for the requested change or share the link where requested change is to be made.", "I would like to work on this TODO.", "@Saduf2019 Can you please review the PR?", "@AdeshChoudhar \r\nThe pr has been assigned for reviewing and once it is merged this issue will move to closed status.", "> I would like to work on this TODO.\r\n\r\n"]}, {"number": 52178, "title": "Different prediction on GPU between `tf.keras.models.load_model(..)` and `tf.saved_model.load(..)`", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.8.2\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: GeForce GTX 1050, 4 GB\r\n\r\n**Describe the current behavior**\r\n\r\nI have a CNN based regression model. Surprisingly, the model is predicting different outputs when loaded with `tf.keras.models.load_model(..)` and `tf.saved_model.load(..)`. However, this only occurs when I am using a GPU and also not always but ~5% of inference times. On CPU, they both produce the same outputs always. The difference is rather small, happens after `1e-7` but still big for my use case.\r\n\r\n**Describe the expected behavior**\r\nIrrespective of the loading method and whether GPU is used for inference or not, predicted values should always be the same.\r\n\r\n**Standalone code to reproduce the issue**\r\nThe link to colab: `https://colab.research.google.com/drive/1JwXNx-MbVqB7HDXF4z9lqa91oWsfvpA0?usp=sharing`\r\n\r\nColab uses different TF and python versions but the issue still exists.\r\n\r\n**Other info / logs** \r\n`AssertionError: [[0.12652352]] and [[0.12652355]]`", "comments": ["@sanatmpa1 ,\r\nI was able to reproduce the issue in tf [v2.4](https://colab.research.google.com/gist/tilakrayal/9376c548820cd18db2bb0340971e4125/untitled85.ipynb), v2.6 and face a different error in nightly.Please find the [gist](https://colab.research.google.com/gist/tilakrayal/3ee5b33e7e762a75b1d8d60c18b53dbd/nightly-and-2-6.ipynb).", "https://github.com/tensorflow/tensorflow/issues/52178#issue-1010644453", "@prakharverma Agree with you that there is small discrepancy in 8th decimal of the predictions between tensorflow model and keras model. As the initial model is a keras model, using `load_model` to load model results in identical results when compared to original model. \r\n\r\nI think (may be I am wrong) when you load a keras model as a TF model, under the hood, there might be some conversions happening which are resulting in very small discrepancy of the order of precision/round-off level. \r\n\r\nPlease check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/710530887644651324a56fe566532bc1/nightly-and-2-6.ipynb). In the gist, I compared predictions from loaded_keras_model (`trained_model1`) with original `model`. Thanks!", "Hi, @jvishnuvardhan! Thanks for the response! Yes, I agree original model and the Keras load model produces similar results. I just wasn't sure why the tf load model is producing different results, that too not always but only for certain inputs and also the precision varies with inputs. \r\n\r\nIs there a way to set the precision in the tf load model or check where this conversion is happening exactly?", "@prakharverma When loading keras model as a TF model, there are some conversions required to be compatible as a TF model. I guess one of those conversion steps is introducing this 8th decimal discrepancy some times. I think TF/Keras teams are working on high priority issues so they may not handle this in near future (i guess).\r\n\r\nIf you have bandwidth, debug the `load_model` path and `tf.saved_model.load` path to find the root-cause and raise a PR to contribute code change. Thanks!", "Sure! Also, just to put the context here for someone going through this later, this only happens if GPU is available. When in CPU, all the methods produce similar results so that conversion magic doesn't happen then. ", "I think this is resolved by loading the model using keras `load_model` (results will be identical). For some reason, If you want to load a keras model as a TF model, then you may use [`tf.debugging.assert_near`](https://www.tensorflow.org/api_docs/python/tf/debugging/assert_near) or [`tf.experimental.numpy.allclose`](https://www.tensorflow.org/api_docs/python/tf/experimental/numpy/allclose) with a tolerance. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52178\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52178\">No</a>\n"]}, {"number": 52176, "title": "Running with multiple GPUs, the model has been successfully loaded into the GPUs, but the program is stuck and there is no error message", "body": "**The issue:**\r\nRunning with multiple GPUs, the model has been successfully loaded into the GPUs, but the program is stuck and there is no error message.\r\n\r\n**Program running status:**\r\n[Program running status](https://drive.google.com/file/d/1i0e1ONe3WQ1qb5Ol_0b9CZWvf2qRqW9k/view?usp=sharing)\r\n\r\n**Execution environment:**\r\nOS: Ubuntu 18.04.5 LTS\r\nGPU: NVIDIA RTX A6000 * 2\r\nTF Version:  tensorflow-gpu  2.4.0\r\nPython Version: python 3.8.8\r\nCUDA Version: V11.1.74\r\nCuDNN Version: 8.1.0 (the 8.0.4 is also tried)\r\n\r\n**Program for running:**\r\n```\r\nimport tensorflow as tf\r\n#from tensorflow.keras.utils import multi_gpu_model\r\nfrom tensorflow.python.keras.utils.multi_gpu_utils import multi_gpu_model\r\ntf.debugging.set_log_device_placement(True)\r\nmirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"GPU:0\", \"GPU:1\"])\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nwith mirrored_strategy.scope():\r\n    model = tf.keras.models.Sequential([\r\n      tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n      tf.keras.layers.Dense(128, activation='relu'),\r\n      tf.keras.layers.BatchNormalization(renorm=False),\r\n      tf.keras.layers.Dropout(0.2),\r\n      tf.keras.layers.Dense(10)\r\n    ])\r\n\r\n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n\r\n\r\n    model.compile(optimizer='adam',\r\n              loss=loss_fn,\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train, epochs=5)\r\n\r\nmodel.evaluate(x_test,  y_test, verbose=2)\r\n\r\n```\r\n\r\nThe program is stuck and there is no progress. However, GPUs resources (memory) have been used by the program.\r\nThe same situation also appears in the jax/flax pmap program, but pytorch (nn.DataParallel) can be executed correctly.\r\nThe problem is as above, Thanks!\r\n\r\n\r\n**Running log:**\r\n[Running log](https://drive.google.com/file/d/1wNXoyNjrpv6LFV3_WRtHYREnKJ_RbTSh/view?usp=sharing)\r\n", "comments": ["@guan-yuan,\r\n\r\nI see that you're using `TF 2.4`, Many issues are fixed in latest stable version and so can you try with `TF 2.6.0` and let us know if the issue still persists? Thanks!", "> @guan-yuan,\r\n> \r\n> I see that you're using `TF 2.4`, Many issues are fixed in latest stable version and so can you try with `TF 2.6.0` and let us know if the issue still persists? Thanks!\r\n\r\n@sanatmpa1 \r\nThank you for your reply, I have tested it with tf 2.6 and the result is still the same (cannot be executed, but the model is loaded to GPUs)", "@guan-yuan,\r\n\r\nWhile updating TF. have you updated your corresponding CUDA version to 11.2 as well? You can refer to the [tested build configurations](https://www.tensorflow.org/install/source#gpu) which shows the configurations which are recommended.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 52175, "title": "Gradients are zero ", "body": "I have an ANN to classify food, however it doesn't work, i kept debugging till i found that the problem is with the weights aren't updated, so i printed the grads to find out that they are always zero except for the first time\r\n\r\nHere is the code\r\n   `def forward(x):\r\n  \r\n        return tf.matmul(x,W) + b`\r\n\r\n   `def activate(x):\r\n        return tf.nn.softmax(forward(x))`\r\n\r\n   `def model(x):\r\n   \r\n        x = flatten(x)\r\n    \r\n        return activate(x)`\r\n\r\n    `def cross_entropy(y_label, y_pred):\r\n  \r\n         return (-tf.reduce_sum(y_label * tf.math.log(y_pred + 1.e-10)))`\r\n\r\n    `optimizer = tf.keras.optimizers.SGD(learning_rate=0.5)`\r\n\r\n```\r\n  `def train_step(x, y ):\r\n             with tf.GradientTape() as tape:\r\n             #compute loss function\r\n             current_loss = cross_entropy( y, model(x))\r\n             print(current_loss)\r\n             # compute gradient of loss \r\n            #(This is automatic! Even with specialized funcctions!)\r\n            grads = tape.gradient( current_loss , [W,b])\r\n            # Apply SGD step to our Variables W and b\r\n            print(grads)\r\n        \r\n            optimizer.apply_gradients( zip( grads , [W,b] ) )   \r\n        \r\n           return current_loss.numpy()`\r\n```\r\n\r\n    `W = tf.Variable(tf.zeros([196608, 3],tf.float32))\r\n      #Bias tensor\r\n       b = tf.Variable(tf.zeros([3],tf.float32))\r\n\r\n       loss_values=[]\r\n       accuracies = []\r\n       epochs = 10\r\n       x_train = np.empty([50,256,256,3])\r\n       y_train = np.empty([50,1,3])\r\n\r\n    for i in range(epochs):\r\n         j=0\r\n         k = 0\r\n        # each batch has 50 examples\r\n        for x_train_batch, y_train_batch in train_ds:\r\n                    if j == 0 and i == 0 :\r\n                        for k in range(50):\r\n                             x_train[k] = x_train_batch[k]\r\n                             y_train[k] = y_train_batch[k]\r\n                        \r\n            \r\n        \r\n                    j+=1\r\n                    current_loss = train_step(x_train_batch/255.0, y_train_batch)\r\n                     if j%500 == 0: #reporting intermittent batch statistics\r\n                         print(\"epoch \", str(i), \"batch\", str(j), \"loss:\", str(current_loss) ) \r\n            \r\n        x_train = tf.convert_to_tensor(x_train)\r\n        y_train = tf.convert_to_tensor(y_train)`\r\n\r\nI also checked for the current lose (if you notice it's the first output just before the gradient) and it isn't zero as you see\r\nOUTPUT:\r\n`tf.Tensor(54.930614, shape=(), dtype=float32)\r\n[<tf.Tensor: shape=(196608, 3), dtype=float32, numpy=\r\narray([[ 2.7033854,  1.9847847, -4.6881704],\r\n       [ 2.6230955,  1.6556438, -4.278739 ],\r\n       [ 2.289314 ,  1.2310266, -3.5203404],\r\n       ...,\r\n       [ 1.6812016,  1.4880723, -3.1692739],\r\n       [ 1.6665692,  1.2746813, -2.9412503],\r\n       [ 1.73799  ,  0.5771449, -2.3151352]], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 2.6666658,  6.6666684, -9.333334 ], dtype=float32)>]\r\ntf.Tensor(483.5429, shape=(), dtype=float32)\r\n[<tf.Tensor: shape=(196608, 3), dtype=float32, numpy=\r\narray([[0., 0., 0.],\r\n       [0., 0., 0.],\r\n       [0., 0., 0.],\r\n       ...,\r\n       [0., 0., 0.],\r\n       [0., 0., 0.],\r\n       [0., 0., 0.]], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]\r\ntf.Tensor(713.80145, shape=(), dtype=float32)\r\n[<tf.Tensor: shape=(196608, 3), dtype=float32, numpy=\r\narray([[0., 0., 0.],\r\n       [0., 0., 0.],\r\n       [0., 0., 0.],\r\n       ...,\r\n       [0., 0., 0.],\r\n       [0., 0., 0.],\r\n       [0., 0., 0.]], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]\r\ntf.Tensor(483.5429, shape=(), dtype=float32)\r\n[<tf.Tensor: shape=(196608, 3), dtype=float32, numpy=\r\narray([[0., 0., 0.],\r\n       [0., 0., 0.],\r\n       [0., 0., 0.],\r\n       ...,\r\n       [0., 0., 0.],\r\n       [0., 0., 0.],\r\n       [0., 0., 0.]], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]\r\ntf.Tensor(483.5429, shape=(), dtype=float32)\r\n[<tf.Tensor: shape=(196608, 3), dtype=float32, numpy=\r\narray([[0., 0., 0.],\r\n       [0., 0., 0.],\r\n       [0., 0., 0.],\r\n       ...,\r\n       [0., 0., 0.],\r\n       [0., 0., 0.],\r\n       [0., 0., 0.]], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]`", "comments": ["@Shobaky, \r\nIn order to expedite the trouble-shooting process, could you please provide a complete code and the TensorFlow version you are using?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 52174, "title": "Different results between model.fit() and model.predict()", "body": "I get the problem when I try to fine-tuned my model.\r\nHere is my `model.fit` code and training progress bar.\r\n\r\n![2021-09-29 10-42-31 \u7684\u87a2\u5e55\u64f7\u5716](https://user-images.githubusercontent.com/42731603/135195990-85ea0501-fd37-4ac9-a575-48a719872c89.png)\r\n\r\n![2021-09-29 10-42-46 \u7684\u87a2\u5e55\u64f7\u5716](https://user-images.githubusercontent.com/42731603/135196092-1e32fdae-5149-445c-a17c-0647512b970d.png)\r\n\r\nI found the `keras.losses.CategoricalCrossentropy()` and `keras.metrics.CategoricalCrossentropy()` return different results when training epoch.\r\n\r\nMoreover, I use the same validation data in the `model.predict` function, and check the AUC-score, **the socre in the final epoch** and **the score which compute manually** not the same. \r\n\r\n![2021-09-29 10-42-59 \u7684\u87a2\u5e55\u64f7\u5716](https://user-images.githubusercontent.com/42731603/135196679-a9245642-9a94-4227-94ea-61a3500a9c51.png)\r\n\r\nAnd I research the similar [issue](https://github.com/keras-team/keras/issues/5140]) before, but I didn't found the solution.\r\n\r\nDoes anyone get the same problem?\r\n\r\nOS and environment information:\r\n- Ubuntu 20.04.3 LTS\r\n- Conda virtual environment\r\n- Cuda 11.2\r\n- Nvidia-driver 460\r\n- Tensorflow 2.5.0 (use Keras inside tensorflow)\r\n\r\nI fix it, sorry.\r\nThe problem is some function using keras, another is tensorflow.keras.\r\n", "comments": ["Hi @houzeyu2683! Could you  please fill the template from [here ](https://github.com/tensorflow/tensorflow/issues/new/choose)and share the complete stand alone code as It will help expedite the issue."]}, {"number": 52173, "title": "Upgrade abseil to 0.14.1", "body": "Please check:\r\nhttps://github.com/keras-team/keras/issues/15338\r\nhttps://github.com/abseil/abseil-py/issues/173\r\n\r\n/cc @haifeng-jin ", "comments": ["It is failing on windows only but I don't have a Windows VM with enough resources to debug the build. \r\nThe news is that new releases don't have anymore `LICENSE` duplicated in `absl/LICENSE` subdir.\r\n\r\n`ERROR: T:/src/github/tensorflow/tensorflow/tools/pip_package/BUILD:172:10: //tensorflow/tools/pip_package:simple_console_for_windows: missing input file 'external/absl_py/absl/LICENSE', owner: '@absl_py//absl:LICENSE'`\r\n\r\n/cc @mihaimaruseac ", "Check https://github.com/abseil/abseil-py/pull/160#issuecomment-930605454"]}, {"number": 52172, "title": "Add better determinism error messages to some random ops.", "body": "Better error messages were added to `tf.image.sample_distorted_bounding_box` and `tf.nn.fractional_max_pool` when determinism is enabled and no seed is passed. Such ops do not use the global seed by default, and so a seed must be passed to such ops.", "comments": []}, {"number": 52171, "title": "Provide pbroadcast functions for Neon", "body": "Fixes https://github.com/tensorflow/tensorflow/issues/52164", "comments": ["Made moot by https://github.com/tensorflow/tensorflow/commit/6d8220b1d847904e9bfec34d990b9436d95cd661"]}, {"number": 52170, "title": "Document description error", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/lite/guide/python\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/\r\n\r\n## Description of issue (what needs changing):\r\nTo convert other TensorFlow models to TensorFlow Lite, read about **the the** TensorFlow Lite Converter.\r\nThere's an extra word the\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\nThis is my PR: https://github.com/tensorflow/tensorflow/pull/52169\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": []}, {"number": 52169, "title": "Document description error", "body": "To convert other TensorFlow models to TensorFlow Lite, read about **the the** TensorFlow Lite Converter.\r\nThere's an extra word the", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52169) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 52168, "title": "Tensorflow lite conversion error when converting the bidirectional LSTM", "body": "### System information\r\n\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows\r\n-   **TensorFlow installed from (source or binary)**: binary\r\n-   **TensorFlow version (use command below)**: 2.3.0\r\n-   **Python version**: 3.8.10\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nI am trying to convert a tensorflow model into the tensorflow lite format. It prompts that it failed to duplicate values for the stateful op when the conversion goes to the bidirectional lstm layer. Here below is the network structure.\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n\r\nconv1d (Conv1D)              (None, 1558, 32)          5024      \r\n\r\nmax_pooling1d (MaxPooling1D) (None, 389, 32)           0       \r\n  \r\nbatch_normalization (BatchNo (None, 389, 32)           128       \r\n\r\nconv1d_1 (Conv1D)            (None, 386, 64)           8256      \r\n\r\nmax_pooling1d_1 (MaxPooling1 (None, 96, 64)            0         \r\n\r\nbatch_normalization_1 (Batch (None, 96, 64)            256       \r\n\r\nbidirectional (Bidirectional (None, 50)                18000     \r\n\r\ndense (Dense)                (None, 32)                1632      \r\n\r\ndense_1 (Dense)              (None, 25)                825       \r\n\r\nTotal params: 34,121\r\nTrainable params: 33,929\r\nNon-trainable params: 192\r\n__________________________\r\n\r\n### Source code / logs\r\nHere is the code to raised this error.\r\n`converter = tf.lite.TFLiteConverter.from_saved_model(save_model_dir)`\r\n`converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                           tf.lite.OpsSet.SELECT_TF_OPS]`\r\n` tflite_model = converter.convert()`\r\n `with open('converted_model.tflite', 'wb') as f:`\r\n`        f.write(tflite_model)`\r\n\r\n**Here below are the logs:**\r\n```\r\n2021-09-28 16:44:25.817579: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.\r\n2021-09-28 16:44:25.817782: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.\r\n2021-09-28 16:44:25.819144: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: D:\\VoiceRecognition\\trained_model\r\n2021-09-28 16:44:25.884916: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\r\n2021-09-28 16:44:25.885104: I tensorflow/cc/saved_model/loader.cc:234] Reading SavedModel debug info (if present) from: D:\\VoiceRecognition\\trained_model\r\n2021-09-28 16:44:26.135034: I tensorflow/cc/saved_model/loader.cc:199] Restoring SavedModel bundle.\r\n2021-09-28 16:44:26.476369: I tensorflow/cc/saved_model/loader.cc:183] Running initialization op on SavedModel bundle at path: D:\\VoiceRecognition\\trained_model\r\n2021-09-28 16:44:26.761966: I tensorflow/cc/saved_model/loader.cc:303] SavedModel load for tags { serve }; Status: success: OK. Took 942808 microseconds.\r\nloc(callsite(callsite(callsite(unknown at \"sequential/bidirectional/backward_lstm/PartitionedCall@__inference__wrapped_model_44427\") at \"StatefulPartitionedCall@__inference_signature_wrapper_52710\") at \"StatefulPartitionedCall\")): error: We cannot duplicate the value since it's not constant.\r\n\r\nerror: Failed to duplicate values for the stateful op\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\lizai\\anaconda3\\envs\\SpectrumAnalysis\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 196, in toco_convert_protos\r\n    model_str = wrap_toco.wrapped_toco_convert(model_flags_str,\r\n  File \"C:\\Users\\lizai\\anaconda3\\envs\\SpectrumAnalysis\\lib\\site-packages\\tensorflow\\lite\\python\\wrap_toco.py\", line 32, in wrapped_toco_convert\r\n    return _pywrap_toco_api.TocoConvert(\r\nException: <unknown>:0: error: loc(callsite(callsite(callsite(unknown at \"sequential/bidirectional/backward_lstm/PartitionedCall@__inference__wrapped_model_44427\") at \"StatefulPartitionedCall@__inference_signature_wrapper_52710\") at \"StatefulPartitionedCall\")): We cannot duplicate the value since it's not constant.\r\n\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: note: loc(callsite(callsite(callsite(unknown at \"sequential/bidirectional/backward_lstm/PartitionedCall@__inference__wrapped_model_44427\") at \"StatefulPartitionedCall@__inference_signature_wrapper_52710\") at \"StatefulPartitionedCall\")): see current operation: %23 = \"tfl.unidirectional_sequence_lstm\"(%22, %cst_20, %cst_21, %cst_22, %cst_23, %cst_12, %cst_13, %cst_14, %cst_15, %cst_7, %cst_7, %cst_7, %cst_16, %cst_17, %cst_18, %cst_19, %cst_7, %cst_7, %21, %21, %cst_7, %cst_7, %cst_7, %cst_7) {cell_clip = 1.000000e+01 : f32, fused_activation_function = \"TANH\", proj_clip = 0.000000e+00 : f32, time_major = false} : (tensor<?x96x64xf32>, tensor<25x64xf32>, tensor<25x64xf32>, tensor<25x64xf32>, tensor<25x64xf32>, tensor<25x25xf32>, tensor<25x25xf32>, tensor<25x25xf32>, tensor<25x25xf32>, none, none, none, tensor<25xf32>, tensor<25xf32>, tensor<25xf32>, tensor<25xf32>, none, none, tensor<?x25xf32>, tensor<?x25xf32>, none, none, none, none) -> tensor<?x96x25xf32>\r\n<unknown>:0: error: Failed to duplicate values for the stateful op\r\n\r\n```", "comments": ["I have solved this problem by converting the bidirectional lstm into to unidirectional lstm. The solution is referenced by [https://www.tensorflow.org/lite/convert/rnn](url)"]}, {"number": 52166, "title": "Model evaluate accuracy drops following Save + Load", "body": "After training EfficientNetB0 on a custom dataset, i want to ensure that the model i save give the same accuracy after i load it:\r\n\r\n```\r\n# Evaluate model\r\nprint(\"After training: \")\r\nmodel.evaluate(dataset)\r\n\r\n# Save model\r\nmodel.save('my_model.hdf5')\r\n\r\n# Reload the saved model\r\nnew_model = tf.keras.models.load_model('my_model.hdf5')\r\n\r\n# Evaluate again\r\nprint(\"After saving and reloading: \")\r\nnew_model.evaluate(dataset)\r\n```\r\n\r\nWhich gives the output :\r\n\r\n```\r\nAfter training : \r\n1000/1000 [==============================] - 20s 19ms/step - loss: 0.4617 - accuracy: 0.8139\r\nAfter saving and reloading : \r\n1000/1000 [==============================] - 20s 18ms/step - loss: 0.5586 - accuracy: 0.7688\r\n```\r\n\r\nBatch size 32. The model got much worse after save + reloading\r\n\r\nI tried EfficientNetB1, B2, B3 as well, all same issue\r\n\r\nThen I tried simply switching to MobileNetV3 and Xception, they both work perfectly fine! The accuracy after Load is identical to that measured before Save...\r\n\r\nWhats going on? Using tf 2.6 with python 3.8 and cudnn 8.2", "comments": ["Found this old [issue](https://github.com/tensorflow/tpu/issues/378) that indicated problems with model saving when using Keras+EfficientNet, so i tried switching to the SavedModel pb format instead of HDF5 format, and it works.\r\n\r\nI.e. just remove \".hdf5\" form the above code, it then saves in .pb instead, which gives the expected identical output after saving+reloading. \r\n\r\nIt also gives a warning though: \r\n\r\n/lib/python3.8/site-packages/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument\r\n\r\nBut it doesn't seem to effect accuracy so i suppose it's ok. Solution is that EfficientNet models should never be saved in hdf5 format.", "@duckduck-sys Could you please move this issue to closed status if it is resolved for you ? Please have a look at the [link1](https://github.com/tensorflow/tensorflow/issues/42459), [link2 ](https://stackoverflow.com/questions/61493605/keras-model-containing-efficientnet-submodel-cant-load-weights-vgg-works-fine)and [link3](https://www.tensorflow.org/api_docs/python/tf/saved_model) for your reference.Thank you!", "ok, thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52166\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52166\">No</a>\n"]}, {"number": 52165, "title": "BoostedTreesClassifier Not Progressing Past Step 0", "body": "OS: macOS Catalina 10.15.7\r\nTF 2.6.0 from binary\r\nPython 3.9.6\r\nRunning inside Jupyter Notebook\r\n\r\n**Describe the current behavior**\r\nTraining a BoostedTreesClassifier based on a transformation of TFRecordsDataset does not progress past step 0, and thus has no trees.  Evaluation results come back as if no prediction is being made and running feature importance afterwords raises an error that says the model must be trained first.\r\n\r\n**Describe the expected behavior**\r\nShould train a model with trees, make predictions, and be able to analyze the trees with say experimental_feature_importances\r\n\r\n- Do you want to contribute a PR? (yes/no): No\r\n\r\n**Code**\r\n```\r\ndef parse(record):\r\n    attributes = {feat: tf.io.FixedLenFeature([], tf.float32) for feat in feature_names}\r\n    attributes[\"label.label_xf\"] = tf.io.FixedLenFeature([], tf.int64)\r\n\r\n    parsed = tf.io.parse_single_example(record, attributes)\r\n    features = {feat: tf.convert_to_tensor(parsed[feat]) for feat in feature_names}\r\n\r\n    return features, tf.convert_to_tensor(parsed[\"label.label_xf\"])\r\n\r\ndef train_fn():\r\n    train_examples = tf.data.TFRecordDataset(TRAIN_PATH, compression_type=\"GZIP\")\r\n    \r\n    return train_examples.map(parse).batch(train_size)\r\n\r\ndef eval_fn():\r\n    eval_examples = tf.data.TFRecordDataset(EVAL_PATH, compression_type=\"GZIP\")\r\n    \r\n    return eval_examples.map(parse).batch(eval_size)\r\n\r\nbtc_est = tf.estimator.BoostedTreesClassifier(feature_columns, 1)\r\n\r\nbtc_est.train(train_fn, max_steps=10)\r\n```\r\ndoes not progress beyond step 0, and\r\n`btc_est.evaluate(eval_fn)`\r\ngives results\r\n```\r\n{'accuracy': 0.5158002,\r\n 'accuracy_baseline': 0.51580024,\r\n 'auc': 0.5,\r\n 'auc_precision_recall': 0.7420999,\r\n 'average_loss': 0.69314647,\r\n 'label/mean': 0.4841998,\r\n 'loss': 0.69314647,\r\n 'precision': 0.0,\r\n 'prediction/mean': 0.5,\r\n 'recall': 0.0,\r\n 'global_step': 0}\r\n```\r\nand\r\n```btc_est.experimental_feature_importances()```\r\nraises the error\r\n```ValueError: Found empty serialized string for TreeEnsemble.You should only call this method after training.```\r\nwhere as a LinearClassifier trained and evaluated in the same way\r\n```\r\nlin_est = tf.estimator.LinearClassifier(feature_columns)\r\n\r\nlin_est.train(train_fn)\r\nlin_est.evaluate(eval_fn)\r\n```\r\nyields sensible results that indicate a model has been successfully trained\r\n```\r\n{'accuracy': 0.5897044,\r\n 'accuracy_baseline': 0.5158002,\r\n 'auc': 0.59089816,\r\n 'auc_precision_recall': 0.59708935,\r\n 'average_loss': 0.6869413,\r\n 'label/mean': 0.4841998,\r\n 'loss': 0.6869413,\r\n 'precision': 0.6975477,\r\n 'prediction/mean': 0.49952453,\r\n 'recall': 0.26947367,\r\n 'global_step': 1}\r\n```\r\n", "comments": ["@tcheath ,\r\nIn order to expedite the trouble-shooting process, could you please provide a complete code  and the dataset you are using.Thanks\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52165\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52165\">No</a>\n"]}, {"number": 52164, "title": "unit test failure kernels:sparse_matmul_op_test on AARCH64", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): git HEAD\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 10.3.0\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nTest fails\r\n\r\n**Describe the expected behavior**\r\n\r\nTest passes\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nbazel test //tensorflow/core/kernels:sparse_matmul_op_test\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n[ RUN      ] SparseMatmulOpTest.BroadcastPacketTest\r\n[0.170094 0.170094 0.170094 0.170094] != [  0.170094    0.14922 -0.0823886   0.026985], differences: [         0 -0.0208738  -0.252482  -0.143109]\r\ntensorflow/core/kernels/sparse_matmul_op_test.cc:329: Failure\r\nValue of: areApprox(ref, data2, PacketSize)\r\nActual: false\r\nExpected: true\r\n[  FAILED  ] SparseMatmulOpTest.BroadcastPacketTest (0 ms)", "comments": ["@cfRod @nSircombe", "It looks like AARCH64 is using the definitions of pbroadcast_first et al from https://github.com/tensorflow/tensorflow/blob/9a9b33a3452f6295167d3a794e157baac51a663c/tensorflow/core/kernels/sparse_matmul_op.h#L92 which are only meant for scalars, so do not pick up the correct value.", "Hi @jvishnuvardhan! Could you please look at this issue.", "Fixed by commit https://github.com/tensorflow/tensorflow/commit/6d8220b1d847904e9bfec34d990b9436d95cd661", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52164\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52164\">No</a>\n"]}, {"number": 52163, "title": "bazel build tensorflow:tensorflow_cc is not working", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 64-bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.6.0\r\n- Python version: 3.9\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): bazel 3.7.2\r\n- GCC/Compiler version (if compiling from source): 8.1.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nI am using conda environment which does not have tensorflow installed. I am using tensorflow 2.6 from github and building it using bazel.\r\n\r\n\r\n**Describe the problem**\r\nI am trying to use tensorflow c++ api to simulate tensorflow from python into c++ to simulate tensorflow models using c++. \r\nI am facing a problem while using bazel to build tensorflow and get .cc and .h files from tensorflow to use in c++ api.\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\ngit checkout v2.6\r\n\r\nbazel build tensorflow:tensorflow_cc\r\n\r\nExtracting Bazel installation...\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n   Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Reading rc options for 'build' from d:\\tensorflow\\tensorflow-master\\.bazelrc:\r\n   Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=D:/Anaconda3/envs/tfcpp/python.exe                                                     \r\nINFO: Reading rc options for 'build' from d:\\tensorflow\\tensorflow-master\\.bazelrc:                                       \r\n    'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils    \r\n\r\nINFO: Found applicable config definition build:short_logs in file d:\\tensorflow\\tensorflow-master\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING                                                                                                 \r\nINFO: Found applicable config definition build:v2 in file d:\\tensorflow\\tensorflow-master\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1                                                                                       \r\nINFO: Found applicable config definition build:windows in file d:\\tensorflow\\tensorflow-master\\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --distinct_host_configuration=false                                                                                                   INFO: Found applicable config definition build:monolithic in file d:\\tensorflow\\tensorflow-master\\.bazelrc: --define framework_shared_object=false                                                                                             \r\n\r\nWARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/8d3c2c75e02d3333df81807ff8f6c64f55353766.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found                                                                                                 \r\n\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/efb284c07e97776e01933f470afb5215a561db3e.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found                                                                          \r\n\r\nINFO: Repository local_config_python instantiated at:                                                                    \r\n  D:/tensorflow/tensorflow-master/WORKSPACE:15:14: in <toplevel>                                                          \r\n  D:/tensorflow/tensorflow-master/tensorflow/workspace2.bzl:1080:19: in workspace                                         \r\n  D:/tensorflow/tensorflow-master/tensorflow/workspace2.bzl:99:21: in _tf_toolchains                                    \r\nRepository rule python_configure defined at:                                                                              \r\n  D:/tensorflow/tensorflow-master/third_party/py/python_configure.bzl:294:35: in <toplevel>                             \r\n\r\nERROR: An error occurred during the fetch of repository 'local_config_python':                                             \r\n   Traceback (most recent call last):                                                                                          \r\n       File \"D:/tensorflow/tensorflow-master/third_party/py/python_configure.bzl\", line 267, column 40, in _python_autoconf_impl                                                                                                                               _create_local_python_repository(repository_ctx)                                                                 \r\n       File \"D:/tensorflow/tensorflow-master/third_party/py/python_configure.bzl\", line 209, column 22, in \r\n               _create_local_python_repository                                                                                                                     \r\n                  _check_python_bin(repository_ctx, python_bin)                                                                   \r\n       File \"D:/tensorflow/tensorflow-master/third_party/py/python_configure.bzl\", line 143, column 52, in _check_python_bin                                                                                                                                   \r\n                result = raw_exec(repository_ctx, [get_bash_bin(repository_ctx), \"-c\", cmd])                                    \r\n       File \"D:/tensorflow/tensorflow-master/third_party/remote_config/common.bzl\", line 88, column 26, in get_bash_bin                \r\n                bash_bin_path = which(repository_ctx, \"bash\")                                                                   \r\n       File \"D:/tensorflow/tensorflow-master/third_party/remote_config/common.bzl\", line 27, column 22, in which                       \r\n                out = execute(                                                                                                  \r\n       File \"D:/tensorflow/tensorflow-master/third_party/remote_config/common.bzl\", line 230, column 13, in execute                    \r\n                fail(                                                                                                   \r\n\r\nError in fail: Repository command failed                                                                                \r\nINFO: Could not find files for the given pattern(s).                                                                    \r\nINFO: Repository go_sdk instantiated at:                                                                                  \r\n   D:/tensorflow/tensorflow-master/WORKSPACE:23:14: in <toplevel>                                                          \r\n   D:/tensorflow/tensorflow-master/tensorflow/workspace0.bzl:120:20: in workspace                                          \r\n   D:/_bazel/lshsjacg/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps                    \r\n   D:/_bazel/lshsjacg/external/io_bazel_rules_go/go/toolchain/toolchains.bzl:379:28: in go_register_toolchains             \r\n   D:/_bazel/lshsjacg/external/io_bazel_rules_go/go/private/sdk.bzl:65:21: in go_download_sdk                            \r\nRepository rule _go_download_sdk defined at:                                                                              \r\n   D:/_bazel/lshsjacg/external/io_bazel_rules_go/go/private/sdk.bzl:53:35: in <toplevel>                                 \r\nERROR: Analysis of target '//tensorflow:tensorflow_cc' failed; build aborted: Repository command failed                 \r\nINFO: Could not find files for the given pattern(s).                                                                    \r\nINFO: Elapsed time: 288.274s                                                                                            \r\nINFO: 0 processes.                                                                                                      \r\nFAILED: Build did NOT complete successfully (11 packages loaded, 15 targets configured)                                     \r\n    Fetching @local_execution_config_python; fetching                                                                       \r\n    Fetching ...docker; Cloning 9bfcd7dbf0294ed9d11a99da6363fc28df904502 of https://github.com/bazelbuild/rules_docker\\ .git \r\n\r\nThe warning that \"download from....Exception GET returned 404 Not Found\",  occurs twice in above trace log. And it occurs everytime I run the same commands. \r\n", "comments": ["@kaustubhjirapure \r\nWe see this issue is already reported and is work in progress please refer to [this issue](https://github.com/tensorflow/tensorflow/issues/45059) and let us know\r\nSimilar issue: [link](https://github.com/google/mediapipe/issues/533),[link2](https://github.com/google/mediapipe/issues/70)", "I have seen the issue and will wait for that issue to be resolved and see if it helps me. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52163\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52163\">No</a>\n"]}, {"number": 52162, "title": "Revert \"[MHLO]: refine delete redundant specialization function\"", "body": null, "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52162) for more info**.\n\n<!-- need_sender_cla -->", "@jessecantu Can you please sign CLA. Thanks!"]}, {"number": 52161, "title": "Unable to use ImageDataGenerator on M1 mac ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS BigSur 11.6\r\n- TensorFlow installed from (source or binary): https://developer.apple.com/metal/tensorflow-plugin/\r\n- TensorFlow version: 2.5.0\r\n- Python version: 3.8.11\r\n- GPU model and memory: MacBook Pro M1, 16GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nI'm creating a Neural Network to classify rock-paper-scissors images. I'm doing it by using an ImageDataGenerator in the script. \r\nEverything works fine, but when I try to train the model the error \"ImportError: Image transformations require SciPy. Install SciPy\" appears. The error appears when calling `model.fit` or `model.fit_generator`\r\nThe scipy library is already installed, version is 1.7.0. I tried to uninstall and re-install it, but nothing changed.\r\nDoes anybody know a solution to this?\r\n\r\nHere is my code:\r\n\r\n```\r\nimport os\r\nimport zipfile\r\n\r\nlocal_zip = './rps.zip'\r\nzip_ref = zipfile.ZipFile(local_zip, 'r')\r\nzip_ref.extractall('tmp/rps-train')\r\nzip_ref.close()\r\n\r\nlocal_zip = './rps-test-set.zip'\r\nzip_ref = zipfile.ZipFile(local_zip, 'r')\r\nzip_ref.extractall('tmp/rps-test')\r\nzip_ref.close()\r\n\r\nbase_dir = 'tmp/rps-train/rps'\r\n\r\nrock_dir = os.path.join(base_dir, 'rock')\r\npaper_dir = os.path.join(base_dir, 'paper')\r\nscissors_dir = os.path.join(base_dir, 'scissors')\r\n\r\nrock_files = os.listdir(rock_dir)\r\npaper_files = os.listdir(paper_dir)\r\nscissors_files = os.listdir(scissors_dir)\r\n\r\nimport tensorflow as tf\r\nimport keras_preprocessing\r\nfrom keras_preprocessing import image\r\nfrom keras_preprocessing.image import ImageDataGenerator\r\n\r\nTRAINING_DIR = 'tmp/rps-train/rps'\r\ntraining_datagen = ImageDataGenerator(rescale=1./255,\r\n                                      rotation_range=40, \r\n                                      width_shift_range=0.2,\r\n                                      height_shift_range=0.2,\r\n                                      horizontal_flip=False\r\n)\r\n\r\nVALIDATION_DIR = 'tmp/rps-test/rps-test-set'\r\nvalidation_datagen = ImageDataGenerator(rescale=1./255)\r\n\r\ntrain_generator = training_datagen.flow_from_directory(\r\n    TRAINING_DIR,\r\n    target_size=(150,150),\r\n    class_mode='categorical',\r\n    batch_size=126\r\n)\r\n\r\nvalidation_generator = validation_datagen.flow_from_directory(\r\n    VALIDATION_DIR,\r\n    target_size=(150,150),\r\n    class_mode='categorical',\r\n    batch_size=126\r\n)\r\n\r\nmodel = tf.keras.models.Sequential([\r\n    # This is the first convolution\r\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150, 150, 3)),\r\n    tf.keras.layers.MaxPooling2D(2, 2),\r\n    # The second convolution\r\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\r\n    tf.keras.layers.MaxPooling2D(2,2),\r\n    # The third convolution\r\n    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\r\n    tf.keras.layers.MaxPooling2D(2,2),\r\n    # The fourth convolution\r\n    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\r\n    tf.keras.layers.MaxPooling2D(2,2),\r\n    # Flatten the results to feed into a DNN\r\n    tf.keras.layers.Flatten(),\r\n    tf.keras.layers.Dropout(0.5),\r\n    # 512 neuron hidden layer\r\n    tf.keras.layers.Dense(512, activation='relu'),\r\n    tf.keras.layers.Dense(3, activation='softmax')\r\n])\r\n\r\nmodel.summary()\r\n\r\nmodel.compile(optimizer=tf.optimizers.RMSprop(learning_rate=0.001),\r\n              loss=tf.metrics.categorical_crossentropy,\r\n              metrics=['accuracy'])\r\n\r\n```\r\n\r\n\r\nAfter running the following line the error occurs:\r\n\r\n```\r\nmodel.fit_generator(train_generator,\r\n                    epochs=3)\r\n\r\nImportError                               Traceback (most recent call last)\r\n/var/folders/j4/flhsd8lj4z7g689p_y84tfjh0000gn/T/ipykernel_89314/1416594977.py in <module>\r\n      1 import scipy\r\n      2 \r\n----> 3 model.fit_generator(train_generator,\r\n      4                     epochs=3)\r\n\r\n~/miniforge3/envs/python3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\r\n   1941                   'will be removed in a future version. '\r\n   1942                   'Please use `Model.fit`, which supports generators.')\r\n-> 1943     return self.fit(\r\n   1944         generator,\r\n   1945         steps_per_epoch=steps_per_epoch,\r\n\r\n~/miniforge3/envs/python3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1131          training_utils.RespectCompiledTrainableState(self):\r\n   1132       # Creates a `tf.data.Dataset` and handles batch and epoch iteration.\r\n-> 1133       data_handler = data_adapter.get_data_handler(\r\n   1134           x=x,\r\n   1135           y=y,\r\n\r\n~/miniforge3/envs/python3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py in get_data_handler(*args, **kwargs)\r\n   1362   if getattr(kwargs[\"model\"], \"_cluster_coordinator\", None):\r\n   1363     return _ClusterCoordinatorDataHandler(*args, **kwargs)\r\n-> 1364   return DataHandler(*args, **kwargs)\r\n   1365 \r\n   1366 \r\n\r\n~/miniforge3/envs/python3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\r\n   1152     adapter_cls = select_data_adapter(x, y)\r\n   1153     self._verify_data_adapter_compatibility(adapter_cls)\r\n-> 1154     self._adapter = adapter_cls(\r\n   1155         x,\r\n   1156         y,\r\n\r\n~/miniforge3/envs/python3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weights, shuffle, workers, use_multiprocessing, max_queue_size, model, **kwargs)\r\n    930     self._keras_sequence = x\r\n    931     self._enqueuer = None\r\n--> 932     super(KerasSequenceAdapter, self).__init__(\r\n    933         x,\r\n    934         shuffle=False,  # Shuffle is handed in the _make_callable override.\r\n\r\n~/miniforge3/envs/python3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\r\n    807     # Since we have to know the dtype of the python generator when we build the\r\n    808     # dataset, we have to look at a batch to infer the structure.\r\n--> 809     peek, x = self._peek_and_restore(x)\r\n    810     peek = self._standardize_batch(peek)\r\n    811     peek = _process_tensorlike(peek)\r\n\r\n~/miniforge3/envs/python3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py in _peek_and_restore(x)\r\n    941   @staticmethod\r\n    942   def _peek_and_restore(x):\r\n--> 943     return x[0], x\r\n    944 \r\n    945   def _handle_multiprocessing(self, x, workers, use_multiprocessing,\r\n\r\n~/miniforge3/envs/python3.8/lib/python3.8/site-packages/keras_preprocessing/image/iterator.py in __getitem__(self, idx)\r\n     63         index_array = self.index_array[self.batch_size * idx:\r\n     64                                        self.batch_size * (idx + 1)]\r\n---> 65         return self._get_batches_of_transformed_samples(index_array)\r\n     66 \r\n     67     def __len__(self):\r\n\r\n~/miniforge3/envs/python3.8/lib/python3.8/site-packages/keras_preprocessing/image/iterator.py in _get_batches_of_transformed_samples(self, index_array)\r\n    236             if self.image_data_generator:\r\n    237                 params = self.image_data_generator.get_random_transform(x.shape)\r\n--> 238                 x = self.image_data_generator.apply_transform(x, params)\r\n    239                 x = self.image_data_generator.standardize(x)\r\n    240             batch_x[i] = x\r\n\r\n~/miniforge3/envs/python3.8/lib/python3.8/site-packages/keras_preprocessing/image/image_data_generator.py in apply_transform(self, x, transform_parameters)\r\n    861         img_channel_axis = self.channel_axis - 1\r\n    862 \r\n--> 863         x = apply_affine_transform(x, transform_parameters.get('theta', 0),\r\n    864                                    transform_parameters.get('tx', 0),\r\n    865                                    transform_parameters.get('ty', 0),\r\n\r\n~/miniforge3/envs/python3.8/lib/python3.8/site-packages/keras_preprocessing/image/affine_transformations.py in apply_affine_transform(x, theta, tx, ty, shear, zx, zy, row_axis, col_axis, channel_axis, fill_mode, cval, order)\r\n    279     \"\"\"\r\n    280     if scipy is None:\r\n--> 281         raise ImportError('Image transformations require SciPy. '\r\n    282                           'Install SciPy.')\r\n    283     transform_matrix = None\r\n\r\nImportError: Image transformations require SciPy. Install SciPy.\r\n```\r\n\r\nHere are the packages installed in the virtual environment I am using:\r\n\r\n\r\n<img width=\"352\" alt=\"Screenshot 2021-09-28 at 8 53 00 AM\" src=\"https://user-images.githubusercontent.com/91497431/135038153-199fe46b-9269-48e7-a01d-4c06a519cdb2.png\">\r\n<img width=\"255\" alt=\"Screenshot 2021-09-28 at 8 53 15 AM\" src=\"https://user-images.githubusercontent.com/91497431/135038191-c0a4038d-0d67-45d5-a7d1-1fccdf0c2682.png\">\r\n<img width=\"282\" alt=\"Screenshot 2021-09-28 at 8 53 34 AM\" src=\"https://user-images.githubusercontent.com/91497431/135038209-9a9a302f-4560-4c82-8223-208d0259b56b.png\">\r\n\r\n\r\n", "comments": ["@FedePiras ,\r\nCan you please take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/51736#issuecomment-908923652) from the issue with similar error.It helps.Thanks", "> @FedePiras , Can you please take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/51736#issuecomment-908923652) from the issue with similar error.It helps.Thanks\r\n\r\nI uninstalled Tensorflow and delete my virtual environment, then I created a new one and re-installed Tensorflow as specified in the apple site, but the Scipy Error is still there. It only appears when I use the `model.fit` method using an `ImageDataGenerator`", "@FedePiras \r\nCould you please try in a different environment if you are facing he same error, i ran the code shared n colab and face the issue as [per this gist](https://colab.research.google.com/gist/Saduf2019/8a4d33c11f5e4ea3a3f26795c33df91c/untitled643.ipynb), can you also try on colab and let us know if you still face the error reported.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52161\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52161\">No</a>\n"]}, {"number": 52159, "title": "Why does tensorflow only use one GPU, but it takes up two GPUs? ", "body": "\r\n|=============================================================================|\r\n|    0   N/A  N/A     27330      C   ...envs/p3.8_t2.2/bin/python      249MiB |\r\n|    1   N/A  N/A     27330      C   ...envs/p3.8_t2.2/bin/python     7517MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\nI have two GPUs on my machine. I run the GPU training code and found that the process occupies 2 GUPs. The actual use is the No. 1 GPU, but the No. 0 GUP occupies 249 MiB of memory but is not used. I don\u2019t understand why if only one GPU is used, the other should be occupied? \r\n\r\n**Reproduce the code\uff1a**\r\n```python\r\nimport tensorflow as tf\r\nimport os\r\n\r\n\r\nmnist = tf.keras.datasets.mnist\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\nx_train = x_train[..., tf.newaxis].astype(\"float32\")\r\nx_test = x_test[..., tf.newaxis].astype(\"float32\")\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Conv2D(10, kernel_size=(3, 3),input_shape=(28, 28, 1)),\r\n  tf.keras.layers.Conv2D(20, kernel_size=(3,3)),\r\n  tf.keras.layers.DepthwiseConv2D(kernel_size=(3,3)),\r\n  tf.keras.layers.Flatten(),\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10, activation='softmax')\r\n])\r\n\r\nmodel.summary()\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\nmodel.fit(x_train, y_train, batch_size=32, epochs=1)\r\n```\r\n", "comments": ["Same issue.\r\n\r\nI find that even I assign gpu-1 to the model construction, it stills occupy two gpus's memory. Only one gpu has work load.\r\n\r\n```\r\n        #with tf.distribute.MirroredStrategy().scope():\r\n        with tf.device('/GPU:{}'.format(1)):\r\n            if model_name == 'albert':\r\n                model = get_model_bert(df_test.label.unique().shape[0])\r\n                \r\n            elif model_name == 'former':\r\n                model = get_model_former(df_test.label.unique().shape[0])\r\n                \r\n            elif model_name == 'cnn':\r\n                model = get_model_cnn(df_test.label.unique().shape[0])\r\n                \r\n            else:\r\n                raise KeyError(\"input model illegal!\")\r\n\r\n        history = model.fit(\r\n            x_train, y_train, batch_size=batch_size, epochs=epochs, \\\r\n            validation_data=(x_test, y_test), verbose=verbose, validation_batch_size=64,validation_freq=freq\r\n            #callbacks = [EarlyStopping(monitor='val_acc', patience=3, mode='max')]\r\n        )\r\n```", "**Supplement:** When the memory of both GUPs is occupied by other processes, and CUDA_VISIBLE_DEVCES is set to specify any one GUP, the program will throw an exception error. (Normal) However, if the CUDA_VISIBLE_DEVCES variable is not set, the program will segfault and crash directly. ", "@yananchen1989 \r\nYou may also refer to [link](https://stackoverflow.com/questions/43577668/tensorflow-seems-to-be-using-two-gpus-but-one-gpu-seems-not-be-doing-anything),[link1](https://jhui.github.io/2017/03/07/TensorFlow-GPU/),[link2](https://datascience.stackexchange.com/questions/90324/why-the-my-tensorflow-code-just-use-one-gpu-when-i-assign-more-than-one)", "@Saduf2019  Thanks, Very helpful.", "@Saduf2019 \r\nThank you for your answers, but my problem is not to use multi-GPU training. The issues is that Tensorflow uses GUP0 by default but occupies some memory of GUP1, and when both GPU memory is occupied by other processes, run training script will segfault instead of throwing an exception. \r\n", "@wqcsim \r\nThat is how any computer device would work [be it cpu,gpu,tpu]\r\nBoth GPU memories are full and you try to run another script obviously it will crash, kindly move tihs to closed status and create an issue on discussion forum as there is a larger community there to support."]}, {"number": 52158, "title": "TensorFlow Lite C library built with CMake, ERROR: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 18.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): **from Source**\r\n- TensorFlow version: \r\n- Python version: \r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): **GNU 7.5.0**\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nThe project I am working on requires the **TensorFlow Lite C API** which I built with **CMake** following this instructions https://www.tensorflow.org/lite/guide/build_cmake#build_tensorflow_lite_c_library (shared library).\r\n\r\nI have built and run pretrained models successfully before, however, I am currently trying to integrate a model that takes 2 images as input such as **HITNet** from google-research group (https://github.com/google-research/google-research/tree/master/hitnet). The model mentioned in the paper is in protobuf format here https://storage.googleapis.com/tensorflow-graphics/models/hitnet/default_models/middlebury_d400.pb, and I found in this repo https://github.com/PINTO0309/PINTO_model_zoo/tree/main/142_HITNET the model converted to tflite model https://drive.google.com/uc?export=download&id=1cqxZ-hCQagdwYQee4U8LsaHAJA3y-Go3\r\n\r\nThe following error arises when creating the model from file with `TfLiteModelCreateFromFile`:\r\n```\r\nERROR: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding \"org.tensorflow:tensorflow-lite-select-tf-ops\" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select\r\nERROR: Node number 271 (FlexStridedSlice) failed to prepare.\r\n```\r\n\r\n_**How to add support of Select TensorFlow op(s)/Flex delegate to TensorFlow Lite C API with CMake?**_\r\nI didn't find any suggestion for C API in https://www.tensorflow.org/lite/guide/ops_select\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n`TfLiteModel* model = TfLiteModelCreateFromFile((test_data_dir() + \"/tflite_hitnet_model/hitnet_model_float32_480x640.tflite\").c_str());`\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["It can be used in the same way as with C++ API. Just build https://www.tensorflow.org/lite/guide/ops_select#c and load it then It should work fine.", "@thaink thanks for your reply.\r\n\r\nThe solution you mentioned is based on **bazel**. Any plans to provide a solution for **CMake**?\r\n\r\nNote: I built tensorflow lite C API with CMake, not with bazel.", "My understanding is that the Flex (Select TensorFlow ops) feature is supported by neither CMake, nor the Makefile build process that CMake replaced. It would be a significant amount of engineering effort to make the entire TensorFlow buildable with CMake on mobile, and there is no plan for this yet. \r\n\r\nThe best way to use Flex delegate is sticking with Bazel build. \r\n\r\nTagging @terryheo who works on CMake build -- Please correct me if I'm wrong. ", "@miaout17 is correct. To support CMake for Flex, we need a CMake rule for TensorFlow kernels. They used to exist but it's not available now.\r\n\r\nIf you can remove Flex usage, you can avoid this issue. I wonder why your model requires FlexStridedSlice since StridedSlice is also available for TFLite.", "@CarlosYeverino, Did you refer solutions discussed here in these threads yet?[link1](https://github.com/tensorflow/tensorflow/issues/52158#issuecomment-932659820), [link2](https://github.com/tensorflow/tensorflow/issues/52158#issuecomment-932743058).  Can you let us know why your model requires FlexStridedSlice since StridedSlice is also available for TFLite?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52158\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52158\">No</a>\n"]}, {"number": 52156, "title": "Keras model saved with user-defined signature works with TensorFlow Serving but not Python", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Any\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): TensorFlow 2.6 and Nightly\r\n- Python version: Python 3.7\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nWhen manually specifying the signature in [tf.keras.Model.save](https://www.tensorflow.org/api_docs/python/tf/keras/Model#save) a correct model that works with TensorFlow Serving is saved. However, when reloading the same model in Python, TensorFlow tries to execute the incorrect concrete function.\r\n\r\nIn the example below, I create a model that accepts a `[None, 4]` input but when saving, change it to a `[None, 2]` input that I simply concatenate along `axis=1` to turn it into a `[None, 4]` input. TensorFlow Serving can run this model without problems, but reloading the model with `restored_model = tf.keras.model.load_model(...)` and calling the restored model as a function incorrectly tries to run a function that expects a `[None, 4]` input instead of my user-specified one that expects a `[None, 2]` input.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\ninput_name = \"abc\"\r\n\r\n#####################################################################################################\r\n# model that accepts [None, 4] input\r\n#####################################################################################################\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.x = tf.keras.layers.Dense(1)\r\n\r\n    def call(self, inputs, training=None):\r\n        return self.x(inputs[input_name])\r\n\r\n\r\ninputs = tf.data.Dataset.from_tensor_slices(({input_name: [[1, 2, 3, 4]]}, [1])).batch(1)\r\nmodel = MyModel()\r\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\")\r\nmodel.fit(inputs)\r\nmodel.save(\"4d_model\")\r\n#####################################################################################################\r\n\r\n\r\n#####################################################################################################\r\n# override the signature to accept [None, 2] and simply concatenate it into [None, 4] for the model\r\n#####################################################################################################\r\n@tf.function(input_signature=[tf.TensorSpec([None, 2], dtype=tf.int32, name=input_name)])\r\ndef serve(x):\r\n    return model({input_name: tf.concat((x, x), axis=1)})\r\n\r\nmodel.save(\"2d_model\", signatures={\"serving_default\": serve})\r\n#####################################################################################################\r\n\r\nrestored_model = tf.keras.models.load_model(\"2d_model\")\r\nx = next(iter(tf.data.Dataset.from_tensor_slices({input_name: [[1, 2]]}).batch(1)))\r\noutputs = restored_model(x)  # incorrectly requires [None, 4] instead of [None, 2]\r\n```\r\n\r\n```python\r\n>>> print(restored_model.signatures[\"serving_default\"].structured_input_signature)\r\n((), {'abc': TensorSpec(shape=(None, 2), dtype=tf.int32, name='abc')})\r\n```\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n```\r\nValueError: Exception encountered when calling layer \"my_model\" (type MyModel).\r\n\r\nCould not find matching concrete function to call loaded from the SavedModel. Got:\r\n  Positional arguments (2 total):\r\n    * {'abc': <tf.Tensor 'inputs:0' shape=(1, 2) dtype=int32>}\r\n    * False\r\n  Keyword arguments: {}\r\n\r\n Expected these arguments to match one of the following 4 option(s):\r\n\r\nOption 1:\r\n  Positional arguments (2 total):\r\n    * {'abc': TensorSpec(shape=(None, 4), dtype=tf.int32, name='inputs/abc')}\r\n    * False\r\n  Keyword arguments: {}\r\n\r\nOption 2:\r\n  Positional arguments (2 total):\r\n    * {'abc': TensorSpec(shape=(None, 4), dtype=tf.int32, name='inputs/abc')}\r\n    * True\r\n  Keyword arguments: {}\r\n\r\nOption 3:\r\n  Positional arguments (2 total):\r\n    * {'abc': TensorSpec(shape=(None, 4), dtype=tf.int32, name='abc')}\r\n    * False\r\n  Keyword arguments: {}\r\n\r\nOption 4:\r\n  Positional arguments (2 total):\r\n    * {'abc': TensorSpec(shape=(None, 4), dtype=tf.int32, name='abc')}\r\n    * True\r\n  Keyword arguments: {}\r\n\r\nCall arguments received:\r\n  \u2022 args=({'abc': 'tf.Tensor(shape=(1, 2), dtype=int32)'},)\r\n  \u2022 kwargs={'training': 'None'}\r\n```\r\n", "comments": ["Hi @dwyatte! ,Could you please look at  these similar issues  [link1](https://github.com/tensorflow/tensorflow/issues/37973),[link2](https://stackoverflow.com/questions/58575586/could-not-find-matching-function-to-call-loaded-from-the-savedmodel).", "@mohantym https://github.com/tensorflow/tensorflow/issues/37973 is similar but note that this bug is present for standard non-subclassed Keras models that use a user-defined signature upon saving.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\ninput_name = \"abc\"\r\n\r\n#####################################################################################################\r\n# model that accepts [None, 4] input\r\n#####################################################################################################\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.Input(4, name=input_name),\r\n    tf.keras.layers.Dense(1)\r\n])\r\n\r\ninputs = tf.data.Dataset.from_tensor_slices(({input_name: [[1, 2, 3, 4]]}, [1])).batch(1)\r\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\")\r\nmodel.fit(inputs)\r\nmodel.save(\"4d_model\")\r\n#####################################################################################################\r\n\r\n\r\n#####################################################################################################\r\n# override the signature to accept [None, 2] and simply concatenate it into [None, 4] for the model\r\n#####################################################################################################\r\n@tf.function(input_signature=[tf.TensorSpec([None, 2], dtype=tf.int32, name=input_name)])\r\ndef serve(x):\r\n    return model({input_name: tf.concat((x, x), axis=1)})\r\n\r\nmodel.save(\"2d_model\", signatures={\"serving_default\": serve})\r\n#####################################################################################################\r\n\r\n\r\nrestored_model = tf.keras.models.load_model(\"2d_model\")\r\nx = next(iter(tf.data.Dataset.from_tensor_slices({input_name: [[1, 2]]}).batch(1)))\r\noutputs = restored_model(x)  # incorrectly requires [None, 4] instead of [None, 2]\r\n```\r\n\r\nWhen the model is constructed from `tf.keras.Sequential`, the error message is \r\n```\r\nValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 4), found shape=(1, 2)\r\n```\r\n\r\nThe specific bug in this case is that Python does not run the user-defined signature code (in this case `tf.concat((x, x), axis=1)`) before running the SavedModel's `call` function. Note that TensorFlow Serving _does_ run this user-defined signature code so it is present somewhere in the SavedModel, but not used by TensorFlow in Python when restoring the SavedModel.", "Update: It looks like I can use `restored_model.signatures[\"serving_default\"](abc=tf.constant([[1, 2,]]))` to run the user-defined signature in Python. This makes sense to me and should be sufficient for my use case.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52156\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52156\">No</a>\n", "> Update: It looks like I can use `restored_model.signatures[\"serving_default\"](abc=tf.constant([[1, 2,]]))` to run the user-defined signature in Python. This makes sense to me and should be sufficient for my use case.\r\n\r\nthx, bro"]}]