[{"number": 44876, "title": "\"FAILED: Build did NOT complete successfully (1 packages loaded)\"", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: I am not sure. I cloned it from the Github repository so it should be the latest version\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: Trying to install with Pip, but I am getting an error when I try to build a pip package via Bazel.\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: Radeon RX 5700XT, G.Skill Trident-Z 2x16GB\r\n\r\n\r\n**Describe the problem**\r\nI am trying to build Tensorflow from the source since my training sessions are performing really slow and barely utilizing my hardware. I read that I can get better performance by building it from the source for my machine. However, I am getting an error when I tried to build a pip package via Bazel as according to the steps from the link:\r\nhttps://www.tensorflow.org/install/source\r\n\r\nThis is the output I am getting when I try to build the package with Bazel:\r\n```\r\nroot@weapon-of-mass-instruction:/home/chris/tensorflow# bazel build [--config=option] //tensorflow/tools/pip_package:build_pip_package\r\nExtracting Bazel installation...\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=140\r\nINFO: Reading rc options for 'build' from /home/chris/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/chris/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /home/chris/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --config=xla --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file /home/chris/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/chris/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /home/chris/tensorflow/.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:linux in file /home/chris/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/chris/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: Rule 'io_bazel_rules_go' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1557349968 -0400\"\r\nDEBUG: Repository io_bazel_rules_go instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule git_repository defined at:\r\n  /root/.cache/bazel/_bazel_root/a819f6789d392dd47d3107e5934b6ad1/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>\r\nERROR: Skipping '[--config=option]': no such target '//:[--config=option]': target '[--config=option]' not declared in package '' defined by /home/chris/tensorflow/BUILD\r\nERROR: no such target '//:[--config=option]': target '[--config=option]' not declared in package '' defined by /home/chris/tensorflow/BUILD\r\nINFO: Elapsed time: 8.402s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (1 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package\r\n    Fetching @local_config_cuda; Restarting.\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\n1. git clone https://github.com/tensorflow/tensorflow.git\r\n2. cd tensorflow\r\n3. ./configure\r\n4. Left everything default and chose no for ROCM/CUDA/Clang/etc.\r\n5. bazel build [--config=option] //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nI do not have an ROCM compatible GPU (at least as far as I am aware. If I am wrong, I would definitely appreciate some advice on making Tensorflow work with my card) so I am trying to optimize Tensorflow to run on my CPU, which is a Ryzen R9 3900X. When I run Tensorflow, it appears to barely utilize my CPU as according to my process monitor, so I am looking to build Tensorflow for my machine in hopes that it will make a difference in performance.\r\n\r\nAll advice and help with issue will be greatly appreciated!", "comments": ["See https://github.com/RadeonOpenCompute/ROCm#supported-gpus\r\nI suggest you also to profile your code with https://www.tensorflow.org/guide/profiler\r\n\r\nTensorflow ROCM is available at https://pypi.org/project/tensorflow-rocm/", "You're not supposed to input `[--config=option]` as is. Brackets usually means \"you can supply something in here in this format.\"\r\n\r\nTry:\r\n```sh\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nor\r\n```sh\r\nbazel build --config=opt --config=mkl //tensorflow/tools/pip_package:build_pip_package\r\n```", "Damn, I feel really dumb now haha. Removing the brackets worked. Also, I will check out RoCM since I see that it supports Ryzen CPUs. I thought it only supported GPUs.\r\n\r\nThanks a ton for your help!", "@cray12399,\r\nThank you for the update. Marking this issue as closed, as it is resolved. Please feel free to re-open if necessary.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44876\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44876\">No</a>\n"]}, {"number": 44875, "title": "Tensor flow: append tensor operation", "body": "Hello. \r\nI have the following operation with **numpy** array:\r\n`data.append(dataset[indices])`\r\n**dataset[indices]** -  numpy.ndarray array with data [0.0345, 0.0522, -0.234, 0.118, -0.568, 0.772,...]\r\nThe number of data is big, so it takes a long time for calculation on CPU. To make calculations faster I used TensorFlow.\r\nI found out that only tensor operations can be performed on GPU. \r\nThat's why I needed such operation as `tf.append(...)` that could be add an element to an array inside of the tensor, declared by the following way: `tf.Variable([],tf.float32)` And after certain number of these operations an inside array of the tensor should be changed from empty to [0.0345, 0.0522, -0.234, 0.118, -0.568, 0.772,...]. After reading of documentation I didn't find such operation in TensorFlow\r\n\r\nAnd the question is following: What is the way to make what I want, using Tensor operations?", "comments": ["If you are working with numpy code you could explore (with tf-nightly) https://www.tensorflow.org/api_docs/python/tf/experimental/numpy/append.\r\n\r\nSee also \r\nhttps://youtu.be/DpOq0PJ0H38\r\n", "I was trying to install tf-nightly and I've got an error:\r\n\r\nERROR: Could not install packages due to an EnvironmentError: [WinError 32] The process cannot access the file because it is being used by another process: 'I:\\\\Python\\\\Python37\\\\Lib\\\\site-packages\\\\tensorflow\\\\python\\\\keras\\\\layers\\\\core.py'\r\nConsider using the `--user` option or check the permissions.", "Cause Tensorflow Is running", "So I have to delete Tensorflow?", "How can I stop TensorFlow? I tried to install tf-nightly and no one of my programs, that uses TensorFlow, was running", "Check https://gallery.technet.microsoft.com/How-to-find-out-which-c0d4e60e", "I installed another version of Python (3.7.6). Then I installed tf-nightly and tf-nightly-gpu (version 2.5), CUDA 11.0 and cuddn v8.0.5\r\nAnd I got an error:\r\nW tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\nThis DLL has written in the Environment Variable PATH", "What else should I install?", "TF by next version (2.4.1) requires CUDA 11", "I have already installed CUDA 11.\r\nBut TF 2.4.1 don't want to install because of installed tf-nightly", "May be my GPU is not compatible with tf-nightly 2.5? That's why an error \"W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\" appears?", "It seems that doesn't find that CUDA file in your system paths. Check https://www.tensorflow.org/install/gpu?hl=en", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44874, "title": "Tensorflow degrading performance throughout training", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No not really\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow version (use command below): TensorFlow 2.4.0-rc1\r\n(Ive also tried using tf-nightly-gpu 2.4.0.dev20201023 and tf-nightly-gpu 2.5.0.dev20201111 but nothing changed)\r\n- Python version: python 3.8.5\r\n- CUDA/cuDNN version: CUDA 11.1 / cuDNN 8.05\r\n- GPU model and memory: RTX 3070 / 16GB RAM\r\n\r\n**Describe the current behavior**\r\nEach epoch takes longer and longer to complete. The first epoch started out at 50 seconds and on epoch 90 it was at almost at 1000 seconds an epoch. In the task manager, I can see that it slowly stops using the GPU after training for a while.\r\n\r\n**Describe the expected behavior**\r\nEach epoch should take the same amount of time.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport os\r\nimport tensorflow as tf\r\nfrom keras.callbacks import EarlyStopping\r\nfrom keras.layers.core import Dense, Dropout, Activation, Flatten\r\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization, LeakyReLU\r\nfrom keras.models import Sequential\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.regularizers import l2\r\n\r\nconfig = tf.compat.v1.ConfigProto(gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.70))\r\nfor device in tf.config.experimental.list_physical_devices(\"GPU\"):\r\n    tf.config.experimental.set_memory_growth(device, True)\r\nsession = tf.compat.v1.Session(config=config)\r\ntf.compat.v1.keras.backend.set_session(session)\r\n\r\nIMG_SIZE = 350\r\nVersion = 1\r\nbatch_size = 8\r\n\r\nval_aug = ImageDataGenerator(rescale=1/255)\r\naug = ImageDataGenerator(\r\n        rescale=1/255, \r\n        rotation_range=30, \r\n        width_shift_range=0.1, \r\n        height_shift_range=0.1, \r\n        shear_range=0.2, \r\n        zoom_range=0.2, \r\n        channel_shift_range=25, \r\n        horizontal_flip=True, \r\n        fill_mode='constant')\r\n\r\ntrain_gen = aug.flow_from_directory('F:/Storage/DataSet_Bal/Train', \r\n        target_size=(IMG_SIZE, IMG_SIZE), \r\n        batch_size=batch_size,\r\n        class_mode='binary',\r\n        shuffle=True)\r\nval_gen = val_aug.flow_from_directory('F:/Storage/DataSet_Bal/Val', \r\n        target_size=(IMG_SIZE, IMG_SIZE), \r\n        batch_size=batch_size,\r\n        class_mode='binary',\r\n        shuffle=True)\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv2D(64, 3, strides=(1,1), padding = 'same', activation = 'relu', input_shape = (IMG_SIZE, IMG_SIZE, 3)))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Conv2D(64, 3, strides=(1,1), activation = 'relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=2))\r\nmodel.add(BatchNormalization())\r\n\r\nmodel.add(Conv2D(128, 3, strides=(1,1), activation = 'relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Conv2D(128, 3, strides=(1,1), activation = 'relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=2))\r\nmodel.add(BatchNormalization())\r\n\r\nmodel.add(Conv2D(256, 3, strides=(1,1), activation = 'relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Conv2D(256, 3, strides=(1,1), activation = 'relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=2))\r\nmodel.add(BatchNormalization())\r\n\r\nmodel.add(Conv2D(512, 3, strides=(1,1), activation = 'relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(BatchNormalization())\r\n\r\nmodel.add(Flatten())\r\nmodel.add(Dense(128, activation = 'relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(32, activation = 'relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Dropout(0.5))\r\n\r\nmodel.add(Dense(1, activation = 'sigmoid'))\r\n\r\n\r\nmodel.compile(loss = \"binary_crossentropy\", optimizer = 'adam', metrics = ['accuracy'])\r\n#model.summary()\r\n\r\nearlyStop = EarlyStopping(monitor = 'val_accuracy', min_delta = 0.0001, patience = 50, restore_best_weights = True)\r\nmodel.fit(\r\n    train_gen,\r\n    workers=8,\r\n    epochs= 250,\r\n    validation_data=val_gen,\r\n    callbacks=earlyStop,\r\n    verbose=2)\r\n\r\nmodel.save(f'F:/Storage/TrainedVersions/YiffModel{Version}')\r\n```\r\n\r\n**Other info / logs** \r\nHere is my output before it started training, maybe it reveals an issue:\r\n\r\n```\r\n2020-11-13 23:10:49.880564: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-11-13 23:10:51.936872: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-11-13 23:10:51.937643: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2020-11-13 23:10:51.962681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\ncoreClock: 1.725GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-11-13 23:10:51.963072: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-11-13 23:10:51.976456: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-11-13 23:10:51.976652: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-11-13 23:10:51.981390: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-11-13 23:10:51.982700: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-11-13 23:10:51.991145: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-11-13 23:10:51.994156: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-11-13 23:10:51.994790: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-11-13 23:10:51.995118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-11-13 23:10:51.996157: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-11-13 23:10:51.997339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\ncoreClock: 1.725GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-11-13 23:10:51.997838: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-11-13 23:10:51.998134: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-11-13 23:10:51.998336: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-11-13 23:10:51.998569: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-11-13 23:10:51.998775: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-11-13 23:10:51.998939: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-11-13 23:10:51.999074: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-11-13 23:10:51.999208: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-11-13 23:10:51.999393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-11-13 23:10:52.570125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-11-13 23:10:52.570326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2020-11-13 23:10:52.570464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2020-11-13 23:10:52.570786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5734 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6)\r\n2020-11-13 23:10:52.571767: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\nWARNING:tensorflow:From e:\\PYTHON\\YiffMiner\\TrainYIFF.py:15: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\r\n\r\n2020-11-13 23:10:52.698623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\ncoreClock: 1.725GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-11-13 23:10:52.699014: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-11-13 23:10:52.699212: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-11-13 23:10:52.699429: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-11-13 23:10:52.699644: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-11-13 23:10:52.699843: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-11-13 23:10:52.700071: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-11-13 23:10:52.700276: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-11-13 23:10:52.700484: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-11-13 23:10:52.700719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-11-13 23:10:52.700911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-11-13 23:10:52.701114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2020-11-13 23:10:52.701242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2020-11-13 23:10:52.701519: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5734 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6)\r\n2020-11-13 23:10:52.701888: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-11-13 23:10:53.763226: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2020-11-13 23:10:55.655709: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-11-13 23:10:56.552988: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-11-13 23:10:57.143012: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-11-13 23:10:59.117292: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0\r\n\r\n2020-11-13 23:10:59.160112: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0\r\n\r\n2020-11-13 23:11:00.955185: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\r\nFound 3266 images belonging to 2 classes.\r\nFound 86 images belonging to 2 classes.\r\n```", "comments": ["@Poofy1,\r\nOn running the code, I am facing an error stating `FileNotFoundError: [Errno 2] No such file or directory: 'F:/Storage/DataSet_Bal/Train'`. \r\n\r\nIn order to reproduce the issue reported here, could you please provide all the supporting files necessary to run the code. Thanks!\r\n", "@Poofy1 Thanks for raising this issue. I have couple of questions.\r\n\r\n1. I see you tried `TF2.4-rc1` and `tf-nightly`. Did you try `TF2.2` or `TF2.3`? Were you facing same issue with older versions also?\r\n\r\n2. I noticed that your are working on binary classification. What data you are using? If your data is private, can you please try public data like `cats versus dogs` data and let us know whether you face same issue. \r\n\r\n3. Did you try distribution strategies with keras? Please check [this guide](https://www.tensorflow.org/tutorials/distribute/keras).\r\n\r\n4. How many GPU's you had used?\r\n\r\n5. Can you please try with other older python version such as 3.7. \r\n\r\nThanks again for creating this issue. \r\n", "@jvishnuvardhan Thanks for the questions but I have found the solution. Turns out TensorFlow does not support CUDA 11.1 right now but it does support 11.0. In order to get CUDA 11.0 to work, I had to move the ptxas.exe from the CUDA 11.1 directory to my 11.0 directory."]}, {"number": 44871, "title": "golang tensorflow issue", "body": "go: finding module for package github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto\r\n../../go/pkg/mod/github.com/tensorflow/tensorflow@v2.3.1+incompatible/tensorflow/go/saved_model.go:25:2: module github.com/tensorflow/tensorflow@latest found (v2.3.1+incompatible), but does not contain package github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto\r\n", "comments": ["It is a duplicate of https://github.com/tensorflow/tensorflow/issues/41808. Can you close this and upvote/subscribe that one?\r\nThanks", "> It is a duplicate of #41808. Can you close this and upvote/subscribe that one?\r\n> Thanks\r\n\r\nyo nice :) \ud83d\udc4d ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44871\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44871\">No</a>\n", "It is a duplicate of #41808. Can you close this and upvote/subscribe that one?\r\nThanks"]}, {"number": 44870, "title": " InvalidArgumentError: 2 root error(s) found.   (0) Invalid argument: You must feed a value for placeholder tensor 'dense_1_target_3' with dtype float and shape [?,?] \t [[{{node dense_1_target_3}}]] \t [[metrics_10/top_k_categorical_accuracy/Identity/_13387]]   (1) Invalid argument: You must feed a value for placeholder tensor 'dense_1_target_3' with dtype float and shape [?,?] \t [[{{node dense_1_target_3}}]] 0 successful operations. 0 derived errors ignored.", "body": "This is the code where I have trained a single model 3 times.\r\n```\r\ndef build_mobilenet(img_vec):\r\n        print(\"mobilenet loading..........\")\r\n        model = Sequential()\r\n        base_mobilenet_model = MobileNet(input_shape = img_vec.shape[1:], \r\n                                 include_top = False, weights = None)\r\n        model.add(Input(shape = img_vec.shape[1:], name='input_layer'))\r\n        model.add(base_mobilenet_model)\r\n        model.add(GlobalAveragePooling2D())\r\n        model.add(Dropout(0.5))\r\n        model.add(Dense(len(all_labels), activation = 'sigmoid'))\r\n        METRICS = [\"binary_accuracy\", \"top_k_categorical_accuracy\", hn_multilabel_loss, tf.keras.metrics.AUC(), 'mae']\r\n        model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = METRICS)\r\n        model.summary()\r\n        return model\r\n```\r\nThe above-mentioned model was trained and saved 3 times and will be used for the ensemble as follows\r\n```\r\ndef build_ensemble(img_vec):\r\n    model_input = Input(shape = img_vec.shape[1:], name='input_layer_main')\r\n    ## define fuctional blocks\r\n    mobilenet1 = keras.models.load_model(\"models/mobilenet_1_30epochs_multi-label.augmented.h5\", \\\r\n                                    custom_objects = {\"hn_multilabel_loss\" : hn_multilabel_loss})\r\n    mobilenet1._name , mobilenet1.trainable = \"model_1\", False\r\n    ###\r\n    mobilenet2 = keras.models.load_model(\"models/mobilenet_2_30epochs_multi-label.augmented.h5\", \\\r\n                                    custom_objects = {\"hn_multilabel_loss\" : hn_multilabel_loss})\r\n    mobilenet2._name , mobilenet2.trainable = \"model_2\", False\r\n    ###\r\n    mobilenet3 = keras.models.load_model(\"models/mobilenet_3_30epochs_multi-label.augmented.h5\", \\\r\n                                    custom_objects = {\"hn_multilabel_loss\" : hn_multilabel_loss})\r\n    mobilenet3._name , mobilenet3.trainable = \"model_3\", False\r\n\r\n    #merge 3 models\r\n    model1, model2, model3=(mobilenet1(model_input), mobilenet2(model_input), mobilenet3(model_input))\r\n    merge = concatenate([model1, model2, model3], name=\"concat_merge_123\")\r\n    output_layer = Dense(len(all_labels), activation = 'sigmoid', name = \"output_layer\")(merge)\r\n    \r\n    model = keras.models.Model(inputs= model_input, outputs= output_layer)## model assign\r\n\r\n    OPTIMIZER = Adam(learning_rate=0.001,beta_1=0.9, beta_2=0.999)\r\n\r\n    METRICS = [\"binary_accuracy\", \"top_k_categorical_accuracy\", hn_multilabel_loss, tf.keras.metrics.AUC(), 'mae']\r\n    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = METRICS)\r\n    model.summary()\r\n    return model\r\n```\r\n`model summary()`,\r\n```\r\nModel: \"model_3\"\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_layer_main (InputLayer)   [(None, 128, 128, 1) 0                                            \r\n__________________________________________________________________________________________________\r\nmodel_1 (Sequential)            (None, 13)           3241613     input_layer_main[0][0]           \r\n__________________________________________________________________________________________________\r\nmodel_2 (Sequential)            (None, 13)           3241613     input_layer_main[0][0]           \r\n__________________________________________________________________________________________________\r\nmodel_3 (Sequential)            (None, 13)           3241613     input_layer_main[0][0]           \r\n__________________________________________________________________________________________________\r\nconcat_merge_123 (Concatenate)  (None, 39)           0           model_1[1][0]                    \r\n                                                                 model_2[1][0]                    \r\n                                                                 model_3[1][0]                    \r\n__________________________________________________________________________________________________\r\noutput_layer (Dense)            (None, 13)           520         concat_merge_123[0][0]           \r\n==================================================================================================\r\nTotal params: 9,725,359\r\nTrainable params: 520\r\nNon-trainable params: 9,724,839\r\n__________________________________________________________________________________________________\r\n```\r\nthen during fitting, It gives the error\r\n\r\n```\r\nweight_path=\"models/ensemble_model_multilabel.best.h5\"\r\n\r\npatience_reduce_lr=1\r\nmin_lr=1e-8\r\noutput_dir=\"models/\"\r\ncallbacks_list = [\r\n            ModelCheckpoint(weight_path, monitor='val_accuracy', verbose=1, \r\n                             save_best_only=True, save_weights_only=False, mode='auto'),\r\n            ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=patience_reduce_lr,\r\n                              verbose=1, mode=\"min\", min_lr=min_lr),\r\n            ]\r\nensemble_model = build_ensemble(t_x)\r\nhist3 = ensemble_model.fit_generator(train_gen, \r\n                              steps_per_epoch=100,\r\n                              validation_data = (test_X, test_Y), \r\n                              epochs = 30, \r\n                              callbacks = callbacks_list)\r\n```\r\nI'm getting error,\r\n```\r\nEpoch 1/30\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-18-fe1e7c41d9e6> in <module>\r\n     18                               validation_data = (test_X, test_Y),\r\n     19                               epochs = 30,\r\n---> 20                               callbacks = callbacks_list)\r\n     21 \r\n     22 ensemble_model.save(\"models/ensemble_model_30epochs_multilabel-label.augmented.h5\")\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\r\n   1431         shuffle=shuffle,\r\n   1432         initial_epoch=initial_epoch,\r\n-> 1433         steps_name='steps_per_epoch')\r\n   1434 \r\n   1435   def evaluate_generator(self,\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\r\n    262 \r\n    263       is_deferred = not model._is_compiled\r\n--> 264       batch_outs = batch_function(*batch_data)\r\n    265       if not isinstance(batch_outs, list):\r\n    266         batch_outs = [batch_outs]\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)\r\n   1173       self._update_sample_weight_modes(sample_weights=sample_weights)\r\n   1174       self._make_train_function()\r\n-> 1175       outputs = self.train_function(ins)  # pylint: disable=not-callable\r\n   1176 \r\n   1177     if reset_metrics:\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py in __call__(self, inputs)\r\n   3290 \r\n   3291     fetched = self._callable_fn(*array_vals,\r\n-> 3292                                 run_metadata=self.run_metadata)\r\n   3293     self._call_fetch_callbacks(fetched[-len(self._fetches):])\r\n   3294     output_structure = nest.pack_sequence_as(\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py in __call__(self, *args, **kwargs)\r\n   1456         ret = tf_session.TF_SessionRunCallable(self._session._session,\r\n   1457                                                self._handle, args,\r\n-> 1458                                                run_metadata_ptr)\r\n   1459         if run_metadata:\r\n   1460           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\nInvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument: You must feed a value for placeholder tensor 'dense_1_target_3' with dtype float and shape [?,?]\r\n\t [[{{node dense_1_target_3}}]]\r\n\t [[metrics_10/top_k_categorical_accuracy/Identity/_13387]]\r\n  (1) Invalid argument: You must feed a value for placeholder tensor 'dense_1_target_3' with dtype float and shape [?,?]\r\n\t [[{{node dense_1_target_3}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n```", "comments": ["Hi!\r\n\r\nHave you fixed the problem?\r\n\r\nHow often do you have serious issue like this? We are creating our platform special for situations like this. There are a lot of mentors that can consult you and help to solute your problems. \r\n\r\nFor example, you may try to discuss it with Maria. She is Site Reliability Engineer from Google. Try it\u0431 if you want: https://solvery.io/mentor/maria_vtyurina", "@WrathofBhuvan11,\r\nOn running the code, I'm facing an error stating `NameError: name 't_x' is not defined`, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/5081a8b0f1a5ac3acae59cc606348928/44870.ipynb).\r\n\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44869, "title": "CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected", "body": "CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n\r\n```\r\n$ export LD_LIBRARY_PATH=/usr/local/cuda-10.1/compat/:$LD_LIBRARY_PATH\r\npython3 -c \"import tensorflow as tf; hello = tf.constant('hello world')\"\r\n2020-11-11 10:46:36.362374: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-11-11 10:46:37.459527: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-11-11 10:46:37.467224: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2020-11-11 10:46:37.467270: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (16c54693e7f2): /proc/driver/nvidia/version does not exist\r\n2020-11-11 10:46:37.467664: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-11-11 10:46:37.493842: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300060000 Hz\r\n2020-11-11 10:46:37.494252: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55dd7ef8bc70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n```", "comments": ["I had CUDA installed, but not the nvidia drivers.\r\n\r\nInstalled them on ubuntu 18.04 as follows to resolve:\r\n```\r\n# add nvidia repos\r\ncurl -fsSL https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub | apt-key add - && \\\r\necho \"deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 /\" > /etc/apt/sources.list.d/cuda.list && \\\r\necho \"deb https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 /\" > /etc/apt/sources.list.d/nvidia-ml.list\r\napt-get update\r\n\r\n# install nvidia kernel module\r\napt-get install -y --no-install-recommends cuda-drivers\r\n```\r\n\r\n```\r\n\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44869\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44869\">No</a>\n"]}, {"number": 44868, "title": "Micro_speech error when given new model", "body": "\r\n**System information**\r\n- Have I written custom code : Only changed the model and everything the model requires (size of it)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Nano 33 BLE Sense \r\n- TensorFlow installed from (source or binary): Arduino Library\r\n- TensorFlow Version: 2.1.0-ALPHA\r\n\r\nWhen ran I get this error:\r\n```\r\nFeature generation failed\r\nRequested feature_data_ size 536907080 doesn't match 1960\r\n\r\n```\r\nIt should be the basic micro_speech output with words found and time of them found.\r\nIt was originally working with this model, but for some reason, it is not longer working. \r\n\r\nCode: https://github.com/michealcarac/TempMicroSpeech\r\n\r\nIn micro_speechwithfinal.ino:\r\n```\r\nint8_t feature_buffer[kFeatureElementCount];\r\n\r\n kFeatureElementCount = (kFeatureSliceSize * kFeatureSliceCount);\r\n\r\n```\r\nin micro_features_micro_model_settings.h\r\n\r\nHowever, \r\nwhen \r\n```\r\nstatic FeatureProvider static_feature_provider(kFeatureElementCount,\r\n                                                 feature_buffer);\r\n```\r\nin  micro_speechwithfinal.ino\r\n\r\nthen in feature_provider.cpp\r\n```\r\n\r\nFeatureProvider::FeatureProvider(int feature_size, int8_t* feature_data)\r\n    : feature_size_(feature_size),\r\n      feature_data_(feature_data),\r\n      is_first_run_(true) {\r\n  // Initialize the feature data to default values.\r\n  for (int n = 0; n < feature_size_; ++n) {\r\n    feature_data_[n] = 0;\r\n  }\r\n}\r\n\r\n```\r\n\r\nBUT for some reason, feature_size_ is 536907080 yet kFeatureElementCount was 1960 in the beginning...", "comments": ["Please take a look at this thread https://github.com/tensorflow/tensorflow/issues/39938", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Issue fixed by creating a new model on TF 1.15. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44868\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44868\">No</a>\n", "It seems that model created with newer tf uses Conv2D op too and the example is not up-to-date with this. Adding\r\n`if (micro_op_resolver.AddConv2D() != kTfLiteOk) {\r\n    return;\r\n  }`\r\n  resolved the issue for me.\r\n  Don't forget to increase opCount of MicroMutableOpResolver if a new op is added:\r\n  `static tflite::MicroMutableOpResolver<5> micro_op_resolver(error_reporter);`"]}, {"number": 44867, "title": "add CheckpointManager example", "body": "add CheckpointManager example\r\nhttps://github.com/tensorflow/tensorflow/pull/44818", "comments": ["@mihaimaruseac ", "The docstring examples in this format need to actually run; you can test by running this target: https://github.com/tensorflow/tensorflow/blob/acd6f26bbaa2f747b88bfd480c6d607b4836b9ef/tensorflow/tools/docs/BUILD#L42\r\n\r\n", "> The docstring examples in this format need to actually run; you can test by running this target:\r\n> \r\n> https://github.com/tensorflow/tensorflow/blob/acd6f26bbaa2f747b88bfd480c6d607b4836b9ef/tensorflow/tools/docs/BUILD#L42\r\n\r\nThis is not the complete code . It may not be run. It just to help developer to learn about how to use `CheckpointManager ` to save model every N steps. I have changed it to markdown format.", "I think we should keep the example doctested; adding a new example at the beginning that isn't seems like a regression.\r\n\r\nI do think a simple, doctested example to start things off is a good idea.", "@SmileTM  Can you please check @allenlavoie's comments and keep us posted ? Thanks!"]}, {"number": 44866, "title": "add CheckpointManager example", "body": "add CheckpointManager example\r\nhttps://github.com/tensorflow/tensorflow/pull/44818", "comments": []}, {"number": 44865, "title": "tensorflow gradient update does not match torch gradient update? ", "body": "```python\r\nimport os\r\nimport sys\r\n\r\nos.environ[\r\n    \"KMP_DUPLICATE_LIB_OK\"\r\n] = \"True\"  # uncomment this line if omp error occurs on OSX for python 3\r\nos.environ[\"MKL_NUM_THREADS\"] = \"1\"  # set number of MKL threads to run in parallel\r\nos.environ[\"OMP_NUM_THREADS\"] = \"4\"\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport torch\r\nfrom tensorflow.keras import Sequential, datasets, layers, metrics, optimizers\r\n\r\n# parameters\r\nN, D_in, H, D_out = 40, 120, 1, 25\r\nlearning_rate = 0.1\r\nopt = \"adam\"  # 'adam'\r\n# opt = 'sgd'\r\n\r\n\r\n# helper utilies\r\ndef detach(tensor):\r\n    if isinstance(tensor, torch.Tensor):\r\n        if tensor.device.type == \"cuda\":\r\n            tensor = tensor.cpu()\r\n        if tensor.requires_grad:\r\n            tensor = tensor.detach()\r\n        tensor = tensor.numpy()\r\n\r\n    return tensor\r\n\r\n\r\n# Create random Tensors to hold inputs and outputs\r\nx = torch.randn(N, D_in)\r\ny = torch.randn(N, D_out)\r\n\r\nx_tf = tf.convert_to_tensor(detach(x))\r\ny_tf = tf.convert_to_tensor(detach(y))\r\n\r\n####### tensorflow\r\nnetwork = Sequential([layers.Dense(D_out, activation=\"relu\")])\r\nnetwork.build(input_shape=(None, D_in))\r\nnetwork.summary()\r\n\r\n\r\nif opt == \"adam\":\r\n    optimizer = optimizers.Adam(lr=learning_rate)\r\nelif opt == \"sgd\":\r\n    optimizer = optimizers.SGD(lr=learning_rate)\r\nelse:\r\n    raise NotImplementedError(opt + \" not implemented.\")\r\n\r\n\r\n####### torch\r\n# Use the nn package to define our model and loss function.\r\nmodel = torch.nn.Sequential(\r\n    torch.nn.Linear(D_in, D_out),\r\n    torch.nn.ReLU(),\r\n)\r\nloss_fn = torch.nn.MSELoss(reduction=\"mean\")\r\n\r\nif opt == \"adam\":\r\n    optimizer_th = torch.optim.Adam(model.parameters(), lr=learning_rate)\r\nelif opt == \"sgd\":\r\n    optimizer_th = torch.optim.SGD(model.parameters(), lr=learning_rate)\r\nelse:\r\n    raise NotImplementedError(opt + \" not implemented.\")\r\n\r\n\r\nparams_torch = list(model.parameters())\r\n\r\nweights = params_torch[0]\r\nbiases = params_torch[1]\r\n\r\nnetwork.layers[0].set_weights([detach(weights).T, detach(biases)])\r\n\r\n\r\nfor t in range(10):\r\n    with tf.GradientTape() as tape:\r\n\r\n        out = network(x_tf)\r\n        loss = tf.square(out - y_tf)\r\n        loss = tf.reduce_sum(loss) / N / D_out\r\n\r\n    grads = tape.gradient(loss, network.trainable_variables)\r\n    optimizer.apply_gradients(zip(grads, network.trainable_variables))\r\n\r\n    # torch\r\n    optimizer_th.zero_grad()\r\n    # Forward pass: compute predicted y by passing x to the model.\r\n    y_pred = model(x)\r\n\r\n    # Compute and print loss.\r\n    loss_th = loss_fn(y_pred, y)\r\n\r\n    loss_th.backward()\r\n\r\n    params_torch = list(model.parameters())\r\n\r\n    weights = params_torch[0]\r\n    biases = params_torch[1]\r\n\r\n    weights_grad = weights.grad\r\n    biases_grad = biases.grad\r\n    optimizer_th.step()\r\n\r\n    print(\r\n        \"check output diff:\", np.linalg.norm(y_pred.detach().numpy() - np.asarray(out))\r\n    )\r\n\r\n    print(t, \"loss: diff = {0:0.14f}.\".format(loss_th.item() - np.asarray(loss)))\r\n\r\n    print(\r\n        \"check weights/biases grads diff:\",\r\n        np.linalg.norm(weights_grad.detach().numpy() - np.asarray(grads[0]).T),\r\n        np.linalg.norm(biases_grad.detach().numpy() - np.asarray(grads[1])),\r\n    )\r\n\r\n    print()\r\n```\r\n\r\nThis is my comparison between torch and tensorflow but they don't match. \r\n\r\n\r\n```bash\r\ncheck output diff: 2.6182365e-06\r\n0 loss: diff = 0.00000000000000.\r\ncheck weights/biases grads diff: 1.1457902e-07 2.150125e-08\r\n\r\ncheck output diff: 0.17770502\r\n1 loss: diff = 0.00141370296478.\r\ncheck weights/biases grads diff: 0.031174246 0.002777245\r\n\r\ncheck output diff: 1.8508035\r\n2 loss: diff = 0.00122940540314.\r\ncheck weights/biases grads diff: 0.0800667 0.0038902021\r\n\r\ncheck output diff: 1.8711921\r\n3 loss: diff = -0.00223851203918.\r\ncheck weights/biases grads diff: 0.05721609 0.0050597424\r\n\r\ncheck output diff: 1.6562314\r\n4 loss: diff = 0.00020694732666.\r\ncheck weights/biases grads diff: 0.031922814 0.0016808284\r\n\r\ncheck output diff: 1.6612757\r\n5 loss: diff = -0.00040757656097.\r\ncheck weights/biases grads diff: 0.03356902 0.0019494247\r\n\r\ncheck output diff: 1.9143565\r\n6 loss: diff = -0.00022947788239.\r\ncheck weights/biases grads diff: 0.055081423 0.005770285\r\n\r\ncheck output diff: 2.6084518\r\n7 loss: diff = 0.00054198503494.\r\ncheck weights/biases grads diff: 0.032326505 0.0040451903\r\n\r\ncheck output diff: 2.745288\r\n8 loss: diff = -0.00023376941681.\r\ncheck weights/biases grads diff: 0.030059196 0.002589117\r\n\r\ncheck output diff: 2.4303644\r\n9 loss: diff = 0.00073283910751.\r\ncheck weights/biases grads diff: 0.018728718 0.0017236237\r\n\r\n```\r\n", "comments": ["Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/3607cdfd623ed9443bdb7c20f7a10517/44865.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/9563f297f291525dcaa9f9b18a5a3c34/44865-tf-nightly.ipynb). Please find the attached gist. Thanks!", "@JiahaoYao are you making sure the two models have identical initializations? They may receive different random values otherwise.", "yes, i extracted the initialization from one to initialize the other and ensured that they started with the same parameters.", "It seems that the two optimizers have different values for epsilon: `keras.optimizers.Adam` uses 1e-7, while `torch.optim.Adam` uses 1e-8. I'd also recommend using SGD which has fewer moving parts. I think that one shows smaller differences. I don't know if they're small enough to attribute to the order of computations, though, so it would be interesting to dig deeper there.", "Oh i see, thanks @mdanatg ", "Feel free to close this issue if its solved. Thanks!", "Cool, thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44865\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44865\">No</a>\n"]}, {"number": 44864, "title": "Save and load models tutorial uses deprecated argument `period`", "body": "\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/tutorials/keras/save_and_load#checkpoint_callback_options\r\n\r\n## Description of issue (what needs changing):\r\n\r\nSave and load tutorial uses deprecated argument `period`. This needs to be updated with `save_freq`.\r\n\r\n```\r\n# Create a callback that saves the model's weights every 5 epochs\r\ncp_callback = tf.keras.callbacks.ModelCheckpoint(\r\n    filepath=checkpoint_path, \r\n    verbose=1, \r\n    save_weights_only=True,\r\n    period=5)\r\n```\r\n\r\nIt also throws several warning as shown below\r\n\r\n```\r\nWARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\r\nWARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\r\n```\r\n\r\nPlease check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/66c6d0ead7f02600f7b0c67dc63bbd16/save_and_load.ipynb). Thanks!\r\n\r\n\r\n\r\n", "comments": ["Can we close this?", "Thanks @bhack. Closing this as this was resolved. "]}, {"number": 44863, "title": "Disabled performance killing ASSERTS", "body": "On embedded targets, the Offset() function is called quite frequently, Within it, these ASSERTS generate compare/branch code which dramatically reduces the performance. By removing them, the inference pipeline on target CPUs like the Cortex-M run nearly 2x faster.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44863) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "Hi Larry,\r\n\r\nCould you help measure the difference on the number of cycles or check the compiled the assembly code.\r\n\r\nMy understanding is that DCHECK is not ran in the release build (built with BUILD_TYPE=release)\r\n\r\nPlease check here: https://github.com/tensorflow/tensorflow/blob/acd6f26bbaa2f747b88bfd480c6d607b4836b9ef/tensorflow/lite/kernels/op_macros.h#L61\r\n\r\nand here: https://github.com/tensorflow/tensorflow/blob/acd6f26bbaa2f747b88bfd480c6d607b4836b9ef/tensorflow/lite/micro/tools/make/Makefile#L200", "I don't have a simple build environment to take your repo and run it on devices. It's easiest to test this on the Arduino library. This assumption about DCHECK may be true on \"big\" Arm and x86, but it's not true on embedded. Run the speech example sketch that comes with the Arduino_TensorFlowLite library (same code is in there) and then make the change I made. You'll see a dramatic perf difference.\r\n", "Ah I see what you meant. It's hard to set compiler flag on Arduino so you're right that the speed on Arduino is not optimized. This PR touches code shared between TFLite and TFLite micro so it would be hard to submit it as it is for now. Let me think see what can we do.", "Hi Larry,\r\n\r\nCould you help verify if this PR helped your case?\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/6d5f02b47af2efb5ed2f5b4ccf34d4abecf8cfde\r\n\r\nThanks,\r\nTiezhen", "@bitbank2 Can you please check @wangtz's comments and keep us posted ? Thanks!", "Hi Tiezhen,\r\nI can't easily test your repo on my equipment, but your fix is correct. I approve.\r\nThanks for taking action on this issue.\r\nL.B.\r\n"]}, {"number": 44862, "title": "[INTEL MKL] DNN 0.x code cleanup - backward conv ops", "body": "DNN 0.x cleanup of backward conv ops:\r\n(1) Remove all DNN 0.x related code;\r\n(2) Replace all DNN 1.x macro usages", "comments": []}, {"number": 44861, "title": "TypeError: An op outside of the function building code is being passed a \"Graph\" tensor", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 2.4-RC0\r\n- Python version: 3.8\r\n- CUDA/cuDNN version: 11/8.0.4\r\n- GPU model and memory: RTX 3080, 10GB \r\n\r\n**Describe the current behavior**\r\nI am trying to build a custom RNN cell, based on this paper:\r\n\r\n> Using Fast Weights to Attend to the Recent Past\r\n> Jimmy Ba, Geoffrey Hinton, Volodymyr Mnih, Joel Z. Leibo, Catalin Ionescu\r\n> NIPS 2016, https://arxiv.org/abs/1610.06258\r\n\r\nHowever, my implementation returns this error, but I have no clue what this means:\r\n\r\n```\r\nTypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  @tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\nThe graph tensor has name: FW-RNN/while/fw_rnn_cell_2/add_2:0\r\n\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\nThe model should simply start training.\r\n\r\n**Standalone code to reproduce the issue**\r\nCode:  https://gist.github.com/ion-elgreco/cc6fed29a4f6fb6813b71d5e7e8f2c87#file-fw_rnn-py\r\n\r\n**Other info / logs** \r\nError log: https://gist.github.com/ion-elgreco/cc6fed29a4f6fb6813b71d5e7e8f2c87#file-fw_rnn_error-txt\r\n", "comments": ["If I comment out self.A:\r\n\r\n```\r\n # Fast weights update rule: A(t) = \u03bb*A(t-1) + \u03b7*h(t) \u22c5 h(t)^T\r\n self.A = (self.l * self.A) + K.dot((self.e * prev_output),K.transpose(prev_output))    \r\n```\r\n\r\nThe TypeError does not show up, so it seems to be related to that. Still not sure how I can update an attribute of the cell with the custom update rule within tensorflow.", "@MarkDaoust I think that like https://github.com/tensorflow/tensorflow/issues/44840 this kind of Error message it is intercepted at too low-level so it is going to create confusion.  We need to look for a solution to emit a more \"Developer friendly\" error earlier in the stack.", "@ion-elgreco Why the downvote? I suppose that if the error was `self-explainable` you wouldn't have opened the ticket. Right?", "> @ion-elgreco Why the downvote? I suppose that if the error was `self-explainable` you wouldn't have opened the ticket. Right?\r\n\r\nSorry missclicked, but the error can still not be explained.  I wouldn't have opened the ticket indeed if it was self-explainable. It's really difficult to debug TensorFlow with such cryptic errors :)\r\n", "@bhack Do you perhaps know how I can update weights with a custom update rule? ", "I can take a look at your specific issue but you really need to minimize your code and share something that I could simply copy, paste and run (or a Colab minimal example).", "> I can take a look at your specific issue but you really need to minimize your code and share something that I could simply copy, paste and run (or a Colab minimal example).\r\n\r\nAlright, thank you! I will create it now on colab", "> I can take a look at your specific issue but you really need to minimize your code and share something that I could simply copy, paste and run (or a Colab minimal example).\r\n\r\n@bhack Here is the google collab link: \r\nhttps://colab.research.google.com/drive/1SaC_Fd9ZXGcscE-G63opAat8k5ElQMXd?usp=sharing\r\n\r\nI put it on commenter, but do I need to give you editor access?", "Is your claim at https://github.com/tensorflow/tensorflow/issues/44861#issuecomment-727202047  still valid with the Colab?", "> Is your claim at [#44861 (comment)](https://github.com/tensorflow/tensorflow/issues/44861#issuecomment-727202047) still valid with the Colab?\r\n\r\nYes, sadly the error still occurs.\r\n", "Did you tried with `self.A.assign_add`?", "> Did you tried with `self.A.assign_add`?\r\n\r\nThis seems to work, but `assign_add` does addition right? However, I need to replace the variable's value, so then I should use `assign`? At least that's how I am reading it from here: https://www.tensorflow.org/api_docs/python/tf/Variable\r\n\r\nSorry, for these noob questions, I am fairly new to tensorflow. ", "Yes if you need only to assign.", "> Yes if you need only to assign.\r\n\r\nThank you so much! \ud83d\udc4d \r\n\r\nNow I just need to simply fix the last error: `Matrix size-incompatible: In[0]: [1,64], In[1]: [128,64] [Op:MatMul]` \r\n\r\nWhich seems to be related to my fast weights matrix initialization, however, if I choose [batch_size, units] as shape to initialize the fast weights matrix, my output shape of the RNN layer will be `[batch_size, sequence_length, features]`, while it has to be `[None, sequence_length, features]`, and it will return this error:\r\n\r\n`Matrix size-incompatible: In[0]: [128,64], In[1]: [128,64] [Op:MatMul]` ", "For other general support questions you need to use https://stackoverflow.com/questions/tagged/tensorflow", "> For other general support questions you need to use https://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nWill do! Thanks for the help :)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44861\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44861\">No</a>\n"]}, {"number": 44860, "title": "Move flatbuffers downloading into its own stand-alone script.", "body": "Addresses http://b/143904317\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 44859, "title": "Update version numbers for TensorFlow 2.4.0-rc2", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 4 -> 4\nPatch: 0 -> 0\n\nNo lingering old version strings \"2.4.0-rc1\" found in source directory \n\"tensorflow/\". Good.\nNo lingering old version strings \"2.4.0rc1\" found in source directory \n\"tensorflow/\". Good.\n```", "comments": []}, {"number": 44858, "title": "Error: \"tensorflow.python.framework.errors_impl.InvalidArgumentError\" when training Mask RCNN Inception Resnet V2 1024x1024 model", "body": "I am training a Mask R-CNN Inception ResNet V2 1024x1024 algorithm using my computer's GPU. This was downloaded from the TensorFlow Detection Model Zoo, and I labeled my images (dimensions of 1100x1100 pixels) with Label-img. Here is what I am working with:\r\n\r\n - GPU: NVIDIA GEFORCE RTX 2060\r\n - GPU: 16GB RAM, 6 processor cores\r\n - TensorFlow: 2.3.1\r\n - Python: 3.8.6\r\n - CUDA: 10.1\r\n - cuDNN: 7.6\r\n - Anaconda 3 command prompt\r\n\r\nAll tfrecord files have been generated, and when I start to train my model using ```python model_main_tf2.py --model_dir=models/my_faster_rcnn --pipeline_config_path=models/my_faster_rcnn/pipeline.config```, I get the following errors:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"model_main_tf2.py\", line 113, in <module>\r\n    tf.compat.v1.app.run()\r\n  File \"C:\\user\\anaconda3\\envs\\object_detection_api\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"C:\\user\\anaconda3\\envs\\object_detection_api\\lib\\site-packages\\absl\\app.py\", line 303, in run\r\n    _run_main(main, args)\r\n  File \"C:\\user\\anaconda3\\envs\\object_detection_api\\lib\\site-packages\\absl\\app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"model_main_tf2.py\", line 104, in main\r\n    model_lib_v2.train_loop(\r\n  File \"C:\\user\\anaconda3\\envs\\object_detection_api\\lib\\site-packages\\object_detection\\model_lib_v2.py\", line 564, in train_loop\r\n    load_fine_tune_checkpoint(detection_model,\r\n  File \"C:\\user\\anaconda3\\envs\\object_detection_api\\lib\\site-packages\\object_detection\\model_lib_v2.py\", line 350, in load_fine_tune_checkpoint\r\n    features, labels = iter(input_dataset).next()\r\n  File \"C:\\user\\anaconda3\\envs\\object_detection_api\\lib\\site-packages\\tensorflow\\python\\distribute\\input_lib.py\", line 645, in next\r\n    return self.__next__()\r\n  File \"C:\\user\\anaconda3\\envs\\object_detection_api\\lib\\site-packages\\tensorflow\\python\\distribute\\input_lib.py\", line 649, in __next__\r\n    return self.get_next()\r\n  File \"C:\\user\\anaconda3\\envs\\object_detection_api\\lib\\site-packages\\tensorflow\\python\\distribute\\input_lib.py\", line 694, in get_next\r\n    self._iterators[i].get_next_as_list_static_shapes(new_name))\r\n  File \"C:\\user\\anaconda3\\envs\\object_detection_api\\lib\\site-packages\\tensorflow\\python\\distribute\\input_lib.py\", line 1474, in get_next_as_list_static_shapes\r\n    return self._iterator.get_next()\r\n  File \"C:\\user\\anaconda3\\envs\\object_detection_api\\lib\\site-packages\\tensorflow\\python\\data\\ops\\multi_device_iterator_ops.py\", line 581, in get_next\r\n    result.append(self._device_iterators[i].get_next())\r\n  File \"C:\\user\\anaconda3\\envs\\object_detection_api\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 825, in get_next\r\n    return self._next_internal()\r\n  File \"C:\\user\\anaconda3\\envs\\object_detection_api\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 764, in _next_internal\r\n    return structure.from_compatible_tensor_list(self._element_spec, ret)\r\n  File \"C:\\user\\anaconda3\\envs\\object_detection_api\\lib\\contextlib.py\", line 131, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"C:\\user\\anaconda3\\envs\\object_detection_api\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 2105, in execution_mode\r\n    executor_new.wait()\r\n  File \"C:\\user\\anaconda3\\envs\\object_detection_api\\lib\\site-packages\\tensorflow\\python\\eager\\executor.py\", line 67, in wait\r\n    pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: indices[16] = 16 is not in [0, 0)\r\n         [[{{node GatherV2_7}}]]\r\n         [[MultiDeviceIteratorGetNextFromShard]]\r\n         [[RemoteCall]]\r\n```\r\n\r\nThe config file that was used to run the model is:\r\n```\r\n# Mask R-CNN with Inception Resnet v2 (no atrous)\r\n# Sync-trained on COCO (with 8 GPUs) with batch size 16 (1024x1024 resolution)\r\n# Initialized from Imagenet classification checkpoint\r\n#\r\n# Train on GPU-8\r\n#\r\n# Achieves 40.4 box mAP and 35.5 mask mAP on COCO17 val\r\n\r\nmodel {\r\n  faster_rcnn {\r\n    number_of_stages: 3\r\n    num_classes: 1\r\n    image_resizer {\r\n      fixed_shape_resizer {\r\n        height: 1024\r\n        width: 1024\r\n      }\r\n    }\r\n    feature_extractor {\r\n      type: 'faster_rcnn_inception_resnet_v2_keras'\r\n    }\r\n    first_stage_anchor_generator {\r\n      grid_anchor_generator {\r\n        scales: [0.25, 0.5, 1.0, 2.0]\r\n        aspect_ratios: [0.5, 1.0, 2.0]\r\n        height_stride: 16\r\n        width_stride: 16\r\n      }\r\n    }\r\n    first_stage_box_predictor_conv_hyperparams {\r\n      op: CONV\r\n      regularizer {\r\n        l2_regularizer {\r\n          weight: 0.0\r\n        }\r\n      }\r\n      initializer {\r\n        truncated_normal_initializer {\r\n          stddev: 0.01\r\n        }\r\n      }\r\n    }\r\n    first_stage_nms_score_threshold: 0.0\r\n    first_stage_nms_iou_threshold: 0.7\r\n    first_stage_max_proposals: 300\r\n    first_stage_localization_loss_weight: 2.0\r\n    first_stage_objectness_loss_weight: 1.0\r\n    initial_crop_size: 17\r\n    maxpool_kernel_size: 1\r\n    maxpool_stride: 1\r\n    second_stage_box_predictor {\r\n      mask_rcnn_box_predictor {\r\n        use_dropout: false\r\n        dropout_keep_probability: 1.0\r\n        fc_hyperparams {\r\n          op: FC\r\n          regularizer {\r\n            l2_regularizer {\r\n              weight: 0.0\r\n            }\r\n          }\r\n          initializer {\r\n            variance_scaling_initializer {\r\n              factor: 1.0\r\n              uniform: true\r\n              mode: FAN_AVG\r\n            }\r\n          }\r\n        }\r\n        mask_height: 33\r\n        mask_width: 33\r\n        mask_prediction_conv_depth: 0\r\n        mask_prediction_num_conv_layers: 4\r\n        conv_hyperparams {\r\n          op: CONV\r\n          regularizer {\r\n            l2_regularizer {\r\n              weight: 0.0\r\n            }\r\n          }\r\n          initializer {\r\n            truncated_normal_initializer {\r\n              stddev: 0.01\r\n            }\r\n          }\r\n        }\r\n        predict_instance_masks: true\r\n      }\r\n    }\r\n    second_stage_post_processing {\r\n      batch_non_max_suppression {\r\n        score_threshold: 0.0\r\n        iou_threshold: 0.6\r\n        max_detections_per_class: 100\r\n        max_total_detections: 100\r\n      }\r\n      score_converter: SOFTMAX\r\n    }\r\n    second_stage_localization_loss_weight: 2.0\r\n    second_stage_classification_loss_weight: 1.0\r\n    second_stage_mask_prediction_loss_weight: 4.0\r\n    resize_masks: false\r\n  }\r\n}\r\n\r\ntrain_config: {\r\n  batch_size: 1\r\n  num_steps: 200000\r\n  optimizer {\r\n    momentum_optimizer: {\r\n      learning_rate: {\r\n        cosine_decay_learning_rate {\r\n          learning_rate_base: 0.008\r\n          total_steps: 200000\r\n          warmup_learning_rate: 0.0\r\n          warmup_steps: 5000\r\n        }\r\n      }\r\n      momentum_optimizer_value: 0.9\r\n    }\r\n    use_moving_average: false\r\n  }\r\n  gradient_clipping_by_norm: 10.0\r\n  fine_tune_checkpoint_version: V2\r\n  fine_tune_checkpoint: \"pre-trained-models/mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8/checkpoint/ckpt-0\"\r\n  fine_tune_checkpoint_type: \"detection\"\r\n  data_augmentation_options {\r\n    random_horizontal_flip {\r\n    }\r\n  }\r\n}\r\n\r\ntrain_input_reader: {\r\n  label_map_path: \"annotations/label_map.pbtxt\"\r\n  tf_record_input_reader {\r\n    input_path: \"annotations/train.record\"\r\n  }\r\n  load_instance_masks: true\r\n  mask_type: PNG_MASKS\r\n}\r\n\r\neval_config: {\r\n  metrics_set: \"coco_detection_metrics\"\r\n  metrics_set: \"coco_mask_metrics\"\r\n  eval_instance_masks: true\r\n  use_moving_averages: false\r\n  batch_size: 1\r\n  include_metrics_per_category: true\r\n}\r\n\r\neval_input_reader: {\r\n  label_map_path: \"annotations/label_map.pbtxt\"\r\n  shuffle: false\r\n  num_epochs: 1\r\n  tf_record_input_reader {\r\n    input_path: \"annotations/test.record\"\r\n  }\r\n  load_instance_masks: true\r\n  mask_type: PNG_MASKS\r\n}\r\n```\r\n\r\n**What can be done to fix this?**\r\n\r\n\r\n\r\n\r\n##############################################\r\n\r\nBelow are the scripts that are referenced in the error:\r\n\r\nFile \"model_main_tf2.py\", line 113:\r\n```\r\n#Lines 74-113:\r\ndef main(unused_argv):\r\n  flags.mark_flag_as_required('model_dir')\r\n  flags.mark_flag_as_required('pipeline_config_path')\r\n  tf.config.set_soft_device_placement(True)\r\n\r\n  if FLAGS.checkpoint_dir:\r\n    model_lib_v2.eval_continuously(\r\n        pipeline_config_path=FLAGS.pipeline_config_path,\r\n        model_dir=FLAGS.model_dir,\r\n        train_steps=FLAGS.num_train_steps,\r\n        sample_1_of_n_eval_examples=FLAGS.sample_1_of_n_eval_examples,\r\n        sample_1_of_n_eval_on_train_examples=(\r\n            FLAGS.sample_1_of_n_eval_on_train_examples),\r\n        checkpoint_dir=FLAGS.checkpoint_dir,\r\n        wait_interval=300, timeout=FLAGS.eval_timeout)\r\n  else:\r\n    if FLAGS.use_tpu:\r\n      # TPU is automatically inferred if tpu_name is None and\r\n      # we are running under cloud ai-platform.\r\n      resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\r\n          FLAGS.tpu_name)\r\n      tf.config.experimental_connect_to_cluster(resolver)\r\n      tf.tpu.experimental.initialize_tpu_system(resolver)\r\n      strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n    elif FLAGS.num_workers > 1:\r\n      strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n    else:\r\n      strategy = tf.compat.v2.distribute.MirroredStrategy()\r\n\r\n    with strategy.scope():\r\n      model_lib_v2.train_loop(\r\n          pipeline_config_path=FLAGS.pipeline_config_path,\r\n          model_dir=FLAGS.model_dir,\r\n          train_steps=FLAGS.num_train_steps,\r\n          use_tpu=FLAGS.use_tpu,\r\n          checkpoint_every_n=FLAGS.checkpoint_every_n,\r\n          record_summaries=FLAGS.record_summaries)\r\n\r\nif __name__ == '__main__':\r\n  tf.compat.v1.app.run()\r\n```\r\n\r\nFile \"C:\\user\\anaconda3\\envs\\object_detection_api\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40:\r\n```\r\n#Lines 17-40:\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport sys as _sys\r\n\r\nfrom absl.app import run as _run\r\n\r\nfrom tensorflow.python.platform import flags\r\nfrom tensorflow.python.util.tf_export import tf_export\r\n\r\n\r\ndef _parse_flags_tolerate_undef(argv):\r\n  \"\"\"Parse args, returning any unknown flags (ABSL defaults to crashing).\"\"\"\r\n  return flags.FLAGS(_sys.argv if argv is None else argv, known_only=True)\r\n\r\n\r\n@tf_export(v1=['app.run'])\r\ndef run(main=None, argv=None):\r\n  \"\"\"Runs the program with an optional 'main' function and 'argv' list.\"\"\"\r\n\r\n  main = main or _sys.modules['__main__'].main\r\n\r\n  _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n```\r\n\r\n\r\nFile \"C:\\user\\anaconda3\\envs\\object_detection_api\\lib\\site-packages\\absl\\app.py\", line 303:\r\n```\r\n#Lines 294-328:\r\ntry:\r\n    args = _run_init(\r\n        sys.argv if argv is None else argv,\r\n        flags_parser,\r\n    )\r\n    while _init_callbacks:\r\n      callback = _init_callbacks.popleft()\r\n      callback()\r\n    try:\r\n      _run_main(main, args)\r\n    except UsageError as error:\r\n      usage(shorthelp=True, detailed_error=error, exitcode=error.exitcode)\r\n    except:\r\n      exc = sys.exc_info()[1]\r\n      # Don't try to post-mortem debug successful SystemExits, since those\r\n      # mean there wasn't actually an error. In particular, the test framework\r\n      # raises SystemExit(False) even if all tests passed.\r\n      if isinstance(exc, SystemExit) and not exc.code:\r\n        raise\r\n\r\n      # Check the tty so that we don't hang waiting for input in an\r\n      # non-interactive scenario.\r\n      if FLAGS.pdb_post_mortem and sys.stdout.isatty():\r\n        traceback.print_exc()\r\n        print()\r\n        print(' *** Entering post-mortem debugging ***')\r\n        print()\r\n        pdb.post_mortem()\r\n      raise\r\n  except Exception as e:\r\n    _call_exception_handlers(e)\r\n    raise\r\n\r\n# Callbacks which have been deferred until after _run_init has been called.\r\n_init_callbacks = collections.deque()\r\n```\r\n\r\n\r\nFile \"C:\\user\\anaconda3\\envs\\object_detection_api\\lib\\site-packages\\absl\\app.py\", line 251:\r\n```\r\n#Lines 231-251:\r\ndef _run_main(main, argv):\r\n  \"\"\"Calls main, optionally with pdb or profiler.\"\"\"\r\n  if FLAGS.run_with_pdb:\r\n    sys.exit(pdb.runcall(main, argv))\r\n  elif FLAGS.run_with_profiling or FLAGS.profile_file:\r\n    # Avoid import overhead since most apps (including performance-sensitive\r\n    # ones) won't be run with profiling.\r\n    import atexit\r\n    if FLAGS.use_cprofile_for_profiling:\r\n      import cProfile as profile\r\n    else:\r\n      import profile\r\n    profiler = profile.Profile()\r\n    if FLAGS.profile_file:\r\n      atexit.register(profiler.dump_stats, FLAGS.profile_file)\r\n    else:\r\n      atexit.register(profiler.print_stats)\r\n    retval = profiler.runcall(main, argv)\r\n    sys.exit(retval)\r\n  else:\r\n    sys.exit(main(argv))\r\n```\r\n\r\n\r\nFile \"model_main_tf2.py\", line 104:\r\n```\r\n#Lines 74-113:\r\ndef main(unused_argv):\r\n  flags.mark_flag_as_required('model_dir')\r\n  flags.mark_flag_as_required('pipeline_config_path')\r\n  tf.config.set_soft_device_placement(True)\r\n\r\n  if FLAGS.checkpoint_dir:\r\n    model_lib_v2.eval_continuously(\r\n        pipeline_config_path=FLAGS.pipeline_config_path,\r\n        model_dir=FLAGS.model_dir,\r\n        train_steps=FLAGS.num_train_steps,\r\n        sample_1_of_n_eval_examples=FLAGS.sample_1_of_n_eval_examples,\r\n        sample_1_of_n_eval_on_train_examples=(\r\n            FLAGS.sample_1_of_n_eval_on_train_examples),\r\n        checkpoint_dir=FLAGS.checkpoint_dir,\r\n        wait_interval=300, timeout=FLAGS.eval_timeout)\r\n  else:\r\n    if FLAGS.use_tpu:\r\n      # TPU is automatically inferred if tpu_name is None and\r\n      # we are running under cloud ai-platform.\r\n      resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\r\n          FLAGS.tpu_name)\r\n      tf.config.experimental_connect_to_cluster(resolver)\r\n      tf.tpu.experimental.initialize_tpu_system(resolver)\r\n      strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n    elif FLAGS.num_workers > 1:\r\n      strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n    else:\r\n      strategy = tf.compat.v2.distribute.MirroredStrategy()\r\n\r\n    with strategy.scope():\r\n      model_lib_v2.train_loop(\r\n          pipeline_config_path=FLAGS.pipeline_config_path,\r\n          model_dir=FLAGS.model_dir,\r\n          train_steps=FLAGS.num_train_steps,\r\n          use_tpu=FLAGS.use_tpu,\r\n          checkpoint_every_n=FLAGS.checkpoint_every_n,\r\n          record_summaries=FLAGS.record_summaries)\r\n\r\nif __name__ == '__main__':\r\n  tf.compat.v1.app.run()\r\n```\r\n\r\n\r\nFile \"C:\\user\\anaconda3\\envs\\object_detection_api\\lib\\site-packages\\object_detection\\model_lib_v2.py\", line 564:\r\n```\r\n#Line 545-569:\r\nif record_summaries:\r\n    summary_writer = tf.compat.v2.summary.create_file_writer(\r\n        summary_writer_filepath)\r\n  else:\r\n    summary_writer = tf2.summary.create_noop_writer()\r\n\r\n  if use_tpu:\r\n    num_steps_per_iteration = 100\r\n  else:\r\n    # TODO(b/135933080) Explore setting to 100 when GPU performance issues\r\n    # are fixed.\r\n    num_steps_per_iteration = 1\r\n\r\n  with summary_writer.as_default():\r\n    with strategy.scope():\r\n      with tf.compat.v2.summary.record_if(\r\n          lambda: global_step % num_steps_per_iteration == 0):\r\n        # Load a fine-tuning checkpoint.\r\n        if train_config.fine_tune_checkpoint:\r\n          load_fine_tune_checkpoint(detection_model,\r\n                                    train_config.fine_tune_checkpoint,\r\n                                    fine_tune_checkpoint_type,\r\n                                    fine_tune_checkpoint_version,\r\n                                    train_input,\r\n                                    unpad_groundtruth_tensors)\r\n```\r\n\r\n\r\nFile \"C:\\user\\anaconda3\\envs\\object_detection_api\\lib\\site-packages\\object_detection\\model_lib_v2.py\", line 350:\r\n```\r\n#Lines 312-350:\r\ndef load_fine_tune_checkpoint(\r\n    model, checkpoint_path, checkpoint_type, checkpoint_version, input_dataset,\r\n    unpad_groundtruth_tensors):\r\n  \"\"\"Load a fine tuning classification or detection checkpoint.\r\n\r\n  To make sure the model variables are all built, this method first executes\r\n  the model by computing a dummy loss. (Models might not have built their\r\n  variables before their first execution)\r\n\r\n  It then loads an object-based classification or detection checkpoint.\r\n\r\n  This method updates the model in-place and does not return a value.\r\n\r\n  Args:\r\n    model: A DetectionModel (based on Keras) to load a fine-tuning\r\n      checkpoint for.\r\n    checkpoint_path: Directory with checkpoints file or path to checkpoint.\r\n    checkpoint_type: Whether to restore from a full detection\r\n      checkpoint (with compatible variable names) or to restore from a\r\n      classification checkpoint for initialization prior to training.\r\n      Valid values: `detection`, `classification`.\r\n    checkpoint_version: train_pb2.CheckpointVersion.V1 or V2 enum indicating\r\n      whether to load checkpoints in V1 style or V2 style.  In this binary\r\n      we only support V2 style (object-based) checkpoints.\r\n    input_dataset: The tf.data Dataset the model is being trained on. Needed\r\n      to get the shapes for the dummy loss computation.\r\n    unpad_groundtruth_tensors: A parameter passed to unstack_batch.\r\n\r\n  Raises:\r\n    IOError: if `checkpoint_path` does not point at a valid object-based\r\n      checkpoint\r\n    ValueError: if `checkpoint_version` is not train_pb2.CheckpointVersion.V2\r\n  \"\"\"\r\n  if not is_object_based_checkpoint(checkpoint_path):\r\n    raise IOError('Checkpoint is expected to be an object-based checkpoint.')\r\n  if checkpoint_version == train_pb2.CheckpointVersion.V1:\r\n    raise ValueError('Checkpoint version should be V2')\r\n\r\n  features, labels = iter(input_dataset).next()\r\n```\r\n\r\n\r\nFile \"C:\\user\\anaconda3\\envs\\object_detection_api\\lib\\site-packages\\tensorflow\\python\\distribute\\input_lib.py\", issues with line 645, 645, 694:\r\n\r\n```\r\n#Lines 615-728:\r\nclass DistributedIteratorBase(DistributedIteratorInterface):\r\n  \"\"\"Common implementation for all input iterators.\"\"\"\r\n\r\n  # pylint: disable=super-init-not-called\r\n  def __init__(self, input_workers, iterators, strategy):\r\n    static_shape = _get_static_shape(iterators)\r\n\r\n    # TODO(b/133073708): we currently need a flag to control the usage because\r\n    # there is a performance difference between get_next() and\r\n    # get_next_as_optional(). And we only enable get_next_as_optional when the\r\n    # output shapes are not static.\r\n    #\r\n    # TODO(rxsang): We want to always enable the get_next_as_optional behavior\r\n    # when user passed input_fn instead of dataset.\r\n    if getattr(\r\n        strategy.extended, \"experimental_enable_get_next_as_optional\", False):\r\n      self._enable_get_next_as_optional = (\r\n          not static_shape) or strategy.extended._in_multi_worker_mode()\r\n    else:\r\n      self._enable_get_next_as_optional = False\r\n\r\n    assert isinstance(input_workers, InputWorkers)\r\n    if not input_workers.worker_devices:\r\n      raise ValueError(\"Should have at least one worker for input iterator.\")\r\n\r\n    self._iterators = iterators\r\n    self._input_workers = input_workers\r\n    self._strategy = strategy\r\n\r\n  def next(self):\r\n    return self.__next__()\r\n\r\n  def __next__(self):\r\n    try:\r\n      return self.get_next()\r\n    except errors.OutOfRangeError:\r\n      raise StopIteration\r\n\r\n  def __iter__(self):\r\n    return self\r\n\r\n  def get_next_as_optional(self):\r\n    global_has_value, replicas = _get_next_as_optional(self, self._strategy)\r\n\r\n    def return_none():\r\n      return optional_ops.Optional.empty(self._element_spec)\r\n\r\n    def return_value(replicas):\r\n      \"\"\"Wraps the inputs for replicas in an `tf.experimental.Optional`.\"\"\"\r\n      results = []\r\n      for i, worker in enumerate(self._input_workers.worker_devices):\r\n        with ops.device(worker):\r\n          devices = self._input_workers.compute_devices_for_worker(i)\r\n          for j, device in enumerate(devices):\r\n            with ops.device(device):\r\n              result = replicas[i][j]\r\n              results.append(result)\r\n      replicas = results\r\n\r\n      return optional_ops.Optional.from_value(\r\n          distribute_utils.regroup(replicas))\r\n\r\n    return control_flow_ops.cond(global_has_value,\r\n                                 lambda: return_value(replicas),\r\n                                 lambda: return_none())  # pylint: disable=unnecessary-lambda\r\n\r\n  def get_next(self, name=None):\r\n    \"\"\"Returns the next input from the iterator for all replicas.\"\"\"\r\n    if not self._enable_get_next_as_optional:\r\n      replicas = []\r\n      for i, worker in enumerate(self._input_workers.worker_devices):\r\n        if name is not None:\r\n          d = tf_device.DeviceSpec.from_string(worker)\r\n          new_name = \"%s_%s_%d\" % (name, d.job, d.task)\r\n        else:\r\n          new_name = None\r\n        with ops.device(worker):\r\n          # Make `replicas` a flat list of values across all replicas.\r\n          replicas.extend(\r\n              self._iterators[i].get_next_as_list_static_shapes(new_name))\r\n      return distribute_utils.regroup(replicas)\r\n\r\n    out_of_range_replicas = []\r\n    def out_of_range_fn(worker_index, device):\r\n      \"\"\"This function will throw an OutOfRange error.\"\"\"\r\n      # As this will be only called when there is no data left, so calling\r\n      # get_next() will trigger an OutOfRange error.\r\n      data = self._iterators[worker_index].get_next(device)\r\n      out_of_range_replicas.append(data)\r\n      return data\r\n\r\n    global_has_value, replicas = _get_next_as_optional(self, self._strategy)\r\n    results = []\r\n    for i, worker in enumerate(self._input_workers.worker_devices):\r\n      with ops.device(worker):\r\n        devices = self._input_workers.compute_devices_for_worker(i)\r\n        for j, device in enumerate(devices):\r\n          with ops.device(device):\r\n            # pylint: disable=undefined-loop-variable\r\n            # pylint: disable=cell-var-from-loop\r\n            # It is fine for the lambda to capture variables from the loop as\r\n            # the lambda is executed in the loop as well.\r\n            result = control_flow_ops.cond(\r\n                global_has_value,\r\n                lambda: replicas[i][j],\r\n                lambda: out_of_range_fn(i, device),\r\n                strict=True,\r\n            )\r\n            # pylint: enable=cell-var-from-loop\r\n            # pylint: enable=undefined-loop-variable\r\n            results.append(result)\r\n    replicas = results\r\n\r\n    return distribute_utils.regroup(replicas)\r\n```\r\n\r\n\r\n\r\nFile \"C:\\user\\anaconda3\\envs\\object_detection_api\\lib\\site-packages\\tensorflow\\python\\distribute\\input_lib.py\", line 1474\r\n```\r\n#Lines 1459-1474:\r\ndef get_next_as_list_static_shapes(self, name=None):\r\n    \"\"\"Get next element from the underlying iterator.\r\n\r\n    Runs the iterator get_next() within a device scope. Since this doesn't use\r\n    get_next_as_optional(), is is considerably faster than get_next_as_list()\r\n    (but can only be used when the shapes are static).\r\n\r\n    Args:\r\n      name: not used.\r\n\r\n    Returns:\r\n      A list consisting of the next data from each device.\r\n    \"\"\"\r\n    del name\r\n    with ops.device(self._worker):\r\n      return self._iterator.get_next()\r\n```\r\n\r\n\r\nFile \"C:\\user\\anaconda3\\envs\\object_detection_api\\lib\\site-packages\\tensorflow\\python\\data\\ops\\multi_device_iterator_ops.py\", line 581:\r\n```\r\n#Lines 572-588:\r\ndef get_next(self, device=None):\r\n    \"\"\"Returns the next element given a `device`, else returns all in a list.\"\"\"\r\n    if device is not None:\r\n      index = self._devices.index(device)\r\n      return self._device_iterators[index].get_next()\r\n\r\n    result = []\r\n    for i, device in enumerate(self._devices):\r\n      with ops.device(device):\r\n        result.append(self._device_iterators[i].get_next())\r\n    return result\r\n\r\n  def __iter__(self):\r\n    return self\r\n\r\n  def __next__(self):\r\n    return self.next()\r\n```\r\n\r\n\r\nFile \"C:\\user\\anaconda3\\envs\\object_detection_api\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 764 and 825:\r\n```\r\n#Lines 750-834:\r\nwith context.execution_mode(context.SYNC):\r\n      with ops.device(self._device):\r\n        # TODO(ashankar): Consider removing this ops.device() context manager\r\n        # and instead mimic ops placement in graphs: Operations on resource\r\n        # handles execute on the same device as where the resource is placed.\r\n        ret = gen_dataset_ops.iterator_get_next(\r\n            self._iterator_resource,\r\n            output_types=self._flat_output_types,\r\n            output_shapes=self._flat_output_shapes)\r\n\r\n      try:\r\n        # Fast path for the case `self._structure` is not a nested structure.\r\n        return self._element_spec._from_compatible_tensor_list(ret)  # pylint: disable=protected-access\r\n      except AttributeError:\r\n        return structure.from_compatible_tensor_list(self._element_spec, ret)\r\n\r\n  @property\r\n  def _type_spec(self):\r\n    return IteratorSpec(self.element_spec)\r\n\r\n  def next(self):\r\n    try:\r\n      return self._next_internal()\r\n    except errors.OutOfRangeError:\r\n      raise StopIteration\r\n\r\n  @property\r\n  @deprecation.deprecated(\r\n      None, \"Use `tf.compat.v1.data.get_output_classes(iterator)`.\")\r\n  def output_classes(self):\r\n    \"\"\"Returns the class of each component of an element of this iterator.\r\n\r\n    The expected values are `tf.Tensor` and `tf.sparse.SparseTensor`.\r\n\r\n    Returns:\r\n      A nested structure of Python `type` objects corresponding to each\r\n      component of an element of this dataset.\r\n    \"\"\"\r\n    return nest.map_structure(\r\n        lambda component_spec: component_spec._to_legacy_output_classes(),  # pylint: disable=protected-access\r\n        self._element_spec)\r\n\r\n  @property\r\n  @deprecation.deprecated(\r\n      None, \"Use `tf.compat.v1.data.get_output_shapes(iterator)`.\")\r\n  def output_shapes(self):\r\n    \"\"\"Returns the shape of each component of an element of this iterator.\r\n\r\n    Returns:\r\n      A nested structure of `tf.TensorShape` objects corresponding to each\r\n      component of an element of this dataset.\r\n    \"\"\"\r\n    return nest.map_structure(\r\n        lambda component_spec: component_spec._to_legacy_output_shapes(),  # pylint: disable=protected-access\r\n        self._element_spec)\r\n\r\n  @property\r\n  @deprecation.deprecated(\r\n      None, \"Use `tf.compat.v1.data.get_output_types(iterator)`.\")\r\n  def output_types(self):\r\n    \"\"\"Returns the type of each component of an element of this iterator.\r\n\r\n    Returns:\r\n      A nested structure of `tf.DType` objects corresponding to each component\r\n      of an element of this dataset.\r\n    \"\"\"\r\n    return nest.map_structure(\r\n        lambda component_spec: component_spec._to_legacy_output_types(),  # pylint: disable=protected-access\r\n        self._element_spec)\r\n\r\n  @property\r\n  def element_spec(self):\r\n    return self._element_spec\r\n\r\n  def get_next(self):\r\n    return self._next_internal()\r\n\r\n  def get_next_as_optional(self):\r\n    # pylint: disable=protected-access\r\n    return optional_ops._OptionalImpl(\r\n        gen_dataset_ops.iterator_get_next_as_optional(\r\n            self._iterator_resource,\r\n            output_types=structure.get_flat_tensor_types(self.element_spec),\r\n            output_shapes=structure.get_flat_tensor_shapes(\r\n                self.element_spec)), self.element_spec)\r\n```\r\n\r\n\r\nFile \"C:\\user\\anaconda3\\envs\\object_detection_api\\lib\\contextlib.py\", line 131:\r\n```\r\n#Lines 97-162:\r\nclass _GeneratorContextManager(_GeneratorContextManagerBase,\r\n                               AbstractContextManager,\r\n                               ContextDecorator):\r\n    \"\"\"Helper for @contextmanager decorator.\"\"\"\r\n\r\n    def _recreate_cm(self):\r\n        # _GCM instances are one-shot context managers, so the\r\n        # CM must be recreated each time a decorated function is\r\n        # called\r\n        return self.__class__(self.func, self.args, self.kwds)\r\n\r\n    def __enter__(self):\r\n        # do not keep args and kwds alive unnecessarily\r\n        # they are only needed for recreation, which is not possible anymore\r\n        del self.args, self.kwds, self.func\r\n        try:\r\n            return next(self.gen)\r\n        except StopIteration:\r\n            raise RuntimeError(\"generator didn't yield\") from None\r\n\r\n    def __exit__(self, type, value, traceback):\r\n        if type is None:\r\n            try:\r\n                next(self.gen)\r\n            except StopIteration:\r\n                return False\r\n            else:\r\n                raise RuntimeError(\"generator didn't stop\")\r\n        else:\r\n            if value is None:\r\n                # Need to force instantiation so we can reliably\r\n                # tell if we get the same exception back\r\n                value = type()\r\n            try:\r\n                self.gen.throw(type, value, traceback)\r\n            except StopIteration as exc:\r\n                # Suppress StopIteration *unless* it's the same exception that\r\n                # was passed to throw().  This prevents a StopIteration\r\n                # raised inside the \"with\" statement from being suppressed.\r\n                return exc is not value\r\n            except RuntimeError as exc:\r\n                # Don't re-raise the passed in exception. (issue27122)\r\n                if exc is value:\r\n                    return False\r\n                # Likewise, avoid suppressing if a StopIteration exception\r\n                # was passed to throw() and later wrapped into a RuntimeError\r\n                # (see PEP 479).\r\n                if type is StopIteration and exc.__cause__ is value:\r\n                    return False\r\n                raise\r\n            except:\r\n                # only re-raise if it's *not* the exception that was\r\n                # passed to throw(), because __exit__() must not raise\r\n                # an exception unless __exit__() itself failed.  But throw()\r\n                # has to raise the exception to signal propagation, so this\r\n                # fixes the impedance mismatch between the throw() protocol\r\n                # and the __exit__() protocol.\r\n                #\r\n                # This cannot use 'except BaseException as exc' (as in the\r\n                # async implementation) to maintain compatibility with\r\n                # Python 2, where old-style class exceptions are not caught\r\n                # by 'except BaseException'.\r\n                if sys.exc_info()[1] is value:\r\n                    return False\r\n                raise\r\n            raise RuntimeError(\"generator didn't stop after throw()\")\r\n```\r\n\r\n\r\n\r\nFile \"C:\\user\\anaconda3\\envs\\object_detection_api\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 2105:\r\n```\r\n#Lines 2001-2013:\r\ndef graph_mode():\r\n  \"\"\"Context-manager to disable eager execution for the current thread.\"\"\"\r\n  return context()._mode(GRAPH_MODE)  # pylint: disable=protected-access\r\n\r\n\r\ndef eager_mode():\r\n  \"\"\"Context-manager to enable eager execution for the current thread.\"\"\"\r\n  return context()._mode(EAGER_MODE)  # pylint: disable=protected-access\r\n\r\n\r\ndef scope_name():\r\n  \"\"\"Name of the current scope.\"\"\"\r\n  return context().scope_name\r\n```\r\n\r\n\r\nFile \"C:\\user\\anaconda3\\envs\\object_detection_api\\lib\\site-packages\\tensorflow\\python\\eager\\executor.py\", line 67:\r\n```\r\n#Lines 24-76:\r\nclass Executor(object):\r\n  \"\"\"A class for handling eager execution.\r\n\r\n  The default behavior for asynchronous execution is to serialize all ops on\r\n  a single thread. Having different `Executor` objects in different threads\r\n  enables executing ops asynchronously in parallel:\r\n\r\n  ```python\r\n  def thread_function():\r\n    executor = executor.Executor(enable_async=True):\r\n    context.set_executor(executor)\r\n\r\n  a = threading.Thread(target=thread_function)\r\n  a.start()\r\n  b = threading.Thread(target=thread_function)\r\n  b.start()\r\n  \r\n  \"\"\"\r\n\r\n  def __init__(self, handle):\r\n    self._handle = handle\r\n\r\n  def __del__(self):\r\n    try:\r\n      # pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\r\n      pywrap_tfe.TFE_DeleteExecutor(self._handle)\r\n    except TypeError:\r\n      # Suppress some exceptions, mainly for the case when we're running on\r\n      # module deletion. Things that can go wrong include the pywrap module\r\n      # already being unloaded, self._handle. no longer being\r\n      # valid, and so on. Printing warnings in these cases is silly\r\n      # (exceptions raised from __del__ are printed as warnings to stderr).\r\n      pass  # 'NoneType' object is not callable when the handle has been\r\n      # partially unloaded.\r\n\r\n  def is_async(self):\r\n    return pywrap_tfe.TFE_ExecutorIsAsync(self._handle)\r\n\r\n  def handle(self):\r\n    return self._handle\r\n\r\n  def wait(self):\r\n    \"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\r\n    pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\r\n\r\n  def clear_error(self):\r\n    \"\"\"Clears errors raised in this executor during execution.\"\"\"\r\n    pywrap_tfe.TFE_ExecutorClearError(self._handle)\r\n\r\n\r\ndef new_executor(enable_async):\r\n  handle = pywrap_tfe.TFE_NewExecutor(enable_async)\r\n  return Executor(handle)\r\n```\r\n", "comments": ["@ib124,\r\nThe code provided is fairly complex hence it would be difficult for us to pinpoint the issue. Could you please get the example down to the simplest possible repro? That will allow us to determine the source of the issue easily.\r\n\r\nAlso, please go through issue [#23698](https://github.com/tensorflow/tensorflow/issues/23698) with a similar error and let us know if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44857, "title": "RuntimeError: The layer has never been called and thus has no defined output shape.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Used Google Colab\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: Doesn't matter\r\n- GPU model and memory: Doesn't matter\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI used to run below mentioned code in the tensorflow version 1.15 but since the time I have upgraded it fails to run.\r\nI have already gone through similar bugs, but they all mention that I should have provided the input shape, I have done that, so I am not sure where the problem lies.\r\n```\r\nmodel = DenseNet121(input_shape=(810, 1440, 3),\r\n                                         include_top=False,\r\n                                         weights='imagenet'\r\n                                         )\r\n\r\ndepth = model.get_output_shape_at(0)[-1]\r\n```\r\n\r\n**Describe the expected behavior**\r\nExpected behaviour should be that I should be able to get the output shape of the last layer.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nEveryone can recreate the issue using the below colab link.\r\nhttps://colab.research.google.com/drive/1TPYkETG8szsCvut6SdzY345jEnqOrgsg?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-4-c67805c58fe0> in <module>()\r\n----> 1 depth = model.get_output_shape_at(0)[-1]\r\n\r\n1 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in get_output_shape_at(self, node_index)\r\n   2028     \"\"\"\r\n   2029     return self._get_node_attribute_at_index(node_index, 'output_shapes',\r\n-> 2030                                              'output shape')\r\n   2031 \r\n   2032   @doc_controls.do_not_doc_inheritable\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in _get_node_attribute_at_index(self, node_index, attr, attr_name)\r\n   2601     if not self._inbound_nodes:\r\n   2602       raise RuntimeError('The layer has never been called '\r\n-> 2603                          'and thus has no defined ' + attr_name + '.')\r\n   2604     if not len(self._inbound_nodes) > node_index:\r\n   2605       raise ValueError('Asked to get ' + attr_name + ' at node ' +\r\n\r\nRuntimeError: The layer has never been called and thus has no defined output shape.\r\n```\r\n", "comments": ["Can you unlock the Colab?", "Yes, I just unlocked it for everyone - https://colab.research.google.com/drive/1TPYkETG8szsCvut6SdzY345jEnqOrgsg?usp=sharing", "You need to call the model with an input:\r\nE.g. \r\n```\r\nmodel(tf.keras.Input((810, 1440, 3)))\r\ndepth = model.get_output_shape_at(0)\r\n```", "But, if you have observed, when I am initializing the densenet model, I am providing the input shape. \n\nThen why do I need to provide it two times? ", "The layer need to be called. See the difference between `call` and `build` https://keras.io/api/layers/base_layer/", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44857\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44857\">No</a>\n"]}, {"number": 44856, "title": "No Operation named [input] in the Graph running in React-Native", "body": "**System information**\r\n- Environment:\r\n  OS: macOS High Sierra 10.13.6\r\n  Node: 14.12.0\r\n  Yarn: 1.22.5\r\n  npm: 6.14.8\r\n  Watchman: 4.9.0\r\n  Xcode: Not Found\r\n  Android Studio: 4.0 AI-193.6911.18.40.6626763\r\n\r\n- Packages: (wanted => installed)\r\n  react: 16.0.0 => 16.0.0\r\n  react-native: 0.51.0 => 0.51.0\r\n\r\n- TensorFlow installed from (source or binary): danjarvis/tensorflow-android:1.0.0\r\n- TensorFlow version (or github SHA if from source): danjarvis/tensorflow-android:1.0.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\n# Copy and paste here the exact command\r\n```\r\npython /tensorflow/tensorflow/examples/image_retraining/retrain.py \\\r\n--bottleneck_dir=/tf_files/bottlenecks \\\r\n--how_many_training_steps 500 \\\r\n--model_dir=/tf_files_inception  \\\r\n--output_graph=/tf_files/star_wars_graph.pb \\\r\n--output_labels=/tf_files/star_wars_labels.txt \\\r\n--image_dir /tf_files/star_wars\r\n\r\nThis command will generate the .pb and txt using inception-v3, but and the tutorial said i have to do the following to \"convert\" to inception-v1.\r\n\r\npython /tensorflow/tensorflow/python/tools/optimize_for_inference.py \\\r\n--input=/tf_files/star_wars_graph.pb \\\r\n--output=/tf_files/star_wars_graph_optimized.pb \\\r\n--frozen_graph=True \\\r\n--input_names=Mul \\\r\n--output_names=final_result\r\n\r\nand then\r\n\r\npython /tensorflow/tensorflow/tools/quantization/quantize_graph.py \\\r\n--input=/tf_files/star_wars_graph_optimized.pb \\\r\n--output=/tf_files/star_wars_rounded_graph.pb \\\r\n--output_node_names=final_result \\\r\n--mode=weights_rounded\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n# Copy and paste the output here.\r\n```\r\nNo out put is showen\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\nThe link is this, but is in portuguese [http://www.davifelipe.com.br/tensorflow-gerando-model-para-dispositivo-movel](url)\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nEverythings runs fine, but an alert saying the error is showed\r\n![Captura de Tela 2020-11-13 a\u0300s 16 27 59](https://user-images.githubusercontent.com/2678092/99112982-4620a500-25cd-11eb-908f-f3641c97fa08.png)\r\n\r\n", "comments": ["@GustavoMSevero \r\nCould you please let us know if this is still an issue in latest stable TF v2.6.0 ?Thank you!", "Sorry @Saduf2019, have been a long time I don't use it and don't remember what project is this kkkkkk But I remember I have this issue.\r\nThank you for your attention.", "@GustavoMSevero \r\nIn that case would you want to move this to closed status, and open an issue when your face the issue.", "Sorry @Saduf2019 ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44856\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44856\">No</a>\n"]}, {"number": 44855, "title": "[CherryPick:r2.4]Suppress warnings when listing functions for serialization.", "body": "PiperOrigin-RevId: 342271395\nChange-Id: Ieedeeb7817f794c41dae3eb06b9b0d3b607fa89a", "comments": []}, {"number": 44854, "title": "Keras custom data generator giving dimension errors with multi input and multi output( functional api model)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 20.04 and Colab):\r\n- TensorFlow installed from; Pip3 install ..\r\n- Tensorflow version: v2.3.0-54-gfcc4b966f1 2.3.1\r\n\r\nColab Code to reproduce: https://colab.research.google.com/drive/1bSJm44MMDCWDU8IrG2GXKBvXNHCuY70G?usp=sharing\r\n\r\nNote: I have updated the issue at the end as well.\r\n\r\n\r\nI have written a generator function with Keras, before returning X,y from `__getitem__` I have double check the shapes of the X's and Y's and they are alright, but generator is giving dimension mismatch array and warnings.\r\n\r\nI suspect that the problem is relating multiple input, which is being cosidered one by the input layer of tf.keras. Each input features is of shape (32,10,1) but yielding `[input_array_1,input_array_2,input_array_3]` makes it (3,32,10,1)\r\n\r\nMy training and validation generators are pretty musch same as\r\n\r\n    class ValidGenerator(Sequence):\r\n        def __init__(self, df, batch_size=64):\r\n            self.batch_size = batch_size\r\n            self.df = df\r\n            self.indices = self.df.index.tolist()\r\n            self.num_classes = num_classes\r\n            self.shuffle = shuffle\r\n            self.on_epoch_end()\r\n    \r\n        def __len__(self):\r\n            return int(len(self.indices) // self.batch_size)\r\n    \r\n        def __getitem__(self, index):\r\n            index = self.index[index * self.batch_size:(index + 1) * self.batch_size]\r\n            batch = [self.indices[k] for k in index]\r\n            \r\n            X, y = self.__get_data(batch)\r\n            return X, y\r\n    \r\n        def on_epoch_end(self):\r\n            self.index = np.arange(len(self.indices))\r\n            if self.shuffle == True:\r\n                np.random.shuffle(self.index)\r\n    \r\n        def __get_data(self, batch):\r\n            #some logic is written here\r\n            #hat prepares 3 X features and 3 Y outputs \r\n            X = [input_array_1,input_array_2,input_array_3]\r\n            y = [out_1,out_2,out_3]\r\n            #print(len(X))\r\n            \r\n            return X, y\r\n\r\nI am returning tuple of X,y from which has 3 input features and 3 output features each, so shape of X is `(3,32,10,1)`\r\n\r\nI am using functional api to build model(I have things like concatenation, multi input/output, which isnt possible with sequential)  with following structure\r\n\r\n[![enter image description here][1]][1]\r\n\r\nWhen I try to fit the model with generator with following code\r\n\r\n    train_datagen = TrainGenerator(df=train_df,  batch_size=32, num_classes=None, shuffle=True)\r\n    valid_datagen = ValidGenerator(df=train_df,  batch_size=32, num_classes=None, shuffle=True)\r\n    model.fit(train_datagen, epochs=2,verbose=1,callbacks=[checkpoint,es])\r\n\r\nI get these warnings and errors, that dont go away\r\n\r\n>Epoch 1/2\r\n>WARNING:tensorflow:Model was constructed with shape (None, 10) for input >Tensor(\"input_1:0\", shape=(None, 10), dtype=float32), but it was called >on an input with incompatible shape (None, None, None).\r\n\r\n>WARNING:tensorflow:Model was constructed with shape (None, 10) for input \r\n> Tensor(\"input_2:0\", shape=(None, 10), dtype=float32), but it was\r\n> called on an input with incompatible shape (None, None, None).\r\n> WARNING:tensorflow:Model was constructed with shape (None, 10) for\r\n> input Tensor(\"input_3:0\", shape=(None, 10), dtype=float32), but it was\r\n> called on an input with incompatible shape (None, None, None).\r\n> ...\r\n>...\r\n> __call__\r\n>         return super(RNN, self).__call__(inputs, **kwargs)\r\n>     /home/eduardo/.virtualenvs/kgpu3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:975\r\n> __call__\r\n>         input_spec.assert_input_compatibility(self.input_spec, inputs,\r\n>     /home/eduardo/.virtualenvs/kgpu3/lib/python3.8/site-packages/tensorflow/python/keras/engine/input_spec.py:176\r\n> assert_input_compatibility\r\n>         raise ValueError('Input ' + str(input_index) + ' of layer ' +\r\n> \r\n>     ValueError: Input 0 of layer lstm is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [None, None, None, 88]\r\n\r\nI have rechecked whole code and it isnt possible to have input (None,None,None) like in warning or in error, my input dimension is `(3,32,10,1)`\r\n  \r\n**Update**\r\n\r\nI have also tried to write a generator function with python and got exactly same error. \r\n\r\nMy generator function\r\n\r\n\r\n    def generate_arrays_from_file(batchsize,df):\r\n        #print(bat)\r\n        inputs = []\r\n        targets = []\r\n        batchcount = 0\r\n        while True:\r\n                \r\n                df3 = df.loc[np.arange(batchcount*batchsize,(batchcount*batchsize)+batchsize)]\r\n                #Some pre processing\r\n                X = [input_array_1,input_array_2,input_array_3]\r\n                y = [out_1,out_2,out_3]\r\n                yield X,y \r\n                batchcount = batchcount +1\r\n\r\nIt seems like it is something wrong internally wit keras (may be due to the fact I am using functional API)\r\n\r\n**Update 2**\r\n\r\nI also tried to output tuple \r\n\r\n              X = (input1_X,input2_X,input3_X)\r\n              y = (output1_y,output2_y,output3_y)\r\n\r\nand also named input/output, but it doesnt work\r\n\r\n            X =  {\"input_1\": input1_X, \"input_2\": input2_X,\"input_3\": input3_X}\r\n            y = {\"output_1\": output1_y, \"output_2\": output2_y,\"output_3\": output3_y}\r\n\r\n**Note about problem formulation:**\r\n\r\nChanging the individual X features to shape (32,10) instead of (32,10,1) might help to get rid of this error but that is not what I want, it changes my problem(I no longer have 10 time steps with one feature each)\r\n\r\n  [1]: https://i.stack.imgur.com/EaShF.png\r\n\r\n", "comments": ["Can you reorganize your code s that we could just copy, paste and run (or share a Colab)? Thanks", "@bhack thankyou for the response, I have updated the answer with the link and other things that i tried to resolve.", "Thanks, but this is a support request not a bug on your specific use case so you need to use https://stackoverflow.com/questions/tagged/tensorflow", "@bhack  Thankyou for the response.\r\n\r\nI have already posted it on stack overflow and also, I am pretty sure it is bug inside fitting function relating to 1) generator as an input 2) functional API", "> @bhack  Thankyou for the response.\n> \n> I have already posted it on stack overflow and also, I am pretty sure it is bug inside fitting function relating to 1) generator as an input 2) functional API\n\nDo you mean a Tensorflow bug?", "@bhack  sorry if my understanding i write, Keras is part of tensorflow right? I mean how `model.fit` handles generator is buggy in case of multiple input. So, I am not sure if you consider it tensorflow bug or keras?\r\n", "If you still think that It is a TF/Keras bug I think you can reproduce it with less than 10 lines of code.\n\nCan you minimize your code surface to reproduce the bug?", "@bhack There are two examples for the bug (each with approximately 10-15 lines), which I am afraid are required as the problem relates to multi input/ multi-output generator.\r\n\r\nthe two examples are of \r\n\r\n- Keras generator\r\n- Custom generator\r\n\r\nAnd model.fit suffers same problem for the both.\r\n\r\nI have [posted it as a question on stackoverflow](https://stackoverflow.com/questions/64811632/keras-custom-data-generator-giving-dimension-errors-with-multi-input-and-multi-o) and it seems that there isnt any solution or problem isnt relating my code or how i write the generator", "I still think it is not a bug but a stackoverflow support question.\r\nYou need to use `reshape(dlen,pad_len)` and `reshape(dlen,pad_len)`", "@bhack I think there is a typo in last part of the comment,you wrote `reshape(dlen,pad_len)` two time.\r\n\r\nAnd `reshape(dlen,pad_len)` is not same as `reshape(dlen,pad_len,1)` and it is totally different modelling situation in case of NLP and LSTMS.\r\n\r\nI dont know if you have interest in sequential modelling, but I will try to explain. If you reshape the data of format (batch,timestep,features), which in my case is (32,10,1) , to (batch,timestep), then you loose the purpose of sequential modelling, because you basically are having 10 features and one time step now, that makes it equal to using feed forward network and will lose purpose of using LSTMs. \r\n\r\nSo, the way generator expects the shape of input, it is not possible to have multiple inputs and input of shape (batch,timestamp,no_features) simultaneously.\r\n\r\nI have added the reference to many-to-one sequence prediction problem below, that can help to understand why `(dlen,pad_len)` is not same as `(dlen,pad_len,1)`\r\n\r\n1.     https://stackoverflow.com/questions/43034960/many-to-one-and-many-to-many-lstm-examples-in-keras\r\n2.     https://stackabuse.com/solving-sequence-problems-with-lstm-in-keras/", "Yes it was a typo the second was `(dlen)`.\n\nThe problem is that for generators in your original code sample you had:\n\n> ValueError: Input 0 of layer lstm is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [None, None, None, 88]\n\nSo It Is ndim=4 and not (batch,timestep,features)", "It is 4 dimensional because I am yielding 3 inputs (`X = [input_array_1,input_array_2,input_array_3]`), which takes `(batch,timestep,features)` to dimension `(3,batch,timestep,features)`.\r\n\r\n`fit()` method doesnt recognises the fact that we are returning 3 input features not one. i,e, it doesn't understands that it is `[(batch,time-step,features),(batch,time-step,features),(batch,timestep,features)]`, it takes it as single input of `(3,batch,time-step,features)`\r\n\r\nI am not sure if I have conveyed the point, let me know if it not, I will explain more :)", "Ok just to investigate more, can you try to reformulate a very small snippet with [`tf.data.Dataset.from_generator`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=nightly#from_generator) with your data (cause internally it is what is used) without the model definition or the other code. \r\n\r\nJust to check what kind of dataset we have in your case.\r\n\r\n\r\n", "@bhack  I have created a generator with as minimum code as possible with `tf.data.Dataset.from_generator`\r\n\r\nPlease the colab working link below, let me know if it is what you asked or i get it worng\r\nhttps://colab.research.google.com/drive/1fwYqGGdhISGlmJGlkZAeVW_qP8mS1t0V?usp=sharing\r\n", "I suppose that this Dataset is working with your model/fit right? Did you try?\r\n```\r\nfor element in ds_counter:\r\n  print(element[0][\"input_1\"].shape)\r\n  print(element[0][\"input_2\"].shape)\r\n  print(element[0][\"input_3\"].shape)\r\n  print(element[1][\"output_1\"].shape)\r\n  print(element[1][\"output_2\"].shape)\r\n  print(element[1][\"output_3\"].shape)\r\n  break\r\n```\r\n\r\n```\r\n(32, 10, 1)\r\n(32, 10, 1)\r\n(32, 10, 1)\r\n(32, 71)\r\n(32, 487)\r\n(32, 85)\r\n```\r\n", "@bhack, can i pass `ds_counter` directly to the fit method like generator(sorry I am not very much aware of this dt.data.dataset thing) as below?\r\n\r\n       model.fit(ds_counter,epochs=2,verbose=1)\r\n\r\nit gives error \r\n\r\nTypeError: 'NoneType' object is not callable\r\n\r\n\r\nOr shoud i do something like\r\n\r\n```\r\nfor element in ds_counter:\r\n  print(element[0][\"input_1\"].shape)\r\n  print(element[0][\"input_2\"].shape)\r\n  print(element[0][\"input_3\"].shape)\r\n  print(element[1][\"output_1\"].shape)\r\n  print(element[1][\"output_2\"].shape)\r\n  print(element[1][\"output_3\"].shape)\r\n  model.fit([element[0][\"input_1\"],element[0][\"input_2\"],element[0][\"input_3\"]],[element[1][\"output_1\"],element[1][\"output_2\"],element[1][\"output_3\"]],epochs=2,verbose=1)\r\n```\r\n\r\n\r\nI get `TypeError: 'NoneType' object is not callable` with this as well", "sorry ignore my previous comment, thankyou for refering to this approach.\r\nthe nonetype error wasnt related with this scenerio.\r\n\r\nShould i run it like this?\r\n\r\n```\r\ntr_dataset = ds_counter.batch(batch_size=32)\r\ntr_dataset = tr_dataset.repeat()\r\nmodel.fit(tr_dataset,epochs=2,verbose=1)\r\n```\r\n\r\n\r\nIf it works with this approach, does it means that it cant be done with usual generator?\r\n", "Yes `fit` can consume dataset as `x`\r\n> A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights).\r\n\r\nLet me know if it works so that we could check how internally is called `tf.data.Dataset.from_generator` in your old Generator/fit case.\r\n", "Thankyou for looking into this\r\n\r\nDoing `model.fit(ds_counter,epochs=2,verbose=1)` gives this error\r\n\r\n> InvalidArgumentError: 2 root error(s) found.\r\n>   (0) Invalid argument:  transpose expects a vector of size 4. But input(1) is a vector of size 3\r\n> \t [[node functional_3/lstm_3/transpose_1 (defined at <ipython-input-38-9f6d4f0db947>:1) ]]\r\n> \t [[gradient_tape/functional_3/embedding_4/embedding_lookup/Reshape/_202]]\r\n\r\n> (1) Invalid argument:  transpose expects a vector of size 4. But input(1) is a vector of size 3\r\n> \t [[node functional_3/lstm_3/transpose_1 (defined at <ipython-input-38-9f6d4f0db947>:1) ]]\r\n> 0 successful operations.\r\n> 0 derived errors ignored. [Op:__inference_train_function_25790]\r\n> \r\n> Function call stack:\r\n> train_function -> train_function", "Can you share this new example?", "I have updated the same colab \r\nhttps://colab.research.google.com/drive/1fwYqGGdhISGlmJGlkZAeVW_qP8mS1t0V#scrollTo=pUzbIleKYhDW\r\n\r\nBut it seems that the I have this error\r\n\r\n>     ValueError: as_list() is not defined on an unknown TensorShape.\r\n\r\nI previously used `ds_counter.batch(batch_size=32)` and `repeat()` etc which might have cuased the last error\r\n", "That one is a known bug https://github.com/tensorflow/tensorflow/issues/32912\r\n\r\nYou could enforce the shape:\r\n```\r\noutput_shapes=({\"input_1\": [None,10,1], \"input_2\": [None,10,1],\"input_3\": [None,10,1]}, {\"output_1\": [None,71], \"output_2\": [None,487],\"output_3\": [None,85]}\r\n```", "As an alternative you could also use https://www.tensorflow.org/tutorials/load_data/pandas_dataframe#alternative_to_feature_columns", "Thankyou for sharing this, it did something as one epoch ran almost midway before crashing. \r\n\r\nit produces following error\r\n\r\n> KeyError: \"Passing list-likes to .loc or [] with any missing labels is no longer supported. The following labels were missing: Int64Index([1000, 1001, 1002, 1003, 1004,\\n            ...\\n            1019, 1020, 1021, 1022, 1023],\\n           dtype='int64', length=24). See https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\"\r\n\r\n", "Edit: i think this error isnt related to the problem here, let me check again \r\n\r\nI added `output_shapes=({\"input_1\": [None,10,1], \"input_2\": [None,10,1],\"input_3\": [None,10,1]}, {\"output_1\": [None,71], \"output_2\": [None,487],\"output_3\": [None,85]}` to `from_generator()`", "This is a different issue check if you have missing labels", "Thankyou. Yes, I was doing a mistake, I was already hanling batch in the function and was trying to use batch() fro dataset as well.\r\n\r\nThis example works now with `tf.data.dataset` and `from_generator`.\r\nhttps://colab.research.google.com/drive/1fwYqGGdhISGlmJGlkZAeVW_qP8mS1t0V#scrollTo=WoDme6jVZQtz\r\n\r\nMay be we can figure out why keras generator with get_item wasnt working looking at this dataset implementation", "Probably you need to limit/reset batchcount", "in keras generator (extends from Sequence) one? or this one? this one is working fine now ", "I meant this one but if you have the batch under control ok..\r\n\r\n> May be we can figure out why keras generator with get_item wasnt working looking at this dataset implementation\r\n\r\nWhat I can see is that with your old Sequence/Generator/fit code it was going to internally create the Dataset object with this parameters:\r\n\r\n```\r\ndataset = tf.data.Dataset.from_generator(\r\n    gen,\r\n    output_types=((tf.int64, tf.int64, tf.int64), (tf.float32, tf.float32,\r\n                                                   tf.float32)),\r\n    output_shapes=((tf.TensorShape([None, None,\r\n                                    None]), tf.TensorShape([None, None, None]),\r\n                    tf.TensorShape([None, None,\r\n                                    None])), (tf.TensorShape([None, None]),\r\n                                              tf.TensorShape([None, None]),\r\n                                              tf.TensorShape([None, None]))))\r\n```", "As you can see the flow to `DatasetV2.from_generator` in:\r\nhttps://github.com/tensorflow/tensorflow/blob/a52c2d085d0ed2fa3f70daf99482fa018cbc0660/tensorflow/python/keras/engine/data_adapter.py#L798-L811", "I think i found a similar issue, will look into your pointers and this issue tommorow\r\nhttps://github.com/tensorflow/tensorflow/issues/20698", "/cc @omalleyt12 What do you think?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44854\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44854\">No</a>\n"]}, {"number": 44853, "title": "[CherryPick:r2.4]Fix single pip package renaming bug", "body": "There is a bug in the single pip package renaming code: we do a replacement over the entire contents of `METADATA` and this causes the `tensorflow_estimator` dependency to be replaced with `tensorflow_gpu_estimator` (on 1.15 it was `tensorflow_cpu_estimator`). These packages don't exist by themseleves, Estimator has no CPU/GPU split. Previously this required a manual alteration of the Estimator package to fake it being the CPU/GPU version and a manual upload for that, but we should move away from this manual step as it always causes issues with new releases.\n\nSee for example #44775 (there are a few more similar issues, both internally and externally, but this is the most recent one).\n\nWe should build each pip package instead of doing the renaming. We do that on Linux/Mac already but Windows builds take too long so rather than rebuilding we just fake the new package via this renaming function. Future work in this area is needed to get rid of the renaming function, eventually removing it completely from both TF and TF ecosystem packages.\n\nPiperOrigin-RevId: 341915841\nChange-Id: I2bd4c3621e581ccf31e7bdd52958937b93971b90", "comments": []}, {"number": 44852, "title": "`GetIntPtr` in `tensorflow/lite/tools/verifier.cc` do not get correct value on Big Endian machine", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nThe [GetIntPtr](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/verifier.cc#L53) in `tensorflow/lite/tools/verifier.cc` is retrieving the `int` value from a typed `const char*` buffer. However, since flatbuffers always stores the data in Little Endian format regardless of host machine endianness, the direct `reinterpret_cast<const uint32_t*>` call will retrieve the wrong value on Big Endian platforms (as `reinterpret_cast` will assume the data is in host endianness), so the retrieved value needs to be byte-swapped.\r\n\r\n**Describe the expected behavior**\r\n`GetIntPtr` function should byteswap the value on Big Endian platform.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nbazel test --host_javabase=\"@local_jdk//:jdk\" --cache_test_results=no --build_tests_only --test_output=errors -- //tensorflow/lite/tools:verifier_test\r\n```\r\nThis test will fail due to this reason on Big Endian systems.\r\n\r\n**Other info / logs** \r\nUsually, to solve this issue we could simply add a guard for `__ORDER_BIG_ENDIAN__` and byteswap accordingly. The issue here is that the input type is `const char*` and the output type is `const uint32_t*`. Apparently, we do not want to make the change in-place, so at least we need to get a copy of the data then byteswap it and return it. Also, since we need to return a pointer, we have to allocate a new variable then return the address of it. The change could look like:\r\n```diff\r\ndiff --git a/tensorflow/lite/tools/verifier.cc b/tensorflow/lite/tools/verifier.cc\r\nindex 1d0b813a2c..063246dcf2 100644\r\n--- a/tensorflow/lite/tools/verifier.cc\r\n+++ b/tensorflow/lite/tools/verifier.cc\r\n@@ -40,7 +40,13 @@ void ReportError(ErrorReporter* error_reporter, const char* format, ...) {\r\n \r\n // Returns the int32_t value pointed by ptr.\r\n const uint32_t* GetIntPtr(const char* ptr) {\r\n+#if __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__\r\n+  uint32_t ret_val = flatbuffers::EndianScalar(*reinterpret_cast<const uint32_t*>(ptr));\r\n+  const uint32_t ret = &ret_val;\r\n+  return ret;\r\n+#else\r\n   return reinterpret_cast<const uint32_t*>(ptr);\r\n+#endif\r\n }\r\n \r\n // Verifies flatbuffer format of the model contents and returns the in-memory\r\n```\r\n\r\nThe problem with this approach is the local variable `ret_val` will not live long enough for its value to be retrieved - so what we can get from `*GetIntPtr(buffer_ptr)` is basically a deallocated memory space. Since it is unavoidable that we need to allocate new memory space, we have to make it outlive the function call. I also tried `static uint32_t ret_val`, and it worked for the first function call but fail for the following ones. Seems like the `const uint32_t*` return type makes the compiler believe `static uint32_t ret_val` is const after the first call.\r\n\r\nDue to these reasons, seems like we could not make a simple modification to the function `GetIntPtr()` to fix the issue; it seems that we have to take care of the issue every time the function is called (by either create a new static variable for each function call or simply deal with the value manually after the function returns) if we keep the current signature of the function.\r\n\r\nHowever, I do have another solution here, which is to change the return type from `const uint32_t*` to `const uint32_t`. This change is applicable because this function is only used within `verifier.cc`, and every time it is called we simply need the value of it (i.e. we only need `*GetIntPtr(buffer_ptr)`). So it actually does not incur any issue if we make the change. The endianness issue will also be easily solved by something like this (plus change from `*GetIntPtr` to `GetIntPtr` wherever it is called):\r\n```diff\r\ndiff --git a/tensorflow/lite/tools/verifier.cc b/tensorflow/lite/tools/verifier.cc\r\nindex 1d0b813a2c..bb71f1228e 100644\r\n--- a/tensorflow/lite/tools/verifier.cc\r\n+++ b/tensorflow/lite/tools/verifier.cc\r\n@@ -39,8 +39,12 @@ void ReportError(ErrorReporter* error_reporter, const char* format, ...) {\r\n }\r\n \r\n // Returns the int32_t value pointed by ptr.\r\n-const uint32_t* GetIntPtr(const char* ptr) {\r\n-  return reinterpret_cast<const uint32_t*>(ptr);\r\n+const uint32_t GetIntPtr(const char* ptr) {\r\n+#if __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__\r\n+  return flatbuffers::EndianScalar(*reinterpret_cast<const uint32_t*>(ptr));\r\n+#else\r\n+  return *reinterpret_cast<const uint32_t*>(ptr);\r\n+#endif\r\n }\r\n \r\n // Verifies flatbuffer format of the model contents and returns the in-memory\r\n```\r\n\r\nI could submit a PR for such a change if it is considered valid, and I will appreciate it if there are any other simple solutions or suggestions.", "comments": ["I think that a PR is ok.", "Hi Sidong,\r\n\r\nI think the second approach makes sense. A PR will be appreciated. \r\n\r\nThanks,\r\nTiezhen", "Thanks for the feedback, I will initiate a PR shortly after.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "PR has been initiated: https://github.com/tensorflow/tensorflow/pull/45010", "Close the issue as PR has been merged.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44852\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44852\">No</a>\n"]}, {"number": 44851, "title": "TensorFlow and TensorFlow Lite to TOSA Legalizations", "body": "This constitutes the current base of stable and tested legalizations from TensorFlow and TensorFlow Lite MLIR dialects to the TOSA dialect. Includes MLIR tests and a document describing the functional operation of the legalizations . \r\n\r\nCurrently does not have conditional build rules for integration - this was only tested locally to a limited extent and was not pushed. We would like guidance on how to ensure we can smoothly build this with the rest of TensorFlow MLIR infrastructure. ", "comments": ["FYI - I have pre-created the directory for you and included a boiler-plate BUILD file. You will need to rebase against master and resolve the small conflict this creates. (this was necessary to opt the project out of the CIs, which we want to do until stable)", "> FYI - I have pre-created the directory for you and included a boiler-plate BUILD file. You will need to rebase against master and resolve the small conflict this creates. (this was necessary to opt the project out of the CIs, which we want to do until stable)\r\n\r\nYes I am working on this. I've rebased and have mostly resolved the conflict. I may need to also incorporate the conditional build flag requested in another piece of feedback here. ", "> Thanks for the contribution! This is very exciting to see the connection coming there :)\r\n> \r\n> Skimming quickly through it this looks great!\r\n> \r\n> There are many pieces that are independent here and that would be easier to integrate/review in a sequence of PRs.\r\n> For example, an easy first split is to leave off the TFL path for now and add it later. A gradual integration would add the basic structure, and follow a \"one PR per pass\" path roughly, keeping the PR size to ~400-600 lines of source code (not counting BUILD files and similar boilerplate). Let me know if you'd like to discuss the plan in more details! We can also GVC if it is more convenient for you.\r\n\r\nI personally find this master PR quite useful to see the whole scope, but I do agree that reviews are going to be unwieldy on this size (GitHub's UI makes this especially hard). I would also like to find a way that reduces your overhead in getting these things landed.\r\n\r\nHow about this as a process, biased purely towards having digestible review chunks (I'm open to any arrangement and the following is just how I would do it with git):\r\n\r\n1. We take a pass through and comment on this master PR for structural nits (file names, etc) that impact how the patch is put together.\r\n2. You keep this master PR and create child branches for the chunks, either one at a time or pre-factored into all chunks. I'm fine/would prefer a pass or logical chunk/file at a time, even if quite large. And I specifically know that I will be spending quite some time reviewing the TFL pass and would value that more. (The others are more formulaic)\r\n3. When we land a chunk, you rebase the branch and push the update for this PR until we've landed everything.\r\n\r\nIf I were doing this in git, I would create a branch off of your current `tosa-lowerings-v4` for the first chunk. Then run something like `git reset HEAD~1` to move all of the files back to the working set. Then delete the things not in the chunk you want to review first, create a new commit and push to a new branch/PR, referencing this one in the description. If you're very good at juggling, you can create the whole stack like this, but I am not that good at juggling and usually prefer to slice out one at a time when using such a workflow.\r\n\r\n\r\n\r\n\r\n\r\n> \r\n> I also noticed you are handling the TF functional control flow, but we're moving away from this. I rather see you handling \"CaseRegionOp\", \"IfRegionOp\", and \"WhileRegionOp\" here. You can use tf-functional-control-flow-to-regions as a pre-step in the pipeline to convert the functional ops to the region equivalent: the plan is to have this be the default when importing TF in the future.\r\n\r\n", "> I also noticed you are handling the TF functional control flow, but we're moving away from this. I rather see you handling \"CaseRegionOp\", \"IfRegionOp\", and \"WhileRegionOp\" here. You can use tf-functional-control-flow-to-regions as a pre-step in the pipeline to convert the functional ops to the region equivalent: the plan is to have this be the default when importing TF in the future.\r\n\r\nYes we haven't kept up with the state of the art in TF and MLIR control flow structures. We can unstage these passes, review what's needed and restore them in a later patch. ", "> If I were doing this in git, I would create a branch off of your current `tosa-lowerings-v4` for the first chunk. Then run something like `git reset HEAD~1` to move all of the files back to the working set. Then delete the things not in the chunk you want to review first, create a new commit and push to a new branch/PR, referencing this one in the description. If you're very good at juggling, you can create the whole stack like this, but I am not that good at juggling and usually prefer to slice out one at a time when using such a workflow.\r\n\r\nYes the current plan is to separate out the control flow passes and a few more things, and send a new update early next week once these updates have cleared IP review. ", "I just force pushed an update to tosa-lowerings-v4 after rebasing and resolving against upstream master . ", "The latest update has: \r\n* Removed the control flow passes.\r\n* Renames and updates behavior of prior tosa-combine as fuse_bias, as Kevin describes. \r\n* Removes the tosa_compiler binary entirely\r\n* Adds macro based conditional building of TOSA libraries. ", "We'll submit an update with the resolutions above once IP review clears. ", "@sjarus Can you please address Ubuntu Sanity errors? Thanks!", "> @sjarus Can you please address Ubuntu Sanity errors? Thanks!\r\n\r\nUbuntu Sanity is entirely Bazel BUILD indent formatting feedback, though it didn't trigger a command-line failure. Apparently it doesn't like our default editor bazel-mode. \r\n\r\n@stellaraccident is currently in the process of making some changes based on internal copybara checks, and I want to avoid making a conflicting push. Is it possible for this file indenting be easily addressed within your editor, @stellaraccident  ? "]}, {"number": 44850, "title": "Bump libjpeg-turbo from 2.0.4 to 2.0.5", "body": "It looks like the latest libjpeg-turbo is 2.0.5 so this PR\r\nbumps the version (currently on 2.0.4).\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\nPatches [CVE-2020-13790](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13790)", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44850) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 44849, "title": "Bump libjpeg-turbo from 2.0.4 to 2.0.5", "body": "It looks like the latest libjpeg-turbo is 2.0.5 so this PR\r\nbumps the version (currently on 2.0.4).\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\nPatches [CVE-2020-13790](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13790)", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44849) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 44848, "title": "Bump libjpeg-turbo from 2.0.4 to 2.0.5", "body": "It looks like the latest libjpeg-turbo is 2.0.5 so this PR\r\nbumps the version (currently on 2.0.4).\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\nPatches [CVE-2020-13790](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13790)", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44848) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 44847, "title": "Bump libjpeg-turbo from 2.0.4 to 2.0.5", "body": "It looks like the latest libjpeg-turbo is 2.0.5 so this PR\r\nbumps the version (currently on 2.0.4).\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\nPatches [CVE-2020-13790](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13790)", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44847) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 44846, "title": "Bump libjpeg-turbo from 2.0.4 to 2.0.5", "body": "It looks like the latest libjpeg-turbo is 2.0.5 so this PR\r\nbumps the version (currently on 2.0.4).\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\nPatches [CVE-2020-13790](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13790)", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44846) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 44845, "title": "Add script to download xa_nn_lib for hifi4 and call from the makefile.", "body": "Moving forward, we will adding specific download scripts in favor of the add_third_party_download function in tools/make/helper_functions.inc\r\n\r\nThe reasons are:\r\n * a standalone script is easier to manage for the developers that care about a particular third_party_download.\r\n\r\n * directly downloading in the first evaluation of the makefile allows    us to use wildcards to generate a list of third party srcs which makes  maintenance easier (we have encountered the same issue with CMSIS)\r\n\r\nAdresses:\r\n * http://b/173043817\r\n * http://b/143904317\r\n\r\n***Note:*** This change removes the xa_nnlib download from xtensa_hifi_makefile. This is done to incrementally clean up deprecated functionality. If we still need to build with xtensa_hifi we can first download xa_nnlib with the command in the testing section and then build with xtensa_hifi.\r\n\r\n#### Testing\r\n\r\nRan this command:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa XTENSA_CORE=blah XTENSA_BASE=blah XTENSA_TOOLS_VERSION=blah TARGET_ARCH=hifi4 clean\r\n```\r\n\r\nAnd confirmed that micro/tools/make/downloads/xa_nnlib_hifi4 is created and the list of .c files is printed on the terminal.\r\n\r\nTried the command with an already downloaded xa_nnlib_hifi4 and confirmed that its a noop.\r\n\r\nAlso tried various error conditions (bad checksum, incorrect paths, bad zip archive) and make command failed gracefully with a suitable error message.\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "tagging @pnikam-cad @nyadla-sys @kpraving"]}]