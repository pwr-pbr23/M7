[{"number": 14791, "title": "TFLite: get closer to build with Bazel on Windows", "body": "Bazel cannot yet build TensorFlow Lite on Windows,\r\nbut this commit gets us closer.\r\n\r\nIn this commit:\r\n- make the -Wno-implicit-fallthrough compiler flag\r\n  in flatbuffers' BUILD file be conditional to\r\n  non-Windows builds, because MSVC doesn't know\r\n  this flag\r\n- fix the Bazel build command in README.md by\r\n  removing single quotes around --cxxflags,\r\n  because it's not needed on Bash and is harmful\r\n  on Windows (because cmd.exe doesn't remove the\r\n  single quotes)\r\n- fix non-ASCII quotes and apostrophes, as well as\r\n  some formatting issues in README.md\r\n\r\nSee https://github.com/bazelbuild/bazel/issues/4148", "comments": ["Can one of the admins verify this patch?", "/cc @aselle @dslomov @meteorcloudy", "Also /cc @gunan ", "Jenkins, test this please.", "Thanks for the quick review @gunan !\r\nI rebased my commit.", "Jenkins, test this please.", "Thanks @gunan!\r\n\r\nThe Sanity Checks failed with this error:\r\n```\r\nStep 1 : FROM ubuntu:14.04\r\n ---> 132b7427a3b4\r\nStep 2 : LABEL maintainer \"Jan Prach <jendap@google.com>\"\r\n ---> Using cache\r\n ---> c7bec845a5f5\r\nStep 3 : COPY install/*.sh /install/\r\nBuild timed out (after 60 minutes). Marking the build as failed.\r\ntensorflow/tools/ci_build/ci_build.sh: line 130: 17639 Terminated              docker build -t ${DOCKER_IMG_NAME} -f \"${DOCKERFILE_PATH}\" \"${DOCKER_CONTEXT_PATH}\"\r\nERROR: docker build failed. Dockerfile is at /var/lib/jenkins/workspace/tensorflow-pull-requests-sanity/tensorflow/tools/ci_build/Dockerfile.cpu\r\nBuild was aborted\r\nUnable to get pull request builder trigger!!\r\n```\r\n\r\nAs far as I can tell, this looks like an infrastructure error.\r\n\r\nHow do I proceed?", "That should be just a flake.\r\n\r\nJenkins, test this please."]}, {"number": 14790, "title": "Fix nccl.BUILD on Windows", "body": "Bazel doesn't allow a random file name in `linkopts` attribute, so use `-DEFAULTLIB:` option to specify `ws2_32.lib`", "comments": []}, {"number": 14789, "title": "Add templated functions for `safe_strto[f|d|i32|i64|u32|u64]`", "body": "While working on #14330 I noticed that there is no templated functions for `safe_strto[f|d|i32|i64|u32|u64]`. As a result, an additional wrapper has to be created in different places to apply the typename in a templated class or function.\r\n\r\nExamples of the wrappers include the existing implementations in `tensorflow/core/kernels/string_to_number_op.cc` (`Convert`), `tensorflow/core/lib/strings/proto_text_util.h` (`ProtoParseNumeric`), and in #14330.\r\n\r\nIt might make sense to add a templated function for `safe_strto[f|d|i32|i64|u32|u64]` to avoid existing and future code duplications.\r\n\r\nThis fix adds\r\n```\r\ntemplate<typename T>\r\nbool SafeStringToNumeric(StringPiece s, T* value);\r\n```\r\nto address the above mentioned issue.\r\n\r\nNote: If this PR is merged then #14330 will needs to be updated accordingly.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@tensorflow-jenkins test this please.", "Perhaps some flakiness.\r\n\r\nJenkins, test this please."]}, {"number": 14788, "title": "tf.print makes a variable a constant?", "body": "```\r\nIn [1]: import tensorflow as tf\r\n\r\nIn [2]: # using print\r\n\r\nIn [3]: entcoeff =  tf.Variable([0], dtype=tf.float32, trainable=False)\r\n   ...: entcoeff = tf.Print(entcoeff,[entcoeff,\"printing\"])\r\n\r\nIn [4]: tf.assign(entcoeff, [-1.])\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-4-dd57efca5923> in <module>()\r\n----> 1 tf.assign(entcoeff, [-1.])\r\n\r\n/nohome/jaan/abhishek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py in assign(ref, value, validate_shape, use_locking, name)\r\n    270         ref, value, use_locking=use_locking, name=name,\r\n    271         validate_shape=validate_shape)\r\n--> 272   return ref.assign(value)\r\n\r\nAttributeError: 'Tensor' object has no attribute 'assign'\r\n\r\nIn [5]: # not using print\r\n\r\nIn [6]: entcoeff =  tf.Variable([0], dtype=tf.float32, trainable=False)\r\n\r\nIn [7]: tf.assign(entcoeff, [-1.])\r\nOut[7]: <tf.Tensor 'Assign:0' shape=(1,) dtype=float32_ref>\r\n\r\n```", "comments": ["Could you give a new name for return value of tf.Print? ", "@facaiy not sure what you mean.\r\n", "You're renaming the Python object `entcoeff` from a `tf.Variable` to the return value of `tf.Print` before trying `tf.assign`, which is why it crashes (Tensors are immutable).\r\n\r\nI think @facaiy wanted something like this which should work:\r\n```python\r\nx = tf.Variable([0], dtype=tf.float32, trainable=False)\r\nentcoeff = tf.Print(x, [x, \"printing\"])\r\ntf.assign(x, [-1.])\r\n```\r\n\r\nGoing by the [documentation of tf.Print](https://www.tensorflow.org/api_docs/python/tf/Print) it seems like your example should work though. A little weird that it turns `Variable` into `Tensor` considering:\r\n>Returns: \r\n>Same tensor as input_.\r\n\r\nIMO a bug or unclear documentation.", "Hi, I have created a PR to fix the issue. If accepted, tf.Prints will convert Variable to mutable Tensor.", "Hi, @abhigenie92 . As @alextp mentioned in #14878 , forwarding mutable references to variables is weird and discouraged, hence the behavior, returning a constant tensor, might be intent. Wish @alextp could clarify it, thanks.\r\n\r\nBy the way, I find the behavior is consistent with `tf.identity`, see:\r\n\r\n```python\r\nIn [19]: v = tf.Variable([1])\r\nIn [20]: v\r\nOut[20]: <tf.Variable 'Variable_2:0' shape=(1,) dtype=int32_ref>\r\n\r\nIn [21]: v3 = tf.identity(v)\r\nIn [22]: v3\r\nOut[22]: <tf.Tensor 'Identity_1:0' shape=(1,) dtype=int32>\r\n```", "`tf.identity` has a clearer docstring.\r\n\r\n>Returns:\r\n>A Tensor. Has the same type as input", "Yes, print should have the same behavior as identity (and it does). I'll happily accept a PR to fix the docstring.\r\n\r\nI can see how the docstring is confusing if you think a variable is a tensor (it's not). A variable is a thing which can be converted to a tensor containing its current value. It also can be assigned to, partitioned, etc, which tensors can't.", "At first I think a variable is not a tensor until I see the official document:\r\n\r\n[Tensors - Programmers' Guide](https://www.tensorflow.org/programmers_guide/tensors)\r\n> Some types of tensors are special, and these will be covered in other units of the Programmer's guide. The main ones are:\r\n>>      tf.Variable\r\n>>      tf.Constant\r\n>>      tf.Placeholder\r\n>>      tf.SparseTensor\r\n\r\nIf I understand it correctly, there are only one kind of `Tensor` in C++ side (except of sparse case): it can be mutable or immutable which depends on `dtype`. However, it is not that simple in Python side:\r\n- `tf.Constant` return immutable `tf.Tensor`.\r\n- `tf.Variable` doesn't return `tf.Tensor`, however they are treated as same class in most cases, say `tf.add`. I think `tf.Variable` is a wrapper for mutable `tf.Tensor`, right?\r\n\r\nIt seems that doc of `tf.Print` is accurate as `tf.identity`, what confuse us is how to regard `tf.Variable`: is it a tensor or not?\r\n\r\n> tf.Print:\r\n>> Returns:\r\n>> Same tensor as input_.\r\n\r\n> tf.identity:\r\n>> Returns:\r\n>> A Tensor. Has the same type as input.", "tf.Variable s are not tensors, and neither are SparseTensors\n\nOn Fri, Dec 1, 2017 at 4:21 PM, Yan Facai (\u989c\u53d1\u624d) <notifications@github.com>\nwrote:\n\n> At first I think a variable is not a tensor until I see the official\n> document:\n>\n> Tensors - Programmers' Guide\n> <https://www.tensorflow.org/programmers_guide/tensors>\n>\n> Some types of tensors are special, and these will be covered in other\n> units of the Programmer's guide. The main ones are:\n>\n>  tf.Variable\n> tf.Constant\n>  tf.Placeholder\n>  tf.SparseTensor\n>\n> If I understand it correctly, there are only one kind of Tensor in C++\n> side: it can be mutable or immutable which depends on dtype. However, it\n> is not that simple in Python side:\n>\n>    - tf.Constant return immutable tf.Tensor.\n>    - tf.Variable doesn't return tf.Tensor, however they are treated as\n>    same class in most cases, say tf.add. I think tf.Variable is a wrapper\n>    for mutable tf.Tensor, right?\n>\n> It seems that doc of tf.Print is accurate as tf.identity, what confuse us\n> is how to regard tf.Variable: is it a tensor or not?\n>\n> tf.Print:\n>\n> Returns:\n> Same tensor as input_.\n>\n> tf.identity:\n>\n> Returns:\n> A Tensor. Has the same type as input.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/14788#issuecomment-348649139>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxUhn3SLxpKB9OQzUil42J-O4TTwrks5s8JgmgaJpZM4QnEuP>\n> .\n>\n\n\n\n-- \n - Alex\n", "Thanks for your explanation, @alextp.\r\n\r\nI create two dissimilar PRs for the issue:\r\n1. #15068 changed `tf.Print`'s behavior, which returns Variable for Variable. If so, the behavior is not consistent with `tf.identity`any more.\r\n2. #15069 is the opposite. It fixes the docstring and states `tf.Print` behaves like `tf.identity`.\r\n\r\nBe free to approve either one, and close the other.", "Fixed by #15069"]}, {"number": 14787, "title": "TF Lite README.md lacks link to the mentioned mobilenet_v1_224.pb file", "body": "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/README.md", "comments": ["I try use  export_inference_graph build mobilenet_v1_224.pb:\r\nhttps://github.com/tensorflow/models/blob/master/research/slim/README.md\r\n$ python export_inference_graph.py \\\r\n  --alsologtostderr \\\r\n  --model_name=mobilenet_v1 \\\r\n  --image_size=224 \\\r\n  --output_file=/tmp/mobilenet_v1_224.pb\r\n\r\nbut when I build tflite,\r\nbazel-bin/tensorflow/contrib/lite/toco/toco --input_file=/tmp/frozen_mobilenet_v1_224.pb --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=/tmp/mobilenet_v1_1.0_224.lite --inference_type=FLOAT --input_type=FLOAT --input_arrays=input --output_arrays=MobilenetV1/Predictions/Reshape_1 --input_shapes=1,224,224,3\r\n2017-12-06 23:16:35.349463: W tensorflow/contrib/lite/toco/toco_cmdline_flags.cc:177] --input_type is deprecated. Use --inference_input_type.\r\n2017-12-06 23:16:35.436850: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 336 operators, 502 arrays (0 quantized)\r\n2017-12-06 23:16:35.475639: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 31 operators, 88 arrays (0 quantized)\r\n2017-12-06 23:16:35.475947: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 31 operators, 88 arrays (0 quantized)\r\n2017-12-06 23:16:35.476144: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:312] Total transient array allocated size: 6422528 bytes, theoretical optimal value: 4816896 bytes.\r\n2017-12-06 23:16:35.476323: I tensorflow/contrib/lite/toco/toco_tooling.cc:264] Estimated count of arithmetic ops: 1.14264 billion (note that a multiply-add is counted as 2 ops).\r\n2017-12-06 23:16:35.476581: F tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: Squeeze\r\nAborted (core dumped)\r\n\r\nWhen I use (https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_1.0_224_frozen.tgz) frozen.pb build tflite file, its success\r\n\r\nTHX!\r\n@aselle ", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "arixlin, can you please clarify where the link is not right ?\r\n\r\nIs the link to the input file not specified in this toco command ?\r\nbazel-bin/tensorflow/contrib/lite/toco/toco --input_file=/tmp/frozen_mobilenet_v1_224.pb -\r\n\r\n", "@anitha-v (https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_1.0_224_frozen.tgz)  is right pb link\uff0c\uff08https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md\uff09has more info about pb file", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @arixlin, @anitha-v: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It is fix\uff01 build success\uff01THX\uff01"]}, {"number": 14786, "title": "compile grpc , fail to download something  from china.", "body": "Is there anyone give me advice on solving the problem? and give some other https to download?", "comments": ["I copy the url and download the file with googlechrome, then copy the file to the direcory as the error shown, at last repeat the command, the problem do not emerge.", "@mrbrantofgithub    Thank you very much!"]}, {"number": 14785, "title": "Update layers pull request", "body": "I have created a new pull request by updating the unit test cases with reference to my previous pull request\r\nhttps://github.com/tensorflow/tensorflow/pull/13829 .\r\nI have already updated layers.py initially by checking for beta in the if condition.\r\nThe initial pull request was raised in accordance with the issue \r\nhttps://github.com/tensorflow/tensorflow/issues/11673 .\r\n\r\nPlease verify and get back.", "comments": ["Can one of the admins verify this patch?", "Please verify and get back.", "Can one of the admins verify this patch?", "@tensorflow-jenkins  test this please\r\n", "it would be great if you could merge this PR.", "@shreyneil do you mind fixing the test? \r\n`NameError: global name 'dtype' is not defined`", "Made the necessary changes, please verify.", "Thanks @shreyneil! @tensorflow-jenkins test this please.", "@yifeif It would be great if you could merge this PR. "]}, {"number": 14783, "title": "Debugging control flow gradients code", "body": "Hi,\r\n\r\nI've been trying to debug some code I have for computing gradients over control flow ops and I've encountered a couple issues:\r\n\r\n1. @itsmeolivia The shape check introduced in commit bac56b3 breaks the code [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/control_flow_ops.py#L2466) when creating a backprop indexed slices accumulator. That's because when concatenating the indices in line [2528](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/control_flow_ops.py#L2528), the shape changes.\r\n2. I have implemented most of the same unit tests and all works well. I also have support for RNNs and things are fine. However, when I implement a decoder that involves a while loop with a cond nested within it, there are cases where I have a problem. More specifically, if I use an existing tensor (created outside the while loop) within a cond branch, I get this error: `Retval[0] does not have value`. I realize this comes from a switch output of a dead branch being used, but can't figure out exactly what's wrong. Note that this only happens when computing gradients and only when I use an existing tensor within a branch. What would be a good way to debug this?\r\n3. More generally, how can I debug error like `Retval[0] does not have value`. No stack trace is provided and I'm not sure how I could configure TensorFlow when compiling to add some debugging information (e.g., stack trace). @asimshankar @skye @alextp Is there some way to setup and run TensorFlow in a debug mode of some sort?\r\n\r\nThanks! :)", "comments": ["Re (2) I wonder if your cond predicate is a Variable? If so, this is not\nsupported and likely broken.\n\nRe (3) what I do is build from source, grep the source code to find the\nplace where this error is being raised, and replace the status with a\nLOG(FATAL) to get a stack trace.\n\nOn Tue, Nov 21, 2017 at 9:53 PM, Anthony Platanios <notifications@github.com\n> wrote:\n\n> Hi,\n>\n> I've been trying to debug some code I have for computing gradients over\n> control flow ops and I've encountered a couple issues:\n>\n>    1. @itsmeolivia <https://github.com/itsmeolivia> The shape check\n>    introduced in commit bac56b3\n>    <https://github.com/tensorflow/tensorflow/commit/bac56b37be7736c9da9a3257696a9c1241327d60>\n>    breaks the code here\n>    <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/control_flow_ops.py#L2466>\n>    when creating a backprop indexed slices accumulator. That's because when\n>    concatenating the indices in line 2528\n>    <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/control_flow_ops.py#L2528>,\n>    the shape changes.\n>    2. I have implemented most of the same unit tests and all works well.\n>    I also have support for RNNs and things are fine. However, when I implement\n>    a decoder that involves a while loop with a cond nested within it, there\n>    are cases where I have a problem. More specifically, if I use an existing\n>    tensor (created outside the while loop) within a cond branch, I get this\n>    error: Retval[0] does not have value. I realize this comes from a\n>    switch output of a dead branch being used, but can't figure out exactly\n>    what's wrong. Note that this only happens when computing gradients and only\n>    when I use an existing tensor within a branch. What would be a good way to\n>    debug this?\n>    3. More generally, how can I debug error like Retval[0] does not have\n>    value. No stack trace is provided and I'm not sure how I could\n>    configure TensorFlow when compiling to add some debugging information\n>    (e.g., stack trace). @asimshankar <https://github.com/asimshankar>\n>    @skye <https://github.com/skye> @alextp <https://github.com/alextp> Is\n>    there some way to setup and run TensorFlow in a debug mode of some sort?\n>\n> Thanks! :)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/14783>, or mute the\n> thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxdASlkM-KX-AU1SckplrBnQtc9RRks5s47bagaJpZM4Qm5Iw>\n> .\n>\n\n\n\n-- \n - Alex\n", "@alextp Regarding the variable predicates. That's not the issue. The problem comes from within one of the branches which uses an external tensor. I create a switch op for that tensor (similar to how it's done in Python) to get lazy cond semantics, but during backprop over that part of the graph, something goes wrong.\r\n\r\nI will try the LOG(FATAL) advice. That sounds really helpful, thanks! :)", "Just a heads up, I'm pretty sure LOG(FATAL) will **not** produce a stack trace unfortunately. (This works inside of Google. It would be great if someone implemented an open-source version, I think there are already open-source utilities/examples that do this in other projects.) I suggest using gdb to get a stacktrace, although I'm not sure the stack will help you much, since the dead tensor has already been propagated.", "In general I find unexpected dead tensors very hard to debug :( You can try using the flag `--vmodule=executor=2` (and just running the failing test) to get more logging information from the executor. You'll probably have to dig in to the executor implementation to understand what it means. If you haven't already, print the GraphDef right before you run it. This is useful for manually trying to find the problem and correlating with the log output. You can also try running the analogous Python test, printing that GraphDef, and comparing to the GraphDef you're producing. This is probably my #1 way to narrow down this kind of problem.", "@skye Yes, I just realized it produces no stack trace. I will try comparing the two GraphDefs as that sounds like it could help. But yeah, I agree. Every time I've had this problem it's been a nightmare to debug. Thanks a lot for the tips thought. I'll let you know how it goes and I'll also check to verify that the same problem does not happen with the Python API.", "@skye On a side note, regarding comparing the GraphDefs, do you have any tools for comparing structure of graph defs while ignoring for example names of ops or do you usually do that manually by going through the text?", "I don't have anything special for comparing the structure. I sometimes try to munge my code to make it produce the same names, etc. to make it easier to compare. Of course try to create a minimal repro, although anything involving a while loop + gradients gets unmanageable pretty fast. I use emacs ediff to actually compare the GraphDefs.\r\n\r\nFeel free to post any updates here, if they give me more debugging ideas/directions I'll let you know.", "@skye Thanks for all the tips. I finally figured it out by comparing the GraphDefs. I use slightly different naming conventions than the Python API (e.g., with respect to proper casing names, etc.), but in the end I created a small script for post-processing the GraphDef and mapping the names. The bug ended up being something ridiculous (i.e., forgetting to exit a gradient state context), but it was hard to trace down. Thank you very much for all the ideas! :)", "Awesome, glad to hear it!", "@skye Could we leave this open for @itsmeolivia to respond to the first point? I'm curious as to how her checks can work with the indexed slices updates.", "@itsmeolivia FYI, the shape checking code is crashing sometimes on a few Ubuntu machines I tried (not on a Mac though), at `graph->refiner.GetContext(&new_src.oper->node)`. It happens because `node_to_context_` inside the refiner is uninitialized I think. I'm not sure what is the root cause of the problem though.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 14782, "title": "[Android] Failed to resolve: org.tensorflow:tensorflow-lite", "body": "Hi, I am trying to run tensorflow lite application on my android device.\r\nI get the following error when syncing gradle project:\r\nFailed to resolve: org.tensorflow:tensorflow-lite\r\n\r\nAny clues on why this is happening?\r\nMy system information is:\r\nOS: Windows 7 64 bit\r\nAndroid Studio 3.0.1\r\nAndroid SDK Platform-tools: 26.0.2\r\nAndroid SDK Tools: 26.1.1\r\n\r\nThanks in advance.", "comments": ["It seems like a proxy issue. Close issue."]}, {"number": 14781, "title": "Improve variance_scaling_initializer description", "body": "Added mention of the \"MRSA initialization\" alias. This name has been mentioned in [multiple publications](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=%22msra+initialization%22&btnG=). Mentioning it in the docs will make this initializer easier to find.", "comments": ["Can one of the admins verify this patch?", "This should be ready for review.", "I merged master to get rid of a test issue that was fixed recently (#14803), but now tests are failing for another reason:\r\n\r\n```\r\nStep 3 : COPY install/*.sh /install/\r\ntensorflow/tools/ci_build/ci_build.sh: line 130: 23646 Terminated              docker build -t ${DOCKER_IMG_NAME} -f \"${DOCKERFILE_PATH}\" \"${DOCKER_CONTEXT_PATH}\"\r\nERROR: docker build failed. Dockerfile is at /var/lib/jenkins/workspace/tensorflow-pull-requests-sanity/tensorflow/tools/ci_build/Dockerfile.cpu\r\nBuild was aborted\r\n```\r\nWas the build manually killed?\r\n", "Infra error: docker.\r\n\r\nJenkins, test this please.", "Tests seem to be failing for reasons unrelated to this PR. Similar to https://github.com/tensorflow/tensorflow/pull/15099#issuecomment-349307648"]}, {"number": 14780, "title": "Bazel Compile tensorflow failure", "body": "System information\r\nwindows 10 64bit\r\n\r\nbazel version\r\nBuild label: 0.7.0\r\nBuild target: bazel-out/msvc_x64-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Oct 18 14:25:56 2017 (1508336756)\r\nBuild timestamp: 1508336756\r\nBuild timestamp as int: 1508336756\r\n\r\ni run ./configure:\r\nconfigure\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nYou have bazel 0.7.0 installed.\r\nPlease specify the location of python. [Default is F:\\barzel\\py2.7\\python.exe]: D:/Anaconda3/python.exe\r\nFound possible Python library paths:\r\n  D:\\Anaconda3\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [D:\\Anaconda3\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: n\r\nNo GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: n\r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: n\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\n\r\n\r\nAdd \"--config=mkl\" to your bazel command to build with MKL support.\r\nPlease note that MKL on MacOS or windows is still not supported.\r\nIf you would like to use a local MKL instead of downloading, please set the environment variable \"TF_MKL_ROOT\" every time before build.\r\n\r\nthen i run:\r\nbazel build --verbose_failures //tensorflow:libtensorflow_cc.so\r\n\r\nERROR: F:/barzel/tensorflow1/tensorflow-master/tensorflow/cc/BUILD:422:1: Linking of rule '//tensorflow/cc:ops/sparse_ops_gen_cc' failed (Exit 1181): link.exe failed: error executing command\r\n  cd C:/users/cfenich/appdata/local/temp/_bazel_cfenich/ku2zcbwb/execroot/org_tensorflow\r\n  SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.10240.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\\\shared;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\\\um;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.10240.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\8.1\\lib\\winv6.3\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\Tools;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Windows Kits\\8.1\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\8.1\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;;C:\\WINDOWS\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET TEMP=C:\\Users\\CFenich\\AppData\\Local\\Temp\r\n    SET TMP=C:\\Users\\CFenich\\AppData\\Local\\Temp\r\n    SET USE_LINKER=1\r\n  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/link.exe /nologo /OUT:bazel-out/host/bin/tensorflow/cc/ops/sparse_ops_gen_cc.exe tensorflow_framework /SUBSYSTEM:CONSOLE -Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..,-rpath,$ORIGIN/../.. -pthread /MACHINE:X64 @bazel-out/host/bin/tensorflow/cc/ops/sparse_ops_gen_cc.exe-2.params /DEFAULTLIB:msvcrt.lib.\r\nLINK : warning LNK4044: \u65e0\u6cd5\u8bc6\u522b\u7684\u9009\u9879\u201c/Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..,-rpath,$ORIGIN/../..\u201d\uff1b\u5df2\u5ffd\u7565\r\nLINK : warning LNK4044: \u65e0\u6cd5\u8bc6\u522b\u7684\u9009\u9879\u201c/pthread\u201d\uff1b\u5df2\u5ffd\u7565\r\nLINK : warning LNK4044: \u65e0\u6cd5\u8bc6\u522b\u7684\u9009\u9879\u201c/lm\u201d\uff1b\u5df2\u5ffd\u7565\r\nLINK : warning LNK4044: \u65e0\u6cd5\u8bc6\u522b\u7684\u9009\u9879\u201c/lm\u201d\uff1b\u5df2\u5ffd\u7565\r\nLINK : warning LNK4044: \u65e0\u6cd5\u8bc6\u522b\u7684\u9009\u9879\u201c/lm\u201d\uff1b\u5df2\u5ffd\u7565\r\nLINK : fatal error LNK1181: \u65e0\u6cd5\u6253\u5f00\u8f93\u5165\u6587\u4ef6\u201ctensorflow_framework.obj\u201d\r\n", "comments": ["@asimshankar", "same problem\uff0c you can try build on linux", "Does it work if you add `--config=monolithic` to the `bazel build` command? Unfortunately we don't have that shared object working on Windows yet (waiting on Bazel support AFAIK). The only downside is that custom ops won't work with libtensorflow_cc built this way.", "@allenlavoie thanks!but, it doesn't work. if i want to use tensorflow c++ on windows, could i build tensorflow by cmake? ", "Oh right, our Bazel build is still not quite there on Windows. Is the issue that we don't have an exported symbol list for the C++ API? CMake would be good to try.", "Right, currently the supported path on windows is cmake.", "Closing this issue due to staleness. Please use the latest version of TensorFlow and build again.\r\nFeel free to report any issues you encounter with latest TensorFlow. Thanks!"]}, {"number": 14779, "title": "\"error in tensorflow setup command\" error when running building the TensorFlow pip package", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: TensorFlow commit 70ba44b46bb9e5f5e55b2357676ffa7196b9bda7\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.7.0\r\n- **GCC/Compiler version (if compiling from source)**: 4.8.4\r\n- **CUDA/cuDNN version**: CUDA 8.0, cuDNN 6.0\r\n- **GPU model and memory**: GTX 1080 8 GB\r\n- **Exact command to reproduce**:\r\n```\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --config=monolithic\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package ~/tensorflow_pkg/\r\n```\r\n\r\n### Describe the problem\r\nWhen I try to build TensorFlow at commit 70ba44b46bb9e5f5e55b2357676ffa7196b9bda7 or later, I get the following error after running `bazel-bin/tensorflow/tools/pip_package/build_pip_package ~/tensorflow_pkg`:\r\n\r\n```\r\nreedwm@reedwm2:~/tensorflow_test$ bazel-bin/tensorflow/tools/pip_package/build_pip_package ~/tensorflow_ec2_pkg/\r\nTue Nov 21 18:22:34 PST 2017 : === Using tmpdir: /tmp/tmp.m24Ub0Z2z4\r\n~/tensorflow_test/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles ~/tensorflow_test\r\n~/tensorflow_test\r\n/tmp/tmp.m24Ub0Z2z4 ~/tensorflow_test\r\nTue Nov 21 18:22:35 PST 2017 : === Building wheel\r\nerror in tensorflow setup command: 'install_requires' must be a string or list of strings containing valid project/version requirement specifiers\r\n```\r\n\r\nThis does not occur on the commit before 70ba44b46bb9e5f5e55b2357676ffa7196b9bda7. When running `./configure`, I choose the default option for everything except that I choose to use CUDA.\r\n\r\nNote I use `--config=monolithic` to get around #13243.\r\n\r\n/CC @alanhdu @gunan, any ideas what the issue could be?", "comments": ["Could you share your pip version?\r\n`pip --version` output should be enough.\r\nUbuntu 14 has an old pip version, so you may need to run `sudo pip install --upgrade pip`, then retry the 2nd command you shared above.", "I have the exactly same problem but  I have ubuntu 16.04, CUDA 9.0 and cuDNN 7.0, GTX 950M 2GB\r\npip 9.0.1 \r\nPython 2.7.12", "Same problem with macOS High Sierra, python 2.7.10 and pip 9.0.1.\r\n\r\nThe problem seems to be in ```setup.py``` (```serving/tensorflow_serving/tools/pip_package/setup.py```). As an intermediate solution I deleted the ```python_version``` checks in ```REQUIRED_PACKAGES```. After everything works.", "Thanks for the information. This is what we tried to base the change on:\r\nhttps://setuptools.readthedocs.io/en/latest/setuptools.html#declaring-platform-specific-dependencies\r\n\r\nWhat happens if you remove all the extra the spaces in the lines with `python_version`?\r\nFor example, the line:\r\n```\r\n'backports.weakref >= 1.0rc1; python_version < \"3.4\"',\r\n```\r\nbecomes\r\n```\r\n'backports.weakref>=1.0rc1;python_version<\"3.4\"',\r\n```", "Ok. I see. Removing the extra spaces doesn't help (same problem).", "i need help cant downlod tensorflow it shows could not find a versone ", "I have the same problem on Mac. I get the following error.\r\n\r\nWed Nov 22 23:38:40 CST 2017 : === Using tmpdir: /var/folders/vt/mp6zqnxd095bt5m88tnf0gq00000gn/T/tmp.XXXXXXXXXX.9u2i5Cuv\r\n~/software/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles ~/software/tensorflow\r\n~/software/tensorflow\r\n/var/folders/vt/mp6zqnxd095bt5m88tnf0gq00000gn/T/tmp.XXXXXXXXXX.9u2i5Cuv ~/software/tensorflow\r\nWed Nov 22 23:38:43 CST 2017 : === Building wheel\r\nerror in tensorflow setup command: 'install_requires' must be a string or list of strings containing valid project/version requirement specifiers; Expected ',' or end-of-list in backports.weakref >= 1.0rc1; python_version < \"3.4\" at ; python_version < \"3.4\"", "Getting the same error in High Sierra ", "Looks like the best course of action is to revert the culprit change, and\nthen we can explore what we need to do.\n\nThe revert may be delayed until Monday, because of the US holidays.\n\nOn Nov 22, 2017 9:42 PM, \"Hossein15051\" <notifications@github.com> wrote:\n\n> I have the same problem on Mac. I get the following error.\n>\n> Wed Nov 22 23:38:40 CST 2017 : === Using tmpdir: /var/folders/vt/\n> mp6zqnxd095bt5m88tnf0gq00000gn/T/tmp.XXXXXXXXXX.9u2i5Cuv\n> ~/software/tensorflow/bazel-bin/tensorflow/tools/pip_\n> package/build_pip_package.runfiles ~/software/tensorflow\n> ~/software/tensorflow\n> /var/folders/vt/mp6zqnxd095bt5m88tnf0gq00000gn/T/tmp.XXXXXXXXXX.9u2i5Cuv\n> ~/software/tensorflow\n> Wed Nov 22 23:38:43 CST 2017 : === Building wheel\n> error in tensorflow setup command: 'install_requires' must be a string or\n> list of strings containing valid project/version requirement specifiers;\n> Expected ',' or end-of-list in backports.weakref >= 1.0rc1; python_version\n> < \"3.4\" at ; python_version < \"3.4\"\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/14779#issuecomment-346535489>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOcg9eOpt4SlMPESWkwKS3TZ-_e8bks5s5QXBgaJpZM4Qm0pO>\n> .\n>\n", "@gunan Is there a reference compile script that is used for the Jenkins' CI that I can use to write a configuration and compile script? looks like\r\n1. the Jenkins test has been passing\r\n2. I looked but couldn't find the script that is used to do the Jenkins test\r\nThanks in advance.", "found under /tools/ci_build. Thanks", "FYI you probably need to update setuptools to 'setuptools-38.2.1' and also install wheel if you don't have it installed.", "I wonder whether this is a problem with an old version of `setuptools` (or maybe `wheel`?). I have `36.5.0`, which works fine.", "@alanhdu I am building on centos7 and setuptools is too old on on centos, the package built as soon as I updated setuptools and installed wheels. \r\n\r\nIt would be nice if the packaging didn't force an update on setuptools for people running on long term supported distros", "I meet the same error and solve it by installing setuptools\r\n`pip install -U setuptools`\r\n\r\nHope this help you.", "Thanks @chjshan. Installing setuptools worked for me too. ", "Thanks @chjshan. Installing setuptools worked for me too.", "Thanks @chjshan. Saved my day. ", "`pip install -U setuptools ` didn't help me so I installed the latest setuptools from https://pypi.python.org/pypi/setuptools and it worked, thanks.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "I think this is reverted, or fixed.\r\n@yifeif please close this issue once/if it is resolved.\r\n  ", "thanks @chjshan ! update setuptools solved my problems", "setup.py has been fixed at head. Closing this issue."]}, {"number": 14778, "title": "v2 to v1", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 14777, "title": "After quantized ssd_mobilenet_v1_coco model, loaded error on Android ", "body": "### System information\r\n- **OS Platform and Distribution** : Linux Ubuntu 14.04.5 LTS\r\n- **TensorFlow installed from** : binary\r\n- **TensorFlow version** : 1.2.1\r\n- **Python version**: 2.7\r\n- **Bazel version**: 0.70\r\n- **GCC/Compiler version (if compiling from source)**: 4.8.4 \r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: Tesla P100-PCIE \r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nLoad a quantized ssd_mobilenet_v1 model on Android meet error\r\nCaused by: java.io.IOException: Not a valid TensorFlow Graph serialization: NodeDef mentions attr 'T' not in Op<name=Where; signature=input:bool -> index:int64>; \r\n\r\nI quantize the ssd_mobilenet_v1 model o ubuntu 14,  using the below command\r\n```\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \r\n--in_graph=/data5/zxt/coco_log/export/frozen_inference_graph.pb  \r\n--out_graph=/home/zxt/git/ssd_optimized.pb --inputs='image_tensor'\r\n --outputs='detection_boxes,detection_scores,num_detections,detection_classes' --transforms='\r\n  add_default_attributes\r\n  strip_unused_nodes(type=float, shape=\"-1,-1,-1,3\")\r\n  remove_nodes(op=Identity, op=CheckNumerics)\r\n  fold_batch_norms\r\n  fold_old_batch_norms\r\n  quantize_weights\r\n  strip_unused_nodes\r\n  sort_by_execution_order'\r\n```\r\nI also have tried some other parameters\uff0cbut all failed with same issue.\r\nthe  frozen_inference_graph.pb   is ok on Android, but the quantized pb is can NOT load.\r\nWhen run the quantized pb on android phone, met errors\r\n```\r\n      at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:799)\r\n   at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:689)\r\n    Caused by: java.io.IOException: Not a valid TensorFlow Graph serialization: NodeDef mentions attr 'T' not in Op<name=Where; signature=input:bool -> index:int64>; \r\nNodeDef: Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/FilterGreaterThan/Where = Where[T=DT_BOOL](Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/FilterGreaterThan/Greater). \r\n(Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n           at org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph(TensorFlowInferenceInterface.java:535)\r\n \r\n```", "comments": ["Hi @offbye, I believe you comment on an issue of mine because I am also experiencing the same error. Were you able to find a solution or a workaround? Thanks. Not even my frozen_inference_graph.pb is working on Android.\r\n\r\nEDIT: Could you explain me how you generated the frozen_inference_graph.pb? I could be doing something wrong.", "@offbye As I mentioned in the other issue you filed, I'm suspecting that there's a mismatch between the version of the code that is running the graph (the android demo?), vs. the code that built the GraphDef.\r\n\r\nCan you provide details on exactly how you're running your models on Android?", "I use the lastest code in master , and I quantized the    `assets/ssd_mobilenet_v1_android_export.pb`\uff0c then I change the `tensorflow\\examples\\android\\src\\org\\tensorflow\\demo\\DetectorActivity.java` ,use the quantized ssd_mobilenet_v1_android_quantized.pb\uff0c and  I met errors:\r\n\r\n```\r\n12-19 16:39:49.231 23831-23855/? E/TensorFlowInferenceInterface: Failed to run TensorFlow inference with inputs:[image_tensor], outputs:[detection_boxes, detection_scores, detection_classes, num_detections]\r\n12-19 16:39:49.231 23831-23855/? E/AndroidRuntime: FATAL EXCEPTION: inference\r\n                                                   Process: org.tensorflow.demo, PID: 23831\r\n                                                   java.lang.IllegalArgumentException: The node 'Postprocessor/BatchMultiClassNonMaxSuppression/map/while/TensorArrayWrite/TensorArrayWriteV3' has inputs from different frames. The input 'Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/Merge' is in frame ''. The input 'Postprocessor/BatchMultiClassNonMaxSuppression/map/while/TensorArrayWrite/TensorArrayWriteV3/Enter' is in frame 'Postprocessor/BatchMultiClassNonMaxSuppression/map/while/while_context'.\r\n                                                       at org.tensorflow.Session.run(Native Method)\r\n                                                       at org.tensorflow.Session.access$100(Session.java:48)\r\n                                                       at org.tensorflow.Session$Runner.runHelper(Session.java:298)\r\n                                                       at org.tensorflow.Session$Runner.run(Session.java:248)\r\n                                                       at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:218)\r\n                                                       at org.tensorflow.demo.TensorFlowObjectDetectionAPIModel.recognizeImage(TensorFlowObjectDetectionAPIModel.java:158)\r\n                                                       at org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:289)\r\n                                                       at android.os.Handler.handleCallback(Handler.java:743)\r\n                                                       at android.os.Handler.dispatchMessage(Handler.java:95)\r\n                                                       at android.os.Looper.loop(Looper.java:150)\r\n                                                       at android.os.HandlerThread.run(HandlerThread.java:61)\r\n```\r\n\r\nI used below command quantized pb \r\n```\r\n  bazel-bin/tensorflow/tools/quantization/quantize_graph --input=/tmp/assets/ssd_mobilenet_v1_android_export.pb  --output_node_names=detection_boxes,detection_scores,detection_classes,num_detections --print_nodes --output=/tmp/assets/ssd_mobilenet_v1_android_quantized.pb  --mode=weights --logtostderr\r\n```\r\n\r\n\r\nBefore quantized the ssd_mobilenet_v1_android_export.pb,  the android Detect Demo is ok on my phone.\r\n\r\nThen I tried use graph_transforms to quantized \r\n```\r\n  bazel-bin/tensorflow/tools/graph_transforms/transform_graph     --in_graph=/data5/zxt/coco_log/export/frozen_inference_graph.pb  --out_graph=/tmp/ssd2.pb --inputs='image_tensor' --outputs='detection_boxes,detection_scores,num_detections,detection_classes' --transforms='\r\n    add_default_attributes\r\n    strip_unused_nodes(type=float, shape=\"-1,-1,-1,3\")\r\n    remove_nodes(op=Identity, op=CheckNumerics)\r\n    fold_batch_norms\r\n    fold_old_batch_norms\r\n    quantize_weights\r\n    sort_by_execution_order'\r\n```\r\nI use the new ssd2.pb, \r\nreinstall and run  android  detect demo log errors:\r\n\r\n```\r\n\r\n12-19 17:12:07.592 27113-27138/org.tensorflow.demo E/TensorFlowInferenceInterface: Failed to run TensorFlow inference with inputs:[image_tensor], outputs:[detection_boxes, detection_scores, detection_classes, num_detections]\r\n12-19 17:12:07.592 27113-27138/org.tensorflow.demo E/AndroidRuntime: FATAL EXCEPTION: inference\r\n                                                                     Process: org.tensorflow.demo, PID: 27113\r\n                                                                     java.lang.IllegalArgumentException: Retval[0] does not have value\r\n                                                                         at org.tensorflow.Session.run(Native Method)\r\n                                                                         at org.tensorflow.Session.access$100(Session.java:48)\r\n                                                                         at org.tensorflow.Session$Runner.runHelper(Session.java:298)\r\n                                                                         at org.tensorflow.Session$Runner.run(Session.java:248)\r\n                                                                         at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:218)\r\n                                                                         at org.tensorflow.demo.TensorFlowObjectDetectionAPIModel.recognizeImage(TensorFlowObjectDetectionAPIModel.java:158)\r\n                                                                         at org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:289)\r\n                                                                         at android.os.Handler.handleCallback(Handler.java:743)\r\n                                                                         at android.os.Handler.dispatchMessage(Handler.java:95)\r\n                                                                         at android.os.Looper.loop(Looper.java:150)\r\n                                                                         at android.os.HandlerThread.run(HandlerThread.java:61)\r\n```\r\n", " I have confirmed that if I only use quantize_weights , the pb model can run on Android, but can not detect anything.\r\nBut I can use rounded_weights to transform the pb model , and the final apk size compressed about 17M !\r\n\r\nSo I think the issue can be closed!", "@offbye is the complete command you used with rounded_weights the following?\r\n```\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \r\n--in_graph=/data5/zxt/coco_log/export/frozen_inference_graph.pb  \r\n--out_graph=/home/zxt/git/ssd_optimized.pb --inputs='image_tensor'\r\n --outputs='detection_boxes,detection_scores,num_detections,detection_classes' --transforms='\r\n  add_default_attributes\r\n  strip_unused_nodes(type=float, shape=\"-1,-1,-1,3\")\r\n  remove_nodes(op=Identity, op=CheckNumerics)\r\n  fold_batch_norms\r\n  fold_old_batch_norms\r\n  round_weights\r\n  strip_unused_nodes\r\n  sort_by_execution_order'\r\n```\r\nI used  the transforms values above on ssd_mobilenet_v1_coco_2017_11_17's frozen_inference_graph.pb (size 29112121) but the transformed graph's size is still about 25MB.\r\n\r\nThanks!"]}, {"number": 14776, "title": "tf.keras.estimator.estimator_from_model does not respect options set in RunConfig", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: tf.VERSION = 1.4.0 tf.GIT_VERSION = v1.4.0-rc1-11-g130a514\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 8.0.61/6.0.21\r\n- **GPU model and memory**: NVIDIA Tesla M60 8 GB\r\n- **Exact command to reproduce**: See Below\r\n\r\n### Describe the problem\r\nWhen trying to use an estimator that is derived from ```tf.keras.estimator.estimator_from_model()``` and training with ```tf.estimator.train_and_evaluate()```, setting ```gpu_options``` in the ```session_config``` of ```tf.estimator.RunConfig``` does not cause the settings to be respected when passed to the estimator_from_model function. For example setting ```per_process_gpu_memory_fraction=0.5``` does not decrease the memory allocated to the process on the GPU, similarly setting ```allow_growth=True``` continues to allocate all of the memory and does not allow memory growth.\r\n\r\nI also tested this with the canned estimator ```tf.estimator.DNNRegressor```, and the settings were applied as expected when the RunConfig was passed to the estimator.\r\n\r\nBelow is code to demonstrate this issue. \r\n\r\n### Source code / logs\r\nMinimal example, runs to completion and trains successfully. But, changing the GPUOptions settings does not cause the GPU memory to be utilized as expected:\r\n```python\r\nimport os\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\n# Neither of these GPUOptions are respected\r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\r\n#gpu_options = tf.GPUOptions(allow_growth=True)\r\nsess_config = tf.ConfigProto(gpu_options=gpu_options)\r\nrun_config = tf.estimator.RunConfig(session_config=sess_config)\r\n\r\ninputs = tf.keras.layers.Input(shape=(10,))\r\noutputs = tf.keras.layers.Dense(10)(inputs)\r\nmodel = tf.keras.models.Model(inputs, outputs)\r\nmodel.compile(optimizer='sgd', loss='mse')\r\nest_keras = tf.keras.estimator.model_to_estimator(keras_model=model, config=run_config)\r\n\r\ninput_name = model.input_names[0]\r\ndata = np.random.rand(1000,10).astype(np.float32)\r\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn({input_name:data}, data, batch_size=10, num_epochs=None, shuffle=False)\r\n\r\ntrain_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=100000)\r\neval_spec = tf.estimator.EvalSpec(input_fn=train_input_fn, steps=10)\r\ntf.estimator.train_and_evaluate(est_keras, train_spec, eval_spec)\r\n```", "comments": ["@fchollet @yifeif May be related to #14504", "@yifeif could you please take a look. ", "@ispirmustafa any idea? The run_config is passed in directly when creating the keras version of the Estimator. Do we need to pass these configurations anywhere else?", "It should not be related with Keras code. It should work since it is handled within Estimator.\r\n@droidicus could you please print est_keras.config and est_keras.config.cluster_spec.as_dict?\r\n", "Sure thing, here is the output:\r\n```python\r\n*********** est_keras.config *************************************\r\n<tensorflow.python.estimator.run_config.RunConfig object at 0x7f7694423fd0>\r\n*********** est_keras.config.cluster_spec.as_dict()  *************\r\n{}\r\n******************************************************************\r\n```\r\n\r\nCode to reproduce (same as the source in the origional issue above, but with print statements after the creation of the keras estimator), and full log output are avaliable here: https://gist.github.com/droidicus/146532eacf88ed57538bb41a8fc7da4b\r\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Gentle ping, this is still an issue for me.", "Hi @shivaniag,\r\nEstimator sends the given session_config directly to the Session constructor. Could you please assign somebody who is more familiar with tf.Session and it's handling of GPU settings?", "FYI, I've checked the keras.model_to_estimator. It's sending the config properly to tf.estimator.Estimator. \r\ntf.estimator.Estimator sends that config to tf.train.SessionManager calls which uses it as a constructor argument to tf.Session.", "Just as an FYI, while this is still a problem in TFv1.5rc0 we were able to do the following as a workaround for now, by setting the default session manually the memory fraction is respected:\r\n```python\r\nimport os\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\r\nsess_config = tf.ConfigProto(gpu_options=gpu_options)\r\n# Manually set the default session instead\r\ntf.Session(config=sess_config).as_default()\r\n#run_config = tf.estimator.RunConfig(session_config=sess_config)\r\n\r\ninputs = tf.keras.layers.Input(shape=(10,))\r\noutputs = tf.keras.layers.Dense(10)(inputs)\r\nmodel = tf.keras.models.Model(inputs, outputs)\r\nmodel.compile(optimizer='sgd', loss='mse')\r\nest_keras = tf.keras.estimator.model_to_estimator(keras_model=model)#, config=run_config)\r\n\r\ninput_name = model.input_names[0]\r\ndata = np.random.rand(1000,10).astype(np.float32)\r\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn({input_name:data}, data, batch_size=10, num_epochs=None, shuffle=False)\r\n\r\ntrain_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=100000)\r\neval_spec = tf.estimator.EvalSpec(input_fn=train_input_fn, steps=10)\r\ntf.estimator.train_and_evaluate(est_keras, train_spec, eval_spec)\r\n```", "@yifeif, @fchollet  is there any place within underlying Keras code that uses default session while building the graph, train_op, ...?  ", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Sorry for the delay. [This](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/_impl/keras/estimator.py#L147) is how model_to_estimator create its model_fn. @ispirmustafa anything you see with session that should be done differently? Thanks!\r\n\r\nAlso tried print out `estimator.config.__dict__` and `estimator.config.cluster_spec.__dict__` for canned estimator, custom estimator and keras converted estimator, and I'm seeing the same results:\r\n`\r\n*********** est_keras.config.__dict__ *************************************\r\n{'_save_checkpoints_secs': 600, '_session_config': gpu_options {\r\n  per_process_gpu_memory_fraction: 0.5\r\n}\r\n, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8e3ebcabd0>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/tmp/tmp8jvdNt', '_save_summary_steps': 100}\r\n*********** est_keras.config.cluster_spec.__dict__  *************\r\n{'_cluster_def': , '_cluster_spec': {}}\r\n******************************************************************\r\n`", "Nagging Assignees @yifeif, @shivaniag, @ispirmustafa: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "A fix has been submitted internally and should make to master tomorrow. Thanks!", "Fantastic, thanks!"]}, {"number": 14775, "title": "tf.set_random_seed doesn't work after any operations have been constructed", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes, just switching the order of this:\r\n```\r\ntf.set_random_seed(1234)\r\na = tf.random_uniform([1])\r\nb = tf.random_normal([1])\r\n```\r\n\r\nto this:\r\n\r\n```\r\na = tf.random_uniform([1])\r\nb = tf.random_normal([1])\r\ntf.set_random_seed(1234)\r\n```\r\n\r\nin this example:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/set_random_seed\r\n\r\nNo longer sets the seed.\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n\r\nBinary via pip\r\n\r\n- **TensorFlow version (use command below)**:\r\n```\r\n$ python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nv1.3.0-rc2-20-g0787eee 1.3.0\r\n```\r\n\r\n- **Python version**: \r\n```\r\n$ python --version\r\nPython 3.6.1\r\n```\r\n\r\n- **Bazel version (if compiling from source)**:\r\nn/a\r\n- **GCC/Compiler version (if compiling from source)**:\r\nn/a\r\n- **CUDA/cuDNN version**:\r\nn/a\r\n- **GPU model and memory**:\r\nn/a\r\n\r\n- **Exact command to reproduce**:\r\n`python tf-test.py`\r\n\r\nwhere tf-test is below:\r\n\r\n\r\n\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\n\r\nIf we would like to deterministically run a tensorflow graph, we want to be able to pass in the seed without rebuilding the graph from scratch (which is slow in our interactive application).\r\n\r\nAlso, this ordering constraint makes it tricky to debug what's going on and no mention is given to the fact that the seed is read in the op _creation_ not _execution_ in the documentation as far as I can tell.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nAs written on example page:\r\n\r\n```\r\n$ python tf-test.py \r\nSession 1\r\n2017-11-21 15:38:24.133822: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-11-21 15:38:24.133854: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n[ 0.96046877]\r\n[ 0.83621562]\r\n[ 0.4987599]\r\n[ 0.54880583]\r\nSession 2\r\n[ 0.96046877]\r\n[ 0.83621562]\r\n[ 0.4987599]\r\n[ 0.54880583]\r\n\r\n\r\n```\r\n\r\n\r\nWith `set_random_seed` after:\r\n```\r\n$ python tf-test.py \r\nSession 1\r\n2017-11-21 15:41:57.602615: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-11-21 15:41:57.602638: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n[ 0.53137994]\r\n[ 0.32236636]\r\n[ 1.07008374]\r\n[ 0.49122357]\r\nSession 2\r\n[ 0.07862437]\r\n[ 0.18420935]\r\n[ 0.76287955]\r\n[ 0.47924194]\r\n```\r\n\r\n\r\n\r\nFull tf-test.py:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\na = tf.random_uniform([1])\r\nb = tf.random_normal([1])\r\n\r\ntf.set_random_seed(1234)\r\n\r\n# Repeatedly running this block with the same graph will generate the same\r\n# sequences of 'a' and 'b'.\r\nprint(\"Session 1\")\r\nwith tf.Session() as sess1:\r\n  print(sess1.run(a))  # generates 'A1'\r\n  print(sess1.run(a))  # generates 'A2'\r\n  print(sess1.run(b))  # generates 'B1'\r\n  print(sess1.run(b))  # generates 'B2'\r\n\r\nprint(\"Session 2\")\r\nwith tf.Session() as sess2:\r\n  print(sess2.run(a))  # generates 'A1'\r\n  print(sess2.run(a))  # generates 'A2'\r\n  print(sess2.run(b))  # generates 'B1'\r\n  print(sess2.run(b))  # generates 'B2'\r\n```", "comments": ["This is working as intended.  Seeds are used during graph construction, so changing them after you've constructed the graph does, indeed, not do anything.", "@ekelsen Hey, thanks for replying. If it's the intent that the seed is remembered in the graph operations and can't be altered after, the documentation could use some clarification.\r\n\r\nIn our use case, we are able to use the `stateless_random_uniform` from contrib and manually pass in the seed in the `feed_dict`, however this diminished the utility of a bunch of built-in operations like [random_brightness](https://www.tensorflow.org/api_docs/python/tf/image/random_brightness), etc.", "@darknoon Thanks for the detailed post.  Indeed the documentation could try to make this more clear - would you like to take a stab at that?", "@darknoon \r\nCan you elaborate on your solution to pass the seed in the feed_dict please for  stateless_random_uniform? Does it actually change the seed that is being used?\r\n\r\nThank you\r\n", "@pierrebaque \r\n\r\n```\r\ndef seeded_random(subseed):\r\n\treturn stateless_random_uniform(shape=random_shape, seed=[seed, subseed])\r\n```\r\n\r\nThe random seed is generated when running the session and passed in the `feed_dict` so it is dynamic but under our control.", "The above mentioned random APIs have been changed into `tf.random.set_seed`, `tf.random.uniform` and `tf.random.normal`.\r\nIn Tensorflow 2, with eager execution by default, the deterministic behavior depends on global level seed and operation level seed. You can find the details for different type of seeds [here](https://www.tensorflow.org/api_docs/python/tf/random/set_seed). \r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 14774, "title": "Load boundaries array into shared memory before hand for `bucketize`", "body": "This fix is a follow up to #13922. This fix loads boundaries array into shared memory before each thread, in order to improve performance for `bucketize` op.\r\n\r\nThe fix is based on feedback (https://github.com/tensorflow/tensorflow/pull/13922#discussion_r150058312).\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["/cc @rmlarsen Please take a look.", "@yongtang can you resolve the merge conflicts.\r\n\r\n@rmlarsen PTAL", "Thanks @sb2nov The PR has been rebased and updated. Please take a look.", "Jenkins, test this please.", "@yongtang Thanks for the optimization!\r\n@tensorflow-jenkins test this please", "Jenkins, test this please.", "@yongtang \r\nIs that possible to support the `boundaries_vector` with `tensor` type, not the `list` ? ", "@guolinke At the moment `boundaries` is passed as an attributes. It is certainly possible to pass it as a `tensor`, though that means we may need to have a `BucketizeV2` op to make it backward compatible. There also might be some implications on GPU implementation so some changes might be needed.\r\n\r\nCan you open a new issue for this feature request?"]}, {"number": 14773, "title": "Upgrade cuda to 9 and cudnn version to 7.", "body": "", "comments": ["@gunan looks like the jenkins GPU failures are new and might be related to the upgrade?", "Looks like there was an eigen bug, we will need #14770 merged, then this change should be good to go.", "@zheng-xq looks like another build issue with cuda 9 crept in. Have you seen this one before?\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-gpu/7826/consoleFull", "Jenkins, test this please.", "@zheng-xq now the transpose_op_test is timing out. Do you think it could be caused by the cuda9 upgrade?", "Jenkins , test this please.", "Feel free to disable some big ones. @yzhwang can take a look afterwards. Thanks!", "Merging, since python3 tests passed on kokoro despite the failure on jenkins.", "Should we remove https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel-gpu-cuda9-cudnn7?", "good idea! I can send a followup PR for that.", "Hi @gunan \r\n\r\nAfter this commit, we'll use Ubuntu 16 for GPU build but Ubuntu 14 for CPU build. This will cause a lot confusions. Can you unify that?", "We still need to continue testing for ubuntu 14, however nvidia docker does not have 14.04 support anymore. So the version skew is partially intentional right now.", "Wait, does it matter for building a Docker image anyway?\r\nYou can still use nvidia-docker 1.0 on trusty, or you can try to use the xenial packages for 2.0, but you will have to setup the new runtime manually.", "All our CI machines are on ubuntu 16.04. But I think the concern is the CPU docker images are still based off of ubuntu 14.04, while the base image for CUDA enabled packages are ubuntu 16.04", "any plan for cuda 9.1?"]}, {"number": 14772, "title": "TFRecordReader keeps files locked after session closes", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:windows 7 64bit\r\n- **TensorFlow installed from (source or binary)**: pip install\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**:-\r\n- **GCC/Compiler version (if compiling from source)**:-\r\n- **CUDA/cuDNN version**:-\r\n- **GPU model and memory**:-\r\n- **Exact command to reproduce**:\r\n\r\nRunning this script (you need some tfrecords from [here][1]):\r\n\r\n```python\r\nimport os\r\nimport shutil\r\nimport sys\r\nimport tempfile\r\n\r\nimport tensorflow as tf\r\n\r\ndata_dir = r'/path/to/tfrecords'\r\n\r\ndef test_generate_tfrecords_from_csv():\r\n    with tempfile.TemporaryDirectory() as tmpdirname:\r\n        filenames = os.listdir(data_dir)\r\n        for f in filenames:\r\n            shutil.copy(os.path.join(data_dir, f), os.path.join(tmpdirname, f))\r\n        filenames = sorted([os.path.join(tmpdirname, f) for f in filenames])\r\n        # Create a queue that produces the filenames to read.\r\n        queue = tf.train.string_input_producer(filenames, num_epochs=1,\r\n                                               shuffle=False)\r\n        with tf.Session() as sess:\r\n            sess.run(tf.local_variables_initializer()) # Local !\r\n            tf.train.start_queue_runners(sess=sess)\r\n            reader = tf.TFRecordReader()\r\n            for j in range(len(filenames)):\r\n                key, value = reader.read(queue)\r\n                features_dict = tf.parse_single_example(value, features={\r\n                    'label': tf.FixedLenFeature([], tf.string),})\r\n                # the decode call below is needed, if you replace it with\r\n                # label = tf.constant(0) no files are locked\r\n                label = tf.decode_raw(features_dict['label'], tf.float32)\r\n                _ = sess.run([label]) # files are locked here\r\n        listdir = os.listdir(tmpdirname)\r\n        print(tmpdirname, listdir)\r\n        for f in sorted(listdir):\r\n            os.remove(os.path.join(tmpdirname, f))\r\n\r\nprint(tf.__version__)\r\nprint(sys.version)\r\ntest_generate_tfrecords_from_csv()\r\n```\r\n\r\n\r\nProduces:\r\n\r\n\r\n```\r\nC:\\_\\Python35>python.exe C:\\Users\\MrD\\.PyCharm2017.2\\config\\scratches\\so_46259067.py\r\n1.4.0\r\n3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)]\r\nC:\\Users\\MrD\\AppData\\Local\\Temp\\tmp3hqhkgy0 ['img_2013-01-01-00-00.tfrecords', 'img_2013-01-01-00-01.tfrecords', 'img_2013-01-01-00-02.tfrecords']\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\MrD\\.PyCharm2017.2\\config\\scratches\\so_46259067.py\", line 38, in <module>\r\n    test_generate_tfrecords_from_csv()\r\n  File \"C:\\Users\\MrD\\.PyCharm2017.2\\config\\scratches\\so_46259067.py\", line 34, in test_generate_tfrecords_from_csv\r\n    os.remove(os.path.join(tmpdirname, f))\r\n  File \"C:\\_\\Python35\\lib\\tempfile.py\", line 808, in __exit__\r\n    self.cleanup()\r\n  File \"C:\\_\\Python35\\lib\\tempfile.py\", line 812, in cleanup\r\n    _shutil.rmtree(self.name)\r\n  File \"C:\\_\\Python35\\lib\\shutil.py\", line 488, in rmtree\r\n    return _rmtree_unsafe(path, onerror)\r\n  File \"C:\\_\\Python35\\lib\\shutil.py\", line 383, in _rmtree_unsafe\r\n    onerror(os.unlink, fullname, sys.exc_info())\r\n  File \"C:\\_\\Python35\\lib\\shutil.py\", line 381, in _rmtree_unsafe\r\n    os.unlink(fullname)\r\nPermissionError: [WinError 5] Access is denied: 'C:\\\\Users\\\\MrD\\\\AppData\\\\Local\\\\Temp\\\\tmp3hqhkgy0\\\\img_2013-01-01-00-02.tfrecords'\r\n```\r\n\r\n(I had also asked at stack overflow [here](https://stackoverflow.com/questions/46259067/tfrecordreader-keeps-files-locked-after-session-closes). Unless I am doing something stupid shouldn't the tfrecord file be free for deleting after the session closes ? Do I have to explicitly close it (is it even possible) ?\r\n\r\nThe equivalent dataset code has the same issue:\r\n\r\n```python\r\ndef test_generate_tfrecords_from_csv_dataset():\r\n    with tempfile.TemporaryDirectory() as tmpdirname:\r\n        filenames = os.listdir(data_dir)\r\n        for f in filenames:\r\n            shutil.copy(os.path.join(data_dir, f), os.path.join(tmpdirname, f))\r\n        filenames = sorted([os.path.join(tmpdirname, f) for f in filenames])\r\n        def _parse_rec(value):\r\n            features_dict = tf.parse_single_example(value, features={\r\n                    'label': tf.FixedLenFeature([], tf.string),})\r\n            # return tf.constant(0, tf.float32)  # files are locked all the same\r\n            return tf.decode_raw(features_dict['label'], tf.float32)\r\n        dataset = tf.data.TFRecordDataset(filenames).map(_parse_rec)\r\n        get_next = dataset.make_one_shot_iterator().get_next\r\n        with tf.Session() as sess:\r\n            for j in range(len(filenames)):\r\n                label = get_next()\r\n                _ = sess.run([label]) # files are locked here\r\n        listdir = os.listdir(tmpdirname)\r\n        print(tmpdirname, listdir)\r\n        for f in sorted(listdir):\r\n            os.remove(os.path.join(tmpdirname, f))\r\n```\r\n\r\nIt seems in both cases it locks the last file - the others are removed ok.\r\n\r\n  [1]: https://www.dropbox.com/sh/wrx8pv546rq4iev/AACER-9HbMxE6T3w9hJdieLCa?dl=0", "comments": ["OK, it looks like it's a problem in the TFRecordReader. I glanced at the code of the reader itself and it seems fine. Would you mind putting a few `LOG(INFO)` statements to see if the lock reset code gets called, and if not, why?", "Thanks for the response @drpngx - where should I add those statements ?", "I would start here:\r\nhttps://github.com/tensorflow/tensorflow/blob/b07791f6e9b306937eb58f7bb6c3300cd26583af/tensorflow/core/kernels/tf_record_reader_op.cc#L47", "This means I will have to recompile tf - I have no time for this now, hopefully soon", "Sounds good, thanks!\r\n/CC @josh11b ", "It is still an issue", "Could you provide more information by printing out what the call sequences look like? Is `OnWorkFinishedLocked` called and if not, why not?", "Sorry I have no time to look into the C++ code now.", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "Any update here?"]}, {"number": 14771, "title": "bazel 0.5.4 says \"ERROR: infinite symlink expansion detected\"", "body": "Hi,\r\n\r\nI just tried to update my build script (that worked perfectly fine on 1.2.1 and 1.3.0) and it fails with a not-understandable error.\r\nEverything is exactly the same, build is done in a clean ephemeral environment. The only difference is that I updated bazel to 0.5.4 per TF configure script request...\r\n\r\n```\r\n____Loading package: @boringssl//\r\n____Loading package: @org_python_license//\r\n____Loading package: tensorflow/compiler/xla\r\n____Loading package: tensorflow/core/kernels/neon\r\n____Downloading http://www.sqlite.org/2017/sqlite-amalgamation-3200000.zip: 41,040 bytes\r\n____Downloading http://www.sqlite.org/2017/sqlite-amalgamation-3200000.zip: 98,480 bytes\r\n____Downloading http://www.sqlite.org/2017/sqlite-amalgamation-3200000.zip: 288,648 bytes\r\n____Downloading http://www.sqlite.org/2017/sqlite-amalgamation-3200000.zip: 430,920 bytes\r\n____Downloading http://www.sqlite.org/2017/sqlite-amalgamation-3200000.zip: 584,136 bytes\r\n____Downloading http://www.sqlite.org/2017/sqlite-amalgamation-3200000.zip: 746,928 bytes\r\n____Downloading http://www.sqlite.org/2017/sqlite-amalgamation-3200000.zip: 918,008 bytes\r\n____Downloading http://www.sqlite.org/2017/sqlite-amalgamation-3200000.zip: 1,090,296 bytes\r\n____Downloading http://www.sqlite.org/2017/sqlite-amalgamation-3200000.zip: 1,254,456 bytes\r\n____Downloading http://www.sqlite.org/2017/sqlite-amalgamation-3200000.zip: 1,363,896 bytes\r\n____Downloading http://www.sqlite.org/2017/sqlite-amalgamation-3200000.zip: 1,545,840 bytes\r\n____Downloading http://www.sqlite.org/2017/sqlite-amalgamation-3200000.zip: 1,716,840 bytes\r\n____Downloading http://www.sqlite.org/2017/sqlite-amalgamation-3200000.zip: 1,838,592 bytes\r\n____Downloading http://www.sqlite.org/2017/sqlite-amalgamation-3200000.zip: 2,023,272 bytes\r\n____Loading package: @sqlite_archive//\r\n____Loading package: tensorflow/core/kernels/hexagon\r\n____Loading package: tensorflow/compiler/xla/legacy_flags\r\nERROR: infinite symlink expansion detected\r\n[start of symlink chain]\r\n/build/python-tensorflow-cuda-1.4.0/.cache/bazel/_bazel_pbuilder/436710022b7d9d872ccd97b57710586f/external/org_tensorflow\r\n/build/python-tensorflow-cuda-1.4.0\r\n[end of symlink chain]\r\n.\r\nERROR: /build/python-tensorflow-cuda-1.4.0/.cache/bazel/_bazel_pbuilder/436710022b7d9d872ccd97b57710586f/external/llvm/BUILD:186:1: no such package '@org_tensorflow//third_party/llvm': Could not access /build/python-tensorflow-cuda-1.4.0/.cache/bazel/_bazel_pbuilder/436710022b7d9d872ccd97b57710586f/external/org_tensorflow: Infinite symlink expansion and referenced by '@llvm//:abi_breaking_gen'.\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\r\n```\r\nAnd the configure-script:\r\n\r\n```\r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3.4\r\n\r\nFound possible Python library paths:\r\n  /usr/local/lib/python3.4/dist-packages\r\n  /usr/lib/python3/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python3.4/dist-packages]\r\n/usr/lib/python3/dist-packages/\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: Y\r\njemalloc as malloc support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: Y\r\nGoogle Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: Y\r\nHadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: Y\r\nAmazon S3 File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: Y\r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: Y\r\nGDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: Y\r\nVERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL support? [y/N]: N\r\nNo OpenCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: N\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: N\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: -mavx -msse4.1 -msse4.2\r\n```", "comments": ["Hi,\r\n\r\nThat's definetely a bazel issue. The user building package had no valid home and thus $HOME was set to TF source tree.\r\nFor some reason that confuses bazel (worked with 0.5.2) so I forced $HOME to be set to /tmp/temp.randomstuff and it works now.\r\n\r\nClosing as it's not TF related obviously."]}, {"number": 14770, "title": "Update Eigen hash for fix of fp16 predux bug", "body": "Attention: @benoitsteiner and @zheng-xq \r\n\r\nFor Maxwell and earlier GPUs, Eigen was incorrectly casting fp16 values to\r\nunsigned int during some reductions. This causes incorrect results in\r\nTensorflow's xent and sparse_xent ops when applied to fp16 data.", "comments": ["Can one of the admins verify this patch?", "Makefile issue should be fixed now (mirrored the tarball).\r\nHowever, GPU python3 issue seems to be related to this change.\r\n```\r\nFAIL: testHalf (__main__.SparseXentTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/tensorflow/python/kernel_tests/sparse_xent_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/sparse_xent_op_test.py\", line 184, in testHalf\r\n    np.array([3, 0]).astype(label_dtype))\r\n  File \"/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/tensorflow/python/kernel_tests/sparse_xent_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/sparse_xent_op_test.py\", line 70, in _testXent\r\n    self.assertAllCloseAccordingToType(np_loss, tf_loss)\r\n  File \"/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/tensorflow/python/kernel_tests/sparse_xent_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1120, in assertAllCloseAccordingToType\r\n    self.assertAllClose(a, b, rtol=rtol, atol=atol)\r\n  File \"/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/tensorflow/python/kernel_tests/sparse_xent_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1084, in assertAllClose\r\n    self._assertArrayLikeAllClose(a, b, rtol=rtol, atol=atol)\r\n  File \"/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/tensorflow/python/kernel_tests/sparse_xent_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1054, in _assertArrayLikeAllClose\r\n    np.testing.assert_allclose(a, b, rtol=rtol, atol=atol, err_msg=msg)\r\n  File \"/usr/local/lib/python3.4/dist-packages/numpy/testing/utils.py\", line 1395, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File \"/usr/local/lib/python3.4/dist-packages/numpy/testing/utils.py\", line 778, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=0.001, atol=0.001\r\nNone\r\n(mismatch 100.0%)\r\n x: array([ 1.386719,  3.441406], dtype=float16)\r\n y: array([ 18656.,  19024.], dtype=float16)\r\n```", "I found the underlying issue. CUDA 8 defines __float2half_rn as returning an unsigned short rather than a __half as CUDA 9 does. I issued a second PR to Eigen to replace __float2half_rn with __float2half (which returns a __half in both 8 and 9). Once that is accepted, I'll update this PR with the correct Eigen revision.", "@tfboyd ", "Eigen PR merged and hash updated in PR. CUDA 8 tests should now pass.", "@tensorflow-jenkins test this please. Thanks @nluehr ", "Jenkins, test this please.", "Hi Folks,\r\nAfter this commit when I try to run tensorflow/contrib/lite/download_dependencies.sh it gives following error:\r\ndownloading https://mirror.bazel.build/bitbucket.org/eigen/eigen/get/b6e6d0cf6a77.tar.gz\r\ntar: Unrecognized archive format\r\ntar: Error exit delayed from previous errors.\r\n\r\nI rolled back one commit to be able to successfully run it.\r\n\r\nRegards,\r\nHovhannes", "Sorry, the issue was because I forgot to mirror the eigen tarball. IT should be fixed now."]}, {"number": 14769, "title": "Type Serialization in as_graph_def function", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**:  v1.3.0-rc2-20-g0787eee\r\n- **Python version**: 2.7.14\r\n- **Bazel version (if compiling from source)**: 0.7\r\n- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 8.1.0 (clang-802.0.42)\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nProblem is that as_graph_def sometimes serialize the type information and sometimes doesn't.\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\n\r\n# bool typed Op, no type serialized\r\nx = tf.placeholder(tf.bool)\r\ny = tf.placeholder(tf.bool)\r\nop = tf.logical_or(x, y)\r\nprint op.graph.as_graph_def(add_shapes=True)\r\n\r\n# float typed Op, type serialized\r\nx = tf.placeholder(tf.float32)\r\ny = tf.placeholder(tf.float32)\r\nop = tf.add(x, y)\r\nprint op.graph.as_graph_def(add_shapes=True)\r\n```\r\n\r\nnode for logical_or is, note no `T` in attr:\r\n```\r\nnode {\r\n  name: \"LogicalOr\"\r\n  op: \"LogicalOr\"\r\n  input: \"Placeholder\"\r\n  input: \"Placeholder_1\"\r\n  attr {\r\n    key: \"_output_shapes\"\r\n    value {\r\n      list {\r\n        shape {\r\n          unknown_rank: true\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nnode for add is, note with `T` in attr:\r\n```\r\nnode {\r\n  name: \"Add\"\r\n  op: \"Add\"\r\n  input: \"Placeholder_2\"\r\n  input: \"Placeholder_3\"\r\n  attr {\r\n    key: \"T\"\r\n    value {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n  attr {\r\n    key: \"_output_shapes\"\r\n    value {\r\n      list {\r\n        shape {\r\n          unknown_rank: true\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\nIf you print out `print tf.logical_or(x, y)`, the output is \r\n```\r\nTensor(\"LogicalOr_1:0\", dtype=bool)\r\n```\r\nSo problem might be with the serialization?\r\nThis seems like a bug to me.\r\n", "comments": ["This is because `tf.logical_or` is only defined for the `bool` type, so there's no need to serialize extra type information into the GraphDef.", "Hi,\r\nI am new to this issue. \r\nI want to serialize my Resnet101 based model.\r\nI am using the script optimize_for_inference.py, I don't know how to edit this script for Resnet101 model.\r\nWhen I run this script, I get the following errors: \r\n\r\n```\r\n/home/arun/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nTraceback (most recent call last):\r\n  File \"optimize_for_inference.py\", line 146, in <module>\r\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/arun/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 124, in run\r\n    _sys.exit(main(argv))\r\n  File \"optimize_for_inference.py\", line 90, in main\r\n    FLAGS.output_names.split(\",\"), FLAGS.placeholder_type_enum)\r\n  File \"/home/arun/anaconda3/lib/python3.6/site-packages/tensorflow/python/tools/optimize_for_inference_lib.py\", line 109, in optimize_for_inference\r\n    placeholder_type_enum)\r\n  File \"/home/arun/anaconda3/lib/python3.6/site-packages/tensorflow/python/tools/strip_unused_lib.py\", line 83, in strip_unused\r\n    raise KeyError(\"The following input nodes were not found: %s\\n\" % not_found)\r\nKeyError: \"The following input nodes were not found: {'input'}\\n\"\r\n```\r\nI want to make this script work to serialize my Resnet model. I need help, if someone can help me.\r\n\r\nKind Regards\r\nArun"]}, {"number": 14768, "title": "Feature request: Control order of 'feature_column.input_layer'", "body": "On master (80e7c9f45c).\r\n\r\nIt seems that the mapping of features to columns in dense the input matrix is always sorted by the alphabetical order of the feature names. It would be nice is this was customizable, perhaps by respecting the order of the feature columns in the second argument to `input_layer`. Mainly useful  for debugging and introspecting the network to know which columns correspond to which features.\r\n\r\neg:\r\n\r\n```python\r\nsess.run(tf.feature_column.input_layer({'a': [1], 'b': [2]}, [tf.feature_column.numeric_column('a'), tf.feature_column.numeric_column('b')]))\r\n```\r\n\r\ngives `[1, 2`],\r\n\r\nas does switching the order of the feature columns:\r\n\r\n```python\r\nsess.run(tf.feature_column.input_layer({'a': [1], 'b': [2]}, [tf.feature_column.numeric_column('b'), tf.feature_column.numeric_column('a')]))\r\n```\r\n\r\nI also tried giving the features as an `OrderedDict`, but `input_layer` doesn't seem to care about the ordering in that  situation either. \r\n", "comments": ["Yes, `feature_columns`'s order is rearranged:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/b5df90f91cde6eb12af9cbe818bd2cf4a9bcc687/tensorflow/python/feature_column/feature_column.py#L224\r\n\r\nHowever, I think your request is reasonable. Say, we have an `index` column and want to let it  always stay at the first. \r\nCan we remove the `sorted` or add a parameter to control the behavior, @ispirmustafa ?", "We sort feature columns to make order of variables/ops deterministic. Each worker should create them in same order. we cannot know the given fc order is deterministic or not.", "Because feature_columns accepts an iterator, which can be either ordered (say,  list ) or unordered (say, set or map), so we always sorted the feature columns by name, right? ", "yes.", "I still think it would be nice to have some kind of interface for exposing the relationship between columns and features to users, even if it's not user-specifiable. ", "I think it's doable without changing order. Currently we're returning vars related to FCs via cols_to_vars. We can similarly add an argument such as cols_to_slices. ", "I'm afraid that cols_to_slices is a little complicated. At least for tuple, it seems safe to respect its iterator's order."]}, {"number": 14767, "title": "Fixed typo in usage docstring", "body": "Changed tf.SyncReplicasOptimizer to tf.train.SyncReplicasOptimizer.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 14766, "title": "Revert \"Fixed typo in usage docstring\"", "body": "Reverts tensorflow/tensorflow#14765\r\n\r\nSorry didn't realize this was sent to r1.2. @millskyle please fix the issue in master. Thanks.", "comments": ["Done. #14767. Sorry about that.\r\n"]}, {"number": 14765, "title": "Fixed typo in usage docstring", "body": "Changed tf.SyncReplicasOptimizer to tf.train.SyncReplicasOptimizer in usage example.", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Can one of the admins verify this patch?", "I signed it.", "CLAs look good, thanks!\n\n<!-- ok -->", "Thanks @millskyle !"]}, {"number": 14764, "title": "./", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 14763, "title": "MKL: Adding MKL-DNN graph pass implementation", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please.", "@mahmoud-abuzaina can you take a quick look at the merge conflicts\r\n", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 14762, "title": "tf.TensorShape concatenation converts shape information to values", "body": "Tensorflow: 1.4.0\r\n\r\nExample code:\r\n```python\r\nimport tensorflow as tf\r\ntf.InteractiveSession()\r\n>>> tf.concat([tf.TensorShape([4,1]), tf.constant([1,1,1,1])], 0).eval()\r\narray([4, 1, 1, 1, 1, 1])\r\n```\r\n\r\nIs it intended that this works and produces the output where the shape information gets converted into actual values?", "comments": ["Yes, this is intentional. `tf.concat()` expects a list of `tf.Tensor` objects as its argument, and it will implicitly convert a `tf.TensorShape` to a 1-D `tf.Tensor` if possible.\r\n\r\nIf you want to concatenate shapes and retain their shape-ness, use `tf.TensorShape([4, 1]).concatenate([1, 1, 1, 1])`.", "Actually I would have like a the following bevavior:\r\n`array([1,1,1,1])` where `tf.TensorShape` denotes an \"empty\" shape that can be concatenated to a `tf.Tensor` to receive that Tensor again if shapes match. This would allow me to `tf.concat` in e.g. a `tf.while_loop` while having nothing to concatenate with in the first loop iteration.\r\n\r\nIs there a reason as to why the shape gets converted to values? It seems pretty inconsistent when the values either denote a shape or values. I cannot find anything in the documentation about it.", "Yes, shapes get converted to values because there are several APIs (e.g. `tf.reshape()`, `tf.fill()`, `tf.ones()`, `tf.zeros()`) that expect a `tf.Tensor` representing a shape, and it is convenient to be able to pass a `tf.TensorShape` directly to these APIs without an explicit conversion to `tf.Tensor`.\r\n\r\nI'm not sure what `tf.TensorShape([4, 1])` would denote an empty shape, but it does not sound like the API you're describing matches the specification of `tf.concat()`. You can create empty tensors using `tf.constant()` and use these as the initial value for loop variables.", "I see, then I misunderstood the meaning of a `tf.TensorShape`. How do I get an empty `tf.constant`? It seems to require a `value`.", "The easiest way is to pass an empty list as the value, and an explicit `shape` argument with at least one dimension set to 0."]}, {"number": 14761, "title": "tensorflow lite: error when convert frozen model to lite format", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 14.04\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n1.3.0\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n0.7.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4\r\n- **CUDA/cuDNN version**:\r\ncuda8.0/cudnn6.0\r\n\r\n\r\nI tried to convert squeezenet frozen model to lite format with the following command:\r\n\"bazel run --config=opt tensorflow/contrib/lite/toco:toco -- --input_file=/home/xxx/caffe-tensorflow/npy2ckpt/squeezenet/frozen_model.pb --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE --output_file=/home/xxx/caffe-tensorflow/npy2ckpt/squeezenet/squeezenet.lite --inference_type=FLOAT --input_type=FLOAT --input_arrays=input --output_arrays=prob --input_shapes=1,227,227,3\"\r\n\r\nthe output is shown below:\r\n2017-11-21 18:35:29.977505: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 170 operators, 231 arrays (0 quantized)\r\n2017-11-21 18:35:29.981856: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 40 operators, 93 arrays (0 quantized)\r\n2017-11-21 18:35:29.982061: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 40 operators, 93 arrays (0 quantized)\r\n2017-11-21 18:35:29.982201: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:312] Total transient array allocated size: 4071680 bytes, theoretical optimal value: 4071680 bytes.\r\n2017-11-21 18:35:29.982317: I tensorflow/contrib/lite/toco/toco_tooling.cc:255] Estimated count of arithmetic ops: 0.781679 billion (note that a multiply-add is counted as 2 ops).\r\n2017-11-21 18:35:29.982482: F tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: Squeeze\r\n\r\nThen I tried to convert mobilenet_v1_1.0_224.pb to lite format, the same error as above.\r\n\"bazel run --config=opt tensorflow/contrib/lite/toco:toco -- --input_file=/home/xxx/Downloads/freeze_mobilenet/MobileNet/img224/mobilenet_v1_1.0_224.pb --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE --output_file=/home/xxx/Downloads/freeze_mobilenet/MobileNet/img224/mobilenet.lite --inference_type=FLOAT --input_type=FLOAT --input_arrays=input --output_arrays=output --input_shapes=1,224,224,3\"\r\n\r\noutput:\r\n2017-11-21 22:07:39.747095: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 418 operators, 584 arrays (0 quantized)\r\n2017-11-21 22:07:39.766175: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 31 operators, 88 arrays (0 quantized)\r\n2017-11-21 22:07:39.766390: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 31 operators, 88 arrays (0 quantized)\r\n2017-11-21 22:07:39.766592: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:312] Total transient array allocated size: 6422528 bytes, theoretical optimal value: 4816896 bytes.\r\n2017-11-21 22:07:39.766751: I tensorflow/contrib/lite/toco/toco_tooling.cc:255] Estimated count of arithmetic ops: 1.14264 billion (note that a multiply-add is counted as 2 ops).\r\n2017-11-21 22:07:39.766952: F tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: Squeeze\r\n\r\nAlthough I installed tensorflow with \"pip install tensorflow-gpu\", in order to convert model to lite format, I git clone the tensorflow files and  configure, bazel to compile the files. I don't know whether this affect the converting of models, but the error is really strange!\r\n", "comments": ["For now, `Squeeze` is supported only when it can be fused into other ops. \r\n\r\nPlease refer to [this doc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/tf_ops_compatibility.md) to see the ops compatibility. The team is working on supporting more ops in Toco and hopefully this will be supported in the near future. ", "Thanks for the info @miaout17, is ssd-mobilenet supported please? I cannot tell based on that document.", "The model `ssd-mobilenet` is currently not supported. The team is working on supporting more ops and models.\r\n\r\nRegarding `mobilenet_v1_1.0_224`, feel free to reference [this doc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/README.md#freeze-graph) for an example on converting an available mobilenet_v1_1.0_224 model using toco.\r\n\r\nA pretrained frozen mobilenet_v1_1.0_224 can be found here: https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_0.25_128_frozen.tgz. Additionally, the list of available tflite models is found in [this doc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md).", "Now that I convert the mobilenet model to lite format failed, how tensorflow offered us the converted mobilenet lite model on the github? @miaout17 ", "As @gargn mentioned, we haven't yet provided an example of Mobilenet SSD, only Mobilenet classification.  We will likely provide support for that in the future. If you are adventurous, we are happy to help you along.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "when the  ssd-mobilenet can be supported by tflite ?", "@aselle any update on mobilenet SSD support? ", "or any anitcipated release date for this feature? I see you added \"contribution welcome\" tag, does it mean you are not planning developing any object detection support in-house?", "how to fuse squeeze to other ops? @miaout17 "]}]