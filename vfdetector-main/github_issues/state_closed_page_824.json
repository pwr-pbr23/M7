[{"number": 28812, "title": "Signature defs lost when calling saved_model_cli convert", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n```\r\nYes.  Using saved_model_cli tool.\r\n```\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n```\r\ncat /etc/os-release \r\nNAME=\"Red Hat Enterprise Linux Server\"\r\nVERSION=\"7.6 (Maipo)\"\r\nID=\"rhel\"\r\nID_LIKE=\"fedora\"\r\nVARIANT=\"Server\"\r\nVARIANT_ID=\"server\"\r\nVERSION_ID=\"7.6\"\r\nPRETTY_NAME=\"Red Hat Enterprise Linux Server 7.6 (Maipo)\"\r\nANSI_COLOR=\"0;31\"\r\nCPE_NAME=\"cpe:/o:redhat:enterprise_linux:7.6:GA:server\"\r\nHOME_URL=\"https://www.redhat.com/\"\r\nBUG_REPORT_URL=\"https://bugzilla.redhat.com/\"\r\n\r\nREDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 7\"\r\nREDHAT_BUGZILLA_PRODUCT_VERSION=7.6\r\nREDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\"\r\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7.6\"\r\n```\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n```\r\nSource\r\n```\r\n- TensorFlow version (use command below):\r\n```\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n2019-05-17 20:37:38.121490: I tensorflow/stream_executor/platform/default/dso_loader.cc:43] Successfully opened dynamic library libcudart.so.10.1\r\nv1.12.1-142-gcd15418 1.14.0-rc0\r\n```\r\n- Python version:\r\n```\r\npython --version\r\nPython 3.6.8 :: Anaconda, Inc.\r\n```\r\n- Bazel version (if compiling from source):\r\n```\r\n0.24.1\r\n```\r\n- GCC/Compiler version (if compiling from source):\r\n```\r\n7.3.0\r\n```\r\n- CUDA/cuDNN version:\r\n```\r\ncudatoolkit               10.1.152 \r\ncudnn                     7.5.1_10.1 \r\n```\r\n- GPU model and memory:\r\n```\r\nTesla V100-SXM2\r\n16130MiB\r\n```\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n```\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n2019-05-17 20:37:38.121490: I tensorflow/stream_executor/platform/default/dso_loader.cc:43] Successfully opened dynamic library libcudart.so.10.1\r\nv1.12.1-142-gcd15418 1.14.0-rc0\r\n```\r\n\r\n**Describe the current behavior**\r\n\r\nThe \"predict\" signature definition is lost when running the `saved_model_cli convert` command against a resnet model.\r\n\r\n**Describe the expected behavior**\r\nBoth the \"predict\" signature, and the \"serving_default\" signature definitions should be in the saved_model.\r\n\r\nCode was removed near these lines in trt_convert.py when the code was restructured in the r1.14 branch:\r\nhttps://github.com/tensorflow/tensorflow/blob/6b657e327cc9ca6e2906ade3e4877e3303065d70/tensorflow/python/compiler/tensorrt/trt_convert.py#L254\r\n\r\nThe original code in the v1.13.1 tagged version looped through all of the signature definitions:\r\nhttps://github.com/tensorflow/tensorflow/blob/6612da89516247503f03ef76e974b51a434fb52e/tensorflow/contrib/tensorrt/python/trt_convert.py#L292\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nThe following steps will show the issue with the converted graph:\r\n```\r\nmkdir -p $HOME/saved-models\r\ncd $HOME/saved-models\r\nwget http://download.tensorflow.org/models/official/20181001_resnet/savedmodels/resnet_v1_fp16_savedmodel_NCHW.tar.gz\r\ntar --no-same-owner -xzvf resnet_v1_fp16_savedmodel_NCHW.tar.gz\r\n\r\ncd $HOME\r\nmkdir -p $HOME/inference-models/resnet_v1_50_fp16\r\n\r\nsaved_model_cli convert --dir $HOME/saved-models/resnet_v1_fp16_savedmodel_NCHW/1538686290 --output_dir $HOME/inference-models/resnet_v1_50_fp16/cli --tag_set serve tensorrt --precision_mode FP16 --max_batch_size 1 --is_dynamic_op False\r\n\r\nsaved_model_cli show --dir $HOME/saved-models/resnet_v1_fp16_savedmodel_NCHW/1538686290 --all # Will show \"predict\" saved model signature definition\r\n\r\nsaved_model_cli show --dir $HOME/inference-models/resnet_v1_50_fp16/cli --all # The \"predict\" signature definition does not exist in the converted model\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["This is intended behavior, since the TRT converter is designed to convert only one signature at a time. If you want to convert two signatures you'll need to run the converter twice by specifying different [`input_saved_model_signature_key`](https://github.com/tensorflow/tensorflow/blob/6b657e327cc9ca6e2906ade3e4877e3303065d70/tensorflow/python/compiler/tensorrt/trt_convert.py#L1066)s (which is not supported by saved_model_cli yet). That will result in two different SavedModels, which is also the behavior for TF2.0.\r\n\r\nIf this doesn't satisfy your requirement, would you please let us know and provide a detailed use case? Thanks.", "The answer only covers the Tensorflow 2.0 case.  The problem was reported against Tensorflow 1.14.0.  If this answer is also for Tensorflow 1.14.0, then this is a breakage between Tensorflow 1.13.1 and Tensorflow 1.14.0.  In Tensorflow 1.13.1, all signatures were converted.  Why would that behavior be changed in Tensorflow 1.14.0?  Thanks.", "@JonTriebenbach that is the answer for both 2.0 and 1.14. I'm sorry for the inconvenience, but this is actually not a breakage, since in 1.13 tensorrt is in contrib/ and which doesn't have [backward compatibility guarantee](https://www.tensorflow.org/guide/version_compat). In 1.14 we change many things of tensorrt integration including many of the APIs in order to move it to TF core, and this is part of those changes. \r\n\r\nAlso, would you please let us know your use case if this change is a blocker for you? Thanks a lot.", "The problem is that without the input_saved_model_signature_key support, which is currently missing, the convert tool chooses the last signature, the \"serving_default\" signature.  It appears the serving_default signature is compatible and will work in the pre-trained models I have downloaded, but I wonder if that is true for every multi-headed model?", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28812\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28812\">No</a>\n"]}, {"number": 28811, "title": "Tensorflow matmul of float64's behaves like numpy float32 matmul", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6.4\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: Titan Xp\r\n\r\n\r\n**Describe the current behavior**\r\nMultiplying two tf.float64s together is giving me a substantial error over multiplying two np.float64s together. \r\n\r\n**Describe the expected behavior**\r\nnp.matmul(a, b) should be very close to tf.matmul(a, b) when a, b are both float64s. This issue is compounding over a spectrogram preprocessing pipeline making tensorflow unusable for pushing my signal preprocessing from numpy into tensorflow. \r\n\r\n**Code to reproduce the issue**\r\n```\r\na = tf.placeholder(tf.float64, [None, None])\r\nb = tf.placeholder(tf.float64, [None, None])\r\n\r\nres_tf = tf.matmul(a, b)\r\n\r\nin1 = np.random.uniform(size=(998, 257))\r\nin2 = np.random.uniform(size=(257, 80))\r\n\r\nprint(in1.dtype)\r\nprint(in2.dtype)\r\n\r\nnumpy_result_f64 = np.matmul(in1, in2)\r\nnumpy_result_f32 = np.matmul(in1.astype(np.float32), in2.astype(np.float32))\r\n\r\nwith tf.Session() as sess:\r\n    tensorflow_result = sess.run(res_tf, feed_dict={a: in1, b: in2})\r\n    \r\nprint(((numpy_result_f64 - tensorflow_result) / numpy_result_f64).max())\r\nprint(((numpy_result_f32 - tensorflow_result) / numpy_result_f32).max())\r\n```\r\nOut:\r\n```\r\nfloat64\r\nfloat64\r\n0.16405904009391822\r\n9.08087494225598e-07\r\n```\r\nThis has the same issue whether the code is run on GPU or CPU. \r\n\r\nThis issue does not appear on tf 1.12. ", "comments": ["@PCerles Able to reproduce the issue with the code provided here\r\n\r\nfloat64\r\nfloat64\r\n2.1063637390863436e-15\r\n9.67467215661267e-07", "@PCerles My output is different from yours. My output shows that tensorflow `matmul` of float64 behaves like numpy's float64 matmul than numpy's float32 matmul.\r\n\r\nPlease take a look at the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/819210b965e4007df38b405e416079dd/untitled201.ipynb). This gist has your code and my output. I have tried gpu and cpu, both produce results as expected. Thanks!\r\n\r\nOutput is as follows\r\n```\r\n1.966516285169991e-15\r\n9.296283790026668e-07\r\n```", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28811\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28811\">No</a>\n"]}, {"number": 28810, "title": "[ROCm] Fix for the broken `--config=rocm` build.", "body": "The --config=rocm build was broken by the following commit.\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/1fde31352238505a45667b196d7d2ebc527c5d3e\r\n\r\nThe changes made by the above commit were incomplete for the ROCm platform, which was leading to the build failure.\r\n\r\n----------------------------\r\n\r\n@tatianashp , @deven-amd, @parallelo, @chsigg FYI\r\n\r\nPlease approve and merge. As with other such PRs, changes here are only applicable for --config=rocm build.\r\n\r\nThanks.", "comments": []}, {"number": 28809, "title": "Update nvptx_compiler.cc", "body": "", "comments": []}, {"number": 28808, "title": "Add CombinedNMS FakeGPU op", "body": "Move FakeGPU CombinedNonMaxSuppression op from #28745 to this PR", "comments": ["@aaroey Did this make into 1.14?", "@samikama unfortunately no, and TRT 5.1 is not in 1.14 yet. Given the situation, I'm not sure if this is the best approach and whether we want a real implementation of CombinedNMS, so I'll delay the review of this one until we figure out. Sorry about that.", "@aaroey I understand. I don't like the solution either but due to discussion in #28051, I had a feeling that it is an acceptable practice.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 28807, "title": "TensorFlow Lite InterpreterBuilder::ParseTensors incorrect for big-endian machines", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2 LTS (GNU/Linux 4.15.0-45-generic s390x)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.13.0\r\n- Python version: Python 2.7.15rc1 (default, Nov 12 2018, 14:31:15)\r\n- Bazel version (if compiling from source): N/A (tflite build via lite/tools/make/Makefile)\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nAny attempt to run one of the TF Lite utilities (e.g. label_image, benchmark_model) or use the interpreter via an import in Python will generate error messages at time of loading a FlatBuffers tflite model file.\r\n\r\n**Describe the expected behavior**\r\nTensorFlow Lite should correctly read a FlatBuffers tflite file and build an internal model structure for its interpreter without any errors.\r\n\r\n**Code to reproduce the issue**\r\nA mere inspection of the model.cc source code (in directory tensorflow/lite) will show that the InterpreterBuilder::ParseTensors function assigns a FlatBuffers buffer pointer directly to the TF Lite tensor's object data field. (See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/model.cc line 350).\r\nThis will cause any multi-byte element of a tensor to be incorrectly interpreted since FlatBuffers use strictly little-endian for all numeric values.\r\n\r\n**Other info / logs**\r\nThe offending source line is line 390:\r\n*buffer_data = reinterpret_cast<const char*>(array->data());\r\n\r\nA tentative solution needs a two-fold change:\r\n1. The tensor type must be used to learn how to interpret the char data and this data must be byte swapped accordingly. FlatBuffers already provides a utility function for that:\r\nflatbuffers::EndianScalar(). Unfortunately this requires a loop over all data in strides of the size of a tensor element. But need only be done once during model build.\r\n\r\nExample of correctly handling 32-bit integers:\r\n<pre>\r\n            *buffer_size = size;                                                                \r\n            *buffer_data = reinterpret_cast&lt;const char*&gt;(array->data());                                                                   \r\n            switch (type) {                                                                     \r\n            case kTfLiteInt32: {                                                                \r\n              int32_t *p = reinterpret_cast&lt;int32_t*&gt;(const_cast&lt;uint8_t*&gt;(array->data()));     \r\n              for (size_t i = 0; i &lt; size; i+=4, ++p)                                           \r\n                *p = flatbuffers::EndianScalar(*p);                                             \r\n              break;                                                                            \r\n            }                                                                                   \r\n</pre>\r\n\r\n2. The FlatBuffers buffer must be writable. One must drop the const specifier and moreover since the tflite file is memory-mapped the mapping options must include PROT_WRITE and the mode must be MAP_PRIVATE in mmap_allocation.cc.\r\n\r\nExample of new mmap call:\r\n<pre>\r\n   mmap(nullptr, buffer_size_bytes_, PROT_READ|PROT_WRITE, MAP_PRIVATE, mmap_fd_, 0);\r\n</pre>\r\n\r\nThere is also a minor issue in the label_image example where a BMP file is read and its header which has some little-endian integers that are not converted on a big-endian machine.\r\n\r\nA simple solution for this (in lite/examples/label_image/bitmap_helpers.cc):\r\n<pre>\r\n header_size = flatbuffers::EndianScalar(header_size);\r\n *width = flatbuffers::EndianScalar(*width);\r\n *height = flatbuffers::EndianScalar(*height);\r\n bpp = flatbuffers::EndianScalar(bpp);\r\n</pre>\r\n", "comments": ["I'd like to update this issue. I've run into exactly the same problem while deploying a TensorFlow lite model to a new big endian target using lite micro. The additional problem here is that the FlatBuffer is compiled into the program binary and referenced from there. This means that step 2 of the proposed solution is not possible in TensorFlow lite micro deployments.\r\n\r\nI'm not sure what the best solution in this case is. Either the values are byte shuffled from little endian to big endian as the tensor operations are being evaluated which would have a terrible effect on performance or a big endian version of the FlatBuffer could be created which would technically break the flat buffer standard. The latter would be the simplest and highest performance solution, but alter the TensorFlow lite micro deployment process. ", "I've actually made the solution presented by @geert56 work successfully in TF lite micro when tested on the Leon 3 (Sparc V8) big endian system. There wasn't any problem updating the values stored in the binary. \r\n\r\nA pull request with these updates has been submitted here https://github.com/tensorflow/tensorflow/pull/30362\r\n\r\nThis doesn't fix the problem in TF lite in general only in TF lite micro.\r\n\r\nTwo files of this PR are specifically dealing with big endian compatability.\r\n\r\ntensorflow/lite/experimental/micro/micro_interpreter.cc - modifies the loaded model weights in the case of a big endian system\r\n\r\ntensorflow/lite/kernels/kernel_util.h - modifies the reading of several array indices which are incorrect if not converted.\r\n\r\nUpdate demonstrates the proposed solution works as indented and can be used as a guide to updating the relevant files for TF lite in general", "This is a known limitation of TFLite schema:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/schema.fbs#L94\r\n\r\nAlso please try out what @PeteBlackerThe3rd suggested. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28807\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28807\">No</a>\n"]}, {"number": 28806, "title": "Optimistic deadness analysis", "body": "The optimistic deadness analysis can complement the original pessimistic\r\nanalysis and reduce false positives due to loop structures such as in\r\ndynamic RNN.\r\n\r\nIn addition, a new rule {S,&,X} & ~X => S is added to try to cancel out\r\nthe leftover symbols after while loops. Otherwise, symbols keep accumulating\r\nin the case of cascaded while loops.", "comments": ["Sanjoy, please help to take a look when you have a moment. Thanks.\r\n\r\n@sanjoy @jlebar ", "Sanjoy, thanks for the review and early feedback. I addressed some of the review comments. Will continue when I have a moment.", "> Just over-communicate: I'm currently waiting on you to address the first round of comments. Once those are addressed I'll do a second review. Please let me know if you'd prefer otherwise (that I review the code in the state it is in currently while you concurrently address the previous round of comments).\r\n\r\nThanks for the status check. I pushed some more commits today. I think I should have addressed all the previous issues except the one related to using string_view to get the returned frame name in `GetRootFrame()`. I will need to study this as I somehow encountered some unexpected behaviors when using string_view in this case.\r\n\r\nPlease proceed your review when you are free. And let me know if all of your previous comments are addressed from your perspective.", "> > Just over-communicate: I'm currently waiting on you to address the first round of comments. Once those are addressed I'll do a second review. Please let me know if you'd prefer otherwise (that I review the code in the state it is in currently while you concurrently address the previous round of comments).\r\n> \r\n> Thanks for the status check. I pushed some more commits today. I think I should have addressed all the previous issues except the one related to using string_view to get the returned frame name in `GetRootFrame()`. I will need to study this as I somehow encountered some unexpected behaviors when using string_view in this case.\r\n> \r\n> Please proceed your review when you are free. And let me know if all of your previous comments are addressed from your perspective.\r\n\r\nSanjoy, you are right that we definitely can use string_view to get the frame name from GetFrameName(). I just was confused by myself. Never mind my previous comments about this.\r\n\r\nNow, all of your previous comments should all have been addressed. Thanks for your time for the review again.", "Just to bug you one more time for this PR. Could you please help to check if the CI fails because of this PR?", "> Just to bug you one more time for this PR. Could you please help to check if the CI fails because of this PR?\r\n\r\nLooks like the Windows CI is failing because of Eigen: https://source.cloud.google.com/results/invocations/f7e0c161-7c6f-4dd8-9bd5-8353a0e5acb8/targets/%2F%2Ftensorflow%2Ftools%2Fci_build%2Fbuilds:gen_win_out/log\r\n\r\nBut I already approved the CL internally so it looks like that failure did not block anything.  @rthadur is that correct?", "Yes , it did not block "]}, {"number": 28805, "title": "[TF-TRT] Simplify ConvertBinary", "body": "This PR contains the changes to ConvertBinary and broadcasting from #27905 and fixes to address review comments from @aaroey ", "comments": ["@trevor-m thanks, but can you merge this to the old PR #27905 so we can see the diffs?"]}, {"number": 28804, "title": "tf.signal.stft supports only float32 inputs ", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.13\r\n- Are you willing to contribute it (Yes/No): maybe but not much experience\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\ntf.signal.stft allows computing the short-time fourier transform over an input signal. Right now, the op only supports float32 inputs. I'm getting significant differences between a numpy and TF implementation because of this. The RFFT implementation in tensorflow supports float64 inputs and none of the other functions called require float32 so I can't see too many obstacles to fixing. \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/ops/signal/spectral_ops.py\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n**Who will benefit with this feature?**\r\nPeople moving their frequency-domain processing into tensorflow. \r\n**Any Other info.**\r\nWould also be good to allow rfft to work on complex128. \r\n", "comments": ["Why is this closed? Has it been decided that there will be no implementation?"]}, {"number": 28803, "title": "cherry-pick vulnerability fixes from r1.13 into r1.12", "body": "", "comments": ["@mrry is there any plans to patch `r1.12`? These address vulnerability issues and it'll be a good idea to resolve them \ud83d\ude42 ", "We recommend using the soon to be released 1.13.2 patch version.\r\n\r\nIf upgrading is not possible, please let me know and I'll add on my roadmap making a new release for 1.12 and then merge these.", "Thanks @mihaimaruseac \r\nWe certainly plan to release `1.13.2` once that patch release is out, but we also need the same patch for `1.12` release i.e `1.12.3`.\r\nIs there no plans to do so.", "There weren't until now, but I think we'll have to do them"]}, {"number": 28802, "title": "Typo in an example of Binary crossentropy", "body": "The Binarycrossentropy estimated using TF2.0.0-alpha0 is `11.522857` (listed on website as `12.0007`) where as Binarycrossentropy estimated using TF1.13.1 is 12.0007 which is correct. Gists are provided below. I suspect there was a change in `tf.math.eps` between TF1.13.1 and TF2.0. Thanks!\r\n\r\n[Gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/cd6abcd1fc7534b17e3f5887b41d91c6/untitled167.ipynb) using TF2.0.0-alpha0 \r\n[Gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/cedb0385df9f5713f462745ab11dc69e/untitled168.ipynb) using TF1,13,1", "comments": ["@yifeif can you please merge this PR", "Closing this PR as this not against `master`, please open a new PR against `master` \r\nCC @mihaimaruseac"]}, {"number": 28801, "title": "Linux: No module named tensorflow_estimator.python.estimator.tpu", "body": "I successfully developed and run a model using Tensorflow Probability with Pruning in a Windows machine, but I get the following error in Linux (Ubuntu 16.04):\r\n\r\n```\r\n  File \"<ipython-input-3-622d85983aea>\", line 4, in <module>\r\n    import tensorflow_probability as tfp\r\n\r\n  File \"/home/rubens/anaconda3/lib/python3.6/site-packages/tensorflow_probability/__init__.py\", line 78, in <module>\r\n    from tensorflow_probability.python import *  # pylint: disable=wildcard-import\r\n\r\n  File \"/home/rubens/anaconda3/lib/python3.6/site-packages/tensorflow_probability/python/__init__.py\", line 21, in <module>\r\n    from tensorflow_probability.python import bijectors\r\n\r\n  File \"/home/rubens/anaconda3/lib/python3.6/site-packages/tensorflow_probability/python/bijectors/__init__.py\", line 23, in <module>\r\n    from tensorflow_probability.python.bijectors.absolute_value import AbsoluteValue\r\n\r\n  File \"/home/rubens/anaconda3/lib/python3.6/site-packages/tensorflow_probability/python/bijectors/absolute_value.py\", line 22, in <module>\r\n    from tensorflow_probability.python.bijectors import bijector\r\n\r\n  File \"/home/rubens/anaconda3/lib/python3.6/site-packages/tensorflow_probability/python/bijectors/bijector.py\", line 31, in <module>\r\n    from tensorflow_probability.python.internal import distribution_util\r\n\r\n  File \"/home/rubens/anaconda3/lib/python3.6/site-packages/tensorflow_probability/python/internal/distribution_util.py\", line 26, in <module>\r\n    from tensorflow_probability.python.internal import dtype_util\r\n\r\n  File \"/home/rubens/anaconda3/lib/python3.6/site-packages/tensorflow_probability/python/internal/dtype_util.py\", line 24, in <module>\r\n    from tensorflow.contrib import framework as contrib_framework\r\n\r\n  File \"/home/rubens/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/__init__.py\", line 41, in <module>\r\n    from tensorflow.contrib import distributions\r\n\r\n  File \"/home/rubens/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/distributions/__init__.py\", line 44, in <module>\r\n    from tensorflow.contrib.distributions.python.ops.estimator import *\r\n\r\n  File \"/home/rubens/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/estimator.py\", line 21, in <module>\r\n    from tensorflow.contrib.learn.python.learn.estimators.head import _compute_weighted_loss\r\n\r\n  File \"/home/rubens/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/__init__.py\", line 93, in <module>\r\n    from tensorflow.contrib.learn.python.learn import *\r\n\r\n  File \"/home/rubens/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/__init__.py\", line 28, in <module>\r\n    from tensorflow.contrib.learn.python.learn import *\r\n\r\n  File \"/home/rubens/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/__init__.py\", line 40, in <module>\r\n    from tensorflow.contrib.learn.python.learn.experiment import Experiment\r\n\r\n  File \"/home/rubens/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\", line 39, in <module>\r\n    from tensorflow.contrib.tpu.python.tpu import tpu_estimator\r\n\r\n  File \"/home/rubens/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/tpu/__init__.py\", line 77, in <module>\r\n    from tensorflow.contrib.tpu.python.tpu.tpu_config import *\r\n\r\n  File \"/home/rubens/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_config.py\", line 22, in <module>\r\n    from tensorflow_estimator.python.estimator.tpu.tpu_config import *\r\n\r\nModuleNotFoundError: No module named 'tensorflow_estimator.python.estimator.tpu'\r\n```\r\nAll the packages involved are up-to-date, including pandas, matplotlib and numpy. Pip / conda reinstalling is not solving this issue. I'm running Python 3.6.8 64bits, Qt 5.11.2, PyQt5 5.11.3 on Linux, Spyder 3.3.1.", "comments": ["@RubensZimbres Please provide details about TensorFlow version. Also, did you compile from source or install a binary?\r\nThanks!\r\n", "I have the same issue, tensorflow_probability seems to be incompatible with tf_nightly (installed via pip).", "Just to check, you have installed both `tf_nightly` and `tf_estimator_nightly` at the same time?", "@gadagashwini Tensorflow version 2.0.0a0 installed via pip\r\n\r\n@mihaimaruseac I have installed both.\r\n\r\nIt doesn't seem to be Spyder issue, as the same problem happens in CLI.\r\n\r\ndir(tfp) brings me:\r\n\r\n`['__doc__', '__loader__', '__name__', '__package__', '__path__', '__spec__']`", "@RubensZimbres Tensorflow Probability 0.6.0 version is tested and stable against TensorFlow version 1.13.1. For more information please visit this [link](https://github.com/tensorflow/probability/releases).\r\nThanks!", "In this case, downgrading Tensorflow to 1.13.1 works for `tensorflow_probability 0.6.0` import , as well as` tfp.distributions`, but for Model Optimization: `ImportError: This version of TensorFlow Model Optimization requires TensorFlow version >= 1.14.0; Detected an installation of version 1.13.1.`. \r\n\r\nNo problem using Tensorflow 2.0.a0 for both libraries in Windows, only in Ubuntu.", "Manually install the tensorflow_estimator 1.14.0  with bazel works for me. https://github.com/tensorflow/estimator", "Oh, the tpu changes came in in 1.14.", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28801\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28801\">No</a>\n"]}, {"number": 28800, "title": "kjj", "body": "", "comments": []}, {"number": 28799, "title": "TypeError: 'Not JSON Serializable' while doing tf.keras.Model.save and using keras variable in loss_weights in tf.keras.Model.compile", "body": "**System information**\r\n- Have I written custom code: NA\r\n- OS Platform and Distribution: Ubuntu 16.04 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: release 9.0, V9.0.176\r\n- GPU model and memory: Tesla K80, 12 GB\r\n\r\n**Describe the current behavior**\r\nWhen I try to save my model using model.save() where model is a tf.keras.Model instance, it throws a _TypeError: ('Not JSON Serializable:', <tf.Variable 'Variable:0' shape=() dtype=float32>)_ .\r\nI am using a tf.keras.backend.variable() in loss_weights in model.compile.\r\nOptimizer: tf.keras.optimizers.Adam\r\nInterestingly, when I try to save my model weights only using model.save_weights where model is a tf.keras.Model instance, it works fine, NO ERROR.\r\n\r\n**Describe the expected behavior**\r\nIt should not throw any type of error during training.\r\n\r\n**Code to reproduce the issue**\r\n`import tensorflow as tf`\r\n`import numpy as np`\r\n`import os`\r\n`import sys`\r\n\r\n`input_layer = tf.keras.layers.Input(shape=([4,3]), batch_size=1)`\r\n`layer1 = tf.keras.layers.Dense(20)(input_layer)`\r\n`layer2 = tf.keras.layers.Dense(1,name=\"output1\")(layer1)`\r\n`layer3 = tf.keras.layers.Dense(1,name=\"output2\")(layer1)`\r\n`model = tf.keras.Model(inputs=input_layer, outputs=[layer2,layer3])`\r\n`alpha = tf.keras.backend.variable(0.25)`\r\n`model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss={\"output1\":tf.keras.metrics.binary_crossentropy,\"output2\":tf.keras.metrics.binary_crossentropy},loss_weights=[1.0,alpha])`\r\n\r\n`# model.fit() is skipped just to get straight to error.`\r\n`model.save(\"./weights.h5\")`\r\n\r\nJust execute this code as it is, It will throw same error!!\r\n\r\n**Other info / logs**\r\nTraceback (most recent call last):\r\n  File \"main_latest.py\", line 45, in <module>\r\n    max_queue_size=10)\r\n  File \"/home/tejal/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py\", line 2177, in fit_generator\r\n    initial_epoch=initial_epoch)\r\n  File \"/home/tejal/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_generator.py\", line 216, in fit_generator\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File \"/home/tejal/.local/lib/python3.5/site-packages/tensorflow/python/keras/callbacks.py\", line 214, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File \"/home/tejal/.local/lib/python3.5/site-packages/tensorflow/python/keras/callbacks.py\", line 601, in on_epoch_end\r\n    self.model.save(filepath, overwrite=True)\r\n  File \"/home/tejal/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/network.py\", line 1363, in save\r\n    save_model(self, filepath, overwrite, include_optimizer)\r\n  File \"/home/tejal/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/saving.py\", line 134, in save_model\r\n    default=serialization.get_json_type).encode('utf8')\r\n  File \"/usr/lib/python3.5/json/__init__.py\", line 237, in dumps\r\n    **kw).encode(obj)\r\n  File \"/usr/lib/python3.5/json/encoder.py\", line 198, in encode\r\n    chunks = self.iterencode(o, _one_shot=True)\r\n  File \"/usr/lib/python3.5/json/encoder.py\", line 256, in iterencode\r\n    return _iterencode(o, 0)\r\n  File \"/home/tejal/.local/lib/python3.5/site-packages/tensorflow/python/util/serialization.py\", line 64, in get_json_type\r\n    raise TypeError('Not JSON Serializable:', obj)\r\nTypeError: ('Not JSON Serializable:', <tf.Variable 'Variable:0' shape=() dtype=float32>)\r\n\r\n![TypeError](https://user-images.githubusercontent.com/14198246/57926377-b8290880-78c8-11e9-92fa-d03803a27123.PNG)\r\n", "comments": ["Similar Issue (Open): https://github.com/tensorflow/tensorflow/issues/28010", "@tejal567 Please provide reproducible full code to investigate this issue. After I ran this code it says : NameError: name 'get_model' is not defined", "`import tensorflow as tf`\r\n`import numpy as np`\r\n`import os`\r\n`import sys`\r\n\r\n`input_layer = tf.keras.layers.Input(shape=([4,3]), batch_size=1)`\r\n`layer1 = tf.keras.layers.Dense(20)(input_layer)`\r\n`layer2 = tf.keras.layers.Dense(1,name=\"output1\")(layer1)`\r\n`layer3 = tf.keras.layers.Dense(1,name=\"output2\")(layer1)`\r\n`model = tf.keras.Model(inputs=input_layer, outputs=[layer2,layer3])`\r\n`alpha = tf.keras.backend.variable(0.25)`\r\n`model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss={\"output1\":tf.keras.metrics.binary_crossentropy,\r\n                                                                \"output2\":tf.keras.metrics.binary_crossentropy},\r\n                                                                loss_weights=[1.0,alpha])`\r\n\r\n`# model.fit() is skipped just to get straight to error.`\r\n`model.save(\"./weights.h5\")`\r\n\r\nJust execute this code as it is, It will throw same error!!", "@tejal567 Able to reproduce the issue with the provided code.\r\nTypeError: ('Not JSON Serializable:', <tf.Variable 'Variable:0' shape=() dtype=float32>)", "@fchollet Can you please have a look at it", "@k-w-w awaiting for your response, please have a look at it ", "@fchollet Is there a fix for this?", "this error also appears when I try to save a transformer model using tensorflow 2.0  \r\n\r\ncode link: https://github.com/bryanlimy/tf2-transformer-chatbot\r\n\r\nhow i try to save the model:\r\n`model.save('hnosa.model')` \r\nand \r\n`  tf.keras.models.save_model(\r\n    model,\r\n    filepath,\r\n    overwrite=True,\r\n    include_optimizer=True,\r\n    save_format=None\r\n)`\r\n\r\nthis error appears: \r\n\r\n`Traceback (most recent call last):`\r\n\r\n  `File \"<ipython-input-1-ce3301d571a9>\", line 1, in <module>\r\n    runfile('C:/Users/alhno/DS/tf2-transformer-chatbot-master/main.py', wdir='C:/Users/alhno/DS/tf2-transformer-chatbot-master')`\r\n\r\n  `File \"C:\\Users\\alhno\\Anaconda3\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 827, in runfile\r\n    execfile(filename, namespace)`\r\n\r\n  `File \"C:\\Users\\alhno\\Anaconda3\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 110, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)`\r\n\r\n  `File \"C:/Users/alhno/DS/tf2-transformer-chatbot-master/main.py\", line 163, in <module>\r\n    main(hparams)`\r\n\r\n  `File \"C:/Users/alhno/DS/tf2-transformer-chatbot-master/main.py\", line 128, in main\r\n    model.save('hnosa.model')`\r\n\r\n  `File \"C:\\Users\\alhno\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\", line 1213, in save\r\n    saving.save_model(self, filepath, overwrite, include_optimizer, save_format)`\r\n\r\n  `File \"C:\\Users\\alhno\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\", line 106, in save_model\r\n    saved_model.save(model, filepath, overwrite, include_optimizer)`\r\n\r\n  `File \"C:\\Users\\alhno\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model.py\", line 1492, in save\r\n    save_lib.save(model, filepath)`\r\n\r\n  `File \"C:\\Users\\alhno\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\save.py\", line 849, in save\r\n    saveable_view, asset_info.asset_index)`\r\n\r\n  `File \"C:\\Users\\alhno\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\save.py\", line 604, in _serialize_object_graph\r\n    _write_object_proto(obj, obj_proto, asset_file_def_index)`\r\n\r\n  `File \"C:\\Users\\alhno\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\save.py\", line 643, in _write_object_proto\r\n    metadata=obj._tracking_metadata)`\r\n\r\n  `File \"C:\\Users\\alhno\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 2705, in _tracking_metadata\r\n    metadata = json.loads(super(Model, self)._tracking_metadata)`\r\n\r\n  `File \"C:\\Users\\alhno\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 2213, in _tracking_metadata\r\n    return json.dumps(metadata, default=serialization.get_json_type)`\r\n\r\n  `File \"C:\\Users\\alhno\\Anaconda3\\lib\\json\\__init__.py\", line 238, in dumps\r\n    **kw).encode(obj)`\r\n\r\n  `File \"C:\\Users\\alhno\\Anaconda3\\lib\\json\\encoder.py\", line 199, in encode\r\n    chunks = self.iterencode(o, _one_shot=True)`\r\n\r\n  `File \"C:\\Users\\alhno\\Anaconda3\\lib\\json\\encoder.py\", line 257, in iterencode\r\n    return _iterencode(o, 0)`\r\n\r\n  `File \"C:\\Users\\alhno\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\serialization.py\", line 69, in get_json_type\r\n    raise TypeError('Not JSON Serializable:', obj)`\r\n\r\n`TypeError: ('Not JSON Serializable:', b'\\n?transformer/encoder/tf_op_layer_mul/encoder/tf_op_layer_mul/mul\\x12\\x03Mul\\x1a\\x12embedding/Identity\\x1a\\x05mul/y*\\x07\\n\\x01T\\x12\\x020\\x01')`", "@vsatyakumar Currently, you can't use tensorflow variables objects in the compiled args. I'll see if there is a fix for this. For now, a workaround would be to save the uncompiled version of the model, then manually re-compile once its loaded.\r\n\r\n@hno1993 I believe if you wrapped the encoder ops (shown below) as a custom layer, the serialization would work.\r\n\r\n```\r\n embeddings = tf.keras.layers.Embedding(hparams.vocab_size,\r\n                                         hparams.d_model)(inputs)\r\n  embeddings *= tf.math.sqrt(tf.cast(hparams.d_model, tf.float32))\r\n  embeddings = PositionalEncoding(hparams)(embeddings)\r\n\r\n  outputs = tf.keras.layers.Dropout(hparams.dropout)(embeddings)\r\n```\r\n\r\nYour code may be buggy if you're running in eager mode, since `tf.keras.layers.Embedding(...)(inputs)` would immediately evaluate the variables in the Embedding layer to constant values.", "@k-w-w, thanks for the suggestion. However, this doesn't help since saving model weights (checkpoints) during training fails aswell.", "Are you saving just the weights or the model? If just the weights, then `model.save_weights` should suffice.", "@hno1993 I see you're using Conda to manage your packages, which may mean your Tensorflow installation doesn't contain all the latest fixes.\r\n\r\nThat error should be fixed by https://github.com/tensorflow/tensorflow/commit/39bc7bcf94983261a3ee8a72802f5de056728a9c#diff-8eb7e20502209f082d0cb15119a50413.\r\n\r\nTry using the latest RC or nightly.", "@tejal567 [Here is the gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/d96b3a426058b99a212a4ec256ff76f6/untitled560.ipynb) with the workaround mentioned by @k-w-w . There is a limitation of what can be part of compile. \r\n\r\nPlease close the issue if this was resolved for you. thanks!\r\n", "Not resolved for me. So can we say that as of now there is no way of saving a model (model.save()) compiled using variables (tf.keras.backend.variable) in loss_weights ? @jvishnuvardhan ", "@tejal567 Correct. If you have a solution, you could contribute by raising Pull-requests(PR) to change the codes. Thanks!", "I would like to add that It is not just throwing this error when trying to save the model at the end of an epoch, the loss_weights do nothing to the total loss. My loss weights for two output network is `{'out1': alpha, 'out2': 1-alpah} `and alpha begins with 0 and ends to 1 by using some update equation which depends on the epoch number and by using callback of course. So the loss for `'out1' `should be zero for the first epoch , but both losses are working without any multiplication with the `loss_weights`. The only solution for me is to build a custom loss function and it works fine, but I still need this issue to be fixed because you want to use the build-in keras losses and sometimes it is hard to build the same loss by yourself. So is there fix in a near future for this?", "I also had this problem when i use checkpoint. btw ,my loss is customLsyer \r\n\r\nthe error log:\r\n`WARNING:tensorflow:From /Users/mkd/Project/PythonProjects/search_query_intention/model/DssmRNN_ali.py:368: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use Model.fit, which supports generators.\r\nWARNING:tensorflow:Model failed to serialize as JSON. Ignoring... ('Not JSON Serializable:', <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.5>)\r\nEpoch 1/2\r\nWARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss.\r\nWARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss.\r\nWARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss.\r\nWARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss.\r\n2020-08-15 22:36:19.344394: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session started.\r\n   1/1000 [..............................] - ETA: 0s - loss: 1440.2056 - cosine_accuracy: 0.8320 - catclass_accuracy: 0.0000e+002020-08-15 22:36:19.432737: I tensorflow/core/profiler/rpc/client/save_profile.cc:168] Creating directory: logs/bigru_sample_twins_char_term_test/train/plugins/profile/2020_08_15_22_36_19\r\n2020-08-15 22:36:19.448521: I tensorflow/core/profiler/rpc/client/save_profile.cc:174] Dumped gzipped tool data for trace.json.gz to logs/bigru_sample_twins_char_term_test/train/plugins/profile/2020_08_15_22_36_19/KDM.local.trace.json.gz\r\n2020-08-15 22:36:19.463320: I tensorflow/core/profiler/utils/event_span.cc:288] Generation of step-events took 0.834 ms\r\n2020-08-15 22:36:19.471340: I tensorflow/python/profiler/internal/profiler_wrapper.cc:87] Creating directory: logs/bigru_sample_twins_char_term_test/train/plugins/profile/2020_08_15_22_36_19Dumped tool data for overview_page.pb to logs/bigru_sample_twins_char_term_test/train/plugins/profile/2020_08_15_22_36_19/KDM.local.overview_page.pb\r\nDumped tool data for input_pipeline.pb to logs/bigru_sample_twins_char_term_test/train/plugins/profile/2020_08_15_22_36_19/KDM.local.input_pipeline.pb\r\nDumped tool data for tensorflow_stats.pb to logs/bigru_sample_twins_char_term_test/train/plugins/profile/2020_08_15_22_36_19/KDM.local.tensorflow_stats.pb\r\nDumped tool data for kernel_stats.pb to logs/bigru_sample_twins_char_term_test/train/plugins/profile/2020_08_15_22_36_19/KDM.local.kernel_stats.pb\r\n1000/1000 [==============================] - ETA: 0s - loss: 489.9940 - cosine_accuracy: 0.6109 - catclass_accuracy: 0.32022020-08-15 22:37:10.213564: W tensorflow/python/util/util.cc:329] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nWARNING:tensorflow:From /Users/mkd/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\nTraceback (most recent call last):\r\n  File \"/Users/mkd/miniconda3/envs/python36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-2-4f1fee32864f>\", line 1, in <module>\r\n    runfile('/Users/mkd/Project/PythonProjects/search_query_intention/bigru_multi_in_and_out_ali_test.py', wdir='/Users/mkd/Project/PythonProjects/search_query_intention')\r\n  File \"/Users/mkd/Library/Application Support/JetBrains/Toolbox/apps/PyCharm-P/ch-0/193.7288.30/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydev_bundle/pydev_umd.py\", line 197, in runfile\r\n    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script\r\n  File \"/Users/mkd/Library/Application Support/JetBrains/Toolbox/apps/PyCharm-P/ch-0/193.7288.30/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"/Users/mkd/Project/PythonProjects/search_query_intention/bigru_multi_in_and_out_ali_test.py\", line 98, in <module>\r\n    callbacks=callback_list, workers=1, use_multiprocessing=False, shuffle=True)\r\n  File \"/Users/mkd/Project/PythonProjects/search_query_intention/model/DssmRNN_ali.py\", line 368, in train\r\n    shuffle=shuffle\r\n  File \"/Users/mkd/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/Users/mkd/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1479, in fit_generator\r\n    initial_epoch=initial_epoch)\r\n  File \"/Users/mkd/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/Users/mkd/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 876, in fit\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File \"/Users/mkd/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 365, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File \"/Users/mkd/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 1177, in on_epoch_end\r\n    self._save_model(epoch=epoch, logs=logs)\r\n  File \"/Users/mkd/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 1214, in _save_model\r\n    self.model.save(filepath, overwrite=True)\r\n  File \"/Users/mkd/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 1052, in save\r\n    signatures, options)\r\n  File \"/Users/mkd/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py\", line 138, in save_model\r\n    signatures, options)\r\n  File \"/Users/mkd/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/save.py\", line 78, in save\r\n    save_lib.save(model, filepath, signatures, options)\r\n  File \"/Users/mkd/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 951, in save\r\n    obj, export_dir, signatures, options, meta_graph_def)\r\n  File \"/Users/mkd/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 1037, in _build_meta_graph\r\n    asset_info.asset_index)\r\n  File \"/Users/mkd/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 697, in _serialize_object_graph\r\n    saveable_view.function_name_map)\r\n  File \"/Users/mkd/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 737, in _write_object_proto\r\n    metadata=obj._tracking_metadata)\r\n  File \"/Users/mkd/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 2742, in _tracking_metadata\r\n    return self._trackable_saved_model_saver.tracking_metadata\r\n  File \"/Users/mkd/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/base_serialization.py\", line 54, in tracking_metadata\r\n    return json_utils.Encoder().encode(self.python_properties)\r\n  File \"/Users/mkd/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/json_utils.py\", line 44, in encode\r\n    return super(Encoder, self).encode(_encode_tuple(obj))\r\n  File \"/Users/mkd/miniconda3/envs/python36/lib/python3.6/json/encoder.py\", line 199, in encode\r\n    chunks = self.iterencode(o, _one_shot=True)\r\n  File \"/Users/mkd/miniconda3/envs/python36/lib/python3.6/json/encoder.py\", line 257, in iterencode\r\n    return _iterencode(o, 0)\r\n  File \"/Users/mkd/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/json_utils.py\", line 41, in default\r\n    return serialization.get_json_type(obj)\r\n  File \"/Users/mkd/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/util/serialization.py\", line 76, in get_json_type\r\n    raise TypeError('Not JSON Serializable:', obj)\r\nTypeError: ('Not JSON Serializable:', <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.5>)`\r\n\r\n\r\n`\r\nclass CustomMultiLossLayer_test(Layer):\r\n    def __init__(self, nb_outputs=2, n_class=1000, **kwargs):\r\n        self.nb_outputs = nb_outputs\r\n        self.n_class = n_class\r\n        self.bin_acc = BinaryAccuracy(name=\"bin_acc\")\r\n        self.spc_acc = CategoricalAccuracy(name=\"nor_acc\")\r\n        super(CustomMultiLossLayer_test, self).__init__(**kwargs)\r\n\r\n    def build(self, input_shape=None):\r\n        self.log_vars = []\r\n\r\n        for i in range(self.nb_outputs):\r\n            self.log_vars += [self.add_weight(name='log_var' + str(i), shape=(1,),\r\n                                              initializer=Constant(0.), trainable=True)]\r\n        super(CustomMultiLossLayer_test, self).build(input_shape)\r\n\r\n    def multi_loss(self, ys_true, ys_pred):\r\n        assert len(ys_true) == self.nb_outputs and len(ys_pred) == self.nb_outputs\r\n        yc_ture = ys_true[0]\r\n        yl_ture = ys_true[1]\r\n        yc_pred = ys_pred[0]\r\n        yl_pred = ys_pred[1]\r\n        loss = 0\r\n        cos_acc = 0\r\n        cat_acc = 0\r\n        # cosine loss\r\n        precision = K.exp(-self.log_vars[0][0])\r\n        loss += K.sum(\r\n            precision * K.binary_crossentropy(yc_ture, yc_pred) + self.log_vars[0][0], -1)\r\n        # cat loss\r\n        precision = K.exp(-self.log_vars[1][0])\r\n        loss += K.sum(\r\n            precision * K.categorical_crossentropy(yl_ture, yl_pred) + self.log_vars[1][0], -1)\r\n        # cosine acc\r\n        _ = self.bin_acc.update_state(yc_ture, yc_pred)\r\n        cos_acc += self.bin_acc.result()\r\n        # cat acc\r\n        _ = self.spc_acc.update_state(yl_ture, yl_pred)\r\n        cat_acc += self.spc_acc.result()\r\n\r\n        return K.mean(loss), K.mean(cos_acc), K.mean(cat_acc)\r\n\r\n    def call(self, inputs, **kwargs):\r\n        ys_true = inputs[:self.nb_outputs]\r\n        ys_pred = inputs[self.nb_outputs:]\r\n        loss, cos_acc, cat_acc = self.multi_loss(ys_true, ys_pred)\r\n        self.add_loss(loss, inputs=inputs)\r\n        self.add_metric(cos_acc, name=\"cosine_accuracy\", aggregation='mean')\r\n        self.add_metric(cat_acc, name=\"catclass_accuracy\", aggregation='mean')\r\n        return ys_true[0]\r\n\r\n    def get_config(self, ):\r\n        config = {\"nb_outputs\": self.nb_outputs, \"bin_acc\": self.bin_acc,\r\n                  \"nor_acc\": self.spc_acc, \"n_class\": self.n_class}\r\n        base_config = super(CustomMultiLossLayer_test, self).get_config()\r\n        return dict(list(base_config.items()) + list(config.items()))\r\n`", "I think enough has been said about this issue", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28799\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28799\">No</a>\n", "Is the problem solved\uff1fI meet the same problem when saving a model with custom layer\uff08in this layer ,I use the add_weight function\uff09,the tensorflow vesion is 2.0.0", "Was the issue solved? I dont get the feel of the solution", "> Not resolved for me. So can we say that as of now there is no way of saving a model (model.save()) compiled using variables (tf.keras.backend.variable) in loss_weights ? @jvishnuvardhan\r\n\r\nRecently I also meet this problem .Because it says tf.Variables can't be json serializable so i just let key variable V(for example) be V.numpy() as parameters' input and it can help.Maybe making it not be type of tf.Variable will matter.", "> **System information**\r\n> \r\n> * Have I written custom code: NA\r\n> * OS Platform and Distribution: Ubuntu 16.04 LTS\r\n> * Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n> * TensorFlow installed from (source or binary): binary\r\n> * TensorFlow version (use command below): 1.12.0\r\n> * Python version: 3.5.2\r\n> * Bazel version (if compiling from source): NA\r\n> * GCC/Compiler version (if compiling from source): NA\r\n> * CUDA/cuDNN version: release 9.0, V9.0.176\r\n> * GPU model and memory: Tesla K80, 12 GB\r\n> \r\n> **Describe the current behavior**\r\n> When I try to save my model using model.save() where model is a tf.keras.Model instance, it throws a _TypeError: ('Not JSON Serializable:', <tf.Variable 'Variable:0' shape=() dtype=float32>)_ .\r\n> I am using a tf.keras.backend.variable() in loss_weights in model.compile.\r\n> Optimizer: tf.keras.optimizers.Adam\r\n> Interestingly, when I try to save my model weights only using model.save_weights where model is a tf.keras.Model instance, it works fine, NO ERROR.\r\n> \r\n> **Describe the expected behavior**\r\n> It should not throw any type of error during training.\r\n> \r\n> **Code to reproduce the issue**\r\n> `import tensorflow as tf`\r\n> `import numpy as np`\r\n> `import os`\r\n> `import sys`\r\n> \r\n> `input_layer = tf.keras.layers.Input(shape=([4,3]), batch_size=1)`\r\n> `layer1 = tf.keras.layers.Dense(20)(input_layer)`\r\n> `layer2 = tf.keras.layers.Dense(1,name=\"output1\")(layer1)`\r\n> `layer3 = tf.keras.layers.Dense(1,name=\"output2\")(layer1)`\r\n> `model = tf.keras.Model(inputs=input_layer, outputs=[layer2,layer3])`\r\n> `alpha = tf.keras.backend.variable(0.25)`\r\n> `model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss={\"output1\":tf.keras.metrics.binary_crossentropy,\"output2\":tf.keras.metrics.binary_crossentropy},loss_weights=[1.0,alpha])`\r\n> \r\n> `# model.fit() is skipped just to get straight to error.`\r\n> `model.save(\"./weights.h5\")`\r\n> \r\n> Just execute this code as it is, It will throw same error!!\r\n> \r\n> **Other info / logs**\r\n> Traceback (most recent call last):\r\n> File \"main_latest.py\", line 45, in\r\n> max_queue_size=10)\r\n> File \"/home/tejal/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py\", line 2177, in fit_generator\r\n> initial_epoch=initial_epoch)\r\n> File \"/home/tejal/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_generator.py\", line 216, in fit_generator\r\n> callbacks.on_epoch_end(epoch, epoch_logs)\r\n> File \"/home/tejal/.local/lib/python3.5/site-packages/tensorflow/python/keras/callbacks.py\", line 214, in on_epoch_end\r\n> callback.on_epoch_end(epoch, logs)\r\n> File \"/home/tejal/.local/lib/python3.5/site-packages/tensorflow/python/keras/callbacks.py\", line 601, in on_epoch_end\r\n> self.model.save(filepath, overwrite=True)\r\n> File \"/home/tejal/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/network.py\", line 1363, in save\r\n> save_model(self, filepath, overwrite, include_optimizer)\r\n> File \"/home/tejal/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/saving.py\", line 134, in save_model\r\n> default=serialization.get_json_type).encode('utf8')\r\n> File \"/usr/lib/python3.5/json/**init**.py\", line 237, in dumps\r\n> **kw).encode(obj)\r\n> File \"/usr/lib/python3.5/json/encoder.py\", line 198, in encode\r\n> chunks = self.iterencode(o, _one_shot=True)\r\n> File \"/usr/lib/python3.5/json/encoder.py\", line 256, in iterencode\r\n> return _iterencode(o, 0)\r\n> File \"/home/tejal/.local/lib/python3.5/site-packages/tensorflow/python/util/serialization.py\", line 64, in get_json_type\r\n> raise TypeError('Not JSON Serializable:', obj)\r\n> TypeError: ('Not JSON Serializable:', <tf.Variable 'Variable:0' shape=() dtype=float32>)\r\n> \r\n> ![TypeError](https://user-images.githubusercontent.com/14198246/57926377-b8290880-78c8-11e9-92fa-d03803a27123.PNG)\r\n\r\nadd .numpy() after alpha ,like the flowing:\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss={\"output1\":tf.keras.metrics.binary_crossentropy,\"output2\":tf.keras.metrics.binary_crossentropy},loss_weights=[1.0,alpha.numpy()])\r\n", "replacing alpha with alpha.numpy() will be ok in my computer.", "I encountered this issue in my project and have fixed it with @Janesefor's solution.\r\n\r\nThe reason for this is that I was using `tf` function when building the model:\r\n```\r\nnum_identity = tf.cast(tf.math.ceil(num_input_channels * split), tf.int32)\r\n```\r\n\r\nAppend `.numpy()` at the end like this:\r\n```\r\nnum_identity = tf.cast(tf.math.ceil(num_input_channels * split), tf.int32).numpy()\r\n```\r\n\r\nThen the model could be saved as `saved_model` format.", "for those who are building custom model using `tf.keras.models.Model` or `tf.keras.layers.Layer` **subclass** and got into the same error, you can try out this solution,\r\ninstead of \r\n```\r\ndim = (512, 512, 3)\r\ndef get_model(dim):\r\n    input = tf.keras.Input(shape=dim)\r\n    .....\r\n```\r\n you can try this,\r\n```\r\ndim = (512, 512, 3)\r\ndef get_model(dim):\r\n    dim  = np.array(dim)\r\n    input = tf.keras.Input(shape=dim)\r\n    .....\r\n```\r\nThis should fix your problem also don't forget to use `get_config()` method.", "This might be related:\r\nhttps://keras.io/guides/serialization_and_saving/\r\n\r\nI had a similar issue when trying to save a model with a custom layer. I had to add .numpy() to the new variable in get_config function as follows:\r\n\r\n```\r\nclass rand_rotate_layer(tf.keras.layers.Layer):\r\n    def __init__(self, batchsize=None, seg_len=None, dim=None, **kwargs):\r\n        super(rand_rotate_layer, self).__init__(**kwargs)\r\n        self.signal_v = tf.Variable(tf.zeros((batchsize, seg_len, dim)), trainable=False)\r\n\r\n\r\n    def get_config(self): \r\n        config = super().get_config().copy()\r\n        config.update({\r\n            'signal_v': self.signal_v.numpy()\r\n        })\r\n        return config\r\n```"]}, {"number": 28798, "title": "tf.data.Dataset::cache doesn't cleanup unused lock and tmp files ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1 / 1.14.dev / 2.0.0-alpha0\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n**Describe the current behavior**\r\n`tf.data.Dataset::cache` doesn't cleanup unused `lock` and `tmp` files if the iterator doesn't complete a pass over the whole dataset or if the process exits before cache has been completed. This means subsequent runs of the same code with an incomplete cache will fail with:\r\n```python traceback\r\ntensorflow.python.framework.errors_impl.AlreadyExistsError: There appears to be a concurrent caching iterator running - cache lockfile already exists ('cache/mnist_0.lockfile'). If you are sure no other running TF computations are using this cache prefix, delete the lockfile and re-initialize the iterator. Lockfile contents: Created at: 1558092594 [Op:IteratorGetNextSync]\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe iterator should cleanup incomplete cache files if finished or if the process exits. This would be really handy when running multiple runs on the machine and if processes might error or finish before the cache has been complete.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf \r\nimport tensorflow_datasets as tfds \r\n\r\ntf.enable_eager_execution() # if version 1.x\r\n\r\ndata = tfds.load(\"mnist\", split=tfds.Split.TRAIN) \r\ncached = data.cache(\"cache/mnist\").shuffle(100).repeat().batch(128) \r\n \r\nfor feat in cached: \r\n    print(\"iterating\")\r\n```\r\n", "comments": ["@lgeiger After running the provided code, got the below error.\r\nNotFoundError: cache/mnist_0.lockfile; No such file or directory [Op:IteratorGetNextSync]", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28798\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28798\">No</a>\n"]}, {"number": 28797, "title": "[TF2.0] Can't use keras validation data with distribution strategy ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): pip tf-nightly-gpu-2.0-preview\r\n- TensorFlow version (use command below): 2.0.0.dev20190517\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version: 10 / 7.4.2.24\r\n- GPU model and memory: 4 x V100\r\n\r\n**Describe the current behavior**\r\nUsing keras validation data and `tf.distribute.MirroredStrategy()` will fail with `'BatchDataset' object is not subscriptable` in TF 2.0 nightly.\r\n\r\n**Describe the expected behavior**\r\nWithout distribution strategy everything works fine.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\r\nmnist_train, mnist_test = tfds.load(name='mnist', split=[tfds.Split.TRAIN, tfds.Split.TEST], as_supervised=True)\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\n\r\ndef scale(image, label):\r\n  image = tf.cast(image, tf.float32)\r\n  image /= 255\r\n\r\n  return image, label\r\n\r\ntrain_dataset = mnist_train.map(scale).shuffle(1000).batch(256)\r\ntest_dataset = mnist_test.map(scale).batch(256)\r\n\r\nwith strategy.scope():\r\n  model = tf.keras.Sequential([\r\n      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n      tf.keras.layers.MaxPooling2D(),\r\n      tf.keras.layers.Flatten(),\r\n      tf.keras.layers.Dense(64, activation='relu'),\r\n      tf.keras.layers.Dense(10, activation='softmax')\r\n  ])\r\n\r\n  model.compile(loss='sparse_categorical_crossentropy',\r\n                optimizer=tf.keras.optimizers.Adam(),\r\n                metrics=['accuracy'])\r\n\r\nmodel.fit(train_dataset, validation_data=test_dataset, epochs=10)\r\n```\r\n\r\n**Other info / logs**\r\n```python traceback\r\n  File \"tf_bug.py\", line 30, in <module>\r\n    model.fit(train_dataset, validation_data=test_dataset, epochs=10)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\", line 649, in fit\r\n    validation_freq=validation_freq)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_distributed.py\", line 143, in fit_distributed\r\n    steps_name='steps_per_epoch')\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_arrays.py\", line 145, in model_iteration\r\n    _print_train_info(inputs, val_inputs, steps_per_epoch, verbose)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_arrays.py\", line 450, in _print_train_info\r\n    hasattr(inputs[0], 'shape') and hasattr(val_inputs[0], 'shape')):\r\nTypeError: 'BatchDataset' object is not subscriptable\r\n```\r\n", "comments": ["@lgeiger Able to reproduce the issue with the provided code.\r\n\r\nTypeError: 'BatchDataset' object does not support indexing", "@lgeiger I don't see any issue when I ran the code with TF2.0 (gist is [here](https://colab.sandbox.google.com/gist/jvishnuvardhan/aca319db01cf71284a0dc7b8001be7f7/untitled191.ipynb)) as well as TF-nightly-gpu-2.0-preview(gist is [here](https://colab.sandbox.google.com/gist/jvishnuvardhan/6d1c69fedc2846663a94d388bd0796ce/untitled192.ipynb)). Thanks!", "I'm also unable to repro, @lgeiger could you try the lastest nightly? This may be already fixed", "@omalleyt12 I am still able to reproduce the error even with the latest nightly (2.0.0.dev20190523).\r\n@jvishnuvardhan If I re-run both your notebooks (inside colab), I also get the same error message.", "@jvishnuvardhan @omalleyt12 Did you run this code on multiple GPUs?", "Updating to the latest nightly (2.0.0.dev20190528), I receive a different error message:\r\n```python\r\n  File \"train_multi.py\", line 60, in <module>\r\n    verbose=verbose)\r\n  File \"~/.virtualenvs/tf_v2_nightly/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 644, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"~/.virtualenvs/tf_v2_nightly/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_distributed.py\", line 899, in fit\r\n    validation_freq=validation_freq)\r\n  File \"~/.virtualenvs/tf_v2_nightly/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_distributed.py\", line 149, in fit_distributed\r\n    steps_name='steps_per_epoch')\r\n  File \"~/.virtualenvs/tf_v2_nightly/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 415, in model_iteration\r\n    steps_name='validation_steps')\r\n  File \"~/.virtualenvs/tf_v2_nightly/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 178, in model_iteration\r\n    steps_per_epoch)\r\n  File \"~/.virtualenvs/tf_v2_nightly/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 469, in _get_num_samples_or_steps\r\n    'steps_per_epoch')\r\n  File \"~/.virtualenvs/tf_v2_nightly/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 256, in check_num_samples\r\n    if hasattr(ins[0], 'shape'):\r\nTypeError: 'function' object is not subscriptable\r\n```", "I was able to reproduce this new error, looking into a fix now. In the meantime a workaround should be to pass the `validation_steps` arg to `fit`", "Similar error here: https://github.com/tensorflow/tensorflow/issues/30843#issuecomment-514534349", "This is fixed in the latest 2.0 nightly", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28797\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28797\">No</a>\n"]}, {"number": 28796, "title": "reduction_ops_gpu_complex128.cu.pic.o was not created", "body": "**System information**\r\n- Ubuntu 18.04.2\r\n- https://github.com/VLOGroup/tensorflow-icg\r\n- CUDA 10\r\n- cudNN 7\r\n- Python 3.6.8 |Anaconda, Inc.|\r\n- GCC 7.3.0\r\n- Bazel version 0.11.1\r\n\r\n\r\nWhile building Tensorflow-icg (https://github.com/VLOGroup/tensorflow-icg) on a Slurm Cluster occurred the error \r\n\r\n```\r\n #warning \"host_defines.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\"\r\n  ^~~~~~~\r\n./tensorflow/core/kernels/reduction_gpu_kernels.cu.h(271): error: initializer not allowed for __shared__ variable\r\n\r\n./tensorflow/core/kernels/reduction_gpu_kernels.cu.h(320): error: initializer not allowed for __shared__ variable\r\n\r\n./tensorflow/core/kernels/reduction_gpu_kernels.cu.h(271): error: initializer not allowed for __shared__ variable\r\n\r\n./tensorflow/core/kernels/reduction_gpu_kernels.cu.h(320): error: initializer not allowed for __shared__ variable\r\n\r\n./tensorflow/core/kernels/reduction_gpu_kernels.cu.h(271): error: initializer not allowed for __shared__ variable\r\n\r\n./tensorflow/core/kernels/reduction_gpu_kernels.cu.h(320): error: initializer not allowed for __shared__ variable\r\n\r\n6 errors detected in the compilation of \"/tmp/tmpxft_00005f84_00000000-7_reduction_ops_gpu_complex128.cu.compute_52.cpp1.ii\".\r\nERROR: /cluster/ty00gyna/tensorflow-icg/tensorflow/core/kernels/BUILD:2709:1: output 'tensorflow/core/kernels/_objs/reduction_ops_gpu/tensorflow/core/kernels/reduction_ops_gpu_complex128.cu.pic.o' was not created\r\n\r\nERROR: /cluster/ty00gyna/tensorflow-icg/tensorflow/core/kernels/BUILD:2709:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n\r\n```\r\n\r\n\r\n", "comments": ["https://github.com/tensorflow/tensorflow/commit/3c245f52912612ed9d8e20245ddb5de055680969\r\nand :\r\n\r\nModify \"./tensorflow/core/kernels/reduction_gpu_kernels.cu.h\"\r\nreplace:\r\n__shared__ value_type partial_sums[32 * 33];\r\nby:\r\n__shared__ __align__(\r\nalignof(value_type)) char partial_sums_raw[32 * 33 * sizeof(value_type)];\r\nvalue_type* partial_sums = reinterpret_cast<value_type*>(partial_sums_raw);\r\n", "@Tayseer-bme Could you provide more details on the issue and context? Thanks!", "@jvishnuvardhan \r\nwhile building tensorflow, we can solve this error by following this steps:\r\n\r\n1- Modify \"tensorflow/core/platform/macros.h\" as same as in the link \r\nhttps://github.com/tensorflow/tensorflow/commit/3c245f52912612ed9d8e20245ddb5de055680969\r\n\r\n2- Modify \"./tensorflow/core/kernels/reduction_gpu_kernels.cu.h\"\r\nreplace:\r\nshared value_type partial_sums[32 * 33];\r\nby:\r\nshared align(\r\nalignof(value_type)) char partial_sums_raw[32 * 33 * sizeof(value_type)];\r\nvalue_type* partial_sums = reinterpret_cast<value_type*>(partial_sums_raw);\r\n\r\nthe problem is solved", "@Tayseer-bme would you like to send a pull request with the fixes you proposed?", "@gunan I don't know a lot about a pull request !", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken. If anyone interested in creating PR, please feel free to create a PR. Thanks!", "I have still the exact same issue, has it been resolved? \r\n", "@Tayseer-bme  Thank you! I follow the steps and then it works. But when I run the train_mri_vn.py, it shows `tensorflow.python.framework.errors_impl.InternalError: Failed to create CUPTI subcriber.` Have you ever met this bug? and how you resolve it?\r\n\r\n\r\n\r\nThis is the details:\r\n2020-01-31 20:11:01.547495: I tensorflow/stream_executor/dso_loader.cc:139] successfully opened CUDA library libcupti.so.10.1 locally\r\n2020-01-31 20:11:01.648157: E tensorflow/core/platform/default/device_tracer.cc:134] cuda call ActivityRegisterCallbacks(BufferRequested, BufferCompleted) failed 35\r\n[DataThread] Loading took  20.266454219818115\r\n[DataThread] Loading took  19.713658094406128\r\nTraceback (most recent call last):\r\n  File \"/home/xinjianli/anaconda3/envs/tficg/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1350, in _do_call\r\n    return fn(*args)\r\n  File \"/home/xinjianli/anaconda3/envs/tficg/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1329, in _run_fn\r\n    status, run_metadata)\r\n  File \"/home/xinjianli/anaconda3/envs/tficg/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InternalError: Failed to create CUPTI subcriber.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"train_mri_vn.py\", line 310, in <module>\r\n    coord.join(enqueue_threads_data)\r\n  File \"/home/xinjianli/anaconda3/envs/tficg/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\", line 387, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/home/xinjianli/anaconda3/envs/tficg/lib/python3.6/site-packages/six.py\", line 703, in reraise\r\n    raise value\r\n  File \"train_mri_vn.py\", line 275, in <module>\r\n    optimizer.minimize(sess, epoch, feed_dict)\r\n  File \"/home/xinjianli/anaconda3/envs/tficg/lib/python3.6/site-packages/tensorflow/contrib/icg/python/optimizer.py\", line 88, in minimize\r\n    feed_dict=feed_dict, options=run_options, run_metadata=run_metadata)\r\n  File \"/home/xinjianli/anaconda3/envs/tficg/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/home/xinjianli/anaconda3/envs/tficg/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1128, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/xinjianli/anaconda3/envs/tficg/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1344, in _do_run\r\n    options, run_metadata)\r\n  File \"/home/xinjianli/anaconda3/envs/tficg/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1363, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Failed to create CUPTI subcriber.", "It arises other issue:\r\n\r\n./tensorflow/core/kernels/reduction_gpu_kernels.cu.h(324): error: identifier \"partial_sums_raw\" is undefined\r\n\r\n./tensorflow/core/kernels/reduction_gpu_kernels.cu.h(322): error: expected a declaration\r\n\r\n ./tensorflow/core/kernels/reduction_gpu_kernels.cu.h(273): error: identifier \"partial_sums_raw\" is undefined\r\n\r\n./tensorflow/core/kernels/reduction_gpu_kernels.cu.h(271): error: expected a declaration\r\n"]}, {"number": 28795, "title": "Use absl::flat_hash_map in UniqueOp kernel.", "body": "Hash map insertion and lookup is dominating in UniqueOp kernel. After\r\nreplacing std::unordered_map with absl_flat_hash_map, much improvement\r\nwas observed in the benchmark.\r\n\r\nBenchmark result run on Xeon E5-2682v4 @ 2.50GHz server is pasted below: \r\n![image](https://user-images.githubusercontent.com/10669111/57911333-1995ab00-78ba-11e9-9b38-d3038f10e09d.png)\r\n", "comments": ["Actually I've implemented a multi-thread version for 1D tensor Unique several months ago, but it is somewhat complicated. Maybe I can open another PR in the future if necessary. ", "Hi Penporn, I noticed some of the CI builds failed and could not figure out what's wrong with the logs. In 'Windows Bazel GPU' CI build log, the failure report said something about absl, but the 'distort_image_ops' target had no  explicit relationship with unique_op. Could you take a look about that? Thanks.", "@lowintelligence 6 tests failed but I only see the links to details for 3 tests. Let's try to fix that first.\r\n- `Ubuntu Sanity`: Buildifier found a formatting error in tensorflow/core/kernels/BUILD. Please fix. The error message from the [log](https://source.cloud.google.com/results/invocations/3abeef23-0ea5-411f-ae97-51212dffe67d/targets/%2F%2Ftensorflow%2Ftools%2Fci_build:gen_ci_sanity_out/log):\r\n```\r\n=== Sanity check step 5 of 14: do_buildifier (buildifier check) ===\r\nRunning do_buildifier on 445 files\r\ntensorflow/core/kernels/BUILD # reformat\r\nbuildifier took 1 s\r\nFAIL: buildifier found errors and/or warnings in above BUILD files.\r\nbuildifier suggested the following changes:\r\n1269c1269\r\n<     ]\r\n---\r\n>     ],\r\nPlease fix manually or run buildifier <file> to auto-fix.\r\n```\r\n- `Windows Bazel GPU`: There's a linking error involving `absl`. Maybe you need to add an absl dependency in the BUILD file? Error message from the [log](https://source.cloud.google.com/results/invocations/c861f2d6-5e63-4ae0-8fad-49a5e7999045/log):\r\n```\r\nERROR: T:/src/github/tensorflow/tensorflow/contrib/image/BUILD:115:1: Linking of rule '//tensorflow/contrib/image:python/ops/_distort_image_ops.so' failed (Exit 1120): link.exe failed: error executing command\r\n...\r\nlib_distort_image_ops_gpu.a(adjust_hsv_in_yiq_op_gpu.cu.o) : error LNK2019: unresolved external symbol \"public: void __cdecl absl::Mutex::ReaderLock(void)\" (?ReaderLock@Mutex@absl@@QEAAXXZ) referenced in function \"public: __cdecl absl::ReaderMutexLock::ReaderMutexLock(class absl::Mutex *)\" (??0ReaderMutexLock@absl@@QEAA@PEAVMutex@1@@Z)\r\nlib_distort_image_ops_gpu.a(adjust_hsv_in_yiq_op_gpu.cu.o) : error LNK2019: unresolved external symbol \"public: void __cdecl absl::Mutex::ReaderUnlock(void)\" (?ReaderUnlock@Mutex@absl@@QEAAXXZ) referenced in function \"public: __cdecl absl::ReaderMutexLock::~ReaderMutexLock(void)\" (??1ReaderMutexLock@absl@@QEAA@XZ)\r\nbazel-out/x64_windows-opt/bin/tensorflow/contrib/image/python/ops/_distort_image_ops.so : fatal error LNK1120: 2 unresolved externals\r\n```\r\n- `Ubuntu Python 3`: Got segfaults in keras backend test. It might just be flaky test and unrelated to this PR. Let's just see what happen after we rerun the tests first. Error message from [log](https://source.cloud.google.com/results/invocations/5788ff76-23e1-4d65-b972-25fc70b8a7da/log) for reference:\r\n```\r\nINFO: From Testing //tensorflow/python/keras:backend_test (shard 1 of 4):\r\n==================== Test output for //tensorflow/python/keras:backend_test (shard 3 of 4):\r\nexternal/bazel_tools/tools/test/test-setup.sh: line 310:    18 Segmentation fault      (core dumped) \"${TEST_PATH}\" \"$@\" 2>&1\r\n================================================================================\r\n==================== Test output for //tensorflow/python/keras:backend_test (shard 1 of 4):\r\nexternal/bazel_tools/tools/test/test-setup.sh: line 310:    19 Segmentation fault      (core dumped) \"${TEST_PATH}\" \"$@\" 2>&1\r\n================================================================================\r\n==================== Test output for //tensorflow/python/keras:backend_test (shard 1 of 4):\r\nexternal/bazel_tools/tools/test/test-setup.sh: line 310:    19 Segmentation fault      (core dumped) \"${TEST_PATH}\" \"$@\" 2>&1\r\n================================================================================\r\n==================== Test output for //tensorflow/python/keras:backend_test (shard 1 of 4):\r\nexternal/bazel_tools/tools/test/test-setup.sh: line 310:    19 Segmentation fault      (core dumped) \"${TEST_PATH}\" \"$@\" 2>&1\r\n================================================================================\r\n```\r\n\r\nI'm on vacation today and won't have access to computers all day, but I can check this again tonight. In the meanwhile, please feel free to ask @gbaned to help rerun the tests.", "Thanks for the explanation. I noticed the missing comma in BUILD file and fixed it. And after checking the codes I believe '_distort_image_ops.so' failure in the Windows Bazel GPU build doesn't explicitly require absl at all, thus I didn't change anything about this target.\r\n\r\nThe new commit was pushed. @gbaned, could you please take a check and trigger a rerun? Thanks."]}, {"number": 28794, "title": "Fix function name", "body": "", "comments": ["Can one of the admins verify this patch?", "We don't update release branches after final release except in case of a security vulnerability. In that case, we only update the branches only as needed to fix the vulnerability and release the patch.\r\n\r\nAs such, since this PR is against one such branch (`r1.13`), I'm closing it. Please open the PR against master if needed there."]}, {"number": 28793, "title": "Tests for ReduceDatasetOp", "body": "This PR adds the tests for ReduceDatasetOp.\r\n\r\ncc:@jsimsa", "comments": ["Internal run of the test segfaults with:\r\n\r\n```\r\n[ RUN      ] ReduceDatasetOpTest/ParameterizedReduceDatasetOpTest.Compute/2\r\n*** SIGSEGV, see go/general-protection-fault received by PID 5524 (TID 5597); stack trace: ***\r\nPC: @     0x7f3c0abd0f23  (unknown)  tensorflow::TensorShapeRep::DebugString()\r\n    @     0x7f3c04a77393       1680  FailureSignalHandler()\r\n    @     0x7f3c042b49a0    3252080  (unknown)\r\n    @     0x5644f6326559        896  std::__g::_Function_handler<>::_M_invoke()\r\n    @     0x7f3c0ab15e7b        144  tensorflow::data::BackgroundWorker::WorkerLoop()\r\n    @     0x7f3c0572c401        208  Thread::ThreadBody()\r\n    @     0x7f3c042ac4e8        176  start_thread\r\n    @     0x7f3c040e722d  (unknown)  clone\r\n```", "@jsimsa Thanks for the log details! The error happens [here](https://github.com/tensorflow/tensorflow/pull/28793/commits/a9ea9782d6db3623ceffac178f49bc67d45528de#diff-66c295fd3d97e1ffbf03d4f2d8bb3dd4R805). It is caused by that the size of `output_shapes_` is smaller than the size of `state`. This commit(https://github.com/tensorflow/tensorflow/pull/28793/commits/a9ea9782d6db3623ceffac178f49bc67d45528de) is submitted to correct the output_shapes in the test case, and also adds the size check for `output_shapes_`, `output_dtypes`, and `state`. Could you please have a look at the changes?    "]}, {"number": 28792, "title": "No gradients provided for any variable, check your graph for ops that do not support gradients while optimization", "body": "ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'Variable:0' shape=(784, 500) dtype=float32_ref>\"\r\n\r\n\r\nthis is coming up on using -  \r\n\r\noptimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\r\n\r\n", "comments": ["@alphamuth \r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "@alphamuth Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 28791, "title": "RandomStandardNormal not supported by TensorFlow Lite runtime", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, 64 bit\r\n- TensorFlow installed from (source or binary): binary (conda install tensorflow for python 3.7)\r\n- TensorFlow version (or github SHA if from source): 1.13.0\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n--------------------------------------------------------------------------\r\nConverterError                           Traceback (most recent call last)\r\n<ipython-input-36-c548bab089a8> in <module>\r\n----> 1 tflite_model = converter.convert()\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\diec\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py in convert(self)\r\n    454           input_tensors=self._input_tensors,\r\n    455           output_tensors=self._output_tensors,\r\n--> 456           **converter_kwargs)\r\n    457     else:\r\n    458       result = _toco_convert_graph_def(\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\diec\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, *args, **kwargs)\r\n    440   data = toco_convert_protos(model_flags.SerializeToString(),\r\n    441                              toco_flags.SerializeToString(),\r\n--> 442                              input_data.SerializeToString())\r\n    443   return data\r\n    444 \r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\diec\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str)\r\n    203       stderr = _try_convert_to_unicode(stderr)\r\n    204       raise ConverterError(\r\n--> 205           \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    206   finally:\r\n    207     # Must manually cleanup files.\r\n\r\nConverterError: TOCO failed. See console for info.\r\n2019-05-17 14:44:52.027648: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RandomStandardNormal\r\n2019-05-17 14:44:52.029852: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: \"WrapDatasetVariant\" device_type: \"CPU\"') for unknown op: WrapDatasetVariant\r\n2019-05-17 14:44:52.030246: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: \"WrapDatasetVariant\" device_type: \"GPU\" host_memory_arg: \"input_handle\" host_memory_arg: \"output_handle\"') for unknown op: WrapDatasetVariant\r\n2019-05-17 14:44:52.030690: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: \"UnwrapDatasetVariant\" device_type: \"CPU\"') for unknown op: UnwrapDatasetVariant\r\n2019-05-17 14:44:52.031017: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: \"UnwrapDatasetVariant\" device_type: \"GPU\" host_memory_arg: \"input_handle\" host_memory_arg: \"output_handle\"') for unknown op: UnwrapDatasetVariant\r\n2019-05-17 14:44:52.031571: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: RandomStandardNormal\r\n2019-05-17 14:44:52.034015: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 43 operators, 65 arrays (0 quantized)\r\n2019-05-17 14:44:52.035043: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 43 operators, 65 arrays (0 quantized)\r\n2019-05-17 14:44:52.040937: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 14 operators, 32 arrays (0 quantized)\r\n2019-05-17 14:44:52.041523: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 14 operators, 32 arrays (0 quantized)\r\n2019-05-17 14:44:52.042039: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 256 bytes, theoretical optimal value: 256 bytes.\r\n2019-05-17 14:44:52.044822: E tensorflow/lite/toco/toco_tooling.cc:421] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, EXP, FULLY_CONNECTED, LOGISTIC, MUL. Here is a list of operators for which you will need custom implementations: RandomStandardNormal.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\issohl\\AppData\\Local\\Continuum\\anaconda3\\envs\\diec\\Scripts\\toco_from_protos-script.py\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"C:\\Users\\issohl\\AppData\\Local\\Continuum\\anaconda3\\envs\\diec\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 59, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"C:\\Users\\issohl\\AppData\\Local\\Continuum\\anaconda3\\envs\\diec\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"C:\\Users\\issohl\\AppData\\Local\\Continuum\\anaconda3\\envs\\diec\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 33, in execute\r\n    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, EXP, FULLY_CONNECTED, LOGISTIC, MUL. Here is a list of operators for which you will need custom implementations: RandomStandardNormal.\r\n\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\nBasically this boils down to the usage of this on the model:\r\n\r\n epsilon = K.random_normal(shape=(batch, dim))\r\n\r\n\r\n```\r\n# This model is based heavily on VAE example from Keras\r\n# https://github.com/keras-team/keras/blob/master/examples/variational_autoencoder.py\r\n\r\nfrom keras.layers import Lambda, Input, Dense\r\nfrom keras.models import Model\r\nfrom keras.utils import plot_model\r\nfrom keras import backend as K\r\nfrom keras.losses import mse\r\n\r\nclass VAE:\r\n    def __init__(self, original_dim, intermediate_dim, latent_dim):\r\n        \"\"\"Creates a variational autoencoder for continuous values\r\n        \"\"\"\r\n        self.original_dim = original_dim\r\n        input_shape = (original_dim,)\r\n        \r\n        # VAE model = encoder + decoder\r\n        # build encoder model\r\n        inputs = Input(shape=input_shape, name='encoder_input')\r\n        x = Dense(intermediate_dim, activation='relu')(inputs)\r\n        x = Dense(intermediate_dim, activation='relu')(x)\r\n        self.z_mean = Dense(latent_dim, name='z_mean')(x)\r\n        self.z_log_var = Dense(latent_dim, name='z_log_var')(x)\r\n        \r\n        # use reparameterization trick to push the sampling out as input\r\n        # note that \"output_shape\" isn't necessary with the TensorFlow backend\r\n        z = Lambda(VAE.sampling, output_shape=(latent_dim,), name='z')([self.z_mean, self.z_log_var])\r\n        \r\n        # instantiate encoder model\r\n        self.encoder = Model(inputs, [self.z_mean, self.z_log_var, z], name='encoder')\r\n        \r\n        # build decoder model\r\n        latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\r\n        x = Dense(intermediate_dim, activation='relu')(latent_inputs)\r\n        x = Dense(intermediate_dim, activation='relu')(x)\r\n        outputs = Dense(original_dim, activation='sigmoid')(x)\r\n\r\n        # instantiate decoder model\r\n        self.decoder = Model(latent_inputs, outputs, name='decoder')\r\n\r\n        # instantiate VAE model\r\n        outputs = self.decoder(self.encoder(inputs)[2])\r\n        self.vae = Model(inputs, outputs, name='vae_mlp')\r\n                         \r\n    def describe(self):\r\n        \"\"\"Display model summaries and saves the architectures to PNG\"\"\"\r\n        self.encoder.summary()\r\n        plot_model(self.encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\r\n        self.decoder.summary()\r\n        plot_model(self.decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\r\n        self.vae.summary()\r\n        plot_model(self.vae, to_file='vae_mlp.png', show_shapes=True)\r\n    \r\n    def fit(self, X, optimizer='adam', **kwargs):\r\n        \"\"\"Fits the model\"\"\"\r\n        \r\n        def vae_loss_func(x, x_true):\r\n            reconstruction_loss = mse(x, x_true)\r\n            reconstruction_loss *= self.original_dim\r\n            kl_loss = 1 + self.z_log_var - K.square(self.z_mean) - K.exp(self.z_log_var)\r\n            kl_loss = K.sum(kl_loss, axis=-1)\r\n            kl_loss *= -0.5\r\n            return K.mean(reconstruction_loss + kl_loss)\r\n        \r\n        self.vae.compile(optimizer=optimizer, loss=vae_loss_func)\r\n        return self.vae.fit(X, X, **kwargs)\r\n    \r\n    def evaluate(self, X, **kwargs):\r\n        \"\"\"Evaluate the model\"\"\"\r\n        return self.vae.evaluate(x=X, y=X, **kwargs)\r\n        \r\n    # reparameterization trick\r\n    # instead of sampling from Q(z|X), sample epsilon = N(0,I)\r\n    # z = z_mean + sqrt(var) * epsilon\r\n    def sampling(args):\r\n        \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\r\n        # Arguments\r\n            args (tensor): mean and log of variance of Q(z|X)\r\n        # Returns\r\n            z (tensor): sampled latent vector\r\n        \"\"\"\r\n        z_mean, z_log_var = args\r\n        batch = K.shape(z_mean)[0]\r\n        dim = K.int_shape(z_mean)[1]\r\n\r\n        # by default, random_normal has mean = 0 and std = 1.0\r\n        epsilon = K.random_normal(shape=(batch, dim))\r\n        return z_mean + K.exp(0.5 * z_log_var) * epsilon\r\n```\r\n\r\nCommand used to reproduce:\r\n\r\nSee Jupyter Notebook: https://github.com/lisaong/diec/blob/tflite-mcu/day3/Anomaly_detection_VAE.ipynb\r\n\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "comments": ["@lisaong Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "Hi @muddham, provided info as requested. Thank you. ", "@lisaong Request you to please refer the [link1](https://github.com/tensorflow/tensorflow/issues/25331) , [link2](https://github.com/tensorflow/tensorflow/issues/27832) .  Please try if that helps and let us know how it progresses.", "@muddham, thanks for the pointers. One part is unclear to me after reading these links: https://www.tensorflow.org/lite/guide/ops_custom\r\n\r\nIn particular, how do I do this part?\r\n\r\n\u201cWhen initializing the OpResolver, add the custom op into the resolver, this will register the operator with Tensorflow Lite so that TensorFlow Lite can use the new implementation. \u201c\r\n\r\nWhere does this code run?\r\n\r\ntflite::ops::builtin::BuiltinOpResolver builtins;\r\nbuiltins.AddCustom(\"Sin\", Register_SIN());\r\n\r\n\r\nIs there an end-to-end example of registering a custom TFLite op?", "Like @lisaong said is (https://github.com/tensorflow/tensorflow/issues/28791#issuecomment-493622520) there a full sample for creating even the simplest custom operation in TensorFlow Lite?\r\n\r\nJust to learn TensorFlow & TensorFlow Lite I created a model in TensorFlow with a simple custom operation (implemented my own addition, even if it exists on both libraries). I then created a model and after training it I converted it to .tflite\r\n\r\nBut now I am unable to convert the source code of my operation to Lite version so that I can use it", "Found the solution, at least for my issue: https://www.tensorflow.org/lite/guide/ops_select\r\n\r\nRandomStandardNormal is part of the whitelist: \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/toco/tflite/whitelisted_flex_ops.cc\r\n\r\n```\r\n# RandomStandardNormal is not part of TensorFlow lite, so we need to use SELECT_TF_OPS to include it\r\n\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                        tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\n```\r\n\r\nThis doesn't resolve the issue reported by @hamlatzis ", "I am closing this issue as it was resolved. Please open a new ticket if this issue persists. For new issues, please open a new ticket so that it will be helpful to the community to follow. Thanks!", "Hi @hamlatzis,\r\n\r\nTurns out the SELECT_TF_OPS option is very unwieldy for target systems that are not iOS or Android. For my case I'm trying to compile the model for Raspberry Pi. The problem with SELECT_TF_OPS is that you have to compile (big) TensorFlow, and your mileage may vary on other platforms.\r\n\r\nHere's an example of how the custom op can be registered. I adapted minimal.cc to register the op. Hopefully this will help.\r\n\r\nStep 1: Implement your custom operator.\r\n\r\n```\r\n\r\nTfLiteStatus RandomStandardNormal_Prepare(TfLiteContext* context, TfLiteNode* node) {\r\n  ...\r\n  return kTfLiteOk;\r\n}\r\n\r\nTfLiteStatus RandomStandardNormal_Eval(TfLiteContext* context, TfLiteNode* node) {\r\n  ...\r\n  return kTfLiteOk;\r\n}\r\n\r\nTfLiteRegistration* Register_RandomStandardNormal() {\r\n  static TfLiteRegistration r = {nullptr, nullptr, \r\n      RandomStandardNormal_Prepare, \r\n      RandomStandardNormal_Eval};\r\n  return &r;\r\n}\r\n```\r\n\r\nStep 2: Register it with the resolver.\r\n```\r\n\r\n  // Build the interpreter\r\n  tflite::ops::builtin::BuiltinOpResolver resolver;\r\n\r\n  // Register custom operators\r\n  resolver.AddCustom(\"RandomStandardNormal\", Register_RandomStandardNormal());\r\n\r\n  InterpreterBuilder builder(*model, resolver);\r\n  std::unique_ptr<Interpreter> interpreter;\r\n  builder(&interpreter);\r\n  TFLITE_MINIMAL_CHECK(interpreter != nullptr);\r\n\r\n\r\n```\r\n\r\nSource: https://github.com/lisaong/diec/tree/master/day3/inference\r\n\r\nRegards,\r\nlisa"]}, {"number": 28790, "title": "Build issue running build from source in Docker container", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 - docker image nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.12\r\n- Python version: 2.7.15\r\n- Installed using virtualenv? pip? conda?: installed using combo pip + bazel as described in installation\r\n- Bazel version (if compiling from source): 0.18\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n- CUDA/cuDNN version: CUDA 9.0 CUDNN 7.0\r\n- GPU model and memory: Titan X - 12 GB\r\n\r\n**Describe the problem**\r\n\r\nWhen running a previously building docker container, we notice that the tensorflow build halts at the following line \r\n\r\n```\r\nERROR: /tensorflow/tensorflow/contrib/tensor_forest/hybrid/BUILD:49:1: Linking of rule '//tensorflow/contrib/tensor_forest/hybrid:gen_training_ops_py_wrappers_cc' failed (Exit 1)\r\n/usr/bin/ld: warning: libcuda.so.1, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Stensor_Uforest_Shybrid_Cgen_Utraining_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)\r\n```\r\n\r\nHowever the docker container follows the suggested installation approach through Docker.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nThe following Docker container is being used\r\n\r\n```\r\nROM nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04\r\n\r\n#--------------------------------------------------------------------------------------------------------------------------------\r\n# Update and install pre-requisites\r\n#--------------------------------------------------------------------------------------------------------------------------------\r\n# Start with updating the existing container\r\nRUN apt update -y && apt upgrade -y \r\nRUN apt install -y gedit octave python-pip python-dev python-virtualenv python-scipy git vim pkg-config zip g++ zlib1g-dev unzip wget python-enum34\r\nRUN pip install pyyaml six numpy wheel mock keras_applications==1.0.6 keras_preprocessing==1.0.5\r\n\r\nRUN wget https://github.com/bazelbuild/bazel/releases/download/0.18.0/bazel-0.18.0-installer-linux-x86_64.sh\r\nRUN chmod +x bazel-0.18.0-installer-linux-x86_64.sh\r\nRUN ./bazel-0.18.0-installer-linux-x86_64.sh\r\n\r\n#--------------------------------------------------------------------------------------------------------------------------------\r\n# Install TensorFlow 1.12 --> If you change anything below\r\n#--------------------------------------------------------------------------------------------------------------------------------\r\nENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64\r\nWORKDIR /\r\nRUN git clone https://github.com/tensorflow/tensorflow.git\r\nWORKDIR /tensorflow/\r\nRUN git checkout r1.12\r\n\r\n#--------------------------------------------------------------------------------------------------------------------------------\r\n# Configure environment variables and system specific properties\r\n#--------------------------------------------------------------------------------------------------------------------------------\r\nENV CI_BUILD_PYTHON=python\r\nENV TF_NEED_CUDA=1\r\nENV TF_CUDA_VERSION=9.0\r\nENV TF_CUDNN_VERSION=7\r\n# TensorRT was checked, but challenge of getting the ScanComplete code to work on this was to large for the remaining project time - - > could be a future approach\r\nENV TF_NEED_TENSORRT=0\r\n# Compute capability - TitanX 6.1 - V100 7.0 - Quadro m2000m 5.0 - Quadro m2200 5.2 - else look up on https://developer.nvidia.com/cuda-gpus\r\nENV TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n\r\n#--------------------------------------------------------------------------------------------------------------------------------\r\n# Install TensorFlow 1.12 --> If you change anything below\r\n#--------------------------------------------------------------------------------------------------------------------------------\r\n\r\nRUN ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1\r\nRUN LD_LIBRARY_PATH=/usr/local/cuda/lib64/stubs:${LD_LIBRARY_PATH}\r\nRUN tensorflow/tools/ci_build/builds/configured GPU\r\n\r\n# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\r\n# EXPLICITLY SELECT WHICH OPTIONS YOU WANT BELOW\r\n# Rule 1 - confirmed for DGX-1 system\r\n# Rule 2 - expected values for Google cloud - should be confirmed\r\n# Rule 3 - confirmed for TitanX, GTX1080 + Intel(R) Xeon(R) CPU E5-2630 v2\r\n# If an optimization is not selected, and your system supports it\r\n# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ \r\n\r\n#RUN bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2 --config=cuda --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" tensorflow/tools/pip_package:build_pip_package\r\n#RUN bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2 --config=cuda --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" tensorflow/tools/pip_package:build_pip_package\r\nRUN bazel build -c opt --copt=-mavx --copt=-msse4.1 --copt=-msse4.2 --config=cuda --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" tensorflow/tools/pip_package:build_pip_package\r\n\r\nRUN bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip\r\nRUN pip --no-cache-dir install --upgrade /tmp/pip/tensorflow-*.whl\r\nRUN rm -rf /tmp/pip\r\nRUN rm -rf /root/.cache\r\n\r\n#--------------------------------------------------------------------------------------------------------------------------------\r\n# Initialize the docker\r\n#--------------------------------------------------------------------------------------------------------------------------------\r\n# Configure the working directory inside the docker\r\n# Set it to the default path to run all the scripts for 3frog\r\nWORKDIR /scancomplete/\r\n\r\n```\r\n\r\n**Any other info / logs**\r\n\r\nWe noticed this bug only last week, since the docker container build just fine before. It might be due to the base container from nvidia having a change, but we are unable to skip this issue. \r\n\r\nAny help appreciated.\r\n", "comments": ["@StevenPuttemans Can you please let us know if you followed the steps mentioned in the tensorflow website [link](https://www.tensorflow.org/install/source#tested_build_configurations). Also Please try to use the relevant bazel version as per the link", "@StevenPuttemans Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "@muddham I am on a leave for now but back next week. I will then try the suggestion and get back if it works! Thanks for the heads up and suggestions already.", "So we changed the dockerfile to specifically force one of the tested configurations in Linux as you can see below\r\n\r\n```\r\nFROM nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04\r\n\r\n#--------------------------------------------------------------------------------------------------------------------------------\r\n# Update and install pre-requisites\r\n#--------------------------------------------------------------------------------------------------------------------------------\r\n# Start with updating the existing container\r\nRUN apt update -y && apt upgrade -y \r\nRUN apt install -y gedit octave python-pip python-dev python-virtualenv python-scipy git vim pkg-config zip g++ zlib1g-dev unzip wget python-enum34\r\nRUN pip install pyyaml six numpy wheel mock keras_applications==1.0.6 keras_preprocessing==1.0.5\r\n\r\nRUN wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-installer-linux-x86_64.sh\r\nRUN chmod +x bazel-0.15.0-installer-linux-x86_64.sh\r\nRUN ./bazel-0.15.0-installer-linux-x86_64.sh\r\n\r\nRUN apt install -y gcc-4.8 g++-4.8\r\nWORKDIR /usr/bin/\r\nRUN rm gcc g++\r\nRUN ln -s gcc-4.8 gcc\r\nRUN ln -s g++-4.8 g++\r\n\r\n#--------------------------------------------------------------------------------------------------------------------------------\r\n# Install TensorFlow 1.12 --> If you change anything below\r\n# Make sure you have the tested build configuration: tensorflow-1.12.0 | Python 2.7 | GCC 4.8 | Bazel 0.15.0\r\n#--------------------------------------------------------------------------------------------------------------------------------\r\nENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64\r\nWORKDIR /\r\nRUN git clone https://github.com/tensorflow/tensorflow.git\r\nWORKDIR /tensorflow/\r\nRUN git checkout r1.12\r\n\r\n#--------------------------------------------------------------------------------------------------------------------------------\r\n# Configure environment variables and system specific properties\r\n#--------------------------------------------------------------------------------------------------------------------------------\r\nENV CI_BUILD_PYTHON=python\r\nENV TF_NEED_CUDA=1\r\nENV TF_CUDA_VERSION=9.0\r\nENV TF_CUDNN_VERSION=7\r\n# TensorRT was checked, but challenge of getting the ScanComplete code to work on this was to large for the remaining project time - - > could be a future approach\r\nENV TF_NEED_TENSORRT=0\r\n# Compute capability - TitanX 6.1 - V100 7.0 - Quadro m2000m 5.0 - Quadro m2200 5.2 - else look up on https://developer.nvidia.com/cuda-gpus\r\nENV TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n\r\n#--------------------------------------------------------------------------------------------------------------------------------\r\n# Install TensorFlow 1.12 --> If you change anything below\r\n#--------------------------------------------------------------------------------------------------------------------------------\r\n\r\nRUN ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1\r\nRUN LD_LIBRARY_PATH=/usr/local/cuda/lib64/stubs:${LD_LIBRARY_PATH}\r\nRUN tensorflow/tools/ci_build/builds/configured GPU\r\n\r\n# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\r\n# EXPLICITLY SELECT WHICH OPTIONS YOU WANT BELOW\r\n# Rule 1 - confirmed for DGX-1 system\r\n# Rule 2 - expected values for Google cloud - should be confirmed\r\n# Rule 3 - confirmed for TitanX, GTX1080 + Intel(R) Xeon(R) CPU E5-2630 v2\r\n# If an optimization is not selected, and your system supports it\r\n# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ \r\n\r\n#RUN bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2 --config=cuda --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" tensorflow/tools/pip_package:build_pip_package\r\n#RUN bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2 --config=cuda --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" tensorflow/tools/pip_package:build_pip_package\r\nRUN bazel build -c opt --copt=-mavx --copt=-msse4.1 --copt=-msse4.2 --config=cuda --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" tensorflow/tools/pip_package:build_pip_package\r\n\r\nRUN bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip\r\nRUN pip --no-cache-dir install --upgrade /tmp/pip/tensorflow-*.whl\r\nRUN rm -rf /tmp/pip\r\nRUN rm -rf /root/.cache\r\n\r\n#--------------------------------------------------------------------------------------------------------------------------------\r\n# Initialize the docker\r\n#--------------------------------------------------------------------------------------------------------------------------------\r\n# Configure the working directory inside the docker\r\n# Set it to the default path to run all the scripts for 3frog\r\nWORKDIR /scancomplete/\r\n\r\n```\r\n\r\nStill gives us the following error\r\n\r\n```\r\nERROR: /tensorflow/tensorflow/python/BUILD:1784:1: Linking of rule '//tensorflow/python:gen_string_ops_py_wrappers_cc' failed (Exit 1)\r\n/usr/bin/ld: warning: libcuda.so.1, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Ustring_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)\r\n```\r\n\r\n@muddham Can this be re-opened?", "We finally found the issue ourselves. It seems that we had a misconfigured environment variable, thus leading to parts not being found. After fixing that, the build succeeded correctly."]}, {"number": 28788, "title": "Bug in tf,keras.layers.Conv2D when use dilation_rate", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n```python\r\nimport tensorflow as tf # TF2\r\n\r\nclass Conv2D_BN_ReLU(tf.keras.Model):\r\n    \"\"\"Conv2D + BN + ReLU\"\"\"\r\n    def __init__(self,\r\n            filters,\r\n            kernel_size,\r\n            strides=1,\r\n            padding=\"SAME\",\r\n            dilation_rate=1,\r\n            use_bias=False,\r\n            kernel_initializer=\"he_normal\",\r\n            kernel_regularizer=None,\r\n            **bn_params):\r\n        super(Conv2D_BN_ReLU, self).__init__()\r\n\r\n        self.conv = tf.keras.layers.Conv2D(filters, kernel_size, strides=strides,\r\n                padding=padding, dilation_rate=dilation_rate, use_bias=use_bias,\r\n                kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer)\r\n        self.bn = tf.keras.layers.BatchNormalization(**bn_params)\r\n\r\n    def call(self, x, training=None):\r\n        x = self.conv(x)\r\n        x = tf.nn.relu(self.bn(x, training=training))\r\n        return x\r\n\r\nif __name__ == \"__main__\":\r\n    import os\r\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\r\n\r\n    net = Conv2D_BN_ReLU(64, 1, 1, dilation_rate=6)\r\n    x = tf.ones((1, 32, 32, 64))\r\n    y = net(x)\r\n\r\n    x = tf.ones((1, 64, 64, 64))\r\n    y = net(x)\r\n```\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: padded_shape[0]=68 is not divisible by block_shape[0]=6 [Op:SpaceToBatchND]\r\n\r\n**Describe the expected behavior**\r\n\r\nI think the second evaluation should also work, because I use padding=\"SAME\"\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@xiaohu2015 Able to reproduce the issue with the provided code.\r\nInvalidArgumentError: padded_shape[0]=68 is not divisible by block_shape[0]=6 [Op:SpaceToBatchND]", "@muddham @jvishnuvardhan I think the padding=\"SAME\" not work with expectation, it is really a big trouble. The reason behind this, I think it is \"deferring weight creation until the shape of the inputs is known\" :\r\n```python\r\nclass Linear(layers.Layer):\r\n\r\n  def __init__(self, units=32):\r\n    super(Linear, self).__init__()\r\n    self.units = units\r\n\r\n  def build(self, input_shape):\r\n    self.w = self.add_weight(shape=(input_shape[-1], self.units),\r\n                             initializer='random_normal',\r\n                             trainable=True)\r\n    self.b = self.add_weight(shape=(self.units,),\r\n                             initializer='random_normal',\r\n                             trainable=True)\r\n\r\n  def call(self, inputs):\r\n    return tf.matmul(inputs, self.w) + self.b\r\n```\r\nThe __call__ method of your layer will automatically run build the first time it is called. You now have a layer that's lazy and easy to use.\r\n\r\nbut for different input shape, maybe the configuration of the first call is improper", "Looks like the same bug I reported 2 months ago: https://github.com/tensorflow/tensorflow/issues/26797\r\nI've already switched to my own implementation of dilated conv https://github.com/tensorpack/tensorpack/commit/353cd04fe81ea8920bc67f702bf492174b000768 since they are slow at fixing bugs.", "@jvishnuvardhan something update???", "having the same issue for tf.keras.layers.conv1D, can't train with data that have various lengths... ", "Thanks for reporting the issue! We have a fix for the issue. Can you run your code with tf-nightly to verify it? Let us know if the issue still exists.", "Close the issue for now as the fix has been submitted.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28788\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28788\">No</a>\n", "Hi @yhliang2018 , thank you for helping with this. We have a question about how to install the tf-nightly you mentioned above. We tried both `pip install tf-nightly-gpu-2.0-preview` and `pip install tf-nightly` but still get that error.\r\n\r\nWe dig a little bit about why we can't it fails and found that this function [tf.space_to_batch_nd](https://www.tensorflow.org/api_docs/python/tf/space_to_batch_nd) will be called when using dilation conv, which requires the length of the input should be divisible by the dilation rate. TF will calculate the number of padding needed to make it divisible using `tf.required_space_to_batch_paddings` just once based on the first input of the model. However since that calculation only happens once, for the second input with different lengths, TF will still do the same number of padding, which with high chance the length of the input is still not divisible by the dilation rate and throw that error.\r\n\r\n\r\n", "Hi @yuanzhedong Yes, you are right. What you descripted was indeed the root cause, and [our fix](https://github.com/tensorflow/tensorflow/blob/7a07f274d65bed12b91b6c6dbe4987a6e1649429/tensorflow/python/keras/layers/convolutional.py#L196) to recreate conv_op when a different input is provided is to resolve this issue. I have verified the code example @xiaohu2015 provided works correctly with tf-nightly [here](https://colab.research.google.com/gist/yhliang2018/ba449f1299ed1476c3ad06560e00f403/verify-github-issue-28788.ipynb). What error do you get for your use case? Could you share a minimal code to reproduce that? Thanks!", "Thank you for sharing the colab! I find `pip install tf-nightly` installed an older version on my own laptop. Retried on ec2 instance and it correctly installs the same version as yours. The code shared by @xiaohu2015 works now!!\r\n\r\nBut when I run my training code (which was working with tf version `2.0.0` if I pad all the inputs to the same length) now fails after two epochs with another error pops up:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"models/ms_tcn/tensorflow/model.py\", line 245, in <module>\r\n    app.run(train_ms_tcn)\r\n  File \"/home/ubuntu/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/ubuntu/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"models/ms_tcn/tensorflow/model.py\", line 237, in train_ms_tcn\r\n    tf.TensorSpec(shape=[], dtype=tf.bool, name=\"training\"),\r\n  File \"/home/ubuntu/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py\", line 909, in save\r\n    meta_graph_def, saveable_view, signatures, options.namespace_whitelist)\r\n  File \"/home/ubuntu/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py\", line 553, in _fill_meta_graph_\\\r\ndef\r\n    object_map, resource_map, asset_info = saveable_view.map_resources()\r\n  File \"/home/ubuntu/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py\", line 280, in map_resources\r\n    capture_constant_value = tensor_util.constant_value(capture)\r\n  File \"/home/ubuntu/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_util.py\", line 827, in constant_val\\\r\nue\r\n    ret = _ConstantValue(tensor, partial)\r\n  File \"/home/ubuntu/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_util.py\", line 673, in _ConstantVal\\\r\nue\r\n    if tensor.op.type == \"Const\":\r\n  File \"/home/ubuntu/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 2220, in type\r\n    return c_api.TF_OperationOpType(self._c_op)\r\nAttributeError: 'Operation' object has no attribute '_c_op'\r\n\r\n```\r\nSeems not related with this issue anymore? We will keep investigating. \r\n\r\nBy the way, do you know when this fix will be officially released?", "The fix should be included in TF 2.1.0. \r\n\r\nLooking at the error message, and looks like it means some non-Eager tensors are accessed outside of tf.function. Not sure if it's caused by this fix. Could you share your code to reproduce this error by opening another issue? I can dig more into it. Thanks!", "Hi @yhliang2018, somehow when I use `conv1d`, the error still exists, \r\n\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: padded_shape[0]=26 is not divisible by block_shape[0]=4 [Op:SpaceToBatchND]\r\n```\r\nthe code below  should be able to reproduce that error.\r\n\r\n```\r\nfrom absl import app\r\nfrom absl import flags\r\nimport numpy as np\r\nimport os\r\n\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nfrom tensorflow.keras import optimizers\r\nfrom tensorflow.keras import losses\r\nfrom tensorflow.keras.layers import Layer\r\nfrom tensorflow.keras.layers import Activation, Lambda, add, Softmax, ReLU, Concatenate\r\nfrom tensorflow.keras.layers import Conv1D, Dropout, Dense, MaxPooling1D, GlobalAvgPool1D\r\n\r\nprint(tf.__version__) #2.1.0-dev20191111\r\n\r\ndef get_gen():\r\n    x_train = [np.ones((1, 16, 1024)),\r\n               np.ones((1, 130, 1024))]\r\n    y_train = np.ones((2, 1))\r\n    while True:\r\n        for i, x in enumerate(x_train):\r\n            print(x.shape)\r\n            yield x, y_train[i]\r\n\r\ntrain_gen = get_gen()\r\n\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(Conv1D(32, 3,\r\n                 activation='relu',\r\n                 #input_shape = (None, 1024),                                                                                                          \r\n                 padding = \"same\",\r\n))\r\n\r\nmodel.add(Conv1D(32, 3,\r\n                 activation='relu',\r\n                 #input_shape = (None, 1024),                                                                                                          \r\n                 padding = \"same\",\r\n                 dilation_rate=2\r\n))\r\n\r\nmodel.add(Conv1D(2, 1, padding=\"same\"))\r\nmodel.add(GlobalAvgPool1D())\r\n\r\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)\r\n\r\ntrain_epoch_loss = []\r\nfor features, labels in train_gen:\r\n    with tf.GradientTape() as tape:\r\n        predictions = model(features, training=True)\r\n        loss = tf.keras.losses.sparse_categorical_crossentropy(labels, predictions)\r\n\r\n    grads = tape.gradient(loss, model.trainable_weights)\r\n    optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n\r\n    train_epoch_loss.append(loss.numpy())\r\n\r\n    print(\"Train_Loss_EPOCH:\", sum(train_epoch_loss) / len(train_epoch_loss))\r\n```", "@yuanzhedong The issue should be fixed for all conv layers, and I cannot reproduce the error you mentioned by [running your code](https://colab.research.google.com/gist/yhliang2018/7eabaa3a107a063741e552dea3ac234c/verify-github-issue-28788-conv1d.ipynb). ", "> @yuanzhedong The issue should be fixed for all conv layers, and I cannot reproduce the error you mentioned by [running your code](https://colab.research.google.com/gist/yhliang2018/7eabaa3a107a063741e552dea3ac234c/verify-github-issue-28788-conv1d.ipynb).\r\n\r\nInteresting...I tried both my local env(ubuntu18.04 + rtx 2080) and ec2 instance(ubuntu16.04 + k80) and the error is still there, I use conda virtual env, not sure if that's the issue\r\n\r\n````\r\nconda create -n=tf-nightly python=3.6\r\nconda activate tf-nightly\r\npip install tf-nightly\r\n# check the tf version is correct but still crash\r\n````", "Not quite related, but do you know how to set GPU env for codelab? I changed the runtime to be GPU but seems the code is running on CPU?\r\n\r\n![Screen Shot 2019-11-12 at 6 27 19 PM](https://user-images.githubusercontent.com/5069709/68728134-43830e00-057b-11ea-802a-b474c826a502.png)\r\n", "@yuanzhedong can you open another thread for your second question? This issue has been closed for a while. We try to resolve one issue in one thread, and this will help other people with similar problems. Thanks!", "Sure, thank you for your help!", "Hi, the issue is still there on tf 2.1.0, is there any proper way to change the code while still making code upgradable to next version (maintainability is preferred...)?\r\nTypical error is \r\n`  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 917, in run\r\n    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py\", line 292, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 433, in train_on_batch\r\n    output_loss_metrics=model._output_loss_metrics)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 312, in train_on_batch\r\n    output_loss_metrics=output_loss_metrics))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 253, in _process_single_batch\r\n    training=training))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 127, in _model_loss\r\n    outs = model(inputs, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 778, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py\", line 717, in call\r\n    convert_kwargs_to_constants=base_layer_utils.call_context().saving)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py\", line 891, in _run_internal_graph\r\n    output_tensors = layer(computed_tensors, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 778, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/convolutional.py\", line 209, in call\r\n    outputs = self._convolution_op(inputs, self.kernel)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py\", line 1135, in __call__\r\n    return self.conv_op(inp, filter)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py\", line 640, in __call__\r\n    return self.call(inp, filter)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py\", line 616, in _with_space_to_batch_call\r\n    block_shape=self.dilation_rate)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py\", line 3303, in required_space_to_batch_paddings\r\n    const_block_shape = tensor_util.constant_value(block_shape)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/tensor_util.py\", line 827, in constant_value\r\n    ret = _ConstantValue(tensor, partial)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/tensor_util.py\", line 673, in _ConstantValue\r\n    if tensor.op.type == \"Const\":\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 2220, in type\r\n    return c_api.TF_OperationOpType(self._c_op)\r\nAttributeError: 'Operation' object has no attribute '_c_op'`", "@yhliang2018 in tf 2.2 the error is still there. \r\nThis is the output i get when calling a conv2d with dilation=16. Where the input is in parentheses \r\n```\r\n(6, 12, 16, 256)\r\nStep : 1 - Loss is : 3.9648172855377197\r\n(6, 8, 16, 256)\r\nStep : 2 - Loss is : 3.227323293685913\r\n(6, 20, 16, 256)\r\nStep : 3 - Loss is : 2.098637819290161\r\n(6, 12, 16, 256)\r\n2020-06-06 10:18:51.440173: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at spacetobatch_op.cc:219 : Invalid argument: padded_shape[0]=56 is not divisible by block_shape[0]=16\r\n\r\n```", "> Hi, the issue is still there on tf 2.1.0, is there any proper way to change the code while still making code upgradable to next version (maintainability is preferred...)?\r\n> Typical error is\r\n> ` File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 917, in run self.main_result = self.main_fn(*self.main_args, **self.main_kwargs) File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py\", line 292, in wrapper return func(*args, **kwargs) File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 433, in train_on_batch output_loss_metrics=model._output_loss_metrics) File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 312, in train_on_batch output_loss_metrics=output_loss_metrics)) File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 253, in _process_single_batch training=training)) File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 127, in _model_loss outs = model(inputs, **kwargs) File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 778, in __call__ outputs = call_fn(cast_inputs, *args, **kwargs) File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py\", line 717, in call convert_kwargs_to_constants=base_layer_utils.call_context().saving) File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py\", line 891, in _run_internal_graph output_tensors = layer(computed_tensors, **kwargs) File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 778, in __call__ outputs = call_fn(cast_inputs, *args, **kwargs) File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/convolutional.py\", line 209, in call outputs = self._convolution_op(inputs, self.kernel) File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py\", line 1135, in __call__ return self.conv_op(inp, filter) File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py\", line 640, in __call__ return self.call(inp, filter) File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py\", line 616, in _with_space_to_batch_call block_shape=self.dilation_rate) File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py\", line 3303, in required_space_to_batch_paddings const_block_shape = tensor_util.constant_value(block_shape) File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/tensor_util.py\", line 827, in constant_value ret = _ConstantValue(tensor, partial) File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/tensor_util.py\", line 673, in _ConstantValue if tensor.op.type == \"Const\": File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 2220, in type return c_api.TF_OperationOpType(self._c_op) AttributeError: 'Operation' object has no attribute '_c_op'`\r\n\r\ndear sir, met the same error, did you solve it?i ran with tf2.1,it still got this error.I changed the input from array to tensor, not working.Could you help me?hope for your reply.\r\nTraceback (most recent call last):\r\n  File \"run.py\", line 118, in <module>\r\n    imgs_mask_test = model.predict(imagestest, verbose=1)\r\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-2.1.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 1013, in predict\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-2.1.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 498, in predict\r\n    workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\r\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-2.1.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 475, in _model_iteration\r\n    total_epochs=1)\r\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-2.1.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 128, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-2.1.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 98, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-2.1.0/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-2.1.0/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 615, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-2.1.0/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 497, in _initialize\r\n    *args, **kwds))\r\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-2.1.0/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2389, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-2.1.0/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-2.1.0/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-2.1.0/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-2.1.0/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-2.1.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 85, in distributed_function\r\n    per_replica_function, args=args)\r\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-2.1.0/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 763, in experimental_run_v2\r\n    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-2.1.0/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 1819, in call_for_each_replica\r\n    return self._call_for_each_replica(fn, args, kwargs)\r\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-2.1.0/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 2164, in _call_for_each_replica\r\n    return fn(*args, **kwargs)\r\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-2.1.0/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 292, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-2.1.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 212, in _predict_on_batch\r\n    result = predict_on_batch(model, x)\r\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-2.1.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 556, in predict_on_batch\r\n    return predict_on_batch_fn(inputs)  # pylint: disable=not-callable\r\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-2.1.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 778, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-2.1.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\", line 717, in call\r\n    convert_kwargs_to_constants=base_layer_utils.call_context().saving)\r\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-2.1.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\", line 891, in _run_internal_graph\r\n    output_tensors = layer(computed_tensors, **kwargs)\r\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-2.1.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 778, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-2.1.0/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/convolutional.py\", line 209, in call\r\n    outputs = self._convolution_op(inputs, self.kernel)\r\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-2.1.0/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_ops.py\", line 1135, in __call__\r\n    return self.conv_op(inp, filter)\r\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-2.1.0/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_ops.py\", line 640, in __call__\r\n    return self.call(inp, filter)\r\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-2.1.0/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_ops.py\", line 616, in _with_space_to_batch_call\r\n    block_shape=self.dilation_rate)\r\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-2.1.0/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\", line 3303, in required_space_to_batch_paddings\r\n    const_block_shape = tensor_util.constant_value(block_shape)\r\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-2.1.0/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py\", line 827, in constant_value\r\n    ret = _ConstantValue(tensor, partial)\r\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-2.1.0/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py\", line 673, in _ConstantValue\r\n    if tensor.op.type == \"Const\":\r\n  File \"/home/ma-user/anaconda3/envs/TensorFlow-2.1.0/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 2220, in type\r\n    return c_api.TF_OperationOpType(self._c_op)\r\nAttributeError: 'Operation' object has no attribute '_c_op'"]}, {"number": 28787, "title": "Unable to run native XLA AOT tests", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave 10.14.4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r1.12\r\n- Python version: python 3.6.3\r\n- Bazel version (if compiling from source): 0.18.1\r\n- GCC/Compiler version (if compiling from source): Apple LLVM version 10.0.1 (clang-1001.0.46.4)\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the problem**\r\nThere is a set of native tests in tensorflow/compiler/aot/tests. I am not able to run them using bazel because of a linking error while building the rule `tfcompile_test` in .../tests/BUILD. Specifically, `tfcompile_test` depends upon `test_graph_tfmatmulandadd_with_profiling` and `test_graph_tfmatmulandadd` which fail to link.\r\n\r\n**Code to reproduce the issue**\r\nIn an untouched Tensorflow repo with r1.12 checkout and bazel 0.18.1 installed, run\r\n`bazel build //tensorflow/compiler/aot/tests:tfcompile_test`\r\n\r\n**Describe the current behavior**\r\nError message is as follows:\r\n`/Users/willxujun/code/tensorflow/tensorflow/compiler/aot/tests/BUILD:225:1: Linking of rule '//tensorflow/compiler/aot/tests:tfcompile_test' failed (Exit 1)\r\nUndefined symbols for architecture x86_64:\r\n \"___tfcompile_MatMulAndAddCompWithProfiling_HloProfilePrinterData_protobuf_array_contents\", referenced from:\r\n      MatMulAndAddCompWithProfiling::StaticData()::'lambda'()::operator()() const in tfcompile_test.o\r\n  \"___tfcompile_MatMulAndAddComp_ProgramShape_protobuf_array_contents\", referenced from:\r\n      MatMulAndAddComp::StaticData()::'lambda'()::operator()() const in tfcompile_test.o\r\nld: symbol(s) not found for architecture x86_64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nTarget //tensorflow/compiler/aot/tests:tfcompile_test failed to build`\r\n\r\nThe problem seems to stem from constructing MatMulAndAddCompWithProfiling or MatMulAndAddComp objects in `tfcompile_test.cc`. These two classes are graphs compiled by XLA. \r\nDisabling the relevant tests in `tfcompile_test.cc` makes the issue go away.\r\nAlternatively, in .../aot/tests/BUILD, removing tfcompile_flags from test_graph_tfmatmulandadd, and setting enable_xla_hlo_profiling to False in test_graph_tfmatmulandadd_with_profiling makes the issue go away.\r\nThis problem seems to happen to only macOS. I could run the same command without issues in Ubuntu 16.04.\r\nThis problem seems to be related to the linking of dependencies. I could successfully run the individual `tf_library`s and get the compiled graph classes in bazel_genfiles/, and both undefined symbols are \"extern C\" variables found in the graphs' header files, and it seems the root definition is not found somehow. As a result, the graph objects cannot be constructed.\r\n\r\n**Describe the expected behavior**\r\nIn Ubuntu 16.04, build succeeds with the following:\r\n`INFO: Build completed successfully, 3560 total actions`\r\nThen a tfcompile_test executable would be produced in bazel_bin/tensorflow/compiler/aot/tests, which can be run successfully.\r\n\r\n**Other info / logs**\r\nN/A", "comments": ["@willxujun Request you to please follow the [link](https://www.tensorflow.org/install/source#tested_build_configurations), as per the reference the bazel version for 1.12 is 0.15.0", "Re-run using Bazel 0.15.0 exhibits identical behaviour", "> TensorFlow version: r1.12\r\n\r\nDoes this reproduce at git head?\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28787\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28787\">No</a>\n"]}, {"number": 28785, "title": "Can't install Tensorflow 1.14.0 ", "body": "When i import tensorflow_model_optimization as tfmot like document, i got error:\r\n`Traceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/lees/Library/Python/3.6/lib/python/site-packages/tensorflow_model_optimization/__init__.py\", line 69, in <module>\r\n    _ensure_tf_install()\r\n  File \"/Users/lees/Library/Python/3.6/lib/python/site-packages/tensorflow_model_optimization/__init__.py\", line 66, in _ensure_tf_install\r\n    required=required_tensorflow_version, present=tf.__version__))\r\nImportError: This version of TensorFlow Model Optimization requires TensorFlow version >= 1.14.0; Detected an installation of version 1.13.1. Please upgrade TensorFlow to proceed.`\r\n\r\nBut I can't upgrade tensorflow to 1.14.0, even cannot find Tensorflow 1.14.0 by pip3 install --upgrade tensorflow==1.14.0.\r\nSo, is it a bug?", "comments": ["You should install tensorflow nightly", "> tensorflow nightly\r\n\r\nThanks. But when training the network, i got lots of warnings, some syntax are different from tensorflow 1.13. Should i change that or ignore the warnings?", "@ZH-Lee Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "@ZH-Lee Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "you can degrade the  TensorFlow Probability .   \r\nIn my case , set the version of TensorFlow Probability  from 0.7.0 to 0.6.0\r\n\r\ntensor2tensor            1.13.4  \r\ntensorboard              1.13.1  \r\ntensorflow-datasets      1.0.2   \r\ntensorflow-estimator     1.13.0  \r\ntensorflow-gpu           1.13.1  \r\ntensorflow-metadata      0.13.0  \r\ntensorflow-probability   ~~0.7.0~~ 0.6.0", "Date April 10th, 2020, at commit 05ad339414ba7c95260bc71ecf79734a2c5e9d84 , issue is still unresolved. Can be reproduced by\r\n`pip install tensorflow==1.14.0`\r\n\r\nEither,\r\n- Include tensorflow_model_optimization in tensorflow 1.13.1\r\n- Or change and edit the documentation for the \r\n```\r\nFile \"/Users/lees/Library/Python/3.6/lib/python/site-packages/tensorflow_model_optimization/__init__.py\", line 66, in _ensure_tf_install required=required_tensorflow_version, present=tf.__version__)\r\n```\r\n\r\n"]}, {"number": 28784, "title": "[Intel Mkl] Fix for Mkl builds in Ubuntu 18.04 containers", "body": "With gcc 7 `gcc -dumpversion` gives only the major version, not the major.minor.release pattern from prior versions. Adding `-dumpfullversion` before `-dumpversion` prints major.minor.release on new and older versions of gcc (back to 4.8).", "comments": ["@gunan Can you take a peek at this? This will fix our nightlies."]}, {"number": 28783, "title": "TF Lite select tf ops example linker error: libtensorflow-lite.a missing tflite subgraph", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone 7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r1.13\r\n- Python version: 2.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.19.2\r\n- GCC/Compiler version (if compiling from source): 4.2.1\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nFollowed guide on [Select TensorFlow operators to use in TensorFlow Lite](https://www.tensorflow.org/lite/guide/ops_select)\r\n\r\nThen tried to run example project `tensorflow/lite/examples/ios/camera/tflite_camera_example_with_select_tf_ops.xcodeproj`\r\n\r\nGave linker errors as below\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nErrors as below:\r\n\r\n> Undefined symbol: tflite::Subgraph::ModifyGraphWithDelegate(_TfLiteDelegate*)\r\n> \r\n> Undefined symbol: tflite::Subgraph::AddTensors(int, int*)\r\n> \r\n> Undefined symbol: tflite::Subgraph::SetTensorParametersReadOnly(int, TfLiteType, char const*, unsigned long, int const*, TfLiteQuantizationParams, char const*, unsigned long, tflite::Allocation const*)\r\n> \r\n> Undefined symbol: tflite::Subgraph::ResetVariableTensors()\r\n> \r\n> Undefined symbol: tflite::Subgraph::Invoke()\r\n> \r\n> Undefined symbol: tflite::Subgraph::ResizeInputTensor(int, std::__1::vector<int, std::__1::allocator<int> > const&)\r\n> \r\n> Undefined symbol: tflite::Subgraph::SetExecutionPlan(std::__1::vector<int, std::__1::allocator<int> > const&)\r\n> \r\n> Undefined symbol: tflite::Subgraph::SetTensorParametersReadWrite(int, TfLiteType, char const*, unsigned long, int const*, TfLiteQuantizationParams, bool)\r\n> \r\n> Undefined symbol: tflite::Subgraph::SetVariables(std::__1::vector<int, std::__1::allocator<int> >)\r\n> \r\n> Undefined symbol: tflite::Subgraph::AddNodeWithParameters(std::__1::vector<int, std::__1::allocator<int> > const&, std::__1::vector<int, std::__1::allocator<int> > const&, char const*, unsigned long, void*, _TfLiteRegistration const*, int*)\r\n> \r\n> Undefined symbol: tflite::Subgraph::AllocateTensors()\r\n> \r\n> Undefined symbol: tflite::Subgraph::SetInputs(std::__1::vector<int, std::__1::allocator<int> >)\r\n> \r\n> Undefined symbol: tflite::Subgraph::UseNNAPI(bool)\r\n> \r\n> Undefined symbol: tflite::Subgraph::SetExternalContext(TfLiteExternalContextType, TfLiteExternalContext*)\r\n> \r\n> Undefined symbol: tflite::Subgraph::SetOutputs(std::__1::vector<int, std::__1::allocator<int> >)\r\n> \r\n> Undefined symbol: tflite::Subgraph::Subgraph(tflite::ErrorReporter*, TfLiteExternalContext**, std::__1::vector<std::__1::unique_ptr<tflite::Subgraph, std::__1::default_delete<tflite::Subgraph> >, std::__1::allocator<std::__1::unique_ptr<tflite::Subgraph, std::__1::default_delete<tflite::Subgraph> > > >*)\r\n", "comments": ["Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28783\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28783\">No</a>\n"]}, {"number": 28782, "title": "[TF2.0] Can't use tf.data.Dataset::cache with distribution strategy", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): pip tf-nightly-gpu-2.0-preview\r\n- TensorFlow version (use command below): 2.0.0.dev20190514\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version: 10 / 7.4.2.24\r\n- GPU model and memory: V100\r\n\r\n**Describe the current behavior**\r\nUsing `tf.data.Dataset::cache` and `tf.distribute.MirroredStrategy()` will fail with `Cache should only be read after it has been completed. [Op:MultiDeviceIteratorInit]` in TF 2.0 nightly.\r\n\r\n**Describe the expected behavior**\r\nUsing `tf-nightly-gpu==1.14.1.dev20190514` graph mode `tf.data` caching works using distribution strategy and doesn't throw an error.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\r\nmnist_train, mnist_test = tfds.load(name='mnist', split=[tfds.Split.TRAIN, tfds.Split.TEST], as_supervised=True, data_dir=\"gs://plumerai/data\")\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\n\r\ndef scale(image, label):\r\n  image = tf.cast(image, tf.float32)\r\n  image /= 255\r\n\r\n  return image, label\r\n\r\ntrain_dataset = mnist_train.cache().repeat().map(scale).shuffle(1000).batch(256)\r\ntest_dataset = mnist_test.cache().repeat().map(scale).batch(256)\r\n\r\nwith strategy.scope():\r\n  model = tf.keras.Sequential([\r\n      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n      tf.keras.layers.MaxPooling2D(),\r\n      tf.keras.layers.Flatten(),\r\n      tf.keras.layers.Dense(64, activation='relu'),\r\n      tf.keras.layers.Dense(10, activation='softmax')\r\n  ])\r\n\r\n  model.compile(loss='sparse_categorical_crossentropy',\r\n                optimizer=tf.keras.optimizers.Adam(),\r\n                metrics=['accuracy'])\r\n\r\nmodel.fit(train_dataset, validation_data=test_dataset, steps_per_epoch=100, validation_steps=10, epochs=10)\r\n```\r\n\r\n**Other info / logs**\r\n```python\r\nFile \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\", line 646, in fit validation_freq=validation_freq)\r\nFile \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_distributed.py\", line 143, in fit_distributed steps_name='steps_per_epoch')\r\nFile \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_arrays.py\", line 194, in model_iteration val_iterator = _get_iterator(val_inputs, model._distribution_strategy)\r\nFile \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_arrays.py\", line 512, in _get_iterator inputs, distribution_strategy)\r\nFile \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/distribute/distributed_training_utils.py\", line 529, in get_iterator initialize_iterator(iterator, distribution_strategy)\r\nFile \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/distribute/distributed_training_utils.py\", line 535, in initialize_iterator init_op = control_flow_ops.group(iterator.initialize())\r\nFile \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py\", line 274, in initialize return super(DistributedIteratorV1, self)._initializer\r\nFile \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py\", line 259, in _initializer init_ops.extend(it.initialize())\r\nFile \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py\", line 660, in initialize self._iterator._eager_reset() # pylint: disable=protected-access\r\nFile \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py\", line 333, in _eager_reset max_buffer_size=self._max_buffer_size)\r\nFile \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 3118, in multi_device_iterator_init _six.raise_from(_core._status_to_exception(e.code, message), None)\r\nFile \"<string>\", line 3, in raise_from tensorflow.python.framework.errors_impl.InternalError: Cache should only be read after it has been completed. [Op:MultiDeviceIteratorInit]\r\n```", "comments": ["Will it be possible to provide full minimal code snippet that can reproduce the issue on version 2.0.0.dev20190514. That would really help us to debug faster. Thanks!", "@achandraa Sorry for the incomplete code sample. This was the result of some late night debugging ;)\r\n\r\nI added a complete code sample above.", "Hi! Any update in this?", "I meet the same problem today with newest released tf2.0.", "Same problem with me. If I cache on the validation data and use the `validation_step` in the `fit()` call, I always get the same error.\r\n\r\nHowever, if I do not include the `validation_step`, and remove the `repeat()` from the validation dataset, it works nicely, as the validation runs until the validation_data dataset is exhausted (see the documentation of `fit()`. As it is not necessary to shuffle the validation data, the repeat is not really necessary here. So this way the problem is *solved*.\r\n\r\nSo I guess I did not use the correct `validation_steps` value after all, but it is not really needed.\r\n\r\n", "> Same problem with me. If I cache on the validation data and use the `validation_step` in the `fit()` call, I always get the same error.\r\n> \r\n> However, if I do not include the `validation_step`, and remove the `repeat()` from the validation dataset, it works nicely, as the validation runs until the validation_data dataset is exhausted (see the documentation of `fit()`. As it is not necessary to shuffle the validation data, the repeat is not really necessary here. So this way the problem is _solved_.\r\n> \r\n> So I guess I did not use the correct `validation_steps` value after all, but it is not really needed.\r\n\r\nI just found my problem come from that I accidently used same tf dataset for both train and valid stage. In the same time, the number of steps I set do not exhausted all data in the cache.\r\n\r\nAfter I use different tf.data.dataset and set a larger number of steps, the problem solved.", "Thanks for @darcula1993 . Using `cache()` after `take()` or `skip()` solved my problem.", "I'm experiencing the same issue.\r\nboth suggestions by darcula1993 and sbl1996 dont help\r\nA minimal reproduction code:\r\n\r\n```\r\n\r\n%tensorflow_version 2.x\r\n#must use tf 1.x to use TPU properly\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport tensorflow_datasets as tfds\r\nimport os\r\n\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\r\ntf.config.experimental_connect_to_cluster(resolver)\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\nstrategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n\r\n#constants\r\nIMG_HEIGHT=IMG_WIDTH=200\r\nBATCH_SIZE=128\r\n\r\ndef create_model():\r\n  model = tf.keras.models.Sequential()\r\n  model.add(tf.keras.layers.BatchNormalization(input_shape=(IMG_HEIGHT,IMG_WIDTH,1)))\r\n  model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='elu'))\r\n  model.add(tf.keras.layers.MaxPooling2D(2))\r\n  model.add(tf.keras.layers.Dropout(0.25))\r\n\r\n  model.add(tf.keras.layers.BatchNormalization())\r\n  model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='elu'))\r\n  model.add(tf.keras.layers.MaxPooling2D(2))\r\n  model.add(tf.keras.layers.Dropout(0.25))\r\n\r\n  model.add(tf.keras.layers.BatchNormalization())\r\n  model.add(tf.keras.layers.Conv2D(64, (5, 5), activation='elu'))\r\n  model.add(tf.keras.layers.MaxPooling2D(2))\r\n  model.add(tf.keras.layers.Dropout(0.25))\r\n\r\n  model.add(tf.keras.layers.BatchNormalization())\r\n  model.add(tf.keras.layers.Conv2D(128, (3, 3),  activation='elu'))\r\n  model.add(tf.keras.layers.MaxPooling2D(2))\r\n  model.add(tf.keras.layers.Dropout(0.25))\r\n\r\n  model.add(tf.keras.layers.BatchNormalization())\r\n  model.add(tf.keras.layers.Conv2D(256, (3, 3),  activation='elu'))\r\n  #model.add(tf.keras.layers.MaxPooling2D(2))\r\n  model.add(tf.keras.layers.Dropout(0.25))\r\n\r\n  model.add(tf.keras.layers.BatchNormalization())\r\n  model.add(tf.keras.layers.Conv2D(512, (3, 3),  activation='elu'))\r\n  model.add(tf.keras.layers.MaxPooling2D(2))\r\n  model.add(tf.keras.layers.Dropout(0.25))\r\n\r\n  model.add(tf.keras.layers.Flatten())\r\n  model.add(tf.keras.layers.Dense(256))\r\n  model.add(tf.keras.layers.Activation('elu'))\r\n  model.add(tf.keras.layers.Dropout(0.5))\r\n  model.add(tf.keras.layers.Dense(10))\r\n  model.add(tf.keras.layers.Activation('softmax'))\r\n  return model\r\n\r\n@tf.function\r\ndef convert(image,label):\r\n  return (tf.image.convert_image_dtype(image, tf.float32),tf.expand_dims(tf.cast(label,tf.float32),0))\r\n@tf.function\r\ndef resize(image,label):\r\n  return (tf.image.resize(image,(IMG_HEIGHT,IMG_WIDTH)),label)\r\n\r\ntest_split, valid_split, train_split = tfds.Split.TRAIN.subsplit([10, 10, 80])\r\ntrain_ds = (tfds.load(\"mnist\", split=train_split, as_supervised=True, try_gcs=True) \r\n  .map(convert)\r\n  .map(resize)\r\n  .batch(BATCH_SIZE).take(144).cache().repeat().prefetch(50))\r\n#if the .cache() is removed the issue disappears\r\n\r\nwith strategy.scope():\r\n  model=create_model()\r\n  model.compile(\r\n        optimizer=tf.keras.optimizers.Adam(),\r\n        loss=\"sparse_categorical_crossentropy\",\r\n        metrics=[\"sparse_categorical_accuracy\"])\r\n\r\nmodel.fit(train_ds,epochs=15,steps_per_epoch=200)\r\n```\r\nProduces this error\r\n```\r\nInternalError: Cache should only be read after it has been completed.\r\nAdditional GRPC error information:\r\n{\"created\":\"@1572919336.377239490\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Cache should only be read after it has been completed.\",\"grpc_status\":13} [Op:MultiDeviceIteratorInit]\r\n```\r\n\r\nThe environment is a Colab notebook with python 3.6 and TPU accelerator.\r\nTF version:v2.0.0-rc2-26-g64c3d38 2.0.0\r\nThe problem disappears if the code is switched to TF 1.x and training runs well.", "> I'm experiencing the same issue.\r\n> both suggestions by darcula1993 and sbl1996 dont help\r\n> A minimal reproduction code:\r\n> \r\n> ```\r\n> \r\n> %tensorflow_version 2.x\r\n> #must use tf 1.x to use TPU properly\r\n> import tensorflow as tf\r\n> import numpy as np\r\n> import matplotlib.pyplot as plt\r\n> import tensorflow_datasets as tfds\r\n> import os\r\n> \r\n> resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\r\n> tf.config.experimental_connect_to_cluster(resolver)\r\n> tf.tpu.experimental.initialize_tpu_system(resolver)\r\n> strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n> \r\n> #constants\r\n> IMG_HEIGHT=IMG_WIDTH=200\r\n> BATCH_SIZE=128\r\n> \r\n> def create_model():\r\n>   model = tf.keras.models.Sequential()\r\n>   model.add(tf.keras.layers.BatchNormalization(input_shape=(IMG_HEIGHT,IMG_WIDTH,1)))\r\n>   model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='elu'))\r\n>   model.add(tf.keras.layers.MaxPooling2D(2))\r\n>   model.add(tf.keras.layers.Dropout(0.25))\r\n> \r\n>   model.add(tf.keras.layers.BatchNormalization())\r\n>   model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='elu'))\r\n>   model.add(tf.keras.layers.MaxPooling2D(2))\r\n>   model.add(tf.keras.layers.Dropout(0.25))\r\n> \r\n>   model.add(tf.keras.layers.BatchNormalization())\r\n>   model.add(tf.keras.layers.Conv2D(64, (5, 5), activation='elu'))\r\n>   model.add(tf.keras.layers.MaxPooling2D(2))\r\n>   model.add(tf.keras.layers.Dropout(0.25))\r\n> \r\n>   model.add(tf.keras.layers.BatchNormalization())\r\n>   model.add(tf.keras.layers.Conv2D(128, (3, 3),  activation='elu'))\r\n>   model.add(tf.keras.layers.MaxPooling2D(2))\r\n>   model.add(tf.keras.layers.Dropout(0.25))\r\n> \r\n>   model.add(tf.keras.layers.BatchNormalization())\r\n>   model.add(tf.keras.layers.Conv2D(256, (3, 3),  activation='elu'))\r\n>   #model.add(tf.keras.layers.MaxPooling2D(2))\r\n>   model.add(tf.keras.layers.Dropout(0.25))\r\n> \r\n>   model.add(tf.keras.layers.BatchNormalization())\r\n>   model.add(tf.keras.layers.Conv2D(512, (3, 3),  activation='elu'))\r\n>   model.add(tf.keras.layers.MaxPooling2D(2))\r\n>   model.add(tf.keras.layers.Dropout(0.25))\r\n> \r\n>   model.add(tf.keras.layers.Flatten())\r\n>   model.add(tf.keras.layers.Dense(256))\r\n>   model.add(tf.keras.layers.Activation('elu'))\r\n>   model.add(tf.keras.layers.Dropout(0.5))\r\n>   model.add(tf.keras.layers.Dense(10))\r\n>   model.add(tf.keras.layers.Activation('softmax'))\r\n>   return model\r\n> \r\n> @tf.function\r\n> def convert(image,label):\r\n>   return (tf.image.convert_image_dtype(image, tf.float32),tf.expand_dims(tf.cast(label,tf.float32),0))\r\n> @tf.function\r\n> def resize(image,label):\r\n>   return (tf.image.resize(image,(IMG_HEIGHT,IMG_WIDTH)),label)\r\n> \r\n> test_split, valid_split, train_split = tfds.Split.TRAIN.subsplit([10, 10, 80])\r\n> train_ds = (tfds.load(\"mnist\", split=train_split, as_supervised=True, try_gcs=True) \r\n>   .map(convert)\r\n>   .map(resize)\r\n>   .batch(BATCH_SIZE).take(144).cache().repeat().prefetch(50))\r\n> #if the .cache() is removed the issue disappears\r\n> \r\n> with strategy.scope():\r\n>   model=create_model()\r\n>   model.compile(\r\n>         optimizer=tf.keras.optimizers.Adam(),\r\n>         loss=\"sparse_categorical_crossentropy\",\r\n>         metrics=[\"sparse_categorical_accuracy\"])\r\n> \r\n> model.fit(train_ds,epochs=15,steps_per_epoch=200)\r\n> ```\r\n> \r\n> Produces this error\r\n> \r\n> ```\r\n> InternalError: Cache should only be read after it has been completed.\r\n> Additional GRPC error information:\r\n> {\"created\":\"@1572919336.377239490\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Cache should only be read after it has been completed.\",\"grpc_status\":13} [Op:MultiDeviceIteratorInit]\r\n> ```\r\n> \r\n> The environment is a Colab notebook with python 3.6 and TPU accelerator.\r\n> TF version:v2.0.0-rc2-26-g64c3d38 2.0.0\r\n> The problem disappears if the code is switched to TF 1.x and training runs well.\r\n\r\ntry catche first then map?", "Can you try with TF 2.1? I believe this issue should have been fixed by https://github.com/tensorflow/tensorflow/commit/08f41c6216d177933ba8eb48cd171a1e004e6ca2", "Closing this issue since it's resolved. Feel free to reopen if still have problems. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28782\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28782\">No</a>\n"]}, {"number": 28781, "title": "[Java] Add default eager session", "body": "As in TensorFlow 2.0, eager execution is the default environment for running operations, I added a default `EagerSession` instance in the Java client. \r\n\r\nThis instance is allocated only when used for the first time and remains valid for the lifetime of the application (i.e. it cannot be closed).\r\n\r\nTo execute operations in the default eager environment, as user can simply do this:\r\n```java\r\nOps tf = Ops.create();\r\ntf.math.add(tf.constant(1), tf.constant(2)); // executed eagerly\r\n```\r\n\r\nNote that this feature is not required to support eager execution in Java, I let it up to the reviewer to decide if it's a nice to have that fits well with the design of TF2.0.\r\n\r\nCC @sjamesr ", "comments": []}]