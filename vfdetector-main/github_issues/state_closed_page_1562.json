[{"number": 6055, "title": "Tensorflow 0.12 failed to install on Window 10", "body": "I have installed python3.5.2+cuda8.0+cudnn5.1 on my computer with Windows 10(64-bits)+ GTX960M.Also excute the command line as following:\r\n \r\npip install --upgrade https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-0.12.0rc0-cp35-cp35m-win_amd64.whl\r\n\r\nwhile when i get into python and input:    import tensorflow as tf , it shows errors:\r\n![1](https://cloud.githubusercontent.com/assets/11583292/20858023/bc4712d2-b975-11e6-818f-b43b04c4f4ff.png)\r\n![2](https://cloud.githubusercontent.com/assets/11583292/20858024/be9e4c8a-b975-11e6-8a3a-a590daab3afa.png)\r\n\r\n\r\nThen i install the bazel but it is useless,what should i do?\r\n\r\n", "comments": ["Are you using 64-bit version of Python? Tensorflow windows version only supports 64-bit version of Python. Install Python 3.5.2 64-bit or later version.", "I can confirm I'm seeing the same error message on the 64-bit version of Python 3.5.2 on Windows 10.\r\n\r\n```\r\nPython 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)] on win32\r\n```\r\n\r\nI installed Tensor flow with the following command:\r\n\r\n```\r\npip install --upgrade https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-0.12.0rc0-cp35-cp35m-win_amd64.whl\r\n```\r\n\r\nI'm using CUDA 8, cudnn-8.0, a GTX 1080 with the 376.09 driver.\r\n\r\nWhen I run the following:\r\n\r\n```\r\n>>> import tensorflow as tf\r\n```\r\n\r\nThe following exception occurs:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\mark\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 19, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\mark\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n```\r\n\r\nThe value of ``mname`` being passed into ``importlib.import_module`` is ``tensorflow.python._pywrap_tensorflow``", "@codeloverr I downloaded \"python-3.5.2-amd64.exe\" from https://www.python.org/downloads/release/python-352/\r\nThe description shows it is used \"for AMD64/EM64T/x64, not Itanium processors\",so i don't think the version of python is a problem.", "I have successfully installed and used Tensorflow on Windows 10.\r\nI am using GTX 1070 and \r\n\r\n- Python 3.5.2 64 -bit\r\n\r\n- CUDA 8.0 & cuDNN for 8.0\r\n\r\nAfter installing above mentioned enter following command in command prompt for CPU version\r\n`pip install \u2013upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-0.12.0rc0-cp35-cp35m-win_amd64.whl`\r\n\r\nFor GPU version\r\n`pip install --upgrade https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-0.12.0rc0-cp35-cp35m-win_amd64.whl`\r\n\r\nI'm having no issues with this. May be python environment have no  PATH variable, causing the problem.\r\n\r\nAlso delete the existing Tensorflow folders in program files. Uninstall python install again. Follow instruction on this [link ](http://bottomstack.com/how-to-install-tensorflow-windows-and-visual-studio/)if your issue does not resolve.", "@qiushi223 @marklit sorry it didn't work for you right away. \r\n\r\nTensorFlow requires `MSVCP140.DLL` and it may not be installed on your system. Please, could you check if it is in your %PATH% and If is the case this DLL is missing:\r\n\r\n1. Install [Microsoft Visual C++ 2015 Redistributable Update 3](https://www.microsoft.com/en-us/download/details.aspx?id=53587) x64.\r\n\r\n2.  [Test your installation](https://github.com/tensorflow/tensorflow/blob/mrry-docs-fix/tensorflow/g3doc/get_started/os_setup.md#test-the-tensorflow-installation) and follow up if that worked.\r\n\r\nThanks.", "Installing Microsoft Visual C++ 2015 Redistributable Update 3 (x64 version) fixed it for me. Thanks.", "@Carmezim @marklit It also works for me.Thanks a lot!", "Installing Microsoft Visual C++ 2015 Redistributable Update 3 (x64 version) fixed it for me. Thanks.", "Hi, I have laid down step-by-step instructions to successfully install Tensorflow GPU on Windows.\r\n\r\nhttps://github.com/bhavsarpratik/install_Tensorflow_GPU_windows", "@bhavsarpratik That is nice you have a guide but I noticed you're are spamming issues with this and that is not cool. The official TensorFlow installation guide is complete and thorough as well and the recommended source for installation instructions", "@Carmezim Pardon me if this looks like spamming. I made the installation guide because I felt that some instructions were missing from the installation guide. ", "@bhavsarpratik you pasted the same message in at least four different issues, which is spamming. \r\nI think PRs are always welcome, feel free to submit one with what you think could be improved in the docs.", "The problem was the cuDNN Library for me - for whatever reason cudnn-8.0-windows10-x64-v6.0 was NOT working - I used cudnn-8.0-windows10-x64-v5.1 - ALL GOOD!\r\n\r\nMy setup working with Win10 64 and the Nvidia GTX780M:\r\n\r\n- Be sure you have the lib MSVCP140.DLL by checking your system/path - if not get it [here](https://www.microsoft.com/en-us/download/details.aspx?id=48145)\r\n- Run the windows installer for python 3.5.3-amd64 from [here](https://www.python.org/downloads/release/python-352/) - DO NOT try newer versions as they probably won't work \r\n- Get the cuDNN v5.1 for CUDA 8.0 from [here](https://developer.nvidia.com/rdp/cudnn-download) - put it under your users folder or in another known location (you will need this in your path)\r\n- Get CUDA 8.0 x86_64 from [here](https://developer.nvidia.com/cuda-downloads)\r\n- Set PATH vars as expected to point at the cuDNN libs and python (the python path should be added during the python install)\r\n\r\nIf you run Windows 32 be sure to get the 32 bit versions (excluding Python) of the files mentioned above.\r\n\r\n", "@drophit TensorFlow only supports Python 64-bit on Windows.", "@Carmezim \r\nInstall Microsoft Visual C++ 2015 Redistributable Update 3 x64. works for me.\r\nThanks!", "having the same issue as what was described above, but I've installed and uninstalled python, created the environment via conda and downloaded the visual studo 2015 .dll files and it still doesn't work. my cuda and cudnn files are also good. not sure what is going on? \r\n\r\n\r\n\r\n(tensorflow) C:\\WINDOWS\\system32>python\r\nPython 3.5.4 |Continuum Analytics, Inc.| (default, Aug 14 2017, 13:41:13) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 985, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 968, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 957, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 938, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 985, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 968, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 957, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 938, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems", "Here are my paths:\r\n\r\n(tensorflow) C:\\WINDOWS\\system32>path\r\nPATH=C:\\ProgramData\\Anaconda3\\envs\\tensorflow;C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\Library\\mingw-w64\\bin;C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\Library\\usr\\bin;C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\Library\\bin;C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\Scripts;C:\\ProgramData\\Anaconda3;C:\\ProgramData\\Anaconda3\\Library\\mingw-w64\\bin;C:\\ProgramData\\Anaconda3\\Library\\usr\\bin;C:\\ProgramData\\Anaconda3\\Library\\bin;C:\\ProgramData\\Anaconda3\\Scripts;C:\\ProgramData\\Oracle\\Java\\javapath;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\libnvvp;C:\\Program Files (x86)\\Intel\\iCLS Client\\;C:\\Program Files\\Intel\\iCLS Client\\;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\IPT;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\IPT;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\Program Files (x86)\\Skype\\Phone\\;C:\\Program Files\\Intel\\WiFi\\bin\\;C:\\Program Files\\Common Files\\Intel\\WirelessCommon\\;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files\\Microsoft SQL Server\\110\\Tools\\Binn\\;C:\\Program Files (x86)\\Microsoft SDKs\\TypeScript\\1.0\\;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;D:\\Visual Studio\\VC\\bin;C:\\Users\\Atish Sawant\\AppData\\Local\\Microsoft\\WindowsApps, C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\bin;C:\\Users\\Atish Sawant\\AppData\\Local\\Microsoft\\WindowsApps", "Fixed the issue, I had installed the cuDNN 7.0 from nVidia."]}, {"number": 6054, "title": "Do the ksize and strides value of tf.nn.max_pool() support tf.Tensor ?", "body": "Here is the problem:\r\nI would like to train a network with two different shapes of input tensor. Each epoch chooses one type. Here I write a small code:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\nwith tf.Session() as sess:\r\n\r\n    imgs1 = tf.placeholder(tf.float32, [4, 224, 224, 3], name = 'input_imgs1')\r\n    imgs2 = tf.placeholder(tf.float32, [4, 180, 180, 3], name = 'input_imgs2')\r\n    epoch_num_tf = tf.placeholder(tf.int32, [], name = 'input_epoch_num')\r\n\r\n    imgs = tf.cond(tf.equal(tf.mod(epoch_num_tf, 2), 0), \r\n                lambda: tf.Print(imgs2, [imgs2.get_shape()], message='(even number) input epoch number is '),\r\n                lambda: tf.Print(imgs1, [imgs1.get_shape()], message='(odd number) input epoch number is'))\r\n\r\n\r\n    print(imgs.get_shape())\r\n\r\n\r\n    for epoch in range(10):\r\n        epoch_num = np.array(epoch).astype(np.int32)\r\n        imgs1_input = np.ones([4, 224, 224, 3], dtype = np.float32)\r\n        imgs2_input = np.ones([4, 180, 180, 3], dtype = np.float32)\r\n\r\n        output = sess.run(imgs, feed_dict = {epoch_num_tf: epoch_num,\r\n                                          imgs1: imgs1_input,\r\n                                          imgs2: imgs2_input})\r\n```\r\nWhen I execute it, the output of `imgs.get_shape()` is `(4, ?, ?, 3)` i.e. `imgs.get_shape()[1]=None, imgs.get_shape()[2]=None`.\r\n\r\nBut I will use the value of the output of `imgs.get_shape()` to define the kernel (`ksize`) and strides size (`strides`) of the `tf.nn.max_pool()` e.g. `ksize=[1,imgs.get_shape()[1]/6, imgs.get_shape()[2]/6, 1]` in the future code. I think ksize and strides cannot support `tf.Tensor` value.\r\n\r\nHow to solve this problem?\r\n\r\nI also asked in the [stackoverflow](http://stackoverflow.com/questions/40932002/tensorflow-how-to-set-the-shape-of-tensor-with-different-conditional-statements).\r\n\r\n\r\n\r\n", "comments": ["Duplicate of https://github.com/tensorflow/tensorflow/issues/4746."]}, {"number": 6053, "title": "Update build status URL", "body": "The previous build name \"tensorflow-master-gpu_pip\" was not accurate\r\nbecause the build is not a pip build. The name has been corrected to\r\n\"tensorflow-master-linux-gpu\". This change fixes the broken link in\r\nREADME.md.", "comments": []}, {"number": 6052, "title": "Fix tutorial docs", "body": "Dear TensorFlow maintainers,\r\n\r\nI noticed a missing closing `)` for the `tf.constant` function in `r0.12` and `r0.11` of `tensorflow/g3doc/tutorials/input_fn/index.md`. This is a fix for the `r0.12` docs.\r\n\r\nShould I submit a separate PR for the `r0.11` documentation too, or will this suffice?\r\n\r\nThanks", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->", "Sorry for the confusion. I noticed the issue in `r0.12` and `r0.11` tags, but it seems that another user has already sent a patch for this and the issue was resolved in the `master` branch. \r\n\r\nmy confusion arose from the fact that the tutorial page seems to default to the latest `r0.` version instead of `master`.\r\n\r\nPlease ignore/close my PR."]}, {"number": 6051, "title": "Branch 140903864", "body": "", "comments": ["Windows failure looks legit.\r\n@mrry It looks like some cuda stuff is leaking into non-cuda build?", "Apparently yes. I don't recognize the symbols `CudaRoot` and `LibdeviceRoot`\u2026 perhaps they're new and whatever CL added them will make it clear what's wrong. (I'm not sure why this wouldn't fail to link on Linux CMake CPU as well, but maybe MSVC is more strict about unused symbols?)", "Maybe this one?\r\n\r\nhttps://github.com/benoitsteiner/tensorflow/commit/e1f44d854e9e6db6ce1f7c70b81d0fef03e4a932", "It'd probably be possible to fix this up by moving this file https://github.com/benoitsteiner/tensorflow/blob/e1f44d854e9e6db6ce1f7c70b81d0fef03e4a932/tensorflow/core/platform/posix/cuda_libdevice_path.cc to `../default` (and modifying the necessary paths in the build config).", "I've fixed the CudaRoot and LibdeviceRoot compilation errors on windows.", "Jenkins, test this please.", "@martinwicke I see experiment_test failures here.\r\nMaybe this PR does not contain the rollback?", "Not included. Was waiting for LGTMs -- it's being submitted now.", "Andrew will push the changes "]}, {"number": 6050, "title": "Run python tests on Windows with Bazel", "body": "The major problem with running TF test on Windows is python tests depend on too many and too big things (eg. pywrap_tensorflow.dll). Since py_test is a zip file on Windows, it's too slow to unzip for running them concurrently.\r\n\r\nWe solve this problem by getting rid of all the deps of py_test, so that every py_test zip only contains it's source code(several of them also contains some data files). Then we install tensorflow pip package into the system before running them.\r\n\r\nBy doing this we can get most of the python test running on Windows, but the disadvantage is the tests are not hermetic in Bazel.\r\n@gunan ", "comments": ["Jenkins, test this please.", "Jenkins, test this please.", "Jenkins is failing to fetch our repo, great.\r\nTrying again.\r\nJenkins, test this please.", "For good measure, one more.\r\nJenkins, test this please.", "@caisq @martinwicke @yifeif looks like this is a solution we can generalize for all our pip tests.\r\nThis is creating py test rules with to dependencies, so we can run these with bazel test with these rules after doing pip install.\r\nWhat do you think?", "This is very cool, I'm all for it. Is redefining py_test an ok thing to do? @caisq FYI.\r\n\r\n@meteorcloudy does this rely on the new and improved sandboxing behavior (allowing access to the installed system by not undeclared dependencies in the source)? \r\n\r\n", "@wicke @gunan @meteorcloudy Cool. I think it's definitely worth a shot to replace the pip tests with this. If it works, it'll bring benefits like better concurrency, better timeout and incrementality.", "I think sandboxing can complicate things here.\r\nWhat happens is, we build tensorflow pip package, we pip install that package, and blaze test these (in the virtual environment we installed the pip package). So the test has to have access to the python pip packages installed. Would sandboxing prevent access to those?", "Actually, I don't think sandbox is a problem for us. Because sandboxing only happens at build time, we don't need the pip package at build time, <del>and there is no sandbox when running the test</del>.\r\n\r\nI tried to use this way to run all the py_test under `//tensorflow/python/...` on Linux, only 2 of them are failing because they depend on something not included in the pip package. And they are also excluded in [test_installation.sh](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/builds/test_installation.sh)\r\n```\r\n//py_test/tensorflow/python:framework_ops_test                           FAILED in 2.0s\r\n  /usr/local/google/home/pcloudy/.cache/bazel/_bazel_pcloudy/e4bf2e19d8ade2926eceaec3a26a8217/execroot/tensorflow/bazel-out/local-fastbuild/testlogs/py_test/tensorflow/python/framework_ops_test/test.log\r\n//py_test/tensorflow/python:protobuf_compare_test                        FAILED in 0.8s\r\n  /usr/local/google/home/pcloudy/.cache/bazel/_bazel_pcloudy/e4bf2e19d8ade2926eceaec3a26a8217/execroot/tensorflow/bazel-out/local-fastbuild/testlogs/py_test/tensorflow/python/protobuf_compare_test/test.log\r\n\r\nExecuted 21 out of 278 tests: 276 tests pass and 2 fail locally.\r\n```\r\n", "Sorry, what I said is wrong. I confirmed with our sandbox expert, there is sandboxing when running tests. But the system installed packages will still be accessible...", "@meteorcloud Thanks for looking into it. It's probably going to work, but it might be good to double check that virtualenv-installed packages work too.", "@caisq you are welcome, I was actually running under virtualenv, and it worked. I didn't specify any sandbox flag, so I believe sandbox is also enabled by default.", "Jenkins, test this please.", "@gunan We just had [Bazel 0.4.2 release](https://github.com/bazelbuild/bazel/releases), after this is merged, I'll send the scripts for GPU tests, so that you can set up the CI jobs!", "I think the failures are all known, but just in case.\r\nJenkins, test this please.", "Jenkins, test this please.\r\n\r\nLinux CPU and GPU issues are known. Windows cmake should be fixed.", "Results as expected, only known issues remaining. merging changes."]}, {"number": 6049, "title": "Fix matmul() 3-D tensor example code", "body": "It is necessary to specify an int32 dtype because the code as written\r\nwould use int64 dtype by default, and int64 is not currently supported\r\nby matmul().\r\n\r\nThe \"2-D tensor\" example code is fine.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 6048, "title": "CUDA_ERROR_OUT_OF_MEMORY (Memory Available)", "body": "Tensorflow is failing like so - very odd since I have memory available and it sees that. This runs fine in CPU only.\r\n\r\nUbuntu 16.04, Cuda 8.0, CUDNN 5.1 for 8.0, Nvidia 367.57 driver, tensorflow_gpu-0.12.0rc0-cp27-none-linux_x86_64.whl. The rest you can see in the log. I have also tried with CUDNN 5.0 with the same result. Cuda 7.5 works for me but is very slow.\r\n\r\n**Log:**\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\nExceptions: 0\r\n1403\r\n1403\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GTX 660\r\nmajor: 3 minor: 0 memoryClockRate (GHz) 1.0975\r\npciBusID 0000:02:00.0\r\nTotal memory: 1.99GiB\r\nFree memory: 1.43GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x2c47900\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: \r\nname: GeForce GTX 660\r\nmajor: 3 minor: 0 memoryClockRate (GHz) 1.0975\r\npciBusID 0000:01:00.0\r\nTotal memory: 1.99GiB\r\nFree memory: 1.41GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 660, pci bus id: 0000:02:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 660, pci bus id: 0000:01:00.0)\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 198.83M (208486400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nTraceback (most recent call last):\r\n  File \"build.py\", line 84, in <module>\r\n    model = tflearn.DNN(net)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tflearn/models/dnn.py\", line 63, in __init__\r\n    best_val_accuracy=best_val_accuracy)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tflearn/helpers/trainer.py\", line 135, in __init__\r\n    keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1000, in __init__\r\n    self.build()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1021, in build\r\n    raise ValueError(\"No variables to save\")\r\nValueError: No variables to save\r\n```\r\n\r\n**Code:**\r\n```\r\nnet = tflearn.input_data(shape=[None, 5])\r\nnet = tflearn.fully_connected(net, 64)\r\nnet = tflearn.fully_connected(net, 64)\r\nnet = tflearn.fully_connected(net, 2, activation='softmax')\r\nnet = tflearn.regression(net)\r\n\r\n# Training\r\nmodel = tflearn.DNN(net)\r\nmodel.fit(X, Y, n_epoch=10000, batch_size=64, show_metric=True)\r\n\r\nfor x in range(0,50):\r\n    rand = randint(0,len(X))\r\n    print(model.predict([X[rand]]), Y[rand])\r\n\r\nmodel.save(\"model.tfl\")\r\n```", "comments": ["The python stack trace doesn't seem to accurately identify where the failing memory allocation is happening.  In general, nothing in a TF program should be calling CudaDriver::DeviceAllocate directly, except the initial allocation of most of the device memory into a pool that will later be sub-allocated.  Your GPUs are rather small.  I can't tell whether the initial allocation is failing, or something later.  \r\n\r\nYou might try adjusting the fraction of visible memory that TF tries to take in its initial allocation.\r\nFor example: Assume that you have 12GB of GPU memory and want to allocate ~4GB:\r\n\r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\r\n\r\nsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))", "Closing due to inactivity. If @poxvoculi's response didn't help and you are still encountering this issue, please let us know, and I'll re-open.", "@jart Hi. I am a new learner of TF. And I have similar problem as the above. I have tested the solution of @poxvoculi , but it didn't work anyway.\r\nThe following is my problem description:\r\nCodes are from the TF official website, [Basic usage](https://www.tensorflow.org/get_started/get_started#basic_usage)\r\nI set up a virtual environment of Python+TensorFlow-gpu on Ubuntu 16.04\r\nterminal prompting information as following:\r\n```\r\n(tfStarted) zero@zero:$ python officialBasicUsage.py \r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpJkInjp\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:From /home/zero/ownCloud/DeepLearning/TensorflowCode/tfStarted/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GTX 1080\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.835\r\npciBusID 0000:03:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 7.30GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:03:00.0)\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 7.92G (8499167232 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:From /home/zero/ownCloud/DeepLearning/TensorflowCode/tfStarted/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:03:00.0)\r\nW tensorflow/core/framework/op_kernel.cc:993] Out of range: RandomShuffleQueue '_1_enqueue_input/random_shuffle_queue' is closed and has insufficient elements (requested 4, current size 0)\r\n\t [[Node: random_shuffle_queue_DequeueUpTo = QueueDequeueUpToV2[component_types=[DT_INT64, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](enqueue_input/random_shuffle_queue, random_shuffle_queue_DequeueUpTo/n)]]\r\nWARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\r\n\r\n```\r\nEven through solution of poxvoculi was involved, problem  is still there. \r\nI have searched such problem in Google with keywords--\"failed to allocate from device CUDA_ERROR_OUT_OF_MEMORY\"\r\nAnd there is a similar Q&A in [stackoverflow](http://stackoverflow.com/questions/34514324/error-using-tensorflow-with-gpu), like poxvoculi's, but it doesn't work.\r\nSo, what's the real problem? GPU capacity allocation error? or others?\r\nAnd how can I solve this? Thank you very much.", "Please look again at my earlier post and the stackoverlfow post you cite.  According to your logging messages, you're trying to allocate 7.92GB from a GPU that has only 7.30GB free.  You need to reduce the allocation fraction using tf.GPUOptions.", "@poxvoculi  You are right. I am a bit careless. \r\nBut there are some warnings even though answers are showed. \r\nMaybe I am a beginner to TensorFlow, it needs some time to grasp TensorFlow basic concepts. \r\nThank you very much. ", "@cnzero We're glad you've chosen to use TensorFlow. We put a lot of work into this library and hope it serves you well. In return we ask that the community be respectful of the fact that this issue tracker is for bugs and feature requests, not an escalation path for [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow).", "meeting same quetion:CUDA_ERROR_OUT_OF_MEMORY\r\n2017-07-12 16:37:11.170870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:2) -> (device: 2, name: GeForce GTX 1080, pci bus id: 0000:05:00.0)\r\n2017-07-12 16:37:11.207585: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 198.29M (207925248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-07-12 16:37:23.575049: E tensorflow/stream_executor/cuda/cuda_blas.cc:365] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-07-12 16:37:23.575072: W tensorflow/stream_executor/stream.cc:1601] attempting to perform BLAS operation using StreamExecutor without BLAS support\r\n2017-07-12 16:37:23.575157: F tensorflow/cc/tutorials/example_trainer.cc:129] Non-OK-status: session->Run({{\"x\", x}}, {\"y:0\", \"y_normalized:0\"}, {}, &outputs) status: Internal: Blas GEMV launch failed:  m=2, n=2\r\n\t [[Node: y = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Cast, _arg_x_0_0/_11)]]\r\nAborted (core dumped)\r\n \r\n", "Since this still seems to be a major issue, just wondering if we can use CUDA through numba and force the GPU to kill all existing processes (which should automatically release memory)...\r\n\r\nWould appreciate if anyone has any insights on this", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "I was experienced memory error in Ubuntu 18.10. When i changed resolution of my monitor from 4k to fullhd (1920-1080) memory available become 438mb and neural network training started. I was really surprised by this behavior.\r\n\r\nBy the way, i have Nvidia 1080 with 8gb memory, still dont know why only 400mb available\r\n"]}, {"number": 6047, "title": "Fix cuda version detect on Windows", "body": "On Windows, the name of the shared libraries is like:\r\ncudart64_80.dll and cudnn64_5.dll\r\n@davidzchen @gunan ", "comments": ["Jenkins, test this please.", "Jenkins, test this please.", "The multiple runs seem to have covered all flaky tests. I will merge this now."]}, {"number": 6046, "title": "ResourceExhaustedError when save model. Both memory and disk are enough", "body": "Hi,\r\n  I was running tensorflow 0.12.0rc0 on ubuntu 14.04. The training is on CPU only. The crash happened when saver saves the model.\r\n\r\n>W tensorflow/core/framework/op_kernel.cc:975] Resource exhausted:/home/code/2016_11_28_90d_strict_win10.emb-354830.data-00000-of-00001\r\nTraceback (most recent call last):\r\n  File \"/home/code/tf_train.py\", line 474, in <module>\r\n    save_model_and_log_step(tf_saver=saver, tf_session=session, global_step=step)\r\n  File \"/home/code/tf_train.py\", line 199, in save_model_and_log_step\r\n    tf_saver.save(tf_session, FLAGS.out_model_path, global_step=global_step)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1323, in save\r\n    {self.saver_def.filename_tensor_name: checkpoint_file})\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: ~/2016_11_28_90d_strict_win10.emb-354830.data-00000-of-00001\r\n         [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_r$\r\ncv_save/Const_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, embeddings, embeddings/Adagrad, sm_b, sm_b/Adagrad, sm_w_t, sm_w_t/Adagrad)]]\r\n>Caused by op u'save/SaveV2', defined at:\r\n  File \"/home/code/train.py\", line 430, in <module>\r\n    saver = tf.train.Saver(max_to_keep=FLAGS.max_to_keep)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1000, in __init__\r\n    self.build()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1030, in build\r\n    restore_sequentially=self._restore_sequentially)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 622, in build\r\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 229, in _AddSaveOps\r\n    save = self.save_op(filename_tensor, saveables)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 172, in save_op\r\n    tensors)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 552, in save_v2\r\n    tensors=tensors, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\r\n    self._traceback = _extract_stack()\r\n>ResourceExhaustedError (see above for traceback): ~/2016_11_28_90d_strict_win10.emb-354830.data-00000-of-00001\r\n         [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_re\r\ncv_save/Const_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, embeddings, embeddings/Adagrad, sm_b, sm_b/Adagrad, sm_w_t, sm_w_t/Adagrad)]]\r\n\r\nNo OOM or OOD happened on the machine. I remember this was a bug in 0.9.0 when saving a large tensor it has tensor size too large (>2G) problem because of a proto field is defined as int32. I am not sure if it's related?\r\n\r\n", "comments": []}, {"number": 6045, "title": "Not able to use tensorflow::mutex (../nptl/pthread_mutex_lock.c: No such file or directory)", "body": "Hi All,\r\n\r\nI am trying to add a self-defined class under the core/common_runtime/ directory to log the information of each step([https://github.com/laosiaudi/tensorflow/blob/test/tensorflow/core/common_runtime/graph_logger.cc]) and try to use the tensorflow::mutex as step_stats_collector does, but when I run the example/tutorials/mnist/mnist_with_summaries.py, it throws out a segmentation fault. After inspecting via gdb, it reports the error **\"../nptl/pthread_mutex_lock.c: No such file or directory.\"**.\r\n\r\nCould you please provide some advice of how to use tensorflow::mutex correctly or how to fix such errors? Should I modify some BUILD files? I have been stuck in this for a while.\r\n\r\nThanks!\r\n\r\nInformation about the configuration:\r\nOperating system: Ubuntu14.04\r\nbazel version:  \r\n-- Build label:0.4.1\r\n-- Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\n\r\nInstalled from source and no CUDA supported", "comments": ["I'm puzzled by your description.  If you're failing to correctly link some mutex-related library, that should be a build error, not a runtime segfault.  Maybe gdb is having trouble finding the symbols corresponding to the binary?\r\n\r\nHere are some third-party instructions for using gdb with bazel and TF.  I have not tried them myself.\r\n\r\nhttps://gist.github.com/Mistobaan/738e76c3a5bb1f9bcc52e2809a23a7a1", "@poxvoculi \r\nThanks for the reply!\r\nSorry for the misunderstanding. But yes, I also think it shouldn't be a runtime segfault and gdb may not give the correct information. And today I put my self-defined class into the step_stats_collector.cc file(because my class uses the mutex in the same way as step_stats_collector does) and tried to run the same python examples, it still failed, giving error like\r\n **\"terminate called after throwing an instance of 'std::system_error'\r\n  what():  Operation not permitted\"**.\r\n\r\nAnd from the gdb, I check the backtrace for all the threads, most of them abort in the **std::condition_variable::wait**, so I think is it because my way of using mutex is not correct?\r\n(my class's member function using mutex gets called in the ExecutorState::NodeDone)\r\n\r\nThanks!\r\n\r\n\r\n", "I think this question is better addressed on [stackoverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is a usage question, not a bug report. ", "@michaelisard Okay, thanks for advice!"]}, {"number": 6044, "title": "Fix bugs in configure and cuda_configure.bzl re. lib paths", "body": "See log of failed mac build from yesterday:\r\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-mac/156/console\r\n\r\nCaused by PRs:\r\nhttps://github.com/tensorflow/tensorflow/commit/dfae1931365dae3354a8980829dfabcd59761232\r\nhttps://github.com/tensorflow/tensorflow/pull/5944/files", "comments": ["Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 6043, "title": "Fixed incorrect use of NamedTemporaryFile", "body": "See http://stackoverflow.com/questions/23212435/permission-denied-to-write-to-my-temporary-file\r\n\r\nThis was causing \"PermissionError: [Errno 13] Permission denied\" on Windows when urlretrieve() attempts to open the already open temp file.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please.", "@gunan This would be a good candidate for cherry-picking into `r0.12`.", "Interesting... I think I have already fixed that previously in #5795. Not sure why it was reverted?", "Sorry about that @vit-stepanovs - it was probably a bad merge.", "The failure is an infra issue. \r\nJenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 6042, "title": "Negative axis support for argmin and argmax", "body": "Fixes #6022 \r\n\r\n", "comments": ["Can one of the admins verify this patch?", "Needs lots of tests!", "@vincentvanhoucke Thanks for the review! I don't have much experience with bazel. Can you tell me how can I run unit tests? Also, how can I run specific tests only like `math_op_test.cc`", "@AnishShah I have little experience myself running outside of Google's environment, which differs somewhat. I'd expect that replacing `bazel build` with `bazel test` gets you most of the way there, and you can narrow down which tests to run by specifying `bazel test //tensorflow/<path to test>`", "@vincentvanhoucke I have made the changes you requested and also added some tests. Thanks!", "LGTM. @aselle FYI.", "@vincentvanhoucke I have made the changes in docs.", "@vincentvanhoucke Done. :)", "Will the tests run now that you have approved the changes? Sorry, I'm not familiar with this as this is my first PR.", "@tensorflow-jenkins test this please", "I think, there's some error with jenkins. Not sure. ", "@vincentvanhoucke Can you run the tests again?\r\n", "@tensorflow-jenkins test this please", "@vincentvanhoucke few tests are failing but I don't think it is because of my changes. Not sure.", "Looks like it's unrelated. I'll let our build cops take this over.", "Thanks all!", "Thanks so much @AnishShah!"]}, {"number": 6041, "title": "replaced deprecated call to all_variables by tf.global_variables()", "body": "Title says it all -> all_variables() is deprecated and should be replaced by global_variables. ", "comments": ["Can one of the admins verify this patch?", "This change is made in #6078.", "#6078 is now merged. Closing this PR."]}, {"number": 6040, "title": "avoid tf.int32-only call to gather in moments, thereby keeping kernel on GPU", "body": "Fixes #6027 (tested locally)\r\n@vincentvanhoucke \r\nSorry for being late here. I think this might be a slightly better fix. I tested this locally, and it did resolve the cpu-gather behavior that I was seeing.\r\n@poxvoculi \r\nI thought about it for a while, and I couldn't figure out how to avoid `tf.gather` completely in a reasonable manner. I hope that this patch is acceptable.", "comments": ["Can one of the admins verify this patch?", "Prefer #6031 to this equivalent fix. Reviewers please comment on that PR. @liuyipei thanks for raising the issue!"]}, {"number": 6039, "title": "Precision of reduce_sum operation", "body": "There seems to be a numerical precision problem with the reduce_sum operation, see the example provided below. Even though everything is set to float32 as datatype tensorflow produces a much more imprecise result than numpy (whose result is numerically exact).\r\n\r\nIs this intended? It seems strange that tensorflow makes such serious numerical errors. Am I doing something wrong?\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nnothing there to my knowledge\r\n\r\n### Environment info\r\nOperating System: Mac OSX 10.12.1\r\nTensorflow 0.11.0 CPU\r\nhttps://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.11.0-py2-none-any.whl\r\n\r\ncould be reproduced in docker-container `tensorflow/tensorflow:0.12.0-rc0` in a virtualbox linux vm\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndata = tf.constant(1.23456, shape=[250,250], dtype=tf.float32)\r\nsum = tf.reduce_sum(data)\r\nsess = tf.Session()\r\nsess.run(sum) #77151.477\r\n\r\ndata2 = np.full([250, 250], np.float32(1.23456), dtype=np.float32)\r\nnp.sum(data2) #77160\r\n```\r\nhttps://gist.github.com/flash1293/84c2719dd646c0314ec0f4ea05df117a\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n-\r\n\r\n### Logs or other output that would be helpful\r\n\r\n-\r\n", "comments": ["I believe numpy implements a better (as in more numerically accurate) algorithm for sum as per this: https://github.com/numpy/numpy/pull/3685\r\n\r\nI would prefer not to say that numpy's result is _exact_ however.  Exactness when dealing with floating point is very hard :)\r\n\r\nThe gpu version of tensorflow sum should be just as numerically stable.  The CPU version unfortunately is implemented in the naive way with a single accumulator.  I'll let Benoit comment on the likelihood of this improving.  A quick hack might be to use a 64-bit accumulator even if the input is 32-bit.\r\n\r\n", "Thanks for the insight - https://github.com/tensorflow/tensorflow/issues/2625 is related to this.\r\n\r\nBy exact I meant exact in the provided example.", "Same as #5527 ?", "@ppwwyyxx yes, this is the same problem.", "Closing this issue as it is a dupe  of #5527 "]}, {"number": 6038, "title": "Fix issue #6036", "body": "As per the issue raised [here](https://github.com/tensorflow/tensorflow/issues/6036), changed the following code to the  [**6_lstm.ipynb**](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/6_lstm.ipynb) file.\r\n\r\n**FROM:**\r\n```python\r\ndef read_data(filename):\r\n  f = zipfile.ZipFile(filename)\r\n  for name in f.namelist():\r\n    return tf.compat.as_str(f.read(name))\r\n  f.close()\r\n```\r\n\r\n**TO:**\r\n\r\n```python\r\ndef read_data(filename):\r\n  with zipfile.ZipFile(filename) as f:\r\n    name = f.namelist()[0]\r\n    data = tf.compat.as_str(f.read(name))\r\n  return data\r\n```\r\n", "comments": ["Can one of the admins verify this patch?", "AFAIK we have no ipynb tests, so I will just merge this."]}, {"number": 6037, "title": "Cannot get the values of tf.contrib.learn.DNNRegressor.predict() in boston.py", "body": "I am following the tutorials on the tensorflow's official website. And when I executed boston.py, I cannot get the prediction values. But when I executed iris.py, I can get the prediction values. The only difference I found between these two files is that boston.py uses DNNRegressor, but iris.py uses DNNClassifier. I am using **0.12 tensorflow, ubuntu 16.04, CUDA 8.0, cuDNN 5.1.5**, my gpu is **TITAN X (Pascal)**.\r\nHere is the ending output of boston.py:\r\n```bash\r\nINFO:tensorflow:Summary name dnn/hiddenlayer_0:fraction_of_zero_values is illegal; using dnn/hiddenlayer_0_fraction_of_zero_values instead.\r\nINFO:tensorflow:Summary name dnn/hiddenlayer_0:activation is illegal; using dnn/hiddenlayer_0_activation instead.\r\nINFO:tensorflow:Summary name dnn/hiddenlayer_1:fraction_of_zero_values is illegal; using dnn/hiddenlayer_1_fraction_of_zero_values instead.\r\nINFO:tensorflow:Summary name dnn/hiddenlayer_1:activation is illegal; using dnn/hiddenlayer_1_activation instead.\r\nINFO:tensorflow:Summary name logits:fraction_of_zero_values is illegal; using logits_fraction_of_zero_values instead.\r\nINFO:tensorflow:Summary name logits:activation is illegal; using logits_activation instead.\r\nPredictions: <generator object _as_iterable at 0x7ff9f406db90>\r\n```\r\n", "comments": ["+1 I have the same problem. Same environment as CTTC, except my gpu is GeForce GTX 1080.", "Waiting for help. Thanks", "The predictions is iterable object. Try explicitly converting them into a list. ", "@terrytangyuan I have tried like `  print(\"Predictions: {}\".format(list(y)))`,but then the program just stuck to this line, and never ended. Besides, it output these two lines when the program ran to this print line:\r\n```\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0)\r\nINFO:tensorflow:Loading model from checkpoint: /tmp/tmpH45TBo/model.ckpt-5000-?????-of-00001.\r\n```\r\nSo using list still not work.", "@CTTC @krishnanareddy, thanks for the bug report, and apologies for the error here.\r\n\r\nWith TF v0.12, the new default behavior for predict() is to return an iterable, as @terrytangyuan notes.\r\n\r\nIf you change the following:\r\n\r\n```python\r\ny = regressor.predict(input_fn=lambda: input_fn(prediction_set))\r\nprint(\"Predictions: {}\".format(str(y)))\r\n```\r\n\r\nto:\r\n\r\n```python\r\ny = regressor.predict(input_fn=lambda: input_fn(prediction_set))\r\n# .predict() returns an iterator; convert to a list and print predictions\r\npredictions = list(itertools.islice(y, 6))\r\nprint(\"Predictions: {}\".format(str(predictions)))\r\n```\r\n\r\nthe example will work. I'll get the code in GitHub updated accordingly ASAP.\r\n\r\nLet me know if you still encounter any errors.", "@sandersk  Thanks, that solved the problem", "Yep, it solved for me too. Thanks for the prompt response.", "Sure thing! Updates to the tutorial and companion code are now posted:\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/f8233e0f937d00a33e4e49cc593688de76159f0d", "Hey @sandersk,\r\n\r\n   Would you mind updating the tutorial in https://www.tensorflow.org/versions/r0.11/tutorials/input_fn/ as well?\r\n\r\n   Btw, before the error aforementioned, there is also a missing brace in the code\r\n\r\nfeature_cols = {k: tf.constant(data_set[k].values\r\n                  for k in FEATURES}\r\n\r\n    which is also there in the same tutorial.\r\n\r\nThanks!", "Hi @ramurti,\r\n\r\nWe version the tutorials in line with the versions of the TensorFlow API. The latest version of the tutorial can currently be found here (and will be published out as the default version of the tutorial on the TF site with the next TF release):\r\n\r\nhttps://www.tensorflow.org/versions/master/tutorials/input_fn/\r\n\r\nThis version contains the updated `predict()` logic, as well as fixes the missing close-parenthesis in the code you cite.\r\n\r\nHope this helps,\r\nSanders\r\n\r\n"]}, {"number": 6036, "title": "File object not being closed properly in 6_lstm.ipynb", "body": "The [**6_lstm.ipynb**](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/6_lstm.ipynb) file contains the following code: \r\n\r\n```python\r\ndef read_data(filename):\r\n  f = zipfile.ZipFile(filename)\r\n  for name in f.namelist():\r\n    return tf.compat.as_str(f.read(name))\r\n  f.close()\r\n```\r\n\r\nThis code has the following issues: \r\n\r\n- The file object `f` does not get to close properly, since the function is \r\n  exited before the `f.close()` method ever gets to be called. \r\n- The `for` loop is uneccesary, since there is only one item in the zip file, \r\n  *and* since the `return` operator forces the loop to end on the first \r\n  loop anyway. \r\n  \r\nI propose the following code instead: \r\n\r\n```python\r\ndef read_data(filename):\r\n  with zipfile.ZipFile(filename) as f:\r\n    name = f.namelist()[0]\r\n    data = tf.compat.as_str(f.read(name))\r\n  return data\r\n```\r\n\r\nThis code fixes the issues in the following ways: \r\n\r\n- The `with` statement will handle the file `f` being closed automatically, even \r\n  if there is an unexpected early termination. \r\n- Since there is actually only one item in the zip file, then we can just get \r\n  the zeroeth element in `f.namelist()`\r\n\r\nI will create a pull request with this suggested code shortly. ", "comments": []}, {"number": 6035, "title": "Error message for running tf.nn.max_pool_with_argmax() on CPU", "body": "Running `tf.nn.max_pool_with_argmax()` on CPU gives a very obscure error:\r\n\r\n`tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'MaxPoolWithArgmax' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n`  <no registered kernels>\r\n\r\nFrom this line:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/bc64f05d4090262025a95438b42a54bfdc5bcc80/tensorflow/core/kernels/maxpooling_op.cc#L672\r\n\r\nI think it's useful to mention `tf.nn.max_pool_with_argmax()` is only implemented for GPU instead.", "comments": ["Rather than trying to fix this one Op, it might be better to have a solution where on an error like this the feasible device implementations are listed.  Opening internal bug, but it may not be acted on soon.", "Perhaps it's about having this registered with the CPU as well. [tf.nn.max_pool_with_argmax](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard9/tf.nn.max_pool_with_argmax.md)\r\n\r\n```python\r\nREGISTER_KERNEL_BUILDER(Name(\"MaxPoolWithArgmax\")\r\n                            .Device(DEVICE_GPU)\r\n                            .TypeConstraint<int64>(\"Targmax\")\r\n                            .TypeConstraint<float>(\"T\"),\r\n                        MaxPoolingWithArgmaxOp<Eigen::GpuDevice, float>);\r\nREGISTER_KERNEL_BUILDER(Name(\"MaxPoolWithArgmax\")\r\n                            .Device(DEVICE_GPU)\r\n                            .TypeConstraint<int64>(\"Targmax\")\r\n                            .TypeConstraint<Eigen::half>(\"T\"),\r\nMaxPoolingWithArgmaxOp<Eigen::GpuDevice, Eigen::half>);\r\n```\r\n\r\nThis might be somehow useful and related as well [Missing gradient for tf.nn.max_pool_with_argmax #1793](https://github.com/tensorflow/tensorflow/issues/1793)", "I believe this is fixed and I see there also was a PR.  ", "Is there an alternative for tf.nn.max_pool_with_argmax? Because now I only have CPUs.", "Is there an alternative for tf.nn.max_pool_with_argmax? Because now I only want to test my trained model on CPU env.", "@MilesZhao @lujian9328 I think it is better to open a new issue for CPU support.", "@tfboyd seems still not working on CPUs. Can you reopen this?", "@tfboyd any chance to have it available also on CPU? I'd like to contribute if I could, but I never worked on tensorflow source code.\r\nShould this issue be reopen or should we file a new one?\r\n", "Yes, would be great to have tf.nn.max_pool_with_argmax working on CPU. \r\n\r\nThis ops seems to be very useful for fast encoder-decoder architectures (I see it two segmentation networks -- ENet and SegNet)  which were designed for 'real time' image segmentation. So supposedly people look at performance with a view of running these networks on CPUs or mobile.", "Great to see attention to this issue!\r\n\r\nmaybe is it better to open a new issue with a proper title as simple as, e.g., \"max_pool_with_argmax unavailable on CPU\" rather than this one that, actually, refers to an error message?\r\n\r\n@tfboyd if you agree I'll open a new issue so you can close this one.\r\n", "Great idea!\n\nI have tried to express maxpool with argmax through operations available on CPU but so far did not succeed. Anyone has simple ideas? \n\n_____________________________\nKind Regards,\nAlexey Simonov\n\n\n> On 22 Oct 2017, at 13:25, Marco Di Benedetto <notifications@github.com> wrote:\n> \n> Great to see attention to this issue!\n> \n> maybe is it better to open a new issue with a proper title as simple as, e.g., \"max_pool_with_argmax unavailable on CPU\" rather than this one that, actually, refers to an error message?\n> \n> @tfboyd if you agree I'll open a new issue so you can close this one.\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n", "I would like to get this function to work on CPU as well. I am trying to implement Segnet and E-net.", "It's a very important Op for my model. Hope it will be solved soon.", "It would be useful for me too....", "I was able to make a workaround for CPU.\r\nWarning: it is slow, bloated, and basically a last resort. But I can get ENet to run on my cpu at 5s/image...\r\nRight now it is only for 2x2 maxpooling, but you could change the numbers and add things to expand upon this example.\r\n```python\r\nnet_main, pooling_indices = tf.nn.max_pool_with_argmax(inputs,\r\n                                                                   ksize=[1,2,2,1],\r\n                                                                   strides=[1,2,2,1],\r\n                                                                   padding='SAME',\r\n                                                                   name=scope+'_main_max_pool')\r\n```\r\nbecomes\r\n```python\r\nnet_main = tf.nn.max_pool(inputs,\r\n                        ksize=[1,2,2,1],\r\n                        strides=[1,2,2,1],\r\n                        padding='SAME',\r\n                        name=scope+'_main_max_pool')\r\n\r\ninput_shape = inputs.get_shape().as_list()\r\nmask_shape = [input_shape[0], input_shape [1]/2,input_shape[2]/2, input_shape[3]]\r\npooling_indices = tf.zeros(mask_shape, dtype=tf.int64)\r\nfor n in range(mask_shape[0]):\r\n    for i in range(mask_shape[1]):\r\n        for j in range(mask_shape[2]):\r\n            in_indices = [ [n, w, h] for w in range(i*2, i*2+2) for h in range(j*2, j*2+2)]\r\n            slice = tf.gather_nd(inputs, in_indices)\r\n            argmax = tf.argmax(slice, axis=0)\r\n            indices_location = [[n, i, j, d] for d in range(input_shape[3])]\r\n            sparse_indices = tf.SparseTensor(indices=indices_location, values=argmax, dense_shape=mask_shape)\r\n            pooling_indices = tf.sparse_add(pooling_indices, sparse_indices)\r\n```\r\n\r\nAlso, that part of the gpu kernel is on [this line](https://github.com/tensorflow/tensorflow/blob/40eef4473bda90442bb55fcc67842f097c024580/tensorflow/core/kernels/maxpooling_op_gpu.cu.cc#L88). Simply copying that code from the gpu kernel to the cpu kernel may not be so bad for this operation. It is a nice operation to have, and even without optimization, it would probably still be much faster than what I ended up doing :stuck_out_tongue: \r\n", "I may submit a workaround for this soon\r\n", "Hello, also need this op for CPU, vital to my project to run on mobile.\r\n@shadySource , when I apply your workaround and try to retrain the model, the training seems to be launched on my CPU instead of my GPU and fails (the GPU device for training is not created, as it is with the original code, and the process eventually kills itself).", "@mmpinso \r\nI think all of the ops for my workaround ended up using multiple gigs of memory simply with ENet. There is a pretty good chance it would not fit in a gpu (or even system ram) during training. Definitely would not fit in most mobile devices, and it is pretty slow.\r\n\r\nIn the mean time, I might try to make a lightweight segmentation architecture which uses features from the downsampling layers to guide the upsampling layers. Maybe an architecture like this already exists...\r\n\r\nAnyway, anyone want to learn [how to make ops](https://www.tensorflow.org/extend/adding_an_op)?", "@shadySource  @mmpinso  I actually just copied the operation from GPU to CPU and was able to load the model and run it. I don't know if tensorflow will take my pull request as it looks hacky. But I can provide the patch directly for you to try if you want.", "@saeed68gm that would be great", "@saeed68gm , would it be possible if you could give the link to PR or write down the steps for the fix ?\r\nThanks.", "@mmpinso here is the patch for TF version 1.4. It is very hacky but it should help you load up your model into tensoflow. May I ask which model you are trying to integrate into the phone? For me I had to add a workout for another operation as well.\r\n\r\n@imransalam The steps are basically what @shadySource shared above in the tensorflow documentation for adding your own op.\r\n\r\nif anybody actually implemented these operations please share you solution. Thank you.\r\n[max_pool_1.4.txt](https://github.com/tensorflow/tensorflow/files/1612099/max_pool_1.4.txt)\r\n", "Thanks a lot @saeed68gm , I'm trying to run [ENet](https://github.com/kwotsin/TensorFlow-ENet) on Android. What was the other op you had to add?\r\nTo get the mobile working version of this workaround should do as following or am I missing something?\r\n\r\n1. [Apply](https://www.tensorflow.org/extend/adding_an_op) the changes to my local tensorflow source code\r\n2. [Compile](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/android) libtensorflow_inference.so with Bazel with the updated tensorflow local source code\r\n3. Remaining usual steps\r\n", "@mmpinso awesome. That is correct. Just make sure you include that in your lib files in android and you are all set. For me I had to add MaxPoolGradWithArgmax to CPU as well, but I couldn't get it working so it is a blank op. MaxPoolGradWithArgmax was only used for training so I need the op to load the model and freeze it, after freezing it I did not need that op anymore.\r\n\r\nGood luck on running your model on the phone!\r\n  ", "@saeed68gm\uff0cthanks,I will follow your instructions next. \r\n@mmpinso ,same with you,I'm doing work with ENet on Android. So how's the latency you run it on your mobile ?", "@mmpinso, @liangxiao05 why do you run enet instead of segnet? Did you run enet on cpu?", "@DenisN03 Until recently I've been the UX guy in our (week-ends/never ending) project, but recently I've had to work on the model part too. Got decent results with ENet (finally) and would like to have a first proto on the phone. Haven't tried Segnet yet but I take note.\r\n@liangxiao05 , because of the reason above I'm still struggling to compile tensorflow with the changes, but I'll comment again once I can share the latency I obtain.\r\n@saeed68gm , @liangxiao05 , could you please share the entire maxpooling_op.cc file ? Having a hard time trying to compile it when I (wrongly) apply the diffs ...", "@mmpinso Sure, I can share it in a bit. Did you make sure you are on tensorflow version 1.4? the patch should apply seamlessly. Just change the extension to .patch and apply using the command git apply max_pool_1.4.patch", "@saeed68gm that was it, I was on the main/default branch. Changed to cuda 8 & cudnn 6 also and recompiling tensorflow now; thanks !!", "@saeed68gm, @mmpinso I tried to apply the command \"git apply max_pool_1.4.patch\" but got the error \"fatal: the patch is corrupted on line 143\"", "@DenisN03 , @saeed68gm I had this same error so I ended applying it manually. It compiled, but I'm afraid I might introduced some kind of fault. It would help a lot to have the full .cc file", "@mmpinso @DenisN03 \r\nNot sure why you get that error, it is pretty strange.\r\nI am uploading the full file here. I have to change the extension to .txt to upload on github, maybe that introduces issues?\r\nEither way, let me know if this helps! sounds like you both have very interesting projects going on :)\r\n\r\n[maxpooling_op.txt](https://github.com/tensorflow/tensorflow/files/1626793/maxpooling_op.txt)\r\n", "@saeed68gm with this version, I'm getting \"redefinition of class\" error on classes MaxPoolingWithArgmaxOp and MaxPoolingGradWithArgmaxOp.\r\n(compiling via bazel build  --config=opt --config=cuda --incompatible_load_argument_is_label=false --action_env=\"LD_LIBRARY_PATH=${LD_LIBRARY_PATH}\" tensorflow/tools/pip_package:build_pip_package )", "@mmpinso  Hmmm... I am not sure why you are getting that. Maybe your config is not correct? I only compiled it for android after that so could be because you are trying to compile it for cuda as well. As far as I know, you do not need the extra arguments when you are compiling the android inference library.", "@saeed68gm Ok I think I understood the context: I should be using this custom tf version only to obtain the android inference library and run the model on the phone. So I'll install it in a separate env and keep my usual tf installation in another one for the trainings and everything else. I reviewed the config accordingly and now it compiles.\r\n( @liangxiao05 I haven't forgot regarding the inference time )", "@liangxiao05 did you get the following error? No OpKernel was registered to support Op '**ScatterNd**' with these attrs.\r\n\r\n[[Node: ENet/unpool/ScatterNd = ScatterNd[T=DT_FLOAT, Tindices=DT_INT32](ENet/unpool/transpose, ENet/unpool/Reshape_2, ENet/unpool/ScatterNd/shape)]]\r\n\r\n@saeed68gm , could this be because of \r\n\r\n- The commented operation below in the modified maxpooling_op.cc ?\r\n\r\nInside Compute() for MaxPoolingGradWithArgmaxOp\r\nlines 1069-1070\r\n    // SpatialMaxPoolWithArgMaxHelper<CPUDevice, T>(\r\n    //     context, grad_out, &argmax, grad_in, params, padding_);\r\n\r\n- SpatialMaxPoolWithArgMaxHelper used instead of LaunchMaxPoolingWithArgmax inside MaxPoolingWithArgmaxOp / Compute ?\r\n\r\nThe intuition behind is that ScatterNd is used by unpool, which uses the pooling indices that had been computed by max_pooling_with_argmax during the downsampling phase. If such indices had not been computed or \"wrongly\" computed, this might have an impact on the unpool operation using such indices.", "@DenisN03\uff0chi, cuz ENet seems to be  mush faster then segnet on Desktop\uff0cif I try to run it  on android in real time ,latency is the most important thing.\r\n@mmpinso Yor are ahead of me ,  I haven't succeeded to run ENet on CPU even on my Desktop . I just see the full maxpooling_op.cc  shared by @saeed68gm\uff0cthere are some differences between mine . I will try more in the next few days ,once there's some progress, and I'll be in sync :)", "> SpatialMaxPoolWithArgMaxHelper used instead of LaunchMaxPoolingWithArgmax inside MaxPoolingWithArgmaxOp / Compute ?\r\n\r\n@mmpinso No, if you look at the code, the LaunchMaxPoolingWithArgmax function will call the cuda code to compute maxPoolingWithArgmax on GPU. The problem is the unpooling operation that uses the scatter function and looks like everybody need the unpooling operation. Can somebody maybe implement this operation for CPU and share the code?\r\n\r\nHere is a sample in python, we need to create it in CPP:\r\n```\r\ndef unpool(pool, ind, ksize=[1, 2, 2, 1], scope='unpool'):\r\n    \r\n\twith tf.variable_scope(scope):\r\n\t\tinput_shape = pool.get_shape().as_list()\r\n\t\toutput_shape = (input_shape[0], input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3])\r\n\r\n\t\tflat_input_size = np.prod(input_shape)\r\n\t\tflat_output_shape = [output_shape[0], output_shape[1] * output_shape[2] * output_shape[3]]\r\n\r\n\t\tpool_ = tf.reshape(pool, [flat_input_size])\r\n\t\tbatch_range = tf.reshape(tf.range(output_shape[0], dtype=ind.dtype), shape=[input_shape[0], 1, 1, 1])\r\n\t\tb = tf.ones_like(ind) * batch_range\r\n\t\tb = tf.reshape(b, [flat_input_size, 1])\r\n\t\tind_ = tf.reshape(ind, [flat_input_size, 1])\r\n\t\tind_ = tf.concat([b, ind_], 1)\r\n\r\n\t\tret = tf.scatter_nd(ind_, pool_, shape=flat_output_shape)\r\n\t\tret = tf.reshape(ret, output_shape)\r\n\t\treturn ret\r\n```\r\n", "@saeed68gm, @mmpinso, @liangxiao05 what ENet implementation are you using? I use this -\r\n https://github.com/fregu856/segmentation.", "@DenisN03  I am not trying to run ENet. I am using it for matting here:\r\nhttps://github.com/Joker316701882/Deep-Image-Matting\r\n\r\nI may try to implement unpooling. It seems pretty straightforward, but multithreading is a bit of challenge for me.", "@DenisN03 the version you use also uses max_pool_with_argmax as [Kwotsin's](https://github.com/kwotsin/TensorFlow-ENet).\r\n@liangxiao05 , @saeed68gm , we are trying the following approach: \r\n- replace max_pool_with_argmax by simple max_pool, without getting the indices\r\n- unpool with resize_nearest_neighbor or resize_bilinear\r\nThe first tests show no apparent loss on our application (binary segmentation), dont know yet regarding the inference time on CPU.\r\n\r\nIf this workaround finally does not suit our needs, we'll probably just switch to Caffe2...", "@mmpinso, did you manage to cope with the mistake related with ScatterNd?", "@DenisN03 We applied the solution I mentioned before and we have it running. Inference time is around 3 seconds on a Huawei P10 Lite.", "@mmpinso did have to retrain your model to achieve that?", "@mmpinso 3s seems to be too long for mobiles applications . In your opition , is it feasible to be optimized in a short term?", "@saeed68gm we did retrain the model (also for parameters tuning)\r\n@liangxiao05 we got 3s on 320x320 images. We'll try using a smaller size like 200x200 + quantize the model and see what we get.", "As @mmpinso noted, SpatialMaxPoolWithArgMaxHelper in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/maxpooling_op.cc actually does the max pool and argmax calculations on the CPU.\r\n\r\nCurrently, it's only used for the gradient calculation. I'm working on also calling it from MaxPoolingWithArgmaxOp::Compute for the CPU.\r\n\r\nhttps://github.com/nio1814/tensorflow/tree/maxpoolwithargmax-cpu", "@nio1814 My patch does what you want to do I believe. Give it a read and let me know what you think", "Yeah, looks pretty similar. Sorry. I didn't realize you had already started working on this. \r\n\r\nAre you planning on doing a PR? I think people would find it useful.\r\nIf there's a branch your working from I could go ahead and add the gradient op. It should just be another call to SpatialMaxPoolWithArgMaxHelper.", "@mmpinso, @liangxiao05, did you succeed in coloring the result after processing the image with a neural network? If so, how do you do this? I'm trying to implement a function from the examples of the opencv - colorizeSegmentation library in the Java language, but so far it has not turned out. ", "@DenisN03 yes, we did it with TF itself. FYI, in our project we do binary segmentation, so we replaced the softmax by sigmoid. Then:\r\n\r\n`\r\n            predictions = tf.cast(tf.round(probabilities), tf.int32, name='output')\r\n\r\n\r\n            #=============PAINTED OUTPUT=============\r\n            #Compute an inverse of the output mask\r\n            predictions_inverse = tf.cast(tf.subtract(tf.ones([inputs_shape[1], inputs_shape[2], inputs_shape[3]], tf.int32), predictions), tf.float32)\r\n\r\n            #Remove the original object to detect from the images by applying the mask\r\n            holes = tf.multiply(images, predictions_inverse) * 255\r\n\r\n            #Transform the output mask into a color mask with 3 channels, according to the rgb input value.\r\n            color_mask = tf.concat([predictions * color[0], predictions * color[1], predictions * color[2]], axis=3)\r\n\r\n            #Fill the holes with the color mask\r\n            painted_output = tf.add(color_mask, holes, name='painted_output')`\r\n\r\nFYI, the entire system came out to be too slow to run on CPU in real time. We are exploring other nets now...", "@mmpinso, that is, you segment only two classes? How long does it take to process one image? what resolution do you use? you managed to find faster architectures?", "@DenisN03 , one class + background.\r\nIt took around 1.6 seconds with quantization on a Huawei P10 Lite with 200x200 images. With smaller images, quality was very bad.\r\nWe are currently trying [MobileUNet](https://github.com/akirasosa/mobile-semantic-segmentation). First results show 0.86 seconds without quantization and 224x224 images.\r\nWe would also like to try Mask RCNN at some point.", "@nio1814 I have a couple of items on my hand, but I am down to maybe create a pull request for this some time soon. I would appreciate your feedback on the code as well.\r\n\r\n@mmpinso @DenisN03 Mobilenet v2 seems very promising for this task as well. The results published are really good and it's even faster than mobilenet v1. But I have yet to get my hands on the pretrained waits (it was published Jan 2018). If you guys investigate I would like to know as well.", "Someone should close this issue, since it has been merged on 6. Apr in #18145 (see above)", "Closed based on @NPetsky 's comment. Thanks!"]}, {"number": 6034, "title": "ctc_beam_search_decoder()'s log_probabilities holds invalid values", "body": "### Environment info\r\nOperating System: OS X 10.11.6\r\nTF Version: 0.10.0rc0 (No GPU)\r\n\r\n### Example\r\nRan `log_probabilities` op created from\r\n```python\r\ndecoded, log_probabilities = ctc_ops.ctc_beam_search_decoder(logits, \\\r\n                                                             seq_length, \\\r\n                                                             beam_width=beam_width, \\\r\n                                                             top_paths=top_paths, \\\r\n                                                             merge_repeated=False)\r\n```\r\nThe `decoded` result is as expected. However, the `log_probabilities` contains positive values which can not be log probabilities. For example, with batch size 4 and top_paths=10 the `log_probabilities` printout is as follows\r\n```\r\n[[ 3.85424066 -1.97321272 -1.99056399 -2.18253303 -2.18592954 -2.40727925\r\n  -2.87798476 -2.88267159 -2.94563317 -2.94854331]\r\n [ 3.85424066 -1.97321272 -1.99056399 -2.18253303 -2.18592954 -2.40727925\r\n  -2.87798476 -2.88267159 -2.94563317 -2.94854331]\r\n [ 3.85424066 -1.97321272 -1.99056399 -2.18253303 -2.18592954 -2.40727925\r\n  -2.87798476 -2.88267159 -2.94563317 -2.94854331]\r\n [ 3.85424066 -1.97321272 -1.99056399 -2.18253303 -2.18592954 -2.40727925\r\n  -2.87798476 -2.88267159 -2.94563317 -2.94854331]]\r\n```\r\n\r\n### Other attempted solutions\r\nNone\r\n\r\n\r\n### Logs or other output that would be helpful\r\nLink to the entire code in context[[1](https://github.com/mozilla/DeepSpeech/blob/issue8++/DeepSpeech.ipynb)]\r\n", "comments": ["I may not get to this before January.", "Is the returned `log_probabilities` supposed to represented the logs of a probabilities, as documented[[1](https://www.tensorflow.org/versions/r0.12/api_docs/python/nn.html#ctc_beam_search_decoder)], or does it represent _un-normalized_  logs of a probabilities?", "Should be normalized, I'll have to trace this exact example to see what's going on", "I have the same problem, with beam search I get positive scores but with greedy I get negative ones ", "@ebrevdo Has there been any progress on this?", "Please provide a small example input that causes the issue so I can debug", "I am not really familiar with the code, but is it possible that it is caused by line 183 in tensorflow/tensorflow/core/util/ctc/ctc_beam_search.h: (*scores)(b, i) = -beam_log_probabilities[i]; ?", "@ebrevdo Until I get time to write a smaller, example one can look at [ctc_decoder_ops_test.py](https://github.com/tensorflow/tensorflow/blob/584d4921014db921a8f4722749adff09737826a8/tensorflow/python/kernel_tests/ctc_decoder_ops_test.py)\r\n\r\nDocumentation of [`ctc_beam_search_decoder`](https://www.tensorflow.org/api_docs/python/nn/connectionist_temporal_classification__ctc_#ctc_beam_search_decoder) states\r\n```\r\n...\r\nReturns:\r\n\r\nA tuple (decoded, log_probabilities) where\r\n\r\ndecoded: A list of length top_paths, where decoded[j] is a SparseTensor ...\r\nlog_probability: A float matrix (batch_size x top_paths) containing sequence log-probabilities.\r\n```\r\n\r\nHowever, for [ctc_decoder_ops_test.py](https://github.com/tensorflow/tensorflow/blob/584d4921014db921a8f4722749adff09737826a8/tensorflow/python/kernel_tests/ctc_decoder_ops_test.py) the expected results, the `log_prob_truth` values,  are\r\n```\r\n    log_prob_truth = np.array(\r\n        [\r\n            0.584855,  # output beam 0\r\n            0.389139  # output beam 1\r\n        ],\r\n        np.float32)[np.newaxis, :]\r\n```\r\nThese are not log probabilities as they are positive.\r\n", "@ebrevdo Is there any progress on this issue ? I have this problem with TF 1.0", "@ebrevdo same issue here! \r\n", "same issue", "This issue actually prevented a project here at Dropbox from migrating our training pipeline from Torch to TensorFlow, unfortunately causing us to stay with Torch.", "@kdavis-mozilla in your initial report, what were the seq_len values passed in?", "@ebrevdo Sorry but I don't even know anymore; it was over a year ago.\r\n\r\nWe've since created our own custom `ctc_beam_search_decoder` operator extending the TF operator with KenLM. We use that now.\r\n\r\nAs far a I remember, the problem is still there. The relative ordering of results via their `log_probabilities` is fine, but the absolute `log_probability` values are not.", "The problem is still there. It's related to the subtraction of the `maxCoeff` from the input and then the subsequent negation of the log probs. I can submit a PR with the fix, but don't have time to workthrough the broken test cases.", "The subtraction is there to avoid overflow/underflow issues iirc.  Is your\nsuggestion to add the values back in at the very end?\n\nOn Fri, Jan 5, 2018 at 9:38 AM, Ryan Leary <notifications@github.com> wrote:\n\n> The problem is still there. It's related to the subtraction of the\n> maxCoeff from the input and then the subsequent negation of the log\n> probs. I can submit a PR with the fix, but don't have time to workthrough\n> the broken test cases.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6034#issuecomment-355616640>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimyYvcnLmtAmySIP3TuRdLm1wa5nPks5tHl4ogaJpZM4LCPHW>\n> .\n>\n", "@ryanleary\r\nCan we fix this issue in Python side with touching C++?\r\n\r\ntop_paths=3, beam_width=400, I get these log-probs:\r\n```\r\narray([[ 0.09569344, -2.35573864, -3.55027127],\r\n       [ 0.31278262, -0.11559263, -0.22166124],\r\n       [ 0.01575552, -0.69348544, -0.84170169],\r\n       [ 0.08385472, -0.10281177, -1.23370361],\r\n       [ 1.34617054,  1.31623089, -0.14258756],\r\n       [ 0.1471574 , -0.13724545, -0.34559944],\r\n       [ 0.3714101 ,  0.02229187, -0.05457714],\r\n       [ 0.72338897, -0.42095903, -0.4398351 ],\r\n       [ 0.66841668, -0.19791916, -0.73667264],\r\n       [ 0.11281018, -0.14468569, -2.61785507],\r\n       [ 0.02421612, -0.29929882, -0.64302784],\r\n       [ 0.01053159, -0.2815102 , -1.38790202],\r\n       [ 0.03269538, -0.30419388, -0.4501299 ],\r\n       [ 0.00933365, -0.72536844, -1.57513046],\r\n       [ 0.40954223,  0.0353958 , -1.72318888],\r\n       ............\r\n      ], dtype=float32)\r\n```", "@ebrevdo yes, I think something like that needs to occur (to the extent that it can be done). What underflow/overflow issues have you experienced? Is that still really an issue even though we're in the log prob space?", "Is there any progress on this issue?", "Up.", "Sorry; unfortunately i do not currently have cycles to look at this.  It's\nan important issue though, so I won't close it for now.\n\nOn Tue, Jul 10, 2018 at 2:13 PM, Igor Macedo Quintanilha <\nnotifications@github.com> wrote:\n\n> Up.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6034#issuecomment-403967625>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim7citQ_I7xw8IKTqZgkdkUjc6d_Dks5uFRj5gaJpZM4LCPHW>\n> .\n>\n", "What do you mean?\r\n\r\nThis issue has more than one year now, and as you said it is an important issue. Isn't there anyone here that can have a look into this?", "It's being triaged\n\nOn Wed, Jul 11, 2018, 5:20 AM Igor Macedo Quintanilha <\nnotifications@github.com> wrote:\n\n> What do you mean?\n>\n> This issue has more than one year now, and as you said is an important\n> issue. Isn't there anyone here that can have a look into this?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6034#issuecomment-404148969>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim_FIjcaaxPkbFdspsz3Ug_ezjH3lks5uFe2PgaJpZM4LCPHW>\n> .\n>\n", "Nagging Assignee @ebrevdo: It has been 34 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I think this is fixed now?", "@ebrevdo What commit fixed this problem?", "https://github.com/tensorflow/tensorflow/pull/21187", "@ebrevdo Thanks for the info"]}, {"number": 6033, "title": "Tensorflow Random Function Always return 0", "body": "So import tensorflow showing following\r\n```\r\nPython 3.5.2 |Anaconda 4.2.0 (64-bit)| (default, Jul  5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)]\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nI c:\\tensorflow\\tensorflow\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA li\r\nl locally\r\nI c:\\tensorflow\\tensorflow\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA li\r\nlocally\r\nI c:\\tensorflow\\tensorflow\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA li\r\n locally\r\nI c:\\tensorflow\\tensorflow\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA li\r\nally\r\nI c:\\tensorflow\\tensorflow\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA li\r\nl locally\r\n>>> sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\nI c:\\tensorflow\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:885] Found device 0 with\r\nname: GeForce GTX 760\r\nmajor: 3 minor: 0 memoryClockRate (GHz) 1.15\r\npciBusID 0000:01:00.0\r\nTotal memory: 4.00GiB\r\nFree memory: 3.35GiB\r\nI c:\\tensorflow\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:906] DMA: 0\r\nI c:\\tensorflow\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:916] 0:   Y\r\nI c:\\tensorflow\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:975] Creating TensorFlow\r\n(device: 0, name: GeForce GTX 760, pci bus id: 0000:01:00.0)\r\nE c:\\tensorflow\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:586] Could not identify\r\ncalhost/replica:0/task:0/gpu:0, defaulting to 0.  Your kernel may not have been built with NUMA supp\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX 760, pci bus id: 0000:01:00.0\r\nI c:\\tensorflow\\tensorflow\\tensorflow\\core\\common_runtime\\direct_session.cc:255] Device mapping:\r\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX 760, pci bus id: 0000:01:00.0\r\n```\r\nAnd generate random matrix\r\n\r\n```\r\n>>> a = tf.random_normal((100,100))\r\n>>> sess.run(a)\r\nrandom_normal/RandomStandardNormal: (RandomStandardNormal): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tensorflow\\tensorflow\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] random_normal/RandomStandardNormal: (Ran\r\ndomStandardNormal)/job:localhost/replica:0/task:0/gpu:0\r\nrandom_normal/mul: (Mul): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tensorflow\\tensorflow\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] random_normal/mul: (Mul)/job:localhost/r\r\neplica:0/task:0/gpu:0\r\nrandom_normal: (Add): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tensorflow\\tensorflow\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] random_normal: (Add)/job:localhost/repli\r\nca:0/task:0/gpu:0\r\nrandom_normal/stddev: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tensorflow\\tensorflow\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] random_normal/stddev: (Const)/job:localh\r\nost/replica:0/task:0/gpu:0\r\nrandom_normal/mean: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tensorflow\\tensorflow\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] random_normal/mean: (Const)/job:localhos\r\nt/replica:0/task:0/gpu:0\r\nrandom_normal/shape: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tensorflow\\tensorflow\\tensorflow\\core\\common_runtime\\simple_placer.cc:827] random_normal/shape: (Const)/job:localho\r\nst/replica:0/task:0/gpu:0\r\narray([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\r\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\r\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\r\n       ...,\r\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\r\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\r\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)\r\n```\r\n\r\nThe basic matrix operation still works fine\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttps://github.com/tensorflow/tensorflow/issues/1734 seems has similar problems\r\n### Environment info\r\nOperating System:\r\nWindows 10\r\nGeForce GTX 760 GPU\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\nCUDA 8.0\r\ncuDNN 5.1\r\n\r\nI follow the CMake instruction build from source (tf 0.12)\r\n\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["I can't reproduce your problem.\r\n1. Does it also occur if you place the operation on CPU, or is it GPU only?\r\n2. Can you include more information about your environment, as requested by the new issue template?  e.g. OS, CUDA version, etc.", "I already provide the environment information in my post~ If you need more just let me know. And I will try your suggestion to try CPU this evening", "I change CUDA_VISIBLE_DEVICES to \"\", and rerun the random function and get the right results\r\n\r\n```\r\n>>> import tensorflow as tf\r\nI c:\\tensorflow\\tensorflow\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cublas64_80.dl\r\nl locally\r\nI c:\\tensorflow\\tensorflow\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cudnn64_5.dll\r\nlocally\r\nI c:\\tensorflow\\tensorflow\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cufft64_80.dll\r\n locally\r\nI c:\\tensorflow\\tensorflow\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library nvcuda.dll loc\r\nally\r\nI c:\\tensorflow\\tensorflow\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library curand64_80.dl\r\nl locally\r\n>>> sess = tf.Session()\r\nE c:\\tensorflow\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVI\r\nCE\r\nI c:\\tensorflow\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_diagnostics.cc:158] retrieving CUDA diagnostic informati\r\non for host: zsx988-Golden\r\nI c:\\tensorflow\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_diagnostics.cc:165] hostname: zsx988-Golden\r\n>>> a = tf.random_normal((100,100))\r\n>>> sess.run(a)\r\narray([[ -1.60603717e-01,  -9.76819277e-01,  -5.30430973e-01, ...,\r\n          3.82902361e-02,   4.57652728e-04,   7.37040758e-01],\r\n       [  3.86225969e-01,   2.95245004e+00,  -5.66983461e-01, ...,\r\n         -1.39389920e+00,   1.87824667e+00,   3.45089823e-01],\r\n       [  1.35846555e+00,   1.45867085e+00,   5.65616488e-02, ...,\r\n         -1.53614789e-01,   7.22914577e-01,  -2.70827681e-01],\r\n       ...,\r\n       [  2.13479862e-01,  -1.67380559e+00,   4.74632114e-01, ...,\r\n          3.85872096e-01,   1.64577377e+00,   2.13960743e+00],\r\n       [ -7.64496267e-01,  -6.11636817e-01,  -3.17500472e-01, ...,\r\n         -2.53556418e+00,  -1.09151840e+00,   4.00447100e-01],\r\n       [ -4.60968226e-01,   8.84875000e-01,  -8.24124396e-01, ...,\r\n         -6.42477036e-01,   5.90658069e-01,  -1.41880393e+00]], dtype=float32)\r\n```", "@zheng-xq looks like a CUDA random problem", "Last time I have seen this was when the -gencode was not correct for the requested cuda compute\r\nability. A gtx 760 has cuda compute ability 3.0 but the default build enables only 3.5 and 5.1. \r\nTensorflow should refuse to use this card unless CMakefile was changed and 3.0 was added to TF_EXTRA_CUDA_CAPABILITIES without adding 3.0 to -gencode.\r\nOR there is some bug in that area.\r\nI can take a look.", "tested this on master (@mrry has enabled cuda 3.0 capability) and it works fine on a gtx 740 (which is 3.0). tf.random_normal((100,100))  works too. \r\nI think the only thing we need to try is that a cuda 3.0 card is not taken by a tensorflow build before cuda 3.0 was enabled. ", "I followed the instruction given by https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake to build tensorflow which I supposed to support 3.0 as I modify on [this line](https://github.com/tensorflow/tensorflow/blob/a7b4f636d50b6fcb9cd1f7f138cc50b720fc86dd/tensorflow/contrib/cmake/CMakeLists.txt#L156) and[ this line](https://github.com/tensorflow/tensorflow/blob/a7b4f636d50b6fcb9cd1f7f138cc50b720fc86dd/tensorflow/contrib/cmake/CMakeLists.txt#L166). Did I miss something?", "ah, need to fix that comment: the -gencode need to change too so nvcc generates the 3.0 code.  That explains why tensorflow took the card but returned zeros. The correct change is here:\r\nhttps://github.com/tensorflow/tensorflow/commit/669b2cc9fa02527c87e9f82bd4ac7d76e715d2a8#diff-1d799fa350437420218e5e5aa680c481\r\n", "Automatically closing due to lack of recent activity. We hope that you were able to resolve it on your own. However, since this is a support issue rather than a bug or feature request, you will probably get more information by posting it on StackOverflow."]}, {"number": 6032, "title": "freeze_graph not working in v0.11.0 . giving V2 error", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttps://github.com/tensorflow/tensorflow/issues/5938\r\nhttps://stackoverflow.com/questions/40616430/load-exported-model-in-tensorflow\r\nhttps://github.com/tensorflow/tensorflow/issues/5639\r\nhttps://stackoverflow.com/questions/40616430/load-exported-model-in-tensorflow\r\n\r\n### Environment info\r\nOperating System: CentOS (AWS p2.xlarge)\r\n\r\nInstalled version of CUDA and cuDNN:  cuda 8.0 , cudnn 5.1.5\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n```\r\n/usr/local/cuda-8.0/lib64/libcudadevrt.a\r\n/usr/local/cuda-8.0/lib64/libcudart.so\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n/usr/local/cuda-8.0/lib64/libcudnn.so\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5\r\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n```\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`. \r\n```\r\n0.11.0\r\n```\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n```\r\nhttps://github.com/tensorflow/tensorflow/archive/v0.11.0.zip\r\n```\r\n2. The output of `bazel version`\r\n```\r\nBuild label: 0.3.2\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Oct 7 17:25:10 2016 (1475861110)\r\nBuild timestamp: 1475861110\r\nBuild timestamp as int: 1475861110\r\n```\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nI trained my convolution network on v0.11.0 and then running freeze_graph on the same tensorflow v0.11.0 . freeze_graph does not work.\r\n```\r\nsaving model:\r\nsaver = tf.train.Saver(tf.all_variables(), max_to_keep=100)\r\nsaver.save(sess, checkpoint_prefix, global_step=current_step)\r\n\r\nwriting graph:\r\ntf.train.write_graph(sess.graph.as_graph_def(), checkpoint_prefix, \"graph\"+str(nn)+\".pb\", as_text=False)\r\n\r\nrunning freeze_graph:\r\ntensorflow-0.11.0/bazel-bin/tensorflow/python/tools/freeze_graph --input_graph=graph4000 --input_checkpoint=runs/1479444509/checkpoints/model-4000 --output_graph=my.pb --output_node_names=output0/predictions0,accuracy0/accuracy0,output0/scores0,output0/Softmax --input_binary=True --clear_devices=True --initializer_nodes=embedding/W\r\n\r\n```\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n```\r\ntensorflow-0.11.0/bazel-bin/tensorflow/python/tools/freeze_graph --input_graph=graph4000 --input_checkpoint=runs/1479444509/checkpoints/model-4000 --output_graph=my.pb --output_node_names=output0/predictions0,accuracy0/accuracy0,output0/scores0,output0/Softmax --input_binary=True --clear_devices=True --initializer_nodes=embedding/W\r\nTraceback (most recent call last):\r\n  File \"/data3/tensorflow-0.11.0/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 134, in <module>\r\n    tf.app.run()\r\n  File \"/data3/tensorflow-0.11.0/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 30, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"/data3/tensorflow-0.11.0/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 131, in main\r\n    FLAGS.output_graph, FLAGS.clear_devices, FLAGS.initializer_nodes)\r\n  File \"/data3/tensorflow-0.11.0/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 103, in freeze_graph\r\n    _ = tf.import_graph_def(input_graph_def, name=\"\")\r\n  File \"/data3/tensorflow-0.11.0/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/importer.py\", line 258, in import_graph_def\r\n    op_def = op_dict[node.op]\r\nKeyError: u'RestoreV2'\r\n```", "comments": ["My best guess is some kind of version skew, similar to the suggestion for #5639.  @petewarden might be the best person for advice, but he's very over-burdened with requests at the moment.\r\n\r\nYour immediate problem is a node name not being in the op_dict.  If I were debugging for myself, I'd try to figure out whether it's just this one node missing, or maybe you somehow failed to get all of the definitions for the graph you're trying to restore.", "I have a code change pending to make the error here a bit more informative. I believe the issue is what @poxvoculi describes though, that you trained the model in 0.12 but are trying to load it in 0.11. I'm closing for now on that assumption, but please re-open and let me know if I'm incorrect!", "I am pretty sure that I trained the model in v0.11.0 and freezing it using v0.11.0 only. Anyway I think whatever the issue was, is not appearing in v0.10.0 and v0.12.0-rc0. So I am fine with other versions."]}, {"number": 6031, "title": "Use gather on floats instead of int32 to keep the kernel on GPU when possible.", "body": "gather() on an int32 type forces the op to go to CPU.\r\nFixes #6027\r\n@liuyipei can you confirm that this addresses what you observe?", "comments": ["Not sure why this one test fails. Works fine locally.\r\n@tensorflow-jenkins test this please."]}, {"number": 6030, "title": "Docker build: add support for python3", "body": "new tags for py3 docker images are such as:\r\nnightly-py3\r\nnightly-gpu-py3\r\nnightly-devel-py3\r\nnightly-gpu-devel-py3", "comments": ["Ready for review.", "@tensorflow-jenkins test this please"]}, {"number": 6029, "title": "[Windows/CMake] Add tensorflow.models.* to the PIP package.", "body": "Also runs the few tests in there, and builds the word2vec kernels.\r\n\r\nFixes #6024. Fixes #5953.", "comments": ["It looks like a few users are having difficulty running our tutorials on Windows without these files - and I don't think they'd work standalone - so I propose we add them to r0.12. We'll remove them again when they move to the models repo (cc @nealwu), so we probably shouldn't backport this to master. Does that sound alright?", "Sounds good.", "sure, but not with a failing test :)\r\n```\r\n180/182 Test #180: C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/models/embedding/word2vec_test.py ...............................***Failed\r\n```\r\n\r\nLooks like the root cause is:\r\n`tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'Skipgram'`", "I've seen that op before. I believe it's related to word2vec.", "That'll teach me not to run all the tests before pushing :)."]}, {"number": 6028, "title": "slim.learning.train and slim.evaluation.evaluation to handle OutOfRange gracefully", "body": "While doing this fix, I noticed that some of the metric operations don't handle OutOfRange gracefully also.  \r\n\r\nFor example, streaming mean computation would update total value and count concurrently, possibly updating count before failing to update total value due to OutOfRange. \r\n\r\nI have added fixes for mean metrics, but I am not sure what other places might be handling OutOfRange incorrectly.", "comments": ["Can one of the admins verify this patch?", "Please update and resolve the conflicts. \r\n\r\nI think the functionality is welcome.", "@sguada thanks for feedback.  PR is updated.", "@alsgrv sorry this fell through the cracks. Could you resolve changes and push again?", "@drpngx merged.", "Looks like there's another conflict, could you push again?", "@drpngx, sure - resolved.", "Jenkins, test this please.", "Looks like evaluation test failed, let me see what's going on.", "Jenkins, test this please.", "@drpngx, I had to put back this guard for both branches:\r\n\r\n```\r\nwith ops.control_dependencies([values]):\r\n  update_count_op = state_ops.assign_add(count, num_values)\r\n```\r\n\r\nThis is because this operation doesn't seem to force evaluation of values:\r\n\r\n```\r\nmath_ops.to_float(array_ops.size(values))\r\n```\r\n\r\nThus, count get incremented while sum is unchanged due to OutOfRangeError.", "Oh, got it. That's a bit of a gotcha.\r\n\r\nJenkins, test this please.", "Looks like it failed due to estimator_test timeout.  I checked another run, and it's pretty slow in general:\r\n```\r\n//tensorflow/contrib/learn:estimator_test                                PASSED in 243.6s\r\n```\r\n\r\nOn my machine is takes ~70 seconds.  Could this be transient failure?", "Yeah, it seems to happen in other builds."]}, {"number": 6027, "title": "tracing batch normalization: apparently unnecessary gpu-cpu-gpu transfer for gather op.", "body": "I am running:\r\nCUDA 8.0. CUDNN 5.1. tensorflow 0.11. Ubuntu 12.04.\r\n\r\nI was experiencing low GPU utilization. As per #1824 I did a trace and found many instances of the following interaction, as shown by the screenshot in chrome:\r\n\r\nThe op I have highlighted (with arrows going in and out in the screenshot) is a `gather` op. I provide a link to the context in which it is called.\r\nhttps://github.com/tensorflow/tensorflow/blob/eb56a8af24695bf8258addf28b0c53fbabff72e6/tensorflow/python/ops/nn_impl.py#L472\r\n\r\nThe call stack (at graph construction) time is:\r\n`tf.contrib.layers.batch_norm`, `python.ops.nn_impl.moments`, `python.ops.nn_impl.sufficient_statics`\r\n\r\nIf I understand correctly, I am seeing that the GPU gets blocked as a result of shuffling a trivial `gather` op unnecessarily off to the CPU only to wait forever for it to come back. \r\n\r\n![screenshot from 2016-12-01 11 45 17](https://cloud.githubusercontent.com/assets/4404828/20810886/67cbd92e-b7c0-11e6-991d-75ab54a1c699.png)\r\n\r\n", "comments": ["The gather Op is being placed on CPU because all of its arguments are int32.  (This is fallout from an early mis-design that hasn't yet been completely remediated.)  In this case, the gather is rather trivial and probably shouldn't be used.  @vincentvanhoucke ", "Sorry to press the issue. @poxvoculi  In this case, what do you think is the best way to address it?", "I'll take a look. If anyone has a solution handy, send me a PR.", "Oops. Closed by mistake.  The python in sufficient_statistics should be rewritten to avoid using array_ops.gather in this case.", "@liuyipei if you confirm that #6040 fixes the problem, then my fix at #6031 is slightly cleaner and should be equivalent.", "@vincentvanhoucke Thank you very much! I have closed my PR."]}, {"number": 6026, "title": "Tensorboard erorr when adding metadata to embedding ", "body": "### Environment info\r\nOperating System: Linux gaia 3.13.0-95-generic\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.0rc0-cp35-cp35m-linux_x86_64.whl\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n0.12.0-rc0\r\n\r\nFollowed the instructions on 'https://www.tensorflow.org/versions/r0.12/how_tos/embedding_viz/index.html'\r\nto add labels to my embedding. My metadata.tsv file is just a single column with the ith row being the label corresponing to the ith row of embedding tensor\r\n\r\nHowever after I did this and ran Tensorboard everything is fine until I click 'embeddings'  then I get this error message  \r\n```\r\n'Exception happened during processing of request from ('127.0.0.1', 56645)\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/socketserver.py\", line 625, in process_request_thread\r\n    self.finish_request(request, client_address)\r\n  File \"/usr/local/lib/python3.5/socketserver.py\", line 354, in finish_request\r\n    self.RequestHandlerClass(request, client_address, self)\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/tensorboard/backend/handler.py\", line 103, in __init__\r\n    BaseHTTPServer.BaseHTTPRequestHandler.__init__(self, *args)\r\n  File \"/usr/local/lib/python3.5/socketserver.py\", line 681, in __init__\r\n    self.handle()\r\n  File \"/usr/local/lib/python3.5/http/server.py\", line 422, in handle\r\n    self.handle_one_request()\r\n  File \"/usr/local/lib/python3.5/http/server.py\", line 410, in handle_one_request\r\n    method()\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/tensorboard/backend/handler.py\", line 531, in do_GET\r\n    self.data_handlers[clean_path](query_params)\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/tensorboard/plugins/projector/plugin.py\", line 278, in _serve_runs\r\n    request.respond(list(self.configs.keys()), 'application/json')\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/tensorboard/plugins/projector/plugin.py\", line 141, in configs\r\n    _latest_checkpoints_changed(self._configs, run_path_pairs)):\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/tensorboard/plugins/projector/plugin.py\", line 65, in _latest_checkpoints_changed\r\n    if run_name not in configs:\r\nTypeError: argument of type 'NoneType' is not iterable\r\n```\r\n\r\nIf i removed the ``` projector_config.pbtxt ```  from the LOGDIR tensoboard runs properly again. \r\n\r\nAlso found on the documentation page \r\n\r\n``` embedding = config.embeddings.add() ```  should be ```embedding = config.embedding.add() ```\r\n", "comments": ["Sorry to make this more complicated then it is, however yesterday when I posted this issue I did not notice that I was trying to do this with logs and models that are saved from `0.11.0rc2`. However everything was working fine on tensorboard, I could see the embedding very nicely and I only got the above error when I tried to add embedding metadata ` projector_config.pbtxt `. Today on a new run with logs and model saved from ` 0.12-rc0 ` , I could not view the embedding tab anymore and I get the same error. (without trying to add any metadata files)", "You probably haven't created the actual `metadata.tsv` file. It isn't maybe immediately clear reading the how-to. Here is an example: http://stackoverflow.com/a/41262360/4218883.", "I have created the metadata file. My metadata.tsv file is just a single column with the ith row being the label corresponding to the ith row of embedding tensor. From the instructions 'row is the label of the embedding' so I assume my metadata file is correct.", "Indeed, sorry I guess I read your initial comment too quickly.\r\nFor info, my system works with a 2 column metadata.tsv file:\r\n* 1st column: data IDs\r\n* 2nd column: data labels (words in my case)", "I encountered this issue as well. The problem was that the metadata_path in projector_config.pbtxt did not point to metadata.tsv.", "I was able to edit the projector_config.pbtxt file to fix the paths and it seems to do the trick. Seems to be an windows issues"]}]