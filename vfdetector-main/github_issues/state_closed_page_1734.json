[{"number": 878, "title": "Tensorflow installer", "body": "Hi TensorFlow Team,\n\nI would like to propose a simple bash script to automate the installation of required tools(based on os_setup.md instructions), I think it will help many newbies. It is available under :  https://github.com/bellaj/tensorflow/blob/master/setup_Tensor.sh\nThis script will help users to install Tensorflow for many Linux distributions .\n\nI have tested it for ubuntu 64, and it works fine.\nI am working now on GPU support\n\nany feedback is welcome\nthank you\n", "comments": ["I like the idea of having a more or less platform independent script. Here are a few concerns I have with this version:\n- I'm skeptical that the package names are all the same for yum, apt-get, zypper, yast. \n- I'm not even sure they're consistent across different distributions that use the same package manager.\n- I don't know whether all of these take the same arguments (e.g. `install` vs. `-i`)\n- variable evaluations should almost always be quoted. Otherwise expressions like `[[ $env -eq 0 ]]` fail with a syntax error if `$env` is empty.\n- Why are you using the 0.5.0 wheel for CPU only python 2.7?\n- If you get into a \"bad choice\" branch, the script still continues\n- If you install without virtualenv, the script will still output instructions for virtualenv\n", "Thank you @martinwicke. Now It is a simple script as POC, but i will make some deep changes and test it in many distributions. I will take in consideration all your remarques.\n", "Hi, i have updated the script please take a look :)\n", "thank you for the information.\n", "@martinwicke: Looks like this fell through the cracks.  \n", "Ok, I think after we know a little more about the environments out there, I doubt we want to maintain such a script. It's a great idea, felled by the messiness of contemporary information technology.\n\nI'll close this issue. \n"]}, {"number": 877, "title": "word2vec.py error", "body": "I download the tensorflow-master version.\n\nWhen I execute the word2vec.py on pycharm, it shows the error:\n\nfrom tensorflow.models.embedding import gen_word2vec as word2vec\nImportError: No module named embedding\n\nIn **init**.py, I see \"Import generated word2vec optimized ops into embedding package.\"\n\nDoes it means it needs a file about \"generated word2vec optimized ops\"? But I can' t find it. How can I fix it?\n\nThe environment is Mac OS X, and I install tensorflow in this way: $ pip install https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl.\n", "comments": ["You should try upgrading to 0.6.0 -- I think that file was broken in our initial release except when via bazel.\n", "@vrv Thank you!\n\nYour tip works. \n\nI upgrade to 0.6.0 by $ pip install https://storage.googleapis.com/tensorflow/mac/tensorflow-0.6.0-py2-none-any.whl.\n\nI am a fresher in tensorflow. May I ask you one more question?\n\nI execute the word2vec_optimized.py on pycharm. The output below seems right. \n\nWhen I inspect the code, I find that it should execute -- print(\"Epoch %4d Step %8d: lr = %5.3f words/sec = %8.0f\\r\" % (epoch, step, lr, rate), end=\"\"). \n\nBut in console its output does not appear, while the later output of [print(\"Eval %4d/%d accuracy = %4.1f%%\" % (correct, total, correct \\* 100.0 / total))] shows.\n\nBesides, I can' t find the file vocab.txt or model.ckpt created by the program. I think the files haven' t been created yet.\n\nHow can I fix it?\n\nConsole Information:\nI tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 4\nI tensorflow/core/common_runtime/direct_session.cc:58] Direct session inter op parallelism threads: 4\nI tensorflow/models/embedding/word2vec_kernels.cc:149] Data file: text8 contains 100000000 bytes, 17005207 words, 253854 unique words, 71290 unique frequent words.\nData file:  text8\nVocab size:  71290  + UNK\nWords per epoch:  17005207\nEval analogy file:  questions-words.txt\nQuestions:  17827\nSkipped:  1717\n\nEval 1446/17827 accuracy =  8.1%\n\nEval 2217/17827 accuracy = 12.4%\n\nEval 3084/17827 accuracy = 17.3%\n\nEval 3669/17827 accuracy = 20.6%\n\nEval 4140/17827 accuracy = 23.2%\n\nEval 4423/17827 accuracy = 24.8%\n\nEval 4699/17827 accuracy = 26.4%\n\nEval 5061/17827 accuracy = 28.4%\n\nEval 5271/17827 accuracy = 29.6%\n\nEval 5542/17827 accuracy = 31.1%\n\nEval 5694/17827 accuracy = 31.9%\n\nEval 5946/17827 accuracy = 33.4%\n\nEval 6053/17827 accuracy = 34.0%\n\nEval 6173/17827 accuracy = 34.6%\n\nEval 6218/17827 accuracy = 34.9%\n\nProcess finished with exit code 0\n", "You'll probably get a better answer for these types of questions on StackOverflow. \n", "word2vec.py goes through without any error on IPython . Then How to run or test the model Ex: giving France, Paris...Russia..to . It does not prompt for inputs..\n\nkatti \n"]}, {"number": 876, "title": "How to set weight cost strength in TensorFlow", "body": "When i use Momentum Gradient Descent, how to set weight cost strength?( The \u03bb in this formula.)\n![img](http://i.stack.imgur.com/305RE.jpg)\n", "comments": ["This seems like a question better suited for StackOverflow, since this isn't a bug / feature request.\n"]}, {"number": 875, "title": "GPU device not found on OSX", "body": "Hello,\n\nI'm trying to install TensorFlow with GPU support for a CUDA Capability 3.0 device. I have followed the instructions found here: https://gist.github.com/Mistobaan/dd32287eeb6859c6668d, and everything compiles without error, but TensorFlow doesn't recognize my GPU device. The steps that I'm taking are below, as is the error output.\n\nI've modified bazel to use gcc-4.8, so these are the commands I'm running:\n\n```\nbazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer ---crosstool_top=//tools/cpp:toolchain\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --crosstool_top=//tools/cpp:toolchain\n```\n\nRunning the example with:\n\n```\nbazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu\n```\n\nproduces the following output:\n\n```\nF tensorflow/cc/tutorials/example_trainer.cc:118] Check failed: ::tensorflow::Status::OK() == (session->Run({{\"x\", x}}, {\"y:0\", \"y_normalized:0\"}, {}, &outputs)) (OK vs. Invalid argument: Cannot assign a device to node 'Const/_2': Could not satisfy explicit device specification '/gpu:0'\n     [[Node: Const/_2 = Const[dtype=DT_INT32, value=Tensor<type: int32 shape: [] values: 0>, _device=\"/gpu:0\"]()]])\nF tensorflow/cc/tutorials/example_trainer.cc:118] Check failed: ::tensorflow::Status::OK() == (session->Run({{\"x\", x}}, {\"y:0\", \"y_normalized:0\"}, {}, &outputs)) (OK vs. Invalid argument: Cannot assign a device to node 'Const/_2': Could not satisfy explicit device specification '/gpu:0'\n     [[Node: Const/_2 = Const[dtype=DT_INT32, value=Tensor<type: in\n```\n\nI can also confirm that after installing the pip package and running a test script, no devices are found.\n\nA few things to note:\n- Running deviceQuery confirms that a CUDA device (Device 0: GeForce GT 650M) is found. \n- I've made sure to run bazel clean before compiling, and have tried compiling using sudo just to see if this was a permissions issue.\n\nThanks in advance for your help. \n", "comments": ["EDIT: I can successfully compile also with Clang 7.0.2, which I can confirm is supported by NVCC. I'm still getting the same error as above.\n\n~~It turns out that I was actually compiling with gcc-5, not gcc-4.8. From what I can tell CUDA doesn't support version 5, so this might be the problem.~~\n\n~~I'm trying now to compile with 4.8, but I'm getting this error:~~\n\n```\n/var/tmp//cccfsYPu.s:2774:expecting string instruction after `rep'~~\n```\n\n~~From what I've read this seems to be a problem with binutils (at least on Linux), so I installed binutils 2.25.1 with Homebrew, but it didn't help. I've also tried including the compiler flag `-march=corei7'~~\n", "De-duping with #491\n"]}, {"number": 874, "title": "Recover NumPy License Header in `numpy.i`.", "body": "The original `numpy.i` file has own NumPy License Header.\nThe copyright header should be retained.\n", "comments": ["Can one of the admins verify this patch?\n", "Jenkins test this please.\n", "@tensorflow-jenkins: test this please\n", "This is merged. Thanks for noticing. \n", "Thank you for merging.\n"]}, {"number": 873, "title": "Mount notebooks into a mounted fs volume to avoid data loss", "body": "The current way this is done:\n\n```\ndocker run -p 8888:8888 -it --rm $USER/assignments\n```\n\ncreates an ephemeral container due to the `--rm` flag. In the first notebook you are doing very expensive operations (gunziping) several GBs. All of this data will be lost when the container shuts down. Using the filesystem as a mounted volume, allows subsequent sessions to have access to the files created (eg pickled data). This is especially important because this is course material so want to avoid time consuming gotchas for the students.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@craigcitro any gotcha going down that route?\nShould I also be advertising not using --rm for users of the pre-built Docker container?\n", "No, these both seem pretty reasonable. Two thoughts:\n1. The `--rm` thing is definitely a smart move -- I tend to spend more time _debugging_ docker images than _using_ them right now. For this case, though, it does make it a bit too easy to lose notebooks.\n2. Mounting the notebooks dir as a volume also makes sense. This is one where mounting it (\"I want to make changes that get saved\") and not mounting it (\"I'm playing around with something and it's a throwaway\") both make sense, so maybe some text explaining when you would or wouldn't want the option? As a minor thing, no need to `cd` -- just pass the subdir path instead of `$(pwd)`?\n", "@timothyjlaurent thanks for the fix. Would you mind changing the doc for the hosted container as well and removing the 'cd'?\n", "Ok @vincentvanhoucke  I updated the readme. \n", "I fixed the typo\n", "LGTM @vrv \n", "Thanks!  Squash the commits and we'll merge.\n", "Commits have been squashed.\n\nThere is one caveat to this PR -- if the user needs to rebuild the docker image -- all the files underneath will need to be added to the context for the build. This is several GB for the first assignment so it takes a LONG time. \n", "We aren't running any `docker build` commands from the top of the tree, though, are we? (Just the one `docker run`?)\n", "Merged. Thanks!\n", "Well in the readme it says:\n\n```\n Building a local Docker container\n ---------------------------------\n\n     cd tensorflow/examples/udacity\n     docker build -t $USER/assignments .\n```\n\nIf you use that same directory to mount the notebooks then it will fill up with data files. Then if you need to remake the image all of that will have to be put in the context. When I had to do this, I just moved the data directories temporarily to `..` and then back in.\n\nSo maybe we should recommend copying the udacity dir to some work dir location and then mount that into the container or mention the issue with context and explain that they should move extraneous files elsewhere while rebuilding the image.  \n", "I'm not sure why you, as a class user, would ever want to build the container?\nThese instructions are for us, when we want to update the hosted containers.\n", "The course materials instructed me to build the image:\n\n![image](https://cloud.githubusercontent.com/assets/2000204/12596029/15c84a4e-c433-11e5-8a5e-bf7662d12036.png)\n\nIf there is an image I could pull, that would definitely be better, but there is no mention of this image in the course materials as far as I can see. \n", "Arpan, there should be no need for anyone to build a container. Was the doc updated?\n", "Can one of the admins verify this patch?\n", "Can one of the admins verify this patch?\n", "If there is a hosted image that should be used for the class, the command should look something like this:\n\n`docker run --rm -v <path to notebooks>:/notebooks tensorflow/udacity`\n", "@timothyjlaurent Access to the hosted version is described on the first line in the document you edited:\ndocker run -p 8888:8888 -it --rm b.gcr.io/tensorflow-udacity/assignments\nMaybe I should separate the rest of this readme so that it stands out more.\n", "Oh I see it now that you point it out. Maybe the course should be updated to instruct users to use that image rather than build their own.\n", "Also now that I see the first line (although it wasn't shown in the course material)\nThis should probably also mount the working directory as a volume.\nIt takes maybe 20-30 minutes to decompress the data files. It also loads all this data into memory -- I had to upgrade my docker machine VM to have 8GB of RAM to avoid running out of memory. All of this should be disclosed to the student.\n", "@timothyjlaurent we pushed some changes this weekend that should help with memory.\n"]}, {"number": 872, "title": "Use portable code in stringpiece.cc", "body": "std::min is defined in <algorithm>[1].\nmemmem is a GNU extension, std::search is standard.\n\n[1] http://en.cppreference.com/w/cpp/algorithm/minmax\n", "comments": ["Can one of the admins verify this patch?\n", "Can you remove the commits which we've already merged?\n", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n"]}, {"number": 871, "title": "fix: notebooks were made python3 compatible.", "body": "Minor modifications were made to the code so that everything is python3 compatible.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "@googlebot I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Hi @lzlarryli, thanks for your help with this. Is there any chance you could make the 'diff' minimal, with only the code changes that you really needed to do, but not mutating the state of the cells? It would make it a lot easier to approve quickly.\nAlso, I don't believe any of my uses of xrange are performance-critical. If it helps readability to just turn them into 'range' for bot python 2.7 and 3.0, then I'd marginally prefer that.\n", "Hi~ @vincentvanhoucke Can you tell me with which version of python and ipython the original notebooks were created? I made the changes with the docker container. But even with that, Jupyter still automatically upgrades the notebook version from 3 to 4, which reformats the raw text and leads to a huge messy diff. This is known issue of Jupyter. It offers the ability to convert version 4 to version 3 but it is known that 3->4->3 is not identity. \n", "I am going to re-export all the notebooks as v4 tomorrow. It should make things a lot easier after your rebase. Thanks again!\n", "@lzlarryli re-export should show up on the github repo soon. Thanks for your patience.\n", "To have a cleaner commit history, I will resubmit this from a separate fork.\n", "I just tried the new one. It still generates a huge diff. I dug a bit more. It seems this is unavoidable because the notebook is saved as a json file with nonlocal changes. This is not diff friendly by construction. There was even a project called nbdiff trying to deal with this. However, nbdiff seems to be no longer maintained. It was based on older ipython API and is not runnable, for example, in the Udacity docker environment. How should I proceed? \n", "Darn, I was hoping it would be enough. I have received a few patches such as https://github.com/tensorflow/tensorflow/pull/884/files that have clean diffs.\nIs it the case that most of the change is along the lines of:\n   \"import sys\\n\",\n-    \"if (sys.version_info > (3, 0)):\\n\",\n-    \"    from urllib.request import urlretrieve\\n\",\n-    \"    import pickle\\n\",\n-    \"    xrange = range\\n\",\n-    \"else:\\n\",\n-    \"    from urllib import urlretrieve\\n\",\n-    \"    import cPickle as pickle\"\n  if so I can take a crack at it.\n", "@vincentvanhoucke Yes, more or less. In addition to that, all 'print' have to be changed to function invocations. There was a tricky part in `6_lstm.ipynb` due to the translation between strings (bytes....) and numbers.\n\nFirst, \n\n``` python\ntext = read_data(filename)\n```\n\nneeds to be changed to:\n\n``` python\ntext = read_data(filename)\n# Python 3 compatibility\nif type(text) is not type(str()):\n    text = text.decode()\n```\n\nSecond, `char2id` needs to be changed this way:\n\n``` python\ndef char2id(char):\n  # Caution: in python 2, len('\u00ef') = 2 but in python 3, len('\u00ef') = 1\n  if (len(char) == 1) and (ord(char) in map(ord, string.ascii_lowercase)):\n    return ord(char) - first_letter + 1\n  elif char == ' ':\n    return 0\n  else:\n    print('Unexpected character:', char)\n    return 0\n```\n\nThis is on the dirty side of things but it minimally changes the current code. In python2 the text file is read as a `string` while in python3 it is `bytes`.\n\nFinally, there are places where the division has to be made to `\\\\` because in python3, the division is floating point by default. For example, in `6_lstm.ipynb`, \n\n``` python\nclass BatchGenerator(object):\n  def __init__(self, text, batch_size, num_unrollings):\n    ...\n    segment = self._text_size / batch_size\n    ...\n```\n\nhas to be changed to something like:\n\n``` python\n    segment = self._text_size // batch_size\n```\n\notherwise `segment` cannot be used as an index for lists.\n", "@lzlarryli I have made updates to all notebooks. 4/5/6 should be pushed shortly.\nI didn't quite understand what you were doing with char2id, it seems to work fine as-is, and exclude non-ascii lowercase characters, but maybe I missed something?\nWould you mind giving it a whirl?\n", "@vincentvanhoucke Interesting... I checked again and I agree with you that as long as `text = text.decode()` is ran in py3 after `text` is read from the file, then the rest of the code runs smoothly (because `zipfile.Zipfile.read` returns `bytes` not a string). \n\nWhen I ran it last time, somehow this didn't work. So tried to keep `text` as `bytes` throughout, which led to the unreadable code~\n", "Hello, thanks for the very interesting course, I really enjoy it: fast-paced, no frills and very nutritive! :-)\n\nI'm getting a Python 3 related error at this line of the fourth notebook:\n\n```\nlayer3_weights = tf.Variable(tf.truncated_normal(\n  [image_size / 4 * image_size / 4 * depth, num_hidden], stddev=0.1))\n```\n\n(sorry I cannot find a way to _not_ render a notebook on GH, so I can't link to it):\n\n```\nTypeError: DataType float32 for attr 'T' not in list of allowed values: int32, int64\n```\n\nUsing integer division solves it:\n\n```\nlayer3_weights = tf.Variable(tf.truncated_normal(\n  [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n```\n", "Thanks for the report. I'll be happy to take a pull request, or I'll fix it myself tomorrow.\n"]}, {"number": 870, "title": "2d gradients have a second dimension of size None", "body": "When calling compute_gradients with 2d variables, the first dimension has the correct size while the second dimension has a size of None.  Is this intentional?  It makes it somewhat difficult when processing the gradients, as a dimension size of None isn't really compatible with anything else.\n\nTested with:\n2d variables - second dimension is size None, first dimension is size of first dimension of variable\n1d variables - dimension is size of dimension of variable\nTested using compute_gradients in AdamOptimizer\n", "comments": ["As I [answered](http://stackoverflow.com/a/34996139/3574081) on StackOverflow, this is intended behavior when a dimension cannot be statically inferred. In fact, a dimension size of `None` is compatible with _every_ concrete dimension size (it acts as a wildcard), and any constraints will be checked at runtime.\n"]}, {"number": 869, "title": "Error in description of outputs of rnn_decoder", "body": "rnn_decoder is described as follows:\n\n```\ndef rnn_decoder(decoder_inputs, initial_state, cell, loop_function=None,\n                scope=None):\n  \"\"\"RNN decoder for the sequence-to-sequence model.\n...\nReturns:\n...\n    states: The state of each cell in each time-step. This is a list with\n      length len(decoder_inputs) -- one item for each time-step.\n```\n\nStates output does not appear to have the length as the decoder inputs. For example, the following script --\n\n```\nimport tensorflow as tf\nfrom tensorflow.models.rnn import rnn_cell\nfrom tensorflow.models.rnn import seq2seq\nbatch_size = 4\nseq_len = 5\nnum_layers = 3\nvocab_size = 26\nrnn_size = 100\ncell = rnn_cell.MultiRNNCell([rnn_cell.BasicLSTMCell(rnn_size)] * num_layers)\ninput_data = tf.placeholder(tf.int32, [batch_size, seq_len])\nembedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])\ndecoder_inputs = tf.split(1, seq_len, tf.nn.embedding_lookup(embedding, input_data))\ndecoder_inputs = [tf.squeeze(input_, [1]) for input_ in decoder_inputs]\noutputs, states = seq2seq.rnn_decoder(decoder_inputs, cell.zero_state(batch_size, tf.float32), cell)\nprint \"Length of states is {}, but length of decoder inputs is {}\".format(len(states), len(decoder_inputs))\n```\n\ngives the following output:\n\n```\nLength of states is 6, but length of decoder inputs is 5\n```\n\nThis is still true if you replace BasicLSTMCell with GRUCell.\n\nIs it correct to say that the length of states is equal to _one greater than_ the length of decoder inputs? Seems to be true from playing around with it. Is this a fencepost error, where the states include the zeroth state _plus_ the state following each decoder input?\n", "comments": ["This api is about to change. From now on rnn() and its cousins will return just the very last state instead of a list of intermediate states.\n", "Makes sense, that's how I've seen it used every time anyway. Thanks. I'll\nclose the issue.\n\nOn Wed, Jan 27, 2016 at 10:00 PM, ebrevdo notifications@github.com wrote:\n\n> This api is about to change. From now on rnn() and its cousins will return\n> just the very last state instead of a list of intermediate states.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/869#issuecomment-175997392\n> .\n"]}, {"number": 868, "title": "Udacity not_mnist notebook broken double quotes", "body": "When trying to run tensorflow/examples/udacity/1_notmnist.ipynb in my ipython notebook (tried versions 4.0 and 3.2.1), I got a NotJSONError:\n\n[W 22:14:13.940 NotebookApp] Unreadable Notebook: /home/pzelasko/tensorflow/tensorflow/examples/udacity/1_notmnist.ipynb NotJSONError('Notebook does not appear to be JSON: u\\'{\\n  \"worksheets\": [\\n    {\\n      \"ce...',)\n\nI found the culprit by loading the file using json module in python:\n\nIn [11]: s = open('1_notmnist.ipynb').read()\nIn [12]: json.loads(s)\n(...)\nValueError: Expecting , delimiter: line 187 column 431 (char 9558)\n\nIt turned out to be non-escaped double quotes inside json string. After chaning them to single quotes the notebook loaded correctly.\n", "comments": ["This was just rolled back.\n", "Unfortunately the problem still exists\n", "Fixed with pr #1611\n", "Vincent.  I just got the same issue:\n\"Error loading notebook\nUnreadable Notebook: C:\\Users\\Yaroslav\\Documents\\Knowledge\\Udacity - Deep Learning\\1_notmnist.ipynb NotJSONError('Notebook does not appear to be JSON: u\\'\\n\\n\\n\\n<!DOCTYPE html>\\n<html lang=\"e...',)\"\n\nshould I use some trick for the notebook to work?\n", "The notebook at head parses well on GitHub:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/1_notmnist.ipynb\nIt didn't last time around, so you have a different problem. Would need to see repro steps.\n", "Vincent, thank you for your response!  I did the following:\n\n1) downloaded 1_notmnist.ipynb from the \"Assignment 1: notMNIST\" page of the Deep Learning course (https://classroom.udacity.com/courses/ud730/lessons/6370362152/concepts/63703142310923)\n2) ran the Jupyter Notebook environment from Anaconda.  Jupyter tab opened in Chrome\n3) put 1_notmnist.ipynb into a folder, visible through the \"Files\" tab of the environment\n3) found the file through the \"Files\" tab and clicked on it to open\n\nP.S. Unfortunately my experience with Jupyter Notebooks is quite limited.  Therefore it is likely that I am trying to open the file in a wrong way..\n", "Happened for me as well.\r\nIPython 5.4.1 (Jupyter 4.3.0)\r\n\r\n$ wget https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/1_notmnist.ipynb\r\n$ md5sum 1_notmnist.ipynb \r\nf8261a76e67ccc7cc1ac2718ad7bc3d9  1_notmnist.ipynb\r\n$ jupyter notebook 1_notmnist.ipynb\r\n[I 20:26:37.275 NotebookApp] Serving notebooks from local directory: /home/USER/udacity/deep_learning\r\n[I 20:26:37.275 NotebookApp] 0 active kernels \r\n[I 20:26:37.275 NotebookApp] The Jupyter Notebook is running at: http://localhost:8888/?token=d9e0b86be1407ebf71dfc18f870c5c705c648ace106ac484\r\n[I 20:26:37.275 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\r\n[C 20:26:37.276 NotebookApp] \r\n    \r\n    Copy/paste this URL into your browser when you connect for the first time,\r\n    to login with a token:\r\n        http://localhost:8888/?token=d9e0b86be1407ebf71dfc18f870c5c705c648ace106ac484\r\nCreated new window in existing browser session.\r\n[I 20:26:37.813 NotebookApp] Accepting one-time-token-authenticated connection from 127.0.0.1\r\n[W 20:26:38.291 NotebookApp] Unreadable Notebook: /home/USER/udacity/deep_learning/1_notmnist.ipynb NotJSONError(\"Notebook does not appear to be JSON: u'\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n<!DOCTYPE html>\\\\n<html lan...\",)\r\n[W 20:26:38.291 NotebookApp] 400 GET /api/contents/1_notmnist.ipynb?type=notebook&_=1501633598051 (127.0.0.1) 4.79ms referer=http://localhost:8888/notebooks/1_notmnist.ipynb\r\n[W 20:26:38.326 NotebookApp] 404 GET /nbextensions/widgets/notebook/js/extension.js?v=20170801202637 (127.0.0.1) 10.89ms referer=http://localhost:8888/notebooks/1_notmnist.ipynb\r\n\r\n", "@TomMiller if the notebook you get starts with 'u'\\n\\n\\n\\n\\n\\n\\n<html lan..., I would take a look at the rest of the HTML, seems like you might be getting an error response to your wget command instead of the actual notebook.", "Thanks Vincent. The issue was that I shouldn't have been using wget at all. I thought that would get me the 1_notmnist.ipynb file, but instead it downloaded the web page view of the 1_notmnist.ipynb file. I cloned the tensorflow repository, and opening the file in the repository worked just fine.", "Click on the 'raw' button and then the url `https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/examples/udacity/1_notmnist.ipynb` turns up. Thats the json representation\r\n"]}, {"number": 867, "title": "Failed to build master with GPU support", "body": "with bazel 0.1.4\n\n```\nINFO: From Compiling tensorflow/core/kernels/cwise_op_complex.cc:\nIn file included from ./tensorflow/core/public/partial_tensor_shape.h:19:0,\n                 from ./tensorflow/core/framework/attr_value_util.h:25,\n                 from ./tensorflow/core/framework/function.h:22,\n                 from ./tensorflow/core/framework/op_kernel.h:26,\n                 from ./tensorflow/core/kernels/cwise_ops_common.h:26,\n                 from tensorflow/core/kernels/cwise_op_complex.cc:16:\n./tensorflow/core/framework/partial_tensor_shape.h: In static member function 'static bool tensorflow::PartialTensorShapeUtils::AreCompatible(const tensorflow::gtl::ArraySlice<tensorflow::PartialTensorShape>&, const tensorflow::gtl::ArraySlice<tensorflow::PartialTensorShape>&)':\n./tensorflow/core/framework/partial_tensor_shape.h:156:40: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n       for (int i = 0; i < shapes0.size(); ++i) {\n                                        ^\nIn file included from ./tensorflow/core/framework/op_kernel.h:38:0,\n                 from ./tensorflow/core/kernels/cwise_ops_common.h:26,\n                 from tensorflow/core/kernels/cwise_op_complex.cc:16:\n./tensorflow/core/framework/unique_tensor_references.h: In member function 'void tensorflow::UniqueTensorReferences::Add(const tensorflow::Tensor&)':\n./tensorflow/core/framework/unique_tensor_references.h:63:61: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n         for (int i = 0; i < referenced_tensors_vector_.size(); ++i) {\n                                                             ^\nINFO: From Compiling tensorflow/core/kernels/tile_ops.cc:\nIn file included from ./tensorflow/core/public/partial_tensor_shape.h:19:0,\n                 from ./tensorflow/core/framework/attr_value_util.h:25,\n                 from ./tensorflow/core/framework/function.h:22,\n                 from ./tensorflow/core/framework/op_kernel.h:26,\n                 from ./tensorflow/core/framework/numeric_op.h:19,\n                 from tensorflow/core/kernels/tile_ops.cc:27:\n./tensorflow/core/framework/partial_tensor_shape.h: In static member function 'static bool tensorflow::PartialTensorShapeUtils::AreCompatible(const tensorflow::gtl::ArraySlice<tensorflow::PartialTensorShape>&, const tensorflow::gtl::ArraySlice<tensorflow::PartialTensorShape>&)':\n./tensorflow/core/framework/partial_tensor_shape.h:156:40: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n       for (int i = 0; i < shapes0.size(); ++i) {\n                                        ^\nIn file included from ./tensorflow/core/framework/op_kernel.h:38:0,\n                 from ./tensorflow/core/framework/numeric_op.h:19,\n                 from tensorflow/core/kernels/tile_ops.cc:27:\n./tensorflow/core/framework/unique_tensor_references.h: In member function 'void tensorflow::UniqueTensorReferences::Add(const tensorflow::Tensor&)':\n./tensorflow/core/framework/unique_tensor_references.h:63:61: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n         for (int i = 0; i < referenced_tensors_vector_.size(); ++i) {\n                                                             ^\nINFO: From Compiling tensorflow/core/kernels/conv_grad_ops.cc:\nIn file included from ./tensorflow/core/public/partial_tensor_shape.h:19:0,\n                 from ./tensorflow/core/framework/attr_value_util.h:25,\n                 from ./tensorflow/core/framework/function.h:22,\n                 from ./tensorflow/core/framework/op_kernel.h:26,\n                 from ./tensorflow/core/framework/numeric_op.h:19,\n                 from tensorflow/core/kernels/conv_grad_ops.cc:22:\n./tensorflow/core/framework/partial_tensor_shape.h: In static member function 'static bool tensorflow::PartialTensorShapeUtils::AreCompatible(const tensorflow::gtl::ArraySlice<tensorflow::PartialTensorShape>&, const tensorflow::gtl::ArraySlice<tensorflow::PartialTensorShape>&)':\n./tensorflow/core/framework/partial_tensor_shape.h:156:40: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n       for (int i = 0; i < shapes0.size(); ++i) {\n                                        ^\nIn file included from ./tensorflow/core/framework/op_kernel.h:38:0,\n                 from ./tensorflow/core/framework/numeric_op.h:19,\n                 from tensorflow/core/kernels/conv_grad_ops.cc:22:\n./tensorflow/core/framework/unique_tensor_references.h: In member function 'void tensorflow::UniqueTensorReferences::Add(const tensorflow::Tensor&)':\n./tensorflow/core/framework/unique_tensor_references.h:63:61: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n         for (int i = 0; i < referenced_tensors_vector_.size(); ++i) {\n                                                             ^\ntensorflow/core/kernels/conv_grad_ops.cc: In instantiation of 'tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda8':\ntensorflow/core/kernels/conv_grad_ops.cc:718:22:   required from 'struct tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda8'\ntensorflow/core/kernels/conv_grad_ops.cc:729:7:   required from 'void tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]'\ntensorflow/core/kernels/conv_grad_ops.cc:1422:1:   required from here\ntensorflow/core/kernels/conv_grad_ops.cc:718:56: error: use of 'tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda8::__col_buffer_data' before deduction of 'auto'\n                     &size_A](int64 start, int64 limit) {\n                                                        ^\ntensorflow/core/kernels/conv_grad_ops.cc:718:56: error: use of 'tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda8::__col_buffer_data' before deduction of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:718:56: error: use of 'tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda8::__input_data' before deduction of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:718:56: error: use of 'tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda8::__input_data' before deduction of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:720:50: error: use of 'input_data' before deduction of 'auto'\n           const T* input_data_shard = input_data + shard_id * input_offset;\n                                                  ^\ntensorflow/core/kernels/conv_grad_ops.cc:720:50: error: invalid use of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:720:63: error: cannot convert 'auto*' to 'const float*' in initialization\n           const T* input_data_shard = input_data + shard_id * input_offset;\n                                                               ^\ntensorflow/core/kernels/conv_grad_ops.cc:721:47: error: use of 'col_buffer_data' before deduction of 'auto'\n           T* col_data_shard = col_buffer_data + shard_id * size_A;\n                                               ^\ntensorflow/core/kernels/conv_grad_ops.cc:721:47: error: invalid use of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:721:60: error: cannot convert 'auto*' to 'float*' in initialization\n           T* col_data_shard = col_buffer_data + shard_id * size_A;\n                                                            ^\ntensorflow/core/kernels/conv_grad_ops.cc: In instantiation of 'void tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]':\ntensorflow/core/kernels/conv_grad_ops.cc:1422:1:   required from here\ntensorflow/core/kernels/conv_grad_ops.cc:715:20: error: invalid initialization of reference of type 'auto*&' from expression of type 'const float*'\n       auto shard = [&input_data, &col_buffer_data, &in_depth, &input_rows,\n                    ^\ntensorflow/core/kernels/conv_grad_ops.cc:715:20: error: invalid initialization of reference of type 'auto*&' from expression of type 'float*'\ntensorflow/core/kernels/conv_grad_ops.cc: In instantiation of 'tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda4':\ntensorflow/core/kernels/conv_grad_ops.cc:496:24:   required from 'struct tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda4'\ntensorflow/core/kernels/conv_grad_ops.cc:514:9:   required from 'void tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]'\ntensorflow/core/kernels/conv_grad_ops.cc:1422:1:   required from here\ntensorflow/core/kernels/conv_grad_ops.cc:496:58: error: use of 'tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda4::__filter_data' before deduction of 'auto'\n                       &size_C](int64 start, int64 limit) {\n                                                          ^\ntensorflow/core/kernels/conv_grad_ops.cc:496:58: error: use of 'tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda4::__filter_data' before deduction of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:496:58: error: use of 'tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda4::__out_backprop_data' before deduction of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:496:58: error: use of 'tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda4::__out_backprop_data' before deduction of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:496:58: error: use of 'tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda4::__col_buffer_data' before deduction of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:496:58: error: use of 'tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda4::__col_buffer_data' before deduction of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:496:58: error: use of 'tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda4::__input_backprop_data' before deduction of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:496:58: error: use of 'tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda4::__input_backprop_data' before deduction of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:498:45: error: use of 'col_buffer_data' before deduction of 'auto'\n             T* im2col_buf = col_buffer_data + shard_id * size_C;\n                                             ^\ntensorflow/core/kernels/conv_grad_ops.cc:498:45: error: invalid use of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:498:58: error: cannot convert 'auto*' to 'float*' in initialization\n             T* im2col_buf = col_buffer_data + shard_id * size_C;\n                                                          ^\ntensorflow/core/kernels/conv_grad_ops.cc:499:49: error: use of 'input_backprop_data' before deduction of 'auto'\n             T* input_data = input_backprop_data + shard_id * input_offset;\n                                                 ^\ntensorflow/core/kernels/conv_grad_ops.cc:499:49: error: invalid use of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:499:62: error: cannot convert 'auto*' to 'float*' in initialization\n             T* input_data = input_backprop_data + shard_id * input_offset;\n                                                              ^\ntensorflow/core/kernels/conv_grad_ops.cc:500:51: error: use of 'out_backprop_data' before deduction of 'auto'\n             const T* out_data = out_backprop_data + shard_id * output_offset;\n                                                   ^\ntensorflow/core/kernels/conv_grad_ops.cc:500:51: error: invalid use of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:500:64: error: cannot convert 'auto*' to 'const float*' in initialization\n             const T* out_data = out_backprop_data + shard_id * output_offset;\n                                                                ^\ntensorflow/core/kernels/conv_grad_ops.cc:506:71: error: use of 'filter_data' before deduction of 'auto'\n             ConstMatrixMap B(filter_data, filter_total_size, out_depth);\n                                                                       ^\ntensorflow/core/kernels/conv_grad_ops.cc:506:71: error: no matching function for call to 'Eigen::Map<const Eigen::Matrix<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >::Map(auto*&, const int&, const int64&)'\ntensorflow/core/kernels/conv_grad_ops.cc:506:71: note: candidates are:\nIn file included from ./external/eigen_archive/eigen-eigen-c8e5d094f3a9/Eigen/Core:378:0,\n                 from third_party/eigen3/Eigen/Core:1,\n                 from ./external/eigen_archive/eigen-eigen-c8e5d094f3a9/unsupported/Eigen/CXX11/Core:14,\n                 from ./external/eigen_archive/eigen-eigen-c8e5d094f3a9/unsupported/Eigen/CXX11/Tensor:14,\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\n                 from ./tensorflow/core/framework/types.h:23,\n                 from ./tensorflow/core/framework/type_traits.h:22,\n                 from ./tensorflow/core/framework/allocator.h:25,\n                 from ./tensorflow/core/framework/op_kernel.h:22,\n                 from ./tensorflow/core/framework/numeric_op.h:19,\n                 from tensorflow/core/kernels/conv_grad_ops.cc:22:\n./external/eigen_archive/eigen-eigen-c8e5d094f3a9/Eigen/src/Core/Map.h:149:12: note: Eigen::Map<MatrixType, MapOptions, StrideType>::Map(Eigen::Map<MatrixType, MapOptions, StrideType>::PointerArgType, Eigen::Index, Eigen::Index, const StrideType&) [with PlainObjectType = const Eigen::Matrix<float, -1, -1, 1, -1, -1>; int MapOptions = 0; StrideType = Eigen::Stride<0, 0>; Eigen::Map<MatrixType, MapOptions, StrideType>::PointerArgType = const float*; Eigen::Index = long int]\n     inline Map(PointerArgType dataPtr, Index rows, Index cols, const StrideType& stride = StrideType())\n            ^\n./external/eigen_archive/eigen-eigen-c8e5d094f3a9/Eigen/src/Core/Map.h:149:12: note:   no known conversion for argument 1 from 'auto*' to 'Eigen::Map<const Eigen::Matrix<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >::PointerArgType {aka const float*}'\n./external/eigen_archive/eigen-eigen-c8e5d094f3a9/Eigen/src/Core/Map.h:135:12: note: Eigen::Map<MatrixType, MapOptions, StrideType>::Map(Eigen::Map<MatrixType, MapOptions, StrideType>::PointerArgType, Eigen::Index, const StrideType&) [with PlainObjectType = const Eigen::Matrix<float, -1, -1, 1, -1, -1>; int MapOptions = 0; StrideType = Eigen::Stride<0, 0>; Eigen::Map<MatrixType, MapOptions, StrideType>::PointerArgType = const float*; Eigen::Index = long int]\n     inline Map(PointerArgType dataPtr, Index size, const StrideType& stride = StrideType())\n            ^\n./external/eigen_archive/eigen-eigen-c8e5d094f3a9/Eigen/src/Core/Map.h:135:12: note:   no known conversion for argument 1 from 'auto*' to 'Eigen::Map<const Eigen::Matrix<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >::PointerArgType {aka const float*}'\n./external/eigen_archive/eigen-eigen-c8e5d094f3a9/Eigen/src/Core/Map.h:122:21: note: Eigen::Map<MatrixType, MapOptions, StrideType>::Map(Eigen::Map<MatrixType, MapOptions, StrideType>::PointerArgType, const StrideType&) [with PlainObjectType = const Eigen::Matrix<float, -1, -1, 1, -1, -1>; int MapOptions = 0; StrideType = Eigen::Stride<0, 0>; Eigen::Map<MatrixType, MapOptions, StrideType>::PointerArgType = const float*]\n     explicit inline Map(PointerArgType dataPtr, const StrideType& stride = StrideType())\n                     ^\n./external/eigen_archive/eigen-eigen-c8e5d094f3a9/Eigen/src/Core/Map.h:122:21: note:   candidate expects 2 arguments, 3 provided\n./external/eigen_archive/eigen-eigen-c8e5d094f3a9/Eigen/src/Core/Map.h:88:79: note: Eigen::Map<const Eigen::Matrix<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >::Map(const Eigen::Map<const Eigen::Matrix<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >&)\n template<typename PlainObjectType, int MapOptions, typename StrideType> class Map\n                                                                               ^\n./external/eigen_archive/eigen-eigen-c8e5d094f3a9/Eigen/src/Core/Map.h:88:79: note:   candidate expects 1 argument, 3 provided\ntensorflow/core/kernels/conv_grad_ops.cc: In instantiation of 'void tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]':\ntensorflow/core/kernels/conv_grad_ops.cc:1422:1:   required from here\ntensorflow/core/kernels/conv_grad_ops.cc:490:22: error: invalid initialization of reference of type 'auto*&' from expression of type 'float*'\n         auto shard = [&in_depth, &input_rows, &input_cols, &filter_rows,\n                      ^\ntensorflow/core/kernels/conv_grad_ops.cc:490:22: error: invalid initialization of reference of type 'auto*&' from expression of type 'float*'\ntensorflow/core/kernels/conv_grad_ops.cc:490:22: error: invalid initialization of reference of type 'auto*&' from expression of type 'const float*'\ntensorflow/core/kernels/conv_grad_ops.cc:490:22: error: invalid initialization of reference of type 'auto*&' from expression of type 'const float*'\nIn file included from ./tensorflow/core/framework/op_kernel.h:22:0,\n                 from ./tensorflow/core/framework/numeric_op.h:19,\n                 from tensorflow/core/kernels/conv_grad_ops.cc:22:\n./tensorflow/core/framework/allocator.h: In member function 'virtual std::size_t tensorflow::Allocator::RequestedSize(void*)':\n./tensorflow/core/framework/allocator.h:122:3: warning: control reaches end of non-void function [-Wreturn-type]\n   }\n   ^\nIn file included from ./tensorflow/core/framework/op_kernel.h:25:0,\n                 from ./tensorflow/core/framework/numeric_op.h:19,\n                 from tensorflow/core/kernels/conv_grad_ops.cc:22:\n./tensorflow/core/framework/device_base.h: In member function 'virtual tensorflow::Allocator* tensorflow::DeviceBase::GetAllocator(tensorflow::AllocatorAttributes)':\n./tensorflow/core/framework/device_base.h:147:3: warning: control reaches end of non-void function [-Wreturn-type]\n   }\n   ^\n./tensorflow/core/framework/device_base.h: In member function 'virtual const tensorflow::DeviceAttributes& tensorflow::DeviceBase::attributes() const':\n./tensorflow/core/framework/device_base.h:175:3: warning: control reaches end of non-void function [-Wreturn-type]\n   }\n   ^\nERROR: /home/phill/tensorflow/tensorflow/core/BUILD:341:1: C++ compilation of rule '//tensorflow/core:kernels' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command\n  (cd /home/phill/.cache/bazel/_bazel_phill/124dc7c235a2311dca36360045bf7452/tensorflow && \\\n  exec env - \\\n    PATH=/home/phill/bin:/usr/local/cuda-7.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/jpeg_archive -iquote bazel-out/local_linux-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/re2 -iquote bazel-out/local_linux-opt/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-c8e5d094f3a9 -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive/eigen-eigen-c8e5d094f3a9 -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda/include -pthread -fno-exceptions -DEIGEN_AVOID_STL_ARRAY '-DGOOGLE_CUDA=1' '-DGOOGLE_CUDA=1' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernels/tensorflow/core/kernels/conv_grad_ops.pic.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernels/tensorflow/core/kernels/conv_grad_ops.pic.d -fPIC -c tensorflow/core/kernels/conv_grad_ops.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernels/tensorflow/core/kernels/conv_grad_ops.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: crosstool_wrapper_driver_is_not_gcc failed: error executing command\n  (cd /home/phill/.cache/bazel/_bazel_phill/124dc7c235a2311dca36360045bf7452/tensorflow && \\\n  exec env - \\\n    PATH=/home/phill/bin:/usr/local/cuda-7.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/jpeg_archive -iquote bazel-out/local_linux-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/re2 -iquote bazel-out/local_linux-opt/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-c8e5d094f3a9 -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive/eigen-eigen-c8e5d094f3a9 -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda/include -pthread -fno-exceptions -DEIGEN_AVOID_STL_ARRAY '-DGOOGLE_CUDA=1' '-DGOOGLE_CUDA=1' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernels/tensorflow/core/kernels/conv_grad_ops.pic.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernels/tensorflow/core/kernels/conv_grad_ops.pic.d -fPIC -c tensorflow/core/kernels/conv_grad_ops.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernels/tensorflow/core/kernels/conv_grad_ops.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\n```\n", "comments": ["There are some other [similar issues](https://github.com/tensorflow/tensorflow/search?q=crosstool_wrapper_driver_is_not_gcc&type=Issues&utf8=%E2%9C%93) that might help you. I also bump into this issue once in a while. What operating system and gcc version are you using?\n", "We could also avoid using auto there, I'm not sure it adds much to readability ...\n", "ubuntu 12.04, gcc 4.8.1\n", "FYI, I ran in to this as well, with the same OS/compiler. I was able to hack around it by replacing the `auto*`s with `T*` or `const T*` as appropriate. Agreed with @vrv that version with `T*` is more readable.\n", "Pull requests welcome if anyone wants to fix this!  @lesniewski: Would you interested in wrapping your fixes up as a PR?\n", "Sure! It was only 8 lines. Might take a little while before I get around to it though.\n"]}, {"number": 866, "title": "Make data_folders have only folders", "body": "On OS X the system created .DS_Store files in every folder. As a result running the same notebook twice\nleads to the exception being thrown.\nReally, |data_folders| should be gated behind  a check for folder type.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "@shreyasva thanks! I would slightly prefer no line wrapping:\ndata_folders = [\n  os.path.join(root, d) for d in sorted(os.listdir(root))\n  if os.path.isdir(os.path.join(root, d))]\n\nPlease sign the CLA so that we can merge!\n@vrv FYI\n", "@googlebot I signed it!\n", "\"commit f13cf6a2b5609a6ed0f8c09c77661fca7ad95467\nAuthor: Shreyas VA shreyasva@Shreyass-MacBook-Pro.local\n\"\n\nYou need to recommit with a proper email\n", "I'm running into rebase issues as well. I think it's better to start afresh.\n", "@shreyasva hold tight a little longer: I've just committed a change that upgrades all the notebooks to v4 to make such diffs easier down the road. Sorry about that :/\n", "No worries :)\nI'll follow up shortly.\n", "Merged, so you can probably re-apply now.\n", "PTAL at https://github.com/tensorflow/tensorflow/pull/948.\n"]}, {"number": 865, "title": "Latest commit (524595) for Udacity Assignment 1 breaks IPython Notebook", "body": "Hi, I attempted to open the latest IPython Notebook for the Udacity course Assignment 1 from commit 524595 (23 Jan 2016), and it fails with the error:\n\n```\nUnreadable Notebook: ~/tensorflow/tensorflow/examples/udacity/1_notmnist.ipynb\nNotJSONError('Notebook does not appear to be JSON: u\\'{\\\\n \"worksheets\": [\\\\n {\\\\n \"ce...',)\n```\n\nThe notebook also fails to load here on Github. If I checkout the previous commit 42154f, the notebook loads correctly.\n\nIt's possible this is just me having bad luck, so if important, I'm running Anaconda Python on Ubuntu 14.04.3 LTS.\n", "comments": ["No, it looks like you have to use single quotes, I'll revert that commit since I can't really read the diff anyway.\n", "Reverted in https://github.com/tensorflow/tensorflow/commit/be1f68e254d38a7545145731ded5873cb914e509\n"]}, {"number": 864, "title": "Fix small missing [by]", "body": "I think there is the word `by` missing:\n\n> Replace the strides [by] a max pooling operation [...]\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Signed the Google CLA.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "LGTM @vrv\n"]}, {"number": 863, "title": "SessionOptions should be declared as defined. <numeric> should be included to use itoa.", "body": "Some minor issues found while porting the code to Windows.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins: test this please\n", "Merged\n"]}, {"number": 862, "title": " How to Fine-tuning a Pretrained Network?", "body": "Any one can give an example code of how to fine tuning a pretrained imagenet network with new data and different classes num. Just like this:\nhttps://github.com/BVLC/caffe/blob/master/examples/03-fine-tuning.ipynb\n", "comments": ["Please use StackOverflow for these types of questions.\n"]}, {"number": 861, "title": "Noise in Variable assignment", "body": "I am writing a code to use an LSTM in the following manner:\n1. Given an input _x_ of dimensionality _d_ at time _t_, two quantities are computed:\n   a. The difference _x(t) - x(t-1)_. I call this the first derivative of _x_: _x'(t)_.\n   b. The difference _x'(t) - x'(t-1)_. I call this the second derivative of _x_: _x''(t)_.\n2. _x(t)_, _x'(t)_, _x''(t)_ are concatenated as the total input to an LSTM of state size _3xd_.\n3. The output from the LSTM, say _L(t)_ is then passed onto a _d_-dimensional densely connected linear layer. So the final output out is _f(t) = W x L(t) + b_\n\nI am using the _AdamOptimizer_ with a learning rate of 0.0005.\n\nThe problem I am facing is, when I do steps 1 and 2 as given above _outside_ TensorFlow, the model works well. However, when I try to do the computation of derivatives inside a TensorFlow Graph, there seems to be some noise seeping into the values of the variables. Heres the code:\n\n```\n##The Input Layer as a Placeholder\n#Since we will provide data sequentially, the 'batch size'\n#is 1.\ninput_layer = tf.placeholder(tf.float32, [1, input_dim])\n\n##First Order Derivative Layer\n#This will store the last recorded value\nlast_value1 = tf.Variable(tf.zeros([1, input_dim]))\n#Subtract last value from current\nsub_value1 = tf.sub(input_layer, last_value1)\n#Update last recorded value\nlast_assign_op1 = last_value1.assign(input_layer)\n\n##Second Order Derivative Layer\n#This will store the last recorded derivative\nlast_value2 = tf.Variable(tf.zeros([1, input_dim]))\n#Subtract last value from current\nsub_value2 = tf.sub(sub_value1, last_value2)\n#Update last recorded value\nlast_assign_op2 = last_value2.assign(sub_value1)\n\n##Overall input to the LSTM\n#x and its first and second order derivatives as outputs of\n#earlier layers\nzero_order = input_layer\nfirst_order = sub_value1\nsecond_order = sub_value2\n#Concatenated\ntotal_input = tf.concat(1, [zero_order, first_order, second_order])\n```\n\nWhile running the model, I run in order: final_output (which is dependent on total_input), then last_assign_op1 and then last_assign_op2.\nHowever, there always seems to be a difference of about 0.001 between what the actual values of _last_value1_ and _last_value2_ should be, and what they really are. This is causing a lot of noise in the final output.\n\nWhat is the reason for this? Is this somehow because of the Optimizer (I can't figure out why that would be)? Or am I doing something wrong?\n", "comments": ["This looks more like a StackOverflow question rather than a bug report.  You might try using a control dependency to make sure your assigns are happening after the subtractions.\n"]}, {"number": 860, "title": ".DS_Store in OSX is not an image file and gives error. So skip over t\u2026", "body": "\u2026his file.\nAm not sure if the patch captured the indentation. Pl check.\n", "comments": ["Can one of the admins verify this patch?\n", "LGTM @vrv \n", "Merged\n"]}, {"number": 859, "title": "Feature request: clear all variable tensors", "body": "When using ipython notebooks, one frequently runs the same cell without restarting the kernel. If the cell contains code of the form:\n\n```\nx = tf.get_variable('x', (1,))\n```\n\nthe cell will run correctly the first time, but throw an error the second. Obviously this error is appropriate in python files, but really just gets in the way for notebooks. It would be great if something like tf.clear_all_variables() were implemented, such that I could have a cell with contents:\n\n```\ntf.clear_all_variables()\nx = tf.get_variable('x', (1,))\n```\n\nthat I could run over and over again. On a larger scale, tensorflow seems to be rather unfriendly to notebooks in general, frequently requiring kernel restarts for small changes. But this issue for definitely contributes to the greatest number of forced kernel restarts for me.\n", "comments": ["I'm not sure about one thing: is your intention to clear the variables in the scope, but preserve them in the graph, or to clear the whole graph? In the first case, I think it's dangerous -- why call it \"x\" if you're re-doing x? I understand if you want to clear the whole graph, but that can be done with tf.graph().as_default(), no? Please, let me know more of the context so we can solve it!\n", "[`tf.reset_default_graph()`](https://www.tensorflow.org/versions/master/api_docs/python/framework.html#reset_default_graph) is probably the closest thing we have to this functionality, although it blows away all ops and tensors, as well as all variables.\n", "I see, yea my intention is to indeed reset everything. It seems that these options will work. Thanks.\n", "Thank mrry, I was looking for this\n", "@mrry When I add `tf.reset_default_graph()` to [my code](https://github.com/MartinThoma/HASY/blob/master/scripts/keras_hasy_cv.py), I get\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"keras_hasy_cv.py\", line 78, in <module>\r\n    verbose=1, validation_data=(test_x, test_y))\r\n  File \"/home/moose/.local/lib/python2.7/site-packages/keras/models.py\", line 672, in fit\r\n    initial_epoch=initial_epoch)\r\n  File \"/home/moose/.local/lib/python2.7/site-packages/keras/engine/training.py\", line 1192, in fit\r\n    initial_epoch=initial_epoch)\r\n  File \"/home/moose/.local/lib/python2.7/site-packages/keras/engine/training.py\", line 892, in _fit_loop\r\n    outs = f(ins_batch)\r\n  File \"/home/moose/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 1898, in __call__\r\n    session = get_session()\r\n  File \"/home/moose/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 118, in get_session\r\n    _initialize_variables()\r\n  File \"/home/moose/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 270, in _initialize_variables\r\n    sess.run(tf.variables_initializer(uninitialized_variables))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 951, in _run\r\n    fetch_handler = _FetchHandler(self._graph, fetches, feed_dict_string)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 407, in __init__\r\n    self._fetch_mapper = _FetchMapper.for_fetch(fetches)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 238, in for_fetch\r\n    return _ElementFetchMapper(fetches, contraction_fn)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 274, in __init__\r\n    'Tensor. (%s)' % (fetch, str(e)))\r\nValueError: Fetch argument <tensorflow.python.framework.ops.Operation object at 0x7f7fe0e2e550> cannot be interpreted as a Tensor. (Operation name: \"init\"\r\nop: \"NoOp\"\r\ninput: \"^convolution2d_3_W/Assign\"\r\ninput: \"^convolution2d_3_b/Assign\"\r\ninput: \"^prelu_3_alphas/Assign\"\r\n```\r\n\r\nDo you have any idea why / how to fix it? (Related to http://stackoverflow.com/q/42173704/562769)", "You have to recreate everything from scratch after resetting graph", "Hi, I am facing something similar, I posted the problem on [stackoverflow](https://stackoverflow.com/questions/47657157/variables-of-tensorflow-generate-error-in-a-loop) as well. But no help. I am calling a function with CNN, so if i return from that function, it should delete all the variable and on return create new ones. But it does not. I tried to use answer by mrry that use tf.reset_default_graph(), but even that gives error that no graph found. Can any body guide how to solve this issue. ", "Is there a TensorFlow 2 equivalent of `tf.reset_default_graph`? (other than `tf.compat.v1.reset_default_graph`)"]}, {"number": 858, "title": "Variable num_classes than num_folders in exception code", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Yep, signed it.\n\nOn Sat, Jan 23, 2016 at 6:25 PM, googlebot notifications@github.com wrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project. Before we can look at your\n> pull request, you'll need to sign a Contributor License Agreement (CLA).\n> \n> [image: :memo:] _Please visit https://cla.developers.google.com/\n> https://cla.developers.google.com/ to sign._\n> \n> Once you've signed, please reply here (e.g. I signed it!) and we'll\n> \n> ## verify. Thanks.\n> - If you've already signed a CLA, it's possible we don't have your\n>   GitHub username or you're using a different email address. Check your\n>   existing CLA data https://cla.developers.google.com/clas and verify\n>   that your email is set on your git commits\n>   https://help.github.com/articles/setting-your-email-in-git/.\n> - If you signed the CLA as a corporation, please let us know the\n>   company's name.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/858#issuecomment-174244053\n> .\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Thanks, though this is a duplicate of #850, which I'm about to merge.\n"]}, {"number": 857, "title": "Assignment 2 of Udacity course", "body": "Lecture one covers logistic regression and stochastic gradient descent. Assignment 2, assignment of this lecture, requests students to implement a neural network which is covered in later chapter.\nI suggest the question should be related to the topic of lecture.\n", "comments": ["Thanks for pointing it out!\n\nThe first part of Assignment 2 (code provided) is relevant immediately after SGD in L1, but the stated problem requires knowledge of ReLUs and the ability to create a multi-layer network. So Assignment 2 has been moved to right after Backprop in L2, and just before discussing deeper networks.\n"]}, {"number": 856, "title": "How to invoke tf.initialize_all_variables() in C++?", "body": "Hi, and thank you for your job)\nI'm newbie in tensorflow.\nBtw, how to execute ops in C++ such as tf.initialize_all_variables() or some ops returned by optimizers\nself.train_op = self.optimizer.apply_gradients(gradients)\n?\n\nI've tried \ninit_all_vars_op = tf.initialize_variables(tf.all_variables(), name='init_all_vars_op')\n...\ntf.train.write_graph(session.graph_def, 'models/', 'graph.pb', as_text=False)\nin python and then in C++:\n\n```\n    status = session->Run(inputs, {\"init_all_vars_op\"}, {}, &outputs);\n    if (!status.ok()) {\n        std::cout << \"tf error: \" << status.ToString() << \"\\n\";\n        return;\n    }\n```\n\nso I got\n\ntf error: Invalid argument: FetchOutputs init_all_vars_op: output index too large, must be < 0\n\nhere is stackowerflow question\nhttp://stackoverflow.com/questions/34975884/how-to-invoke-tf-initialize-all-variables-in-c-tensorflow\n", "comments": ["Looks like this is an unclear error message.  You are asking for the output of the \"init_all_vars_op\", but it doesn't have any output.  You just want to run that, which you do by supplying it to the third parameter to Run(), see https://www.tensorflow.org/versions/master/api_docs/cc/ClassSession.html#virtual_Status_tensorflow_Session_Run\n\nstatus = session->Run(inputs, {}, {\"init_all_vars_op\"}, &outputs);\n"]}, {"number": 855, "title": "Avoids an extra copy of the graph def.", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "signed it\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins: test this please\n"]}, {"number": 854, "title": "fix typos in docstrings", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Merged\n"]}, {"number": 853, "title": "Add links to new tensorflow Udacity courses #852", "body": "Hey TF-Team,\n\nI added two links to the new tensorflow Udacity course in the Main Readme.md and the Udacity Courses examples Readme.md\n\nCheers,\nPatrick\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins ok to test\n", "@tensorflow-jenkins: test this please\n", "(I would prefer squashing these two commits, since they are small)\n", "Is there anything I can do for you guys?\n", "(yeah, squash the commits into one.  See [here](https://github.com/ginatrapani/todo.txt-android/wiki/Squash-All-Commits-Related-to-a-Single-Issue-into-a-Single-Commit))\n", "Merged, thanks!\n"]}, {"number": 852, "title": "Add new Tensorflow Udacity Course too the main Readme or Readme in master/tensorflow/examples/udacity", "body": "Dear Tensorflow-Team,\n\nplease add a link of the new Tensorflow Udacity course to your Readme and Docs.\nhttps://www.udacity.com/course/deep-learning--ud730\n\nThanks and best Wishes,\nPatrick\n", "comments": ["+1\n", "PR #853 merged\n"]}, {"number": 851, "title": "tensorflow for Nvidia TX1", "body": "Hello,\n\n@maxcuda has recently got tensorflow running on the tk1 as documented in blogpost http://cudamusing.blogspot.de/2015/11/building-tensorflow-for-jetson-tk1.html but since then been unable to repeatedly build it. I am now trying to get tensorflow running on a tx1 tegra platform and need some support.\n\nMuch trouble seems to come from Eigen variadic templates and using C++11 initializer lists, both of wich could work according to http://devblogs.nvidia.com/parallelforall/cplusplus-11-in-cuda-variadic-templates/.\nIn theory std=c++11 should be set according to crosstool. Nevertheless, nvcc crashes happily on all of them. This smells as if the  \"-std=c++11\"  flag is not properly set. \nHow can I verify/enforce this?\n\nAlso in tensorflow.bzl, variadic templates in Eigen are said to be disabled\n`We have to disable variadic templates in Eigen for NVCC even though std=c++11 are enabled`\nis that still necessary?\n\nHere is my build workflow:\n\n```\ngit clone \u2014recurse-submodules git@github.com:jmtatsch/tensorflow.git\ncd tensorflow\ngrep -Rl \"lib64\"| xargs sed -i 's/lib64/lib/g' # no lib64 for tx1 yet \n./configure\nbazel build -c opt --local_resources 2048,0.5,1.0 --verbose_failures --config=cuda //tensorflow/cc:tutorials_example_trainer\n```\n", "comments": ["See http://devblogs.nvidia.com/parallelforall/power-cpp11-cuda-7/\n", "Are you using [jetpack 2](https://developer.nvidia.com/embedded/jetson-development-pack)?\n", "No, JetPack does not support running directly on the L4T platform.\n", "I meant if you have flashed the board with jetpack 2 to have cuda 7 support.\n", "Ah, yes I have Cuda 7 support and used jetpack 2. To be more precise, the target is not actually the Jetson TX1 but an repurposed Nvida Sield TV flashed to L4T 23.1 for Jetson.\n", "@Yangqing FYI\n", "I think there is a TX1 that I could use to take a look. I'll see what I can do.\n", "In theory, can TensorFlow run usefully on the TK1?  Or is the 2G memory too small for, say, face verification? \n", "@robagar It all depends on how large your network is and whether you intend to train the model on TK1 or just run inference. Two GB of memory is plenty to run inference on almost any model. \n", "I have worked around an issue that prevented nvcc from compiling the Eigen codebase on Tegra X1 (https://bitbucket.org/eigen/eigen/commits/d0950ac79c0404047379eb5a927a176dbb9d12a5).\nHowever, so far I haven't succeeded in setting up bazel on the Tegra X1, so I haven't been able to start working on the other issues reported in http://cudamusing.blogspot.de/2015/11/building-tensorflow-for-jetson-tk1.html\n", "That's good news ;) Whats the problem with bazel? maxcuda's instructions for building bazel worked quite well for me..\n", "For building bazel I had to use a special java build which can cope with the 32bit rootfs on a 64bit machine\n\n```\nwget http://www.java.net/download/jdk8u76/archive/b02/binaries/jdk-8u76-ea-bin-b02-linux-arm-vfp-hflt-04_jan_2016.tar.gz\nsudo tar -zxvf jdk-8u76-ea-bin-b02-linux-arm-vfp-hflt-04_jan_2016.tar.gz -C /usr/lib/jvm\nsudo update-alternatives --install \"/usr/bin/java\" \"java\" \"/usr/lib/jvm/jdk1.8.0_76/bin/java\" 1\nsudo update-alternatives --config java\n```\n\nThere seems to be one eigen issue I can't get around:\n\n```\nbazel build -c opt --local_resources 2048,0.5,1.0 --verbose_failures --config=cuda //tensorflow/cc:tutorials_example_trainer\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nINFO: Found 1 target...\nINFO: From Compiling tensorflow/core/kernels/cross_op_gpu.cu.cc:\nAt end of source: warning: routine is both \"inline\" and \"noinline\"\n\nexternal/eigen_archive/eigen-eigen-c5e90d9e764e/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h(125): warning: routine is both \"inline\" and \"noinline\"\n\nAt end of source: warning: routine is both \"inline\" and \"noinline\"\n\nexternal/eigen_archive/eigen-eigen-c5e90d9e764e/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h(125): warning: routine is both \"inline\" and \"noinline\"\n\n./tensorflow/core/lib/strings/strcat.h(195): internal error: assertion failed at: \"/dvs/p4/build/sw/rel/gpu_drv/r346/r346_00/drivers/compiler/edg/EDG_4.9/src/decl_inits.c\", line 3251\n\n\n1 catastrophic error detected in the compilation of \"/tmp/tmpxft_0000682d_00000000-8_cross_op_gpu.cu.cpp4.ii\".\nCompilation aborted.\nAborted\nERROR: /opt/tensorflow/tensorflow/core/BUILD:331:1: output 'tensorflow/core/_objs/gpu_kernels/tensorflow/core/kernels/cross_op_gpu.cu.o' was not created.\nERROR: /opt/tensorflow/tensorflow/core/BUILD:331:1: not all outputs were created.\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nINFO: Elapsed time: 2271.358s, Critical Path: 2260.25s\n```\n\nCan you have a look at TensorEvaluator.h please?\n", "I still haven't been able to install bazel. That said, the assertion you're facing seems to be triggered by the variadic template at line 195 of ./tensorflow/core/lib/strings/strcat.h. I would just comment this code and see how it goes.\n", "When you say maxcuda has \"been unable to repeatedly build it\" since then, does that mean that tensorflow is no longer working on the TK1 again? Because I just ordered the TK1 with the express purpose of being able to run tensorflow :-/\n", "Yes, I have been unable to recompile the latest versions. The wheel I built around Thanksgiving should still work but it is quite an old version. \n", "Commenting the variadic template at line 195 helps a little but at line 234 there is a another template that seems to be required. Any hints how to rewrite that in nvcc friendly manner?\n", "@benoitsteiner \nany suggestions how this could be rewritten in a nvcc compatible manner?\n\n```\n// Support 5 or more arguments\ntemplate <typename... AV>\ninline void StrAppend(string *dest, const AlphaNum &a, const AlphaNum &b,\n                      const AlphaNum &c, const AlphaNum &d, const AlphaNum &e,\n                      const AV &... args) {\n  internal::AppendPieces(dest,\n                         {a.Piece(), b.Piece(), c.Piece(), d.Piece(), e.Piece(),\n                          static_cast<const AlphaNum &>(args).Piece()...});\n}\n```\n", "@damienmg FYI\n", "Hi folks, I'm also working on building everything from scratch on tx1. There is lots of discussions here and also on nvidia developer forums. But by now I haven't seen any well summarized instruction besides that tk1's.  Can we start another repo or script file so people can work on it more efficient?\n", "Imho we have to first solve the fundamental issue of the variadic templates not working with nvcc. Either the developers would have to do without those templates which is backwards and probably not going to happen or nvidia has to step up and make nvcc more compatible? In theory nvcc should already be able to deal  with your own variadic templates, but external e.g. STL headers won't \"just work\" because of the need to annotate all functions called on the device with \"**host** **device**\". Maybe someone knows a good way how to get around this issue....\n", "@jmtatsch At the moment, the version of cuda that is shipped with the tegra x1 has problems with variadic templates. Nvidia is aware of this and working on a fix. I updated Eigen a few weeks ago to disable the use of variadic templates when compiling on tegra x1, and that seems to fix the bulk of the problem. However, StrCat and StrAppend still rely on variadic templates. Until nvidia releases a fix, the best solution is to comment out the variadic versions of StrCat and StrAppend, and create non variadic versions of StrCat and StrAppend with up to 11 arguments (since that's what TensorFlow currently needs).\nThere are a couple of ways to avoid the STL issues: a brittle solution is to only compile optimized kernels. The compiler then inlines the STL code at which point the lack of host device annotation doesn't matter since there is no function call to resolve. A better solution is to replace all the STL functionality with custom code. We've started to do this in Eigen by reimplementing most of the STL functions we need in the Eigen::numext namespace. This is tedious by much more reliable than relying on inlining to bypass the problem.\n", "I have a build of TF 0.8 but it requires a new 7.0 compiler that is not yet available to the general public. \nI am building a wheel  on a Jetson TK1, I will make it available after some testing.\nI will update the instructions on how to build from source on cudamusing. \n", "Good work @maxcuda!  Will it build on the TX1 too?\n", "Yes, it will build on TX1 too. I fixed a problem with the new memory allocator to take in account the 32bit OS. Some basic tests are passing but the label_image test is giving the wrong results so there may be some other places with 32bit issues.\n", "@benoitsteiner , with the new compiler your change to Eigen is not required anymore ( and it is forcing to edit a bunch of files). Could you please remove the check and re-enable variadic templates ?\n", "@maxcuda Where can I download the new cuda compiler? I'd like to make sure that I don't introduce new problems when I enable variadic templates again.\n", "@maxcuda is the new 7.0 compiler you were referencing part of Jetpack 2.2 that was just released? \n", "Yes, you can get it with:\nwget http://developer.download.nvidia.com/embedded/L4T/r24_Release_v1.0/CUDA/cuda-repo-l4t-7-0-local_7.0-76_armhf.deb\n\nThe good news are that I was able to build v0.8 but some of the results are incorrect. I will update the blog with the changes. With v0.9 I had problem with the cudnn.cc file, it looks like it cannot handle cuddn v2.\n", "Thanks so much. Looking forward to your post so I can get tensorflow running on the TX1\n", "I updated my building instruction on cudamusing and also posted a wheel file.\n", "Has anyone tested this on jetson tx1? I can't seem to get bazel build on aarch64.\n", "@syed-ahmed I tested it on TX1. This is my configurations.\n- Cuda Toolkit 7.0, JetPack 2.2(32bit)\n- Bazel 0.2.1\n- jdk-8u76-ea-bin-b02-linux-arm-vfp-hflt-04_jan_2016.tar.gz\n- ./configure : compute capability 5.3\n- bazel option : --local_resources 2048,2.0,1.0\n", "@syed-ahmed I got it to build on an aarch64 TX1. I mostly followed the instructions for the TK1 at cudamusing.blogspot.de. The only additional things I did were \n- add aarch64 to the ARM enum in /bazel/src/main/java/com/google/devtools/build/lib/util/CPU.java by changing line 28 to   \"ARM(\"arm\", ImmutableSet.of(\"arm\", \"armv7l\", \"aarch64\")),\" without quotes \n- Added aarch64 as valid ARM machine type in /bazel/scripts/bootstrap/buildenv.sh by changing line 35 to \"if [ \"${MACHINE_TYPE}\" = 'arm' -o \"${MACHINE_TYPE}\" = 'armv7l' -o \"${MACHINE_TYPE}\" = 'aarch64' ]; then\" without quotes\n\nOr, if you prefer, here is the bazel executable for aarch64 I ended up with: https://drive.google.com/file/d/0B8Gc_oVaYC7CWEhOMHJhc0hLY0U/view?usp=sharing\n", "Maybe make a PR against bazel?\n\nOn Wed, Jul 6, 2016 at 8:38 AM Tyler Fox notifications@github.com wrote:\n\n> @syed-ahmed https://github.com/syed-ahmed I got it to build on an\n> aarch64 TX1. I mostly followed the instructions for the TK1 at\n> cudamusing.blogspot.de. The only additional things I did was\n> - add aarch64 to the ARM enum in\n>   /bazel/src/main/java/com/google/devtools/build/lib/util/CPU.java by\n>   changing line 28 to \"ARM(\"arm\", ImmutableSet.of(\"arm\", \"armv7l\",\n>   \"aarch64\")),\" without quotes\n> - Added aarch64 as valid ARM machine type in\n>   /bazel/scripts/bootstrap/buildenv.sh by changing line 35 to \"if [\n>   \"${MACHINE_TYPE}\" = 'arm' -o \"${MACHINE_TYPE}\" = 'armv7l' -o\n>   \"${MACHINE_TYPE}\" = 'aarch64' ]; then\" without quotes\n> \n> Or, if you prefer, here is the bazel executable for aarch64 I ended up\n> with:\n> https://drive.google.com/file/d/0B8Gc_oVaYC7CWEhOMHJhc0hLY0U/view?usp=sharing\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/851#issuecomment-230810921,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAjO_SFJWCHTe1vT-jcv8t5tp51x9clmks5qS8vjgaJpZM4HK5_C\n> .\n", "@tylerfox Thank you! I'll try your suggestions. In the meanwhile, any thoughts on this: https://github.com/bazelbuild/bazel/issues/1264 and @wtfuzz 's [change for cc_configure.bzl](https://github.com/wtfuzz/bazel/commit/ddc37b2a69b53e54a0664719fe9326925c909422). I was getting a toolchain error. So wondering if you encountered it. \n\nDid you also build with latest bazel release or 0.1.4.? And how about the tensorflow version - r0.8?\n", "@syed-ahmed yes, changing the buildenv.sh should fix that issue. Also it's worth noting that I used bazel 0.1.4 per the instructions on cudamusing. I should probably also test on the current version of bazel, but for now I know 0.1.4 works\n", "I am trying to build the tensorflow r0.9 release. I got bazel 0.2.1 installed following @tylerfox 's suggestions. Getting this following error when trying to build tensorflow. Any thoughts? Appreciate all the help.\n\n```\n>>>>> # @farmhash_archive//:configure [action 'Executing genrule @farmhash_archive//:configure [for host]']\n(cd /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/tensorflow && \\\n  exec env - \\\n    PATH=/usr/local/cuda-7.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/ubuntu/bazel/output/ \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; pushd external/farmhash_archive/farmhash-34c13ddfab0e35422f4c3979f360635a8c050260; workdir=$(mktemp -d -t tmp.XXXXXXXXXX); cp -a * $workdir; pushd $workdir; ./configure; popd; popd; cp $workdir/config.h bazel-out/host/genfiles/external/farmhash_archive/farmhash-34c13ddfab0e35422f4c3979f360635a8c050260; rm -rf $workdir;')\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/farmhash_archive/BUILD:5:1: Executing genrule @farmhash_archive//:configure failed: bash failed: error executing command \n  (cd /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/tensorflow && \\\n  exec env - \\\n    PATH=/usr/local/cuda-7.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/ubuntu/bazel/output/ \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; pushd external/farmhash_archive/farmhash-34c13ddfab0e35422f4c3979f360635a8c050260; workdir=$(mktemp -d -t tmp.XXXXXXXXXX); cp -a * $workdir; pushd $workdir; ./configure; popd; popd; cp $workdir/config.h bazel-out/host/genfiles/external/farmhash_archive/farmhash-34c13ddfab0e35422f4c3979f360635a8c050260; rm -rf $workdir;'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n/home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/tensorflow/external/farmhash_archive/farmhash-34c13ddfab0e35422f4c3979f360635a8c050260 /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/tensorflow\n/tmp/tmp.ZKGtjQ4mLO /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/tensorflow/external/farmhash_archive/farmhash-34c13ddfab0e35422f4c3979f360635a8c050260 /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/tensorflow\nchecking for a BSD-compatible install... /usr/bin/install -c\nchecking whether build environment is sane... yes\nchecking for a thread-safe mkdir -p... /bin/mkdir -p\nchecking for gawk... no\nchecking for mawk... mawk\nchecking whether make sets $(MAKE)... yes\nchecking whether make supports nested variables... yes\nchecking build system type... /tmp/tmp.ZKGtjQ4mLO/missing: Unknown `--is-lightweight' option\nTry `/tmp/tmp.ZKGtjQ4mLO/missing --help' for more information\nconfigure: WARNING: 'missing' script is too old or missing\n./config.guess: unable to guess system type\n\nThis script, last modified 2010-08-21, has failed to recognize\nthe operating system you are using. It is advised that you\ndownload the most up to date version of the config scripts from\n\n  http://git.savannah.gnu.org/gitweb/?p=config.git;a=blob_plain;f=config.guess;hb=HEAD\nand\n  http://git.savannah.gnu.org/gitweb/?p=config.git;a=blob_plain;f=config.sub;hb=HEAD\n\nIf the version you run (./config.guess) is already up to date, please\nsend the following data and any information you think might be\npertinent to <config-patches@gnu.org> in order to provide the needed\ninformation to handle your system.\n\nconfig.guess timestamp = 2010-08-21\n\nuname -m = aarch64\nuname -r = 3.10.96-tegra\nuname -s = Linux\nuname -v = #1 SMP PREEMPT Tue May 17 16:29:05 PDT 2016\n\n/usr/bin/uname -p = \n/bin/uname -X     = \n\nhostinfo               = \n/bin/universe          = \n/usr/bin/arch -k       = \n/bin/arch              = \n/usr/bin/oslevel       = \n/usr/convex/getsysinfo = \n\nUNAME_MACHINE = aarch64\nUNAME_RELEASE = 3.10.96-tegra\nUNAME_SYSTEM  = Linux\nUNAME_VERSION = #1 SMP PREEMPT Tue May 17 16:29:05 PDT 2016\nconfigure: error: cannot guess build type; you must specify one\n```\n", "Anyone knows what farmhash is being used for in tensorflow r0.9? My motivation for installing tensorflow 0.9 on the jetson tx1 is to solely utilize some of the fp16 ops. Hence, if farmhash is not doing anything important, may be I could remove the firmhash related code and build without it. Here is the farmhash [commit](https://github.com/tensorflow/tensorflow/commit/50fd1301afe9214fab42abbafbe96374b2567a52).\n", "Temporary sources used in the build process can be found in ~/.cache/bazel. Cd to this directory and search for config.guess: find ./ -name \"config.guess\".\nYou might get several files but the paths should give you a clue which config.guess is the one of farmhash. In my case it is ./_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/farmhash_archive/farmhash-34c13ddfab0e35422f4c3979f360635a8c050260/config.guess \nIn this file replace line\nUNAME_MACHINE=`(uname -m) 2>/dev/null` || UNAME_MACHINE=unknown\nwith\nUNAME_MACHINE=armhf\n\nOn my machine (Nvidia Shield TV flashed to L4T 23.1) farmhash built successfully after this change.\n", "I successfully build the tensorflow on TX1 24.1 64 bit, with the following patch. But, run example failed with following kernel message.\n**`tutorials_examp[31026]: unhandled level 1 translation fault (11) at 0xffffffffffffe8, esr 0x92000005`**\n\nMaybe farmhard.BUILD with `--build=arm-linux-gnu` is wrong? But, I failed to compile it with `--build=aaarch64-linux-gnu`. I'm still trying to figure out what caused the runtime fails. \n[tx1_patch.zip](https://github.com/tensorflow/tensorflow/files/364991/tx1_patch.zip)\n", "@benoitsteiner has reenablling variadic templates been verified to work?\n", "@shingchuang  have you found the root cause of segmentation fault issue?  I have the same problem on aarch64 platform.\n", "I tried to reenable variadic templates last night after upgrading the cuda compiler using http://developer.download.nvidia.com/embedded/L4T/r24_Release_v1.0/CUDA/cuda-repo-l4t-7-0-local_7.0-76_armhf.deb. This new compiler appears to fix some of the issues but I still get some crashes.\n\nI noticed that nvidia released an even more recent version of the compiler. @maxcuda, is there a debian package that I can use to install the latest version of the cuda sdk ?\n", "Re-install / re-flash using [JetPack 2.3](https://developer.nvidia.com/embedded/jetpack) because the latest release also updated to Ubuntu 16.04 aarch64 in addition to CUDA 8 and L4T R24.2.   The underlying CUDA version is tied to the L4T BSP in JetPack.\n", "Hi all. I'm trying to build TensorFlow for the Google Pixel C in order to use the GPU TX1. Do you build it on your machine (e.g. Mac) or on the device itself (e.g. Pixel C)? Does anyone have the already generated files for TX1 or can point me in the right direction? Thanks.\n", "Hi all - haven't gotten TensorFlow r0.11 working yet, but do have a working path to r0.9 TensorFlow install on TX1 with JetPack 2.3. Have tested basic nets MLP/LSTM/Conv and seems to work, though it OOMS out pretty easily on bigger convs.\n\nWrote down all my steps and patches below if it's helpful to anyone. Really appreciated all above commentary was critical to tracking down right path.\n\nhttp://stackoverflow.com/questions/39783919/tensorflow-on-nvidia-tx1/\n", "@dwightcrow , I tried your solution, and it works on TX1, thank you. And the version 0.11.0rc0 can be built with bazel with version of 0.3.2\n", "That's fantastic. Bazel 0.3.2 builds fairly easily on TX1? \n", "Wondering if there's a concise summary of everything in this issue? It would definitely make it easier for others trying to get TF working on a TX1. \n", "Following up on the request for a summary to build tensorflow on a Jetson TX1. Any help is appreciated. \n", "The problem is that there are too many moving pieces. Each set of instructions may fail when Bazel/Protobuf/Eigen/TF are updated.\n", "@dwightcrow Hi Dwight, at some point in the instructions you say:\n\"Need an edit to recognize aarch64 as ARM\"\n\nCan you please expand, edit what? Also, can we update the answer to build the latest version?\n\nI agree with @sunils27 and @maxcuda that we need a more stable set of instructions for specific components..\n\nThank you very much for the effort and time to support the community. \n", "Furthermore, if there is a stable set of build instructions, it becomes accessible to more people who can help in its upkeep when the aforementioned packages (by @maxcuda ) are updated. \n", "I've reenabled support for variadic templates on Tegra-X1 provided that one uses JetPack 2.3 (in previous versions nvcc crashes when compiling some of the variadic templates). I haven't tried yet to compile TensorFlow itself but this should reduce the number of code changes necessary to work around the lack of IndexList on Tegra.\n", "While a stable set of instructions may remain elusive, one effective way of documenting a working set is to create your own fork of each of the repos and push any changes you need to make as commits on one branch for each version of TF you're targeting.  Then in a write-up you can refer to specfic branches / commits that are known to work.  You could even go a step further by creating a meta repo which has references to each of those commits; git submodules (as much as I dislike them) are one way, another is using simple scripts to automate what your writeup describes.\n\nIn other words: have a personal github fork of basel, tensorflow, etc. and a branch called something like \"topic/tf_v0.10\" on each fork.  Then optionally a new repo altogether which unifies them and a community of folks such as we have on this thread could collaborate to push updates to it as we try different things.\n", "right, is anyone able to advise where do those changes need to go in the bazel part of the instructions on SstackOverflow? Any help is greatly appreciated. While this doesn't solve the bigger problems with getting tensorflow to work on SATV, it does offer me (and others) the chance to get it going in the current format\n", "# Build tensorflow r0.11 on Nvidia TX1 failed\n\n## Error message:\n\n``` bash\nERROR: .../tensorflow/core/kernels/BUILD:1096:1: C++ compilation of rule '//tensorflow/core/kernels:svd_op' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command\n ...\ncom.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4.\ngcc: internal compiler error: Killed (program cc1plus)\nPlease submit a full bug report,\nwith preprocessed source if appropriate.\nSee <file:///usr/share/doc/gcc-5.4/README.Bugs> for instructions.\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\n```\n\nMy build steps and environment:\n\n### Environment\n- Hardware: Nvidia TX1\n- OS: JetPack 2.3 (Ubuntu 16.04)\n- cuDNN:5.1\n- CUDA: 8\n\n### Install Java\n\n``` bash\n$ sudo add-apt-repository ppa:webupd8team/java\n$ sudo apt-get update\n$ sudo apt-get install oracle-java8-installer\n```\n\n### Install some deps\n\n``` bash\n$ sudo apt-get install git zip unzip autoconf automake libtool curl zlib1g-dev maven\n$ sudo apt-get install python-numpy swig python-dev python-wheel\n```\n\n### Build protobuf\n\n``` bash\n# For grpc-java build\n$ git clone https://github.com/google/protobuf.git\n$ cd protobuf\n$ git checkout master\n$ ./autogen.sh\n$ git checkout v3.0.0-beta-3\n$ ./autogen.sh\n$ LDFLAGS=-static ./configure --prefix=$(pwd)/../\n$ sed -i -e 's/LDFLAGS = -static/LDFLAGS = -all-static/' ./src/Makefile\n$ make -j 4\n$ make install\n\n\n# For bazel build\n$ git checkout v3.0.0-beta-2\n$./autogen.sh\n$ LDFLAGS=-static ./configure --prefix=$(pwd)/../\n$ sed -i -e 's/LDFLAGS = -static/LDFLAGS = -all-static/' ./src/Makefile\n$ make -j 4\n$ cd ..\n```\n\n### Build grpc-java compiler\n\n``` bash\n$ git clone https://github.com/neo-titans/odroid.git\n$ git clone https://github.com/grpc/grpc-java-git\n$ cd grpc-java/\n$ git checkout v0.15.0\n$ patch -p0 < ../odroid/build_tensorflow/grpc-java.v0.15.0.patch\n$ CXXFLAGS=\"-I$(pwd)/../include\" LDFLAGS=\"-L$(pwd)/../lib\" ./gradlew java_pluginExecutable -Pprotoc=$(pwd)/../bin/protoc\n$ cd ..\n```\n\n### Build bazel\n\n``` bash\n$ git clone https://github.com/bazelbuild/bazel.git\n$ cd bazel\n$ git checkout 0.3.2\n$ cp ../protobuf/src/protoc third_party/protobuf/protoc-linux-arm32.exe\n$ cp ../grpc-java/compiler/build/exe/java_plugin/protoc-gen-grpc-java third_party/grpc/protoc-gen-grpc-java-0.15.0-linux-arm32.exe\n```\n\nModify some files for build on aarch64  \n\n``` code\ndiff --git a/compile.sh b/compile.sh\nindex 53fc412..11035d9 100755\n--- a/compile.sh\n+++ b/compile.sh\n@@ -27,7 +27,7 @@ cd \"$(dirname \"$0\")\"\n # Set the default verbose mode in buildenv.sh so that we do not display command\n # output unless there is a failure.  We do this conditionally to offer the user\n # a chance of overriding this in case they want to do so.\n-: ${VERBOSE:=no}\n+: ${VERBOSE:=yes}\n\n source scripts/bootstrap/buildenv.sh\n\ndiff --git a/scripts/bootstrap/compile.sh b/scripts/bootstrap/compile.sh\nindex 77372f0..657b254 100755\n--- a/scripts/bootstrap/compile.sh\n+++ b/scripts/bootstrap/compile.sh\n@@ -48,6 +48,7 @@ linux)\n   else\n     if [ \"${MACHINE_IS_ARM}\" = 'yes' ]; then\n       PROTOC=${PROTOC:-third_party/protobuf/protoc-linux-arm32.exe}\n+      GRPC_JAVA_PLUGIN=${GRPC_JAVA_PLUGIN:-third_party/grpc/protoc-gen-grpc-java-0.15.0-linux-arm32.exe}\n     else\n       PROTOC=${PROTOC:-third_party/protobuf/protoc-linux-x86_32.exe}\n       GRPC_JAVA_PLUGIN=${GRPC_JAVA_PLUGIN:-third_party/grpc/protoc-gen-grpc-java-0.15.0-linux-x86_32.exe}\n@@ -150,7 +151,7 @@ function java_compilation() {\n\n   run \"${JAVAC}\" -classpath \"${classpath}\" -sourcepath \"${sourcepath}\" \\\n       -d \"${output}/classes\" -source \"$JAVA_VERSION\" -target \"$JAVA_VERSION\" \\\n-      -encoding UTF-8 \"@${paramfile}\"\n+      -encoding UTF-8 \"@${paramfile}\" -J-Xmx500M\n\n   log \"Extracting helper classes for $name...\"\n   for f in ${library_jars} ; do\ndiff --git a/src/main/java/com/google/devtools/build/lib/util/CPU.java b/src/main/java/com/google/devtools/build/lib/util/CPU.java\nindex 41af4b1..4d80610 100644\n--- a/src/main/java/com/google/devtools/build/lib/util/CPU.java\n+++ b/src/main/java/com/google/devtools/build/lib/util/CPU.java\n@@ -26,7 +26,7 @@ public enum CPU {\n   X86_32(\"x86_32\", ImmutableSet.of(\"i386\", \"i486\", \"i586\", \"i686\", \"i786\", \"x86\")),\n   X86_64(\"x86_64\", ImmutableSet.of(\"amd64\", \"x86_64\", \"x64\")),\n   PPC(\"ppc\", ImmutableSet.of(\"ppc\", \"ppc64\", \"ppc64le\")),\n-  ARM(\"arm\", ImmutableSet.of(\"arm\", \"armv7l\")),\n+  ARM(\"arm\", ImmutableSet.of(\"arm\", \"armv7l\", \"aarch64\")),\n   UNKNOWN(\"unknown\", ImmutableSet.<String>of());\n\n   private final String canonicalName;\ndiff --git a/third_party/grpc/BUILD b/third_party/grpc/BUILD\nindex 2ba07e3..c7925ff 100644\n--- a/third_party/grpc/BUILD\n+++ b/third_party/grpc/BUILD\n@@ -29,7 +29,7 @@ filegroup(\n         \"//third_party:darwin\": [\"protoc-gen-grpc-java-0.15.0-osx-x86_64.exe\"],\n         \"//third_party:k8\": [\"protoc-gen-grpc-java-0.15.0-linux-x86_64.exe\"],\n         \"//third_party:piii\": [\"protoc-gen-grpc-java-0.15.0-linux-x86_32.exe\"],\n-        \"//third_party:arm\": [\"protoc-gen-grpc-java-0.15.0-linux-x86_32.exe\"],\n+        \"//third_party:arm\": [\"protoc-gen-grpc-java-0.15.0-linux-arm32.exe\"],\n         \"//third_party:freebsd\": [\"protoc-gen-grpc-java-0.15.0-linux-x86_32.exe\"],\n     }),\n )\ndiff --git a/third_party/protobuf/BUILD b/third_party/protobuf/BUILD\nindex 203fe51..4c2a316 100644\n--- a/third_party/protobuf/BUILD\n+++ b/third_party/protobuf/BUILD\n@@ -28,6 +28,7 @@ filegroup(\n         \"//third_party:darwin\": [\"protoc-osx-x86_32.exe\"],\n         \"//third_party:k8\": [\"protoc-linux-x86_64.exe\"],\n         \"//third_party:piii\": [\"protoc-linux-x86_32.exe\"],\n+        \"//third_party:arm\": [\"protoc-linux-arm32.exe\"],\n         \"//third_party:freebsd\": [\"protoc-linux-x86_32.exe\"],\n     }),\n )\ndiff --git a/tools/cpp/cc_configure.bzl b/tools/cpp/cc_configure.bzl\nindex aeb0715..688835d 100644\n--- a/tools/cpp/cc_configure.bzl\n+++ b/tools/cpp/cc_configure.bzl\n@@ -150,7 +150,12 @@ def _get_cpu_value(repository_ctx):\n     return \"x64_windows\"\n   # Use uname to figure out whether we are on x86_32 or x86_64\n   result = repository_ctx.execute([\"uname\", \"-m\"])\n-  return \"k8\" if result.stdout.strip() in [\"amd64\", \"x86_64\", \"x64\"] else \"piii\"\n+  machine = result.stdout.strip()\n+  if machine in [\"arm\", \"armv7l\", \"aarch64\"]:\n+   return \"arm\"\n+  elif machine in [\"amd64\", \"x86_64\", \"x64\"]:\n+   return \"k8\"\n+  return \"piii\"\n\n\n _INC_DIR_MARKER_BEGIN = \"#include <...>\"\n```\n\ncompile\n\n``` bash\n$ ./compile.sh \n$ cd..\n```\n\n### Build Tensorflow\n\n``` bash\n$ git clone https://github.com/tensorflow/tensorflow.git\n$ git checkout v0.11.0.rc2\n```\n\nAccording to StackOverflow's [tensorflow-on-nvidia-tx1](http://stackoverflow.com/questions/39783919/tensorflow-on-nvidia-tx1/) to modify\n\n``` code\ndiff --git a/tensorflow/core/kernels/BUILD b/tensorflow/core/kernels/BUILD\nindex 2e04827..9d81923 100644\n--- a/tensorflow/core/kernels/BUILD\n+++ b/tensorflow/core/kernels/BUILD\n@@ -1184,7 +1184,7 @@ tf_kernel_libraries(\n         \"segment_reduction_ops\",\n         \"scan_ops\",\n         \"sequence_ops\",\n-        \"sparse_matmul_op\",\n+        #DC \"sparse_matmul_op\",\n     ],\n     deps = [\n         \":bounds_check\",\ndiff --git a/tensorflow/core/kernels/cwise_op_gpu_select.cu.cc b/tensorflow/core/kernels/cwise_op_gpu_select.cu.cc\nindex 02058a8..880a0c3 100644\n--- a/tensorflow/core/kernels/cwise_op_gpu_select.cu.cc\n+++ b/tensorflow/core/kernels/cwise_op_gpu_select.cu.cc\n@@ -43,8 +43,14 @@ struct BatchSelectFunctor<GPUDevice, T> {\n     const int all_but_batch = then_flat_outer_dims.dimension(1);\n\n #if !defined(EIGEN_HAS_INDEX_LIST)\n-    Eigen::array<int, 2> broadcast_dims{{ 1, all_but_batch }};\n-    Eigen::Tensor<int, 2>::Dimensions reshape_dims{{ batch, 1 }};\n+    // Eigen::array<int, 2> broadcast_dims{{ 1, all_but_batch }};\n+    Eigen::array<int, 2> broadcast_dims;\n+   broadcast_dims[0] = 1;\n+    broadcast_dims[1] = all_but_batch;\n+    // Eigen::Tensor<int, 2>::Dimensions reshape_dims{{ batch, 1 }};\n+    Eigen::Tensor<int, 2>::Dimensions reshape_dims;\n+   reshape_dims[0] = batch;\n+   reshape_dims[1] = 1;\n #else\n     Eigen::IndexList<Eigen::type2index<1>, int> broadcast_dims;\n     broadcast_dims.set(1, all_but_batch);\ndiff --git a/tensorflow/core/kernels/sparse_tensor_dense_matmul_op_gpu.cu.cc b/tensorflow/core/kernels/sparse_tensor_dense_matmul_op_gpu.cu.cc\nindex a177696..28d2f59 100644\n--- a/tensorflow/core/kernels/sparse_tensor_dense_matmul_op_gpu.cu.cc\n+++ b/tensorflow/core/kernels/sparse_tensor_dense_matmul_op_gpu.cu.cc\n@@ -104,9 +104,17 @@ struct SparseTensorDenseMatMulFunctor<GPUDevice, T, ADJ_A, ADJ_B> {\n     int n = (ADJ_B) ? b.dimension(0) : b.dimension(1);\n\n #if !defined(EIGEN_HAS_INDEX_LIST)\n-    Eigen::Tensor<int, 2>::Dimensions matrix_1_by_nnz{{ 1, nnz }};\n-    Eigen::array<int, 2> n_by_1{{ n, 1 }};\n-    Eigen::array<int, 1> reduce_on_rows{{ 0 }};\n+    // Eigen::Tensor<int, 2>::Dimensions matrix_1_by_nnz{{ 1, nnz }};\n+    Eigen::Tensor<int, 2>::Dimensions matrix_1_by_nnz;\n+   matrix_1_by_nnz[0] = 1;\n+   matrix_1_by_nnz[1] = nnz;\n+    // Eigen::array<int, 2> n_by_1{{ n, 1 }};\n+    Eigen::array<int, 2> n_by_1;\n+   n_by_1[0] = n;\n+   n_by_1[1] = 1;\n+    // Eigen::array<int, 1> reduce_on_rows{{ 0 }};\n+    Eigen::array<int, 1> reduce_on_rows;\n+   reduce_on_rows[0]= 0;\n #else\n     Eigen::IndexList<Eigen::type2index<1>, int> matrix_1_by_nnz;\n     matrix_1_by_nnz.set(1, nnz);\ndiff --git a/tensorflow/stream_executor/cuda/cuda_gpu_executor.cc b/tensorflow/stream_executor/cuda/cuda_gpu_executor.cc\nindex 52256a7..1d027b9 100644\n--- a/tensorflow/stream_executor/cuda/cuda_gpu_executor.cc\n+++ b/tensorflow/stream_executor/cuda/cuda_gpu_executor.cc\n@@ -888,6 +888,9 @@ CudaContext* CUDAExecutor::cuda_context() { return context_; }\n // For anything more complicated/prod-focused than this, you'll likely want to\n // turn to gsys' topology modeling.\n static int TryToReadNumaNode(const string &pci_bus_id, int device_ordinal) {\n+// DC - make this clever later. ARM has no NUMA node, just return 0\n+LOG(INFO) << \"ARM has no NUMA node, hardcoding to return zero\";\n+return 0;\n #if defined(__APPLE__)\n   LOG(INFO) << \"OS X does not support NUMA - returning NUMA node zero\";\n   return 0;\n\n```\n\nbuild\n\n``` bash\n$ ./configure\n$ bazel build -c opt --jobs 2 --local_resources 1024,4.0,1.0 --config=cuda //tensorflow/tools/pip_package:build_pip_package\n```\n\n### References\n- http://stackoverflow.com/questions/39783919/tensorflow-on-nvidia-tx1/\n- https://www.neotitans.net/install-tensorflow-on-odroid-c2.html\n", "@elirex I'm pretty sure you're still running out of memory even with the --local_resources flag. Try adding some swap space\n", "@tylerfox I tried that, but it doesn't work.\n", "After enabling the swap space, I'd still opt for more memory and less cpu when building. Try something like bazel build -c opt --local_resources 3072,0.5,1.0 --verbose_failures --config=cuda //tensorflow/tools/pip_package:build_pip_package\n\nNote this will take several hours to build with these settings, but it's the only way I've found to work. Hope that helps.\n", "@elirex, Hi, you mentioned \"Modify some files for build on aarch64\", I don't know which files need to be modified and how? \nsimilar description in [tensorflow-on-nvidia-tx1](http://stackoverflow.com/questions/39783919/tensorflow-on-nvidia-tx1/) \"Need an edit to recognize aarch64 as ARM\".\nThanks!\n", "@ShawnXuan - these are files in the cloned bazel repo. The change proposed on StackOverflow for example would be made to [CPU.java](https://github.com/bazelbuild/bazel/blob/master/src/main/java/com/google/devtools/build/lib/util/CPU.java) as shown in the diff. You can see which files elirex changed in addition by looking at their diff. Hope that helps\n", "@elirex Did you manage to compile ? \n", "@piotrchmiel Yes, I successfully completed the compilation. I add 8GB swap space and run bazel build -c opt --local_resources 3072,4.0,1.0 --verbose_failures --config=cuda //tensorflow/tools/pip_package:build_pip_package\n\nAt compiling, I through free -h and top command to look the memory usage status. Tensorflow need to use   about 8GB memory to compile.\n", "Thank you \ud83d\udc4d I will try to repeat your steps :-)\n", "## Question:\r\n\r\nFor those that compiled TensorFlow 0.9 on the Jetson TX1, which options did you use during the TensorFlow `./configure` step?\r\n\r\n## Error 1:\r\n\r\nI received `Error: unexpected EOF from Bazel server` after following the steps from [this StackOverflow guide](\r\nhttp://stackoverflow.com/questions/39783919/tensorflow-on-nvidia-tx1/) from a fresh install of JetPack 2.3.\r\n\r\nTwo bazel issue responders ([1](https://github.com/bazelbuild/bazel/issues/920), [2](https://github.com/bazelbuild/bazel/issues/1338)) suggested people use the `--jobs 4` or `--jobs 20` option when receiving this error, in case the error was due to a lack of memory.\r\n\r\nI'm ran bazel again, this time with the `--jobs 4`; however, I received a new error (\"Error 2\", below).\r\n\r\nThe remainder of the error said, `Contents of `/home/ubuntu/.cache/bazel/_bazel_ubuntu/(xxxx)/server/jvm.out':` with no further output.\r\n\r\n## Error 2:\r\n\r\n`ERROR: /home/ubuntu/tensorflow/tensorflow/core/kernels/BUILD:309:1: C++ compilation of rule '//tensorflow/core/kernels:mirror_pad_op' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object ... (remaining 105 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4.\r\ngcc: internal compiler error: Killed (program cc1plus)`\r\n\r\nI didn't use `bazel clean --expunge` before the second attempt. Maybe that caused the error.\r\n\r\n## Plan:\r\n\r\n- Run `bazel clean --expunge`\r\n- Rerun bazel to create the cache folder\r\n- Readd `config.guess` and `config.sub` to the cache folder\r\n- Create 8GB of swap space\r\n- Try `bazel build -c opt --local_resources 3072,4.0,1.0 --verbose_failures --config=cuda //tensorflow/tools/pip_package:build_pip_package` because  @elirex had success with it.", "## It worked\r\n\r\nFollowing [this StackOverflow guide](http://stackoverflow.com/questions/39783919/tensorflow-on-nvidia-tx1/) but with an 8 GB swap file and using the following command successfully built TensorFlow 0.9 on the Jetson TX1 from a fresh install of [JetPack 2.3](https://developer.nvidia.com/embedded/jetpack-2_3):\r\n\r\n`bazel build -c opt --local_resources 3072,4.0,1.0 --verbose_failures --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nI used the default settings for TensorFlow's `./configure` script except to enable GPU support.\r\n\r\nMy build took at least 6 hours. It'll be faster if you use an SSD instead of a USB drive.\r\n\r\nThanks to [Dwight Crow](http://stackoverflow.com/a/39845817/3822261), @elirex, @tylerfox, everyone that helped them, and everyone in this thread for spending time on this problem.\r\n\r\n## Creating a swap file\r\n\r\n```\r\n# Create a swapfile for Ubuntu at the current directory location\r\nfallocate -l *G swapfile\r\n# List out the file\r\nls -lh swapfile\r\n# Change permissions so that only root can use it\r\nchmod 600 swapfile\r\n# List out the file\r\nls -lh swapfile\r\n# Set up the Linux swap area\r\nmkswap swapfile\r\n# Now start using the swapfile\r\nsudo swapon swapfile\r\n# Show that it's now being used\r\nswapon -s\r\n```\r\n\r\nAdapted from [JetsonHack's gist](https://gist.github.com/jetsonhacks/b80d6130066e3f351bc8).\r\n\r\nI used [this USB drive](https://www.amazon.com/Patriot-256GB-Supersonic-Boost-Flash/dp/B00F9V72H4/) to store my swap file.\r\n\r\nThe most memory I saw my system use was 7.7 GB (3.8 GB on Mem and 3.9 GB on Swap). The most swap memory I saw used at once was 4.4 GB. I used `free -h` to view memory usage.\r\n\r\n## Creating the pip package and installing\r\n\r\nAdapted from [the TensorFlow docs](https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#create-the-pip-package-and-install):\r\n\r\n```\r\n$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n\r\n# The name of the .whl file will depend on your platform.\r\n$ pip install /tmp/tensorflow_pkg/tensorflow-0.9.0-py2-none-any.whl\r\n```", "I use ``` bazel build -c opt --local_resources 1024,4.0,1.0 --jobs 4 --verbose_failures --config=cuda //tensorflow/tools/pip_package:build_pip_package ```  and without allocate swap to build tensorflow r.09 on the TX1 JetPack 2.3 that pass compilation. ", "Could anyone build TF r0.11 on TX1 yet?", "Thanks for all the information here, got tensorflow r0.11.0 installed with Jetpack 2.3.1 on tx1.  Following @elirex 's steps, make sure using the exact version of protobuf, grpc and bazel. I build tensorflow r0.11.0 instead of v0.11.0.rc2. When compiling, following @MatthewKleinsmith 's step to add swap file, you need a big swap, I tried 6G but failed in the middle with out of memory error, tried again with 10G swap file works. It took me about 5 hours for the compiling with swapfile allocated on usb drive.", "Is tensorflow working correctly on the TX1, ie. able to run inference and get good results? When I installed tensorflow on a TK1 it ran just fine however the convolutional layers where producing bad results. I could train fully connected models on mnist just fine but when I tried to use conv layers it stopped converging. Is this problem persistent in the TX1 build?", "Continually get this when running `./compile.sh` for Bazel:\r\n`Building Bazel from scratch`\r\n`gPRC Java plugin not found in`\r\n\r\nIf I pull 0.2.3 I don't get the error, only with 0.3.x", "@zxwind How is TF 0.11 performance working for you on the TX1?", "FYI, I've got a branch off r1.0 with some hacks to build the r1.0 release on TX1 with Jetpack 2.3.1. \r\n\r\nIn addition to the previously mentioned issues, there is a change in Eigen after the revision used on the TF r0.11 branch that causes the CUDA compiler to crash with an internal error. I changed workspace.bzl on r1.0 branch to point to the older Eigen revision. In order for that to build I had to remove the EXPM1 op that was added after r0.11. It's all rather ugly but got me up and running.\r\n\r\nInteresting to note, with the r1.0.0a build I'm able to run inference on a Resnet50 based network at 128x96 resolution that was running out of memory on r0.11. For anyone curious on benchmark numbers, was getting approx 15fps with single frame batches. \r\n\r\nLink to a tag on my clone of TF with binary wheels for anyone interested. The wheels will likely only work on a Jetpack 2.3.1 (L4T 24.2.1). No guarantees there aren't some serious issues but I've verified results on the networks I'm using right now.\r\nhttps://github.com/rwightman/tensorflow/releases/tag/v1.0.0-alpha-tegra-ugly_hack", "Closing since @rwightman / @MatthewKleinsmith solution seems to work, though not quite a seamless out-the-box experience. Feel free to reopen.", "@rwightman May I humbly ask you to provide another wheel for the r1.0 stable version?", "@rwightman How were you able to build tensorflow without gRPC? Thanks!\r\n\r\nEdit: never mind, I saw your repo : https://github.com/jetsonhacks/installTensorFlowTX1/\r\n\r\nThanks for setting that up.", "@sunsided Here's the Python 3.5.2 version for TF 1.0.1 that @dkopljar and I managed to build: https://drive.google.com/open?id=0B2jw9AHXtUJ_OFJDV19TWTEyaWc", "Hello all, I was able to install TensorFlow v1.0.1 on the new Jetson TX2. I had to follow similar process as mentioned above in this thread (protobuf, grpc, swapfile etc). For bazel, I downloaded [bazel-0.4.5-dist.zip](https://github.com/bazelbuild/bazel/releases/download/0.4.5/bazel-0.4.5-dist.zip) and applied @dtrebbien's change. Here is the pip wheel of my installation if it helps anyone. It's for Python 2.7: https://drive.google.com/file/d/0Bxl-G9VJ61mBYmZPY0hLSlFaUDg/view?usp=sharing\r\nAnd here the step by step procedure: https://syed-ahmed.gitbooks.io/nvidia-jetson-tx2-recipes/content/first-question.html", "Hello all, I was able to install TensorFlow v1.0.1 on Tegra X1 using the build by @barty777 \r\nIs there build availabke for TensorFlow v1.2 ?", "@barty777 you wouldn't happen to have 3.6 wheels, would you? :pray: ", "@gvoysey Unfortunately no. :(", "Here is the wheel file for TensorFlow 1.2, Nvidia TX1 and Python 2.7: https://drive.google.com/file/d/0B-Ljdh8jFZRbTnVNdGtGMHA2Ymc/view?usp=sharing", "i've been able to build a tensorflow wheel for python 3.6 for TX1, but i cannot build tensorflow-GPU support successfully.  See https://stackoverflow.com/questions/45825708/error-building-tensorflow-gpu-1-1-0-on-nvidia-jetson-tx1-aarch64  for details.", "Sorry for the late comment, can anyone please help me regarding setting up tensorflow in Nvidia tk1?"]}, {"number": 850, "title": "Fix Undefined Error on Exception", "body": "`num_folders` is undefined and probably `num_classes` is meant. But as the UndefinedError happens in an exception, the program fails anyway.\n", "comments": ["Can one of the admins verify this patch?\n", "LGTM, thanks.\n@vrv feel free to pull (let me know if there is a different protocol to signify changes are ready, I'll just reassign to you)\n", "Merged\n"]}, {"number": 849, "title": "Fix typo in ops.py", "body": "typo. see PR844\n", "comments": ["Can one of the admins verify this patch?\n", "Merged\n"]}]