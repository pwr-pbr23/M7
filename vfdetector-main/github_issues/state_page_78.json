[{"number": 13039, "title": "Reading tfrecord reaches deadock or crushes in one computer and works just fine on another.", "body": "### Laptop -System information\r\n- Linux Ubuntu 16.04\r\n- TensorFlow installed with pip:\r\n- TensorFlow 1.3.0\r\n- Python 2.7.12\r\n- Spyder 2.3.8\r\n\r\n### Workstation -System information\r\n- Linux Ubuntu 16.04\r\n- TensorFlow installed with pip:\r\n- TensorFlow-gpu 1.0.0\r\n- Python 2.7.13\r\n\r\n### Problem description\r\nI want to create and read a tfrecord file with [build_cgd_dataset.py](https://github.com/tnikolla/grasp-detection/blob/master/build_cgd_dataset.py) as the writer and [reader_iter.py](https://github.com/tnikolla/grasp-detection/blob/master/reader_iter.py) as the reader. Everything works smooth in the workstation but it reaches a deadlock or gets stuck in the laptop. The dataset are from [Cornell grasping dataset](http://pr.cs.cornell.edu/grasping/rect_data/data.php).\r\n\r\nI narrowed down the error in this special cases where it doesn't show any problem, for example:\r\n  - if it reads a int64_list with one value, so a list with one elemnet\r\n  - if it reads a float_list with one element\r\n\r\nIf the lists have more than one element the program freezes.\r\n", "comments": ["I could not reproduce on my workstation, after changing the paths in the programs to point to directories on my local workstation. To clarify, you are waiting for build_cgd_dataset.py to finish before starting reader_iter.py, right?\r\n\r\nTry to reproduce the problem in a very simple case. Before, you were limiting the the number of elements in the lists, but instead try limiting the number of `tf.train.Example`s. Also try creating the tfrecords on your workstation and read them on the laptop, or vice versa. And try using the same TensorFlow versions on both, to see if the issue is caused by a certain TensorFlow version.", "So, I write the tfrecord with build_cgd_dataset.py. This works always. The problem lies in reading them.\r\n\r\nI've done some more careful tests today. I'm writing here the programs:\r\n\r\n**writer.py**\r\n\r\n    import tensorflow as tf\r\n\r\n    def _int64_feature(v):\r\n        return tf.train.Feature(int64_list=tf.train.Int64List(value=v))\r\n\r\n    def _floats_feature(v):\r\n        return tf.train.Feature(float_list=tf.train.FloatList(value=v))\r\n    \r\n    def convert_to_example():\r\n        return tf.train.Example(features=tf.train.Features(feature={\r\n                'a': _floats_feature(a)}))\r\n    \r\n    #a = [1, 2, 3]\r\n    a = [1., 2., 3.]\r\n    def main():\r\n        writer = tf.python_io.TFRecordWriter('tfrecord_float')\r\n        example = convert_to_example()\r\n        writer.write(example.SerializeToString())\r\n        writer.close()\r\n\r\n    if __name__ == '__main__':\r\n        main()\r\n\r\n**reader.py**\r\n\r\n    import tensorflow as tf\r\n\r\n    filename = '/home/tomi/issue/tfrecord_float'\r\n    record_iter = tf.python_io.tf_record_iterator(path=filename)\r\n    example = tf.train.Example()\r\n    for record in record_iter:\r\n        example.ParseFromString(record)\r\n        a = example.features.feature['a'].float_list.value[:]\r\n        print(a)\r\n\r\n**Running the reader from the terminal** (and not Spyder 2.3.8) everything works as should. Running the reader from Spyder reading a list of floats crushes Spyder. With a list of integers, more often than not, the reader works fine. Maybe the issue should be closed.", "I also could not reproduce running from the terminal. You should verify that Spyder is using the same versions of Python and Tensorflow as when you run from the terminal.\r\n\r\nMarking as contributions welcome if someone can reproduce in Spyder."]}, {"number": 13020, "title": "tf.Session() freezes on GPU nodes of a SGE cluster", "body": "Hello,\r\n\r\nI'm trying to run the following script on the GPU nodes of my university's SGE cluster:\r\n\r\n```python\r\n#!/usr/bin/env python3\r\n# -*- coding: utf-8 -*-\r\n\r\nimport os\r\nimport sys\r\n\r\nimport tensorflow as tf\r\n\r\nvar1 = tf.get_variable('var1', [1024, 32], initializer=tf.random_normal_initializer())\r\nvar2 = tf.get_variable('var2', [32, 1024], initializer=tf.random_normal_initializer())\r\n\r\nm = tf.matmul(var1, var2)\r\n\r\ninit_op = tf.global_variables_initializer()\r\n\r\nwith tf.Session() as session:\r\n\tsession.run(init_op)\r\n\tsession.run(m)\r\n```\r\n\r\nEverything works if, either:\r\n- I allocate obscene amounts of memory, like 60GB, to the job;\r\n- Or if I run it from the command line on the cluster nodes.\r\n\r\nHowever, if I allocate a reasonable amount of memory to the job (8GB), it freezes on the `tf.Session()` call. A `strace -p <pid>` on the process gives the following trace, over and over, endlessly:\r\n\r\n```\r\nopen(\"/proc/self/maps\", O_RDONLY)       = 18\r\nfstat(18, {st_mode=S_IFREG|0444, st_size=0, ...}) = 0\r\nmmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x2ab462000000\r\nread(18, \"00400000-00634000 r-xp 00000000 \"..., 1024) = 1024\r\nread(18, \"001d000 09:01 1056305           \"..., 1024) = 1024\r\nread(18, \"0 r--p 001b9000 09:01 1583003   \"..., 1024) = 1024\r\nread(18, \" ---p 00041000 09:01 1056331    \"..., 1024) = 1024\r\nread(18, \"b4aa02000-3b4aa03000 rw-p 000020\"..., 1024) = 1024\r\nclose(18)                               = 0\r\nmunmap(0x2ab462000000, 4096)            = 0\r\nmmap(0x36be1200000, 4294967296, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 1103806595072, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 554050781184, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 279172874240, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 141733920768, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 73014444032, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 38654705664, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 21474836480, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 12884901888, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 8589934592, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 6442450944, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 5368709120, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 4831838208, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 4563402752, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 4429185024, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 4362076160, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 4328521728, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 4311744512, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 4303355904, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 4299161600, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 4297064448, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 4296015872, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 4295491584, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 4295229440, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 4295098368, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 4295032832, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 4295000064, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 4294983680, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 4294975488, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 4294971392, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 4294969344, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 4294968320, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 4294967808, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 4294967552, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 4294967424, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 4294967360, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 4294967328, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 4294967312, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 4294967304, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 4294967300, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 4294967298, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x36be1200000, 4294967297, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\n```\r\n\r\nHow comes TensorFlow (unsuccessfully) tries to allocate obscene amount of memory when instantiating a new session? It only happens when I run the script as a cluster GPU job, without an obscene amount of memory.\r\n\r\nThank you in advance for all your help.\r\n\r\n\r\n\r\n\r\n", "comments": ["Hm, is it the nvidia driver going crazy with memory allocations? Could try setting TF_CUDA_HOST_MEM_LIMIT_IN_MB to lower value\r\n\r\nAlso could try using alternative memory allocator for TF:\r\n```\r\nsudo apt-get install google-perftools\r\nexport LD_PRELOAD=\"/usr/lib/libtcmalloc.so.4\" \r\n\r\n```", "@yaroslavvb thank you, it seems like a sensible way to go.\r\n\r\nI've tried prepending a `export TF_CUDA_HOST_MEM_LIMIT_IN_MB=4096` to the command, but the problem persists.\r\n\r\nI've compiled and installed `google-perftools` in `$HOME`, and prepended a `export LD_PRELOAD=\"$HOME/lib/libtcmalloc.so.4\"` to the command, but the problem still persists.", "For the sake of completeness, here's my job details:\r\n\r\n```\r\n#!/bin/bash\r\n\r\n#$ -cwd\r\n#$ -S /bin/bash\r\n#$ -o [..]/tfbug/hello.out\r\n#$ -e [..]/tfbug/hello.err\r\n#$ -t 1-1\r\n#$ -l h_vmem=8G,tmem=8G\r\n#$ -l h_rt=24:00:00\r\n#$ -P gpu\r\n#$ -l gpu=1\r\n\r\nnvidia-smi\r\n\r\nexport TF_CUDA_HOST_MEM_LIMIT_IN_MB=512\r\nexport LD_PRELOAD=\"[..]/lib/libtcmalloc.so.4\"\r\npython3 [..]/tfbug/hello.py > [..]/tfbug/hello.log 2>&1\r\n```\r\n\r\nI've tried logging in on the node; the process is still trying to `mmap()` endlessly, and the process does not show up on `nvidia-smi`:\r\n\r\n```\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n```", "Here's another user who is saying TensorFlow tries to allocate 55GB of virtual memory for trivial computation which is a problem for Open Grid Scheduler  https://stackoverflow.com/q/46114462/419116\r\n\r\nThey are suggesting it's the CUDA driver trying to allocate this memory, maybe there's some other CUDA flag to disable this behavior.\r\n\r\nAs a sanity check, does any other CUDA framework (ie, PyTorch) have same problem on your cluster?", "I'm trying the following PyTorch snippet:\r\n\r\n```python\r\n#!/usr/bin/env python3\r\n# -*- coding: utf-8 -*-\r\n\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\nprint(torch.cuda.current_device())\r\n\r\na = Variable(torch.rand(1024, 32)).cuda()\r\nb = Variable(torch.rand(32, 1024)).cuda()\r\n\r\nc = torch.mm(a, b)\r\n\r\nprint(c.data.numpy().shape)\r\n```\r\n\r\nIt also gets stuck in a similar way:\r\n\r\n```\r\nopen(\"/proc/self/maps\", O_RDONLY)       = 18\r\nfstat(18, {st_mode=S_IFREG|0444, st_size=0, ...}) = 0\r\nmmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x2b24b6661000\r\nread(18, \"00400000-00634000 r-xp 00000000 \"..., 1024) = 1024\r\nread(18, \"001d000 09:01 1721791           \"..., 1024) = 1024\r\nread(18, \"               /lib64/libbz2.so.\"..., 1024) = 1024\r\nread(18, \"17                        /lib64\"..., 1024) = 1024\r\nread(18, \"         /usr/lib64/libssl.so.1.\"..., 1024) = 1024\r\nclose(18)                               = 0\r\nmunmap(0x2b24b6661000, 4096)            = 0\r\nmmap(0x2aca5000000, 4294967296, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 1103806595072, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 554050781184, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 279172874240, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 141733920768, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 73014444032, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 38654705664, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 21474836480, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 12884901888, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 8589934592, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 6442450944, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 5368709120, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 4831838208, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 4563402752, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 4429185024, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 4362076160, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 4328521728, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 4311744512, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 4303355904, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 4299161600, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 4297064448, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 4296015872, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 4295491584, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 4295229440, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 4295098368, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 4295032832, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 4295000064, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 4294983680, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 4294975488, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 4294971392, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 4294969344, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 4294968320, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 4294967808, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 4294967552, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 4294967424, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 4294967360, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 4294967328, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 4294967312, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 4294967304, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 4294967300, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 4294967298, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nmmap(0x2aca5000000, 4294967297, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\n```\r\n\r\nThen it's probably a CUDA issue rather than a TensorFlow issue.", "ok, I'll leave this issue open as community support since a work-around is needed to make TensorFlow usable in such environment", "Some background on why CUDA is doing this -- https://stackoverflow.com/questions/6445109/why-is-my-c-program-suddenly-using-30g-of-virtual-memory", "I'm observing the same problem on our SGE cluster.  Here is more info from my local experience.\r\n\r\n1. We have a variety of different GPUs in our cluster.  The job runs flawlessly in all GPUs except Tesla P40's.  This is true for K80 and GTX1080 GPUs, which have less memory.  This makes me conclude this is not a memory issue.\r\n2. If I qrsh login to a Tesla P40 node (interactive session), then the job works on that node.\r\n\r\nSo, my understanding is that the environment in which the job is executed triggers this problem.  If you have any other ideas, I can test it on our cluster."]}, {"number": 13016, "title": "Distributed variable initialization never reaches some nodes (affects MonitoredTrainingSession too)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Multiple affected in different ways, including Linux Ubuntu 16.04.03, Mac OS X 10.12.6, Windows 10 and Bash on Windows 10 running Ubuntu 16.04.03.\r\n- **TensorFlow installed from (source or binary)**: binary, followed the pip install instructions\r\n- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0\r\n- **Python version**: 3.5.2, 3.6.0\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: See instructions below.\r\n\r\n### Describe the problem\r\nIn a distributed environment with 2 nodes (a chief 'foo', a non-chief 'bar') variable initialization performed by foo never reaches bar in some particular distributed combinations. If using MonitoredTrainingSession, this leads to the session in bar never starting (it keeps trying forever every 30 seconds). Further research showed that no matter what I do (including restarting sessions, delays and anything I could think of) the session in 'bar' is unable to see the initialization of the variables that 'foo' confirms as initialized.\r\n\r\nThe source code below can be used to reproduce the problem depending on the hosts used for the 'foo' and 'bar' jobs. In particular, the following has been observed and reproduced multiple times:\r\n\r\n- When foo and bar are in the same host and OS, the problem never happens.\r\n- When ran foo in a Linux host and bar in a Mac OS X host the problem *always* happened.\r\n- However, if the roles are reversed (Linux runs bar, Mac OS X runs foo) the problem never happens.\r\n- Making foo_variables below an empty list also avoids the problem entirely.\r\n\r\nAdditionally I also tested this in another couple of platforms in the same host.\r\n- When ran foo in Windows 10 and bar in bash on Windows 10 running Ubuntu Linux 16.04, the problem *always* happened.\r\n- When reversing it and making Linux run foo and Windows 10 run bar the problem disappeared.\r\n- In this case both OS run in the same host and communicate through localhost sockets. Other tests suggest there's no problem in this socket communication.\r\n\r\n### Source code / logs\r\n```python\r\n#!/usr/bin/env python\r\n\r\nimport sys\r\nimport tensorflow as tf\r\n\r\nwith tf.variable_scope('foo'), tf.device('job:foo/task:0'):\r\n  foo_variables = [tf.get_variable('W', shape=(10, 5), dtype=tf.float32)]\r\n  foo_init_vars = tf.variables_initializer(foo_variables)\r\n  foo_pending_vars = tf.report_uninitialized_variables(foo_variables)\r\n  foo_pending_vars.mark_used()\r\n\r\n\r\nwith tf.variable_scope('bar'), tf.device('job:bar/task:0'):\r\n  # Expect more stuff and ops in bar in a real use case. This is just an example.\r\n  bar_pending_vars = tf.report_uninitialized_variables(foo_variables)\r\n  bar_pending_vars.mark_used()\r\n\r\n\r\ncluster_spec = tf.train.ClusterSpec({\r\n  \"foo\": [\"<insert_ip_here>:55700\"],\r\n  \"bar\": [\"<insert_ip_here>:55701\"],\r\n})\r\n\r\n\r\ndef foo_job():\r\n  server = tf.train.Server(cluster_spec, job_name='foo', task_index=0)\r\n  with tf.train.MonitoredTrainingSession(server.target, is_chief=True) as session:\r\n\r\n    # This always runs fine.\r\n    session.run(foo_init_vars)\r\n    print(\"Foo -- Variables not initialized: \", session.run(foo_pending_vars))\r\n    server.join()\r\n\r\n\r\ndef bar_job():\r\n  server = tf.train.Server(cluster_spec, job_name='bar', task_index=0)\r\n  with tf.train.MonitoredTrainingSession(server.target, is_chief=False) as session:\r\n    # On failure, this never gets executed...\r\n    print(\"**** Session started! ****\")\r\n\r\n    vars_left = session.run(bar_pending_vars)\r\n    if len(vars_left) == 0:\r\n      print(\"Bar -- Variables initialized!\")\r\n      return\r\n\r\n    print(\"Bar -- Variables not initialized: \", vars_left)\r\n    server.join()\r\n\r\n\r\nif __name__ == '__main__':\r\n  if len(sys.argv) < 2:\r\n    print(\"Usage: %s {foo|bar}\" % sys.argv[0])\r\n    exit(1)\r\n\r\n  if sys.argv[1] == 'foo':\r\n    foo_job()\r\n  else:\r\n    bar_job()\r\n```\r\n\r\n@mrry, this looks like something you might have some intuition about. Any ideas of what might be going on?\r\n\r\nThis is affecting my distributed system pretty badly. I'd be happy to do more experiments to help diagnosing the problem.", "comments": ["Any clues about this?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Catching up on old issues: This is mysterious, and there's nothing obviously wrong with the TF code, so my suspicion is that it's a network configuration issue. Can you try running some simpler code that starts a single-task `tf.train.Server` on each VM and attempt to connect to it using a `tf.Session(\"<insert_ip_here>:<port>\")` from each other VM?\r\n\r\nAlternatively, can you try setting the environment variable `GRPC_VERBOSITY=DEBUG` in your client program and let us know if there are any suspicious error messages?", "I've just tried again the source above in a recent TF build (from the 1.5.0rc0 branch) and I'm getting the following error when trying to run the foo task:\r\n\r\n> tensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef mentions attr 'T' not in Op<name=Where; signature=input:bool -> index:int64>; NodeDef: report_uninitialized_variables_1/boolean_mask/Where = Where[T=DT_BOOL, _device=\"/job:bar/replica:0/task:0/device:CPU:0\"](report_uninitialized_variables_1/boolean_mask/Reshape_1). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n\r\nI ignore if this might be related or not. I'll try with a 1.4.1 build just in case and report back.", "The error I mentioned immediately above does not happen with 1.4.1. It seems I accidentally caused it while testing when I used different versions of Tensorflow (1.5.0rc0 and 1.4.0) on different machines. This wasn't the case when I reported the bug, so please ignore that last comment.\r\n\r\nBack to the original problem, I'm still able to reproduce it in some configurations.\r\n- Fails when running foo in Windows 10 and bar in Linux, but for some reason it works if doing the reverse (bar in Windows 10, foo in Linux).\r\n- Fails when running foo in Windows 10 and bar in Bash on Windows 10 in the same host. Again, the reverse works.\r\n- Works both ways when using Linux and Bash on Windows 10 (which is a virtualized Linux).\r\n- Using Windows 10 on both sides (either localhost or 2 different hosts) works too.\r\n\r\nSo, this seems to suggest that the problem happens when running the task foo directly on Windows 10 and involving some other OS (even if it's in the same host). However,  although I couldn't test it this time, I see that my original bug report also mentioned the problem happened between Linux and Mac OS X servers. If this is still the case then the problem would not be Windows-specific, but actually triggered by running tf.train.Servers in different OS, at least in some foo/bar task placement configurations.\r\n\r\nThe fact that it works between 2 Windows 10 hosts or by reversing the roles, and that it can also fail in localhost across 2 OS in the same machine makes me doubt this is a networking problem. If anything, it would be some kind of low-level networking issue arising from differences between OS.\r\n\r\nI tried setting `GRPC_VERBOSITY=DEBUG` but I didn't see anything suspicious. Just in case, I forced some errors by putting IPs intentionally wrong, and I did get error logs in that case.\r\n\r\nI've also tried the following simpler code as suggested and it works fine. Both sessions are initialized without any problems.\r\n```python\r\ndef foo_job():\r\n  server = tf.train.Server(cluster_spec, job_name='foo', task_index=0)\r\n  with tf.Session('grpc://' + cluster_spec.as_dict()['bar'][0]) as session:\r\n    # Run something to force the use of the remote session.\r\n    with tf.device('job:bar/task:0'):\r\n      session.run(tf.no_op())\r\n    print(\"**** Session started! ****\")\r\n    server.join()\r\n\r\n\r\ndef bar_job():\r\n  server = tf.train.Server(cluster_spec, job_name='bar', task_index=0)\r\n  with tf.Session('grpc://' + cluster_spec.as_dict()['foo'][0]) as session:\r\n    # Run something to force the use of the remote session.\r\n    with tf.device('job:foo/task:0'):\r\n      session.run(tf.no_op())\r\n    print(\"**** Session started! ****\")\r\n    server.join()\r\n```\r\n\r\nFurthermore, if after a working session initialization I try to run the variable initialization, then the bar side never sees it.\r\n```python\r\ndef foo_job():\r\n  server = tf.train.Server(cluster_spec, job_name='foo', task_index=0)\r\n  with tf.Session('grpc://' + cluster_spec.as_dict()['bar'][0]) as session:\r\n    with tf.device('job:bar/task:0'):\r\n      session.run(tf.no_op())\r\n    print(\"**** Session started! ****\")\r\n\r\n    while True:\r\n      # This loops forever saying b'foo/W' is not initialized. Doesn't matter how long you wait.\r\n      # Meanwhile, the other job says all variables have been initialized.\r\n      vars_left = session.run(bar_pending_vars)\r\n      if len(vars_left) == 0:\r\n        break\r\n\r\n      print(\"Bar -- Variables not initialized: \", vars_left)\r\n      sleep(1)\r\n\r\n    print(\"Bar -- Variables initialized!\")\r\n    server.join()\r\n\r\ndef bar_job():\r\n  server = tf.train.Server(cluster_spec, job_name='bar', task_index=0)\r\n  with tf.Session('grpc://' + cluster_spec.as_dict()['foo'][0]) as session:\r\n    with tf.device('job:foo/task:0'):\r\n      session.run(tf.no_op())\r\n    print(\"**** Session started! ****\")\r\n\r\n    session.run(foo_init_vars)\r\n    print(\"Foo -- Variables not initialized: \", session.run(foo_pending_vars))\r\n\r\n    server.join()\r\n```\r\n\r\nNote that with that same code, if you swap in which hosts the foo and bar servers are started (foo goes to Linux instead of Windows) then the problem does not happen.\r\n\r\nSo, the issue is definitely still there. If you want me to try anything else please let me know. I'll be happy to help.\r\n  \r\n  ", "Just out of curiosity, is there anything that GRPC uses in its communication that is left to the OS to decide how to represent? Like string encoding format (UTF-8 vs UTF-16), byte ordering (although in this particular case all machines are little endian), or similar.", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "It's hard for us to debug on Windows.\r\n\r\n@guschmue does that ring a bell perhaps?", "Just tried this and think I see the same if foo is on windows and bar on linux. \r\nOdd, don't see a reason why this should not work. Let me debug this a little.", "Glad to see this can be reproduced. Let me know if there's anything else I should try from my side. I'll be happy to help.", "Any updates on this? Let me know if there's anything I should try and test."]}, {"number": 12968, "title": "Feature Request / Workaround for Variable size multi-label candidate sampling in TensorFlow.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: +\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS/Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: both\r\n- **TensorFlow version (use command below)**:  ('v1.3.0-rc2-20-g0787eee', '1.3.0')\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: n/a\r\n\r\n\r\n### Context:\r\nSuppose we have a dataset with an arbitrary amount of labels per each training example (image segmentation, multi-label classification, etc.). Labels (classes) are NOT mutually exclusive, thus vary in size between examples. \r\n\r\n### Problem:\r\nWhen trying to use provided standard `nce_loss()` -  a static `int` value for `num_true` option is required. \r\n- `num_true`: An `int`. The number of target classes per training example.\r\n\r\nThis probably works well for problems where we have same amount of labels per training examples and we know them in advance.\r\nWhen labels have a variable shape `[None]`, (and in our case, they are also being batched and bucketed by bucket size with `.padded_batch()` + `.group_by_window()`) it is necessary to supply a variable size `num_true` in order to accustom for all training examples. This is currently unsupported to my knowledge (correct me if I'm wrong).\r\n\r\n### Statement:\r\nIs there a proper way to do this or any other workarounds? If not I would like to request a feature. If yes, I would appreciate a working example here or on stackoverflow.\r\n\r\nRelated [stackoverflow question](https://stackoverflow.com/questions/46085454/variable-size-multi-label-candidate-sampling-in-tensorflow).\r\n\r\nDesired behaviour:\r\n```\r\nnce_loss = tf.nn.nce_loss(\r\n    weights=nce_weights,\r\n    biases=nce_biases,\r\n    labels=labels,\r\n    inputs=inputs,\r\n    num_sampled=num_sampled,\r\n    num_true=(tf.shape(labels)[-1]), # or tf.placeholder(\"int32\", [], name=\"num_trve\")\r\n    num_classes=self.num_classes)\r\n```\r\n\r\nAlso, is it possible to add support for weighted losses to `nce_loss()` (specifically to `_compute_sampled_logits()` as it is partially generated from a .cc) in the same fashion as it is implemented in `tf.losses.sparse_softmax_cross_entropy` or `tf.losses.sigmoid_cross_entropy`?\r\nThanks in advance.", "comments": ["@drpngx, do you know of any workarounds for @MtDersvan 's shape problem? ", "Sorry for the delay. What happens when you feed it an unknown shape `num_true`? Do you have a short example / stack trace you could put on pastebin? Thanks.", "@drpngx Sorry for the delay, too.\r\nHere is a simple short toy example: https://pastebin.com/bNH8F4K5\r\nHere is a stack trace: https://pastebin.com/H9fctCxt\r\nBasically there's a hardcoded `int(v)` in `_MakeInt` in `log_uniform_candidate_sampler` hard type restriction. And `_log_uniform_candidate_sampler` code is generated from a kernel which makes it harder to hack it on my own.\r\n\r\nIf I understand `num_true` correctly (maybe I'm wrong?), but it should be a common case where the number of labels differs per example. Maybe there is a different sampler that supports variable `num_true`?", "I would also appreciate if someone can suggest the best way to add support for weighted losses to `nce_loss` (or maybe to `_compute_sampled_logits`?) in the same fashion as it is implemented in `tf.losses.sparse_softmax_cross_entropy` or `tf.losses.sigmoid_cross_entropy`? Because, multi-label tasks often have imbalanced datasets.", "Sorry, but is there any progress here?", "@MtDersvan : `num_true` is an [attr](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/candidate_sampling_ops.cc#L100), you can't pass a tensor to it. So the right way is to have it passed as an input. Does that make sense?", "@drpngx \r\nI fully understand that right now you can't pass a tensor.\r\n\r\nWhat do you mean as an input?\r\nIf you mean just a const number, than sampling losses have a very narrow application.\r\nIf I understand correctly, in TF I can either pass a tensor or a numeric value (which is a const in itself in the context of a static graph).\r\n\r\nDocumentation:\r\n`num_true: Number of true labels per context.` \r\nand\r\n`num_true: An int. The number of target classes per training example.`\r\n\r\nAlso, If I understand correctly it is the amount of true labels per each example, isn't it? \r\nE.g. if a picture has ['dog', 'car', 'panda'] true labels, while there are 1000 possible classes, than `num_true=3` and `num_classes=1000`.\r\n\r\nIf yes, then what exactly needs to be done for it to support a variable size`num_true`?\r\nE.g. if I have images with different amount of `num_true` per example.", "Oh, I see you updated docs recently. \r\n```\r\nNote: It would be useful to allow a variable number of target classes per example. We hope to \r\nprovide this functionality in a future release. For now, if you have a variable number of target \r\nclasses, you can pad them out to a constant number by either repeating them or by padding with \r\nan otherwise unused class.\r\n```\r\nWon't such padding interfere the results? Because of this:\r\n```\r\nNote: In the case where num_true > 1, we assign to each target class the target probability 1 / num_true so that the target probabilities sum to 1 per-example.\r\n```\r\nDo proportions/distribution matter?\r\n\r\nIt looks like you are already working on it, still if you need help implementing this feature, I would be willing to contribute.", "Yes, the normalizer has to know about the padding. Also, changing `num_true` to an input tensor will change the order in the parameters, so it's an interface change.\r\n\r\nProbably the best is to create a new copy of the operator in `contrib`. I think that would be a fine contribution!\r\n\r\n(To use a tensor parameter, simply change the `.attr` to `.input` and then change the kernel to get the input from the context each time `Compute` is requested.)\r\n", "@drpngx I think the order of parameters doesn't need to be changed, `num_true` is always the first `.attr`, so switching it to an `.input` preserves the order. I have a simple working fix that leaves almost everything intact [here](https://github.com/MtDersvan/tensorflow/tree/r1.4) on `r1.4` branch. If you're interested I can submit a PR. \r\nI did direct changes to `core/ops` and `core/kernels` instead of a `contrib` clone, due to the fact that it was too much of a hassle to replicate all candidate_samplers and range_samplers.\r\nRight now ops take a single `tensor` instead of an `int` per batch.\r\n\r\nFuture Work:\r\nI haven't figured out yet how to make it to work with a list of tensors of shape `batch_size`, so each example in a batch can have it's own `num_true`. Because rn it needs all items in a batch to have the same number of `num_true`, which is computationally inefficient. Also, I left out one check in `ComputeAccidentalHits` - `.SetShapeFn()`.", "Oh good, yes feel free to send the PR!\n\nOn Nov 28, 2017 12:51 PM, \"MtDersvan\" <notifications@github.com> wrote:\n\n@drpngx <https://github.com/drpngx> I think the order of parameters doesn't\nneed to be changed, num_true is always the first .attr, so switching it to\nan .input preserves the order. I have a simple working fix that leaves\nalmost everything intact here\n<https://github.com/MtDersvan/tensorflow/tree/r1.4> onr1.4 branch. If\nyou're interested I can submit a PR.\nI did direct changes to core/ops and core/kernels instead of a contrib\nclone, due to the fact that it was too much of a hassle to replicate all\ncandidate_samplers and range_samplers.\nRight now ops take a single tensor instead of an int per batch.\n\nFuture Work:\nI haven't figured out yet how to make it to work with a list of tensors of\nshape batch_size, so each example in a batch can have it's own num_true.\nBecause rn it needs all items in a batch to have the same number of num_true,\nwhich is inefficient computationally. Also, I left out one check in\nComputeAccidentalHits - .SetShapeFn().\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\n<https://github.com/tensorflow/tensorflow/issues/12968#issuecomment-347412520>,\nor mute the thread\n<https://github.com/notifications/unsubscribe-auth/AT_SbcnvdiUCf0BnrXRhgvPhdDCdEm-bks5s65E_gaJpZM4PTY71>\n.\n"]}, {"number": 12964, "title": "Feature request: Google Authentication support for OAuth2 AccessTokenCredentials", "body": "\r\nI would like to use the BigQueryReader to access  someone else's project with OAuth2 AccessTokenCredentials. I noticed that currently the google_auth_provider only uses application default credentials. I think it would be a great addition to add other methods of authentication as well.", "comments": ["I am trying to work on this and send an update later"]}, {"number": 12901, "title": "get_session_handle has no effect if not directly fetched", "body": "Version `v1.3.0-rc1-1612-ga2e1a5e`, recent master.\r\n\r\n```python\r\nhandle = tf.identity(tf.get_session_handle(tf.constant(0))).eval()\r\ngen_data_flow_ops._get_session_tensor(handle, tf.int32).eval()\r\n# InvalidArgumentError: The tensor with handle 'GetSessionHandle;0;/job:localhost/replica:0/task:0/device:GPU:0' is not in the session store.\r\n```\r\n\r\n```python\r\nhandle = tf.get_session_handle(tf.constant(0)).eval().handle\r\ngen_data_flow_ops._get_session_tensor(handle, tf.int32).eval()\r\n# OK\r\n```\r\n\r\n", "comments": ["Sorry, `get_session_handle` seems to be experimental. Have you tried the `_v2` version?", "There is `GetSessionHandleV2` but no `GetSessionTensorV2`. \r\n\r\n```python\r\nv = tf.Variable(gen_data_flow_ops._get_session_handle_v2(tf.constant(0)))\r\nv.initializer.run()\r\ngen_data_flow_ops._get_session_tensor(v, tf.int32).eval()\r\n# ValueError: Incompatible type conversion requested to type 'string' for variable of type 'resource_ref'\r\n```", "@gaohuazuo this is by (suboptimal) design.\r\n\r\nget_session_tensor requires a TensorHandle object in order to figure to pass some extra information (device assignment).\r\n\r\nThis object is obtained when you issue session.run on get_session_tensor op. It comes from special logic in client/session.py which is only triggered for get_session_handle ops. In your failing case, you are evaluating a different op (identity), which gives you a regular numpy array rather than TensorHandle\r\n\r\nSee https://github.com/tensorflow/tensorflow/issues/6131", "@yaroslavvb Looks like python wrapper is irrelevant.\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python import pywrap_tensorflow\r\nfrom tensorflow.python.ops import gen_data_flow_ops\r\n\r\nconfig = tf.ConfigProto(graph_options=tf.GraphOptions(optimizer_options=tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)))\r\nprint(config)\r\n# graph_options {\r\n#   optimizer_options {\r\n#     opt_level: L0\r\n#   }\r\n# }\r\n\r\nsess = tf.InteractiveSession(config=config)\r\ndef run(x):\r\n    sess._extend_graph()\r\n    with tf.errors.raise_exception_on_not_ok_status() as status:\r\n        result, = pywrap_tensorflow.TF_Run(\r\n            sess._session, None, {}, [x.name.encode()], [], status, None)\r\n    return result\r\n\r\nhandle_tensor = gen_data_flow_ops._get_session_handle(tf.constant(0))\r\nindirect_handle_tensor = tf.identity(handle_tensor)\r\nhandle_ph = tf.placeholder(tf.string, [])\r\nsession_tensor = gen_data_flow_ops._get_session_tensor(handle_ph, tf.int32)\r\n\r\nhandle = run(handle_tensor)\r\nindirect_handle = run(indirect_handle_tensor)\r\n\r\nsession_tensor.eval(feed_dict={handle_ph: handle}) # OK\r\nsession_tensor.eval(feed_dict={handle_ph: indirect_handle}) # InvalidArgumentError\r\n```", "You are right, this seems to be a deeper runtime issue. It looks as if the tensor is added to session store when get_session_handle is fetched directly, and not added when it's executed as a dependency of a fetch\r\n\r\nThe relevant logic is in this commit\r\nhttps://github.com/tensorflow/tensorflow/commit/098f930de4ef044021f3ef1d3cdd6848c23eddb0\r\n\r\nNote that the author of this functionality has left Google"]}, {"number": 12851, "title": "Dataset Unzip Operation", "body": "@mrry This is a comment I originally posted to the \"Redesigning input pipelines\" issue, but I think it went unnoticed given the amount of comments on that thread. Given that issue is now closed (for good reason :)), I decided to post it as a separate issue.\r\n\r\nI currently cannot see a way currently to \"unzip\" a dataset. Let's say we have a trainable model that has both a train/fit method and a infer/predict method. Let's call the type of the (potentially) nested structure of inputs to our model `I` and the type of training inputs, which are only needed when training (e.g., supervision labels), `TI`. In this case, we want the train method to accept datasets with elements of type `(I, TI)` (i.e., a tuple of `I` and `TI`) and the predict method to accept datasets with elements of type `I` or `(I, TI)` (in which case it would ignore the labels). We also want the model to only have one underlying graph, supporting all these types of input. The way I could see doing that was for the underlying model to construct two iterators (one with elements type `I` and one with type `TI`) and initialize them according to the provided datasets. However, if somebody provides a dataset with elements of type `(I, TI)` to the train method, there is no way to unzip this dataset and initialize both iterators. One has to use `Dataset.map` twice, which is not efficient (I think but please correct me if I'm wrong) and which may also not pull matching elements from the datasets (if each pull advances the current index in the original first dataset -- I'm not sure if that happens).", "comments": ["Thanks for reposting this @eaplatanios! I'd like to understand the use case more, so we can focus on the appropriate efficient solution. Just to check my understanding, are you proposing a transformation with the following type? (Please forgive the abuse of Haskell notation below :)....)\r\n\r\n```haskell\r\nunzip :: Dataset (I, TI) -> (Dataset I, Dataset TI)\r\n```\r\n\r\nIf that's the case, you could do that by mapping twice over the the input, with the first map selecting the first component of the tuple, and the second map selecting the second component of the tuple. This would work\u2014`Dataset` objects are stateless, and the state is in the `Iterator`\u2014but you're right that it would be inefficient because (at least currently) the upstream work of producing the input (reading from files, parsing, etc.) would be duplicated.\r\n\r\nHow could we avoid that duplication? Today, the easiest way to do that would be by adding a `Dataset.cache()` before the `unzip`, so that the elements are computed once and subsequently read from the cache (either in memory or on disk). However, this only works with finite `Dataset` objects, which is not as featureful as a true `unzip`! Plus there's not (necessarily) any need to cache the entire dataset, depending on how you are planning to consume the iterators.\r\n\r\nI'm a little unclear about why you'd want *two* iterators as input to the graph in your example. I'd presume that training and test data come from two different sources, so it wouldn't be a problem to create different iterators for those cases, and these could have the appropriate type `(I, TI)` or `(I)` as necessary. However, if you have a training graph and an input dataset of type `(I, TI)`, why would you prefer to unzip the datasets and make two iterators, rather than making one iterator and destructuring the elements it yields? The only reason I can think of\u2014but I haven't thought too hard about this\u2014is that you'd like to advance the iterators at different rates, but this creates a buffering issue: how many elements of type `TI` should be buffered while we're consuming the iterator of type `I` (or vice versa) to avoid recomputation?\r\n\r\nHowever, just because I can't think of an application doesn't mean that there isn't one! I'd be interested to learn more about your feature request....\r\n\r\n", "@mrry Thanks for the fast and elaborate response! :)\r\n\r\nFirst of all, let me provide a simplified example of the use case I have in mind. I want to create a functional API for constructing and training networks. I'm thinking of having a notion of `Layer[T, R]`, which maps from input type `T` to output type `R`. A layer is a function that given a `T` will construct relevant ops and return an `R`. These types could be tuples of op outputs, or other arbitrary structures for that matter. Layers can be combined in various ways (e.g., composed or mapped) to create new layers. A model is then defined using:\r\n- Input layer that returns type `I`\r\n- Inference layer that goes from `I` to `P` (think of this as the main model -- the prediction function -- e.g., a multi-layer perceptron)\r\n- Train input layer that returns type `IT` (optional -- e.g., labels used when training a classifier)\r\n- Train processing layer that goes from `IT` to `PT` (optional -- e.g., one-hot encoding of labels)\r\n- Loss layer that goes from `(P, PT)` to a scalar tensor (optional -- e.g., cross-entropy)\r\n- ...\r\nThe optional components are only used when training so if you create a model that requires no training, you can skip them. The model defines `train` and `predict` methods that accept data (train accepts a tuple of `(I, IT)`, but predict can accept either such a tuple, or simply data of type `I`).\r\n\r\nLayers do not need to construct any ops before being \"applied\" to some input. Before creating a model, no interaction with TensorFlow is necessary and no ops are created. The TF graph is created, populated, and managed by the model. So, while initializing, the model will create a graph and use it for all its operations. I want the model to use the same graph for both training and prediction and I want to avoid adding redundant ops to that graph as much as possible. I think it's more elegant to limit graph management to the model so the user never has to deal with that (I've noticed it often confuses many people using TF).\r\n\r\n**This point is important to our discussion:** Since I want to have a fixed graph, I need the inputs of that graph to be fixed at construction time and I want to use the dataset API iterators for that to allow flexibility with respect to how users provide data for training and/or prediction. Given this constraint, at graph construction time I need to either use two iterators (one for type `I` and one for `IT`), or an iterator over tuples `(I, IT)`. _If I use an iterator over tuples (which makes sense given that we want to advance the iterators at the same time), then how do I initialize that iterator with a dataset of type `I` for the model predict method?_\r\n\r\nI hope that this simplified description of what I'm hoping to achieve makes sense.\r\n\r\nWith respect to your points, I have a couple of comments/questions:\r\n- What you mention about the unzip operation and the semantics of traversing the unzipped iterators makes sense and I hadn't really thought about it. I always had the synchronized advancement of both in my mind. In this case, not unzipping them also makes sense as it incorporates that constraint in the structure.\r\n- In general, I'm a bit unclear as to when the iterators are advanced. The simple use cases makes sense, but what about the following example: we have a model that has multiple entry points for a single input (i.e., multiple ops consuming the same input). So, during a single `Session.run` call, multiple ops will consume the same input provided by a single iterator. Will the iterator only advance once per `Session.run` call, or will it advance once for every single op that consumes its output? If the latter happens, then how can we enforce the once per `Session.run` call behavior?\r\n\r\nThank you for your response and I'm sorry for the super long post. I just want to try and make the setting clear so the use case may make sense. I do realize now though that unzipping may not be the right approach and I might be missing some simple solution to my example. :)", "I guess another not-so-pretty solution would be to have an iterator over tuples `(I, IT)` in my model, always, and then, if a user provides a dataset of `I`, I map that to one of `(I, TI)`, with dummy values for the `TI` elements. Although, I feel that this would be inefficient and also a bit ugly.", "@mrry  i also need `unzip`, cause of some transormations which expect dataset, containing only single component (i.e. tf.contrib.data.dense_to_sparse_batch).", "Confirming this; `unzip` would be very useful.\r\n\r\nIn my application, I have to read data from JSON files and I extract two (or more) pieces of information for them, so I have a Dataset which yields (tf.string, tf.int32, tf.int32), for example. I then have some tensorflow code that transforms the tf.string into a tf.float32 (loading files, doing some compute), but leaves the two tf.int32's alone. Right now, I have to write a function like this:\r\n\r\n```python\r\ndef transformer(a, b, c):\r\n    a_new = fix_my_value(a)\r\n    return a_new, b, c\r\n```\r\nBut I would rather be able to unzip a dataset, apply `fix_my_value` directly, and then zip it back up.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "what is the status of this issue? I also need an unzip operation!", "Nagging Assignee @mrry: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @mrry: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @mrry: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @mrry: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @mrry: It has been 74 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi folks, I haven't had the time to study this problem, but it looks like several people would find it useful, so I'm going to throw it open to contributions.", "cache() doesn't seem to prevent repeated work in my simple test below:\r\n\r\n```\r\ndef one_to_two(x):\r\n  def work(x):\r\n    print('work', x)\r\n    return np.ones([2], dtype=np.int32) * x, np.ones([3], dtype=np.int32) * -x\r\n  \r\n  pos, neg = tf.py_func(work, [x], [np.int32, np.int32], stateful=False)\r\n  return pos, neg\r\n\r\nwith tf.Graph().as_default():\r\n  d0 = tf.data.Dataset.range(12).map(one_to_two).cache()\r\n  d1 = d0.map(lambda *x: x[0])\r\n  d2 = d0.map(lambda *x: x[1])\r\n\r\n  v1 = d1.make_one_shot_iterator().get_next()\r\n  v2 = d2.make_one_shot_iterator().get_next()\r\n\r\n  with tf.Session() as sess:\r\n    # \"work 0\" printed twice.\r\n    print(sess.run([v1, v2]))\r\n```", "Manually unzipping doesn't seem to respect the order of the tuple elements when shuffling the zipped dataset. Take a look at my minimum working example below:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nds = tf.data.Dataset.range(100)\r\nds = tf.data.Dataset.zip((ds, ds))\r\nds = ds.shuffle(100)\r\n\r\nx0 = ds.map(lambda *x: x[0])\r\nx1 = ds.map(lambda *x: x[1])\r\n\r\nds = tf.data.Dataset.zip((x0, x1))\r\n\r\nit = ds.make_one_shot_iterator()\r\ntup = it.get_next()\r\n\r\nwith tf.Session() as sess:\r\n    t0, t1 = sess.run(tup)\r\n    print((t0, t1))\r\n    \r\n>> (8, 94)\r\n```\r\n\r\nI would have expected t0 == t1, since the shuffle was only performed on the zipped dataset. \r\n\r\nMy use case is that I have input filenames and label filenames, and I want to zip them, shuffle, and unzip while maintaining the pairing between input and label. I want to unzip since I have a complicated input pipeline which processes inputs and labels separately. Anyone have a suggestion for how to do this sort of shuffle? Seems like it should be an extremely common use case.\r\n\r\nEdit1: I should probably just make a new issue, but I'm a bit astonished by the following simple example as well. Is this really the expected behavior? This seems a bit unintuitive.\r\n\r\nEdit2: Passing an explicit nonzero seed to the shuffle solves the problem. But that defeats the entire purpose of my shuffle, which is to randomize the order every epoch. I suppose using the epoch itself as the seed could work, but seems like a very annoying work-around for such a simple problem.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nds = tf.data.Dataset.range(100).shuffle(100)\r\nds = tf.data.Dataset.zip((ds, ds))\r\ntup = ds.make_one_shot_iterator().get_next()\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(tup))\r\n\r\n>> (42, 28)\r\n```", "I agree, I find the behavior of `zip(ds, ds)` above unintuitive. The `dataset` framework seems to behavior less like a dataflow graph with mutable state and more like a pure function composition framework, unlike the [core tensorflow framework](http://delivery.acm.org/10.1145/3090000/3088527/pldiws17mapl-maplmainid2.pdf?ip=131.107.174.21&id=3088527&acc=OA&key=4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E5945DC2EABF3343C&__acm__=1539194417_890fe79cd2079ceacf4d1229c27b4fee  ). Note that @mrry is a co-author on the linked paper, so perhaps has a more enlightening perspective on this.", "> Manually unzipping doesn't seem to respect the order of the tuple elements when shuffling the zipped dataset. Take a look at my minimum working example below:\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> \r\n> ds = tf.data.Dataset.range(100)\r\n> ds = tf.data.Dataset.zip((ds, ds))\r\n> ds = ds.shuffle(100)\r\n> \r\n> x0 = ds.map(lambda *x: x[0])\r\n> x1 = ds.map(lambda *x: x[1])\r\n> \r\n> ds = tf.data.Dataset.zip((x0, x1))\r\n> \r\n> it = ds.make_one_shot_iterator()\r\n> tup = it.get_next()\r\n> \r\n> with tf.Session() as sess:\r\n>     t0, t1 = sess.run(tup)\r\n>     print((t0, t1))\r\n>     \r\n> >> (8, 94)\r\n> ```\r\n> \r\n> I would have expected t0 == t1, since the shuffle was only performed on the zipped dataset.\r\n> \r\n> My use case is that I have input filenames and label filenames, and I want to zip them, shuffle, and unzip while maintaining the pairing between input and label. I want to unzip since I have a complicated input pipeline which processes inputs and labels separately. Anyone have a suggestion for how to do this sort of shuffle? Seems like it should be an extremely common use case.\r\n> \r\n> Edit1: I should probably just make a new issue, but I'm a bit astonished by the following simple example as well. Is this really the expected behavior? This seems a bit unintuitive.\r\n> \r\n> Edit2: Passing an explicit nonzero seed to the shuffle solves the problem. But that defeats the entire purpose of my shuffle, which is to randomize the order every epoch. I suppose using the epoch itself as the seed could work, but seems like a very annoying work-around for such a simple problem.\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> \r\n> ds = tf.data.Dataset.range(100).shuffle(100)\r\n> ds = tf.data.Dataset.zip((ds, ds))\r\n> tup = ds.make_one_shot_iterator().get_next()\r\n> \r\n> with tf.Session() as sess:\r\n>     print(sess.run(tup))\r\n> \r\n> >> (42, 28)\r\n> ```\r\n\r\nHi,\r\nHave you found a way to shuffle a zipped dataset while still preserving the pairing ?\r\nThanks in advance ", "Hi guys, is there any progress on this issue? I also need an unzip operation!", "Any update?", "I just lost a day of work because of this :O", "I stumbled on this discussion while searching for a built-in unzip operator, so I wanted to share my solution (actually two variants, can't decide which one is bettter). It works for shuffled datasets too, since it only iterates through the original dataset once.\r\n\r\nYou can use the Python unzip function as follows, which even works with batched datasets:\r\n```feature_ds, label_ds = [tf.data.Dataset.from_tensor_slices(tf.concat(x, axis=0)) for x in zip(*ds)]```\r\n\r\nAlternatively, for non-batched datasets you don't need the `tf.concat` (or you can `unbatch()` your `Dataset` first as below):\r\n```feature_ds, label_ds = [tf.data.Dataset.from_tensor_slices(list(x)) for x in zip(*ds.unbatch())]```\r\n\r\nThe only drawback I can see is that both options need to load the entire `Dataset` into memory, so this won't work for very large datasets. That's why I still would like to see a native operator, to handle distributed `Datasets`.\r\n\r\nMy usecase is to split features from labels to create a confusion matrix, which I'd say is pretty common. Or am I missing a simpler solution for this?", "We can use same seed to control the order of feature and label after shuffle. In this case we don't have to zip two datasets.\r\n\r\n```\r\nfeature_ds.shuffle(size, seed=0).repeat()\r\nlabel_ds.shuffle(size, seed=0).repeat()\r\n``` "]}, {"number": 12840, "title": "User-defined functions loaders", "body": "I noticed user-defined functions are still [experimental](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/graph.proto#L27). I have an idea about functions loaders that could allow us to use dynamically loaded functions from different sources.\r\n\r\nFormat of function address would be [multiaddr](https://github.com/multiformats/multiaddr#string-format)-like.\r\n\r\n```\r\n/<loader>/<namespace>/<function>\r\n```\r\n\r\nLoaders can be embed into tensorflow like kernels.\r\n\r\n```\r\n/tf/custom\r\n/core/add\r\n/ipfs/QmVv4Wz46JaZJeH5PMV4LGbRi3MKEmszPYY3g6fjGnVXBS\r\n```", "comments": ["I'm not quite seeing/understanding the complete proposal / feature request you're making. Can you explain how you'd use the different sources? ", "Main feature would be a dynamic functions loading from remote sources (eq. `/google/` functions).\r\n\r\nFor example: compiled kernels can be loaded straight from `/ipfs/<hash>` address, this would require an `ipfs` function loader.\r\n\r\nThe idea is mainly to provide tools to develop new kinds of neurons.", "@josh11b it's not clear to me that this is consistent with where we're going with user-defined functions. Can you comment, or refer someone else, please?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Yeah, I think it's a great idea. We'd need to have a good design doc. This also ties in with more ideas that we have internally to make tensorflow have modular packages. CC @gunan @zffchen78 "]}, {"number": 12761, "title": "Allow to build tensorflow as a bazel external dependency.", "body": "It would be nice to be able to just add a @org_tensorflow in bazel and be able to build it as a dependency.  \r\n\r\nAt the best of my knowledge the syntaxnet dockerfile is the best example on having tensorflow as a submodule.\r\nhttps://github.com/tensorflow/models/blob/c259259299db3b486ccdfd6cdec44b884623053a/syntaxnet/Dockerfile#L63\r\n\r\nBut still building it is not straightforward and probably does not work if you activate CUDA/XLA/MLK because those depends on other bazel subprojects.\r\n\r\nAre there any plans to support this use case or I am missing something in the best practices to build tensorflow as a subpackage ? \r\n", "comments": ["With the latest bazel releases, it should be possible to build TensorFlow as a subproject quite easily. Now you can avoid needing to run `configure` script with having a `bazelrc` file with the necessary options in your primary project. An example project that uses TensorFlow as a subproject you can look into `tensorflow/serving`. \r\nI know that there are still some kinks that need to be straightened out. We can discuss particular problems that immediately block you and try to work around them or apply necessary fixes to TensorFlow.\r\n\r\nCould you specify any blocking issues you have?", "I tried to create a simple Python library that depends on the `./tensorflow` workspace configured like in #12735,  and then I try to create a pip package that contains everything embedded (tensorflow + your library). \r\nA simple rule like this will fail with the advanced options enabled. \r\n```python\r\npy_library(\r\nname = 'mylibrary',\r\ndeps = [\r\n    \"@org_tensorflow//tensorflow/tensorflow_py\"\r\n])\r\n```\r\nThis is useful for training on Google ML Engine because you can have a single bazel target that packages everything and runs the job from a single project with the exact binary configuration. Otherwise, you have to do many steps separately (package Tensorflow, then package your library) and then specify it on the google cloud ml job library target both zips.  \r\n\r\nMaybe I am doing something wrong. A small GitHub repo showing how is done using the extra configurations (XLA, MLK, CUDA) parameters would be useful as a reference. \r\n", "The tensorflow/serving project still says we need to run ./configure to be able to build tensorflow: https://github.com/tensorflow/serving/blob/master/tensorflow_serving/g3doc/setup.md\r\n\r\nIs there any example of anyone using tensorflow with bazel without running ./configure? We just tried and it fails because it can't find these files: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/git/BUILD#L10", "Any updates on this?"]}, {"number": 12701, "title": "Feature Request: callback argument for tf.contrib.data.Dataset.ignore_errors() to enable error logging", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: 8/6\r\n- **GPU model and memory**: GTX 1080, 8GB\r\n- **Exact command to reproduce**: -\r\n\r\n### Describe the problem\r\nThe [`tf.contrib.data.Dataset.ignore_errors()`](https://www.tensorflow.org/versions/r1.3/api_docs/python/tf/contrib/data/Dataset#ignore_errors) function is extremely useful when processing data that has not been fully cleaned beforehand (e.g., with streaming input), but I find the \"silently ignoring all errors\" a bit too strict.\r\nIt would be very useful, for example, to know which files raise an error when processing the dataset, while keeping the exception-masking feature of the function.\r\n\r\nIMO, the most flexible way to obtain this would be to include an optional `callback` argument to the function that gets called passing as arguments the value in the dataset that raised an error.\r\nThis way custom logging can be done for each erroneous data sample and dataset inspection becomes much simpler.\r\n\r\nWould this be very hard to implement?\r\n", "comments": ["@mrry Feature request for tf.contrib.data.", "It's certainly possible to add richer error handling, though it might end up not being ideal to add a callback to `Dataset.ignore_errors()` because at that point in the input pipeline you only have an error message (or the successfully transformed data), and it's not necessarily obvious what input element caused the error.\r\n\r\nCan you sketch out in more detail your ideal interface for a method like this?", "Well, I don't know all possible use-cases for this function, so I'll just describe mine.\r\nI'm loading images from a set of folders, doing inference on every image, recording the prediction result and the filename.\r\nMy pipeline looks like this:\r\n```\r\n  def preprocess_image(fn):\r\n    im_s = tf.read_file(fn)\r\n    im = tf.image.decode_jpeg(im_s, channels=3)\r\n    im = inception_preprocessing.preprocess_for_eval(im, width=299, height=299)\r\n    return (fn, im)\r\n\r\n  dataset = tf.contrib.data.Dataset.list_files('{}/*/*/*.jpg'.format(FLAGS.dataset_dir))\r\n  dataset = dataset.map(preprocess_image, num_threads=FLAGS.num_threads, output_buffer_size=3*FLAGS.batch_size)\r\n  dataset = dataset.ignore_errors()\r\n  iterator = dataset.make_one_shot_iterator()\r\n```\r\n\r\nMy dataset contains a small number of corrupted images that raise errors when processed. As of now, the `ignore_errors()` function does what it promises, but I'd like to have a way to find out which files are the ones raising the error, log them and later remove them from the dataset.\r\n\r\nThis is why I thought of the callback. The dataset ideally would give me as much information as possible about the specific sample that raised the error. At the very least, the exception details, but possibly also the data sample available up to the point in the pipeline where the error is raised.\r\nIn my use-case, it's the `dataset.map()` call that internally raises errors when decoding images. It would be great to have, on error, a way to log/inspect/process the sample taken from the pipeline stage after the `list_files` call that, once processed in `map()`, raises the error.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Catching up on old issues: I agree that it would be useful to log more information on an error, without crashing the pipeline. I'm marking this as \"contributions welcome\" in the hope that somebody has ideas in this space.\r\n\r\nA couple of notes: \r\n\r\n* We now have room for unsupported `Dataset` transformations using `Dataset.apply()` and custom transformation functions in the `tf.contrib.data` module\u2014indeed, the `tf.contrib.data.Dataset.ignore_errors()` method has been deprecated in favor of `tf.contrib.data.ignore_errors()`\u2014so it would be easier to experiment with different APIs, if somebody wants to contribute one.\r\n* Similarly, there is some experimental code for aggregating metadata in [`tensorflow/contrib/data/python/ops/stats_ops.py`](https://github.com/tensorflow/tensorflow/blob/136697ecdc64b5171522fb7f89cfe51a02f0f1c1/tensorflow/contrib/data/python/ops/stats_ops.py) and it might be appropriate to log error information there.", "On a related topic, can we please add the ability to skip `tf.errors.OutOfRangeError` from being ignored? At the moment it seems to be stalled at the end of a dataset when `ignore_errors` is applied.", "@sjain-stanford `ignore_errors()` shouldn't be ignoring an `OutOfRangeError`, so that sounds like a bug. Can you please open a new issue with some code that reproduces the problem? Thanks!", "@mrry On a closer look, it may not have to do with `OutofRangeError` but execution seems to be stalled indefinitely when it encounters a corrupt tfrecord, despite using `tf.data.experimental.ignore_errors()`. I've opened #25700 with the minimal code to reproduce this issue. Thanks for taking a look."]}, {"number": 12596, "title": "Feature request: QueueRunner for C++ API that initializes from queues and ops and start with ClientSession", "body": "Right now there is an implementation of QueueRunner in \"tensorflow/cc/training/queue_runner.h\". It's created from a QueueRunnerDef that is initialized with string names rather than queues and ops objects. QueueRunner's current implementation is also started by using the \"Session\" low level class rather than the newer \"ClientSession\" class.", "comments": ["@reedwm @yuefengz seems like there is a plan to change the implementation of QueueRunner from queues and ops. Could you take a look at this?", "Unassigning myself since I don't know anything about queue runners. @yuefengz, can you take a look, or unassign yourself and leave this as contributions welcome?", "The QueueRunner is deprecated. It possible for you to use `tf.data` for input processing?", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you."]}, {"number": 12570, "title": "No gradient for `cdf`, `sample` and other functions for several distributions in `tf.distributions`", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary (pip)\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: CUDA 8.0 cuDNN 6\r\n- **GPU model and memory**: nVIDIA K2100M, 2G\r\n- **Exact command to reproduce**: See below\r\ndist_par = tf.Variable(1.0)\r\ndist = tf.distributions.Beta(dist_par,1.0)\r\nprint(tf.gradients(dist.cdf(0.5), dist_par))\r\n\r\n>> [None]\r\n\r\nThe output says there is no gradient.\r\n\r\n### Describe the problem\r\nFor several distributions such as `Beta` and `Gamma`, there is no gradient of their functions such as `cdf`, `log_cdf`, `sample` with respect to the parameters of these distributions. While gradients are provided for these functions of distributions such as `Normal` and `Laplace`.\r\nI think theoretically the gradients should exist. And they are useful when people build a model in which samples drawn from these distributions as prior distributions are marginalized while the parameters of these distributions are optimized. It would be nice if they can be implemented\r\nThanks!\r\n\r\n### Source code / logs\r\nSee above\r\n", "comments": ["We currently don't have anybody working on this. It would be great if you could help us by working on this and submitting a PR. Let us know if you need further clarification. Thanks!", "I can work on this. It looks like you just need to add the gradient calculations here. https://github.com/tensorflow/tensorflow/blob/084d29e67a72e369958c18ae6abfe2752fcddcbf/tensorflow/python/ops/math_grad.py#L536 \r\n\r\nI assume the issue is similar the reset of the distributions calculations. There's actually a todo in the code for this. ", "@Thenerdstation Thanks a lot for help! I am not too familiar with the internal tensorflow code. A few other distributions seem to be marked TODO for their gradients as well.", "Yeah I see that. I might have to make a few PRs to fix them all.", "Well I see now why this gradient was put as a TODO.\r\nhttps://www.wolframalpha.com/input/?i=derivative+with+respect+to++a+of+beta(x,+a,+b)", "Wow, that is much more complex than I thought", "Well, `sample` should be dealt with separately really, but for `log_cdf` please see 2.3 in:\r\nhttps://arxiv.org/pdf/1509.01631.pdf\r\nThis should hopefully extend to non-log versions and to the Beta distribution.", "Hey thanks for this!\r\nSeeing how there is a lot of looping for the approximation, we might have to create a new op for this. Not sure on timeline but I'll keep at it.", "@Thenerdstation, do you know what's the status of the issue? :) \r\n\r\nI do like to find all these issues raised a long time ago, in time when you need them the most. But they are all forgotten...", "I haven't worked on this in years haha.\r\n\r\nI won't be able to work on this at this point, so if you wanna take it up go for it.", "Hi, have there been any updates on this topic? I would find it very useful to use this distributions.\r\n\r\nThanks!"]}, {"number": 12538, "title": "FPGA Implementation on TensorFlow", "body": "Hi all,\r\n\r\nRecently, I read some papers about implementing Intel Xeon Phi on Tensorflow. Therefore, I want to make TensorFlow support FPGA board. But there is no information in Google and other forum/community.\r\n\r\nFor my understanding, we should implement the FPGA supporting in a class of TensorFlow which distributes tasks to different devices (like GPU/CPU). Is possible that implementing the FPGA supporting in that class? Or is there any good suggestions to make the progress to support FPGA on TensorFlow? \r\n\r\nI am looking forward any help or advice!\r\n\r\nThanks,\r\nKevin", "comments": ["How about implement an ops of Tensorflow using FPGA?\r\n\r\nhttps://www.tensorflow.org/extend/adding_an_op", "Related: https://github.com/tensorflow/tensorflow/issues/8820"]}, {"number": 12486, "title": "Feature request: stop requiring the same dtype for inputs in tf.shape_n", "body": "As for Tensorflow 1.3, [tf.shape_n](https://www.tensorflow.org/api_docs/python/tf/shape_n) takes a list of tensors as input to produce a list of shapes as output. However, it produces an error if tensors of different types are provided. As far as I can tell, whether tensors are the same type or not is completely irrelevant to the behavior of this function, making it an arbitrary constraint that limits its functionality for no particular reason.\r\n\r\nWould it be possible to remove such restriction if there's no good reason to keep it?\r\n", "comments": ["Note: this should probably have a very low priority since there's a quite simple workaround:\r\n\r\n```python\r\ndef shape_n(input, out_type=tf.int32):\r\n  return [tf.cast(tf.shape(t), out_type) for t in input]\r\n```\r\n\r\nThat's not exactly equivalent since it's not a single named op with n outputs, but n unnamed ops with a single output. Yet, the result should be the same.\r\n\r\nIn any case, I'm merely rising the point for any future updates.", "We currently don't have anybody working on this. It would be great if you could help us by working on this and submitting a PR. Let us know if you need further clarification. Thanks!", "Hi @leandro-gracia-gil @ali01, I am new to open source, how can I start working on this?"]}, {"number": 12475, "title": "Feature request: sparse_tensor_dense_matmul optimization on GPU", "body": "\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian GNU/Linux 8 (jessie)\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: ('v1.3.0-rc2-0-g2784b1c', '1.3.0-rc2')\r\n- **Python version**: 2.7.9\r\n- **Bazel version (if compiling from source)**: 0.5.3\r\n- **CUDA/cuDNN version**: 8.0 / 5.0\r\n- **GPU model and memory**: Nvidia GeForce GTX TITAN X\r\n- **Exact command to reproduce**:\r\n\r\nI would like to optimize sparse_tensor_dense_matmul operation on GPU to process sparse input completely on GPU. Now code like this:\r\n```\r\nimport tensorflow as tf\r\n\r\nwith tf.device('/gpu:0'):\r\n    st = tf.SparseTensor(\r\n        tf.constant([[0, 0], [1, 1]], dtype=tf.int64),\r\n        tf.constant([1.2, 3.4], dtype=tf.float32),\r\n        tf.constant([2, 2], dtype=tf.int64)\r\n    ) \r\n    v = tf.Variable([[1.0, 0.0], [0.0, 1.0]], dtype=tf.float32)\r\n    st = tf.sparse_tensor_dense_matmul(st, v)\r\n    st = tf.reduce_min(st)\r\n    optimizer = tf.train.AdamOptimizer()\r\n    trainer = optimizer.minimize(st)\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(trainer))\r\n```\r\nFails with error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"test_tf3.py\", line 18, in <module>\r\n    print(sess.run(trainer))\r\n  File \"/media/awork/home/astepochkin/drecs/repo/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/media/awork/home/astepochkin/drecs/repo/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/media/awork/home/astepochkin/drecs/repo/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n  File \"/media/awork/home/astepochkin/drecs/repo/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'gradients/SparseTensorDenseMatMul/SparseTensorDenseMatMul_grad/strided_slice_1': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n     [[Node: gradients/SparseTensorDenseMatMul/SparseTensorDenseMatMul_grad/strided_slice_1 = StridedSlice[Index=DT_INT32, T=DT_INT64, begin_mask=1, ellipsis_mask=0, end_mask=1, new_axis_mask=0, shrink_axis_mask=2, _device=\"/device:GPU:0\"](Const, gradients/SparseTensorDenseMatMul/SparseTensorDenseMatMul_grad/strided_slice_1/stack, gradients/SparseTensorDenseMatMul/SparseTensorDenseMatMul_grad/strided_slice_1/stack_1, gradients/SparseTensorDenseMatMul/SparseTensorDenseMatMul_grad/strided_slice_1/stack_2)]]\r\n```\r\nIt looks like it requires \"int64 strided slice\" to be executed on GPU. So maybe it just needs to enable int64 strided slice on GPU.", "comments": ["@ebrevdo Can you comment on this?", "Probably relevant, an issue I am running into with big matrices:\r\n\r\n> The GPU implementation is optimized to use 32 bit indexing, so give a friendly error to the programmer early on if they exceed.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/ce02c770fb269e5e607da459bde4f580ef108137/tensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc#L96-L114", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Would you like to enable strided slice with index=int64 on gpu?  Feel free to send a PR; in case this hasn't recently already been done.", "Any news? I also got that running tf 1.15 (ignore if fixed in 2, I did not convert yet)"]}, {"number": 12345, "title": "PEP 484 Type Annotations (feature request)", "body": "### System information\r\nN/A\r\n\r\n### Describe the problem\r\n## Background\r\nPEP 484 [1] added support for type hints in Python. These are purely annotations and are not enforced by the interpreter, however there are tools such as mypy [2] which can be run to check for consistency in the annotations. The typeshed initiative [3] has started to build external collections of type annotations for commonly used libraries.\r\n\r\nWhen adding type annotations to a codebase, it is best if you can achieve near 100% coverage, otherwise uncertainty propagates out from everywhere the \"untyped\" code is called. A codebase using TF would likely struggle to gain much benefit from type-checking in any of the core code built on top of TF.\r\n\r\n## Benefits of Adding Type Annotations\r\n * The expected inputs and outputs of functions become much clearer\r\n * Code completion is able to provide more useful suggestions, boosting productivity by reducing amount of time spent referring to docs\r\n * Static analysis can uncover latent bugs (case study here[5])\r\n\r\n## Difficulties/Drawbacks\r\n * People may be encouraged to overly constrain types, removing some of the flexibility of a dynamic language. But given that Google's Python style-guide discourages \"Power Features\" [4] I would argue that striving towards code that is explicit is a similar philosophy\r\n * The protobuf compiler would need to be augmented to generate type annotations.\r\n * The Tensorflow Python codebase is huge, so at this point adding the annotations would be a huge undertaking.\r\n * Tensorflow still supports python 2.7, 3.3 and 3.4 which do not have the type annotation syntax. So if this were implemented it would probably have to be in external *.pyi files, which is harder to maintain compared to inline type annotations in the source code.\r\n\r\n## Final thoughts\r\nI realise that this would be a major undertaking and wouldn't be likely to ship any time soon, but I'm curious to gauge Google's thoughts on this new feature in Python. I'm about to start building a new codebase from scratch and was keen to use it as a chance to try out type annotations. I probably still will give it a shot, but I suspect that unless most of the common data science libs out there adopt this standard then its usefulness will be quite limited.\r\n\r\n[1] https://www.python.org/dev/peps/pep-0484/\r\n[2] http://mypy-lang.org/\r\n[3] https://github.com/python/typeshed\r\n[4] https://google.github.io/styleguide/pyguide.html#Power_Features\r\n[5] http://blog.zulip.org/2016/10/13/static-types-in-python-oh-mypy/", "comments": ["This sounds like a good idea to me. We would of course need to add the type annotations incrementally, not all at once. This seems like the kind of thing that would benefit from community contributions as well.\r\n\r\n@martinwicke am I missing anything? Please unmark \"contributions welcome\" if this is actually not feasible or prudent for some reason.", "@aselle", "\u200bThis is great, but some parts of our infrastructure may break because it\nused py32 only inspect.getcallargs. It should be easy to fix (has already\nbeen done in part in another PR, but I expect more issues if this becomes\nwidespread).\n", "Awesome to hear you're open to the idea :) One minor detail to add to the above - instead of external *.pyi files, another option to maintain syntax compatibility with earlier Python versions could be to embed \"Type Comments\" as defined in PEP 484. E.g. here is a snippet I just pulled from Zulip:\r\n```\r\ndef add_request_metadata(report, request):\r\n    # type: (Dict[str, Any], HttpRequest) -> None\r\n    <method body starts here>\r\n```", "I prefer the comments over the extra files (if only because I don't like to have separately maintained files, they tend to rot). \r\n\r\nWe'll have to test whether all our infrastructure (doc generation and whatnot) works with this, but in principle this is a good idea.", "I think the most significant benefit of specifying types, improved auto-completion, can be achieved by just specifying the return types. \r\nThese are usually already there, but not in the correct format (at least for PyCharm). Simply changing\r\n```\r\nReturns:\r\n    A `Tensor`...\r\n```\r\nto \r\n```\r\nReturns: \r\n    Tensor: A `Tensor` ...\r\n```\r\nwould make these hints accessible to tools.\r\n\r\nFinally, the automatic type inference just by the structure of the code should not be underestimated.\r\nAt least in PyCharm, `if not isinstance(A, Type): raise TypeError()` is enough for code completion to know that following this line, A is of type Type. It also follows types through attribute assignments in classes and function calls. This significantly reduces the amount of places where type hints become necessary.\r\n\r\nWhile playing around with it, I found, for example, that for Tensor().dtype to be recognized correctly, it was sufficient to annotate the as_dtype function correctly.", "Would be great if we can support Python 3 Typing in TensorFlow.\r\n\r\nWe will benefit from both \r\n1. Static Type Checking, and \r\n2. Hint Linting.", "Agreed. I see there's some stuff in contrib and couple of TODOs but has anything been agreed? Would make it a lot easier to integrate in production systems.", "Probably a silly question, but is there currently any way to integrate tensorflow with mypy? (or any other way to get typechecking with TF)", "I think external type-stubs for TensorFlow would make sense, along the lines of what we've started doing for numpy with [numpy-stubs](https://github.com/numpy/numpy-stubs). This would allow for integrating tensorflow with mypy (which is currently not supported).", "We started writing some stubs for our project: https://github.com/persephone-tools/persephone/tree/master/stubs/tensorflow this works well enough for our needs but something more comprehensive would be a lot better.", "We decided to move our attempt at making stubs into an external library to make it easier to collaborate: https://github.com/persephone-tools/tensorflow-stubs", "It might make sense to copy some of the test infrastructure we wrote for\nnumpy-stubs: https://github.com/numpy/numpy-stubs\nOn Thu, Jun 14, 2018 at 7:06 AM Janis Lesinskis <notifications@github.com>\nwrote:\n\n> We decided to move our attempt at making stubs into an external library to\n> make it easier to collaborate:\n> https://github.com/persephone-tools/tensorflow-stubs\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12345#issuecomment-397308219>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABKS1mYfjVtC45r7PKnvrl9dW3MXEcCoks5t8m39gaJpZM4O5tU0>\n> .\n>\n", "@shoyer thanks for that suggestion, I created an issue for it https://github.com/persephone-tools/tensorflow-stubs/issues/2", "Just for the information of @shuttle1987 and others in the thread, there exist a [MonkeyType](http://monkeytype.readthedocs.io) project that aims at generating stubs by tracing actual code execution. Considering vast tensorflow test suites and official repositories, encountering all possible output types for the majority of function calls seems likely. One can even have a validation split and check whether stubs inferred from 90% of the code cover the 10% validation portion to get an estimate of the overall coverage :)", "Just as a small note regarding MonkeyType, it does work quite well. The only \"problem\" I encountered so far was when you're various tuples as `Iterable[T]` (don't ask me why), but it infers `Union[Tuple[T], Tuple[T, T], ...]`.", "@MInner if you want to make some test cases that should pass and expected-fail a PR would be very welcome over on https://github.com/persephone-tools/tensorflow-stubs/ I see this as a really good use case for the MonkeyType that you mentioned", "PyTorch just implemented this https://github.com/pytorch/pytorch/pull/12500 via stubs and the code completion it enables is a huge convenience. No pressure though :)", "It will be awesome if we could support this feature by stubs, or it will be more awesome if we could support typing natively!", "wouldn't it be possible to add python 3 style type hints, and just automatically remove them when building python2 wheels?", "Since TF2.1 is the last version to support Python 2, shouldn't all later versions support at least 3.5? Then the native syntax can be used instead of the comments.\r\n\r\nSeems like https://github.com/persephone-tools/tensorflow-stubs/ hasn't been updated in a while. Would love to see (or even start making) some progress on this front!", "Here is an rfc which takes a step in this direction:\nhttps://github.com/tensorflow/community/pull/208\n\nAnd yes, we can use inline annotations, although some tooling still needs\nto be upgraded.\n\n@mdanatg\n\nOn Fri, Mar 13, 2020, 16:43 Srinivas Lade <notifications@github.com> wrote:\n\n> Since TF2.1 is the last version to support Python 2, shouldn't all later\n> versions support at least 3.5? Then the native syntax can be used instead\n> of the comments.\n>\n> Seems like the https://github.com/persephone-tools/tensorflow-stubs/\n> hasn't been updated in a while. Would love to see (or even start making)\n> some progress on this front!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12345#issuecomment-598974338>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAEM57PGS6P3SF2KFW5B7F3RHLAIXANCNFSM4DXG2U2A>\n> .\n>\n", "Funny enough @martinwicke I had just found that RFC right when you commented. Looks great to me! But from my understanding the RFC is to consolidate custom types into a single module rather than adding type hints to the public API. Is that true @mdanatg?", "The RFC sets up the initial support and guidance for types, including\nannotations, but it does not add the annotations themselves. My hope is\nthat the community will be able to help with that bit - once we set up a\nfew examples, it should largely be straightforward to annotate the APIs one\nby one.\n\nCheers,\nDan\n\nOn Sat, Mar 14, 2020 at 8:54 AM Srinivas Lade <notifications@github.com>\nwrote:\n\n> Funny enough @martinwicke <https://github.com/martinwicke> I had just\n> found that RFC right when you commented. Looks great to me! But from my\n> understanding the RFC is to consolidate custom types into a single module\n> rather than adding type hints to the public API. Is that true @mdanatg\n> <https://github.com/mdanatg>?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12345#issuecomment-599055793>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AGLFDQ5KT7EUXZZ2K7HNBYLRHN5ABANCNFSM4DXG2U2A>\n> .\n>\n", "I add also https://github.com/microsoft/pyright to the resources cause was not mentioned in the RFC.", "Now that tensorflow/community#208 is accepted, may we start submitting pull requests that add type hints to the code base?", "@mdanatg I think type hints should be acceptable now? Or do we need to wait for the type classes?", "Yes, we can start adding type annotations, with a few caveats, see below. In addition, there is now [TensorLike](https://www.tensorflow.org/api_docs/python/tf/types/experimental/TensorLike) (also unparameterized for now) which captures some of the types that TensorFlow autoboxes. More will be added to the list.\r\n\r\nThe caveats:\r\n * some directories may need to be upgraded, but that's a straightforward change that we can make as PRs are being sent; that means initial PRs may take a bit longer to merge\r\n * only simple `tf.Tensor` annotations can be currently added. We're still working on letting you specify `tf.Tensor[tf.Int32]` or `tf.Tensor[Any]`, but there are some compatibility issues delaying that. In the future, `tf.Tensor` will be equivalent with `tf.Tensor[Any]`\r\n * all this is still in early stages. Expect a few bugs until we've stabilized everything - would appreciate opening up new issues when coming across such bugs\r\n\r\n", "@mdanatg are there any examples of adding type annotations so far? I'd be interested in helping.", "@EPronovost I'm still working to sort out some blockers to making Tensor a generic class (#40921, which was temporarily rolled back as an internal test broke), and to extend DType so that it can be used in annotations (#40696). But until those land, we can still use a plain tf.Tensor as type annotation. There isn't an example yet, but would you be interested in creating one, for example by choosing a simple op (like tf.Identity) and trying to add annotations for it? It's mainly a matter of adding the annotation and making sure build/tests still work.", "@mdanatg I'd be interested in helping.  Is `mypy` included as part of the TF test suite? If `Tensor` annotations are still a WIP, would it be easier to start with some of the higher-level APIs (e.g. `Dataset` or `keras.Model` methods)?  Sorry if I'm missing something; I wasn't able to find much just searching.", "@EPronovost working on other types first sounds like a good idea too. Note that Keras is moving to a stand-alone repository, and TF will not depend on it. But things like Dataset, TensorShape, etc. all sound good to me. I think it would be a good idea to do some experimentation first.\r\nCurrently, TF only has internal tests for type annotations, which use `pytype`. Setting up a set of tests which use `mypy` would also be a good initial step.", "@mdanatg Would it be reasonable to work on type stubs (.pyi) files in typeshed that try to approximate public api while issues of tensor genericness/runtime get worked out.\r\n\r\nAs right now it's unclear what expected progress of this issue is. I think having runtime genericness while nice isn't needed initially. The primary motivation I have for types is a better IDE/type checker experience. Worst case I can use either `from __future__ import annotations` or string for generic tensors. \r\n\r\nThe other complicating aspect is how accurate the first set of types need to be. Which functions handle tf.Tensor vs SparseTensor vs EagerTensor vs etc/genericness of shape/data type/device. I think imperfect type stubs (as long as they lean towards false negatives instead of false positives) would be much better than no types given probable complexity of correct types on first try.", "I think working on stubs is reasonable to me but I think it's orthogonal to fixing the type hierarchy. Whether you add stubs or the annotations directly in the code (which is possible now), the main factor is how accurate those annotations will be. I think it's important for a type checker to be accurate and not have false positives or negatives, otherwise users will just start to ignore it.", "I view false negatives as an acceptable evil here. The current lack of types is essentially an extreme false negative of never report an error. If we add type stubs that detect some type errors, but miss others then any information they give to a user will at least help them fix something. It will miss things that they may expect a type checker to catch, but current untyped status quo misses everything. If you add fully correct types to a single file at a time you can call the codebase as partly typed but still be full of false negatives for the other files. So I can't see a situation where types with false negatives is worse than current status quo.\r\n\r\nFalse positives are very bad as noisy fake error messages condition users to treat all errors as mistakes from the type checker.\r\n\r\nLong term goal I agree is to have minimal false negatives/positives (ideally 0 within constraints of type system). ", "> @mdanatg [...] But things like Dataset, TensorShape, etc. all sound good to me. [...] [link](https://github.com/tensorflow/tensorflow/issues/12345#issuecomment-688845752)\r\n\r\nLink with my issue: https://github.com/tensorflow/datasets/issues/3822"]}, {"number": 12302, "title": "memory leak bug", "body": "Hi, I have seen many issues reporting Tensorflow memory leak, so I wanted to report another sample of leak. \r\nI was testing the GA3C code and after a while (about a day) the process taking all of 32g of memory.\r\nsystem spec is \r\nOS : Ubuntu 17.04\r\ntensorflow version : 1.2.1 binary instalation\r\ncuda version : 7.5\r\ngpu : GeForce GTX 1080 SLI, 8gig of gpu memory\r\n\r\nyou can get the code from : \r\nhttps://github.com/babak-badnava/GA3C\r\n\r\nwith many thanks \r\n", "comments": ["This seems similar to #11721. If possible, can you post a simpler example? It is difficult to debug this issues on large projects, and it is unknown if the memory leak is in TensorFlow itself or in the GA3C code.\r\n\r\nMarking as contributions welcome if anyone wants to take a stab at finding where the memory leak is in the GA3C or TensorFlow code.", "I want to start contributing to this. Can you tell me where to start?", "@ciphx see if you can reproduce the problem. If you can, try removing parts of GA3C, checking if the problem still occurs as you remove pieces of it. The goal would be to get a small and simple piece of code that reproduces the problem.\r\n\r\nYou can also enable verbose logging and look at the \\_\\_LOG_MEMORY\\_\\_  messages to see if there are many Tensors being allocated without being deallocated."]}, {"number": 12264, "title": "Simple EditDistance constructor is missing in C++", "body": "### System information\r\n- **Windows 10**\r\n- **TensorFlow installed from source**\r\n- **TensorFlow version 1.3-rc2**\r\n- **Python version 3.5.3**:\r\n- **Bazel N/A**\r\n- **CUDA/cuDNN version N/A**\r\n- **GPU model and memory N/A**\r\n- **N/A**\r\n\r\n### Describe the problem\r\nThere is no simplier EditDistance::EditDistance() constructor there which only accept sparse::SparseTensor arguments. The EditDistance constructor wants a lot of arguments\r\n\r\n`EditDistance::EditDistance(const ::tensorflow::Scope& scope,\r\n                           ::tensorflow::Input hypothesis_indices,\r\n                           ::tensorflow::Input hypothesis_values,\r\n                           ::tensorflow::Input hypothesis_shape,\r\n                           ::tensorflow::Input truth_indices,\r\n                           ::tensorflow::Input truth_values,\r\n                           ::tensorflow::Input truth_shape, const\r\n                           EditDistance::Attrs& attrs)`\r\n\r\ninstead of\r\n\r\n`EditDistance::EditDistance(const ::tensorflow::Scope& scope,\r\n                           const sparse::SparseTensor& hypothesis,\r\n                            const sparse::SparseTensor& truth,\r\n                           EditDistance::Attrs& attrs)`\r\n\r\nInternally all hypothesis and truth parameters are put into SparseTensor object. Why SparseTensor parameters are not used in the constructor?\r\n\r\n### Source code / logs\r\nN/A\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", " `sparse::SpareTensor` represents a sparse value, not a symbolic input to an op. Note that SparseTensor is used in op kernels, but not in the C++ graph-building API.\r\n\r\nI agree though that it would be useful to have a symbolic SparseTensor concept in the C++ API, similar to in the Python API. I'll mark this as a feature request.", "Hello I was looking to take it up. Please let me know if there are any specific points to beware of while going ahead with this. I am new to tensorflow code so please do let me know incase this is not the best place to start. ", "Hey @sharathppss, thanks for your interest, but I don't think this will be a good feature for a TF newbie. This will require some design work and it's best to get familiar using/working with the code first.", "Thanks @skye. Yeah I was looking at the code, this might be a bit much to get started."]}, {"number": 12162, "title": "build_all_xxx.sh support for Tizen target", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 LTS 64bit\r\n- **TensorFlow installed from (source or binary)**: Yes\r\n- **TensorFlow version (use command below)**: 1.2.1\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.1.3\r\n- **CUDA/cuDNN version**: 8.0/6.0\r\n- **GPU model and memory**: GTX Titan XP\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nI can find appropriate build scripts for iOS, Android, and Linux at the below webpage.\r\n- https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile/\r\n\r\nWhere can I also find a build script in case that I want to build Tensorflow on the Tizen  target device ?\r\n\r\n- Tizen (for platform developers): https://source.tizen.org/\r\n- Tizen (for application developers): https://developer.tizen.org/\r\n\r\n### Source code / logs\r\nNothing\r\n", "comments": ["This sounds like a good issue for community to support. Maybe Samsung wants to help with integration? cc @yunjey in case they know people in Samsung to take it on", "In case of Caffe2, I have merged(https://github.com/caffe2/caffe2/pull/877) a build script to run Caffe2 library on the Tizen software platform previously. \r\n\r\nAnyone who tried to do related works to support the Tizen platform in case of Tensorflow? Any comments and experiences will be helpful to us. ", "Yesterday, Google officially announced how to use Neural Network API on Android 8.1. @yaroslavvb Do they are planning to open Tensorflow-Lite (for mobile) as well as Tensorflow?\r\n\r\n@yunjey Can I give you some questions to try to enable Tensorflow (or Tensorflow Lite) on Tizen platform? As a first question, Can we use Cmake instead of Bazel  to build Tensorflow on real ARM/Linux device (or on QEMU-ARM/Linux emulator)", "Some work is being done here : <https://review.tizen.org/gerrit/#/admin/projects/platform/upstream/tensorflow>\r\nCurrently, Tensorflow Lite RPM from r1.7 is built for tizen platform.", "We have worked to build Tensorflow-lite only. We have to also enable how to build Tensorflow as well as Tensorflow-lite"]}, {"number": 12154, "title": "Feature request: document which inputs have gradients", "body": "For most operations, the documentation does not make clear what gradients are implemented. Including this information for the inputs (e.g. annotating the inputs that do not have gradients implemented) would help the user better understand the resulting graph. \r\n\r\nIt appears a related issue was closed, as it was not a feature request. https://github.com/tensorflow/tensorflow/issues/6025", "comments": ["Seems like a reasonable thing to ask for but I'm not sure whether this is something we're thinking of working on actively in the near future. We do welcome contributions though!", "Since the gradients are registered in the code, shouldn't it be possible to build the list programatically?\r\n\r\nParse each \"ops\" file in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/ops, and match the names with the corresponding \"grad\".", "As a heuristic, you could search for all instances of `ops.RegisterGradient`, that gives you which op has a gradient associated. You can then get source, and parse out `None`s out of `return` line, which gives you the inputs without gradients.\r\n\r\nNote that this works only for atomic ops. Something like `tf.norm` will support gradients, but there are no gradients for this op in particular because it's defined in terms of sqrt/reduce_sum/etc"]}, {"number": 12071, "title": "Numerical instability of gradient calculation of tf.norm (nan at 0, inf for small values) ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes see below\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.11.6\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: #v1.2.0-5-g435cdfc    1.2.1\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: On CPU\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: tf.norm at [0,0] see below for code\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nprint(tf.GIT_VERSION, \"  \", tf.VERSION) #v1.2.0-5-g435cdfc    1.2.1\r\n\r\nX = tf.placeholder(tf.float32, shape=(4,None))\r\nZ = tf.norm(X, ord='euclidean', axis=1, name='logit')\r\nvar_grad = tf.gradients(Z, [X])\r\n\r\nwith tf.Session() as sess:\r\n    X_ = np.array([\r\n        [1],  # Grad OK\r\n        [0],  # Grad NaN\r\n        [1e-16],  # Grad OK\r\n        [1e-19] #Grad Inf\r\n    ], dtype=np.float32)\r\n    sess.run(tf.global_variables_initializer())\r\n    print(sess.run((Z, var_grad), feed_dict={X: X_}))\r\n    # Result:\r\n    #(array([9.99999940e-01, 0.00000000e+00, 9.99999951e-17,\r\n    #        0.00000000e+00], dtype=float32), [array([[1.00000012],\r\n    #                                                 [nan],\r\n    #                                                 [1.],\r\n    #                                                 [inf]], dtype=float32)])\r\n```\r\n\r\n### Describe the problem\r\n`nan` is calculated for the gradient of `tf.norm` at zero values. For extremely small values `inf` is calculated. Note that the exact result should be 1 in all cases above. \r\n\r\nAbove is a minimal example to reproduce it. The problem occurred in a real world scenario, when implementing a custom loss function (the entropy in https://arxiv.org/abs/1611.01449) and two embeddings where too close to each other (distance practically 0).\r\n\r\n### Source code / logs\r\nSee above \r\n\r\n#### Output of logfile\r\n```\r\n== cat /etc/issue ===============================================\r\nDarwin Olivers-MBP-5.fritz.box 15.6.0 Darwin Kernel Version 15.6.0: Tue Apr 11 16:00:51 PDT 2017; root:xnu-3248.60.11.5.3~1/RELEASE_X86_64 x86_64\r\nMac OS X 10.11.6\r\n\r\n== are we in docker =========================================  echo == are we in docker ====================================num echo == are we in docker =========================================  ec==  echo == are we in docker =======================================c++ --version\r\n\r\n== uname -a =====================================================\r\nDarwin Olivers-MBP-5.fritz.box 15.6.0 Darwin Kernel Version 15.6.0: Tue Apr 11 16:00:51 PDT 2017; root:xnu-3248.60.11.5.3~1/RELEASE_X86_64 x86_64\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.0)\r\nprotobuf (3.3.0)\r\ntensorflow (1.2.1)\r\n\r\n== check for virtualenv ==============  echo == check for virtualenv =====on_b echo == check fo sys  echo == check for virtualenv ============== echo == check for virtualenv ============================================\r\n\r\n== cat /etc/issue ===============================================\r\nDarwin Olivers-MBP-5.fritz.box 15.6.0 Darwin Kernel Version 15.6.0: Tue Apr 11 16:00:51 PDT 2017; root:xnu-3248.60.11.5.3~1/RELEASE_X86_64 x86_64\r\nMac OS X 10.11.6\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 7.3.0 (clang-703.0.31)\r\nTarget: x86_64-apple-darwin15.6.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n\r\n== uname -a =====================================================\r\nDarwin Olivers-MBP-5.fritz.box 15.6.0 Darwin Kernel Version 15.6.0: Tue Apr 11 16:00:51 PDT 2017; root:xnu-3248.60.11.5.3~1/RELEASE_X86_64 x86_64\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.0)\r\nprotobuf (3.3.0)\r\ntensorflow (1.2.1)\r\n\r\n== check for virtualenv =========================================\r\nTrue\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.2.1\r\ntf.GIT_VERSION = v1.2.0-5-g435cdfc\r\ntf.COMPILER_VERSION = v1.2.0-5-g435cdfc\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\ntf_env_collect.sh.txt: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n```", "comments": ["This is caused by square root in definition of tf.norm. IE you are taking gradient of sqrt(x^2). Gradient of sqrt approaches infinity, whereas gradient of x^2 approaches 0, so computing them separately then multiplying is a problem. @goodfeli -- do you remember if Theano uses some standard stabilizing transformation for this kind of case?", "Theano gives NaN as the gradient of the norm of a vector with zero norm:\r\n\r\n    >>> x = theano.tensor.vector()\r\n    >>> y = theano.tensor.square(x)\r\n    >>> z = y.sum()\r\n    >>> norm = theano.tensor.sqrt(z)\r\n    >>> d = theano.tensor.grad(norm, x)\r\n    >>> d.eval({x: [0., 0.]})\r\n    array([ nan,  nan])\r\n\r\nTheano does give 0 as the gradient of the norm of a *scalar*. I think for this it is probably using a patternsub to turn sqrt(square(x)) into abs(x). Note that in Theano's conventions, the derivative of abs(x) at 0 is treated as 0 rather than undefined.", "Thinking philosophically, the general problem is that computational graph ends up with things like`a/a` where `a` is 0. Numerically it's undefined, but the limit exists. Similar issue exists with gradient of tf.select (https://github.com/tensorflow/tensorflow/issues/2540) and gradient of `tf.exp(-tf.exp(x))`\r\n\r\nYou have to do some algebraic massaging to get numerically defined result.\r\n\r\nIn your particular case, you could replace automatic gradient with a stable version:\r\n\r\n```\r\nfrom tensorflow.python.framework import function\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n@function.Defun(tf.float32, tf.float32)\r\ndef norm_grad(x, dy):\r\n    return dy*(x/tf.norm(x))\r\n\r\n@function.Defun(tf.float32, grad_func=norm_grad)\r\ndef norm(x):\r\n    return tf.norm(x)\r\n\r\nsess = tf.InteractiveSession()\r\nX = tf.placeholder(tf.float32, shape=(4,None))\r\nX_ = np.array([\r\n    [1],  # Grad OK\r\n    [0],  # Grad NaN\r\n    [1e-16],  # Grad OK\r\n    [1e-19] #Grad Inf\r\n], dtype=np.float32)\r\nZ = norm(X)\r\nvar_grad = tf.gradients(Z, [X])\r\nprint(sess.run((Z, var_grad), feed_dict={X: X_}))\r\n\r\n#1.0, [array([[  1.00000000e+00],\r\n#      [  0.00000000e+00],\r\n#     [  1.00000002e-16],\r\n#    [  9.99999968e-20]], dtype=float32)])\r\n\r\n\r\n```", "First @yaroslavvb thanks a lot for code for the stable version of the gradient (I did not know that it's possible to overwrite the gradient calculation).\r\n\r\nI agree that the corner case 0 is not really defined but the same is true for the gradient of a ReLU at 0. For `float32` exact 0 is not a singular event, you might get it by rounding errors and then the whole optimization is broken. Furthermore (even more problematic) there are instabilities in the gradient of `tf.norm` for small values, e.g the `Inf` for e-19, which you could encounter in an optimization process containing the norm.  In my case part of the loss function was `exp(-norm(x-y))` (it took quite some time to nail down the problem to the gradient of the tf.norm()). Therefore, I think this is a real bug and not just a pure mathematical problem and deserves fixing.\r\n\r\nI did not include this in my original report, but the problem is also present for non-scalars. See\r\n```\r\nX = tf.placeholder(tf.float32, shape=(1,3))\r\nZ = tf.norm(X, ord='euclidean', axis=1, name='logit')\r\npik = tf.nn.softmax(logits=Z)\r\nres = tf.reduce_sum(pik)\r\nvar_grad = tf.gradients(res, [X])\r\n\r\nwith tf.Session() as sess:\r\n    X_ = np.array([\r\n        [1e-19, 1e-19, 0]\r\n    ], dtype=np.float32)\r\n\r\n    sess.run(tf.global_variables_initializer())\r\n    print(sess.run((res, var_grad), feed_dict={X:X_}))\r\n```\r\nwhich gives `(nan, nan, nan)`. \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "Yes, I agree that this should be fixed, the question is how. The bug with tf.select-gradient-caused NaN's linked above is still open a year later and it's a similar problem.\r\n\r\nPython-specific solution would be to replace norm with a fused version like above, and corresponding gradient. Fused versions are not really preferred but are done if it's important enough, like was done for `tf.fused_batch_norm`.\r\n\r\nMore general solution is to have a numerically stabilizing transformation which simplifies expressions that lead to \"0/0\" or \"infinity*0\" to behave more like their limits. @benoitsteiner @petewarden -- do you know if there's anything on numerically stabilizing graph transformations on the roadmap?", "+1\r\n\r\nAlready several times I've been hitting that.", "@yaroslavvb  Hi, I don't understand why ## dy*(x/tf.norm(x)) ##  this could lead to a stable gradient. Could you please explain it more? Thank you.", "+1 I'm also facing this issue while computing eucledian norm for the paper :  [A Structured Self-attentive Sentence Embedding](https://arxiv.org/abs/1703.03130)\r\n\r\nEdit : Pytorch applied a fix for it. May be tf can do the same https://github.com/pytorch/pytorch/pull/2775/files", "Any update on this?", "Bump for my honors thesis XD", "Assigning gradient issue to @girving", "I posted a stable implementation above which you can substitute instead of TF default version. Don't know if this works for an external contributor, that depends if there internal tests which may be broken by changing numerics of tf.norm", "Marking as contributions welcome, I gave an example solution above, someone wanting to be a part of TF could work the solution into tensorflow. ", "Hi @yaroslavvb ,\r\nHere's the problem:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nX = tf.placeholder(tf.float32, shape=(4,None))\r\nZ = tf.norm(X)\r\nvar_grad = tf.gradients(Z, [X])\r\n\r\nwith tf.Session() as sess:\r\n    X_ = np.array([\r\n        [0],\r\n        [0],\r\n        [0],\r\n        [0]\r\n    ], dtype=np.float32)\r\n    \r\n    [Z_val, Z_grad] = sess.run([Z, var_grad], feed_dict={X: X_})\r\n    \r\n    print \"Z_val = \", Z_val\r\n    print \"Z_grad = \", Z_grad\r\n```\r\nIt will give output:\r\n\r\n> Z_val =  0.0\r\n> Z_grad =  [array([[nan],\r\n>        [nan],\r\n>        [nan],\r\n>        [nan]], dtype=float32)]\r\n\r\nUsing your suggestion:\r\n```\r\nfrom tensorflow.python.framework import function\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n@function.Defun(tf.float32, tf.float32)\r\ndef norm_grad(x, dy):\r\n    return dy*(x/(tf.norm(x, ord=2)))\r\n\r\n@function.Defun(tf.float32, grad_func=norm_grad)\r\ndef norm(x):\r\n    return tf.norm(x, ord=2)\r\n\r\nX = tf.placeholder(tf.float32, shape=(4,None))\r\nZ = norm(X)\r\nvar_grad = tf.gradients(Z, [X])\r\n\r\nwith tf.Session() as sess:\r\n    X_ = np.array([\r\n        [0],\r\n        [0],\r\n        [0],\r\n        [0]\r\n    ], dtype=np.float32)\r\n    \r\n    [Z_val, Z_grad] = sess.run([Z, var_grad], feed_dict={X: X_})\r\n    \r\n    print \"Z_val = \", Z_val\r\n    print \"Z_grad = \", Z_grad\r\n```\r\nI still get:\r\n\r\n> Z_val =  0.0\r\n> Z_grad =  [array([[nan],\r\n>        [nan],\r\n>        [nan],\r\n>        [nan]], dtype=float32)]\r\n\r\nSo here's my simple modification that seems to work (just by adding a small epsilon to the gradient's denominator):\r\n```\r\nfrom tensorflow.python.framework import function\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n@function.Defun(tf.float32, tf.float32)\r\ndef norm_grad(x, dy):\r\n    return dy*(x/(tf.norm(x, ord=2)+1.0e-19))\r\n\r\n@function.Defun(tf.float32, grad_func=norm_grad)\r\ndef norm(x):\r\n    return tf.norm(x, ord=2)\r\n\r\nX = tf.placeholder(tf.float32, shape=(4,None))\r\nZ = norm(X)\r\nvar_grad = tf.gradients(Z, [X])\r\n\r\nwith tf.Session() as sess:\r\n    X_ = np.array([\r\n        [0],\r\n        [0],\r\n        [0],\r\n        [0]\r\n    ], dtype=np.float32)\r\n    \r\n    [Z_val, Z_grad] = sess.run([Z, var_grad], feed_dict={X: X_})\r\n    \r\n    print \"Z_val = \", Z_val\r\n    print \"Z_grad = \", Z_grad\r\n```\r\nThis will gives:\r\n\r\n> Z_val =  0.0\r\n> Z_grad =  [array([[0.],\r\n>        [0.],\r\n>        [0.],\r\n>        [0.]], dtype=float32)]", "@yaroslavvb Thanks for the snippet!. Unfortunately, with eager execution enabled,\r\nI get the following error\r\n`AttributeError: Tensor.op is meaningless when eager execution is enabled.`\r\n\r\nThe following works with eager execution enabled\r\n\r\n```\r\n@tf.custom_gradient\r\n  def norm(x, axis=None, keep_dims=False):\r\n      y = tf.norm(x, axis=axis, keep_dims=keep_dims)\r\n  \r\n      def grad(dy):\r\n          return dy * (x / (y + 1e-19))\r\n  \r\n      return y, grad\r\n\r\n```", "Thanks, @gsutanto for your solution. I wanted to ask what the correct way to force calculate gradients is when there is a complicated interaction of variables, such as when we're using gradient descent to optimize a loss function term with the norm as penalty.\r\n\r\nDo I have to force the gradient of the entire loss function or should only the gradient of norm be computed separately?\r\n\r\nDrawing from your code, what I'm saying is something like in the case below:\r\n\r\n```python\r\n...\r\nX = tf.placeholder(tf.float32, shape=(4, 1))\r\nY = tf.placeholder(tf.float32, shape=(1, 1))\r\nW = tf.Variable(tf.zeros(shape=[1, 4]))\r\n\r\nZ =  norm(X)\r\nvar_grad = tf.gradients(Z, [X])\r\n\r\nJ = tf.reduce_mean((Y - tf.matmul(W, X))) + (.5 * Z)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    X_ = np.array([\r\n        [0],\r\n        [0],\r\n        [0],\r\n        [0]\r\n    ], dtype=np.float32)\r\n    Y_ = np.array([\r\n        [1]\r\n    ], dtype=np.float32)\r\n    \r\n    optimizer = tf.train.GradientDescentOptimizer(1).minimize(J)                                                   \r\n    sess.run([optimizer, J, var_grad], feed_dict={X: X_, Y: Y_})\r\n```", "Hi, @dust0x if you work on a batch of data, it would be something like:\r\n```\r\nfrom tensorflow.python.framework import function\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n@function.Defun(tf.float32, tf.float32)\r\ndef norm_grad(x, dy):\r\n    return dy*(x/(tf.norm(x, ord=2)+1.0e-19))\r\n\r\n@function.Defun(tf.float32, grad_func=norm_grad)\r\ndef norm(x):\r\n    return tf.norm(x, ord=2)\r\n\r\ndef norm_axis1(X):\r\n    return tf.map_fn(lambda x: norm(x), X)\r\n\r\nX = tf.placeholder(tf.float32, shape=(4,3))\r\nZ = norm_axis1(X)\r\nvar_grad = tf.gradients(Z, [X])\r\nnorm_var_grad = norm_axis1(var_grad[0])\r\n\r\nwith tf.Session() as sess:\r\n    X_ = np.array([\r\n        [0,0,0],\r\n        [1.,1.,1.],\r\n        [0.,0.,0.],\r\n        [2.,2.,2.]\r\n    ], dtype=np.float32)\r\n    \r\n    [Z_val, Z_grad, norm_Z_grad] = sess.run([Z, var_grad, norm_var_grad], feed_dict={X: X_})\r\n    \r\n    print \"Z_val = \", Z_val\r\n    print \"Z_grad = \", Z_grad\r\n    print \"norm_Z_grad = \", norm_Z_grad\r\n```\r\n(say if X_ were NxM=4x3; each row is a feature vector of size 3 that you would like to compute norm() on, and there are N=4 samples; there might be a better way to do this, if anyone knows, please post your code here; in terms of TensorFlow, I believe it will automatically use the newly defined norm gradient in any cost functions you are using it; please let me know if my understanding is incorrect...)", "Thanks again, @gsutanto. Luckily for me, the input to the norm only needs to be fed once in my case and so I don't (yet) have to worry about batch computations. The way I am currently solving this is by simply calling the forced gradient of norm in the same session as the optimizer and if what you say is correct \u2014 nothing seems to have broken so far, so it must be \u2014 this should work just fine.\r\n", "Hi, I am hitting this too when trying to use the pairwise distance norm of a batch of vectors, here is the [SO](https://stackoverflow.com/questions/54346263/tensorflow-gradient-getting-all-nan-values) question, am I getting this issue because of this bug? ", "> Hi, I am hitting this too when trying to use the pairwise distance norm of a batch of vectors, here is the [SO](https://stackoverflow.com/questions/54346263/tensorflow-gradient-getting-all-nan-values) question, am I getting this issue because of this bug?\r\n\r\nI am getting the same bug on a distance norm of a batch of vectors. We have very similar situations.\r\n\r\n--\r\n\r\nEDIT: Although this is certainly not the same, I found that taking the absolute value was without the gradient instabilities. I.e. |x_1| + |x_2| + |x_3| + ...", "Hi, @gsutanto, @gokul-uf , I've tried to implement your code, like this:\r\n\r\n`@tf.custom_gradient\r\ndef custom_norm(x):\r\n    y = tf.norm(x, axis= - 1 , keepdims=False)\r\n    def grad(dy):\r\n        return dy * (x / (tf.expand_dims(y, -1) + 1e-19))\r\n    return y, grad\r\n\r\ndef custom_norm1(X):\r\n    return tf.map_fn(lambda x: custom_norm(x), X)`\r\n\r\nI've add the `tf.expand_dims(y, -1)` in the gradient calculation and the `axis= -1` in the norm computation because of my current problem: I'm working with batch of data so the tensor I have to compute the norm on has shape `x = [2, 256000, 2]` and I have to compute the norm on the last dimension, so the result should have shape `x_norm = [2, 256000]` (that's why I added the `axis = -1` argument. \r\nThe problem comes when trying to compute the gradient, because of the shape. I've had to expand the dimensions of `x_norm` to be able to divide `x / (x_norm + 1e-19)`, otherwise TensorFlow gives an error. So that calculus yields a tensor of shape `[2, 256000, 1]`, but know I get another error: \r\n\r\n`ValueError: Dimensions must be equal, but are 256000 and 2 for 'gradients_30/map_7/while/IdentityN_grad/mul' (op: 'Mul') with input shapes: [256000], [256000,2].`\r\n\r\nI can tell it is beacuse know I have to multiply that tensor by `dy` and it has shape `[256000]`. I can't figure out how to solve this and being able to compute the norm on a tensor with batches, without mixing batches. Thank you in advance.", "@macarena1807, I believe you can apply `tf.expand_dims(dy, -1)` in order to make the `dy` shape match with the normalized `x` vector, so that the output of `grad(dy)` has the same shape as `x`:\r\n\r\n```\r\n@tf.custom_gradient\r\ndef custom_norm(x):\r\n    y = tf.norm(x, axis= - 1 , keepdims=False)\r\n    def grad(dy):\r\n        return tf.expand_dims(dy, -1)*x/(tf.expand_dims(y, -1) + 1e-19)\r\n    return y, grad\r\n\r\ndef custom_norm1(X):\r\n    return tf.map_fn(lambda x: custom_norm(x), X)\r\n```", "Thank you for the solution @yaroslavvb, @gsutanto and @gokul-uf.\r\n\r\nI needed the L2 Norm for a personal project and I was having a lot of issues. During the first epochs the network was stable and even able to converge, and then, suddenly the loss exploded to infinity, without any kind of warning or symptoms. \r\n\r\nI testet every technique I knew to try to solve the problem without really knowing what was going on, including gradient clipping, regularization, different architectures, activation functions and initialization techniques, residual connections, different optimizers and learning rates, and so on, but nothing really worked. Then keeping the same structure I changed the Norm to L1 and everything worked fine, however L2 was a necessary requirement. \r\n\r\nThis solution completely solved the exploding gradient issue while using the L2 Norm, stabilized the training and provided the best results so far.\r\n\r\nImports:\r\n```\r\nfrom tensorflow.keras import layers, models, regularizers, activations\r\nimport tensorflow.keras.backend as K\r\nimport tensorflow as tf\r\n```\r\n\r\nCustom Keras L2 Norm Layer with custom gradient to prevent numerical instability:\r\n- I modified the part of adding a small epsilon to the gradient's denominator (`y+1.0e-19`) to `K.maximum(y, K.epsilon())`, replicating the same designed used in Keras loss functions.\r\n```\r\n@tf.custom_gradient\r\ndef l2_norm(x):\r\n   y = tf.norm(x, ord='euclidean', keepdims=True, axis=1)\r\n   def grad(dy):\r\n       return dy*(x/K.maximum(y,K.epsilon()))\r\n   return y, grad\r\n```\r\n\r\nExample of how to apply layer in a `tf.Keras` model:\r\n```\r\ndef base_model():\r\n    \r\n  input_single = layers.Input(shape=(n,))\r\n  x = layers.Dense(...)(input_single)\r\n  x = layers.Dense(...)(x)    \r\n    \r\n  return tf.keras.Model(input_single, x)\r\n\r\ndef full_model():\r\n  \r\n  input_pair = layers.Input(shape=(2,n))\r\n  x = layers.TimeDistributed(base_model())(input_pair)\r\n  x = layers.Subtract()([x[:,0],x[:,1]])\r\n  x = layers.Lambda(l2_norm)(x)\r\n\r\n  return tf.keras.Model(input_pair, x)"]}, {"number": 12019, "title": "foldr is too restrictive:  dimension 0 in both shapes must be equal", "body": "Using TF 1.2.0-rc1 on Ubuntu 16.04.\r\n\r\nThere are cases where `tf.foldr` should work, but is unable to because of a restriction that the first shape dimensions be identical for all list elements.  Below is a self-contained example that demonstrates the problem:\r\n\r\n```\r\nimport tensorflow as tf\r\nsess = tf.InteractiveSession()\r\n\r\ndef concat2(A, B):\r\n    return tf.concat([A, B], axis=0)\r\n\r\nA = tf.constant([[10,10]])             # A.shape => (1,2)\r\nB = tf.constant([[20, 20], [30, 30]])  # B.shape => (2,2)\r\n\r\nprint(concat2(A, B).eval())              # => [[10, 10], [20, 20], [30, 30]]\r\nprint(tf.foldr(concat2, [A, B]).eval())  # => ERROR!\r\n```\r\n", "comments": ["I suppose it might be possible for foldr to work on binary functions whose two inputs need not have the same dimensions, but this might have unfortunate consequences for shape inference.  @mrry would we welcome a contribution of that kind?", "Currently `tf.foldl` and `tf.foldr` converts input elems into a `Tensor`, and then the converted `Tensor` is converted into a `TensorArray`:\r\nhttps://github.com/tensorflow/tensorflow/blob/1e1b3d90295f9396a25b9cfd4c571f7949716182/tensorflow/python/ops/functional_ops.py#L100-L106\r\n\r\nIt is possible to bypass the step of `ops.convert_to_tensor()` and convert the input elems directly to a `TensorArray`. In this way, the restriction mentioned in this issue would be gone and the input elems could be any type as long as it is `Iterable`.\r\n\r\nHowever, this might be a change in behavior so I am not sure. Nevertheless I created a PR #13282 so that further discussion could be done there."]}, {"number": 12009, "title": "Expose Tensorflow Go library as cgo_library rule in Bazel", "body": "Currently it's not possible to reference @org_tensorflow//tensorflow/go:go_default_library from a BUILD file. It would be great to have this ability.", "comments": ["Currently, the go libraries are structured to be friendly to the common use of the go toolchain (i.e., the common use with `go get`). \r\n\r\nCould you elaborate a bit on the feature requested here? Is it that you have a project that uses `bazel` as the build system and want to build all of tensorflow (including the go libraries) from source after adding tensorflow as an external dependency to the `WORKSPACE` of your project? If so, it's unlikely that anyone on the TensorFlow team will get to making that work in the near future. Or is it something else?", "That's exactly what I want. If no one objects against this idea, I'll probably spend some time implementing it.", "Sure, if the change isn't particularly intrusive, we'd be happy to accept it.\r\nMarking this as contributions welcome."]}, {"number": 12001, "title": "Feature Request: Add separable_conv2d_transpose operation", "body": "Some recent papers (e.g.) have shown that transposed separable convolutions can be a great choice for decoders in encoder decoder architectures.\r\n\r\nCan you add a seperable_conv2d_transpose operation comparable to the conv2d_transpose operation?", "comments": ["Hi, I will try to work on this one.\r\n\r\n@andreas-eberle do you have specific examples in which a seperable_conv2d_transpose operation is profitable? Do you have a link to your e.g.?", "Please provide the links of the reference papers", "Sorry, didn't notice that I forgot the link. \r\n\r\nIn [The Devil is in the Decoder](https://arxiv.org/abs/1707.05847) they compare several \"deconvolution\" strategies and show that separable transposed convolution has very good performance.\r\n\r\nIn section 2.1.1 they give a short explanation about separable transposed convolution.\r\n\r\nAfaik, separable (not transposed) convolution was introduced in [Xception: Deep Learning with Depthwise Separable Convolutions](https://arxiv.org/abs/1610.02357)\r\n\r\n", "+1 on that. \r\nBesides the examples given by andreas-eberle, It is generally helpful to have operations that are not allowed to mix the channels, so that channels from different layers can be trivially combined (i.e. summed without any learned parameters).", "Hi, is anyone working on this anymore?\r\nI'd like to start work on this, if anyone can provide some supervision that would be helpful.", "useful feature to be implemented", "any updates on this ?\r\n", "I'd like to try a GAN with separable_conv2d and separable_conv2d_transpose, and i was surprised to see that separable_conv2d_transpose isn't available yet. Some have stated they started working on an implementation, how is that work going?\r\n ", "I would also be able to help with an implementation, @tjingrant have you started on it? \r\nAlso seems like the authors of [The Devil is in the Decoder](https://arxiv.org/abs/1707.05847) must have access to it.", "Any idea when there will be an implementation for separable_conv2d_transpose?", "Any update to this feature?", "Easiest to just use`tf.keras.layers.Conv2DTranspose` followed by `tf.keras.layers.DepthwiseConv2D`", "@chris-boson. This is not equivalent.  One of the major selling points of DepthwiseConv2DTranspose (if it existed) is a reduction of parameters, which would not be achieved by a transpose followed by a depthwise conv.", "@HouseOfFinwe It does in fact reduce the parameter count considerably, especially in the case of many output channels. Use filters of shape `[stride, stride]` instead of `[1, 1]` for the pointwise conv in separable convolution to avoid checkerboarding.", "+1 on this, would be highly appreciated.", "@chris-boson could you give a clearer example of using Conv2DTranspose followed by DepthwiseConv2D? Regardless, I still think a pure depthwise transpose would be even more efficient", "Any update to this feature?", "Any updates or progress on this?", "I would be interested in this", "That would be a nice addition indeed.", "Seems like a feature that makes a lot of sense", "Would this require a new op? Is it difficult because of lack of hw support? ", "Any update on this?", "Any update on this?", "Would like to see this feature implemented", "+1", "Hi, is anyone working on this currently?\r\nI'd like to work on this feature. Also can somebody please provide some resources to start from.", "Is it very different to use upsampling + separableconv ?", "@edmondja the problem with upsampling + separableconv is that it increases the number of computations compared to sep-conv2d-transpose", "Anyone find a workaround? I attempted to do a workaround with a stack of Conv2DTranspose, each with filters=1... but it was not very efficient. No promise this works but this was my attempt fwiw. \r\n\r\n\r\nclass DepthwiseConv2DTranspose(layers.Layer):\r\n    def __init__(self, filters, **kwargs):\r\n        super(DepthwiseConv2DTranspose, self).__init__(**kwargs)\r\n        self._filters = filters\r\n        self._t = []\r\n        for _ in range(filters):\r\n            self._t.append(layers.Conv2DTranspose(filters=1, kernel_size=5, strides=2, output_padding=1))\r\n\r\n    def __call__(self, img):\r\n        upsample = []\r\n        for i in range(self._filters):\r\n            t = self._t[i](img[:,:,:,i:i+1])\r\n            t = t[:,:,:,0]\r\n            upsample.append(t)\r\n        upsample = tf.stack(upsample, axis=-1)\r\n        return upsample", "Any update? ", "Here are two sample ones that do some of it, the first doesn't compile on TPU it does work on CPU \r\n ```\r\nclass Depthwise_Conv2D_Transpose(tf.keras.layers.Layer):\r\n      def __init__(self, filters,kernel_size,strides,padding='same',use_bias=False,kernel_initializer=None,name=\"\",**kwargs):\r\n          super(Depthwise_Conv2D_Transpose, self).__init__(**kwargs)\r\n          self.kernel_size = kernel_size\r\n          self.strides = strides[0]\r\n          self.padding = padding\r\n          self.use_bias = use_bias\r\n          self.kernel_init = kernel_initializer\r\n          self.lambdas =[]\r\n          for i in tf.range(filters):  \r\n             self.lambdas.append(layers.Conv2DTranspose(filters=1, kernel_size=kernel_size,strides=self.strides,padding=self.padding))\r\n          self.filters = filters\r\n          self.input_image_shape = 0\r\n          self.nm = name\r\n      def call(self, inputs):\r\n          #tf.print(inputs.shape,[-1]+[self.deconv_length(self.input_image_shape,self.strides,self.kernel_size,self.padding)]*2+[1])\r\n          inputs_channel_wise =   tf.split(inputs,self.filters, -1)#\r\n          x_outputs = [c(x) for x, c in zip(inputs_channel_wise, self.lambdas)]\r\n          \r\n          channel_wise_conv =  tf.concat(x_outputs, -1)#tf.transpose(tf.squeeze(channel_wise_conv,axis = -1),[0,2,3,1])\r\n          return channel_wise_conv\r\n```\r\n The second compiles also on TPU and works well on CPU but doesn't train on TPU\r\n```\r\nclass Depthwise_Conv2D_Transpose(tf.keras.layers.Layer):\r\n    def __init__(self, filters,kernel_size,strides,padding='same',use_bias=False,kernel_initializer=None,name=\"\",**kwargs):\r\n        super(Depthwise_Conv2D_Transpose, self).__init__(**kwargs)\r\n        self.kernel_size = kernel_size\r\n        self.strides = strides[0]\r\n        self.padding = padding\r\n        self.use_bias = use_bias\r\n        self.kernel_init = kernel_initializer\r\n        self.lambdas =[]\r\n        self.filters = filters\r\n        self.input_image_shape = 0\r\n        self.nm = name\r\n        \r\n    def deconv_length(self,dim_size, stride_size, kernel_size, padding, output_padding=None, dilation=1):\r\n\r\n        assert padding in {'same', 'valid', 'full'}\r\n        if dim_size is None:\r\n            return None\r\n\r\n        # Get the dilated kernel size\r\n        kernel_size = kernel_size + (kernel_size - 1) * (dilation - 1)\r\n\r\n        # Infer length if output padding is None, else compute the exact length\r\n        if output_padding is None:\r\n            if padding == 'valid':\r\n                dim_size = dim_size * stride_size + max(kernel_size - stride_size, 0)\r\n            elif padding == 'full':\r\n                dim_size = dim_size * stride_size - (stride_size + kernel_size - 2)\r\n            elif padding == 'same':\r\n                dim_size = dim_size * stride_size\r\n        else:\r\n            if padding == 'same':\r\n                pad = kernel_size // 2\r\n            elif padding == 'valid':\r\n                pad = 0\r\n            elif padding == 'full':\r\n                pad = kernel_size - 1\r\n\r\n            dim_size = ((dim_size - 1) * stride_size + kernel_size - 2 * pad + output_padding)\r\n\r\n        return dim_size\r\n\r\n    def build(self, input_shape):\r\n        for i in range(input_shape[-1]):  \r\n           self.lambdas.append(self.add_weight(name = self.nm +\"weights\"+ str(i),initializer=tf.keras.initializers.deserialize(self.kernel_init),shape=(self.kernel_size,self.kernel_size,1,1), trainable=True))\r\n        self.input_image_shape = input_shape[1]\r\n        #self.lambdas = tf.stack(self.lambdas,axis = 0)\r\n        self.image_shape = input_shape[-1]\r\n        super(Depthwise_Conv2D_Transpose, self).build(input_shape)\r\n    @tf.function\r\n    def call(self, inputs):\r\n        \r\n        inputs_channel_wise =   tf.split(inputs,self.image_shape, -1)\r\n\r\n        channel_wise_conv = tf.map_fn(lambda x:tf.nn.conv2d_transpose(x[0], filters=x[1], \r\n                                                                              output_shape=[tf.shape(inputs)[0]]+[self.deconv_length(self.input_image_shape,self.strides,self.kernel_size,self.padding)]*2+[1], \r\n                                                                              strides=self.strides, \r\n                                                                              padding=self.padding.upper()),(inputs_channel_wise,self.lambdas), fn_output_signature=tf.float32)\r\n        \r\n        channel_wise_conv = tf.transpose(tf.squeeze(channel_wise_conv,axis = -1),[0,2,3,1])\r\n        return channel_wise_conv  \r\n```      \r\nIf any solutions found do update thanks!    \r\n ", "any update ? \r\n"]}, {"number": 11954, "title": "Dilated convolution does not preserve tensor shape", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 14.04.5 LTS\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\n('unknown', '1.3.0-rc0')\r\n- **Python version**: \r\n2.7.12\r\n\r\n- **Exact command to reproduce**:\r\n```\r\ninput_tensor = tf.placeholder(tf.float32, (10, None, 256, 3))\r\n\r\ndilated = tf.nn.convolution(input_tensor,\r\n                            tf.zeros((3, 1, 3, 16)),\r\n                            dilation_rate=[2, 1],\r\n                            padding='SAME')\r\n\r\nprint(dilated.get_shape()) # Displays: [10, ?, ?, 16], expected [10, ?, 256, 16]\r\n```\r\n\r\n### Describe the problem\r\nThe documentation for tf.nn.convolution has the spatial dimensions of the output given as:\r\n\r\n```\r\nIf padding == \"SAME\": output_spatial_shape[i] = ceil(input_spatial_shape[i] / strides[i])\r\n```\r\n\r\nWhich suggests that input_spatial_shape[0] should not affect output_spatial_shape[1], as is the case in the code block above.\r\n\r\nThis problem arises when using dilated convolutions as part of a larger model containing recurrent layers, in which one spatial dimension is left undefined to allow for unrolling the recurrent layers out during training along the undefined dimension.\r\n\r\nThis might be related to a [previously fixed problem with undefined batch sizes](https://github.com/tensorflow/tensorflow/issues/4742).\r\n", "comments": ["@jbms could you please take a look into this. I am able to reproduce the problem.", "This is a limitation in the shape inference.  It may be difficult to fix without introducing a fair amount of additional complexity/code duplication, which is why it hasn't already been fixed.  The difficulty lies in the fact that tensorflow has special support for constant-valued tensors whose values are known at graph creation time, but it doesn't have support for partially-constant tensors, where some but not all values are known at graph creation time.  A contribution to fix this would be welcome, though.  As a workaround, you can use Tensor.set_shape to manually fix the shape.", "@yzhwang   This is a very old issue and I am not asking you to resolve it.  With the new dilated convolution support would this be magically resolved.  My guess is no, you do not have to keep this assigned to yourself after you check.", "@tfboyd I just tried with new dilated conv API and now it does show the correct shape inference.\r\n\r\n```\r\ninput_tensor = tf.placeholder(tf.float32, (10, None, 256, 3))\r\n\r\ndilated = tf.nn.conv2d(input_tensor,\r\n                            tf.zeros((3, 1, 3, 16)),\r\n                            strides=[1,1,1,1],\r\n                            dilations=[1, 2, 1, 1],\r\n                            padding='SAME')\r\n\r\nprint(dilated.get_shape())\r\n```\r\nI'm not sure how to fix the shape inference for convolution() though.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you."]}, {"number": 11846, "title": "Fractional 3d Max Pooling", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.2.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:8/5\r\n- **GPU model and memory**:Nvidia 740M\r\n\r\n### Describe the problem\r\nThere is a fractional maxpool op for 4D tensors. For medical imaging, data is available in the form of 3d images. Hence a fractional maxpool/avgpool  op for 5D tensors i.e., [batch_size, n_channels, depth, height, width] would be really useful.\r\n\r\n", "comments": ["As far as I know we're not working on this currently, but it would make a useful feature.", "I'd like to try working on this.", "What is the guideline for test coverage? Should I write tests analogous to the 2D op?", "@wuharvey Hi did you work on this? If the issue is still open and it would be a useful contribution I am interested in working on it."]}, {"number": 11689, "title": "Distributed Tensorflow Authorization", "body": "Could somebody comment on the security design of distributed tensorflow? \r\nIs there some kind of authorization in place for the grpc calls in distributed tensorflow?\r\nE.g., are the clients validated based on the IP or anything else?\r\n\r\nIf not, how difficult would it be to add authorization support and what would be the best place in the code to get started?\r\n", "comments": ["/CC @mrry, can you comment on TensorFlow's security?", "Given the comments [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/rpc/grpc_server_lib.h#L80), there has been some considerations for adding such feature in the future.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "TensorFlow is designed to run on private networks that rely on other mechanisms (firewalls, VLANs, VPC, etc.) for network security. The present implementation performs no additional authentication, authorization, or encryption of requests or responses. As the comments in `grpc_server_lib.h` suggest, it would be possible to override the gRPC server and client classes to employ encryption, but there is no API for doing this at present.", "@mrry,\r\n1. How to specify a host for the server?\r\nAs of now the source code does as follows:\r\n- starts a grpc server and binds it to every interface, host is not being used;\r\n[tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc#L194](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc#L194)\r\n- [original commit](https://github.com/tensorflow/tensorflow/commit/00986d48bb646daab659503ad3a713919865f32d#diff-20455d3b7c774e29dbf2f3eafe4ce546R98);\r\n- port option inside ClusterSpec is being used though.\r\n2. The latest commit which treated ```host:port``` better is [00986d48bb646daab659503ad3a713919865f32d](https://github.com/tensorflow/tensorflow/commit/00986d48bb646daab659503ad3a713919865f32d):\r\n- Here AddListeningPort takes server_addr based on task specification;\r\n- [TranslateTask](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/rpc/grpc_channel.cc#L209);\r\n- [Commit That Removes That Logic](https://github.com/tensorflow/tensorflow/commit/77887d9f79d0573ceb6254cd322fdbe79f106978).", "> TensorFlow is designed to run on **private networks that rely on other mechanisms** (firewalls, VLANs, VPC, etc.) for network security. The present implementation performs no additional authentication, authorization, or encryption of requests or responses. As the comments in `grpc_server_lib.h` suggest, it would be possible to override the gRPC server and client classes to employ encryption, but there is no API for doing this at present.\r\n\r\nIs there a better option rather than a client source addr pinning?\r\n```\r\nsudo iptables -A INPUT -p tcp --dport <tensorflow_server_port> ! -s <tensorflow_client_host> -j REJECT --reject-with icmp-port-unreachable\r\n```"]}, {"number": 11604, "title": "Unable to compile a quantized graph using XLA AOT?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: Master\r\n- **Python version**: 2.7.13\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: 8.0/5.1\r\n- **GPU model and memory**: GTX 860M\r\n- **Exact command to reproduce**: \r\n\r\n`bazel build -c opt --cxxopt='-std=c++11' --linkopt='-lm'    --cpu=armeabi-v7a    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --crosstool_top=//external:android/crosstool    //tensorflow/compiler/aot:inception_v3 --verbose_failures`\r\n\r\n### Describe the problem\r\nI am currently trying to use `tfcompile` to compile a quantized inception_v3 model for android, following the instructions given in the documentation [here](https://www.tensorflow.org/performance/xla/tfcompile), but I have gotten this error below:\r\n\r\n```\r\nINFO: Found 1 target...\r\nERROR: /home/kwotsin/Android/tensorflow/tensorflow/compiler/aot/BUILD:11:1: Executing genrule //tensorflow/compiler/aot:gen_inception_v3 failed: bash failed: error executing command \r\n  (cd /home/kwotsin/.cache/bazel/_bazel_kwotsin/655cf5567faa2deb9e3725ec794eb35d/execroot/tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=:/usr/local/cuda/lib64:/usr/local/cuda/lib64 \\\r\n    PATH=/usr/local/cuda/bin:/usr/local/cuda/bin:/home/kwotsin/bin:/home/kwotsin/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/kwotsin/Android/Sdk/tools:/home/kwotsin/Android/Sdk/platform-tools \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL=0 \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/compiler/aot/tfcompile --graph=tensorflow/compiler/aot/inception_v3.pb --config=tensorflow/compiler/aot/inception_v3.config.pbtxt --entry_point=__tensorflow_compiler_aot__inception_v3 --cpp_class=inception_v3_cpp --target_triple=armv7-none-android --out_header=bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/tensorflow/compiler/aot/inception_v3.h --out_object=bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/tensorflow/compiler/aot/inception_v3.o '): com.google.devtools.build.lib.shell.AbnormalTerminationException: Process terminated by signal 6.\r\n2017-07-19 19:53:26.407268: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-19 19:53:26.407310: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-19 19:53:26.407326: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-19 19:53:26.407330: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-19 19:53:26.407333: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-19 19:53:26.424064: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n2017-07-19 19:53:26.426662: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Not found: No registered 'Const' OpKernel for XLA_CPU_JIT devices compatible with node InceptionV3/Conv2d_1a_3x3/weights/read/_224__cf__224_quantized_const = Const[dtype=DT_QUINT8, value=Tensor<type: quint8 shape: [3,3,3,32] values: [[[97 115 118]]]...>]()\r\n\t (OpKernel was found, but attributes didn't match)\r\n\t.  Registered:  device='XLA_CPU_JIT'; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BOOL]\r\n  device='XLA_GPU_JIT'; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BOOL]\r\n  device='CPU'\r\n\r\n\t [[Node: InceptionV3/Conv2d_1a_3x3/weights/read/_224__cf__224_quantized_const = Const[dtype=DT_QUINT8, value=Tensor<type: quint8 shape: [3,3,3,32] values: [[[97 115 118]]]...>]()]]\r\n2017-07-19 19:53:26.429093: F tensorflow/compiler/aot/tfcompile_main.cc:154] Non-OK-status: status status: Not found: No registered 'Const' OpKernel for XLA_CPU_JIT devices compatible with node InceptionV3/Conv2d_1a_3x3/weights/read/_224__cf__224_quantized_const = Const[dtype=DT_QUINT8, value=Tensor<type: quint8 shape: [3,3,3,32] values: [[[97 115 118]]]...>]()\r\n\t (OpKernel was found, but attributes didn't match)\r\n\t.  Registered:  device='XLA_CPU_JIT'; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BOOL]\r\n  device='XLA_GPU_JIT'; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BOOL]\r\n  device='CPU'\r\n\r\n\t [[Node: InceptionV3/Conv2d_1a_3x3/weights/read/_224__cf__224_quantized_const = Const[dtype=DT_QUINT8, value=Tensor<type: quint8 shape: [3,3,3,32] values: [[[97 115 118]]]...>]()]]\r\n/bin/bash: line 1:  6904 Aborted                 (core dumped) bazel-out/host/bin/tensorflow/compiler/aot/tfcompile --graph=tensorflow/compiler/aot/inception_v3.pb --config=tensorflow/compiler/aot/inception_v3.config.pbtxt --entry_point=__tensorflow_compiler_aot__inception_v3 --cpp_class=inception_v3_cpp --target_triple=armv7-none-android --out_header=bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/tensorflow/compiler/aot/inception_v3.h --out_object=bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/tensorflow/compiler/aot/inception_v3.o\r\nTarget //tensorflow/compiler/aot:inception_v3 failed to build\r\nINFO: Elapsed time: 0.339s, Critical Path: 0.18s\r\n```\r\n\r\nDespite the \"SSE4.1 etc.\" instructions that popped up, I made sure that I configured the tensorflow installation with XLA enabled, so it shouldn't have popped up. \r\n\r\nAlso, my quantized graph was created using the Graph Transform Tool with the following command, producing a graph that worked exactly as expected:\r\n\r\n```\r\n/home/kwotsin/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=./frozen_model.pb \\\r\n--out_graph=./quantized_model.pb \\\r\n--inputs='Placeholder_only' \\\r\n--outputs='InceptionV3/Predictions/Softmax' \\\r\n--transforms='\r\n  add_default_attributes\r\n  strip_unused_nodes(type=float, shape=\"1,299,299,3\")\r\n  fold_constants(ignore_errors=true)\r\n  fold_batch_norms\r\n  fold_old_batch_norms\r\n  quantize_weights\r\n  strip_unused_nodes\r\n  sort_by_execution_order'\r\n```\r\n\r\nIs XLA AOT compilation of quantized models currently supported? Because when I tried to build with a frozen, non-quantized graph, I got the correct output - a cpp object file and a header file. I thought it would be nice if XLA AOT could be used concurrently with a quantized model to obtain the maximum level of mobile optimization.", "comments": ["I don't think quantized types are currently supported, but @tatatodd might have a better answer.\r\n", "Quantized types aren't yet supported by the XLA compiler, but we do plan to bring quantization support to XLA in the future. If you have ideas on this front, we'd definitely love to hear from you.", "Thank you all for the comments. Hypothetically, what would be required to get this compilation done? I think I'm probably not technically skilled enough to get this done right now but I'm hoping to try out this possibility in the future, so it would be great if I can have some idea of how this could be done.", ":+1: on being able to use quantized ops with XLA AOT.", "I will start working implementing support for quantization in XLA in the coming weeks, and hopefully update here with more details / PRs.", "@kayzhu Hi, could you please confirm if there is any update on implementing support for quantization in XLA ? ", "Bumping this thread - are there any updates for XLA quantization? This would be a fantastic feature and I'm happy to help contribute if there is any work that I can do!", "Sorry for the very late reply.\r\n\r\nI have some change to add the semantic support for quantization in XLA last year. However, it alone won't get the whole quantization story very far: there is a big missing piece in quantization rewriter that rewrites a trained TF graph to a quantized TF graph, that in turn can be lowered to a quantized XLA graph, and likely some necessary additions to CPU backend's codegen.\r\n\r\nWe had some discussions along these directions, though the stakeholders were not able to prioritize it due to various constraints. My hope is that there will be more concrete steps and milestones in the entire quantization story around the TF ecosystem in 2019 Q1/Q2. I would continue to be involved in the discussion to the best of my capacity, but for now I will unassign myself.\r\n\r\n\r\n"]}, {"number": 11541, "title": "tfcompile won't work with graph that has no inputs", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.2.1 (git version v1.2.0-2140-g85d4102)\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: 0.5.2\r\n- **CUDA/cuDNN version**: 5.1.5\r\n- **GPU model and memory**: GeForce GTX 970 4GB\r\n\r\n### Describe the problem\r\n\r\nI have been working on a char-rnn like network that is trained on a corpus of text, and then produces similar text as output. I'm trying to compile the graph, but tfcompile asserts that the list of feeds is non-empty. However, since for my network there are no inputs to specify as feeds, tfcompile fails.\r\n\r\n### Source code / logs\r\n\r\nThe configuration I tried to use looks like this:\r\n\r\n```\r\nfetch {\r\n  id { node_name: \"predictions\" }\r\n}\r\n```\r\n\r\nThe error message:\r\n```\r\nINVALID ARGUMENTS: feeds and fetches must be specified\r\n```\r\nwhich comes from https://github.com/tensorflow/tensorflow/blob/85d4102862c781af2346b4aa568054522e8946ea/tensorflow/compiler/aot/tfcompile_util.cc#L119\r\n\r\nI don't really know anything about how the compiler works, but is this check in place because it was assumed that you'd always want to have an input, or because of some other limitation that would cause the compilation to fail with no feeds?", "comments": ["@tatatodd mind taking a look?\r\n\r\n@LordPython Do you have more information on how you arrived at this error, perhaps the script that generated the graph?", "@LordPython The check is in-place because it was assumed you'd always want to have an input.\r\n\r\nI don't think there's any fundamental reason why a computation with no feeds should fail; you might try just commenting-out the check.  But there might be other bits of code that need to be fixed up for the no-feeds use case.\r\n\r\nI'm assigning this to myself, and will update when a fix is in.", "Actually, I thought about this some more, and realized that there is a problem.  If your computation has no feeds, it's likely that the values you're fetching are constant.  And we don't currently support AOT-compilation for completely constant results.\r\n\r\n@LordPython a workaround would be to feed in your weights, rather than baking them in as constants.  Would that work for you?", "The values I'm fetching aren't constant as the graph involves a random sampling operation. I quickly try just commenting out the check, but I ran into #11275 (or something similar, I'm using seq2seq.dynamic_decode), so it looks I won't actually be able to compile my particular graph until there's more support for loops anyway. If that happens before this, then I think I've got a parameter or two that it could make sense to turn into an input/fetch instead of a constant as a work around.", "Having the same issue. I have a computation with no feeds and don't see a fundamental reason why this wouldn't work. Are there updates on this?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@LordPython is this issue still current?", "@drpngx Yes the issue still exists; tfcompile still requires at least one feed.  Way back in July 2017 I had tried to make a quick change to remove this requirement, but it was more complicated than I had time for, so I didn't pursue it further.\r\n\r\nThis should be a rare use-case, and there's a workaround by passing in a dummy feed for some part of your computation.  I'll mark contributions welcome in case anyone wants to take a shot at it.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you."]}, {"number": 11447, "title": "Feature Request: sparse matrix triangular solver", "body": "I have just written about this in stack overflow,\r\nbut I just realized it would be more relevant if be put here.\r\n\r\nI think it would be very helpful if we could do solve linear equation Ax = b,\r\nwhere A has a sparse matrix representation, for example,\r\ncontaining lower triangular entries for a banded symmetric-matrix.\r\n\r\nBecause, AFAIK, in this case, we need to convert the sparse matrix A\r\nwith the `tf.sparse_to_dense()`, to run the `tf.matrix_triangular_solve()`.\r\n\r\nBut, if the dimension of A is very large, e.g., about 16000x16000,\r\nand with very sparse entries, e.g., about 46000 non-zeros (0.018%),\r\nit would take a huge amount of memory.\r\n\r\nOr maybe there is another way to do it in Tensorflow?", "comments": ["@ysuematsu Please take a look at this question / feature request regarding sparse tensors.", "I am also interested in such feature. This is currently available with [Scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.spsolve.html#scipy.sparse.linalg.spsolve) and it would be awsome to have it in tensorflow!", "Any updates @ysuematsu?", "@rmlarsen does this fall into the category of \"more general linear algebra support\"? Could you please answer or redirect to someone who can talk about our plans here?", "I would also be very interested in such a feature! I would need GPU-Support for my use-case.", "I would like this feature as well. Are there deep learning frameworks that implement sparse solvers on the GPU? It seems to be possible, looking at the cuSPARSE framework by NVIDIA."]}]