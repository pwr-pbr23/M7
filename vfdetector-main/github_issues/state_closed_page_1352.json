[{"number": 12520, "title": "[XLA] Replace TEST_F with XLA_TEST_F to allow for disabling", "body": "This will allow any of the tests in the CopyTest suite to be disabled by using a disable manifest.\r\n", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please"]}, {"number": 12519, "title": "Bug on the gradients graph computation - C++ API", "body": "The gradient computation in the C++ API works as follow.\r\nLet's say that we have the following graph:\r\n```\r\n                Tanh\r\n                  |\r\n         Assign MatMul\r\n         /    \\ /    \\\r\n      Const   Var   Const\r\n```\r\nHere our Output is Tanh and our Input is Var. The gradient method does a BFS from Var to Tanh to count for each node how many backprop we should expect. If a node has two outgoing edges, it will be ready only when both would have been backpropagated and the gradient summed. In our case, Var has 2, Assign 0, MatMul 1. These values are saved into a pending array.\r\n\r\nThen the gradient method does a BFS from Tanh to Var, this is the actual backpropagation, the error is backpropagated until we reach Var. When a Node is reached, pending is decreased by one and if pending == 0, the Node is ready and is added to the queue of Nodes to be processed. If it is not ready, it will be reached again in the future and will be ready at some point.\r\n\r\nIn our case, doing a BFS from Var to Tanh give us 2 expected gradients, one from Assign and one from MatMul, whereas doing a BFS from Tanh to Var, we will reach Var only once, because we can't reach Assign from Tanh. In that case, the pending count will never reach Zero, Var will never be ready and the BFS will end. At the end, a check is done and if pending nodes are still there, an error is raised.\r\n\r\nThe PR https://github.com/tensorflow/tensorflow/pull/12397 updates the gradient method to ignore nodes that have 0 outgoing edges and are not in the list of Output (Tanh is the only one in the list of outputs in our case).\r\n\r\nAs @skye pointed it out as a comment in the PR, there is other cases where there is still a problem. I am working on it.", "comments": ["I updated the PR to handle the case outlined.", "@skye Skye can you close this out once the PR is merged? Thanks..", "@andydavis1 I'll do it if necessary.", "Merged."]}, {"number": 12518, "title": "core dump when running tensorflow", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n    yes.\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n   Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**: 1.2.1\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: GeForce GTX 1070 8GB gpu memory\r\n- **Exact command to reproduce**:\r\n \r\n\r\n### Describe the problem\r\nhttps://github.com/ilovin/lstm_ctc_ocr/tree/master\r\npython lstm_ocr.py\r\nwhen I run it, core dumps.\r\nI use gdb and got this message:\r\n\r\n(gdb) file python\r\nReading symbols from python...(no debugging symbols found)...done.\r\n(gdb) run lstm_ocr.py \r\nStarting program: /home/lili/tf1.2.1-py3/bin/python lstm_ocr.py\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n[New Thread 0x7ffff3a36700 (LWP 350)]\r\n[New Thread 0x7ffff3235700 (LWP 351)]\r\n[New Thread 0x7ffff0a34700 (LWP 352)]\r\n[New Thread 0x7fffee233700 (LWP 353)]\r\n[New Thread 0x7fffe9a32700 (LWP 354)]\r\n[New Thread 0x7fffe9231700 (LWP 355)]\r\n[New Thread 0x7fffe6a30700 (LWP 356)]\r\n[Thread 0x7ffff3235700 (LWP 351) exited]\r\n[Thread 0x7fffe6a30700 (LWP 356) exited]\r\n[Thread 0x7fffe9231700 (LWP 355) exited]\r\n[Thread 0x7fffe9a32700 (LWP 354) exited]\r\n[Thread 0x7fffee233700 (LWP 353) exited]\r\n[Thread 0x7ffff0a34700 (LWP 352) exited]\r\n[Thread 0x7ffff3a36700 (LWP 350) exited]\r\n[New Thread 0x7fffe6a30700 (LWP 364)]\r\n[New Thread 0x7fffe9231700 (LWP 365)]\r\n[New Thread 0x7fffe9a32700 (LWP 366)]\r\n[New Thread 0x7fffee233700 (LWP 367)]\r\n[New Thread 0x7fffa1d72700 (LWP 368)]\r\n[New Thread 0x7fffa1571700 (LWP 369)]\r\n[New Thread 0x7fffa0d70700 (LWP 371)]\r\n[New Thread 0x7fff9bfff700 (LWP 372)]\r\nloading train data, please wait--------------------- get image:  128000\r\nloading validation data, please wait--------------------- get image:  500\r\n2017-08-23 15:42:02.794111: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-23 15:42:02.794131: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-23 15:42:02.794135: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-23 15:42:02.794161: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-23 15:42:02.794166: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n[New Thread 0x7fffaa462700 (LWP 374)]\r\n[New Thread 0x7fffa9c61700 (LWP 375)]\r\n[New Thread 0x7fffa9460700 (LWP 376)]\r\n[New Thread 0x7fff9a13e700 (LWP 377)]\r\n[New Thread 0x7fff9993d700 (LWP 378)]\r\n[New Thread 0x7fff9913c700 (LWP 379)]\r\n[New Thread 0x7fff9893b700 (LWP 380)]\r\n[New Thread 0x7fff7ffff700 (LWP 381)]\r\n\r\nThread 1 \"python\" received signal SIGSEGV, Segmentation fault.\r\n__GI___pthread_mutex_lock (mutex=0x3028) at ../nptl/pthread_mutex_lock.c:67\r\n67      ../nptl/pthread_mutex_lock.c: no such file or directory.\r\n\r\n\r\n### Source code / logs\r\n\r\n", "comments": ["when I use a cpu version in another vituralenv. it's fine. So it seems this problem relates to gpu", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12517, "title": "building from source for iOS crashes because of nsync compilation", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.12.5\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.3\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: sh build_all_ios.sh\r\n\r\n### Describe the problem\r\nI am building tensorflow from source for iOS. it crashes when executing\r\n\r\n`TARGET_NSYNC_LIB=tensorflow/contrib/makefile/compile_nsync.sh -t ios`\r\n\r\nFirst, it looks for a specific version of the iPhoneSimulator (rather than the latest). When I change my symlink to point to the correct version, it crashes saying that <string.h> cannot be found\r\n\r\nNote: the previous call to `HOST_NSYNC_LIB=`tensorflow/contrib/makefile/compile_nsync.sh` executes correctly\r\n\r\n### Source code / logs\r\n", "comments": ["**SOLVED**\r\n\r\nIt is necessary to update both compile_nsync.sh and downloads/nsync/builds/i386.ios.c++11/Makefile with the actual system version of the iPhoneSimulator SDK. I had 10.3, which was correctly detected by the rest of the process, while nsync has the value 10.0 hardcoded\r\n"]}, {"number": 12516, "title": "Update head.py", "body": "-Use tf.metrics instead of contrib version\r\n-Use core lookup instead of contrib version (Nvm, another PR)\r\n-Remove TODO in tensorflow/contrib/lookup/__init__.py since already done\r\n", "comments": ["Can one of the admins verify this patch?", "@alanyee, thanks for your PR! By analyzing the history of the files in this pull request, we identified @jart, @charlesnicholson and @martinwicke to be potential reviewers.", "Jenkins, test this please"]}, {"number": 12515, "title": "Inconsistent use of naming `Backprop`", "body": "### Describe the problem\r\nTensorflow code has two types of naming `Backprop` and `BackProp`. How about to rename all `BackProp` to `Backprop` for consistency?\r\n\r\n### Source code / logs\r\n- `Backprop` in code : [control_flow_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/control_flow_ops.py#L909), [op_level_cost_estimator.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/costs/op_level_cost_estimator.cc#L672)", "comments": ["Hi! I want to make this my first contribution to tensorflow. Has anyone started working on this?", "Hey @ngetahun, I just started working on this. I can look for another starter issue if you want to pick this up. Saw your comment after I forked. Let me know. Doing a `grep -rn . -e 'BackProp'` lists some more places other than those mentioned above that we might want to rename from BackProp to Backprop.", "Closed via https://github.com/tensorflow/tensorflow/pull/12623"]}, {"number": 12514, "title": "incorrect gradient of reduce_prod(tf.complex*)", "body": "### Describe the problem\r\nTensorflow computes the wrong result for the following gradient:\r\n```python\r\nimport tensorflow as tf\r\nx = tf.Variable(1.0)\r\nE = tf.real(tf.reduce_prod(tf.complex( [x,x], [2*x,2*x] )))\r\nsess = tf.Session()\r\nsess.run(tf.variables_initializer([x]))\r\nsess.run(tf.gradients(E,x))\r\n```\r\nTensorflow returns 10.0\r\nThe correct result is -6 since:\r\n```\r\nE = real((x+2i*x)^2) = real((1+2i)^2) * x^2 = real(1+4i-4) * x^2 = -3*x^2\r\ndE/dx = -6*x = -6 for x=1\r\n```\r\nBelow is mathematically equivalent code for E, for which Tensorflow returns the correct result of -6.0:\r\n```python\r\nE = tf.real( tf.complex(x,2*x) * tf.complex(x,2*x) )\r\nE = tf.real(tf.exp(tf.reduce_sum(tf.log(tf.complex( [x,x], [2*x,2*x] )))))\r\n```\r\n\r\n### System information\r\nLinux distribution = Arch Linux (up to date)\r\nTensorFlow was installed from the Arch Linux package python-tensorflow\r\nI'm using an x86_64 CPU. I'm not using my GPU.\r\nnumpy (1.13.1)\r\nprotobuf (3.3.2)\r\ntensorflow (1.3.0)\r\npython (3.6.2)", "comments": ["It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@rmlarsen Any timeline on when this might be looked at?\r\n@brianwa84 - you might be interested in this \r\n\r\nFor others that run into this issue, you can use `gradient_override_map`. Ex **(please note that this has not been unit tested)**:\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops import array_ops\r\nfrom tensorflow.python.ops import math_ops\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.ops.math_grad import _safe_shape_div\r\n\r\n@tf.RegisterGradient(\"ModifiedProdGrad\")\r\ndef _ModifiedProdGrad(op, grad):\r\n    \"\"\"Gradient for Prod.\"\"\"\r\n    # The gradient can be expressed by dividing the product by each entry of the\r\n    # input tensor, but this approach can't deal with zeros in the input.\r\n    # Here, we avoid this problem by composing the output as a product of two\r\n    # cumprod operations.\r\n\r\n    input_shape = array_ops.shape(op.inputs[0])\r\n    # Reshape reduction indices for the case where the parameter is a scalar\r\n    reduction_indices = array_ops.reshape(op.inputs[1], [-1])\r\n\r\n    # Expand grad to full input shape\r\n    output_shape_kept_dims = math_ops.reduced_shape(input_shape, op.inputs[1])\r\n    tile_scaling = _safe_shape_div(input_shape, output_shape_kept_dims)\r\n    grad = array_ops.reshape(grad, output_shape_kept_dims)\r\n    grad = array_ops.tile(grad, tile_scaling)\r\n\r\n    # Pack all reduced dimensions into a single one, so we can perform the\r\n    # cumprod ops. If the reduction dims list is empty, it defaults to float32,\r\n    # so we need to cast here.  We put all the shape-related ops on CPU to avoid\r\n    # copying back and forth, and since listdiff is CPU only.\r\n    with ops.device(\"/cpu:0\"):\r\n        rank = array_ops.rank(op.inputs[0])\r\n        reduction_indices = (reduction_indices + rank) % rank\r\n        reduced = math_ops.cast(reduction_indices, dtypes.int32)\r\n        idx = math_ops.range(0, rank)\r\n        other, _ = array_ops.setdiff1d(idx, reduced)\r\n        perm = array_ops.concat([reduced, other], 0)\r\n        reduced_num = math_ops.reduce_prod(array_ops.gather(input_shape, reduced))\r\n        other_num = math_ops.reduce_prod(array_ops.gather(input_shape, other))\r\n    permuted = array_ops.transpose(op.inputs[0], perm)\r\n    permuted_shape = array_ops.shape(permuted)\r\n    reshaped = array_ops.reshape(permuted, (reduced_num, other_num))\r\n\r\n    # Calculate product, leaving out the current entry\r\n    left = math_ops.cumprod(reshaped, axis=0, exclusive=True)\r\n    right = math_ops.cumprod(reshaped, axis=0, exclusive=True, reverse=True)\r\n    y = array_ops.reshape(tf.conj(left) * tf.conj(right), permuted_shape)\r\n\r\n    # Invert the transpose and reshape operations.\r\n    # Make sure to set the statically known shape information through a reshape.\r\n    out = grad * array_ops.transpose(y, array_ops.invert_permutation(perm))\r\n    return array_ops.reshape(out, input_shape), None\r\n```\r\n\r\nWith TF gradient:\r\n```python\r\nwith tf.Graph().as_default() as g:\r\n    x = tf.Variable(1.0)\r\n    E = tf.real(tf.reduce_prod(tf.complex( [x,x], [2*x,2*x] )))\r\n    with tf.Session() as sess:\r\n        sess.run(tf.variables_initializer([x]))\r\n        print(sess.run(tf.gradients(E,x)))\r\n\r\n>>> [10.0]\r\n```\r\n\r\nWith modified gradient:\r\n```python\r\nwith tf.Graph().as_default() as g:\r\n    with g.gradient_override_map({\"Prod\": \"ModifiedProdGrad\"}):\r\n        x = tf.Variable(1.0)\r\n        E = tf.real(tf.reduce_prod(tf.complex( [x,x], [2*x,2*x] )))\r\n        with tf.Session() as sess:\r\n            sess.run(tf.variables_initializer([x]))\r\n            print(sess.run(tf.gradients(E,x)))\r\n\r\n>>> [-6.0]\r\n```\r\n", "Also note that the title of this issue should be \"incorrect gradient of reduce_prod\"", "Nagging Assignee @rmlarsen: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 12513, "title": "make Dimension be compatible with integer", "body": "The PR is another solution to fix #11974 .  I believe that it is better than #12127 .\r\n\r\n### What changes were proposed in this pull request?\r\n\r\nMake Dimension to be compatible with integer, so `[1, Dimension(2)]` will be casted to `[1, 2]` automatically.\r\n\r\n### How was this patch tested?\r\n\r\n+ [x] add a doctest. \r\n+ [x] add an unit test.\r\n+ [x] pass all unit tests.\r\n", "comments": ["Can one of the admins verify this patch?", "cc @yaroslavvb @pmccarter @martinwicke , who I believe are interested in this PR. Could you take a look please?", "Can one of the admins verify this patch?", "I am ready to approve this when the change to ops.pbtxt gets reverted.", "Jenkins, test this please.", "Jenkins, test this please.", "Thanks. The new commits are modified on my personal laptop. It is no surprise if some unit tests are broken. If so, I'll fix them on next Monday.", "It seems a trivial mistake, and has been fixed. Could you retest the PR? Thanks.", "Jenkins, test this please.", "Thanks, @alextp .  All tests pass. "]}, {"number": 12512, "title": "NaNs during training with `tf.contrib.rnn.LayerNormBasicLSTMCell`", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.3\r\n- **Python version**:  3.5\r\n- **CUDA/cuDNN version**: CUDA 8.0 and CuDNN 6 (although I can replicate without a GPU)\r\n- **GPU model and memory**: Nvidia K80 (from Amazon P2 instance)\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nWhen training a model using `tf.contrib.rnn.LayerNormBasicLSTMCell`, sometimes my weights go to `nan`, even though the training data looks perfectly innocent.\r\n\r\n~~I have **not** seen this with Tensorflow 1.2.1, which leads me to suspect that there's been a regression somewhere, but I could've just been luckier (\ud83c\udf40) with TF 1.2~~ (nevermind -- I have reproduced this with TF 1.2.1)\r\n\r\n### Source code / logs\r\n\r\nI've created two examples of this in https://github.com/alanhdu/tensorflow-12512 (clone the repo, enter a folder, and run `test.py` or build and run the `Dockerfile`. The key line(s) there are:\r\n\r\n```python\r\nprint([np.isfinite(v).all() for v in sess.run(tf.trainable_variables())])\r\nsess.run(train_step, feed_dict)\r\nprint([np.isfinite(v).all() for v in sess.run(tf.trainable_variables())])\r\n```\r\n\r\nThe first print statement prints all `True`s, which is good -- but after the training step, suddenly some of the weights have `nan`s in them (and hence there's one `False` in the second print statement).", "comments": ["Hm... after slogging through `tfdbg`, the first example looks like an overflow thing (although the gradient seems absurdly high)\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.training import training_ops\r\n\r\ngrad = np.array(\r\n    [\r\n        -7.22536593e+16, -2.11869319e+16, 1.31751609e+19, 6.53208027e+19,\r\n        8.03179962e+19, 2.80957841e+16, 1.08204829e+16, -5.71410878e+15,\r\n        -1.96436248e+15, 2.08896901e+16, -9.15358559e+19, 1.59031152e+19,\r\n        -1.18680801e+19, -8.42016120e+19, 4.48369031e+16, 2.01818666e+16,\r\n        3.37646465e+15, 1.25098911e+16, 2.19448051e+19, 1.66769521e+16,\r\n        3.23241365e+16, 4.21188462e+18, -4.90144856e+18, 2.47123573e+19,\r\n        2.10178466e+15, 1.37156984e+19, -1.02427969e+16, -6.87006047e+19,\r\n        -8.75682902e+19, 3.87922260e+19, 7.33888846e+15, -4.27278298e+15,\r\n        -6.55280261e+15, -1.24261527e+19, -2.42708770e+14, 1.20307027e+16,\r\n        -7.98193545e+19, 5.26116710e+19, 1.41174265e+16, 1.92888710e+19,\r\n        -3.83293183e+14, 1.04124438e+16, -7.68158516e+18, 3.37947113e+15,\r\n        1.91912045e+16, -6.33320791e+15, -4.76087407e+15, 6.54729217e+15,\r\n        2.32993309e+19, -4.43404496e+19, 3.57337199e+16, 4.92618898e+15,\r\n        2.30337893e+16, -6.57711578e+16, -6.21250215e+15, 3.39375055e+15,\r\n        4.12803196e+19, 4.95062884e+16, 4.91424489e+16, -1.25883009e+19,\r\n        6.75019995e+16, -1.07368472e+20, -1.72674288e+19, -4.15710970e+18,\r\n        5.90043635e+19, 2.97534074e+15, 9.87953683e+19, -1.51910824e+16,\r\n        9.75750255e+16, 2.14883079e+15, 9.92727718e+18, -5.58309663e+19,\r\n        6.07305369e+16, 4.11130619e+19, -2.76947914e+19, 1.46329901e+16,\r\n        9.57690131e+18, 3.04936960e+15, 3.83920426e+16, 7.89846668e+19,\r\n        -2.52319529e+18, 4.89968624e+16, 2.77158535e+15, -9.70234459e+18,\r\n        7.63919392e+19, -7.99243622e+19, 7.39167363e+19, 1.37408169e+16,\r\n        -5.07268442e+19, 1.93819755e+19, -1.04015647e+16, -5.94786560e+15,\r\n        7.09745708e+15, -5.47805236e+19, -2.97251429e+19, 1.26310253e+16\r\n    ],\r\n    dtype=np.float32)\r\n\r\nvar = tf.get_variable(\"var\", [96], initializer=tf.zeros_initializer)\r\nm = tf.get_variable(\"m\", [96], initializer=tf.zeros_initializer)\r\nv = tf.get_variable(\"v\", [96], initializer=tf.zeros_initializer)\r\n\r\nbeta1_power = tf.constant(0.8999999761581421)\r\nbeta2_power = tf.constant(0.9990000128746033)\r\nlr = tf.constant(0.0010000000474974513)\r\nbeta1 = tf.constant(0.8999999761581421)\r\nbeta2 = tf.constant(0.9990000128746033)\r\nepsilon = tf.constant(9.99999993922529e-09)\r\n\r\nop = training_ops.apply_adam(var, m, v, beta1_power, beta2_power, lr, beta1,  beta2, epsilon, grad)\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(op)\r\nprint(sess.run(var))   # see the NaNs\r\n```\r\n\r\nThere's definitely another problem lurking here though, because I've also seen `NaN`s even with gradient clipping (which should prevent the overflow) -- I'll report back with more details.\r\n\r\nEDIT: Posted second test case with gradient clipping", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@ali01 I'm happy to create a question on StackOverflow too, but:\r\n\r\nI'm using built-in layers from Tensorflow `tf.layers.dense` and `tf.contrib.rnn.LayerNormBasicLSTMCell` on perfectly normal data and am getting garbage output (all `NaN`s). How is that not a bug? If there's a numerical stability problem here, it almost certainly exists inside the Tensorflow codebase.", "did you solve this problem now?", "I have the same issue with tf.contrib.rnn.LayerNormBasicLSTMCell and the loss is NAN and the model's performance is worse."]}, {"number": 12511, "title": "remove redundant line in fused batch norm", "body": "", "comments": ["@Corea, thanks for your PR! By analyzing the history of the files in this pull request, we identified @zhangyaobit, @taehoonlee and @jlebar to be potential reviewers.", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Can one of the admins verify this patch?", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please"]}, {"number": 12510, "title": "Support layer wise batch normalization parameter in tensorflow/contrib estimators", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:   yes\r\n- **OS Platform and Distribution:    Linux Ubuntu 14.04)\r\n- **TensorFlow installed from (source or binary)**: installed from source\r\n- **TensorFlow version (use command below)**:   1.2.0\r\n- **Python version**: Python 2.7.6\r\n- **Bazel version (if compiling from source)**: 0.5.2\r\n- **CUDA/cuDNN version**: \r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nTensorflow's current DNN classifier, regressor do not provide support to plugin in  layer-wise normalization function. This issue is fired to provide support to add a layer-wise norm func parameter in their constructors.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nFollowing is a snippet of my proposed update in DNNClassifier. In case this change passes your review, I will submit the full code change including updates to all other related estimators:\r\n\r\ndiff\r\n--git a/tensorflow/contrib/learn/python/learn/estimators/dnn.py b/tensorflow/contrib/learn/python/learn/estimators/dnn.py\r\nindex cb15ef2..a3d9c01 100644\r\n--- a/tensorflow/contrib/learn/python/learn/estimators/dnn.py\r\n+++ b/tensorflow/contrib/learn/python/learn/estimators/dnn.py\r\n@@ -127,6 +127,10 @@ def _dnn_model_fn(features, labels, mode, params, config=None):\r\n       params.get(\"input_layer_min_slice_size\") or 64 << 20)\r\n   num_ps_replicas = config.num_ps_replicas if config else 0\r\n   embedding_lr_multipliers = params.get(\"embedding_lr_multipliers\", {})\r\n+  layer_norm_func = params.get(\"layer_norm_func\")\r\n+  layer_norm_params = params.get(\"layer_norm_params\", {})\r\n+\r\n+  layer_norm_params[\"mode\"] = mode\r\n \r\n   features = _get_feature_dict(features)\r\n   parent_scope = \"dnn\"\r\n@@ -168,6 +172,8 @@ def _dnn_model_fn(features, labels, mode, params, config=None):\r\n             net,\r\n             num_hidden_units,\r\n             activation_fn=activation_fn,\r\n+           normalizer_fn=layer_norm_func,\r\n+           normalizer_params=layer_norm_params,\r\n             variables_collections=[parent_scope],\r\n             scope=hidden_layer_scope)\r\n         if dropout is not None and mode == model_fn.ModeKeys.TRAIN:\r\n@@ -297,6 +303,8 @@ class DNNClassifier(estimator.Estimator):\r\n                weight_column_name=None,\r\n                optimizer=None,\r\n                activation_fn=nn.relu,\r\n+              layer_norm_func=None,\r\n+              layer_norm_params=None,\r\n                dropout=None,\r\n                gradient_clip_norm=None,\r\n                enable_centered_bias=False,\r\n@@ -372,6 +380,8 @@ class DNNClassifier(estimator.Estimator):\r\n             \"optimizer\": optimizer,\r\n             \"activation_fn\": activation_fn,\r\n             \"dropout\": dropout,\r\n+           \"layer_norm_func\": layer_norm_func,\r\n+           \"layer_norm_params\": layer_norm_params,\r\n             \"gradient_clip_norm\": gradient_clip_norm,\r\n             \"embedding_lr_multipliers\": embedding_lr_multipliers,\r\n             \"input_layer_min_slice_size\": input_layer_min_slice_size,\r\n\r\n\r\n", "comments": ["It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "We evaluated adding batchnorm into DNNClassifier/Regressor. For the sake of minimal configuration parameters, we decided not to add. As a workaround users can copy the model_fn of DNNClassifier and modifies it to add batchnorm."]}, {"number": 12509, "title": "remove unused code, it does not happen", "body": "I remove some implements for `GetDeviceLocalityAsync`, because it does not happen. ", "comments": ["@horance-liu, thanks for your PR! By analyzing the history of the files in this pull request, we identified @mrry, @tensorflower-gardener and @vrv to be potential reviewers.", "Can one of the admins verify this patch?", "Transfering this to @poxvoculi, because I think he wrote the code that relies on this check.", "We might need this in the future. See https://github.com/tensorflow/tensorflow/pull/11392#issuecomment-315818751", "@byronyi I think this PR is essentially just removing the error case\r\n   errors::Internal(\"Failed to find locality for \", device_name);\r\nwhich cannot be hit, due to the immediately prior call to GetDeviceLocalityNonBlocking.\r\n(Now a default value for DeviceAttributes.locality is returned, but perhaps an earlier version of GetDeviceLocalityNonBlocking could have returned an error if the table entry existed but had no locality value.)\r\n\r\nHow do you anticipate hitting this case in the future?  ", "Sorry, I shall be fine with the changes here. As a side note, there could be three different cases here:\r\n\r\n1. There's a valid table entry (the happy path)\r\n2. There's no _valid_ table entry, because the corresponding device attributes can't be found\r\n3. There's no table entry at all, as the RPC has not returned from the remote\r\n\r\nCorrect me if I'm wrong, but now it seems the 2nd and 3rd cases could not be differentiated. Like you said, an early implementation could mark the 2nd case with an _invalid_ table entry, and return false from GetDeviceLocalityNonBlocking (and thus we might need what will be removed in this PR). And for the current implementation, would there be any chances for the caller to keep refreshing the table for nonexistent device attributes, i.e. mistaken case 2 as case 3?", "@byronyi In an earlier version of protobufs it was possible to distinguish a missing member from a default value.  That's no longer the case, so [this line ](https://github.com/petewarden/tensorflow_makefile/blob/master/tensorflow/core/distributed_runtime/worker_cache_partial.cc#L34) should either return a default value or a good value set by RefreshDeviceStatus, and we can't tell which one.  In the future, if useful, we could set the default value to some distinguishable invalid value.\r\n\r\nYou are correct that 2 and 3 cannot be distinguished.  We could support that in the future if useful.\r\n", "Thanks for the explanation! Very much helpful as always :)", "Jenkins, test this please.", "Jenkins, test this please"]}, {"number": 12508, "title": "DNNClassifier estimator cannot be exported", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **tensorflow/tensorflow:latest container**\r\n- **ubuntu linux**\r\n- **installed from pip**\r\n- **TensorFlow version 'v1.2.0-5-g435cdfc', '1.2.1'**:\r\n- **Python 2.7**: \r\n- **Bazel version (if compiling from source)**:\r\n\r\n### Exact command to reproduce\r\n```python\r\nclassifier = DNNClassifier(feature_columns=feature_columns,\r\n                         hidden_units=[10, 20, 10],\r\n                         n_classes=3,\r\n                         model_dir=model_path)\r\n\r\nclassifier.export_savedmodel(MODEL_PATH, script.serving_input_receiver_fn)\r\n```\r\n### Describe the problem\r\nTrying to export the model DNNClassifier throws the exception:\r\n```bash\r\nException during training: A default input_alternative must be provided.\r\n Traceback (most recent call last):\r\n  File \"algo.py\", line 78, in train\r\n    nn.export_savedmodel(MODEL_PATH, script.serving_input_receiver_fn, default_output_alternative_key=None)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1280, in export_savedmodel\r\n    actual_default_output_alternative_key)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py\", line 259, in build_all_signature_defs\r\n    raise ValueError('A default input_alternative must be provided.')\r\n```\r\nThe problem happens because DNNClassifier constructor creates a head with name `None`: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/dnn.py#L365\r\n\r\n", "comments": ["Hi, @mvsusp . Could you give a minimal reproducible test case?  Something seems wrong with your `serving_input_fn`.", "Hi, @facaiy. Here is a minimal reproducible test case:\r\n\r\n```python\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport os\r\nimport urllib\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# Data sets\r\nfrom tensorflow.contrib.learn import DNNClassifier\r\n\r\nIRIS_TRAINING = \"iris_training.csv\"\r\nIRIS_TRAINING_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\r\n\r\nIRIS_TEST = \"iris_test.csv\"\r\nIRIS_TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\r\n\r\n\r\ndef main():\r\n    # If the training and test sets aren't stored locally, download them.\r\n    if not os.path.exists(IRIS_TRAINING):\r\n        raw = urllib.urlopen(IRIS_TRAINING_URL).read()\r\n        with open(IRIS_TRAINING, \"w\") as f:\r\n            f.write(raw)\r\n\r\n    if not os.path.exists(IRIS_TEST):\r\n        raw = urllib.urlopen(IRIS_TEST_URL).read()\r\n        with open(IRIS_TEST, \"w\") as f:\r\n            f.write(raw)\r\n\r\n    # Load datasets.\r\n    training_set = tf.contrib.learn.datasets.base.load_csv_with_header(\r\n        filename=IRIS_TRAINING,\r\n        target_dtype=np.int,\r\n        features_dtype=np.float32)\r\n\r\n    # Specify that all features have real-value data\r\n    feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[4])]\r\n\r\n    # Build 3 layer DNN with 10, 20, 10 units respectively.\r\n    classifier = DNNClassifier(feature_columns=feature_columns,\r\n                               hidden_units=[10, 20, 10],\r\n                               n_classes=3,\r\n                               model_dir=\"/tmp/iris_model\")\r\n    # Define the training inputs\r\n    train_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={\"x\": np.array(training_set.data)},\r\n        y=np.array(training_set.target),\r\n        num_epochs=None,\r\n        shuffle=True)\r\n\r\n    # Train model.\r\n    classifier.fit(input_fn=train_input_fn, steps=2000)\r\n\r\n    def serving_input_fn():\r\n        inputs = {'x': tf.placeholder(tf.float32, [4])}\r\n        return tf.estimator.export.ServingInputReceiver(inputs, inputs)\r\n\r\n    classifier.export_savedmodel(export_dir_base=\"/tmp/iris_model\", serving_input_fn=serving_input_fn)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\nThe error is:\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"/reproducible_example.py\", line 64, in <module>\r\n    main()\r\n  File \"/reproducible_example.py\", line 61, in main\r\n    classifier.export_savedmodel(export_dir_base=\"/tmp/iris_model\", serving_input_fn=serving_input_fn)\r\n  File \"/Users/mvs/python2.7/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1280, in export_savedmodel\r\n    actual_default_output_alternative_key)\r\n  File \"/Users/mvs/python2.7/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py\", line 259, in build_all_signature_defs\r\n    raise ValueError('A default input_alternative must be provided.')\r\nValueError: A default input_alternative must be provided.\r\n```", "Thanks for your test case, @mvsusp . It's really concise.\r\n\r\n If I understand correctly, [learn.DNNClassifier](https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/contrib/learn/DNNClassifier#export_savedmodel) expects an `InputFnOps`, not `ServingInputReceiver`.  Hence, perhaps you'd better to try build `InputFnOps` by youself or use `tf.contrib.learn.build_parsing_serving_input_fn` for tf.Example, more to see [contrib.learn#Input_processing](https://www.tensorflow.org/versions/r1.2/api_guides/python/contrib.learn#Input_processing).\r\n\r\nCorrect me if I'm wrong, I believe that `ServingInputReceiver` is prepared for [tf.Estimator.DNNClassifier](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier#export_savedmodel), which is introduced later in 1.3, not 1.2.1.  So, to upgrade your tensorflow is also a choice.\r\n\r\nBy the way, `tf.Estimator` and `tf.contrib.learn` are different modules and might be incompatible.", "Unfortunately, I don't know yet. Perhaps API is the most reliable source, except source code itself.\r\n\r\nThanks, happy weekend, @mvsusp .", "@facaiy following your suggestion I got stuck if another issue: I got a ```Classification input must be a single string Tensor; got {'x': <tf.Tensor 'Placeholder:0' shape=(4,) dtype=float32>}```\r\n\r\nIt seems that I cannot feed a float tensor for classification using tf serving?\r\n\r\nMy code: \r\n```python\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport os\r\nimport urllib\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# Data sets\r\n\r\nIRIS_TRAINING = \"iris_training.csv\"\r\nIRIS_TRAINING_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\r\n\r\nIRIS_TEST = \"iris_test.csv\"\r\nIRIS_TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\r\n\r\n\r\ndef main():\r\n    # If the training and test sets aren't stored locally, download them.\r\n    if not os.path.exists(IRIS_TRAINING):\r\n        raw = urllib.urlopen(IRIS_TRAINING_URL).read()\r\n        with open(IRIS_TRAINING, \"w\") as f:\r\n            f.write(raw)\r\n\r\n    if not os.path.exists(IRIS_TEST):\r\n        raw = urllib.urlopen(IRIS_TEST_URL).read()\r\n        with open(IRIS_TEST, \"w\") as f:\r\n            f.write(raw)\r\n\r\n    # Load datasets.\r\n    training_set = tf.contrib.learn.datasets.base.load_csv_with_header(\r\n        filename=IRIS_TRAINING,\r\n        target_dtype=np.int,\r\n        features_dtype=np.float32)\r\n\r\n    # Specify that all features have real-value data\r\n    feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[4])]\r\n\r\n    # Build 3 layer DNN with 10, 20, 10 units respectively.\r\n    classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,\r\n                               hidden_units=[10, 20, 10],\r\n                               n_classes=3,\r\n                               model_dir=\"/tmp/iris_model\")\r\n    # Define the training inputs\r\n    train_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={\"x\": np.array(training_set.data)},\r\n        y=np.array(training_set.target),\r\n        num_epochs=None,\r\n        shuffle=True)\r\n\r\n    # Train model.\r\n    classifier.train(input_fn=train_input_fn, steps=2000)\r\n\r\n    def serving_input_fn():\r\n        inputs = {'x': tf.placeholder(tf.float32, [4])}\r\n        return tf.estimator.export.ServingInputReceiver(inputs, inputs)\r\n\r\n    classifier.export_savedmodel(export_dir_base=\"/tmp/iris_model\", serving_input_receiver_fn=serving_input_fn)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\nHow can I fix that? Thank you for all the support.", "How about using [build_raw_serving_input_receiver_fn](https://www.tensorflow.org/api_docs/python/tf/estimator/export/build_raw_serving_input_receiver_fn) ?\r\n\r\nPerhaps [tf.estimator.export](https://www.tensorflow.org/api_docs/python/tf/estimator/export) will be useful for you. Good luck.", "It does not work as well. I will study tf.estimator.export better. Thanks\nfor your help.\nOn Sat, Aug 26, 2017 at 17:07 Yan Facai (\u989c\u53d1\u624d) <notifications@github.com>\nwrote:\n\n> How about using build_raw_serving_input_receiver_fn\n> <https://www.tensorflow.org/api_docs/python/tf/estimator/export/build_raw_serving_input_receiver_fn>\n> ?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12508#issuecomment-325168759>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAIq-sSPdkib45rNZA87QfOEcCyxKXnKks5scLNVgaJpZM4O_XNK>\n> .\n>\n", "Hi @facaiy \r\n\r\nThe issue was solved using tf.estimator.export.build_parsing_serving_input_receiver_fn.\r\n\r\nThank you!", "@mvsusp Can you please post the fixed code for your example ,I'm trying tho export and serve a DNNLinearCombinedRegressor model , and i cant find any working example ", "Hello @samithaj \r\n\r\nCanned estimator don't have a lot of documentation yet. Here it go my code:\r\n\r\n```python\r\nINPUT_TENSOR_NAME = 'inputs'\r\n\r\n\r\ndef estimator(model_path):\r\n    feature_columns = [tf.feature_column.numeric_column(INPUT_TENSOR_NAME, shape=[4])]\r\n    return tf.estimator.DNNClassifier(feature_columns=feature_columns,\r\n                                      hidden_units=[10, 20, 10],\r\n                                      n_classes=3,\r\n                                      model_dir=model_path)\r\n\r\n\r\ndef serving_input_receiver_fn():\r\n    feature_spec = {INPUT_TENSOR_NAME: tf.FixedLenFeature(dtype=tf.float32, shape=[4])}\r\n    return tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)()\r\n\r\n\r\ndef train_input_fn(training_dir):\r\n    training_set = tf.contrib.learn.datasets.base.load_csv_with_header(\r\n        filename=os.path.join(training_dir, 'iris_training.csv'),\r\n        target_dtype=np.int,\r\n        features_dtype=np.float32)\r\n\r\n    return tf.estimator.inputs.numpy_input_fn(\r\n        x={INPUT_TENSOR_NAME: np.array(training_set.data)},\r\n        y=np.array(training_set.target),\r\n        num_epochs=None,\r\n        shuffle=True)()\r\n\r\n```\r\n\r\nHope it helps you!", "Cool, @mvsusp .\r\n\r\nBy the way, if `tf.feature_column` is used, `feature_spec` can be generated automatically, like:\r\n\r\n```python\r\n    feature_spec = tf.feature_column.make_parse_example_spec(feature_columns)\r\n```", "thanks @mvsusp \r\nCheck here anyone looking for complete example: https://github.com/MtDersvan/tf_playground", "@mvsusp @MtDersvan this doesnt work in version 1.4.0", "@AKhilGarg91 Do you have any tracebacks?", "@MtDersvan No when I run this file I got error during export_savedmodel that too many values to unpack. \r\nhttps://github.com/AKhilGarg91/tf_playground/blob/master/wide_and_deep_tutorial/wide_and_deep_export_r1.3.ipynb", "@MtDersvan have u tried to run below command in window?\r\nServing is available for windows or not??\r\n$>bazel build //tensorflow_serving/model_servers:tensorflow_model_server", "@AKhilGarg91 \r\n\r\nI got the too many values to unpack on 1.4, too.\r\n\r\na small dig into that I changed\r\n`tf.estimator.export.build_parsing_serving_input_receiver_fn`\r\nto\r\n`tensorflow.contrib.learn.build_parsing_serving_input_fn`\r\n\r\n\r\nIn `tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py` line 157-161\r\n```\r\n  if isinstance(input_ops, input_fn_utils.InputFnOps):    \r\n    features, unused_labels, default_inputs = input_ops #<- should go here\r\n    input_alternatives[DEFAULT_INPUT_ALTERNATIVE_KEY] = default_inputs\r\n  else:\r\n    features, unused_labels = input_ops #<- this line fails\r\n```\r\nIn `InputFnOps`, it says\r\n```\r\nContents of this file are moved to tensorflow/python/estimator/export.py.\r\nInputFnOps is renamed to ServingInputReceiver.\r\nbuild_parsing_serving_input_fn is renamed to\r\n  build_parsing_serving_input_receiver_fn.\r\nbuild_default_serving_input_fn is renamed to\r\n  build_raw_serving_input_receiver_fn.\r\n```\r\n\r\nSeems the new class causes the error.\r\n```\r\nif isinstance(input_ops, input_fn_utils.InputFnOps):\r\n```\r\nshould change to something like\r\n```\r\nif isinstance(input_ops, input_fn_utils.InputFnOps) or isinstance(input_ops, export.ServingInputReceiver):\r\n```\r\n\r\nMy work around is to use `tensorflow.contrib.learn.build_parsing_serving_input_fn` instead for now.", "@twksos Thanks. .  Yes that worked for me as well and I also tried that one only before.", "hey everyone, i'm trying to export a CNN model which accepts 200x200 rgb images as inputs, however while exporting the model i'm getting the following error\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-35-bdf37a7e35ac> in <module>()\r\n----> 1 model.export_savedmodel(export_dir_base=\"/home/ubuntu/CNNexport/\",serving_input_receiver_fn=serving_input_fn)\r\n\r\n~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in export_savedmodel(self, export_dir_base, serving_input_receiver_fn, assets_extra, as_text, checkpoint_path)\r\n    515           serving_input_receiver.receiver_tensors,\r\n    516           estimator_spec.export_outputs,\r\n--> 517           serving_input_receiver.receiver_tensors_alternatives)\r\n    518 \r\n    519       if not checkpoint_path:\r\n\r\n~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/export/export.py in build_all_signature_defs(receiver_tensors, export_outputs, receiver_tensors_alternatives)\r\n    191     receiver_tensors = {_SINGLE_RECEIVER_DEFAULT_NAME: receiver_tensors}\r\n    192   if export_outputs is None or not isinstance(export_outputs, dict):\r\n--> 193     raise ValueError('export_outputs must be a dict.')\r\n    194 \r\n    195   signature_def_map = {}\r\n\r\nValueError: export_outputs must be a dict.\r\n```\r\n\r\nHere is my feature spec and serving input fn definition : \r\n```\r\nfeature_spec = {'images': tf.FixedLenFeature([200,200,3],tf.float32)}\r\ndef serving_input_fn():\r\n    serialized_tf_example = tf.placeholder(dtype=tf.string,\r\n                                         shape=[None],\r\n                                         name='input_tensors')\r\n    receiver_tensors = {'inputs': serialized_tf_example}\r\n    features = tf.parse_example(serialized_tf_example, feature_spec)\r\n    print(features)\r\n    print(tf.estimator.export.ServingInputReceiver(features, receiver_tensors))\r\n    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\r\n```\r\n\r\nand call to export_savedmodel : `model.export_savedmodel(export_dir_base=\"/home/ubuntu/CNNexport/\",serving_input_receiver_fn=serving_input_fn)`\r\n\r\nI'm not sure whats causing export_outputs to be not a dict object. Can anyone help me figure out what i'm doing wrong?", "I have also tried using `build_parsing_serving_input_receiver_fn` and `build_raw_serving_input_receiver_fn` but at the end i always get the same error.", "@AKhilGarg91 @twksos Thanks for the information, can you open an issue or a PR in the tutorial repo, so we can deal with it there? Also, @twksos it looks like a `r1.4` backward compatibility issue as\r\n```\r\ntf.estimator.export.build_parsing_serving_input_receiver_fn()\r\n```\r\nis already in the core library, so technically it should work. Maybe a tf core team can verify this? Or else, a new issue might be needed to be created.\r\nDoes `tensorflow.contrib.learn.build_parsing_serving_input_fn` work adequately and as expected for you?", "@Anmol-Sharma I guess you need to return export_outputs in your model_fn, like\r\n```\r\n        return tf.estimator.EstimatorSpec(\r\n            mode=mode,\r\n            predictions=predictions,\r\n            loss=loss,\r\n            train_op=train_op,\r\n            export_outputs=export_outputs)\r\n```", "@felicitywang did it work for your use case?", "> `TensorServingInputReceiver` that can accept and pass along raw tensors\r\n> https://github.com/tensorflow/tensorflow/issues/11674#issuecomment-371879086\r\n\r\nis out on TensorFlow 1.7.0-rc0 ", "@MtDersvan \r\n\r\n> Does tensorflow.contrib.learn.build_parsing_serving_input_fn work adequately and as expected for you?\r\n\r\nThanks! You are right, if I trained a model from `contrib.learn` (like [DNNRegressor](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/DNNRegressor)) I have to use the `tf.contrib.learn.build_parsing_serving_input_fn`.\r\n\r\nWorking example:\r\n\r\n```python\r\ndef serving_input_receiver_fn():\r\n    feature_spec = tf.feature_column.make_parse_example_spec(feature_columns)\r\n    return tf.contrib.learn.build_parsing_serving_input_fn(feature_spec)()\r\n\r\nservable_model_dir = \"./DNNRegressors/Servable/\"\r\n\r\nregressor.export_savedmodel(servable_model_dir, serving_input_receiver_fn)\r\n```"]}, {"number": 12507, "title": "typo missing ')'", "body": "it is missing the ) at end of android_ndk_repository(", "comments": ["@cloudbank, thanks for your PR! By analyzing the history of the files in this pull request, we identified @jart, @keveman and @dsmilkov to be potential reviewers.", "Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please", "@jhseu \r\nCan you roll this back?  My copy of the file was missing the line \r\n    api_level=14)\r\nfor some reason and don't want to add an unnecessary comment to the file\r\nor I could re-fix it : )", "Ah, ok, closing"]}, {"number": 12506, "title": "Update doc and imports for crf_decode", "body": "A new method crf_decode was added recently (does the equivalent of viterbi_decode but with tf.Tensors as input).\r\nHowever it is still not in the docs and it is not importable in tf.contrib.crf; hence the proposed changes.", "comments": ["@cBournhonesque, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener to be a potential reviewer.", "Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I've signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "See #12490."]}, {"number": 12505, "title": "Building Error Solved", "body": "", "comments": []}, {"number": 12504, "title": "NaN propagation for GPU pooling ops", "body": "Attention @zheng-xq\r\n\r\n- Changes the custom fwd maxpooling kernel to propagate NaNs. This\r\n  makes it match the behavior of CUDNN, and ensures that CUDNN's\r\n  bwd maxpooling kernel behaves as expected (propagating NaNs).\r\n- Previous behavior can be restored with environment variable\r\n  TF_ENABLE_MAXPOOL_NANPROP=0.\r\n- Changes the GPU bwd maxpool op tests to expect propagated NaNs\r\n  (matching the behavior of the CPU path).", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Please handle the CLA before we look at this change. Thanks!", "Can one of the admins verify this patch?", "Asking team about CLA for associate corporation.   ", "CLAs look good, thanks!\n\n<!-- ok -->", "@nluehr did you address @zheng-xq comments?\r\n\r\nJenkins, test this please.", "Yes, all comments have been addressed.", "Thanks! @zheng-xq PTAL?", "@zheng-xq, are further changes needed here?", "Unassigning myself for review since I'm not very familiar with the pooling code. Let's wait for @zheng-xq to get back.", " @zheng-xq can this be merged?", "LGTM\r\n\r\nPlease move ahead with merging.", "Jenkins, test this please.", "Some kind of infra error (CC @gunan FYI)\r\n\r\n```\r\nAnalyzing: 367 targets (111 packages loaded)\r\nERROR: no such package '@bazel_toolchains//configs/debian8_clang/0.1.0/bazel_0.6.0': BUILD file not found on package path\r\n```", "@nlopezgi any idea on the bazel_toolchains error?", "The only reason I can think of that would manifest with this error is if the repo is behind commit 6425dbd10e9bc5a765807c25d3da109230840096 ( https://github.com/tensorflow/tensorflow/commit/6425dbd10e9bc5a765807c25d3da109230840096#diff-455a4c7f8e22d7c514e8c2caa27506c5), as those configs should be available after that commit (https://github.com/bazelbuild/bazel-toolchains/tree/b2b4b38433bf2d1159360855ea4004378308711b/configs/debian8_clang/0.1.0). Other than that, it might be a transient error? So I'd recommend checking the repo is sync'd and retrying. Let me know if the issue persists (and please, ideally, send a link with a full log of the command executed and the error). Sorry for the issues.\r\n\r\n", "You are right @nlopezgi , the version in this PR branch is old. But we should really be testing against master. I will check with kokoro. Meanwhile to unblock this PR, we might need to do a rebase.", "The branch infra issue should be fixed now."]}, {"number": 12503, "title": "Add cudnn7 support", "body": "Attention @zheng-xq \r\n\r\n- Account for cudnnSetRNNDescriptor API change\r\n- Add support for CUDNN_TENSOR_OP_MATH in cudnn v7\r\n  - Applies to forward and backward convolutions that have fp16\r\n  input/output. Computations will fall back to pseudo-fp16\r\n  if tensor op math is disabled or not supported.\r\n  - Enabled by default, but can be disabled using the environment\r\n  variable TF_ENABLE_TENSOR_OP_MATH=0.\r\n  - The choice of whether to use tensor op math is included in the\r\n  autotuning of convolutions.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "@nluehr, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @keveman and @zheng-xq to be potential reviewers.", "Please handle the CLA before we look at this change. Thanks!", "Asking team about CLA for associate corporation.   @zheng-xq  Please review or assign others before CLA so we can merge as soon as we figure that out.  ", "I signed it!", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n<!-- ok -->", "In general, I'm fine with this change. We can keep improving it afterwards. \r\n\r\nNathan, please confirm that the performance still stays the same with Cuda9+Cudnn7 on Volta. And nothing changes for Cuda8+Cudnn6 on P100. I'll grant my LGTM afterwards. Thanks!", "@nluehr Can you confirm that the performance is the same on Volta, and that it also works on cuda8+cudnn6 on P100?\r\n\r\nAlso, did you address @jlebar comments?", "I believe all comments from @jlebar have been addressed.\r\nCode still builds on cuda 8.0 / cudnn 6.\r\nPerf remains unchanged: Resnet 50 (batch size = 64) on a P100 gets 224.5 img/sec before and 224.7 after.", "> I believe all comments from @jlebar have been addressed.\r\n\r\nSorry to be a broken record, but to be totally clear, the GetAlgorithms issue (i.e. requiring a loop over {true, false} every time we call the function) is still outstanding.  I believe XQ is OK waiting for you to fix that in a later PR because of the time-criticality of this change, and that's fine with me.\r\n\r\nI don't immediately see any clang-format issues, thank you for fixing those.  (Or, if you didn't, blame it on my eyes not being as sharp this time of day. :)", "Jenkins, test this please.", "Looking forward to follow up PR ;-)", "Windows build reported as failure but I don't see an obvious place: https://ci.tensorflow.org/job/tensorflow-pr-win-cmake-py/4420/consoleFull\r\n\r\nJenkins, test this please."]}, {"number": 12502, "title": "Support for CUDA 9.0", "body": "For review by @zheng-xq.\r\n\r\nAdd explicit __syncwarp to bias_op\r\n - Makes warp-synchronous code safe on Volta\r\nAdd sync mask to __shfl intrinsics\r\nAdd libdevice bytecode paths for CUDA 9\r\n - In CUDA 9, all supported architectures are merged into a single file\r\nUpdate code gating for CUDA 9\r\nAdd sm_70 to the lookup table used by XLA\r\nChange the default sm arch from 20 to 30.\r\nFix for NVPTX not yet supporting sm_70\r\nRemove unnecessary cuda decorators from defaulted constructors\r\nUse updated NCCL for CUDA 9 fp16 support", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Please handle the CLA before we look at this change. Thanks!", "Asking team about CLA for associate corporation.  Should not take long to sort out.  Thank you for starting the review.", "Nice work! Thanks for these changes. I have these cuda-9 and cudnn-v7 (#12503) patches built and working.\r\n\r\nNotes for others trying this out:\r\n1) This patch alone either requires prior versions of cuDNN (up to v6), or cuDNN must be disabled (either option does not support new mixed-precision fp16 NVIDIA Volta V100). Another patch is required to also support cuDNN v7 (and Volta). @nluehr has posted a patch for cuDNN 7 support here: #12503\r\n\r\n2) Eigen must also be updated to work with cuda-9 per this thread: #12489. For now, @nluehr has a modified Eigen archive available with these changes to `tensorflow/workspace.bzl`:\r\n```\r\ndiff --git a/tensorflow/workspace.bzl b/tensorflow/workspace.bzl\r\nindex e5c2ab5..1b3314a 100644\r\n--- a/tensorflow/workspace.bzl\r\n+++ b/tensorflow/workspace.bzl\r\n@@ -169,12 +169,14 @@ def tf_workspace(path_prefix=\"\", tf_repo_name=\"\"):\r\n   native.new_http_archive(\r\n       name = \"eigen_archive\",\r\n       urls = [\r\n-          \"http://mirror.bazel.build/bitbucket.org/eigen/eigen/get/f3a22f35b044.tar.gz\",\r\n-          \"https://bitbucket.org/eigen/eigen/get/f3a22f35b044.tar.gz\",\r\n+          \"http://bazel-mirror.storage.googleapis.com/bitbucket.org/ncluehr/eigen/get/6cc72014d888.tar.gz\",\r\n+          \"https://bitbucket.org/ncluehr/eigen/get/6cc72014d888.tar.gz\",\r\n       ],\r\n-      sha256 = \"ca7beac153d4059c02c8fc59816c82d54ea47fe58365e8aded4082ded0b820c4\",\r\n-      strip_prefix = \"eigen-eigen-f3a22f35b044\",\r\n+      sha256 = \"549fc01af565d185123016c12baf997eeb462b088c1885a4c3dfaf2a5c82d677\",\r\n+      strip_prefix = \"ncluehr-eigen-6cc72014d888\",\r\n       build_file = str(Label(\"//third_party:eigen.BUILD\")),\r\n+\r\n+\r\n   )\r\n \r\n   native.new_http_archive(\r\n```\r\n", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "LGTM", "What tools did you use to figure out what needed to be changed and to verify that the code now works?", "Approval, but I would really like to know what tools were used and so on so we can (ideally) add tests that this continues to be clean wrt whatever sanitizers you used", "So being this PR merged into master, should we compile by hand to use it?\r\nHow much time before it goes into the version distributed by `pip`?\r\n\r\nThank you :)", "I think tf-nightly is specifically for the impatient.", "thank you @byronyi, unfortunately tf-nightly is not yet (but soon will) suport GPUs. From the readme:\r\n\r\n`We are pleased to announce that TensorFlow now offers nightly pip packages under the tf-nightly project on pypi. Simply run pip install tf-nightly in a clean environment to install the nightly tensorflow build. We currently only support CPU packages on Linux, Mac, and Windows. GPU packages on all platforms will arrive soon!`\r\n\r\n", "FWIW I'm seeing a tf-gpu-nightly link in https://github.com/tensorflow/tensorflow/commit/4270c4a00bb90d97418df5b0e9b6b3e148a72a1e, in case they just haven't updated the README? Worth a shot trying to install it with pip, just in case", "@lefnire cannot find it on pip. I guess it's a matter of days ;)", "It will not be in the nightly as far as linking to CUDA 9.  TF 1.4 is next, which will **most likely** stay CUDA 8 + cuDNN 6 and then 1.5 would move to cuDNN 7 and CUDA 9 if all goes well.  cuDNN 7 + Cuda 9 gives you Volta support along with other internal changes like batch_norm supporting FP16 optimally.  There may be other benefits but Volta is the big deal.", "I really hoped TF 1.4 would go cuDNN 7 + Cuda 9, as I've had too many issues already with Cuda and my display drivers setup. Oh well! ", "I'm gonna roll back to cuDNN 6 + CUDA 8 and wait some time. GPU computing is quite a mess :D \r\nThank you guys"]}, {"number": 12501, "title": "EditDistance crashes under C++ environment", "body": "### System information\r\n- **Windows 10**:\r\n- **TensorFlow installed from source**:\r\n- **TensorFlow version 1.3-rc2**:\r\n- **Python version 3.5**:\r\n- **VisualStudio 2017**:\r\n\r\n### Describe the problem\r\nI have created a C++ example with EditDistance and linked a debug version of tensorflow.dll which is created from tensorflow source.  The compiled EditDistance example crashed at **session.Run( outputs, &output_tensor );**. I have tried in this example also in the tensorflow environment it crashes also. It seems the EditDistance operator doesn't work under C++. I have searched another EditDistance examples but it seems nobody has tested it outside the python environment.\r\n\r\n### Source code / logs\r\n```\r\n#include <iostream>\r\n#include <vector>\r\n#include <string>\r\n#include <unordered_set>\r\n\r\n#include \"tensorflow/cc/client/client_session.h\"\r\n#include \"tensorflow/cc/ops/standard_ops.h\"\r\n#include \"tensorflow/core/framework/tensor.h\"\r\n#include \"tensorflow/core/lib/gtl/edit_distance.h\"\r\n#include \"tensorflow/core/util/sparse/sparse_tensor.h\"\r\n#include \"tensorflow/core/protobuf/config.pb.h\"\r\n\r\nusing namespace std;\r\n```\r\n\r\n    int main( int argc, char *argv[] )\r\n```\r\n{\r\n           using namespace tensorflow;\r\n           using namespace tensorflow::ops;\r\n           using namespace tensorflow::gtl;\r\n           Scope root = Scope::NewRootScope();\r\n```\r\n\r\n           std::vector<Tensor> output_tensor;\r\n           ClientSession session( root );\r\n\r\n\r\n           Tensor hypho2_ix( DT_INT64, TensorShape( { static_cast<int64_t>( 4 ), 3 } ) );\r\n           Tensor hypho2_vals( DT_STRING, TensorShape( {static_cast<int64_t>( 4 )} ) );\r\n\r\n           makeIndex( {\"bear\"}, hypho2_ix );\r\n           makeChar ( {\"bear\"}, hypho2_vals );\r\n\r\n           Tensor truth2_ix( DT_INT64, TensorShape( { static_cast<int64_t>( 5 ), 3 } ) );\r\n           Tensor truth2_vals( DT_STRING, TensorShape( {static_cast<int64_t>( 5 )} ) );\r\n\r\n           makeIndex( { \"beers\" }, truth2_ix );\r\n           makeChar ( { \"beers\" }, truth2_vals );\r\n\r\n           // Declaration of edit distance\r\n           auto address_dist = EditDistance( root\r\n                                   , hypho2_ix\r\n                                   , hypho2_vals\r\n                                   , {3,1,1}// {static_cast<int64_t>(1),3}//test_address_shape\r\n                                   , truth2_ix\r\n                                   , truth2_vals\r\n                                   , {3,1,1} //ref_address_shape\r\n                                   , EditDistance::Normalize(false) );\r\n\r\n                                      const std::vector<Output> outputs = {address_dist};\r\n           session.Run( outputs, &output_tensor );\r\n     }   \r\n        void makeIndex( const std::vector<string>& rsoStringVector, tensorflow::Tensor& roIndexTensor )\r\n        {\r\n           auto ix_t = roIndexTensor.matrix<int64_t>();\r\n           std::size_t stCounter = 0;\r\n           for( std::size_t stX = 0; stX < rsoStringVector.size() ; stX++ )\r\n           {\r\n              const std::string& rsString = rsoStringVector[ stX ];\r\n              for( std::size_t stY = 0; stY < rsString.size(); stY++ )\r\n              {\r\n                 ix_t( stCounter, 0 ) = stX;\r\n                 ix_t( stCounter, 1 ) = 0;\r\n                 ix_t( stCounter, 2 ) = stY;\r\n                 stCounter++;\r\n              }\r\n           }\r\n        }\r\n\r\n        void makeChar( const std::vector<std::string>& rsoStringVector, tensorflow::Tensor& roCharTensor )\r\n        {\r\n           auto vals_t = roCharTensor.vec<std::string>();\r\n\r\n           int64_t i64Index = 0;\r\n           for( std::size_t stX = 0; stX < rsoStringVector.size(); stX++ )\r\n           {\r\n              const std::string& rsString = rsoStringVector[ stX ];\r\n              for( std::size_t stY = 0; stY < rsString.size(); stY++ )\r\n              {\r\n                 vals_t( i64Index++ ) = ( rsString[ stY ] );\r\n              }\r\n           }\r\n        }\r\n", "comments": ["I have figured out that the crash occurs  in ClientSessionRun::Run() method. The reason is the Output object in fetch_outputs vector is not correctly installed. The Node member of Output is null. This causes an exception at **output_tensor_names.push_back(output.name());**\r\n\r\n```\r\nStatus ClientSession::Run(const RunOptions& run_options, const FeedType& inputs,\r\n                          const std::vector<Output>& fetch_outputs,\r\n                          const std::vector<Operation>& run_outputs,\r\n                          std::vector<Tensor>* outputs,\r\n                          RunMetadata* run_metadata) const {\r\n  std::vector<std::pair<string, Tensor>> feeds;\r\n  for (auto const& feed : inputs) {\r\n    TF_RETURN_IF_ERROR(feed.second.status);\r\n    feeds.emplace_back(feed.first.name(), feed.second.tensor);\r\n  }\r\n  std::vector<string> output_tensor_names;\r\n  output_tensor_names.reserve(fetch_outputs.size());\r\n  for (auto const& output : fetch_outputs) {\r\n    output_tensor_names.push_back(output.name());\r\n  }\r\n  std::vector<string> target_node_names;\r\n  target_node_names.reserve(run_outputs.size());\r\n  for (auto const& output : run_outputs) {\r\n    target_node_names.push_back(output.node()->name());\r\n  }\r\n  TF_RETURN_IF_ERROR(impl()->MaybeExtendGraph());\r\n  return impl()->session_->Run(run_options, feeds, output_tensor_names,\r\n                               target_node_names, outputs, run_metadata);\r\n}\r\n```\r\n\r\nAs I see the default Output object initializes the node member **op_** with null. The question is how the Output should be initialized? The documentation says it will be filled automatically. Is this not a discrepancy? \r\n\r\n```\r\n\r\n/// Represents a tensor value produced by an Operation.\r\nclass Output {\r\n public:\r\n  Output() = default;\r\n  explicit Output(Node* n) : op_(n) {}\r\n  Output(Node* n, int32 index) : op_(n), index_(index) {}\r\n  Output(const Operation& op, int32 index) : op_(op), index_(index) {}\r\n\r\n  Operation op() const { return op_; }\r\n  Node* node() const { return op().node(); }\r\n  int32 index() const { return index_; }\r\n  DataType type() const { return op_.output_type(index_); }\r\n  string name() const { return strings::StrCat(node()->name(), \":\", index()); }\r\n  bool operator==(const Output& other) const {\r\n    return op_ == other.op_ && index_ == other.index_;\r\n  }\r\n\r\n  uint64 hash() const { return op_.hash(index_); }\r\n\r\n private:\r\n  Operation op_ = Operation(nullptr);\r\n  int32 index_ = 0;\r\n};\r\n\r\n```", "I have found this errors:\r\n\r\n- Input 'hypothesis_shape' passed int32 expected int64\r\n- Input 'truth_shape' passed int32 expected int64\r\n\r\nThese errors are preventing to build the internal node. It is not obvious why EditDistance construction has failed and the node pointer is null.", "@asimshankar Can you take a look of assign to another person? Thanks...", "I have solved the problems. The main problem of Tensorflow usage in C++ environment is not checking the variable types at compile time. Tensorflow checks variable types at runtime, which makes troubleshooting more difficult.\r\nE.g. Tensor hypho2_ix( DT_INT64, TensorShape( { 4, 3 } ) ); \r\nThis definition of hypho2_ix is wrong due the numbers 4 and 3 parameters are not int64_t type. But the C++ compiler is not checking the type at compile time. If you using this defined tensor with operations which requires DT_INT64 (int64_t) type then you will have not obvious error. Why check parameter types of Tensor definitions at compile time?\r\nThe correct definition is (C++11):\r\n**Tensor hypho2_ix( DT_INT64, TensorShape( { 4i64, 3i64 } ) );** "]}, {"number": 12500, "title": "Tensorflow Debugger crashes on tab complete", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Binary (pip wheel)\r\n- **TensorFlow version (use command below)**: 1.3\r\n- **Python version**:  3.5\r\n- **GPU model and memory**: (CPU only)\r\n\r\n### Describe the problem\r\nSometimes when using `tfdbg`, tab-completing on `pt` will crash the debugger (unfortunately, I haven't figured out how to reproduce this consistently yet) and my terminal will look like:\r\n\r\n![screenshot from 2017-08-22 14-44-11](https://user-images.githubusercontent.com/1914111/29582837-2a0cd4d8-874c-11e7-899d-8430575ce1ba.png)\r\n\r\nthe terminal will be unresponsive, and scrolling + regular mouse selection will be disabled (although `SHIFT` + mouse selection will work).\r\n\r\n### Source code / logs\r\n\r\nReconstructing the traceback:\r\n\r\n```\r\nTraceback (most recent call last):\r\nFile \"GenerativeLSTM.py\", line 151, in <module>\r\n    m.train(klabels, config={\"epochs\": 30, \"batch_size\": 1})\r\nFile \"GenerativeLSTM.py\", line 140, in train\r\n    [next_batch, (train_step, merged)], feed_dict)\r\nFile \"/home/alan/workspace/cognescent/py3.venv/lib/python3.5/site-packages/tensorflow/python/debug/wrappers/framework.py\", line 532, in run\r\n    run_end_resp = self.on_run_end(run_end_req)\r\nFile \"/home/alan/workspace/cognescent/py3.venv/lib/python3.5/site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py\", line 338, in on_run_end\r\n    self._run_start_response = self._launch_cli()\r\nFile \"/home/alan/workspace/cognescent/py3.venv/lib/python3.5/site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py\", line 429, in _launch_cli\r\n        title_color=self._title_color)\r\nFile \"/home/alan/workspace/cognescent/py3.venv/lib/python3.5/site-packages/tensorflow/python/debug/cli/curses_ui.py\", line 502, in run_ui\r\n    exit_token = self._ui_loop()\r\nFile \"/home/alan/workspace/cognescent/py3.venv/lib/python3.5/site-packages/tensorflow/python/debug/cli/curses_ui.py\", line 578, in _ui_loop\r\n    tab_completed = self._tab_complete(command)\r\nFile \"/home/alan/workspace/cognescent/py3.venv/lib/python3.5/site-packages/tensorflow/python/debug/cli/curses_ui.py\", line 1507, in _tab_complete\r\n    self._display_candidates(candidates)\r\nFile \"/home/alan/workspace/cognescent/py3.venv/lib/python3.5/site-packages/tensorflow/python/debug/cli/curses_ui.py\", line 1554, in _display_candidates\r\n    pad, _, _ = self._display_lines(candidates_output, 0)\r\nFile \"/home/alan/workspace/cognescent/py3.venv/lib/python3.5/site-packages/tensorflow/python/debug/cli/curses_ui.py\", line 1130, in _display_lines\r\n    pad = self._screen_new_output_pad(rows, cols)\r\nFile \"/home/alan/workspace/cognescent/py3.venv/lib/python3.5/site-packages/tensorflow/python/debug/cli/curses_ui.py\", line 978, in _screen_new_output_pad\r\n    return curses.newpad(rows, cols)\r\n_curses.error: curses function returned NULL\r\n```\r\n\r\nAs I said, I unfortunately have not figured out how to reproduce this regularly -- this happens every once in a while when I'm working on the LSTM from #12465. I'll try to create a minimal reproducible test case later, but I figured I'd file this ticket first.", "comments": ["Sorry for the inconvenience, @alanhdu . Given a fixed screen size, is this reproducible? What happens if you slightly change the screen width?", "I've always run `tfdbg` with a maximized window, but even then this doesn't happen all the time.\r\n\r\nChanging the screen width doesn't seem to do anything (in general, `tfdbg` doesn't really resize when I resize my window).\r\n\r\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I can't reproduce this bug. Closing it. Feel free to re-open if someone sees the bug again."]}, {"number": 12499, "title": "Cherrypicks", "body": "Docs updates Mark requested.", "comments": ["@av8ramit, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @dsmilkov and @dandelionmane to be potential reviewers.", "Just to be sure, @MarkDaoust -- in this commit, I don't see the supervisor.md contents actually moved; it looks like it's just deleted, which might contrast to the commit description.  Secondly, leftnav_files seems to still contain supervisor.md.", "For supervisor.md I forgot to update the commit description after @ispir asked that I make it a deletion. It's the commit message that's wrong, not the files.", "Jenkins, test this please."]}, {"number": 12498, "title": "Cannot open https://www.tensorflow.org in Safari", "body": "As subject says, always error msg saying \"Safari cannot open the page, because it cannot open make the secure connection with the server\". Has anybody encountered the problem and can you share your experience how to fix ?\r\nOpening the website from Firefox is OK.\r\nSystem: Mac 10.12.6.", "comments": ["https://**www**.tensorflow.org/ opens without problem for me on Safari (same os version as you report)\r\n\r\nhttps://tensorflow.org/ presents a warning panel which is due to the certificate having expired ( https://github.com/tensorflow/tensorflow/issues/12496 )\r\n", "@quaeler thanks for you reply.\r\n\r\nMaybe the problem is that, there's no warning panel showing when I open tensorflow webpage in Safari. I tried to find if there're any relevant setting options in Safari, unfortunately I didn't find a clue. Any idea from you (if you know) ? Thanks.", "I thought maybe it was because i'd turned on the Developer menu, but turning that back off still displays the certificate warning for me.\r\n\r\nOddly, Firefox warns me too - ( on https://tensorflow.org )\r\n\r\nThat all being written, if you are having problems with https://www.tensorflow.org and not the www-less version, then i'm at a total loss as its certificate is valid. :- /\r\n", "The HTTPS cert is expired. Chrome says:\r\n\r\nThis server could not prove that it is tensorflow.org; its security certificate expired 57 days ago. This may be caused by a misconfiguration or an attacker intercepting your connection. Your computer's clock is currently set to Friday, August 25, 2017. Does that look right? If not, you should correct your system's clock and then refresh this page.", "My system clock setting is right as I double-confirmed.\r\n\r\nI thought there should be a warning dialogue for me but there isn't, it simply says the secure connection cannot be established :(\r\n\r\nAnd, in my Safari the problem is valid for both www.tensorflow.org and tensorflow.org...", "This is a duplicate of issue #12496. Closing here."]}, {"number": 12497, "title": "Update get_started.md", "body": "-Add python 2 compatibility for tutorial code\r\n-Corrected console outputs\r\n-Included or rearrange some import statements", "comments": ["Can one of the admins verify this patch?", "Is there are way this document change be incorporated into r1.3?", "Jenkins, test this please"]}, {"number": 12496, "title": "SSL certificate for tensorflow.org expired", "body": "The SSL certificate for https://tensorflow.org (*not* https://www.tensorflow.org) expired on June 29.", "comments": ["@martinwicke, we should fix this. Could you please reassign appropriately? Thanks!", "@wolffg I suppose we were wrong about that cert being unused. Are we ready to move or should we renew what we have?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "@gunan, I think you got a new one, no?", "Not for tensorflow.org, I got a new one for www.tensorflow.org.\r\nWe may need to discuss what exactly needs to be done here.", "Perhaps use [Let's Encrypt](https://letsencrypt.org/)? Free and wildcarded.\r\n\r\nEdit: apparently full wildcard support is not available until the end of February; regardless, their existing certificate can be issued for base.domain + www.base.domain which satisfies this case. It's also very simple to get running - [certbot](https://certbot.eff.org/) will do everything for you.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "SSL certificate is updated.", "Could this certificate be applied to *.tensorflow.org as well? https://playground.tensorflow.org/ provides a certificate error.", "Playground is served from github pages, which doesn't support cname forwards combined with SSL (a common problem). @dsmilkov I think it's worth finding an alternative for this (GAE?) Come July the pure http will be [effectively inaccessible in Chrome](https://www.theregister.co.uk/2018/02/08/google_chrome_http_shame/).", "Makes sense. Migrating playground to https is long overdue. We are going to use GAE on another project very soon, so I'll do it then.", "Still not working Feb 25 2018 on Chrome:\r\n\r\nYour connection is not private\r\nAttackers might be trying to steal your information from www.tensorflow.org (for example, passwords, messages, or credit cards). Learn more\r\nNET::ERR_CERT_DATE_INVALID\r\nSubject: tensorflow.org\r\n\r\nIssuer: Google Internet Authority G2\r\n\r\nExpires on: Jun 29, 2017\r\n\r\nCurrent date: Feb 25, 2018", "I recently went to China and was using tensorflow.google.cn back there to circumvent the GFW. Upon coming back to the US, I am no longer able to open tensorflow.org on either safari or chrome, but tensorflow.google.cn continues to work. \r\n\r\nOn safari, I simply get \"too many redirects occurred trying to open ...\" message\r\nOn chrome, I get a \"Your connection is not private\" message, but if I proceed anyway to the unsafe address, I would get \"This page isn\u2019t working\r\nwww.tensorflow.org redirected you too many times.\", similar to safari.\r\n\r\nI have double checked that my datetime is set correctly to PDT (automatically synced). I have tried clearing all cache, browser history, cookies, etc to no effect. Help is greatly appreciated!", "Below is the certificate warning statement. My colleagues are able to access tensorflow.org without problem. I suspect it has to do with my diligent updating of MacOS and browsers, that brought about heightened SSL security?\r\n\r\nThis server could not prove that it is www.tensorflow.org; its security certificate expired 387 days ago. This may be caused by a misconfiguration or an attacker intercepting your connection. Your computer's clock is currently set to Saturday, July 21, 2018. Does that look right? If not, you should correct your system's clock and then refresh this page."]}, {"number": 12495, "title": "Fix ProfileContext location in README", "body": "", "comments": ["Can one of the admins verify this patch?", "Are you sure you can call it like: tf.python.profiler.profile_context.ProfileContext?\r\nI only expose it via contrib on purpose.", "> Are you sure you can call it like: tf.python.profiler.profile_context.ProfileContext?\r\n> I only expose it via contrib on purpose.\r\n\r\nYeah, after 2b51e0ba27af69c914a7523d9aae232de09e3206 it's accessible that way, I have tested it. Furthermore, the README way like it is now doesn't work.\r\n\r\nBy looking at af23ae65db2585f4a18d0bc5f21f15e94805aa4f, it seems to be migrated to core.", "@bryant1410 \r\nI just build from source and install the latest package\r\n\r\nPython 3.4.3 (default, Nov 17 2016, 01:08:31) \r\n[GCC 4.8.4] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.contrib.tfprof.ProfileContext\r\n<class 'tensorflow.python.profiler.profile_context.ProfileContext'>\r\n>>> tf.python.profiler.profile_context.ProfileContext\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: 'module' object has no attribute 'python'", "@bryant1410 please fix, pull rebase and push again. Thanks!", "@bryant1410 ping", "I have just confirmed that the original readme is correct.\r\nMaybe we had a bad release which you saw the issue?\r\nOr is it possible maybe on windows our API is messed up somehow?\r\n```\r\n>>> tf.python.profiler.profile_context.ProfileContext\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: 'module' object has no attribute 'python'\r\n>>> tf.contrib.tfprof.ProfileContext\r\n<class 'tensorflow.python.profiler.profile_context.ProfileContext'>\r\n>>> tf.__version__\r\n'1.4.0'\r\n```\r\n\r\nTherefore, I will close this PR. Thank you very much for the proposed fix, and if you can find the root cause of the issue, I am happy to work with you to fix the issue on our end."]}, {"number": 12494, "title": "Fix images not showing in profiler README", "body": "", "comments": ["Can one of the admins verify this patch?", "@caisq LGTM. Could you approve this?\r\n\r\nThanks! I was wondering why it doesn't show on github : )"]}, {"number": 12493, "title": "Fix code highlight language in profiler README", "body": "", "comments": ["Can one of the admins verify this patch?", "@caisq LGTM", "@bryant1410 Can you fix the merge conflicts? They seem to be caused by your other PR, which is just merged. ", "done", "Thanks for the fixes, @bryant1410 ", "Hi, \r\nI want to use the profiler, by following \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/README.md\r\nbut there is no tf.contrib.tfprof.ProfileContext() anymore in r1.3\r\nand there's no ProfileContext in tf.profiler also.\r\nCan you please fix this? Thanks", "@qianyizhang Are you using a pip package or building from source?\r\n\r\nThe profiler is current under active development. Hence some features might not be available in pip package yet.\r\n\r\nLet me know if building from source still can't solve you problem. tf.contrib.tfprof.ProfileContext() should be the expected API. It is kept in contrib because the API is not stable and might be changed. Once it gets stable, it will be migrated to tf.profiler.ProfileContext", "> Hi,\r\n> I want to use the profiler, by following\r\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/README.md\r\n> but there is no tf.contrib.tfprof.ProfileContext() anymore in r1.3\r\n> and there's no ProfileContext in tf.profiler also.\r\n> Can you please fix this? Thanks\r\n\r\nmaybe try with #12495 ", "@panyx0718 \r\nThanks for you info, you are correct that I am using the pip install on windows, and it's not a easy task to build from source. Fortunately with the rapid development and recent change in namespace t seems this feature will be open to test in the next patch or so ;-)\r\n\r\nIn the meanwhile, is there a good way to measure to GPU memory usage? (nvidia-smi.exe monitor report is highly inaccurate)\r\nAnd is there any trick of memory optimization (eg. maxnet has force_mirroring to force in-place calculation, https://github.com/dmlc/mxnet-memonger)", "See this for generating a timeline with memory usage:\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler#visualize-time-and-memory\n\nYou can do profiling without ProfileContext, using the old way. If you are\nusing high-level API, you might need to do some hacks to get the RunMetadata\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/command_line.md#command-line-inputs\n\nOn Tue, Sep 5, 2017 at 6:33 PM, qianyizhang <notifications@github.com>\nwrote:\n\n> @panyx0718 <https://github.com/panyx0718>\n> Thanks for you info, you are correct that I am using the pip install on\n> windows, and it's not a easy task to build from source. Fortunately with\n> the rapid development and recent change in namespace t seems this feature\n> will be open to test in the next patch or so ;-)\n>\n> In the meanwhile, is there a good way to measure to GPU memory usage?\n> (nvidia-smi.exe monitor report is highly inaccurate)\n> And is there any trick of memory optimization (eg. maxnet has\n> force_mirroring to force in-place calculation,\n> https://github.com/dmlc/mxnet-memonger)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/12493#issuecomment-327348424>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ACwQe5yxZyHUZcaEKRFDdXOAN8FLU-wuks5sffZ6gaJpZM4O-pcL>\n> .\n>\n\n\n\n-- \nThanks\nXin\n", "In term of actual value, this should be consistent with timeline.Timeline(run_metadata.step_stats)? (since they all parse the same information from run_metadata?)\r\nBut then someone says it's only the approximated memory usage by simulation, no the actual footprint, is it still the case?\r\nhttps://stackoverflow.com/questions/36123740/is-there-a-way-of-determining-how-much-gpu-memory-is-in-use-by-tensorflow", "No, the memory are not the same. The timeline.Timeline is based on\nestimation (which could be far from reality given today's TensorFlow\ncomplexity).\nThe profiler one is based on The real value (groundtruth) collected from\nTensorFlow's gpu memory allocator.\n\nThe currently limitation is, we don't yet know where are the current memory\nfrom (due to the ref-counted Tensor). You could have some guess by relating\ncomputation and memory timeline\n\nOn Tue, Sep 5, 2017 at 10:09 PM, qianyizhang <notifications@github.com>\nwrote:\n\n> In term of actual value, this should be consistent with\n> timeline.Timeline(run_metadata.step_stats)? (since they all parse the\n> same information from run_metadata?)\n> But then someone says it's only the approximated memory usage by\n> simulation, no the actual footprint, is it still the case?\n> https://stackoverflow.com/questions/36123740/is-there-a-\n> way-of-determining-how-much-gpu-memory-is-in-use-by-tensorflow\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/12493#issuecomment-327375922>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ACwQe9RpB_IjyqEBu6WDL8VpAA9cpvJ-ks5sfikWgaJpZM4O-pcL>\n> .\n>\n\n\n\n-- \nThanks\nXin\n", "@panyx0718 \r\n\r\n> You can do profiling without ProfileContext, using the old way\r\n\r\nI am not sure what's missing, here is the example of profiling mnist example:\r\n```python\r\n  #I assume this is somewhat equivalent to the ProfileContext's role\r\n  #activate some tracker in the backend?\r\n  profiler = tf.profiler.Profiler(tf.get_default_graph())\r\n\r\n  for i in range(FLAGS.max_steps):\r\n    if i % 10 == 0:  \r\n      run_meta = tf.RunMetadata()\r\n      run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n      summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(True),\r\n                               options=run_options, run_metadata=run_meta)\r\n      profiler.add_step(i, run_meta)\r\n      with tf.gfile.Open(os.path.join(\"d:\\\\\", \"run_meta_{}\".format(str(i))), \"w\") as f:\r\n        f.write(run_meta.SerializeToString())\r\n      #specify opts to save?\r\n      run_meta2 = tf.RunMetadata()\r\n      builder = tf.profiler.ProfileOptionBuilder\r\n      opts = builder(builder.time_and_memory()).order_by('micros').build()\r\n      tf.profiler.profile(tf.get_default_graph(),\r\n                        run_meta=run_meta2,\r\n                        cmd=\"op\",\r\n                        options=opts)\r\n      with tf.gfile.Open(os.path.join(\"d:\\\\\", \"run_meta2_{}\".format(str(i))), \"w\") as f:\r\n        f.write(run_meta2.SerializeToString())\r\n```\r\nBy calling profiler = tf.profiler.Profiler(tf.get_default_graph()) beforehand, I would get groundtruth memory from the sess.run?\r\nI thought run_meta would give me some general info; while feeding opts into tf.profiler.profile would give me some more specific info, but it turns out I don't get anything for the run_meta2. Am I missing something?"]}, {"number": 12492, "title": "Feature request: manual parallel", "body": "when I run my code in that way:\r\n\r\n    for i in range(num):\r\n        with tf.control_dependencies(None):\r\n            output[i] = tf.identity(deepnn(x))\r\n\r\nTF will run the deepnn(x) one by one.\r\nWould it be possible to parallel it manually like this:\r\n\r\n    #pragma omp for", "comments": ["As I know, control_depencies() tensorflow can't treat 'for' loop in paralell\r\n\r\nor, how about using \r\n```\r\nwhile_loop(\r\n    cond,\r\n    body,\r\n    loop_vars,\r\n    shape_invariants=None,\r\n    parallel_iterations=10,\r\n    back_prop=True,\r\n    swap_memory=False,\r\n    name=None\r\n)\r\n```\r\ninstead of ' for ' loop ?", "Indeed, using TensorFlow's native control flow operations like tf.while is the best way to enable parallelism.", "Now that tf.function has become central in 2.0, a feature named tf.prange (parallel range) can be extremely helpful to manually place a single graph in parallel on GPU. This will help code to be debugged better.\r\n\r\nI request you to reconsider this to enable easier and faster code in tf2.0"]}, {"number": 12491, "title": "AttributeError: 'int' object attribute '__doc__' is read-only", "body": "In the file `tensorflow/compiler/tests/adagrad_test_poplar.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py\", line 191`, I have an error when trying to run unit tests.\r\n\r\nPython is unhappy with trying to assign to __doc__.\r\n\r\nThis is on OS/X, with a virtualenv containing all of the modules required for the head of master on 22nd August 2017.\r\n\r\nI have had to add `autograd` and `enum`, and as a consequence of the OS/X built-in numpy, I have switched to building tensorflow in a virtualenv.\r\n\r\n", "comments": ["I encountered a similar issue in Ubuntu 16.04 with python 2.7. Addressed the issue by installing `python-enum34` package which has the enum34 backport.", "Please re-open if the above-mentioned work around does not solve your issue...", "Hi\r\ni face the same problem on Ubuntu 16.10  when i try \r\n\r\n`:~/tensorflow$ bazel-bin/tensorflow/examples/image_retraining/retrain --image_dir ~/flower_photos`\r\nafter enter above line in terminal, the out put is \r\n\r\n`Traceback (most recent call last):\r\n  File \"/home/sam/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/examples/image_retraining/retrain.py\", line 108, in <module>\r\n    import tensorflow as tf\r\n  File \"/home/sam/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/sam/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 63, in <module>\r\n    from tensorflow.python.framework.framework_lib import *\r\n  File \"/home/sam/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/framework/framework_lib.py\", line 102, in <module>\r\n    from tensorflow.python.framework.importer import import_graph_def\r\n  File \"/home/sam/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/framework/importer.py\", line 30, in <module>\r\n    from tensorflow.python.framework import function\r\n  File \"/home/sam/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/framework/function.py\", line 34, in <module>\r\n    from tensorflow.python.ops import variable_scope as vs\r\n  File \"/home/sam/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py\", line 192, in <module>\r\n    \"\"\"\r\nAttributeError: 'int' object attribute '__doc__' is read-only`\r\n\r\n\r\n`\r\nI already install autograd and enum and python-enum34  but problem still there.\r\n", "I had the same result as you after installing `python-enum34` in Ubuntu 16.04 with python 2.7. I had to run `pip uninstall enum` to get back on track:", "@MrShaham  fyi `pip install enum34` for mac solves it for me", "I had same issue on mac, @leonmak suggestion worked for me.", "@leonmak  sorry for late reply. I've been busy with my studies. This method not for me either . i take different rote for achieving my goal. i upload my work as soon as possible.", "Uninstall enum and intall enum34.", "@wenguanwang  when i uninstall enum. it throws the error about. I had been installed enum34. \r\n```\r\nImportError: No module named enum\r\n```\r\n", "i had same issue on centos7,@leonmak suggestion worked for me.", "conda install enum34", "> Uninstall enum and intall enum34.\r\n\r\nthis is the right answer,thx!"]}]