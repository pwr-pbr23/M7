[{"number": 15997, "title": "Allow tensorflow/tensorflow/workspace.bzl to customize dependencies", "body": "When including tensorflow as a dependency of a Bazel project, it requires you to take all the declared dependencies in  `tensorflow/tensorflow/workspace.bzl` or none of them. Some projects, like the closure_rules allow you to customize the dependencies:\r\nhttps://github.com/bazelbuild/rules_closure/blob/master/closure/repositories.bzl#L21\r\n\r\nThis allows you to use whatever versions you want for specific dependencies that may be different from what tensorflow's workspace.bzl declares.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code N/A\r\nOS Platform and Distribution: Linux\r\nTensorFlow installed from: Source\r\nTensorFlow version: 1.4\r\nBazel version: Custom\r\nCUDA/cuDNN version: 7\r\nGPU model and memory: N/A", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "/CC @gunan what do you think of this idea?", "Bazel should not download the unused stuff in your workspace anyway, so I do not see a big upside compared to the much longer workspace file.\r\nYou said you are trying to use a different version of one of the TF dependencies? Is that your motivation?", "We use tensorflow as an external repository/dependency to our Bazel build.  In order to get all the dependencies in place we need to call tf_workspace() in our WORKSPACE file.  Sometimes though, for things like Eigen, we want to use our own version which corresponds to the rest of our project. So we want to exclude the eigen declaration in tf_workspace() and use our own.", "For a limited set of dependencies, I can be OK with this.\r\nMaybe not an exhaustive list like closure_rules, but maybe we can start with eigen."]}, {"number": 15920, "title": "cmake CUDA include-path whitespaces not supported", "body": "I'm not sure if this is intentional, but I just had some trouble with compiling tf_core_gpu_kernels due to a invalid include-dir path as command line argument given to nvcc (caused by a whitespace in the path).\r\n\r\nIn windows I was able to solve this by adding double quotes at line 277 in CMakeLists.txt:\r\n`set(CUDA_NVCC_FLAGS ${CUDA_NVCC_FLAGS};--include-path \\\"${PROJECT_BINARY_DIR}/$\\{build_configuration\\}\\\";--expt-relaxed-constexpr)`\r\n\r\nPossible errors when not using double quotes:\r\nnvcc fatal : A single input file is required for a non-link phase when an outputfile is specified\r\n\r\nAlso note that there will be problems because of CUDA not supporting some versions of msvc.\r\nThe current version for example is not yet supported.\r\nCUDA_HOST_COMPILER path is automatically set to $(VCInstallDir)/bin, what will cause problems on some systems. For VS2015 this works fine but not for VS2017 if you are using the recent compiler.\r\n\r\nThere are also several errors when using the intel compiler, as for example the typename TType<...> declarations, which are not really required. Either remove the typename keyword or use 'auto'.\r\nOther than that the master branch is compilable with ICC 18.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Marking as contributions welcome, since the CMake build is community supported.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you."]}, {"number": 15911, "title": "Crash when using CUDA API while using Tensorflow. \"current context was not created by the StreamExecutor cuda_driver API\"", "body": "### Expected behavior\r\n\r\nI expect to work with the CUDA library by using libcudart directly in my Python script before and while Tensorflow is being used - without errors.\r\n\r\n### Actual behavior\r\n\r\nThe script crashes with following error as soon as I've used libcudart through ctypes before tensorflow is imported.\r\n\r\n```\r\n2018-01-06 14:45:50.641211: F tensorflow/stream_executor/cuda/cuda_driver.cc:232] current context was not created by the StreamExecutor cuda_driver API: 0x7fe122002400; a CUDA runtime call was likely performed without using a StreamExecutor context\r\n```\r\n\r\n### Environment\r\n\r\n| Name  | Value |\r\n| ------------- | ------------- |\r\n| OS  | Mac OS 10.12.4 |\r\n| tensorflow-gpu  | pip 1.1.0, v1.1.0-rc0-61-g1ec6ed5  |\r\n| Device  | GPU Titan X pascal  |\r\n| Python  | 2.7.12  |\r\n| Python  | 3.5.2 |\r\n| CUDA  | 8.0  |\r\n| cuDNN  | 5.1.5 |\r\n\r\n\r\n### Reproduce\r\n\r\n```python\r\nfrom __future__ import print_function\r\n\r\nimport ctypes\r\nimport platform\r\n\r\nsystem = platform.system()\r\n\r\nif system == \"Linux\":\r\n    libcudart = ctypes.cdll.LoadLibrary(\"libcudart.so\")\r\nelif system == \"Darwin\":\r\n    libcudart = ctypes.cdll.LoadLibrary(\"libcudart.dylib\")\r\nelif system == \"Windows\":\r\n    libcudart = ctypes.windll.LoadLibrary(\"libcudart.dll\")\r\nelse:\r\n    raise Exception(\"Cannot identify system.\")\r\n\r\nversion = ctypes.c_int()\r\nrc = libcudart.cudaRuntimeGetVersion(ctypes.byref(version))\r\nif rc != 0:\r\n    raise ValueError(\"Could not get version\")\r\nif version.value < 6050:\r\n    raise Exception(\"CUDA version must be >= 6.5\")\r\n\r\nlibcudart.cudaSetDevice(0)\r\n\r\nfree = ctypes.c_size_t()\r\ntotal = ctypes.c_size_t()\r\nrc = libcudart.cudaMemGetInfo(ctypes.byref(free), ctypes.byref(total))\r\n\r\nprint(\"Memory \" + str(free.value) +  \" of \" + str(total.value))\r\n\r\ndel libcudart\r\n\r\nimport tensorflow as tf\r\n\r\nhello = tf.constant('Hello, TensorFlow!')\r\n\r\n# Start tf session\r\nsess = tf.Session()\r\n\r\n# Run the op\r\nprint(sess.run(hello))\r\n````\r\n\r\nExecute \r\n\r\n```\r\n$ python3 tf-cuda-crash.py\r\n```\r\n\r\nOutput will be like\r\n\r\n```\r\n$ python3 tf-cuda-crash.py\r\nMemory 9916960768 of 12884574208\r\n2018-01-06 14:45:50.639837: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-01-06 14:45:50.639863: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-01-06 14:45:50.639870: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-01-06 14:45:50.641211: F tensorflow/stream_executor/cuda/cuda_driver.cc:232] current context was not created by the StreamExecutor cuda_driver API: 0x7fe122002400; a CUDA runtime call was likely performed without using a StreamExecutor context\r\n[1]    4240 abort      python3 tf-cuda-crash.py\r\n```\r\n\r\nHappens with both Python2 and 3.\r\n  ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "They are filled out.", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "/CC @gunan, can you comment?", "Looks like the error is emitted from this line:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_driver.cc#L233\r\nI am not familiar with the code. @zheng-xq @jlebar any ideas?", "It would be helpful if you could do a bit of debugging yourself.  CUDADriver::CreateContext is responsible for creating the driver context TF uses.  Is it being called and successfully getting down to the cuCtxSetCurrent call?  If it is, it is CUDADriver::CreateContext called before or after you finish mucking with libcuda directly?\r\n\r\nI think the question is, why is the context not set correctly?  What's possibly happening is that by touching libcudart, you're implicitly creating a context.  Then when you import tensorflow, perhaps CUDADriver::CreateContext is called (it seems to be called on initialization).  I see a call to GetContext inside of CreateContext: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_driver.cc#L541.  That may be what's crashing.\r\n\r\nIf my guess at the etiology is correct above, I think I'd take a patch that modifies the relevant code so that, when we're in this particular codepath, GetContext does not crash if the current context was not created by streamexecutor.\r\n\r\nAlternatively, it's possible if you just moved the \"import tensorflow\" bit up, the initialization would happen before you touch libcudart, and everything would work.  I'm not sure.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "> It would be helpful if you could do a bit of debugging yourself. CUDADriver::CreateContext is responsible for creating the driver context TF uses. Is it being called and successfully getting down to the cuCtxSetCurrent call?\r\n\r\nI don't know C++ well enough to do any debugging here.\r\nI expect however, that a Cuda context is application bound, that means it shouldn't matter if some program (in my case Python script) should create a Cuda context and another program (in Tensorflow's case a Python C++ module), right?\r\n\r\n\r\n> Alternatively, it's possible if you just moved the \"import tensorflow\" bit up, the initialization would happen before you touch libcudart, and everything would work. I'm not sure.\r\n\r\nWhen I move `import tensorflow as tf` to line 2, I get exact same error.", "Well if you or anyone wants to fix this, I think my previous comment gives a pretty concrete outline of what to do, or at least something to try.  LMK if you're interested in taking this on and you can't figure out what I meant.", "I think TF loads all cuda libraries and creates the context when you create the session.\r\nSo maybe try importing TF and creating a session first, Then try doing all the rest?"]}, {"number": 15880, "title": "Allow full deallocation of GPU memory", "body": "When using the TF C++ library inside an application that also uses GPUs for other tasks (not implemented in TF), it would be useful to be able to deallocate all the GPU memory TF has allocated once the session is closed, and no further TF calls are expected for the time being. gpu_options.allow_growth keeps TF's allocated pool small, but it still can grow to several GB. Even after the session is deleted, the pool doesn't shrink. To free it up, the whole application must be restarted, if I'm not mistaken.\r\n\r\nBeing able to destroy the [ProcessState](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/process_state.cc) singleton seems to solve it without breaking anything. However, its destructor is protected. Alternatively, getting the Allocator for each GPU from ProcessState and manually destroying them does the trick, but renders TF unusable for all future operations because ProcessState still thinks the Allocators exist and doesn't recreate them when they are required again.\r\n\r\nI think making the ProcessState destructor public (or adding a public method to invoke similar code) would be the best solution, but maybe I'm missing an obvious solution that already exists?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@dtegunov I am also facing the same issue, I want to run multiple iteration and each iteration is model generation. Can you please provide your code to deallocate GPU memory by manually destroying them?", "@asimshankar Can you comment on this?", "@zheng-xq may be in a better position to comment on the pros-and-cons of allowing `ProcessState` destruction or a backdoor to releasing memory.", "@asimshankar The reason the memory belongs to ProcessState even after session is closed is because Variables lives longer than session. But with \"reset\", all variables and other resources are also released.\r\n\r\nSo for DirectSession, we can release all memory if the \"reset\" is called. If that is already done that way, we can certainly add that functionality.\r\n\r\nWhat do you think?", "From my understand the TensorFlow memory allocation is tied to the process state, given that as a fix I spawn sub process and ran the model generation as part of it. Once the subprocess is done, memory is clear out. ", "The fact that Tensorflow does not release GPU resources upon Session termination has been a gotcha causing users trouble for (evidently) at least 2 years: https://github.com/tensorflow/tensorflow/issues/1578 \r\n\r\nMany Github Issues have been opened, but for whatever reason the Tensorflow leadership has closed those issues without a fix.  \r\n\r\nIt seems reasonable that Tensorflow should provide proper storage duration.  The current behavior goes against the Google C++ style guide recommendations ( https://google.github.io/styleguide/cppguide.html#Static_and_Global_Variables ), so it's no surprise this problem causes Tensorflow users so much trouble and confusion.\r\n\r\nA proper fix likely requires considerable alignment among Tensorflow leadership and users.  Is there any chance this issue might be addressed before 1.13?", "@tensorflowbutler see for example https://github.com/tensorflow/tensorflow/issues/20387 ", "Since other users are still looking for a solution, here is the workaround I've been using in my projects:\r\n\r\nThe trick is to make the destructor in tensorflow/core/common_runtime/process_state.h public instead of protected. Now you can call tensorflow::ProcessState::singleton()->~ProcessState() to free all memory. TF 1.10 is the latest version I've tested for this. Here is a fork with the code already modified: https://github.com/dtegunov/tensorflow_1.10_windows\r\n\r\nMake sure to destroy any TF objects that rely on GPU memory prior to doing this, as there is no mechanism to make them aware of the deallocation. After calling the destructor, you can create new TF objects without any issues.\r\n\r\nThis solution is only guaranteed to work if you can ship a custom TF binary with your product. With the official builds, I think you're running into undefined behavior in C++ when forcibly calling a method that wasn't marked public at compile time.\r\n\r\nIf performance is not a priority, you can also look into different allocation mechanisms offered in TF. You can force it to call cudaMalloc/cudaFree for every allocation/deallocation instead of grabbing a large chunk and holding it indefinitely. However, this will be much slower than the default allocator. See tensorflow/core/common_runtime/gpu/gpu_process_state.cc for details.", "Note that Sessions are deprecated with TensorFlow 2.0 so this shouldn't be an issue for training.\r\nhttps://medium.com/tensorflow/effective-tensorflow-2-0-best-practices-and-whats-changed-a0ca48767aff\r\n\r\nHowever, Sessions are also useful for inference in C++ so I will keep this open and will look into deallocating memory on session destruction at a lower priority. ", "Very Hope tensorflow official can provide the release gpu memory function, or just make it release after end this thread (not process).", "I'm also encountering this issue with TF 1.13.1 in eager mode so it's not only TF session related. So far I'm out of luck for finding a solution =/", "> Since other users are still looking for a solution, here is the workaround I've been using in my projects:\r\n> \r\n> The trick is to make the destructor in tensorflow/core/common_runtime/process_state.h public instead of protected. Now you can call tensorflow::ProcessState::singleton()->~ProcessState() to free all memory. TF 1.10 is the latest version I've tested for this. Here is a fork with the code already modified: https://github.com/dtegunov/tensorflow_1.10_windows\r\n> \r\n> Make sure to destroy any TF objects that rely on GPU memory prior to doing this, as there is no mechanism to make them aware of the deallocation. After calling the destructor, you can create new TF objects without any issues.\r\n> \r\n> This solution is only guaranteed to work if you can ship a custom TF binary with your product. With the official builds, I think you're running into undefined behavior in C++ when forcibly calling a method that wasn't marked public at compile time.\r\n> \r\n> If performance is not a priority, you can also look into different allocation mechanisms offered in TF. You can force it to call cudaMalloc/cudaFree for every allocation/deallocation instead of grabbing a large chunk and holding it indefinitely. However, this will be much slower than the default allocator. See tensorflow/core/common_runtime/gpu/gpu_process_state.cc for details.\r\n\r\nHi @dtegunov thanks for your suggesting. Can you expand more on \"Make sure to destroy any TF objects that rely on GPU memory prior to doing this\"? How to destroy TF objects? Thanks in advance.", "TensorFlow's C API has a method called TF_DeleteTensor if I remember correctly, and probably similar methods for other object types that can consume GPU memory. If you don't destroy the objects, they will still have pointers to memory that has been freed through ~ProcessState(), and will fail the next time they try to access that memory.", "Same issue, need to solve.", "Hi @dtegunov ! Have you checked this[ thread](https://www.py4u.net/discuss/17174) on deallocating Gpu memory yet?", "@mohantym the proposed workarounds in that thread are for python, @dtegunov asked about C++."]}, {"number": 15847, "title": "Reinitializing an iterator throws an OutOfRangeError when using a MonitoredSession with NanTensorHook", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: \r\nWindows 10\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.4.0\r\n- **Python version**: \r\n3.5.4\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\nedit: none, using CPU only\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\nSee below under source code\r\n\r\n### Describe the problem\r\nWhen using a Monitored Training Session with a NanTensorHook and an Iterator from a Dataset, reinitializing the Iterator causes an OutOfRangeError. \r\nThis is likely because the NanTensorHook adds the loss value to the SessionRunArgs, but the evaluation of the loss-value then fails since no more data are available from the iterator.\r\n\r\nIdeally, there should be a way to reinitialize the iterator without the hooks beeing executed.\r\n\r\n### Source code / logs\r\nexample code to reproduce the problem\r\n```\r\nimport tensorflow as tf\r\n\r\ndataset = tf.data.Dataset.range(100)\r\ndataset = dataset.map(lambda x: (0, x))\r\ndataset = dataset.batch(64)\r\niterator = dataset.make_initializable_iterator()\r\n(label, element) = iterator.get_next()\r\n\r\n# pseudo loss for the NanTensorHook\r\nloss = tf.reduce_mean(1 - label)\r\n\r\nglobal_step = tf.train.get_or_create_global_step()\r\nscaffold = tf.train.Scaffold(local_init_op=iterator.initializer)\r\n\r\nwith tf.train.MonitoredTrainingSession(\r\n        scaffold=scaffold,\r\n        hooks=[tf.train.NanTensorHook(loss_tensor=loss)]) as sess:\r\n    # Compute for 5 epochs.\r\n    for epoch in range(5):\r\n        print('epoch: ' + str(epoch))\r\n        try:\r\n            while not sess.should_stop():\r\n                sess.run(element)\r\n        except tf.errors.OutOfRangeError:\r\n            print('end')\r\n\r\n        if sess.should_stop():\r\n            break\r\n\r\n        # the following line silently fails, since an OutOfRangeError is thrown\r\n        sess.run(iterator.initializer)\r\n```\r\nRemoving the NanTensorHook or placing the `sess.run(iterator.initializer)` call inside an try-except-statement provides a workaround for this problem\r\n```\r\n        try:\r\n            sess.run(iterator.initializer)\r\n        except tf.errors.OutOfRangeError:\r\n            print('Out of range errors occurs')\r\n```\r\n\r\n  ", "comments": ["@Hackempluf this feels like a feature request to me, rather than a bug. OK to label it as such, or is there a particular buggy behavior that you want addressed?", "Yes, this would be a feature request, although I'm not sure how useful/needed it really is.\r\n\r\nIn the mean-time I switched to ```tf.train.SingularMonitoredSession``` instead, as it allows me to access the raw session object to reinitialise the iterator and evaluate the model every n epochs without any hooks beeing called. Since I don't have a distributed setting this solves my problem.\r\n\r\nIf there is no need for it in distributes settings, fell free to close the issue."]}, {"number": 15771, "title": "Flexible input shape for map method in class RandomFourierFeatureMapper", "body": "This is a feature request. In tensorflow r1.4, Class RandomFourierFeatureMapper, map method,\r\nThe shape of the input must be, [batch_size, self._output_dim].   \r\n\r\nThere can be scenarios where in each batch there are multiple training point, essentially i'm proposing a scenario where the input shape  must be  [batch_size, x ,self._output_dim].   \r\n\r\nWhich is not possible in current API.        \r\nIn the matmul we can see that it is mentioned, \"The inputs must, following any transpositions, be tensors of rank >= 2 where the inner 2 dimensions specify valid matrix multiplication arguments, and any further outer dimensions match.\" which is in-fact very flexible.     \r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler As this is a feature request, not a bug, I think those fields are irrelevant here.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "/CC @petrosmol", "@dchatterjee172 \r\nThanks for submitting this feature request. Just to clarify:\r\nThe current operation does support multiple training points per batch but each data point is a 1-dim tensor. However, I could definitely see a scenario where each data point is a multi-dim tensor. For instance batch of 2-D images, in which case input tensor would be 3-D ([batch_size, i, j]). Is this the use case you have in mind?", "@petrosmol \r\nYes that's a possible case. But there can be many possible other use cases. For example in machine translation or machine comprehension you can have, [batch_size, words ,self._output_dim]. In my opinion as kernel tricks are essentially important part of machine learning, [batch_size, x ,self._output_dim] this input format will generalize this kernel approximation method.\r\n\r\n", "Just to clarify, you are referring to self._input_dim not self._output_dim in this thread right? For instance you want to allow shape [batch_size, x, self._input_dim] for ]the input dimension, correct?\r\n\r\nIn any case, I can add a TODO, but I don't have an ETA for this.", "@petrosmol \r\nYa you are right [batch_size, x, self._input_dim]. \r\nYa that's fine ", "If you can add a \"contributions welcome\" label then that would be great", "@dchatterjee172 \r\n\r\nI have been working on some updates on this library recently and rethinking your suggestion. In the examples you have in mind, what is the intended **output**?\r\nIs it [batch_size, self._output_dim] or [batch_size, x, self._output_dim]?\r\n\r\nOr are you suggesting the method internally reshapes [batch_size, x, self._input_dim] (rank 3) tensors to [batch_size, x * self._input_dim] (rank 2)?\r\n\r\nFor the former, I need to think more as to what explicit feature maps (with random Fourier features) mean in that case. The latter is tricky. It is better if the user reshapes before calling the map method.\r\n\r\nThanks for clarifying", "@petrosmol \r\nThe output should be also [batch_size, x, self._output_dim]. For example if you have a sentence, where x is the sentence length or how many words the sentence has, and self._output_dim will be the size of word vectors. "]}, {"number": 15722, "title": "Image Adjustments API doesn't clearly specify input range", "body": "### Describe the problem\r\n\r\nThere is a documentation issue with a possible corresponding tf.keras bug.\r\nThe [tf image adjustments guide](https://www.tensorflow.org/api_guides/python/image#Image_Adjustments) doesn't document the inputs. It appears they can be in a `[0, 1]` range or `[0, MAX]` based on comments in tf.slim where the relevant APIs are used. \r\n\r\nThis may have led to preprocessing bugs in other utilities such as keras and tf.keras, which believe tf expects the range `[-1, 1]` see https://github.com/keras-team/keras/issues/8916, the following includes additional details from that issue:\r\n\r\nIt appears Keras' imagenet image preprocessing may be inconsistent with how it is done in tf, in particular keras sets values to `[-1, 1]` while in tf the expected range is `[0, 1]`.\r\n\r\n- [slim/preprocessing/inception_preprocessing.py](https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py#L284) \r\n    - notes `If dtype is tf.float32 then the range should be [0, 1]`\r\n- [keras preprocess_input()](https://github.com/keras-team/keras/blob/master/keras/applications/imagenet_utils.py#L72) \r\n    - notes `tf: will scale pixels between -1 and 1, sample-wise`.\r\n\r\nHere is the key doc from Keras' `preprocess_input()`:\r\n\r\n```python\r\n\r\ndef preprocess_input(x, data_format=None, mode='caffe'):\r\n    \"\"\"Preprocesses a tensor encoding a batch of images.\r\n    # Arguments\r\n        x: input Numpy or symoblic tensor, 3D or 4D.\r\n        data_format: data format of the image tensor.\r\n        mode: One of \"caffe\", \"tf\".\r\n            - caffe: will convert the images from RGB to BGR,\r\n                then will zero-center each color channel with\r\n                respect to the ImageNet dataset,\r\n                without scaling.\r\n            - tf: will scale pixels between -1 and 1,\r\n                sample-wise.\r\n    # Returns\r\n        Preprocessed tensor.\r\n    \"\"\"\r\n```\r\n\r\nHere are the key docs from the tf slim function `preprocess_for_train()` which specify a [0, 1] range:\r\n\r\n```python\r\ndef preprocess_for_train(image, height, width, bbox,\r\n                         fast_mode=True,\r\n                         scope=None,\r\n                         add_image_summaries=True):\r\n  \"\"\"Distort one image for training a network.\r\n  Distorting images provides a useful technique for augmenting the data\r\n  set during training in order to make the network invariant to aspects\r\n  of the image that do not effect the label.\r\n  Additionally it would create image_summaries to display the different\r\n  transformations applied to the image.\r\n  Args:\r\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\r\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\r\n      is [0, MAX], where MAX is largest positive representable number for\r\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\r\n    height: integer\r\n    width: integer\r\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\r\n      where each coordinate is [0, 1) and the coordinates are arranged\r\n      as [ymin, xmin, ymax, xmax].\r\n    fast_mode: Optional boolean, if True avoids slower transformations (i.e.\r\n      bi-cubic resizing, random_hue or random_contrast).\r\n    scope: Optional scope for name_scope.\r\n    add_image_summaries: Enable image summaries.\r\n  Returns:\r\n    3-D float Tensor of distorted image used for training with range [-1, 1].\r\n  \"\"\"\r\n```\r\n\r\nside note: \r\n\r\n- https://github.com/tensorflow/models/issues/2217 seems relevant\r\n- https://github.com/keras-team/keras/issues/8916 is the corresponding keras issue\r\n\r\n### System information\r\n\r\nNot relevant, this is a documentation + input value range issue.\r\n\r\n### Source code / logs\r\n\r\nrelevant links + code included above", "comments": ["@fchollet @anj-s : Mind taking a look?", "I believe I've got it working, there isn't a bug, but the APIs were simply not clear about the preconditions and postconditions of each function. Here is a summary, please correct me if I misunderstood:\r\n\r\n[tf image adjustments guide](https://www.tensorflow.org/api_guides/python/image#Image_Adjustments) APIs:\r\n\r\n1. Given raw RGB image with values in the range `[0, 255]` \r\n2. divide by 255, output values are in `[0, 1]` range.\r\n3. expected input range of the image augmentation functions is `[0, 1]`, they can be called at this point if training, skip otherwise\r\n4. subtract 0.5 from that output and multiply by 2 to get a range of `[-1, 1]`\r\n5. The expected input range when you actually feed the data to the various pretrained neural networks in the model zoo is `[-1, 1]`.\r\n\r\nfor [keras preprocess_input()](https://github.com/keras-team/keras/blob/master/keras/applications/imagenet_utils.py#L72) in `tf` mode:\r\n\r\n1. Input is a raw RGB 0-255 image and output is `[-1, 1]`\r\n    - if it is inference time this can be fed directly to networks and in that case you are done.\r\n2. If you wanted to utilize tf augmentation you must divide by 2 and add 0.5, to put values in `[0, 1]` range.\r\n3. apply the augmentation from [tf image adjustments guide](https://www.tensorflow.org/api_guides/python/image#Image_Adjustments) APIs\r\n4. subtract 0.5 from that output and multiply by 2 to get a range of `[-1, 1]`\r\n5. perform training step\r\n\r\n\r\nPerhaps this can be noted clearly in these APIs?", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "If someone could just point me to the location in the code, I could at least write a small docstring.", "@ahundt I am currently working on verifying which APIs will need this doc change. Will send a PR asap. Thanks for the clear explanation above.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "@ahundt Why will you call [tf.image.per_image_standardization()](https://www.tensorflow.org/api_docs/python/tf/image/per_image_standardization) to bring the image from `[0, 255]` to `[0, 1]`? Why shouldn't we just cast the image as `float` first and then divide by `255`? Then subtract 0.5 and multiply by 2. You will get an image that has values lying in `[-1, 1]`. This is followed in Keras Inception preprocessing as seen [here](https://stackoverflow.com/questions/44341258/preprocessing-function-of-inception-v3-in-keras), [here](https://github.com/keras-team/keras/issues/5416), [here](https://github.com/fchollet/deep-learning-models/blob/ccd0eb24996b4cbff4231b90cd44b057c0b20f14/inception_v3.py#L391) and [here](https://stackoverflow.com/a/42276427/1586200).\r\n\r\n@anj-s  Which version of preprocessing is correct? The one suggested by @ahundt or the one mentioned in this comment?\r\n\r\nAlso, [inception_preprocessing.py](https://github.com/tensorflow/models/blob/31adae5327c0258ce630f7b3e9f3cf0df78e3ba6/research/slim/preprocessing/inception_preprocessing.py#L279) does not do per image standardization, just subtracts 0.5 and multiplies by 2. I hope that is correct.", "@parag2489 good question. I may have it incorrect and that's why I created this issue hoping to clarify things. It seems there are a couple styles of preprocessing:\r\n\r\n```\r\n      'cifarnet': cifarnet_preprocessing,\r\n      'inception': inception_preprocessing,\r\n      'resnet_v1_200': vgg_preprocessing,\r\n```\r\n\r\nAs found in https://github.com/tensorflow/models/issues/2217 and https://github.com/tensorflow/models/tree/31adae5327c0258ce630f7b3e9f3cf0df78e3ba6/research/slim/preprocessing", "@parag2489 Thanks for your comments. I re-read several places where preprocessing is done and it seems you're right, dividing by 255 is the most common image preprocessing to do in tf. I've updated https://github.com/tensorflow/tensorflow/issues/15722#issuecomment-354625196 accordingly. \r\n\r\nHowever, It is important to note `img/255` is not the only valid approach, as you can see from [keras' preprocess_input function](https://github.com/keras-team/keras/blob/master/keras/applications/imagenet_utils.py#L149), which has improved since my initial post. There are options for 'tf', 'torch', and 'caffe' style preprocessing which all vary. The most important conclusion is to evaluate using the same preprocessing algorithm you trained your weights on, excluding the image augmentation steps.\r\n\r\nAs for the tf documentation here I'm hopeful it can be updated to capture my outline, so others can be saved the trouble. \r\n\r\n@anj-s just wanted to ping you, any availability on this?\r\n", "ahundt@ sorry for the delay, I haven't had a chance to get to this yet. However you can update the doc string [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/image_ops_impl.py#L847) and open a PR. \r\n\r\nBoth types of image processing are consistent from what I see.  \r\n@asimshankar  Do you know the recommended approach?", "hi \r\ngreat post,\r\ni m trying to run balloon detection API Mask_RCNN, and i get stuck with this error \r\n\r\ninput that isn't a symbolic tensor\r\n\r\nany suggestion to work this out?"]}, {"number": 15694, "title": "Latency of simple tf.data.Dataset transformations is higher than raw Python", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: pip install tensorflow (CPU only)\r\n- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0\r\n- **Python version**: 3.5.2\r\n\r\n### Describe the problem\r\nI'm trying to improve performance by moving to `tf.data.Datasets` to manage epochs and minibatches (particularly I/O performance on a GPU when I come to scaling up). However I'm finding that this is much slower than just using nested `for` loops in Python.\r\n\r\n### Source code / logs\r\nHere's an example using a dataset with 10,000 numbers, 10 epochs and a minibatch size of 100:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom time import time\r\n\r\nMINI_BATCH = 100\r\nEPOCHS = 10\r\n\r\n# Dataset consisting of 10000 random numbers\r\nraw_data = np.random.randn(10000)\r\n\r\n# Raw Python implementation\r\nstart = time()\r\nsplit_data = np.split(raw_data, 10000 // MINI_BATCH)\r\n\r\nfor _ in range(EPOCHS):\r\n    for i, batch in enumerate(split_data):\r\n        # Do stuff with batch data\r\n        x = batch * 2\r\nprint(\"Raw Python done in\", time() - start)\r\n\r\n\r\n# TensorFlow implementation\r\nstart = time()\r\ndataset = tf.data.Dataset.from_tensor_slices(raw_data)\r\ndataset = dataset.repeat(EPOCHS)\r\ndataset = dataset.batch(MINI_BATCH)\r\niterator = dataset.make_one_shot_iterator()\r\nnext_chunk = iterator.get_next()\r\n\r\nwith tf.Session() as sess:\r\n    while True:\r\n        try:\r\n            batch = sess.run(next_chunk)\r\n            # Do stuff with batch data\r\n            x = batch * 2\r\n        except tf.errors.OutOfRangeError:\r\n            break\r\nprint(\"TensorFlow done in\", time() - start)\r\n\r\n```\r\nOutput:\r\n```\r\nRaw Python done in 0.0011773109436035156\r\nTensorFlow done in 0.14212393760681152\r\n```\r\n\r\nDoes anyone know why this might be the case? \r\n\r\nI'm guessing that most of the overhead is in the evaluation of `iterator.get_next()` on every loop. If this is not supposed to be evaluated it would be useful to have some examples of how it should be used without using `sess.run` each time.\r\n", "comments": ["The short answer here is that building a single minibatch of 100 elements is too small an operation to benefit from TensorFlow as it's currently implemented. (You'll also find that multiplying two small matrices is faster in NumPy than loading it into TensorFlow.)\r\n\r\nThe `Dataset` implementation is designed to perform non-trivial amount of preprocessing (decoding images, parsing text, etc.); and the `Session` implementation is geared towards computations that take at least a few milliseconds to perform. In the intended use case, you would pass the output of `iterator.get_next()` to the input of a neural network, for inference or training.\r\n\r\nEach of the TensorFlow operations in your example takes approximately 140 microseconds. Calling into TensorFlow from Python currently has an overhead of around 60 microseconds, a trivial step takes around 20 microseconds, and invoking an operation such as `Iterator.get_next()` incurs a few more microseconds of overhead, plus a context switch. \r\n\r\nThere are a couple of things you could try:\r\n\r\n* Try using a `tfe.Iterator` in Eager mode, which acts more like a Python iterator, and takes approximately 100us per batch in my tests.\r\n* Creating a callable for the step, using `get_next_chunk = sess.make_callable(next_chunk)` and then using `get_next_chunk()` in place of `sess.run(next_chunk)` shaves about 65us off each invocation (to approximately 92us per batch) in my tests.\r\n* Rewrite the pipeline to cache the processed data for an epoch:\r\n\r\n    ```python\r\n    dataset = tf.data.Dataset.from_tensor_slices(raw_data)\r\n    dataset = dataset.batch(MINI_BATCH)\r\n    dataset = dataset.cache()\r\n    dataset = dataset.repeat(EPOCHS)\r\n    iterator = dataset.make_one_shot_iterator()\r\n    ```\r\n\r\n  This change reduces the cost of each mini-batch to about 42us in my tests (and ~52us in Eager mode). \r\n\r\nWhere does the rest of the time go? In the non-caching versions, the data are currently copied twice, out of the original array in `Dataset.from_tensor_slices()`, then into the batch in `Dataset.batch()`. We could potentially use `Tensor::Slice()` to avoid a deep copy in some cases, although that only works when the slices are aligned to 32-byte boundaries, which might not always be the case. (It isn't for a vector of shape `(10000,)` `int64` elements but it is if e.g. you reshape it to `(100, 100)`.) Alternatively, and at the cost of some memory inflation, `Dataset.from_tensor_slices()` could eagerly slice up its input ahead of time, where the up-front initialized cost would be amortized over multiple epochs... this is what the explicit `Dataset.cache()` does for you. There are more context switches than are strictly necessary, especially when running in Eager mode, where the work can happen on a donated Python thread. Finally, profiling suggests that there are some non-trivial overheads when dealing with tensors of a single element (shape calculations, in particular, weigh heavily in the profile when the cost of the actual copy is small). \r\n\r\nThere are definitely some overheads worth paring down here, although they are likely to have only a small effect on headline performance numbers, so fixing them won't be top priority. I'll add some benchmarks to track the various versions, and invite contributions that improve things for all workloads.", "@mrry  I have another small example where seems it cannot fulfill all the cpus with num_parallel_calls. \r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\nimport sys\r\nimport psutil\r\n\r\ndef read_fn(pairs):\r\n    res = np.zeros((5000, 5000, 3)).astype(np.float32)\r\n    return res\r\n\r\n\r\ndef main():\r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n\r\n    pairs = [['a'],['b'],['c'],['d'],['e'],['f'],['g'],['h'],['i'],['l'],['m'],['n']]\r\n\r\n    num_cpu = len(os.sched_getaffinity(0))\r\n    print(num_cpu)\r\n    dataset = tf.data.Dataset.from_tensor_slices(pairs)\r\n\r\n    dataset = dataset.map(\r\n           lambda x: tf.py_func(\r\n             func=read_fn, inp=[x],\r\n             Tout=[tf.float32], stateful=False\r\n           ),\r\n           num_parallel_calls=num_cpu)\r\n    dataset = dataset.repeat(20000)\r\n\r\n    iterator = dataset.make_one_shot_iterator()\r\n    next_element = iterator.get_next()\r\n\r\n    count = 0\r\n    with tf.Session() as sess:\r\n        while (True):\r\n            try:\r\n                a = (sess.run(next_element))\r\n                print(psutil.cpu_percent(interval=0.0, percpu=False))\r\n                count=count+1\r\n                print(count)\r\n\r\n            except Exception as err:\r\n                print(count)\r\n                print(err)\r\n                sys.exit(0)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```", "@mrry What do you think?", "@bhack `tf.py_func()` has to acquire the GIL in order to run `read_fn`, so unless there is a substantial amount of work in that function that happens while the GIL is released, it is unlikely that you will see much of a parallel speedup for that code.", "@mrry We need to have a decent amount of computation to surpass the GIL overhead right? ", "Yes, plus that computation has to happen in a method that releases the GIL (e.g. many NumPy routines).", "@mrry Do you think to add something about this in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/datasets_performance.md? I think it could be useful."]}, {"number": 15644, "title": "[Feature request] define axis in 'tf.unique()' and 'tf.unique_with_counts'", "body": "Hi, just a small feature request:\r\nIt would be cool if one could directly\r\n* use 'tf.unique()' and 'tf.unique_with_counts' in n-dimensional arrays\r\n* define an axis along which 'tf.unique()' and 'tf.unique_with_counts' are applied\r\n", "comments": ["The `axis` support for `tf.unique` was added in PR #12952 though it has not been enabled in python yet, as it needs to go through the API deprecation process of 3 weeks. I think it could be enabled by now. Will submit a PR shortly.", "Marking as contributions welcome since @yongtang has a PR to address this. Thanks!", "I have created a workaround for tf.unique(2D_tensor). Pass any 2D tensor to the below function and it will return a unique 2D tensor.\r\n\r\n```\r\ndef tf_unique_2d(x):\r\n    x_shape=x.get_shape() #(3,2)\r\n    x1=tf.tile(x,[1,x_shape[0]]) #[[1,2],[1,2],[1,2],[3,4],[3,4],[3,4]..]\r\n    x2=tf.tile(x,[x_shape[0],1]) #[[1,2],[1,2],[1,2],[3,4],[3,4],[3,4]..]\r\n\r\n    x1_2=tf.reshape(x1,[x_shape[0]*x_shape[0],x_shape[1]])\r\n    x2_2=tf.reshape(x2,[x_shape[0]*x_shape[0],x_shape[1]])\r\n    cond=tf.reduce_all(tf.equal(x1_2,x2_2),axis=1)\r\n    cond=tf.reshape(cond,[x_shape[0],x_shape[0]]) #reshaping cond to match x1_2 & x2_2\r\n    cond_shape=cond.get_shape()\r\n    cond_cast=tf.cast(cond,tf.int32) #convertin condition boolean to int\r\n    cond_zeros=tf.zeros(cond_shape,tf.int32) #replicating condition tensor into all 0's\r\n\r\n    #CREATING RANGE TENSOR\r\n    r=tf.range(x_shape[0])\r\n    r=tf.add(tf.tile(r,[x_shape[0]]),1)\r\n    r=tf.reshape(r,[x_shape[0],x_shape[0]])\r\n\r\n    #converting TRUE=1 FALSE=MAX(index)+1 (which is invalid by default) so when we take min it wont get selected & in end we will only take values <max(indx).\r\n    f1 = tf.multiply(tf.ones(cond_shape,tf.int32),x_shape[0]+1)\r\n    f2 =tf.ones(cond_shape,tf.int32)\r\n    cond_cast2 = tf.where(tf.equal(cond_cast,cond_zeros),f1,f2) #if false make it max_index+1 else keep it 1\r\n\r\n    #multiply range with new int boolean mask\r\n    r_cond_mul=tf.multiply(r,cond_cast2)\r\n    r_cond_mul2=tf.reduce_min(r_cond_mul,axis=1)\r\n    r_cond_mul3,unique_idx=tf.unique(r_cond_mul2)\r\n    r_cond_mul4=tf.subtract(r_cond_mul3,1)\r\n\r\n    #get actual values from unique indexes\r\n    op=tf.gather(x,r_cond_mul4)\r\n\r\n    sess=tf.Session()\r\n    return (op)\r\n```\r\n\r\n```\r\nimport tensorflow as tf\r\nip=tf.constant([[1,2,1],[3,4,1],[5,6,1],[1,2,1]])\r\nop=tf_unique_2d(ip)\r\nprint(sess.run(op))\r\n#op = [[1,2,1],[3,4,1],[5,6,1]]\r\n```", "Automatically closing this out since I understand it to be resolved by the PR #12952 (merged already), but please let me know if I'm mistaken.Thanks!", "> * use 'tf.unique()' and 'tf.unique_with_counts' in n-dimensional arrays\r\n\r\nHello, is there still no support for tf unique with multi-dimensional arrays?", "Ya I also need to use unique with axis. Please fix this as soon as possible.", "@jvishnuvardhan You should clearly re-open this issue. This has not yet been solved in TF 2.1. The Python API does not seem to support this. I don't even know why you introduced a method `tf.unique` only for 1-D arrays in a framework/library that should clearly support multi-dimensional arrays by default. It seems that something went wrong when accepting feature requests in the past. This feature is so useful. It's incredible that a framework like TF does not support some basic features like this one (since like its first release).\r\n\r\nThis easiest workaround is to `tf.reshape` the tensor to be 1d and then use `tf.unique` (or the other method). See https://stackoverflow.com/a/60046022/3924118.", "@nbro Reopening this. Thanks!", "Any progress on this?  My use case is to count unique bitstrings in a 2-D tensor.  Example input tensor:\r\n```python\r\ntf.Tensor(\r\n[[ True False  True]\r\n [ True False  True]\r\n [ True False  True]\r\n [False False False]\r\n [False False False]], shape=(5, 3), dtype=bool)\r\n```\r\nwhich should return\r\n```python\r\ny ==> [[True, False, True], [False, False, False]]\r\nidx ==> [0, 0, 0, 1, 1]\r\ncount ==> [3, 2]\r\n```", "Lordy! Can we please have the `tf.unique` with axis!? It's been **964 days** since this issue was created. ", "+1", "@spate141 - one workaround (no gradients available) - \r\n```\r\nx.shape  # (100, 2)\r\nunique_results = tf.raw_ops.UniqueV2(x=not_unique, axis=0).y\r\nunique_results.shape  # (89, 2)\r\n```\r\n\r\nSee here: \r\nhttps://www.tensorflow.org/api_docs/python/tf/raw_ops\r\nhttps://www.tensorflow.org/api_docs/python/tf/raw_ops/UniqueV2\r\n", "@Hoeze,\r\nSorry for the delayed response. It looks like [UniqueV2 ](https://www.tensorflow.org/api_docs/python/tf/raw_ops/UniqueV2) and [UniqueWithCountsV2](https://www.tensorflow.org/api_docs/python/tf/raw_ops/) has **`axis`** argument. Can you please confirm if this is what you are looking for? Thanks!", "Thanks @rmothukuru, this looks indeed like what I was looking for.\r\nI remember that the UniqueV2 and similar functions were not accessible via the normal Tensorflow API, but unfortunately I did not try tensorflow for years now.\r\n\r\nSince there are many other people interested in this feature, maybe someone of you could have a try?", "@Hoeze,\r\nThank you for your response. Sure, I will wait for the community to respond about it. ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Commenting just to reopen this issue because I believe that having `tf.unique()` working only for 1D tensor is an avoiding limitation.", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 15624, "title": "Problem with tf.data.Dataset managing shapes of sparse tensors", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04.3 LTS (Xenial Xerus)\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.5.0-dev20171224\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n\r\n### Describe the problem\r\nThe tf-1.5-dev supports `tf.SparseTensor` when using `tf.data.Dataset.from_tensor_slices`, but it cannot infer the shape of new tensor after some operations such as `tf.data.Dataset.map`.\r\n\r\nThe shape of tensor becomes `Unknown`, which is troublesome for downstream operations. For example, we have to call `set_shape()` if we want to feed the new tensor into a `tf.layers.dense`.\r\n\r\n### Source code / logs\r\n<pre>\r\nimport tensorflow as tf\r\nx = tf.SparseTensor([[0,0],[1,1],[2,2]], [1,1,1], dense_shape=[3,3])\r\nds = tf.data.Dataset.from_tensor_slices(x)\r\nds.output_shapes   # TensorShape([Dimension(3)])\r\nds = ds.map(lambda x: tf.sparse_tensor_to_dense(x))\r\nds = ds.batch(1)\r\nds.output_shapes   # TensorShape([Dimension(None), Dimension(None)])\r\n\r\niterator = ds.make_one_shot_iterator()\r\nnext_elem = iterator.get_next()   # TensorShape([Dimension(None), Dimension(None)])\r\n\r\ny = tf.layers.dense(next_elem, 100)\r\nValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.\r\n\r\n</pre>\r\n@mrry ", "comments": ["Maybe you can get answer in strack overflow.", "@Achencan I know that we can explicitly set the shape of new tensor via `next_elem.set_shape([None, 3])`.\r\nBut I do not understand why the last dimension becomes `Unknown` after `tf.sparse_tensor_to_dense`.", "@mrry can you comment?", "The `tf.SparseTensor` support for shape inference apparently cannot handle the case where the sparse tensor is the input to a function or the output of a `tf.data.Iterator`. Unfortunately, `tf.SparseTensor` lacks the equivalent of `tf.Tensor.set_shape()`, which we use to provide additional shape information when it is available, so this information gets dropped on the floor.\r\n\r\nI'll assign this issue to @jsimsa, since he implemented the `tf.SparseTensor` support in `tf.data`, but note that he's busy with more important things right now, so I'll also mark it as \"contributions welcome\" if anybody wants to contribute a fix in the meantime.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "I hacked together a quick set_shape for SparseTensor in tensorflow/python/framework/sparse_tensor.py. I don't know if this is acceptable for merging into the  main Tensorflow code but I was basically desperate for something to use for my project.\r\n\r\nHere's the relevant part:\r\n\r\n```\r\n  def get_shape(self):\r\n    \"\"\"Get the TensorShape representing the shape of the dense tensor.\r\n    Returns:\r\n      A TensorShape object.\r\n    \"\"\"\r\n    if self.static_shape is not None:\r\n      return self.static_shape\r\n    else:\r\n      return tensor_util.constant_value_as_shape(self._dense_shape)\r\n\r\n  def set_shape(self, shape):\r\n      \"\"\"Updates the shape of this tensor.\r\n      \"\"\"\r\n      if not isinstance(shape, tensor_shape.TensorShape):\r\n        shape = tensor_shape.TensorShape(shape)\r\n      self.static_shape = shape \r\n```\r\n\r\nAnd then I added the following into __init__:\r\n\r\n```\r\nself.static_shape = None\r\n```", "I know this is an old issue, but is there any progress towards official support for `SparseTensor.set_shape`?"]}, {"number": 15557, "title": "tensorflow::gtl::string_as_array crashes on Windows", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.4.1\r\n- **Python version**: \r\n3.5.4\r\n- **Bazel version (if compiling from source)**:\r\nNone\r\n- **GCC/Compiler version (if compiling from source)**:\r\nNone\r\n- **CUDA/cuDNN version**:\r\nNone\r\n- **GPU model and memory**:\r\nNone\r\n- **Exact command to reproduce**:\r\nNone\r\n\r\n### Describe the problem\r\nCompile and run the following code in VS 2017 in **Debug** Mode.\r\n```c++\r\n#include <iostream>\r\n#include <stdint.h>\r\nusing namespace std;\r\n\r\ninline char* string_as_array(string* str) { return &*str->begin(); }\r\nint main()\r\n{\r\n\tstring str;\r\n\tchar* p = string_as_array(&str);\r\n\tstd::cout << (uint64_t)p << std::endl;\r\n\treturn 0;\r\n}\r\n```\r\n\r\nIt will crash.\r\n\r\n```\r\nDebug Assertion Failed!\r\n\r\nProgram: C:\\WINDOWS\\SYSTEM32\\MSVCP140D.dll\r\nFile: c:\\program files (x86)\\microsoft visual studio\\2017\\enterprise\\vc\\tools\\msvc\\14.12.25827\\include\\xstring\r\nLine: 1219\r\n\r\nExpression: cannot dereference string iterator because it is out of range (e.g. an end iterator)\r\n\r\nFor information on how your program can cause an assertion\r\nfailure, see the Visual C++ documentation on asserts.\r\n```\r\n\r\nThis function is extracted from  tensorflow\\core\\lib\\gtl\\stl_util.h\r\n\r\n### Source code / logs\r\n\r\n", "comments": ["We have the same issue in  tensorflow\\core\\lib\\strings\\strcat.cc:94", "AFAIK we're going to be migrating these GTL containers to [ABSL](https://github.com/abseil/abseil-cpp) at some point in the future (hopefully soon) and the two probably share overlapping code. Until then, we would welcome contributions helping to improve our MSVC story.", "@guschmue @mrry FYI", "Function string_as_array may be implemented as below:\r\n\r\ninline char* string_as_array(string* str)\r\n{\r\n\treturn (str->empty()) ? nullptr : &*str->begin();\r\n}\r\n\r\nor\r\n\r\ninline char* string_as_array(string* str)\r\n{\r\n\treturn (str->begin() == str->end()) ? nullptr : &*str->begin();\r\n}\r\n"]}, {"number": 15542, "title": "Cannot run run_cc_test_windows.bat: command line too long", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows Server 2012\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n1.4.1\r\n376e2cfdab31f4da251ea2e50992a9bf97fd171b\r\n- **Python version**: \r\n3.5.4\r\n- **Bazel version (if compiling from source)**:\r\n0.9.0, \r\n- **GCC/Compiler version (if compiling from source)**:\r\nVS 2015\r\n- **CUDA/cuDNN version**:\r\nNone\r\n- **GPU model and memory**:\r\nNone\r\n- **Exact command to reproduce**:\r\ntensorflow/tools/ci_build/windows/cpu/bazel/run_cc_test_windows.bat\r\n\r\n### Describe the problem\r\nIndeed, I have some local modification.  I'm trying to fix some failed cc_tests on Windows, and turn on them in bazel_test_lib.sh \r\n\r\nThe problem is: As the number of tests is growing,  we can't pass all their names through command line. We need a new way to filter the tests. \r\nWhat about: \r\nRemove the settings in tensorflow/tools/ci_build/windows/cpu/bazel/run_cc_test_windows.bat,  replace them with bazel tags like \"no_windows\", \"gpu_only\", ... ?\r\n\r\n### Source code / logs\r\n[build_log.txt](https://github.com/tensorflow/tensorflow/files/1577981/build_log.txt)\r\n", "comments": ["Perhaps related to bazelbuild/bazel#4335?", "Yes... It's that issue. \r\n\r\nInteresting. We encountered the same problem at the same time. ", "@meteorcloudy PTAL", "Yes, we had the same problem on our CI, it's a limitation of Windows command line length, which is only 8191 characters...", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@meteorcloudy do you thin we can use any workarounds?\r\nDo you have any recommendations?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @meteorcloudy, @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @meteorcloudy, @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @meteorcloudy, @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @meteorcloudy, @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @meteorcloudy, @gunan: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @meteorcloudy, @gunan: It has been 108 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I think we should mark all the failing cc tests as \"no_windows\", just like the python tests. Then we won't have the command line length limit.\r\n\r\nWe're not running cc tests on TF CI yet, so it's not my first priority currently, feel free to send a PR for this."]}, {"number": 15465, "title": "No gradient defined for operation 'MatrixExponential' (op type: MatrixExponential)", "body": "I want to optimize a function which contains tf.linalg.expm, however,\r\n\r\nNo gradient defined for operation 'MatrixExponential' (op type: MatrixExponential)\r\n\r\n", "comments": ["Yup, I believe that the gradient of `tf.linalg.expm` is not implemented. Perhaps we can refer to theano.tensor.slinalg.ExpmGrad:\r\n\r\nhttps://github.com/bishax/Theano/blob/a61962e10f3813ab68ba28d8253bb002144bd4b8/theano/tensor/slinalg.py#L387", "@LionSR, if your matrix is Hermitian or symmetric, you could create your own op with self_adjoint_eig, elementwise exponentiation, and matrix multiplication which would be differentiable.\r\nhttps://en.wikipedia.org/wiki/Matrix_exponential#Diagonalizable_case", "Thoughts on this gradient request @girving?", "TensorFlow matrix exponentiates via Eigen, which does repeated squaring + Pad\u00e9 approximation.  The Eigen code is https://bitbucket.org/eigen/eigen/src/034b6c3e101792a3cc3ccabd9bfaddcabe85bb58/unsupported/Eigen/src/MatrixFunctions/MatrixExponential.h?at=default&fileviewer=file-view-default.\r\n\r\nThe whole algorithm is differentiable, so one option is to differentiate through Eigen's implementation including all approximation errors.  If you're doing optimization based on matrix exponentiation, that would be optimal including treatment of errors.  It's \"technically straightforward\", but would require either mimicking the Eigen code or heavily modifying it.\r\n\r\n@rmlarsen probably has a better sense for the best practical solution.", "Does current version support GPU?", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "I think modifying Eigen to enable automatic differentiation is probably not feasible in this case, since the matrix exponentiation code is neither in Eigen core, nor in the tensor library originally co-developed with TensorFlow. I think a more viable approach would be to derive the gradients mathematically and implementing that in TensorFlow, possibly as a custom C++ gradient op (or preferably in Python using existing primitives/ops).", "I added the original MatrixExponential op. It should be straightforward to add a gradient op written in Python using the closed-form expression for the backward pass. In the meantime, you can compute the matrix exponential by computing the eigendecomposition, exponentiating the eigenvalues, and multiplying the matrices back together. It will be slower, but those ops all should have gradients implemented.", "@dpfau Thanks, David. Looking forward to reviewing your change :-) \r\n\r\nWe only have a symmetric/Hermitian eigensolver in TF, so your suggested workaround has more narrow scope. But thanks for the suggestion.", "If that's helpful, I needed the derivative of the matrix exponential some time ago for a non tensor flow-based project and tried a few algorithms (including the one in theano but also some more generic ones). The simple scipy-based test code is available here: https://gist.github.com/tvercaut/61832fd5af87d0278475d01d40860d82\r\n\r\nI found the approach of Najfeld to work well for my small scale problem:\r\n> Najfeld and T. F. Havel, \u201cDerivatives of the matrix exponential and their computation,\u201d Advances in Applied Mathematics, vol. 16, no. 3, 1995. [Online]. Available http: //dx.doi.org/10.1006/aama.1995.1017\r\n\r\nAs per the discussion in https://github.com/Theano/Theano/pull/3998, a Taylor expansion is needed in some cases to avoid a division by zero.", "This applies to the matrix logarithm (tf.linalg.logm) as well. @dpfau's suggestion is a fine workaround but it'd be nice if the built-in matrix log had its gradient implemented.", "I'm also interested in the gradient implementation of the matrix exponential op. Has someone implemented it yet? Are there works in progress for future releases?", "There is a branch that implements the Pade approximation to the matrix\nexponential in pure tensorflow, but it is untested.\n\nOn Thu, Jul 12, 2018 at 4:22 AM, Ceveloper <notifications@github.com> wrote:\n\n> I'm also interested in the gradient implementation of the matrix\n> exponential op. Has someone implemented it yet? Are there works in progress\n> for future releases?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15465#issuecomment-404376817>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAIwCPnhG_62Gb2L_iETcbCpU5M85lf1ks5uFsEOgaJpZM4RGWDH>\n> .\n>\n", "In case this is helpful for others, I used this as an excuse to learn about custom ops in tensorflow. It looks like getting a pure python version is only a couple of lines:\r\n```python\r\n@ops.RegisterGradient(\"MatrixExponential\")\r\ndef _expm_grad(op, grad):\r\n    # If I got it right we would need a left multiplication with the\r\n    # output gradient but since the expm_frechet function assumes\r\n    # column major vectorisation while tensorflow assumes row major vectorisation\r\n    # we can just do a right multiplication...\r\n    out_expm, new_grad = tf.py_func(scipy.linalg.expm_frechet, [op.inputs[0], grad], tf.float64)\r\n    return [tf.transpose(new_grad)] # List of one Tensor, since we have one input\r\n```\r\n\r\nI am not 100% confident about the row major complication so additional eyeballs with better understanding of the internals of TF would be great. A short runnable example using this code can be found here:\r\nhttps://gist.github.com/tvercaut/bd9fe8c5d12ab529babd9bf5434d7cda", "Implementing the gradient in python could work for me! But let's say I implement a graph in python with this gradient op and they I write the graph to a protobuf file. If I load the graph with the C++ APi, is the optimization going to work? Or there are some disadvantages because of the custom gradient? I hope this is not off-topic.", "Is this the correct backwards pass? I thought the frechet derivative was\nthe same as forward-mode AD, not backward-mode. Though I could be wrong.\n\nOn Fri, Jul 13, 2018 at 1:56 AM, Ceveloper <notifications@github.com> wrote:\n\n> Implementing the gradient in python could work for me! But let's say I\n> implement a graph in python with this gradient op and they I write the\n> graph to a protobuf file. If I load the graph with the C++ APi, is the\n> optimization going to work? Or there are some disadvantages because of the\n> custom gradient? I hope this is not off-topic.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15465#issuecomment-404693528>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAIwCFiPeyBAUNECeZM1oajvr773oj8Xks5uF_BUgaJpZM4RGWDH>\n> .\n>\n", "My understanding is that the Fr\u00e9chet derivative indeed corresponds to forward-mode AD but because we have `expm(A^T)=expm(A)^T` I believe the Fr\u00e9chet derivative function can still be used for backward-mode AD provided the right transpositions / index ordering is used. This is where a more careful look would be helpful especially since the default vectorisation in the math formulas is based on column major assumptions.", "I did some further experimentation and while I am still not 100% confident I get all the indexing right, this seems more consistent:\r\n```python\r\n@ops.RegisterGradient(\"MatrixExponential\")\r\ndef _expm_grad(op, grad):\r\n    # We want the backward-mode gradient (left multiplication).\r\n    # Let X be the NxN input matrix.\r\n    # Let J(X) be the the N^2xN^2 complete Jacobian matrix of expm at X.\r\n    # Let Y be the NxN previous gradient in the backward AD (left multiplication)\r\n    # We have\r\n    # unvec( ( vec(Y)^T . J(X) )^T )\r\n    #   = unvec( J(X)^T . vec(Y) )\r\n    #   = unvec( J(X^T) . vec(Y) )\r\n    # where the last part (if I am not mistaken) holds in the case of the\r\n    # exponential and other matrix power series.\r\n    # It can be seen that this is now the forward-mode derivative\r\n    # (right multiplication) applied to the Jacobian of the transpose.\r\n    grad_func = lambda x, y: scipy.linalg.expm_frechet(x, y, compute_expm=False)\r\n    return tf.py_func(grad_func, [tf.transpose(op.inputs[0]), grad], tf.float64) # List of one Tensor, since we have one input\r\n```\r\n\r\nThe updated \"tests\" are in the same gist: https://gist.github.com/tvercaut/bd9fe8c5d12ab529babd9bf5434d7cda\r\n\r\nIt'd be great to have some proper unit tests at some point.", "@tvercaut I haven't thought through your math/indexing but I've empirically tested your gradient against @dpfau's suggestion, which uses existing differentiable tf ops but only works for symmetric matrices (so we can test your method for symmetric matrices).\r\n\r\nThe gradients seem to agree with your gist tests (modulo the note below). See my gist, based on yours: \r\nhttps://gist.github.com/kmcnaught/e499cf89b76fa05b9bfb75f5b419404b\r\n\r\nNote that `self_adjoint_eig` uses only the lower triangular part of the input matrix, so the resulting gradients are either double or zero on the off-diagonals, compared to your version which distributes them on both triangles. The two are equivalent in the case we have where the input is constrained to be symmetric. \r\n\r\n", "We have implemented the Pad\u00e9 approximant for the matrix exponential entirely with TensorFlow ops and deprecated the op based on the Eigen C++ implementation. This allows for gradients and ops on GPU automatically. Commit [here](https://github.com/tensorflow/tensorflow/commit/a4f755678102a4fb8861fec3238ddc78bb9b2a6e).", "Thanks! Did you benchmark the accuracy and efficiency of the resulting autodiff gradient with respect to a version based on \r\n> Awad H. Al-Mohy and Nicholas J. Higham (2009) Computing the Frechet Derivative of the Matrix Exponential, with an application to Condition Number Estimation. SIAM Journal On Matrix Analysis and Applications., 30 (4). pp. 1639-1657. ISSN 1095-7162\r\n\r\nsuch as the one implemented in `scipy.linalg.expm_frechet`:\r\nhttp://scipy.github.io/devdocs/generated/scipy.linalg.expm_frechet.html#scipy.linalg.expm_frechet\r\nhttps://github.com/scipy/scipy/blob/v1.1.0/scipy/linalg/_expm_frechet.py#L225-L278\r\n\r\nThe algorithm is also based on Pad\u00e9 and scaling and squaring. As mentioned above it would need to be applied to the transpose of the input matrix to get the backward-mode gradient.", "No I did not. Want to try?\n\nOn Thu, Aug 2, 2018 at 12:24 PM, Tom Vercauteren <notifications@github.com>\nwrote:\n\n> Thanks! Did you benchmark the accuracy and efficiency of the resulting\n> autodiff gradient with respect to a version based on\n>\n> Awad H. Al-Mohy and Nicholas J. Higham (2009) Computing the Frechet\n> Derivative of the Matrix Exponential, with an application to Condition\n> Number Estimation. SIAM Journal On Matrix Analysis and Applications., 30\n> (4). pp. 1639-1657. ISSN 1095-7162\n>\n> such as the one implemented in scipy.linalg.expm_frechet:\n> http://scipy.github.io/devdocs/generated/scipy.linalg.expm_frechet.html#\n> scipy.linalg.expm_frechet\n> https://github.com/scipy/scipy/blob/v1.1.0/scipy/\n> linalg/_expm_frechet.py#L225-L278\n>\n> The algorithm is also based on Pad\u00e9 and scaling and squaring. As mentioned\n> above it would need to be applied to the transpose of the input matrix to\n> get the backward-mode gradient.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15465#issuecomment-409893393>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAIwCNuXwoL9r0rsFL61vW77c1s0DFW-ks5uMuGLgaJpZM4RGWDH>\n> .\n>\n", "> We have implemented the Pad\u00e9 approximant for the matrix exponential entirely with TensorFlow ops and deprecated the op based on the Eigen C++ implementation. This allows for gradients and ops on GPU automatically. Commit [here](https://github.com/tensorflow/tensorflow/commit/a4f755678102a4fb8861fec3238ddc78bb9b2a6e).\r\n\r\nHi dpfau, \r\n\r\ncould you show an example (say starting with@ops.RegisterGradient(\"MatrixExponential\") ) that allows for gradient computation? the pade approximant only allows for computing matrix exponential? ", "Because the Pade approximant is implemented with differentiable tensorflow\nops, the op itself is differentiable. Gradients should work normally as\nwith any other op.\n\nOn Thu, Mar 14, 2019 at 12:07 AM Shaowu Pan <notifications@github.com>\nwrote:\n\n> We have implemented the Pad\u00e9 approximant for the matrix exponential\n> entirely with TensorFlow ops and deprecated the op based on the Eigen C++\n> implementation. This allows for gradients and ops on GPU automatically.\n> Commit here\n> <https://github.com/tensorflow/tensorflow/commit/a4f755678102a4fb8861fec3238ddc78bb9b2a6e>\n> .\n>\n> Hi dpfau,\n>\n> could you show an example (say starting with@ops.RegisterGradient(\"MatrixExponential\")\n> ) that allows for gradient computation? the pade approximant only allows\n> for computing matrix exponential?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15465#issuecomment-472653202>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAIwCGJOP0-tQ4WBoj9yg347RzfaDpVxks5vWZLBgaJpZM4RGWDH>\n> .\n>\n", "> Because the Pade approximant is implemented with differentiable tensorflow ops, the op itself is differentiable. Gradients should work normally as with any other op.\r\n> [\u2026](#)\r\n> On Thu, Mar 14, 2019 at 12:07 AM Shaowu Pan ***@***.***> wrote: We have implemented the Pad\u00e9 approximant for the matrix exponential entirely with TensorFlow ops and deprecated the op based on the Eigen C++ implementation. This allows for gradients and ops on GPU automatically. Commit here <[a4f7556](https://github.com/tensorflow/tensorflow/commit/a4f755678102a4fb8861fec3238ddc78bb9b2a6e)> . Hi dpfau, could you show an example (say starting ***@***.***RegisterGradient(\"MatrixExponential\") ) that allows for gradient computation? the pade approximant only allows for computing matrix exponential? \u2014 You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#15465 (comment)](https://github.com/tensorflow/tensorflow/issues/15465#issuecomment-472653202)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AAIwCGJOP0-tQ4WBoj9yg347RzfaDpVxks5vWZLBgaJpZM4RGWDH> .\r\n\r\nHi dpfau,\r\n\r\nThank you for your reply. However, I was trying to directly modify the v1.8.0 version of tensorflow in order to have this op working. I do this because I have another package that only compatible to 1.8.0 of tensorflow which makes the life pain. \r\n\r\nI tried directly copying and paste the *commit* you submitted, however, it still says \" no gradient found for MatrixExponential type\". Could you tell me where it could be wrong? Am I missing something? "]}, {"number": 15447, "title": "[cmake] CPU only build error in tf_stream_executor.cmake", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master\r\n- **Python version**: N/A, building with cmake\r\n- **Bazel version (if compiling from source)**: N/A, building with cmake\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\ncd tensorflow/contrib/cmake\r\nmkdir build && cd build\r\ncmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=\"../bin\" ..\r\n\r\n### Describe the problem\r\ncmake build fails when \r\noption(tensorflow_ENABLE_GPU \"Enable GPU support\" OFF)\r\nwith following error:\r\n.../tensorflow-master/tensorflow/stream_executor/dso_loader.h:32:30: fatal error: cuda/cuda_config.h: No such file or directory\r\ncompilation terminated.\r\n\r\n### Source code / logs\r\nPretty sure that the issue lies in this commit:\r\nhttps://github.com/tensorflow/tensorflow/commit/f1582cf82f06810900ee99870f5d5d3a7478d044#diff-1d799fa350437420218e5e5aa680c481\r\n\r\nin CMakeLists.txt the line\r\n\"  include_directories(${tensorflow_source_dir}/third_party/gpus)\"\r\nis still under tensorflow_ENABLE_GPU\r\n\r\nWhich is why dso_loader cannot find cuda_config.h\r\n\r\nOn the other hand, I suppose it should not use this include at all in the CPU mode.", "comments": ["@mrry Problems are being reported with some cmake cleanup that was done two weeks ago in https://github.com/tensorflow/tensorflow/pull/15099", "Since we only continuously test the CMake build for Windows, it looks like #15099 has broken the Linux CPU build.\r\n\r\nWe'd accept a contribution to revert #15099 or fix it properly.\r\n\r\n/cc @Androbin "]}, {"number": 15396, "title": "Bug: reshape shape inference for parital defined shape", "body": "-----------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0\r\n- **Python version**: Python 3.6.3 :: Anaconda custom (64-bit)\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: see example\r\n\r\n### Describe the problem\r\nWhen the input shape for 'tf.reshape' is partial defined and the new shape contains a `-1` for known dimensions, 'tf.reshape' does not predict the shape. See the example code.\r\n\r\n### Source code / logs\r\n\r\nSecond and third example have working shape inference:\r\n```python\r\ndef foo(*shape):\r\n    x = tf.placeholder(tf.float32, shape)\r\n    return tf.reshape(x, tf.concat([tf.shape(x)[:-2], [-1]], 0))\r\n\r\nprint(foo(2, 3, 4, 5))  # Tensor(\"Reshape_8:0\", shape=(2, 3, 20), dtype=float32)  # correct\r\nprint(foo(None, 3, 4, 5))  # Tensor(\"Reshape_9:0\", shape=(?, 3, ?), dtype=float32)  # shape inference possible\r\nprint(foo(None, None, 4, 5))  # Tensor(\"Reshape_10:0\", shape=(?, ?, ?), dtype=float32)  # shape inference possible\r\nprint(foo(2, 3, 4, None))  # Tensor(\"Reshape_11:0\", shape=(2, 3, ?), dtype=float32)  # correct\r\n```\r\n\r\n#### Proof that shape inference is possible:\r\n```python\r\nimport functools, operator \r\ndef bar(*shape):\r\n    x = tf.placeholder(tf.float32, shape)\r\n    \r\n    tmp = x.shape[-2:]\r\n    if not tmp == tf.TensorShape(None):\r\n        tmp = functools.reduce(operator.mul, tmp, tf.Dimension(1))\r\n    if str(tmp) == '?' or tmp == tf.TensorShape(None):\r\n        shape = [-1]\r\n    else:\r\n        shape = [tmp]\r\n    \r\n    return tf.reshape(x, tf.concat([tf.shape(x)[:-2], shape], 0))\r\n\r\n\r\nprint(bar(2, 3, 4, 5))  # Tensor(\"Reshape_21:0\", shape=(2, 3, 20), dtype=float32)\r\nprint(bar(None, 3, 4, 5))  # Tensor(\"Reshape_22:0\", shape=(?, 3, 20), dtype=float32)\r\nprint(bar(None, None, 4, 5))  # Tensor(\"Reshape_23:0\", shape=(?, ?, 20), dtype=float32)\r\nprint(bar(2, 3, 4, None))  # Tensor(\"Reshape_24:0\", shape=(2, 3, ?), dtype=float32)\r\n```", "comments": ["seems related to #14998 ?", "Indirect yes. I used the code from #14998 where I hit this bug, but the bug is independent.\r\nHere is the problem (reduced to the minimum) that I need to solve with reshape:\r\n```python\r\nx = tf.placeholder(tf.float32, [None, None, 32, 32])\r\ny = tf.reshape(x, ...)\r\ny.shape == [None, None, 1024]\r\n```\r\nIt would be nice to do the reshape without predicting the shape and/or use `set_shape`", "@martinwicke @aselle Can either of you comment on this one?", "Shape inference cannot use the values inside a Tensor. In full generality, that would require evaluating that tensor. For some tensors, that is possible (i.e., constant folding), and in this case, arguably all involved tensors are constants, and all operations are simple enough that constants could still be constants after. So this is possible in theory, but I wouldn't want to predict how complicated it would be to implement. \r\n\r\nconcat and stack/unstack on shapes is probably very common, so it may be worth thinking about this special case in particular.\r\n\r\n@skyewm do you have an educated guess how hard it would be to do?", "Do you mean @skye with @skyewm?\r\n", "Yes. Too easy to remember. Sorry!", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "ping @skye, do you have a notion of how hard this would be?", "Hey, sorry for not looking at this sooner, too many issues!\r\n\r\nOne note: I'm working on switching the Python API to call the C API, which means it'll switch to using the C++ shape inference implementation. I tried running the example with the C API enabled (by setting the env variable TF_C_API_GRAPH_CONSTRUCTION=1), and unfortunately it does even worse:\r\n```\r\nTensor(\"Reshape:0\", shape=(2, 3, 20), dtype=float32)\r\nTensor(\"Reshape_1:0\", shape=(?, ?, ?), dtype=float32)\r\nTensor(\"Reshape_2:0\", shape=(?, ?, ?), dtype=float32)\r\nTensor(\"Reshape_3:0\", shape=(?, ?, ?), dtype=float32)\r\n```\r\n\r\nHowever, whoever takes this on should still fix the C++ shape inference instead of the Python version, since the Python version will go away soon.\r\n\r\nI'm not super familiar with shape inference, but I would guess this is doable if someone is willing to take the time to debug the example and get familiar with the shape inference code. It's pretty tricky, but also somewhat self-contained and easy to read, so someone familiar with C++ and debugging should be able to do it given some time, even if they're not super familiar with TF (I can offer pointers for how to get started). I'll probably eventually do this myself if no one else does it, but it'll take a while for me to get to it.", "@skye and @martinwicke If nobody else is working on this issue, I would like to work on it.", "Go for it!"]}, {"number": 15369, "title": "CMake OBJECT library Xcode problems", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OS X 10.12.2 (Any)\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: Master @HEAD\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: N/A (CMake)\r\n- **GCC/Compiler version (if compiling from source)**: Xcode 8.3.3 (Any)\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: `cmake -GXcode ...`\r\n\r\n### Describe the problem\r\n\r\nCMake's OBJECT libraries don't play well with Xcode generators.  In particular, there is an incompatibility (effectively a bug) that prevents an object library from containing multiple files with the same base filename (\"stem\"), i.e.\r\n\r\n* tensorflow//core/platform/env_time.cc\r\n* tensorflow//core/platform/posix/env_time.cc\r\n\r\nThis pattern occurs in quite a few places in the tensorflow source code (and is otherwise perfectly reasonable).  For reference, here is a minimal sample project that directly reproduces a test case originally shared in a [post by Matthew Wheeler](http://cmake.3232098.n2.nabble.com/OBJECT-Libraries-with-Xcode-Generator-td7593197.html) on the CMake mailing list:\r\n\r\nhttps://github.com/headupinclouds/cmake_xcode_object_lib\r\n\r\nI'd like to help provide a fix for this, but would like some input on the preferred approach prior to implementing anything.  I see a few options:\r\n\r\n1. identify duplicates manually and add an alias for the offending files in the repository: `for i in ${FAILURES}; do echo -e \"#include \\\"$i\\\"\" > ${i%.cc}_fix.cc; done` and then update CMake to include those files.  Maybe `_fix` could be replaced with a more unique directory name.  PRO: Reasonably easy; CON: The problem will likely occur again, `#include \"source.cc\"` violates some style guides\r\n2. iterate through each list of object files in CMake at generate time and identify duplicates automatically, then map each of these files to an alias for the build using something like `configure_file(${duplicate_file} ${CMAKE_CURRENT_BINARY_DIR}/${duplicat_file_w_suffix} COPYONLY)` (for Xcode only).  PRO: Automatic (future proof); CON: More complicated and users can't apply changes directly in their IDE \r\n3. (long term) replace OBJECT libraries with standard libraries (static or shared based on `CMAKE_BUILD_SHARED`): In addition to the Xcode related bug above, OBJECT libraries have a number of other limitations which make the CMake code more complicated, or rather, standard libraries have a number of benefits that could make the CMake code cleaner.  Perhaps the most significant drawback is that OBJECT libraries can't be used with `target_link_libraries()`, so we lose the ability to pass along transitive dependency chains and scoped [usage requirements](https://cgold.readthedocs.io/en/latest/rejected/object-libraries.html#usage-requirements) from  `find_package()` (future) system dependencies using `target_link_libraries()`.  This relates to [Proposal: Making the cmake build distribution friendly](https://github.com/tensorflow/tensorflow/issues/13061), where common system dependencies would be included using `find_package()` calls and linked directly to the tensorflow submodules: `target_link_libraries(tf_core_lib PRIVATE ${tensorflow_EXTERNAL_PACKAGES}) # zlib, etc`.  This would also allow most of the manual `add_dependencies()` calls to be removed.  (Note: I've already added CMake package config installation steps to most of the google repository dependencies in forks, and will try to get this stuff accepted upstream.)\r\n\r\nThe last one is broader in scope, so I'm hoping there is an initial workaround based on some variation of (1) or (2) that would be accepted upstream in the near future for CMake + Xcode.  If there is interest in using standard libraries (3), I can help work on putting an initial solution together in a branch for evaluation as a follow up effort.\r\n\r\nI understand CMake status is [currently under discussion](https://github.com/tensorflow/tensorflow/issues/14014#issuecomment-340344678).  In any event, I'd like to help get tensorflow building through CMake for easy integration with other CMake based projects, including iOS builds, where Xcode is required.  I also appreciate tensorflow is an incredibly complicated piece of SW, and I appreciate the work that has gone in to supporting CMake builds to date.  Thanks!\r\n\r\n### Source code / logs\r\n\r\nNumerous \"no such file or directory\" errors such as these:\r\n```\r\nclang: error: no such file or directory: '/Users/developer/tensorflow/tensorflow/contrib/cmake/_builds/xcode-hid-sections/tensorflow.build/Release/tf_core_lib.build/Objects-normal/x86_64/env.o'\r\nclang: error: no such file or directory: '/Users/developer/tensorflow/tensorflow/contrib/cmake/_builds/xcode-hid-sections/tensorflow.build/Release/tf_core_lib.build/Objects-normal/x86_64/env_time.o'\r\nclang: error: no such file or directory: '/Users/developer/tensorflow/tensorflow/contrib/cmake/_builds/xcode-hid-sections/tensorflow.build/Release/tf_core_lib.build/Objects-normal/x86_64/tracing.o\r\n```", "comments": ["@jart Can you comment on this one?", "@mrry has more state on CMake build and might be able to help.", "I don't have a strong opinion, but I think we'd prefer options like (3) that improve the overall maintainability of the CMake build scripts and don't impose additional constraints on the non-CMake build. We'd appreciate contributions on this \r\n\r\n/cc @gunan @ewilderj for tf-distribute SIG relevance.", "Yes, @headupinclouds please join [tf-distribute](https://groups.google.com/a/tensorflow.org/forum/#!forum/tf-distribute) if you are able, and talk about this there. We are looking for people who are interested in helping with the build, and it'd be great if you'd introduce yourself and reference this issue."]}, {"number": 15293, "title": "Encrypted Data", "body": "It is not easy/possible to use TensorFlow with encrypted data, such as via Homomorphic Encryption or Multi-Party Computation.  It would be useful if future versions of TensorFlow could support HE/MPC or provide the ability for third-party HE/MPC support.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A\r\n- **TensorFlow installed from (source or binary)**: N/A\r\n- **TensorFlow version (use command below)**: N/A\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "This is a type:feature but I am not a member so I cannot add the label.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Sorry, at the moment I am traveling in places (mostly) without the Internet.  It is still an issue.  The label is type:feature.", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "@martinwicke what do you think of this proposal?", "Currently, the constants for HE are prohibitive. I don't see this changing. I know less about MPC. \r\n\r\nI'm not sure this FR is useful, since it is so broad. I will close it for that reason. I think if someone wanted to make a specific proposal of how to support either in TF, that would be quite welcome.", "@martinwicke What do you think about \"low latency\"\r\napproaches like [Gazelle](https://arxiv.org/abs/1801.05507)?\r\nI think that we can leave an issue open to track privacy preserving efforts. Probably we could reprhase this orginal issue.", "If I read those numbers correctly, that's still roughly a 1000x slowdown over plain computation, not considering accelerators. It's hard to tell because the paper does not compare to non-HE runtimes.\r\n\r\nI'm sure this will get better. If someone were to propose a set of ops that are useful in the context of  HE, I would want those, but in a separate repo (I want everything in a separate repo for maintainability, see lattice, agents, ...)\r\n\r\nI'm ok with keeping this open in case people want to organize such an effort.\r\n\r\n@ewilderj FYI.", "Just to reference some interesting effort: https://github.com/OpenMined/PySyft", "For who is interested in this topic related to TF check this new post https://mortendahl.github.io/2018/03/01/secure-computation-as-dataflow-programs/", "/cc @mortendahl", "just an update to anyone interested: work on bringing secure computation to tensorflow has been progressing nicely recently as [`tf-encrypted`](https://github.com/mortendahl/tf-encrypted)", "https://ai.intel.com/he-transformer-for-ngraph-enabling-deep-learning-on-encrypted-data/"]}, {"number": 15290, "title": "Feature request: provide a means to configure, build, and install that includes cc", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Both Mac and Linux\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 3.6.3, but not relevant \r\n- **Bazel version (if compiling from source)**: 0.8.1, but not relevant\r\n- **GCC/Compiler version (if compiling from source)**: Both GCC and clang, but not relevant\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nTensorflow doesn't seem to have a clean way to install from sources to support both python and C++ development. When we install from sources, only the core tensorflow framework is installed in the python site-packages directory. The `cc` headers (and maybe others) are not included. Likewise the `libtensorflow_cc.so` is not built. It's surprising that there is so little documentation for how C++ developers are expected to develop tensorflow applications. My use case is probably common: I want to train and test my model using python, but I want to deploy an application that does prediction/inference with the app written in C++. \r\n\r\nI have managed once to successfully install the headers and libraries I need into `/usr/local/...` on a Mac, but in doing so I lost some of the CPU optimizations that I had specified when doing the standard build from sources. Now I need to repeat this process on Linux, where I need GPU support, and want to make sure I get it right.\r\n\r\nIt would be so nice if there was something close to the standard `./configure; make; make install` that could install headers and libraries into a chosen directory.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Have you followed this page:\r\nhttps://www.tensorflow.org/install/install_sources\r\n\r\nI agree that ultimate installation instructions are for python pip packages. That may be confusing. But once you have the source directory, you should be able to build anything.\r\n\r\n@asimshankar do we have a document to describe `libtensorflow_cc.so` build and installation?", "Yes, I have gone through all of the steps to build and install the python components into `site-packages` from source. I've also used `bazel` to build `libtensorflow_cc.so`. But there are still several other manual steps required to arrange for all of the necessary C++ headers to be installed in such a way that C++ applications can compile independently of the tensorflow repository.\r\n\r\nIn my search for the best way to do this, the best resource I have found is on stackoverflow: \r\nhttps://stackoverflow.com/questions/33620794/how-to-build-and-use-google-tensorflow-c-api\r\n\r\nThat question is two years old(!!) and none of the answers are (IMHO) satisfactory. The answer that comes closest is [this one](https://stackoverflow.com/a/43920376/376518). But the steps as shown there did not work for me, possibly due to changes to tensorflow between the time the answer was written and the current r1.4 release. I have been able modify the steps to make something that does work, but it is really unacceptable for the Tensorflow team to expect every developer to discover on their own how to do this. There really should be something like the standard unix recipe: `.configure; make cc-installer; sudo install-cc.sh;`. With `bazel`, the steps might look like this:\r\n\r\n1. `./configure`      # a new question is added: \"Do you want to build for C++ development?\"\r\n2. `bazel build --config=opt --config=cuda //tensorflow/tools/cc_package:build_cc_package`\r\n3. `sudo install-cc-package.sh`\r\n\r\nThe result from the above steps would be the installation of a directory at the default path `/usr/local/include/tf`, but the actual path should be an option provided during the `./configure` step. The `tf` directory should contain all of the headers necessary for C++ development. It would a superset of all of the headers currently installed into `../site-packages/tensforflow/include`, but following the same directory organization. The installer script would also install `libtensorflow_cc.so` and `libtensorflow_framework.so` into `/usr/local/lib` or some other location chosen during the `./configure` step.\r\n\r\nLet me be clear: I personally am not blocked, as I now have a working configuration on both my Mac and Linux development machines. But other developers (and me sometime in the future) will need to do this, and it seems bizarre that the Tensorflow team does not have a documented and easy to follow process that all developers who need C++ development can follow, and will continue to work with each new tensorflow release. I could document what I do, possibly on the above stackoverflow thread, but the Tensorflow community is not served well by that hack. ", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'm curious if there has been any progress on this issue. I'm currently working on making a Docker image to deploy to Google Cloud and I've discovered that I am once again blocked by this issue.\r\n\r\nIf I pull one of the docker images, I find that libtensorflow_cc.so is not included anywhere in the image. What is worse, when I try to build it (with gpu support) I get errors, similar to this issue: https://github.com/tensorflow/tensorflow/issues/2143\r\n\r\nI tried both `nightly-devel-gpu-py3` and `1.5.0-devel-gpu-py3` and obtained the same error.\r\n\r\nThis error seems to indicate that `libtensorflow_cc.so` is not even built in your continuous integration. Is that correct?", "@asimshankar @allenl to comment.\r\nI think many symbols exporrted by libtensorflow_cc.so are also exported by the .so files distributed by our pip packages. We should wait for confirmation though.\r\nYou are correct that in our dockerfiles we do not have anything other than the py libraries. Maybe we should also make C and Java libraries available in these.", "The pip package only includes `libtensorflow_framework.so`, which is not sufficient for C++ development.", "`pywrap_tensorflow.so` is also in the pip package, it may be in a harder to locate place, and may need to be moved around in our pip package. And as far as I know, that `.so` file should be exporting every single C++ object in TF (which is something we are working to fix).", "Nagging Assignee @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "\"you should be able to build anything\"\r\n\r\nI agree, I _should_ be.\r\nBut how do I do it?\r\nHow do I even know what it is I want to build?\r\nHow do I know that libtensorflow_cc.so is a target I should ask Bazel to build?\r\nWhat other targets should I build?\r\nWhich headers should go into a public include directory, as opposed to are for internal use only?\r\n\r\nI can't find any documentation at all on how to build and user TensorFlow as a C and C++ library, or much about what all can be built with the Bazel builder at all. I'm sure some of that is me not being good with Bazel, but that can't be the only thing getting in the way here.\r\n\r\nNote that https://www.tensorflow.org/install/install_sources explicitly talks about how to build the pip packages, not how to build for C++.", "@asimshankar Do we have documentation for libtensorflow?", "Nagging Assignee @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Can @asimshankar please reply to this thread?", "The C library package (consisting essentially of a single header file and a few shared libraries) can be built using the process we use for building release binaries. (See [README](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/lib_package/README.md) and [StackOverflow answer](https://stackoverflow.com/questions/49788062/how-to-build-the-c-library-for-tensorflow/49789918#49789918)). \r\n\r\nFor the C++ library, we haven't had the bandwidth to support that ourselves yet and have relied on community support for that (see https://github.com/tensorflow/tensorflow/issues/2412#issuecomment-299572173 and https://github.com/tensorflow/tensorflow/issues/2412#issuecomment-304433971 and https://github.com/tensorflow/tensorflow/issues/2412#issuecomment-381959842 - credit to @FloopCZ and others). \r\n\r\nIf someone would like to contribute a packaging rule for the C++ library by adding a BUILD target similar to `libtensorflow` in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/lib_package/BUILD, that would be awesome.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Propose that this project should be part of the official tensorflow docker images: https://github.com/FloopCZ/tensorflow_cc\r\n\r\nBuilding and using libtensorflow_cc.so from source is non-trivial and burns dev time. Would be good for community if the official images support easy c++ dev and integration. Thanks."]}, {"number": 15214, "title": "tf.profiler overrides shape_invariants in while_loop", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.5.0-dev20171120\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n``` python\r\nimport tensorflow as tf\r\n\r\na = tf.zeros((1,))\r\nn = tf.constant(10.)\r\ndo_profile = True\r\n\r\n_, b = tf.while_loop(\r\n    lambda x, y: x[0] < n,\r\n    lambda x, y: (x + 1, tf.concat((y, x), 0)),\r\n    (a, tf.zeros((0,))),\r\n    shape_invariants=(tf.TensorShape((1,)), tf.TensorShape((None,))))\r\n\r\nwith tf.Session() as sess:\r\n    for _ in range(2):\r\n        grads = tf.gradients(b, a)\r\n\r\n        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n        run_metadata = tf.RunMetadata()\r\n        print(sess.run((b, grads), options=run_options,\r\n                       run_metadata=run_metadata))\r\n\r\n        if do_profile:\r\n            tf.profiler.profile(tf.get_default_graph(), run_meta=run_metadata)\r\n```\r\n\r\nIf `do_profile=True`, this will give an error on the second pass through the for loop:\r\n```\r\nTraceback (most recent call last):\r\n  File \".../tmp3.py\", line 15, in <module>\r\n    grads = tf.gradients(b, a)\r\n  File \"...\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 638, in gradients\r\n    % (op.name, i, t_in.shape, in_grad.shape))\r\nValueError: Incompatible shapes between op input and calculated input gradient.  Forward operation: while/Switch_1.  Input index: 0. Original input shape: (0,).  Calculated input gradient shape: (10,)\r\n```\r\n\r\n### Describe the problem\r\n\r\nI believe this is caused by [this function](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/profiler/tfprof_logger.py#L37), which goes through the graph and fills in missing shapes from the `run_metadata`.  This modifies the graph in-place, so on the second pass through the for loop the loop variable that was intentionally defined with an unknown shape has been overwritten with the fixed shape from the last run.  This causes the shape mismatch error.\r\n  ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Yes this is still an issue on 1.5.0-dev20180103", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Yes this is still an issue on 1.6.0-dev20180118", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "Ugh, thanks for debugging. Are you in a position to send a PR?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Sorry, don't have time to work on a fix at the moment.", "Nagging Assignee @skye: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @skye: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @skye: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It looks like this error has been reproduced at https://github.com/melodyguan/enas/issues/4 with tf 1.7.", "Unfortunately I'm not sure anyone maintains the profiler anymore :\\ Marking community support.", "@skye I can reproduce this regression without using the profiler. Perhaps I should create a separate issue?"]}, {"number": 15213, "title": "XLA/AOT Windows support", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 x64\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.8.0\r\n- **GCC/Compiler version (if compiling from source)**: VS 2017 15.5\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\nSimilar to #8310, but specifically about running `tfCompile` on Windows rather than Linux to produce `x86_64-windows-msvc` binaries.\r\n\r\nXLA/AOT depends on LLVM which has excellent Windows support via CMake, but Bazel cannot interop with CMake. [llvm.BUILD](https://github.com/tensorflow/tensorflow/blob/master/third_party/llvm/llvm.BUILD) is auto-generated and the script to generate it is not open-sourced, this make it difficult for external contributor to make improvement. [tensorflow/compiler](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler) might not need too much changes as #9908 already addressed some of them.\r\n\r\nOne possible path is to let user to run CMake in host machine when invoking `configure.py`, then feed CMake generated files into custom script to generate `LLVM.BUILD`.\r\n\r\nNote:\r\n\r\n[Rumour has it](https://chromium-review.googlesource.com/c/chromium/src/+/753588) that there is a Google-internal tool called `tfNative` to generate `.h/.cpp` files instead of `.lib` binaries, though I suspect that even if the tool is open-sourced, it might not be immediately available for Windows developers.", "comments": ["/CC @tatatodd", "@rongjiecomputer seems you can compile tensorflow with bazel on windows, can you share how you get that?", "@argman Building Tensorflow with Bazel on Windows should work out of the box now.\r\n\r\n1. Download `bazel-0.8.1-windows-x86_64.exe` from https://github.com/bazelbuild/bazel/releases, rename it to `bazel.exe` and put it in `PATH`.\r\n1. set `BAZEL_VS` or `BAZEL_VC` to point to your VS 2017 installation directory (see https://docs.bazel.build/versions/master/windows.html)\r\n1. Clone Tensorflow.\r\n1. Run `configure.py` (see https://www.tensorflow.org/install/install_sources).\r\n1. Open `.tf_configure.bazelrc` and remove `-march=native` (otherwise the console output will be very noisy).\r\n1. Set `TMP`, `TEMP` and `TMPDIR` to shorter name like `C:\\tmp` due to bazelbuild/bazel#4149.\r\n1. Run `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`.\r\n\r\nIf the build _seems_ to stuck at compiling `conv_grad_ops_3d.cc` and `conv_ops_3d.cc`, don't worry about it. These two files are known to take a very long time to compile in MSVC due to `__force_inline`. The compilation will eventually finish. See #10521.", "@tatatodd I managed to build `tfcompile` with Bazel in Windows and build simple XLA/AOT examples.\r\n\r\nFirst two PRs are at #15310 and #15335, but no reviews received. Can you help me to assign them to suitable reviewers?\r\n\r\nI can only send the third PR when #15229, #14893 and #14531 get merged (not by me).\r\n\r\nOnce these PRs are merged, I will post a guide on how to write new `BUILD` file to import pre-built LLVM binaries built with CMake into Bazel to build `tfcompile` with Bazel. I don't intend to touch [llvm.BUILD](https://github.com/tensorflow/tensorflow/blob/master/third_party/llvm/llvm.BUILD) since it is marked as \"Do not edit\".\r\n\r\nCaveat:\r\n\r\n`tf_library` macro from [tfcompile.bz](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/aot/tfcompile.bzl#L21) can't be used as labels with prefix of `@org_tensorflow` will cause Bazel to generate path name that exceeds Windows path length limit (See bazelbuild/bazel#4149). Workaround is to write another macro without `@org_tensorflow` prefix and build targets in-tree.", "@rongjiecomputer Thanks very much for all the work on Windows!  I've responded to the outstanding PRs that I know of; feel free to ping if I've missed any.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Marking as community support, since @rongjiecomputer has already been sending many great PRs.", "Status update:\r\n\r\nMost of the changes needed to make `tfcompile` built and run on Windows have been merged. As promised, I made a repo with all the instructions to build and use XLA/AOT on Windows. (Contributions are welcomed to improve my silly MNIST C++ demo).\r\n\r\nhttps://github.com/rongjiecomputer/tensorflow-xla-aot-windows\r\n\r\nThere are some obstacles that prevent XLA/AOT to work out of the box for Windows users:\r\n- https://github.com/tensorflow/tensorflow/pull/15466 : Tensorflow team and Bazel team are still discussing about how to set global compile flags, a solved problem in build system like CMake.\r\n- https://github.com/bazelbuild/bazel/issues/4149 : Bazel tends to generate long path name, sometime it exceed the max path length on Windows. Changes to Bazel's design are needed to fix this.\r\n- Bazel runfiles on Windows (used by `freeze_graph` target). Waiting https://github.com/bazelbuild/bazel/issues/4149 and https://github.com/tensorflow/tensorflow/pull/15475.\r\n\r\nIt should be possible to build components of XLA/AOT with CMake, but unfortunately Tensorflow's CMake build scripts are written specifically for MSBuild. It just won't work with Ninja which I prefer. Any CMake expert is welcomed to bring XLA/AOT to CMake so that both LLVM and Tensorflow can be built together in CMake directly.\r\n\r\n@tatatodd @hawkinsp\r\n\r\n> llvm.BUILD is auto-generated and the script to generate it is not open-sourced.\r\n\r\nAny thoughts on this? My handcrafted [BUILD.bazel](https://github.com/rongjiecomputer/tensorflow-xla-aot-windows/blob/master/BUILD.bazel) is quite similar to the auto-generated [llvm.BUILD](https://github.com/tensorflow/tensorflow/blob/master/third_party/llvm/llvm.BUILD). Since its purpose is to import prebuilt libraries, headers and some defines to Tensorflow, I think it is possible to make it works on Linux and Mac OS as well (the only problem is I don't have machines to test).\r\n\r\nPlease do not close this issue yet. It is far from being \"fixed\".", "@rongjiecomputer @tatatodd @hawkinsp Any thoughts on possible next steps?", "@tellarin There was an issue in highwayhash in https://github.com/rongjiecomputer/tensorflow-xla-aot-windows/issues/1#issuecomment-454713660 but I have not sent out the PR yet.\r\n\r\nI have not build XLA/AOT on Windows for awhile, so not sure if current master can be built with MSVC or not. (internal team occasionally write some template code that behaves differently with MSVC)", "It seems the function AllOf(...) in pattern_matcher.h doesn't compile with MSVC (in VS2017 or VS2019)  or LLVM 8.", "I use `clang-cl` with MSVC and `-fno-delayed-template-parsing` flag to compile Tensorflow. If you use `/permissive-` flag with pure MSVC from latest update of VS2017/VS2019, it might work too.", "/permissive- did the trick, the code can be compiled, thanks.", "In VS2019 /permissive does not work as of this post & master branch commit bd67e65577f77656806797ee8e671eb236fb6c4a . \r\n\"tensorflow/compiler/xla/service/gpu/hlo_algorithm_blacklist.cc(28): error C2131: expression did not evaluate to a constant\". This was with /O2 and /arch:AVX2 options."]}, {"number": 15075, "title": "CUDA_ERROR_LAUNCH_FAILED with TensorFlow example", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, this error happens when using TensorFlow example \"mnist_deep.py\". Modified the file to go for 1.000 iterations instead of the default 20.000 because it took a long time to finish, everything else is the same.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 64-bit\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.3.0-0-g9e76bf3 1.3.0\r\n- **Python version**: Python 3 (Though the issue persists with python 2.7)\r\n- **Bazel version (if compiling from source)**: bazel-0.5.2-dist\r\n- **GCC/Compiler version (if compiling from source)**: GCC 5.4.0 20160609\r\n- **CUDA/cuDNN version**: CUDA: 8.0 cuDNN: 6.0\r\n- **GPU model and memory**: NVIDIA Tegra X2 8GB\r\n- **Exact command to reproduce**: python3 mnist_deep.py\r\n\r\nI am using the Nvidia Jetson TX2 developer kit with Jetpack 3.1 (The architecture is aarch64). I have tried installing Tensorflow for Python 2 and 3, but the issue persists with both installations.\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI had no problems installing from source for either Python 2.7 and 3. When executing \"mnist_deep.py\" I get a CUDA_ERROR_LAUNCH_FAILED error. Since I had no problems during install, I believe there is something wrong with communication between Tensorflow and CUDA, which is why I'm posting here.\r\n\r\nI had no problems excuting \"mnist_softmax.py\", and so the issue seem to be related to more sophisticated CNNs.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nThis is the terminal output:\r\n`nvidia@tegra-ubuntu:~/Desktop/tensorflow-r1.3/tensorflow/examples/tutorials/mnist$ ./mnist_deep.py\r\nSuccessfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\r\nExtracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\r\nSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\r\nExtracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\r\nSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\r\nSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\r\nSaving graph to: /tmp/tmpyJvseo\r\n2017-12-02 00:56:23.092487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:857] ARM64 does not support NUMA - returning NUMA node zero\r\n2017-12-02 00:56:23.092610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\r\nname: NVIDIA Tegra X2\r\nmajor: 6 minor: 2 memoryClockRate (GHz) 1.3005\r\npciBusID 0000:00:00.0\r\nTotal memory: 7.67GiB\r\nFree memory: 5.76GiB\r\n2017-12-02 00:56:23.092659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\r\n2017-12-02 00:56:23.092684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\r\n2017-12-02 00:56:23.092710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: NVIDIA Tegra X2, pci bus id: 0000:00:00.0)\r\nstep 0, training accuracy 0.04\r\nstep 100, training accuracy 0.86\r\nstep 200, training accuracy 0.96\r\nstep 300, training accuracy 0.94\r\nstep 400, training accuracy 0.86\r\nstep 500, training accuracy 0.92\r\nstep 600, training accuracy 0.96\r\nstep 700, training accuracy 0.96\r\nstep 800, training accuracy 0.96\r\nstep 900, training accuracy 1\r\n2017-12-02 00:57:17.461035: E tensorflow/stream_executor/cuda/cuda_driver.cc:1068] failed to synchronize the stop event: CUDA_ERROR_LAUNCH_FAILED\r\n2017-12-02 00:57:17.461146: E tensorflow/stream_executor/cuda/cuda_timer.cc:54] Internal: error destroying CUDA event in context 0x3372070: CUDA_ERROR_LAUNCH_FAILED\r\n2017-12-02 00:57:17.461188: E tensorflow/stream_executor/cuda/cuda_timer.cc:59] Internal: error destroying CUDA event in context 0x3372070: CUDA_ERROR_LAUNCH_FAILED\r\nTraceback (most recent call last):\r\n  File \"./mnist_deep.py\", line 177, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"./mnist_deep.py\", line 169, in main\r\n    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 541, in eval\r\n    return _eval_using_default_session(self, feed_dict, self.graph, session)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 4085, in _eval_using_default_session\r\n    return session.run(tensors, feed_dict)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: No algorithm worked!\r\n     [[Node: conv1/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](reshape/Reshape, conv1/Variable/read)]]\r\n     [[Node: Mean_1/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_79_Mean_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\nCaused by op u'conv1/Conv2D', defined at:\r\n  File \"./mnist_deep.py\", line 177, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"./mnist_deep.py\", line 138, in main\r\n    y_conv, keep_prob = deepnn(x)\r\n  File \"./mnist_deep.py\", line 64, in deepnn\r\n    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\r\n  File \"./mnist_deep.py\", line 106, in conv2d\r\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 397, in conv2d\r\n    data_format=data_format, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nNotFoundError (see above for traceback): No algorithm worked!\r\n     [[Node: conv1/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](reshape/Reshape, conv1/Variable/read)]]\r\n     [[Node: Mean_1/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_79_Mean_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\n2017-12-02 00:57:17.738653: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x3372070: CUDA_ERROR_LAUNCH_FAILED\r\n2017-12-02 00:57:17.738769: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x3372070: CUDA_ERROR_LAUNCH_FAILED\r\n2017-12-02 00:57:17.738800: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x3372070: CUDA_ERROR_LAUNCH_FAILED\r\n2017-12-02 00:57:17.738824: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x3372070: CUDA_ERROR_LAUNCH_FAILED\r\nnvidia@tegra-ubuntu:~/Desktop/tensorflow-r1.3/tensorflow/examples/tutorials/mnist$`\r\n", "comments": ["I don't have a Jetson TX2 developer kit to reproduce this on. @tfboyd do you know if anyone on the TensorFlow team has one?\r\n\r\n@zheng-xq @yzhwang any guesses to what the issue might be?\r\n", "James does.  If we are really interested (and have time to work on it) in this we should order them.  We almost came home with 2 from our visit last week.  ", "I have tried installing cuDNN 7 also. Jetpack 3.1 only comes with cuDNN 6, but apparently, [this](https://developer.nvidia.com/nvidia-tensorrt3rc-download) install of Tensor RT from NVIDIA can update the system to version 7 as well. I did that, but the error persists. ", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "/CC @aaroey, any plans for Jetson TX2 support?", "@aaroey is not working on Jetson TX2 support. Since no one is working on it currently, marking as contributions welcome.", "I didn't use Jetson but got the same error on my side. The code could run for a couple of epochs but suddenly threw the CUDA_ERROR_LAUNCH_FAILED error like on your side. Please let me know if you have a solution, thank you!"]}, {"number": 15028, "title": "ctc_loss with best align path", "body": "The tf.nn.ctc_loss function only returns negative log probabilities according to truth labels for the sequence's best align path. \r\nBut we also need this best align path in some situations.\r\nCould you please supply this feature, thanks!\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MINGW64_NT-6.1 xsk-PC 2.6.0(0.304/5/3) 2016-09-07 20:45 x86_64 Msys\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: 3.6.0\r\n- **Bazel version (if compiling from source)**: None\r\n- **GCC/Compiler version (if compiling from source)**: None\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**: None", "comments": ["Use `tf.nn.ctc_beam_search_decoder`?", "ctc_beam_search_decoder and ctc_greedy_decoder can only make predications, but can't get the best alignment path according to the target.", "According to \"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/ctc/ctc_loss_calculator.cc\", I think the best alignment and probs may be as below and this is not a waste of resources during calculating ctc loss.\r\n\r\nbest_align = argmax(log_alpha, axis=0) \r\nblank_pos = where(best_align % 2 == 0)\r\ntarget_pos = where(best_align % 2 != 0)\r\nbest_align[target_pos] = target[best_align[target_pos] // 2]\r\nbest_align[blank_pos] = blank_index_\r\nbest_prob = logit[best_align, range(T)]", "If you count the blanks, you should have the alignment.", "@drpngx Could you please explain the meaning of 'count the blanks' more detail. After I used Viterbi with ctc to find the most probable path, which is not the true align path. I think ctc loss just make sure the sum of probs is biggest for all probable paths, but the prob of true align path may be very small among the candidate paths. And we can't get the real align path in fact according to ctc loss. ", "Could you clarify the difference between _the most probable path_ and _the true align path_? Do you mean you want the output sequence without collapsed repeats and including blanks so it's of the same time resolution as your input?", "@carlthome , @xushenkun is saying that the decode does not allow you to constrain the path for alignment, and the loss function computes the full forward probability (in the log semiring, not max). For instance if you computed the top1 from beam search and used the `ctc_loss`, then you'd get a probability that's greater than or equal t the one returned by the decoder.\r\n\r\n@xushenkun you'd need to either have an option to the ctc loss to use max instead of logsum, or have another version of the decoder that takes in the sequence (possibly without epsilons).", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "+1; if anyone is interested in implementing either of @drpngx's suggested options, should send us a PR.  it would require a new op (not modifying the existing CTC Loss / Decoders) because it would change the signature.  The new op can be exposed in tf.contrib.", "> @drpngx Could you please explain the meaning of 'count the blanks' more detail. After I used Viterbi with ctc to find the most probable path, which is not the true align path. I think ctc loss just make sure the sum of probs is biggest for all probable paths, but the prob of true align path may be very small among the candidate paths. And we can't get the real align path in fact according to ctc loss.\r\n\r\nhave you solved this problem? I am trying to algin the ctc decoded output to the raw timeline, can you help me?"]}, {"number": 14998, "title": "Extend reshape with begin_axis and end_axis like in cntk", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nCNTK has a generalized version of reshape and it would be nice to have such a version also in Tensorflow\r\n(https://github.com/Microsoft/CNTK/blob/master/bindings/python/cntk/ops/__init__.py#L1972).\r\n```python\r\ndef reshape(x, shape, begin_axis=None, end_axis=None, name=''):\r\n    ...\r\n```\r\n\r\nThe difference is, that the user can provide `begin_axis` and `end_axis` and if they are specified reshape only operate on a subset of the shape.\r\n\r\nI can make a PR, when somebody says me, where I have to write the code.\r\n\r\n### Source code / logs\r\nHere a working example:\r\n```python\r\ndef reshape(tensor, shape, begin_axis=None, end_axis=None, name=None) -> tf.Tensor:\r\n    if begin_axis is None and end_axis is None:\r\n        return tf.reshape(tensor, shape, name=name)\r\n\r\n    with tf.name_scope(name, 'reshape', [tensor]):\r\n        tensor_shape = tf.shape(tensor)\r\n        to_concat = [shape]\r\n        if begin_axis is not None:\r\n            bs = tensor_shape[:begin_axis]\r\n            to_concat.insert(0, bs)\r\n        if end_axis is not None:\r\n            es = tensor_shape[end_axis:]\r\n            to_concat.append(es)\r\n\r\n        tensor_shape = tf.concat(to_concat, 0)\r\n\r\n        return = tf.reshape(tensor, tensor_shape)\r\n```\r\n\r\nand an example doctest\r\n```python\r\n    \"\"\"\r\n\r\n    Inspired from cntk.reshape to allow begin_axis and end_axis\r\n\r\n    Assume you call reshape\r\n    >> out = reshape(in, shape, b, e)\r\n    Than the following will hold\r\n    (Note: If b or e is None, the are interpreted as 0 and/or include the last axis)\r\n    >> in_shape = in.shape\r\n    >> in_shape[b:e] = shape\r\n    >> assert out.shape == in_shape\r\n\r\n    First example normal reshape, where the input has unknown dimension\r\n    >>> import numpy as np\r\n    >>> _ = tf.InteractiveSession()\r\n    >>> x = tf.placeholder(tf.float32)\r\n    >>> y = reshape(x, [-1])\r\n    >>> y\r\n    <tf.Tensor 'Reshape:0' shape=(?,) dtype=float32>\r\n    >>> y.eval({x: np.zeros([3, 4])}).shape\r\n    (12,)\r\n\r\n    Now keep first and last axis: (No shape inference expected, to difficult)\r\n    >>> y = reshape(x, [-1], begin_axis=1, end_axis=-1)\r\n    >>> y\r\n    <tf.Tensor 'reshape/Reshape:0' shape=<unknown> dtype=float32>\r\n    >>> y.eval({x: np.zeros([3, 4, 5, 6])}).shape\r\n    (3, 20, 6)\r\n\r\n    Now with ndim defined:\r\n    >>> x = tf.placeholder(tf.float32, shape=[None, None, None, None])\r\n    >>> y = reshape(x, [-1], begin_axis=1, end_axis=-1)\r\n    >>> y\r\n    <tf.Tensor 'reshape_1/Reshape:0' shape=(?, ?, ?) dtype=float32>\r\n    >>> y.eval({x: np.zeros([3, 4, 5, 6])}).shape\r\n    (3, 20, 6)\r\n\r\n    Now with partial defined shape:\r\n    >>> x = tf.placeholder(tf.float32, shape=[3, 4, None, 6])\r\n    >>> y = reshape(x, [-1], begin_axis=1, end_axis=-1)\r\n    >>> y\r\n    <tf.Tensor 'reshape_2/Reshape:0' shape=(3, ?, 6) dtype=float32>\r\n    >>> y.eval({x: np.zeros([3, 4, 5, 6])}).shape\r\n    (3, 20, 6)\r\n\r\n    Now with full defined shape:\r\n    >>> x = tf.placeholder(tf.float32, shape=[3, 4, 5, 6])\r\n    >>> y = reshape(x, [-1], begin_axis=1, end_axis=-1)\r\n    >>> y\r\n    <tf.Tensor 'reshape_3/Reshape:0' shape=(3, 20, 6) dtype=float32>\r\n    >>> y.eval({x: np.zeros([3, 4, 5, 6])}).shape\r\n    (3, 20, 6)\r\n\r\n    :param tensor:\r\n    :param shape:\r\n    :param name:\r\n    :return:\r\n    \"\"\"\r\n```\r\n", "comments": ["It seems that `tf.reshape` supports to merge its dimensions, for example:\r\n\r\n```python\r\nx = tf.placeholder(tf.float32, shape=[3, 4, 5, 6])\r\ny = tf.reshape(x, (3, 20, 6))\r\n# or\r\ny = tf.reshape(x, (3, -1, 6))\r\n```\r\n\r\nCould you explain in which situation those two parameters could be useful?", "Currently `tf.reshape` is powerful enough to solve all problems, but there are two reasons in my mind, why the further arguments are useful, not necessary.\r\n\r\nFirst: Less \"hidden bugs\" on user side\r\nExample: I modified your example (swapped 6 and 4), it is still executable, but the intent was to flatten the inner dims.\r\nSo the output shape is correct, but it hides a bug in user code.\r\n```python\r\nx = tf.placeholder(tf.float32, shape=[3, 6, 5, 4])\r\ny = tf.reshape(x, (3, -1, 6))\r\n```\r\n\r\nSecond: It is easier when there are unknown dimensions\r\nFor example the following does not work\r\n```python\r\nx = tf.placeholder(tf.float32, shape=[3, None, 5, None])\r\ny = tf.reshape(x, (3, -1, -1))\r\n```\r\nFor above example I had to learn the difference between `tensor.shape` and `tf.shape(tensor)` to make it possible\r\n```python\r\nx = tf.placeholder(tf.float32, shape=[3, None, 5, None])\r\nshape = x.shape  # does not work, first shape[-1] is None, second shape[-1] can not be set to -1, because there is already a -1\r\nshape = tf.shape(x)  # work\r\ny = tf.reshape(x, (shape[0], -1, shape[-1]))\r\ny = tf.reshape(x, (*shape[:1], -1, *shape[3:]))  # does not work, because the code `*tensor` is not allowed, but `*tensor.shape` work.\r\n```\r\nFurther with the new arguments you can easily flatten the inner dims, while the tensor has a unkown shape.\r\n\r\n", "@martinwicke, do you have any api thoughts. My thought is that I'd prefer this to be a separate call as this would keep it more similar to numpy (which doesn't have begin_axis and end_axis teither). You could call it partial_reshape() or something.\r\n", "I agree. I'd prefer a separate function, `partial_reshape` seems like a good name. Added API review to figure this out before anyone writes any code.", "The Code is already finished (see initial comment) and since the current reshape is powerful enough, only a small wrapper is necessary to extend the API. (The function is inside my code base and I thought that maybe also some other people find it useful.)\r\n\r\nI agree that `partial_reshape` would highlight, what the function will do, but it is an extension of `reshape` and not a modification of `reshape`. So the default arguments of `partial_reshape` represent a wrapper around `reshape`. Then some people would not use `partial_reshape`, because they do not know it and other will make an alias and use only `partial_reshape`. (In the case somebody can imagine another extension of reshape that is incompatible with my suggestion, I agree that a separate function will be better.)\r\n\r\nWhen you compare `tf.matmul` with `np.matmul` both have a common interface, but `tf` extends the base idea to be more powerful. So it is already done in tensorflow to add more keywords.\r\n\r\nAnother point is, that it is quite easy to calculate the desired shape in numpy, because in numpy the shape is a list of int, while in tensorflow it can happen that the shape is undefined or there are more than one undefined shapes.\r\n\r\n\r\n\r\n", "The API review consensus was not excited by modifying reshape's signature since it is already a widely used function and matches numpy. \"partial_reshape\" seemed more acceptable.", "Ok, if there is an interest for `partial_reshape` (It sounds like nobody, except me, find such an extension useful.), I can make a PR.\r\nBut I need to know, where to place the code. Afaik it is common in tensorflow to initially place code under contrib.\r\n", "You could add it contrib, in a new module, possibly called contrib/ops. I am skeptical that it's worthwhile, although you never know how many people silently want this as well.", "@martinwicke When I make a new module, do you have a suggestion, which module I can use as a template? `partial_reshape` is only a thin wrapper around reshape, so I only need a single python file and I am not familiar with the structure of a tensorflow module.", "I suggest you to refer to [tf.contrib.lookup](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lookup), which is quite simple.", "Where can I find what the `BUILD` file means (i.e. the documentation)? \r\nIt may be difficult to program it correctly because I use tensorflow binary and not from source.", "You will have to use the source code. The binary distribution is missing\nall the BUILD files and tests.\n\nOn Dec 19, 2017 6:46 AM, \"Christoph Boeddeker\" <notifications@github.com>\nwrote:\n\n> Where can I find what the BUILD file mean?\n> It may be difficult to program it correctly because I use tensorflow\n> binary and not from source.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/14998#issuecomment-352776604>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAjO_a5rD7oJJrRto4mhp41q0AXN_LUgks5tB8xfgaJpZM4QwTtN>\n> .\n>\n", "I know that I need the source code, else I can not use git.\r\nThe problem is, that I made the mistake to install tensorflow from source and hit weird bugs (Maybe I got a bad commit). Since then I use tensorflow binary and use a diff programme to sync the installed and the source code in the git repo because I only change python files. (As far as I know it is not possible to install tensorflow from binary (i.e. without bazel) as editable with the python files inside the git repo (i.e. `git clone ... && cd tensorflow && pip install -e .`).)\r\n\r\n\r\nMy question regarding the BUILD file is that I do not know what language it is or what style.\r\nI know that it is not a Makefile, cmake, bash script and python.\r\nAlso, I do not know how to execute it (Likely something with bazel (problem) and not python).\r\nSo when I would write a BUILD file I would always guess.\r\n\r\nMaybe I should keep the `partial_reshape` in my own code base until I understand how to add a new module and decided to build tensorflow. Or maybe someone has interest in `partial_reshape` and knows how to add a new module.\r\n\r\n\r\n\r\n\r\n", "@boeddeker, I understand that it might be not easy.\r\nBUILD file is configured for [bazel](https://docs.bazel.build/versions/master/bazel-overview.html), which is an open-source build and test tool similar to Make, Maven, and Gradle. Those are some tutorials which perhaps are useful for you:\r\n+ [Getting Started with Bazel](https://docs.bazel.build/versions/master/getting-started.html)\r\n+ [Introduction to Bazel: Building a C++ Project](https://docs.bazel.build/versions/master/tutorial/cpp.html)\r\n+ [Concepts and Terminology](https://docs.bazel.build/versions/master/build-ref.html#BUILD_files)", "@facaiy Thanks for the links, when I find the time, I will read them and make a PR.\r\nBut it may take a while, so when someone is interested to add `partial_reshape` earlier, feel free to copy my example code."]}, {"number": 14995, "title": "Bug: tf.estimator.Estimator.export_savedmodel does not work with pathlib.Path in py36", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: \r\n\r\n### Describe the problem\r\nThe function `tf.estimator.Estimator.export_savedmodel` does not accept a `pathlib.Path` object, because \r\n`tensorflow.python.util.compat.as_bytes` used in `tensorflow.python.estimator.export.get_timestamped_export_dir` can not convert `pathlib.Path` to bytes.\r\nHere the code snippet from `tensorflow.python.estimator.export.get_timestamped_export_dir`:\r\n```python\r\n    export_dir = os.path.join(\r\n        compat.as_bytes(export_dir_base),\r\n        compat.as_bytes(str(export_timestamp)))\r\n```\r\nI would write a PR, but I am not sure how to solve this problem in Python2. The following works in Python3.6 (If I remember correctly it was py36, where `os.path` start to accept `pathlib.Path`):\r\n\r\n```python\r\n    export_dir = compat.as_bytes(os.path.join(\r\n        export_dir_base,\r\n        str(export_timestamp)))\r\n```\r\nSince the name `tensorflow.python.util.compat.as_bytes` does not imply that the input is a path, I am not sure if that would be a better place to solve the problem.\r\n\r\n### Source code / logs\r\nHere some pseudo code (I hope with this example the tensoflowers are able to reproduce this bug in py36): \r\n```python\r\nfrom pathlib import Path\r\ntf.estimator.Estimator(...).export_savedmodel(\r\n            Path('path/to/save'),\r\n            export_input_fn,\r\n            as_text=True,\r\n        )\r\n```", "comments": ["I prefer to use `tf.gfile`.", "If I interpret `tf.gfile` correct its made for file handling (i.e. a replacement for `open`) and not for path handling (like `os.path` and `pathlib`).\r\nThe code that causes the problem is related to py2 vs py3 with `bytes` and `str`.\r\nSo I do not know how to solve this with `tf.gfile`.\r\n", "@boeddeker Thanks for the great description of the issue!\r\n\r\n@davidsoergel seems to have touched some of this code, and might have thoughts on appropriate fixes.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "@drpngx This problem is still there. I only got no answer how to fix this problem. So as @tensorflowbutler alwais mention: `awaiting tensorflower`\r\n\r\nOne solution would be, that the following code is executable in python 3:\r\n```\r\nfrom tensorflow.python.util import compat\r\ncompat.as_bytes(Path('abc'))\r\n```\r\nbut an argument against it is, that `compat.as_bytes` has nothing to do with paths.\r\n\r\nAnother solution would be to use the code that I wrote initially (i.e. swap `as_bytes` and `join`), but that would require in python3 that `export_dir_base` is a `str`. \r\n`os.path.join` in python2 does not care about the input type, so there the change will not effect something.\r\n\r\n", "Do you have a PR in mind you want to send?", "I can open a PR, when someone says which solution.", "Deferring to @sukritiramesh ", "Nagging Assignee @drpngx: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @drpngx: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @drpngx: It has been 48 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @drpngx: It has been 63 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Anyway, I think it would be nice to have, but I'm not sure about using `as_bytes`."]}, {"number": 14934, "title": "Multi-arch docker images", "body": "Are there non-x86 (ppc64le, arm etc) docker images available for tensorflow? \r\n\r\nThis looks like an intel image : https://hub.docker.com/r/tensorflow/tensorflow/\r\n\r\n", "comments": ["Not to my knowledge.  @caisq, care to comment on strategy or thoughts.", "I've recently proposed a change to TensorFlow that obsoletes parameterized_docker_build.sh, which may help alleviate this issue (because it could lead to more easily-maintained non-x86 images). If anyone following this thread is interested in making TensorFlow's Dockerfile story better for everyone, [please take a look at the RFC](https://github.com/tensorflow/community/pull/8).", "Just a quick note here that @angersson has a PR up for the new docker build method.\r\nhttps://github.com/tensorflow/tensorflow/pull/21291\r\n\r\nOnce this is merged we'll be working on using the current community environment for ppc64le to also build docker images.", "Is this issue resolved ?", "It is not. I plan on getting back to this shortly for the ppc64le image, which should lay a blueprint for other arch's.", "I'm starting to look at the ppc64le image", "I have a pull request that adds ppc64le docker files. \r\nhttps://github.com/tensorflow/tensorflow/pull/23194", "Thank you @tjakob ", "There was a new PR to rework the new docker build method some. https://github.com/tensorflow/tensorflow/pull/24051. I'm reworking my PR to use this newer method.", "@tjakob  Please keep us posted about your new PR", "My new PR for the latest rework of the dockerfiles can be found here: https://github.com/tensorflow/tensorflow/pull/24180\r\n", "https://github.com/tensorflow/tensorflow/pull/24180 just merged. This should provide a place to start for other non-x86 docker images.", "Thanks @tjakob  and @angersson !", "By the way, the ppc64le docker images are published to:\r\nhttps://hub.docker.com/r/ibmcom/tensorflow-ppc64le/", "@tjakob and @angerson - I would like to investigate the possibility of adding s390x arch images (be) in the mix.  Just wondering if the TF CI infrastructure now supports multi-arch Docker images?  I could not spot `ppc64_le` Docker images on tensorflow/Dockerhub repo.  Are there plans of publishing multi-arch images on tensorflow repo on Dockerhub? Thanks.\r\n", "> Are there plans of publishing multi-arch images on tensorflow repo on Dockerhub? Thanks.\r\n\r\nWe don't have plans for this. Unfortunately, due to loss of staff on our team, it's more likely that we'd have to shrink our Docker support instead, but there are no plans as of now."]}, {"number": 14924, "title": "What is supported by broadcasting? Is the dimensions still limited?", "body": "This is related to issue #1519.\r\n\r\nThis is OK:\r\n\r\n```\r\nimport tensorflow as tf\r\nsess = tf.InteractiveSession()\r\nxx = tf.constant(1, shape=[32,1,4,4,1], dtype=tf.float32)\r\nyy = tf.constant(1, shape=[1,32,1,4,4], dtype=tf.float32)\r\nzz = xx * yy\r\nsess.run([zz])\r\n```\r\n\r\nHowever:\r\n```\r\nx2 = tf.constant(1, shape=[10,32,1,4,4,1])\r\ny2 = tf.constant(1, shape=[10,1,32,1,4,4])\r\nz2 = x2 * y2\r\nsess.run(z2)\r\n```\r\nGives an error:\r\n`UnimplementedError (see above for traceback): Broadcast between [10,32,1,4,4,1] and [10,1,32,1,4,4] is not supported yet.\r\n\t [[Node: mul_1 = Mul[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Const_2, Const_3)]]`\r\n\r\nLog:\r\n```\r\n\r\n---------------------------------------------------------------------------\r\nUnimplementedError                        Traceback (most recent call last)\r\n<ipython-input-2-eef82717f8d8> in <module>()\r\n      2 y2 = tf.constant(1, shape=[10,1,32,1,4,4])\r\n      3 z2 = x2 * y2\r\n----> 4 sess.run(z2)\r\n\r\n/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)\r\n    887     try:\r\n    888       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 889                          run_metadata_ptr)\r\n    890       if run_metadata:\r\n    891         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1118     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1119       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1120                              feed_dict_tensor, options, run_metadata)\r\n   1121     else:\r\n   1122       results = []\r\n\r\n/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1315     if handle is None:\r\n   1316       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\r\n-> 1317                            options, run_metadata)\r\n   1318     else:\r\n   1319       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)\r\n\r\n/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)\r\n   1334         except KeyError:\r\n   1335           pass\r\n-> 1336       raise type(e)(node_def, op, message)\r\n   1337 \r\n   1338   def _extend_graph(self):\r\n\r\nUnimplementedError: Broadcast between [10,32,1,4,4,1] and [10,1,32,1,4,4] is not supported yet.\r\n\t [[Node: mul_1 = Mul[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Const_2, Const_3)]]\r\n\r\nCaused by op u'mul_1', defined at:\r\n  File \"/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\r\n    \"__main__\", fname, loader, pkg_name)\r\n  File \"/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/runpy.py\", line 72, in _run_code\r\n    exec code in run_globals\r\n  File \"/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\r\n    app.launch_new_instance()\r\n  File \"/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\r\n    ioloop.IOLoop.instance().start()\r\n  File \"/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\r\n    super(ZMQIOLoop, self).start()\r\n  File \"/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\r\n    handler_func(fd_obj, events)\r\n  File \"/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\r\n    self._handle_recv()\r\n  File \"/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-2-eef82717f8d8>\", line 3, in <module>\r\n    z2 = x2 * y2\r\n  File \"/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py\", line 894, in binary_op_wrapper\r\n    return func(x, y, name=name)\r\n  File \"/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py\", line 1117, in _mul_dispatch\r\n    return gen_math_ops._mul(x, y, name=name)\r\n  File \"/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2726, in _mul\r\n    \"Mul\", x=x, y=y, name=name)\r\n  File \"/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nUnimplementedError (see above for traceback): Broadcast between [10,32,1,4,4,1] and [10,1,32,1,4,4] is not supported yet.\r\n\t [[Node: mul_1 = Mul[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Const_2, Const_3)]]\r\n```", "comments": ["An update:\r\n\r\nI assume the reason is related to how the dimensions are matching, instead of the total number of dimensions, or the number of mis-match. Because the following script runs OK, where x3 has the 2nd to last dimension changes from 4 to 1, adding one more places of mismatch.\r\n\r\n```\r\nx3 = tf.constant(1, shape=[10,32,1,4,1,1])\r\ny3 = tf.constant(1, shape=[10,1,32,1,4,4])\r\nz3 = x3 * y3\r\nsess.run(z3)\r\n```", "For the record, I get the same error, for another pair of shapes, on an element-wise multiplication:\r\n\r\n```UnimplementedError (see above for traceback): Broadcast between [10,6,6,4,4,1] and [10,1,6,1,4,5] is not supported yet.```", "It looks like broadcasting for dimensions 6 and above is not implemented see\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cwise_ops_common.h#L142\r\n\r\nYou can likely add a case for 6 dimensional and see if that works.\r\n", "@aselle it might be, but then what i don't understand is why the following works just fine.\r\n\r\n```\r\nx3 = tf.constant(1, shape=[10,32,1,4,1,1])\r\ny3 = tf.constant(1, shape=[10,1,32,1,4,4])\r\nz3 = x3 * y3\r\nsess.run(z3)\r\n```\r\n\r\nThis case has **one more dimension mismatch** and need broadcast than the case failed.", "It works for me if I add broadcasting for dimension. But it is still puzzling that @gyang274's 6-dimensional example works even without adding that test case.\r\n\r\nPerhaps that is because `y3` could be reshaped to [10,1,32,1,16] and multiplied by `x3` with the last dimension squeezed out and the result would be the same. There seem to be calls to reshape all around the code that deals with broadcasting, so perhaps this case is taken into account.", "This works fine in Numpy, by the way. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/fddd56667e2bd25ecb57353330ddc542/gh_14924.ipynb). ", "Verified this still behaves this way (modulo the session's which are no longer used in tf2). There are no plans to change the broadcasting, but this does work fine on tf's numpy mode i.e.\r\n```\r\nx3 = tf.constant(1, shape=[10,32,1,4,1,1])\r\ny3 = tf.constant(1, shape=[10,1,32,1,4,4])\r\nnp.multiply(x3,y3)\r\n```\r\n@wangpengmit "]}, {"number": 14798, "title": "Provide a list of supported XLA operations like TensorFlow Lite", "body": "TensorFlow Lite provides a list of currently supported ops [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/tf_ops_compatibility.md) and I wonder if XLA could also have such a list. It's rough to develop and train a model with the full TensorFlow Python API only to get stuck during AOT compilation because of missing ops kernels in the tf2xla bridge.", "comments": ["We now have some auto-generated tables listing the supported ops on CPU and GPU:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2xla/g3doc/cpu_supported_ops.md\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2xla/g3doc/gpu_supported_ops.md\r\n\r\nUnlike the TFLite docs, we don't have a breakdown starting from the Python APIs; the above tables are based on the op names in the GraphDef.  At the moment, if we wanted the Python API breakdown, we'd need to do that manually, and that seems unlikely to remain up-to-date.  I hope the above tables are still useful though.", "Thanks!", "@tatatodd  @joker-eph @MarkDaoust Do you know who is going to re-generate the tables mentioned by @tatatodd? It seems that last time they were updated in 2018.", "It seems it was introduced many years ago by @caisq with https://github.com/caisq/tensorflow/commit/4b0a23684852fe68ac2248fe2e04e118a6173848", "@lamberta @mihaimaruseac Do you know what kind of internal infra is going to \"regularly\" run https://github.com/tensorflow/tensorflow/blob/07c2bcc534a6b41c4953a1db70f6386b144cc5b7/tensorflow/compiler/tf2xla/tf2xla_supported_ops_main.cc#L20 to update the Markdown tables?", "The tflite page doesn't get regular updates either: https://www.tensorflow.org/lite/guide/ops_compatibility\r\n\r\n\r\nThat xla command still works. \r\n\r\nOne solution would be to integrate this into api-reference generator, add an XLA column to the https://www.tensorflow.org/api_docs/python/tf/raw_ops page:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docs/generate2.py#L104\r\n\r\ntensorflow.org/xla comes from tensorflow/compiler/xla/g3doc/ maybe someone there would have interest in pushing this through.", "> One solution would be to integrate this into api-reference generator, add an XLA column to the https://www.tensorflow.org/api_docs/python/tf/raw_ops page:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docs/generate2.py#L104\r\n\r\nIs this orchestrated by public available Github actions or with internal scripts? \r\nIf not:\r\nIs this  bazel target `tensorflow/compiler/tf2xla:tf2xla_supported_ops` available in the wheel? \r\nIs the wheel installed in the orchestrated environment of `docs/generate2.py`?", "At least can we reopen this ticket adding also the XLA label?", "> can we reopen this ticket adding also the XLA label?\r\nDone.\r\n\r\nIt's an internal tool that runs those. \r\nThey're run from the target version's github branch, with bazel available, so just calling that bazel command and merging the output into that raw-ops table would work.\r\n", "> t's an internal tool that runs those.\\nThey're run from the target version's github branch, with bazel available, so just calling that bazel command and \n\nThanks so probably It Is a little bit hard to contribute a PR with only the OSS/Github visibilty.", "Yes.\r\n\r\nIt's possible that just integrating it into generate2.py with `subprocess.check_output(['bazel', 'run', '-c', 'opt', '--',  'tensorflow/compiler/tf2xla:tf2xla_supported_ops'])` could get the job done. ", "I meant could it be tested locally when we have no visibility of the CI logs?", "If anyone gets it working locally then it's my job to be sure it works in the CI.", "Yes when we don't have or we want to have the orchestration/environment with public visibility we need to have exrta docs on how to test this locally if we want to collect community contribution. \r\nAs also the TFlite markdown is on hold since 2020 we could ping also the TFlite team", "Can we find an owner? As I don't know if @tatatodd is still on this project."]}, {"number": 14784, "title": "Does TF provide C++ interface corresponding to the Python ops \"tensorflow.nn.ctc_greedy_decoder\"?", "body": "As the title, thanksss!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "not yet", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "The short answer is, no, there isn't, and I think it might be a long way from there. Feel free to submit a PR.\r\n\r\nCC @asimshankar @skye Back in the days, some people would write automatic converters, then polish up the code manually. That might be an interesting experimental thing to do."]}, {"number": 14749, "title": "Feature Request : Hierarchical Softmax implementation using Tensorflow", "body": "I am trying to multi-level classify with Hierarchical Softmax by using Tensorflow. But I could find any existing HS implementation in Tensorflow.\r\n\r\nIs there any other way to implement HS using tensorflow?\r\n\r\nIt would be helpful if Hierarchical Softmax support is given in TF.", "comments": ["I don't think there is an existing HS implementation in TensorFlow.  Contributions are welcome!\r\n\r\nFYI here is the first result from a google search for [[hierarchical softmax tensorflow]](https://www.google.com/search?q=hierarchical+softmax+tensorflow)\r\n\r\nhttps://stackoverflow.com/questions/44140486/scalable-efficient-hierarchical-softmax-in-tensorflow", "Hi, I'd like to contribute, some initial guidance for this feature is appreciated. Thanks\r\n", "This is a reference implementation (though it's for balanced complete binary tree): [code](https://github.com/tansey/sdp/tree/87e701c9b0ff3eacab29713cb2c9e7181d5c26aa/tfsdp)\r\n\r\nIt's released with paper \"Deep Nonparametric Estimation of Discrete Conditional Distributions via Smoothed Dyadic Partitioning\".", "Would it make sense to have this added in [TensorFlow Addons](https://www.github.com/tensorflow/addons), instead?", "HS is not popular nowadays, so maybe there won't be too many people caring about in this.\r\nBut surely you can add this to TF Addons instead of the TF main repo if you have time.", "Hi team, I'm a bit new to submitting PRs to mainstream repos. This is tagged as contributions welcome. If I go ahead and build this out and submit a PR will it be looked at? Thanks!\r\n\r\n**EDIT** If you're an admin and see this, please don't assume I've dropped it. I check back here often."]}, {"number": 14675, "title": "Small change to graph changes initial values of variables", "body": "I'm running TensorFlow 1.2 with CUDA.\r\n\r\nError case:\r\nI call ``tf.set_random_seed(2017)`` and then build a graph ``g1`` that includes trainable variables and an optimizer. I create a session, run the ``tf.global_variables_initializer()`` op, and then immediately fetch the value of a scalar variable (without running any train steps, so this is the initial value of the variable). As expected, this value stays the same if I launch this process multiple times.\r\n\r\nI then build a graph ``g2`` that is identical to ``g1`` except for a slight change. ``g1`` contained ``mu = tf.reduce_sum(x) / tf.size(x)[0]``, and ``g2`` contains ``mu = tf.reduce_mean(x)``. ``g2`` is seeded the same way as ``g1`` and has the same variable names and shapes as ``g1``. The only differently named tensors are those relating to the modification mentioned above. When I fetch the initial value of the same scalar variable from ``g2``, there is a completely different value from when fetched from ``g1``.\r\n\r\nI've tried to isolate this into a small test case but have not been successful yet. I will continue to work on this. Apologies for bug report without test case.\r\nMy current intention is to workaround this with a Numpy based initialization scheme.\r\n\r\nQuestions:\r\n(1) Is this expected behavior? Ideally the variables would be initialized in the same way to help make results more reproducible. In my case, the different variable initialization makes it more difficult to test that ``g1`` and ``g2`` produce the same values. If variables were initialized the same way, would be easy to see that refactoring the mean computation in the graph did not break anything.\r\n\r\n(2) Any idea why this occurs? Perhaps relevantly, ``tf.make_template`` is used within this graph. My current (evidence-free) hunch is the small change in graph causes a variable to move from CPU resident to GPU resident and caused a different PRNG kernel (provided with the same seed) to be used.", "comments": ["Hi, could you give a reproducable code? Thanks.", "I've created a small reproducible example.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef build_graph(toggle=False):\r\n    tf.set_random_seed(2017)\r\n    x = tf.placeholder(tf.float32, [20], name='x')\r\n\r\n    if toggle:\r\n        z = tf.reduce_mean(x)\r\n    else:\r\n        z = tf.reduce_sum(x) / tf.cast(tf.shape(x)[0], tf.float32)\r\n\r\n    b = tf.get_variable('b', [])\r\n    y = x + b\r\n\r\n    return z\r\n\r\ndef main():\r\n    for toggle in [False, True]:\r\n        for i in range(2):\r\n            g = tf.Graph()\r\n            with g.as_default():\r\n                build_graph(toggle)\r\n\r\n                with tf.Session() as sess:\r\n                    sess.run(tf.global_variables_initializer())\r\n\r\n                    b = tf.trainable_variables()[0]\r\n                    print ('toggle=%s\\titer=%d\\tbias_name=%s\\tbias_val = %.4f' %\r\n                           (toggle, i, b.name, sess.run(b)))\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n\r\n```\r\n\r\nOutput when I run this:\r\n```\r\ntoggle=False    iter=0  bias_name=b:0   bias_val = -0.3459\r\ntoggle=False    iter=1  bias_name=b:0   bias_val = -0.3459\r\ntoggle=True     iter=0  bias_name=b:0   bias_val = -1.4101\r\ntoggle=True     iter=1  bias_name=b:0   bias_val = -1.4101\r\n```\r\n\r\nNote that the ``bias_val`` is consistent with a given value of toggle but not across changing values of toggle. \r\n\r\nObservation: If I move the ``get_variable`` call before the toggle branch then the variable is always initialized to the same value.", "I guess that operators consume the random number, see the example below, `-0.3459` is the second number generated by random in fact:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef build_graph(toggle=False):\r\n    tf.set_random_seed(2017)\r\n    for n in range(10):\r\n        tf.get_variable('b_%d' % n, [])\r\n\r\n\r\ndef main():\r\n    for toggle in [False]:\r\n        for i in range(1):\r\n            g = tf.Graph()\r\n            with g.as_default():\r\n                build_graph(toggle)\r\n\r\n                with tf.Session() as sess:\r\n                    sess.run(tf.global_variables_initializer())\r\n\r\n                    for b in tf.trainable_variables():\r\n                        print('bias_name=%s\\tbias_val = %.4f' % (b.name, sess.run(b)))\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\noutput:\r\n```python\r\nbias_name=b_0:0 bias_val = 1.4373\r\nbias_name=b_1:0 bias_val = -0.3459\r\nbias_name=b_2:0 bias_val = 0.2771\r\nbias_name=b_3:0 bias_val = -0.4072\r\nbias_name=b_4:0 bias_val = 0.6182\r\nbias_name=b_5:0 bias_val = 1.6991\r\nbias_name=b_6:0 bias_val = 0.3959\r\nbias_name=b_7:0 bias_val = 0.7480\r\nbias_name=b_8:0 bias_val = -1.3848\r\nbias_name=b_9:0 bias_val = -0.9725\r\n```", "The [``tf.set_random_seed`` docs](https://www.tensorflow.org/api_docs/python/tf/set_random_seed) describe the semantics behind graph and op seeds. In my example, a graph level seed is set but the op level seed for each initializer tensor is a function of the the graph level seed. It appears this function depends on the order of invocation (to create different op seeds for each op).\r\n\r\nI think a more intuitive op-seed generation procedure would be a function of (graph seed, tensor name, tensor shape)", "@eamartin I think you've answered your own question with your further research into graph and op seeds.  The documentation you've pointed to also describes the use-cases for the different seeds.\r\n\r\nThe basic rule is that if you want to ensure certain variables in your graph are initialized to the same random value, you should set the op seed.\r\n\r\nChanging the behavior of the implicit (unset) op-seed to behave differently might be \"more intuitive\" in some cases, but less in others.  Basically if you're relying on the implicit op-seed to ensure your variables are initialized the same, you're doing it wrong.  You should just explicitly set the op seed in those cases.\r\n\r\nClosing this out, since the original question was answered.  If you feel there's still a bug or feature to discuss, just ping this thread, and I'll re-open.", "I still feel that there's a feature/bug to discuss here. Namely, I think TensorFlow should strive to make reproducibility as easy as possible for the user, and that that's not the case here.\r\n\r\nCan you think of any cases where the current default op seed is more intuitive than what I've suggested (default op seed is a function of graph seed and tensor name)?\r\n\r\nI don't think I've ever seen a TF program that sets an op seed on every variable initializer (and other random ops). Given this, results from TF are generally not reproducible under the slightest of tweaks.\r\n\r\nUse case I'd like to support:\r\nUser has a function that takes in a seed, builds a graph, trains a model. User saves the output of the training (learning curves, model weights, etc) along with the git commit of the code and the random seed used. User then wants to try a tiny change (for instance, changing the implementation of mean). User checks out commit, makes small change, runs again with the same seed, and sees nearly identical numerical results.\r\n\r\nThoughts on implementation:\r\nWe definitely can't break reproducibility of existing serialized graphs. We probably shouldn't break reproducibility of existing graph building programs. I think this means that the default op seed scheme cannot be changed. Two options I think might be possible: \r\n* a ``tf.set_op_seed_default`` function that changes the method in which op seeds are generated\r\n* an additional ``tf.variable_scope`` kwarg that controls op seeds within the scope.", "Adding @ekelsen, who has thought alot about TF determinism.\r\n\r\n@eamartin I defnitely agree that TF should strive for \"easy reproducibility\".\r\n\r\nFor reference, I think the current logic for determining the implicit `op_seed` is based on the `id` of the last graph node:\r\nhttps://github.com/tensorflow/tensorflow/blob/a83154967bb2955acc234f4a64b63b505508b728/tensorflow/python/framework/random_seed.py#L64\r\n\r\nAnother way to think of this is that the current logic is based on the structure of the graph; if the user doesn't change the structure of the graph, the `op_seed` will remain the same.  Your suggestion is to base it on the tensor name instead.\r\n\r\nYou've given a great example of how a \"tiny change\" to the user's implementation of `mean` causes unexpected changes to random init, given the existing logic.  If instead we changed to your proposal based on tensor names, the same unexpected changes to random init would occur, if the user made a \"tiny change\" to the name of a tensor, but kept the structure of the graph the same.\r\n\r\nSo what this really boils down to is which approach we think is more intuitive.  My point is that neither approach is a clear winner.  I wouldn't want to add even more complexity (e.g. `tf.set_op_seed_default_algorithm`) just for this.", "I agree with @tatatodd . \r\nIn the example above, we create two structures by condition in fact. I believe that reproducibility is only guaranteed for one deterministic structure, say, results always are same for `toggle=True` or `toggle=False`.\r\n\r\nEven though we switch to use tensor name, we'll still meet the dilemma if only structure is changed:\r\n```python\r\nc0 = ...\r\n\r\nif toggle:\r\n   v1 = tf.Variable(...)\r\n   c1 = ...\r\nelse:\r\n   c2 = ...\r\n\r\nv = tf.Variable(...)  # undetermined\r\n```\r\n", "@facaiy I agree that changing the graph at all breaks the current conditions for reproducibility, but I'm suggesting that there should be a best-effort attempt to stay reproducible even with small graph changes. \r\n\r\nBeing able to directly reproduce a experiment is powerful, but I think it's even more powerful to hold all things constant and tweak just one aspect of an experiment. \r\n\r\n@tatatodd I agree that both names or op id based seeding are fragile to small changes. However, the user generally has more control over op names than op id based seeding. I think it''s significantly easier to control op names (by setting name kwarg) than it is control graph building order (restructure your entire program so all random ops are declared first). Variable name is a local property where op id is a global property.\r\n\r\nPersonally, I assign name to all of my variables by using ``variable_scope`` and ``get_variable``. These names generally don't change while I edit my experiments unless I do something like adding an extra variable scope or manually a name. I believe this variable naming scheme is TF best practice and practiced relatively widely, so name based seeding would get many users a significant amount of control over reproducibility for free.", "Yup, I agree that operator's name is more easily designed,  and sounds more robust than its id. Especially when combined with name scope, for example:\r\n```python\r\nwith tf.name_scope(\"fixed\"):\r\n  v = tf.Variable(\"unique\")  # persistent if only its name won't collide in the name scope.\r\n```\r\n\r\nHowever, I don't know how much users can benefit from the change, and whether the change is worthful. \r\nAnyway, thanks for clarify, @eamartin .", "@eamartin I agree that name-based op seeding is probably an overall better choice; as you point out, it's more commonly (and easily) controlled by the user.  But I'm unsure whether the benefit is worth the implementation cost - let's discuss the concrete points below.\r\n\r\nPing @ekelsen @alextp @asimshankar to get their input as well.\r\n\r\nAssuming for a moment that we wanted to make this change, there are two things that worry me.\r\n\r\n1. It might not be easy to implement.  Note that the `op_seed` is attached as an attribute to each GraphNode, for ops that need the `op_seed`.  Right now we determine the `op_seed` purely in python code.  But (I think) we assign unique values to node names in C++ code, during graph building:\r\nhttps://github.com/tensorflow/tensorflow/blob/ad1310a87caa14c495ad7ab47db7572443b2e7ef/tensorflow/core/graph/graph_constructor.cc#L201\r\nThere might be a chicken-and-egg sequencing problem here, where we don't know the unique node name until we've built the graph, but we need the node name to compute the `op_seed`.  I don't think there's a theoretical problem here that prevents us from making this work, but it might be hard in practice.\r\n\r\n2. Should we retain support for both name-based and order-based op seeding, and keep the default as order-based?  My preference is that our final state only supports name-based op seeding, and not both. \r\n Having both just seems like unnecessary complexity, and will probably lead to more confusion down the road.  However I'm not sure whether it's feasible to make this change.  Seems ok to me, but I'll let others weigh in with their thoughts.", "FWIW, we were bitten by this issue -- we wanted to make the learning rate of our optimizer a placeholder (to experiment with some more exotic learning rate decay schemes) and were very confused why changing the learning rate from a constant to a placeholder fed the same value learned a different network.", "I just get caught by this issue today. I vote for that small change to graph should not change initial values of variables.", "Sadly it's a tradeoff, and there's no good default. The glorot_uniform was\nchosen because for the common case of variables as kernels in dense or\nconvolutional layers it's a much better default than 0 (which will not\ntrain) or random_uniform (which will not train well on deeper networks).\n\nOn Wed, Jul 4, 2018 at 4:56 AM saluto <notifications@github.com> wrote:\n\n> (Sorry for my English.) I have another problem with random initializers,\n> but don't want to create a separate issue yet: Until now I assumed that\n> tf.get_variable() by default initializes with zeros (\n> tf.zeros_initializer()) and just now was really shocked that it seems to\n> use tf.glorot_uniform_initializer(), which is not documented in the\n> section for tf.get_variable(). That destroyed a couple of research days.\n> In my opinion there should be no assumption about how something basic like\n> a variable is used. But tf.glorot_uniform_initializer() seems to make\n> assumptions just based on the shape. Am I wrong on this? Why not use\n> tf.zeros_initializer() or maybe tf.random_uniform_initializer() by\n> default? It's extremely confusing. At least put it in the documentation.\n> Now I'm really intimidated by the whole tensorflow framework, because I'm\n> unsure which other assumptions are implemented. I would love to be proven\n> wrong.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/14675#issuecomment-402456286>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxbthNuVhtQ4NPcJOrN5Os9qUh2fBks5uDK19gaJpZM4QiwcH>\n> .\n>\n\n\n-- \n - Alex\n"]}]