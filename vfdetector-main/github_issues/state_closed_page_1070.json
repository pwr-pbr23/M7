[{"number": 21176, "title": "Windows gpu compute", "body": "", "comments": ["@gunan should we also add a requirement in setup.py?"]}, {"number": 21175, "title": "Unable to convert keras_model.h5 to .tflite", "body": "I enter this command : tflite_convert \\ --output_file=/tmp/foo.tflite \\ --keras_model_file=C:/Users/n.muthuraj/Desktop/cnn/02/dataset/ashik.h5\r\n\r\nThe Output is : Traceback (most recent call last):\r\n\r\n File \"c:\\users\\n.muthuraj\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper return importlib.import_module(mname) File \"c:\\users\\n.muthuraj\\appdata\\local\\programs\\python\\python35\\lib\\importlib_init_.py\", line 126, in import_module return _bootstrap._gcd_import(name[level:], package, level) File \"\", line 986, in _gcd_import File \"\", line 969, in _find_and_load File \"\", line 958, in _find_and_load_unlocked File \"\", line 666, in _load_unlocked File \"\", line 577, in module_from_spec File \"\", line 906, in create_module File \"\", line 222, in _call_with_frames_removed ImportError: DLL load failed: The specified module could not be found. During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"c:\\users\\n.muthuraj\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in from tensorflow.python.pywrap_tensorflow_internal import * File \"c:\\users\\n.muthuraj\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in _pywrap_tensorflow_internal = swig_import_helper() File \"c:\\users\\n.muthuraj\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper return importlib.import_module('pywrap_tensorflow_internal') File \"c:\\users\\n.muthuraj\\appdata\\local\\programs\\python\\python35\\lib\\importlib_init.py\", line 126, in import_module return _bootstrap.gcd_import(name[level:], package, level) ImportError: No module named 'pywrap_tensorflow_internal' During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"c:\\users\\n.muthuraj\\appdata\\local\\programs\\python\\python35\\lib\\runpy.py\", line 184, in run_module_as_main \"main\", mod_spec) File \"c:\\users\\n.muthuraj\\appdata\\local\\programs\\python\\python35\\lib\\runpy.py\", line 85, in run_code exec(code, run_globals) File \"C:\\Users\\n.muthuraj\\AppData\\Local\\Programs\\Python\\Python35\\Scripts\\tflite_convert.exe_main.py\", line 5, in File \"c:\\users\\n.muthuraj\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow_init.py\", line 22, in from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import File \"c:\\users\\n.muthuraj\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python_init.py\", line 49, in from tensorflow.python import pywrap_tensorflow File \"c:\\users\\n.muthuraj\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in raise ImportError(msg) ImportError: Traceback (most recent call last): File \"c:\\users\\n.muthuraj\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper return importlib.import_module(mname) File \"c:\\users\\n.muthuraj\\appdata\\local\\programs\\python\\python35\\lib\\importlib_init.py\", line 126, in import_module return _bootstrap._gcd_import(name[level:], package, level) File \"\", line 986, in _gcd_import File \"\", line 969, in _find_and_load File \"\", line 958, in _find_and_load_unlocked File \"\", line 666, in _load_unlocked File \"\", line 577, in module_from_spec File \"\", line 906, in create_module File \"\", line 222, in _call_with_frames_removed ImportError: DLL load failed: The specified module could not be found. During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"c:\\users\\n.muthuraj\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in from tensorflow.python.pywrap_tensorflow_internal import * File \"c:\\users\\n.muthuraj\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in _pywrap_tensorflow_internal = swig_import_helper() File \"c:\\users\\n.muthuraj\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper return importlib.import_module('pywrap_tensorflow_internal') File \"c:\\users\\n.muthuraj\\appdata\\local\\programs\\python\\python35\\lib\\importlib_init.py\", line 126, in import_module return _bootstrap._gcd_import(name[level:], package, level) ImportError: No module named '_pywrap_tensorflow_internal' Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/install_sources#common_installation_problems for some common reasons and solutions. Include the entire stack trace above this error message when asking for help.\r\n\r\nI am not sure where i am going wrong", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "OS Platform and Distribution- win 10 \r\nTensorFlow installed from -pip install & updated to \r\nTensorFlow version - 1.9.0\r\nBazel version -NA\r\nCUDA/cuDNN version-NA\r\nGPU model and memory - NA\r\nExact command to reproduce :python keras_to_tensorflow.py -input_model_file ashik.h5 -input_fld C:\\Users\\n.muthuraj\\Desktop\\cnn\\02\\dataset -output_node_prefix 'dense_6'\r\nMobile device- NA", "I was able to resolve it as i found out later that  toco is not supported on windows !!\r\nI had another issue while i tried converting the model on linux system. I ran theano in the backend which was accepting the image data in (number , channel , height , width)- NCHW was the input for the model and the core was dumped while conversion. On the other hand when i ran Tensorflow in backend (number , height,width, channel)-NHWC format the model was converted to tflite format .\r\nSO while conversion make sure that you are\r\n1) running tensorflow in backend by:\r\nfrom keras import backend as K\r\nK.set_image_dim_ordering('tf'),\r\n2) install tf_nightly\r\nHope it helps !! \r\n", "@nitheeshmuthuraj  Glad you got it to work. Thanks for reporting back solutions; we really appreciate help the community. Also, we're close to supporting windows, and we'll update this when it's fully working.", "Duplicate of #21085 ", "The toco conversion utility should now run on Windows. It hasn't been put through exhaustive testing, though, due to some limitations with our testing infrastructure, so feel free to file bugs for any issues you encounter."]}, {"number": 21174, "title": "Allow Keras Callbacks to access predictions on_batch_end, on_epoch_end", "body": "### System information\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.9.0-rc0-35-g17d6639b55 1.9.0-rc1\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: n/a\r\n\r\n### Describe the problem\r\n\r\nI wanted to call attention to an issue and a PR on the keras-team/keras project so that TF could track and integrate with TF-core Keras. I believe these issues are of high importance to the community; for my project at least they are just short of critical. In summary:\r\n\r\n* PR: Keras Callbacks should be able to access predictions made during training in `on_batch_end()` and `on_epoch_end()` to eliminate expensive duplicate calls to `predict()` in Callbacks that require prediction results -- https://github.com/keras-team/keras/pull/10513\r\n* Writing a Keras metric should not require a graph -- https://github.com/keras-team/keras/issues/4506#issuecomment-405452870\r\n\r\nThe reason these are related is because a viable solution to the second issue depends on the first; in particular, if you wanted to write a \"fancy\" metric function that did calcs in Python, for example, you could leverage already-made predictions from the API described in issue 1 and circumvent having to `predict()` all over again, which is prohibitively expensive in all but the simplest models.", "comments": ["@fchollet PTAL. ", "Accessing true and predict data on_epoch_end callback will probably not  be possible (tf is not storing them and size will be number_batches * batch_size * size_sample).\r\nEasy way to access the tf predicted and targets stored in model_targets and model.outputs should be considered. See also  #32981 ", "@doiko Preds are at least already available without extra overhead for on_batch_end, see linked line: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training_arrays.py#L304\r\n\r\nWhen I get a free few minutes I will submit a PR exposing these to on_batch_end; it is a trivial change now that I am looking at the code.\r\n\r\nI can see what you mean re: not exposing to on_epoch_end, which would require accumulation of outputs. Perhaps that could be an opt-in flag in Callback. I know at least for my use case, though predict itself requires tons of resources, the predictions themselves are small and an epoch's worth will make negligible impact on memory.", "Would be a cool function - also her predictions are a small overhead compared to the overhead of prediction time. ", "Just having the predictions and truth in the callback's on_batch_end as @evictor suggests would be a great start.  The callback can accumulate them if desired, and it wouldn't be hard for users to put this logic into a parent MetricCallback class if they desire.  The current solution of instantiating the callback with the training data and truth is much more awkward.\r\n\r\nAlso, @evictor you linked to a line number in Master, and since master has been updated since your post, I don't think that line is what you meant anymore.", "@evictor,\r\nCan you please confirm if [on_predict_batch_begin](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback#on_predict_batch_begin)  and [on_predict_batch_end](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback#on_predict_batch_end) are what you are looking for?\r\n\r\nThanks!", "It's been a while since I worked on that project, but yes this appears to solve the original problem we faced.", "@evictor,\r\nThank you for your confirmation. Closing the issue. "]}, {"number": 21173, "title": "tf.nn.top_k returns wrong indices", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes?\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.9.0 / 1.8.0 / 1.5.0\r\n- **Python version**: 2.7 / 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA 9.0 / cuDNN 7.0.5\r\n- **GPU model and memory**: GeForce GTX 1050Ti 4GB (Laptop)\r\n- **Exact command to reproduce**: [tf.nn.top_k](https://www.tensorflow.org/api_docs/python/tf/nn/top_k)\r\n\r\n### Describe the problem\r\nThe [tf.nn.top_k](https://www.tensorflow.org/api_docs/python/tf/nn/top_k) command return wrong ranked indices. \r\nE.g.\r\nif I run the following commands\r\n```\r\nimport tensorflow as tf\r\n\r\na = tf.constant([[5,6,4],[0.1,2,3]])\r\nv, r = tf.nn.top_k(a, 3)\r\n\r\nsess.run(a)\r\nsess.run(r)\r\nsess.run(v)\r\n```\r\nthe results are as expected \r\n\r\n```\r\n>>> sess.run(a)\r\narray([[5. , 6. , 4. ],\r\n       [0.1, 2. , 3. ]], dtype=float32)\r\n>>> sess.run(r)\r\narray([[1, 0, 2],\r\n       [2, 1, 0]], dtype=int32)\r\n>>> sess.run(v)\r\narray([[6. , 5. , 4. ],\r\n       [3. , 2. , 0.1]], dtype=float32)\r\n```\r\nbut if I try the following example\r\n```\r\nimport tensorflow as tf\r\n\r\na = tf.constant([[-5,0.8,0.4],[0.5,0.3,2.6]])\r\nv, r = tf.nn.top_k(a, 3)\r\n\r\nsess.run(a)\r\nsess.run(r)\r\nsess.run(v)\r\n```\r\nthe results of the indices tensor are totally wrong \r\n```\r\n>>> sess.run(a)\r\narray([[-5. ,  0.8,  0.4],\r\n       [ 0.5,  0.3,  2.6]], dtype=float32)\r\n>>> sess.run(r)\r\narray([[1, 2, 0],\r\n       [2, 0, 1]], dtype=int32)\r\n>>> sess.run(v)\r\narray([[ 0.8,  0.4, -5. ],\r\n       [ 2.6,  0.5,  0.3]], dtype=float32)\r\n```\r\nSame thing happens for higher rank tensors and more complex ordering of the input values.\r\n", "comments": ["It works correctly, my bad."]}, {"number": 21172, "title": "request wrapper tutorial", "body": "hi all,\r\n\r\nit would be really great to have a tutorial on how to use the many awesome wrappers like dropoutwrapper, attentionwrapper, etc. ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Yes. This is a good idea. \r\n\r\nThe RNN tutorials need an update. We should make this a part of that update.\r\n\r\n#18405 \r\n\r\nI'm going to close this as duplicate and, make this part of that bug."]}, {"number": 21171, "title": "Fix contrib in r1.10", "body": "", "comments": []}, {"number": 21170, "title": "Converting the model to TFLite format getting \u201cCheck failed: is_rnn_state_array \u201d", "body": "toco \\--input_file=$TRAINING_DIR/retrained_graph.pb \\\r\n--input_format=TENSORFLOW_GRAPHDEF \\\r\n--output_format=TFLITE \\\r\n--output_file=/$TRAINING_DIR/${ARCHITECTURE}.tflite \\\r\n--inference_type=QUANTIZED_UINT8 \\\r\n--input_arrays=input \\\r\n--output_arrays=final_result \\ \r\n--input_shapes=1,224,224,3 \\inference_input_type=QUANTIZED_UNIT8 \\\r\n--mean_values=128 \\\r\n--std_values=128 \\\r\n--default_ranges_min=0 \\\r\n--quantize_weights=true \\\r\n--default_ranges_max=6 ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "tensorflow - 1.7.1\r\nmacOS Sierra\r\nFollowing this instructions\r\nhttps://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2/#0", "please use the binary \"tflite_convert\" on the nightly build and see if you are still hitting this problem.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 21169, "title": "tf.Estimator can't load weights at runtime", "body": "This is not a StackOverflow question because it simply can't be done the way tf.Estimator is currently designed.  This is a design flaw.\r\n\r\nLanguage: Anaconda Python 3.6\r\nOS Platform and Distribution: Ubuntu 17.10\r\nTensorFlow version: 1.8\r\nCuda compilation tools, release 9.0, V9.0.176 /\r\nCUDNN 7.0.5\r\nGPU model and memory: NVIDIA Titan V, Driver Version: 387.34, 12057MiB\r\n\r\nI'm working on building a distributed TensorFlow model for semantic segmentation (pixel-based, not bounding-box-based) using the https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10_estimator example.  The code fragments below are based on that example. \r\n\r\nThis model is for production, not research so as part of the input pipeline, I have to extract training images from production images and calculate training class weights on the fly.  However, the current design of tf.Estimator appears to make this impossible.\r\n\r\nIf the class weights are uploaded into a static variable and then I attempt to collate them in _resnet_model_fn to use in the loss function I get errors telling me that the class weights are in the wrong class.\r\n\r\nRuntime class weights assembly:\r\n```\r\nclass MyDataSet(object):\r\n    _class_weights = None\r\n\r\n    def __init__(self, data_dir):\r\n        self.data_dir = data_dir\r\n        if MyDataSet._class_weights is None:\r\n            MyDataSet._class_weights = MyDataSet.map_weights(data_dir)\r\n\r\n    @staticmethod\r\n    def train_id_to_weight_map_func(label):\r\n        return MyDataSet._class_weights[label]\r\n\r\n    @staticmethod\r\n    def map_weights(data_dir):\r\n        weights = pickle.load(open(os.path.join(data_dir, 'weight.pkl'), 'rb'))\r\n        class_weights = [0] * weights.size\r\n        for i, w in enumerate(weights):\r\n            class_weights[i] = w\r\n        return tf.convert_to_tensor(class_weights, np.float32)\r\n\r\n```\r\n \r\ncyfar10_main.py:\r\n\r\n```\r\ndef _resnet_model_fn(features, labels, mode, params):\r\n    tower_labels = labels\r\n    tower_weight = []\r\n    for label in tower_labels:\r\n        tower_weight.append(tf.map_fn(lambda x: MyDataSet.train_id_to_weight_map_func(x), label, np.float32)\r\n```\r\nError:\r\n```\r\nValueError: Tensor(\"map/while/TensorArrayReadV3:0\", shape=(256, 512), dtype=int32) must be from the same graph as Tensor(\"Const:0\", shape=(25,), dtype=float32)\r\nprint(label.graph)\r\n<tensorflow.python.framework.ops.Graph object at 0x7f9a206630f0>\r\nprint(LinkNetDataSet._class_weights.graph)\r\n<tensorflow.python.framework.ops.Graph object at 0x7f9a2a8b1cf8>`\r\n```\r\n\r\nIf I next try to load the weights into input_fn to get them into the same graph, I am unable to do so because Estimator is only expecting two lists of Tensors from input_fn: features and labels.  And features is already taken up with the training image so there's no place to put the class weights.", "comments": []}, {"number": 21168, "title": "Avoid installing newest pip and wheel versions", "body": "https://bootstrap.pypa.io/get-pip.py  was updated (pip-18.0, setuptools-40.0.0 and wheel-0.31.1) on 22-Jul-2018", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Is there a bug that requires us to avoid installing the latest version?", "Nagging Assignee @caisq: It has been 46 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing, because there are changes coming to the dockerfiles, and was not able to verify if this is still an issue."]}, {"number": 21167, "title": "Rebuilding the docker devel container fails", "body": "### System information\r\n- **Have I written custom code**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7.4 (Tensorflow is build inside the devel docker container executed by parameterized_docker_build.sh, which is based on Ubuntu 16.04)\r\n- **Docker version**: 18.06.0-ce, build 0ffa825\r\n- **TensorFlow installed from**: Docker devel container build (i.e. source) executed by  by parameterized_docker_build.sh.\r\n- **TensorFlow version**: 1.9\r\n- **Python version**: 3.5 (inside executed docker container) \r\n- **Bazel version (if compiling from source)**: 0.15.0 (inside executed docker container  by parameterized_docker_build.sh) \r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0 (inside executed docker container by parameterized_docker_build.sh)\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nmkdir tensorflow && cd tensorflow\r\ngit clone https://github.com/tensorflow/tensorflow.git .\r\nexport TF_DOCKER_BUILD_IS_DEVEL=YES\r\nexport TF_DOCKER_BUILD_TYPE=CPU\r\nexport TF_DOCKER_BUILD_PYTHON_VERSION=PYTHON3\r\nexport TF_DOCKER_BUILD_DEVEL_BRANCH=r1.9\r\ntensorflow/tools/docker/parameterized_docker_build.sh\r\n```\r\n\r\n\r\n### Describe the problem\r\nThe rebuild of the devel docker container fails after the compilation of tensorflow, when trying to install the whl file\r\n\r\n### Source code / logs\r\n\r\n```\r\nInstalling collected packages: setuptools, termcolor, absl-py, gast, protobuf, grpcio, werkzeug, markdown, tensorboard, astor, tensorflow\r\n  Found existing installation: setuptools 40.0.0\r\n    Uninstalling setuptools-40.0.0:\r\n      Successfully uninstalled setuptools-40.0.0\r\n  Running setup.py install for termcolor: started\r\n    Running setup.py install for termcolor: finished with status 'error'\r\n    Complete output from command /usr/bin/python3 -u -c \"import setuptools, tokenize;__file__='/tmp/pip-install-lv_39b1o/termcolor/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /tmp/pip-record-gjq_zk43/install-record.txt --single-version-externally-managed --compile:\r\n    usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\r\n       or: -c --help [cmd1 cmd2 ...]\r\n       or: -c --help-commands\r\n       or: -c cmd --help\r\n\r\n    error: option --single-version-externally-managed not recognized\r\n\r\n    ----------------------------------------\r\nCommand \"/usr/bin/python3 -u -c \"import setuptools, tokenize;__file__='/tmp/pip-install-lv_39b1o/termcolor/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /tmp/pip-record-gjq_zk43/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /tmp/pip-install-lv_39b1o/termcolor/\r\nThe command '/bin/sh -c tensorflow/tools/ci_build/builds/configured CPU     bazel build -c opt --copt=-mavx --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"         tensorflow/tools/pip_package:build_pip_package &&     bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip3 &&     pip3 --no-cache-dir install --upgrade /tmp/pip3/tensorflow-*.whl &&     rm -rf /tmp/pip3 &&     rm -rf /root/.cache' returned a non-zero code: 1\r\n```\r\n\r\n### Probable cause\r\n\r\nThe dockerfile installs pip, setuptools and wheel with:\r\n\r\n```\r\nRUN curl -fSsL -O https://bootstrap.pypa.io/get-pip.py && \\\r\n    python get-pip.py && \\\r\n    rm get-pip.py\r\n```\r\nOn 22-Jul-2018 this package was updated, so the installed versions are now: `pip-18.0` , `setuptools-40.0.0` and `wheel-0.31.1` . Proved using the previous version, i.e.:\r\n\r\n```\r\nRUN curl -fSsL -O https://bootstrap.pypa.io/3.3/get-pip.py && \\\r\n    python get-pip.py && \\\r\n    rm get-pip.py\r\n```\r\n\r\nWhich installs `pip-10.0.1` , `setuptools-40.0.0` and `wheel-0.29.0`,  and the rebuild worked. The exact command to reproduce the working build was:\r\n\r\n```\r\nmkdir tensorflow && cd tensorflow\r\ngit clone https://github.com/tensorflow/tensorflow.git .\r\nsed -i 's/bootstrap.pypa.io\\/get-pip.py/bootstrap.pypa.io\\/3.3\\/get-pip.py/' tensorflow/tools/docker/Dockerfile.devel\r\nexport TF_DOCKER_BUILD_IS_DEVEL=YES\r\nexport TF_DOCKER_BUILD_TYPE=CPU\r\nexport TF_DOCKER_BUILD_PYTHON_VERSION=PYTHON3\r\nexport TF_DOCKER_BUILD_DEVEL_BRANCH=master\r\ntensorflow/tools/docker/parameterized_docker_build.sh\r\n```\r\n\r\n### Pull request: #21168\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nMobile device", "I updated the info and removed any related to GPU/CUDA. Please, note that this issue happens while running the `tensorflow/tools/docker/parameterized_docker_build.sh` script on a CentOS 7.4 host. The script creates a devel docker container that is used to build TensorFlow. That is, all the environment fields are related to the docker container configured by the script (i.e. Ubuntu:16.04, Python 3.5, Bazel 0.15.0 and GCC 5.4.0) and not the host OS where the script is executed.", "I've recently proposed a change to TensorFlow that obsoletes parameterized_docker_build.sh, which may help alleviate this issue. If anyone following this thread is interested in making TensorFlow's Dockerfile story better for everyone, [please take a look at the RFC](https://github.com/tensorflow/community/pull/8).", "@av8ramit you recently did an updpate to the pip and setuptools versions we are using. Could you take a look at this bug?", "@mpenagar is this still an issue? I've been unable to reproduce it with master. 1.9 docker builds worked with python3.5", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 21166, "title": "Numpy with Tensorflow Eager as backend?", "body": "It would be really cool if the eager execution would not be exclusive in one session.\r\n\r\nIf we could use default and eager execution in parallel, it would be possible to use Tensorflow Eager Execution as backend to numpy.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: None\r\n- **TensorFlow installed from (source or binary)**: None\r\n- **TensorFlow version (use command below)**: None\r\n- **Python version**: 3.6.6\r\n- **Bazel version (if compiling from source)**: None\r\n- **GCC/Compiler version (if compiling from source)**: None\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**: None\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nMobile device", "It has been 35 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 21165, "title": "[BUG] About your Android App, problem with fragment lifecycle.", "body": "", "comments": []}, {"number": 21164, "title": "Default floating point values in single_image_random_dot_stereograms_ops.cc cause syntax error", "body": "Have I written custom code: yes\r\nOS Platform and Distribution: Ubuntu 18.04\r\nTensorFlow installed from: origin master\r\nTensorFlow version: 1.9\r\nBazel version 0.15.2\r\nCUDA/cuDNN version: 9.1/7.1\r\nGPU model and memory:  NVIDIA-SMI 390.48\r\nExact command to reproduce: use code below in python3 script \r\nMobile device: N/A\r\n     \r\n     import tensorflow as tf\r\n      vol = tf.contrib.util.make_tensor_proto([256, 256])\r\n\r\nImport of /localhome/local/projects/lme_custom_ops/tensorflow/contrib/image/__init__.py causes syntax error on my system (see issue https://github.com/tensorflow/serving/issues/421).\r\n\r\n    Traceback (most recent call last):\r\n    File \"main.py\", line 6, in <module>\r\n        import geometry as geometry\r\n    File \"/localhome/local/projects/deep-iterative-reco/geometry.py\", line 20, in <module>\r\n        volume_origin = tf.contrib.util.make_tensor_proto([-((_volume_xy-1)/2 * _volume_spacing),-((_volume_xy-1)/2 * _volume_spacing)], tf.float32 )\r\n    File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/lazy_loader.py\", line 53, in __getattr__\r\n        module = self._load()\r\n    File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/lazy_loader.py\", line 42, in _load\r\n        module = importlib.import_module(self.__name__)\r\n    File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\r\n        return _bootstrap._gcd_import(name[level:], package, level)\r\n    File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/__init__.py\", line 48, in <module>\r\n        from tensorflow.contrib import image\r\n    File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/image/__init__.py\", line 70, in <module>\r\n        from tensorflow.contrib.image.python.ops.single_image_random_dot_stereograms import single_image_random_dot_stereograms\r\n    File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/image/python/ops/single_image_random_dot_stereograms.py\", line 27, in <module>\r\n        \"_single_image_random_dot_stereograms.so\"))\r\n    File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/util/loader.py\", line 56, in load_op_library\r\n        ret = load_library.load_op_library(path)\r\n    File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/load_library.py\", line 73, in load_op_library\r\n        exec(wrappers, module.__dict__)\r\n    File \"<string>\", line 28\r\n        def single_image_random_dot_stereograms(depth_values, hidden_surface_removal=True, convergence_dots_size=8, dots_per_inch=72, eye_separation=3, mu=0,333299994, normalize=True, normalize_max=-100, normalize_min=100, border_level=0, number_colors=256, output_image_shape=[1024, 768, 1], output_data_window=[1022, 757], name=None):\r\n                                                                                                                                                                    ^\r\n    SyntaxError: invalid syntax\r\n\r\nWorks again if I change the default kwargs float arguments to integer values. Reason maybe localization option (I am from Germany which uses , to indicate decimal point. E.g. 1.2 == 1,2 in Germany). Though I think I am using the US standard.\r\n\r\nUsing this code in /tensorflow/contrib/image/ops/single_image_random_dot_stereograms_ops.cc solves the problem (not using floating values).\r\n\r\n    REGISTER_OP( \"SingleImageRandomDotStereograms\" )\r\n        .Attr( \"T: {double,float,int64,int32}\" )\r\n        .Input( \"depth_values: T\" )\r\n        .Output( \"image: uint8\" )\r\n        .Attr( \"hidden_surface_removal: bool = true\" )\r\n        .Attr( \"convergence_dots_size: int = 8\" )\r\n        .Attr( \"dots_per_inch: int = 72\" )\r\n        .Attr( \"eye_separation: float = 3\" )\r\n        .Attr( \"mu: float = 3333\" )\r\n        .Attr( \"normalize: bool = true\" )\r\n        .Attr( \"normalize_max: float = -100.0\" )\r\n        .Attr( \"normalize_min: float = 100.0\" )\r\n        .Attr( \"border_level: float = 0.0\" )\r\n        .Attr( \"number_colors: int = 256\" )\r\n\r\n\r\nOn python2 executing the following...  \r\n\r\n      Python 2.7.15rc1 (default, Apr 15 2018, 21:51:34) \r\n      [GCC 7.3.0] on linux2\r\n      Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n      >>> import locale\r\n      >>> locale.setlocale(locale.LC_ALL, '')\r\n            'LC_CTYPE=en_US.UTF-8;LC_NUMERIC=de_DE.UTF-8;LC_TIME=de_DE.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=de_DE.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=de_DE.UTF-8;LC_NAME=de_DE.UTF-8;LC_ADDRESS=de_DE.UTF-8;LC_TELEPHONE=de_DE.UTF-8;LC_MEASUREMENT=de_DE.UTF-8;LC_IDENTIFICATION=de_DE.UTF-8'\r\n\r\nThough I compiled for python3:\r\n\r\n    Python 3.6.5 (default, Apr  1 2018, 05:46:30) \r\n    Type \"copyright\", \"credits\" or \"license\" for more information.\r\n\r\n    IPython 5.5.0 -- An enhanced Interactive Python.\r\n    ?         -> Introduction and overview of IPython's features.\r\n    %quickref -> Quick reference.\r\n    help      -> Python's own help system.\r\n    object?   -> Details about 'object', use 'object??' for extra details.\r\n\r\n    In [1]: \r\n    ...: \r\n    ...: import locale\r\n\r\n    In [2]: locale.localeconv()\r\n    Out[2]: \r\n    {'currency_symbol': '',\r\n    'decimal_point': '.',\r\n    'frac_digits': 127,\r\n    'grouping': [],\r\n    'int_curr_symbol': '',\r\n    'int_frac_digits': 127,\r\n    'mon_decimal_point': '',\r\n    'mon_grouping': [],\r\n    'mon_thousands_sep': '',\r\n    'n_cs_precedes': 127,\r\n    'n_sep_by_space': 127,\r\n    'n_sign_posn': 127,\r\n    'negative_sign': '',\r\n    'p_cs_precedes': 127,\r\n    'p_sep_by_space': 127,\r\n    'p_sign_posn': 127,\r\n    'positive_sign': '',\r\n    'thousands_sep': ''}\r\n\r\nI am using tensorflow revision 6d71d3fc659b317a38586f71ae94410ad3261f55 on Ubuntu 18.08. cuDNN 7.1, CUDA 9.1\r\n\r\nThis seems to be related: https://github.com/tensorflow/tensorflow/issues/2974", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "I'm gonna mark this as contributions welcome in case anyone wants to submit a PR addressing this.\r\n\r\n@gunan do you know anything about localization or who would?", "No idea, as we all use US settings, I think this is best handled by the community, who has a high chance of reproducing the problem.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Seems to be also related to #10370\r\n\r\nAnyway, I only have this problem with tensorflow versions >= 1.9  (1.9, 1.10)\r\nI never had troubles with tensorflow from v1.0 - v1.8\r\nSo the following code sinppet, that reproduces the error, works in my old tensorflow environments but fails in the new one.\r\n\r\n``` python\r\nimport tensorflow as tf\r\nimport locale\r\nlocale.setlocale(locale.LC_ALL, locale=\"de_AT.UTF-8\") \r\ndataset = tf.data.Dataset.from_tensor_slices([ [\"a\",\"b\",\"c\",\"e\"]])\r\ndataset = dataset.apply(tf.contrib.data.assert_element_shape( tf.TensorShape(4) ) )\r\n```\r\n\r\nIt yields the same error as above (in tf >=1.9) when tf tries to import single_image_random_dot_stereograms and does so with the wrong floating point conversion style.\r\n\r\nWhen I ran the debugger in tf 1.10 to the [load_library.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/load_library.py) file, it generates the wrong floating point style in [line 61](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/load_library.py#L61-L73) when it calls `wrappers = py_tf.GetPythonWrappers(op_list_str)` which calls this [C++ function.](https://github.com/tensorflow/tensorflow/blob/c4ccab7a95835ec12af16938ba0c17cc7c1f17e0/tensorflow/python/framework/python_op_gen.cc#L1029)\r\nSetting the locale setting of LC_NUMERIC to US style before this function call and back to the original style fixes this error for me.\r\n\r\n``` python\r\n  import locale\r\n  bak = locale.getlocale(locale.LC_NUMERIC)\r\n  locale.setlocale(locale.LC_NUMERIC,\"en_US.UTF-8\")\r\n  wrappers = py_tf.GetPythonWrappers(op_list_str)\r\n  locale.setlocale(locale.LC_NUMERIC,bak)\r\n```\r\nHowever, I don't have experience with i18n, so I don't know about side effects... (I guess US style needs to be installed?). \r\n\r\nAlternatively, until a true fix is implemented, a clear warning / error message would already help to figure out that the error comes from the locale settings, so that users can react.\r\n``` python\r\nimport locale\r\nassert locale.localeconv()[\"decimal_point\"] == \".\", \"Tensorflows calling of C++ libraries currently requires that locale Internationalization and localization (i18n) settings like the 'decimal_point' are set to US standards\"\r\nwrappers = py_tf.GetPythonWrappers(op_list_str)\r\n```", "Added a PR #22044 for the fix.", "After this solution, I have still the same issue in the last version of tensorflow (1.11) when I try to debug my program (It's fine in runtime). Why does that happen?\r\nThe last working version is still 1.7.1.", "@grzegorz700 the PR was only merged recently and may not be part of 1.11. Can you try with the nightly build if possible?", "I'm using TensorFlow 1.12.0 and still facing the same error. Any suggestions please?\r\n\r\n\r\n` File \"/home/deepakvellampalli/carla_9_3_rllib/models.py\", line 7, in <module>\r\n    import tensorflow.contrib.slim as slim \r\n\r\n  File \"/home/deepakvellampalli/.local/lib/python3.5/site-packages/tensorflow/contrib/__init__.py\", line 57, in <module>\r\n    from tensorflow.contrib import image\r\n   \r\nFile \"/home/deepakvellampalli/.local/lib/python3.5/site-packages/tensorflow/contrib/image/__init__.py\", line 70, in <module>\r\n    from tensorflow.contrib.image.python.ops.single_image_random_dot_stereograms import single_image_random_dot_stereograms\r\n   \r\nFile \"/home/deepakvellampalli/.local/lib/python3.5/site-packages/tensorflow/contrib/image/python/ops/single_image_random_dot_stereograms.py\", line 27, in <module>\r\n    \"_single_image_random_dot_stereograms.so\"))\r\n   \r\nFile \"/home/deepakvellampalli/.local/lib/python3.5/site-packages/tensorflow/contrib/util/loader.py\", line 56, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n   \r\nFile \"/home/deepakvellampalli/.local/lib/python3.5/site-packages/tensorflow/python/framework/load_library.py\", line 77, in load_op_library\r\n    exec(wrappers, module.__dict__)\r\n   \r\nFile \"<string>\", line 28\r\n    def single_image_random_dot_stereograms(depth_values, hidden_surface_removal=True, convergence_dots_size=8, dots_per_inch=72, eye_separation=, mu=, normalize=True, normalize_max=, normalize_min=, border_level=, number_colors=256, output_image_shape=[1024, 768, 1], output_data_window=[1022, 757], name=None):\r\n                                                                                                                                                 ^\r\nSyntaxError: invalid syntax `\r\n\r\n\r\nThanks in advance!", "Same here!", "did someone solve this?", "It happens when you also have an incompatible import statement.", "@varunjammula @medard0 the PR for the fix should have been in the latest version of TensorFlow. If the issue persist (or different but similar issue), can you open a new issue with the exact steps to reproduce?", "> It happens when you also have an incompatible import statement.\r\n\r\nHi @varunjammula \r\n\r\nCould elaborate a little bit more what do you mean by 'incompatible import statement' ? I have similar issue now and I would like to understand it more.\r\n\r\nIn new posted example it is a little bit different SyntaxError. We have empty assignments there (`normalize_max=, normalize_min=, border_level=,`) instead of illegal commas.", "What worked for me:\r\n\r\nInstead of the call\r\n```\r\nimport tensorflow.contrib.slim as slim\r\n```\r\nuse\r\n```\r\nbak = locale.getlocale(locale.LC_NUMERIC)\r\nlocale.setlocale(locale.LC_NUMERIC,\"en_US.UTF-8\")\r\nimport tensorflow.contrib.slim as slim\r\nlocale.setlocale(locale.LC_NUMERIC,bak)\r\n```\r\nfollowing the rationale set in comment https://github.com/tensorflow/tensorflow/issues/21164#issuecomment-417274023"]}, {"number": 21163, "title": "FAILED: Build did NOT complete successfully (101 packages loaded)", "body": "lyq@lyq-virtual-machine:~/yqli/tensorflow$ sudo bazel build tensorflow/tools/quantization:quantize_graph\r\nERROR: /home/lyq/yqli/tensorflow/tensorflow/BUILD:449:1: no such package '@grpc//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/grpc/grpc/archive/v1.13.0.tar.gz, https://github.com/grpc/grpc/archive/v1.13.0.tar.gz] to /home/lyq/.cache/bazel/_bazel_root/aa56a649b479b5a19f4ce65a6e36da7d/external/grpc/v1.13.0.tar.gz: Checksum was 8ed4a3a6eeed153cc54921c8f3aebb9a5fe2a9d9923b5bbbce7e2989c42f2039 but wanted 50db9cf2221354485eb7c3bd55a4c27190caef7048a2a1a15fbe60a498f98b44 and referenced by '//tensorflow:grpc++'\r\nERROR: Analysis of target '//tensorflow/tools/quantization:quantize_graph' failed; build aborted: Analysis failed\r\nINFO: Elapsed time: 775.107s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "This is my negligence, I am sorry for that\r\nOS :  ubauntu 16.04 64bit\r\nTensorFlow installed from :  pip install tensorflow\r\nTensorFlow version :  1.9.0\r\nNO CUDA/cuDNN\r\nuse only CPU\r\n\r\nmy device is rk3288\r\n", "Looks like your connection was interrupted when downloading one of tensorflow's dependencies:\r\n```\r\nError downloading [https://mirror.bazel.build/github.com/grpc/grpc/archive/v1.13.0.tar.gz, https://github.com/grpc/grpc/archive/v1.13.0.tar.gz] to /home/lyq/.cache/bazel/_bazel_root/aa56a649b479b5a19f4ce65a6e36da7d/external/grpc/v1.13.0.tar.gz: Checksum was 8ed4a3a6eeed153cc54921c8f3aebb9a5fe2a9d9923b5bbbce7e2989c42f2039 but wanted 50db9cf2221354485eb7c3bd55a4c27190caef7048a2a1a15fbe60a498f98b44 and referenced by '//tensorflow:grpc++'\r\n```\r\nDid you retry? Can you access the URLs listed?", "Yes, I have tried many times,  but all failed,  maybe it's a network problem,  I am in China now...\r\n\r\n\r\n", "Then it is indeed a network problem. mirror.bazel.build is hosted on GCS, which I think is blocked in china.\r\ngithub should be accessible, but maybe sometimes it is not.", "Closing since this is caused by a network issue"]}, {"number": 21162, "title": "Eager execution support in tf.keras fit_generator breaks Keras callbacks", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: RHEL 7.5 ppc64le but would occur on others\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: Anaconda3, Python 3.6.4\r\n- **Bazel version (if compiling from source)**:  N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.2\r\n- **GPU model and memory**: NVIDIA Volta V100\r\n- **Exact command to reproduce**: complicated, see below\r\n\r\n### Describe the problem\r\nWhen using tf.keras fit_generator with Keras callbacks the backward phase of the model is not present when the set_model, set_params, and on_train_begin callback methods are called.  This breaks any callback what needs the gradients, ops, etc present in the model before training begins.\r\n\r\nThis change was injected by this commit for eager execution support:\r\nhttps://github.com/tensorflow/tensorflow/commit/1b67ccbe8006eacffd268553abd01310e8b187d6#diff-abfd39a5a5f655f8d92c482e91081f8c\r\n\r\nThe _make_train_function that was removed calls the optimizer's get_updates() method which adds all the operations under the training/<optimizer class name>/ scope. \r\n\r\nOnce possible solution to this may be to put the calls back in but guard them with `if context.executing_eagerly()` as is done here:\r\nhttps://github.com/tensorflow/tensorflow/blob/939237af1479cf61a7c16ad4a1ead521da65a3a6/tensorflow/python/keras/engine/training.py#L1640-L1650\r\n\r\n### Source code / logs\r\nAn example of a Keras callback that depends on the full model being present when the set_model function is called is this model graph analysis / rewriting callback:\r\nhttps://github.com/tensorflow/tensorflow/pull/19845/files#diff-ae5805990d1ab77911507c9ae0b1cda2", "comments": ["@robieta, I didn't know when I'd get a chance to write and code a test for this so I didn't self assign.  I've since had a chance to test out the patch with both my callback and the original eager example that was the cause for the removal.  I've put up PR:\r\nhttps://github.com/tensorflow/tensorflow/pull/21244\r\n\r\nI can't seem to find a button to re-assign this issue to myself.", "Nagging Assignee @robieta: It has been 76 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This issue was fixed under #21244"]}, {"number": 21161, "title": "Merge pull request #1 from tensorflow/master", "body": "update from origin", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Hi @Zhangyantsing looks like this PR was created unintentionally. Closing for now. Feel free to re-open if otherwise. Thank you!"]}, {"number": 21160, "title": " W T:\\src\\github\\tensorflow\\tensorflow\\core\\framework\\op_kernel.cc:1318] OP_REQUIRES failed at save_restore_tensor.cc:170 ", "body": "System information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No. I am trying to run the Speech_Commands example\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version (use command below): 1.9.0\r\nPython version: 3.5.4\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\nExact command to reproduce:\r\nDescribe the problem\r\nThe training of 18000 steps is over successfully and I have got both the confusion matrix and the accuracy. I am trying to freeze the model by running freeze.py. I am getting errors.\r\n\r\nSource code / logs\r\nThe command I typed is as follows: as given in the tutorial.\r\nC:\\>python C:\\Users\\EDDY\\tensorflow\\tensorflow\\examples\\speech_commands\\freeze.py\r\nThe output I got is as follows:\r\n I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n W T:\\src\\github\\tensorflow\\tensorflow\\core\\framework\\op_kernel.cc:1318] OP_REQUIRES failed at save_restore_tensor.cc:170 : Invalid argument: Unsuccessful TensorSliceReader constructor: Failed to get matching files on : Unknown: FindFirstFile failed for: ./Documents and Settings : \u5b58\u53d6\u88ab\u62d2\u3002\r\n; Input/output error", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nMobile device", "Hi @da-iyoulin -- can you provide a link to the code you are trying to run? What precisely is the command you are using to run the freeze script?", "Link\uff1ahttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/speech_commands/freeze.py", "Based on the error above, it looks like a required file is not being found. What is the exact command you are using to run the script, and are the input files correctly passed?", "Nagging Assignee @karmel: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 21159, "title": "The name 'import/Mul' refers to an Operation not in the graph", "body": "#12736  This did not work for me is there any other solution for this ?\r\nI changed my code as following\r\n  ```\r\ninput_height = 299\r\n  input_width = 299\r\n  input_mean = 0\r\n  input_std = 299\r\n  input_layer = \"Mul\"\r\n  output_layer = \"final_result\"\r\n\r\n  input_name = \"import/\" + input_layer\r\n  output_name = \"import/\" + output_layer\r\n  input_operation = graph.get_operation_by_name(input_name)\r\n  output_operation = graph.get_operation_by_name(output_name)\r\n\r\n```\r\nPlease suggest me a solution\r\nThank you", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "This does not appear to be a bug in TensorFlow, and so might be more appropriate for Stack Overflow. However, if there is no node in the graph called `\"import/Mul\"`  and we don't know what the graph is or how it was produced, there's no chance that anyone will be able to suggest an answer. You might try printing the list of operations using `graph.get_operations()` and attempting to locate an appropriate-sounding node."]}, {"number": 21158, "title": "'ConfigProto' object has no attribute 'name'", "body": "Dear guys!!\r\n\r\nI have encountered an issue when I train the neural network. It shows 'ConfigProto' object has no attribute 'name' in the terminal and I have no idea what's going on. Does anyone face similar problem and how to solve it. Thanks a lot. \r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 35 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 21157, "title": "DecodeError: truncated message", "body": "retrain.py ran successfully and saved_model.pb and output_labels.txt are been created inside the \"tmp\" folder. When I try to use it inside label_image.py I get the following error\r\n\r\n```\r\n  File \"C:\\Users\\Srikanth1.R\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\google\\protobuf\\internal\\decoder.py\", line 814, in _SkipFixed32\r\n    raise _DecodeError('Truncated message.')\r\n\r\nDecodeError: Truncated message.\r\n```\r\n\r\nHas anyone else faced this issue ? How to solve this ?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 35 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 21156, "title": "Feature Idea: Checkpoint API \"split\" assert_consumed", "body": "I would like to see a function that is called something like `not_consumed()` for the checkpoint API here (https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restore) that returns all objects in the dependency graph that were not found in the checkpoint but without raising a python assertion like `assert_consumed` does.\r\n\r\nI would use it because sometimes I want to make sure that I restored certain variables but about others I do not care if they are restored or not.\r\n\r\nIn general I believe this would be a minor change to the code (e.g. splitting `assert_consumed` into two) that would greatly help to write tests to validate that variables are indeed restored and probably also be helpful in other scenarios as well.\r\n\r\nWhat is your take on this @allenlavoie ?\r\n", "comments": ["Seems reasonable to me. The assertion as-is is quite strict.\r\n\r\nIf it's part of the public API, I'd be worried about returning random proto nonsense for things in the checkpoint that didn't get loaded. But maybe something like `variables_not_restored` for Python variable objects which don't have a checkpointed value? It's still slightly odd, since the collection will grow over time as graph building (or eager execution) progresses and variables get created. But seems useful enough.", "Yes, something along these lines. If possible I think that returning something like a tuple `variables_not_restored=(variables_not_in_checkpoint_file, variables_in_checkpoint_file_but_not_in_current_dependency_graph)`... a little verbose here :D would be great", "Slightly off topic, but still related to checkpointing. Is there a way to configure the underlying `tf.train.Saver` when using `tf.train.Checkpoint`?", "Not at the moment. I have changes pending to configure garbage collection of checkpoints (will probably be a separate object), and for other ways of numbering checkpoints. Did you have something else in mind?", "Currently I would just like to set the `max_to_keep` setting of `tf.train.Saver` to a different value.", "@sleighsoft max_to_keep should now be addressed with https://github.com/tensorflow/tensorflow/commit/7a81491366afb444efb914b93594a11fdfc19896\r\n\r\nUsage example from the docstring:\r\n```\r\nimport tensorflow as tf\r\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\r\nmanager = tf.contrib.checkpoint.CheckpointManager(\r\n    checkpoint, directory=\"/tmp/model\", max_to_keep=5)\r\nstatus = checkpoint.restore(manager.latest_checkpoint)\r\nwhile True:\r\n  # train\r\n  manager.save()\r\n ```", "The CheckpointManager looks great :)\r\nExactly what I imagined!", "@sleighsoft,\r\nCan you please confirm if this issue is resolved and if we can close it? Thanks! ", "I guess @allenlavoie is the better person to ask here. I cannot answer as I haven't had much exposure recently ", "Closing the issue as the **`API`**, [CheckPointManager](https://www.tensorflow.org/api_docs/python/tf/train/CheckpointManager) addresses the issue that has been raised. Thanks!"]}, {"number": 21155, "title": "Add command line option to specify the version of the tensorflow wheel", "body": "There is a command line option to build_pip_package.sh (with support in setup.py) for setting the name of the pip package which is created.  This is so that Tensorflow can be called things like `tensorflow_gpu`. \r\n\r\nThis adds the same scaffolding for altering the version number of the PIP wheel file.\r\n\r\nWhile it seems less useful, possibly even confusing, to alter the version number, this is so that a 3rd party (Graphcore) can release versions of Tensorflow which are targeted at their hardware.  It would be difficult to manage that release process without being able to change the versioning of the releases independently of the versions of Tensorflow.\r\n\r\nThe version reported by Tensorflow in python (tensorflow.__version__) is still the tensorflow release.  This only affects the versioning of the pip package, just like the package name does.\r\n\r\n", "comments": ["@DavidNorman This is really old, and I just noticed it. Is this still useful?", "> @DavidNorman This is really old, and I just noticed it. Is this still useful?\r\n\r\n@DavidNorman  Could you please respond to the above question. Also request you to close this if it is not still useful.", "closing this PR due to lack of activity, thank you "]}, {"number": 21154, "title": "tf.Print() print wrong value", "body": "environment\r\nubuntu 16.04\r\ntf-gpu 1.8\r\n\r\n`tf_print = tf.Print(labels_0_new, [labels_0_new])`\r\n\r\nThe shape of labels_0_new is [16, 64]\r\nBut the output of tf_print is [[0 1 0]...]. The shape is [16?, 3]\r\n\r\nWhat's the problem?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "https://www.tensorflow.org/api_docs/python/tf/Print\r\n\r\n> summarize: Only print this many entries of each tensor. If None, then a maximum of 3 elements are printed per input tensor."]}, {"number": 21153, "title": "tf.contrib.image.sparse_image_warp", "body": "\r\nSystem:\r\n\r\n== cat /etc/issue ===============================================\r\nLinux george-OMEN-by-HP-Laptop 4.10.0-38-generic #42~16.04.1-Ubuntu SMP Tue Oct 10 16:32:20 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"18.3 (Sylvia)\"\r\nVERSION_ID=\"18.3\"\r\nVERSION_CODENAME=sylvia\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux george-OMEN-by-HP-Laptop 4.10.0-38-generic #42~16.04.1-Ubuntu SMP Tue Oct 10 16:32:20 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy                              1.14.5            \r\nnumpydoc                           0.7.0             \r\nprotobuf                           3.6.0             \r\ntensorflow-gpu                     1.9.0             \r\ntensorflow-lattice                 0.9.6             \r\ntensorflow-probability-gpu         0.2.0             \r\ntensorflow-tensorboard             1.5.1             \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.9.0\r\ntf.GIT_VERSION = v1.9.0-0-g25c197e023\r\ntf.COMPILER_VERSION = v1.9.0-0-g25c197e023\r\nSanity check: array([1], dtype=int32)\r\n/home/george/anaconda3/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda-9.1/lib64:/usr/local/cuda/lib64:/usr/local/cuda-9.1/lib64:/usr/local/cuda/extras/CUPTI/lib64/:\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nThu Jul 26 11:04:57 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 387.26                 Driver Version: 387.26                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1050    Off  | 00000000:01:00.0 Off |                  N/A |\r\n| N/A   54C    P0    N/A /  N/A |    355MiB /  4038MiB |      2%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1451      G   /usr/lib/xorg/Xorg                           187MiB |\r\n|    0      2194      G   cinnamon                                      79MiB |\r\n|    0      2864      G   ...-token=685C4795125F550AF469D7ED70DAE1ED    81MiB |\r\n|    0      3149      G   /usr/lib/firefox/firefox                       3MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.1/targets/x86_64-linux/lib/libcudart.so.9.1.85\r\n/usr/local/cuda-9.1/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-9.1/lib64/libcudart.so.9.1.85\r\n/usr/local/cuda-9.1/lib64/libcudart_static.a\r\n/usr/local/cuda-9.1/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-9.1/doc/man/man7/libcudart.7\r\n/usr/local/cuda-9.0/lib64/libcudart.so.9.0.176\r\n/usr/local/cuda-9.0/lib64/libcudart_static.a\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.7\r\n\r\n\r\nVERSION: v1.9.0-0-g25c197e023 1.9.0\r\n\r\nCode:\r\ndef warp(image, R0, C0, R, C):\r\n    A = tf.reshape(tf.stack([R0, C0], axis=-1), (1, -1, 2))\r\n    B = tf.reshape(tf.stack([R, C], axis=-1), (1, -1, 2))\r\n    image = tf.expand_dims(tf.cast(image, tf.float32), axis=0)\r\n    print(image, A, B)\r\n    image = tf.contrib.image.sparse_image_warp(image, A, B)[0]\r\n    print(image)\r\n    image = tf.cast(image, tf.uint8)\r\n    return image\r\nError:\r\n\r\nTraceback (most recent call last):\r\n  File \"rotate.py\", line 167, in <module>\r\n    test_warp()\r\n  File \"rotate.py\", line 154, in test_warp\r\n    new_image = sess.run(warp(image, R0, C0, R, C))\r\n  File \"rotate.py\", line 70, in warp\r\n    image = tf.contrib.image.sparse_image_warp(image, A, B)[0]\r\n  File \"/home/george/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/util/lazy_loader.py\", line 53, in __getattr__\r\n    module = self._load()\r\n  File \"/home/george/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/util/lazy_loader.py\", line 42, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/home/george/anaconda3/envs/py36/lib/python3.6/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/home/george/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/__init__.py\", line 47, in <module>\r\n    from tensorflow.contrib import image\r\n  File \"/home/george/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/image/__init__.py\", line 70, in <module>\r\n    from tensorflow.contrib.image.python.ops.single_image_random_dot_stereograms import single_image_random_dot_stereograms\r\n  File \"/home/george/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/image/python/ops/single_image_random_dot_stereograms.py\", line 27, in <module>\r\n    \"_single_image_random_dot_stereograms.so\"))\r\n  File \"/home/george/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/util/loader.py\", line 56, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"/home/george/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py\", line 73, in load_op_library\r\n    exec(wrappers, module.__dict__)\r\n  File \"<string>\", line 27\r\n    def single_image_random_dot_stereograms(depth_values, hidden_surface_removal=True, convergence_dots_size=8, dots_per_inch=72, eye_separation=2,5, mu=0,333299994, normalize=True, normalize_max=-100, normalize_min=100, border_level=0, number_colors=256, output_image_shape=[1024, 768, 1], output_data_window=[1022, 757], name=None):\r\n                                                                                                                                                   ^\r\nSyntaxError: invalid syntax\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Can I send pull request for this issue?", "Same issue here during\r\n```python\r\nds_finite = tf.data.Dataset.from_generator(\r\n                python_iterator, output_types=output_types, output_shapes=output_shapes)\r\nds_finite = ds_finite.apply(tf.contrib.data.batch_and_drop_remainder(self._batch_size))\r\n```\r\n\r\nUbuntu v18.04.1 LTS\r\nCPython v3.6.6\r\nNumpy v1.15.0\r\nTensorflow v1.11.0\r\n\r\n```\r\n[...]\r\nds_finite = ds_finite.apply(tf.contrib.data.batch_and_drop_remainder(self._batch_size))\r\nFile \"/home/usr/miniconda3/envs/myenv/lib/python3.6/site-packages/tensorflow/python/util/lazy_loader.py\", line 53, in __getattr__\r\nmodule = self._load()\r\nFile \"/home/usr/miniconda3/envs/myenv/lib/python3.6/site-packages/tensorflow/python/util/lazy_loader.py\", line 42, in _load\r\nmodule = importlib.import_module(self.__name__)\r\nFile \"/home/usr/miniconda3/envs/myenv/lib/python3.6/importlib/__init__.py\", line 126, in import_module\r\nreturn _bootstrap._gcd_import(name[level:], package, level)\r\nFile \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\nFile \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\nFile \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\nFile \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\r\nFile \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\r\nFile \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nFile \"/home/usr/miniconda3/envs/myenv/lib/python3.6/site-packages/tensorflow/contrib/__init__.py\", line 49, in <module>\r\nfrom tensorflow.contrib import image\r\nFile \"/home/usr/miniconda3/envs/myenv/lib/python3.6/site-packages/tensorflow/contrib/image/__init__.py\", line 70, in <module>\r\nfrom tensorflow.contrib.image.python.ops.single_image_random_dot_stereograms import single_image_random_dot_stereograms\r\nFile \"/home/usr/miniconda3/envs/myenv/lib/python3.6/site-packages/tensorflow/contrib/image/python/ops/single_image_random_dot_stereograms.py\", line 27, in <module>\r\n\"_single_image_random_dot_stereograms.so\"))\r\nFile \"/home/usr/miniconda3/envs/myenv/lib/python3.6/site-packages/tensorflow/contrib/util/loader.py\", line 56, in load_op_library\r\nret = load_library.load_op_library(path)\r\nFile \"/home/usr/miniconda3/envs/myenv/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py\", line 73, in load_op_library\r\nexec(wrappers, module.__dict__)\r\nFile \"<string>\", line 28\r\ndef single_image_random_dot_stereograms(depth_values, hidden_surface_removal=True, convergence_dots_size=8, dots_per_inch=72, eye_separation=2,5, mu=0,333299994, normalize=True, normalize_max=-100, normalize_min=100, border_level=0, number_colors=256, output_image_shape=[1024, 768, 1], output_data_window=[1022, 757], name=None):\r\n                                                                                                                                               ^\r\nSyntaxError: invalid syntax\r\n```\r\n\r\nThe generated code string seems to use a colon instead of a dot for its floating point value:\r\n`dots_per_inch=72, eye_separation=2,5, mu=0,333299994`\r\nsince [\"`.Attr(\"eye_separation: float = 2.5\")`\"](https://github.com/tensorflow/tensorflow/blob/9590c4c32dd4346ea5c35673336f5912c6072bf2/tensorflow/contrib/image/ops/single_image_random_dot_stereograms_ops.cc#L33)."]}, {"number": 21152, "title": "Native TF operations drop Keras tensor metadata", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution: Ubuntu 16.04\r\n- **Mobile device**: N/A\r\n- **TensorFlow installed from**: from pip\r\n- **TensorFlow version**: ('v1.9.0-0-g25c197e023', '1.9.0')\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA-9.0/cuDNN-7.1.4\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n```\r\n$ python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Input\r\n\r\nx = Input((28, 28, 3))\r\ny = Input((28, 28, 3))\r\nz = x + y\r\n\r\nmodel = tf.keras.Model([x, y], z)\r\n```\r\n\r\n### Description\r\nThe code mentioned above throws the error (see logs). As it turned out, almost all native tf operations , such padding, convolution, etc., drop tensor metadata.\r\n\r\nI do understand that these operations can be replaced with similar Keras layers or wrapped with Lambda. However, since Keras is officially moved to tensorflow package, it would be nice to preserve tensor compatibility between tf and Keras.\r\n\r\n### Source code / logs\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/oleg/tf/venv/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/training.py\", line 111, in __init__\r\n    super(Model, self).__init__(*args, **kwargs)\r\n  File \"/home/oleg/tf/venv/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/network.py\", line 78, in __init__\r\n    self._init_graph_network(*args, **kwargs)\r\n  File \"/home/oleg/tf/venv/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/network.py\", line 201, in _init_graph_network\r\n    '(thus holding past layer metadata). Found: ' + str(x))\r\nValueError: Output tensors to a Model must be the output of a TensorFlow `Layer` (thus holding past layer metadata). Found: Tensor(\"add:0\", shape=(?, 28, 28, 3), dtype=float32)\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nGPU model and memory\nMobile device", "- **Mobile device**: N/A\r\n- **Bazel version (if compiling from source)**: N/A (installed from pip)\r\n- **GCC/Compiler version (if compiling from source)**: N/A (installed from pip)\r\n- **GPU model and memory**: N/A (Irrelevant, sample code is for CPU)\r\n", "Nagging Assignee @bignamehyp: It has been 18 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Keras is a higher level API. It doesn't mean naive ops need to support higher level meta-data."]}, {"number": 21151, "title": "Fix compilation error due to typo", "body": "Fixed compilation error below due to the typo in identifier name 'cient_mu_' -> 'client_mu_'.\r\n\r\nERROR: /home/cos/tensorflow/tensorflow/contrib/gdr/BUILD:40:1: C++ compilation of rule '//tensorflow/contrib/gdr:gdr_memory_manager' failed (Exit 1)\r\ntensorflow/contrib/gdr/gdr_memory_manager.cc:175:18: error: use of undeclared identifier 'cient_mu_'; did you mean 'client_mu_'?\r\n      GUARDED_BY(cient_mu_);\r\n                 ^~~~~~~~~\r\n                 client_mu_\r\n./tensorflow/core/platform/default/thread_annotations.h:52:64: note: expanded from macro 'GUARDED_BY'\r\n#define GUARDED_BY(x) THREAD_ANNOTATION_ATTRIBUTE__(guarded_by(x))\r\n                                                               ^\r\n./tensorflow/core/platform/default/thread_annotations.h:42:57: note: expanded from macro 'THREAD_ANNOTATION_ATTRIBUTE__'\r\n#define THREAD_ANNOTATION_ATTRIBUTE__(x) __attribute__((x))\r\n                                                        ^\r\ntensorflow/contrib/gdr/gdr_memory_manager.cc:173:9: note: 'client_mu_' declared here\r\n  mutex client_mu_;\r\n        ^\r\ntensorflow/contrib/gdr/gdr_memory_manager.cc:631:12: warning: unused variable 'checksum' [-Wunused-variable]\r\n  uint64_t checksum = 0;\r\n           ^\r\n1 warning and 1 error generated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 24.192s, Critical Path: 11.63s\r\nINFO: 26 processes: 26 local.\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed the CLA.", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 21150, "title": "How can use mobileNet Classification Model in opencv(c++ platform)?", "body": "I am trying to using mobileNet model in opencv(c++),the object-dection Model is working.but the classification Model can not work.Hope to get help .So many thanks.", "comments": ["I am also facing the same problem. The SSD-Mobilenet is working successfully in opencv dnn (object detection), but tensorflow trained mobilenet model is not working for classification. Please help!!! thanks ", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "It has been 17 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@cy89 this problem has been resolved. thanks", "@amaresh943 yay! Can you please describe the resolution (even if it was as simple as reinstalling)?", "@cy89 \"please describe the resolution\" ........................which resolution?", "@amaresh943 you said \"this problem has been resolved\". Did it go away with a new version, or did you find some other solution to resolve it? It'd be nice to record in case others hit the same thing.\r\n\r\nThanks!", "@cy89 To know in detail,  Please visit the link given below.\r\nhttps://github.com/opencv/opencv/issues/12066\r\n\r\nthanks!", "Thanks!"]}, {"number": 21149, "title": "[mkl]Fix gcc6.3 build link issue", "body": "when we build tensorflow with gcc6.3 cpu  it will meet the errors below, this PR will fix this issue.\r\n\r\n\r\nIn file included from tensorflow/contrib/lite/kernels/fully_connected.cc:28:0:\r\n./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h: In function 'void tflite::optimized_ops::LogSoftmax(const uint8*, const tflite::Dims<4>&, int32, int32, int32, int32, int, uint8*, const tflite::Dims<4>&)':\r\n./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:4577:53: warning: typedef 'using FixedPoint0 = class gemmlowp::FixedPoint<int, 0>' locally defined but not used [-Wunused-local-typedefs]\r\n   using FixedPoint0 = gemmlowp::FixedPoint<int32, 0>;\r\n                                                     ^\r\nERROR: /home/tensorflow/code/1/dl_framework-intel_tensorflow/tensorflow/core/BUILD:2830:1: Linking of rule '//tensorflow/core:lib_strings_stringprintf_test' failed (Exit 1)\r\nbazel-out/k8-opt/bin/_solib_k8/libexternal_Scom_Ugoogle_Uabsl_Sabsl_Sbase_Slibbase.so: error: undefined reference to 'pthread_key_create'\r\nbazel-out/k8-opt/bin/_solib_k8/libexternal_Scom_Ugoogle_Uabsl_Sabsl_Sbase_Slibbase.so: error: undefined reference to 'pthread_getspecific'\r\nbazel-out/k8-opt/bin/_solib_k8/libexternal_Scom_Ugoogle_Uabsl_Sabsl_Sbase_Slibbase.so: error: undefined reference to 'pthread_setspecific'\r\nbazel-out/k8-opt/bin/_solib_k8/libexternal_Scom_Ugoogle_Uabsl_Sabsl_Sbase_Slibbase.so: error: undefined reference to 'pthread_sigmask'\r\nbazel-out/k8-opt/bin/_solib_k8/_U@mkl_Ulinux_S_S_Cmkl_Ulibs_Ulinux___Uexternal_Smkl_Ulinux_Slib/libmklml_intel.so: error: undefined reference to 'dladdr'\r\nbazel-out/k8-opt/bin/_solib_k8/_U@mkl_Ulinux_S_S_Cmkl_Ulibs_Ulinux___Uexternal_Smkl_Ulinux_Slib/libmklml_intel.so: error: undefined reference to 'dlopen'\r\nbazel-out/k8-opt/bin/_solib_k8/_U@mkl_Ulinux_S_S_Cmkl_Ulibs_Ulinux___Uexternal_Smkl_Ulinux_Slib/libmklml_intel.so: error: undefined reference to 'dlerror'\r\nbazel-out/k8-opt/bin/_solib_k8/_U@mkl_Ulinux_S_S_Cmkl_Ulibs_Ulinux___Uexternal_Smkl_Ulinux_Slib/libmklml_intel.so: error: undefined reference to 'dlsym'\r\n\r\n\r\nSigned-off-by: shaohua <shaohua.zhang@intel.com>", "comments": ["Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 21148, "title": "Add tf.contrib.data.LMDBDataset support", "body": "\r\nThis fix tries to address the issue raised in #21129 where\r\nthere was no LMDBDataset support (only LMDBReader).\r\n\r\nThis fix fixes #21129.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["tentatively assigning this to @jhseu ", "Derek, thoughts on tf.contrib.data or tf.data for this? We already have LMDBReader in TensorFlow because it's a common format used with Caffe.", "I'd be fine with `tf.contrib.data` for this.\r\n\r\nIs there any hope of making liblmdb work with TF's file readers? One concern I have is that, unlike the other I/O datasets, this one only works with local files, and not HDFS/GCS/S3/etc.", "Thanks @mrry for the review. The PR has been reworked and now LMDBDataset is exposed through `tf.contrib.data`. The binary file `data.mdb` has also been removed. I take a look at the mdb library but couldn't find an easy way to pass different stream than filename. Please take a look.", "Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 21147, "title": "support CI_DOCKER_BUILD_EXTRA_PARAMS for ci_build.sh", "body": "Allow user to specify docker build options.\r\n\r\nA typical scenario is: user want to set `CI_DOCKER_BUILD_EXTRA_PARAMS=--pull` to get latest ubuntu or cuda base image\r\n\r\n\r\n", "comments": []}]