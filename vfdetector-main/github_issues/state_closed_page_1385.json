[{"number": 11513, "title": "Update variables.py", "body": "Changed ValueError messages to format the passed parameter in the error message.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 11512, "title": "Error in table.lookup from tf.contrib.lookup when using dataset api", "body": "\r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: All platforms\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.2.1\r\n- **Python version**: 3.6, 2.7\r\n\r\nI try the following code:\r\n\r\n```\r\nvocab =tf.contrib.lookup.index_table_from_file('./vocab.txt', num_oov_buckets=1)\r\ndataset = tf.contrib.data.Dataset.from_tensor_slices(tf.constant(['./test1.txt']))\r\ndataset = dataset.flat_map(lambda filename: tf.contrib.data.TextLineDataset(filename))\r\ndataset = dataset.map(lambda line: tf.string_split([line]).values)\r\ndataset = dataset.map(lambda words: table.lookup(words))\r\n```\r\ngives me the following errors. This is different from the stack overflow issue:\r\nhttps://stackoverflow.com/questions/44519045/new-dataset-map-transformation-and-lookup-table-incompatible-string-type and appears before referencing the iterator\r\n\r\n\r\n```\r\n----> 1 dataset.map(lambda x: table.lookup(x))\r\n\r\n/Users/xxx/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/contrib/lookup/lookup_ops.pyc in lookup(self, keys, name)\r\n    803             name=\"hash_bucket\")\r\n    804         if self._table:\r\n--> 805           ids = self._table.lookup(values)\r\n    806           buckets = math_ops.add(buckets, self._table.size())\r\n    807           is_id_non_default = math_ops.not_equal(ids, self._table.default_value)\r\n\r\n/Users/xxx/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/contrib/lookup/lookup_ops.pyc in lookup(self, keys, name)\r\n    184       # pylint: disable=protected-access\r\n    185       values = gen_lookup_ops._lookup_table_find(\r\n--> 186           self._table_ref, key_tensor, self._default_value, name=scope)\r\n    187       # pylint: enable=protected-access\r\n    188 \r\n\r\n/Users/xxx/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/ops/gen_lookup_ops.pyc in _lookup_table_find(table_handle, keys, default_value, name)\r\n    286   result = _op_def_lib.apply_op(\"LookupTableFind\", table_handle=table_handle,\r\n    287                                 keys=keys, default_value=default_value,\r\n--> 288                                 name=name)\r\n    289   return result\r\n    290 \r\n\r\n/Users/xxx/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)\r\n    765         op = g.create_op(op_type_name, inputs, output_types, name=scope,\r\n    766                          input_types=input_types, attrs=attr_protos,\r\n--> 767                          op_def=op_def)\r\n    768         if output_structure:\r\n    769           outputs = op.outputs\r\n\r\n/Users/xxx/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/contrib/data/python/framework/function.pyc in create_op(self, op_type, inputs, data_types, **kwargs)\r\n     79           self.extra_args.append(ph)\r\n     80     return super(_ExperimentalFuncGraph, self).create_op(op_type, inputs,\r\n---> 81                                                          data_types, **kwargs)\r\n     82 \r\n     83   def _add_tensor_and_parents(self, tensor):\r\n\r\n/Users/xxx/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/framework/function.pyc in create_op(self, op_type, inputs, data_types, **kwargs)\r\n    359           self.extra_args.append(ph)\r\n    360     return super(_FuncGraph, self).create_op(op_type, inputs, data_types,\r\n--> 361                                              **kwargs)\r\n    362 \r\n    363 \r\n\r\n/Users/xxx/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\r\n   2504     ret = Operation(node_def, self, inputs=inputs, output_types=dtypes,\r\n   2505                     control_inputs=control_inputs, input_types=input_types,\r\n-> 2506                     original_op=self._default_original_op, op_def=op_def)\r\n   2507     if compute_shapes:\r\n   2508       set_shapes_for_outputs(ret)\r\n\r\n/Users/xxx/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\r\n   1248                             self.node_def.name,\r\n   1249                             [i.dtype for i in self._inputs],\r\n-> 1250                             input_types))\r\n   1251     self._input_types = input_types\r\n   1252 \r\n\r\nTypeError: In op 'string_to_index_Lookup/hash_table_Lookup', \r\ninput types ([tf.string, tf.string, tf.int64]) are not compatible with expected types\r\n ([tf.string_ref, tf.string, tf.int64])\r\n```\r\nHowever if I use the following referencing python_lookup_ops as outline in NMT, there is no issue..\r\nWhy is this?\r\n\r\n```\r\nfrom tensorflow.python.ops import lookup_ops\r\ntable = lookup_ops.index_table_from_file('./vocab.txt', num_oov_buckets=1)\r\n```", "comments": ["Can you try this again with a nightly build? I believe @ysuematsu fixed this problem in 55f987692a25645a9db06e915c3fa248c3e5193c.", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 11511, "title": "Remove RTLD_GLOBAL when loading pywrap_tensorflow", "body": "Note: I'm making this a pull request to test it. I don't propose merging as-is, and this will likely need to be split up to be reviewed sensibly.\r\n\r\nSplits TensorFlow into //tensorflow/core:framework and //tensorflow/core:lib shared objects. Custom ops will be able to rely on these shared objects (e.g. for op registration) rather than relying on the global symbol table. Should enable custom ops in languages other than Python, among other things.", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "Jenkins, test this please.", "Ugh, I'll figure out how to amend the email for those first few commits on Monday."]}, {"number": 11510, "title": "Add \"name\" variable support for tf.einsum", "body": "This fix adds \"name\" variable support for tf.einsum\r\n\r\nThis fix fixes #11501\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@vrv Thanks for the review. If we use `name=None` at the end then it will thrown out a syntax error:\r\n```\r\n  File \"/home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/special_math_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/special_math_ops.py\", line 85\r\n    def einsum(equation, *inputs, name=None):\r\n                                     ^\r\nSyntaxError: invalid syntax\r\n```\r\n\r\nSo I used `**kwargs` to get around it.", "@tensorflow-jenkins test this please", "Thanks @vrv for the review. The PR has been updated so that all invalid argument keys would be printed in case of error. A test case has been added as well.\r\n\r\nI also updated with\r\n```\r\n   $ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test \\\r\n          --update_goldens True\r\n```\r\nso that api_compatibility_test could pass on Jenkins.\r\n\r\nPlease take a look and let me know if there are any issues.", "Thanks!  This looks good.\r\n\r\nI'm not sure if this qualifies as something that needs API review -- presumably it does and it should be fine.\r\n\r\n@tensorflow-jenkins test this please", "Fine for API review.", "@tensorflow-jenkins test this please", "@yongtang It looks like you need to fix the expected error message in your test:\r\n\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/bazel_pip/tensorflow/python/special_math_ops_test.runfiles/org_tensorflow/bazel_pip/tensorflow/python/ops/special_math_ops_test.py\", line 251, in test_invalid_keyword_arguments\r\n    invalid1=\"value1\", invalid2=\"value2\")\r\n**AssertionError: \"invalid keyword arguments for this function: invalid1, invalid2\" does not match \"invalid keyword arguments for this function: invalid2, invalid1\"**\r\n", "@rmlarsen Thanks for the review. The Jenkins error for was because dict in python is not ordered when iterate through. I have sorted the error message so that the output is repeatable. The PR has been updated now.", "@yongtang thanks for the quick fix. Yeah, iterating over an unordered container is asking for trouble :-)\r\n@tensorflow-jenkins test this please"]}, {"number": 11509, "title": "Updating the version to 1.3.0-rc0.", "body": "", "comments": []}, {"number": 11508, "title": "Branch 162017464", "body": "", "comments": []}, {"number": 11507, "title": "Fix syntax error in sample code", "body": "This would have otherwise caused a non-keyword arg after keyword arg", "comments": ["Can one of the admins verify this patch?", "There are actually a bunch more issues in this sample code. I can fix some of these (or the author can take a look) later", "@MyHumbleSelf definitely feel free to fix anything that you think is wrong.\r\n\r\nJenkins, test this please."]}, {"number": 11506, "title": "Change `as_cluster_spec` to `cluster_spec`", "body": "", "comments": []}, {"number": 11505, "title": "R1.2", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I think this PR is opened erroneously. ", "Correct, apologies, \n\nChris \n\n----- Original Message -----\n\nFrom: \"Frank Chen\" <notifications@github.com> \nTo: \"tensorflow/tensorflow\" <tensorflow@noreply.github.com> \nCc: \"cj-bradley\" <cj-bradley@comcast.net>, \"Author\" <author@noreply.github.com> \nSent: Friday, July 14, 2017 2:59:52 PM \nSubject: Re: [tensorflow/tensorflow] R1.2 (#11505) \n\n\n\nI think this PR is opened erroneously. \n\n\u2014 \nYou are receiving this because you authored the thread. \nReply to this email directly, view it on GitHub , or mute the thread . \n\n"]}, {"number": 11504, "title": "The previous commit attempted to fix a problem for PowerPC", "body": "architectures, which require a different gcc optimization argument.\r\nHowever, this addition had two implementation errors, which caused all\r\narchitectures to receive this deprecated compiler flag and therefore\r\nproduce a non-optimized tensorflow compilation.\r\n\r\nSpecifically, the introduced 'is_ppc()' used a syntactically incorrect variable\r\nidentification '${}' in the statement where command substitution '$()' should\r\nhave been used. This caused the command to always silently fail, and the\r\nfunction to always return true regardless of architecture.\r\n\r\nAdditionally, where the function was called, the return value should be\r\ncontrolling the if statement, but by encapsulating the call with '[..]', the\r\nresulting expression evaluation was also always true.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 11503, "title": "Branch 161949540", "body": "", "comments": ["@alextp these deletions to ops.pbtxt are auto-generated.\r\n\r\nLooks like @jhseu should know about the underlying change.\r\n", "@allenlavoie FYI, tensorflow/contrib/timeseries/python/timeseries:state_management_test is segfaulting in the PIP build:\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-cpu-python3/5400/consoleFull\r\n\r\n@tensorflow-jenkins test this please\r\n\r\ntesting again to see whether it is reproducible.", "@allenlavoie Looks the failure in state_management_test is random. There may be other tests in contrib/timeseries that have the same problem. We will keep an eye on them. I'm merging this PR now. ", "Yeah, I can reproduce the same kind of flakiness for that test locally (passes, but segfaults after) without this PR. Odd since we're not using any timeseries-specific custom ops and it's not at all flaky internally. I'll try to track it down."]}, {"number": 11502, "title": "float16 depthwise conv support", "body": "It seems that the current implementation of depthwise convolution doesn't support half-precision : \r\n`/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.pyc in depthwise_conv2d_native(input, filter, strides, padding, data_format, name)`\r\n`TypeError: Value passed to parameter 'input' has DataType float16 not in list of allowed values: float32, float64`\r\nAny chances of having float16 support for this important op anytime soon? So training on P100s would be faster.\r\n", "comments": []}, {"number": 11501, "title": "tf.einsum does not have \"name\" variable", "body": "I noticed there is no ability to name a tensor generated from from tf.einsum(). Is this on purpose?", "comments": ["It sounds like an oversight! Would you like to send a PR to fix this?", "This sounds like an good opportunity for a community member to become a TensorFlow contributor.", "Added PR #11510", "@jart I'm so new to coding that I wouldn't even know where to start; I'm math guy who just learned what a pull request was now. If you have suggestions on how to learn more about being a contributor, I'd be happy to learn; feel free to send me a message."]}, {"number": 11500, "title": "allocate_output can result in OOM error", "body": "TF-commit: 7f008453ca1c7b\r\n\r\n### Describe the problem\r\nWhen implementing custom operations, allocating output twice (as a bug) should give at least a warning. The following code\r\n\r\n```c++\r\n    Tensor* tensor = nullptr;\r\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, output_shape, &tensor));\r\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, output_shape, &tensor));\r\n```\r\n\r\ngives a nearly un-debuggable OOM error after many iterations during runtime. Is it possible to prevent  `allocate_output(i,...)` being called with the same `i` twice?", "comments": ["It looks as if this [is checked](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/op_kernel.cc#L644) but only in debug builds. I'm sorry that this was hard for you to track down, but it's probably appropriate to have that checking turned off for performance. It is worth running a debug build when investigating mysterious issues.\r\n\r\nI'll close this for now but please reopen if you still think there's a bug.", "Looking again, I think you're right that it would be good if this were flagged up. However this function is a very hot code path that has been worked over carefully in the past for optimization so I'm not completely comfortable just suggesting you send a PR to fix this (e.g., make the call return an error rather than CHECK). @asimshankar would you take a look and see if you're happy with the performance implications of this check?", "The debug mode works as expected. Then this isn't an issue anymore. But most people probably only compile TF in release and then see the same behavior.\r\n\r\nMaybe a\r\n```c++\r\nctx->allocate_and_append_output(output_shape, &tensor));\r\n```\r\nwould solve these issues. Don't know a single operation, where the output shape isn't known in the very beginning of the function.\r\n\r\nI close this for now (knowing such an issue exists)."]}, {"number": 11499, "title": "Tensorflow runs matmul, bincount and other \"heavy\" funcions only on cpu", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux virgo-wn100.roma1.infn.it 3.10.0-514.el7.x86_64 #1 SMP Thu Nov 3 15:10:49 CDT 2016 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"7.3 (Nitrogen)\"\r\nVERSION_ID=\"7.3\"\r\nREDHAT_BUGZILLA_PRODUCT_VERSION=7.3\r\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7.3\"\r\n- **TensorFlow installed from (source or binary)**: from pip3\r\n- **TensorFlow version (use command below)**: v1.2.0-5-g435cdfc 1.2.1\r\n- **Python version**: Python 3.5.2\r\n- **CUDA/cuDNN version**: CUDA: 8.0, cuDNN : 5\r\n- **GPU model and memory**: Tesla K20m, memory 4742MiB\r\n\r\n### Describe the problem\r\nEverytime i run a code with simple operations and heavy computation functions, the simple ones, like add or reshape, run on the gpu, while the heavy ones, like matmul, bincount, tensordot run always on cpu. This is very strange, because tensorflow sees the gpu but uses it only for simple functions, while i expect the inverse behavior.\r\nI noticed it when i used Timeline to profile my codes.\r\n\r\n### Source code / logs\r\nA simple example of the code i use:\r\n```\r\nimport numpy\r\nimport tensorflow as tf\r\nfrom tensorflow.python.client import timeline\r\n\r\nmatrix1 = tf.zeros([5000,5000], dtype = tf.int32)\r\nmatrix2 = tf.ones([5000,5000], dtype = tf.int32)\r\nmatrix1 = tf.add(matrix1,2)\r\nproduct = tf.matmul(matrix1,matrix2)\r\n    \r\nsession = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n\r\nrun_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\nrun_metadata = tf.RunMetadata()\r\n\r\nimage = session.run(product, options=run_options, run_metadata=run_metadata)\r\n\r\n\r\n    # Create the Timeline object, and write it to a json\r\ntl = timeline.Timeline(run_metadata.step_stats)\r\nctf = tl.generate_chrome_trace_format()\r\nwith open('timelineDB.json', 'w') as f:\r\n\tf.write(ctf)\r\n```\r\n\r\nand here the logs:\r\n``` \r\n2017-07-14 12:33:41.431036: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-14 12:33:41.431080: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-14 12:33:41.431089: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-14 12:33:41.431096: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-14 12:33:41.431104: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-14 12:33:42.860681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: \r\nname: Tesla K20m\r\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.7055\r\npciBusID 0000:02:00.0\r\nTotal memory: 4.63GiB\r\nFree memory: 4.57GiB\r\n2017-07-14 12:33:42.860741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 \r\n2017-07-14 12:33:42.860778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y \r\n2017-07-14 12:33:42.860791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K20m, pci bus id: 0000:02:00.0)\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: Tesla K20m, pci bus id: 0000:02:00.0\r\n2017-07-14 12:33:42.901828: I tensorflow/core/common_runtime/direct_session.cc:265] Device mapping:\r\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: Tesla K20m, pci bus id: 0000:02:00.0\r\n\r\nAdd: (Add): /job:localhost/replica:0/task:0/gpu:0\r\n2017-07-14 12:33:42.902645: I tensorflow/core/common_runtime/simple_placer.cc:847] Add: (Add)/job:localhost/replica:0/task:0/gpu:0\r\nMatMul: (MatMul): /job:localhost/replica:0/task:0/cpu:0\r\n2017-07-14 12:33:42.902669: I tensorflow/core/common_runtime/simple_placer.cc:847] MatMul: (MatMul)/job:localhost/replica:0/task:0/cpu:0\r\nAdd/y: (Const): /job:localhost/replica:0/task:0/gpu:0\r\n2017-07-14 12:33:42.902714: I tensorflow/core/common_runtime/simple_placer.cc:847] Add/y: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nones: (Const): /job:localhost/replica:0/task:0/cpu:0\r\n2017-07-14 12:33:42.902736: I tensorflow/core/common_runtime/simple_placer.cc:847] ones: (Const)/job:localhost/replica:0/task:0/cpu:0\r\nzeros: (Const): /job:localhost/replica:0/task:0/gpu:0\r\n2017-07-14 12:33:42.902756: I tensorflow/core/common_runtime/simple_placer.cc:847] zeros: (Const)/job:localhost/replica:0/task:0/gpu:0\r\n2017-07-14 12:33:43.110880: I tensorflow/stream_executor/dso_loader.cc:139] successfully opened CUDA library libcupti.so.8.0 locally\r\n```\r\nI attach the .json generated (change .txt to .json and open it with chrome://tracing/)\r\n[timeline.txt](https://github.com/tensorflow/tensorflow/files/1148128/timeline.txt)\r\n", "comments": ["This because tf.matmul does not have a GPU kernel for integer matrix multiplication. If you change those to float32 it should be placed on the GPU.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/matmul_op.cc#L377\r\n", "So for the same reason, `tf.bincount`, that has argument `arr` as 'An int32 tensor of non-negative values.', can't work on GPU, right?", "Integer arguments alone don't mean that there is not a GPU kernel, but there tend to be far more GPU kernels for float types than integer types.\r\n\r\nBincount seems to not have any GPU support -- only CPU. Unfortunately there's no good way to know this from the documentation. The best bet is to look in the source code. If you look in this directory and search for the op you're using:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/kernels\r\n\r\nYou can read the source. Scroll all the way to the bottom to find the \"registration\" macros. If you don't see a line that looks like `REGISTER_BLAH_GPU(float32)` or `TF_CALL_float32(REGISTER_GPU)` ... basically some line mentioning the type you care about and the word \"GPU\" then it probably means there's no GPU registration for that type/op combination.", "Ok, thank you so much. It could be useful or interesting for someone to open a feature request about bincount on GPU?", "Yes! Feel free to file an issue requesting GPU support for tf.bincount. If it's not a priority for the TF team, maybe some external contributor will take it up.", "Closing this out since it seems that the original question has been addressed. "]}, {"number": 11498, "title": "R0.10", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "@szkszk95, can you talk about what this pull request is for?"]}, {"number": 11497, "title": "Tensorflow-serving(CPU) for xbox one/ps4/switch ?", "body": "Hello,\r\nFirst of all I would like to thank the developer of tensorflow for this great library.\r\n\r\nCan tensorflow-serving(CPU) works on xbox1 or ps4? (i mean to compile it)\r\n\r\nThank you.", "comments": ["TensorFlow can be built from sources (see https://www.tensorflow.org/install/install_sources). However, the platforms you mention are not officially supported and we might not be able to provide much help in dealing with problems that arise.\r\n\r\nThat said, we encourage community contributions, so if you do get things to work, we'd encourage you to contribute back with the changes to code and/or documentation necessary.\r\n\r\nThanks!"]}, {"number": 11496, "title": "Turning on XLA JIT Compilation during session crashes computer?", "body": "## System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.2.1\r\n- **Python version**:  2.7.12\r\n- **Bazel version (if compiling from source)**: 0.5.2\r\n- **CUDA/cuDNN version**: 8.0/5.1\r\n- **GPU model and memory**: Titan X Pascal 12GB\r\n- **Exact command to reproduce**: Running any code with the following configuration passed to a supervisor managed session:\r\n\r\n```\r\n# Config to turn on JIT compilation\r\nconfig = tf.ConfigProto()\r\nconfig.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\r\n\r\nwith sv.managed_session(config=config) as sess:\r\n...\r\n```\r\n\r\n### Describe the problem\r\nNo idea why but when I tried to turn on the XLA JIT compilation according to this guide:\r\nhttps://www.tensorflow.org/performance/xla/jit\r\n\r\nMy computer crashes immediately and I've no way of diagnosing the problem. Note that I installed TF from source and enabled the XLA JIT option. Does this option consume more power than normal? \r\n\r\nPreviously, I encountered a similar problem when I ran the TF-slim VGG model with a pip-installed TensorFlow-gpu r1.2, and the computer crashes simply. I could not find any issue until I ran the exact same code in another computer (a laptop with weaker power supply) with source-compiled tensorflow, which works. I then replaced the pip-installed TensorFlow with a source-compiled one, and everything works fine --- this led me to think there might be some issue with either my installation or the source code (although I can't verify). \r\n\r\nHas anyone faced a similar problem? Also, is XLA activated by default if I configured it to be enabled during the source installation? i.e. it could be because I called the compilation twice when it was already activated by default, causing the system failure. Is there a way to verify whether XLA is activated - so far the only indication I see are the messages \"XLA service 0x62bb180 executing computations on platform Host\" and \"XLA service 0x62a43b0 executing computations on platform CUDA\". Is this sufficient, i.e. to say I do not have to manually activate XLA?\r\n", "comments": ["I'm afraid it sounds as if this is more of an issue with your computer setup than with TensorFlow, so I'm closing the issue since it isn't a TensorFlow bug or feature request. Please feel free to ask on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) for more advice."]}, {"number": 11495, "title": "MultiGPU multi-session ", "body": "It would be nice if there is an official way to do limit devices initialised by tensorflow other than the os level environment variable settings specifically as in[ this](https://stackoverflow.com/a/34776814) answer posted by @mrry in stackoverflow. As of my understanding the setup mentioned in [another](https://stackoverflow.com/a/34200194) answer by @mrry will not prevent tensorflow from grabbing all the GPU resources available on a same workstation at initialization as it is in by `tf.train.Server()`.  It would be essentially helpful for those sharing a machine among multiple users, so that they could independently initiate tensorflow graphs in different GPU units. Although this could be achieved via `nvidia-docker `or other resource orchestration tools, a native API would be a great addition to tensorflow.  Please ignore this if there is already an official way or my understanding is wrong.  ", "comments": ["Does [Manual GPU Device Placement](https://www.tensorflow.org/tutorials/using_gpu#manual_device_placement) answer your question?", "I think it may not, because tensorflow grabs all the available GPU RAM (a good share of it) while a call to `tf.train.Server()` is made from one process, so other process that could possibly co-exist with it may not get enough RAM which would essentially fail.  The option of  manual device placement or allocating finite amount of GPU RAM might sounds to be a solution, but the previous condition do not allows that, if I understood it correctly. I have tried `config.gpu_options.visible_device_list` also but it didn't help. In the stackoverflow answer @mrry points out as calling separate process with a prefix from the shell, and similarly [here](https://stackoverflow.com/a/37901914) in the python code itself. Both of it works but it would be great if there is a native way to do this in tensorflow API to make thing straight forward. This would be particularly preferable when a node with many GPU is shared among multiple users, so that each of them could independently run their task on the same node. It may not be a problem with `ingraphreplication` because as you mentioned all GPUs are utilised by a single process with manual device placement. Is there anything fundamentally prevents from doing this, such as os level GPU arbitration or something?. Though `apache mesos` or similar systems provides much higher level ways to do this, it would be nice if this could be done with the native tensorflow code. ", "Are you creating a `tf.train.Server` in your program? In that case you can programmatically limit the GPUs used by passing a `tf.ConfigProto` (with `config.gpu_options.visible_device_list` set as desired) as the `config` argument to the `Server` initializer. (The same option will be ignored when you create a `tf.Session` against a server, because the GPU devices are configured at server construction time, and shared between all sessions that use that server.)", "Thanks for the response. Yes I am creating a `tf.train.Server` from the program. As exactly @mrry mentioned the  `config.gpu_options.visible_device_list` has been passed into a` tf.Session`, where it is mostly expected and definitely it is ignored by the session. I will try passing the option to server initialiser and let you know.\r\n```\r\n    config = tf.ConfigProto(log_device_placement=False, allow_soft_placement=True)\r\n    config.gpu_options.per_process_gpu_memory_fraction = 0.4\r\n    config.gpu_options.allow_growth=True\r\n    config.gpu_options.visible_device_list = str(gpu_index)\r\n    server = tf.train.Server(spec, job_name=job_name, task_index=task_index, config=config)\r\n```\r\n\r\nYes, it is working, I should add the GPU RAM fraction specification is also working while it is passed to the server initialiser nicely (what are the `tf.ConfigProto` options that the `tf.Session` takes in really?).  However this is not clear from the current documentation, in fact not in many other discussions on the same matter(consider [this](https://github.com/tensorflow/tensorflow/issues/1888) feature request). Hope this would help other people trying to get the same functionality with Tensorflow without depending on other complicated resource scheduling schemes. Great work, thank you.", "@surajkamal closing, since it seems that it is working out for you. Please re-open if I misunderstood.\r\n\r\n/CC @MarkDaoust for consideration of the documentation."]}, {"number": 11494, "title": "Error reading filenames with special characters on Windows", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.2.1\r\n- **Python version**: 3.6\r\n\r\n### Describe the problem\r\nMy tensorflow script on Ubuntu 16.04 works with the following code.\r\n\r\n```\r\nfile=tf.gfile.GFile('../../data/Gutenberg/txt/Anthony Trollope___La Vend\u00e9e.txt')\r\ntext = file.read()\r\n```\r\n\r\nHowever on Windows 10\r\n```\r\nfile=tf.gfile.GFile('../../data/Gutenberg/txt/Anthony Trollope___La Vend\u00e9e.txt')\r\ntext = file.read()\r\n```\r\ngives the following error.  This happens when the filenames have special characters in them\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-22-669eb528a808> in <module>()\r\n----> 1 text = file.read()\r\n\r\nC:\\Users\\xxx\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py in read(self, n)\r\n    116       string if in string (regular) mode.\r\n    117     \"\"\"\r\n--> 118     self._preread_check()\r\n    119     with errors.raise_exception_on_not_ok_status() as status:\r\n    120       if n == -1:\r\n\r\nC:\\Users\\xxx\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py in _preread_check(self)\r\n     76       with errors.raise_exception_on_not_ok_status() as status:\r\n     77         self._read_buf = pywrap_tensorflow.CreateBufferedInputStream(\r\n---> 78             compat.as_bytes(self.__name), 1024 * 512, status)\r\n     79 \r\n     80   def _prewrite_check(self):\r\n\r\nC:\\Users\\xxx\\Anaconda3\\lib\\contextlib.py in __exit__(self, type, value, traceback)\r\n     87         if type is None:\r\n     88             try:\r\n---> 89                 next(self.gen)\r\n     90             except StopIteration:\r\n     91                 return\r\n\r\nC:\\Users\\xxx\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py in raise_exception_on_not_ok_status()\r\n    464           None, None,\r\n    465           compat.as_text(pywrap_tensorflow.TF_Message(status)),\r\n--> 466           pywrap_tensorflow.TF_GetCode(status))\r\n    467   finally:\r\n    468     pywrap_tensorflow.TF_DeleteStatus(status)\r\n\r\nNotFoundError: NewRandomAccessFile failed to Create/Open: ../../data/Gutenberg/txt/Anthony Trollope___La Vend\u00e9e.txt : The system cannot find the file specified.\r\n```\r\nNote: pythons open()/read() works well on both platforms.  This error occurs for other file names  with special characters on windows too. I moved my data readers from python to tf api and data readers, the pipeline works on ubuntu but breaks in windows.\r\n", "comments": ["@mrry do you know who owns this code?", "This looks like a duplicate of #9481, so let's close this and follow along on that thread. It's \"contributions welcome\" (and I think it might be on @guschmue's radar)."]}, {"number": 11493, "title": "Using TF-slim DatasetDataProvider generate batch data, but get an error", "body": "------------------------\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:macOS\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.2.1\r\n- **Python version**: 3.5.0\r\n\r\nIn the [StackOverflow](https://stackoverflow.com/search?q=slim+OutOfRangeError) I find similar problems but no one answered.\r\nPlease give me some advice. Thanks.\r\n```\r\ndef read_and_decode():\r\n    file_pattern = './voc*.tfrecord'\r\n    keys_to_features = {\r\n        'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\r\n        'image/format': tf.FixedLenFeature((), tf.string, default_value='jpeg'),\r\n        'image/height': tf.FixedLenFeature([1], tf.int64),\r\n        'image/width': tf.FixedLenFeature([1], tf.int64),\r\n        'image/channels': tf.FixedLenFeature([1], tf.int64),\r\n        'image/shape': tf.FixedLenFeature([3], tf.int64),\r\n        'image/object/bbox/xmin': tf.VarLenFeature(dtype=tf.float32),\r\n        'image/object/bbox/ymin': tf.VarLenFeature(dtype=tf.float32),\r\n        'image/object/bbox/xmax': tf.VarLenFeature(dtype=tf.float32),\r\n        'image/object/bbox/ymax': tf.VarLenFeature(dtype=tf.float32),\r\n        'image/object/bbox/label': tf.VarLenFeature(dtype=tf.int64),\r\n        'image/object/bbox/difficult': tf.VarLenFeature(dtype=tf.int64),\r\n        'image/object/bbox/truncated': tf.VarLenFeature(dtype=tf.int64),\r\n    }\r\n    items_to_handlers = {\r\n        'image': slim.tfexample_decoder.Image('image/encoded', 'image/format'),\r\n        'shape': slim.tfexample_decoder.Tensor('image/shape'),\r\n        'object/bbox': slim.tfexample_decoder.BoundingBox(\r\n            ['ymin', 'xmin', 'ymax', 'xmax'], 'image/object/bbox/'),\r\n        'object/label': slim.tfexample_decoder.Tensor('image/object/bbox/label'),\r\n        'object/difficult': slim.tfexample_decoder.Tensor('image/object/bbox/difficult'),\r\n        'object/truncated': slim.tfexample_decoder.Tensor('image/object/bbox/truncated'),\r\n    }\r\n    decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)\r\n    return slim.dataset.Dataset(\r\n        data_sources=file_pattern,\r\n        reader=tf.TFRecordReader,\r\n        num_samples=1,\r\n        decoder=decoder,\r\n        items_to_descriptions={},\r\n        num_classes=21)\r\n\r\n\r\nslim = tf.contrib.slim\r\ndataset = read_and_decode()\r\nprovider = slim.dataset_data_provider.DatasetDataProvider(dataset, num_readers=3, shuffle=False)\r\n\r\n[image, shape, glabels, gbboxes] = provider.get(['image', 'shape', 'object/label', 'object/bbox'])\r\nimage = tf.expand_dims(image, 0)\r\nimage = tf.image.resize_images(image, [224,224],tf.image.ResizeMethod.BILINEAR)\r\nglabels.set_shape([1])\r\n\r\nimg_batch, label_batch = tf.train.batch([image, glabels], 64, capacity=2000)\r\nwith tf.Session() as sess:\r\n    ini_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\r\n    sess.run(ini_op)\r\n    coord = tf.train.Coordinator()\r\n    thread = tf.train.start_queue_runners(sess=sess,coord=coord)\r\n    for i in range(2):\r\n        cur_example_batch,cur_label_batch = sess.run([img_batch,label_batch])\r\n        print(cur_example_batch.shape)\r\n    coord.request_stop()\r\n    coord.join(thread)\r\n```\r\n\r\nThe error infomation:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/szp/Documents/github/Tianchi_MedicalAI/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1139, in _do_call\r\n    return fn(*args)\r\n  File \"/Users/szp/Documents/github/Tianchi_MedicalAI/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1121, in _run_fn\r\n    status, run_metadata)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/Users/szp/Documents/github/Tianchi_MedicalAI/venv/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: FIFOQueue '_3_batch/fifo_queue' is closed and has insufficient elements (requested 64, current size 0)\r\n\t [[Node: batch = QueueDequeueManyV2[component_types=[DT_FLOAT, DT_INT64], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](batch/fifo_queue, batch/n)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/szp/Documents/github/RCNN/RCNN.py\", line 140, in <module>\r\n    cur_example_batch,cur_label_batch = sess.run([img_batch,label_batch])\r\n  File \"/Users/szp/Documents/github/Tianchi_MedicalAI/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 789, in run\r\n    run_metadata_ptr)\r\n  File \"/Users/szp/Documents/github/Tianchi_MedicalAI/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 997, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/Users/szp/Documents/github/Tianchi_MedicalAI/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1132, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/Users/szp/Documents/github/Tianchi_MedicalAI/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1152, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: FIFOQueue '_3_batch/fifo_queue' is closed and has insufficient elements (requested 64, current size 0)\r\n\t [[Node: batch = QueueDequeueManyV2[component_types=[DT_FLOAT, DT_INT64], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](batch/fifo_queue, batch/n)]]\r\n\r\nCaused by op 'batch', defined at:\r\n  File \"/Users/szp/Documents/github/RCNN/RCNN.py\", line 133, in <module>\r\n    img_batch, label_batch = tf.train.batch([image, glabels], 64, capacity=2000)\r\n  File \"/Users/szp/Documents/github/Tianchi_MedicalAI/venv/lib/python3.5/site-packages/tensorflow/python/training/input.py\", line 919, in batch\r\n    name=name)\r\n  File \"/Users/szp/Documents/github/Tianchi_MedicalAI/venv/lib/python3.5/site-packages/tensorflow/python/training/input.py\", line 716, in _batch\r\n    dequeued = queue.dequeue_many(batch_size, name=name)\r\n  File \"/Users/szp/Documents/github/Tianchi_MedicalAI/venv/lib/python3.5/site-packages/tensorflow/python/ops/data_flow_ops.py\", line 457, in dequeue_many\r\n    self._queue_ref, n=n, component_types=self._dtypes, name=name)\r\n  File \"/Users/szp/Documents/github/Tianchi_MedicalAI/venv/lib/python3.5/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 946, in _queue_dequeue_many_v2\r\n    timeout_ms=timeout_ms, name=name)\r\n  File \"/Users/szp/Documents/github/Tianchi_MedicalAI/venv/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/Users/szp/Documents/github/Tianchi_MedicalAI/venv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/Users/szp/Documents/github/Tianchi_MedicalAI/venv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nOutOfRangeError (see above for traceback): FIFOQueue '_3_batch/fifo_queue' is closed and has insufficient elements (requested 64, current size 0)\r\n\t [[Node: batch = QueueDequeueManyV2[component_types=[DT_FLOAT, DT_INT64], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](batch/fifo_queue, batch/n)]]\r\n```\r\n\r\n", "comments": ["(1) Can you confirm that you are using all features which are present in your database ?  If not then please include all in your code and try.\r\n(2) Please include the sample database's link online if the above trick fails.", "@szpssky Could you please elaborate on how you have solved this issue, since you have closed it. I am using the imagenet tfRecords generated by the script available at tf-slim for imagenet. everything works fine, but only when I use \"object/bbox\" and \"object/label\" in my provider.get method, I get empty values for them, which doesn't happen for image or label data. And there is no easy way to check that bbox data are actually included in the tfRecord data, other than using TF-slim DatasetDataProvider since it is in byte format. Could you please elaborate if you have done the same and it works smoothly for you or not, and whether you can read the bbox data."]}, {"number": 11492, "title": "R1.2", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I think this is an erroneous pull request. The 1.2.1 documentations has already been updated."]}, {"number": 11491, "title": "Adding a link and fixing up two titles", "body": "This should make it easier to find the newest documentation.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 11490, "title": "Branch 161868747", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 11489, "title": "import_graph_def input_map not updating all attributes", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Reproducible script below\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary/pip\r\n- **TensorFlow version (use command below)**: 1.2.1\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n```py\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import graph_util\r\nfrom tensorflow.core.framework import graph_pb2\r\n\r\norig_model = \"input_map_orig.pb\"\r\nmapped_model = \"input_map_modified.pb\"\r\n\r\n# Create a graph with uint8 placeholder that gets\r\n# cast to float32 before going through image whitening\r\ntf.reset_default_graph()\r\nimg = tf.placeholder(tf.uint8, shape=(1, None, None, 3), name=\"input_tensor\")\r\nfloat_img = tf.cast(img, tf.float32, \"ToFloat\")\r\nwhiten_fn = lambda img: tf.image.per_image_standardization(img)\r\nwhitened = tf.map_fn(whiten_fn, float_img, name='image_whitening')\r\nidentity = tf.identity(whitened, name=\"id\")\r\n\r\n# Write graph\r\noutput_names = [\"id\"]\r\nwith tf.Session() as sess:\r\n  input_graph_def = sess.graph_def\r\n  output_graph_def = graph_util.convert_variables_to_constants(sess,\r\n                                                               input_graph_def,\r\n                                                               output_names)\r\n  tf.train.write_graph(output_graph_def, \"./\", orig_model, False)\r\n\r\n# Load the graph again\r\ntf.reset_default_graph()\r\ngraph_def = graph_pb2.GraphDef()\r\nwith open(orig_model, \"rb\") as f:\r\n  graph_def.ParseFromString(f.read())\r\n\r\n# Rewire the graph with a float32 placeholder directly\r\nimg = tf.placeholder(tf.float32, shape=(1, None, None, 3), name=\"input\")\r\ninput_map = {\r\n  \"ToFloat\": img,\r\n  }\r\ntf.import_graph_def(graph_def, input_map=input_map, name=\"\")\r\n\r\n# Write the modified graph\r\ngraph = tf.get_default_graph()\r\noutput_names = [\"id\"]\r\nwith tf.Session(graph=graph) as sess:\r\n  graph = tf.get_default_graph()\r\n  input_graph_def = graph.as_graph_def()\r\n  output_graph_def = graph_util.convert_variables_to_constants(sess,\r\n                                                               input_graph_def,\r\n                                                               output_names)\r\n  tf.train.write_graph(output_graph_def, \"./\", mapped_model, False)\r\n\r\n# Try to load the modified graph\r\ntf.reset_default_graph()\r\ngraph_def = graph_pb2.GraphDef()\r\nwith open(mapped_model, \"rb\") as f:\r\n  graph_def.ParseFromString(f.read())\r\n\r\ng = tf.import_graph_def(graph_def)\r\n```\r\n\r\n### Describe the problem\r\nI noticed this issue trying to modify (via input_map) an object detection model (from [tf/models/object_detection](https://github.com/tensorflow/models/tree/master/object_detection)) to directly accept float32 instead of uint8. I have managed to reproduce the using the above minimal example. It seems like one of the attributes of one of the operations doesn't get fully updated. Possibly related to  https://github.com/tensorflow/tensorflow/issues/9925 .\r\n\r\n### Source code / logs\r\nTraceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"input_map_bug.py\", line 71, in <module>\r\n    g = tf.import_graph_def(graph_def)\r\n  File \"/home/s.antol/cv/py3virtualenvs/tf1.2/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 331, in import_graph_def\r\n    op_to_bind_to, node.name))\r\nValueError: Specified colocation to an op that does not exist during import: ToFloat in image_whitening/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3\r\n```\r\n\r\nLooking through the protobuf text version, the node below has `s: \"loc:@ToFloat\"` instead of (what I think should be) `s:\"loc:@input\"`:\r\n```\r\nnode {\r\n  name: \"image_whitening/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3\"\r\n  op: \"TensorArrayScatterV3\"\r\n  input: \"image_whitening/TensorArray\"\r\n  input: \"image_whitening/TensorArrayUnstack/range\"\r\n  input: \"input\"\r\n  input: \"image_whitening/TensorArray:1\"\r\n  attr {\r\n    key: \"T\"\r\n    value {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n  attr {\r\n    key: \"_class\"\r\n    value {\r\n      list {\r\n        s: \"loc:@ToFloat\"\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n", "comments": ["@sherrym Our friend is trying to manipulate a graph and believes he's encountered a bug.", "I believe that @vrv or I made a conscious decision not to update colocation constraint attrs in `tf.import_graph_def()`, so the fact that the `loc:@ToFloat` is left in there is \"working as intended\".\r\n\r\nI just tried to replicate this, and it looks like the error is not raised if you don't perform the second `graph_util.convert_variables_to_constants()`. It looks like that function leaves the graph in an inconsistent state (i.e. with a dangling colocation constraint) because it is unaware of colocation constraints.\r\n\r\n@petewarden Do you have any opinions on how colocation constraints should be handled in `graph_util.convert_variables_to_constants()`?", "@mrry, thanks for the (quick!) update. Your suggestion of removing `graph_util.convert_variables_to_constants()` was helpful in avoiding my hacky float32 placeholder->tf.cast(uint8) prepending. It seems like this fixing this issue would remove extra work-around steps when one is trying to save a model that isn't already turned into constants (e.g., from a checkpoint), so good luck figuring out the best solution.\r\n\r\nEDIT:\r\nActually, what exactly did you do when you don't perform the 2nd convert? I tried commenting out the convert and changing the write to `tf.train.write_graph(input_graph_def, \"./\", mapped_model, False)` (i.e., `input_graph_def` instead of `output_graph_def`) and, though it still loads, it doesn't cause an error because the 2 nodes that were unconnected are still a part of the graph, but the `loc@ToFloat` is still present in the scatter node, which I think would result in undesired behavior. (These are just my guesses based on my very limited understanding of TF graphs.)", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Sorry for the lack of activity on this issue. It doesn't look like we're likely to have a chance to dig more deeply, so I'm tagging @suharshs so he's aware of another freeze_graph issue, and closing it as unlikely to be fixed. Please update with more information if this is still a problem for you.", "I'm also encountering this issue, and have had to use a similar wrapping mechanism to work around it. Here are my steps to reproduce when using an embedding feature column:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport difflib\r\n\r\norig_model = \"input_map_orig.pb\"\r\nmapped_model = \"input_map_modified.pb\"\r\n\r\ntf.reset_default_graph()\r\n\r\ndef define_graph(use_placeholder, wrap):\r\n  feature = tf.feature_column.embedding_column(\r\n      tf.feature_column.categorical_column_with_identity(\"feature\", 10),\r\n      dimension=10)\r\n  feature_columns = [feature]\r\n  if use_placeholder:\r\n    features = tf.placeholder(dtype=tf.int32, name=\"input\")\r\n  else:\r\n    features = tf.convert_to_tensor([3], name=\"input\")\r\n  if wrap:\r\n    features = tf.identity(features, name=\"colocated\")\r\n  features = {\"feature\": features}\r\n  input_layer = tf.feature_column.input_layer(features, feature_columns)\r\n  int_data = tf.to_float(input_layer)\r\n  process_fn = lambda data: tf.square(data)\r\n  op = tf.map_fn(process_fn, int_data, name='process')\r\n  identity = tf.identity(op, name=\"id\")\r\n\r\n# Set 'wrap' to False to reproduce bug.\r\ndefine_graph(use_placeholder=True, wrap=True)\r\n\r\ndef freeze(graph, filename):\r\n  output_names = [\"id\"]\r\n  with tf.Session(graph=graph) as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    input_graph_def = sess.graph_def\r\n    output_graph_def = tf.graph_util.convert_variables_to_constants(\r\n        sess, input_graph_def, output_names)\r\n    tf.train.write_graph(output_graph_def, \"./\", filename, False)\r\n\r\ndef load_frozen(filename, input_map={}):\r\n  graph_def = tf.GraphDef()\r\n  with open(filename, \"rb\") as f:\r\n    graph_def.ParseFromString(f.read())\r\n  tf.import_graph_def(graph_def, input_map=input_map, name=\"\")\r\n\r\ndef make_change_in_graph():\r\n  op = tf.convert_to_tensor([3], name=\"input\") \r\n\r\n  input_map = {\r\n    \"input\": op,\r\n  }\r\n  return input_map\r\n\r\n# Save graph.\r\ngraph = tf.get_default_graph()\r\n# print(graph.as_graph_def())\r\ngraph_proto0 = str(tf.get_default_graph().as_graph_def()).splitlines()\r\nfreeze(graph, orig_model)\r\n\r\n# Load the graph again\r\ntf.reset_default_graph()\r\ninput_map = make_change_in_graph()\r\nload_frozen(orig_model, input_map)\r\n\r\ngraph_proto1 = str(graph.as_graph_def()).splitlines()\r\n# print('\\n'.join(difflib.ndiff(graph_proto0, graph_proto1)))\r\n\r\n\r\n# Write the modified graph\r\ngraph = tf.get_default_graph()\r\nfreeze(graph, mapped_model)\r\n\r\n# Load the graph again\r\ntf.reset_default_graph()\r\ninput_map = make_change_in_graph()\r\n\r\nload_frozen(mapped_model) # <------ Bug occurs on second load:\r\n\r\n# Write the modified graph\r\ngraph = tf.get_default_graph()\r\nfreeze(graph, mapped_model)\r\n\r\n# Load the graph again\r\ntf.reset_default_graph()\r\nload_frozen(mapped_model)\r\n```\r\n\r\nIs there any reason why the colocation op can't be overwritten?"]}, {"number": 11488, "title": "datset.md - Fix minor typo in code", "body": "Change tf.contrib.data.Dataset.TextLineDataset(filename) to tf.contrib.data.TextLineDataset(filename)", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please."]}, {"number": 11487, "title": "'return_state' in keras rnn layers", "body": "Im using tensorflow r1.2.\r\n In the original Keras, There's an option to get the last hidden state of a rnn layer. (\r\nhttps://github.com/fchollet/keras/blob/master/keras/layers/recurrent.py#L99 )\r\nAnd it disappeared in tensorflow version of the keras.\r\n(https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/contrib/keras/python/keras/layers/recurrent.py#L89)\r\n\r\nIs there any reason for this?? I think the 'return_state' option is quite useful.\r\n", "comments": ["@fchollet Here's a question about changes to Keras for you.", "Ah I just figured out that there is the 'return_state' option in the latest code.\r\nI'll wait for r1.3 :)", "Yes, it will be in the next release. It is already on `master`."]}, {"number": 11486, "title": "Link for iOS doc doesn't work", "body": "https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/ios_examples/", "comments": ["Thanks for the report. Could you point to where the source of the bad link was?", "Sure: https://www.tensorflow.org/mobile/", "Thanks!", "We are deprecating/updating that page soon, but I've fixed the link for you for now.", "Thank you very much. I'm happy to use whatever is the current reference. This is the page I was led to by tensorflow.org. Thanks"]}, {"number": 11485, "title": "Update CODEOWNERS, add owners to Windows, TPU components", "body": "", "comments": []}, {"number": 11484, "title": "tf.device with function leads to incompatible device names", "body": "### System information\r\n- **OS Platform and Distribution: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: ('v1.2.0-rc2-21-g12f033d', '1.2.0')\r\n- **Python version**:  2\r\n\r\n```python\r\nimport tensorflow as tf\r\nwith tf.device('/gpu:0'):\r\n    x = tf.constant([1, 2, 3])\r\n    print(x.device)\r\nwith tf.device(lambda op: '/gpu:0'):\r\n    y = tf.constant([1, 2, 3])\r\n    print(y.device)\r\nG = tf.get_default_graph()\r\nwith G.colocate_with(y):\r\n    with tf.device('/gpu:0'):\r\n        z = tf.constant([1,2,3])\r\n        print(z.device)\r\n```\r\n\r\nOutputs:\r\n```\r\n/device:GPU:0\r\n/gpu:0\r\nWARNING:tensorflow:Tried to colocate Const_2 with an op Const_1 that had a different device: /device:GPU:0 vs /gpu:0. Ignoring colocation property.\r\n/device:GPU:0\r\n```\r\n\r\nWhen using device function, '/gpu:0' doesn't get translated to '/device:GPU:0'. This can lead to very confusing warning messages when used together with colocate.", "comments": ["@vrv I'm not sure who has taken this over from you. The names should be canonicalized I guess.", "eh, I added the code recently, I'll fix this one.", "https://github.com/tensorflow/benchmarks/issues/44\r\nSeems like the device names are not canonicalized in other places either."]}]