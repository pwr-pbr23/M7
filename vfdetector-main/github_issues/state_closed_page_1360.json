[{"number": 12276, "title": "Add annotations support for tf.estimator.Estimator", "body": "This fix adds annotations support for tf.estimator.Estimator so that the following works in python 3:\r\n```\r\nimport tensorflow as tf\r\n\r\ndef model_fn(features: dict, labels: tf.Tensor, mode: str):\r\n    pass\r\n\r\nestimator = tf.estimator.Estimator(model_fn)\r\n```\r\n\r\nThis fix fixes #12249.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@martinwicke I traced through the model_fn and the call with args. The model_fn is actually called with the args directly so as you pointed out the unwrapping may not be appropriate here.\r\n\r\nI updated the PR and now the getfullargspec is called to the function directly.\r\n\r\nPlease take a look and let me know if there are any issues. ", "@martinwicke take another look?", "@martinwicke ping", "So this works. I'm sorry I wasn't clear before, it is good to use tf_inspect instead of inspect, so it can use TFDecorator decorated functions as model_fn. \r\n\r\nTo do that, use tf_inspect. getargspec, and ideally make that function compatible with py3 by adding the switch for getfullargspec as you did inline.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/util/tf_inspect.py#L32", "Hi, this also happens to me when I use tf.contrib.variance_scaling_initializer()\r\n\r\n`initializer = tf.contrib.variance_scaling_initializer()\r\n\r\n  File \"/Users/WeeYin/anaconda/lib/python3.5/inspect.py\", line 1050, in getargspec\r\n    raise ValueError(\"Function has keyword-only arguments or annotations\"\r\n\r\nValueError: Function has keyword-only arguments or annotations, use getfullargspec() API which can support them`", "@Wee7 I am not able to reproduce with the current head:\r\n```python\r\nubuntu@ubuntu:~$ python3\r\nPython 3.5.2 (default, Nov 17 2016, 17:05:23) \r\n[GCC 5.4.0 20160609] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> initializer = tf.contrib.layers.variance_scaling_initializer()\r\n```\r\n\r\n```python\r\nubuntu@ubuntu:~$ python\r\nPython 2.7.12 (default, Nov 19 2016, 06:48:10) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> initializer = tf.contrib.layers.variance_scaling_initializer()\r\n>>>\r\n```\r\n\r\nIn case you are able to reproduce, could you open a separate issue and list the details steps to reach the error?", "@martinwicke Thanks for the review. The PR has been updated. Please take a look.", "Sorry for the delay, can you update the PR and resolve the conflicts? ", "Also, there's one unresolved comment (use of \"object\" as a arg name)", "Thanks @martinwicke. The PR has been rebased and `object` has been changed to `obj` as well. Please take a look.", "Jenkins, test this please.", "@tensorflow-jenkins test this please", "ignoring stale Kokoro."]}, {"number": 12275, "title": "Add missing 'type' keyword to ArgumentParser add_argument", "body": "Fixes #12210", "comments": ["@vrv, thanks for your PR! By analyzing the history of the files in this pull request, we identified @nfiedel, @tensorflower-gardener and @sukritiramesh to be potential reviewers."]}, {"number": 12274, "title": "Updating CUB urls for r1.3 branch.", "body": "", "comments": []}, {"number": 12273, "title": "Fix strides format for data format in contrib.layers.separable convolution2d", "body": "Addresses feature request from issue #10432. Continuation of feature implementation--initial changes did not address the case where num_outputs = None, in which case strides and the input channels must be reformulated according to data format. The previous commit is  rmlarsen@d52e15a.\r\n\r\nI created a previous PR #12120 for a similar issue that @rmlarsen fixed in the above commit, but the strides were still mismatched to data format--I thought it would be cleaner if I just made a new PR.\r\n\r\nEdit--I thought it would be cleaner because my CLA would be recognized, but it still hasn't. See the comment below.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I don't know why my CLA is failing... The commits list my CLA email: joe@ctxrobotics.com. When I click the redirect link from the CLA bot, it shows I am covered under a corporate CLA. As far as I am aware, I have taken the necessary steps to be certified under the CLA. Anybody know what I am missing here?", "@rmlarsen Is there any movement on this issue? It's really just a one line fix plus the test update, but until it is merged the channels first separable convolution from contrib is wrong.", "Keep in mind that we can't look at the code until the CLA is resolved. It looks like you have commits without e-mails in the list of commits. Perhaps resolve that?", "@jcastagneri any luck on the CLA? We'll have to close this PR otherwise. Thanks.", "@jcastagneri It might be useful to try checking the CLA incognito to make sure it is validating the correct account.", "Ah, @jcastagneri you have signed via a company. You need to have the company on your public github profile for the CLA checking to work. Thanks.", "Thanks for the fix @drpngx. I added the company name, but the company is not actually on Github. Will this still block the CLA bot? I just added 'ctxrobotics' in the company section of my bio, as this is what I believe to be the company name registered to the CLA.", "@willnorris for knowledge of how exactly company CLAs work.\r\n\r\nI believe your company has to have the email on file with the CLA, and that email must be the same as the one used in the commits.", "@jcastagneri: it looks like you need to add your ctx robotics email [to your github account](https://help.github.com/articles/adding-an-email-address-to-your-github-account/).", "I have just added the email. Do I have to kickoff the CLA bot manually or will it check automatically?\r\n\r\nEdit: CLAs are passed!", "CLAs look good, thanks!\n\n<!-- ok -->", "Great, I think it's ready for review now.\r\n\r\n@rmlarsen it looks like this is something you might have some prior knowledge about?", "Jenkins. test this please."]}, {"number": 12272, "title": "Update requirement for numpy.", "body": "Autograd requires numpy >= 1.12.1. If numpy 1.11.0 is already installed on the system, pip will not upgrade numpy when installing tensorflow.  Fix #12185.", "comments": ["@yifeif, thanks for your PR! By analyzing the history of the files in this pull request, we identified @martinwicke, @keveman and @tensorflower-gardener to be potential reviewers."]}, {"number": 12271, "title": "Updating the hashes for NVlabs/cub package.", "body": "PR to fix master cmake builds.", "comments": ["Jenkins, test this please."]}, {"number": 12270, "title": "tensorboard 0.1.1: No such file or directory: '/usr/local/lib/python3.4/dist-packages/tensorflow/tensorboard/TAG'", "body": "Just upgrade to TF 1.3 and tensorboard 0.1.1 from Pypi. Ubuntu 14.04 64 bit and python 3.4. \r\n\r\nRunning tensorboard gives me the following error: \r\n```\r\nweiliu@roundvalley:~/projects/seismic$ tensorboard --logdir ./summay_2d\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/tensorboard\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/weiliu/.local/lib/python3.4/site-packages/tensorboard/main.py\", line 219, in main\r\n    tb = create_tb_app(plugins)\r\n  File \"/home/weiliu/.local/lib/python3.4/site-packages/tensorboard/main.py\", line 122, in create_tb_app\r\n    plugins=plugins)\r\n  File \"/home/weiliu/.local/lib/python3.4/site-packages/tensorboard/backend/application.py\", line 83, in standard_tensorboard_wsgi\r\n    return TensorBoardWSGIApp(logdir, plugins, multiplexer, reload_interval)\r\n  File \"/home/weiliu/.local/lib/python3.4/site-packages/tensorboard/backend/application.py\", line 121, in __init__\r\n    self.tag = get_tensorboard_tag()\r\n  File \"/home/weiliu/.local/lib/python3.4/site-packages/tensorboard/backend/application.py\", line 372, in get_tensorboard_tag\r\n    tag = tf.resource_loader.load_resource('tensorboard/TAG').strip()\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/platform/resource_loader.py\", line 50, in load_resource\r\n    with open(path, 'rb') as f:\r\nFileNotFoundError: [Errno 2] No such file or directory: '/usr/local/lib/python3.4/dist-packages/tensorflow/tensorboard/TAG'\r\nError in sys.excepthook:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3/dist-packages/apport_python_hook.py\", line 63, in apport_excepthook\r\n    from apport.fileutils import likely_packaged, get_recent_crashes\r\n  File \"/usr/lib/python3/dist-packages/apport/__init__.py\", line 5, in <module>\r\n    from apport.report import Report\r\n  File \"/usr/lib/python3/dist-packages/apport/report.py\", line 30, in <module>\r\n    import apport.fileutils\r\n  File \"/usr/lib/python3/dist-packages/apport/fileutils.py\", line 23, in <module>\r\n    from apport.packaging_impl import impl as packaging\r\n  File \"/usr/lib/python3/dist-packages/apport/packaging_impl.py\", line 20, in <module>\r\n    import apt\r\n  File \"/usr/lib/python3/dist-packages/apt/__init__.py\", line 34, in <module>\r\n    apt_pkg.init_config()\r\nSystemError: E:Opening configuration file /etc/apt/apt.conf - ifstream::ifstream (13: Permission denied)\r\n\r\nOriginal exception was:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/tensorboard\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/weiliu/.local/lib/python3.4/site-packages/tensorboard/main.py\", line 219, in main\r\n    tb = create_tb_app(plugins)\r\n  File \"/home/weiliu/.local/lib/python3.4/site-packages/tensorboard/main.py\", line 122, in create_tb_app\r\n    plugins=plugins)\r\n  File \"/home/weiliu/.local/lib/python3.4/site-packages/tensorboard/backend/application.py\", line 83, in standard_tensorboard_wsgi\r\n    return TensorBoardWSGIApp(logdir, plugins, multiplexer, reload_interval)\r\n  File \"/home/weiliu/.local/lib/python3.4/site-packages/tensorboard/backend/application.py\", line 121, in __init__\r\n    self.tag = get_tensorboard_tag()\r\n  File \"/home/weiliu/.local/lib/python3.4/site-packages/tensorboard/backend/application.py\", line 372, in get_tensorboard_tag\r\n    tag = tf.resource_loader.load_resource('tensorboard/TAG').strip()\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/platform/resource_loader.py\", line 50, in load_resource\r\n    with open(path, 'rb') as f:\r\nFileNotFoundError: [Errno 2] No such file or directory: '/usr/local/lib/python3.4/dist-packages/tensorflow/tensorboard/TAG'\r\n```\r\n\r\nI checked `/usr/local/lib/python3.4/dist-packages/tensorflow/` and didn't find `tensorboard` folder there. \r\n\r\nHere is related `pip3 list` output: \r\n```\r\ntensorflow-gpu (1.3.0rc2)\r\ntensorflow-tensorboard (0.1.1)\r\n``` \r\n\r\nEDIT: I realize this is more like a issue of Tensorboard than Tensorflow. Please feel free to move it to correct place if that helps solve it. ", "comments": ["let me move it to Tensorboard . "]}, {"number": 12269, "title": "Clean up in sparse_fill_empty_rows_op.cc", "body": "Both `int64 prev_row = -1;` and `const int64 row = indices(i, 0);`\r\nwas not used. It generates quite a few warnings in the output.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@yongtang, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ebrevdo and @yifeif to be potential reviewers.", "Jenkins, test this please", "Jenkins, test this please", "Jenkins, test this please", "Jenkins, test this please"]}, {"number": 12268, "title": "add args to control padding in the definition of pretrained model in slim", "body": "in some situation, the outputs of \"same\" and \"valid\" padding are same for original model, but when do fine-tuning with a new model, they are not,  adding an arg to control padding is meaningful.", "comments": ["Can you clarify what you mean? What do you mean by adding an argument to control padding?", "Thanks! @reedwm \r\n\r\nThis is the source code of definition of vgg16 in slim:\r\n```\r\ndef vgg_16(inputs,\r\n           num_classes=1000,\r\n           is_training=True,\r\n           dropout_keep_prob=0.5,\r\n           spatial_squeeze=True,\r\n           scope='vgg_16'):\r\n  \"\"\"Oxford Net VGG 16-Layers version D Example.\r\n  Note: All the fully_connected layers have been transformed to conv2d layers.\r\n        To use in classification mode, resize input to 224x224.\r\n  Args:\r\n    inputs: a tensor of size [batch_size, height, width, channels].\r\n    num_classes: number of predicted classes.\r\n    is_training: whether or not the model is being trained.\r\n    dropout_keep_prob: the probability that activations are kept in the dropout\r\n      layers during training.\r\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\r\n      outputs. Useful to remove unnecessary dimensions for classification.\r\n    scope: Optional scope for the variables.\r\n  Returns:\r\n    the last op containing the log predictions and end_points dict.\r\n  \"\"\"\r\n  with variable_scope.variable_scope(scope, 'vgg_16', [inputs]) as sc:\r\n    end_points_collection = sc.original_name_scope + '_end_points'\r\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\r\n    with arg_scope(\r\n        [layers.conv2d, layers_lib.fully_connected, layers_lib.max_pool2d],\r\n        outputs_collections=end_points_collection):\r\n      net = layers_lib.repeat(\r\n          inputs, 2, layers.conv2d, 64, [3, 3], scope='conv1')\r\n      net = layers_lib.max_pool2d(net, [2, 2], scope='pool1')\r\n      net = layers_lib.repeat(net, 2, layers.conv2d, 128, [3, 3], scope='conv2')\r\n      net = layers_lib.max_pool2d(net, [2, 2], scope='pool2')\r\n      net = layers_lib.repeat(net, 3, layers.conv2d, 256, [3, 3], scope='conv3')\r\n      net = layers_lib.max_pool2d(net, [2, 2], scope='pool3')\r\n      net = layers_lib.repeat(net, 3, layers.conv2d, 512, [3, 3], scope='conv4')\r\n      net = layers_lib.max_pool2d(net, [2, 2], scope='pool4')\r\n      net = layers_lib.repeat(net, 3, layers.conv2d, 512, [3, 3], scope='conv5')\r\n      net = layers_lib.max_pool2d(net, [2, 2], scope='pool5')\r\n      # Use conv2d instead of fully_connected layers.\r\n      net = layers.conv2d(net, 4096, [7, 7], padding='VALID', scope='fc6')\r\n      net = layers_lib.dropout(\r\n          net, dropout_keep_prob, is_training=is_training, scope='dropout6')\r\n      net = layers.conv2d(net, 4096, [1, 1], scope='fc7')\r\n      net = layers_lib.dropout(\r\n          net, dropout_keep_prob, is_training=is_training, scope='dropout7')\r\n      net = layers.conv2d(\r\n          net,\r\n          num_classes, [1, 1],\r\n          activation_fn=None,\r\n          normalizer_fn=None,\r\n          scope='fc8')\r\n      # Convert end_points_collection into a end_point dict.\r\n      end_points = utils.convert_collection_to_dict(end_points_collection)\r\n      if spatial_squeeze:\r\n        net = array_ops.squeeze(net, [1, 2], name='fc8/squeezed')\r\n        end_points[sc.name + '/fc8'] = net\r\n      return net, end_points\r\n```\r\nAs you can see,  both functions(conv2d and maxpooling) use default padding argument which is \"same\" for conv2d and \"valid\" for maxpooling, for an input image with shape `[None, 224, 224, 3]` where 224 is a multiple of 32, \"valid\" padding of maxpooling has the same effect with \"same\" padding, but if the shape is `[None, 300, 300, 3]`(which is the input shape of ssd), output shape of \"valid\" padding is different from \"same\", so I want an argument in the definition of `vgg_16` to determine whether the padding for maxpooling is \"same\" or \"valid\".", "The `vgg_16` model assumes its input is 224x224. If you want to run it on images of a different size, you must first resize it to 224x224. Note the comment in the docstring which states \"To use in classification mode, resize input to 224x224.\"", "![image](http://joshua881228.webfactional.com/media/uploads/ReadingNote/arXiv_SSD/SSD.png)\r\n\r\nssd chooses vgg 16 as base network, if the padding is \"valid\" which is default in the definition, the conv4_3 layer shape should be `[None, 37, 37, some_channels]`  which is not the correct shape in ssd model(should be `[None, 38, 38, some_channels]`). adding an argument to change the padding from \"valid\" to \"same\" can solve this.", "I see, so you're feeding in larger images to vgg16, which causes each layer to have a larger height and width. And you want an option to change the padding of pool layers to \"SAME\" because the [SSD model](https://arxiv.org/abs/1512.02325) requires it when using vgg16 as a base.\r\n\r\n@sguada what do you think of this proposal?", "@zakizhou I'm not sure if you want to change the padding of the last max_pool layer of all of them, can you clarify what is the change you propose?", "@sguada all pooling layers, not just the last one, for the first 3 pooling layers, \"valid\" padding works exactly same with \"same\" padding.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Having \"VALID\" for max_pooling with kernel size [2, 2] would change the size of the output, while \"SAME\" would not.\r\n\r\nIt seems that changing the polling of all max_poling layers would have different effects depending on the input size, so I guess it would be hard to control all possible cases.\r\n\r\nYou can build your own version of VGG with the padding that you need, isn't it? ", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 12267, "title": "[XLA] Add F16 as type for the literal conversion functions", "body": "The F16 (synonym for eigen half) type was not included in the literal conversion functions.  float/double is supported as a type conversion by the eigen half, so adding identical lines as the native types.\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "mm.  that's odd.  i think our buildbot runs that test.  I'll double check.\r\n", "i see.  we run tensorflow/compiler/xla/tests..., and not tensorflow/compiler/xla:...\r\n\r\nwill fix asap.\r\n", "fixed.\r\n", "Jenkins, test this please."]}, {"number": 12266, "title": "compile using `bazel test`  failed on mac", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac 10.11.6 \r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: 0.5.3-homebrew\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nIt seems that tensorflow cannot work with clang of Mac.  And even though I install gcc 4.8 with brew, and set `export CC=gcc_4_8_install_dir`,  compile still failed. \r\n\r\n```bash\r\n~ \u276f\u276f\u276f clang --version\r\nApple LLVM version 8.0.0 (clang-800.0.42.1)\r\nTarget: x86_64-apple-darwin15.6.0\r\nThread model: posix\r\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n```\r\n\r\n### Source code / logs\r\n\r\n```python\r\n~/W/g/tensorflow \u276f\u276f\u276f bazel test -c opt //tensorflow/contrib/learn:estimators_test\r\nWARNING: /Users/facai/Workshop/github/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /Users/facai/Workshop/github/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nINFO: Found 1 test target...\r\nERROR: /private/var/tmp/_bazel_facai/c1230027f58dd63b64621179de2d0b21/external/boringssl/BUILD:116:1: C++ compilation of rule '@boringssl//:crypto' failed (Exit 1).\r\nerror: unknown warning option '-Wunused-but-set-parameter'; did you mean '-Wunused-parameter'? [-Werror,-Wunknown-warning-option]\r\nerror: unknown warning option '-Wno-free-nonheap-object'; did you mean '-Wno-sequence-point'? [-Werror,-Wunknown-warning-option]\r\nTarget //tensorflow/contrib/learn:estimators_test failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1.079s, Critical Path: 0.07s\r\n\r\nExecuted 0 out of 1 test: 1 fails to build.\r\n```\r\n", "comments": ["@gunan do you know what the problem is?", "```\r\nERROR: /private/var/tmp/_bazel_facai/c1230027f58dd63b64621179de2d0b21/external/boringssl/BUILD:116:1: C++ compilation of rule '@boringssl//:crypto' failed (Exit 1).\r\nerror: unknown warning option '-Wunused-but-set-parameter'; did you mean '-Wunused-parameter'? [-Werror,-Wunknown-warning-option]\r\nerror: unknown warning option '-Wno-free-nonheap-object'; did you mean '-Wno-sequence-point'? [-Werror,-Wunknown-warning-option]\r\n```\r\n\r\nThis is the problem. Maybe we setup our  build on MAC to specificly use clang? not sure why these errors are being thrown. Did you try googling these error messages?", "Thanks for your quick reply.\r\n\r\nGCC is 4.2.1 in Mac, while tensorflow works well without any warning on ubuntu (GCC 5.4.0).  Hence I guess that these warnings are related to gcc version. \r\n\r\nIt seems that many people try to update gcc, while most get in trouble, see #336, #75, #649.\r\n\r\nIn all, if tensorflow supports to compile with the default compiler on Mac (or more detailed  Instruction about how to test on Mac for developer), life will be easy.", "Just piping up with some anecdotal (and so useless) news, but when i run tests (e.g `bazel test //tensorflow/c:c_api_test`) on my Mac (10.12.6) with current XCode and no attempted gcc updates, it uses clang successfully.\r\n```\r\nautogenic:tensorflow xx$ clang -v\r\nApple LLVM version 8.1.0 (clang-802.0.42)\r\nTarget: x86_64-apple-darwin16.7.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\nautogenic:tensorflow xx$ \r\n```\r\n\r\nThe only peeps of complaint regarding flags that i see are\r\n```\r\nclang: warning: argument unused during compilation: '-pthread' [-Wunused-command-line-argument]\r\n```\r\n\r\nI also just reran the c_api_test including `-c opt` to see if that triggered a similar warning but saw no new problems.", "As @quaeler stated, we build and test TensorFlow on MacOS using default clang that comes with Xcode. Our build system has some hardcoded compile and link options based on OS, and the MacOS options in the system may not match what GCC on MacOS provides.\r\n\r\n@facaiy Would it be possible for you to try using clang (that is distributed with Xcode) on your machine instead of GCC?", "Thanks, @gunan and @quaeler .\r\n\r\nI indeed use default clang, sorry for misunderstanding. However, my mac is 10.11.6, which is older than @quaeler . Perhaps upgrading system can solve my problem, I'll try to upgrade later.", "I tried briefly to find a changelog which might describe what was changed between Apple's clang 800.0.42.1 and 802.0.42 but could not.\r\n\r\nI noticed the issue you cited cropped up during the build of BoringSSL, and there is also this open issue https://github.com/tensorflow/tensorflow/issues/12123 so perhaps you are both experiencing the same problem?", "@quaeler I agree with you. #12123 is exactly the same issue. I'll close the issue because it is a duplicate of #12123. \r\n\r\nThanks.", "Just noticing I too am seeing this issue as well. In my case, clang version is:\r\n```\r\ngregde@gregde-macbookpro:~/tensorflow$ clang -v\r\nApple LLVM version 8.1.0 (clang-802.0.42)\r\nTarget: x86_64-apple-darwin16.7.0\r\nThread model: posix\r\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n```"]}, {"number": 12265, "title": "Android demo - SDD-Mobilenet model download failed", "body": "**System information**\r\n\r\nOS Platform and Distribution:Windows 10\r\nTensorFlow installed from:Source\r\nTensorFlow version:master branch\r\nPython version: 2.7.12\r\nBazel version:NA\r\nCUDA/cuDNN version:NA\r\nGPU model and memory:NA\r\n\r\n**Describe the problem**\r\nHi,\r\nI'm trying to compile the android demo example project just released the August 11, 2017.\r\nI'm building it with Android Studio and gradlle.\r\nThe build fails downloading the SSD-Mobilenet model with this error : \r\n```\r\nExecution failed for task ':downloadFile'.\r\n> org.apache.http.client.ClientProtocolException: Forbidden\r\n```\r\n\r\nI can download the other models, but not this one.\r\nHow can I fix it ?\r\nThanks", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "It looks like the model archive is available at:\r\n\r\nhttp://storage.googleapis.com/download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_android_export.zip, while the download-models.gradle script expects it at:\r\n http://storage.googleapis.com/download.tensorflow.org/models/ssd_mobilenet_v1_android_export.zip.\r\n\r\n@jch1 @martinwicke All of our other models (include multibox, which is also for object detection) are just under the models/ dir -- is there a strong argument for keeping this one under models/object_detection?\r\n\r\nBesides copying the file, the most straightforward fix may be to just add \"object_detection/\" to the beginning of the entry in [download-models.gradle](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/download-models.gradle#L13).\r\n", "I think in general it would be better to place new models in subfolders, in particular if they have several files. We also shouldn't move downloadable assets around. \r\n\r\nI would fix the gradle file, that seems like the simplest solution.", "I ran into this just now, and as a temporary fix (since it's blocking anyone from building the Android demos with the top-of-tree code) I've copied the relevant file to the location it's expected. We should also fix the code, but the simplest fix (adding a prefix to the name) failed because it's used as a file name for the destination of the download.", "I've fixed this internally using a split to separate the filename from the url -- should be included in the next push.", "Perfect ! \r\nThanks."]}, {"number": 12263, "title": "How do I build a whole compute graph  and Variables/Weights  arguments then training it in C++?", "body": "", "comments": []}, {"number": 12262, "title": "Where is gen_logging_ops source code?", "body": "Hi,\r\n\r\nI have a question about this import from tensorflow/tensorflow/python/summary/summary.py line 53:\r\n`from tensorflow.python.ops import gen_logging_ops as _gen_logging_ops`\r\n\r\nWhen I go to the directory  tensorflow/python/ops, there is no such file called gen_logging_ops.\r\n\r\nI am wondering how this worked out? BTW, I am tracking the r1.3 version.\r\n\r\nAny suggestion is highly appreciated!\r\n\r\n\r\nBest wishes,\r\nBinhang \r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12261, "title": "Using Bazel to bulid a tensorflow C++ source in Windows, but stuck at this problem. ", "body": "OS Platform and Distribution : Windows 10\r\nBazel version :0.5.3\r\n\r\nI download the source on https://github.com/tensorflow/tensorflow, and I want to build the example from it. But when I use command \"bazel build ...\", it prompt error.\r\n\r\n(tensorflow) D:\\FsCode\\tensorflow>bazel build tensorflow/examples/label_image/...\r\nERROR: D:/fscode/tensorflow/tensorflow/core/BUILD:1528:1: no such target '//tensorflow/tools/git:gen/spec.json': target 'gen/spec.json' not declared in package 'tensorflow/tools/git' defined by D:/fscode/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: D:/fscode/tensorflow/tensorflow/core/BUILD:1528:1: no such target '//tensorflow/tools/git:gen/head': target 'gen/head' not declared in package 'tensorflow/tools/git' defined by D:/fscode/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: D:/fscode/tensorflow/tensorflow/core/BUILD:1528:1: no such target '//tensorflow/tools/git:gen/branch_ref': target 'gen/branch_ref' not declared in package 'tensorflow/tools/git' defined by D:/fscode/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: Analysis of target '//tensorflow/examples/label_image:label_image' failed; build aborted.\r\nINFO: Elapsed time: 1.569s\r\n\r\nI search the problem on issues. Somebody suggest to run ./configure. But in Windows system, this can not be run.\r\n", "comments": ["I found the answer. I have installed msys2, which includes a bash shell. This shell can run command ./configure, and use this shell to build.\r\n\r\nThe next problem is compile error:\r\n`.\\tensorflow/core/platform/profile_utils/cpu_utils.h(46) : error C2146: \u8bed\u6cd5\u9519\u8bef: \u7f3a\u5c11\u201c;\u201d(\u5728\u6807\u8bc6\u7b26\u201cint64\u201d\u7684\u524d\u9762)\r\n.\\tensorflow/core/platform/profile_utils/cpu_utils.h(46) : error C4430: \u7f3a\u5c11\u7c7b\u578b\u8bf4\u660e\u7b26 - \u5047\u5b9a\u4e3a int\u3002\u6ce8\u610f:  C++ \u4e0d\u652f\u6301\u9ed8\u8ba4 int\r\n.\\tensorflow/core/platform/profile_utils/cpu_utils.h(47) : error C2146: \u8bed\u6cd5\u9519\u8bef: \u7f3a\u5c11\u201c;\u201d(\u5728\u6807\u8bc6\u7b26\u201cuint64\u201d\u7684\u524d\u9762)\r\n.\\tensorflow/core/platform/profile_utils/cpu_utils.h(47) : error C4430: \u7f3a\u5c11\u7c7b\u578b\u8bf4\u660e\u7b26 - \u5047\u5b9a\u4e3a int\u3002\u6ce8\u610f:  C++ \u4e0d\u652f\u6301\u9ed8\u8ba4 int\r\n.\\tensorflow/core/platform/profile_utils/cpu_utils.h(47) : error C2086: \u201cint tensorflow::profile_utils::CpuUtils::constexpr\u201d: \u91cd\u5b9a\u4e49\r\n        .\\tensorflow/core/platform/profile_utils/cpu_utils.h(46) : \u53c2\u89c1\u201ctensorflow::profile_utils::CpuUtils::constexpr\u201d\u7684 \u58f0\u660e\r\n.\\tensorflow/core/platform/profile_utils/cpu_utils.h(93) : error C2597: \u5bf9\u975e\u9759\u6001\u6210\u5458\u201ctensorflow::profile_utils::CpuUtils::DUMMY_CYCLE_CLOCK\u201d\u7684\u975e\u6cd5\u5f15\u7528\r\n.\\tensorflow/core/platform/profile_utils/cpu_utils.h(129) : error C2327: \u201ctensorflow::profile_utils::CpuUtils::DUMMY_CYCLE_CLOCK\u201d: \u4e0d\u662f\u7c7b\u578b\u540d\u79f0\u3001\u9759\u6001\u6216\u679a\u4e3e\u6570\r\n.\\tensorflow/core/platform/profile_utils/cpu_utils.h(129) : error C2065: \u201cDUMMY_CYCLE_CLOCK\u201d: \u672a\u58f0\u660e\u7684\u6807\u8bc6\u7b26\r\n.\\tensorflow/core/platform/profile_utils/cpu_utils.h(131) : error C2327: \u201ctensorflow::profile_utils::CpuUtils::INVALID_FREQUENCY\u201d: \u4e0d\u662f\u7c7b\u578b\u540d\u79f0\u3001\u9759\u6001\u6216\u679a\u4e3e\u6570\r\n.\\tensorflow/core/platform/profile_utils/cpu_utils.h(131) : error C2065: \u201cINVALID_FREQUENCY\u201d: \u672a\u58f0\u660e\u7684\u6807\u8bc6\u7b26\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:  C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\INCLUDE\\mutex\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:   C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\INCLUDE\\thr/xthread\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:    C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\INCLUDE\\thr/xtime\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:    C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\INCLUDE\\thr/xthreads.h\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:   C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\INCLUDE\\functional\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:    C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\INCLUDE\\xfunctional\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:    C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\INCLUDE\\tuple\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:  .\\tensorflow/core/platform/logging.h\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:   .\\tensorflow/core/platform/default/logging.h\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:    C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\INCLUDE\\sstream\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:  .\\tensorflow/core/platform/profile_utils/android_armv7a_cpu_utils_helper.h\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:   C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\INCLUDE\\sys/types.h\r\ntensorflow/core/platform/profile_utils/cpu_utils.cc(27) : error C2146: \u8bed\u6cd5\u9519\u8bef: \u7f3a\u5c11\u201c;\u201d(\u5728\u6807\u8bc6\u7b26\u201cint64\u201d\u7684\u524d\u9762)\r\ntensorflow/core/platform/profile_utils/cpu_utils.cc(27) : error C4430: \u7f3a\u5c11\u7c7b\u578b\u8bf4\u660e\u7b26 - \u5047\u5b9a\u4e3a int\u3002\u6ce8\u610f:  C++ \u4e0d\u652f\u6301\u9ed8\u8ba4 int\r\ntensorflow/core/platform/profile_utils/cpu_utils.cc(27) : error C2761: \u201ctensorflow::int64 tensorflow::profile_utils::CpuUtils::INVALID_FREQUENCY\u201d: \u4e0d\u5141\u8bb8\u6210\u5458\u51fd\u6570\u91cd\u65b0\u58f0\u660e\r\ntensorflow/core/platform/profile_utils/cpu_utils.cc(103) : error C2597: \u5bf9\u975e\u9759\u6001\u6210\u5458\u201ctensorflow::profile_utils::CpuUtils::INVALID_FREQUENCY\u201d\u7684\u975e\u6cd5\u5f15\u7528\r\nINFO: Elapsed time: 48.502s, Critical Path: 10.16s`\r\n\r\nI will try to use VS2015. \r\n\r\nContinue...", "Thank you for posting your solution! We suggest that people post to [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) with build problems, as GitHub issues are reserved for bugs and feature requests. Thanks!"]}, {"number": 12260, "title": "Hexagon build fails, readme.md needs to be revisited.", "body": "OS: Ubuntu 16.04 64bits\r\nAndroid Version: 7.1 (Nougat)\r\nNDK Version: android-ndk-r12b\r\n\r\nHexagon build readme should be revisited after recent code changes.\r\n\r\n\r\nI used same command as given in the readme.md\r\n`\r\ntensorflow/tensorflow/contrib/makefile/build_all_android.sh -x /home/kzos/TFHEXLIBS -t hexagon_graph_execution -s /home/kzos/experiment/tensorflow/tensorflow/contrib/makefile/sub_makefiles/hexagon_graph_execution/Makefile.in\r\n`\r\n\r\nI am getting below error:\r\n'\r\ntensorflow/contrib/makefile/Makefile:46: *** \"hexagon is only supported on Android\".  Stop.\r\n`\r\n\r\n\r\n", "comments": ["@satok16 , mind to share quick build-command/script for hexagon before you publish ?", "The code has not been changed while.  Try checking the value in TARGET.", "@satok16 \r\nI am checking, \r\nwere you able to get the hexagon_graph_execution built using same command mentioned in README?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 12259, "title": "tensorflow compile error", "body": "When I run :bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 --config=opt --config=cuda //tensorflow/...\r\nERROR: /home/wangmeng/tools/tensorflow/tensorflow/compiler/xla/tools/BUILD:109:1: Linking of rule '//tensorflow/compiler/xla/tools:replay_computation_hlo_evaluator' failed (Exit 1)\r\nbazel-out/local_linux-opt/bin/tensorflow/compiler/plugin/executor/libplugin_lib.lo(device.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#1}::_FUN(tensorflow::OpKernelConstruction*)':\r\ndevice.cc:(.text._ZN10tensorflowUlPNS_20OpKernelConstructionEE_4_FUNES1_+0x1e): undefined reference to `tensorflow::XlaDeviceLaunchOp::XlaDeviceLaunchOp(tensorflow::OpKernelConstruction*)'\r\ncollect2: error: ld returned 1 exit status\r\nINFO: Elapsed time: 61.551s, Critical Path: 36.75s\r\nFAILED: Build did NOT complete successfully\r\n\r\ntensorflow r1.3,bazel 0.5.3\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@wm901115nwpu do you have an answer for this?"]}, {"number": 12258, "title": "variable_scope.name is not displayed correctly", "body": "![snip20170814_1](https://user-images.githubusercontent.com/25893395/29259398-45c4299a-80f5-11e7-854e-6cdcfd76f5d7.png)\r\n\r\nAs shown, scope2's name should be new_scope_1\r\n", "comments": ["No. `new_scope_1` is the name of the name scope. The name of the variable scope is still `new_scope`."]}, {"number": 12257, "title": "Add focal_loss", "body": "Add a new loss function, focal_loss, which was described in [this paper](https://arxiv.org/pdf/1708.02002.pdf).", "comments": ["Can one of the admins verify this patch?", "@Kongsea, thanks for your PR! By analyzing the history of the files in this pull request, we identified @fchollet, @tensorflower-gardener and @drpngx to be potential reviewers.", "This PR adds a new loss in core TF, `focal_loss`, which was described in an unreviewed paper that was released on Arxiv on August 7, two weeks ago.\r\n\r\nI believe we should not add to core TF (in near-real time) every new idea that gets pushed to Arxiv. All code in core TF needs to be maintained in the long-term: adding new code has a cost, which gets integrated over time. Until it is demonstrated that a given feature is useful, we should not add it to core TF.", "Can you add this to contrib (contrib/losses) instead?", "OK, I will move it to contrib.", "Is this function working properly?\r\nI have constant loss all the train time.", "@Kongsea any luck with the move to contrib?", "@drpngx I have moved it to contrib. Please check it. Thank you.", "There has been a change, I think you need to pull rebase and push again. Thanks.", "Now it seems OK. Please check it again.", "Jenkins, test this please.", "Based on the definition of cross-entropy, should the `labels` be added to this [equation](https://github.com/tensorflow/tensorflow/pull/12257/files#diff-01ec5a7b62cbe21e7fda63c7a92947e6R716)? otherwise, when alpha=0, gamma=1, the FL value is different from what tf.nn.softmax_cross_entropy returns.", "@vrv please review", "Jenkins, test this please.", "Adding gpapan@ who knows about this area, or feel free to re-assign it to someone who knows it well (I don't know it well enough).", "@gpapan are you able to review this? Let me know if not.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "CLA confused by me.", "@martinwicke care to suggest an alternative reviewer perhaps? It looks like @gpapan is not available.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @fchollet: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Any news?", "Nagging Assignee @fchollet: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@tensorflowbutler Can you label nagging issue as nagging?"]}, {"number": 12256, "title": "No module named 'tensorflow'", "body": "I downloaded the Anaconda 4.1.1 For Windows with Python 3.5.2 version.\r\n\r\nCreate a conda environment named tensorflow by invoking the following command:\r\nC:> conda create -n tensorflow\r\n\r\nActivate the conda environment by issuing the following command:\r\nC:> activate tensorflow\r\n(tensorflow)C:> \r\n\r\nI install the CPU-only version of TensorFlow\r\n\r\nI installed it successfully.\r\nbut then....\r\n\r\nc>python\r\npython version 3.5.2 .... (anaconda).............\r\n\r\nimport tensorflow as tf\r\n[it appears this Error : ]\r\nTraceback :\r\nFile \"\", line 1, in \r\nModuleNotFoundError: No module named 'tensorflow'", "comments": ["Maybe you need to use command \"cd ( Tensorflow Interpreter Path )\" to try it", "Make sure you are not sitting inside your anaconda directory. cd to somewhere else (like C), **ensure that you activate your environment** then try again. From your issue file it suggests that you weren't inside your tensorflow environment when entering your python console.", "in command Prompt after I activate tensorflow it works fine,\r\nbut I am using sublime and I am not able to build it", "Activate tensorflow in your command prompt then write your code in sublime and run\r\n(tensorflow)C:> python yourfilename.py", "thanks for your support"]}, {"number": 12255, "title": "Fail to tune the number of CPU for training", "body": "### System information\r\n\r\n== uname -a =====================================================\r\nLinux aws-prophet-tf01 4.4.53-1.el7.centos.x86_64 #1 SMP Sun Mar 12 12:38:41 EDT 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.1)\r\nprotobuf (3.3.0)\r\ntensorflow (1.2.1)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.2.1\r\ntf.GIT_VERSION = v1.2.0-5-g435cdfc\r\ntf.COMPILER_VERSION = v1.2.0-5-g435cdfc\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n### Describe the problem\r\n\r\nWe are using TensorFlow to benchmark and train an simple neural network model. We found that the usage of CPU was around 200% while we have more idle CPUs. And we have manually set the configuration of session like this which will not work.\r\n\r\n```\r\nconfig = tf.ConfigProto(device_count={\"CPU\": 4}, # limit to num_cpu_core CPU usage  \r\n                inter_op_parallelism_threads = 1,   \r\n                intra_op_parallelism_threads = 4,  \r\n                log_device_placement=True)  \r\nwith tf.Session(config = config) as sess:  \r\n```\r\n\r\nHere is my script to benchmark and using `tf.embedding_lookup_sparse` for the space dataset in https://github.com/tobegit3hub/tensorflow_template_application/blob/master/sparse_classifier.py .\r\n\r\n### Source code / logs\r\n\r\nThe usage of CPU looked like this.\r\n\r\n<img width=\"762\" alt=\"screen shot 2017-08-11 at 5 15 29 pm\" src=\"https://user-images.githubusercontent.com/2715000/29255695-4dd7deb6-80d6-11e7-8d7b-2c9f7ef3d687.png\">\r\n\r\nAlthough we had set the configuration of session. It seemed that all op are placed in `cpu:0`. The log looks like this.\r\n\r\n```\r\n2017-08-14 01:44:00.058798: I tensorflow/core/common_runtime/simple_placer.cc:847] ParseExample/Parse[54/1040]\r\nparse_keys_0: (Const)/job:localhost/replica:0/task:0/cpu:0\r\nParseExample/ParseExample/names: (Const): /job:localhost/replica:0/task:0/cpu:0\r\n2017-08-14 01:44:00.058818: I tensorflow/core/common_runtime/simple_placer.cc:847] ParseExample/ParseExample/names: (Const)/job:localhost/replica:0/task:0/cpu:0\r\nParseExample/Const: (Const): /job:localhost/replica:0/task:0/cpu:0                                            2017-08-14 01:44:00.058849: I tensorflow/core/common_runtime/simple_placer.cc:847] ParseExample/Const: (Const)/job:localhost/replica:0/task:0/cpu:0\r\nshuffle_batch/n: (Const): /job:localhost/replica:0/task:0/cpu:0\r\n2017-08-14 01:44:00.058868: I tensorflow/core/common_runtime/simple_placer.cc:847] shuffle_batch/n: (Const)/job:localhost/replica:0/task:0/cpu:0\r\nshuffle_batch/fraction_over_100_of_1024_full/tags: (Const): /job:localhost/replica:0/task:0/cpu:0\r\n2017-08-14 01:44:00.058886: I tensorflow/core/common_runtime/simple_placer.cc:847] shuffle_batch/fraction_over_100_of_1024_full/tags: (Const)/job:localhost/replica:0/task:0/cpu:0\r\nshuffle_batch/mul/y: (Const): /job:localhost/replica:0/task:0/cpu:0\r\n2017-08-14 01:44:00.058901: I tensorflow/core/common_runtime/simple_placer.cc:847] shuffle_batch/mul/y: (Const)/job:localhost/replica:0/task:0/cpu:0\r\n```", "comments": ["By default `cpu:0` refers to all CPU cores on the system. You can create virtual CPU devices like \"cpu:1\", but there's no physical pinning of cores, so ops pinned to \"cpu:1\" will run on same cores as ops pinned to \"cpu:0\". To see what cores are actually being used to run ops you need to use something like `ktrace` utility.\r\n\r\nFurthermore, I don't see anything in your script that will place ops onto `cpu:1` virtual device, so that's why you see `cpu:0` messages from placement algorithm\r\n\r\nIt's common to see less than full utilization because parallelization is hard (ie, some core are idle because they are waiting for input).", "Thanks @yaroslavvb . It's reasonable and we have increased the utility of all CPUs by adding the number of threads for reading TFRecords files.\r\n\r\nSo is it right if we don't need to specify `cpu:1` and we may get the best performance?", "Correct. BTW unless you are pretty sure it's a bug, it's better to ask these questions on stackoverflow, you'll get response faster and it'll be easier to find by others", "Well, thanks @yaroslavvb very much \ud83d\ude03 "]}, {"number": 12254, "title": "Principle of setting 'hash_bucket_size' parameter ?", "body": "Question 1: \r\nIn [`wide_n_deep_tutorial.py`,](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/wide_n_deep_tutorial.py) there is a hyper-parameter named `hash_bucket_size` for both `tf.feature_column.categorical_column_with_hash_bucket` and `tf.feature_column.crossed_column` methods,  and the value is `hash_bucket_size=1000`.\r\n\r\nBut why `1000`? How to set this parameter ?\r\n\r\n\r\n\r\nQuestion 2:\r\nThe second question about `crossed_columns `, that is,\r\n`crossed_columns = [\r\n    tf.feature_column.crossed_column(\r\n        [\"education\", \"occupation\"], hash_bucket_size=1000),\r\n    tf.feature_column.crossed_column(\r\n        [age_buckets, \"education\", \"occupation\"], hash_bucket_size=1000),\r\n    tf.feature_column.crossed_column(\r\n        [\"native_country\", \"occupation\"], hash_bucket_size=1000)\r\n]\r\n` in [`wide_n_deep_tutorial.py`,](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/wide_n_deep_tutorial.py)\r\n\r\nWhy choose `[\"education\", \"occupation\"]`, `[age_buckets, \"education\", \"occupation\"]` and `[\"native_country\", \"occupation\"]` as crossed_columns , are there any rule of thumb ?\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12253, "title": "not installing ", "body": "Collecting tensorflow\r\n  Could not find a version that satisfies the requirement tensorflow (from versions: )\r\nNo matching distribution found for tensorflow\r\n", "comments": ["_Please make sure you fill in the issue template to ensure that your issue can be troubleshooted correctly. Key information includes System Specs and Tensorflow/CUDA/cuDNN versions. Without this it's harder to help!_\r\n\r\nAbsolutely not enough information here to help. This is usually caused by an incompatible system/python version. Please fill in the Issue Template and it will be possible to give you an answer.", "Hi\r\ni have the same problem... so i wonder if tensorflow doesn' t work with linux 32 bit ????", "No, tensorflow is not supported and does not work out of the box on 32-bit desktop operating systems.\r\nYou can probably make it work by building from sources, but you need to reach out to the community in stackoverflow for support with issues on that.", "Thanks gunan\r\ni just tried to do it from sources.But unfortunately, Bazel doesn't work in 32-bits:\r\nN: Skipping acquire of configured file 'jdk1.8/binary-i386/Packages' as repository 'http://storage.googleapis.com/bazel-apt stable InRelease' doesn't support architecture 'i386'\r\nThanks anyway"]}, {"number": 12252, "title": "Cannot use mean_absolute_error", "body": "I made a very simple neural network on tf and I wanted to use mean absolute error loss function, however I got this error right after I created the optimizer:\r\n\r\nNo gradients provided for any variable, check your graph for ops that do not support gradients, between variables [...] and loss Tensor(...)\r\n\r\nThis is what I did:\r\n```\r\ncost = tf.metrics.mean_absolute_error(pred, y)[0]\r\noptimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate).minimize(cost)\r\n```\r\n\r\nI tried another loss function and it worked. In fact, I read that it's difficult to provide a gradient when the absolute value is involved, but I did exactly the same in Keras and it works. In fact, I also did the following (which is basically the mean absolute error) and it works as well!\r\n\r\n```\r\ncost = tf.reduce_mean(tf.abs(tf.subtract(pred, y)))\r\noptimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate).minimize(cost)\r\n```\r\n\r\nWhy the function doesn't work?\r\n\r\nJM.\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\nTo answer your question, `mean_absolute_error` is intended for evaluation and so it doesn't have a gradient. `mean_absolute_error` also returns an update op (which are you ignoring in the code above) that must be used to update the mean, so the concept of a gradient for this function doesn't really make sense. The update op for `tf.metrics.mean_absolute_error(pred, y)` must be called before the mean can be obtained.", "@jmlipman you might want to use another loss function like mean_squared_error for the optimization. Apart from that you can use mean_absolute_error to measure the \"accuracy\".", "@gissemari can you please explain more detail how to implement `mean_absolute_error` to evaluate? \r\nCC @reedwm ", "Sure. According to the example here, _cost_ can be used for the optimization, while _performance_ can be just the metric (as the module tf.metrics suggest).\r\ncost = tf.losses.mean_squared_error(y, predictions)\r\noptimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate).minimize(cost)\r\nperformance  = tf.metrics.mean_absolute_error(pred, y)[0]", "Why do you say that mean absolute error has no gradient? It is piecewise smooth, just like a ReLU layer, and the gradient should be the mean of the gradients of each absolute error (well-defined everywhere except at 0, but can be taken to be 0 there). I was surprised not to find a mean_absolute_error in tf.losses, as right now I thought I could be better off training for that, because I suspect squared error is too strict for my data (perhaps encouraging the model to output the mean when it is uncertain, rather than risking a huge squared error by making a guess. Maybe I am wrong). Anyway, it should be possible to use `tf.reduce_mean(tf.abs(predictions-targets))`.\r\n\r\nUpdate: I get it that you didn't claim that mean absolute error has no gradient, only that tf.metrics.mean_absolute_error has no gradient. Indeed absolute values are beging optimized by gradient descent when using L1 regularization."]}, {"number": 12251, "title": "an error occurred which is \"No algorithm without scratch worked \" when using tensorflow to train data", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.2.1-cp36-cp36m-linux_x86_64.whl\r\n- **TensorFlow version (use command below)**:'1.2.1'\r\n- **Python version**: 3.6.2\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:cuda-8.0/cudnn-5.1\r\n- **GPU model and memory**:nvidia Tesla K80 /11G\r\n\r\n\r\n### Describe the problem\r\nwhen I use tensorflow to train my model, an error always occur when my my data size is larger than [250,250,250]. \r\n\r\n### Source code / logs\r\n### this  is my code :\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nn=250\r\nx_train=np.arange(n*n*n).reshape([1,n,n,n,1])\r\ny_train=np.zeros(n*n*n)\r\ntf_x = tf.placeholder(tf.float32,[1,None,None,None,1])\r\ntf_y = tf.placeholder(tf.int64,[None])\r\noutput=tf.layers.conv3d(tf_x,\r\n                        filters=2,\r\n                        kernel_size=(2,2,2),\r\n                        strides=(1,1,1), \r\n                        padding=\"same\",\r\n                        activation=tf.nn.relu)\r\n\r\nlogits = tf.reshape(output, [-1, 2])\r\ncross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=tf_y)\r\nloss = tf.reduce_mean(cross_entropy)\r\ntrain_step = tf.train.AdamOptimizer(1e-6).minimize(loss)\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run(train_step,feed_dict={tf_x:x_train,tf_y:y_train})\r\n    print(sess.run(loss,feed_dict={tf_x:x_train,tf_y:y_train}))\r\n```\r\n\r\n### This is the error I met:\r\n\r\n```\r\n`NotFoundError                             Traceback (most recent call last)\r\n/data/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1138     try:\r\n-> 1139       return fn(*args)\r\n   1140     except errors.OpError as e:\r\n\r\n/data/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1120                                  feed_dict, fetch_list, target_list,\r\n-> 1121                                  status, run_metadata)\r\n   1122 \r\n\r\n/data/conda/lib/python3.6/contextlib.py in __exit__(self, type, value, traceback)\r\n     87             try:\r\n---> 88                 next(self.gen)\r\n     89             except StopIteration:\r\n\r\n/data/conda/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py in raise_exception_on_not_ok_status()\r\n    465           compat.as_text(pywrap_tensorflow.TF_Message(status)),\r\n--> 466           pywrap_tensorflow.TF_GetCode(status))\r\n    467   finally:\r\n\r\nNotFoundError: No algorithm without scratch worked!\r\n\t [[Node: gradients_2/conv3d_3/convolution_grad/Conv3DBackpropInputV2 = Conv3DBackpropInputV2[T=DT_FLOAT, data_format=\"NDHWC\", padding=\"SAME\", strides=[1, 1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients_2/conv3d_3/convolution_grad/Shape, conv3d_2/kernel/read, gradients_2/conv3d_3/BiasAdd_grad/tuple/control_dependency)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-10-78d05b6135a4> in <module>()\r\n     19 with tf.Session() as sess:\r\n     20     sess.run(tf.global_variables_initializer())\r\n---> 21     sess.run(train_step,feed_dict={tf_x:x_train,tf_y:y_train})\r\n     22     print(sess.run(loss,feed_dict={tf_x:x_train,tf_y:y_train}))\r\n     23 #     print(sess.run(loss,feed_dict={tf_x:x_train,tf_y:y_train}))\r\n\r\n/data/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    787     try:\r\n    788       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 789                          run_metadata_ptr)\r\n    790       if run_metadata:\r\n    791         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/data/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n    995     if final_fetches or final_targets:\r\n    996       results = self._do_run(handle, final_targets, final_fetches,\r\n--> 997                              feed_dict_string, options, run_metadata)\r\n    998     else:\r\n    999       results = []\r\n\r\n/data/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1130     if handle is None:\r\n   1131       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\r\n-> 1132                            target_list, options, run_metadata)\r\n   1133     else:\r\n   1134       return self._do_call(_prun_fn, self._session, handle, feed_dict,\r\n\r\n/data/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1150         except KeyError:\r\n   1151           pass\r\n-> 1152       raise type(e)(node_def, op, message)\r\n   1153 \r\n   1154   def _extend_graph(self):\r\n\r\nNotFoundError: No algorithm without scratch worked!\r\n\t [[Node: gradients_2/conv3d_3/convolution_grad/Conv3DBackpropInputV2 = Conv3DBackpropInputV2[T=DT_FLOAT, data_format=\"NDHWC\", padding=\"SAME\", strides=[1, 1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients_2/conv3d_3/convolution_grad/Shape, conv3d_2/kernel/read, gradients_2/conv3d_3/BiasAdd_grad/tuple/control_dependency)]]\r\n\r\nCaused by op 'gradients_2/conv3d_3/convolution_grad/Conv3DBackpropInputV2', defined at:\r\n  File \"/data/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/data/conda/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/data/conda/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"/data/conda/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/data/conda/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\r\n    ioloop.IOLoop.instance().start()\r\n  File \"/data/conda/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\r\n    super(ZMQIOLoop, self).start()\r\n  File \"/data/conda/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\r\n    handler_func(fd_obj, events)\r\n  File \"/data/conda/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/data/conda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\r\n    self._handle_recv()\r\n  File \"/data/conda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"/data/conda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"/data/conda/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/data/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"/data/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"/data/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"/data/conda/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/data/conda/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/data/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/data/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/data/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-10-78d05b6135a4>\", line 18, in <module>\r\n    train_step = tf.train.AdamOptimizer(1e-6).minimize(loss)\r\n  File \"/data/conda/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 315, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"/data/conda/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 386, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/data/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 540, in gradients\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/data/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 346, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/data/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 540, in <lambda>\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/data/conda/lib/python3.6/site-packages/tensorflow/python/ops/nn_grad.py\", line 80, in _Conv3DGrad\r\n    data_format=data_format),\r\n  File \"/data/conda/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 666, in conv3d_backprop_input_v2\r\n    name=name)\r\n  File \"/data/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/data/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/data/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\n...which was originally created as op 'conv3d_3/convolution', defined at:\r\n  File \"/data/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n[elided 18 identical lines from previous traceback]\r\n  File \"/data/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-10-78d05b6135a4>\", line 13, in <module>\r\n    activation=tf.nn.relu)\r\n  File \"/data/conda/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py\", line 728, in conv3d\r\n    return layer.apply(inputs)\r\n  File \"/data/conda/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 492, in apply\r\n    return self.__call__(inputs, *args, **kwargs)\r\n  File \"/data/conda/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 441, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/data/conda/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py\", line 158, in call\r\n    data_format=utils.convert_data_format(self.data_format, self.rank + 2))\r\n  File \"/data/conda/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 670, in convolution\r\n    op=op)\r\n  File \"/data/conda/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 338, in with_space_to_batch\r\n    return op(input, num_spatial_dims, padding)\r\n  File \"/data/conda/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 662, in op\r\n    name=name)\r\n  File \"/data/conda/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 146, in _non_atrous_convolution\r\n    name=name)\r\n  File \"/data/conda/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 526, in conv3d\r\n    data_format=data_format, name=name)\r\n  File \"/data/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n\r\nNotFoundError (see above for traceback): No algorithm without scratch worked!\r\n\t [[Node: gradients_2/conv3d_3/convolution_grad/Conv3DBackpropInputV2 = Conv3DBackpropInputV2[T=DT_FLOAT, data_format=\"NDHWC\", padding=\"SAME\", strides=[1, 1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients_2/conv3d_3/convolution_grad/Shape, conv3d_2/kernel/read, gradients_2/conv3d_3/BiasAdd_grad/tuple/control_dependency)]]`\r\n```\r\n", "comments": ["This looks related to #9576. Perhaps you're out of GPU memory?", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 12250, "title": "TypeError: Cannot interpret feed_dict key as Tensor: The name 'DecodeJpeg/contents:0' refers to a Tensor which does not exist. The operation, 'DecodeJpeg/contents', does not exist in the graph.", "body": "TypeError: Cannot interpret feed_dict key as Tensor: The name 'DecodeJpeg/contents:0' refers to a Tensor which does not exist. The operation, 'DecodeJpeg/contents', does not exist in the graph.\r\n\r\nI am trying to run retrain_test.py on image but not getting output instead getting error.\r\n\r\nTraceback (most recent call last):\r\n  File \"retraining-example.py\", line 88, in <module>\r\n    run_inference_on_image()\r\n  File \"retraining-example.py\", line 71, in run_inference_on_image\r\n    {'DecodeJpeg/contents:0': image_data})\r\n  File \"/home/omer/installed/acaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 778, in run\r\n    run_metadata_ptr)\r\n  File \"/home/omer/installed/acaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 933, in _run\r\n    + e.args[0])\r\nTypeError: Cannot interpret feed_dict key as Tensor: The name 'DecodeJpeg/contents:0' refers to a Tensor which does not exist. The operation, 'DecodeJpeg/contents', does not exist in the graph.", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "I met the same problem in tf 1.2.1 when I try to freeze graph", "I have solve the problem, check that --filename_tensor_name=save/Const:0 is your situation, the default filename_tensor_name is \"save/Const:0\". I found that my  filename_tensor_name is \"deploy/save/Const:0\", and restore_op_name is \"deploy/save/restore_all\".\r\nSo just pass the params to the script like this:--restore_op_name=deploy/save/restore_all --filename_tensor_name=deploy/save/Const:0, the problem solved.", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!", "This bug is specific to mobilenet_1.0_224 Architecture. If i retrain on Inception model it works fine without any errors.", "Im facing the same problem, the inceptionv3 java code its not compatible with the python graph model mobilenet\r\n\r\nDoing the modifications that **tangyanlin** suggested:\r\n\r\nat exporter.py:346 I find those parameters:\r\n```\r\nrestore_op_name='save/restore_all',\r\nfilename_tensor_name='save/Const:0',\r\n```\r\n\r\nAfter the change I have the following code\r\n```\r\nrestore_op_name='deploy/save/restore_all',\r\nfilename_tensor_name='deploy/save/Const:0',\r\n```\r\nHow can I proceed now?\r\n\r\n- I run exporter.py and will it automatically generate a new graph.pb? I tried with the parameters specified on the file, and not worked\r\n- Do I need to retrain with the new modification? How can you be sure that a new training will use the new parameter?\r\n- Do I need to train again from zero? At the final will generate a new graph.pb?", "Anyone solve this issue?", "I found the retrain.py I download form https://github.com/tensorflow didn't contain \"DecodeJpeg/contents:0\" in it.\r\nSo I change that file , then it work.", "@funny602tw \"Change that file\"... how?", "my solution is to use this loop to find the tensor name of the graph, to see if \"DecodeJpeg/contents\" is the first tensor in the graph, my first tensor is \"Placeholder\".  use {'Placeholder:0': image_data} replace {'DecodeJpeg/contents:0': image_data}) \r\n\r\ntensor_name_list = [tensor.name for tensor in tf.get_default_graph().as_graph_def().node]\r\n    for tensor_name in tensor_name_list:\r\n        print(tensor_name, '\\n')", "Simply use older retrain.py using the following link\r\nhttps://github.com/tensorflow/tensorflow/blob/c565660e008cf666c582668cb0d0937ca86e71fb/tensorflow/examples/image_retraining/retrain.py", "> my solution is to use this loop to find the tensor name of the graph, to see if \"DecodeJpeg/contents\" is the first tensor in the graph, my first tensor is \"Placeholder\". use {'Placeholder:0': image_data} replace {'DecodeJpeg/contents:0': image_data})\r\n> \r\n> tensor_name_list = [tensor.name for tensor in tf.get_default_graph().as_graph_def().node]\r\n> for tensor_name in tensor_name_list:\r\n> print(tensor_name, '\\n')\r\n\r\nI tried,but it can not solve this issue."]}, {"number": 12249, "title": "tf.estimator.Estimator breaks when using python 3.5 type annotations", "body": "Minimal example:\r\n```\r\nimport tensorflow as tf\r\n\r\ndef model_fn(features: dict, labels: tf.Tensor, mode: str):\r\n    pass\r\n\r\nestimator = tf.estimator.Estimator(model_fn)\r\n```\r\n\r\nresults in \r\n```\r\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 173, in __init__\r\n    _verify_model_fn_args(model_fn, params)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 742, in _verify_model_fn_args\r\n    args = set(_model_fn_args(model_fn))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 737, in _model_fn_args\r\n    return tuple(tf_inspect.getargspec(fn).args)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/tf_inspect.py\", line 45, in getargspec\r\n    if d.decorator_argspec is not None), _inspect.getargspec(target))\r\n  File \"/usr/lib/python3.5/inspect.py\", line 1045, in getargspec\r\n    raise ValueError(\"Function has keyword-only arguments or annotations\"\r\nValueError: Function has keyword-only arguments or annotations, use getfullargspec() API which can support them\r\n```\r\n\r\n### System information\r\n- **OS Platform and Distribution**: Linux Mint 17.2\r\n- **TensorFlow version**: v1.2.0-0-g12f033d 1.2.0\r\n- **Python version**: 3.5\r\n\r\n", "comments": ["@martinwicke can you comment or redirect? Thanks.", "Yes, we're using getargspec, which doesn't support type annotations. This is worthwhile to fix. If someone wants to take it on, the code should be relatively isolated in a single function which is checking arguments. ", "Added a PR #12276 for this issue."]}, {"number": 12248, "title": "r1.3 fix Bazel 0.5.3 build error", "body": "This is the patch necessary for TensorFlow r1.3 to compile with Bazel 0.5.3, which is set to be merged before the 1.3 release.\r\n\r\n@martinwicke Mentioned in https://github.com/tensorflow/tensorflow/pull/11949#issuecomment-321956462 from which this change is cherry picked:\r\n\r\n> @av8ramit is this on the 1.3 branch? If not, can you cherry-pick it? It's build only so no need for another RC because of this.\r\n\r\nI can confirm it isn't on the 1.3 branch. I'm building on Bazel 0.5.3, and I can also confirm that the r1.3 build works locally on my Ubuntu 16.04 once this patch is added, but does not work without it.\r\n\r\nchange description:\r\nFix \"depsets cannot contain mutable items\" error with CUDA builds in Bazel 0.5.3\"\r\n\r\n(cherry picked from commit c5d311eaf8cc6471643b5c43810a1feb19662d6c)", "comments": ["Can one of the admins verify this patch?", "@ahundt, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @andrewharp and @rohan100jain to be potential reviewers.", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 12247, "title": "Update documentations", "body": "-Add links\r\n-Replace OS X with macOS\r\n-Minor url cleanups", "comments": ["@alanyee, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @alextp and @shitian-ni to be potential reviewers.", "Can one of the admins verify this patch?", "@av8ramit Is there a way for my documentation changes to be incorporated into the online docs for 1.3?"]}, {"number": 12246, "title": "make `gather` cpu kernel to be multiple threads.", "body": "related to [11709](https://github.com/tensorflow/tensorflow/issues/11709).\r\nOn a single machine, profiling the [embedding_lookup_sparse](https://gist.github.com/nolanliou/c00af5938b2aecfdc5ea1189426b8624) with tfprof, the result shows that gather Op takes a lot of time. I checked the code and found that the CPU version gather op is single-thread. \r\n\r\nThen I modify the gather Op to multi-threads, the result shows about 6x speedup. \r\n\r\n### Profiling result\r\n```\r\nnode name | requested bytes | total execution time | accelerator execution t    ime | cpu execution time`\r\n# old `gather`\r\nGather     2048.00MB (100.00%, 2.27%),   622.96ms (99.99%, 22.97%),    0us (0.00%, 0.00%),  622.96ms (99.99%, 22.97%)\r\n# multi-threads `gather`\r\nGather    2048.00MB (100.00%, 49.96%),   107.91ms (99.99%, 5.50%),     0us (0.00%, 0.00%),   107.91ms (99.99%, 5.50%)\r\n```", "comments": ["Can one of the admins verify this patch?", "@nolanliou, thanks for your PR! By analyzing the history of the files in this pull request, we identified @rryan, @dm-jrae and @tensorflower-gardener to be potential reviewers.", "Fixed.", "Jenkins, test this please.", "done", "Jenkins, test this please", "Is there something wrong about Jenkins?", "Jenkins, test this please", "Error: \r\n./tensorflow/core/framework/variant.h:26:78: fatal error: tensorflow/core/framework/tensor.pb.h: No such file or directory\r\n #include \"tensorflow/core/framework/tensor.pb.h\"  // TODO(b/62899350): Remove\r\n\r\nShould remove the include?", "Can you rebase on top of master? If that doesn't work, try removing the\ninclude.\n\nOn Wed, Aug 23, 2017 at 11:36 PM, nolan liu <notifications@github.com>\nwrote:\n\n> Error:\n> ./tensorflow/core/framework/variant.h:26:78: fatal error:\n> tensorflow/core/framework/tensor.pb.h: No such file or directory\n> #include \"tensorflow/core/framework/tensor.pb.h\" // TODO(b/62899350\n> <https://buganizer.corp.google.com/62899350>): Remove\n>\n> Should remove the include?\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/12246#issuecomment-324546516>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxWDlx_CGzdTBssXwsyBdGZkH8NUsks5sbRnhgaJpZM4O1mqc>\n> .\n>\n\n\n\n-- \n - Alex\n", "I have sync the master branch with the upstream, if rebase will import other changes. So anyone could help to rebase the PR?", "@nolanliou `git merge` or `git pull` will import new changes into your PR branch, `git rebase` will keep your PR commits the same", "@yaroslavvb have a look at the [question](https://stackoverflow.com/questions/22335012/updating-a-git-pull-request-after-a-local-rebase). I met the same [problem](https://github.com/tensorflow/tensorflow/pull/12230).", "@yaroslavvb I figured out what's going on, thanks.\r\n@alextp rebase done.", "Jenkins, test this please", "Remove the tensor.pb.h in variant.h file.", "Jenkins, test this please", "Unable to get pull request builder trigger!! what's going on?", "Jenkins, test this please.\n\nOn Mon, Aug 28, 2017 at 6:23 PM, nolan liu <notifications@github.com> wrote:\n\n> Unable to get pull request builder trigger!! what's going on?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/12246#issuecomment-325526809>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxQpGHiWHDijgL0Zz_BER4n90KZ35ks5sc2f3gaJpZM4O1mqc>\n> .\n>\n\n\n\n-- \n - Alex\n", "fatal error: tensorflow/core/lib/core/error_codes.pb.h: No such file or directory ?", "Can you rebase? This include should not exist anymore.\n\nOn Tue, Aug 29, 2017 at 7:36 PM, nolan liu <notifications@github.com> wrote:\n\n> fatal error: tensorflow/core/lib/core/error_codes.pb.h: No such file or\n> directory ?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/12246#issuecomment-325834354>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxcsPtQgikyGYpW-lrunlV-3eZHz2ks5sdKCJgaJpZM4O1mqc>\n> .\n>\n\n\n\n-- \n - Alex\n", "It seems the master branch contain the include. see [code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/core/status.h#L23).", "@gunan you were looking at this proto header issue. Do you know what's going on here?", "I still have not removed error_codes.proto.h.\r\nI worked with tensor.proto.h tensor_shape, and a few other until now.", "How is going about the proto header issues? ", "Jenkins, test this please. \r\n\r\nThere has been a bunch of work on this, trying again.", "Jenkins, test this please.", "Can you rebase before we rerun the tests?", "Rebase done.", "@tensorflow-jenkins test this please", "You need to include the proto file which defines `VariantTensorDataProto`:\r\n\r\n```\r\nERROR: /workspace/tensorflow/core/BUILD:1570:1: C++ compilation of rule '//tensorflow/core:framework_internal' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /var/lib/jenkins/workspace/tensorflow-pull-requests-gpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda \\\r\n    CUDNN_INSTALL_PATH=/usr/local/cuda-8.0 \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\n    LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_CUDA_CLANG=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=3.0 \\\r\n    TF_CUDA_VERSION=8.0 \\\r\n    TF_CUDNN_VERSION=6 \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_OPENCL=0 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/_objs/framework_internal/tensorflow/core/framework/variant_op_registry.d '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/_objs/framework_internal/tensorflow/core/framework/variant_op_registry.o' -DEIGEN_MPL2_ONLY -DTENSORFLOW_USE_JEMALLOC -DSNAPPY -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/nsync -iquote bazel-out/local_linux-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/jemalloc -iquote bazel-out/local_linux-opt/genfiles/external/jemalloc -iquote external/protobuf_archive -iquote bazel-out/local_linux-opt/genfiles/external/protobuf_archive -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/local_linux-opt/genfiles/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/local_linux-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/local_linux-opt/genfiles/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/local_linux-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/local_linux-opt/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/local_linux-opt/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/local_linux-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/local_linux-opt/genfiles/external/zlib_archive -iquote external/snappy -iquote bazel-out/local_linux-opt/genfiles/external/snappy -iquote external/local_config_cuda -iquote bazel-out/local_linux-opt/genfiles/external/local_config_cuda -isystem external/nsync/public -isystem bazel-out/local_linux-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jemalloc/include -isystem bazel-out/local_linux-opt/genfiles/external/jemalloc/include -isystem external/protobuf_archive/src -isystem bazel-out/local_linux-opt/genfiles/external/protobuf_archive/src -isystem external/eigen_archive -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/local_linux-opt/genfiles/external/gif_archive/lib -isystem external/farmhash_archive/src -isystem bazel-out/local_linux-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/local_linux-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/local_linux-opt/genfiles/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/local_linux-opt/genfiles/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/local_linux-opt/genfiles/external/local_config_cuda/cuda/cuda/include -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-DGOOGLE_CUDA=1' -msse3 -pthread '-DGOOGLE_CUDA=1' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers -c tensorflow/core/framework/variant_op_registry.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/_objs/framework_internal/tensorflow/core/framework/variant_op_registry.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nIn file included from tensorflow/core/framework/variant_op_registry.cc:20:0:\r\n./tensorflow/core/framework/variant.h: In instantiation of 'struct tensorflow::Variant::Value<tensorflow::VariantTensorDataProto>':\r\n./tensorflow/core/framework/variant.h:210:72:   required from 'T* tensorflow::Variant::get() [with T = tensorflow::VariantTensorDataProto]'\r\n./tensorflow/core/framework/variant_op_registry.h:151:66:   required from 'tensorflow::variant_op_registry_fn_registration::UnaryVariantDecodeRegistration<T>::UnaryVariantDecodeRegistration(const string&) [with T = int; std::string = std::basic_string<char>]::__lambda1'\r\n./tensorflow/core/framework/variant_op_registry.h:149:24:   required from 'struct tensorflow::variant_op_registry_fn_registration::UnaryVariantDecodeRegistration<T>::UnaryVariantDecodeRegistration(const string&) [with T = int; std::string = std::basic_string<char>]::__lambda1'\r\n./tensorflow/core/framework/variant_op_registry.h:162:5:   required from 'tensorflow::variant_op_registry_fn_registration::UnaryVariantDecodeRegistration<T>::UnaryVariantDecodeRegistration(const string&) [with T = int; std::string = std::basic_string<char>]'\r\ntensorflow/core/framework/variant_op_registry.cc:118:1:   required from here\r\n./tensorflow/core/framework/variant.h:337:7: error: 'tensorflow::Variant::Value<T>::value' has incomplete type\r\n     T value;\r\n       ^\r\nIn file included from ./tensorflow/core/framework/variant.h:27:0,\r\n                 from tensorflow/core/framework/variant_op_registry.cc:20:\r\n./tensorflow/core/framework/variant_tensor_data.h:27:7: error: forward declaration of 'class tensorflow::VariantTensorDataProto'\r\n class VariantTensorDataProto;\r\n       ^\r\n```", "It seems `VariantTensorDataProto` is in tensor.proto file. And I deleted the tensor.pb.h in variant.h file before for the below reason.\r\n```\r\nError:\r\n./tensorflow/core/framework/variant.h:26:78: fatal error: tensorflow/core/framework/tensor.pb.h: No such file or directory\r\n```\r\nShould revert the change of tensor.pb.h", "Yes, if it's not generated, then it means that there is a missing dependency in the `BUILD` file.", "@drpngx done.", "Thanks! Could you push the changes?", "I have reverted last commit and pushed it.", "OK, github doesn't report anything. Strange.\r\n\r\nJenkins, test this please.", "The same error as I mentioned before.", "@nolanliou you have to modify the `BUILD` file to depend on the tensor proto library, otherwise it won't be generated in time.", "This is the error. I just checked that `framework` depends on `framework_internal` which depends on `protos_all`, so it's not a problem there. You're probably not depending on `framework` properly.\r\n```\r\nIn file included from ./tensorflow/core/framework/allocator.h:26:0,\r\n                 from ./tensorflow/core/framework/op_kernel.h:23,\r\n                 from ./tensorflow/core/kernels/gather_functor.h:26,\r\n                 from tensorflow/core/kernels/gather_functor.cc:52:\r\n./tensorflow/core/framework/variant.h:26:78: fatal error: tensorflow/core/framework/tensor.pb.h: No such file or directory\r\n #include \"tensorflow/core/framework/tensor.pb.h\"  // TODO(b/62899350): Remove\r\n```", "/CC: @gunan FYI", "It seems the deps of `gather_functor` contain `//tensorflow/core:framework_lite` in kernel `BUILD` file.\r\nI changed it to `//tensorflow/core:framework`.\r\nPlease have a another try.", "Jenkins, test this please.\r\n\r\n", "Somehow, this got stuck. Jenkins, test this please.", "It seems there are some errors about xla gather op. Because the gather functor api changed, so I modified the corresponding xla gather code, the new gather functor need the OpKernelContext object. But I'm not familar with the XLA, so any suggestion? ", "This is the error:\r\n\r\n```\r\n status 1.\r\ntensorflow/compiler/tf2xla/kernels/gather_op.cc: In member function 'virtual void tensorflow::{anonymous}::GatherOpCustomCall::Compile(tensorflow::XlaOpKernelContext*)':\r\ntensorflow/compiler/tf2xla/kernels/gather_op.cc:236:48: error: no matching function for call to 'std::vector<xla::ComputationDataHandle, std::allocator<xla::ComputationDataHandle> >::push_back(tensorflow::OpKernelContext*)'\r\n     args.push_back(context->op_kernel_context());\r\n                                                ^\r\ntensorflow/compiler/tf2xla/kernels/gather_op.cc:236:48: note: candidates are:\r\nIn file included from /usr/include/c++/4.8/vector:64:0,\r\n                 from /usr/include/c++/4.8/bits/random.h:34,\r\n                 from /usr/include/c++/4.8/random:50,\r\n                 from /usr/include/c++/4.8/bits/stl_algo.h:65,\r\n                 from /usr/include/c++/4.8/algorithm:62,\r\n                 from external/protobuf_archive/src/google/protobuf/stubs/common.h:38,\r\n                 from bazel-out/local-opt/genfiles/tensorflow/compiler/xla/xla_data.pb.h:9,\r\n                 from ./tensorflow/compiler/tf2xla/xla_compilation_device.h:21,\r\n                 from ./tensorflow/compiler/tf2xla/xla_compiler.h:19,\r\n                 from ./tensorflow/compiler/tf2xla/xla_op_kernel.h:19,\r\n                 from ./tensorflow/compiler/tf2xla/kernels/gather_op.h:21,\r\n                 from tensorflow/compiler/tf2xla/kernels/gather_op.cc:16:\r\n/usr/include/c++/4.8/bits/stl_vector.h:901:7: note: void std::vector<_Tp, _Alloc>::push_back(const value_type&) [with _Tp = xla::ComputationDataHandle; _Alloc = std::allocator<xla::ComputationDataHandle>; std::vector<_Tp, _Alloc>::value_type = xla::ComputationDataHandle]\r\n       push_back(const value_type& __x)\r\n       ^\r\n/usr/include/c++/4.8/bits/stl_vector.h:901:7: note:   no known conversion for argument 1 from 'tensorflow::OpKernelContext*' to 'const value_type& {aka const xla::ComputationDataHandle&}'\r\n/usr/include/c++/4.8/bits/stl_vector.h:919:7: note: void std::vector<_Tp, _Alloc>::push_back(std::vector<_Tp, _Alloc>::value_type&&) [with _Tp = xla::ComputationDataHandle; _Alloc = std::allocator<xla::ComputationDataHandle>; std::vector<_Tp, _Alloc>::value_type = xla::ComputationDataHandle]\r\n       push_back(value_type&& __x)\r\n       ^\r\n/usr/include/c++/4.8/bits/stl_vector.h:919:7: note:   no known conversion for argument 1 from 'tensorflow::OpKernelContext*' to 'std::vector<xla::ComputationDataHandle, std::allocator<xla::ComputationDataHandle> >::value_type&& {aka xla::ComputationDataHandle&&}'\r\nINFO: Building complete.\r\n```\r\n@tatatodd any suggestion?", "@tatatodd ping?", "@nolanliou seem the error below in the tests. Can you take a look\r\n\r\n```\r\ntensorflow/compiler/tf2xla/kernels/gather_op.cc: In member function 'virtual void tensorflow::{anonymous}::GatherOpCustomCall::Compile(tensorflow::XlaOpKernelContext*)':\r\ntensorflow/compiler/tf2xla/kernels/gather_op.cc:236:48: error: no matching function for call to 'std::vector<xla::ComputationDataHandle, std::allocator<xla::ComputationDataHandle> >::push_back(tensorflow::OpKernelContext*)'\r\n     args.push_back(context->op_kernel_context());\r\n```", "Done. use the nullptr to replace the context->op_kernel_context().", "Jenkins, test this please.", "It seems the problem about dependency, have fixed that. Please have another try.", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.", "@nolanliou pull rebase and push again?", "Done.", "Jenkins, test this please.\n\nOn Mon, Oct 23, 2017 at 8:30 PM, nolan liu <notifications@github.com> wrote:\n\n> Done.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/12246#issuecomment-338861704>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxaCiVNpaqEfo2doljHt6lYuwcIm6ks5svVnMgaJpZM4O1mqc>\n> .\n>\n\n\n\n-- \n - Alex\n", "Remove the commits of fixing gather xla kernel. It seems the gather xla kernel doesn't depend on the gather functor anymore.", "Jenkins, test this please.\n\nOn Tue, Oct 24, 2017 at 7:54 PM, nolan liu <notifications@github.com> wrote:\n\n> Remove the commits of fixing gather xla kernel. It seems the gather xla\n> kernel doesn't depend on the gather functor anymore.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/12246#issuecomment-339198204>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxd1ZWUACelJ185W7Bw_nJweJtPgTks5svqL0gaJpZM4O1mqc>\n> .\n>\n\n\n\n-- \n - Alex\n", "Passed finally, It's a long journey.", "Yay! Doing internal testing.", "@gunan the Ubuntu Makefile appears to be broken, but the link doesn't list anything. Any idea?", "This was my mistake, an issue with the setup. it should be fixed now.", "Should I rebase?", "Not sure what this is:\r\n```\r\nERROR: /tmpfs/src/github/tensorflow/tensorflow/core/BUILD:561:1: Couldn't build file tensorflow/core/_objs/functional_ops_op_lib/tensorflow/core/ops/functional_ops.pic.o: C++ compilation of rule '//tensorflow/core:functional_ops_op_lib' failed (Exit 34). Note: Remote connection/protocol failed with: connection failed: com.google.devtools.build.lib.remote.RetryException: after 6 attempts: io.grpc.StatusRuntimeException: UNAVAILABLE: Subchannel shutdown invoked\r\n```\r\nassuming it's transient.", "It's transient, I believe\n\nOn Tue, Oct 31, 2017 at 9:45 AM, drpngx <notifications@github.com> wrote:\n\n> Not sure what this is:\n>\n> ERROR: /tmpfs/src/github/tensorflow/tensorflow/core/BUILD:561:1: Couldn't build file tensorflow/core/_objs/functional_ops_op_lib/tensorflow/core/ops/functional_ops.pic.o: C++ compilation of rule '//tensorflow/core:functional_ops_op_lib' failed (Exit 34). Note: Remote connection/protocol failed with: connection failed: com.google.devtools.build.lib.remote.RetryException: after 6 attempts: io.grpc.StatusRuntimeException: UNAVAILABLE: Subchannel shutdown invoked\n>\n> assuming it's transient.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/12246#issuecomment-340824032>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxRwCBerFxWjVApK4-uQNBBaDxxvyks5sx06QgaJpZM4O1mqc>\n> .\n>\n\n\n\n-- \n - Alex\n", "Yeap"]}]