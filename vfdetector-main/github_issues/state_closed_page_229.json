[{"number": 47695, "title": "Transpose of a Bi-LSTM CRF working fine on TFLite interpreter of Python but throwing error on Android", "body": "System.Exception: 'Java.Lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/transpose.cc:56 op_context->perm->dims->data[0] != dims (3 != 2)\r\nNode number 6 (TRANSPOSE) failed to prepare.\r\nPlease help me, how we can debug this error.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: All\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.3\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nsequence_input (InputLayer)  [(None, 75)]              0         \r\n_________________________________________________________________\r\nembedding_37 (Embedding)     (None, 75, 20)            703000    \r\n_________________________________________________________________\r\nbidirectional_37 (Bidirectio (None, 75, 100)           28400     \r\n_________________________________________________________________\r\ntime_distributed_37 (TimeDis (None, 75, 50)            5050      \r\n_________________________________________________________________\r\ncrf_64 (CRF)                 (None, 75, 21)            1554      \r\n=================================================================\r\nTotal params: 738,004\r\nTrainable params: 738,004\r\nNon-trainable params: 0", "comments": ["It is hard to reproduce your issue with the limited information. Could you share more information about the model and your inference code? It would be better to share your model to us if possible and also share your code snippet regarding the TFLite interpreter usage for android.", "Please make sure your android aar files have the same or higher TF version used for the TFLite conversion.", "Thanks for your prompt response ->\r\n```\r\n# Model definition\r\nsequence_input = Input(shape=(MAX_LEN,), name='sequence_input')\r\nmodel = Embedding(input_dim=n_words+2, output_dim=EMBEDDING, # n_words + 2 (PAD & UNK)\r\n                  input_length=MAX_LEN)(sequence_input)  # default: 20-dim embedding\r\nmodel = Bidirectional(LSTM(units=50, return_sequences=True,\r\n                           recurrent_dropout=0.1))(model)  # variational biLSTM\r\n\r\nmodel = TimeDistributed(Dense(50, activation=\"relu\"))(model)  # a dense layer as suggested by neuralNer\r\n\r\n\r\ncrf = CRF(n_tags+1)  # CRF layer, n_tags+1(PAD)\r\n\r\n\r\nsequence_mask = tf.keras.layers.Lambda(lambda x: tf.greater(x, 0))(sequence_input)\r\n\r\noutputs = crf(model)\r\n\r\nmodel = Model(sequence_input, outputs)\r\n\r\nmodel.compile(\r\n    loss=crf.neg_log_likelihood,\r\n    metrics=[crf.accuracy],\r\n    optimizer=tf.keras.optimizers.Adam(5e-5)\r\n    )\r\n```\r\n\r\nPrediction on TFLite interface of Python working fine ->\r\n\r\n```\r\ninterpreter = tf.lite.Interpreter(model_path=\"model1.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\nprint(output_details)\r\nprint(input_details)\r\n# Test model on random input data.\r\ninput_shape = input_details[0]['shape']\r\ntest_sentence = nltk.word_tokenize(\"A series of explosions shook the Iraqi capital\")\r\n# Preprocessing\r\nx_test_sent = pad_sequences(sequences=[[word2idx.get(w, 0) for w in test_sentence]],\r\n                        padding=\"post\", value=word2idx[\"PAD\"], maxlen=MAX_LEN)\r\n    \r\ninput_data = np.array([x_test_sent[0]], dtype=np.float32)\r\n# input_data = input_data.astype(np.float) \r\nprint(input_data)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\ninterpreter.invoke()\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nnp.set_printoptions(threshold=sys.maxsize)\r\n\r\nprint(output_data)\r\n```\r\n\r\nBut prediction on Android giving error ->\r\n          \r\n```\r\n            // Tokenize sentence\r\n            // sentence = sentence.ToLower();\r\n            SimpleTokenizer tokenizer = SimpleTokenizer.Instance;\r\n            String[] tokens = tokenizer.Tokenize(sentence);\r\n\r\n            int i = 0;\r\n            var word2idx = new Dictionary<string, int>();\r\n            ner_words.ForEach(w =>\r\n           {\r\n               if (!word2idx.ContainsKey(w))\r\n                   word2idx.Add(w, i+2);\r\n               else\r\n                   word2idx[w] = i+2;\r\n               i++;\r\n           });\r\n            if (!word2idx.ContainsKey(\"UNK\"))\r\n                word2idx.Add(\"UNK\", 1); // Unknown words\r\n            else\r\n                word2idx[\"UNK\"] = 1;\r\n            if (!word2idx.ContainsKey(\"PAD\"))\r\n                word2idx.Add(\"PAD\", 0); // Padding\r\n            else\r\n                word2idx[\"PAD\"] = 0;\r\n\r\n            // Preprocessing\r\n            List<int> x_test_sent = new List<int>();\r\n            for (int j = 0; j < MAX_LEN; j++)\r\n            {\r\n                if (j >= tokens.Length)\r\n                {\r\n                    x_test_sent.Add(word2idx[\"PAD\"]);\r\n                }\r\n                else if (!word2idx.ContainsKey(tokens[j]))\r\n                {\r\n                    x_test_sent.Add(word2idx[\"UNK\"]);\r\n                }\r\n                else\r\n                {\r\n                    x_test_sent.Add(word2idx[tokens[j]]);\r\n                }\r\n            }\r\n\r\n            // Start loading tflite model\r\n            var assets = Application.Context.Assets;\r\n            System.Console.WriteLine(\"Assets\", assets);\r\n            Android.Content.Res.AssetFileDescriptor fileDescriptor = assets.OpenFd(\"model1.tflite\");\r\n            FileInputStream inputStream = new FileInputStream(fileDescriptor.FileDescriptor);\r\n            FileChannel fileChannel = inputStream.Channel;\r\n            long startOffset = fileDescriptor.StartOffset;\r\n            long declaredLength = fileDescriptor.DeclaredLength;\r\n            var asd = fileChannel.Map(FileChannel.MapMode.ReadOnly, startOffset, declaredLength);\r\n            var model = new Xamarin.TensorFlow.Lite.Interpreter(asd);\r\n\r\n\r\n            try\r\n            {\r\n                List<List<float>> record = new List<List<float>>();\r\n                record.Add(new List<float>());\r\n                int x = 0;\r\n                x_test_sent.ForEach(j =>\r\n                {\r\n                    record[0].Add( j );\r\n                    x++;\r\n                });\r\n                var Input_tensor = record.SelectMany(x => x).ToArray();\r\n\r\n                x = 0;\r\n\r\n                // var Input_tensor = record.SelectMany(x => x).ToArray();\r\n                FloatBuffer ith_output = FloatBuffer.Allocate(1 * MAX_LEN * 21);  // Float tensor\r\n                ith_output.Order();\r\n\r\n                model.Run(Input_tensor, ith_output);\r\n```", "BTW I'm using Xamarin TFLite interface", "Could you make sure that your Xamarin TFLite library has built with the same or higher than TF version used for conversion?", "Can you please help me how to find that? my python side version is 2.4.0, and I'm using the latest TFLite library", "The latest version of Xamarin TFLite library is 2.1.0 and the TF version for TFLite model conversion is 2.4. The old runtime may not be able to run the models generated with a newer version. My recommendation is upgrading TF version of your android client. Maybe there is a way to link a native android library in Xamarin to use the latest TFLite library.", "As the latest TFLite is not available as the Nuget package, how can we upgrade this in xamarin?", "Issue is fixed after a downgrade of TF version where it's built.\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47695\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47695\">No</a>\n"]}, {"number": 47694, "title": "_PREEMPTION_ERRORS in one worker cause other workers to get stuck in between-graph async distributed training ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.15.2\r\n- Python version: Python3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nWe are using between-graph async distributed training like this:\r\n```python\r\n# https://github.com/bytedance/fedlearner/blob/master/fedlearner/trainer/estimator.py#L237\r\ndevice_fn = tf.train.replica_device_setter(\r\n    worker_device=\"/job:worker/task:%d\" % self._worker_rank,\r\n    merge_devices=True,\r\n    cluster=self._cluster_spec)\r\ncluster_def = self._cluster_spec.as_cluster_def()\r\nlocal_address = self._cluster_spec.job_tasks('worker')[\r\n    self._worker_rank]\r\nserver = tf.train.Server(tf.train.ClusterSpec(\r\n    {'local': {\r\n        0: local_address\r\n    }}),\r\n    job_name='local',\r\n    task_index=0)\r\ntarget = 'grpc://' + local_address\r\n\r\nconfig = tf.ConfigProto(cluster_def=cluster_def)\r\nconfig.inter_op_parallelism_threads = 4\r\nconfig.intra_op_parallelism_threads = 4\r\nconfig.experimental.share_session_state_in_clusterspec_propagation \\\r\n    = True\r\n\r\n```\r\n\r\nWe have 5 workers and 15 PS. Occasionally one worker would fail in session.run with error like this:\r\n```python\r\n2021-03-10 05:03:22,706 [monitored_session.py:1269] INFO An error was raised. This may be due to a preemption in a connected worker or parameter server. The current session will be closed and a new session will be created. This error may also occur due to a gRPC failure caused by high memory or network bandwidth usage in the parameter servers. If this error occurs repeatedly, try increasing the number of parameter servers assigned to the job. Error: From /job:ps/replica:0/task:14:\r\n```\r\nMonitored session would recreate another session and the worker would resume training.\r\n\r\n**The problem is other workers would get stuck in session.run and never return. This doesn't happen to every other worker, but just some of them.**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@piiswrong,\r\nTensorFlow 1.x is not actively supported. Could you please update TensorFlow to the latest stable version v2.4.1 and check if you are facing the same issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47694\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47694\">No</a>\n"]}, {"number": 47693, "title": "[tf.data] move checkpoint tests to kernel tests part 5", "body": "This PR is a continuation of #47668  and moves the checkpoint tests of:\r\n\r\n1. batch\r\n2. cache\r\n3. concatenate\r\n4. filter\r\n5. fixed length record\r\n6. flat map\r\n7. interleave\r\n8. padded batch\r\n9. prefetch\r\n10. range\r\n11. rebatch\r\n12. shard\r\n13. shuffle\r\n14. text line\r\n15. tf record\r\n16. unbatch and\r\n17. zip\r\n dataset serialization tests to kernel tests\r\n\r\nTEST LOG\r\n```\r\n//tensorflow/python/data/kernel_tests:batch_test                         PASSED in 15.5s\r\n//tensorflow/python/data/kernel_tests:cache_test                         PASSED in 13.4s\r\n//tensorflow/python/data/kernel_tests:concatenate_test                   PASSED in 3.7s\r\n//tensorflow/python/data/kernel_tests:filter_test                        PASSED in 12.4s\r\n//tensorflow/python/data/kernel_tests:fixed_length_record_dataset_test   PASSED in 3.6s\r\n  Stats over 4 runs: max = 3.6s, min = 2.8s, avg = 3.3s, dev = 0.4s\r\n//tensorflow/python/data/kernel_tests:flat_map_test                      PASSED in 15.7s\r\n//tensorflow/python/data/kernel_tests:interleave_test                    PASSED in 24.4s\r\n//tensorflow/python/data/kernel_tests:padded_batch_test                  PASSED in 8.0s\r\n//tensorflow/python/data/kernel_tests:prefetch_test                      PASSED in 4.7s\r\n//tensorflow/python/data/kernel_tests:range_test                         PASSED in 3.9s\r\n//tensorflow/python/data/experimental/kernel_tests:rebatch_dataset_test  PASSED in 10.3s\r\n//tensorflow/python/data/kernel_tests:shard_test                         PASSED in 13.4s\r\n//tensorflow/python/data/kernel_tests:shuffle_test                       PASSED in 38.8s\r\n//tensorflow/python/data/kernel_tests:text_line_dataset_test             PASSED in 5.6s\r\n  Stats over 4 runs: max = 5.6s, min = 2.8s, avg = 4.2s, dev = 1.3s\r\n//tensorflow/python/data/kernel_tests:tf_record_dataset_test             PASSED in 8.5s\r\n  Stats over 4 runs: max = 8.5s, min = 2.7s, avg = 5.6s, dev = 2.9s\r\n//tensorflow/python/data/kernel_tests:unbatch_test                       PASSED in 5.0s\r\n//tensorflow/python/data/kernel_tests:zip_test                           PASSED in 5.0s\r\n```\r\n\r\nAlso, this PR is a part of the larger cleanup as discussed in point 4 of https://github.com/tensorflow/tensorflow/pull/46761#issuecomment-770059963\r\ncc: @jsimsa I went ahead with a bulk change to speed things up before the 2.5 branch cut.\r\n\r\n", "comments": ["@jsimsa made the changes!", "@jsimsa I was working on the next PR and was able to move all the tests except these two:\r\n- [checkpoint_input_pipeline_hook_test.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/experimental/kernel_tests/serialization/checkpoint_input_pipeline_hook_test.py)\r\n- [serialization_integration_test.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/experimental/kernel_tests/serialization/serialization_integration_test.py)\r\n\r\nWhere do you suggest that we put them?", "> @jsimsa I was working on the next PR and was able to move all the tests except these two:\r\n> \r\n> * [checkpoint_input_pipeline_hook_test.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/experimental/kernel_tests/serialization/checkpoint_input_pipeline_hook_test.py)\r\n> * [serialization_integration_test.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/experimental/kernel_tests/serialization/serialization_integration_test.py)\r\n> \r\n> Where do you suggest that we put them?\r\n\r\nBoth should go to `experimental/kernel_tests` (i.e. move one level up in the directory hierarchy). The second test should also be renamed to `make_saveable_from_iterator_test.py`.", "@jsimsa thanks. I made the change locally and will raise the PR once this is merged."]}, {"number": 47692, "title": "multi-threading on tensorflow models", "body": "Hello;\r\n\r\nI have a TensorFlow model in .h5. I call it from a Flask App via /predict\r\n\r\nmy project consist on predicting the sentiment of each entering comment from tweeter in streaming. \r\nI want to load the model one time (at the beginning) , then many threads or processes use this model (share it)\r\n\r\nI get a Lock error.\r\n\r\nHow can I fix this bug, how to allow many threads to share the same model at the same time without locking.?\r\n\r\nThanks", "comments": ["Same problem here", "@dihiaselma \r\nPlease share simple stand alone code to replicate the issue faced.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47692\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47692\">No</a>\n"]}, {"number": 47691, "title": "Numpy v1.20+ compatibility", "body": "**System information**\r\n- TensorFlow version (you are using): tf-nightly-gpu v2.5.0-dev20201214\r\n- Are you willing to contribute it (Yes/No):\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nTensorflow is incompatible with numpy v1.20+ (recently released). This version introduces new functions and changes behaviours.\r\n\r\n**Will this change the current api? How?**\r\nThere seems to be an issue with the way tensorflow or numpy  handles symbolic tensors\r\n\r\n**Who will benefit with this feature?**\r\nAnyone wishing to use the new features or packages where numpy is a dependency. Virtual environments can circumvent this, but sometimes having numpy v1.20 in the same environment as tf can be neecssary or preferable\r\n\r\n**Any Other info.**\r\n", "comments": ["Given that numpy is backwards incompatible, TF needs a lot of work to support both pre 1.20 and after 1.20 numpy.", "Also we are not strictly aligned with numpy versioning everywhere in the repository:\r\n\r\n```\r\ntensorflow/lite/tools/pip_package/setup.py:        'numpy >= 1.16.0',\r\ntensorflow/lite/tools/pip_package/setup_with_binary.py:        'numpy ~= 1.19.2',  # Higher versions have a compatibility issue.\r\ntensorflow/lite/micro/examples/magic_wand/train/requirements.txt:numpy==1.16.2\r\ntensorflow/lite/micro/examples/hello_world/train/train_hello_world_model.ipynb:            \"Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.4.0rc0) (1.19.4)\\n\",\r\ntensorflow/tools/dockerfiles/dockerfiles/devel-gpu-jupyter.Dockerfile:    'numpy<1.19.0' \\\r\ntensorflow/tools/dockerfiles/dockerfiles/devel-cpu-jupyter.Dockerfile:    'numpy<1.19.0' \\\r\ntensorflow/tools/dockerfiles/dockerfiles/ppc64le/devel-cpu-ppc64le-jupyter.Dockerfile:    'numpy<1.19.0' \\\r\ntensorflow/tools/dockerfiles/dockerfiles/ppc64le/devel-gpu-ppc64le.Dockerfile:    'numpy<1.19.0' \\\r\ntensorflow/tools/dockerfiles/dockerfiles/ppc64le/devel-cpu-ppc64le.Dockerfile:    'numpy<1.19.0' \\\r\ntensorflow/tools/dockerfiles/dockerfiles/ppc64le/devel-gpu-ppc64le-jupyter.Dockerfile:    'numpy<1.19.0' \\\r\ntensorflow/tools/dockerfiles/dockerfiles/devel-gpu.Dockerfile:    'numpy<1.19.0' \\\r\ntensorflow/tools/dockerfiles/dockerfiles/arm64v8/devel-cpu-arm64v8-jupyter.Dockerfile:    'numpy<1.19.0' \\\r\ntensorflow/tools/dockerfiles/dockerfiles/arm64v8/devel-cpu-arm64v8.Dockerfile:    'numpy<1.19.0' \\\r\ntensorflow/tools/dockerfiles/dockerfiles/devel-cpu.Dockerfile:    'numpy<1.19.0' \\\r\ntensorflow/tools/dockerfiles/partials/ubuntu/bazel.partial.Dockerfile:    'numpy<1.19.0' \\\r\ntensorflow/tools/dockerfiles/partials/ubuntu/bazelbuild-arm64v8.partial.Dockerfile:    'numpy<1.19.0' \\\r\ntensorflow/tools/dockerfiles/partials/ubuntu/bazelbuild.partial.Dockerfile:    'numpy<1.19.0' \\\r\ntensorflow/tools/pip_package/setup.py:    'numpy ~= 1.19.2',\r\ntensorflow/tools/ci_build/install/install_centos_pip_packages.sh:pip2 install --upgrade numpy==1.14.5\r\ntensorflow/tools/ci_build/install/install_centos_pip_packages.sh:pip3 install --upgrade numpy==1.14.5\r\ntensorflow/tools/ci_build/install/install_python3.6_pip_packages.sh:pip3 install --no-binary=:all: --upgrade numpy==1.14.5\r\ntensorflow/tools/ci_build/install/install_pip_packages.sh:  pip3 install --no-binary=:all: --upgrade numpy==1.14.5\r\ntensorflow/tools/ci_build/install/install_pip_packages.sh:  pip3 install --upgrade numpy==1.14.5\r\ntensorflow/tools/ci_build/install/install_pip_packages_by_version.sh:  \"numpy ~= 1.19.2\"\r\ntensorflow/tools/ci_build/release/common.sh:  \"${PIP_CMD}\" install --user 'numpy ~= 1.19.2'\r\ntensorflow/tools/ci_build/release/common.sh:  ${PIP_CMD} install --user 'numpy ~= 1.19.2'\r\ntensorflow/tools/ci_build/release/common.sh:  ${PIP_CMD} install 'numpy ~= 1.19.2'\r\ntensorflow/tools/ci_build/release/common.sh:  ${PIP_CMD} install 'numpy ~= 1.19.2' --user\r\ntensorflow/tools/ci_build/release/common_win.bat:%PY_EXE% -m pip install \"numpy ~= 1.19.2\"\r\n```", "This answer on StackOverflow has a solution for the often reported \"Cannot convert a symbolic Tensor (gru/strided_slice:0) to a numpy array.\" Exception thrown by array_ops.py: https://stackoverflow.com/a/66486051\r\n\r\nPerhaps you want take it into account when updating the compatibility with numpy 1.20", "@mihaimaruseac Which \"backwards incompatibilities\" ([release notes](https://numpy.org/doc/1.20/release/1.20.0-notes.html)) are preventing TensorFlow from supporting NumPy 1.20?", "Is there any estimation when Tensorflow will work with numpy 1.20.x?", "Yea, I really need it to be updated, I've been unable to use tensorflow for over a month now \ud83d\ude15 ", "Here the PR to explore CI fails https://github.com/tensorflow/tensorflow/pull/48918", "CI fails because the installer cannot install Numpy 1.20 for a Python 3.6 ecosystem don't help you at all.", "> Given that numpy is backwards incompatible, TF needs a lot of work to support both pre 1.20 and after 1.20 numpy.\r\n\r\nI don't think that is true. There should be nothing in NumPy 1.20 which has not been deprecated a long time ago.\r\nhttps://numpy.org/doc/stable/release/1.20.0-notes.html#expired-deprecations.\r\nAny fixes of failures because old TF code does not work with NumPy 1.20 anymore will also happily work with NumPy 1.19.", "@bnavigator Before you downvoting if you are talking about:\r\n```\r\n AMD ROCm -- Community CI Build \u2014 rocm CI build failed\r\nIntel\u00ae oneDNN -- Community CI Build \u2014 oneDNN Threadpool native format unit tests failed\r\n```\r\nThese are external builds, on external \"community\" build servers/VM so they are not under TF control.\r\nThen we have our [TF Macos fails](https://source.cloud.google.com/results/invocations/7ba94070-327a-4429-b736-0903f9c4f06f/targets/%2F%2Ftensorflow%2Flite%2Fpython:lite_v2_test/tests). That CI VM probably is still under python 3.7.3.\r\n\r\nThen, as you can see, we still release the next TF version (2.5.0) for python 3.6: https://pypi.org/project/tensorflow/2.5.0rc3/#files\r\n", "So why are you *requiring* 1.20 for all of your containers? That's not how you check compatibility with 1.20. As I said, you don't have to drop support for older NumPy versions and thus python 3.6. The failures with NumPy 1.20 are already partially reported. Just fix those.", "> So why are you requiring 1.20 for all of your containers?\r\nThis is not the problem as it is already mentioned in the PRs the there is another one to upgrade some base images as requested in another ticket (Docker images, wheels and CI VM are not strictly aligned). \r\n\r\nAs you have commented also on the PR I will reply on the PR.", "I see there is a work-around identified, but is there also an underlying NumPy bug in the 1.20 release? If so, it would be good to identify that and report it upstream.", "I thinkt that it is an incompatibility how Tensorflow calls NumPy functions like np.prod(). NumPy cannot automatically know what to do with a Tensorflow symbolic Tensor passed to it. NumPy's API says it must be array_like.", "@bnavigator Would you happen to know why it works in numpy 1.19?", "NumPy tries very hard to preserve backwards compatibility. So it's a little surprising that there is a change in behavior here from NumPy 1.19 to 1.20.\r\n\r\nMy guess is that TensorFlow is doing something relatively \"unusual\" here that isn't well tested inside NumPy. But it would be great to know exactly what that it. In the ideal case, we would identify a reproduction of this issue in an artificial example without using TensorFlow itself.", "Yeah, but even the error is thrown by TF not by np:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/368c14ecb296e87c67e8d77aba175e02d5b93741/tensorflow/python/framework/ops.py#L857-L870\r\n\r\nTensorflow itself says, that there is no support for passing a Tensor to `np.prod()`.\r\n\r\nI can't find the release note for it, but I think 1.20 internally moved some function implementations to ufuncs. See the tracebacks in #47242, #47263.\r\n\r\nNote that np 1.21 expects nothing else than `AttributeError`. https://github.com/numpy/numpy/pull/19001", "FYI: On https://github.com/tensorflow/models/issues/9706  @Amir22010 provides this fix for numpy 1.20's NotImplementedError: Cannot convert a symbolic Tensor (strided_slice:0) to a numpy array error:\r\n```patch\r\n--- tensorflow/python/ops/array_ops.py.orig\t2021-07-06 16:14:00.000000000 -0600\r\n+++ tensorflow/python/ops/array_ops.py\t2021-07-06 16:15:07.000000000 -0600\r\n@@ -39,6 +39,7 @@\r\n # pylint: disable=wildcard-import\r\n from tensorflow.python.ops.gen_array_ops import *\r\n from tensorflow.python.ops.gen_array_ops import reverse_v2 as reverse  # pylint: disable=unused-import\r\n+from tensorflow.python.ops.math_ops import reduce_prod\r\n from tensorflow.python.types import core\r\n from tensorflow.python.util import deprecation\r\n from tensorflow.python.util import dispatch\r\n@@ -2801,7 +2802,7 @@\r\n \r\n def _constant_if_small(value, shape, dtype, name):\r\n   try:\r\n-    if np.prod(shape) < 1000:\r\n+    if reduce_prod(shape) < 1000:\r\n       return constant(value, shape=shape, dtype=dtype, name=name)\r\n   except TypeError:\r\n     # Happens when shape is a Tensor, list with Tensor elements, etc.\r\n```", "@joshua-cogliati-inl, it's the same fix as in #48935 (0f8fde42d09b199d02cf5d9d79fe76ebf1d260ef)", "> Given that numpy is backwards incompatible, TF needs a lot of work to support both pre 1.20 and after 1.20 numpy.\r\n\r\nI'd argue that current compatibility is more important than assuring backwards compatibility. However, backwards compatibility could be placed on the TF backlog. ", "As I said 2 months ago, I haven't seen anything that breaks compatibility with a reasonable recent numpy. They do not change the API between years long deprecations in between.", "> > Given that numpy is backwards incompatible, TF needs a lot of work to support both pre 1.20 and after 1.20 numpy.\r\n> \r\n> I'd argue that current compatibility is more important than assuring backwards compatibility. However, backwards compatibility could be placed on the TF backlog.\r\n\r\nTF is Google first. If Google code uses a certain version of a TF dependency we have to use/support that in OSS too", "> Given that numpy is backwards incompatible, TF needs a lot of work to support both pre 1.20 and after 1.20 numpy.\r\n\r\nUm, I am not sure what you are talking about.  The pull request https://github.com/tensorflow/tensorflow/pull/48935 makes 3 changes:\r\n 1.  Switches from calling numpy.prod to tensorflow.python.ops.math_ops.reduce_prod \r\n 2. Changes some shape constants from floats to integers (things like 1.0 -> 1)\r\n 3. Updates which numpy versions are supported\r\n\r\nNone of those should cause any backwards compatibility problems.", "Is there a 'best guess' on far out the fix is? a month? 6 months?", "> Is there a 'best guess' on far out the fix is? a month? 6 months?\r\n\r\nAs soon as someone approves the existing fix: https://github.com/tensorflow/tensorflow/pull/48935 (which was created on 2021-May-6)", "They cannot approve the fix because they have no CI to properly check it. There might be more problems with NumPy 1.20 and 1.21 compatibility.", "An alternate fix (which also works for me) for numpy 1.20's NotImplementedError: Cannot convert a symbolic Tensor (strided_slice:0) to a numpy array error was in https://github.com/tensorflow/tensorflow/pull/49008 which was merged and then reverted:\r\n```patch\r\n\r\ndiff --git a/tensorflow/python/ops/array_ops.py b/tensorflow/python/ops/array_ops.py\r\nindex f7fc323971461..a8d7bb04cc759 100644\r\n--- a/tensorflow/python/ops/array_ops.py\r\n+++ b/tensorflow/python/ops/array_ops.py\r\n@@ -2893,6 +2893,7 @@ def matrix_set_diag(\r\n \r\n def _constant_if_small(value, shape, dtype, name):\r\n   try:\r\n+    shape = nest.map_structure(tensor_util.constant_value, shape)\r\n     if np.prod(shape) < 1000:\r\n       return constant(value, shape=shape, dtype=dtype, name=name)\r\n   except TypeError:\r\n@@ -3204,6 +3205,7 @@ def ones(shape, dtype=dtypes.float32, name=None):\r\n       one = np.ones([]).astype(dtype.as_numpy_dtype)\r\n     else:\r\n       one = 1\r\n+\r\n     if not isinstance(shape, ops.Tensor):\r\n       try:\r\n         if not context.executing_eagerly():\r\n ```", "Also, here is a short piece of code that fails with numpy 1.21.0 and tensorflow 2.4.1:\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nmodel = keras.models.Sequential([\r\n    keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 2]),\r\n    keras.layers.TimeDistributed(keras.layers.Dense(20))\r\n])\r\n```\r\n\r\nwith the error:\r\n```\r\n2021-07-29 16:40:33.172597: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nTraceback (most recent call last):\r\n  File \"/Users/fred/learning/tensorflow/trigger_error.py\", line 4, in <module>\r\n    model = keras.models.Sequential([\r\n  File \"/Users/fred/miniconda3/envs/bad_conda_env/lib/python3.9/site-packages/tensorflow/python/training/tracking/base.py\", line 517, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/Users/fred/miniconda3/envs/bad_conda_env/lib/python3.9/site-packages/tensorflow/python/keras/engine/sequential.py\", line 144, in __init__\r\n    self.add(layer)\r\n  File \"/Users/fred/miniconda3/envs/bad_conda_env/lib/python3.9/site-packages/tensorflow/python/training/tracking/base.py\", line 517, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/Users/fred/miniconda3/envs/bad_conda_env/lib/python3.9/site-packages/tensorflow/python/keras/engine/sequential.py\", line 208, in add\r\n    layer(x)\r\n  File \"/Users/fred/miniconda3/envs/bad_conda_env/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 660, in __call__\r\n    return super(RNN, self).__call__(inputs, **kwargs)\r\n  File \"/Users/fred/miniconda3/envs/bad_conda_env/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 951, in __call__\r\n    return self._functional_construction_call(inputs, args, kwargs,\r\n  File \"/Users/fred/miniconda3/envs/bad_conda_env/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1090, in _functional_construction_call\r\n    outputs = self._keras_tensor_symbolic_call(\r\n  File \"/Users/fred/miniconda3/envs/bad_conda_env/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 822, in _keras_tensor_symbolic_call\r\n    return self._infer_output_signature(inputs, args, kwargs, input_masks)\r\n  File \"/Users/fred/miniconda3/envs/bad_conda_env/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 863, in _infer_output_signature\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"/Users/fred/miniconda3/envs/bad_conda_env/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent_v2.py\", line 1157, in call\r\n    inputs, initial_state, _ = self._process_inputs(inputs, initial_state, None)\r\n  File \"/Users/fred/miniconda3/envs/bad_conda_env/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 859, in _process_inputs\r\n    initial_state = self.get_initial_state(inputs)\r\n  File \"/Users/fred/miniconda3/envs/bad_conda_env/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 642, in get_initial_state\r\n    init_state = get_initial_state_fn(\r\n  File \"/Users/fred/miniconda3/envs/bad_conda_env/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 2506, in get_initial_state\r\n    return list(_generate_zero_filled_state_for_cell(\r\n  File \"/Users/fred/miniconda3/envs/bad_conda_env/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 2987, in _generate_zero_filled_state_for_cell\r\n    return _generate_zero_filled_state(batch_size, cell.state_size, dtype)\r\n  File \"/Users/fred/miniconda3/envs/bad_conda_env/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 3003, in _generate_zero_filled_state\r\n    return nest.map_structure(create_zeros, state_size)\r\n  File \"/Users/fred/miniconda3/envs/bad_conda_env/lib/python3.9/site-packages/tensorflow/python/util/nest.py\", line 659, in map_structure\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"/Users/fred/miniconda3/envs/bad_conda_env/lib/python3.9/site-packages/tensorflow/python/util/nest.py\", line 659, in <listcomp>\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"/Users/fred/miniconda3/envs/bad_conda_env/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 3000, in create_zeros\r\n    return array_ops.zeros(init_state_size, dtype=dtype)\r\n  File \"/Users/fred/miniconda3/envs/bad_conda_env/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/Users/fred/miniconda3/envs/bad_conda_env/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\", line 2820, in wrapped\r\n    tensor = fun(*args, **kwargs)\r\n  File \"/Users/fred/miniconda3/envs/bad_conda_env/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\", line 2869, in zeros\r\n    output = _constant_if_small(zero, shape, dtype, name)\r\n  File \"/Users/fred/miniconda3/envs/bad_conda_env/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\", line 2805, in _constant_if_small\r\n    if np.prod(shape) < 1000:\r\n  File \"<__array_function__ internals>\", line 5, in prod\r\n  File \"/Users/fred/miniconda3/envs/bad_conda_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py\", line 3051, in prod\r\n    return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\r\n  File \"/Users/fred/miniconda3/envs/bad_conda_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py\", line 86, in _wrapreduction\r\n    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\r\n  File \"/Users/fred/miniconda3/envs/bad_conda_env/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\", line 852, in __array__\r\n    raise NotImplementedError(\r\nNotImplementedError: Cannot convert a symbolic Tensor (lstm/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\r\n```", "What's the status on this? i see that #48935 has been approved", "> What's the status on this? i see that #48935 has been approved\r\n\r\nIf I understand correctly, the \"approval\" is just a trigger to run the tests.", "We now have wide ranges of dependencies", "i'm unsure about what that means", "Hi, is this a matter of days or rather of weeks?", "months :(", "Has anyone built this for ubuntu with gpu support and would share the binaries?", "I think latest version 2.6 and nightly support Numpy 1.21.2 now. Attaching [issue](https://github.com/tensorflow/tensorflow/issues/52380#issuecomment-943971924) for reference.", "I think this is now resolved", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47691\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47691\">No</a>\n", "For me, I could not upgrade to `numpy`>1.20 with TF 2.6.2 (i.e. the latest). However, after upgrading to TF 2.7.0 it does. "]}, {"number": 47690, "title": "build_pip_package_with_bazel: add support of Bazel startup option", "body": "Bazel startup options must appear before the name of the Bazel command.\r\n\r\nThis commit adds a BAZEL_ARGS variable to be able to specify startup option.\r\n\r\nSigned-off-by: Julien STEPHAN <jstephan@baylibre.com>", "comments": ["Hi all, \r\n\r\nI am writing a bitbake tensorflow lite recipe based on [meta-tensorflow](http://git.yoctoproject.org/cgit/cgit.cgi/meta-tensorflow). In order to correctly cross compile tensorflow a `BAZEL_ARGS` variable is added defining startup options such as `--output_user_root` or `--output_base`.\r\n\r\nI was not able to correctly compile the `tflite_runtime` using `build_pip_package_with_bazel.sh` script because startup options were not given to the script.\r\n\r\nI submit this merge request, because I think it can be useful to be able to specify startup option in other situation as well, not only in the case of the bitbake recipe. I added the variable as `BAZEL_ARGS` because it is defined this way in `meta-tensorflow` but I am open to a different name.\r\n\r\nJulien\r\n\r\n \r\n", "Can't you use CUSTOM_BAZEL_FLAGS instead?\r\n```\r\nCUSTOM_BAZEL_FLAGS=--output_user_root tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh\r\n```", "Hi @terryheo \r\n\r\n> Can't you use CUSTOM_BAZEL_FLAGS instead?\r\n> \r\n> ```\r\n> CUSTOM_BAZEL_FLAGS=--output_user_root tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh\r\n> ```\r\n\r\nno I can't use CUSTOM_BAZEL_FLAGS instead. According to the [bazel manual](https://docs.bazel.build/versions/master/user-manual.html#startup_options) startup options must appear _before_ the bazel command:\r\n\r\n> Also, these options must appear before the name of the Bazel command. \r\n\r\nCUSTOM_BAZEL_FLAGS appear after the `build` command inside `tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh`, so it cannot be used to pass such arguments (I do use CUSTOM_BAZEL_FLAGS  in my recipe to pass other arguments such as `--jobs=` or `--cpu=`)  \r\n\r\nAnyway, I tried to put it inside the CUSTOM_BAZEL_ARGS and I got the following error:\r\n```\r\nUnrecognized option: --output_user_root=\r\n```\r\n\r\n\r\nI explained it in the commit message, but maybe it needs to be more clear? "]}, {"number": 47689, "title": "layers.LocallyConnected2D throws error when saving a model in tf, saving in .h5 works", "body": "Hi, tensorflow community\r\n\r\nI'm playing around with LocallyConnected2D and found some weird error when I tried to save a model (model.save).\r\n\r\nWhen I gave the layer implementation=1, the model was saved without any errors.\r\nBut, if I set implementation=2, it gave me this error.\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/tonglab/Documents/Project/PycharmProjects/LCN/LCN_Keras/LocalConn2d_Keras.py\", line 84, in <module>\r\n    model.save('keras1')\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 2002, in save\r\n    signatures, options, save_traces)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\", line 157, in save_model\r\n    signatures, options, save_traces)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save.py\", line 89, in save\r\n    save_lib.save(model, filepath, signatures, options)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\", line 1033, in save\r\n    obj, signatures, options, meta_graph_def)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\", line 1198, in _build_meta_graph\r\n    return _build_meta_graph_impl(obj, signatures, options, meta_graph_def)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\", line 1133, in _build_meta_graph_impl\r\n    checkpoint_graph_view)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/saved_model/signature_serialization.py\", line 75, in find_function_to_export\r\n    functions = saveable_view.list_functions(saveable_view.root)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\", line 151, in list_functions\r\n    self._serialization_cache)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 2613, in _list_functions_for_serialization\r\n    Model, self)._list_functions_for_serialization(serialization_cache)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 3087, in _list_functions_for_serialization\r\n    .list_functions_for_serialization(serialization_cache))\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/base_serialization.py\", line 94, in list_functions_for_serialization\r\n    fns = self.functions_to_serialize(serialization_cache)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py\", line 79, in functions_to_serialize\r\n    serialization_cache).functions_to_serialize)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py\", line 95, in _get_serialized_attributes\r\n    serialization_cache)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/model_serialization.py\", line 57, in _get_serialized_attributes_internal\r\n    serialization_cache))\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py\", line 104, in _get_serialized_attributes_internal\r\n    functions = save_impl.wrap_layer_functions(self.obj, serialization_cache)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py\", line 155, in wrap_layer_functions\r\n    original_fns = _replace_child_layer_functions(layer, serialization_cache)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py\", line 274, in _replace_child_layer_functions\r\n    serialization_cache).functions)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py\", line 95, in _get_serialized_attributes\r\n    serialization_cache)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py\", line 104, in _get_serialized_attributes_internal\r\n    functions = save_impl.wrap_layer_functions(self.obj, serialization_cache)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py\", line 193, in wrap_layer_functions\r\n    fn.get_concrete_function()\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py\", line 549, in get_concrete_function\r\n    self.call_collection.add_trace(*args, **kwargs)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py\", line 423, in add_trace\r\n    fn.get_concrete_function(*args, **kwargs)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py\", line 550, in get_concrete_function\r\n    return super(LayerCall, self).get_concrete_function(*args, **kwargs)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 1299, in get_concrete_function\r\n    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 1205, in _get_concrete_function_garbage_collected\r\n    self._initialize(args, kwargs, add_initializers_to=initializers)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 726, in _initialize\r\n    *args, **kwds))\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2969, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3361, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3206, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 990, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 634, in wrapped_fn\r\n    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py\", line 527, in wrapper\r\n    ret = method(*args, **kwargs)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py\", line 570, in call_and_return_conditional_losses\r\n    call_output = layer_call(inputs, *args, **kwargs)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/layers/local.py\", line 615, in call\r\n    self.compute_output_shape(inputs.shape))\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/layers/local.py\", line 782, in local_conv_matmul\r\n    [K.shape(output_flat)[0],] + output_shape.as_list()[1:])\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\", line 3020, in reshape\r\n    return array_ops.reshape(x, shape)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 195, in reshape\r\n    result = gen_array_ops.reshape(tensor, shape, name)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 8378, in reshape\r\n    \"Reshape\", tensor=tensor, shape=shape, name=name)\r\n  File \"/home/tonglab/anaconda3/envs/tf_2_4_1/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 540, in _apply_op_helper\r\n    (input_name, err))\r\nValueError: Tried to convert 'shape' to a tensor and failed. Error: None values not supported.\r\n\r\nMy tensorflow version is 2.4.1.\r\n\r\nAny advice will be appreciated.\r\nThank you.", "comments": ["@hojin89 \r\nIt would be great if you shared the source code of this error. \r\nAlso, from the log, I speculate that the layer and its weights are initialized with a placeholder (dummy Tensor), and thus it says that the shape is not defined. Other cause may be that the shapes of some Tensors may not be defined before runtime, as layers are executed in graph mode.", "@hojin89,\r\nAs mentioned by @AdityaKane2001, could you please provide the complete code and the TensorFlow version you are using so that we can reproduce the issue on our end. Thanks!\r\n", "Thanks for the replies.\r\n\r\nHere is the complete code:\r\n\r\n    import numpy as np\r\n    import matplotlib.pyplot as plt\r\n    import tensorflow as tf\r\n    from tensorflow import keras\r\n    from tensorflow.keras import layers\r\n    from tensorflow.keras import initializers\r\n    \r\n    num_classes = 10\r\n    input_shape = (28, 28, 1)\r\n    \r\n    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\n    \r\n    x_train = x_train.astype(\"float32\") / 255\r\n    x_test = x_test.astype(\"float32\") / 255\r\n    \r\n    x_train = np.expand_dims(x_train, -1)\r\n    x_test = np.expand_dims(x_test, -1)\r\n    print(\"x_train shape:\", x_train.shape)\r\n    print(x_train.shape[0], \"train samples\")\r\n    print(x_test.shape[0], \"test samples\")\r\n    \r\n    y_train = keras.utils.to_categorical(y_train, num_classes)\r\n    y_test = keras.utils.to_categorical(y_test, num_classes)\r\n    \r\n    strategy = tf.distribute.MirroredStrategy()\r\n    with strategy.scope():\r\n\r\n          model = keras.Sequential(\r\n              [\r\n                  keras.Input(shape=input_shape),\r\n                  layers.LocallyConnected2D(16, kernel_size=(9, 9), activation=\"relu\", implementation=2),\r\n                  layers.MaxPooling2D(pool_size=(2, 2)),\r\n                  layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\r\n                  layers.MaxPooling2D(pool_size=(2, 2)),\r\n                  layers.Flatten(),\r\n                  layers.Dropout(0.5),\r\n                  layers.Dense(num_classes, activation=\"softmax\"),\r\n              ]\r\n          )\r\n      \r\n          model.summary()\r\n      \r\n          batch_size = 128\r\n          epochs = 1\r\n      \r\n          opt = keras.optimizers.Adam(learning_rate=0.001)\r\n          model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\r\n          model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\r\n      \r\n          score = model.evaluate(x_test, y_test, verbose=0)\r\n          print(\"Test loss:\", score[0])\r\n          print(\"Test accuracy:\", score[1])\r\n      \r\n          model.save('keras1')\r\n\r\nAs mentioned earlier, my tensorflow version is 2.4.1.\r\nIf I use \"implementation=1\", the code works well without any errors.\r\n\r\nIt seems like the error occurs when it calls \"self.compute_output_shape(inputs.shape))\" in local.py.\r\ninputs.shape should be (None, 28, 28, 1), but for some reason, it shows me (None, None, None, 1) at some point.\r\n\r\nI'm still trying to figure this out. \r\nIf you could give any advice, that'll be appreciated.\r\n\r\n\r\n", "`model.save('keras1.h5')` works as expected. Saving model in a directory results in the aforementioned error. Gist [here](https://colab.research.google.com/gist/AdityaKane2001/c2072f8433a13975b815dbdfa3819d7c/47689.ipynb).", "@ymodak,\r\nI was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/cfbf7c9942e89e7b286509fe7776cdf5/47689.ipynb). Thanks!", "Adding the contributions welcome label to this issue for further investigation by the community. If you are interested in working on this issue, please leave a comment and I will assign it to you. Thanks!", "@AdityaKane2001 @hojin89  \r\nOne can observe in this [file](https://github.com/tensorflow/tensorflow/blob/be1598592ee45a0abe37a5d8fa8e53420a8c554e/tensorflow/python/keras/layers/local.py#L325) that LocallyConnected2D layer uses deprecated version of keras and a bunch of deprecated ops. According to its documention, implementation=2, stores weights of each layer in a dense (but sparsely-populated) 2D matrix and implements the forward pass as a single matrix-multiply. So while saving as TF SavedModel, compute_output_shape() is not able to convert the shape and raises the error you mentioned. However it gets saved as .h5 keras model. You can see the difference between TF SavedModel and .H5 model [here](https://www.tensorflow.org/guide/keras/save_and_serialize).\r\n", "@Suraj1199 \r\nThe link is broken..\r\n", "@AdityaKane2001  It should be working now.", "@Suraj1199\r\nThanks", "> `model.save('keras1.h5')` works as expected. Saving model in a directory results in the aforementioned error. Gist [here](https://colab.research.google.com/gist/AdityaKane2001/c2072f8433a13975b815dbdfa3819d7c/47689.ipynb).\r\n\r\nTo save the model in TF SavedModel format in a directory ('keras1' in this case) use Conv2D layer intead of LocallyConnected2D.\r\n", "Side note, this is true for `implementation=3` as well. Only `implementation=1` works.", "@hojin89 \r\nyou can Save the entire model to a HDF5 file.\r\n The '.h5' extension indicates that the model should be saved to HDF5.\r\n```model.save('my_model.h5')```", "@nikitamaia , please assign this issue to me since I am working on it as part of PR #49230 .", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47689\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47689\">No</a>\n"]}, {"number": 47688, "title": "Create Trying demo", "body": "ad", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47688) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 47687, "title": "Extremely poor data loader performance, taking too much time to load first batch.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not applicable.\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7.10\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: Tesla T4, 15109MiB.\r\n\r\n**Describe the current behaviour**\r\nI'm trying to create a custom data loader for training data, where the data has been downloaded from `tensorflow_datasets`,\r\nand a number of data preprocessing have been applied.\r\n\r\nBut the data loader is taking too much time to load the first batch of each epoch. \r\n\r\n```python\r\nfor lrs, hrs in train_data:\r\n    break\r\n```\r\n\r\nThe code above is taking well over 5 minutes, which means it is taking 5 minutes just to load the first batch. **Though, once the first batch is loaded, then the loading of next batches are faster, and the rest of the batches are being loaded without any issues, each taking not more than 1 second**\r\n\r\n**But again once the loop/epoch is over, then starting through the first batch again is taking 5 minutes.**\r\n\r\nI've tested the issue with/without `model.fit` function, facing the same issue in both cases.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe loading of the first batch is actually expected to be slow, but not this slow, cuz even the training of the model each batch is not taking this much time.\r\n\r\n**Standalone code to reproduce the issue**\r\nBelow is the completed code to reproduce the issue.\r\n\r\n```python\r\nHR_SIZE = 128\r\nSCALE = 4\r\nBATCH_SIZE = 32\r\n\r\ntrain_data = tfds.load(f'div2k/bicubic_x4', split = 'train')\r\n# this line will download about 4GB of data....\r\n# so using a colab notebook is recommended to reproduce the issue.\r\n\r\n@tf.function\r\ndef random_crop(example):\r\n    lr = example['lr']\r\n    hr = example['hr']\r\n\r\n    lr_crop_size = HR_SIZE // SCALE\r\n    lr_shape = tf.shape(lr)[:2]\r\n\r\n    lr_w = tf.random.uniform(shape = (), maxval = lr_shape[1] - lr_crop_size + 1, dtype = tf.int32)\r\n    lr_h = tf.random.uniform(shape = (), maxval = lr_shape[0] - lr_crop_size + 1, dtype = tf.int32)\r\n\r\n    hr_w = lr_w * SCALE\r\n    hr_h = lr_h * SCALE\r\n\r\n    lr_cropped = lr[lr_h:lr_h + lr_crop_size, lr_w: lr_w + lr_crop_size]\r\n    hr_cropped = hr[hr_h:hr_h + HR_SIZE, hr_w: hr_w + HR_SIZE]\r\n\r\n    return lr_cropped, hr_cropped\r\n\r\n@tf.function\r\ndef random_horizontal_flip(lr, hr):\r\n    return tf.cond(tf.random.uniform(shape = (), maxval = 1) < 0.5,\r\n                   lambda: (lr, hr),\r\n                   lambda: (tf.image.flip_left_right(lr),\r\n                            tf.image.flip_left_right(hr)))\r\n\r\n@tf.function\r\ndef random_vertical_flip(lr, hr):\r\n    return tf.cond(tf.random.uniform(shape = (), maxval = 1) < 0.5,\r\n                   lambda: (lr, hr),\r\n                   lambda: (tf.image.flip_up_down(lr),\r\n                            tf.image.flip_up_down(hr)))\r\n\r\n@tf.function\r\ndef random_gain(lr, hr):\r\n    gain_delta = tf.random.uniform(shape = (), minval = 0.7, maxval = 1.6)\r\n    return tf.cond(tf.random.uniform(shape = (), maxval = 1) < 0.5,\r\n                   lambda: (lr, hr),\r\n                   lambda: (tf.image.adjust_gamma(lr, gamma = 1.0, gain = gain_delta),\r\n                            tf.image.adjust_gamma(hr, gamma = 1.0, gain = gain_delta)))\r\n\r\n@tf.function\r\ndef random_contrast(lr, hr):\r\n    contrast_delta = tf.random.uniform(shape = (), minval = 0.8, maxval = 1.2)\r\n    return tf.cond(tf.random.uniform(shape = (), maxval = 1) < 0.5,\r\n                   lambda: (lr, hr),\r\n                   lambda: (tf.image.adjust_contrast(lr, contrast_factor = contrast_delta),\r\n                            tf.image.adjust_contrast(hr, contrast_factor = contrast_delta)))\r\n\r\n@tf.function\r\ndef random_rotate(lr, hr):\r\n    rn = tf.random.uniform(shape = (), maxval = 4, dtype=tf.int32)\r\n    return tf.image.rot90(lr, rn), tf.image.rot90(hr, rn)\r\n\r\ntrain_data = train_data.map(random_crop, num_parallel_calls = tf.data.AUTOTUNE)\r\ntrain_data = train_data.map(random_horizontal_flip, num_parallel_calls = tf.data.AUTOTUNE)\r\ntrain_data = train_data.map(random_vertical_flip, num_parallel_calls = tf.data.AUTOTUNE)\r\ntrain_data = train_data.map(random_gain, num_parallel_calls = tf.data.AUTOTUNE)\r\ntrain_data = train_data.map(random_contrast, num_parallel_calls = tf.data.AUTOTUNE)\r\ntrain_data = train_data.map(random_rotate, num_parallel_calls = tf.data.AUTOTUNE)\r\n\r\ntrain_data = train_data.batch(BATCH_SIZE, drop_remainder = True)\r\ntrain_data = train_data.prefetch(tf.data.AUTOTUNE)\r\ntrain_data = train_data.shuffle(800)\r\n\r\nfor lrs, hrs in train_data:\r\n    break\r\n```\r\n\r\nAnd I'm running all of my code in google colab.", "comments": ["@braindotai \r\nThe code shared is not reproducible, please find [gist here](https://colab.research.google.com/gist/Saduf2019/5e00a3e32ff530b8a9c826ee024d1980/untitled561.ipynb), and share the colab gist with the error reporeted.", "Here's [the link](https://colab.research.google.com/gist/braindotai/3b917caf1d9f72569e5af1162d77ca42/untitled561.ipynb#scrollTo=cRyFuojRqV6f) to the notebook for reproducing the issue. ", "@braindotai \r\nThe main cause of low performance is the size of the data and the number of operations. Note that the `tf.data.Dataset` actually consumes and preprocesses the data on the first call. At the first call, all of the data is preprocessed all at once.  Also, all the future batches are almost instantaneous, which means that the accelerator(GPU or TPU) will never be underfed during training. ", "@AdityaKane2001 \r\nSo does that mean this is expected behaviour?\r\nIf yes, then is there any way to apply the `.map` functions batch by batch instead of blocking the main thread process?\r\n\r\nI'm really worried because my data is only about 3 GB... with only 1600 quality images, and still in my case just the preprocessing of data is taking well over 4 to 5 minutes... which is surprisingly way greater than training my model for 1 epoch.\r\n\r\nSo in the case of a much larger and heavy dataset, using `tf.data` would not be a good experience... cuz in that case we'll just be spending most of our time applying `.map` functions. \ud83d\ude10\r\n ", "@braindotai \r\nI logged the shapes of each of the images in the dataset. The shapes of the images are not the same, some images have the same shape whereas others don't. This causes retracing of all the mapping functions, and that may be the reason of poor performance. If your application permits, please try adding a resizing function. Also, make sure that your mapping functions are taking  tf constructs as input. Else, the graphs will be retraced on every call.", "> Also, make sure that your mapping functions are taking tf constructs as input. Else, the graphs will be retraced on every call.\r\n\r\n@AdityaKane2001 \r\nI'm not sure if I know how to implement this. Could you please give an example for this?", "@braindotai \r\nAdd a `print('<function name>')` statement in all functions. When the dataset is transformed for the first time, these statements will appear each time a function is called. If it appears exactly once, then it's alright, as all functions decorated with `@tf.function` must be traced atleast once.\r\nMake sure that all arguments of a given function are TensorFlow constructs only, which means that arguments must be one of the following:\r\n`Tensor`,`TensorSpec`,`TensorArray`,etc..\r\nIf they are not, then your function is getting retraced multiple times, adding to the time and RAM usage.\r\n", "@AdityaKane2001 \r\nThanks for the support, I'll try it out and get back to you.", "Hey @braindotai  and @AdityaKane2001 , just wanted to also point out that in the code provided, a very costly `shuffle` operation is being done. \r\n\r\nI checked how large the original data is with:\r\n```\r\ntrain_data = tfds.load(f'div2k/bicubic_x4', split = 'train')\r\nprint(train_data.cardinality()) # --> 800\r\n```\r\n\r\nThe notebook in question has, as a final operation, `train_data = train_data.shuffle(800)`. Note that the dataset has already been batched into 32 at this point. With this shuffle op, `tf.data` actually iterates AND PREPROCESSES the entire dataset a whopping 32 times unnecessarily before producing the 1st batch. The correct `shuffle_buffer_size` to achieve a perfect shuffle should be 800/32 = 25 at this stage in the pipeline.\r\n\r\nFurthermore, I would suggest pushing the shuffle earlier into the `tf.data` pipeline as the inputs do not need to be preprocessed before they are shuffled. In fact the `tfds.load` function has a parameter for shuffling files called `shuffle_files` which you should probably use. (especially if you are working on Colab with slow IO)\r\n\r\nAs a final suggestion, similar to the comments above regarding retracing and input tensors with different sizes (which cause retracing), I think you might be better off by undecorating your functions with `tf.function`. \r\n\r\nhere are some benchmarks:\r\n\r\n1. Moving `.shuffle` to beginning of pipeline. --> 64 seconds for 1 COMPLETE ITERATION OVER DATASET\r\n2. Removing `.shuffle` altogether. --> 61 seconds for 1 COMPLETE ITERATION OVER DATASET\r\n3. Using `tfds.load(..., shuffle_files=True)`. --> 61 seconds for 1 COMPLETE ITERATION OVER DATASET\r\n\r\nThese benchmarks suggest that the slowness of your pipeline is primarily caused by the positioning of your `shuffle` op and the incorrect buffer size you provide it. ", "@kdonbekci \r\nThanks for bringing this to notice! I had completely missed that \ud83d\ude05 ", "No problemo! ", "Thanks a lot @AdityaKane2001 and @kdonbekci,\r\n\r\nI tested out multiple ways of applying `.map` functions, though in all tests the results were better/worse by a margin of not more than 5-8 seconds.\r\n\r\n# **Test1**:\r\nOn printing function name inside each function decorated by `tf.function`, the names for each function were printed just once. This implies tracing of function is happening just once. So no known performance regression is there due to `@tf.function`.\r\nI also tried removing the decorator `tf.function` from all functions but found no noticable difference.\r\n\r\n# **Test2**\r\nOn changing the different location of `.shuffle` with buffersize set to `train_data.cardinality()`, there was noticeable difference of 5-8 seconds based on different locations.\r\n\r\n# **Major Difference Test**\r\n**Here I completely removed the `.shuffle`, and voila, now we are down to ~6.5 seconds from minutes.** (To load the first batch)\r\n\r\nSo it's just the shuffling that's blocking the main process.\r\n\r\nI'm guessing the reason why shuffling, in this case is so slow, is because usually the shuffling of file paths is done, instead of image files stored in memory.\r\n\r\nNow in my use case, since I'm already applying a lot of random augmentations, so I don't think shuffling the data is that much required in the first place. So technically my issue is solved.\r\n\r\nBut still, any further clarifications of questions below would be really appreciated.\r\n\r\n- Why shuffling is slow here exactly?\r\n- And what an ideal approach would be for shuffling the image files stored in memory?\r\n\r\nThanks for your support.\r\n\r\nHere's [the notebook](https://colab.research.google.com/gist/braindotai/3b917caf1d9f72569e5af1162d77ca42/untitled561.ipynb#scrollTo=o3BhLI-_2uI4) for the final test.\r\n\r\n", "Great to hear your input pipeline is matching your performance needs. As a compromise, I'd suggest you use the `shuffle_files` option of `tfds.load` and select a relatively low `shuffle_buffer_size` for an early `tf.data` `.shuffle` operation in your pipeline (It should really be the first thing). \r\n\r\nYour intuition is right that the `shuffle` operation is bottlenecking your pipeline. \r\nThe reasons for this is a combination of:\r\n1. Placement of your `.shuffle` at the end means that all the augmentations + batching need to happen before shuffling. Plus your unnecessarily high buffer size meant A TON of loading&preprocessing (although not quite sure on this anymore for what happens when `shuffle_buffer_size >> .cardinality()`) \r\n2. You are benchmarking the first example rather than a full iteration over the dataset. To understand the effect of this, let's say producing 1 batch with your augmentations takes `x` seconds and you want to shuffle with buffer size `b`. Now, since you've composed your pipeline to shuffle after the augmentations+batching is done, `tf.data` needs to load&augment `b` many batches so the time to come up with the first batch will take `b * x` seconds. Now, for the second iteration, `tf.data` needs to load&preprocess only a single batch to fill its shuffle buffer to the desired size. Thus the second batch will take `x` seconds. \r\n3. Google Colab machines have network attached storage devices which make loading files into RAM much slower than say reading it from your local SSD. With each `.tfrecord` file being ~100MB and the need to load all of them for a full shuffle, this could alone be the culprit. Like you say, for that reason, shuffling of file paths is preferred, which is what `tfds.load(shuffle_files=True)` does. `tf.data`'s `.interleave` (https://www.tensorflow.org/api_docs/python/tf/data/Dataset#interleave)  could help here, although complexifying your application a bit. ", "@kdonbekci \r\nThanks mate, really appreciate such detailed explanations.\r\n\r\nJust want to highlight one thing mentioned by @AdityaKane2001, about the varying shape input to the functions decorated with `tf.function`, which is my case since many images are of different sizes. Based on official documentation, we can pass info in the `tf.function` containing input signature as:\r\n\r\n```python\r\n@tf.function(input_signature = (tf.TensorSpec(shape = [None, None, 3], dtype = tf.uint8)))\r\ndef function():\r\n   ...\r\n```\r\nSince TensorFlow matches tensors based on their shape, using a `None` dimension as a wildcard will allow Functions to reuse traces for variably-sized input.\r\n\r\nAlright guys, thanks again for the support :)"]}, {"number": 47686, "title": "SavedModel file does not exist error when running Image classification with TensorFlow Lite Model Maker tutorial", "body": "**System information**\r\nI am just following instructions found on the tutorial page: https://www.tensorflow.org/lite/tutorials/model_maker_image_classification\r\n\r\nOS:  windows 10 home\r\n\r\ntensorflow lite model maker installed via pip install tflite-model-maker\r\nprint(tf.version.GIT_VERSION, tf.version.VERSION):  v2.4.0-49-g85c8b2a817f 2.4.1\r\n\r\nPython 3.8.6\r\n\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2020 NVIDIA Corporation\r\nBuilt on Thu_Jun_11_22:26:48_Pacific_Daylight_Time_2020\r\nCuda compilation tools, release 11.0, V11.0.194\r\nBuild cuda_11.0_bu.relgpu_drvr445TC445_37.28540450_0\r\n\r\n**Describe the current behavior**\r\nwhen executing \r\n\r\nmodel = image_classifier.create(train_data)\r\n\r\nerror ending with the following occur:\r\n\r\n....\r\n\r\n File \"c:\\Users\\raywl\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\", line 111, in parse_saved_model\r\n    raise IOError(\"SavedModel file does not exist at: %s/{%s|%s}\" %\r\nOSError: SavedModel file does not exist at: C:\\Users\\raywl\\AppData\\Local\\Temp\\tfhub_modules\\87f7b0d2504a48175f521bcaed174acabc93672c/{saved_model.pbtxt|saved_model.pb}\r\n\r\n**Describe the expected behavior**\r\n\r\nINFO:tensorflow:Retraining the models...\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nhub_keras_layer_v1v2 (HubKer (None, 1280)              3413024   \r\n......\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nimport os\r\n\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\nassert tf.__version__.startswith('2')\r\n\r\nfrom tflite_model_maker import configs\r\nfrom tflite_model_maker import ExportFormat\r\nfrom tflite_model_maker import image_classifier\r\nfrom tflite_model_maker import ImageClassifierDataLoader\r\nfrom tflite_model_maker import model_spec\r\n\r\nimport matplotlib.pyplot as plt\r\n\r\nimage_path = tf.keras.utils.get_file(\r\n      'flower_photos.tgz',\r\n      'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\r\n      extract=True)\r\nimage_path = os.path.join(os.path.dirname(image_path), 'flower_photos')\r\n\r\ndata = ImageClassifierDataLoader.from_folder(image_path)\r\ntrain_data, test_data = data.split(0.9)\r\nmodel = image_classifier.create(train_data)\r\n\r\n*error hit*\r\n\r\nattached is the full screen dump of the repeatable step until the error. \r\n\r\nI check the path \r\n\r\nC:\\Users\\raywl\\AppData\\Local\\Temp\\tfhub_modules\\87f7b0d2504a48175f521bcaed174acabc93672c\r\n\r\nand found it to only contain 2 empty folders assets and variables (see screen capture sc_tfhub.png)\r\n\r\nI am also able to complete this colab tutorial on chrome without issue (https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb#scrollTo=6cv3K3oaksJv)\r\n\r\nPlease advise how I can troubleshoot this issue.\r\n\r\n\r\n<img width=\"576\" alt=\"sc_tfhub\" src=\"https://user-images.githubusercontent.com/10432466/110570140-6f72cb80-8190-11eb-9bcd-a2548c5032f2.PNG\">\r\n\r\n\r\n[fullScreenDumpMM.txt](https://github.com/tensorflow/tensorflow/files/6112868/fullScreenDumpMM.txt)\r\n\r\n\r\nThanks\r\nRaymond\r\n", "comments": ["@ziyeqinghan could you take a look at this?", "From the error message, it seems that your hub module doesn't download correctly. Can you try downloading it from https://tfhub.dev/tensorflow/efficientnet/lite0/feature-vector/2 and put the unzipped files in that path?", "Hi\n\nMay I know which path?\n\nThanks\n\nGet Outlook for Android<https://aka.ms/ghei36>\n\n________________________________\nFrom: Tian Lin <notifications@github.com>\nSent: Wednesday, March 10, 2021 12:31:00 PM\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: FlyWong <raywly@hotmail.com>; Author <author@noreply.github.com>\nSubject: Re: [tensorflow/tensorflow] SavedModel file does not exist error when running Image classification with TensorFlow Lite Model Maker tutorial (#47686)\n\n\nFrom the error message, it seems that your hub module doesn't download correctly. Can you try downloading it from https://tfhub.dev/tensorflow/efficientnet/lite0/feature-vector/2 and put the unzipped files in that path?\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/47686#issuecomment-794858553>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/ACPS7UUZGWEELPXAGKVNCDLTC3YYJANCNFSM4Y5AILAA>.\n", "Hi all,\n\nThanks it worked after copying in the files extracted from the .gz\n\nGet Outlook for Android<https://aka.ms/ghei36>\n\n________________________________\nFrom: raymond wong <raywly@hotmail.com>\nSent: Wednesday, March 10, 2021 12:34:15 PM\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>; tensorflow/tensorflow <reply@reply.github.com>\nCc: Author <author@noreply.github.com>\nSubject: Re: [tensorflow/tensorflow] SavedModel file does not exist error when running Image classification with TensorFlow Lite Model Maker tutorial (#47686)\n\nHi\n\nMay I know which path?\n\nThanks\n\nGet Outlook for Android<https://aka.ms/ghei36>\n\n________________________________\nFrom: Tian Lin <notifications@github.com>\nSent: Wednesday, March 10, 2021 12:31:00 PM\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: FlyWong <raywly@hotmail.com>; Author <author@noreply.github.com>\nSubject: Re: [tensorflow/tensorflow] SavedModel file does not exist error when running Image classification with TensorFlow Lite Model Maker tutorial (#47686)\n\n\nFrom the error message, it seems that your hub module doesn't download correctly. Can you try downloading it from https://tfhub.dev/tensorflow/efficientnet/lite0/feature-vector/2 and put the unzipped files in that path?\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/47686#issuecomment-794858553>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/ACPS7UUZGWEELPXAGKVNCDLTC3YYJANCNFSM4Y5AILAA>.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47686\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47686\">No</a>\n"]}, {"number": 47685, "title": "Random seed produces different results for different TF versions", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSx 11.2.1\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: 3.8.7\r\n\r\n**Describe the current behavior**\r\nI'm not sure whether this qualifies as a bug but I'm pretty sure this is not the intended behavior. I'm trying to get the same results using tf 1.15 and tf 2.4.1. Please find the 2 examples below which I'm expecting to produce similar results. The first example using `tf.layers.conv2d()` and the second using `tf.keras.layers.Conv2D()`. I tried using `kernel_initializer=some_tf_initializer(seed=seed)` and I also tried without using a kernel initializer, still the same. \r\n\r\n**Describe the expected behavior**\r\n\r\nSimilar results.\r\n\r\n**Standalone code to reproduce the issue** Here's a colab [notebook](https://colab.research.google.com/drive/1PH5ILC1CI1-qSqvvjmwfOP629-QJ-4Hs?usp=sharing) that contains the same code below.\r\n\r\n**TF1**\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n    \r\n    \r\n    if __name__ == '__main__':\r\n        seed = 1\r\n        np.random.seed(seed)\r\n        tf.set_random_seed(seed)\r\n        session = tf.InteractiveSession()\r\n        initializer = tf.initializers.orthogonal(1.0, seed)\r\n        x = np.random.random((2, 84, 84, 1))\r\n        xp = tf.placeholder(x.dtype, x.shape)\r\n        result = tf.layers.conv2d(xp, 32, 8, 4, kernel_initializer=initializer)\r\n        session.run(tf.global_variables_initializer())\r\n        print(f'Conv2D output:\\n{session.run([result], {xp: x})}\\n{100 * \"=\"}')\r\n        print(f'x:\\n{x}')\r\n\r\n\r\n**Results in**\r\n\r\n    Conv2D output:\r\n    [array([[[[-4.92592295e-01,  5.84946930e-01,  6.45957303e-01, ...,\r\n               3.65328020e-01,  4.03869755e-01, -1.06886940e+00],\r\n             [ 5.37549724e-02,  5.47611947e-01,  3.91425858e-01, ...,\r\n               1.20702194e-01,  9.21035825e-02, -1.10599980e+00],\r\n             [-4.68301348e-01,  8.54547125e-01,  2.99738041e-01, ...,\r\n              -7.47656723e-02, -3.29838560e-01, -6.43979360e-01],\r\n             ...,\r\n             [-3.61929553e-01,  5.70854964e-01,  1.49096153e-01, ...,\r\n               4.05001572e-01, -5.70764458e-01, -8.50054931e-01],\r\n             [-5.23792632e-01,  5.79328891e-01,  4.36402398e-01, ...,\r\n               4.66264733e-01, -8.00687780e-02, -1.05017933e+00],\r\n             [-9.59200260e-01,  9.77023018e-01,  7.22006476e-01, ...,\r\n               3.29199395e-01, -6.48041695e-02, -9.65927090e-01]],\r\n    \r\n            [[-3.05192775e-01,  5.62304495e-01,  2.66985564e-01, ...,\r\n               4.01866526e-01, -5.27853938e-01, -1.36294176e+00],\r\n             [-3.33627733e-01,  8.70909716e-01, -1.90685411e-01, ...,\r\n               5.82781510e-01, -9.54447222e-04, -5.93152081e-01],\r\n             [ 1.05312097e-01,  1.34995604e+00,  4.18403345e-01, ...,\r\n               7.29398371e-01, -1.52729852e-01, -1.44087877e+00],\r\n             ...,\r\n             [-2.65877698e-01,  1.30702457e+00,  2.12075863e-01, ...,\r\n               4.02235128e-01, -4.83213829e-02, -9.84811507e-01],\r\n             [-2.33065665e-01,  8.06240360e-01,  2.04250988e-01, ...,\r\n               5.11907848e-01, -2.97176027e-01, -5.89994050e-01],\r\n             [-6.06616196e-01,  7.52128441e-01,  6.05931022e-01, ...,\r\n               8.25940123e-01, -8.11965401e-01, -1.38039418e+00]],\r\n    \r\n            [[-1.14268620e-02,  9.44562211e-01,  7.30843200e-01, ...,\r\n               7.71483717e-01, -3.62299259e-01, -5.19022458e-01],\r\n             [-1.25248650e-01,  8.31288483e-01,  3.01203507e-01, ...,\r\n               6.10087249e-02, -1.37678504e-01, -1.12305468e+00],\r\n             [-2.37495350e-01,  5.80913482e-01,  2.48314393e-01, ...,\r\n               6.06333851e-01, -3.30380816e-01, -1.00515721e+00],\r\n             ...,\r\n             [ 1.95822525e-01,  9.85462620e-01,  9.30753642e-02, ...,\r\n               5.07316387e-01, -2.72563623e-01, -3.44717273e-01],\r\n             [-7.58518148e-01,  7.18211754e-01,  3.56392246e-01, ...,\r\n               5.41345861e-01, -2.84327606e-01, -5.78967512e-01],\r\n             [-9.36528091e-01,  4.51066108e-01,  3.60469945e-01, ...,\r\n               6.06252149e-01, -9.82606865e-02, -1.12673980e+00]],\r\n    \r\n            ...,\r\n    \r\n            [[-3.23906425e-01,  4.89133765e-01,  5.51718357e-01, ...,\r\n              -1.33718077e-01,  1.23015200e-03, -5.54514943e-01],\r\n             [-5.04556877e-01,  7.36618066e-01,  2.02579467e-01, ...,\r\n              -3.74142562e-01,  6.00494771e-03, -1.31111289e+00],\r\n             [-4.16236220e-01,  5.98196128e-01,  6.97848461e-01, ...,\r\n               6.23122747e-01, -1.07252840e-01, -4.83301913e-01],\r\n             ...,\r\n             [ 1.06692401e-01,  1.73388947e+00,  3.22292122e-01, ...,\r\n              -1.75720933e-01, -2.96764992e-01, -1.06044579e+00],\r\n             [-5.62972482e-01,  1.36246077e+00,  9.13179694e-01, ...,\r\n               6.32824517e-01, -4.73498606e-01, -9.47552266e-01],\r\n             [-5.11697389e-01,  6.66774542e-01,  3.82947609e-01, ...,\r\n               5.67110785e-01, -7.11579248e-03, -7.77461939e-01]],\r\n    \r\n            [[-1.11410557e-01,  1.10325702e+00,  5.50558807e-01, ...,\r\n               5.51697720e-01, -3.75442814e-01, -4.68653786e-01],\r\n             [-3.26447172e-01,  1.19972335e+00,  3.82313314e-01, ...,\r\n               2.20346417e-01, -3.39678412e-01, -8.66913575e-01],\r\n             [-3.98474415e-01,  7.14095329e-01,  8.04559140e-02, ...,\r\n               1.19488448e-01, -2.52660129e-01, -9.91879947e-01],\r\n             ...,\r\n             [-5.70385227e-01,  8.52797253e-01,  4.66251675e-01, ...,\r\n              -3.24400644e-01,  3.20423484e-03, -1.39903236e+00],\r\n             [-2.07675682e-01,  6.29120258e-01,  6.79053796e-01, ...,\r\n               1.07336154e-01,  1.87886060e-01, -3.86794041e-01],\r\n             [-2.67700849e-02,  4.49128504e-01,  3.40760390e-01, ...,\r\n               6.36000637e-01, -3.77890278e-01, -3.30207391e-01]],\r\n    \r\n            [[-3.10700139e-01,  8.18823907e-01,  3.67007768e-03, ...,\r\n               4.90558846e-01, -8.36860336e-01, -1.03869365e+00],\r\n             [-4.88181365e-01,  8.00080028e-01,  3.56097390e-01, ...,\r\n               4.76738039e-01, -3.42056012e-01, -5.30593649e-01],\r\n             [-4.60314405e-01,  7.07439805e-01,  4.22496599e-01, ...,\r\n               5.05574451e-01, -1.24705048e-01, -2.87646769e-01],\r\n             ...,\r\n             [-2.57950491e-01,  1.20126622e+00,  5.56477074e-02, ...,\r\n               6.11127028e-01, -1.93377726e-01, -1.07765356e+00],\r\n             [-6.50455755e-01,  1.29336304e+00,  8.44637456e-01, ...,\r\n               1.81637465e-01, -2.88107846e-01, -4.49444781e-01],\r\n             [-4.21331048e-01,  3.20901642e-01,  8.83963968e-01, ...,\r\n               8.65426315e-01, -4.92428131e-01, -8.49800993e-01]]],\r\n    \r\n    \r\n           [[[-5.16387221e-01,  6.99757393e-01,  7.85561831e-01, ...,\r\n               1.03644383e-01, -3.70012470e-01, -5.32289379e-01],\r\n             [-7.05045607e-02,  6.10474017e-01,  3.49680420e-01, ...,\r\n               7.92201453e-01, -5.39308419e-01, -3.42154387e-01],\r\n             [-4.22583634e-01,  8.01291482e-01,  1.48925846e-01, ...,\r\n               7.68131504e-01, -7.38695500e-01, -1.09516989e+00],\r\n             ...,\r\n             [-7.26389962e-01,  6.26188865e-01,  5.42793169e-01, ...,\r\n               3.52803865e-01, -3.46388144e-01, -7.60162696e-01],\r\n             [-9.04680334e-02,  1.75359623e-01,  8.00145598e-01, ...,\r\n               2.51379785e-01, -2.79905131e-01, -8.72223633e-01],\r\n             [-4.20990087e-01,  3.09422635e-01,  4.11481559e-01, ...,\r\n               3.69259084e-02, -1.60866470e-01, -5.05859647e-01]],\r\n    \r\n            [[-4.38932989e-01,  9.72496331e-01,  2.50341307e-01, ...,\r\n               2.83248632e-01, -3.26580435e-01, -1.03773839e+00],\r\n             [-8.19208455e-01,  8.96083502e-01,  4.24397325e-01, ...,\r\n              -5.64658536e-02, -6.09974865e-01, -1.23144756e+00],\r\n             [-9.45132686e-01,  9.13248242e-01,  8.86299832e-01, ...,\r\n               2.15502049e-01, -5.58705913e-01, -2.25369791e-01],\r\n             ...,\r\n             [-8.56722975e-01,  7.94114365e-01,  5.81065297e-01, ...,\r\n               4.96281428e-01, -1.03129758e+00, -1.05092158e+00],\r\n             [-2.82233182e-02,  9.48533077e-01,  6.68768609e-01, ...,\r\n               5.78499983e-01, -6.95651561e-01, -9.76414384e-01],\r\n             [-8.06302266e-01,  1.05147010e+00,  4.22578075e-01, ...,\r\n               2.94475123e-01,  1.74168406e-01, -1.14952206e+00]],\r\n    \r\n            [[-3.06847178e-01,  9.29719098e-01,  2.68849689e-01, ...,\r\n               3.96790007e-01, -3.59629268e-01, -9.25076133e-01],\r\n             [-2.38372207e-01,  5.78896860e-01,  2.17575482e-01, ...,\r\n              -6.97642069e-02, -2.63976905e-01, -9.10845525e-01],\r\n             [-5.40468423e-01,  1.20147694e+00,  4.04620974e-01, ...,\r\n               1.76649501e-01,  1.46967870e-01, -6.74134783e-01],\r\n             ...,\r\n             [-6.27551970e-01,  5.05884731e-01,  2.10479188e-01, ...,\r\n               3.09780840e-01,  1.46682047e-01, -1.07307698e+00],\r\n             [-6.44653887e-01,  8.90779216e-01,  3.82747674e-01, ...,\r\n               1.72155001e-01, -4.48190676e-01, -8.62522695e-01],\r\n             [-1.50221285e-01,  9.35658435e-01,  3.59703911e-01, ...,\r\n              -8.34232530e-02, -1.89649715e-01, -7.73705172e-01]],\r\n    \r\n            ...,\r\n    \r\n            [[-3.47146995e-01,  6.34597952e-02,  4.73544353e-01, ...,\r\n              -8.20012972e-02, -1.78752555e-01, -1.36296041e+00],\r\n             [-8.95298221e-01,  8.66479595e-01,  5.61457267e-01, ...,\r\n               2.22826074e-01, -4.86448503e-01, -8.56547191e-01],\r\n             [-8.62118822e-01,  4.83077034e-01,  3.60908309e-02, ...,\r\n               3.29259093e-01, -4.08073639e-02, -6.45683881e-01],\r\n             ...,\r\n             [-5.01558985e-01,  4.62753228e-01,  4.01966678e-01, ...,\r\n               6.35652593e-01, -7.45519465e-02, -5.39741380e-01],\r\n             [ 1.29982837e-01,  7.31941977e-01, -2.05750614e-01, ...,\r\n               3.16325608e-01, -3.28495177e-01, -1.09927128e+00],\r\n             [-4.90335504e-01,  4.94757973e-01,  9.40801327e-02, ...,\r\n               5.66634378e-02, -7.30613447e-01, -7.25730750e-01]],\r\n    \r\n            [[-2.75066335e-01,  7.35209409e-01,  5.99839688e-01, ...,\r\n               6.71254510e-02,  7.97677772e-02, -7.72461196e-01],\r\n             [-1.58777502e-01,  7.51857910e-01,  3.80584693e-01, ...,\r\n              -1.01868390e-01, -7.12568409e-02, -6.42932271e-01],\r\n             [-1.45951405e-01,  1.03692197e+00,  4.91333873e-01, ...,\r\n               2.98796942e-01, -5.46416283e-01, -1.04881221e+00],\r\n             ...,\r\n             [-9.75652891e-03,  8.28500896e-01,  3.48450207e-01, ...,\r\n               4.07241092e-01, -2.34265134e-01, -5.27081486e-01],\r\n             [-5.65917326e-01,  7.37496827e-01,  1.65005917e-01, ...,\r\n               6.61606291e-01, -2.20420580e-01, -1.11307865e+00],\r\n             [-2.21167732e-01,  4.83734785e-01,  4.59793140e-01, ...,\r\n               4.19304500e-01, -2.96987262e-01, -2.01353157e-01]],\r\n    \r\n            [[-6.33535626e-01,  1.11787318e+00,  6.41380845e-01, ...,\r\n               1.12652086e-01, -1.99377096e-02, -6.69645437e-01],\r\n             [-1.05662073e-01,  3.87718948e-01,  4.30142659e-01, ...,\r\n               3.27637078e-01, -3.59568870e-01, -9.63431155e-01],\r\n             [-1.26793132e-01,  1.29664648e+00,  3.28922837e-01, ...,\r\n               3.21313848e-01, -7.53446525e-01, -7.93674733e-01],\r\n             ...,\r\n             [-5.32937597e-01,  1.09915270e+00,  6.23443352e-01, ...,\r\n               9.96585500e-01, -6.21343220e-01, -1.01232184e+00],\r\n             [ 5.05322966e-02,  1.18874480e+00,  4.57358272e-01, ...,\r\n               4.80935716e-01, -2.04122013e-01, -1.13864994e+00],\r\n             [-1.03084733e-01,  1.14916096e+00,  2.73508528e-01, ...,\r\n               6.76093153e-01, -3.34324702e-01, -1.28436283e+00]]]])]\r\n    ====================================================================================================\r\n    x:\r\n    [[[[4.17022005e-01]\r\n       [7.20324493e-01]\r\n       [1.14374817e-04]\r\n       ...\r\n       [6.23672207e-01]\r\n       [7.50942434e-01]\r\n       [3.48898342e-01]]\r\n    \r\n      [[2.69927892e-01]\r\n       [8.95886218e-01]\r\n       [4.28091190e-01]\r\n       ...\r\n       [1.85762022e-02]\r\n       [7.00221437e-02]\r\n       [4.86345111e-01]]\r\n    \r\n      [[6.06329462e-01]\r\n       [5.68851437e-01]\r\n       [3.17362409e-01]\r\n       ...\r\n       [9.18601778e-01]\r\n       [4.02024891e-04]\r\n       [9.76759149e-01]]\r\n    \r\n      ...\r\n    \r\n      [[5.89549934e-01]\r\n       [3.89137609e-01]\r\n       [5.05975232e-01]\r\n       ...\r\n       [4.35888475e-01]\r\n       [7.89075202e-01]\r\n       [4.66467704e-01]]\r\n    \r\n      [[6.73554921e-01]\r\n       [8.84836452e-01]\r\n       [9.38138449e-01]\r\n       ...\r\n       [7.93970466e-01]\r\n       [2.13784215e-01]\r\n       [6.41105035e-01]]\r\n    \r\n      [[7.31134736e-01]\r\n       [9.50619892e-02]\r\n       [7.00729238e-02]\r\n       ...\r\n       [9.95522026e-01]\r\n       [4.81429517e-01]\r\n       [8.37812754e-01]]]\r\n    \r\n    \r\n     [[[6.03655452e-01]\r\n       [6.64374944e-01]\r\n       [2.72461392e-01]\r\n       ...\r\n       [1.14069927e-01]\r\n       [2.93705095e-01]\r\n       [8.78904978e-03]]\r\n    \r\n      [[2.53263696e-01]\r\n       [8.37712781e-01]\r\n       [8.07756027e-01]\r\n       ...\r\n       [7.37152445e-01]\r\n       [6.00521471e-01]\r\n       [7.37999367e-01]]\r\n    \r\n      [[3.75760828e-01]\r\n       [9.11106703e-01]\r\n       [8.72308594e-01]\r\n       ...\r\n       [9.80268932e-01]\r\n       [1.13198035e-01]\r\n       [4.65678949e-01]]\r\n    \r\n      ...\r\n    \r\n      [[4.94241516e-02]\r\n       [6.34450548e-01]\r\n       [8.93053413e-01]\r\n       ...\r\n       [7.97760317e-01]\r\n       [3.18871974e-01]\r\n       [6.47314782e-01]]\r\n    \r\n      [[4.65136696e-01]\r\n       [5.07669096e-01]\r\n       [4.23295851e-01]\r\n       ...\r\n       [2.98177206e-01]\r\n       [5.32380132e-01]\r\n       [6.12348886e-01]]\r\n    \r\n      [[2.58528146e-01]\r\n       [5.20561003e-02]\r\n       [7.82628170e-01]\r\n       ...\r\n       [8.26242775e-03]\r\n       [7.43071396e-01]\r\n       [3.29652868e-01]]]]\r\n\r\n**TF2**\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n    from tensorflow.keras.layers import Conv2D\r\n    \r\n    \r\n    if __name__ == '__main__':\r\n        seed = 1\r\n        np.random.seed(seed)\r\n        tf.random.set_seed(seed)\r\n        x = np.random.random((2, 84, 84, 1))\r\n        initializer = tf.initializers.Orthogonal(1.0, seed)\r\n        print(f'Conv2D output:\\n{Conv2D(32, 8, 4, kernel_initializer=initializer)(x)}\\n{100 * \"=\"}')\r\n        print(f'x:\\n{x}')\r\n\r\n**Results in**\r\n\r\n        Conv2D output:\r\n    [[[[-5.41361034e-01  3.66543412e-01 -1.51497812e-03 ... -5.08555949e-01\r\n        -2.81920075e-01  2.03241315e-02]\r\n       [-5.59882522e-01  4.86802310e-01 -6.99946359e-02 ... -2.40252942e-01\r\n        -3.65630835e-01 -3.15424293e-01]\r\n       [-9.42213356e-01  2.15654269e-01 -1.55082747e-01 ... -4.04906332e-01\r\n        -2.01114044e-01  7.04537705e-02]\r\n       ...\r\n       [-3.46490562e-01  3.19557160e-01 -2.92775959e-01 ... -4.28674221e-01\r\n         2.79810458e-01  4.20070797e-01]\r\n       [-3.45766425e-01  1.90711677e-01 -1.57374702e-02 ... -5.71854293e-01\r\n         4.97741327e-02 -1.16687842e-01]\r\n       [-9.33221936e-01 -1.68129373e-02  1.63893417e-01 ... -4.79621142e-01\r\n        -2.14530781e-01  5.61090052e-01]]\r\n    \r\n      [[-7.58794546e-01  5.37562370e-01 -4.74378794e-01 ... -5.63471198e-01\r\n        -2.08135564e-02  3.88803691e-01]\r\n       [-4.21465009e-01  1.65287077e-01 -4.43225324e-01 ... -1.72018170e-01\r\n        -7.92833045e-02  3.92330065e-02]\r\n       [-4.81306076e-01  3.66574019e-01 -3.64440233e-01 ... -5.52142262e-01\r\n        -2.36726597e-01 -3.12441528e-01]\r\n       ...\r\n       [-8.76132190e-01  3.22705477e-01 -1.90438583e-01 ... -6.52492762e-01\r\n        -6.05543517e-02  3.59102860e-02]\r\n       [-7.17206061e-01  1.32312387e-01 -4.24121648e-01 ...  1.82857260e-01\r\n         1.84026435e-01 -1.85313985e-01]\r\n       [-1.20653558e+00  4.54987772e-02 -2.89574206e-01 ... -3.28905612e-01\r\n         3.50747108e-02  1.20641785e-02]]\r\n    \r\n      [[-5.34970939e-01  7.13559866e-01 -4.85820472e-01 ... -4.59470123e-01\r\n         3.37272674e-01  1.63189009e-01]\r\n       [-7.05033600e-01  3.56267929e-01 -4.30038758e-02 ... -5.83572984e-01\r\n        -1.40957370e-01 -2.53099173e-01]\r\n       [-5.99712431e-01  7.10705340e-01 -3.39112192e-01 ... -4.14105803e-01\r\n         1.87255502e-01 -3.33267421e-01]\r\n       ...\r\n       [-5.80564499e-01  4.36445504e-01  3.36502224e-01 ...  1.45576835e-01\r\n         3.74677926e-01 -3.91695678e-01]\r\n       [-7.22398818e-01  3.15642595e-01 -2.88131565e-01 ... -3.45985293e-01\r\n         2.34268203e-01  3.29410911e-01]\r\n       [-7.46637762e-01  2.26648629e-01 -1.73913926e-01 ... -1.78611025e-01\r\n         7.35538006e-02  2.55084813e-01]]\r\n    \r\n      ...\r\n    \r\n      [[-5.49425662e-01 -3.49880159e-02  4.77346703e-02 ... -4.04859513e-01\r\n         3.71269658e-02 -1.85562521e-02]\r\n       [-8.26125443e-01  1.29296139e-01 -2.88902789e-01 ... -8.50088000e-01\r\n        -5.04021049e-01 -4.97736454e-01]\r\n       [-7.37619460e-01  5.00219822e-01  7.76109993e-02 ... -1.41733378e-01\r\n         2.81554639e-01  3.69791657e-01]\r\n       ...\r\n       [-3.06938559e-01  4.47962463e-01 -2.05980629e-01 ... -8.48388433e-01\r\n        -1.59557834e-01  3.17021906e-02]\r\n       [-3.87109697e-01  9.46836114e-01 -2.71658182e-01 ... -6.23931110e-01\r\n        -2.16598317e-01 -1.88935712e-01]\r\n       [-1.31429803e+00  3.86178017e-01 -6.20340466e-01 ... -3.68769616e-02\r\n         1.81194678e-01 -1.06644318e-01]]\r\n    \r\n      [[-7.55118549e-01 -1.59330755e-01 -1.55888349e-01 ... -8.46641809e-02\r\n         5.47275543e-02 -3.14989388e-01]\r\n       [-7.29794323e-01  1.63924411e-01  5.99410906e-02 ...  1.31567806e-01\r\n        -4.67930377e-01  2.95255750e-01]\r\n       [-6.91133916e-01  5.20268269e-02 -7.29867145e-02 ... -1.88851178e-01\r\n         4.07196075e-01  4.01087821e-01]\r\n       ...\r\n       [-5.79473913e-01  7.94014513e-01 -7.01249093e-02 ... -6.76721931e-01\r\n        -1.01779945e-01 -5.97994268e-01]\r\n       [-6.84648752e-01  1.01270474e-01 -4.07643348e-01 ...  3.85033749e-02\r\n         8.26148912e-02  5.69675528e-02]\r\n       [-4.62479711e-01  4.19429392e-01  1.77284703e-04 ... -1.42741054e-01\r\n         3.54655176e-01  2.56564021e-01]]\r\n    \r\n      [[-2.46302426e-01  7.49420345e-01  8.12041853e-03 ... -3.32499892e-01\r\n         1.33527100e-01  6.35570288e-02]\r\n       [-9.58327591e-01  1.98065817e-01  1.93995520e-01 ... -2.94785231e-01\r\n         2.52389193e-01 -1.20501839e-01]\r\n       [-8.17039490e-01  2.07326993e-01 -5.11587203e-01 ... -4.24753636e-01\r\n         2.88008928e-01  5.03057897e-01]\r\n       ...\r\n       [-7.03616858e-01  3.78316432e-01 -5.60314238e-01 ... -4.05634254e-01\r\n        -3.88694972e-01 -1.93394765e-01]\r\n       [-6.37941658e-01  4.12149638e-01  4.30523872e-01 ... -1.07135192e-01\r\n        -4.06805426e-01  2.13325843e-01]\r\n       [-1.10811067e+00  1.87466890e-01 -4.25169766e-01 ... -3.90125453e-01\r\n        -2.32264772e-01 -1.13592006e-01]]]\r\n    \r\n    \r\n     [[[-1.12212503e+00  4.90197897e-01 -1.32902622e-01 ... -2.71363735e-01\r\n         5.97870303e-03 -1.82653069e-01]\r\n       [-5.94440103e-01 -2.69649085e-02 -8.82392377e-02 ... -2.57950276e-01\r\n        -5.64375184e-02  3.79566044e-01]\r\n       [-9.26148295e-01  7.57718146e-01 -1.35923713e-01 ... -3.23437661e-01\r\n         4.14071977e-01  1.71241298e-01]\r\n       ...\r\n       [-7.43670583e-01  5.69033444e-01  8.93335044e-02 ... -4.33211058e-01\r\n         9.38811079e-02 -2.02697456e-01]\r\n       [-7.01980829e-01  8.19573849e-02  1.15745321e-01 ... -3.05091172e-01\r\n         2.67599583e-01  4.00184661e-01]\r\n       [-1.23418115e-01 -7.21083656e-02 -2.83473641e-01 ... -5.09245217e-01\r\n         1.67642578e-01  3.97953391e-02]]\r\n    \r\n      [[-7.82243788e-01  4.62321520e-01 -1.03323981e-01 ... -6.02696836e-01\r\n         1.12790830e-01  6.60246331e-03]\r\n       [-1.18653631e+00  1.10369611e+00  4.03279841e-01 ... -3.35117340e-01\r\n         1.34733140e-01  1.24090314e-01]\r\n       [-7.56276011e-01  4.68323886e-01  2.55332291e-01 ... -5.64108253e-01\r\n        -1.23212464e-01  9.44344103e-02]\r\n       ...\r\n       [-1.07282424e+00  4.46680576e-01  2.62999147e-01 ... -4.93109912e-01\r\n         1.20379120e-01  2.00786784e-01]\r\n       [-5.91401279e-01 -4.76840436e-02 -6.31224453e-01 ... -1.33297965e-02\r\n        -6.85213029e-01  3.06965083e-01]\r\n       [-1.26961339e+00  4.00966257e-01  1.58335969e-01 ... -4.71471280e-01\r\n        -5.21351993e-01 -2.68614203e-01]]\r\n    \r\n      [[-7.59719312e-01 -2.15684757e-01 -5.51491380e-01 ... -4.05447990e-01\r\n         2.04430401e-01 -4.03630167e-01]\r\n       [-7.50933051e-01 -2.70830095e-01 -4.10136461e-01 ... -2.66372442e-01\r\n        -4.25907433e-01  2.79703408e-01]\r\n       [-1.15558457e+00  1.44730762e-01 -1.11366399e-01 ... -3.16568702e-01\r\n        -1.65987372e-01 -2.50013694e-02]\r\n       ...\r\n       [-1.26355612e+00  1.83538213e-01  2.64952302e-01 ... -4.03630674e-01\r\n        -1.93098515e-01  3.03720146e-01]\r\n       [-1.04506040e+00  9.86333251e-01 -4.76556689e-01 ... -5.65628707e-01\r\n         2.07612868e-02  7.63047487e-02]\r\n       [-1.07622218e+00  2.96826065e-01 -5.04598498e-01 ... -4.09753144e-01\r\n        -2.73265153e-01  6.33615702e-02]]\r\n    \r\n      ...\r\n    \r\n      [[-1.13511002e+00  2.11719826e-01 -8.60134482e-01 ... -3.91803771e-01\r\n        -6.59739316e-01 -2.00912848e-01]\r\n       [-1.29758418e+00  2.52690554e-01 -3.54235232e-01 ... -4.23384011e-01\r\n        -5.52129559e-02 -1.58944473e-01]\r\n       [-1.17037618e+00  3.93378854e-01 -2.55860239e-01 ... -1.89175576e-01\r\n        -3.52438241e-01  4.47871268e-01]\r\n       ...\r\n       [-6.99247956e-01 -4.01002228e-01 -9.55614746e-02 ... -5.12549996e-01\r\n        -2.93750733e-01  6.53216466e-02]\r\n       [-7.55659997e-01  6.40419498e-02 -3.80645603e-01 ... -3.38304751e-02\r\n        -2.81535774e-01  3.27474415e-01]\r\n       [-8.98592114e-01  6.04611039e-01 -2.70986766e-01 ... -3.77177030e-01\r\n         6.39083028e-01  1.36269689e-01]]\r\n    \r\n      [[-2.06768334e-01  4.33980107e-01 -2.79342651e-01 ... -5.97317517e-01\r\n        -3.16711366e-01 -3.49702388e-02]\r\n       [-8.91763270e-01  5.25228903e-02 -1.25209495e-01 ... -2.35961720e-01\r\n        -8.85192119e-03  2.04528570e-01]\r\n       [-6.51450276e-01 -3.80111784e-01 -7.60694802e-01 ... -1.16580009e+00\r\n         4.66391236e-01 -1.49288952e-01]\r\n       ...\r\n       [-8.66602838e-01 -5.96193299e-02 -1.27994597e-01 ... -4.30833064e-02\r\n         1.16992146e-01 -6.62802998e-03]\r\n       [-9.14786756e-01  1.67500958e-01 -2.82980531e-01 ... -6.82374537e-01\r\n        -1.63795985e-02  1.82587206e-01]\r\n       [-5.49697280e-01  6.72760680e-02 -7.42559731e-02 ... -1.59981385e-01\r\n         1.14814062e-02  2.45045304e-01]]\r\n    \r\n      [[-6.60416663e-01  4.52319860e-01  1.04573436e-01 ... -8.84432197e-01\r\n        -1.99507281e-01 -5.85323572e-02]\r\n       [-5.59800625e-01  4.29061443e-01  6.59240410e-02 ... -2.68544644e-01\r\n        -9.68336090e-02  1.81482792e-01]\r\n       [-5.66595554e-01  7.11271226e-01 -4.41362828e-01 ... -4.43202615e-01\r\n         8.01286697e-01 -1.05126597e-01]\r\n       ...\r\n       [-7.27143824e-01  4.69138294e-01 -3.05642545e-01 ... -3.89702499e-01\r\n        -9.79036614e-02  3.28408331e-01]\r\n       [-3.61066341e-01  4.04266417e-01 -2.91205943e-01 ...  5.48217893e-02\r\n        -9.46037620e-02 -1.06996156e-01]\r\n       [-6.50965929e-01  5.33826649e-01 -3.92165482e-01 ... -2.94952631e-01\r\n        -8.13799322e-01  1.03757977e-01]]]]\r\n    ====================================================================================================\r\n    x:\r\n    [[[[4.17022005e-01]\r\n       [7.20324493e-01]\r\n       [1.14374817e-04]\r\n       ...\r\n       [6.23672207e-01]\r\n       [7.50942434e-01]\r\n       [3.48898342e-01]]\r\n    \r\n      [[2.69927892e-01]\r\n       [8.95886218e-01]\r\n       [4.28091190e-01]\r\n       ...\r\n       [1.85762022e-02]\r\n       [7.00221437e-02]\r\n       [4.86345111e-01]]\r\n    \r\n      [[6.06329462e-01]\r\n       [5.68851437e-01]\r\n       [3.17362409e-01]\r\n       ...\r\n       [9.18601778e-01]\r\n       [4.02024891e-04]\r\n       [9.76759149e-01]]\r\n    \r\n      ...\r\n    \r\n      [[5.89549934e-01]\r\n       [3.89137609e-01]\r\n       [5.05975232e-01]\r\n       ...\r\n       [4.35888475e-01]\r\n       [7.89075202e-01]\r\n       [4.66467704e-01]]\r\n    \r\n      [[6.73554921e-01]\r\n       [8.84836452e-01]\r\n       [9.38138449e-01]\r\n       ...\r\n       [7.93970466e-01]\r\n       [2.13784215e-01]\r\n       [6.41105035e-01]]\r\n    \r\n      [[7.31134736e-01]\r\n       [9.50619892e-02]\r\n       [7.00729238e-02]\r\n       ...\r\n       [9.95522026e-01]\r\n       [4.81429517e-01]\r\n       [8.37812754e-01]]]\r\n    \r\n    \r\n     [[[6.03655452e-01]\r\n       [6.64374944e-01]\r\n       [2.72461392e-01]\r\n       ...\r\n       [1.14069927e-01]\r\n       [2.93705095e-01]\r\n       [8.78904978e-03]]\r\n    \r\n      [[2.53263696e-01]\r\n       [8.37712781e-01]\r\n       [8.07756027e-01]\r\n       ...\r\n       [7.37152445e-01]\r\n       [6.00521471e-01]\r\n       [7.37999367e-01]]\r\n    \r\n      [[3.75760828e-01]\r\n       [9.11106703e-01]\r\n       [8.72308594e-01]\r\n       ...\r\n       [9.80268932e-01]\r\n       [1.13198035e-01]\r\n       [4.65678949e-01]]\r\n    \r\n      ...\r\n    \r\n      [[4.94241516e-02]\r\n       [6.34450548e-01]\r\n       [8.93053413e-01]\r\n       ...\r\n       [7.97760317e-01]\r\n       [3.18871974e-01]\r\n       [6.47314782e-01]]\r\n    \r\n      [[4.65136696e-01]\r\n       [5.07669096e-01]\r\n       [4.23295851e-01]\r\n       ...\r\n       [2.98177206e-01]\r\n       [5.32380132e-01]\r\n       [6.12348886e-01]]\r\n    \r\n      [[2.58528146e-01]\r\n       [5.20561003e-02]\r\n       [7.82628170e-01]\r\n       ...\r\n       [8.26242775e-03]\r\n       [7.43071396e-01]\r\n       [3.29652868e-01]]]]\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/f9cd20a051de19cf6016ebf5221a8563/47685.ipynb). Thanks!", "Was able to replicate the issue in TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/ca5261ad3798c5a13e71874ae90f0275/untitled158.ipynb)..Thanks !", "You can verify if you have followed [these](https://www.tensorflow.org/guide/migrate/validate_correctness) validate correctness steps and note that there will be still some numerical mismatch due to floating point precision errors.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Well, closing the issue does not change the fact that tensorflow does not support reproducible results in migrated code. It's stale because there are no new findings on my behalf and the suggestions in the comments are not helpful. ", "The random numbers are not guaranteed to be consistent across TensorFlow versions. Please see [here](https://www.tensorflow.org/guide/versions#what_is_not_covered) and [here](https://www.tensorflow.org/guide/random_numbers).", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47685\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47685\">No</a>\n"]}, {"number": 47682, "title": "Building tensorflow 2.4 from source failure: Lowering to LLVM IR failed", "body": "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 2.4\r\n- Python version: Python 3.7\r\n- Bazel version (if compiling from source): 2.3\r\n- GCC/Compiler version (if compiling from source): Clang 11.0\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: GTX 1080 Ti 11GB\r\n\r\n**Describe the current behavior**\r\n\r\nWhen building the tensorflow from source using clang, following issue happens:\r\n\r\n```\r\nERROR: /home/hzhao/.cache/bazel/_bazel_hzhao/dd20e335f94826116d98d72ed6038fbd/external/org_tensorflow/tensorflow/core/kernels/mlir_generated/BUILD:149:19: compile external/org_tensorflow/tensorflow/core/kernels/mlir_generated/abs_f16_kernel_cubin.7.5.bin failed (Exit 1): linux-sandbox failed: error executing command\r\n  (cd /home/hzhao/.cache/bazel/_bazel_hzhao/dd20e335f94826116d98d72ed6038fbd/sandbox/linux-sandbox/5469/execroot/ && \\\r\n  exec env - \\\r\n    TMPDIR=/tmp \\\r\n  /home/hzhao/.cache/bazel/_bazel_hzhao/install/bc435791ccf8b3a8a0fe11b973c1ebb7/linux-sandbox -t 15 -w /home/hzhao/.cache/bazel/_bazel_hzhao/dd20e335f94826116d98d72ed6038fbd/sandbox/linux-sandbox/5469/execroot/ -w /tmp -w /dev/shm -D -- bazel-out/host/bin/external/org_tensorflow/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_gpu_binary '--same_shape=0,1' '--unroll_factors=4' '--tile_sizes=256' '--arch=7.5' '--input=bazel-out/k8-opt/bin/external/org_tensorflow/tensorflow/core/kernels/mlir_generated/abs_f16.mlir' '--output=bazel-out/k8-opt/bin/external/org_tensorflow/tensorflow/core/kernels/mlir_generated/abs_f16_kernel_cubin.7.5.bin') linux-sandbox failed: error executing command\r\n  (cd /home/hzhao/.cache/bazel/_bazel_hzhao/dd20e335f94826116d98d72ed6038fbd/sandbox/linux-sandbox/5469/execroot/ && \\\r\n  exec env - \\\r\n    TMPDIR=/tmp \\\r\n  /home/hzhao/.cache/bazel/_bazel_hzhao/install/bc435791ccf8b3a8a0fe11b973c1ebb7/linux-sandbox -t 15 -w /home/hzhao/.cache/bazel/_bazel_hzhao/dd20e335f94826116d98d72ed6038fbd/sandbox/linux-sandbox/5469/execroot/ -w /tmp -w /dev/shm -D -- bazel-out/host/bin/external/org_tensorflow/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_gpu_binary '--same_shape=0,1' '--unroll_factors=4' '--tile_sizes=256' '--arch=7.5' '--input=bazel-out/k8-opt/bin/external/org_tensorflow/tensorflow/core/kernels/mlir_generated/abs_f16.mlir' '--output=bazel-out/k8-opt/bin/external/org_tensorflow/tensorflow/core/kernels/mlir_generated/abs_f16_kernel_cubin.7.5.bin')\r\n1615275075.984938474: src/main/tools/linux-sandbox-pid1.cc:208: writable: /home/hzhao/.cache/bazel/_bazel_hzhao/dd20e335f94826116d98d72ed6038fbd/sandbox/linux-sandbox/5469/execrootexternal/org_tensorflow/tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:194] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n2021-03-08 23:31:16.021181: E external/org_tensorflow/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_gpu_binary.cc:97] Internal: Lowering to LLVM IR failed.\r\n1615275076.027019459: src/main/tools/linux-sandbox-pid1.cc:410: wait returned pid=2, status=0x100\r\n1615275076.027040792: src/main/tools/linux-sandbox-pid1.cc:428: child exited normally with code 1\r\n1615275076.093379128: src/main/tools/linux-sandbox.cc:233: child exited normally with code 1\r\nTarget @org_tensorflow//tensorflow/core/kernels/mlir_generated:abs_f16_kernel failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1.442s, Critical Path: 0.12s\r\nINFO: 3 processes: 3 internal.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nTensorflow can be built without issue.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nRun `tf_to_gpu_binary` with following `abs_f16.mlir`\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n\r\nContent of abs_f16.mlir:\r\n```\r\nfunc @Abs(%arg0: tensor<?xf16>)\r\n    -> tensor<?xf16> attributes {tf_entry, llvm.emit_c_interface} {\r\n  %0 = \"tf.Abs\"(%arg0) : (tensor<?xf16>) -> tensor<?xf16>\r\n  return %0 : tensor<?xf16>\r\n}\r\n```\r\n\r\nMLIR reproducer:\r\n```\r\n// configuration: -pass-pipeline='gpu.module(llvm.func(propagate-tf-abi-knowledge{same-shape=0,1}), strip-debuginfo, gpu-kernel-to-blob{arch=7.5 blob-annotation=gpu.binary generate-fatbin=false})'\r\n// note: verifyPasses=true\r\n\r\n\r\nmodule attributes {gpu.container_module} {\r\n  func @Abs(%arg0: memref<?xf16>) -> memref<?xf16> attributes {llvm.emit_c_interface, tf_entry} {\r\n    %c256 = constant 256 : index\r\n    %c1024 = constant 1024 : index\r\n    %c0 = constant 0 : index\r\n    %c1 = constant 1 : index\r\n    %0 = dim %arg0, %c0 : memref<?xf16>\r\n    %1 = alloc(%0) : memref<?xf16>\r\n    %2 = cmpi \"sle\", %0, %c0 : index\r\n    %3 = subi %c0, %0 : index\r\n    %4 = subi %0, %c1 : index\r\n    %5 = select %2, %3, %4 : index\r\n    %6 = divi_signed %5, %c1024 : index\r\n    %7 = subi %c0, %6 : index\r\n    %8 = addi %6, %c1 : index\r\n    %9 = select %2, %7, %8 : index\r\n    \"gpu.launch_func\"(%9, %c1, %c1, %c256, %c1, %c1, %arg0, %1) {kernel = @Abs_kernel::@Abs_kernel} : (index, index, index, index, index, index, memref<?xf16>, memref<?xf16>) -> ()\r\n    return %1 : memref<?xf16>\r\n  }\r\n  gpu.module @Abs_kernel {\r\n    llvm.func @__nv_fabsf(!llvm.float) -> !llvm.float\r\n    llvm.func @Abs_kernel(%arg0: !llvm.ptr<half>, %arg1: !llvm.ptr<half>, %arg2: !llvm.i64, %arg3: !llvm.i64, %arg4: !llvm.i64, %arg5: !llvm.ptr<half>, %arg6: !llvm.ptr<half>, %arg7: !llvm.i64, %arg8: !llvm.i64, %arg9: !llvm.i64) attributes {gpu.kernel} {\r\n      %0 = llvm.mlir.undef : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %1 = llvm.insertvalue %arg0, %0[0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %2 = llvm.insertvalue %arg1, %1[1] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %3 = llvm.insertvalue %arg2, %2[2] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %4 = llvm.insertvalue %arg3, %3[3, 0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %5 = llvm.insertvalue %arg4, %4[4, 0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %6 = llvm.mlir.undef : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %7 = llvm.insertvalue %arg5, %6[0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %8 = llvm.insertvalue %arg6, %7[1] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %9 = llvm.insertvalue %arg7, %8[2] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %10 = llvm.insertvalue %arg8, %9[3, 0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %11 = llvm.insertvalue %arg9, %10[4, 0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      llvm.br ^bb1\r\n    ^bb1:  // pred: ^bb0\r\n      %12 = nvvm.read.ptx.sreg.ctaid.x : !llvm.i32\r\n      %13 = llvm.sext %12 : !llvm.i32 to !llvm.i64\r\n      %14 = nvvm.read.ptx.sreg.ctaid.y : !llvm.i32\r\n      %15 = llvm.sext %14 : !llvm.i32 to !llvm.i64\r\n      %16 = nvvm.read.ptx.sreg.ctaid.z : !llvm.i32\r\n      %17 = llvm.sext %16 : !llvm.i32 to !llvm.i64\r\n      %18 = nvvm.read.ptx.sreg.tid.x : !llvm.i32\r\n      %19 = llvm.sext %18 : !llvm.i32 to !llvm.i64\r\n      %20 = nvvm.read.ptx.sreg.tid.y : !llvm.i32\r\n      %21 = llvm.sext %20 : !llvm.i32 to !llvm.i64\r\n      %22 = nvvm.read.ptx.sreg.tid.z : !llvm.i32\r\n      %23 = llvm.sext %22 : !llvm.i32 to !llvm.i64\r\n      %24 = nvvm.read.ptx.sreg.nctaid.x : !llvm.i32\r\n      %25 = llvm.sext %24 : !llvm.i32 to !llvm.i64\r\n      %26 = nvvm.read.ptx.sreg.nctaid.y : !llvm.i32\r\n      %27 = llvm.sext %26 : !llvm.i32 to !llvm.i64\r\n      %28 = nvvm.read.ptx.sreg.nctaid.z : !llvm.i32\r\n      %29 = llvm.sext %28 : !llvm.i32 to !llvm.i64\r\n      %30 = nvvm.read.ptx.sreg.ntid.x : !llvm.i32\r\n      %31 = llvm.sext %30 : !llvm.i32 to !llvm.i64\r\n      %32 = nvvm.read.ptx.sreg.ntid.y : !llvm.i32\r\n      %33 = llvm.sext %32 : !llvm.i32 to !llvm.i64\r\n      %34 = nvvm.read.ptx.sreg.ntid.z : !llvm.i32\r\n      %35 = llvm.sext %34 : !llvm.i32 to !llvm.i64\r\n      llvm.br ^bb2\r\n    ^bb2:  // pred: ^bb1\r\n      %36 = llvm.mlir.constant(0 : index) : !llvm.i64\r\n      %37 = llvm.extractvalue %5[3, 0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %38 = llvm.mlir.constant(4 : index) : !llvm.i64\r\n      %39 = llvm.mlir.constant(1 : index) : !llvm.i64\r\n      %40 = llvm.mlir.constant(1024 : index) : !llvm.i64\r\n      %41 = llvm.mul %13, %40 : !llvm.i64\r\n      %42 = llvm.mlir.constant(1024 : index) : !llvm.i64\r\n      %43 = llvm.mlir.constant(-1024 : index) : !llvm.i64\r\n      %44 = llvm.mul %13, %43 : !llvm.i64\r\n      %45 = llvm.add %44, %37 : !llvm.i64\r\n      %46 = llvm.icmp \"slt\" %42, %45 : !llvm.i64\r\n      %47 = llvm.select %46, %42, %45 : !llvm.i1, !llvm.i64\r\n      %48 = llvm.mlir.undef : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %49 = llvm.extractvalue %5[0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %50 = llvm.bitcast %49 : !llvm.ptr<half> to !llvm.ptr<half>\r\n      %51 = llvm.insertvalue %50, %48[0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %52 = llvm.extractvalue %5[1] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %53 = llvm.bitcast %52 : !llvm.ptr<half> to !llvm.ptr<half>\r\n      %54 = llvm.insertvalue %53, %51[1] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %55 = llvm.extractvalue %5[4, 0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %56 = llvm.extractvalue %5[2] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %57 = llvm.mul %41, %55 : !llvm.i64\r\n      %58 = llvm.add %56, %57 : !llvm.i64\r\n      %59 = llvm.insertvalue %58, %54[2] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %60 = llvm.insertvalue %47, %59[3, 0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %61 = llvm.mlir.constant(1 : i64) : !llvm.i64\r\n      %62 = llvm.insertvalue %61, %60[4, 0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %63 = llvm.mlir.undef : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %64 = llvm.extractvalue %11[0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %65 = llvm.bitcast %64 : !llvm.ptr<half> to !llvm.ptr<half>\r\n      %66 = llvm.insertvalue %65, %63[0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %67 = llvm.extractvalue %11[1] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %68 = llvm.bitcast %67 : !llvm.ptr<half> to !llvm.ptr<half>\r\n      %69 = llvm.insertvalue %68, %66[1] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %70 = llvm.extractvalue %11[4, 0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %71 = llvm.extractvalue %11[2] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %72 = llvm.mul %41, %70 : !llvm.i64\r\n      %73 = llvm.add %71, %72 : !llvm.i64\r\n      %74 = llvm.insertvalue %73, %69[2] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %75 = llvm.insertvalue %47, %74[3, 0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %76 = llvm.mlir.constant(1 : i64) : !llvm.i64\r\n      %77 = llvm.insertvalue %76, %75[4, 0] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %78 = llvm.mlir.constant(4 : index) : !llvm.i64\r\n      %79 = llvm.mul %19, %78 : !llvm.i64\r\n      %80 = llvm.icmp \"slt\" %79, %47 : !llvm.i64\r\n      llvm.cond_br %80, ^bb3, ^bb13\r\n    ^bb3:  // pred: ^bb2\r\n      %81 = llvm.mlir.constant(4 : index) : !llvm.i64\r\n      %82 = llvm.mlir.constant(-4 : index) : !llvm.i64\r\n      %83 = llvm.mul %19, %82 : !llvm.i64\r\n      %84 = llvm.add %47, %83 : !llvm.i64\r\n      %85 = llvm.icmp \"slt\" %81, %84 : !llvm.i64\r\n      %86 = llvm.select %85, %81, %84 : !llvm.i1, !llvm.i64\r\n      %87 = llvm.icmp \"eq\" %86, %38 : !llvm.i64\r\n      llvm.cond_br %87, ^bb4, ^bb8\r\n    ^bb4:  // pred: ^bb3\r\n      llvm.br ^bb5(%36 : !llvm.i64)\r\n    ^bb5(%88: !llvm.i64):  // 2 preds: ^bb4, ^bb6\r\n      %89 = llvm.icmp \"slt\" %88, %38 : !llvm.i64\r\n      llvm.cond_br %89, ^bb6, ^bb7\r\n    ^bb6:  // pred: ^bb5\r\n      %90 = llvm.add %88, %79 : !llvm.i64\r\n      %91 = llvm.extractvalue %62[1] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %92 = llvm.extractvalue %62[2] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %93 = llvm.mlir.constant(1 : index) : !llvm.i64\r\n      %94 = llvm.mul %90, %93 : !llvm.i64\r\n      %95 = llvm.add %92, %94 : !llvm.i64\r\n      %96 = llvm.getelementptr %91[%95] : (!llvm.ptr<half>, !llvm.i64) -> !llvm.ptr<half>\r\n      %97 = llvm.load %96 : !llvm.ptr<half>\r\n      %98 = llvm.fpext %97 : !llvm.half to !llvm.float\r\n      %99 = llvm.call @__nv_fabsf(%98) : (!llvm.float) -> !llvm.float\r\n      %100 = llvm.fptrunc %99 : !llvm.float to !llvm.half\r\n      %101 = llvm.extractvalue %77[1] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %102 = llvm.extractvalue %77[2] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %103 = llvm.mlir.constant(1 : index) : !llvm.i64\r\n      %104 = llvm.mul %90, %103 : !llvm.i64\r\n      %105 = llvm.add %102, %104 : !llvm.i64\r\n      %106 = llvm.getelementptr %101[%105] : (!llvm.ptr<half>, !llvm.i64) -> !llvm.ptr<half>\r\n      llvm.store %100, %106 : !llvm.ptr<half>\r\n      %107 = llvm.add %88, %39 : !llvm.i64\r\n      llvm.br ^bb5(%107 : !llvm.i64)\r\n    ^bb7:  // pred: ^bb5\r\n      llvm.br ^bb12\r\n    ^bb8:  // pred: ^bb3\r\n      llvm.br ^bb9(%36 : !llvm.i64)\r\n    ^bb9(%108: !llvm.i64):  // 2 preds: ^bb8, ^bb10\r\n      %109 = llvm.icmp \"slt\" %108, %86 : !llvm.i64\r\n      llvm.cond_br %109, ^bb10, ^bb11\r\n    ^bb10:  // pred: ^bb9\r\n      %110 = llvm.add %108, %79 : !llvm.i64\r\n      %111 = llvm.extractvalue %62[1] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %112 = llvm.extractvalue %62[2] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %113 = llvm.mlir.constant(1 : index) : !llvm.i64\r\n      %114 = llvm.mul %110, %113 : !llvm.i64\r\n      %115 = llvm.add %112, %114 : !llvm.i64\r\n      %116 = llvm.getelementptr %111[%115] : (!llvm.ptr<half>, !llvm.i64) -> !llvm.ptr<half>\r\n      %117 = llvm.load %116 : !llvm.ptr<half>\r\n      %118 = llvm.fpext %117 : !llvm.half to !llvm.float\r\n      %119 = llvm.call @__nv_fabsf(%118) : (!llvm.float) -> !llvm.float\r\n      %120 = llvm.fptrunc %119 : !llvm.float to !llvm.half\r\n      %121 = llvm.extractvalue %77[1] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %122 = llvm.extractvalue %77[2] : !llvm.struct<(ptr<half>, ptr<half>, i64, array<1 x i64>, array<1 x i64>)>\r\n      %123 = llvm.mlir.constant(1 : index) : !llvm.i64\r\n      %124 = llvm.mul %110, %123 : !llvm.i64\r\n      %125 = llvm.add %122, %124 : !llvm.i64\r\n      %126 = llvm.getelementptr %121[%125] : (!llvm.ptr<half>, !llvm.i64) -> !llvm.ptr<half>\r\n      llvm.store %120, %126 : !llvm.ptr<half>\r\n      %127 = llvm.add %108, %39 : !llvm.i64\r\n      llvm.br ^bb9(%127 : !llvm.i64)\r\n    ^bb11:  // pred: ^bb9\r\n      llvm.br ^bb12\r\n    ^bb12:  // 2 preds: ^bb7, ^bb11\r\n      llvm.br ^bb13\r\n    ^bb13:  // 2 preds: ^bb2, ^bb12\r\n      llvm.return\r\n    }\r\n  }\r\n}%\r\n```\r\n\r\n\r\n", "comments": ["@randyzhao,\r\nCould you please check if you are facing the same issue with Bazel v3.1.0 as well? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47682\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47682\">No</a>\n"]}, {"number": 47681, "title": "Add command line option to select `-stdlib=libc++` with Xtensa.", "body": "Confirmed that the following two commands build and we see different sizes with and without `XTENSA_USE_LIBC=true`.\r\n\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade keyword_benchmark -j8 BUILD_TYPE=release XTENSA_USE_LIBC=true\r\nxt-size tensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1_release/bin/keyword_benchmark\r\n```\r\ngives:\r\n```\r\n   text\t   data\t    bss\t    dec\t    hex\tfilename\r\n  84496\t    384\t  22704\t 107584\t  1a440\ttensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1_release/bin/keyword_benchmark\r\n```\r\n\r\nAnd\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade keyword_benchmark -j8 BUILD_TYPE=release\r\nxt-size tensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1_release/bin/keyword_benchmark\r\n```\r\n\r\ngives:\r\n```\r\n   text\t   data\t    bss\t    dec\t    hex\tfilename\r\n  66696\t  40212\t  24856\t 131764\t  202b4 tensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1_release/bin/keyword_benchmark\r\n```\r\n\r\nProgress towards #47575 and http://b/182209217\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 47680, "title": "LLVM MLIR Tensorflow Compatibility", "body": "You have a great [table of tested build configurations for tensorflow in general](https://www.tensorflow.org/install/source#tested_build_configurations) \u2014 do you have something similar for getting TF's MLIR integration to work.  That is, can you suggest commits from llvm-project to use with commits from this repository that will guarantee a successful build of `mlir:tf-opt` as described in [the tensorflow MLIR readme](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/README.md)?  In that readme, there is a line suggesting that manual BUILD file changes may be required, for example if \"you are using a different version of LLVM than set in tensorflow/workspace.bzl's LLVM_COMMIT\", but I don't see a reference to `LLVM_COMMIT` in any of the 4 workspace*.bzl files in the tensorflow directory.\r\n\r\nAlternatively/Additionally, is there any further guidance toward building `tf-opt` from source than that given in the aforementioned readme?\r\n\r\nAny help would be appreciated.", "comments": ["At the moment, TensorFlow is revision-locked with LLVM, there is no guarantee that you can build TensorFlow with any other revision that the one specified in the workspace.bzl. We upgrade to the top of the main LLVM branch almost every day, sometimes multiple times per day.\r\n\r\nThe LLVM_COMMIT mentioned in the doc has been refactored an is now in third_party/llvm/workspace.bzl ; the instructions are only useful for testing purpose.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Thanks for the info!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47680\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47680\">No</a>\n"]}, {"number": 47679, "title": "[Intel MKL] Make Vanilla TF and MKL-based TF use common oneDNN build", "body": "", "comments": ["@penpornk Here is the PR to unify the oneDNN build. Thank you!", "@mahmoud-abuzaina Thank you for the PR!\r\nhttps://github.com/tensorflow/tensorflow/pull/47543 might be reverted soon because of a nightly docker test build failure. I won't review this PR yet until we are sure the oneDNN v2.1 upgrade will stay.", "@penpornk Thank you for the update. If you can share the oneDNN-related failure details, that would help us to look at it and fix it.", "@mahmoud-abuzaina Can you please resolve conflicts? Thanks!", "@gbaned this PR depends on https://github.com/tensorflow/tensorflow/pull/47743. I will need to resolve the conflicts anyway once that is merged. ", "Thank you for pointing out the solution for the build error. I have made the changes. "]}, {"number": 47678, "title": "When Tensorflow support python 3.9 with m1 chip", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Mac os 11.2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version:??\r\n- Python version:3.9.0\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen could tensorflow support python 3.9 with m1 chip? And in other sense when tensorflow dev summit 2021 convening\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@gabrielpondc,\r\nThere is a detailed thread about `Tensorflow's` support for `Python 3.9` in #44485. Can you  please confirm if we can close this issue as it is being already tracked? Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47678\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47678\">No</a>\n"]}, {"number": 47677, "title": "[ROCm] Fix for ROCm CSB breakage - 210309", "body": "The following commit breaks the ROCm build\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/abe97cb58c3eb139d4628b5d2c268a7e7a282115\r\n\r\n```\r\nIn file included from tensorflow/core/kernels/sparse_tensor_dense_matmul_op_gpu.cu.cc:23:\r\nIn file included from ./tensorflow/core/util/gpu_kernel_helper.h:25:\r\n./tensorflow/core/util/gpu_device_functions.h:721:10: error: no matching function for call to 'atomicAdd'\r\n  return atomicAdd(detail::ToCudaSupportedPtr(ptr), value);\r\n         ^~~~~~~~~\r\ntensorflow/core/kernels/sparse_tensor_dense_matmul_op_gpu.cu.cc:63:7: note: in instantiation of function template specialization 'tensorflow::GpuAtomicAdd<std::complex<float>, std::complex<float>>' requested here\r\n      GpuAtomicAdd(out_location, OutOfBoundsValue<Tsum>::value());\r\n      ^\r\ntensorflow/core/kernels/sparse_tensor_dense_matmul_op_gpu.cu.cc:119:9: note: in instantiation of function template specialization 'tensorflow::SparseTensorDenseMatMulKernel<std::complex<float>, std::complex<float>, int, false, false>' requested here\r\n        SparseTensorDenseMatMulKernel<T, Tsum, Tindices, ADJ_A, ADJ_B>,\r\n        ^\r\n/opt/rocm-4.0.1/hip/include/hip/hcc_detail/hip_atomic.h:39:5: note: candidate function not viable: no known conversion from 'CudaSupportedType<std::complex<float>> *' (aka 'std::complex<float> *') to 'int *' for 1st argument\r\nint atomicAdd(int* address, int val)\r\n    ^\r\n/opt/rocm-4.0.1/hip/include/hip/hcc_detail/hip_atomic.h:45:14: note: candidate function not viable: no known conversion from 'CudaSupportedType<std::complex<float>> *' (aka 'std::complex<float> *') to 'unsigned int *' for 1st argument\r\nunsigned int atomicAdd(unsigned int* address, unsigned int val)\r\n             ^\r\n/opt/rocm-4.0.1/hip/include/hip/hcc_detail/hip_atomic.h:51:20: note: candidate function not viable: no known conversion from 'CudaSupportedType<std::complex<float>> *' (aka 'std::complex<float> *') to 'unsigned long long *' for 1st argument\r\nunsigned long long atomicAdd(\r\n                   ^\r\n/opt/rocm-4.0.1/hip/include/hip/hcc_detail/hip_atomic.h:58:7: note: candidate function not viable: no known conversion from 'CudaSupportedType<std::complex<float>> *' (aka 'std::complex<float> *') to 'float *' for 1st argument\r\nfloat atomicAdd(float* address, float val)\r\n      ^\r\n/opt/rocm-4.0.1/hip/include/hip/hcc_detail/hip_atomic.h:86:8: note: candidate function not viable: no known conversion from 'CudaSupportedType<std::complex<float>> *' (aka 'std::complex<float> *') to 'double *' for 1st argument\r\ndouble atomicAdd(double* address, double val)\r\n```\r\n\r\nThis commit fixes the build error by enabling for ROCm, the `GpuAtomicAdd` specialization for std::complex types, already in place for CUDA\r\n\r\n--------------------------------------------------\r\n\r\n/cc @cheshire @chsigg ", "comments": ["@gbaned this PR seems to have gotten stuck in the merge pipeline...anything we can do on our end to get it merged?"]}, {"number": 47676, "title": "Multi Worker Mirrored Strategy -- math", "body": "In the Multi Worker Mirrored Strategy (seen here https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras) it is not clear what is going on with the gradients and backpropogation under the hood (i.e., the math). For example, are gradients being averaged among the workers in the cluster? I cannot seem to figure this out as when I run on one machine without using a distributed strategy and on GPU my training/validation loss and training/validation metric differ significantly when I run on a cluster of machines using the distributed strategy Multi Worker Mirrored Strategy.  I was hoping this would not happen, i.e., you would add machines to the cluster to ideally speed up training, with loss and performance metric (MSE, RMSE) being roughly equal.\r\n\r\nAlso, if there is some reading/documentation on the math/how this is being implemented, I would appreciate. it. Could not find any so far.", "comments": ["@inespancorbo,\r\nCould you please go through [this guide](https://www.tensorflow.org/guide/distributed_training#multiworkermirroredstrategy) which gives a brief overview of MultiWorkerMirroredStrategy and let us know if it helps. Thanks!", "I have already looked at that unfortunately. I am asking about gradient updates/losses etc. How are these handled when many machines are involved?", "Hi @inespancorbo, I think the mental model here is easier to understand if we start with the `MirroredStrategy` case.\r\n\r\n`MirroredStrategy` is a synchronous data parallelism strategy. This means that when you call `model.fit`, `MirroredStrategy` will make a copy (known as a replica) of your model on each GPU. The CPU (host) is responsible for preparing the `tf.data.Dataset` batches and sending the data to the GPUs (devices).\r\n\r\nThe subsequent gradient updates will happen in a synchronous manner. This means that each replica computes the forward and backward passes through the model on a different slice of the input data. \r\n\r\nThe computed gradients from each of these slices are then aggregated across all of the devices and reduced (usually an average) in a process known as all-reduce. There are many all-reduce algorithms and implementations available, depending on the type of communication available between devices. By default, `MirroredStrategy` uses [NVIDIA NCCL](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html) as the all-reduce implementation.\r\n\r\nAfter the reduction, the optimizer then performs the parameter updates with these reduced gradients thereby keeping the devices in sync. Because each worker cannot proceed to the next training step until all the other workers have finished the current step, this gradient calculation becomes the main overhead in distributed training for synchronous strategies.\r\n\r\nSo that's what happens in the case of a single machine with multiple GPUs.\r\n\r\nAbstracting out to multiple machines is not much different. Like its single-worker counterpart, `MirroredStrategy`, `MultiWorkerMirroredStrategy` is a synchronous data parallelism strategy.\r\n\r\nThe main difference when moving from synchronous data parallelism on one machine to many machines is that the gradients at the end of each step now need to be synchronized across all GPUs in a machine and across all machines in the cluster. This additional step of synchronizing across the machines increases the overhead of distribution. \r\n\r\nIn TensorFlow, the multi-worker all-reduce communication is achieved via CollectiveOps. You don\u2019t need to know much detail to execute a successful and performant training job, but at a high level,  a collective op is a single op in the TensorFlow graph that can automatically choose an all-reduce algorithm according to factors such as hardware, network topology, and tensor sizes.\r\n\r\nThe last thing to note is the distribution of the data. In the single worker case, at each step your dataset is divided up across the replicas on your machine. So when you do distributed training with the `tf.distribute.Strategy` API and `tf.data`, the batch size now refers to the global batch size. In other words, if you pass a batch size of 10, and you have two GPUs, then each machine will process 5 examples per step. In this case, 10 is known as the _global batch size_, and 5 as the per replica batch size. To make the most out of your GPUs, you will want to scale the batch size by the number of replicas, which is two in this case because there is one replica on each GPU.\r\n\r\n```\r\nGLOBAL_BATCH_SIZE = PER_REPLICA_BATCH_SIZE * strategy.num_replicas_in_sync\r\n```\r\n\r\nThis data splitting process becomes slightly more complicated in the multi-worker case. The data now also needs to be sharded, meaning that each worker is assigned a subset of the entire dataset. Therefore, at each step a global batch size of non overlapping dataset elements will be processed by each worker. This sharding happens automatically with `tf.data.experimental.AutoShardPolicy.` You can read more about the different sharding policies and see examples in the [Distributed Input guide](https://www.tensorflow.org/tutorials/distribute/input).\r\n\r\nHopefully this explanation at least helps with the mental model of what's going on and gives a starting point if you'd like to dig into the math details.\r\n\r\nI'm closing this issue now because Github is best for bugs/performance issues. For further support on this topic, I recommend posting in Stack Overflow. And if you think you've discovered a bug, please open a new issue following the bug template. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47676\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47676\">No</a>\n"]}, {"number": 47675, "title": "set_weights crashes in custom train_step() method", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: Yes\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**: binary\r\n-   **TensorFlow version (use command below)**: 2.4.1\r\n-   **Python version**: 3.8.0\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**: 11.0/8\r\n-   **GPU model and memory**: Quadro RTX 6000 (24GB)\r\n-   **Exact command to reproduce**:\r\n\r\n```python\r\nclass DummyExample(tf.keras.Model):\r\n    def __init__(\r\n        self,\r\n        output_dim: int,\r\n        decay_rate: float\r\n    ):\r\n        super(DummyExample, self).__init__()\r\n        self.output_dim = output_dim\r\n        self.decay = decay_rate\r\n        self.encoder_1 = self.create_encoder()\r\n        self.encoder_2 = self.create_encoder()\r\n\r\n    def create_encoder(self):\r\n        encoder = tf.keras.Sequential()\r\n        encoder.add(tf.keras.Input(shape=(4,), dtype=tf.float32))\r\n        encoder.add(tf.keras.layers.Dense(\r\n            units=self.output_dim,\r\n            activation='relu'))\r\n        return encoder\r\n\r\n    def call(self, x: tf.Tensor, training: bool = False):\r\n        return self.encoder_1(x, training=training)\r\n    \r\n    def train_step(self, data):\r\n        x = data\r\n        out_2 = self.encoder_2(x, training=True)\r\n        with tf.GradientTape() as tape:\r\n            out_1 = self.encoder_1(x, training=True)\r\n            loss = self.compiled_loss(out_1, out_2)\r\n        enc_1_grads = tape.gradient(loss, self.encoder_1.trainable_weights)\r\n        self.optimizer.apply_gradients(zip(enc_1_grads, self.encoder_1.trainable_weights))\r\n        \r\n        # update encoder_2\r\n        enc_1_weights = self.encoder_1.weights\r\n        enc_2_weights = self.encoder_2.weights\r\n        for i in range(len(enc_1_weights)):\r\n            enc_2_weights[i] = self.decay * enc_2_weights[i] + (1-self.decay) * enc_1_weights[i]\r\n        self.encoder_2.set_weights(enc_2_weights)\r\n        return {'loss': loss}\r\n\r\ndummy_model = DummyExample(2, 0.99)\r\ndummy_model.compile(optimizer=tf.keras.optimizers.Adam(3e-4), loss='mse')\r\n\r\nx = np.random.normal(size=(100, 4))\r\nds = tf.data.Dataset.from_tensor_slices(x)\r\nds = ds.batch(10)\r\n\r\ndummy_model.fit(ds, epochs=2)\r\n```\r\n### Describe the problem\r\nThe above fit() call for the dummy example raises \r\n```bash\r\nTypeError: in user code:\r\n\r\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\r\n        return step_function(self, iterator)\r\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\r\n        outputs = model.train_step(data)\r\n    <ipython-input-63-77e2fc2e32e6>:36 train_step\r\n        self.encoder_2.set_weights(enc_2_weights)\r\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer.py:1877 set_weights\r\n        backend.batch_set_value(weight_value_tuples)\r\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\r\n        return target(*args, **kwargs)\r\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/backend.py:3706 batch_set_value\r\n        x.assign(np.asarray(value, dtype=dtype(x)))\r\n    /usr/local/lib/python3.8/site-packages/numpy/core/_asarray.py:83 asarray\r\n        return array(a, dtype, copy=False, order=order)\r\n\r\n    TypeError: __array__() takes 1 positional argument but 2 were given\r\n``` \r\n\r\nI think set_weights works only with list of numpy arrays, which somehow doesn't seem to work when the train logic is defined in the train_step() function. [This issue](https://github.com/tensorflow/tensorflow/issues/32927#issue-500463734) has a similar problem when the function containing set_weights is decorated by tf.function. Any workaround is greatly appreciated. Thanks in advance.\r\n", "comments": ["Issue is reproducible on tf 2.3,2.4 and nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/2c65d01df54c08c047db873224be9720/untitled561.ipynb)", "Was able to replicate the issue in TF 2.6.0-dev20210531,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/5bad04b88809c4043670f3eb63ee005e/untitled156.ipynb)..Thanks !", "@hm2092 hi, have you solved this problem and by how? @rmothukuru ,why there's no update on this issue for half an year?\r\n\r\n", "Hello @sjtusmartboy, i implemented a workaround with callbacks. I wrapped the EMA update for `self.encoder_2` in a `tf.keras.callbacks.Callback` wrapper class. Although the training was a little bit slow due to this, it was enough for my use case. `set_weights` method will work within the callback class.", "@hm2092 \r\nCan you please move this to closed status as its resolved.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47675\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47675\">No</a>\n", "Any news on this? "]}, {"number": 47674, "title": "Deallocation not happening when running on GPU in iOS swift", "body": "System information\r\n\r\n    OS Platform - iOS, swift\r\n    Mobile device : iPhone, iPad\r\n    TensorFlow installed from (source or binary): Pod.\r\n    TensorFlow version (use command below): 2.3.0\r\n\r\n**Execution**\r\nInitialised interpreter once with below code\r\nvar delegate = MetalDelegate()\r\nself.interpreter = try Interpreter(modelPath: modelPath, options: nil, delegates: [delegate])\r\nself.interpreter?.allocateTensors()\r\n\r\nThen executed the below code each time on running the inference\r\nvar inputTensor = try interpreter?.copy(input, toInputAt:0)\r\ntry interpreter?.invoke()\r\nvar outputTensor = try interpreter?.output(at: 0)\r\n\r\n**Issue**\r\nOn running the .invoke() in GPU (using MetalDelegate) the memory consumption is increasing and not decreasing. How do i deallocate the unused memory? This is working fine in CPU.", "comments": ["@jestingeorgejacob1,\r\nSeems like this is a duplicate of issue [#47640](https://github.com/tensorflow/tensorflow/issues/47640). I'm closing this issue as it is already being tracked there. \r\n\r\nPlease feel free to re-open if I'm mistaken. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47674\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47674\">No</a>\n"]}, {"number": 47673, "title": "Possible memory leak cloning a model in memory with TF 2.3.1", "body": "Hi,\r\n\r\nI created a flask application which loads tensorflow models on startup. When an inference request is coming, the model is cloned in memory for the request like this:\r\n\r\n```\r\nmodel_copy =  tf.keras.models.clone_model(model)\r\nmodel_copy.build()\r\nmodel_copy.set_weights(model.get_weights())\r\n```\r\n\r\nThe flask app runs in docker. After several requests the docker container crashed. With each request the RAM is increasing. After disabling the model cloning and doing the requests with one model in memory with locking, the memory does no more increasing.\r\n\r\n**System information**\r\n- OS Platform and Distribution: debian 10\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: 3.6.9\r\n", "comments": ["@OliverPfau,\r\nIn order to expedite the trouble-shooting process, could you please provide a minimal code snippet to reproduce the issue reported here and also the dataset you are using. Thanks!", "The model is a tf.keras.Sequential model learning image classification problem:\r\n\r\n```\r\nmodel = tf.keras.Sequential([\r\n...\r\n])\r\n```\r\nsave it like this:\r\n`model.save(model_save_path)`\r\n\r\nload it:\r\n`model = tf.keras.models.load_model(model_path)`\r\n\r\neach time I make a in memory copy the RAM is increasing.\r\nCloning:\r\n```\r\nmodel_copy =  tf.keras.models.clone_model(model)\r\nmodel_copy.build()\r\nmodel_copy.set_weights(model.get_weights())\r\n```\r\n\r\n", "@OliverPfau,\r\nSimiliar issue [#45453](https://github.com/tensorflow/tensorflow/issues/45453#issuecomment-768554610) was fixed in the tf-nightly version.\r\n\r\nCould you please update TensorFlow and check if you are facing the same issue with TF v2.4.1 and TF-nightly as well? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "I did some tests inside docker containers. I have 3 tensorflow image classification models. Each model is invoked with the same image data 100 times. So I have 300 model invokations in summary for my test.\r\n\r\nNo model cloning -> use one instance for each model (3 in summary) and locking\r\nWith model cloning -> for each model invokation the model is cloned\r\n\r\nNo model cloning TF 2.3.1: used RAM on start: 0,982 Gb; used RAM after test: 1,121 Gb\r\nWith model cloning TF 2.3.1: used RAM on start: 0,977 Gb; used RAM after test: 9,667 Gb\r\n\r\nNo model cloning TF 2.4.1: used RAM on start: 1,278 Gb; used RAM after test: 1,365 Gb\r\nWith model cloning TF 2.4.1: used RAM on start: 1,134 Gb; used RAM after test: 3,039 Gb\r\n\r\nSo with TF 2.4.1 the used RAM is 3 times smaller, but it seems like there a memory leak with increasing RAM consumption. With both TF versions the RAM is increasing and gc.collect() does not free any RAM.\r\n", "@OliverPfau Sorry for the late response.\r\n\r\nJust a quick question. Why do you want to clone a model when you can use original model? Cloning creates new layers and new weights which will increase memory requirements. Thanks!\r\n\r\nCheck https://www.tensorflow.org/api_docs/python/tf/keras/models/clone_model for more details ", "Every request making a prediction runs inside docker. So the idea was to preload the models, clone the appropriate model for the request, do the request in docker and dispose the copied model.", "@OliverPfau Please note that Keras development moved to another repository to focus entirely on only keras. Could you please repost this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 47672, "title": "How to use a fractionally strided deconvolution layer? Please mention its initialization and also corresponding upsampling of its input", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["Hi,\r\n  I was trying to implement and integrate refinedet object detection module into the tensorflow/models repo as a experimental model. The feature extractor contains an transfer connection block which need fractionally strided deconv in it. How to implement that particular deconv using tensorflow?\r\nThanks in advance", "@VenuGopalVasarla \r\n\r\nAs this is not a bug or a feature request, can you please create an issue on [stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow) for this and move this to closed status.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 47671, "title": "More info on the deprecation of predict_x method for Sequential model?", "body": "I traced down to https://github.com/tensorflow/tensorflow/commit/9795964342fbb0b6104b5ae131600d62faddb8c9 which marked predict_x method as deprecated.\r\n\r\nBut are there more information on the deprecation? Who made the decision? Are there any discussion on the deprecation? Are there any whatsnew/breaking changes items?", "comments": ["Many of the API are depreciated and moved to new mode mainly because it runs on v1.Session style code and may fall under compatibility guarantees and can behave unexpectedly when combined with TF 2 code also to make the user experience better with the improved performance these changes will be made.", "Fine. If you don't want to revise some docs, I will close the issue.", "Depreciation warning will be added initially as you see in the linked PR, in the subsequent Tensorflow versions it will be removed. You can go ahead and close the issue. Thanks!"]}, {"number": 47670, "title": "Does tensorflow support AMD?", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n- Are you willing to contribute it (Yes/No):\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n", "comments": ["Dear sir,\r\n  Please tell me whether tensorflow can be running on AMD gpu? \r\nos: linux, such as fedora or debian\r\nMy gpu is AMD, I want to running tensorflow on AMD gpu.\r\nhow can I build tensorflow with AMD gpu? \r\n If use AMD gpu,  only should ROCm or OpenCL be supported?  or both ROCm and OpenCL should be supported\uff1f\r\nwhich series AMD gpu should used?\r\n \r\nThank you!", "@zhangqiang-316,\r\nGPU support for TensorFlow is available only for Ubuntu and Windows with CUDA enabled cards. \r\n\r\nFor more information, please take a look at [this guide](https://www.tensorflow.org/install/gpu) and [this comment](https://github.com/tensorflow/tensorflow/issues/32358#issuecomment-530234081) from a member of the TensorFlow team from similar issue [#32358](https://github.com/tensorflow/tensorflow/issues/32358). Thanks!", "> @zhangqiang-316,\r\n> GPU support for TensorFlow is available only for Ubuntu and Windows with CUDA enabled cards.\r\n> \r\n> For more information, please take a look at [this guide](https://www.tensorflow.org/install/gpu) and [this comment](https://github.com/tensorflow/tensorflow/issues/32358#issuecomment-530234081) from a member of the TensorFlow team from similar issue [#32358](https://github.com/tensorflow/tensorflow/issues/32358). Thanks!\r\n\r\nbut,from the github:   https://github.com/tensorflow/tensorflow\r\nCommunity Supported Builds: it tells Linux AMD build ok.\r\n\r\nBuild Type | Status | Artifacts\r\n-- | -- | --\r\nLinux AMD ROCm GPU\u00a0Nightly | \u00a0 | Nightly\r\nLinux AMD ROCm GPU\u00a0Stable Release | \u00a0 | Release\u00a01.15\u00a0/\u00a02.x\r\n\r\nalso, from the source code: tensorflow/core, it include ROCm directory! So, Maybe tensorflow support AMD GPU with ROCm?", "> @zhangqiang-316,\r\n> GPU support for TensorFlow is available only for Ubuntu and Windows with CUDA enabled cards.\r\n> \r\n> For more information, please take a look at [this guide](https://www.tensorflow.org/install/gpu) and [this comment](https://github.com/tensorflow/tensorflow/issues/32358#issuecomment-530234081) from a member of the TensorFlow team from similar issue [#32358](https://github.com/tensorflow/tensorflow/issues/32358). Thanks!\r\n\r\nbut,from the github:   https://github.com/tensorflow/tensorflow\r\nCommunity Supported Builds: it tells Linux AMD build ok.\r\n\r\nBuild Type | Status | Artifacts\r\n-- | -- | --\r\nLinux AMD ROCm GPU\u00a0Nightly | \u00a0 | Nightly\r\nLinux AMD ROCm GPU\u00a0Stable Release | \u00a0 | Release\u00a01.15\u00a0/\u00a02.x\r\n\r\nalso, from the source code: tensorflow/core, it include ROCm directory! So, Maybe tensorflow support AMD GPU with ROCm?"]}, {"number": 47669, "title": "Tensorflow 2.3.1 training does not use GPU but CPU with  CUDA 11.2 ", "body": "\r\nI am trying to train a custom dataset with the following backbone \r\n\r\nssd_resnet50_v1_fpn_1024x1024_coco17_tpu-8\r\n\r\nI have the following information about the system \r\n\r\n### System information\r\n Ubuntu 18.04)**:\r\nTensorFlow 2.3.1\r\nTensorFlow version\r\nPython version 3.6\r\nCUDNN 11.2 cuDNN 8.5\r\nGPU K80\r\n\r\n\r\nI get the following error when I try to train the model \r\n\r\n\r\nWhen I try to train my model it showed the following log when starting training and I see that it does not use GPU at all..\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/47017344/110425796-f5005980-8072-11eb-99ca-281c03cdeed0.png)\r\n\r\n\r\nHere is the output of the NVIDA GPU - that is nothing is used\r\n\r\n![image](https://user-images.githubusercontent.com/47017344/110425826-00538500-8073-11eb-8035-532a14ab0e27.png)\r\n\r\n\r\n\r\n\r\n", "comments": ["The training has started but it is using CPU\r\n\r\n![image](https://user-images.githubusercontent.com/47017344/110427640-0f880200-8076-11eb-9f26-e92723b9bcd7.png)\r\n", "@awaisbajwaml \r\nPlease upgrade to tf 2.4 and let us know, as 11.0 is tested for tf 2.4 onwards.\r\nCuda 11.2 is work in progress, you may refer to [this issue](47568), for future updates on it.", "@Saduf2019 \r\n\r\nI am able to solve the issue, here are the steps:\r\n\r\nFirst I upgraded to 2.4.0 but it gave me multiple errors.\r\n\r\nAfter that, I used symlinks to start the training again, and now it is working fine. \r\n\r\nI have summarized a blog post [here](https://awaisbajwa.medium.com/tensorflow-2-4-with-cuda-11-2-gpu-training-fix-87f205215419) and hence closing the issue. \r\n\r\nThanks for your help "]}, {"number": 47668, "title": "[tf.data] move checkpoint tests to kernel tests part 4", "body": "This PR is a continuation of https://github.com/tensorflow/tensorflow/pull/47648 and moves the checkpoint tests of:\r\n\r\n1. scan\r\n2. shuffle_and_repeat\r\n3. snapshot\r\n4. sql \r\n5. stats (no-oss)\r\n6. take_while\r\n7. unique\r\ndatasets to kernel tests\r\n\r\nTEST LOG\r\n```\r\n//tensorflow/python/data/experimental/kernel_tests:scan_test             PASSED in 5.8s\r\n//tensorflow/python/data/experimental/kernel_tests:shuffle_and_repeat_test PASSED in 5.0s\r\n//tensorflow/python/data/experimental/kernel_tests:snapshot_test         PASSED in 32.1s\r\n//tensorflow/python/data/experimental/kernel_tests:sql_dataset_test      PASSED in 6.2s\r\n//tensorflow/python/data/experimental/kernel_tests:take_while_test       PASSED in 9.1s\r\n//tensorflow/python/data/experimental/kernel_tests:unique_test           PASSED in 4.3s\r\n```\r\n\r\nAlso, this PR is a part of the larger cleanup as discussed in point 4 of https://github.com/tensorflow/tensorflow/pull/46761#issuecomment-770059963\r\n\r\ncc: @jsimsa", "comments": ["FYI this change is getting rolled back because `snapshot_test` is breaking on Windows (and Windows tests are not executed as part of presubmit). I suggest resubmitting this PR in two parts: 1) one for snapshot_test changes and 2) everything else. I will investigate (and fix) the `snaphost_test` breakage once you prepare the PR.", "Actually, the easier thing to do will be to roll forward your change, disabling the offending test on windows. I can take care of that.", "@jsimsa thanks. Please let me know how it goes."]}, {"number": 47667, "title": "Fix for issue #47216", "body": "FIX TypeError if set the weights to the current weights via `set_weights, \r\nIncorporated suggestions by @qlzh727 i.e. included type checking to reduce overhead.\r\nSimply, this change will allow users to use .weights to initialize the set_weights() method of a layer without any type error improving the versatility of keras.", "comments": ["@fchollet, Thank you for your advice, I will include the changes you suggested.", "Please include a unit test.", "\"\"\"\r\n    >>> a = tf.keras.layers.Dense(1,\r\n    ...   kernel_initializer=tf.constant_initializer(1.))\r\n    >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\r\n    >>> a.get_weights()\r\n    [array([[1.],\r\n           [1.],\r\n           [1.]], dtype=float32), array([0.], dtype=float32)]\r\n    >>> b = tf.keras.layers.Dense(1,\r\n    ...   kernel_initializer=tf.constant_initializer(2.))\r\n    >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\r\n    >>> b.get_weights()\r\n    [array([[2.],\r\n           [2.],\r\n           [2.]], dtype=float32), array([0.], dtype=float32)]\r\n    >>> b.set_weights(a.weights)\r\n    >>> b.get_weights()\r\n    [array([[1.],\r\n           [1.],\r\n           [1.]], dtype=float32), array([0.], dtype=float32)]\r\n\"\"\"\r\nimport doctest\r\ndoctest.testmod()\r\n`", "@fchollet, I found this test on the tensorflow website for set_weights() method, I made modifications to the doctest to include .weights instead of get_weights() method to assign weights. ", "Sorry for the long wait. The Keras code base has actually been migrated to github.com/keras-team/keras. If you still would like to contribute the PR, feel free to post your change to the new repository. I am closing this PR for now."]}, {"number": 47666, "title": "Runtime error of executing HLO when adding SPMD pass", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux n130-024-068 4.14.81.bm.26-amd64 #1 SMP Debian 4.14.81.bm.26 Mon Sep 14 09:46:45 UTC 2020 x86_64 GNU/Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): TF master branch with commit id: a4ac4894d536bde05dafe74a3a3fec3aa0cb93b8\r\n- Python version:3.8.5\r\n- Bazel version (if compiling from source):3.7.2\r\n- GCC/Compiler version (if compiling from source): gcc (Debian 8.3.0-6) 8.3.0\r\n- CUDA/cuDNN version:V11.0.221\r\n- GPU model and memory:V100/32GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI want to test SPMD pass. \r\nThe input is a single-device HLO-IR with SPMD sharding.\r\nThe program firstly would modify the single-device hlo-ir to 4-device hlo-ir;\r\nThen, it would compile the modified hlo-ir to and executable;\r\nAfter that, the pjrt client would run the executable on 4 GPUs, and return the result.  \r\nCurrently, I use the latest TF code (TF master branch with commit id: a4ac4894d536bde05dafe74a3a3fec3aa0cb93b8), it can successfully compile the code. However, it fails to execute the code and return the expected results.\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\nIt should compile the hlo, execute the executable, and return the result successfully.(i.e., [[3,3],[7,7]])\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nthere are four steps to reproduce the problem:\r\n\r\n**step1: tensorflow/compiler/xla/tools/BUILD**\r\n```bazel\r\n### new added tf_cc_binary to build the pjrt_demo\r\ntf_cc_binary(\r\n    name = \"pjrt_demo\",\r\n    #testonly = True,\r\n    srcs = [\"pjrt_client_main.cc\"],\r\n    deps = [\r\n        \"@com_google_absl//absl/strings\",\r\n        \"//tensorflow/compiler/xla/pjrt:gpu_device\",\r\n        \"//tensorflow/compiler/xla/pjrt:pjrt_client\",\r\n        \"//tensorflow/compiler/xla/pjrt:cpu_device\",\r\n        \"//tensorflow/compiler/xla/tools:hlo_module_loader\",\r\n\r\n        ### SPMD pass\r\n        \"//tensorflow/compiler/xla/service/spmd:spmd_partitioner\",\r\n        \"//tensorflow/compiler/xla/service:hlo_pass_pipeline\",\r\n\r\n        \"//tensorflow/compiler/xla/service:computation_layout\",\r\n        \"//tensorflow/compiler/xla/service:layout_assignment\",\r\n\r\n        \r\n    ] + if_cuda_or_rocm([\r\n        \"//tensorflow/compiler/xla/service:gpu_plugin\",\r\n    ]),\r\n)\r\n\r\n```\r\n\r\n**step2: tensorflow/compiler/xla/tools/pjrt_client_main.cc**\r\nnew added file to present the pjrt demo in CPP\r\n```CPP\r\n#include <memory>\r\n#include <string>\r\n#include <vector>\r\n\r\n#include \"tensorflow/compiler/xla/literal.h\"\r\n#include \"tensorflow/compiler/xla/literal_util.h\"\r\n#include \"tensorflow/compiler/xla/pjrt/cpu_device.h\"\r\n#include \"tensorflow/compiler/xla/pjrt/gpu_device.h\"\r\n#include \"tensorflow/compiler/xla/pjrt/pjrt_client.h\"\r\n#include \"tensorflow/compiler/xla/status.h\"\r\n#include \"tensorflow/compiler/xla/statusor.h\"\r\n#include \"tensorflow/compiler/xla/tools/hlo_module_loader.h\"\r\n#include \"tensorflow/core/platform/init_main.h\"\r\n#include \"tensorflow/core/platform/logging.h\"\r\n\r\n#include \"tensorflow/compiler/xla/service/spmd/spmd_partitioner.h\"\r\n#include \"tensorflow/compiler/xla/service/hlo_pass_pipeline.h\"\r\n#include \"tensorflow/compiler/xla/service/hlo_verifier.h\"\r\n\r\n#include \"tensorflow/compiler/xla/service/computation_layout.h\"\r\n#include \"tensorflow/compiler/xla/service/layout_assignment.h\"\r\n\r\n\r\n\r\nusing namespace xla;\r\nusing namespace xla::spmd;\r\n\r\nnamespace\r\n{\r\n    void AssignLayouts(HloModule* m, ComputationLayout* entry_computation_layout,\r\n                     ChannelLayoutConstraints* channel_constraints = nullptr) {\r\n    LayoutAssignment layout_assignment(\r\n        entry_computation_layout, LayoutAssignment::InstructionCanChangeLayout,\r\n        /*channel_constraints=*/channel_constraints);\r\n    if(!layout_assignment.Run(m).status().ok())\r\n        LOG(FATAL)<<\"Layout assign failed\";\r\n  }\r\n\r\n  struct PjrtDemoArgs\r\n        {\r\n            PjrtDemoArgs()\r\n                : platform(\"GPU\"),\r\n                  reference_platform(\"default\"),\r\n                  print_literals(false),\r\n                  run_test_hlo_passes(true),\r\n                  run_reference_hlo_passes(true),\r\n                  use_large_float_range(false),\r\n                  // TODO(b/68721786): These tolerances are set to match the values in the\r\n                  // isolation test. The goal is to lower these to 0.001.\r\n                  abs_error_bound(0.1),\r\n                  rel_error_bound(0.1),\r\n                  input_format(\"hlo\"),\r\n                  input_text(\"\"),\r\n                  iterations(1),\r\n                  print_hlo_ir(false),\r\n                  num_replicas(1),\r\n                  dump_to_file(false),\r\n                  speficy_layout(false),\r\n                  execute_shared(false),\r\n                  enable_spmd_pass(false)\r\n            {\r\n            }\r\n            std::string platform;\r\n            std::string reference_platform;\r\n            bool print_literals;\r\n            bool run_test_hlo_passes;\r\n            bool run_reference_hlo_passes;\r\n            bool use_large_float_range;\r\n            float abs_error_bound;\r\n            float rel_error_bound;\r\n            std::string input_format;\r\n            std::string input_text;\r\n            int iterations;\r\n            bool print_hlo_ir;\r\n            int num_replicas;\r\n            bool dump_to_file;\r\n            bool speficy_layout;\r\n            bool execute_shared;\r\n            bool enable_spmd_pass;\r\n        };\r\n\r\n    PjrtDemoArgs get_args(int argc, char** argv)\r\n    {\r\n        PjrtDemoArgs opts;\r\n        std::vector<tensorflow::Flag> flag_list = {\r\n            tensorflow::Flag(\r\n                \"platform\", &opts.platform,\r\n                \"The test platform that the HLO module will be executed on \"\r\n                \"(gpu, cpu, etc).\"),\r\n            tensorflow::Flag(\r\n                \"reference_platform\", &opts.reference_platform,\r\n                \"The reference platform that HLO module will be \"\r\n                \"executed on. The result produced on the reference platform will \"\r\n                \"be compared against the result produced on the test platform. A \"\r\n                \"value of 'default' will use the TPU_Interpreter as a reference if \"\r\n                \"the test platform is a TPU, and 'interpreter' otherwise. If the \"\r\n                \"flag value is the empty string, then the module will not be run \"\r\n                \"on a reference platform at all.\"),\r\n            tensorflow::Flag(\"print_literals\", &opts.print_literals,\r\n                            \"Print the input and result literals to stdout.\"),\r\n            tensorflow::Flag(\r\n                \"run_test_hlo_passes\", &opts.run_test_hlo_passes,\r\n                \"Run HLO pass pipeline for the test platform on the HLO module \"\r\n                \"before running the module on the test platform. This should be \"\r\n                \"set to true if the HLO module is unoptimized and set to false if \"\r\n                \"the HLO module already has been optimized.\"),\r\n            tensorflow::Flag(\r\n                \"run_reference_hlo_passes\", &opts.run_reference_hlo_passes,\r\n                \"Run HLO pass pipeline for the reference platform on the HLO module \"\r\n                \"before running the module on the reference platform. \"\r\n                \"In general, if the given HLO module was optimized for a platform \"\r\n                \"other \"\r\n                \"than the reference this is necessary because some HLO passes are \"\r\n                \"legalization passes which must be run prior to code generation.\"),\r\n\r\n            tensorflow::Flag(\r\n                \"use_large_float_range\", &opts.use_large_float_range,\r\n                \"Generate floating point values using a large uniform-log \"\r\n                \"distribution as opposed to a small uniform distribution.\"),\r\n            tensorflow::Flag(\r\n                \"abs_error_bound\", &opts.abs_error_bound,\r\n                \"The absolute error bound used when comparing the test and \"\r\n                \"reference results.\"),\r\n            tensorflow::Flag(\r\n                \"rel_error_bound\", &opts.rel_error_bound,\r\n                \"The relative error bound used when comparing the test and \"\r\n                \"reference results.\"),\r\n            tensorflow::Flag(\"input_format\", &opts.input_format,\r\n                            \"The format of the input file. Valid values:\\n\"\r\n                            \"  hlo : HLO textual format\\n\"\r\n                            \"  pb : xla::HloProto in binary proto format\\n\"\r\n                            \"  pbtxt : xla::HloProto in text proto format\"),\r\n            tensorflow::Flag(\r\n                \"input_text\", &opts.input_text,\r\n                \"A path to a file containing the HLO module. Can also pass \"\r\n                \"a this as argv[1], but this flag is more explicit.\"),\r\n            tensorflow::Flag(\r\n                \"iterations\", &opts.iterations,\r\n                \"The number of times to run the module. Each iteration will be run \"\r\n                \"with different input data.\"),\r\n            tensorflow::Flag(\r\n                \"print_hlo_ir\", &opts.print_hlo_ir,\r\n                \"decide whether print the hlo ir or not\"),\r\n            tensorflow::Flag(\r\n                \"num_replicas\", &opts.num_replicas,\r\n                \"decide how many replicas to used in data paralleism\"),\r\n            tensorflow::Flag(\r\n                \"dump_to_file\", &opts.dump_to_file,\r\n                \"dump to stdio, default is true\"),\r\n            tensorflow::Flag(\r\n                \"speficy_layout\", &opts.speficy_layout,\r\n                \"decide whether speficy layout\"),\r\n                tensorflow::Flag(\r\n                \"execute_shared\", &opts.execute_shared,\r\n                \"decide whether using executable shared\"),\r\n                tensorflow::Flag(\r\n                \"enable_spmd_pass\", &opts.enable_spmd_pass,\r\n                \"decide whether enable spmd pass\")\r\n                \r\n        };\r\n\r\n        \r\n\r\n        xla::AppendDebugOptionsFlags(&flag_list);\r\n        // The usage string includes the message at the top of the file, the\r\n        // DebugOptions flags and the flags defined above.\r\n        \r\n        bool parse_ok = tensorflow::Flags::Parse(&argc, argv, flag_list);\r\n        tensorflow::port::InitMain(\"\", &argc, &argv);\r\n        if (!parse_ok)\r\n        {\r\n            LOG(QFATAL) << \"cannot parse cmd data\";\r\n        }\r\n\r\n        return opts;\r\n    }\r\n}\r\n\r\nint main(int argc, char **argv)\r\n{\r\n    //tensorflow::port::InitMain(\"\", &argc, &argv);\r\n    PjrtDemoArgs opts = get_args(argc, argv);\r\n\r\n    VLOG(0)<<\"Using input data: \"<< opts.input_text;\r\n    VLOG(0)<<\"Using replicas: \" << opts.num_replicas;\r\n\r\n    if(opts.input_text.length()==0)\r\n        LOG(FATAL) << \"please speficy the input data (hlo text): \";\r\n\r\n    // Load HloModule from file.\r\n    std::string hlo_filename = opts.input_text;\r\n    std::function<void(xla::HloModuleConfig *)> config_modifier_hook =\r\n        [](xla::HloModuleConfig *config) { config->set_seed(42); };\r\n    std::unique_ptr<xla::HloModule> test_module =\r\n        LoadModuleFromFile(hlo_filename, xla::hlo_module_loader_details::Config(),\r\n                           \"txt\", config_modifier_hook)\r\n            .ValueOrDie();\r\n\r\n    int num_devices = opts.num_replicas;\r\n\r\n    //if(opts.enable_spmd_pass)\r\n    //{\r\n        //VLOG(0)<<\"Enable SPMD pass\";\r\n        // Some tests (BackpropFilter convs) set this flag false to test two\r\n        // different paths of the implementation.\r\n        SpmdPartitionerOptions options;\r\n        options.conv_halo_exchange_always_on_lhs = true;\r\n        options.allow_module_signature_change = true;\r\n        options.choose_faster_windowed_einsum_over_mem = false;\r\n\r\n        auto collective_ops_creator =\r\n            GetDefaultCollectiveOpsCreator(num_devices, /*num_replicas=*/opts.num_replicas);\r\n        // Do not use all-gather for pattern-matching purpose, as the partitioner\r\n        // might create reshape/transposes around it.\r\n        collective_ops_creator.create_cross_partition_all_gather = nullptr;\r\n\r\n        // NOTE: RUN SPMD Partitioning if necessary\r\n        HloPassPipeline pass(\"spmd-partitioning\");\r\n        pass.AddPass<HloVerifier>(/*layout_sensitive=*/false,\r\n                                    /*allow_mixed_precision=*/true);\r\n        pass.AddPass<SpmdPartitioner>(num_devices, /*num_replicas=*/opts.num_replicas, options,\r\n                                        collective_ops_creator);\r\n        pass.AddPass<HloVerifier>(/*layout_sensitive=*/false,\r\n                                    /*allow_mixed_precision=*/true);\r\n    //}\r\n\r\n    \r\n    if(opts.speficy_layout)\r\n    {\r\n        VLOG(0)<<\"Speficy layout\";\r\n        // layout_assignment_test.cc: 888\r\n        // layoutassignment\r\n        ComputationLayout computation_layout(\r\n            test_module->entry_computation()->ComputeProgramShape(), false);\r\n        VLOG(0) << \"before: computation layout:\\n\" << computation_layout.ToString();\r\n        computation_layout.SetToDefaultLayoutIfEmpty();\r\n        VLOG(0) << \"After: computation layout:\\n\" << computation_layout.ToString();\r\n        Shape param_shape = ShapeUtil::MakeShape(F32, {2, 2});\r\n        if(computation_layout.mutable_parameter_layout(0)->CopyLayoutFromShape(param_shape)!=Status::OK())\r\n        {\r\n            LOG(FATAL)<<\"Error of assigning computation layout for param 0\";\r\n        }\r\n\r\n        if(computation_layout.mutable_parameter_layout(1)->CopyLayoutFromShape(param_shape)!=Status::OK())\r\n        {\r\n            LOG(FATAL)<<\"Error of assigning computation layout for param 1\";\r\n        }\r\n\r\n        computation_layout.mutable_result_layout()->ResetLayout(\r\n            LayoutUtil::MakeLayout({1, 0}));\r\n\r\n        ChannelLayoutConstraints channel_constraints;\r\n        AssignLayouts(test_module.get(), &computation_layout, &channel_constraints);\r\n\r\n    \r\n        for(int index=1; index < 3; index++)\r\n        {\r\n            VLOG(0)<<\"Channel \"<<index<<\": \" << channel_constraints.IsChannelConstrained(index);\r\n            channel_constraints.ConstrainChannel(index, *param_shape.mutable_layout());\r\n            //channel_constraints.LayoutShapeForChannel(param_shape, index);\r\n            if(channel_constraints.IsChannelConstrained(index))\r\n            {\r\n                VLOG(0)<<\"set up layout: \" << channel_constraints.LayoutForChannel(index).ToString();\r\n            }\r\n        }\r\n    \r\n        \r\n    }\r\n\r\n    VLOG(0)<<\"Before spmd pass: \\n\" << test_module->ToString();\r\n\r\n    if(opts.enable_spmd_pass)\r\n    {\r\n        if (!pass.Run(test_module.get()).status().ok())\r\n        {\r\n            LOG(FATAL) << \"Error of runing hlo pass\";\r\n        }\r\n\r\n        VLOG(0)<<\"After spmd pass: \\n\" << test_module->ToString();\r\n    }\r\n\r\n    const xla::HloModuleProto test_module_proto = test_module->ToProto();\r\n\r\n    // Run it using JAX C++ Runtime (PJRT).\r\n\r\n    // Get a GPU client.\r\n    std::unique_ptr<xla::PjRtClient> client =\r\n        xla::GetGpuClient(/*asynchronous=*/true, xla::GpuAllocatorConfig(),\r\n                          /*distributed_client=*/nullptr, /*node_id=*/0)\r\n            .ValueOrDie();\r\n\r\n    LOG(INFO) << \"Compile the code\";\r\n\r\n    \r\n\r\n    // Compile XlaComputation to PjRtExecutable.\r\n    xla::XlaComputation xla_computation(test_module_proto);\r\n    if(opts.speficy_layout)\r\n    {\r\n        ComputationLayout computation_layout(\r\n            test_module->entry_computation()->ComputeProgramShape(),false);\r\n        VLOG(0) << \"before: computation layout:\\n\" << computation_layout.ToString();\r\n        computation_layout.SetToDefaultLayoutIfEmpty();\r\n        VLOG(0) << \"After: computation layout:\\n\" << computation_layout.ToString();\r\n\r\n\r\n        Shape param_shape = ShapeUtil::MakeShape(F32, {2, 2});\r\n        if(computation_layout.mutable_parameter_layout(0)->CopyLayoutFromShape(param_shape)!=Status::OK())\r\n        {\r\n            LOG(FATAL)<<\"Error of assigning computation layout for param 0\";\r\n        }\r\n\r\n        if(computation_layout.mutable_parameter_layout(1)->CopyLayoutFromShape(param_shape)!=Status::OK())\r\n        {\r\n            LOG(FATAL)<<\"Error of assigning computation layout for param 1\";\r\n        }\r\n\r\n        computation_layout.mutable_result_layout()->ResetLayout(\r\n            LayoutUtil::MakeLayout({1, 0}));\r\n\r\n        ChannelLayoutConstraints channel_constraints;\r\n        AssignLayouts(test_module.get(), &computation_layout, &channel_constraints);\r\n\r\n    \r\n        for(int index=1; index < 3; index++)\r\n        {\r\n            VLOG(0)<<\"Channel \"<<index<<\": \" << channel_constraints.IsChannelConstrained(index);\r\n            channel_constraints.ConstrainChannel(index, *param_shape.mutable_layout());\r\n            //channel_constraints.LayoutShapeForChannel(param_shape, index);\r\n            if(channel_constraints.IsChannelConstrained(index))\r\n            {\r\n                VLOG(0)<<\"set up layout: \" << channel_constraints.LayoutForChannel(index).ToString();\r\n            }\r\n        }\r\n    }\r\n\r\n    xla::CompileOptions compile_options;\r\n    compile_options.executable_build_options.mutable_debug_options()\r\n        ->set_xla_gpu_disable_multi_streaming(false);\r\n    compile_options.executable_build_options.mutable_debug_options()\r\n        ->set_xla_gpu_use_random_streams(true);\r\n    compile_options.executable_build_options.set_num_replicas(opts.num_replicas);//cy\r\n\r\n    //compile_options.executable_build_options.set_run_id(RunId());\r\n    \r\n    DeviceAssignment device_assignment(opts.num_replicas, 1);\r\n    for(int index=0; index<client->addressable_devices().size();index++)\r\n    {\r\n        PjRtDevice* device = client->addressable_devices().at(index);\r\n        \r\n        device_assignment(index, 0) = device->id();\r\n        VLOG(0)<< \"device id: \" << device->id();\r\n        \r\n    }\r\n    compile_options.executable_build_options.set_device_assignment(\r\n            device_assignment);\r\n    \r\n    std::unique_ptr<xla::PjRtExecutable> executable =\r\n        client->Compile(xla_computation, compile_options).ValueOrDie();\r\n\r\n    // Prepare inputs.\r\n    xla::Literal literal_x =\r\n        xla::LiteralUtil::CreateR2<float>({{1.0f, 2.0f}, {3.0f, 4.0f}});\r\n    xla::Literal literal_y =\r\n        xla::LiteralUtil::CreateR2<float>({{1.0f, 1.0f}, {1.0f, 1.0f}});\r\n    \r\n    auto devices_list = client->addressable_devices();\r\n    if(opts.execute_shared)\r\n    {\r\n        VLOG(0)<<\"Run iterations: \"<< opts.iterations;\r\n        for(int iter=0; iter < opts.iterations; iter++)\r\n        {\r\n            VLOG(0)<<\"enable execute_shared\";\r\n            std::vector<std::vector<std::unique_ptr<xla::PjRtBuffer> > > results;\r\n            VLOG(0)<<\"devices_list: \" << devices_list.size();\r\n            //tensorflow::thread::ThreadPool* execution_pool=nullptr;\r\n            tensorflow::thread::ThreadPool pool(tensorflow::Env::Default(), \"replicas\", opts.num_replicas);\r\n            auto a_id = RunId();\r\n            for (int64 index = 0; index < opts.num_replicas; ++index)\r\n            {\r\n                pool.Schedule([&, index] {\r\n                    VLOG(0)<<\"Submit task on device: \"<< index;\r\n                    std::unique_ptr<xla::PjRtBuffer> param_x =\r\n                    client->BufferFromHostLiteral(literal_x, devices_list[index])\r\n                        .ValueOrDie();\r\n                    std::unique_ptr<xla::PjRtBuffer> param_y =\r\n                    client->BufferFromHostLiteral(literal_y, devices_list[index])\r\n                        .ValueOrDie();\r\n\r\n                    // Execute on GPU.\r\n                    xla::ExecuteOptions execute_options;\r\n                    execute_options.context = (const ExecuteContext*)(&a_id);\r\n                    std::vector<std::unique_ptr<xla::PjRtBuffer> >  _results =\r\n                    executable->ExecuteSharded({{param_x.get(), param_y.get()}}, \r\n                                                devices_list[index], execute_options)\r\n                                .ValueOrDie();\r\n                    VLOG(0)<<\"[Done] task executed on device: \"<< index;\r\n                    // Get result.\r\n                std::shared_ptr<xla::Literal> result_literal =\r\n                    _results[0]->ToLiteral().ValueOrDie();\r\n                LOG(INFO) << \"result = \" << *result_literal;\r\n                });\r\n            }\r\n            VLOG(0) << \"Task are executed done from main thread\";\r\n        }\r\n    return 0;\r\n    }\r\n\r\n\r\n    std::unique_ptr<xla::PjRtBuffer> param_x =\r\n        client->BufferFromHostLiteral(literal_x, client->addressable_devices()[0])\r\n            .ValueOrDie();\r\n    std::unique_ptr<xla::PjRtBuffer> param_y =\r\n        client->BufferFromHostLiteral(literal_y, client->addressable_devices()[0])\r\n            .ValueOrDie();\r\n    std::unique_ptr<xla::PjRtBuffer> param_x1 =\r\n        client->BufferFromHostLiteral(literal_x, client->addressable_devices()[1])\r\n            .ValueOrDie();\r\n    std::unique_ptr<xla::PjRtBuffer> param_y1 =\r\n        client->BufferFromHostLiteral(literal_y, client->addressable_devices()[1])\r\n            .ValueOrDie();\r\n    std::unique_ptr<xla::PjRtBuffer> param_x2 =\r\n        client->BufferFromHostLiteral(literal_x, client->addressable_devices()[2])\r\n            .ValueOrDie();\r\n    std::unique_ptr<xla::PjRtBuffer> param_y2 =\r\n        client->BufferFromHostLiteral(literal_y, client->addressable_devices()[2])\r\n            .ValueOrDie();\r\n    std::unique_ptr<xla::PjRtBuffer> param_x3 =\r\n        client->BufferFromHostLiteral(literal_x, client->addressable_devices()[3])\r\n            .ValueOrDie();\r\n    std::unique_ptr<xla::PjRtBuffer> param_y3 =\r\n        client->BufferFromHostLiteral(literal_y, client->addressable_devices()[3])\r\n            .ValueOrDie();\r\n    \r\n    // std::vector<const std::vector<PjRtBuffer*> > tmp_buffers;\r\n    // for(int index=0;index<opts.num_replicas;index++)\r\n    // {\r\n    //     std::unique_ptr<xla::PjRtBuffer> param_x =\r\n    //     client->BufferFromHostLiteral(literal_x, client->addressable_devices()[index])\r\n    //         .ValueOrDie();\r\n    //     std::unique_ptr<xla::PjRtBuffer> param_y =\r\n    //     client->BufferFromHostLiteral(literal_y, client->addressable_devices()[index])\r\n    //         .ValueOrDie();\r\n\r\n    //     const std::vector<PjRtBuffer*> buf = {param_x.get(), param_y.get()};\r\n    //     tmp_buffers.push_back(std::move(buf));\r\n    // }\r\n    // absl::Span<const std::vector<PjRtBuffer*>> input_buffers = absl::MakeSpan(tmp_buffers);\r\n\r\n    // Execute on GPU.\r\n    xla::ExecuteOptions execute_options;\r\n    // One vector<buffer> for each device.\r\n    std::vector<std::vector<std::unique_ptr<xla::PjRtBuffer> > > results =\r\n        executable->Execute({{param_x.get(), param_y.get()}, {param_x1.get(), param_y1.get()}, {param_x2.get(), param_y2.get()}, {param_x3.get(), param_y3.get()}}, execute_options)\r\n            .ValueOrDie();\r\n        //executable->ExecuteSharded({{param_x.get(), param_y.get()}},nullptr, execute_options)\r\n        //    .ValueOrDie();\r\n\r\n\r\n    // Get result.\r\n    std::shared_ptr<xla::Literal> result_literal =\r\n        results[0][0]->ToLiteral().ValueOrDie();\r\n    LOG(INFO) << \"result = \" << *result_literal;\r\n    return 0;\r\n}\r\n```\r\n\r\n**step3: fn_hlo.txt**\r\na text that indicates a computation (a[2x2] dot b [2x2]) with SPMD sharding\r\n```shell\r\nHloModule module\r\n\r\nENTRY %entry (p0: f32[2,2], p1: f32[2,2]) -> f32[2,2] {\r\n  %p0 = f32[2,2]{1,0} parameter(0), parameter_replication={false}, sharding={maximal device=0}\r\n  %p1 = f32[2,2]{1,0} parameter(1), parameter_replication={false}, sharding={maximal device=0}\r\n  ROOT %my_add = f32[2,2]{1,0} dot(f32[2,2]{1,0} %p0, f32[2,2]{1,0} %p1), lhs_contracting_dims={1}, rhs_contracting_dims={0}, sharding={replicated}\r\n}\r\n```\r\n\r\n**step4: compile and run the code**\r\n```bash\r\n./bazel-bin/tensorflow/compiler/xla/tools/pjrt_demo --input_text=PATH-TO-YOUR/fn_hlo.txt --num_replicas=4 --execute_shared --enable_spmd_pass\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nThe full logs are as follows:\r\n\r\n```bash\r\n2021-03-09 11:11:38.345132: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-03-09 11:11:38.349232: I tensorflow/compiler/xla/tools/pjrt_client_main.cc:184] Using input data: tensorflow/compiler/xla/tools/hlo_files/fn_hlo.txt\r\n2021-03-09 11:11:38.349258: I tensorflow/compiler/xla/tools/pjrt_client_main.cc:185] Using replicas: 4\r\n2021-03-09 11:11:38.351253: I tensorflow/compiler/xla/tools/pjrt_client_main.cc:270] Before spmd pass:\r\nHloModule module\r\n\r\nENTRY %entry (p0: f32[2,2], p1: f32[2,2]) -> f32[2,2] {\r\n  %p0 = f32[2,2]{1,0} parameter(0), parameter_replication={false}, sharding={maximal device=0}\r\n  %p1 = f32[2,2]{1,0} parameter(1), parameter_replication={false}, sharding={maximal device=0}\r\n  ROOT %my_add = f32[2,2]{1,0} dot(f32[2,2]{1,0} %p0, f32[2,2]{1,0} %p1), lhs_contracting_dims={1}, rhs_contracting_dims={0}, sharding={replicated}\r\n}\r\n\r\n\r\n2021-03-09 11:11:38.354686: I tensorflow/compiler/xla/tools/pjrt_client_main.cc:279] After spmd pass:\r\nHloModule module\r\n\r\n%add (x: f32[], y: f32[]) -> f32[] {\r\n  %x = f32[] parameter(0)\r\n  %y = f32[] parameter(1)\r\n  ROOT %add = f32[] add(f32[] %x, f32[] %y)\r\n}\r\n\r\n%add.1 (x.1: f32[], y.1: f32[]) -> f32[] {\r\n  %x.1 = f32[] parameter(0)\r\n  %y.1 = f32[] parameter(1)\r\n  ROOT %add.1 = f32[] add(f32[] %x.1, f32[] %y.1)\r\n}\r\n\r\nENTRY %entry_spmd (param: f32[2,2], param.1: f32[2,2]) -> f32[2,2] {\r\n  %partition-id = u32[] partition-id()\r\n  %constant = u32[] constant(0)\r\n  %compare.1 = pred[] compare(u32[] %partition-id, u32[] %constant), direction=EQ\r\n  %broadcast.2 = pred[2,2]{1,0} broadcast(pred[] %compare.1), dimensions={}\r\n  %param = f32[2,2]{1,0} parameter(0), parameter_replication={false}, sharding={maximal device=0}\r\n  %constant.1 = f32[] constant(0)\r\n  %broadcast.3 = f32[2,2]{1,0} broadcast(f32[] %constant.1), dimensions={}\r\n  %select.1 = f32[2,2]{1,0} select(pred[2,2]{1,0} %broadcast.2, f32[2,2]{1,0} %param, f32[2,2]{1,0} %broadcast.3)\r\n  %all-reduce.1 = f32[2,2]{1,0} all-reduce(f32[2,2]{1,0} %select.1), channel_id=2, replica_groups={{0},{1},{2},{3}}, to_apply=%add.1\r\n  %param.1 = f32[2,2]{1,0} parameter(1), parameter_replication={false}, sharding={maximal device=0}\r\n  %select = f32[2,2]{1,0} select(pred[2,2]{1,0} %broadcast.2, f32[2,2]{1,0} %param.1, f32[2,2]{1,0} %broadcast.3)\r\n  %all-reduce = f32[2,2]{1,0} all-reduce(f32[2,2]{1,0} %select), channel_id=1, replica_groups={{0},{1},{2},{3}}, to_apply=%add\r\n  ROOT %dot = f32[2,2]{1,0} dot(f32[2,2]{1,0} %all-reduce.1, f32[2,2]{1,0} %all-reduce), lhs_contracting_dims={1}, rhs_contracting_dims={0}\r\n}\r\n\r\n\r\n2021-03-09 11:11:38.355791: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-03-09 11:11:39.274557: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x55b7a4c08a00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2021-03-09 11:11:39.274596: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0\r\n2021-03-09 11:11:39.274607: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (1): Tesla V100-SXM2-32GB, Compute Capability 7.0\r\n2021-03-09 11:11:39.274615: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (2): Tesla V100-SXM2-32GB, Compute Capability 7.0\r\n2021-03-09 11:11:39.274622: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (3): Tesla V100-SXM2-32GB, Compute Capability 7.0\r\n2021-03-09 11:11:39.276501: I tensorflow/compiler/xla/pjrt/gpu_device.cc:125] XLA backend allocating 30063919104 bytes on device 0 for BFCAllocator.\r\n2021-03-09 11:11:39.276640: I tensorflow/compiler/xla/pjrt/gpu_device.cc:125] XLA backend allocating 30063919104 bytes on device 1 for BFCAllocator.\r\n2021-03-09 11:11:39.276756: I tensorflow/compiler/xla/pjrt/gpu_device.cc:125] XLA backend allocating 30063919104 bytes on device 2 for BFCAllocator.\r\n2021-03-09 11:11:39.276865: I tensorflow/compiler/xla/pjrt/gpu_device.cc:125] XLA backend allocating 29774197555 bytes on device 3 for BFCAllocator.\r\n2021-03-09 11:11:39.283129: I tensorflow/compiler/xla/tools/pjrt_client_main.cc:292] Compile the code\r\n2021-03-09 11:11:39.283303: I tensorflow/compiler/xla/tools/pjrt_client_main.cc:352] device id: 0\r\n2021-03-09 11:11:39.283317: I tensorflow/compiler/xla/tools/pjrt_client_main.cc:352] device id: 1\r\n2021-03-09 11:11:39.283325: I tensorflow/compiler/xla/tools/pjrt_client_main.cc:352] device id: 2\r\n2021-03-09 11:11:39.283332: I tensorflow/compiler/xla/tools/pjrt_client_main.cc:352] device id: 3\r\n2021-03-09 11:11:39.747483: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-03-09 11:11:40.299515: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-03-09 11:11:40.485692: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Unimplemented: Requested AllReduce not implemented on GPU; replica_count: 4; partition_count: 1, group_mode: kCrossReplicaAndPartition, operand_count: 2; NCCL support: 1; first operand array element-type: F32\r\nfish: \u201c./bazel-bin/tensorflow/compiler\u2026\u201d terminated by signal SIGABRT (Abort)\r\n```", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47666\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47666\">No</a>\n"]}, {"number": 47665, "title": "`ImportError: cannot import name 'Deconvolution3D' from 'keras.layers' (/home/mona/venv/fall/lib/python3.8/site-packages/keras/layers/__init__.py)`", "body": "How should I fix the following error?\r\n`ImportError: cannot import name 'Deconvolution3D' from 'keras.layers' (/home/mona/venv/fall/lib/python3.8/site-packages/keras/layers/__init__.py)`\r\n\r\nI have:\r\n```\r\n\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-14-8ac95d90a6a0> in <module>\r\n      4 import config\r\n      5 from trainer.fusiondiffroigan import Params,Fusion_Diff_ROI_3DCAE_GAN3D\r\n----> 6 from models import diff_ROI_C3D_AE_no_pool\r\n\r\n~/research/code/GAN-fall/Fall-detection/mrfd/models.py in <module>\r\n      5 from keras.layers import Activation, Dropout, Flatten, Dense, Input, Reshape, BatchNormalization\r\n      6 # from keras.layers import Conv3DTranspose as Deconvolution3D\r\n----> 7 from keras.layers import Deconvolution3D\r\n      8 from keras.optimizers import SGD\r\n      9 from keras import regularizers\r\n\r\nImportError: cannot import name 'Deconvolution3D' from 'keras.layers' (/home/mona/venv/fall/lib/python3.8/site-packages/keras/layers/__init__.py)\r\n```\r\n\r\nalso:\r\n\r\n![Screenshot from 2021-03-08 22-22-41](https://user-images.githubusercontent.com/76495162/110413941-dd1ddb00-805c-11eb-8eb7-497ce7f3a0f1.png)\r\nand\r\n\r\n```\r\nPython 3.8.5 (default, Jan 27 2021, 15:41:15) \r\n[GCC 9.3.0] on Linux\r\n\r\n$ lsb_release -a\r\nLSB Version:\tcore-11.1.0ubuntu2-noarch:security-11.1.0ubuntu2-noarch\r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 20.04.2 LTS\r\nRelease:\t20.04\r\nCodename:\tfocal\r\n\r\n\r\n\r\n```", "comments": ["@monacv \r\nAFAIK there is no direct `Deconvolution3D` layers, there is an alias for the same [here](https://github.com/tensorflow/tensorflow/blob/8a472aa665eb67bdadd5311cec12858f4e9b1345/tensorflow/python/keras/layers/convolutional.py#L3416). Can you try uncommenting line 6 to\r\n\r\n```\r\n~/research/code/GAN-fall/Fall-detection/mrfd/models.py \r\n      5 from keras.layers import Activation, Dropout, Flatten, Dense, Input, Reshape, BatchNormalization\r\n      6 from keras.layers import Conv3DTranspose as Deconvolution3D\r\n      7 # from keras.layers import Deconvolution3D\r\n      8 from keras.optimizers import SGD\r\n      9 from keras import regularizers\r\n```", "@monacv,\r\nAs suggested by @AdityaKane2001, I was able to import `Conv3DTranspose` as `Deconvolution3D`. \r\n\r\nPlease find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/76474fa0478bde565287b48e48accc81/47665.ipynb). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47665\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47665\">No</a>\n"]}, {"number": 47664, "title": "[tf.data] Add SkipInternal for flat_map and interleave", "body": "This is a PR from JIZHI Team & TaiJi AI platform in Tencent.\r\n\r\nThis is a follow up of #46358. I added the `SkipInternal` for `flat_map` and `interleave` so that user could utilize the speed up of `TFRecordDataset` with `tf.data.TFRecordDataset`.\r\n\r\nAlso, I hope we could discuss about how to deal with the `SkipInternal` for `ParallelInterleaveDataset`, as the output of it could be nondeterministic.\r\n\r\nThank you for your time on reviewing this PR :).\r\n\r\ngently ping @aaudiber ", "comments": ["@aaudiber \r\nI've updated the code based on the reiview, could you take another look? Thank you~", "The build errors seems to be related to mlir instead of this pr.", "@zhuzilin The change seems to be creating flaky failures in AutoShardDatasetTest.testDatasetOfReaderDatasetsPipeline, ptal\r\n\r\n```\r\n  File \"/build/work/086335f9cfbc4a1e080b442a48a951e93a9d/google3/runfiles/google3/third_party/tensorflow/python/data/experimental/kernel_tests/auto_shard_dataset_test.py\", line 142, in testDatasetOfReaderDatasetsPipeline\r\n    self.assertDatasetProduces(dataset, expected)\r\n  File \"/build/work/086335f9cfbc4a1e080b442a48a951e93a9d/google3/runfiles/google3/third_party/tensorflow/python/data/kernel_tests/test_base.py\", line 225, in assertDatasetProduces\r\n    self._compareOutputToExpected(result, expected_output, assert_items_equal)\r\n  File \"/build/work/086335f9cfbc4a1e080b442a48a951e93a9d/google3/runfiles/google3/third_party/tensorflow/python/data/kernel_tests/test_base.py\", line 150, in _compareOutputToExpected\r\n    self.assertValuesEqual(expected_value, result_value)\r\n  File \"/build/work/086335f9cfbc4a1e080b442a48a951e93a9d/google3/runfiles/google3/third_party/tensorflow/python/data/kernel_tests/test_base.py\", line 91, in assertValuesEqual\r\n    self.assertAllEqual(expected, actual)\r\n  File \"/build/work/086335f9cfbc4a1e080b442a48a951e93a9d/google3/runfiles/google3/third_party/tensorflow/python/framework/test_util.py\", line 1246, in decorated\r\n    return f(*args, **kwds)\r\n  File \"/build/work/086335f9cfbc4a1e080b442a48a951e93a9d/google3/runfiles/google3/third_party/tensorflow/python/framework/test_util.py\", line 2871, in assertAllEqual\r\n    np.testing.assert_array_equal(a, b, err_msg=\"\\n\".join(msgs))\r\n  File \"/build/work/086335f9cfbc4a1e080b442a48a951e93a9d/google3/runfiles/google3/third_party/py/numpy/testing/_private/utils.py\", line 904, in assert_array_equal\r\n    verbose=verbose, header='Arrays are not equal')\r\n  File \"/build/work/086335f9cfbc4a1e080b442a48a951e93a9d/google3/runfiles/google3/third_party/py/numpy/testing/_private/utils.py\", line 827, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nArrays are not equal\r\n\r\nnot equal lhs = array(b'Record 0 of file 5', dtype='|S18')\r\nnot equal rhs = array(b'Record 0 of file 6', dtype='|S18')\r\nMismatch: 100%\r\n x: array(b'Record 0 of file 5', dtype='|S18')\r\n y: array(b'Record 0 of file 6', dtype='|S18')\r\n```\r\n\r\n", "@aaudiber Thank you for the information~ I've tried to run the auto shard test locally, and they passed... Do you have any thoughts on why the failure happens?", "@aaudiber  Can you please assist on above comments from @zhuzilin. Thanks!", "@zhuzilin I restructured the `flat_map` skip implementation and the error went away \ud83d\udc4d ", "@aaudiber Great! Thanks a lot!"]}]