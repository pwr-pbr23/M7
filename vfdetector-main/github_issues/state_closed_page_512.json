[{"number": 38388, "title": "TFLite New Converter with 1.X frozen graph", "body": "I tried to covert 1.X frozen graph with TFLite New Converter following this guidelines(https://www.tensorflow.org/guide/migrate#a_graphpb_or_graphpbtxt).\r\n\r\nHere is my full python script.\r\n```python\r\ngraph_def = tf.compat.v1.GraphDef()\r\ngraph_def.ParseFromString(open(flags.input_path, 'rb').read())\r\n\r\nwrap_func = wrap_frozen_graph(\r\n    graph_def,\r\n    inputs=[_str + \":0\" for _str in _parse_array(flags.input_arrays)],\r\n    outputs=[_str + \":0\" for _str in _parse_array(flags.output_arrays)])\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([wrap_func])\r\n```\r\nIt works well. But, is there any method to set input shapes from users? It's very hard to find this:(\r\n\r\nlike `tf.compat.v1.lite.TFLiteConverter.from_frozen_graph` method in 1.X version.\r\n```python\r\ndef from_frozen_graph(cls,\r\n                        graph_def_file,\r\n                        input_arrays,\r\n                        output_arrays,\r\n                        input_shapes=None)\r\n```\r\n\r\nI found something similar like\r\n```python\r\n# Set the correct data type and shape; shape can be (None, 224, 224, 3) also\r\nnew_placeholder = tf.placeholder(tf.float32, shape=(1, 224, 224, 3), name='inputs_new_name') \r\n# here you need to state the name of the placeholder you used in your original input placeholder  \r\n\r\nsaver = tf.import_graph_def(path/to/.meta, input_map={\"original_inputs_placeholder_name:0\": new_placeholder})\r\n```\r\nBut, this is very inconvenient to use.\r\n\r\nI'm looking for similar features in 2.X converter, but I haven't found any:(\r\n", "comments": ["@mhs4670go Yes. there is an option to change size during inference. Please check this [comment](https://github.com/tensorflow/tensorflow/issues/29590#issuecomment-580951882). Please post a standalone code to reproduce your issue. Thanks!", "@jvishnuvardhan Thank you for your comment. Changing size during inference might be enough, But, actually, I want to know if I can convert `.pb` file with None shape to `.tflite` file with user specified shape not None shape like v1 converter as I said above.\r\n```python\r\n# This is v1 converter having `input_shape` option\r\ninput_shapes = None\r\n  if flags.input_shapes: # By this line, `pb` with None shape can be converted to `.tflite` with known shape. \r\n                                     # Does v2 converter have similar method or sth? \r\n    input_shapes_list = [\r\n        _parse_array(shape, type_fn=int)\r\n        for shape in six.ensure_str(flags.input_shapes).split(\":\")\r\n    ]\r\n    input_shapes = dict(list(zip(input_arrays, input_shapes_list)))\r\n```", "The link you are referring to discusses how a frozen graph (in TensorFlow 1 model format) can be converted to a TensorFlow 2 model: https://www.tensorflow.org/guide/migrate#a_graphpb_or_graphpbtxt. This is **not required**.\r\n\r\nTo convert the frozen graph model to a TensorFlow Lite model:\r\n1. If you are using TensorFlow 1, the API is `tf.lite.TFLiteConverter.from_frozen_graph`\r\n2. If you are using TensorFlow 2, the API is `tf.compat.v1.lite.TFLiteConverter.from_frozen_graph`\r\n\r\n``` \r\n# TensorFlow Version 2.2\r\nimport tensorflow as tf\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(\r\n    graph_def_file='....path/to/frozen_graph.pb', \r\n    input_arrays=...,\r\n    output_arrays=....,\r\n    input_shapes={'...' : [_, _,....]}\r\n)\r\ntflite_model = converter.convert()\r\n\r\ntflite_model_size = open('model.tflite', 'wb').write(tflite_model)\r\nprint('TFLite Model is %d bytes' % tflite_model_size)\r\n\r\n```\r\n\r\nThe `.from_frozen_graph` API can be defined [this](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/lite/TFLiteConverter#from_frozen_graph) way and the attributes which can be added are [here](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/lite/TFLiteConverter#attributes). To find the names of these arrays, visualize the `.pb` file in [Netron](https://lutzroeder.github.io/netron/)\r\n\r\n"]}, {"number": 38387, "title": "[MLIR][XLA] Adding new tests for buffer assignment, and fixing its addDynamicallyLegalFuncOp.", "body": "In this PR, new tests are added for testing [Buffer Assignment](https://github.com/tensorflow/tensorflow/pull/37212) and addDynamicallyLegalFuncOp is fixed.", "comments": ["@dfki-ehna Can you please resolve conflicts? Thanks!", "@dfki-ehna Can you please check @joker-eph's comments and keep us posted. Thanks!", "There are a few places still missing testing coverage apparently:\r\n\r\nLine 194:\r\n```\r\n   if (value.use_empty()) {\r\n      return BufferAssignmentPositions(value.getDefiningOp(),\r\n                                       value.getDefiningOp());\r\n```\r\n\r\nLine 375:\r\n```\r\n      if (!nextOp)\r\n          nextOp = &positions.getDeallocPosition()->getBlock()->back();\r\n```\r\n\r\nLine 404:\r\n```\r\n   auto domNode = dominators.getNode(arg.getOwner());\r\n    assert(domNode != nullptr && \"Cannot find dominator info\");\r\n    auto idomNode = domNode->getIDom();\r\n    assert(idomNode != nullptr && \"There is no parent dominator\");\r\n    insertOp = idomNode->getBlock()->getTerminator();\r\n```\r\n", "The mentioned lines are covered now. There is no need for code block in line 404 anymore since it's not required to allocate a buffer for a block argument in the latest version. Line 375 has been changed to an assert for a corner case that is considered to be illegal in the scope of IR. Line 194 is covered with an extra test case."]}, {"number": 38386, "title": "Distributed training causes cuda OOM", "body": "**System information** \r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: CentOS \r\n- TensorFlow installed from:\r\n`pip3 install tensorflow-gpu==2.0.0` \r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.0/7.6\r\n\r\n**Describe the current behavior**\r\nWhen starting the worker using v1 distirbuted training, we could observe the following error:\r\n```bash\r\nWARNING:tensorflow:From /usr/local/lib64/python3.6/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\nW0409 18:55:45.554535 140176829970240 deprecation.py:323] From /usr/local/lib64/python3.6/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\n2020-04-09 18:55:45.563568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: Tesla P40 major: 6 minor: 1 memoryClockRate(GHz): 1.531\r\npciBusID: 0000:0d:00.0\r\n2020-04-09 18:55:45.563627: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-04-09 18:55:45.563644: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-04-09 18:55:45.563658: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2020-04-09 18:55:45.563671: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2020-04-09 18:55:45.563687: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-04-09 18:55:45.563702: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-04-09 18:55:45.563716: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-09 18:55:45.568025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\nINFO:tensorflow:Graph was finalized.\r\nI0409 18:55:45.699084 140176829970240 monitored_session.py:240] Graph was finalized.\r\n2020-04-09 18:55:47.161048: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 22.40G (24056830208 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-04-09 18:55:47.162735: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 20.16G (21651146752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-04-09 18:55:47.164113: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 18.15G (19486031872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-04-09 18:55:47.165473: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 16.33G (17537427456 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-04-09 18:55:47.166829: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 14.70G (15783684096 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-04-09 18:55:47.168178: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 13.23G (14205315072 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-04-09 18:55:47.169530: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 11.91G (12784783360 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-04-09 18:55:47.170874: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 10.72G (11506305024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-04-09 18:55:47.172225: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 9.64G (10355674112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-04-09 18:55:47.173575: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 8.68G (9320105984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-04-09 18:55:47.174945: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 7.81G (8388094976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-04-09 18:55:47.176291: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 7.03G (7549285376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-04-09 18:55:47.177639: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 6.33G (6794356736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-04-09 18:55:47.178980: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 5.69G (6114920960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-04-09 18:55:47.180325: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 5.12G (5503428608 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-04-09 18:55:47.181714: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 4.61G (4953085440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-04-09 18:55:47.183053: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 4.15G (4457776640 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-04-09 18:55:47.184402: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 3.74G (4011998976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-04-09 18:55:47.185768: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 3.36G (3610799104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-04-09 18:55:47.187108: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 3.03G (3249719040 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-04-09 18:55:47.188455: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 2.72G (2924747008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-04-09 18:55:47.189805: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 2.45G (2632272128 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-04-09 18:55:47.191145: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 2.21G (2369044736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-04-09 18:55:47.192492: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 1.99G (2132140288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-04-09 18:55:47.193864: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 1.79G (1918926336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-04-09 18:55:47.195203: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 1.61G (1727033600 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-04-09 18:55:47.196724: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 1.45G (1554330368 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-04-09 18:55:47.198186: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 1.30G (1398897408 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-04-09 18:55:47.199649: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 1.17G (1259007744 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-04-09 18:55:47.201138: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 1.05G (1133106944 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-04-09 18:55:47.202527: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 972.55M (1019796224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\nINFO:tensorflow:Running local_init_op.\r\nI0409 18:55:47.254083 140176829970240 session_manager.py:500] Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nI0409 18:55:47.262292 140176829970240 session_manager.py:502] Done running local_init_op.\r\n2020-04-09 18:55:51.072444: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-04-09 18:55:51.096914: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2020-04-09 18:55:51.110052: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2020-04-09 18:55:51.122872: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2020-04-09 18:55:51.135759: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2020-04-09 18:55:51.148557: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2020-04-09 18:55:51.161419: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2020-04-09 18:55:51.174027: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2020-04-09 18:55:51.187117: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2020-04-09 18:55:51.262033: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2020-04-09 18:55:51.262068: W tensorflow/stream_executor/stream.cc:1919] attempting to perform BLAS operation using StreamExecutor without BLAS support\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1365, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\r\n    target_list, run_metadata)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InternalError: From /job:worker/replica:0/task:0:\r\nBlas GEMM launch failed : a.shape=(32, 784), b.shape=(784, 500), m=32, n=500, k=784\r\n         [[{{node dense/MatMul}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"distributed_mnist.py\", line 123, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python3.6/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"distributed_mnist.py\", line 116, in main\r\n    _, ls, step = mon_sess.run([train_op, loss, global_step])#,\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py\", line 756, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py\", line 1261, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py\", line 1362, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/usr/local/lib/python3.6/site-packages/six.py\", line 703, in reraise\r\n    raise value\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py\", line 1347, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py\", line 1420, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py\", line 1178, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 956, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1180, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1359, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1384, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: From /job:worker/replica:0/task:0:\r\nBlas GEMM launch failed : a.shape=(32, 784), b.shape=(784, 500), m=32, n=500, k=784\r\n         [[node dense/MatMul (defined at /usr/local/lib64/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]\r\n\r\nOriginal stack trace for 'dense/MatMul':\r\n  File \"distributed_mnist.py\", line 123, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python3.6/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"distributed_mnist.py\", line 82, in main\r\n    logits = model(images)\r\n  File \"distributed_mnist.py\", line 28, in model\r\n    net = tf.layers.dense(images, 500, activation=tf.nn.relu)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/util/deprecation.py\", line 324, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/layers/core.py\", line 187, in dense\r\n    return layer.apply(inputs)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/util/deprecation.py\", line 324, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 1695, in apply\r\n    return self.__call__(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/layers/base.py\", line 548, in __call__\r\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 847, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 234, in wrapper\r\n    return converted_call(f, options, args, kwargs)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 439, in converted_call\r\n    return _call_unconverted(f, args, kwargs, options)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 330, in _call_unconverted\r\n    return f(*args, **kwargs)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/layers/core.py\", line 1056, in call\r\n    outputs = gen_math_ops.mat_mul(inputs, self.kernel)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/gen_math_ops.py\", line 6136, in mat_mul\r\n    name=name)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 793, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 3360, in create_op\r\n    attrs, op_def, compute_device)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 3429, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 1751, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n```\r\nWe've tried with different task number but observed the same problem.\r\n\r\n**Standalone code to reproduce the issue** \r\n```python\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_eager_execution()\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\ntf.app.flags.DEFINE_string(\"ps_hosts\", \"localhost:2222\", \"ps hosts\")\r\ntf.app.flags.DEFINE_string(\"worker_hosts\", \"localhost:2223,localhost:2224\", \"worker hosts\")\r\ntf.app.flags.DEFINE_string(\"job_name\", \"worker\", \"'ps' or'worker'\")\r\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\r\ntf.app.flags.DEFINE_integer(\"num_workers\", 2, \"Number of workers\")\r\ntf.app.flags.DEFINE_boolean(\"is_sync\", False, \"using synchronous training or not\")\r\n\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\ndef model(images):\r\n    \"\"\"Define a simple mnist classifier\"\"\"\r\n    net = tf.layers.dense(images, 500, activation=tf.nn.relu)\r\n    net = tf.layers.dense(net, 500, activation=tf.nn.relu)\r\n    net = tf.layers.dense(net, 10, activation=None)\r\n    return net\r\n\r\ndef main(_):\r\n    ps_hosts = FLAGS.ps_hosts.split(\",\")\r\n    worker_hosts = FLAGS.worker_hosts.split(\",\")\r\n\r\n    # create the cluster configured by `ps_hosts' and 'worker_hosts'\r\n    cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\r\n\r\n    # create a server for local task\r\n    server = tf.train.Server(cluster, job_name=FLAGS.job_name,\r\n                             task_index=FLAGS.task_index)\r\n\r\n    if FLAGS.job_name == \"ps\":\r\n        server.join()  # ps hosts only join\r\n    elif FLAGS.job_name == \"worker\":\r\n        # workers perform the operation\r\n        # ps_strategy = tf.contrib.training.GreedyLoadBalancingStrategy(FLAGS.num_ps)\r\n\r\n        # Note: tf.train.replica_device_setter automatically place the paramters (Variables)\r\n        # on the ps hosts (default placement strategy:  round-robin over all ps hosts, and also\r\n        # place multi copies of operations to each worker host\r\n        with tf.device(tf.train.replica_device_setter(worker_device=\"/job:worker/task:%d\" % (FLAGS.task_index),\r\n                                                      cluster=cluster)):\r\n            # load mnist dataset\r\n            (x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n            x_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\n            x_train = x_train.reshape(x_train.shape[0], -1)\r\n            x_test = x_test.reshape(x_test.shape[0], -1)\r\n\r\n            train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)\r\n            train_iter = train_ds.make_initializable_iterator()\r\n            train_init = train_iter.make_initializer(train_ds)\r\n            test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\r\n\r\n            print(x_train.shape)\r\n            print(y_train.shape)\r\n\r\n            # the model\r\n            #images = tf.placeholder(tf.float32, [None, 784])\r\n            #labels = tf.placeholder(tf.int32, [None, 10])\r\n            xs, ys = train_iter.get_next()\r\n            xs = tf.cast(xs, dtype=tf.float32)\r\n            ys = tf.cast(ys, dtype=tf.int32)\r\n            images = xs\r\n            labels = tf.one_hot(ys, 10)\r\n\r\n            logits = model(images)\r\n            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\r\n\r\n            # The StopAtStepHook handles stopping after running given steps.\r\n            hooks = [tf.train.StopAtStepHook(last_step=2000)]\r\n\r\n            global_step = tf.train.get_or_create_global_step()\r\n            optimizer = tf.train.AdamOptimizer(learning_rate=1e-04)\r\n\r\n            if FLAGS.is_sync:\r\n                # synchronous training\r\n                # use tf.train.SyncReplicasOptimizer wrap optimizer\r\n                # ref: https://www.tensorflow.org/api_docs/python/tf/train/SyncReplicasOptimizer\r\n                optimizer = tf.train.SyncReplicasOptimizer(optimizer, replicas_to_aggregate=FLAGS.num_workers,\r\n                                                       total_num_replicas=FLAGS.num_workers)\r\n                # create the hook which handles initialization and queues\r\n                hooks.append(optimizer.make_session_run_hook((FLAGS.task_index==0)))\r\n\r\n            train_op = optimizer.minimize(loss, global_step=global_step,\r\n                                          aggregation_method=tf.AggregationMethod.ADD_N)\r\n\r\n            # The MonitoredTrainingSession takes care of session initialization,\r\n            # restoring from a checkpoint, saving to a checkpoint, and closing when done\r\n            # or an error occurs.\r\n            with tf.train.MonitoredTrainingSession(master=server.target,\r\n                                                   is_chief=(FLAGS.task_index == 0),\r\n                                                   #checkpoint_dir=\"./checkpoint_dir\",\r\n                                                   hooks=hooks) as mon_sess:\r\n                mon_sess.run(train_init)\r\n                while not mon_sess.should_stop():\r\n                    # mon_sess.run handles AbortedError in case of preempted PS.\r\n                    _, ls, step = mon_sess.run([train_op, loss, global_step])\r\n                    if step % 100 == 0:\r\n                        print(\"Train step %d, loss: %f\" % (step, ls))\r\n\r\nif __name__ == \"__main__\":\r\n    tf.app.run()\r\n```\r\nThe bash used to run this code (this is a one worker one ps example, we've varied the number of tasks and observed the similar behavior.)\r\n```bash\r\npython3 distributed_mnist.py --ps_hosts=localhost:2222 --worker_hosts=localhost:2224 --job_name=ps --task_index=0 --num_worker=1\r\npython3 distributed_mnist.py --ps_hosts=localhost:2222 --worker_hosts=localhost:2224 --job_name=worker --task_index=0 --num_worker=1\r\n```\r\n", "comments": ["You may try [limiting gpu memory growth](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) in this case.\r\nPut following snippet on top of your code;\r\n```python\r\nimport tensorflow as tf\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(gpus[0], True)\r\n# your code\r\n```", "> You may try [limiting gpu memory growth](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) in this case.\r\n> Put following snippet on top of your code;\r\n> \r\n> ```python\r\n> import tensorflow as tf\r\n> gpus = tf.config.experimental.list_physical_devices('GPU')\r\n> tf.config.experimental.set_memory_growth(gpus[0], True)\r\n> # your code\r\n> ```\r\n\r\n@ymodak I tried with this snippet, but same error occured. I've also tried `allow_soft_placement` and it didn't work either.", "Apologies for the delay in response. Is this still an issue? Did you try killing all python processes/ exit interpreter and try again? Thanks!", "No... The problem remains and our team has to swtich to `tf.distribute.Strategy`.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38386\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38386\">No</a>\n"]}, {"number": 38385, "title": "docker image to build manylinux, ubuntu16", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n\r\n**Describe the problem**\r\nI am building tensorflow pip wheel from source and I would like to build it for Ubuntu 16.04.\r\nI found the docker image with ubuntu16 and installed auditwheel that looks like a fit for my purpose:\r\ntensorflow:custom-op-ubuntu16,\r\nbut it is very heavy - 6Gb.\r\n\r\nCould you please recommend which tensorflow docker image should I use to build manylinux wheel ?\r\nIs there an easy way to build tensorflow pip package for Ubuntu 16.04 from source ? \r\n\r\n\r\n", "comments": ["@wwwind \r\nplease share the tensorflow version", "@Saduf2019 nightly builds, i want to build from the current source in tensorflow", "You can use the `devel` image tags. See https://hub.docker.com/r/tensorflow/tensorflow/ for details.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38385\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38385\">No</a>\n"]}, {"number": 38384, "title": "Cannot save to SavedModel when custom layers accepts args and kwargs", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes and no\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or\r\nbinary): pip\r\n- TensorFlow version (use command below): 2.2.0rc2\r\n- Python version: 3.7.3\r\n\r\n**Describe the current behavior**\r\nI was having trouble with saving a custom tf.keras model to the SavedModel format, while saving to .h5 would go fine. The model is a custom ResNet model, subclassing tf.keras.Model.\r\nhttps://stackoverflow.com/a/55386355/11470044 and https://www.tensorflow.org/guide/keras/save_and_serialize#part_ii_saving_and_loading_of_subclassed_models have led me to believe that it should now be possible to save such custom imperative models.\r\nAfter some digging around, I've found the issue to be the following: A custom keras layer's `call()` method may not be defined like `def call(self, *args, **kwargs)`, but should strictly adhere to `def call(self, inputs, **kwargs)`.\r\n\r\nI can show with the following [gist](https://gist.github.com/kriskorrel-cw/c90e349fddec1c2a20a031660431f80a), which is almost entirely the same as in https://www.tensorflow.org/guide/keras/save_and_serialize#part_ii_saving_and_loading_of_subclassed_models \r\nUsing `BatchNormalization` or `BatchNormalization1`, all goes well. But when using `BatchNormalization2`, we get the following stack trace:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\", line 1008, in save\r\n    signatures, options)\r\n  File \"/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\", line 115, in save_model\r\n    signatures, options)\r\n  File \"/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save.py\", line 78, in save\r\n    save_lib.save(model, filepath, signatures, options)\r\n  File \"/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py\", line 886, in save\r\n    checkpoint_graph_view)\r\n  File \"/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/saved_model/signature_serialization.py\", line 74, in find_function_to_export\r\n    functions = saveable_view.list_functions(saveable_view.root)\r\n  File \"/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py\", line 142, in list_functions\r\n    self._serialization_cache)\r\n  File \"/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 2420, in _list_functions_for_serialization\r\n    .list_functions_for_serialization(serialization_cache))\r\n  File \"/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/base_serialization.py\", line 91, in list_functions_for_serialization\r\n    fns = self.functions_to_serialize(serialization_cache)\r\n  File \"/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/layer_serialization.py\", line 80, in functions_to_serialize\r\n    serialization_cache).functions_to_serialize)\r\n  File \"/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/layer_serialization.py\", line 95, in _get_serialized_attributes\r\n    serialization_cache)\r\n  File \"/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/model_serialization.py\", line 53, in _get_serialized_attributes_internal\r\n    serialization_cache))\r\n  File \"/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/layer_serialization.py\", line 104, in _get_serialized_attributes_internal\r\n    functions = save_impl.wrap_layer_functions(self.obj, serialization_cache)\r\n  File \"/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py\", line 162, in wrap_layer_functions\r\n    original_fns = _replace_child_layer_functions(layer, serialization_cache)\r\n  File \"/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py\", line 258, in _replace_child_layer_functions\r\n    serialization_cache).functions)\r\n  File \"/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/layer_serialization.py\", line 95, in _get_serialized_attributes\r\n    serialization_cache)\r\n  File \"/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/layer_serialization.py\", line 104, in _get_serialized_attributes_internal\r\n    functions = save_impl.wrap_layer_functions(self.obj, serialization_cache)\r\n  File \"/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py\", line 172, in wrap_layer_functions\r\n    '{}_layer_call_and_return_conditional_losses'.format(layer.name))\r\n  File \"/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py\", line 513, in add_function\r\n    self.add_trace(*self._input_signature)\r\n  File \"/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py\", line 428, in add_trace\r\n    trace_with_training(True)\r\n  File \"/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py\", line 426, in trace_with_training\r\n    fn.get_concrete_function(*args, **kwargs)\r\n  File \"/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py\", line 557, in get_concrete_function\r\n    return super(LayerCall, self).get_concrete_function(*args, **kwargs)\r\n  File \"/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 909, in get_concrete_function\r\n    self._initialize(args, kwargs, add_initializers_to=initializers)\r\n  File \"/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 497, in _initialize\r\n    *args, **kwds))\r\n  File \"/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2389, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py\", line 523, in wrapper\r\n    inputs = call_collection.get_input_arg_value(args, kwargs)\r\n  File \"/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py\", line 463, in get_input_arg_value\r\n    self._input_arg_name, args, kwargs, inputs_in_args=True)\r\n  File \"/home/kriskorrel/.local/share/virtualenvs/blicker-1BQtdZDZ/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 1991, in _get_call_arg_value\r\n    return args_dict[arg_name]\r\nKeyError: 'inputs'\r\n```\r\n\r\nAlthough this is somewhat easy to debug in the toy example, you could imagine that such an exception is hard to find in a large codebase.\r\n\r\n**Describe the expected behavior**\r\nI therefore think that either (a) using the `*args, **kwargs` signature should be supported, (b) a warning/exception should be raised whenever a `Model` is defined or compiled with incorrect signatures, or (c) the error message above must be improved such that a similar issue can be tracked down more easily.\r\n\r\n**Standalone code to reproduce the issue** \r\n[gist](https://gist.github.com/kriskorrel-cw/c90e349fddec1c2a20a031660431f80a)\r\n\r\n**Other info / logs**\r\nP.S. I first tried to reproduce the issue by subclassing `Dense` instead of `BatchNormalization`, but this does not work since `Dense.call()` only accepts `inputs`. It does not support kwargs (like 'training'). I also find this inconsistency between different Layers weird, but I'm sure there's a reason for it.\r\n", "comments": ["I have tried on colab with TF version 2.2 and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/1b729d38569460d39c110b9bf86b6324/untitled30.ipynb).Using **BatchNormalization1,** all goes well. But when using **BatchNormalization2** we are facing the issue. Thanks!", "Was able to run your code without any errors in Tf Nightly 2.6.0-dev20210525, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/47f9a19be3bd9e26e056d52486af8863/38384.ipynb). Thanks!", "Closing the issue since issue has been fixed in latest version, feel free to reopen the issue in case of any concerns. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38384\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38384\">No</a>\n"]}, {"number": 38383, "title": "TFX on Taxi_pipelines_interactive", "body": "model_resolver = ResolverNode(\r\n      instance_name='latest_blessed_model_resolver',\r\n      resolver_class=latest_blessed_model_resolver.LatestBlessedModelResolver,\r\n      model=Channel(type=Model),\r\n      model_blessing=Channel(type=ModelBlessing))\r\ncontext.run(model_resolver)\r\n\r\nevaluator = Evaluator(\r\n    examples=example_gen.outputs['examples'],\r\n    model=trainer.outputs['model'],\r\n    #baseline_model=model_resolver.outputs['model'],\r\n    # Change threshold will be ignored if there is no baseline (first run).\r\n    eval_config=eval_config)\r\ncontext.run(evaluator)\r\n\r\nINFO:absl:Running driver for ResolverNode.latest_blessed_model_resolver\r\nINFO:absl:MetadataStore with DB connection initialized\r\nWARNING:absl:Artifact type ModelBlessing not registered\r\nINFO:absl:Running publisher for ResolverNode.latest_blessed_model_resolver\r\nINFO:absl:MetadataStore with DB connection initialized\r\nINFO:absl:Running driver for Evaluator\r\nINFO:absl:MetadataStore with DB connection initialized\r\nINFO:absl:Running executor for Evaluator\r\n---------------------------------------------------------------------------\r\nUnboundLocalError                         Traceback (most recent call last)\r\n<ipython-input-44-205b472f5776> in <module>\r\n     19     # Change threshold will be ignored if there is no baseline (first run).\r\n     20     eval_config=eval_config)\r\n---> 21 context.run(evaluator)\r\n\r\n~/taxi_pipeline/lib/python3.6/site-packages/tfx/orchestration/experimental/interactive/interactive_context.py in run_if_ipython(*args, **kwargs)\r\n     64       # __IPYTHON__ variable is set by IPython, see\r\n     65       # https://ipython.org/ipython-doc/rel-0.10.2/html/interactive/reference.html#embedding-ipython.\r\n---> 66       return fn(*args, **kwargs)\r\n     67     else:\r\n     68       absl.logging.warning(\r\n\r\n~/taxi_pipeline/lib/python3.6/site-packages/tfx/orchestration/experimental/interactive/interactive_context.py in run(self, component, enable_cache, beam_pipeline_args)\r\n    166         component, pipeline_info, driver_args, metadata_connection,\r\n    167         beam_pipeline_args, additional_pipeline_args)\r\n--> 168     execution_id = launcher.launch().execution_id\r\n    169 \r\n    170     return execution_result.ExecutionResult(\r\n\r\n~/taxi_pipeline/lib/python3.6/site-packages/tfx/orchestration/launcher/base_component_launcher.py in launch(self)\r\n    203                          execution_decision.input_dict,\r\n    204                          execution_decision.output_dict,\r\n--> 205                          execution_decision.exec_properties)\r\n    206 \r\n    207     absl.logging.info('Running publisher for %s',\r\n\r\n~/taxi_pipeline/lib/python3.6/site-packages/tfx/orchestration/launcher/in_process_component_launcher.py in _run_executor(self, execution_id, input_dict, output_dict, exec_properties)\r\n     65         executor_context)  # type: ignore\r\n     66 \r\n---> 67     executor.Do(input_dict, output_dict, exec_properties)\r\n\r\n~/taxi_pipeline/lib/python3.6/site-packages/tfx/components/evaluator/executor.py in Do(self, input_dict, output_dict, exec_properties)\r\n    161         else:\r\n    162           models[model_spec.name] = _get_eval_saved_model(\r\n--> 163               input_dict[MODEL_KEY], tags)\r\n    164           absl.logging.info('Using {} for model eval.'.format(\r\n    165               models[model_spec.name].model_path))\r\n\r\nUnboundLocalError: local variable 'tags' referenced before assignment\r\n", "comments": ["@pradeepmishra11,\r\n\r\nSince this issue is related to TFX, could you please raise an issue in the TFX repo using [this link](https://github.com/tensorflow/tfx/issues/new). Thanks!", "Any updates regarding this issue? Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 38382, "title": "Google Developers Certification", "body": "Google Developers Certification plugin is not available in PyCharm 2020.1 (it is needed to take the Tensorfow Certification exam):\r\n\r\n![gdc](https://user-images.githubusercontent.com/50177956/78879616-69daa600-7a54-11ea-9ad4-1212c1e289bf.png)\r\n", "comments": ["I faced the same issue. Downgraded PyCharm to pycharm-community-2019.3.4", "Hello,\r\n\r\nThe Google Developers Certification plugin is now available in PyCharm 2020."]}, {"number": 38381, "title": "Casting from float32 to uint8 results in wrong value despite being from 0 to 255", "body": "I'm using tensorflow-gpu 1.14 with Cuda 10 on Ubuntu 18.04. I'm running tf in **eager mode**.\r\n\r\nI'm training a model using Trax. In my input processing pipeline, I read a batch of uint8 images (3-channel each value from 0 to 255) and resize them using `tf.image.resize_images` which results in `float32` but values are still from 0 to 255. Then I cast the `float32` values to `uint8` :\r\n\r\n```\r\n print(\"==================\")\r\n print(\"0,0,0 before cast:  \", image_target_before_cast[0][0][0])\r\n image_target = tf.cast(image_target_before_cast, dtype=tf.dtypes.uint8)\r\n print(\"0,0,0 after cast:  \", image_target[0][0][0])\r\n print(\"==================\")\r\n```\r\n\r\n On first batch of data (16 prints but i'm bringing only one of them) everything is fine:\r\n```\r\n==================\r\n0,0,0 before cast:   tf.Tensor(146.21089, shape=(), dtype=float32)\r\n0,0,0 after cast:   tf.Tensor(146, shape=(), dtype=uint8)\r\n==================\r\n```\r\n\r\nNow here is the weird part. From the second batch of images the problem happens:\r\n```\r\n==================\r\n0,0,0 before cast:   tf.Tensor(139.61038, shape=(), dtype=float32)\r\n0,0,0 after cast:   tf.Tensor(115, shape=(), dtype=uint8)\r\n==================\r\n```\r\n\r\nI also tried to conver the tensor to numpy array and then cast it using ` image_target_before_cast[0][0][0].numpy().astype(np.uint8)` and it worked. But still don't know what is wrong with the tf cast.\r\n\r\nAny idea what might be the issue? \r\n", "comments": ["@py4 \r\nplease provide us with a simple stand alone ode for us to replicate the issue.\r\n\r\nplease refer to this [link](https://stackoverflow.com/questions/46689428/convert-np-array-of-type-float64-to-type-uint8-scaling-values) for reference #[link2](https://github.com/tensorflow/tensorflow/issues/19691)", "@Saduf2019 It's in the middle of a project. I think I cannot provide a stand alone code to replicate issue...\r\nI already had seen those links but I'm not using `convert_image_dtype` to scale the values. the `image_target_before_cast` is a float32 and all elements are between 0 to 255. So I expect to `tf.cast` to be able to cast to `uint8` properly.\r\n", "@py4 Standalone code helps us to resolve the issue faster. May be you can use some public data to demonstrate the issue. I tried to mimic your issue with `tf.cast` and I don't see any issue. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/b79a6df14e95f8619dfae0912e16f578/untitled82.ipynb). \r\n\r\nCan you please play with the gist and update it to reproduce your issue. Thanks!", "@py4 Can you please verify and close if this was resolved for you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38381\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38381\">No</a>\n"]}, {"number": 38380, "title": "TFLite makefile build rule for compile label_image example", "body": "label_image is not build by default.\r\nYou must build it expressly using the command\r\n\r\n   ./tensorflow/lite/tools/make/build_aarch64_lib.sh label_image", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38380) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38380) for more info**.\n\n<!-- ok -->"]}, {"number": 38379, "title": "Regarding Ear Landmark Dataset", "body": "I have made ear landmark dataset as shown below image.\r\n![image](https://user-images.githubusercontent.com/29560869/78864360-1cad0300-7a59-11ea-96ad-d68f3be53bb3.png)\r\nI want to make a model to predict the landmarks. So I want some advice on whether I should create my own model or should I use any pre-built model to train it with this dataset.\r\nThank you", "comments": ["This question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!", "Okay, thanks for the reply. \r\nI will ask on StackOverflow and closing this issue."]}, {"number": 38378, "title": "fix _force_enable always is None", "body": "", "comments": ["Is there a filled issue for this? A bug report? What does this fix?", "My fault. Sorry for disturb."]}, {"number": 38377, "title": "Error about repository 'mkl_dnn' occurred during the process of building Tensorflow from source", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04 LTS\r\n- TensorFlow installed from (source or binary): source\r\n- Python version: 3.6.9\r\n- Installed using virtualenv? pip? conda?: no\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n- CPU model: AMD Opteron 4122\r\n\r\n\r\n\r\n**Describe the problem**\r\nThe following problems occurred when using bazel to build Tensorflow:\r\n>**ERROR:** An error occurred during the fetch of repository 'mkl_dnn':\r\n>   java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz, https://github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz] to /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/mkl_dnn/v0.21.3.tar.gz: Checksum was a0211aeb5e7dad50b97fa5dffc1a2fe2fe732572d4164e1ee8750a2ede43fbec but wanted 31e78581e59d7e60d4becaba3834fc6a5bf2dccdae3e16b7f70d89ceab38423f\r\n\r\n>**ERROR:** /root/tensorflow/tensorflow/core/kernels/BUILD:799:1: //tensorflow/core/kernels:eigen_contraction_kernel_with_mkl depends on @mkl_dnn//:mkldnn_single_threaded in repository @mkl_dnn which failed to fetch. no such package '@mkl_dnn//': java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz, https://github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz] to /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/mkl_dnn/v0.21.3.tar.gz: Checksum was a0211aeb5e7dad50b97fa5dffc1a2fe2fe732572d4164e1ee8750a2ede43fbec but wanted 31e78581e59d7e60d4becaba3834fc6a5bf2dccdae3e16b7f70d89ceab38423f\r\n\r\n>**ERROR:** Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@mkl_dnn//': java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz, https://github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz] to /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/mkl_dnn/v0.21.3.tar.gz: Checksum was a0211aeb5e7dad50b97fa5dffc1a2fe2fe732572d4164e1ee8750a2ede43fbec but wanted 31e78581e59d7e60d4becaba3834fc6a5bf2dccdae3e16b7f70d89ceab38423f\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n`bazel build -c opt //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n\r\n**Any other info / logs**\r\n>**INFO:** Options provided by the client:\r\n>  Inherited 'common' options: --isatty=1 --terminal_columns=241\r\n\r\n>**INFO:** Reading rc options for 'build' from /root/tensorflow/.bazelrc:\r\n>  Inherited 'common' options: --experimental_repo_remote_exec\r\n\r\n>**INFO:** Reading rc options for 'build' from /root/tensorflow/.bazelrc:\r\n>  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\n\r\n>**INFO:** Reading rc options for 'build' from /root/tensorflow/.tf_configure.bazelrc:\r\n>  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --config=xla --action_env TF_CONFIGURE_IOS=0\r\n\r\n>**INFO:** Found applicable config definition build:v2 in file /root/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\n\r\n>**INFO:** Found applicable config definition build:xla in file /root/tensorflow/.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\n\r\n>**INFO:** Found applicable config definition build:linux in file /root/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\n\r\n>**INFO:** Found applicable config definition build:dynamic_kernels in file /root/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\n\r\n>**WARNING:** Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz failed: class java.io.IOException connect timed out\r\n\r\n>**WARNING:** Download from https://github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was a0211aeb5e7dad50b97fa5dffc1a2fe2fe732572d4164e1ee8750a2ede43fbec but wanted 31e78581e59d7e60d4becaba3834fc6a5bf2dccdae3e16b7f70d89ceab38423f\r\n\r\n>**ERROR:** An error occurred during the fetch of repository 'mkl_dnn':\r\n>   java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz, https://github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz] to /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/mkl_dnn/v0.21.3.tar.gz: Checksum was a0211aeb5e7dad50b97fa5dffc1a2fe2fe732572d4164e1ee8750a2ede43fbec but wanted 31e78581e59d7e60d4becaba3834fc6a5bf2dccdae3e16b7f70d89ceab38423f\r\n\r\n>**INFO:** Call stack for the definition of repository 'icu' which is a third_party_http_archive (rule definition at /root/tensorflow/third_party/repo.bzl:219:28):\r\n> \\- /root/tensorflow/third_party/icu/workspace.bzl:11:5\r\n> \\- /root/tensorflow/tensorflow/workspace.bzl:59:5\r\n> \\- /root/tensorflow/tensorflow/workspace.bzl:103:5\r\n> \\- /root/tensorflow/WORKSPACE:19:1\r\n\r\n>**INFO:** Call stack for the definition of repository 'llvm-project' which is a tf_http_archive (rule definition at /root/tensorflow/third_party/repo.bzl:134:19):\r\n> \\- /root/tensorflow/tensorflow/workspace.bzl:600:5\r\n> \\- /root/tensorflow/WORKSPACE:19:1\r\n\r\n>**INFO:** Call stack for the definition of repository 'functools32_archive' which is a tf_http_archive (rule definition at /root/tensorflow/third_party/repo.bzl:134:19):\r\n> \\- /root/tensorflow/tensorflow/workspace.bzl:378:5\r\n> \\- /root/tensorflow/WORKSPACE:19:1\r\n\r\n>**ERROR:** /root/tensorflow/tensorflow/core/kernels/BUILD:799:1: //tensorflow/core/kernels:eigen_contraction_kernel_with_mkl depends on @mkl_dnn//:mkldnn_single_threaded in repository @mkl_dnn which failed to fetch. no such package '@mkl_dnn//': java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz, https://github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz] to /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/mkl_dnn/v0.21.3.tar.gz: Checksum was a0211aeb5e7dad50b97fa5dffc1a2fe2fe732572d4164e1ee8750a2ede43fbec but wanted 31e78581e59d7e60d4becaba3834fc6a5bf2dccdae3e16b7f70d89ceab38423f\r\n\r\n>**ERROR:** Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@mkl_dnn//': java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz, https://github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz] to /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/mkl_dnn/v0.21.3.tar.gz: Checksum was a0211aeb5e7dad50b97fa5dffc1a2fe2fe732572d4164e1ee8750a2ede43fbec but wanted 31e78581e59d7e60d4becaba3834fc6a5bf2dccdae3e16b7f70d89ceab38423f\r\n\r\n>**INFO:** Elapsed time: 102.424s\r\n\r\n>**INFO:** 0 processes.\r\n\r\n>**FAILED:** Build did NOT complete successfully (312 packages loaded, 10125 targets configured)\r\n\r\n", "comments": ["Same problem, have you solved? \r\n\r\n`ERROR: /home1/marong/tensorflow/tensorflow/core/kernels/BUILD:761:1: no such package '@mkl_dnn//': java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/intel/mkl-dnn/archive/v0.20.6.tar.gz, https://github.com/intel/mkl-dnn/archive/v0.20.6.tar.gz] to /home1/marong/.cache/bazel/_bazel_marong/76b40f17b3be2ddbb9aa8139c2f8684b/external/mkl_dnn/v0.20.6.tar.gz: Checksum was f1de676fddeb94132c5ae480fb03a64f03deda10b09b3141373f2b2fe5cd031d but wanted 74675e93eef339ff3d9a9be95c15d0c7ad8736a5356c23428ab2e33dcdb8e3e1 and referenced by '//tensorflow/core/kernels:eigen_contraction_kernel_with_mkl'`\r\n\r\nI try to modify sha checksum by the correct one(f1de67.....) in `tensorflow/workspace.bzl`, but another error occurs :\r\n \r\n`tensorflow/tensorflow/core/kernels/BUILD:761:1: no such package '@mkl_dnn//': java.io.IOException: Error extracting /home1/marong/.cache/bazel/_bazel_marong/76b40f17b3be2ddbb9aa8139c2f8684b/external/mkl_dnn/v0.20.6.tar.gz to /home1/marong/.cache/bazel/_bazel_marong/76b40f17b3be2ddbb9aa8139c2f8684b/external/mkl_dnn: Prefix \"mkl-dnn-0.20.6\" was given, but not found in the archive. Here are possible prefixes for this archive: \"oneDNN-0.20.6\". and referenced by '//tensorflow/core/kernels:eigen_contraction_kernel_with_mkl'`\r\n\r\nI think the `possible prefix \"oneDNN-0.20.6\"` is not correct, but I have no idea  to deal with the error.\r\nAny advice would be greatly appreciated.", "@MRinel \r\nvim tensorflow/workspace.bzl\r\nmodify the line: strip_prefix = \"mkl-dnn-0.20.3\" to strip_prefix = \"oneDNN-0.20.3\"\r\n", "@jlbhdfsl .Thank u for timely reply. At last, I used version1.13 and compiled normally. Maybe the higher version build from source has a few unknown errors. Anyway, I'll try it later :)", "Was this issue solved? I'm facing it while building TensorFlow 1.15.2 from the source in Ubuntu 20.04, even after changing the strip prefix and sha checksum.", "@BeardyMan37, this was resolved. Could you please post the logs for errors you see? ", "Sorry about that. I managed to resolve the error, only to face subsequent errors of replacing checksumsha. It\u2019s okay, I've managed to resolve all the errors.    ", "I'm building tf_serving from codes, also meet this error.How to resolve?", "> I'm building tf_serving from codes, also meet this error.How to resolve?\r\n@cslizhang\r\nvim tensorflow/workspace.bzl\r\nmodify the line: strip_prefix = \"mkl-dnn-0.20.3\" to strip_prefix = \"oneDNN-0.20.3\"\r\n", "@leonelacs Have the above answer resolved your issue? If yes, could you close the issue?", "No, it's not fixed.\r\nDo you guys maintain the availability of released versions?\r\n", "@kevint324 \r\nYes.\r\nCould you provide the detailed info about your compile? like Tensorflow version and log?\r\n", "source version is r1.15\r\n\r\nErrros are related to dead links and illegal sha256 sum.\r\nIt should be fairly easy to reproduce. just git checkout r1.15 and follow standard compile procedure on https://www.tensorflow.org/install/source. \r\n\r\nI managed to compile tf after these changes\r\n\r\n\r\nthw@thw-s5:~/code/tf-gitee/tensorflow$ git diff\r\n```\r\ndiff --git a/tensorflow/workspace.bzl b/tensorflow/workspace.bzl\r\nindex 84b2bdcb2d..615d8c3cc3 100755\r\n--- a/tensorflow/workspace.bzl\r\n+++ b/tensorflow/workspace.bzl\r\n@@ -135,11 +135,14 @@ def tf_repositories(path_prefix = \"\", tf_repo_name = \"\"):\r\n     tf_http_archive(\r\n         name = \"mkl_dnn\",\r\n         build_file = clean_dep(\"//third_party/mkl_dnn:mkldnn.BUILD\"),\r\n-        sha256 = \"74675e93eef339ff3d9a9be95c15d0c7ad8736a5356c23428ab2e33dcdb8e3e1\",\r\n-        strip_prefix = \"mkl-dnn-0.20.6\",\r\n+        #sha256 = \"74675e93eef339ff3d9a9be95c15d0c7ad8736a5356c23428ab2e33dcdb8e3e1\",\r\n+        sha256 = \"f1de676fddeb94132c5ae480fb03a64f03deda10b09b3141373f2b2fe5cd031d\",\r\n+        #strip_prefix = \"mkl-dnn-0.20.6\",\r\n+        strip_prefix = \"oneDNN-0.20.6\",\r\n         urls = [\r\n             \"https://storage.googleapis.com/mirror.tensorflow.org/github.com/intel/mkl-dnn/archive/v0.20.6.tar.gz\",\r\n             \"https://github.com/intel/mkl-dnn/archive/v0.20.6.tar.gz\",\r\n+            \"https://github.com/oneapi-src/oneDNN/archive/v0.20.6.tar.gz\",\r\n         ],\r\n     )\r\n \r\n@@ -409,9 +412,11 @@ def tf_repositories(path_prefix = \"\", tf_repo_name = \"\"):\r\n         name = \"org_python_license\",\r\n         licenses = [\"notice\"],  # Python 2.0\r\n         sha256_urls = {\r\n-            \"e76cacdf0bdd265ff074ccca03671c33126f597f39d0ed97bc3e5673d9170cf6\": [\r\n-                \"https://storage.googleapis.com/mirror.tensorflow.org/docs.python.org/2.7/_sources/license.rst.txt\",\r\n-                \"https://docs.python.org/2.7/_sources/license.rst.txt\",\r\n+            #\"e76cacdf0bdd265ff074ccca03671c33126f597f39d0ed97bc3e5673d9170cf6\": [\r\n+            \"b5556e921715ddb9242c076cae3963f483aa47266c5e37ea4c187f77cc79501c\": [\r\n+                #\"https://storage.googleapis.com/mirror.tensorflow.org/docs.python.org/2.7/_sources/license.rst.txt\",\r\n+                #\"https://docs.python.org/2.7/_sources/license.rst.txt\",\r\n+                \"https://mirror.bazel.build/docs.python.org/2.7/_sources/license.txt\",\r\n             ],\r\n         },\r\n     )\r\n```\r\n\r\nIt compiles with this build command.  config=noXXXX must be included otherwise there will be more dead links.\r\n\r\n```\r\nbazel build --config=opt \\\r\n            --config=cuda \\\r\n            --config=nonccl \\\r\n            --config=nokafka \\\r\n            --config=noaws \\\r\n            --config=nogcp \\\r\n            --config=nohdfs \\\r\n            --config=noignite \\\r\n            //tensorflow/tools/pip_package:build_pip_package\r\n```", "@leonelacs @kevint324 \r\nIf build the TF with above bazel cmd,  it won't use mkldnn code to build. (miss '--config=mkl')\r\nSo the code change above won't impact the build.\r\n\r\nCould you try tf with tag v1.15.4", "@leonelacs @kevint324 \r\nAre you working for same issue together?\r\n\r\nHow about the suggestion?\r\n", "@leonelacs \r\nCould you feedback the suggestion?\r\n\r\nThank you!", "Sorry for the late reply.\r\n1.15.4 could compile fine. issue resolved!", "@kevint324 \r\nIt's great!\r\n\r\nCould you close this issue?\r\n\r\nThank you!", "I don't think so. I'm not the reporter.", "@kevint324  Sorry, I forgot it again.\r\n\r\n@leonelacs  Could you try with the suggestion and feedback?", "@leonelacs\r\nAny updates regarding this issue? Could you please take a look at @NeoZhangJianyu's comment and let us know if you are still facing the same issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38377\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38377\">No</a>\n"]}, {"number": 38376, "title": "Extract Model Variable from Serialize Graph File", "body": "**System information**\r\n- TensorFlow version (you are using): 1.14.0\r\n- Are you willing to contribute it (Yes/No): Yes.\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n![image](https://user-images.githubusercontent.com/16629752/78857533-2edc7080-7a5c-11ea-80a7-a69ccd51fc63.png)\r\nWhen I save a graph in to keras.pb. Then I wan to acquire <b>model</b> variable. However, I cannot find a way to achieve.\r\n\r\n**Will this change the current api? How?**\r\nI am not sure.\r\n\r\n**Who will benefit with this feature?**\r\nThis feature can help people transfer released model parameters into other format, such as ML flow, Keras etc.\r\n\r\n**Any Other info.**\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 38375, "title": "tf.tile each dimension individual  operation not supported", "body": "**System information** \r\n\r\n     OS Platform and Distribution : macOS Catalina 10.15.3\r\n\r\n    TensorFlow installed from : binary\r\n\r\n    TensorFlow version : 1.15.0\r\n\r\n    Python version: 3.7.3\r\n\r\n**Describe the current behavior**\r\n\r\nI have tensor \r\n\r\na = `tf.Tensor(\r\n[[[ 16  15  16  15  87]\r\n  [  3   4   3   4  87]\r\n  [  3   4   8   9 133]\r\n  [  3   4   8   9  871]\r\n  [  3   4  14   95  87]\r\n  [  3   7   9  250  87]]\r\n [[  3   4   3   41  87]\r\n  [  3   4   8   93 133]\r\n  [  3   4   18   9  87]\r\n  [  3   4  141   5  87]\r\n  [  3   7   19  25  87]\r\n  [  0   0   0   0   0]]], shape=(2, 6, 5), dtype=int64)`\r\n\r\nb = `tf.Tensor(\r\n[[[ 1]\r\n  [2]\r\n  [ 3]\r\n  [ 1]\r\n  [ 2]\r\n  [2]]\r\n [[2]\r\n  [ 2]\r\n  [ 2]\r\n  [ 2]\r\n  [1]\r\n  [ 0]]], shape=(2, 6, 1), dtype=int64)`\r\n**Describe the expected behavior**\r\n\r\nI want vector c such that \r\n\r\n[ 16  15  16  15  87] is repeated [1] times , \r\n[  3   4   3   4  87] is repeated [2] times , \r\n[  3   4   8   9 133] is repeated [3] times,\r\n[  3   4   8   9  871] is repated [1] times,\r\n[  3   4  14   95  87] is repeated [2] times,\r\n[  3   7   9  250  87] is repated [2] times and so on ............\r\n\r\nso c should be \r\n\r\nc= `tf.Tensor(\r\n[[[ 16  15  16  15  87]\r\n[  3   4   3   4  87]\r\n[  3   4   3   4  87]\r\n[  3   4   8   9 133]\r\n[  3   4   8   9 133]\r\n[  3   4   8   9 133]\r\n[  3   4   8   9  871]\r\n[  3   4  14   95  87]\r\n[  3   4  14   95  87]\r\n[  3   7   9  250  87]\r\n[  3   7   9  250  87]]\r\n [[  3   4   3   41  87]\r\n[  3   4   3   41  87]\r\n  [  3   4   8   93 133]\r\n [  3   4   8   93 133]\r\n  [  3   4   18   9  87]\r\n [  3   4   18   9  87]\r\n  [  3   4  141   5  87]\r\n[  3   4  141   5  87]\r\n  [  3   7   19  25  87]\r\n [  3   7   19  25  87]\r\n  [  0   0   0   0   0]]])`\r\n\r\n\r\nI tried using tf.tile but it only takes same repeation for each dimension . \r\n", "comments": ["@17patelumang \r\nplease share a simple standalone code for us to replicate the issue faced along with the error.", "`import tensorflow as tf `\r\n\r\n`a = tf.constant([[[16,15,16,15,87],[3,4,3,4,87],[3,4,8,9,133],[3,4,8,9,871],[3,4,14,95,87],[3,7,9,250,87]],[[ 3,4,3,41,87],[3,4,8,93,133],[3,4,18,9,87],[3,4,141,5,87],[3,7,19,25,87],[0,0,0,0,0]]]) `\r\n`print(a)`\r\n\r\n`b = tf.constant([[[1],[2],[3],[1],[2],[2]],[[2],[2],[2],[2],[1],[0]]])`\r\n\r\n`print(b)`\r\n\r\n`c = tf.tile(a, b)  ### this will fail`\r\n\r\n`print(c)`\r\n\r\n`####  c = I was c as described in questions`\r\n\r\n> \"\"\"\r\n> Traceback (most recent call last):\r\n>   File \"tensor_tf.py\", line 10, in <module>\r\n>     c = tf.tile(a, b) \r\n>   File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\", line 11309, in tile\r\n>     \"Tile\", input=input, multiples=multiples, name=name)\r\n>   File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\r\n>     op_def=op_def)\r\n>   File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\r\n>     return func(*args, **kwargs)\r\n>   File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\r\n>     attrs, op_def, compute_device)\r\n>   File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\r\n>     op_def=op_def)\r\n>   File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1770, in __init__\r\n>     control_input_ops)\r\n>   File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1610, in _create_c_op\r\n>     raise ValueError(str(e))\r\n> ValueError: Shape must be rank 1 but is rank 3 for 'Tile' (op: 'Tile') with input shapes: [2,6,5], [2,6,1\r\n> \"\"\"", "i was able to replicate this, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/da216942754e945a4865be16bb969dc1/untitled132.ipynb)", "@jvishnuvardhan any update ? ", "@17patelumang you wrote in your example:\r\n\r\n```\r\nb = tf.constant([[[1],[2],[3],[1],[2],[2]],[[2],[2],[2],[2],[1],[0]]])\r\n```\r\n\r\nBut according to the output you described, I believe you wanted:\r\n\r\n```\r\nb = tf.constant([[[1],[2],[3],[1],[2],[2]],[[2],[2],[2],[2],[1],[1]]])\r\n```\r\n\r\n(that is, the last element is 1, not 0).\r\n\r\nThe answer to that question is important because in the first case, you'd end up with a tensor with 11 elements on the first row, but only 10 elements on the second, which cannot be represented as a dense tensor and would require a ragged tensor. But in the second case, a dense tensor would be sufficient.", "@mdanatg  I want empty tenor where its zero in tensor c.  I agree I would need ragged tensor.\r\n\r\nSo basically let me ask exact question for more clarity \r\n\r\n\r\n`a1 = tf.ragged.constant([[[b'a1', b'a2', b'a3'], [b'b1', b'b2', b'b3'], [b'c1', b'c2','c3']], [[b'd1'], [b'e1', b'e2']]])`\r\n\r\n\r\n`b1 = tf.ragged.constant([[[b't1', b't2', b't3'], [b'u1', b'u2'], [b'v1', b'v2']], [[b'w1'], [b'x1', b'x2', b'x3']]])`\r\n\r\n\r\nI have a1 , b1 as above , i want c as below \r\n\r\n`c1 = tf.ragged.constant([[[b'a1', b'a2', b'a3'],[b'a1', b'a2', b'a3'],[b'a1', b'a2', b'a3'],[b'b1', b'b2', b'b3'],[b'b1', b'b2', b'b3'],[b'c1', b'c2','c3'],[b'c1', b'c2','c3']], [[b'd1'], [b'e1', b'e2'],[b'e1', b'e2'],[b'e1', b'e2']]])`\r\n\r\nie [b'a1', b'a2', b'a3'] is repeated equal to number of elements in  [b't1', b't2', b't3'] ie 3 so in c1 we will have [b'a1', b'a2', b'a3'],[b'a1', b'a2', b'a3'],[b'a1', b'a2', b'a3'],\r\n\r\nsimilarly [b'b1', b'b2', b'b3'] is repeated equal to number of elements in  [b'u1', b'u2'] ie 2 so in c 1we will more have [b'b1', b'b2', b'b3'],[b'b1', b'b2', b'b3'],\r\n\r\nsimilarly [b'c1', b'c2','c3'] is repeated equal to number of elements in  [[b'v1', b'v2'] ie 2 so in c1 we will more have [b'c1', b'c2','c3'],[b'c1', b'c2','c3']\r\n\r\ntill here all will be past of first tensor  ie `[[b'a1', b'a2', b'a3'],[b'a1', b'a2', b'a3'],[b'a1', b'a2', b'a3'],[b'b1', b'b2', b'b3'],[b'b1', b'b2', b'b3'],[b'c1', b'c2','c3'],[b'c1', b'c2','c3']]` now for second ragged tensor \r\n\r\nsame process as above where [b'd1'] should be repeated 1 times as number of elements in [b'w1'] ..... \r\n\r\n\r\n\r\n\r\n\r\nI was trying by converting b1 into length as b in above ticket but a1,b1,c1 is the actual question \r\n\r\n\r\n\r\n\r\ni am trying to fiddle with it since last 2 days , could you please help me with that. ", "So `tf.tile` is not powerful enough to handle this use case. It only knows how to repeat entire dimensions a constant number of times.\r\n\r\nYou'll need to build it manually instead, here's a way to do it. The code uses autograph and I've only tested it in TF 2, but let me know if you have trouble using it in TF 1.\r\n\r\n```\r\na = tf.constant(\r\n    [[[ 16, 15, 16, 15, 87],\r\n      [ 3, 4, 3, 4, 87],\r\n      [ 3, 4, 8, 9, 133],\r\n      [ 3, 4, 8, 9, 871],\r\n      [ 3, 4, 14, 95, 87],\r\n      [ 3, 7, 9, 250, 87]],\r\n     [[ 3, 4, 3, 41, 87],\r\n      [ 3, 4, 8, 93, 133],\r\n      [ 3, 4, 18, 9, 87],\r\n      [ 3, 4, 141, 5, 87],\r\n      [ 3, 7, 19, 25, 87],\r\n      [ 0, 0, 0, 0, 0]]])\r\n\r\nb = tf.constant(\r\n    [[[1], [2], [3], [1], [2], [2]],\r\n     [[2], [2], [2], [2], [1], [0]]])\r\n\r\n@tf.function\r\ndef tile_nd_ragged(a, b):\r\n  # Need a sentinel, otherwise it's hard to give it the initial shape we need.\r\n  # We'll drop the sentinel at the end.\r\n  acc = tf.ragged.constant([[[0] * a.shape[2]]])\r\n\r\n  # Work one row at a time...\r\n  for i1 in range(len(a)):    # Should be able to write `for a1, b1 in zip(a, b)` soon.\r\n    a1 = a[i1]\r\n    b1 = b[i1]\r\n    acc1 = tf.TensorArray(dtype=a1.dtype, size=0, dynamic_size=True)\r\n \r\n    # Do the actual tiling...\r\n    for i2 in tf.range(len(a1)):\r\n      a2 = a1[i2]\r\n      b2 = b1[i2]\r\n      for _ in range(b2[0]):\r\n        acc1 = acc1.write(acc1.size(), a2)\r\n    tmp = tf.expand_dims(acc1.stack(), 0)    # Now `tmp` is row `i1`, tiled.\r\n\r\n    acc = tf.concat([acc, tmp], axis=0)    # Add the row to the final result.\r\n\r\n  acc = acc[1:]  # Drop the sentinel.\r\n  return acc\r\n\r\nprint(tile_nd_ragged(a, b))\r\n```\r\n\r\nEdit: added a few code comments.", "@mdanatg  thank you for reply . I asked the actual question for tensor a1,b1,c1 . \r\n\r\n1. Could you please look into that so basically I want (apologies initially i was trying to convert to b tensor having length and some dummy tensor a)\r\n\r\n`a1 = tf.ragged.constant([[[b'a1', b'a2', b'a3'], [b'b1', b'b2', b'b3'], [b'c1', b'c2','c3']], [[b'd1'], [b'e1', b'e2']]])`\r\n\r\n`b1 = tf.ragged.constant([[[b't1', b't2', b't3'], [b'u1', b'u2'], [b'v1', b'v2']], [[b'w1'], [b'x1', b'x2', b'x3']]])`\r\n\r\n`I have a1 , b1 as above , i want c as below\r\n\r\n`c1 = tf.ragged.constant([[[b'a1', b'a2', b'a3'],[b'a1', b'a2', b'a3'],[b'a1', b'a2', b'a3'],[b'b1', b'b2', b'b3'],[b'b1', b'b2', b'b3'],[b'c1', b'c2','c3'],[b'c1', b'c2','c3']], [[b'd1'], [b'e1', b'e2'],[b'e1', b'e2'],[b'e1', b'e2']]])`\r\n\r\nie [b'a1', b'a2', b'a3'] is repeated equal to number of elements in [b't1', b't2', b't3'] ie 3 so in c1 we will have [b'a1', b'a2', b'a3'],[b'a1', b'a2', b'a3'],[b'a1', b'a2', b'a3'],\r\n\r\nsimilarly [b'b1', b'b2', b'b3'] is repeated equal to number of elements in [b'u1', b'u2'] ie 2 so in c 1we will more have [b'b1', b'b2', b'b3'],[b'b1', b'b2', b'b3'],\r\n\r\nsimilarly [b'c1', b'c2','c3'] is repeated equal to number of elements in [[b'v1', b'v2'] ie 2 so in c1 we will more have [b'c1', b'c2','c3'],[b'c1', b'c2','c3']\r\n\r\ntill here all will be past of first tensor ie [[b'a1', b'a2', b'a3'],[b'a1', b'a2', b'a3'],[b'a1', b'a2', b'a3'],[b'b1', b'b2', b'b3'],[b'b1', b'b2', b'b3'],[b'c1', b'c2','c3'],[b'c1', b'c2','c3']] now for second ragged tensor\r\n\r\nsame process as above where [b'd1'] should be repeated 1 times as number of elements in [b'w1']\r\n\r\n\r\n2. Also i am using this to convert raw signal to feature as part of my savedModel , so will `for i2 in range(len(a1)):` work in savedmodel ? or should we need to use tf API to do that (I am using  1.15.2) ?", "1. So I suspect in your original code b was the lengths of the rows in `b1`. Please give the code I showed you a try. You should be able to easily modify it so that the inner loop uses `len(b2)` instead of `b2[0]`.\r\n\r\n2. Yes, this should work with SavedModel, even in TF 1. The loop you indicated is converted to a `tf.while_loop` by autograph, see https://www.tensorflow.org/guide/function.", "@mdanatg thank you again ! :) .\r\n\r\n2. Wrt to 2 thank you , so does it mean using `@tf.function` we can convert our python code feature processing into TF api feature processing and port it to saved model ?  If thats the case then we can ideally convert our python code to TF API feature transformation to port to saved model and then once we have savedmodel it will become platform independent correct ? \r\n\r\nCould you please share more light on what we need to take care while writing customized python function so that it can be easily ported to TF API ? esp in above code `range(len(a))`\r\n\r\n1. Wrt to I tried your code by replacing `b2[0]` with `len(b2)` it gives output as below\r\n\r\n`<tf.RaggedTensor [[[16, 15, 16, 15, 87], [3, 4, 3, 4, 87], [3, 4, 8, 9, 133], [3, 4, 8, 9, 871], [3, 4, 14, 95, 87], [3, 7, 9, 250, 87]], [[3, 4, 3, 41, 87], [3, 4, 8, 93, 133], [3, 4, 18, 9, 87], [3, 4, 141, 5, 87], [3, 7, 19, 25, 87], [0, 0, 0, 0, 0]]]>` which is correct however when i try to convert this to raggedTensor  iin below code it fails \r\n\r\n```\r\na1 = tf.ragged.constant([[[b'a1', b'a2', b'a3'], [b'b1', b'b2', b'b3'], [b'c1', b'c2','c3']], [[b'd1'], [b'e1', b'e2']]])\r\nb1 = tf.ragged.constant([[[b't1', b't2', b't3'], [b'u1', b'u2'], [b'v1', b'v2']], [[b'w1'], [b'x1', b'x2', b'x3']]])\r\n\r\n@tf.function\r\ndef tile_nd_ragged(a, b):\r\n  # Need a sentinel, otherwise it's hard to give it the initial shape we need.\r\n  # We'll drop the sentinel at the end.\r\n  acc = tf.ragged.constant([[[0] * a.shape[2]]])\r\n\r\n  # Work one row at a time...\r\n  for i1 in range(len(a)):    # Should be able to write `for a1, b1 in zip(a, b)` soon.\r\n    a1 = a[i1]\r\n    b1 = b[i1]\r\n    acc1 = tf.TensorArray(dtype=a1.dtype, size=0, dynamic_size=True)\r\n \r\n    # Do the actual tiling...\r\n    for i2 in tf.range(len(a1)):\r\n      a2 = a1[i2]\r\n      b2 = b1[i2]\r\n      for _ in range(b2[0]):\r\n        acc1 = acc1.write(acc1.size(), a2)\r\n    tmp = tf.expand_dims(acc1.stack(), 0)    # Now `tmp` is row `i1`, tiled.\r\n\r\n    acc = tf.concat([acc, tmp], axis=0)    # Add the row to the final result.\r\n\r\n  acc = acc[1:]  # Drop the sentinel.\r\n  return acc\r\n\r\nprint(tile_nd_ragged(a1, b1))\r\n```\r\n\r\n```\r\nIt gives error \r\n\r\nTraceback (most recent call last):\r\n  File \"tf_3.py\", line 53, in <module>\r\n    print(tile_nd_ragged(a1, b1))\r\n  File \"<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 449, in __call__\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \"<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 392, in _initialize\r\n    *args, **kwds))\r\n  File \"<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1847, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2147, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2038, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 335, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 905, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nTypeError: in converted code:\r\n\r\n    tf_3.py:32 tile_nd_ragged  *\r\n        acc = tf.ragged.constant([[[0] * a.shape[2]]])\r\n    <path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py:446 __rmul__\r\n        return self * other\r\n\r\n    TypeError: __index__ returned non-int (type NoneType)\r\n```\r\n\r\n", "A few good places to start are the documentation of [tf.function](https://www.tensorflow.org/api_docs/python/tf/function), and the [autograph reference](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/index.md). The hardest part is until you get used to understanding which parts of Python work and which don't.\r\n\r\nRe: (2), yes, this lets you write platform-independent code, but the caveat is that it's a (small) subset of Python. And in some cases there are missing features (you just encountered one) that we're still working on.\r\n\r\nRe: (1), if you want the inputs to be RaggedTensor it's a bit more complicated, because they have more complex shape. The error that you saw is a bug - sorry about that! - we'll fix it soon. Anyway, even avoiding that bug, there are a few subtleties around dynamic tensor shapes that are tricky to deal with, so I rewrote the code instead.\r\n\r\nI recommend carefully reading through it to understand what each operation does - it's a useful exercise.\r\n\r\n```\r\na1 = tf.ragged.constant([[[b'a1', b'a2', b'a3'], [b'b1', b'b2', b'b3'], [b'c1', b'c2','c3']], [[b'd1'], [b'e1', b'e2']]])\r\nb1 = tf.ragged.constant([[[b't1', b't2', b't3'], [b'u1', b'u2'], [b'v1', b'v2']], [[b'w1'], [b'x1', b'x2', b'x3']]])\r\n\r\n@tf.function\r\ndef tile_nd_ragged(a, b):\r\n  # Need a sentinel, otherwise it's hard to give it the initial shape we need.\r\n  # We'll drop the sentinel at the end.\r\n  acc = tf.ragged.constant([[[]]], dtype=a.dtype)\r\n\r\n  # Work one row at a time...\r\n  for i1 in range(len(a.nested_row_lengths()[0])):    # Should be able to write `for a1, b1 in zip(a, b)` soon.\r\n    a1 = a[i1]\r\n    b1 = b[i1]\r\n    # If the components have variable length, we can't use a TensorArray anymore,\r\n    # so use a RaggedTensor instead.\r\n    acc1 = tf.ragged.constant([[]], dtype=a.dtype)\r\n \r\n    # Do the actual tiling...\r\n    for i2 in tf.range(a1.nested_row_lengths()[0][i1]):\r\n      # Need this workaround to let tensors change shape in a loop.\r\n      tf.autograph.experimental.set_loop_options(\r\n          shape_invariants=[(acc1, tf.TensorShape([None, None]))]\r\n      )\r\n\r\n      a2 = a1[i2]\r\n      b2 = b1[i2]\r\n      for _ in range(len(b2)):\r\n        acc1 = tf.concat([acc1, tf.expand_dims(a2, 0)], axis=0)\r\n\r\n    acc1 = acc1[1:]  # Drop the sentinel.\r\n    acc = tf.concat([acc, tf.expand_dims(acc1, 0)], axis=0)    # Add the row to the final result.\r\n\r\n  acc = acc[1:]  # Drop the sentinel.\r\n  return acc\r\n\r\ntile_nd_ragged(a1, b1)\r\n```", "@mdanatg thank you.\r\n\r\nRe 2: wrt to feature missing we are talking about `range(len(a))` ? \r\nRe1 : wrt to 2 , i understood your code but when i try to run it gives error -\r\n\r\n\r\n`    ValueError: Input tensor 'placeholder_2:0' enters the loop with shape (2,), but has shape (?,) after one iteration. To allow the shape to vary across iterations, use the `shape_invariants` argument of tf.while_loop to specify a less-specific shape`\r\n\r\nAlso does it mean that since we are using `range(len(b2))` it cannot be suceessfully ported into savedmodel ? \r\n", "`range(len(a))` works when `a` is a Tensor. But other things, like `zip(a, b)` when `a` or `b` are tensors, don't work yet. That kind of things. And all things that do work should be compatible with SavedModel. If they don't, then it's a bug :) . Does this answer your question?\r\n\r\nThat error is strange. The line `tf.autograph.experimental.set_loop_options(shape_invariants=[(acc1, tf.TensorShape([None, None]))])` should have taken care of that, so it means that `set_loop_options` doesn't work properly in TF 1.15, which is unfortunate. I'd recommend switching to TF 2, but if that's not practical then you'd have to rewrite the `for i2` loop using `tf.while_loop`, like this:\r\n\r\n```\r\n    ...\r\n    acc1 = tf.ragged.constant([[]], dtype=a.dtype)\r\n \r\n    # Do the actual tiling...\r\n    def loop_test(i2, _):\r\n      return i2 < tf.cast(a1.nested_row_lengths()[0][i1], tf.int32)\r\n\r\n    def loop_body(i2, acc1):\r\n      a2 = a1[i2]\r\n      b2 = b1[i2]\r\n      for _ in range(len(b2)):\r\n        acc1 = tf.concat([acc1, tf.expand_dims(a2, 0)], axis=0)\r\n      return i2 + 1, acc1\r\n\r\n    _, acc1 = tf.while_loop(\r\n        loop_test, loop_body, [0, acc1],\r\n        shape_invariants=[None, tf.TensorShape([None, None])])\r\n\r\n    acc1 = acc1[1:]  # Drop the sentinel.\r\n    ...\r\n```", "@mdanatg  yes i got it `range(len(a))` , thank you :) .\r\n\r\nAlso switching to TF 2.0 is not practical as of now , so i tried the code you suggested as below and i am seeing something strange .\r\n\r\nWhen I try below \r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\ndef tile_nd_ragged2(a, b):\r\n  # Need a sentinel, otherwise it's hard to give it the initial shape we need.\r\n  # We'll drop the sentinel at the end.\r\n  acc = tf.ragged.constant([[[]]], dtype=a.dtype)\r\n\r\n  # Work one row at a time...\r\n  for i1 in range(len(a.nested_row_lengths()[0])):    # Should be able to write `for a1, b1 in zip(a, b)` soon.\r\n    a1 = a[i1]\r\n    b1 = b[i1]\r\n    # If the components have variable length, we can't use a TensorArray anymore,\r\n    # so use a RaggedTensor instead.\r\n    acc1 = tf.ragged.constant([[]], dtype=a.dtype)\r\n    def loop_test(i2, _):\r\n      return i2 < tf.cast(a1.nested_row_lengths()[0][i1], tf.int32)\r\n    def loop_body(i2, acc1):\r\n      a2 = a1[i2]\r\n      b2 = b1[i2]\r\n      for _ in range(len(b2)):\r\n        acc1 = tf.concat([acc1, tf.expand_dims(a2, 0)], axis=0)\r\n      return i2 + 1, acc1\r\n\r\n    _, acc1 = tf.while_loop(loop_test, loop_body, [0, acc1],shape_invariants=[None, tf.TensorShape([None, None])])\r\n    acc1 = acc1[1:]  # Drop the sentinel.\r\n    acc = tf.concat([acc, tf.expand_dims(acc1, 0)], axis=0)    # Add the row to the final result.\r\n\r\n  acc = acc[1:]  # Drop the sentinel.\r\n  return acc\r\n\r\n\r\nx = tf.ragged.constant([[[b'a1', b'a2', b'a3'], [b'b1', b'b2', b'b3'], [b'c1', b'c2','c3']], [[b'd1'], [b'e1', b'e2']]])\r\ny = tf.ragged.constant([[[b't1', b't2', b't3'], [b'u1', b'u2'], [b'v1', b'v2']], [[b'w1'], [b'x1', b'x2', b'x3']]])\r\nprint(x)\r\nprint(y)\r\nprint(tile_nd_ragged2(x, y))\r\n```\r\nI get below output  \r\n```\r\ninput \r\na = <tf.RaggedTensor [[[b'a1', b'a2', b'a3'], [b'b1', b'b2', b'b3'], [b'c1', b'c2', b'c3']], [[b'd1'], [b'e1', b'e2']]]>\r\nb = <tf.RaggedTensor [[[b't1', b't2', b't3'], [b'u1', b'u2'], [b'v1', b'v2']], [[b'w1'], [b'x1', b'x2', b'x3']]]>\r\noutput = <tf.RaggedTensor [[[b'a1', b'a2', b'a3'], [b'a1', b'a2', b'a3'], [b'a1', b'a2', b'a3'], [b'b1', b'b2', b'b3'], [b'b1', b'b2', b'b3'], [b'c1', b'c2', b'c3'], [b'c1', b'c2', b'c3']], [[b'd1'], [b'e1', b'e2'], [b'e1', b'e2'], [b'e1', b'e2']]]>\r\n```\r\n\r\nBut when i try to put `@tf.function` above the function and run exactly same way it gives below error \r\n\r\n ```\r\n File \"tf_3.py\", line 101, in <module>\r\n    print(tile_nd_ragged2(x, y))\r\n  File \"/<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 449, in __call__\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \"/<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 392, in _initialize\r\n    *args, **kwds))\r\n  File \"/<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1847, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2147, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2038, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 335, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 905, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nTypeError: in converted code:\r\n\r\n    tf_3.py:88 tile_nd_ragged2  *\r\n        _, acc1 = tf.while_loop(loop_test, loop_body, [0, acc1],shape_invariants=[None, tf.TensorShape([None, None])])\r\n    /<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py:2675 while_loop\r\n        back_prop=back_prop)\r\n    /<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/while_v2.py:87 while_loop\r\n        list(shape_invariants), expand_composites=False)\r\n    /<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py:536 map_structure\r\n        structure[0], [func(*x) for x in entries],\r\n    /<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py:536 <listcomp>\r\n        structure[0], [func(*x) for x in entries],\r\n    /<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py:512 _shape_invariant_to_type_spec\r\n        % shape)\r\n\r\n    TypeError: Expected shape to be a TypeSpec or TensorShape, got None\r\n```\r\n1. I  need to port this as part of TF API transformation so i need to put `@tf.function` correct ? , if so then i think it fails when I try to run by keeping it as top `@tf.function` , am i missing anything ?\r\n\r\n2. Wrt to `set_loop_options doesn't work properly in TF 1.15,` should I open a bug or feature request ? \r\n(updated the code with import and `tf.enable_eager_execution()`)", "Ah, that's another bug fix that landed in TF 2 and not in TF 1.15.... previously, shape_invariants required a value for all loop vars, and only recently we allowed setting some of them to None. Could you try changing this line:\r\n\r\n`_, acc1 = tf.while_loop(loop_test, loop_body, [0, acc1],shape_invariants=[tf.TensorShape([]), tf.TensorShape([None, None])])`\r\n\r\nThe error message it throws is quite bad, and that's worth filing a bug request.\r\n\r\nAs for #2, the problem is that there are no plans for a TF 1.16, and we can't backport fixes into 1.15 because it's already released. That's another good reason to consider upgrading when possible.", "@mdanatg thank you again ! :) \r\nJust for other who wants working code below is the code \r\n```\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\n\r\n@tf.function\r\ndef tile_nd_ragged2(a, b):\r\n  # Need a sentinel, otherwise it's hard to give it the initial shape we need.\r\n  # We'll drop the sentinel at the end.\r\n  acc = tf.ragged.constant([[[]]], dtype=a.dtype)\r\n\r\n  # Work one row at a time...\r\n  for i1 in range(len(a.nested_row_lengths()[0])):    # Should be able to write `for a1, b1 in zip(a, b)` soon.\r\n    a1 = a[i1]\r\n    b1 = b[i1]\r\n    # If the components have variable length, we can't use a TensorArray anymore,\r\n    # so use a RaggedTensor instead.\r\n    acc1 = tf.ragged.constant([[]], dtype=a.dtype)\r\n    def loop_test(i2, _):\r\n      return i2 < tf.cast(a1.nested_row_lengths()[0][i1], tf.int32)\r\n    def loop_body(i2, acc1):\r\n      a2 = a1[i2]\r\n      b2 = b1[i2]\r\n      for _ in range(len(b2)):\r\n        acc1 = tf.concat([acc1, tf.expand_dims(a2, 0)], axis=0)\r\n      return i2 + 1, acc1\r\n\r\n    _, acc1 = tf.while_loop(loop_test, loop_body, [0, acc1],shape_invariants=[tf.TensorShape([]), tf.TensorShape([None, None])])\r\n    acc1 = acc1[1:]  # Drop the sentinel.\r\n    acc = tf.concat([acc, tf.expand_dims(acc1, 0)], axis=0)    # Add the row to the final result.\r\n\r\n  acc = acc[1:]  # Drop the sentinel.\r\n  return acc\r\n\r\nx = tf.ragged.constant([[[b'a1', b'a2', b'a3'], [b'b1', b'b2', b'b3'], [b'c1', b'c2','c3']], [[b'd1'], [b'e1', b'e2']]])\r\ny = tf.ragged.constant([[[b't1', b't2', b't3'], [b'u1', b'u2'], [b'v1', b'v2']], [[b'w1'], [b'x1', b'x2', b'x3']]])\r\n\r\n\r\nprint(x)\r\nprint(y)\r\nprint(tile_nd_ragged2(x, y))\r\n```\r\n\r\noutput \r\n```\r\n<tf.RaggedTensor [[[b'a1', b'a2', b'a3'], [b'b1', b'b2', b'b3'], [b'c1', b'c2', b'c3']], [[b'd1'], [b'e1', b'e2']]]>\r\n\r\n<tf.RaggedTensor [[[b't1', b't2', b't3'], [b'u1', b'u2'], [b'v1', b'v2']], [[b'w1'], [b'x1', b'x2', b'x3']]]>\r\n<tf.RaggedTensor [[[b'a1', b'a2', b'a3'], [b'a1', b'a2', b'a3'], [b'a1', b'a2', b'a3'], [b'b1', b'b2', b'b3'], [b'b1', b'b2', b'b3'], [b'c1', b'c2', b'c3'], [b'c1', b'c2', b'c3']], [[b'd1'], [b'e1', b'e2'], [b'e1', b'e2'], [b'e1', b'e2']]]>\r\n```\r\n\r\n1. Wrt to filing feature/bug request , i will do it referencing this ticket . https://github.com/tensorflow/tensorflow/issues/38438 (feature/bug ticket)\r\n2. Also Now the above function has all the function which will work in TF API correct ? (please correct me if i am wrong.) or I need to specially handle like zip(a,b) ? Could you please comment on that esp `for i1 in range` \r\n\r\n(updated ticket with feature/bug request - https://github.com/tensorflow/tensorflow/issues/38438)\r\n\r\n ", "If it works with this test than the function should work with SavedModel and all the other APIs - let me know if it doesn't. Be sure you keep the @tf.function decorator.", "@mdanatg i will try to see if it works in savedmodel in TF 1.15.x and will respond by Tuesday next week max. ", "@mdanatg I think  there is  bug in script\r\nfor same  above script its giving wrong output \r\n\r\n```\r\nx = tf.ragged.constant([[[b'd1'], [b'e1', b'e2']],[[b'd1'], [b'e1', b'e2']]])\r\ny = tf.ragged.constant([[[b'w1'], [b'x1', b'x2', b'x3']],[[b'w1'], [b'x1', b'x2', b'x3']]])\r\n\r\nprint(x)\r\nprint(y)\r\nprint(tile_nd_ragged2(x, y))\r\n```\r\noutput\r\n```\r\n<tf.RaggedTensor [[[b'd1'], [b'e1', b'e2']], [[b'd1'], [b'e1', b'e2']]]>\r\n<tf.RaggedTensor [[[b'w1'], [b'x1', b'x2', b'x3']], [[b'w1'], [b'x1', b'x2', b'x3']]]>\r\n<tf.RaggedTensor [[[b'd1']], [[b'd1'], [b'e1', b'e2'], [b'e1', b'e2'], [b'e1', b'e2']]]>\r\n```\r\n\r\nalso when I try \r\n```\r\nx = tf.ragged.constant([[[b'a1', b'a2', b'a3'], [b'b1', b'b2', b'b3'], [b'c1', b'c2','c3']], [[b'd1'], [b'e1', b'e2']       ],[[b'f2'],[b'g2']]])\r\ny = tf.ragged.constant([[[b't1', b't2', b't3'], [b'u1', b'u2'],             [b'v1', b'v2']], [[b'w1'], [b'x1', b'x2', b'x3']],[[b'y2'],[b'z2']]])\r\n```\r\n\r\n```\r\nit gives error \r\n\r\n  File \"tf_3.py\", line 43, in <module>\r\n    print(tile_nd_ragged2(x, y))\r\n  File \"/<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 467, in __call__\r\n    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n  File \"/<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1141, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1224, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n  File \"/<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 511, in call\r\n    ctx=ctx)\r\n  File \"/<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError:  slice index 2 of dimension 0 out of bounds.\r\n\t [[{{node while_2/cond/_101/strided_slice}}]] [Op:__inference_tile_nd_ragged2_1422]\r\n\r\nFunction call stack:\r\ntile_nd_ragged2\r\n```\r\n\r\n", "@mdanatg what we need is for dynamic input -\r\neg - \r\nx= <tf.RaggedTensor [ [ [b'd1'], [b'e1', b'e2'] ]........ ]>\r\ntaking  (just for example)\r\nP = [b'd1']\r\nQ = [ [b'd1'], [b'e1', b'e2'] ]\r\n\r\nthe number of elements in P and Q can change . \r\n\r\nSo to reiterate if x 's 1st level list has N elements , y's 1st level list will also have N elements , the number of elements in 2nd level list of x's will also be same as y's 2nd level list . The number elements in third level of x's list can differ  in y's third level list . Also there is not 4th level list ever at all .\r\n\r\nSo can we make it more generalised ?\r\n(updated)", "@mdanatg  I found the bug in the code , instead of doing `return i2 < tf.cast(a1.nested_row_lengths()[0][i1], tf.int32)` we need to loop over number of elements in second array so `return i2 < tf.shape(a1.nested_row_lengths()[0])[0]`", "@mdanatg its not working in TF API , below is the code and error \r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n############### Function we wrote ############\r\n@tf.function\r\ndef tile_nd_ragged2(a, b):\r\n  # Need a sentinel, otherwise it's hard to give it the initial shape we need.\r\n  # We'll drop the sentinel at the end.\r\n  acc = tf.ragged.constant([[[]]], dtype=a.dtype)\r\n  \r\n  # Work one row at a time...\r\n  for i1 in range(len(a.nested_row_lengths()[0])):    # Should be able to write `for a1, b1 in zip(a, b)` soon.\r\n    a1 = a[i1]\r\n    b1 = b[i1]\r\n    # If the components have variable length, we can't use a TensorArray anymore,\r\n    # so use a RaggedTensor instead.\r\n    acc1 = tf.ragged.constant([[]], dtype=a.dtype)\r\n    def loop_test(i2, _):\r\n      return i2 < tf.shape(a1.nested_row_lengths()[0])[0]\r\n      #return i2 < tf.cast(a1.nested_row_lengths()[0][i1], tf.int32)\r\n    def loop_body(i2, acc1):\r\n      a2 = a1[i2]\r\n      b2 = b1[i2]\r\n      for _ in range(len(b2)):\r\n        acc1 = tf.concat([acc1, tf.expand_dims(a2, 0)], axis=0)\r\n      return i2 + 1, acc1\r\n\r\n    _, acc1 = tf.while_loop(loop_test, loop_body, [0, acc1],shape_invariants=[tf.TensorShape([]), tf.TensorShape([None, None])])\r\n    acc1 = acc1[1:]  # Drop the sentinel.\r\n    acc = tf.concat([acc, tf.expand_dims(acc1, 0)], axis=0)    # Add the row to the final result.\r\n  acc = acc[1:]  # Drop the sentinel.\r\n  return acc\r\n\r\n\r\n###############  export_input_fn ############\r\n\r\ndef export_input_fn():\r\n    serialized_tf_example = tf.placeholder(dtype=tf.string, name =\"text\") \r\n\r\n    s1Split = tf.strings.split([serialized_tf_example],result_type=\"RaggedTensor\")\r\n    result  = tile_nd_ragged2(s1Split,s1Split)\r\n    result_tf = result.to_tensor()[:1,:1,:1][0][0] ### just take very first element for simplicity\r\n    result_int = tf.strings.to_number(result_tf,out_type=tf.int32)\r\n    \r\n    features ={}\r\n    features[\"f1\"]=result_int\r\n    \r\n    reciever_tensor = {\"text\": serialized_tf_example}\r\n    return tf.estimator.export.ServingInputReceiver(features, reciever_tensor)\r\n\r\n########### Training ###############\r\nx_feature = tf.feature_column.numeric_column('f1')\r\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n      x = {\"f1\": np.array([1., 2., 3., 4.])},      # Input features\r\n      y = np.array([1.5, 3.5, 5.5, 7.5]),         # true labels\r\n      batch_size=1,\r\n      num_epochs=1,\r\n      shuffle=True)\r\n\r\nregressor = tf.estimator.LinearRegressor(feature_columns=[x_feature])\r\nregressor.train(input_fn=train_input_fn, steps=10)\r\n\r\nsamples = np.array([1])\r\npredict_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"f1\": samples},num_epochs=1,shuffle=False)\r\npredictions = list(regressor.predict(input_fn=predict_input_fn))\r\nprint(\"---------\")\r\nprint(predictions)\r\n\r\n\r\nprint(\"--------- training finished ---------\")\r\n\r\n\r\nregressor.export_saved_model(\"./model\",export_input_fn,as_text=False)\r\n```\r\n\r\n\r\nError is \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"saved_model.py\", line 73, in <module>\r\n    regressor.export_saved_model(\"./model\",export_input_fn,as_text=False)\r\n  File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 735, in export_saved_model\r\n    strip_default_attrs=True)\r\n  File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 859, in _export_all_saved_models\r\n    strip_default_attrs=strip_default_attrs)\r\n  File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 925, in _add_meta_graph_for_mode\r\n    input_receiver = input_receiver_fn()\r\n  File \"saved_model.py\", line 41, in export_input_fn\r\n    result  = tile_nd_ragged2(s1Split,s1Split)\r\n  File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 449, in __call__\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 392, in _initialize\r\n    *args, **kwds))\r\n  File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1847, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2147, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2038, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 335, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 905, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nAttributeError: in converted code:\r\n\r\n    saved_model.py:19 loop_test  *\r\n        return i2 < tf.shape(a1.nested_row_lengths()[0])[0]\r\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py:2675 while_loop\r\n        back_prop=back_prop)\r\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/while_v2.py:153 while_loop\r\n        add_control_dependencies=add_control_dependencies)\r\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py:915 func_graph_from_py_func\r\n        func_outputs = python_func(*func_args, **func_kwargs)\r\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/while_v2.py:132 wrapped_cond\r\n        pred = cond(*_pack_sequence_as(orig_loop_vars, args))\r\n    /var/folders/zl/fgh1kxxj7612sw7nv3v6c7hw0000gn/T/tmpl9mey_a1.py:30 loop_test\r\n        retval__1 = loop_test_scope.mark_return_value(i2 < ag__.converted_call(tf.shape, loop_test_scope.callopts, (ag__.converted_call(a1.nested_row_lengths, loop_test_scope.callopts, (), None, loop_test_scope)[0],), None, loop_test_scope)[0])\r\n\r\n    AttributeError: 'Tensor' object has no attribute 'nested_row_lengths'\r\n```\r\n\r\nIts complaining at `return i2 < tf.shape(a1.nested_row_lengths()[0])[0]` but a1 is suppose to be RaggedTensor but its coming to be Tensor. \r\n\r\nWhat do we need to change here ? ", "It's possible that `a[i1]` returns a Tensor instead of Ragged when it can (that is when all dimensions are equal). Add a few print statements to see what you get. You can handle both by adding an `if isinstance(a1, tf.RaggedTensor):`.", "@mdanatg  I tried as you suggested but it still fails but now giving different error \r\n\r\n\r\n```\r\n    saved_model.py:14 tile_nd_ragged2  *\r\n        for i1 in range(len(a.nested_row_lengths()[0])):    # Should be able to write `for a1, b1 in zip(a, b)` soon.\r\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/operators/control_flow.py:315 for_stmt\r\n        composite_symbol_names)\r\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/operators/control_flow.py:478 _tf_range_for_stmt\r\n        opts=opts,\r\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/operators/control_flow.py:769 _tf_while_stmt\r\n        aug_init_vars, **opts)\r\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py:2675 while_loop\r\n        back_prop=back_prop)\r\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/while_v2.py:234 while_loop\r\n        len_orig_loop_vars], expand_composites=True))\r\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/while_v2.py:1068 _check_shapes_compat\r\n        \"specify a less-specific shape.\" % (input_t.name, shape, t.shape))\r\n\r\n    ValueError: Input tensor 'RaggedConstant/Const_1:0' enters the loop with shape (2,), but has shape (3,) after one iteration. To allow the shape to vary across iterations, use the `shape_invariants` argument of tf.while_loop to specify a less-specific shape.\r\n```\r\n\r\nCode :- \r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ntf.enable_eager_execution()\r\n\r\n############### Function we wrote ############\r\n@tf.function\r\ndef tile_nd_ragged2(a, b):\r\n  # Need a sentinel, otherwise it's hard to give it the initial shape we need.\r\n  # We'll drop the sentinel at the end.\r\n  acc = tf.ragged.constant([[[]]], dtype=a.dtype)\r\n  \r\n  # Work one row at a time...\r\n  for i1 in range(len(a.nested_row_lengths()[0])):    # Should be able to write `for a1, b1 in zip(a, b)` soon.\r\n    \r\n    a1 = a[i1]\r\n    b1 = b[i1]\r\n\r\n    # If the components have variable length, we can't use a TensorArray anymore,\r\n    # so use a RaggedTensor instead.\r\n    acc1 = tf.ragged.constant([[]], dtype=a.dtype)\r\n    def loop_test(i2, _):\r\n    \tshape = None\r\n    \tif isinstance(a1, tf.RaggedTensor):\r\n    \t\tprint(\" --- ragged --- \")\r\n    \t\tprint(a1)\r\n    \t\tprint(shape)\r\n    \t\tshape = tf.shape(a1.nested_row_lengths()[0])[0]\r\n    \telse:\r\n    \t\tprint(\" --- tensor --- \")\r\n    \t\tprint(a1)\r\n    \t\t\r\n    \t\tshape = tf.shape(a1)[0]\r\n    \t\tprint(tf.shape(a1))\r\n    \t\tprint(shape)\r\n\r\n    \treturn i2 < shape\r\n      #return i2 < tf.cast(a1.nested_row_lengths()[0][i1], tf.int32)\r\n    def loop_body(i2, acc1):\r\n      #print(a1[i2])\r\n      a2 = a1[i2]\r\n      b2 = b1[i2]\r\n\r\n      for _ in range(len(b2)):\r\n        acc1 = tf.concat([acc1, tf.expand_dims(a2, 0)], axis=0)\r\n      return i2 + 1, acc1\r\n\r\n    _, acc1 = tf.while_loop(loop_test, loop_body, [0, acc1],shape_invariants=[tf.TensorShape([]), tf.TensorShape([None, None])])\r\n    acc1 = acc1[1:]  # Drop the sentinel.\r\n    acc = tf.concat([acc, tf.expand_dims(acc1, 0)], axis=0)    # Add the row to the final result.\r\n  acc = acc[1:]  # Drop the sentinel.\r\n  return acc\r\n\r\n\r\n###############  export_input_fn ############\r\n\r\n# def export_input_fn():\r\n#     serialized_tf_example = tf.placeholder(dtype=tf.string, name =\"text\") \r\n\r\n#     s1Split = tf.strings.split([serialized_tf_example],result_type=\"RaggedTensor\")\r\n#     result  = tile_nd_ragged2(s1Split,s1Split)\r\n#     result_tf = result.to_tensor()[:1,:1,:1][0][0] ### just take very first element for simplicity\r\n#     result_int = tf.strings.to_number(result_tf,out_type=tf.int32)\r\n    \r\n#     features ={}\r\n#     features[\"f1\"]=result_int\r\n    \r\n#     reciever_tensor = {\"text\": serialized_tf_example}\r\n#     return tf.estimator.export.ServingInputReceiver(features, reciever_tensor)\r\n\r\ndef export_input_fn():\r\n    serialized_tf_example = tf.placeholder(dtype=tf.string, shape=(None), name =\"text\") \r\n\r\n    s1Split = tf.strings.split([serialized_tf_example],result_type=\"RaggedTensor\")\r\n    s1Split = tf.strings.split(s1Split,sep='@',result_type=\"RaggedTensor\")\r\n    result  = tile_nd_ragged2(s1Split,s1Split)\r\n    result_tf = result.to_tensor()[:1,:,:][0][0]\r\n    #result_tf = result.to_tensor()\r\n    result_int = tf.strings.to_number(result_tf,out_type=tf.int32)\r\n\r\n    features ={}\r\n    features[\"f1\"]=result_int ### this will be tf.Tensor([1], shape=(1,), dtype=int32)\r\n    \r\n    reciever_tensor = {\"text\": serialized_tf_example}\r\n    print(reciever_tensor)   \r\n    return tf.estimator.export.ServingInputReceiver(features, reciever_tensor)\r\n\r\n########### Training ###############\r\nx_feature = tf.feature_column.numeric_column('f1')\r\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n      x = {\"f1\": np.array([1., 2., 3., 4.])},      # Input features\r\n      y = np.array([1.5, 3.5, 5.5, 7.5]),         # true labels\r\n      batch_size=1,\r\n      num_epochs=1,\r\n      shuffle=True)\r\n\r\nregressor = tf.estimator.LinearRegressor(feature_columns=[x_feature])\r\nregressor.train(input_fn=train_input_fn, steps=10)\r\n\r\nsamples = np.array([1])\r\npredict_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"f1\": samples},num_epochs=1,shuffle=False)\r\npredictions = list(regressor.predict(input_fn=predict_input_fn))\r\nprint(\"---------\")\r\nprint(predictions)\r\n\r\n\r\nprint(\"--------- training finished ---------\")\r\n\r\n\r\nregressor.export_saved_model(\"./model\",export_input_fn,as_text=False)\r\n```\r\n\r\nChanges in code \r\n1. Added specific handling as you suggested \r\n2. bit changed `export_input_fn` \r\nAlso why does the code fail when we replace it by placeholders ? \r\n\r\n@mdaley kindly please advice ", "I ran the code in TF 2 where the error message is cleaner. Looks like shape invariants once more, so the outer loop needs to be rewritten for TF 1 as well. Unfortunately that makes the code quite ugly now:\r\n\r\n```\r\n@tf.function\r\ndef tile_nd_ragged2(a, b):\r\n  # Need a sentinel, otherwise it's hard to give it the initial shape we need.\r\n  # We'll drop the sentinel at the end.\r\n  acc = tf.ragged.constant([[[]]], dtype=a.dtype)\r\n  \r\n  print('acc is', acc)\r\n  # Work one row at a time...\r\n  # for i1 in tf.range(len(a.nested_row_lengths()[0])):    # Should be able to write `for a1, b1 in zip(a, b)` soon.\r\n  def outer_loop_test(i1, _):\r\n    return i1 < len(a.nested_row_lengths()[0])\r\n\r\n  def outer_loop_body(i1, acc):\r\n    \r\n    a1 = a[i1]\r\n    b1 = b[i1]\r\n\r\n    # If the components have variable length, we can't use a TensorArray anymore,\r\n    # so use a RaggedTensor instead.\r\n    acc1 = tf.ragged.constant([[]], dtype=a.dtype)\r\n    def loop_test(i2, _):\r\n    \tshape = None\r\n    \tif isinstance(a1, tf.RaggedTensor):\r\n    \t\tprint(\" --- ragged --- \")\r\n    \t\tprint(a1)\r\n    \t\tprint(shape)\r\n    \t\tshape = tf.shape(a1.nested_row_lengths()[0])[0]\r\n    \telse:\r\n    \t\tprint(\" --- tensor --- \")\r\n    \t\tprint(a1)\r\n    \t\t\r\n    \t\tshape = tf.shape(a1)[0]\r\n    \t\tprint(tf.shape(a1))\r\n    \t\tprint(shape)\r\n\r\n    \treturn i2 < shape\r\n      #return i2 < tf.cast(a1.nested_row_lengths()[0][i1], tf.int32)\r\n    def loop_body(i2, acc1):\r\n      #print(a1[i2])\r\n      a2 = a1[i2]\r\n      b2 = b1[i2]\r\n\r\n      for _ in range(len(b2)):\r\n        acc1 = tf.concat([acc1, tf.expand_dims(a2, 0)], axis=0)\r\n      return i2 + 1, acc1\r\n\r\n    _, acc1 = tf.while_loop(loop_test, loop_body, [0, acc1],shape_invariants=[tf.TensorShape([]), tf.TensorShape([None, None])])\r\n    acc1 = acc1[1:]  # Drop the sentinel.\r\n    acc = tf.concat([acc, tf.expand_dims(acc1, 0)], axis=0)    # Add the row to the final result.\r\n\r\n    return i1 + 1, acc\r\n\r\n  _, acc = tf.while_loop(\r\n      outer_loop_test, outer_loop_body,\r\n      [0, acc],\r\n      shape_invariants=[\r\n                        tf.TensorShape([]),\r\n                        tf.TensorShape([None, None, None])\r\n                        ]\r\n      )\r\n  acc = acc[1:]  # Drop the sentinel.\r\n  return acc\r\n```", "@mdanatg  thank you  .  Looks like it working now . \r\n\r\nI have a question when we tested using placeholder why it failed and not when we input specific string ? ", "Glad to hear it! That's a good question. I think in the first tests `a.nested_row_lengths()[0]` was returning a Python value, the the loop was being unrolled in Python. Unrolled loops don't have the shape restrictions of `tf.while_loop` (they just repeat the graph ops). Once you added the placeholder, the result of that became a Tensor, causing the loop to become a `tf.while_loop`, which is more restrictive. Anyway, we should have written the loop with `tf.range` in the first place to avoid that unrolling.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38375\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38375\">No</a>\n", "@mdanatg  we are using `range(len(b2)):` , does len(b2)  need to  be changed to tf api ? ", "Only if it gives an error. len(b2) works if b2 is a Tensor, but currently fails if it's a RaggedTensor. Working to support both.", "@mdanatg  thank you , what to use for RaggedTensor ? ", "b2.nrows() should work.", "That said, check out [this comment](https://github.com/tensorflow/tensorflow/issues/38438#issuecomment-613200554) for a shorter solution that users some special tricks from RaggedTensors.", "@mdaley which comment i should look at ? I only opened this 38412 ticket :) ? ", "Oops, wrong link - updated.", "@mdanatg  thank you "]}, {"number": 38374, "title": "[Intel MKL] add attr to fix ut", "body": "This PR fix ut //tensorflow/python/debug:dumping_callback_test, //tensorflow/python/keras/mixed_precision/experimental:layer_correctness_test,\r\n//tensorflow/python/keras/layers:convolutional_test\r\n due to lack of attr 'explicit_paddings' ", "comments": []}, {"number": 38373, "title": "Add several Python api-doc examples", "body": "Added examples for several Python API functions.", "comments": ["I see, sorry I was not aware of that. Is there any way I can make the examples appear only in the Python API docs? Then I could perhaps try to amend it. Thank you for your feedback. ", "See 8582a58", "@tomasvr Can you please check reviewer comments and keep us posted. Thanks!\r\n", "@gbaned I removed the examples from the general API for now. I'm currently preoccupied with new courses and not quite sure how to correctly add the remaining examples.", "Can you fix the Ubuntu CPU test, please?", "@tomasvr Can you please check @mihaimaruseac comments and keep us posted. Thanks!", "@gbaned I believe it's fixed now.", "@tomasvr Can you please address Ubuntu Sanity errors? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on."]}, {"number": 38372, "title": "Windows 10 shuts down when using GPU (tf 2.1.0 with tf.keras and tf 1.14 with keras)", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes, custom code\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Windows 10 64 bit\r\n- TensorFlow installed from (source or\r\nbinary): Tensorflow was installed using `pip install tensorflow`\r\n- TensorFlow version (use command below): 2.1.0 (but occurs on 1.14, 1.15, 2.0 as well)\r\n- Python version: Occurs on Python 3.5, 3.6, 3.7\r\n- Bazel\r\nversion (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: I have the following installed: CUDA 9.2, CUDA 10.0, CUDA 10.1, CUDA 10.2 (with appropriate CuDNN versions)\r\n- GPU model and memory: Occurs on new RTX 2080 ti (11 GB) and my old GTX 1070 (8GB)\r\n\r\n**Describe the current behavior**: When using either tensorflow or tensorflow-gpu with two of my graphics cards (1070, 2080 ti) my system shuts off completely (no warning, as if I unplugged the power) if I train a model with a high batch size (~32-42). Specifically using tf.keras's Conv2D and CuDNNLSTM/CuDNNGRU. It does not seem to have any problems training a fully connected model with batch sizes up to 512. Everything is fine if I train my Conv2D model with a batch size of 16 or lower. Also, if I use nvidia-smi to limit my GPU's power consumption from stock 250W to 150W it works fine, but is very slow. However when it trains with 16 batch size, it goes above 200W regularly and has no problem.\r\n\r\n**Describe the expected behavior**: System does not shut off and should spit back a memory error if it's running out of memory.\r\n\r\n**Standalone code to reproduce the issue**:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport random\r\n\r\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\r\nfrom tensorflow.keras import Sequential\r\n\r\nnum_samples = 500\r\nh = 665\r\nw = 814\r\nc = 3\r\n\r\nx = np.random.rand(num_samples, h, w, c)\r\ny = [[0] * 4 for _ in range(num_samples)]\r\nfor i in range(len(y)):\r\n    y[i][random.randint(0, 3)] = 1\r\ny = np.array(y)\r\n\r\nkernel_size = (3, 3)\r\nmodel = Sequential()\r\nmodel.add(Conv2D(12, kernel_size, strides=1, activation='relu', input_shape=x.shape[1:]))\r\n\r\nmodel.add(Conv2D(24, kernel_size, strides=1, activation='relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\n\r\nmodel.add(Conv2D(48, kernel_size, strides=1, activation='relu'))\r\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\r\n\r\nmodel.add(Conv2D(64, kernel_size, strides=1, activation='relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\n\r\nmodel.add(Conv2D(12, kernel_size, strides=1, activation='relu'))\r\n\r\nmodel.add(Flatten())\r\nmodel.add(Dense(32, activation='relu'))\r\nmodel.add(Dense(64, activation='relu'))\r\nmodel.add(Dense(64, activation='relu'))\r\nmodel.add(Dense(4, activation='softmax'))\r\n\r\nmodel.compile(loss='categorical_crossentropy',\r\n              optimizer='adam',\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x, y, batch_size=64, epochs=500)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem: No errors are generated when it shuts down, it just cuts power immediately.\r\n\r\nExtra info: I've fully replaced my CPU, power supply, graphics card, motherboard, and RAM and this issue still occurs. PSU was purchased about a month ago brand new. Only thing that is the same in the system since it last occurred is my PC case, storage (SSDs), and the Windows 10 install.\r\n\r\nIt also *never* occurs in GPU and CPU stress tests and neither GPU or CPU overheats when it shuts down. It's always after the first epoch starts training, or just before.", "comments": ["I have tried in colab and i am able to reproduce the issue. I am seeing the error message (`Your session crashed after using all available RAM`).Thanks!", "Hmm, that sounds like it could be related to the amount of RAM Colab gives you, I'm not super experienced with Colab. But that script takes about 16GB to load the data into system memory then of course only 64 images are loaded into GPU memory, which should be fine", "Update: It seems if I use this model here instead of the one posted above, everything is fine up to batch size 64 while the one above shuts down my desktop immediately at batch size 32 (the below model is a snippet from my actual script I'm using to train an image model, I converted the one above into a standalone file however this model in the place of the model above also shuts down my PC):\r\n\r\n```\r\nmodel = Sequential()\r\nmodel.add(Conv2D(12, kernel_size, activation='relu', input_shape=self.cropped_shape))\r\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\r\n\r\nmodel.add(Conv2D(12, kernel_size, activation='relu'))\r\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\r\n\r\nmodel.add(Conv2D(24, kernel_size, activation='relu'))\r\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\r\n\r\nmodel.add(Conv2D(36, kernel_size, activation='relu'))\r\n\r\nmodel.add(Flatten())\r\nmodel.add(Dense(32, activation='relu'))\r\nmodel.add(Dense(64, activation='relu'))\r\nmodel.add(Dense(3, activation='softmax'))\r\n```", "@ShaneSmiskol Similar issue has been discussed [here](https://www.nvidia.com/en-us/geforce/forums/discover/260522/pc-auto-shuts-down-when-running-keras-tensorflow-backend-may-be-a-psu-issue-/). Please take a look at it.", "I've seen that and many more posts online about tensorflow or just gaming in general that shuts down their computer. However nearly all of them conclude that the PSU is the issue which is strange because it happened with my old 550W PSU as well as my brand new 700W Thermaltake PSU I just installed, so I don't understand what's going wrong here. I've seen the GPU power usage above 225W yet it trains fine if I set my batch size lower. It seems only specific models end up shutting down my PC and I can't find a pattern. Any tips would be appreciated, thanks", "Seems to be a GPU memory issue at runtime, let me add someone from runtime team with more insights. I can repro the error above on colab, which crash the session with \"Your session crashed after using all available RAM.\"", "Reassigning to Sanjoy to take a look at potential GPU memory issues. Thanks!", "I switched back to a different GTX 1070 and everything seems to be running fine as far as the random shut downs with high batch sizes. Not too sure why it occurred on two different card with a new PSU but not with this 1070. I'll report back if it ever occurs again", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38372\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38372\">No</a>\n"]}, {"number": 38371, "title": "Reduce code duplication tpu_embedding", "body": "I was browsing over the code of the `tpu_embedding.py` file and noticed some code duplication in the `_validate_generate_enqueue_ops_enqueue_datas_list` method which I thought could be simplified. \r\nAlso removed an unnecessary `else` statement.\r\n\r\nLet me know what you think, cheers!", "comments": ["I see the import/copybara check has failed, but I cannot retrieve any more information regarding this check. Could you please tell me what I can do to resolve this? Thank you in advance."]}, {"number": 38370, "title": "Cannot convert between a TensorFlowLite buffer with 76800 bytes and a Java Buffer with 230400 bytes", "body": "I modify the image classification code https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android to run inference on a float type grayscale bitmap with size (1, 120, 160, 1). However, an exception is throw regarding \"Cannot convert between a TensorFlowLite buffer with 76800 bytes and a Java Buffer with 230400 bytes\". I checked the input tensor image shape which is indeed (1, 120, 160, 1).\r\n\r\nBasically, java buffer is still assuming the input size is (1, 120, 160, 3). Therefore, what need be modified in order to run inference on grayscale image for image classification code? I feel like TensorImage class is somehow hard-coded for RBG image instead of grayscale one.\r\n\r\n    Process: org.tensorflow.lite.examples.classification, PID: 20064\r\n    java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 76800 bytes and a Java Buffer with 230400 bytes.\r\n        at org.tensorflow.lite.Tensor.throwIfShapeIsIncompatible(Tensor.java:402)\r\n        at org.tensorflow.lite.Tensor.throwIfDataIsIncompatible(Tensor.java:369)\r\n        at org.tensorflow.lite.Tensor.setTo(Tensor.java:187)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:150)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:314)\r\n        at org.tensorflow.lite.Interpreter.run(Interpreter.java:275)\r\n        at org.tensorflow.lite.examples.classification.tflite.Classifier.recognizeImage(Classifier.java:266)\r\n        at org.tensorflow.lite.examples.classification.ClassifierActivity$3.run(ClassifierActivity.java:98)\r\n        at java.lang.Thread.run(Thread.java:919)\r\n", "comments": ["Sorry, but yes the TensorImage class only supports RGB image. It's on our radar now but may not be very soon.", "Please let me know if you need further help, or you want to raise a general issue on grayscale support.", "Got it and thanks for the update! \r\n\r\nI just want to double check on the complexity of modifying the TensorImage source code to get grayscale image working to see whether it worths myself to spend time on that. Otherwise, I might simply raise a general issue on grayscale image to let you guys help resolve.", "I don't think it's too hard. Let me try it tomorrow and will let you know.", "Thanks. I feel this is related to this line in TensorImage.class by assuming the channel size is 3\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/support/java/src/java/org/tensorflow/lite/support/image/TensorImage.java#L302\r\n\r\nHowever, it will be nice that you can help confirm and let me know if I missed something else.\r\n\r\nThanks!", "That's one. Also the `TensorImage.load` methods, and the conversion code in `ImageConversions.java`.\r\n\r\nIt's not really difficult to modify to get it running, but design consistent interfaces might take longer.\r\n\r\nFor your usage, I'm curious about how you store a gray image in Android - or saying how you expect to convert the Bitmap to values. Are you just interested in one particular channel? Or you want to use formulas in https://en.wikipedia.org/wiki/Grayscale to compute the gray scale?\r\n\r\nThanks!", "Regarding `TensorImage.load`, I am loading a bitmap image. From what I see, https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/support/java/src/java/org/tensorflow/lite/support/image/TensorImage.java#L119, it should be fine. However, I see what you are saying that if I am going to load a int or float pixel array, I have to make an modification. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/support/java/src/java/org/tensorflow/lite/support/image/TensorImage.java#L151\r\n\r\nRegarding `ImageConversions.java`, I see I have to modify this method https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/support/java/src/java/org/tensorflow/lite/support/image/ImageConversions.java#L90 regarding couple place assuming that size is (w, h, 3).\r\n\r\nI am actually using use formulas in https://en.wikipedia.org/wiki/Grayscale to compute the gray scale to store grayscale image because our Neural Network is trained on grayscale images. \r\n\r\nAlso, one more question, is there a quick method to apply a patch on Gradle file because I never patch a Gradle file before.\r\n\r\nThanks a lot for the help! ", "Thanks for the info. We're considering the best way to support single channel images, so understanding usages are essential. That's really helpful.\r\n\r\nWhat do you mean by \"patching a gradle file\"? If you're talking about adding a new class or modifying existing class in a gradle dependency, I googled a bit and seems there are several different ways to do that. You may need to take a look and choose what's the best one for you.", "Sorry about the confusion and I was talking about modifying gradle dependency. I was initially wondering that whether I can directly apply a patch during the build to modify the necessary files. However, it looks like the easiest way is to add as an external library and the modify.\r\nhttps://stackoverflow.com/questions/16588064/how-do-i-add-a-library-project-to-android-studio/16639227#16639227\r\nhttps://stackoverflow.com/questions/30344053/how-do-you-edit-a-dependency-external-library-in-android-studio\r\n\r\nTake your time to figure out the best way to support multi/single channel images and thanks for the help!"]}, {"number": 38369, "title": "AttributeError: 'Tensor' object has no attribute '_in_graph_mode'", "body": "I am having an error: 'Tensor' object has no attribute '_in_graph_mode'. I've debugged the code, and I think it's in this GradientTape function, but I don't know why. If anyone knows, please help me! :)\r\n\r\n**System information** \r\n- TensorFlow version: 2.0 - '2.2.0-dev20200407'\r\n- OS Platform and Distribution: Linux Mint \r\n- Python version: Python 3.7.4\r\n\r\n**Describe the current behavior**\r\nI am trying to minimize a function using opt = tf.keras.optimizers.Adam() and I am getting a TypeError when I apply _opt.apply_gradients_.\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\n  ```\r\n\r\n def explain(\r\n        self,\r\n        validation_data,\r\n        model,\r\n        class_index,\r\n        layer_name=None,\r\n        colormap=cv2.COLORMAP_VIRIDIS,\r\n        image_weight=0.7,\r\n        _grid=True\r\n    ):\r\n\r\n\r\n# Returns: numpy.ndarray: Grid of all the inverted image or 4D array (batch_size, height, width, channels)\r\n       \r\n        tf.executing_eagerly()\r\n        images, _ = validation_data\r\n\r\n        if layer_name is None:\r\n            layer_name = self.infer_target_layer(model)\r\n        \r\n        inverted_image = InvertedImage.get_optimize_image(\r\n            images, model, class_index, layer_name\r\n        )\r\n\r\n        if _grid:\r\n            return grid_display(inverted_image)\r\n        else:\r\n            return inverted_image\r\n\r\n @staticmethod\r\n  def infer_target_layer(model):\r\n   \r\n       # Returns: str: Name of the target layer\r\n\r\n        for layer in reversed(model.layers):\r\n            # Select closest 4D layer to the end of the network.\r\n            if len(layer.output_shape) == 4 and layer.name.count('conv') > 0:\r\n                return layer.name\r\n\r\n        raise ValueError(\r\n            \"Model does not seem to contain 4D layer. Inverted image cannot be applied.\"\r\n        )\r\n\r\n @tf.function\r\n  def get_optimize_image(images, model, class_index, layer_name):\r\n   \r\n        grad_model = tf.keras.models.Model(\r\n            [model.inputs], [model.get_layer(layer_name).output]\r\n        )\r\n\r\n        opt = tf.keras.optimizers.SGD(learning_rate=1-4, momentum=0.9)\r\n        dtype = model.get_layer(layer_name).output.dtype\r\n        tensor_image = tf.convert_to_tensor(images)\r\n\r\n        opt_img = tf.Variable(1e-1 * tf.random.normal((tensor_image.shape[0], tensor_image.shape[1], tensor_image.shape[2], tensor_image.shape[3])), trainable=True)\r\n\r\n        steps = 50\r\n        for i in range(steps):\r\n             with tf.GradientTape() as tape:\r\n                \r\n                inverted_feature = tf.cast(opt_img, dtype)\r\n                content_feature = tf.cast(images, dtype)\r\n                    \r\n                conv_inverted_outputs = grad_model(inverted_feature)\r\n                conv_content_outputs = grad_model(content_feature)\r\n            \r\n                loss = InvertedImage.get_loss(conv_content_outputs, conv_inverted_outputs, content_feature, inverted_feature)\r\n                #print(\"Initial loss: {:.3f}\".format(loss))\r\n\r\n            grad = tape.gradient(loss, [conv_inverted_outputs, conv_content_outputs])\r\n            print(grad)\r\n            processed_grads = [g for g in grad]\r\n            opt.apply_gradients(zip(processed_grads, [conv_inverted_outputs, conv_content_outputs]))\r\n\r\nreturn opt_img\r\n```\r\n\r\n**Loss function**\r\n```\r\ndef get_loss(conv_content_outputs, conv_inverted_outputs, content_feature, inverted_feature):\r\n        euclidian = tf.norm(conv_content_outputs - conv_inverted_outputs, ord='euclidean') / tf.norm(conv_content_outputs, ord='euclidean')\r\n        reg_alpha = 1e-7 * tf.math.reduce_sum(tf.norm(inverted_feature, ord=6))\r\n        total_variation = 1e-8 * tf.math.reduce_sum(tf.image.total_variation(content_feature+inverted_feature))\r\n\r\n        return euclidian + reg_alpha + total_variation \r\n```\r\n\r\n**Traceback** \r\n\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/local/lib/python3.7/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/helena/.vscode/extensions/ms-python.python-2020.2.64397/pythonFiles/lib/python/new_ptvsd/wheels/ptvsd/__main__.py\", line 45, in <module>\r\n    cli.main()\r\n  File \"/home/helena/.vscode/extensions/ms-python.python-2020.2.64397/pythonFiles/lib/python/new_ptvsd/wheels/ptvsd/../ptvsd/server/cli.py\", line 361, in main\r\n    run()\r\n  File \"/home/helena/.vscode/extensions/ms-python.python-2020.2.64397/pythonFiles/lib/python/new_ptvsd/wheels/ptvsd/../ptvsd/server/cli.py\", line 203, in run_file\r\n    runpy.run_path(options.target, run_name=\"__main__\")\r\n  File \"/usr/local/lib/python3.7/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"/usr/local/lib/python3.7/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"/usr/local/lib/python3.7/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/helena/Documents/LAR_Celesc/lar-computer-vision/objdet-api/test_inverted_image.py\", line 20, in <module>\r\n    data, model, class_index=tabby_cat_class_index, layer_name=\"block5_conv3\"\r\n  File \"/home/helena/Documents/LAR_Celesc/lar-computer-vision/objdet-api/tf_explain/core/inverted_image.py\", line 54, in explain\r\n    images, model, class_index, layer_name\r\n  File \"/home/helena/Documents/LAR_Celesc/larenv/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/helena/Documents/LAR_Celesc/larenv/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 615, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/home/helena/Documents/LAR_Celesc/larenv/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 497, in _initialize\r\n    *args, **kwds))\r\n  File \"/home/helena/Documents/LAR_Celesc/larenv/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2389, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/helena/Documents/LAR_Celesc/larenv/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/helena/Documents/LAR_Celesc/larenv/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/helena/Documents/LAR_Celesc/larenv/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/helena/Documents/LAR_Celesc/larenv/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/helena/Documents/LAR_Celesc/larenv/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 968, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nAttributeError: in converted code:\r\n    /home/helena/Documents/LAR_Celesc/lar-computer-vision/objdet-api/tf_explain/core/inverted_image.py:125 get_optimize_image  *\r\n        opt.apply_gradients(grads_and_vars)\r\n    /home/helena/Documents/LAR_Celesc/larenv/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:434 apply_gradients\r\n        self._create_slots(var_list)\r\n    /home/helena/Documents/LAR_Celesc/larenv/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/gradient_descent.py:100 _create_slots\r\n        self.add_slot(var, \"momentum\")\r\n    /home/helena/Documents/LAR_Celesc/larenv/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:574 add_slot\r\n        var_key = _var_key(var)\r\n    /home/helena/Documents/LAR_Celesc/larenv/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:1065 _var_key\r\n        if var._in_graph_mode:\r\n\r\n    AttributeError: 'Tensor' object has no attribute '_in_graph_mode'\r\n\r\n```", "comments": ["@helenabdr, Please provide the complete standalone code to reproduce the issue. Thanks", "@helenabdr,\r\nplease update on the above comment", "> @helenabdr,\r\n> please update on the above comment\r\n@Saduf2019, this is the complete code that I am using. \r\n", "@helenabdr \r\ni ran the code shared by you and face [this error](https://colab.sandbox.google.com/gist/Saduf2019/294176cf5f322c2bde08fea72527d888/38369.ipynb), please share a colab gist or code such that we can replicate the issue faced by you.", "@Saduf2019 \r\nYou're right, here's Colab's link:https://colab.research.google.com/drive/1QILUHXwpCbLpNh79pDGYxuhnwd1pplrR . But I managed to fix the error, changing the Gradient Tape, but another error appeared as I described in the notebook. Thanks :)\r\n", "@helenabdr \r\ni have tried to replicate the code shared in nightly and face [this error](https://colab.sandbox.google.com/gist/Saduf2019/00a761f99042a9511654abced38d746b/38776.ipynb)", "> @helenabdr\r\n> i have tried to replicate the code shared in nightly and face [this error](https://colab.sandbox.google.com/gist/Saduf2019/00a761f99042a9511654abced38d746b/38776.ipynb)\r\n\r\n@Saduf2019 \r\nYes, now there is another error :/. But if you remove _@tf.function_ from the code, it will run. Not perfectly or for something useful, but it will execute. ", "The error message did not get saved in the colab - could you copy it here? At a glance, I recommend creating the model and the optimizer outside the tf.function. You will definitely get errors because tf.function doesn't let you create variables inside it (and whenever you create a new model object, it creates a new set of variables).", "Just dropping by to say that I encountered this issue as well,and I'm using `2.2.0rc3`. I'll try to put together a reproduceable snippet/colab later today", "@mdanatg Here is the error trace and [here](https://colab.research.google.com/gist/jvishnuvardhan/38d375eed6af5933282c8907def34971/project1.ipynb) is the colab gist.\r\n\r\n```\r\n_________________________________________________________________\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-6-f1109448c319> in <module>()\r\n     18     # Compute InvertedImage on VGG16\r\n     19     grid = explainer.explain(\r\n---> 20         data, model, class_index=tabby_cat_class_index, layer_name=\"block5_conv3\"\r\n     21     )\r\n     22     plt.figure(figsize=(10,10))\r\n\r\n8 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    966           except Exception as e:  # pylint:disable=broad-except\r\n    967             if hasattr(e, \"ag_error_metadata\"):\r\n--> 968               raise e.ag_error_metadata.to_exception(e)\r\n    969             else:\r\n    970               raise\r\n\r\nValueError: in user code:\r\n\r\n    <ipython-input-4-9f3866d1d8a5>:93 get_optimize_image  *\r\n        grad_model = tf.keras.models.Model(\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:205 __init__  **\r\n        self._init_batch_counters()\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py:456 _method_wrapper\r\n        result = method(self, *args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:212 _init_batch_counters\r\n        self._train_counter = variables.Variable(0, dtype='int64', aggregation=agg)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:261 __call__\r\n        return cls._variable_v2_call(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:255 _variable_v2_call\r\n        shape=shape)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:66 getter\r\n        return captured_getter(captured_previous, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:622 invalid_creator_scope\r\n        \"tf.function-decorated function tried to create \"\r\n\r\n    ValueError: tf.function-decorated function tried to create variables on non-first call.\r\n```", "The problem is in the following line of code:\r\n\r\n```\r\n          opt.apply_gradients(zip(processed_grads, [conv_inverted_outputs, conv_content_outputs]))\r\n```\r\n\r\nHere conv_inverted_inputs and conv_content_outputs are not tf.Variable, and TF cannot apply optimizer updates on things which are not tf.Variables.\r\n\r\nThat said the error message could be better. @tanzhenyu can you work on the error message, by maybe checking for tensors earlier?", "> The problem is in the following line of code:\r\n> \r\n> ```\r\n>           opt.apply_gradients(zip(processed_grads, [conv_inverted_outputs, conv_content_outputs]))\r\n> ```\r\n> \r\n> Here conv_inverted_inputs and conv_content_outputs are not tf.Variable, and TF cannot apply optimizer updates on things which are not tf.Variables.\r\n> \r\n> That said the error message could be better. @tanzhenyu can you work on the error message, by maybe checking for tensors earlier?\r\n\r\nHi @alextp , I fixed this error by changing to the following code:\r\n\r\n`opt.apply_gradients(zip(grad, grad_model.trainable_variables))`\r\n\r\nO link the updated link is [here](https://colab.research.google.com/drive/1QILUHXwpCbLpNh79pDGYxuhnwd1pplrR), as @jvishnuvardhan mentioned. But as said earlier, if you remove @tf.function from the code, it will run.\r\n", "This is a well-documented limitation where you're not allowed to unconditionally create new keras models (including new variables) inside a tf.function.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38369\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38369\">No</a>\n", "> The error message did not get saved in the colab - could you copy it here? At a glance, I recommend creating the model and the optimizer outside the tf.function. You will definitely get errors because tf.function doesn't let you create variables inside it (and whenever you create a new model object, it creates a new set of variables).\r\n\r\n@mdanatg, I don't know if I understood correctly. Should I set  `opt = tf.keras.optimizers.SGD(learning_rate=1-4, momentum=0.9)` outside _@tf.function_?", "@helenabdr yes, and pass it as argument to the function. Although for a simple, stateless optimizer like SGD it might not be necessary. But definitely do that with the model."]}, {"number": 38368, "title": "Cadence NNLib:Fixed various unit test failures", "body": "Fixed person detection test,max pool and svdf unit tests\r\n\r\nSigned-off-by: Bhanu Prakash Bandaru Venkata <bhanup@cadence.com>", "comments": []}, {"number": 38367, "title": "Memory Growth and Memory Limit incompatibility", "body": "**System information** \r\n- Custom Code\r\n- OS Platform and Distribution: Windows 10 Home 1909\r\n- TensorFlow installed with pip\r\n- TensorFlow version: v2.1.0-rc2-17-ge5bf8de410, 2.1.0\r\n- Python version: 3.7.6\r\n- CUDA/cuDNN version: CUDA 10.1, cuDNN 7.6.5\r\n- GPU model and memory: NVIDIA GTX 1050 notebook, 4GB memory\r\n\r\n**Describe the current behavior**\r\nTensorflow \"forgets\" about the setting on memory_growth once you create a vritual device to further limit maximum VRAM usage.\r\n\r\n**Describe the expected behavior**\r\nIn practice I would like to be able to both limit maximum VRAM usage AND to tell TF to only use the VRAM that it needs with the memory_growth option. This is something that I was able to do in TF 1.x by passing an appropriate config to a Session, but it doesn't seem like it's possible in TF 2.x (without using 1.x code that is...)\r\n\r\n**Standalone code to reproduce the issue** \r\nhttps://colab.research.google.com/drive/1MJgZiHernOjT_3vB57YMnjjTRJhgpspo\r\n", "comments": ["As mentioned [here](https://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth#used-in-the-notebooks) If memory growth is enabled for a PhysicalDevice, the runtime initialization will not allocate all memory on the device. Memory growth cannot be configured on a PhysicalDevice with virtual devices configured.", "I understand, but is it possible as it was before to both set memory growth and a hard limit on the maximum amount of memory that it can occupy", "As per the documentation, it is not possible @FMalerba ", "Thank you very much for the info ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38367\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38367\">No</a>\n"]}, {"number": 38366, "title": "Strange behavior of ODE solving toy model", "body": "**System information** \r\n- OS Platform and Distribution Windows 10\r\n- TensorFlow installed from source TensorFlow 2.2.0 \r\n- Python version: 3.8\r\n- CUDA/cuDNN version: 10.1\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nimport matplotlib.pyplot as plt\r\n\r\nN = 1000\r\nx = tf.convert_to_tensor(np.linspace(0, 1, num=N), dtype=tf.float32)\r\n\r\nwith tf.GradientTape() as t:\r\n    x_inp = keras.Input(shape=(1,), name='indvar')\r\n    t.watch(x_inp)\r\n    dense = layers.Dense(100, activation='tanh')(x_inp)\r\n    dense = layers.Dense(100, activation='tanh')(dense)\r\n    dense = layers.Dense(100, activation='tanh')(dense)\r\n    dense = layers.Dense(100, activation='tanh')(dense)\r\n    dense = layers.Dense(100, activation='tanh')(dense)\r\n    dense = layers.Dense(100, activation='tanh')(dense)\r\n    dense = layers.Dense(100, activation='tanh')(dense)\r\n    yhat = layers.Dense(1, name='y')(dense)\r\n    dyhat = t.gradient(yhat, x_inp)\r\n    model = keras.Model(inputs=x_inp, outputs=[yhat, dyhat])\r\n    opt=tf.keras.optimizers.Adam()\r\n    model.compile(optimizer=opt, loss='MSE')\r\n    model.summary()\r\n    model.fit(x=x, y=[x, np.sin(10*x)], epochs=10000, batch_size=N)\r\n    y, dy = model(x)\r\n    plt.plot(x, np.array(y))\r\n    plt.plot(x, np.array(dy))\r\n    plt.show()\r\n```\r\nI try to make model output y=x, while y'=sin(10*x). Surely this is impossible so the expected behavior of the model is NOT converge. However model converges and have both y=x and y'=sin(10*x) satisfied, however it is impossible.    ", "comments": ["No, in my PC it works well. I found even smaller toy example that shows the bug\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nimport matplotlib.pyplot as plt\r\n\r\nN = 1000\r\nx = np.linspace(0, 1, num=N)\r\n\r\nwith tf.GradientTape() as t:\r\n    x_inp = keras.Input(shape=(1,), name='indvar')\r\n    t.watch(x_inp)\r\n    reg=tf.keras.regularizers.l2(l=0.01)\r\n    dense = layers.Dense(100, activation='tanh', kernel_regularizer=reg)(x_inp)\r\n    for i in range(2):\r\n        dense = layers.Dense(100, activation='tanh', kernel_regularizer=reg)(dense)\r\n    yhat = layers.Dense(1, name='y')(dense)\r\n    dyhat = t.gradient(yhat, x_inp)\r\n    model = keras.Model(inputs=x_inp, outputs=[yhat, dyhat])\r\n    opt=tf.keras.optimizers.Adam()\r\n    model.compile(optimizer=opt, loss=[lambda y, y_act: 0,'MSE'])\r\n    model.summary()\r\n    model.fit(x=x, y=[x, x*x], epochs=1, batch_size=N)  #with this line,  numerical derivative will differ from analytical\r\n    y, dy = model(x)\r\n    dy_num = np.gradient(np.array(y).squeeze(), x)\r\n    plt.plot(x, y)\r\n    plt.plot(x, dy)\r\n    plt.plot(x, dy_num)\r\n    plt.legend(['y', 'dy', 'dynum'])\r\n    plt.show()\r\n```", "@yarik1988 \r\n\r\nI tried to reproduce the issue in colab .Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/bfe689e64e1026192a4b554929d0fb5a/untitled26.ipynb). Is this the expected behavior.Thanks!", "> @yarik1988\r\n> \r\n> I tried to reproduce the issue in colab .Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/bfe689e64e1026192a4b554929d0fb5a/untitled26.ipynb). Is this the expected behavior.Thanks!\r\n\r\nFrom the mathematical point of view this behavior is neither logical neither expected. I created gist of my another example that shows the problem very clearly\r\nhttps://colab.research.google.com/gist/yarik1988/92890b0d9a690773f2c1204d348f8478/untitled0.ipynb#scrollTo=JOB6TP8OuLmw", "However with this line commented\r\n`model.fit(x=x, y=[x, x*x], epochs=1, batch_size=N)`\r\nnumerical derivative coincides with analytical one, which is logical.", "While trying to reproduce your issue in Tf Nightly faced some error, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/71d635e442d48d6fa41e37977975ea3f/38366.ipynb). Thanks!", "@yarik1988, Sorry for the late response. Is this still an issue for you?\r\n\r\nWhile trying to reproduce your issue in `TF v2.7` and `tf-nightly (2.8.0-dev20211124)` faced some error, please find the [gist here](https://colab.research.google.com/gist/chunduriv/d8a0cf48da88aa06f47c2171dd6fd68a/38366.ipynb). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38366\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38366\">No</a>\n"]}, {"number": 38365, "title": "AutoGraph Warning on Tensorflow-gpu==2.0.0b1", "body": "Running the Image Captioning tutorial (https://www.tensorflow.org/tutorials/text/image_captioning) on google colab with tensorflow-gpu==2.0.0b1 produces the next warning.\r\n\r\n\r\nWARNING: Entity <function standard_gru at 0x7fa68b50b488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x7fa68b50b488>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <function cudnn_gru at 0x7fa68b50b730> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x7fa68b50b730>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING: Entity <function cudnn_gru at 0x7fa68b50b730> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x7fa68b50b730>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7fa60d01b588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7fa60d01b588>>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING: Entity <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7fa60d01b588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7fa60d01b588>>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7fa60d0772b0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7fa60d0772b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7fa60d0772b0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7fa60d0772b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <function standard_gru at 0x7fa68b50b488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x7fa68b50b488>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING: Entity <function standard_gru at 0x7fa68b50b488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x7fa68b50b488>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <function cudnn_gru at 0x7fa68b50b730> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x7fa68b50b730>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING: Entity <function cudnn_gru at 0x7fa68b50b730> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x7fa68b50b730>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7fa60d01b588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7fa60d01b588>>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING: Entity <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7fa60d01b588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7fa60d01b588>>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7fa60d0772b0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7fa60d0772b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7fa60d0772b0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7fa60d0772b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <function standard_gru at 0x7fa68b50b488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x7fa68b50b488>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING: Entity <function standard_gru at 0x7fa68b50b488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x7fa68b50b488>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <function cudnn_gru at 0x7fa68b50b730> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x7fa68b50b730>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING: Entity <function cudnn_gru at 0x7fa68b50b730> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x7fa68b50b730>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7fa60d01b588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7fa60d01b588>>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING: Entity <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7fa60d01b588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7fa60d01b588>>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7fa60d0772b0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7fa60d0772b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7fa60d0772b0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7fa60d0772b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <function standard_gru at 0x7fa68b50b488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x7fa68b50b488>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING: Entity <function standard_gru at 0x7fa68b50b488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x7fa68b50b488>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <function cudnn_gru at 0x7fa68b50b730> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x7fa68b50b730>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING: Entity <function cudnn_gru at 0x7fa68b50b730> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x7fa68b50b730>: AttributeError: module 'gast' has no attribute 'Num'\r\n\r\n\r\n\r\n\r\n", "comments": ["@abpalaciot,\r\n\r\nIs there any specific reason you are using the beta version of TF v2.0? I'd suggest you to upgrade to the stable version of either TF v2.0 or TF v2.1. \r\n\r\nI was able to run the tutorial without any issues or warnings using the stable version of TF v2.0, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/b65d52b10c3a57056262c9aa059dce2e/38365.ipynb). Thanks!", "Any updates regarding this issue? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 38364, "title": "Only enable `use_fused_avg_updates` in non-XLA context.", "body": "PiperOrigin-RevId: 305503998\nChange-Id: Ibe12750b5b545e5ba9e29c3e28aea9f8e44ea2a4", "comments": []}, {"number": 38363, "title": "TFLite - number of detections", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04):  Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary):binary\r\n - TensorFlow version (use command below):  1.14\r\n- Python version:3.6\r\n - Bazel\r\nversion (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version:10.0\r\n - GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nHi\r\nI've trained a model to produce only one detection. I have converted it to tflite, I used this command to freeze the model: \"export_tflite_ssd_graph.py --pipeline_config_path=/pipeline.config --trained_checkpoint_prefix=plate_detector/training/model.ckpt-33539 --output_directory=tflite --add_postprocessing_op=true --max_detections=1\"\r\nAnd I have changed NUM_DETECTIONS  to 1 in TFLiteObjectDetectionAPIModel,\r\nbut still getting this error:\"Cannot copy between a TensorFlowLite tensor with shape [1, 10, 4] and a Java object with shape [1, 1, 4]\".\r\nAny help?\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38363\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38363\">No</a>\n"]}, {"number": 38362, "title": "what's the difference between tensorflow functions and tensorflow.keras.backend functions?", "body": "I know this is not a Technical Issue, but i found no answers regarding this, hence i came here for support.\r\n\r\n\r\nMany of the tensorflow functions overlap with the tensorflow.keras.backend functions.\r\n\r\nFor eg:\r\ntensorflow.keras.backend have sum\r\ntensorflow have reduced_sum\r\n\r\ntensorflow.keras.backend have square\r\ntensorflow have math.square\r\n\r\ntensorflow.keras.backend have mean\r\ntensorflow have reduced_mean\r\n\r\ntensorflow.keras.backend have exp\r\ntensorflow have exp\r\n\r\nand many more functions overlap.\r\n\r\nare these functions different in some perspective?", "comments": ["@saahiluppal \r\nplease refer to these links and let us know if it helps [link1](https://www.pyimagesearch.com/2019/10/21/keras-vs-tf-keras-whats-the-difference-in-tensorflow-2-0/) [link2](https://analyticsindiamag.com/tensorflow-vs-keras-which-one-should-you-choose/) [link3](https://stackoverflow.com/questions/60768547/what-is-the-difference-between-functions-from-keras-backend-and-tensorflow)", "It seems that both are same. Thanks"]}, {"number": 38361, "title": "2 different \"no module\" errors in two different versions", "body": "**System information** \r\n- I wrote custom code\r\n- Google Colab\r\n- TensorFlow installed from default, then I tried multiple versions using Pip \r\n- TensorFlow version (use command below): \r\n1.4, 1.9, 1.11, 2.0\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\nIf I use a version of Tensorflow 1, I get the error \"tensorflow.data has no attribute: experimental\" and if I use tensorflow 2 I get: tensorflow has no atrribute: placeholder.\r\n\r\n**Describe the expected behavior**\r\nI was hoping for the script to work without errors.\r\n\r\n**Standalone code to reproduce the issue** \r\nhttps://colab.research.google.com/drive/1N0CtpO6VZvcyE72r3eEkBQthQsUlm6Hf\r\n\r\n", "comments": ["Tensorflow 1.x uses graph mode by default, and placeholders can only be run in sessions. But Tensorflow 2.x eagerly executes by default hence no need to manage sessions, so no need to manage placeholders. \r\nBut still if you want to use placeholders and manage sessions. here you go\r\n<a href='https://www.tensorflow.org/api_docs/python/tf/compat/v1/placeholder'>tensorflow.compat.v1.placeholder</a>\r\n\r\nAnd that **experimental** attribute, that's constantly changing. The functions inside experimental that are frequently used by users, they are included in tensorflow. others are removed from there.\r\n_as name suggests **experimental**_", "Thank you for your help!  I'm sorry for my ignorance, but how do I use the tensorflow.compat.v1.placeholder in the above colab notebook?", "Have a look at the documentation <a href='https://www.tensorflow.org/api_docs/python/tf/compat/v1/placeholder'>here<a>. And pick up the use case which suits your needs.\r\nBut it's highly reccomended to use Tensorflow 2.x\r\nHave a look if you wanna migrate from Tensorflow 1.x to Tensorflow 2.x here <a href='https://www.tensorflow.org/guide/migrate'>here</a>", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@Tylersuard \r\n\r\nAny update on this issue please. Please, close this thread if it solves your question.Thanks!", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38361\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38361\">No</a>\n"]}, {"number": 38360, "title": "Update arc build system and merge upstream", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38360) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 38359, "title": "Marginal Probablities in CRF", "body": "After training a CRF model, I can get the best tag sequence y and its unormalized score for each test input sequence x through tf.contrib.crf.viterbi_decode() or tf.contrib.crf.crf_decode() but what changes might require in CRF implementation to get the marginal probabilities?\r\n\r\nTensorflow version: 1.15.0\r\n\r\n\r\n", "comments": ["@FallakAsad,\r\nIf this is a bug, could you please provide the complete code to reproduce the issue reported here.\r\n\r\nIf the issue reported here is not a bug or a feature request, the question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) as there is a larger community that reads questions there. Thanks!", "Any updates regarding this issue? Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I also want to know how to calculate the margin probability in CRF", "I also want the same.", "@sakusss, @agarwalishan,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!"]}]