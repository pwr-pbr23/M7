[{"number": 9673, "title": "Branch 155146664", "body": "", "comments": ["@tensorflow-jenkins test this please"]}, {"number": 9672, "title": "tf.contrib.image.rotate errors under Windows", "body": "### Description\r\nTrying to use `tf.contrib.image.rotate` produces the error: `tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'ImageProjectiveTransform'`. This appears to happen on Windows using both CPU and GPU, and does not appear to happen on Linux (Ubuntu 14.0.4, Tensorflow 1.0.1).\r\n\r\n\r\n### Test Case\r\n```\r\nimport tensorflow as tf\r\nimages = tf.zeros([2, 2])\r\nrotated_images = tf.contrib.image.rotate(images, 0)\r\n```\r\n\r\n### Traceback\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 3, in <module>\r\n    rotated_images = tf.contrib.image.rotate(images, 0)\r\n  File \"C:\\Users\\tim\\didi-car\\tf_env\\lib\\site-packages\\tensorflow\\contrib\\image\\python\\ops\\image_ops.py\", line 72, in rotate\r\n    angles_to_projective_transforms(angles, image_width, image_height))\r\n  File \"C:\\Users\\tim\\didi-car\\tf_env\\lib\\site-packages\\tensorflow\\contrib\\image\\python\\ops\\image_ops.py\", line 166, in transform\r\n    output = gen_image_ops.image_projective_transform(images, transforms)\r\n  File \"C:\\Users\\tim\\didi-car\\tf_env\\lib\\site-packages\\tensorflow\\contrib\\image\\ops\\gen_image_ops.py\", line 49, in image_projective_transform\r\n    transforms=transforms, name=name)\r\n  File \"C:\\Users\\tim\\didi-car\\tf_env\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\tim\\didi-car\\tf_env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2338, in create_op\r\n    set_shapes_for_outputs(ret)\r\n  File \"C:\\Users\\tim\\didi-car\\tf_env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1719, in set_shapes_for_outputs\r\n    shapes = shape_func(op)\r\n  File \"C:\\Users\\tim\\didi-car\\tf_env\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\", line 610, in call_cpp_shape_fn\r\n    debug_python_shape_fn, require_shape_fn)\r\n  File \"C:\\Users\\tim\\didi-car\\tf_env\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\", line 671, in _call_cpp_shape_fn_impl\r\n    input_tensors_as_shapes, status)\r\n  File \"c:\\users\\tim\\anaconda3\\Lib\\contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"C:\\Users\\tim\\didi-car\\tf_env\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'ImageProjectiveTransform'\r\n```\r\n\r\n### System information\r\n- Windows 10 Pro\r\n- TensorFlow 1.1.0 installed via pip into virtual environment\r\n- Python 3.5.1 (Anaconda)\r\n", "comments": ["If the PIP package for Windows is built with CMake, it seems like perhaps this is a known limitation of the TensorFlow CMake build?\r\n\r\nFrom the TensorFlow CMake README:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md\r\n\r\n> It is not possible to load a custom Op library.\r\n\r\nThe CMake README also claims that:\r\n\r\n> The tf.contrib libraries are not currently included in the PIP package.\r\n\r\nwhich doesn't *seem* to be the case anymore, since I have been using tf.contrib.layers without issue.", "I'm getting this as well. Did you solve it?", "I'm traveling but will spin up a Windows VM and debug this when I get home in 2 weeks.", "Having the same issue here. \r\n\r\n- Win 10 \r\n- TensorFlow 1.2 installed via pip into a virtual environment.\r\n- Python 3.5.2 (Anaconda)\r\n\r\nThe test case is similar.", "Same problem, using non-anaconda python, win10.", "I have the same problem . Instead i am trying to apply transformation matrix via contrib.image.transform but it seems to use the same function 'ImageProjectiveTransform'\r\n\r\n- Win 7\r\n- TensorFlow 1.2 installed via pip.\r\n- Python 3.5 (Anaconda)\r\n", "I have the same problem even with 1.3.0rc0 win7 CPU.\r\ntensorflow-1.3.0rc0-cp35-cp35m-win_amd64.whl\r\nhttps://pypi.python.org/packages/44/43/bcacb6d0a1b63bb0e000a5301079dc66f4da842923878b371a63645ee188/tensorflow-1.3.0rc1-cp35-cp35m-win_amd64.whl#md5=95b003c917ce4f535fab12a149a953d1\r\n\r\nJupyter notebook:\r\nhttp://www.anishathalye.com/media/2017/07/25/adversarial.ipynb\r\n\r\nNotFoundError: Op type not registered 'ImageProjectiveTransform' in binary running on DHUB2362. Make sure the Op and Kernel are registered in the binary running in this process.\r\n", "Same issue here\r\n\r\n- Windows 10\r\n- GTX 960M\r\n- Python 3.5 (Anaconda)\r\n- tensorflow_gpu-1.2.1-cp35-cp35m-win_amd64.whl\r\n- tf 1.2.1", "Same problem. Did anyone solved it or is there any work around?", "Hello, my work around was to switch to Ubuntu Linux. It took about an hour an 10GB HD space to install VirtualBox on my Windows, and Linux and python and tensorflow, and all the other packages and the rotate operation works. Regards. Tibi", "Thanks @av8ramit!", "@av8ramit This function can only be used by building tf from source? The error persists.", "Does pip package include this update?", "The nightly pip packages have this update.\r\n", "@av8ramit Thank you. It helps.", "Can it be backpropagated? It seems it can't...", "Same problem here:\r\n\r\n- Windows 7\r\n\r\n- Tensorflow 1.3\r\n\r\n- Python 3.6\r\n\r\n", "We are working on TF 1.4. I think it will have this fix.\r\nWe will not create a patch release for 1.3, because this issue is in contrib.\r\n(That is not saying for all issues in core we will create patch releases.)", "that's true . \r\nthe problem is handled well after I installed tensorflow 1.4 \r\nbefore that I used tf 1.3 on Win 10\r\ntf1.4 seems to not be installed by pip current now \r\nI downloaded a copy from the link https://pypi.python.org/pypi/tensorflow"]}, {"number": 9671, "title": "fix symbol count in def file for windows", "body": "This fixes some issue introduced in the previous version where RTTI\r\nwas removed from the exclusion list. Because of this the number of\r\nsymbols in the def file was close to 64K for gpu builds and yesterday\r\na few added symbols pushed us over the 64K limit for the windows linker, breaking the windows gpu build\r\nAdding RTTI back to the exclusion list.\r\n", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 9670, "title": "InvalidArgumentError even finished feeding the values, maybe hidden bug of variable dependency", "body": "I have defined two classes of models, `x` and `y`. \r\n\r\n    class x():\r\n        def __init__(self, x_inp1, x_inp2):\r\n            # do sth...\r\n\r\n        def step(self, session, encoder_inputs):\r\n            input_feed = {}\r\n            for l in range(encoder_size):\r\n                 input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\r\n            ...\r\n            output_feed = [x_output]\r\n            return session.run(x_output)\r\n\r\n    class y():\r\n        def __init__(self, y_inp1, y_inp2):\r\n            # do sth...\r\n\r\n        def step(self, encoder_inputs):\r\n            input_feed = {}\r\n            for l in range(encoder_size):\r\n                 input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\r\n            ...\r\n\r\nThey have quite similar functions. And then I define another class to group them up.\r\n\r\n    class gp():\r\n        def __init__(self, x_inp1, x_inp2, y_inp1, y_inp2):\r\n            with tf.variable_scope('x'):\r\n                  self.x_model = x(x_inp1, x_inp2)\r\n            with tf.variable_scope('y'):\r\n                  self.y_model = y(y_inp1, y_inp2)\r\n        def step(self, session, encoder_inputs):\r\n            print('train x....')\r\n            x_output = self.x_model.step(session, encoder_inputs)\r\n            print('train y....')\r\n            y_output = self.y_model.step(session, x_output)\r\n            ...\r\n\r\nPlease notice that the `y_model` takes the output of `x_model` as input. And I run the `gp()` in the `main` function:\r\n    \r\n    with tf.Session() as sess:\r\n         gp_m = gp(x_inp1, x_inp2, y_inp1, y_inp2)\r\n         gp_m.step(sess, x_inp1, x_inp2, y_inp1, y_inp2)\r\n\r\nAnd after running  `x_output = self.x_model.step(encoder_inputs)` and begin to do `y_output = self.y_model.step(x_output)`, I got such an error:\r\n    \r\n    InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'x/encoder0' with dtype int32\r\n\t [[Node: x/encoder0 = Placeholder[dtype=DT_INT32, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\nPlease notice this error points to the `x_model` even the step function of it has been finished, i.e., both `train x...` and `train y...` are shown. The `x_output` should be python list and numpy array, and it is not Tensorflow object anymore. So maybe there is any bug of the variable dependency? Or I have any wrong operation that cause such a kind of error? \r\n\r\nHere are the information maybe useful:\r\n\r\n### operation system:  mac os 10.10\r\n### python: 3.5\r\n### tensorflow: r1.0\r\n### install: from binary\r\n### install using pip", "comments": ["Thank you for reporting, please fill the [issue template with all information required](https://github.com/tensorflow/tensorflow/issues/new).", "@carlthome Thanks for your reply. I have provided some information below. But indeed I am not sure that whether it is really a bug or just I have built up the model in improper way. Could you please take a look at my code and see if I have done anything wrong? Thanks.", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9669, "title": "Unsound GPU driver version scheme assumption in StringToDriverVersion", "body": "Tensorflow's `cuda_diagnostics.cc` [here](https://github.com/tensorflow/tensorflow/blob/076799bdfae0057723d96f47cf78cb623c8bcd57/tensorflow/stream_executor/cuda/cuda_diagnostics.cc#L82) assumes that the GPU driver version will match `%d.%d.%d` but that doesn't seem to be a safe assumption.\r\n\r\n```\r\n2017-05-04 09:48:39.543664: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: Invalid argument: expected %d.%d or %d.%d.%d form for driver version; got \"378.05.05.05\"\r\n```\r\n\r\nIn `FindDsoVersion()`, TF is parsing the name of the loaded libcuda library to get the version number, and the current Nvidia web driver library is:\r\n\r\n    /Library/Frameworks/CUDA.framework/Versions/A/Libraries/libcuda_378.05.05.05_mercury.dylib\r\n", "comments": ["Would you care to send a PR?"]}, {"number": 9668, "title": "Merge remote-tracking branch 'refs/remotes/tensorflow/master'", "body": null, "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->"]}, {"number": 9667, "title": "verbs: Configure IB/RoCE QPs MTU according to the port MTU", "body": "For the verbs RDMA code to work on standard Ethernet networks,\r\nwhere ports are configured with MTU 1500, need to set the path\r\nMTU to 1K, as the previous 4K value is typical for IB networks.\r\n\r\n@junshi15  @llhe @jhseu ", "comments": ["Can one of the admins verify this patch?", "Can you make this a compile time or load time option, instead of hard-coding the MTU value?  I would guess there's no one value that's going to be optimal for everyone.", "Up to @poxvoculi, but I'd favor defaulting to a lower MTU for better compatibility and proving that a higher MTU matters for performance before offering a configuration option. @shamoya did some basic tests and it didn't seem to matter.", "A colleague has a better suggestion: Why not just use the library ibv_port_attr.active_mtu value at runtime, instead of compiling in a fixed value?", "Thanks @poxvoculi @jhseu,\r\nYou are right of course and it's better to query the port (ibv_qeury_port) attributes for the MTU.\r\nThe attention was to create a (very) temporary fix just to get things working in Roce for the time being, as \r\nI am working on a patch that adds rdmacm connection management support to the verbs code.\r\n\r\nThis is the best to go, and removes all the manual connection configurations the code is doing today.\r\nIt will also save the need to get all configurations from the user in Server creation (device, port, gid index, roce version, mtu, etc..) since all the connection configuration will be set automatically accordingly to the grpc network provided by the cluster spec (user controls the verbs network according the GRPC network interfaces). It means they are binded (grpc and verbs on the same hw interface ) but this is the way to go according the grpc+verbs protocol @junshi15 created.\r\nWhat do you think ?\r\n\r\nAnyway, I will change for selecting the MTU from the port query.", "Yes, I think this is a temporary fix for the RoCE folks to be able to use Infiniband.\r\n\r\n@shamoya is working on a better version with rdmacm, which is smart enough to set the network parameters automatically and properly. I assume that end users, especially those unfamiliar with hardware, would rather not tweak the network parameters, unless the goal is benchmarking.", "@poxvoculi - Changed to take active_mtu from port attribures", "@tensorflow-jenkins test this please", "(A spurious failure)", "Jenkins, test this please.", "What the PR a status?", "Can one of the admins verify this patch?", "Why it still not merged?\r\n", "@tensorflow-jenkins test this please", "@bkovalev It seems that the tests failed on the last try. Let's give it another try.", "Thanks", "Hi,\r\nROCE is working, but with poor performance. In inception v3 arround 1/8 from Ethernet speed.\r\n", "@bkovalev did you by any chance checked IB as well? It should not affect IB performance. Verbs supports RoCE, not sure why the performance penalty."]}, {"number": 9666, "title": "Allows direct builds from a Visual Studio solution & Enables Debug builds", "body": "**Problem1**: When using CMake GUI, it by default generates a Visual Studio solution.  Visual Studio solutions cannot be built within Visual Studio because the `${CMAKE_BUILD_TYPE}` symbol is not defined for multi-configuration environments like Visual Studio.\r\n**Solution1**: Replace the `${CMAKE_BUILD_TYPE}` symbol with `$(Configuration)`.  The latter symbol works on both command line building and building within Visual Studio.\r\n\r\n**Problem2**: The build scripts were written for Windows to assume only optimized builds.  There are references to libraries (eg: \"_zlibstatic.lib_\").  However on a debug build these references have a different name (eg: \"_zlibstaticd.lib_\").  There are also some warnings as errors that needed to be fixed.  _(A function that asserts and does not return any value.)_  Lastly one of the kernels did not compile because Eigen does not define a double code path on a Debug build.\r\n**Solution2**: Use the CMake 'debug' and 'optimized' keywords to specify different libraries depending on the build.  Adds `return NULL` to functions that have not return values.  Wraps the declaration of kernels that have double precision definitions in pre-processor statements that will prevent it from being build on Windows.", "comments": ["Can one of the admins verify this patch?", "**NOTE**: This is an identical pull request to one earlier.  That has been approved but I was asked to rebase.  GitHub was having trouble with the diffs after a rebase, so I took HEAD revision and manually reapplied all changes again and force pushed.  I though GitHub would still consider it valid for the pull request.  However, it seems I was sorely wrong.", "**NOTE**: After reapplying the changes... I have not run a test build on this.  Let me run a test build first.  I cannot run a test build at home because my home machine has 8gb of system memory, which is not enough to successfully run a Visual Studio build.  I will run a build on another machine tonight.", "ok let us know when this is ready to look at!", "Verified the build.  It's ready to be looked at.", "Okay, great!  Testing this ahead of time before review.\r\n\r\n@tensorflow-jenkins test this please", "Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 9665, "title": "Android Inference AAR Release Builds", "body": "### Android Release request\r\nYou currently have [nightly Android builds](https://ci.tensorflow.org/view/Nightly/job/nightly-android/).  Is it possible to also have a Jenkins job that publishes stable release (and maybe RC) builds for Android?\r\n\r\nOr alternatively, can you publish release and RC builds of your Android Inference AAR out somewhere they can be pulled as dependencies by Gradle (Maven Central or wherever other Android dependencies are stored)?", "comments": ["@yifeif what do you think?", "We've actually just published an initial aar yesterday: https://bintray.com/google/tensorflow/tensorflow-aar/1.2.0-preview#files\r\n\r\nWe'll be updating the Gradle code in the Android demo soon to show how to use it.\r\n\r\nPlans going forward are to update this on every point release as part of the release process.", "Great news, thank you!", "@andrewharp One more question, do you guys plan on making this available in jcenter?  It doesn't appear to be there as I would expect to see it in the `org.tensorflow` group on jcenter (and its not currently there).", "Yes, we're currently in the process of doing so, will update here with final details when available.", "The AAR is [live](https://bintray.com/google/tensorflow/tensorflow-android)! See [this comment](https://github.com/tensorflow/tensorflow/issues/6385#issuecomment-299913647) for instructions on using.", "the AAR doesn't seem to exist online can anybody send it to me please"]}, {"number": 9663, "title": "replace sleep macro with an inline function", "body": "The file third_party/eigen3/unsupported/Eigen/CXX11/Tensor contains a macro called sleep which is only defined for windows. I assume this is added to cover the use of the posix function sleep(seconds) which is not available on windows. However, the function Sleep(microseconds) is. Unfortunately this macro has the nasty effect that you can't define or use any functions named sleep, even if it is in another namespace or class. A function with this name exists in most thread libraries such as boost::thread and qthread. Thus if you try to use tensorflow in an application which also use a threading library it will not compile.\r\n\r\nI suggest replacing the macro with an inline function instead. As far as I can see, this function is not really used that much in the tensorflow code, thus other options than creating a global sleep function may be possible and more desirable.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 9662, "title": "Fail to compile binary for Android", "body": "Is there simple way to compile a native binary for inference on Android? Now I can successfully build the [benchmark](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/benchmark) and run it on my Android device. But it does not contain any actions about ops. I want to do something similar to the [benchmark_model_test](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/benchmark/benchmark_model_test.cc) but it fails to build. I think it now lacks a android version of //tensorflow/cc APIs. How can I tackle it?", "comments": ["Have you read this? https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/android", "Thanks @flawless2011 I've read that. I want to use the apis under //tensorflow/cc/..., but it seems that the lib you provide doesn't refer to these apis? Code in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/benchmark/benchmark_model_test.cc uses some ops apis instead. For most TF apis, we already have //tensorflow/core:android_tensorflow_lib, but in my case I want to compile more (under //tensorflow/cc) but failed.", "@flawless2011 Not sure if I've made my question clear. I hope you can have comments or help refer to someone who should be familiar with this. Really appreciate that.", "Sorry I'm not getting this. What do you want to do? Compile directly against the C++ library on android?\r\n\r\n/CC: @asimshankar ", "@drpngx Basically, I want to build a native binary on Android, which simply creates a graph with some ops and execute it with some random input. The [benchmark](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/benchmark) is a good example which can run on Android, but it just reads the graph from disk without constructing its own graph. From what I've found via the [benchmark_model_test](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/benchmark/benchmark_model_test.cc) and [label_image](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/label_image) examples, I need to include apis under //tensorflow//cc to use ::tensorflow::ops::XXX (such as ResizeBilinear).\r\nSo my question is how to compile the //tensorflow/cc for Android? I didn't find any target inside //tensorflow/cc.BUILD, and failed to build it by myself.\r\nI'm kind of new on this so maybe it's a very stupid question :)", "@PKUEcho : So you want to compile essentially an arbitrary binary that uses the C++ API for graph construction for Android?\r\n\r\nThis seems more like a bazel question :), but the good news is that that the only difference between compiling for Android or for your host machine should be flags to bazel.\r\n\r\nI suppose you could do one of the following:\r\n\r\n- Modify [`benchmark.cc`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/benchmark/benchmark_model.cc) with the code you want, and add `//tensorflow/cc:cc_ops` to the `deps` [section in the BUILD file](https://github.com/tensorflow/tensorflow/blob/bc456e361d49d1d89a74b80060c70efb51fd7d87/tensorflow/tools/benchmark/BUILD#L33). And build the binary as per the instructions you pointed to. Those instructions show how to build any bazel target for Android.\r\n\r\n- Compile the shared library for Android using something like:\r\n```\r\nbazel build -c opt \\\r\n  --crosstool_top=//external:android/crosstool \\\r\n  --cpu=armeabi-v7a \\\r\n  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n  tensorflow:libtensorflow_cc.so\r\n```\r\nThis would be a shared library that you can link against in any native code you build for Android.\r\n\r\nHope that helps.\r\n", "@asimshankar thanks for your reply! I'm missing some details here. So, basically you provide two solutions here, right?\r\n\r\n- Inside the `deps`, I think I need to insert `//tensorflow/cc:cc_ops` after `//tensorflow:android` rather than `//conditions:default`? Because I think that's the true `deps` when I build it for Android.After that, I tried use the following command to build it but failed. Did I miss anything from your answer?\r\n`$bazel build -c opt \\\r\n  --crosstool_top=//external:android/crosstool \\\r\n  --cpu=armeabi-v7a \\\r\n  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n  tensorflow/tools/benchmark:benchmark_model`\r\n\r\n- So the second solution is to add a target inside //tensorflow/cc/BUILD, and build it myself?", "@asimshankar I tried the second way you provided as following.\r\n`bazel build -c opt \\\r\n  --crosstool_top=//external:android/crosstool \\\r\n  --cpu=armeabi-v7a \\\r\n  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n  tensorflow:libtensorflow_cc.so`\r\n\r\nBut there are many errors such as \r\n`external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/atomic:655:7: note: C++11 'constexpr' only available with -std=c++11 or -std=gnu++11\r\nexternal/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/atomic:668:14: error: expected ';' at end of member declaration\r\n       atomic() noexcept = default;\r\n              ^\r\nexternal/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/atomic:668:16: error: 'noexcept' does not name a type\r\n       atomic() noexcept = default;\r\n                ^\r\nexternal/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/atomic:669:15: error: expected ';' at end of member declaration\r\n       ~atomic() noexcept = default;\r\n               ^\r\nexternal/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/atomic:669:17: error: 'noexcept' does not name a type\r\n       ~atomic() noexcept = default;\r\n                 ^\r\nexternal/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/atomic:674:7: error: 'constexpr' does not name a type\r\n       constexpr atomic(__integral_type __i) noexcept : __base_type(__i) { }`\r\n\r\nI think there's something wrong about the compilers. How do you think?", "@PKUEcho : What errors were you getting when you tried to build `benchmark_model`? \r\n\r\n(Based on updates in #2412 - I'd avoid that second suggestion altogether for now, though the errors you're seeing are different, but it seems using `libtensorflow_cc.so` isn't convenient yet)\r\n\r\nThat said, CCing @andrewharp in case he has ideas, but marking this as \"community support\" since building native binaries using the C++ API for Android isn't something that we are currently actively working on or supporting.", "Right, unifying the android_* libs with the other cc_library TF targets isn't really something on our radar.\r\n\r\nHowever, if you take a look at https://github.com/tensorflow/tensorflow/pull/9563, they manage to build cc_ops into an Android binary, so that might be a possible route to investigate.", "@asimshankar @andrewharp Thanks for your help. After reading the [#9563](https://github.com/tensorflow/tensorflow/pull/9563), I found that I missed the flags `--cxxopt=\"-std=c++11\" --cxxopt=\"-DTENSORFLOW_DISABLE_META\"` which makes my previous compilation fail. Though I'm not sure why in other cases such as [benchmark_mode](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/benchmark) I don't need these flags?\r\nAlso, I found that some ops are not supported in mobile version currently. How can I check which ops are supported and which are not? This question might be out of the scope of this thread so you may close this.", "There are kernels that definitely work, and kernels that may work.\r\n\r\nThe kernels that may work are [here](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/BUILD#L4267).\r\n\r\nThe ones that definitely work are [core and extended](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/BUILD#L4317).\r\n\r\nClosing for now.", "@PKUEcho some ops use C++11 features, but the default cross-toolchain in NDK (gcc-4.9) uses C++98. That's why you need --cxxopt=\"-std=c++11\"", "@freedomtan Thanks for your reply, that makes sense :)", "Hi, I'm trying to build the standard TensorFlow C API (not the JNI API) on Android so I can build a standalone binary in Rust on Android, what would be the recommended way? I saw the `contrib/android` builds the JNI lib, should I simply do a cross-compile bazel build of `//tensorflow:libtensorflow.so` ?", "@maelp You'll need a target that depends on //tensorflow/core:android_tensorflow_lib + //tensorflow/c:c_api (such as //tensorflow/java/src/main/native:native -- you don't have to use the JNI interfaces). See //tensorflow/contrib/android/README.md for the set of build flags to use.\r\n\r\n(for further assistance on this issue please refer to StackOverflow, as integrating with Rust is outside the scope of TF github support)", "Hi @andrewharp thanks for your answer, my question is not so much about Rust integration as it is about getting the C or C++ library cross-compiled on Android\r\n\r\nFrom your comment I assume that I need to add a build target with both `android_tensorflow_lib` and `c_api` to build those? There is not yet an existing cross-compile build target which compiles the C .so for Android without the rest of the JNI bindings?", "Correct: the c_api target depends on android_tensorflow_lib_lite, which is runtime-only (no ops), so you'll need a target that adds them in. My suggestion would be to take //tensorflow/contrib/android:libtensorflow_inference.so as a starting point and modify it to fit your needs (remove the jni files, change the whitelisted methods so they don't get stripped out, etc)."]}, {"number": 9660, "title": "Fix error of version_info.cc not being generated on windows", "body": "If tensorflow_BUILD_PYTHON_BINDINGS is disabled in CMake, the Python interpreter is never resolved in CMake, but it is needed in tf_core_framework.cmake for generating the version_info.cc file with the script tensorflow/tools/git/gen_git_source.py. This seems to only be a problem on windows, resulting in a failed build complaining about version_info.cc missing. This is not an issue on linux, probably because of the shebang #!/usr/bin/env python in the script. \r\n\r\nI suggest moving the part in tf_python.cmake that sets up the python interpreter to the CMakeLists.txt as done in this PR so that it is always executed.", "comments": ["Can one of the admins verify this patch?", "@smistad it helps to let us know when the code is ready to review again -- let us know!", "Ok, the commit is updated and ready for review", "@tensorflow-jenkins test this please"]}, {"number": 9659, "title": "Fix Bazel CI / TensorFlow project.", "body": "This change:\r\n\r\n1. updates common_env.sh to export PYTHON_LIB_PATH\r\nalong with PYTHON_BIN_PATH so the configure\r\nscripts doesn't have to guess\r\n\r\n2. writes these paths to bazelrc with quotes\r\naround, to guard agains spaces in the path (e.g.\r\n\"C:/Program Files/Anaconda3/python\")\r\n\r\nFixes https://github.com/bazelbuild/bazel/issues/2892", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "@tensorflow-jenkins test this please\r\n\r\n(what's the correct incantation? :)", "(only project members can run tests; we don't have enough test machines to let anyone run tests :)\r\n\r\n@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please\r\n\r\nI don't know why these tests are flaky, seems like github timeouts."]}, {"number": 9658, "title": "Add input function for training and testing (#9617)", "body": "* Add input function for training and testing\r\n\r\nEstimator is decoupled from Scikit Learn interface by moving into separate class SKCompat. Arguments x, y and batch_size are only available in the SKCompat class, Estimator will only accept input_fn\r\n\r\n* remove extra comma", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->"]}, {"number": 9657, "title": "> gcc: error: $opt: No such file or directory", "body": "\r\nHi, I am using Ubuntu 16.04 (GCC5.2) and CUDA 8 to compile TF. I meet the following problem. Does anyone knows how to solve it? Thanks!\r\n\r\n> ERROR: /home/xxx/.cache/bazel/_bazel_root/88a2645c89e061c3b0fe8e82d8c21312/external/protobuf/BUILD:609:1: C++ compilation of rule '@protobuf//:python/google/protobuf/internal/_api_implementation.so' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 50 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\n> gcc: error: $opt: No such file or directory\r\n> gcc: error: $opt: No such file or directory\r\n> Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n", "comments": []}, {"number": 9656, "title": "add a image with summary for showing  in tensorboard", "body": "Hi,\r\nfor add a image to graph for showing in tensorboard , i used below code. but the tensorflow error that :\"TypeError: Parameter to MergeFrom() must be instance of same class: expected Summary got Tensor. for field Event.summary\"\r\n\r\nthe code :\r\n<<  _**writer =tf.summary.FileWriter('C:\\Program Files\\python3.5\\logs',graph=tf.get_default_graph())**_\r\n<v\r\n<<tf.Tensor 'Slice_2:0' shape=(1, 152, 138, 1) dtype=float32>\r\n<<**_imgsumarry=tf.summary.image('image', v, 3 ,collections=None)_**\r\n<imgsumarry\r\n<<tf.Tensor 'image_2:0' shape=() dtype=string>\r\n<< **writer.add_summary(imgsumarry)**\r\n\r\n\r\nfull error:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\python3.5\\python-3.5.3.amd64\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\", line 516, in init\r\n    copy.MergeFrom(new_val)\r\n  File \"C:\\Program Files\\python3.5\\python-3.5.3.amd64\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\", line 1208, in MergeFrom\r\n    'expected %s got %s.' % (cls.__name__, msg.__class__.__name__))\r\nTypeError: Parameter to MergeFrom() must be instance of same class: expected Summary got Tensor.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<pyshell#579>\", line 1, in <module>\r\n    writer.add_summary(imgsumarry)\r\n  File \"C:\\Program Files\\python3.5\\python-3.5.3.amd64\\lib\\site-packages\\tensorflow\\python\\summary\\writer\\writer.py\", line 113, in add_summary\r\n    event = event_pb2.Event(summary=summary)\r\n  File \"C:\\Program Files\\python3.5\\python-3.5.3.amd64\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\", line 518, in init\r\n    _ReraiseTypeErrorWithFieldName(message_descriptor.name, field_name)\r\n  File \"C:\\Program Files\\python3.5\\python-3.5.3.amd64\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\", line 446, in _ReraiseTypeErrorWithFieldName\r\n    six.reraise(type(exc), exc, sys.exc_info()[2])\r\n  File \"C:\\Program Files\\python3.5\\python-3.5.3.amd64\\lib\\site-packages\\six.py\", line 685, in reraise\r\n    raise value.with_traceback(tb)\r\n  File \"C:\\Program Files\\python3.5\\python-3.5.3.amd64\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\", line 516, in init\r\n    copy.MergeFrom(new_val)\r\n  File \"C:\\Program Files\\python3.5\\python-3.5.3.amd64\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\", line 1208, in MergeFrom\r\n    'expected %s got %s.' % (cls.__name__, msg.__class__.__name__))\r\nTypeError: Parameter to MergeFrom() must be instance of same class: expected Summary got Tensor. for field Event.summary\r\n", "comments": ["You need to eval the `imgsumarry` tensor to get a string first.", "Many Thanks"]}, {"number": 9655, "title": "why can't i recover cifar-100/cifar-10 image from binary files using tensorflow?", "body": "**my code likes following:**\r\n\r\n////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\r\nimport tensorflow as tf\r\nfrom matplotlib import pyplot as plt\r\n\r\ndef read_cifar_100_bin(batch_size):\r\n\r\n    coarse_label_bytes = 1\r\n    final_lable_bytes = 1\r\n    image_bytes = 3072\r\n    filenames = [\"./cifar-100-binary/train.bin\"]\r\n    filename_queue = tf.train.string_input_producer(filenames, shuffle=False)\r\n    reader = tf.FixedLengthRecordReader(record_bytes=3074)\r\n    key,value = reader.read(filename_queue)\r\n    record_bytes = tf.decode_raw(value, tf.uint8)\r\n    coarse_label = tf.cast(tf.slice(record_bytes, [0], [coarse_label_bytes]), tf.uint8)   \r\n    final_label = tf.cast(tf.slice(record_bytes, [1], [final_lable_bytes]), tf.uint8)  #final_label:0~99\r\n    example = tf.reshape(tf.slice(record_bytes, [2], [image_bytes]),[3,32,32])\r\n    example_tr = tf.transpose(example, [1, 2, 0])\r\n    example_batch, coarse_label_batch,final_label_batch = tf.train.batch([example_tr, coarse_label,final_label],` batch_size=batch_size)\r\n\r\n    # example_batch, coarse_label_batch,final_label_batch  = tf.train.shuffle_batch(\r\n    #     [example, coarse_label,final_label],\r\n    #     batch_size=batch_size,\r\n    #     num_threads=4,\r\n    #     capacity=50000,\r\n    #     min_after_dequeue=10000)\r\n\r\n    with tf.Session() as sess:\r\n        coord = tf.train.Coordinator()  \r\n        threads = tf.train.start_queue_runners(coord=coord)\r\n        #sess.run(tf.global_variables_initializer())\r\n\r\n        images_batch = example_batch.eval()\r\n        coarse_labels_batch = coarse_label_batch.eval()\r\n        final_labels_batch = final_label_batch.eval()\r\n        plt.imshow(images_batch[0])\r\n        plt.show()\r\n\r\n        coord.request_stop()\r\n        coord.join(threads)\r\n\r\n    return images_batch,coarse_labels_batch,final_labels_batch\r\nimages, coarse_labels,final_labels = read_cifar_100_bin(5)\r\nprint(images[0])\r\n/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\r\n**my problem is: when i use plt to recover image from images_batch[0],it show like this:**\r\n![figure_1 copy](https://cloud.githubusercontent.com/assets/11583292/25697490/ba6a2f2c-30ed-11e7-9af5-636b11607d76.png)\r\n\r\n\r\n**Anyone knows why? Thanks a lot**\r\n\r\n**Another question:**\r\n  I want to choose random mini-batch data from whole dataset so i use tf.train.shuffle_batch() function you can see above,i wonder if it's a good way to realize it w.r.t speed and efficiency.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9654, "title": "Estimator's argument checking overzealous : model_fn has following not expected args: ['self']", "body": "I'm using TensorFlow (1.1) high-level API Estimators to create my neural net. But I'm using it into a class and I have to call an instance of my class to generate the model of the neural network. (Here `self.a`)\r\n\r\n```\r\nclass NeuralNetwork(object):\r\n  def __init__(self):\r\n    \"\"\" Create neural net \"\"\"\r\n    regressor = tf.estimator.Estimator(model_fn=self.my_model_fn,\r\n                                       model_dir=\"/tmp/data\")\r\n    // ...\r\n\r\n  def my_model_fn(self, features, labels, mode):\r\n  \"\"\" Generate neural net model \"\"\"\r\n    self.a = a\r\n    predictions = ...\r\n    loss = ...\r\n    train_op = ...\r\n    return tf.estimator.EstimatorSpec(\r\n      mode=mode,\r\n      predictions=predictions,\r\n      loss=loss,\r\n      train_op=train_op)\r\n```\r\n\r\nBut I get the error : ValueError: model_fn [...] has following not expected args: ['self']. I tried to remove the self for the args of my model but got another error TypeError: \u2026 got multiple values for keyword argument. \r\nSolution for now (as it was suggested on [StackOverflow](http://stackoverflow.com/questions/43755609/tensorflow-estimator-model-fn-has-following-not-expected-args-self)) is to use a lambda function to wrap my function `my_model_fn` (see below) but it will be nicer without it.\r\n```\r\nclass NeuralNetwork(object):\r\n  def __init__(self):\r\n    \"\"\" Create neural net \"\"\"\r\n    regressor = tf.estimator.Estimator(\r\n        model_fn=lambda features, labels, mode: self.my_model_fn(features, labels, mode),\r\n        model_dir=\"/tmp/data\")\r\n    // ...\r\n```\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "This is a feature request (and came from StackOverflow). Assigning @ispirmustafa: I think we just want to special-case member functions?", "Yes this is a valid feature request.", "+1 to this request, facing the same problem and was advised to open a feature request.", "This is a good feature request. Anybody who wants to send a PR, please do. We won't get to this for a few weeks at least.", "Can we use a lambda as a workaround in the meantime?", "@JulesKzl could you post the stackoverflow link? Thanks.", "Yes, the lambda workaround should work fine.", "@drpngx As I wrote and now edited to cite the [Stackoverflow](http://stackoverflow.com/questions/43755609/tensorflow-estimator-model-fn-has-following-not-expected-args-self) topic : `Solution for now (as it was suggested on StackOverflow) is to use a lambda function to wrap my function my_model_fn (see below) but it will be nicer without it.`"]}, {"number": 9653, "title": "macOS 10.12.4 GPU Installing with native pip python3.6 (error: not loaded: @rpath/libcublas.8.0.dylib)", "body": "-- macOS 10.12.4\r\n    python3.6\r\n    \"native pip \" way\r\n    gtx 960\r\n    cuda is working\r\n\r\n-- error:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 51, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: dlopen(/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 10): Library not loaded: @rpath/libcublas.8.0.dylib\r\n  Referenced from: /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n  Reason: image not found", "comments": ["We currently use StackOverflow to document installation problems (see https://www.tensorflow.org/install/install_mac#common_installation_problems). Please post this on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow), thanks!\r\n\r\nI'm gonna close this for now, but please reopen if further investigation finds a specific TensorFlow bug."]}, {"number": 9652, "title": "Including batch_norm as the normalizer function by default, as mentioned in function description", "body": "Changed the description error that includes `batch_norm_params`, which no longer exists, to `normalizer_fn`. `normalizer_fn = None` is changed to `normalizer_fn = batch_norm` by default, following what the original description says the function will do.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 9651, "title": "crosstool_wrapper_driver_is_not_gcc failed: error executing command", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request. **This is a bug**\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: n/a, GitHub clone, 11:12pm CST May 3 2017.\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**:  8.0.61\r\n- **GPU model and memory**: NVIDIA Titan X Pascal (12GB)\r\n- **Exact command to reproduce**: bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nThis is a bug relating to installation.  I'm getting a similar error to #336  (https://github.com/tensorflow/serving/issues/336) except without any reference to nccl, similar to #8709 except I'm on Ubuntu 16.04 instead of 14, similar to #8790 (https://github.com/tensorflow/tensorflow/issues/8790) except that one seems to have been closed prematurely.\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\n$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n.......\r\nWARNING: /home/anaconda/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': Use SavedModel Builder instead.\r\nWARNING: /home/anaconda/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': Use SavedModel instead.\r\nINFO: Found 1 target...\r\nERROR: /home/anaconda/tensorflow/tensorflow/core/BUILD:1299:1: C++ compilation of rule '//tensorflow/core:lib_hash_crc32c_accelerate_internal' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/anaconda/.cache/bazel/_bazel_anaconda/8174be50bc0029f23c89ddd1973ff713/execroot/tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda \\\r\n    CUDNN_INSTALL_PATH=/usr/local/cuda-8.0 \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\n    PYTHON_BIN_PATH=/opt/anaconda/envs/py27/bin/python \\\r\n    PYTHON_LIB_PATH=/opt/anaconda/envs/py27/lib/python2.7/site-packages \\\r\n    TF_CUDA_CLANG=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \\\r\n    TF_CUDA_VERSION='' \\\r\n    TF_CUDNN_VERSION='' \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_OPENCL=0 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '$opt' '-std=c++11' '$opt' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/_objs/lib_hash_crc32c_accelerate_internal/tensorflow/core/lib/hash/crc32c_accelerate.pic.d '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/_objs/lib_hash_crc32c_accelerate_internal/tensorflow/core/lib/hash/crc32c_accelerate.pic.o' -fPIC -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -isystem external/bazel_tools/tools/cpp/gcc3 -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-DGOOGLE_CUDA=1' -msse3 -pthread -msse4.2 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers -c tensorflow/core/lib/hash/crc32c_accelerate.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/_objs/lib_hash_crc32c_accelerate_internal/tensorflow/core/lib/hash/crc32c_accelerate.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\ngcc: error: $opt: No such file or directory\r\ngcc: error: $opt: No such file or directory\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 15.639s, Critical Path: 6.51s\r\n$\r\n\r\n\r\n$ bazel version\r\nBuild label: 0.4.5\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Mar 16 12:19:38 2017 (1489666778)\r\nBuild timestamp: 1489666778\r\nBuild timestamp as int: 1489666778\r\n\r\n$ cat /usr/local/cuda/version.txt \r\nCUDA Version 8.0.61\r\n\r\n$ gcc --version\r\ngcc (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n```\r\n\r\n\r\nOn Ubuntu 16.04, building for Python 2.7 (from Anaconda). Running from bash.\r\nI grabbed tensorflow from GitHub tonight, accepted all the the defaults when running configure, said yes CUDA support (more defaults) and used compute capability 6.1.\r\nTried bazel clean, re-configure, etc...same result.   \r\n\r\nI've built this successfully for my Python 3.5 environment on this computer a couple months ago, just trying to add a Python 2.7 version and now it's failing.", "comments": ["I meet the same problem. Hope it could be solved ASAP.", "Same here.", "@gunan can you advise? From the error output, it looks like there's an environment variable (?) `$opt` that's not defined.", "Same here.\r\nThis what git blame says:\r\n$ git show 22586bdf configure\r\n.\r\n.\r\n<pre>\r\n+echo >> tools/bazel.rc                                   \r\n for opt in $CC_OPT_FLAGS; do                             \r\n-  write_to_bazelrc 'build:opt --cxxopt=$opt --copt=$opt' \r\n+  echo \"build:opt --cxxopt=$opt --copt=$opt\" >> tools/bazel.rc\r\n done \r\n</pre>\r\n\r\nIf I revert this code the compilation succeeds.", "@nlopezgi could you comment?", "Actually, it looks like we are using single quotes which doesn't interpolate, rather than double quotes previously.\r\n\r\nFixing internally. Should be in the next push.", "Confirmed.  Changing from single quotes to double quotes on line 357 of configure fixed it for me.  \r\n\r\nFor others: make that change in configure (or wait for the push by @drpngx ), re-run configure, then do bazel build...everything works from there. :-) ", "Hi, I faced same problem when I tried to run the command on Jetson TX2 by following [this instruction](https://syed-ahmed.gitbooks.io/nvidia-jetson-tx2-recipes/content/first-question.html).\r\n\r\nBut when I changed the parameter of command from 3072 to 1800 by following [another instruction](https://github.com/dat-ai/tensorflow-on-nvidia-jetson/blob/master/build_from_source.md), it worked well :)\r\n\r\nThe root cause may be ran out of memory."]}, {"number": 9650, "title": "Add input function for training and testing (#9617)", "body": "* Add input function for training and testing\r\n\r\nEstimator is decoupled from Scikit Learn interface by moving into separate class SKCompat. Arguments x, y and batch_size are only available in the SKCompat class, Estimator will only accept input_fn\r\n\r\n* remove extra comma", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "This is merging a commit from another branch on which the CLA was signed.  Testing and merging.", "@tensorflow-jenkins test this please"]}, {"number": 9649, "title": "fix `iris_test.csv` download link", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 9648, "title": "Fixed typo: date -> data", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 9647, "title": "Branch 155030326", "body": "I squashed the last branch by accident, so resolving conflicts had to be manual.  Let's hope I got them right.", "comments": []}, {"number": 9646, "title": "Memory leaking when session.run in a certain situation", "body": "Hi,\r\nHere I find a possible memory leaking bug.\r\n\r\n### Describe the problem\r\nWhen we doing \r\n`session.run( var )`\r\n\r\nif we add operator before that variable, such as \r\n`session.run(-var )`\r\n\r\nMemory will keep growing by the iteration, when we call the train function with -var thousands of times it will use up all the memory and also let the running time become much slower.\r\n\r\n### Source code / logs\r\nHere is a minimal example code:\r\n\r\n\r\n**Normal situation:**\r\n\r\n```\r\nimport tensorflow as tf\r\n@profile\r\ndef run():\r\n    a = tf.constant(5.0)\r\n    b = tf.constant(6.0)\r\n    c = a * b\r\n    sess = tf.Session()\r\n    for i in range(1000):\r\n        b = sess.run(c) \r\nrun()\r\n\r\n\r\nLine     Mem usage    Increment   Line Contents\r\n================================================\r\n     4   80.816 MiB    0.000 MiB   @profile\r\n     5                             def run():\r\n     6   80.957 MiB    0.141 MiB       a = tf.constant(5.0)\r\n     7   80.980 MiB    0.023 MiB       b = tf.constant(6.0)\r\n     8   82.691 MiB    1.711 MiB       c = a * b\r\n     9   82.879 MiB    0.188 MiB       sess = tf.Session()\r\n    10                             \r\n    11   83.570 MiB    0.691 MiB       for i in range(1000):\r\n    12   83.570 MiB    0.000 MiB           b = sess.run(c)\r\n\r\n```\r\n\r\n**Memory leaking **\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n@profile\r\ndef run():\r\n    a = tf.constant(5.0)\r\n    b = tf.constant(6.0)\r\n    c = a * b\r\n    sess = tf.Session()\r\n\r\n    for i in range(1000):\r\n        b = sess.run(-c)\r\n\r\nrun()\r\n\r\n \r\n\r\nLine     Mem usage    Increment   Line Contents\r\n================================================\r\n     4   80.805 MiB    0.000 MiB   @profile\r\n     5                             def run():\r\n     6   80.938 MiB    0.133 MiB       a = tf.constant(5.0)\r\n     7   80.949 MiB    0.012 MiB       b = tf.constant(6.0)\r\n     8   82.652 MiB    1.703 MiB       c = a * b\r\n     9   82.848 MiB    0.195 MiB       sess = tf.Session()\r\n    10                             \r\n    11  134.258 MiB   51.410 MiB       for i in range(1000):\r\n    12  134.258 MiB    0.000 MiB           b = sess.run(-c)\r\n\r\n```\r\n\r\n\r\n\r\n### System information\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: mac os 10.12.4  and Linux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: pip version  \r\n- **TensorFlow version (use command below)**: ('v1.0.0-65-g4763edf-dirty', '1.0.1')\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: N/A \r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: see above\r\n", "comments": ["That happens because every time you call `-c` in the loop, a new op is created in the graph. If you set `d = -c` before entering the loop and then you fetch `d` in the loop your memory consumption should not increase. To me that looks like expected behavior. Note that most Python operators (such as the negation operator you are using) are overloaded and construct new ops in the graph behind the scenes.", "@eaplatanios is right.", "@eaplatanios @drpngx Sorry to comment on this closed issue, but I have a similar problem as code shown below:\r\n\r\n```python\r\nwith tf.Session() as sess:\r\n    for i in range(0, 1000):\r\n        fread = tf.read_file('./data.text')\r\n        sess.run(fread)\r\n```\r\n\r\nIn the loop, memory consumption will increase time by time.\r\n\r\nMy question is how to free this `tf.Tensor`(fread) explicit?\r\nCould you take a look please, Thanks!", "@ScorpioCPH You should move the `fread = tf.read_file('./data.text')` line outside the loop. That line is not reading the file...it's just creating an op that will read it when you execute it using `sess.run`. You just keep re-creating that op in your graph and you end up with 1000 copies of it.", "I think it's because the graph grows. There is no simple way to prove there\ngraph. You can reset the graph.\n\nMay I suggest you take a look at the tf.contrib.dataset?\n\nOn Sep 4, 2017 10:17 PM, \"Penghao Cen\" <notifications@github.com> wrote:\n\n> @eaplatanios <https://github.com/eaplatanios> @drpngx\n> <https://github.com/drpngx> Sorry to comment on this closed issue, but I\n> have a similar problem as code shown below:\n>\n> with tf.Session() as sess:\n>     for i in range(0, 1000):\n>         fread = tf.read_file('./data.text')\n>         sess.run(fread)\n>\n> In the loop, memory consumption will increase time by time.\n>\n> My question is how to free this tf.Tensor(fread) explicit?\n> Could you take a look please, Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9646#issuecomment-327054020>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbfTxa7s5Fldgup-uS3wG5032sECGks5sfK9IgaJpZM4NQIUp>\n> .\n>\n", "@eaplatanios @drpngx Thanks for your suggestions!"]}, {"number": 9644, "title": "Indexed Slices Support", "body": "Hi,\r\n\r\nI am wondering what the reasoning behind IndexedSlices is and whether they are actually necessary. My understanding is that they are only constructed for the gradient of the gather op. Is there any other place in the Python API that IndexedSlices are being constructed? Now, even though they are only constructed there, there are special cases throughout the Python codebase for dealing with indexed slices. Is the performance benefit so important to justify all this special treatment code? I am asking because I am wondering if it is useful to implement that functionality in an API for a different language. \r\n\r\nAnd if I am to rephrase this question, should one put effort into adding support to the C++ API for indexed slices, or is their use a remnant of a design choice that is not that useful looking in retrospect?\r\n\r\nThank you!", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9643, "title": "bazel coverage build failure", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS 10.12\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: \r\n('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')\r\n- **Bazel version (if compiling from source)**: Build label: \r\n0.4.5\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Mar 16 12:50:12 2017 (1489668612)\r\nBuild timestamp: 1489668612\r\nBuild timestamp as int: 1489668612\r\n\r\n- **CUDA/cuDNN version**: 7.0\r\n- **GPU model and memory**: AMD Radeon R9 M370X 2048 MB\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nI'm trying to run `bazel coverage` on my Mac. But it fails to build.\r\n```\r\n$ bazel coverage //tensorflow/tensorboard/backend/... --verbose_failures\r\n```\r\nor\r\n```\r\n$ bazel build //tensorflow/tensorboard/backend/...   --verbose_failures  --collect_code_coverage\r\n```\r\n\r\nIt returns error message like below\r\n```\r\ncom.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nld: library not found for -lgcov\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n```\r\n\r\nIs there any way to fix `ld: library not found for -lgcov` problem?\r\n\r\n\r\n### Source code / logs\r\n```\r\n$ bazel coverage //tensorflow/tensorboard/backend/... --verbose_failures\r\n..\r\nINFO: Using default value for --instrumentation_filter: \"//tensorflow/tensorboard/backend\".\r\nINFO: Override the above default with --instrumentation_filter\r\nINFO: Found 14 targets and 9 test targets...\r\nERROR: /private/var/tmp/_bazel_Chris/b0f26c43826ae438107dcf403665fcf5/external/protobuf/BUILD:609:1: Linking of rule '@protobuf//:python/google/protobuf/internal/_api_implementation.so' failed: link_dynamic_library.sh failed: error executing command \r\n  (cd /private/var/tmp/_bazel_Chris/b0f26c43826ae438107dcf403665fcf5/execroot/tensorflow && \\\r\n  exec env - \\\r\n    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda \\\r\n    CUDNN_INSTALL_PATH=/usr/local/cuda \\\r\n    PATH=/Users/Chris/.rvm/gems/ruby-2.0.0-p648/bin:/Users/Chris/.rvm/gems/ruby-2.0.0-p648@global/bin:/Users/Chris/.rvm/rubies/ruby-2.0.0-p648/bin:/Users/Chris/anaconda/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/share/dotnet:/Library/TeX/texbin:/Users/Chris/.rvm/bin:/Users/Chris/.rvm/bin \\\r\n    PYTHON_BIN_PATH=/Users/Chris/anaconda/bin/python \\\r\n    PYTHON_LIB_PATH=/Users/Chris/anaconda/lib/python2.7/site-packages \\\r\n    TF_CUDA_CLANG=1 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=3.5,5.2 \\\r\n    TF_CUDA_VERSION='' \\\r\n    TF_CUDNN_VERSION='' \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_OPENCL=0 \\\r\n    TMPDIR=/var/folders/tr/tl261hjj2wl4662x8msxh6fm0000gn/T/ \\\r\n  external/bazel_tools/tools/cpp/link_dynamic_library.sh no ignored ignored ignored external/local_config_cc/cc_wrapper.sh -shared -o bazel-out/local-opt/bin/external/protobuf/python/google/protobuf/internal/_api_implementation.so -Wl,-force_load,bazel-out/local-opt/bin/external/protobuf/_objs/python/google/protobuf/internal/_api_implementation.so/external/protobuf/python/google/protobuf/internal/api_implementation.pic.o -lstdc++ -lm -undefined dynamic_lookup -headerpad_max_install_names -lgcov): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nld: library not found for -lgcov\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nINFO: Elapsed time: 17.559s, Critical Path: 2.03s\r\n//tensorflow/tensorboard/backend/event_processing:directory_watcher_test NO STATUS\r\n//tensorflow/tensorboard/backend/event_processing:event_accumulator_test NO STATUS\r\n//tensorflow/tensorboard/backend/event_processing:event_file_inspector_test NO STATUS\r\n//tensorflow/tensorboard/backend/event_processing:event_file_loader_test NO STATUS\r\n//tensorflow/tensorboard/backend/event_processing:event_multiplexer_test NO STATUS\r\n//tensorflow/tensorboard/backend/event_processing:plugin_asset_util_test NO STATUS\r\n//tensorflow/tensorboard/backend/event_processing:reservoir_test      NO STATUS\r\n//tensorflow/tensorboard/backend:http_util_test                       NO STATUS\r\n//tensorflow/tensorboard/backend:json_util_test                       NO STATUS\r\n\r\nExecuted 0 out of 9 tests: 9 were skipped.\r\n\r\n```\r\n\r\n\r\n\r\n```\r\n$ bazel build //tensorflow/tensorboard/backend/...   --verbose_failures  --collect_code_coverage\r\nINFO: Found 23 targets...\r\nERROR: /private/var/tmp/_bazel_Chris/b0f26c43826ae438107dcf403665fcf5/external/protobuf/BUILD:609:1: Linking of rule '@protobuf//:python/google/protobuf/internal/_api_implementation.so' failed: link_dynamic_library.sh failed: error executing command \r\n  (cd /private/var/tmp/_bazel_Chris/b0f26c43826ae438107dcf403665fcf5/execroot/tensorflow && \\\r\n  exec env - \\\r\n    CLANG_CUDA_COMPILER_PATH=/usr/bin/clang \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda \\\r\n    CUDNN_INSTALL_PATH=/usr/local/cuda \\\r\n    PATH=/Users/Chris/.rvm/gems/ruby-2.0.0-p648/bin:/Users/Chris/.rvm/gems/ruby-2.0.0-p648@global/bin:/Users/Chris/.rvm/rubies/ruby-2.0.0-p648/bin:/Users/Chris/anaconda/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/share/dotnet:/Library/TeX/texbin:/Users/Chris/.rvm/bin:/Users/Chris/.rvm/bin \\\r\n    PYTHON_BIN_PATH=/Users/Chris/anaconda/bin/python \\\r\n    PYTHON_LIB_PATH=/Users/Chris/anaconda/lib/python2.7/site-packages \\\r\n    TF_CUDA_CLANG=1 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=3.5,5.2 \\\r\n    TF_CUDA_VERSION='' \\\r\n    TF_CUDNN_VERSION='' \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_OPENCL=0 \\\r\n    TMPDIR=/var/folders/tr/tl261hjj2wl4662x8msxh6fm0000gn/T/ \\\r\n  external/bazel_tools/tools/cpp/link_dynamic_library.sh no ignored ignored ignored external/local_config_cc/cc_wrapper.sh -shared -o bazel-out/local-opt/bin/external/protobuf/python/google/protobuf/internal/_api_implementation.so -Wl,-force_load,bazel-out/local-opt/bin/external/protobuf/_objs/python/google/protobuf/internal/_api_implementation.so/external/protobuf/python/google/protobuf/internal/api_implementation.pic.o -lstdc++ -lm -undefined dynamic_lookup -headerpad_max_install_names -lgcov): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nld: library not found for -lgcov\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n```\r\n", "comments": ["I see you've listed two TF versions, can you explain what you mean by tensorflow-gpu? Are these two separate installs?\r\n\r\nWere you successfully able to follow all the steps at https://www.tensorflow.org/install/install_sources? I see that you're trying to build tensorboard, so I'm not sure if you've already done all the steps from the docs or not.", "I think it's probably this:\r\nhttp://stackoverflow.com/questions/17059961/library-not-found-for-lgcov\r\n", "@skye Thank you for the comment.\r\nI built it again from source code.\r\nLet me fix my tensorflow version to \r\n`('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')`\r\n\r\n@drpngx Thanks! I'll check this issue from your link.\r\nFor `-lgcov`, I'll leave a comment when I found the solution for MacOS.\r\n\r\nBy the way, I ran the `bazel coverage` command on my ubuntu machine to avoid `-lgcov` issue. \r\nAnd I got another problem.\r\n\r\n```\r\nbazel coverage --test_strategy=standalone --test_output=streamed //tensorflow/tensorboard/backend:application_test --collect_code_coverage --instrumentation_filter=// --linkopt=--coverage --linkopt=-lgcov --linkopt=-lgcc --linkopt=-lc --nocache_test_results --test_output=all\r\n```\r\n\r\nAnd the error log is like below.\r\n```\r\n...\r\n----------------------------------------------------------------------\r\nRan 62 tests in 21.883s\r\n\r\nOK\r\n================================================================================\r\nERROR: /home/ubuntu/tensorflow/tensorflow/tensorboard/backend/BUILD:73:1: output 'tensorflow/tensorboard/backend/application_test/coverage.dat' was not created.\r\nERROR: /home/ubuntu/tensorflow/tensorflow/tensorboard/backend/BUILD:73:1: not all outputs were created or valid.\r\nTarget //tensorflow/tensorboard/backend:application_test up-to-date:\r\n  bazel-bin/tensorflow/tensorboard/backend/application_test\r\nINFO: Elapsed time: 2082.067s, Critical Path: 1751.51s\r\n//tensorflow/tensorboard/backend:application_test                        PASSED in 23.1s\r\n\r\nExecuted 1 out of 1 test: 1 test passes.\r\nAll tests passed but there were other errors during the build.\r\n```\r\n\r\nI found similar code coverage issue on bazel issues page.\r\nhttps://github.com/bazelbuild/bazel/issues/1118\r\n\r\nIt seems like they are trying solve this problem(`coverage.dat was not created`).", "I got the right reason for this issue.\r\nCurrently, bazel does not support `coverage` for python test codes.\r\nAnd some people got it working now for C++.\r\n\r\nReplied as below\r\n```\r\nPython, definitely not. C++, some people got it working, but it's\r\nbest-effort at the moment.\r\n```", "Sounds good, thanks for finding out!"]}, {"number": 9642, "title": "Link to gcc_s and gcc if compiler is GCC version 5. Fixes issue #9593", "body": "Link to gcc_s and gcc if compiler is GCC version 5 to fix issue #9593 ", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 9641, "title": "Added identity op and a fix for LRN", "body": "This is branched off the current PR so please review starting with the last 3 commits.", "comments": ["Can one of the admins verify this patch?", "(we'll review the other PR first, and then once that's in, we can review this one with just the diffs).", "Please proceed with the review :)", "btw our contribution page now has some helpful tools to deal with style issues: https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#c-coding-style", "Style issues should be fixed now. I think we can proceed with unit tests.", "@tensorflow-jenkins test this please", "\ud83d\udc4d "]}]