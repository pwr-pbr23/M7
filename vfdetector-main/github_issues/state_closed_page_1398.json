[{"number": 11117, "title": "extract common functions", "body": "1. extract common function(ComputeStride) which used both in Transpose and Tile ops.\r\n2. merge same code in Transpose\r\n\r\nrelated to https://github.com/tensorflow/tensorflow/pull/10793 and https://github.com/tensorflow/tensorflow/issues/8873", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 11116, "title": "enable mkl in eigen for qr_op", "body": "#7128 ", "comments": ["Can one of the admins verify this patch?", "`--define=using_mkl=true` must be added when using bazel to build, sometimes you need to clean `/path/to/tensorflow/third_party/mkl` first(remove `.so` file),  I added `/path/to/tensorflow/third_party/mkl` to `LD_LIBRARY_PATH` in `~/.bash_profile`.", "Jenkins, test this please.", "I'm a bit wary of having special instructions out. Could you add them in the documentation?\r\n\r\n@benoitsteiner WDYT?", "it seems mkl is not enabled even if I configure to use mkl when I build from source, so maybe it's still under developing?", "/CC: @gunan ", "We made a recent change about how we build with MKL.\r\nI removed the configure option for MKL, but now you can just add `--config=mkl` to your build command to build with MKL support."]}, {"number": 11115, "title": "Unecessary type checking for shape_invariants in tf.while_loop()", "body": "Apologies in advance if this is not the right place to post this or I did something wrong, I am new to GitHhub issues and TensorFlow.\r\n\r\n### Describe the problem\r\nIn `tf.while_loop`, when passing in `shape_invariants`, it is not (easily) possible to specify a shape invariant for a state variable belonging to a `BasicLSTMCell`. This is because the `tf.while_loop` makes a `nest.assert_same_structure(loop_vars, shape_invariants)` call, and uses the default parameter `type_check=True`. what this means, however, is that there is no way to manually pass a nested tuple in to specify an invariant for the state. For example, if the shape invariant for the LSTM state in the `shape_invariants` tuple is\r\n\r\n`tuple(tf.TensorShape((None, size)) for size in lstm_cell.state_size)` \r\n\r\nthen `tf.while_loop` fails with the exception \r\n\r\n`TypeError: The two structures don't have the same sequence type. First structure has type <class 'tensorflow.python.ops.rnn_cell_impl.LSTMStateTuple'>, while second structure has type <type 'tuple'>.`\r\n\r\nHowever, LSTMStateTuple is simply a named tuple, so why does this not work? You can circumvent this restriction with the following code:\r\n\r\n`tf.contrib.rnn.LSTMStateTuple(*tuple(tf.TensorShape((None, size)) for size in lstm_cell.state_size))`\r\n\r\nBut this seems like a hack and just feels wrong. I think that either type checking should be turned off for the purposes of `shape_invariants`, or some more intelligent type checking should be applied. Would this make sense?", "comments": ["I think this makes sense. Would you be willing to create a patch for this?", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 11114, "title": "Make TensorFlow build with wrapper-free MSVC CROSSTOOL", "body": "With the latest Bazel release (0.5.2), we can now build C++ code on Windows without the python wrapper scripts! This gives faster and more reliable build, but gcc flags in BUILD file won't be translated.\r\n\r\nThis change removes gcc flags(mostly warning flags) that will cause an error if passed to cl.exe.\r\n\r\nThe protobuf change is to get https://github.com/google/protobuf/commit/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66, not sure if there is a requirement to use release version of protobuf.\r\n@gunan \r\n\r\n\r\n\r\n", "comments": ["jenkins, test this please", "@jart WDYT?", "@jhseu are we ok to use a non-released protobuf?", "Jenkins, test this please.", "Hmm..  Looks like CI doesn't like my change on protobuf... \r\n", "Generally it's fine to build with a non-released protobuf, but since it was reverted, seems fine to me.", "@gunan Can you update Bazel to 0.5.2 on Windows slaves?", "All machines should now be upgraded.", "@gunan Great! http://ci.tensorflow.org/job/tf-master-win-bzl/1213/console is green again!"]}, {"number": 11113, "title": "Add cosine annealing for learning rate decay", "body": "[SGDR: Stochastic Gradient Descent With Warm Restarts](https://openreview.net/pdf?id=Skq89Scxx), proposes decaying the learning rate according to\r\n\r\n![image](https://user-images.githubusercontent.com/2202312/27641761-1bf302c8-5c1d-11e7-8d4b-15988701ff3f.png)\r\n\r\n\r\nwhere ![image](https://user-images.githubusercontent.com/2202312/27641775-28b4e0a8-5c1d-11e7-8adc-1dcaea55d77c.png) is the minimum step length, ![image](https://user-images.githubusercontent.com/2202312/27641798-3be6a80a-5c1d-11e7-9b41-d3c25b0b2b96.png) is the maximum step length, ![image](https://user-images.githubusercontent.com/2202312/27641818-4686daf0-5c1d-11e7-8991-533ce8710e8f.png) is the global step and ![image](https://user-images.githubusercontent.com/2202312/27641841-5594f2ac-5c1d-11e7-9f54-e3cc8ccd1566.png) is the maximum number of iterations.\r\n\r\nI've personally found this strategy to  be easy to use given that the number of hyperparameters is relatively small and results are good.\r\n\r\nIs this something we want added to tensorflow? Would you accept submissions?\r\n\r\n\r\n\r\n", "comments": ["Can i get assigned to this ?", "@ebrevdo @martinwicke can you comment?", "@shin-migami Consider yourself assigned!\r\n\r\nThe right spot for this is in contrib.\r\n\r\nI cannot formally assign you, but everybody who wants to work on this should check this issue first, and find that you are already working on it. Please reference this issue in any PR you create, and update the issue when you are making progress.", "Adding contributions welcome tag just so it's clear that this issue has been triaged: this doesn't contradict the fact that @shin-migami is working on it!", "Hi @shin-migami \r\nI was asked by Ilya and Frank (authors of the paper) to implement this learning rate decay algorithm in tensorflow. Now I found out you also work on it. \r\n\r\nWe would like to include more general version where you can also use restarts.\r\nDid you make any progress already?\r\n", "I guess this can be closed now?"]}, {"number": 11112, "title": "What is the possible substitute of cc_ops for android/mobile environment", "body": "OS: Ubuntu 16.04 64bits\r\nAndroid Version: 7.1 (Nougat)\r\nNDK Version: android-ndk-r12b\r\nDevelopment: develop shared library that exposes op's to NDK CPP application.\r\n\r\nI am aware, that there is no explicit support for cc_ops on android/mobile env,\r\nI am using few methods in my application. \r\n\r\n```\r\ntensorflow::ops::ReadFile` \r\nDecodeJpeg\r\nConst(root.WithOpName(\"size\"), {input_height, input_width})\r\n```\r\n\r\n\r\n\r\nI just copied these headers from **//bazel-out** to **//tensorflow**\r\nbut it requires dependencies, \r\n\r\n```\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/imgproject/imgproject_model.cc':\r\n  '/tensorflow/tensorflow/tensorflow/tensorflow/cc/ops/const_op.h'\r\n  '/tensorflow/tensorflow/tensorflow/tensorflow/cc/framework/ops.h'\r\n  '/tensorflow/tensorflow/tensorflow/tensorflow/cc/framework/scope.h'\r\n  '/tensorflow/tensorflow/tensorflow/tensorflow/cc/ops/array_ops.h'\r\n  '/tensorflow/tensorflow/tensorflow/tensorflow/cc/ops/io_ops.h'\r\n  '/tensorflow/tensorflow/tensorflow/tensorflow/cc/ops/image_ops.h'\r\n  '/tensorflow/tensorflow/tensorflow/tensorflow/cc/ops/math_ops.h\r\n```\r\n\r\nI am not sure whether libtensorflow_inference.so can be used with this\r\nIs there any possible substitute for above op's in android/mobile?\r\nplease provide pointers.\r\n\r\n--\r\nthanks", "comments": ["You probably want to be using [tensorflow/core:android_tensorflow_lib](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/BUILD#L989) or //tensorflow/contrib/android:libtensorflow_inference.so\r\n\r\nYou can attempt to add ops to the dependencies of those libraries but note that DecodeJpeg is not guaranteed to work on Android without some hassle (I'd recommend stripping out that node with optimize_for_inference and passing in the image raster directly to the nodes it feeds).\r\n\r\nFor further assistance on this issue please refer to StackOverflow, as this is neither a bug nor a feature request. You'll find more community support there."]}, {"number": 11111, "title": "Remove implicit iteration from third_party/toolchains/cpu/CROSSTOOL", "body": "Bazel will stop supporting implicit iteration soon, and will require explicit\r\n'iterate_over' message. This cl updates the only affected Tensorflow crosstool.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Transient error.\r\n\r\nJenkins, test this please.", "Thank you for a super quick review and merge!"]}, {"number": 11110, "title": "change an error in annotation`", "body": "there is an error in annotation of ops.py in tensorflow\\tensorflow\\python\\ops. I think it is just a spelling mistake", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "\"iff\" means \"if and only if\"\r\nhttps://www.merriam-webster.com/dictionary/iff"]}, {"number": 11109, "title": "How can i use saved_model api to save chatbot model? ", "body": "Rencently i train a chatbot model based on seq2seq model , but i failed to save the model use saved_model api! So i wonder to know where i can get some demo about this task?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\n(When you do ask there, you should probably include more detail on what exactly you tried and what failed)"]}, {"number": 11108, "title": "fixing a single typo", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please."]}, {"number": 11107, "title": "[Java] Add support for list(string) attributes in OperationBuilder", "body": "Feature: implement setAttrStringList method in Operation Builder with Java api", "comments": ["Can one of the admins verify this patch?", "Hi, @asimshankar  please help me check this pr, many thanks!", "Jenkins, test this please.", "@asimshankar ok, will be fixed, thank you.", "@asimshankar Indeed I didn't find a core operations which contains the list(string) attribute by now, however I saw it was implemented in go. Also I found a todo in OperationBuilder.java. So I'm wondering perhaps It will be needed in some certain operation if created in future. So do you think it is unnecessary?", "Jenkins, test this please."]}, {"number": 11106, "title": "Feature: implement setAttrStringList method in Operation Builder with java api", "body": "Implement setAttrStringList method in Operation Builder with java api.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "@asimshankar please help me check this, many thanks!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "Will open another pr."]}, {"number": 11105, "title": "Fix broken link in programmers_guide/embedding.md", "body": "This fix fixes broken link in programmers_guide/embedding.md:\r\n\r\n`tensorflow/tensorboard -> tensorflow/contrib/tensorboard`\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 11104, "title": "Bug fixes to input function tutorial", "body": "I tried running this example today and hit a few issues. This PR fixes the issues I found:\r\n\r\n- DNNRegressor is located within `tf.contrib.learn.DNNRegressor` not `tf.estimator.DNNRegressor`\r\n- the method `train` does not exist, it's `fit`\r\n- returning `p` instead of `p['predictions']`\r\n\r\nWith the above fixes I was able to get the example running as expected.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please.", "Hi,\r\n\r\nThanks for the patch, but I don't think we can take this one.\r\n\r\nThis is part of the effort to get the docs updated *ahead* of moving the canned estimators out of `tf.contrib.learn` into `tf.estimators`. \r\n\r\nThe current version is consistent with the behavior of `tf.estimator.Estimator`.\r\n\r\nThe 1.2 docs and examples work, but `master` is always a work in progress.\r\n\r\nThanks Again."]}, {"number": 11103, "title": "Batch normalization layer has new name for each call to `__init__`", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OSX 10.12.5\r\n- **TensorFlow installed from (source or binary)**: `pip install`\r\n- **TensorFlow version (use command below)**: 1.1.0\r\n- **CUDA/cuDNN version**: no GPU\r\n- **Exact command to reproduce**:\r\n```python\r\nimport tensorflow as tf\r\nx = tf.placeholder(tf.float32, [None, 28])\r\nx_test = tf.placeholder(tf.float32, [None, 28])\r\nnormalized_x = tf.layers.batch_normalization(x, training=True, reuse=None)\r\nnormalized_x_test = tf.layers.batch_normalization(x_test, training=False, reuse=True)\r\n```\r\n\r\n### Describe the problem\r\nThis is the error I get:\r\n```\r\nValueError: Variable batch_normalization_1/beta does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n```\r\nHowever, I can fix it by setting the name of the batch normalization layer. I think the default behavior should not set a new name each time I create a new batch normalization layer (even if it's an easy thing to debug).\r\n\r\n### Source code / logs\r\nWorking piece of code:\r\n```python\r\nimport tensorflow as tf\r\nx = tf.placeholder(tf.float32, [None, 28])\r\nx_test = tf.placeholder(tf.float32, [None, 28])\r\nnormalized_x = tf.layers.batch_normalization(x, training=True, reuse=None, name=\"batch_normalization\")\r\nnormalized_x_test = tf.layers.batch_normalization(x_test, training=False, reuse=True, name=\"batch_normalization\")\r\n```\r\n", "comments": ["@fchollet can you comment?", "User error; in order to reuse a layer by name you should be giving it a name via the `name` argument:\r\n\r\n```python\r\nnormalized_x = tf.layers.batch_normalization(\r\n    x, training=True, reuse=None, name='my_bn')\r\nnormalized_x_test = tf.layers.batch_normalization(\r\n    x_test, training=False, reuse=True, name='my_bn')\r\n```\r\n\r\nIndeed, if you don't provide a name to your layer, TF has no way to know *which* layer you are referring to when specifying `reuse=True`.", "Sorry to answer only now. I understand the fix that you suggested and had applied it.\r\nHowever, I don't understand with your explanation how the following works:\r\n```python\r\nimport tensorflow as tf\r\nx = tf.placeholder(tf.float32, [None, 28])\r\nx_test = tf.placeholder(tf.float32, [None, 28])\r\nwith tf.variable_scope(\"forward\"):\r\n        normalized_x = tf.layers.batch_normalization(\r\n            x, training=True, reuse=None)\r\nwith tf.variable_scope(\"forward\"):\r\n        normalized_x_test = tf.layers.batch_normalization(\r\n            x_test, training=False, reuse=True)\r\n```\r\n and this doesn't:\r\n```python\r\nimport tensorflow as tf\r\nx = tf.placeholder(tf.float32, [None, 28])\r\nx_test = tf.placeholder(tf.float32, [None, 28])\r\nwith tf.variable_scope(\"forward\"):\r\n        normalized_x = tf.layers.batch_normalization(\r\n            x, training=True, reuse=None)\r\n        normalized_x_test = tf.layers.batch_normalization(\r\n            x_test, training=False, reuse=True)\r\n```\r\nIf I follow your explanation correctly, both shouldn't work, right?\r\n"]}, {"number": 11102, "title": "using output_layer in seq2seq model?", "body": "Os\uff1aliunx\r\nversion: v1.2.0\r\n\r\ni am writing a seq2seq model using tensorflow. my problem is \r\n\r\nhelper = TrainingHelper(...)\r\ndecoder = BasicDecoder(...)\r\ndecoder_outputs, final_state, seq_len  = tf.contrib.seq2seq.dynamic_decode(decoder)\r\nrnn_out, sample_ids = decoder_outputs\r\n\r\nin traing period, we should calculate the loss:\r\nloss = tf.contrib.seq2seq.sequence_loss(rnn_out, targets, weights)\r\n\r\nthe rnn_out's shape should be (batch_size, seq_len, target_vocab_size), but (batch_size, seq_len, cell_hidden_size)\r\n\r\nhow to change the shape ? should i add a output_layer in decoder ? \r\nif add, how to define a output_layer? i found tf.layers.dense has two params (inputs, units), units may be target_vocab_size, but what about inputs?\r\n\r\nIn addition, is the output_layer is same as the beamSearcheDecoder when decoding?\r\n\r\n", "comments": ["helper = TrainingHelper(...)\r\ndecoder = BasicDecoder(...)\r\ndecoder_outputs, final_state, seq_len = tf.contrib.seq2seq.dynamic_decode(decoder, ## **output_time_major=True**)\r\nrnn_out, sample_ids = decoder_outputs\r\n\r\nlogits = tf.layers.dense(rnn_out[-1], target_vocab_size)\r\n", "@TrsNium \r\nthe BasicDecoder has a param output_layer, how can i using this?\r\ndecoder = BasicDecoder(..., output_layer=?)\r\n", "Sorry, I don't know how to use that  param too.\r\nAccording to BasicDecoder's code output_layer should be instance of tf.layersLayer.\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py", "I think the issue is that `tf.contrib.seq2seq.BasicDecoder` expects an instance of the `tf.python.layers.core.Layer` class, e.g., `tf.python.layers.core.Dense`, which is an internal class used in the `tf.layers.dense` function:\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/layers/core.py#L41\r\n\r\nThis can be seen in the tests for `BasicDecoder`:\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/contrib/seq2seq/python/kernel_tests/basic_decoder_test.py#L38\r\n\r\nNote in particular the line:\r\n` output_layer = layers_core.Dense(output_layer_depth, use_bias=False)`\r\n\r\nHowever, I don't think the Dense layer class is exposed in the Python API as of v1.2. Does anyone have any ideas on how to cleanly resolve this?\r\n\r\nEdit: whoops, my mistake, you just have to import it. Here's a minimal example:\r\n```\r\nfrom tensorflow.python.layers.core import Dense\r\noutput_layer = Dense(units=_num_units)\r\n# ... define your decoder RNN cell, initial state, and sampling/training helper here ...\r\ndecoder = tf.contrib.seq2seq.BasicDecoder(\r\n  cell=_rnn_cell_here,\r\n  helper=_training_helper_here,\r\n  initial_state=_initial_state_here,\r\n  output_layer=output_layer)\r\n```", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "I beg to differ since this is an issue caused by unclear documentation. @ali01 ", "BTW, how can I add multiple dense layers to the output layer, Dense(n) is too simple, I want to add more non-linearity", "> BTW, how can I add multiple dense layers to the output layer, Dense(n) is too simple, I want to add more non-linearity\r\n\r\nAfter you get the decoder rnn output, you can pile several tf.layers.dense() to add non-linearity."]}, {"number": 11101, "title": "Negative indices support for tf.gather", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.06\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.2.0\r\n- **Bazel version (if compiling from source)**: 5.2\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\n\r\n`tf.gather` does not support negative indices yet. I have implemented this feature. If you are okay with this feature, then can I create a pull request?", "comments": ["@ebrevdo @martinwicke what do you think?", "ping", "Having just looked at this, I think it runs the risk of slowing down a performance critical kernel with a comparison + addition per index in the CPU kernel (since the inner dimensions of the gather are a bulk memcpy on CPU), and a comparison + addition per *gathered value* in the GPU kernel.\r\n\r\n@AnishShah -- have you run the gather benchmarks with your implementation? Do you have a GPU for benchmarking the GPU kernel?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Feel free to reopen if our reasoning is not sufficient."]}, {"number": 11100, "title": "Fix typos", "body": "This PR fixes some typos: `to to`, `of of`, `that that`, and `this this`.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 11099, "title": "Typo in illustrating figure for XLA/Concatenation operation", "body": "Illustrating image for [Concatenate](https://www.tensorflow.org/performance/xla/operation_semantics#concatenate)\r\nsuggests Concat({ 2x4, 2x8 }, dimension=0) is 2x12. Should be dimension=1, and same for the other examples.", "comments": ["@wolffg @hawkinsp this appears to be true, do either of you know who can fix this?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "This image has been updated."]}, {"number": 11098, "title": "tf.check_numerics does not raise error when used in tf.control_dependencies", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS Sierra 10.12.4\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.2.0\r\n- **Bazel version (if compiling from source)**: none\r\n- **CUDA/cuDNN version**: none\r\n- **GPU model and memory**: none\r\n- **Exact command to reproduce**: run the code below\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n`tf.check_numerics` does not raise error when used in `tf.control_dependencies`.  The code below should raise error, but it does not in TensorFlow 1.2.0\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nx = tf.constant(np.nan)\r\nwith tf.control_dependencies([tf.check_numerics(x, 'nan')]):\r\n    x = tf.identity(x)\r\n\r\nwith tf.Session().as_default():\r\n    print(x.eval())\r\n```", "comments": ["Thanks for reaching out. This works as intended.\r\n\r\ntf.check_numerics adds an operation to the graph that is only executed if its return value is fetched. The code snippet above ignores the return value of tf.check_numerics. The only tensor that is fetched is x, so check_numerics never runs.\r\n\r\nThe right way to use check_numerics is to assign its return value to the handle of the original tensor:\r\n    x = tf.check_numerics(x, 'nan')\r\n", "@ali01 \r\n\r\nI think your justification is weird.  Take a look at the documentation of [tf.control_dependencies](https://www.tensorflow.org/api_docs/python/tf/control_dependencies):\r\n\r\n>  `control_inputs`: A list of Operation or Tensor objects which must be executed or computed before running the operations defined in the context. Can also be None to clear the control dependencies.\r\n\r\nI have fed `tf.check_numerics` into `tf.control_dependencies` via `control_inputs`, no matter what it is, it should have been executed before any operation in the context being called.  And since I set `x = tf.identity(x)` within this control dependencies context, when `x.eval()` is being executed, the `tf.check_numerics` should have been done.\r\n\r\nHowever, it turns out not to have been called.  As a contrary, if you replace `tf.check_numerics` by `tf.Print`, you will see the value of x, right there in the output.  So I think there must be something wrong with `tf.check_numerics` and `tf.control_dependencies`."]}, {"number": 11097, "title": "Checkpoint restore problem with version 1.1.0", "body": "\r\n#saving\r\nimport tensorflow as  tf\r\nv1 = tf.Variable([1,2,3], name=\"v1\")\r\nv2 = tf.Variable([4,5,6], name=\"v2\")\r\ninit_op = tf.global_variables_initializer()\r\nsaver = tf.train.Saver()\r\nwith tf.Session() as sess:\r\n  sess.run(init_op)\r\n  save_path = saver.save(sess, \"/home/abc/Documents/tensorflow/tmp/model.ckpt\")\r\n  print(\"Model saved in file: %s\" % save_path)\r\n\r\n\r\n#restore \r\nv1 = tf.Variable([1,1,1], name=\"v1\")\r\nv2 = tf.Variable([1,1,1], name=\"v2\")\r\nsaver = tf.train.Saver()\r\nwith tf.Session() as sess:\r\n  saver.restore(sess, \"/home/abc/Documents/tensorflow/tmp/model.ckpt\")\r\n  print(\"Model restored.\")\r\n  print(sess.run(v1))\r\n\r\n\r\nNotFoundError: Key v1_5 not found in checkpoint\r\n\t [[Node: save_4/RestoreV2_4 = RestoreV2[dtypes=[DT_INT32], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_4/Const_0, save_4/RestoreV2_4/tensor_names, save_4/RestoreV2_4/shape_and_slices)]]\r\n\r\nCaused by op 'save_4/RestoreV2_4', defined at:\r\n  File \"/home/abc/.conda/envs/tfenv/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/home/noor/.conda/envs/tfenv/lib/python3.5/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/abc/.conda/envs/tfenv/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"/home/abc/.conda/envs/tfenv/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n\r\n\r\n", "comments": []}, {"number": 11096, "title": "C1002 error when building on Windows 10 64 bit, with vs 2017", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 64 bit, version 1511\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master branch, commit 90b2a38a1\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: CPU only\r\n- **Exact command to reproduce**:\r\n\r\n      C:\\cmake-3.9.0-rc4-win64-x64\\bin\\cmake.exe .. -G \"Visual Studio 15 2017 Win64\" ^\r\n      -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=C:\\swigwin-3.0.12\\swig.exe ^\r\n      -DPYTHON_EXECUTABLE=C:\\Users\\x\\.conda\\envs\\tensorflow\\python.exe ^\r\n      -DPYTHON_LIBRARIES=C:\\Users\\x\\.conda\\envs\\tensorflow\\libs\\python35.lib ^\r\n      -Dtensorflow_BUILD_CC_TESTS=ON ^\r\n      -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX\r\n\r\n      C:\\cmake-3.9.0-rc4-win64-x64\\bin\\cmake.exe --build .\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\n~https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh~\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\n~python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"~\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nIssued above build command, and got the following error at last.\r\n\r\n    \"C:\\Users\\X\\github\\tensorflow\\tensorflow\\contrib\\cmake\\build\\ALL_BUILD.vcxproj\" (default target) (1) ->\r\n    \"C:\\Users\\X\\github\\tensorflow\\tensorflow\\contrib\\cmake\\build\\_beam_search_ops.vcxproj\" (default target) (3) ->\r\n    \"C:\\Users\\X\\github\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal.vcxproj\" (default target)\r\n    (4) ->\r\n    \"C:\\Users\\X\\github\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal_static.vcxproj\" (default t\r\n    arget) (5) ->\r\n    \"C:\\Users\\X\\github\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_kernels.vcxproj\" (default target) (108) ->\r\n    (ClCompile target) ->\r\n      c:\\users\\x\\github\\tensorflow\\tensorflow\\contrib\\cmake\\build\\external\\eigen_archive\\eigen\\src\\core\\products\\gener\r\n    alblockpanelkernel.h(2011): fatal error C1002: compiler is out of heap space in pass 2 [C:\\Users\\X\\github\\tensorfl\r\n    ow\\tensorflow\\contrib\\cmake\\build\\tf_core_kernels.vcxproj]\r\n      cl : Command line error D8040: error creating or communicating with child process [C:\\Users\\X\\github\\tensorflow\\\r\n    tensorflow\\contrib\\cmake\\build\\tf_core_kernels.vcxproj]\r\n\r\n\t86 Warning(s)\r\n\t2 Error(s)\r\n\r\nFor the warnings, there are two kinds:\r\n\r\n      C:\\Users\\X\\github\\tensorflow\\tensorflow\\c\\c_api.cc(1938): warning C4190: 'TF_NewWhile' has C-linkage specified,\r\n    but returns UDT 'TF_WhileParams' which is incompatible with C [C:\\Users\\X\\github\\tensorflow\\tensorflow\\contrib\\cma\r\n    ke\\build\\tf_test_lib.vcxproj]\r\n\r\n**and**\r\n\r\n      c:\\users\\X\\github\\tensorflow\\tensorflow\\core\\kernels\\eigen_spatial_convolutions.h(724): warning C4789: buffer ''\r\n     of size 8 bytes will be overrun; 32 bytes will be written starting at offset 0 [C:\\Users\\X\\github\\tensorflow\\tens\r\n    orflow\\contrib\\cmake\\build\\tf_core_kernels.vcxproj]\r\n\r\nI saw there're many complains about *compiler is out of heap space in pass 2* error, and some say adding \"/Zm2000\" to the compiler would solve the problem. I applied this patch:\r\n\r\n    @@ -78,6 +78,8 @@ if(WIN32)\r\n       set(CMAKE_CXX_FLAGS_RELEASE \"${CMAKE_CXX_FLAGS_RELEASE} /D_ITERATOR_DEBUG_LEVEL=0\")\r\n       set(CMAKE_CXX_FLAGS_MINSIZEREL \"${CMAKE_CXX_FLAGS_MINSIZEREL} /D_ITERATOR_DEBUG_LEVEL=0\")\r\n       set(CMAKE_CXX_FLAGS_RELWITHDEBINFO \"${CMAKE_CXX_FLAGS_RELWITHDEBINFO} /D_ITERATOR_DEBUG_LEVEL=0\")\r\n    +  # Increase heap size\r\n    +  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /Zm2000\")\r\n\r\nBut did not solve this problem.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["My computer has 16 GB of memory, and during the course of the compilation, the memory usage is below 55%.", "~~This issue seems to be related to [88a6cde](https://github.com/tensorflow/tensorflow/commit/88a6cdeb5ee79765462932a611d4d16dd715007c) and a solution can be found at [#9470](https://github.com/tensorflow/tensorflow/issues/9470).~~\r\n\r\n~~The error only occurs if you try to compile tf with CUDA support and AVX optimizations enabled. \r\nTry removing~~\r\n\r\n~~lines [109 to 114] from [reduction_ops_gpu.cu.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/reduction_ops_gpu.cu.cc#L109-L114)~~\r\n\r\n~~lines [41 to 42] from [reduction_ops_mean.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/reduction_ops_mean.cc#L41-L42)~~\r\n\r\n~~lines [42 to 43] from [reduction_ops_prod.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/reduction_ops_prod.cc#L42-L43)~~\r\n\r\n~~lines [41 to 42] from [reduction_ops_sum.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/reduction_ops_sum.cc#L41-L42)~~\r\n\r\nEdit: Misread and thought this was CUDA related. However the builds below should still be valid.\r\n\r\nAdditionally I also provide builds from my repo [aluo-x/tensorflow_windows](https://github.com/aluo-x/tensorflow_windows)", "@mrry, could you take a quick look?", "HI @davidshen84 ,\r\n\r\nThis may or may not help you.\r\n\r\nI've encountered several heap errors with VS2015 when trying to build tensorflow. The problem was that I was using the 32bits compiler instead of the 64bits one. This is mentioned as one of the first step in\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake (although their path does not exist for me).\r\n\r\nYou can easily check by looking at the Task Manager when compiling. In Windows 10, go to the Details tab and add the Platform column. See if the CL.EXE instances are 32bits or 64bits.\r\n\r\nIn order to use the 64bits CL.EXE, I had to start MSBUILD on the command line via \"VS2015 x64 Native Tools Command Prompt\". I haven't tried VS2017 yet, but have a look at \"x64 Native Tools Command Prompt for VS 2017\" in your start menu.\r\n\r\nTell us if that works.\r\n\r\nPS. I am now stuck with the CUDA compiler error mentioned above by @aluo-x.", "I am sure I was using the **Native 64 Command Tools**. I also double confirmed by:\r\n\r\n    cl /?\r\n    Microsoft (R) C/C++ Optimizing Compiler Version 19.10.25019 for x64\r\n    Copyright (C) Microsoft Corporation.  All rights reserved.\r\n\r\n\t\t\t     C/C++ COMPILER OPTIONS\r\n\r\nMore over, I used the **Visual Studio 15 2017 Win64** cmake generator. I think the compilation process won't be able to start if the compiler is 32 bit.\r\n\r\n\r\nThanks.\r\n", "I tried not using SIMD options, and I still get similar errors\r\n\r\n    \"C:\\Users\\X\\github\\tensorflow\\tensorflow\\contrib\\cmake\\build\\ALL_BUILD.vcxproj\"\r\n     (default target) (1) ->\r\n    \"C:\\Users\\X\\github\\tensorflow\\tensorflow\\contrib\\cmake\\build\\_beam_search_ops.v\r\n    cxproj\" (default target) (3) ->\r\n    \"C:\\Users\\X\\github\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_\r\n    internal.vcxproj\" (default target) (4) ->\r\n    \"C:\\Users\\X\\github\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_\r\n    internal_static.vcxproj\" (default target) (5) ->\r\n    \"C:\\Users\\X\\github\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_kernels.vc\r\n    xproj\" (default target) (108) ->\r\n    (ClCompile target) ->\r\n      c:\\users\\x\\github\\tensorflow\\tensorflow\\contrib\\cmake\\build\\external\\eigen_ar\r\n    chive\\eigen\\src\\core\\products\\generalblockpanelkernel.h(2011): fatal error C1002: co\r\n    mpiler is out of heap space in pass 2 [C:\\Users\\X\\github\\tensorflow\\tensorflow\\\r\n    contrib\\cmake\\build\\tf_core_kernels.vcxproj]\r\n      c:\\users\\x\\github\\tensorflow\\tensorflow\\contrib\\cmake\\build\\external\\eigen_ar\r\n    chive\\eigen\\src\\core\\products\\generalblockpanelkernel.h(2011): fatal error C1002: co\r\n    mpiler is out of heap space in pass 2 [C:\\Users\\X\\github\\tensorflow\\tensorflow\\\r\n    contrib\\cmake\\build\\tf_core_kernels.vcxproj]\r\n      cl : Command line error D8040: error creating or communicating with child process\r\n    [C:\\Users\\X\\github\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_kernels.vc\r\n    xproj]\r\n\r\n\t78 Warning(s)\r\n\t3 Error(s)\r\n\r\nMaybe it is a problem with vs 2017 compiler, not with tensorflow code?", "So after testing my successful CPU only AVX enabled build using VS 2015, I have found that it is seriously broken. Specifically I believe there is something with batchnorm. There is a [discussion here](https://github.com/aluo-x/tensorflow_windows/issues/1). Tensorflow throws errors in a number of different ways non-deterministically. The exact same code runs fine with batchnorm removed, or with the batchnorm layer but on a stock build.\r\n\r\nExtra note, my CPU only builds are built using unmodified code, unlike my GPU builds which contain modifications to work around the imaginary number error.", "I just tried building with VS 2015, got the same error...maybe it is the master branch that is broken? I will try tag v1.2.", "Finally, I got the build pass. I think the problem is not related to the `AVX2` option, but the `  -Dtensorflow_BUILD_CC_TESTS=ON` option.\r\n\r\nI guess some of the unit tests are really complicate and relies on features only available on *gcc*.\r\n\r\nIt would be nice to verify the build with unit test on Windows, but I do not think it is of high priority. :)", "@davidshen84 Could you try running the code I [linked to here](https://github.com/aluo-x/tensorflow_windows/issues/1#issuecomment-312762361), but with the batch norm layer. My builds were successful, but did not have correct behavior when running batch norm. ", "@aluo-x , I uncommented your code and used\r\n\r\n        h_flat = tf.reshape(h_norm4, [-1, 28 * 28 * 16])\r\n\r\nThe training is slow, but no error so far.\r\n\r\nBut in terms of normalization, I think you should have a separated step to normalize all your data at once and save it some where; then use the normalized data, rather than normalize the data as they are loaded.", "The code was meant to be a functional test. Could you give the full configuration for your build? VS version, python version, commands used etc.", "# VS version\r\n\r\n    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC>msbuild /version\r\n    Microsoft (R) Build Engine version 14.0.25420.1\r\n    Copyright (C) Microsoft Corporation. All rights reserved.\r\n\r\n    14.0.25420.1\r\n\r\n    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC>cl\r\n    Microsoft (R) C/C++ Optimizing Compiler Version 19.00.24215.1 for x64\r\n    Copyright (C) Microsoft Corporation.  All rights reserved.\r\n\r\n# cmake command\r\n\r\n    C:\\cmake-3.9.0-rc4-win64-x64\\bin\\cmake.exe .. -G \"Visual Studio 15 2017 Win64\" ^\r\n    -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=C:\\swigwin-3.0.12\\swig.exe ^\r\n    -DPYTHON_EXECUTABLE=C:\\Users\\User\\.conda\\envs\\tensorflow\\python.exe ^\r\n    -DPYTHON_LIBRARIES=C:\\Users\\User\\.conda\\envs\\tensorflow\\libs\\python35.lib ^\r\n    -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX\r\n\r\n# python version\r\n\r\n    Python 3.5.3 :: Continuum Analytics, Inc.\r\n\r\n# Command used to build\r\n\r\n    msbuild /p:Configuration=Release all_project.vcxproj\r\n    msbuild /p:Configuration=Release tf_python_build_pip_package.vcxproj\r\n\r\nAfter that, I created a python virtual environment and installed the output `whl` file.", "Thank you so much. The CPU AVX build using VS2017 and Python 3.5.3 from Anaconda seems to working fine. MSVC failed a few times with C1060 out of heap space, consistently with cwise_op_*, but eventually it worked.\r\n\r\nGPU + AVX build is still failing with VS 2017 with internal compiler errors (compared to VS 2015 which created partially functional builds). \r\n\r\nEdit: \r\nCPU AVX Build here: https://github.com/aluo-x/tensorflow_windows", "@aluo-x In the link you given, you use `VS 2015` and `VS 2017` interchangeably. Could you please check? Because I could not get the build work with `VS 2017`. ", "To clarify I am using VS 2017. The CPU build failed the first few times but eventually worked. GPU version is still not working (either internal compiler error or 2nd pass out of heap space). \r\n\r\nIn contrast to VS 2015, where both the CPU and GPU version were able to be built without error first go. \r\nHowever I have noted before that my builds using Intel distribution for Python and VS 2015 builds were corrupt. I think the Intel distribution is probably at fault here, but haven't had to to check due to the long build times.", "I followed the steps by @davidshen84 with a few differences:\r\n\r\n1. **cmake command:**\r\n```\r\ncmake .. -G \"Visual Studio 15 2017 Win64\" ^\r\n-DCMAKE_BUILD_TYPE=Release ^\r\n-DSWIG_EXECUTABLE=C:/tools/swigwin-3.0.12/swig.exe ^\r\n-DPYTHON_EXECUTABLE=C:/Python36/python.exe ^\r\n-DPYTHON_LIBRARIES=C:/Python36/libs/python36.lib ^\r\n-Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX\r\n```\r\n\r\n2. **Command used to build:**\r\n```\r\n\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\MSBuild\\15.0\\Bin\\amd64\\MSBuild.exe\" /m:2 /p:CL_MPCount=1 /p:Configuration=Release /p:Platform=x64 /p:PreferredToolArchitecture=x64 ALL_BUILD.vcxproj\r\n\r\n\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\MSBuild\\15.0\\Bin\\amd64\\MSBuild.exe\" /m:2 /p:CL_MPCount=1 /p:Configuration=Release /p:Platform=x64 /p:PreferredToolArchitecture=x64 tf_python_build_pip_package.vcxproj\r\n```\r\n\r\n- Despite being in the 64-bit VS 2017 command line, I still had to explicitly run the 64-bit version of MSBuild. The default is 32-bit. \r\n- I used `/m:2` and `/p:CL_MPCount=1` to limit the number of parallel compilations running in order to avoid running out of heap space.\r\n- I'm not sure `/p:Platform=x64` and `/p:PreferredToolArchitecture=x64` added any value, but had them in there anyway to explicitly emphasize 64-bit tools.\r\n", "@kalengi Did you successfully build the CPU-only version? Did you try building the GPU-version?", "I built the CPU-only version successfully on Windows 7. Haven't tried building the GPU version yet.", "@aluo-x Building GPU version is not working for me as well. Did you have any solutions?", "     7>Done Building Project \"D:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_framework.vcxproj\" (default targets) -- FAILED.\r\n     6>Done Building Project \"D:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_cc_framework.vcxproj\" (default targets) -- FAILED.\r\n    48>Done Building Project \"D:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_cc_ops.vcxproj\" (default targets) -- FAILED.\r\n    47>Done Building Project \"D:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_cc_while_loop.vcxproj\" (default targets) -- FAILED.\r\n     5>Done Building Project \"D:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_c.vcxproj\" (default targets) -- FAILED.\r\n\r\nDid anyone has similar error messages?", "@chrisplyn I did not attempt a build of 1.3.0 because #11865 never got a reply. I will try again once 1.4.0 gets released with cudnn 7 compatibility. \r\nExisting successful builds can be found in my repo.", "@aluo-x I can't even build the CPU version with AVX enable, do you know what might cause that?", "@kalengi I can't build CPU version, I encountered can't open pywrap_lib error ... ", "@chrisplyn By the time you see the message `7>Done Building Project \"D:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_framework.vcxproj\" (default targets) -- FAILED.` the corresponding error will have been displayed long before. Try logging the build output messages to a file so that you can search later for the actual error that caused the build to fail. Using **Powershell** you can view the messages on screen AND output to log file like this:\r\n\r\n`powershell \"& 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\MSBuild\\15.0\\Bin\\amd64\\MSBuild.exe' /p:Configuration=Release ALL_BUILD.vcxproj | tee 'C:\\logs\\tensorflow_build.log' \"`", "@chrisplyn @kalengi I have 1.4.0 AVX2 (VS2017), Python 3.6.3 builds [on my repo here](https://github.com/aluo-x/tensorflow_windows). ", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Looks like @aluo-x has figured out a solution. \ud83d\udc4d "]}, {"number": 11095, "title": "Symbol not found with adding new op", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac Sierra\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.1.0\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: python test.py\r\n\r\n### Describe the problem\r\nCreated a custom op, but cannot import it from python. Error at command line:\r\n\r\n```\r\ntensorflow.python.framework.errors_impl.NotFoundError: dlopen(src/./ops/build/preprocessing.so, 6): Symbol not found: __ZN10tensorflow14AugmentFunctorIN5Eigen9GpuDeviceEEclERKS2_iiiiiiiPKfPfS7_\r\n  Referenced from: src/./ops/build/preprocessing.so\r\n  Expected in: flat namespace\r\n in src/./ops/build/preprocessing.so\r\n\r\n```\r\n\r\n### Source code / logs\r\nMakefile\r\n```\r\nTF_INC = `python -c \"import tensorflow; print(tensorflow.sysconfig.get_include())\"`\r\n\r\nifndef CUDA_HOME\r\n    CUDA_HOME := /usr/local/cuda\r\nendif\r\n\r\nCC        = gcc -O2 -pthread\r\nCXX       = g++\r\nGPUCC     = nvcc\r\nCFLAGS    = -std=c++11 -I$(TF_INC) -I\"$(CUDA_HOME)/include\" -DGOOGLE_CUDA=1\r\nGPUCFLAGS = -c\r\nLFLAGS    = -pthread -shared -fPIC\r\nGPULFLAGS = -x cu -Xcompiler -fPIC\r\nCGPUFLAGS = -L$(CUDA_HOME)/lib -L$(CUDA_HOME)/lib64 -lcudart -undefined dynamic_lookup\r\n\r\nOUT_DIR   = src/ops/build\r\nPREPROCESSING_SRC = \"src/ops/preprocessing/preprocessing.cc\" \"src/ops/preprocessing/kernels/data_augmentation.cc\"\r\nGPU_SRC_DATA_AUG  \t= \"src/ops/preprocessing/kernels/data_augmentation.cu.cc\"\r\nGPU_PROD_DATA_AUG \t= $(OUT_DIR)/data_augmentation.o\r\nPREPROCESSING_PROD\t= $(OUT_DIR)/preprocessing.so\r\n\r\npreprocessing:\r\n\t$(GPUCC) -g $(CFLAGS) $(GPUCFLAGS) $(GPU_SRC_DATA_AUG) $(GPULFLAGS) $(GPUDEF) -o $(GPU_PROD_DATA_AUG)\r\n\t$(CXX) -g $(CFLAGS)  $(PREPROCESSING_SRC) $(GPU_PROD_DATA_AUG) $(LFLAGS) $(CGPUFLAGS) -o $(PREPROCESSING_PROD)\r\n\r\n```\r\n\r\ntest.py\r\n```\r\nimport tensorflow as tf\r\n_preprocessing_ops = tf.load_op_library(\r\n    tf.resource_loader.get_path_to_datafile(\"./ops/build/preprocessing.so\"))\r\n```\r\n\r\ndata_augmentation.h\r\n```\r\n#ifndef FLOWNET_DATA_AUGMENTATION_H_\r\n#define FLOWNET_DATA_AUGMENTATION_H_\r\n\r\nnamespace tensorflow {\r\ntemplate<typename Device>\r\nstruct AugmentFunctor {\r\n  void operator()(const Device& d);\r\n};\r\n} // namespace tensorflow\r\n#endif // FLOWNET_DATA_AUGMENTATION_H_\r\n```\r\n\r\ndata_augmentation.cc\r\n```\r\n#define EIGEN_USE_THREADS\r\n\r\n#include \"data_augmentation.h\"\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n\r\nnamespace tensorflow {\r\ntypedef Eigen::ThreadPoolDevice CPUDevice;\r\ntypedef Eigen::GpuDevice        GPUDevice;\r\n\r\ntemplate<>\r\nstruct AugmentFunctor<CPUDevice>{\r\n  void operator()(const CPUDevice& d) {\r\n    // CPU implementation here\r\n  }\r\n};\r\n\r\ntemplate<typename Device>\r\nclass DataAugmentation : public OpKernel {\r\n  public:\r\n    explicit DataAugmentation(OpKernelConstruction *ctx) : OpKernel(ctx) {}\r\n\r\n    void Compute(OpKernelContext *ctx) override {\r\n      // Perform augmentation either on CPU or GPU\r\n      AugmentFunctor<Device>()(ctx->eigen_device<Device>());\r\n    }\r\n};\r\n\r\nREGISTER_KERNEL_BUILDER(Name(\"DataAugmentation\")\r\n                        .Device(DEVICE_CPU),\r\n                        DataAugmentation<CPUDevice>)\r\n\r\n#if GOOGLE_CUDA\r\n\r\nREGISTER_KERNEL_BUILDER(Name(\"DataAugmentation\")\r\n                        .Device(DEVICE_GPU),\r\n                        DataAugmentation<GPUDevice>)\r\n#endif // GOOGLE_CUDA\r\n} // namespace tensorflow\r\n```\r\ndata_augmentation.cu.cc\r\n```\r\n#if GOOGLE_CUDA\r\n\r\n#define EIGEN_USE_GPU\r\n\r\n#include \"augmentation_base.h\"\r\n#include \"data_augmentation.h\"\r\n#include \"tensorflow/core/util/cuda_kernel_helper.h\"\r\n\r\nnamespace tensorflow {\r\n__global__ void SpatialAugmentation() {\r\n   // CUDA kernel code goes here\r\n}\r\n\r\ntemplate<>\r\nstruct AugmentFunctor<GPUDevice>{\r\n  void operator()(const GPUDevice& d) {\r\n    // GPU implementation goes here\r\n    CudaLaunchConfig config = GetCudaLaunchConfig(10, d);\r\n    SpatialAugmentation<<<config.block_count, config.thread_per_block, 0, d.stream()>>>(config.virtual_thread_count);\r\n  }\r\n};\r\n\r\ntypedef Eigen::GpuDevice GPUDevice;\r\ntemplate struct AugmentFunctor<GPUDevice>;\r\n} // namespace tensorflow\r\n```", "comments": ["@petewarden can you comment or redirect? Thanks!", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Assignee @tatatodd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatatodd: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatatodd: It has been 132 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Assigning to @allenlavoie, who knows more about custom ops / dynamic loading.", "Probably fixed in TF 1.4+? Let me know if it's still an issue (following the [updated custom op instructions](https://www.tensorflow.org/extend/adding_an_op#gpu_support)) and I can investigate, but I'll close for now. "]}, {"number": 11094, "title": "Branch 160346151", "body": "", "comments": ["cc @ebrevdo @craffel", "This is ready to be merged once the builds pass.", "@tensorflow-jenkins test this please.", "Strange it reports a Mac error at that commit, but the Mac looks green.\r\n\r\n```\r\nERROR: /private/var/tmp/_bazel_jenkins/13e370a18c169b19baeafefb05212b85/external/local_config_python/BUILD:138:1: declared output 'external/local_config_python/numpy_include/numpy/__multiarray_api.h' is a dangling symbolic link.\r\n```", "ok, it was a stale result."]}, {"number": 11093, "title": "Python checkpoint to use with C++", "body": "I have tried everything from freeze_graph.py to bazel  to try and use a python trained saved checkpoint of a model in c++.\r\n\r\nWhy is it so complicated in TF?Caffe was so much easier.!!\r\n\r\nThese are the steps that I follow:\r\n\r\n1)While training I added the following line just before saving each checkpoint :  \r\ntf.train.write_graph(sess.graph_def, 'modelsprototxt/', 'trainingmodel.pb', as_text=True)\r\n\r\n2)I used freeze_graphy.py to send in a trained checkpoint file,the written graph fle and the output graph file and I get the following error:\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/anarayanan/TenserflowPlayground/TF_DEEPLAB_UNTOUCHED/tensorflow-deeplab-resnet-master/freeze_graph.py\", line 175, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/home/anarayanan/TenserflowPlayground/TF_DEEPLAB_UNTOUCHED/tensorflow-deeplab-resnet-master/freeze_graph.py\", line 172, in main\r\n    FLAGS.output_graph, FLAGS.clear_devices)\r\n  File \"/home/anarayanan/TenserflowPlayground/TF_DEEPLAB_UNTOUCHED/tensorflow-deeplab-resnet-master/freeze_graph.py\", line 115, in freeze_graph\r\n    text_format.Merge(f.read(), input_graph_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py\", line 476, in Merge\r\n    descriptor_pool=descriptor_pool)\r\n  File \"/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py\", line 526, in MergeLines\r\n    return parser.MergeLines(lines, message)\r\n  File \"/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py\", line 559, in MergeLines\r\n    self._ParseOrMerge(lines, message)\r\n  File \"/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py\", line 574, in _ParseOrMerge\r\n    self._MergeField(tokenizer, message)\r\n  File \"/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py\", line 619, in _MergeField\r\n    name = tokenizer.ConsumeIdentifierOrNumber()\r\n  File \"/usr/local/lib/python2.7/dist-packages/google/protobuf/text_format.py\", line 1066, in ConsumeIdentifierOrNumber\r\n    raise self.ParseError('Expected identifier or number.')\r\ngoogle.protobuf.text_format.ParseError: 2:1 : Expected identifier or number.\r\n\r\nCan someone please help me understand where the error is coming from?", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.\r\n\r\n(In particular, it isn't clear to me how you're invoking the `freeze_graph` tool, which version of TensorFlow and the tools is being used etc. Detailed instructions on reproducing the problem will be very helpful in being able to diagnose the problem. You may also want to see https://stackoverflow.com/questions/44724700/using-tensorflow-checkpoint-to-restore-model-in-c/44726029#44726029 )", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 11092, "title": "conv2d on CPU does not pass numerical gradient check, possibly because the forward has an offset but the backward not when padding values are negative.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: ('v1.2.0-rc2-21-g12f033d', '1.2.0')\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nconv2d (CPU version) gradient function does not pass my gradient check tests when performing 'SAME' convolution in some special cases.\r\nSee my code below for details.\r\n\r\nTensorFlow uses eigen_spatial_convolution.h and eigen_backward_spatial_convolution.h for performaing conv2d on CPU.\r\nThe possible reason is that in this case the padding will be negative. During forward the eigen function SpatialConvolution will apply this negative padding as an offset. However, in backward, it does not apply this offset -- the forward and backward are inconsistent. \r\n\r\n### Source code / logs\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nbH = 4 \r\nbW = 4 \r\nH = 2 \r\nW = 2    \r\nin_c = 2 \r\nout_c = 3 \r\n\r\nstride = 4 \r\nbatch_size = 2 \r\nbatch_data  = np.ones([batch_size, bH, bW, in_c])\r\nfor n in range(batch_size):\r\n  for c in range(in_c):\r\n    for h in range(bH):\r\n      for w in range(bW):\r\n        batch_data[n, h, w, c] = n*0.001 + c*0.002 + h*0.003 + w*0.004   \r\n    \r\nbatch = tf.placeholder(tf.float32, [2, bH, bW, 2]) \r\nf = tf.placeholder(tf.float32, [H, W, in_c, out_c])\r\noutput = tf.nn.conv2d(batch, f, strides = [1, stride, stride, 1], padding = 'SAME')\r\ns = tf.reduce_sum(output)\r\ngrad_y = tf.gradients(s, f)\r\ninit = tf.global_variables_initializer()\r\n\r\nalpha = 5e-4 \r\nwith tf.Session() as sess:\r\n  sess.run(init)\r\n  filters = np.ones([H, W, in_c, out_c], dtype = float)\r\n  result, grads = sess.run([s, grad_y], feed_dict = {batch: batch_data, f: filters})\r\n  print(result)\r\n  for n in range(out_c):\r\n    for c in range(in_c):\r\n      for h in range(H):\r\n        for w in range(W):\r\n          old = filters[h, w, c, n]\r\n          filters[h, w, c, n] = old - alpha\r\n          [result_left] = sess.run([s], feed_dict = {batch: batch_data, f: filters}) \r\n          filters[h, w, c, n] = old + alpha\r\n          [result_right] = sess.run([s], feed_dict = {batch: batch_data, f: filters})\r\n          filters[h, w, c, n] = old \r\n          grad_est = (result_right - result_left) / (2 * alpha)\r\n          grad_act = grads[0][h, w, c, n]\r\n          print(\"(%d,%d,%d,%d): %f, %f\" % (n, c, h, w, grad_act, grad_est))\r\n```\r\n```\r\n2017-06-27 18:16:36.858424: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-27 18:16:36.858460: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-27 18:16:36.858464: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-27 18:16:36.858468: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-27 18:16:36.858472: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n0.576\r\n(0,0,0,0): 0.001000, 0.014961\r\n(0,0,0,1): 0.009000, 0.023007\r\n(0,0,1,0): 0.007000, 0.020981\r\n(0,0,1,1): 0.015000, 0.028968\r\n(0,1,0,0): 0.005000, 0.018954\r\n(0,1,0,1): 0.013000, 0.027061\r\n(0,1,1,0): 0.011000, 0.024974\r\n(0,1,1,1): 0.019000, 0.033021\r\n(1,0,0,0): 0.001000, 0.014961\r\n(1,0,0,1): 0.009000, 0.023007\r\n(1,0,1,0): 0.007000, 0.020981\r\n(1,0,1,1): 0.015000, 0.028968\r\n(1,1,0,0): 0.005000, 0.018954\r\n(1,1,0,1): 0.013000, 0.027001\r\n(1,1,1,0): 0.011000, 0.024974\r\n(1,1,1,1): 0.019000, 0.033021\r\n(2,0,0,0): 0.001000, 0.014961\r\n(2,0,0,1): 0.009000, 0.023007\r\n(2,0,1,0): 0.007000, 0.020981\r\n(2,0,1,1): 0.015000, 0.028968\r\n(2,1,0,0): 0.005000, 0.018954\r\n(2,1,0,1): 0.013000, 0.027001\r\n(2,1,1,0): 0.011000, 0.024974\r\n(2,1,1,1): 0.019000, 0.033021\r\n```\r\n\r\n", "comments": ["Any reply on this issue?", "Any follow-up on this thread??? thanks!", "@cwhipkey assigning to you for triage: please reassign to someone else if appropriate thanks!", "@benoitsteiner as well.  I will be able to take a closer look on Friday...", "I'm sorry, I didn't have a chance to look at this one.  Unassigning myself as I won't be able to look.", "Hi, is this still an issue?", "yes it is.\nOn Fri, Aug 31, 2018 at 8:05 PM Skye Wanderman-Milne <\nnotifications@github.com> wrote:\n\n> Hi, is this still an issue?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11092#issuecomment-417817154>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABk9LkYNp4uj1Izx1ubGiiZ8WCH04NAHks5uWc9bgaJpZM4OHQ6K>\n> .\n>\n-- \nsent from my phone\n", "Nagging Assignee @zheng-xq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@zhisbug Just ran the above snippet, here are results:\r\n```\r\n0.24\r\n(0,0,0,0): 0.001000, 0.000998\r\n(0,0,0,1): 0.009000, 0.008985\r\n(0,0,1,0): 0.007000, 0.007004\r\n(0,0,1,1): 0.015000, 0.014991\r\n(0,1,0,0): 0.005000, 0.004977\r\n(0,1,0,1): 0.013000, 0.013009\r\n(0,1,1,0): 0.011000, 0.010997\r\n(0,1,1,1): 0.019000, 0.018999\r\n(1,0,0,0): 0.001000, 0.000998\r\n(1,0,0,1): 0.009000, 0.008985\r\n(1,0,1,0): 0.007000, 0.007004\r\n(1,0,1,1): 0.015000, 0.014991\r\n(1,1,0,0): 0.005000, 0.004977\r\n(1,1,0,1): 0.013000, 0.012994\r\n(1,1,1,0): 0.011000, 0.010997\r\n(1,1,1,1): 0.019000, 0.018999\r\n(2,0,0,0): 0.001000, 0.000998\r\n(2,0,0,1): 0.009000, 0.008985\r\n(2,0,1,0): 0.007000, 0.007004\r\n(2,0,1,1): 0.015000, 0.014991\r\n(2,1,0,0): 0.005000, 0.004977\r\n(2,1,0,1): 0.013000, 0.012994\r\n(2,1,1,0): 0.011000, 0.010997\r\n(2,1,1,1): 0.019000, 0.018999\r\n```\r\nLooks okay to me, padding SAME and VALID lead to the same results.", "@zhisbug Closing this one, feel free to open a new issue if any errors occur."]}, {"number": 11091, "title": "tf.nn.elu: incorrect second derivative", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.2.0\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA 8.0 / cuDNN 6.0\r\n- **GPU model and memory**: GTX 1080 Ti 11GB\r\n- **Exact command to reproduce**: see below\r\n\r\n`tf.nn.elu` gives incorrect second derivatives:\r\n\r\nConsider the graph `y = 2 * elu(-x)`.\r\n\r\n```python\r\nx = tf.placeholder(tf.float32, ())\r\ny = 2 * tf.nn.elu(-x)\r\n```\r\n\r\nWe'll be evaluating at x=1:\r\n\r\n```python\r\nx_ = 1\r\n```\r\n\r\nWe can evaluate first derivatives with automatic differentiation:\r\n\r\n```python\r\ndy, = tf.gradients(y, x)\r\ndy.eval({x: x_})\r\n=> -0.7357589\r\n```\r\n\r\nThis lines up with the analytic answer: `y' = -2e^(-x)`\r\n\r\nHowever, for the second derivative:\r\n\r\n```python\r\nddy, = tf.gradients(dy, x)\r\nddy.eval({x: x_})\r\n=> 0.36787945\r\n```\r\n\r\nWhoops, this doesn't look right! Analytically, the derivative is `y'' = 2e^(-x)`. Evaluated at `x=1`, this is `0.7357588`!\r\n\r\n### Workaround\r\n\r\nJust in case anyone else needs to work around this until it's fixed:\r\n\r\n```python\r\ndef elu(x):\r\n    return tf.where(x >= 0.0, x, tf.exp(x) - 1)\r\n```\r\n\r\nLooks like second derivatives work with that.", "comments": ["Possibly related to #7403", "@alextp do you know why this might be?\r\n\r\n@anishathalye is it possible to make a simpler repro script that you can share?", "Can you use the tensorflow gradient checker to test the gradient of your particular graph? See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradient_checker.py and usages of it in the tensorflow tests.", "Okay, so I tried the following:\r\n\r\nI have a network `y(x)`, where `y` contains no `tf.gradient` ops, and I checked:\r\n\r\n1. `tf.check_numerics(tf.gradients(y, x)[0])` -- result is ok\r\n\r\n2. `tf.test.compute_gradient_error(x, ..., y, ...)` -- result is ~ 983 (is that okay? in any case, for training, first derivatives seem to work)\r\n\r\n3. `tf.check_numerics(tf.gradients(tf.gradients(y, x)[0], x)[0])` -- result is ok\r\n\r\n4. `tf.test.compute_gradient_error(x, ..., tf.gradients(y, x)[0], ...)` - result is ~ 1522423936\r\n\r\nFinite differences probably isn't producing great results because `y` is a fairly big graph. But still, having a maximum error of 1e9 seems kind of large.\r\n\r\nWhat do you suggest looking into next?", "This is really hard to debug without having access to the full graph.\n\nWhat I'd do if I did have access  to the full graph would be bisect it;\nremoving chunks of graph at a time until the error in second derivative\ngoes down to see if there's some part of the code which is less numerically\nstable than it should be.\n\nOn Wed, Jul 5, 2017 at 3:17 PM, Anish Athalye <notifications@github.com>\nwrote:\n\n> Okay, so I tried the following:\n>\n> I have a network y(x), where y contains no tf.gradient ops, and I checked:\n>\n>    1.\n>\n>    tf.check_numerics(tf.gradients(y, x)[0]) -- result is ok\n>    2.\n>\n>    tf.test.compute_gradient_error(x, ..., y, ...) -- result is ~ 983 (is\n>    that okay? in any case, for training, first derivatives seem to work)\n>    3.\n>\n>    tf.check_numerics(tf.gradients(tf.gradients(y, x)[0], x)[0]) -- result\n>    is ok\n>    4.\n>\n>    tf.test.compute_gradient_error(x, ..., tf.gradients(y, x)[0], ...) -\n>    result is ~ 1522423936\n>\n> Finite differences probably isn't producing great results because y is a\n> fairly big graph. But still, having a maximum error of 1e9 seems kind of\n> large.\n>\n> What do you suggest looking into next?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11091#issuecomment-313241964>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxdLiF38xxI13fIJsHsvl7rooT44Cks5sLAuTgaJpZM4OHLlJ>\n> .\n>\n\n\n\n-- \n - Alex\n", "Ok, I have some more evidence indicating that there is in fact a bug. Also, I can share the full graph with you.\r\n\r\nI don't want to post it publicly, so I've sent you an email with this additional information.", "I found the bug: I updated the original post.", "The commit from last week should have fixed this."]}, {"number": 11090, "title": "1.2.1 PR", "body": "Patch release PR. Updating versions to 1.2.1. Updating the release notes. Updating markdown version. Patching Github issue 11005. ", "comments": ["Missing the commit to modify setup.py\r\n", "It's in the main commit (just to clarify)."]}, {"number": 11089, "title": "Support S3 Filesystem for Tensorflow", "body": "This is for S3 Filesystem support in Tensorflow. \r\n\r\nIt utilizes Tensorflow's FileSystem C++ interface, similar to currently available HDFS or GCS support. The code depends on AWS's C++ SDK (Apache License) https://github.com/aws/aws-sdk-cpp\r\nThe code is placed under the directory of tensorflow/contrib/s3\r\n\r\nTODO List:\r\n\r\n- [X] Add the implementation of S3 file system\r\n- [X] Work out the BUILD configuration (currently build a .so, not sure the best way in bazel)\r\n- [X] Add tests similar to `gcs_file_system_test.cc`\r\n\r\nThis fix is related to #10616\r\n", "comments": ["Can one of the admins verify this patch?", "@jhseu FYI", "@yongtang Thanks for your contribution! Now the code can only connect to AWS S3. Is it right? One suggestion: it is better to make it compatible with other 3rd-party S3 implementations (e.g. Eucalyptus Walrus, Scality S3 server, Minio). I think it can be done by setting `ClientConfiguration::endpointOverride` and `ClientConfiguration::scheme`.  Some custom options may be added.", "Thanks @sswv. I will take a look at the configurations you mentioned.", "Hi, @yongtang,\r\n\r\nI wrote some small patches to allow running the S3 component with 3rd-party S3 implementations, and I also fixed a potential bug. Please see [my branch](https://github.com/sswv/tensorflow/commits/s3-patch). Note that these patches are all simple but not mature solutions, so I did not make them as pull requests. You can refer to them or pick up code from them, and provide more complete solutions based on my patches.\r\n\r\n* [Patch 1](https://github.com/sswv/tensorflow/commit/d3eb70e56fd1c590245414f628e168168339ec56):\r\n\r\nGet S3 configurations from environment variables to support 3rd-party S3 implementations. Do you think it is a good way to get configurations from environment variables? If not, we can choose a better way. More configuration items of `Aws::Client::ClientConfiguration` might be added.\r\n\r\n* [Patch 2](https://github.com/sswv/tensorflow/commit/a89294358662d0f82a1f51530f016ae3e353b18e):\r\n\r\nUse old `ListObject` (V1) instead of `ListObjectV2` to support some 3rd-party S3 implementations, especially legacy systems. A better way is adding a switch to allow users choosing from V1 and V2.\r\n\r\n* [Patch 3](https://github.com/sswv/tensorflow/commit/c3f28c9cd233ecf1923dfaa435b069acf8937dc1):\r\n\r\nA workaround for the \"Invalid Range Error\" when reading ImageNet data set. When I train the `inception_v3` network from `tensorflow/models/slim` by using the ImageNet data set, I see an \"Invalid Range Error\" after 120 steps and the program crashes immediately. It happens only with the S3 file system, but not happen with local file system and HDFS. I can reproduce it with 2 different kinds of 3rd-party S3 implementations. After some debugging, I found it is because of `offset == size of file` in `S3RandomAccessFile::Read`. In this case, `s3Client.GetObject` will report the \"Invalid Range Error\".\r\n\r\nRefer to the `HDFSRandomAccessFile::Read` function in `hadoop_file_system.cc`. It has a more elegant behavior when out-of-range error happens. For the HDFS case, A non-fatal `error::OUT_OF_RANGE` status will be returned to `TFRecordReader::ReadLocked` when out-of-range error happens, and the program will not crash.\r\n\r\nThus, I made a workaround patch. It works well in my simple tests. However, I am not a S3 expert, so I am not sure whether my solution is correct and good enough. I think you can find a more reasonable way to fix the issue.\r\n", "@yongtang please let me know when this pull request is ready for review.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "@yongtang is this ready to review?  Also, can you fix the CLA issue?  Probably by fixing your git commit email on some of the newer commits?", "Oh, @sswv you probably need to sign the CLA for us to accept these patches.", "@vrv I had signed CLA. I think the check will pass when the bot run next time.", "Did the email you used for CLA match the email used in https://patch-diff.githubusercontent.com/raw/tensorflow/tensorflow/pull/11089.patch ?  The bot checks the CLA every time a new commit is added, so maybe we have to wait.", "@vrv Yes. I checked the patch and the CLA page. I am using the same email address in the two places.\r\n", "I try this branch but when setup there is an error:\r\ntensorflow.python.framework.errors_impl.UnimplementedError: File system scheme s3 not implemented\r\nI install this as these steps:\r\n1.  git clone\r\n2. ./configure\r\n3. bazel build \r\n4. bazel-bin/**\r\n5. pip install \r\n\r\nafter that i find s3_file_system.so  is not exist? is that right ? \r\nany help will be vary thanks.", "You may have to answer some question in the configure part that enables it\n\nOn Jul 25, 2017 8:42 AM, \"TK-blost\" <notifications@github.com> wrote:\n\n> I try this branch but when setup there is an error:\n> tensorflow.python.framework.errors_impl.UnimplementedError: File system\n> scheme s3 not implemented\n> I install this as these steps:\n>\n>    1. git clone\n>    2. ./configure\n>    3. bazel build\n>    4. bazel-bin/**\n>    5. pip install\n>\n> after that i find s3_file_system.so is not exist? is that right ?\n> any help will be vary thanks.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/11089#issuecomment-317644373>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbYgcd4PzJGzq8mI0jAMh1oEIWpd9ks5sRY42gaJpZM4OHHsD>\n> .\n>\n", "@drpngx  thank you for your attention,  I check again not find any option about s3 configuration?", "@TK-blost Did you add `tf.load_file_system_library('/path/to/XXX_file_system.so')` in your code? Refer to: <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/extend/add_filesys.md#registering-and-loading-the-filesystem>", "@sswv  wow it worked as you said. ", "Thanks for your contribution! I'm very interested in the S3 feature and I hope it can release soon. I notice that there is no Unit Test for the S3 feature. I think I can contribute a group of Unit Tests. Please let me know whether it is OK. @yongtang  ", "@neuzxy Thanks. That would be great! You can add commits on top of the PR.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n<!-- ok -->", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Hi, @yongtang ,\r\n\r\nI wrote some small patches to do refactor and unit test for S3 file system, and I also found and fixed a bug through unit test. Please see [my branch](https://github.com/neuzxy/tensorflow/tree/s3-ut). These patches are simple, you can refer them or pick up useful code.\r\n\r\n- [Patch 1](https://github.com/neuzxy/tensorflow/commit/d4a297518c8027f3b365f5e3af2a759b8e48eacb)\r\n\r\nSplit the header and source of s3_file_system.cc. To test s3_file_system.cc, I should create a S3FileSystem Object, but class S3FileSystem was defined in s3_file_system.cc. So I splited the header and source of s3_file_system.cc. Due to the indent problem, you may see the diff with confusion, you may infer [`this patch`](https://gist.github.com/neuzxy/0ea925c053d25d54b4b722ab9d764616) to see the change in an easy way.\r\n\r\n- [Patch 2](https://github.com/neuzxy/tensorflow/commit/7b35e561d45727056eb7af11e6ae94cc04792ed6)\r\n\r\nFix a return status issue in RandomAccessFile::Read. When I tested the `NewAppendableFile` method and couldn't pass the test. I have contacted to the code contributor @sswv , he has confirmed the bug.\r\n\r\n- [Patch 3](https://github.com/neuzxy/tensorflow/commit/aec29c77675dec4f58c03237fd1e5a3036056bc9)\r\n\r\nAdd the unit test for S3 file system. I have created unit tests for all methods of class S3FileSystem and passed all tests in my environment. Before the test, you should configure `TFS3_ENDPOINT`, `TFS3_REGION`, `TFS3_USE_HTTPS`, `TFS3_VERIFY_SSL` and the tmp dir `S3_TMPDIR`. If you execute the test for more than once, you should empty `S3_TMPDIR` first.\r\n\r\n```\r\nbazel test \\\r\n    //tensorflow/contrib/s3:s3_file_system_test \\\r\n    --test_env=TFS3_ENDPOINT=$TFS3_ENDPOINT \\\r\n    --test_env=TFS3_REGION=$TFS3_REGION \\\r\n    --test_env=TFS3_USE_HTTPS=$TFS3_USE_HTTPS \\\r\n    --test_env=TFS3_VERIFY_SSL=$TFS3_VERIFY_SSL \\\r\n    --test_env=S3_TMPDIR=S3://test\r\n```\r\n\r\nIt works well in my simple tests. However, I am a junior engineer, I wrote the unit test by referring hadoop_file_system_test.cc and gcs_file_system_test.cc. It would be great if the author or other contributors provide more solid unit test.\r\n", "Thanks, @neuzxy . As we discussed in email, I agree with your Patch 2. Returning a non-fatal `error::OUT_OF_RANGE` in this case is similar to the implementation of HDFS plugin. I tested your patch with the ImageNet dataset in local Minio storage, and it worked well.\r\n\r\nBTW, I noticed the @googlebot said: *\"We need to confirm that they're okay with their commits being contributed to this project.\"* It is good for me that my commits being contributed. Thanks!", "@neuzxy Thanks. You could actually create a pull request to the branch in my forked repo: \"yongtang:s3\". In that way once it is merged in my forked repo then your commits will atomically update this PR and show up. I just did that for you.", "Ping @jhseu I think the PR is ready for review. Please take a look.", "Jenkins, test this please\n\nOn Aug 5, 2017 9:08 AM, \"Yong Tang\" <notifications@github.com> wrote:\n\n> Ping @jhseu <https://github.com/jhseu> I think the PR is ready for\n> review. Please take a look.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/11089#issuecomment-320451897>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_Sba_P-JF5ML5DmgieZaDoS908kRWiks5sVJOUgaJpZM4OHHsD>\n> .\n>\n", "  @drpngx The Jenkins build failure was caused by the change from other PR's `@protobuf//` -> `@protobuf_archive//`. I rebased and updated the PR. Now it the Jenkins should work.", "Did you push?\n\nOn Aug 5, 2017 10:10 AM, \"Yong Tang\" <notifications@github.com> wrote:\n\n> @drpngx <https://github.com/drpngx> The Jenkins build failure was caused\n> by the change from other PR's @protobuf// -> @protobuf_archive//. I\n> rebased and updated the PR. Now it the Jenkins should work.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/11089#issuecomment-320455325>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbfAgzga67CfRwx9P6CMeVLqFqVX6ks5sVKIagaJpZM4OHHsD>\n> .\n>\n", "@drpngx Yes the change has been pushed to fix `@protobuf` -> `@protobuf_archive` for bazel.", "Jenkins, test this please.\n\nOn Aug 5, 2017 12:40 PM, \"Yong Tang\" <notifications@github.com> wrote:\n\n> @drpngx <https://github.com/drpngx> Yes the change has been pushed to fix\n> @protobuf -> @protobuf_archive for bazel.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/11089#issuecomment-320465976>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbTNiQTz8Ud1vdNlC6nlz3UT7-Ravks5sVMUqgaJpZM4OHHsD>\n> .\n>\n", "@tensorflow-jenkins test this please", "@sb2nov @frankchn @saeta FYI", "@neuzxy can you confirm here that you're ok with this pull request being committed? Thanks!", "@jhseu Yes, I'm ok with this pull request being committed.", "Thanks @jhseu for the review. The PR has been rebased and updated. Please take a look.", "Jenkins, test this please", "Jenkins, test this please.", "@martinwicke The Jenkins build error on Mac OS X was caused by no target specification on Bazel aws.BUILD file. I have updated the PR and the issue should have been fixed now. Sorry for the inconvenience.", "No problem at all, thank you!\r\n\r\nJenkins, test this please.", "Re-running for infra failures.\r\n\r\nJenkins, test this please.", "Trying again.\r\n\r\nJenkins, test this please.", "It failed the sanity check:\r\n\r\n```\r\nERROR: /workspace/tensorflow/contrib/s3/BUILD:39:1: no such package '@aws//': java.io.IOException: Error downloading [http://bazel-mirror.storage.googleapis.com/github.com/aws/aws-sdk-cpp/archive/1.0.90.tar.gz, https://github.com/aws/aws-sdk-cpp/archive/1.0.90.tar.gz] to /var/lib/jenkins/workspace/tensorflow-pull-requests-sanity/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/external/aws/1.0.90.tar.gz: Checksum was f599b57aec4f03ad696044dd430b2d201864113937353adc346f53ad47991319 but wanted ffa501fa00e18d789cd10c768b55a2907c314816f09bcaf96e6d816a5b67f6e7 and referenced by '//tensorflow/contrib/s3:s3_crypto'.\r\nERROR: /workspace/tensorflow/contrib/s3/BUILD:56:1: no such package '@aws//': java.io.IOException: Error downloading [http://bazel-mirror.storage.googleapis.com/github.com/aws/aws-sdk-cpp/archive/1.0.90.tar.gz, https://github.com/aws/aws-sdk-cpp/archive/1.0.90.tar.gz] to /var/lib/jenkins/workspace/tensorflow-pull-requests-sanity/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/external/aws/1.0.90.tar.gz: Checksum was f599b57aec4f03ad696044dd430b2d201864113937353adc346f53ad47991319 but wanted ffa501fa00e18d789cd10c768b55a2907c314816f09bcaf96e6d816a5b67f6e7 and referenced by '//tensorflow/contrib/s3:s3_file_system'.\r\nERROR: Analysis of target '//tensorflow/contrib/s3:s3_file_system' failed; build aborted.\r\n```", "@drpngx The error was caused by hash mismatch, likely the same issue encounter related to GitHub changing the download content recetly. I have updated the PR. Please take a look.", "Would you mind pull rebase and push agaian? It is confusing the CLA bot.", "@drpngx I think all commits have signed CLA. The `cla: no` flag is likely caused by the fact that this PR consists of commits from @sswv and @neuzxy.\r\n\r\nBoth of them have confirmed (in previous comments) they are OK with the commits in the PR.", "Jenkins, test this please.\r\n\r\nIt used to be that we could override the CLA bot, but it looks like this no longer works.", "The error is\r\n```\r\nmissing input file '@androidsdk//:build-tools/25.0.2/aapt'.\r\n```\r\n\r\nNot sure where it comes from, though I assume a rerun might be able to fix it?", "Oh, that's a transient failure.\r\n\r\nJenkins, test this please.", "@sswv and @neuzxy could you confirm that the email address is correct on the commits? Thanks.", "@drpngx I checked the git log and confirmed that my email is correct. It is the same as the email I used for CLA.", "@drpngx I also checked the git log and confirmed that my email address is correct. I signed CLA with the same email.", "Thanks guys for checking!\n\nOn Sep 19, 2017 8:04 PM, \"Allen Zhang\" <notifications@github.com> wrote:\n\n> @drpngx <https://github.com/drpngx> I also checked the git log and\n> confirmed that my email address is correct. I signed CLA with the same\n> email.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/11089#issuecomment-330731919>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbQ1Ii-Zzeypv_E885RbMVuDWtUJhks5skIDBgaJpZM4OHHsD>\n> .\n>\n", "@caisq it's stuck on the CLA. If you know how to override this, LMK, otherwise I can look it up.", "Talked to @gunan, he will change the config.", "Merging the PR.", "Thanks all for the help! \ud83d\udc4d \ud83c\udf7a \ud83d\udca5", "does this support Python API tf.train.string_input_producer ?", "@sswv @drpngx What released version of TensorFlow would includes the S3 feature? And I also have an  interest in the supports of  `tf.train.string_input_producer`.", "The next version will include it: 1.4. \r\n\r\n`string_input_producer` just produces strings, if you make it produce strings with a \"s3://\" prefix it will now use S3.", "@martinwicke Got it. Thanks for your reply!  ", "@martinwicke   @yongtang  I have complied tensorflow in master branch, and I use string_input_producer with a \"s3://\" prefix ,but It won't work. Also ,how do I provide access key and secret key for this API .", "@sandyskies To build s3 you could invoke `./configure` then:\r\n```\r\nbazel build -s --config=opt --verbose_failures //tensorflow/contrib/s3:s3_file_system.so\r\n```\r\nAfter that, you should see an s3_file_system.so in the binary directory `bazel-bin/tensorflow/contrib/s3/s3_file_system.so`.\r\n\r\nTo use S3 plugin with tensorflow, you will need to load the generated dynamic .so file:\r\n```python\r\nimport tensorflow as tf\r\ntf.load_file_system_library('/path/to/s3_file_system.so')\r\n\r\nwith gfile.Open(\"s3://path/to/file/in/s3.txt\") as w:\r\n  w.write(\"hi\") \r\n```\r\n\r\nYou could config the access and secret key the same way you config for you system and it will be picked up automically.", "@martinwicke @yifeif Can we consider adding a question to https://github.com/tensorflow/tensorflow/blob/master/configure.py to enable s3 support at build time, so there is no need to build and load the .so file separately.", "We are working on enabling s3 support by default.\n\nOn Wed, Sep 27, 2017 at 10:47 AM, Shanqing Cai <notifications@github.com>\nwrote:\n\n> @martinwicke <https://github.com/martinwicke> @yifeif\n> <https://github.com/yifeif> Can we consider adding a question to\n> https://github.com/tensorflow/tensorflow/blob/master/configure.py to\n> enable s3 support at build time, so there is no need to build and load the\n> .so file separately.\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/11089#issuecomment-332601019>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOYSruIQmx8R2N8AdkCslDd3zWXOoks5smoozgaJpZM4OHHsD>\n> .\n>\n", "@yongtang  Can't this patch do just like hdfs\uff0c just use filename_queue = tf.train.string_input_producer([\"s3://path/to/file/in/s3.txt\"]) ?\r\n", "@gunan @case540 I guess since we'll enable it by default it would make sense to simply move it to core.", "@yongtang @martinwicke  can u look at this?\r\n>>> import tensorflow as tf\r\n>>> tf.load_file_system_library('s3_file_system.so')\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/data/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py\", line 100, in load_file_system_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename, status)\r\n  File \"/data/anaconda3/lib/python3.6/contextlib.py\", line 89, in __exit__\r\n    next(self.gen)\r\n  File \"/data/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 467, in raise_exception_on_not_ok_status\r\n    c_api.TF_GetCode(status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: s3_file_system.so: undefined symbol: _ZTIN10tensorflow10FileSystemE", "@yongtang @martinwicke can u look at the problem?", "I have an internal CL to fix this. I think you can just change s3_file_system.so from a cc_binary target to a tf_cc_binary target (this fixed it for me)", "Thank you @case540! Please submit and reference this `#11089` in the public description.", "Thanks @case540 a lot!\r\n\r\n@sandyskies Sorry for the late reply. I am out of the town for the past week and couldn't get a quick fix done. Will try to help if there are any further issues once I am back.", "I have use the workaround that @case540 provide, and it works fine for me. Should I create a pr for it , or wait for @yongtang to create himself.", "If I want to use this feature (I'd really want to try it out in tensorflow-serving), Do I need to compile tensorflow with special flags? I am not sure how I need to configure it\r\n\r\nI tried with settings these environment variables,\r\nENV S3_REGION=us-east-2\r\nENV S3_VERIFY_SSL=0\r\nENV S3_USE_HTTPS=1\r\n\r\nThe tensorflow serving uses `Env::Default()->FileExists` from tensorflow/core/platform/env.h for checking if a file exists. From what I understand, The s3 filesystem is registered with\r\n`REGISTER_FILE_SYSTEM(\"s3\", S3FileSystem);`, but I am not sure if it's actually working because the error gets swallowed in the tf-serving code ( this error https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/env.cc#L91 gets swallowed herehttps://github.com/tensorflow/serving/blob/master/tensorflow_serving/sources/storage_path/file_system_storage_path_source.cc#L221).\r\n\r\nSo what I am asking is: Do I need to pass additional options at build to\r\na - be sure the s3 filesystem is usable\r\nb - get additional debug information to know \r\n\r\nPS: Is this the right place to ask this question? Should I create an issue or stackoverflow question perhaps?", "@sandyskies I think @case540 will create an internal PR for the fix. The internal PR will show up in GitHub once it is merged internally and merged back to public GitHub.", "@quantumlicht I would assume you may get more help from StackOverflow as there are more users there. Beside this PR has been merged so less attention here.\r\n\r\nAt the moment s3 support is not part of the 1.3 release so you could only build from the master branch of the TensorFlow.\r\n\r\nSee comment: https://github.com/tensorflow/tensorflow/pull/11089#issuecomment-332589113 for steps of building and using s3 with TensorFlow. \r\n\r\n@case540 has been generous enough to provide a fix so that loading `s3_file_system.so` is not needed anymore. As far as I understand the fix will be pushed internally so it will show up later in the GitHub.", "Can one of the admins verify this patch?", "Hello! I want to use model files on S3 for TF Serving. Can I use an S3 URI for `--model_base_path` when starting `tensorflow_model_server`? Is there any code changes to TF Serving required? (According to the code comments, TF Serving has already supported GCS.) Thanks!", "@coldsheephot  let me refer you to this issue I opened https://github.com/tensorflow/serving/issues/615#issuecomment-335875164\r\n\r\nIf you could try it and report if it works for you that would be lovely. I will do the same thing", "Is there a python example on how to use this? I'd like to be able to access an HDF5 file from my S3 bucket within TensorFlow.\r\n\r\nThanks.", "@mas-dse-greina I added a very basic doc for s3 usage with TensorFlow in PR #16923.", "Thanks.\n\nI'd like to be able to access an HDF5 file on an S3 bucket. This way I\ndon't have to download the entire file at once.\n\nI've managed to accomplish this with goofys mounting the S3 bucket as a\nlocal directory. Do you think your code could be used in the same way with\nh5py?\n\nThanks.\n-Tony\n\n\nOn Sat, Feb 10, 2018 at 1:40 PM, Yong Tang <notifications@github.com> wrote:\n\n> @mas-dse-greina <https://github.com/mas-dse-greina> I added a very basic\n> doc for s3 usage with TensorFlow in PR #16923\n> <https://github.com/tensorflow/tensorflow/pull/16923>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/11089#issuecomment-364695561>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AVReEiS1E9lSp1aujHq0MC9-P0bdJzf_ks5tTgzHgaJpZM4OHHsD>\n> .\n>\n"]}, {"number": 11088, "title": "Fixed some warnings related to signed/unsigned comparison", "body": "This fix tries to fix some warning related to\r\n```\r\nwarning: comparison between signed and unsigned integer expressions\r\n```\r\nby address some easy fix in changing `int -> size_t` in case of loop index.\r\n\r\nThis fix is related to #10838.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}]