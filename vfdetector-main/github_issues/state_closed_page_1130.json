[{"number": 19323, "title": "Error while building tensorflow using cmake", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux\r\n- **TensorFlow installed from (source or binary)**: NA\r\n- **TensorFlow version (use command below)**: NA\r\n- **Python version**:  NA\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: GCC version 6.2.1 \r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: `cmake ../tensorflow/contrib/cmake -DCMAKE_INSTALL_PREFIX=/usr -DLIB_INSTALL_DIR=/usr/lib -DINCLUDE_INSTALL_DIR=/usr/include ; make`\r\n\r\n\r\n### Describe the problem\r\nWhile building tensorflow using cmake, files like `tensorflow/core/example/feature.pb_text.cc` , `tensorflow/core/framework/attr_value.pb_text.cc` are generated with incorrect header includes.\r\n(They are missing `#include <algorithm>` ). Similar issue can be found in [this pull request](https://github.com/tensorflow/tensorflow/pull/19321)\r\n### Source code / logs\r\n```\r\n[ 1011s] /home/abuild/rpmbuild/BUILD/tensorflow-1.8.0/build/tensorflow/core/example/feature.pb_text.cc: In function 'void tensorflow::internal::AppendProtoDebugString(tensorflow::strings::ProtoTextOutput*, const tensorflow::Features&)':\r\n[ 1011s] /home/abuild/rpmbuild/BUILD/tensorflow-1.8.0/build/tensorflow/core/example/feature.pb_text.cc:464:5: error: 'stable_sort' is not a member of 'std'\r\n[ 1011s]      std::stable_sort(keys.begin(), keys.end());\r\n[ 1011s]      ^~~\r\n[ 1011s] /home/abuild/rpmbuild/BUILD/tensorflow-1.8.0/build/tensorflow/core/example/feature.pb_text.cc: In function 'void tensorflow::internal::AppendProtoDebugString(tensorflow::strings::ProtoTextOutput*, const tensorflow::FeatureLists&)':\r\n[ 1011s] /home/abuild/rpmbuild/BUILD/tensorflow-1.8.0/build/tensorflow/core/example/feature.pb_text.cc:708:5: error: 'stable_sort' is not a member of 'std'\r\n[ 1011s]      std::stable_sort(keys.begin(), keys.end());\r\n[ 1011s]      ^~~\r\n[ 1011s] [ 30%] Building CXX object CMakeFiles/tf_core_framework.dir/tensorflow/core/framework/attr_value.pb_text.cc.o\r\n[ 1012s] CMakeFiles/tf_core_framework.dir/build.make:2614: recipe for target 'CMakeFiles/tf_core_framework.dir/tensorflow/core/example/feature.pb_text.cc.o' failed\r\n[ 1012s] make[2]: *** [CMakeFiles/tf_core_framework.dir/tensorflow/core/example/feature.pb_text.cc.o] Error 1\r\n[ 1012s] make[2]: *** Waiting for unfinished jobs....\r\n[ 1013s] /home/abuild/rpmbuild/BUILD/tensorflow-1.8.0/build/tensorflow/core/framework/attr_value.pb_text.cc: In function 'void tensorflow::internal::AppendProtoDebugString(tensorflow::strings::ProtoTextOutput*, const tensorflow::NameAttrList&)':\r\n[ 1013s] /home/abuild/rpmbuild/BUILD/tensorflow-1.8.0/build/tensorflow/core/framework/attr_value.pb_text.cc:698:5: error: 'stable_sort' is not a member of 'std'\r\n[ 1013s]      std::stable_sort(keys.begin(), keys.end());\r\n[ 1013s]      ^~~\r\n```\r\n", "comments": ["@mrry do you have an idea? Could be a proto issue.", "I'd guess the `*.pb_text.cc` files are relying on an indirect inclusion that isn't present in @krantideep95's environment. One could add something that prints the appropriate `#include <algorithm>` line to [the code generator](https://github.com/tensorflow/tensorflow/blob/51ef16057b4625e0a3e2943a9f1bbf856cf098ca/tensorflow/tools/proto_text/gen_proto_text_functions_lib.cc#L752).", "I was able to build by adding `\"/n#include <algorithm>\"` to [Print(\"// GENERATED FILE - DO NOT MODIFY\");](https://github.com/tensorflow/tensorflow/blob/51ef16057b4625e0a3e2943a9f1bbf856cf098ca/tensorflow/tools/proto_text/gen_proto_text_functions_lib.cc#L766). It's not optimized way, but atleast it's building now.\r\nThanks for the help!", "Thanks for confirming! I've sent out PR #19355 with a fix that localizes the `#include` as much as possible."]}, {"number": 19322, "title": "tf.gfile.GFile deadlocks on HDFS access after a fork()", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.13\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\n\r\nAccessing HDFS via `tf.gfile` functions leads to a deadlock (?) if the HDFS has been initialized, i.e. there has been at least a single HDFS call from the parent process. Not sure if this is a `libhdfs` bug, or a TF one. If the former, then maybe #16919 could help.\r\n\r\n### Source code / logs\r\n\r\n#### Case 1: HDFS is not initialized in the parent process\r\n\r\n```python\r\n>>> import tensorflow as tf\r\n>>> def f():\r\n...     print(tf.gfile.ListDirectory(\"hdfs://root/user/[...]\"))\r\n...\r\n>>> from multiprocessing import Process\r\n>>> p = Process(target=f)\r\n>>> p.start()\r\n>>> p.join()\r\nSLF4J: Class path contains multiple SLF4J bindings.\r\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\r\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\r\n\r\n['foo', 'bar', 'boo']\r\n```\r\n\r\n#### Case 2: HDFS is initialized in the parent process\r\n\r\n```python\r\n>>> f()\r\nSLF4J: Class path contains multiple SLF4J bindings.\r\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\r\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\r\n['foo', 'bar', 'boo']\r\n>>> p = Process(target=f)\r\n>>> p.start()\r\n>>> p.join()  # Never returns.\r\n```", "comments": ["@jhseu any thoughts on this?", "My best guess is that it's a libhdfs issue as you mentioned, since we don't do anything special that would affect forking. Fixes are welcome (even an option to link to libhdfs3).", "Apparently libhdfs3 [is not fork-safe](https://hdfs3.readthedocs.io/en/latest/limitations.html) either, so I guess this should be outlined in the HDFS section of the docs.", "how to fix this problem", "I suspect the only fix is to avoid initializing HDFS prior to forking.", "\n            https://hdfs3.readthedocs.io/en/latest/limitations.htmlstart process with spawn mode\n\n\n\n    \n\n\n            On 08/26/2020 16:55, Sergei Lebedev wrote: \nI suspect the only fix is to avoid initializing HDFS prior to forking.\n\n\u2014You are receiving this because you commented.Reply to this email directly, view it on GitHub, or unsubscribe.\n[\n{\n\"@context\": \"http://schema.org\",\n\"@type\": \"EmailMessage\",\n\"potentialAction\": {\n\"@type\": \"ViewAction\",\n\"target\": \"https://github.com/tensorflow/tensorflow/issues/19322#issuecomment-680751182\",\n\"url\": \"https://github.com/tensorflow/tensorflow/issues/19322#issuecomment-680751182\",\n\"name\": \"View Issue\"\n},\n\"description\": \"View this Issue on GitHub\",\n\"publisher\": {\n\"@type\": \"Organization\",\n\"name\": \"GitHub\",\n\"url\": \"https://github.com\"\n}\n}\n]", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/19322\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/19322\">No</a>\n", "> Hi There,\r\n> \r\n> We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions.\r\n> \r\n> This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help.\r\n\r\nok. this is a libhdfs's bug\uff0cuse python process spawn/forkserver mode to avoid deadlock problem. "]}, {"number": 19321, "title": "core/framework/op_gen_lib.cc: include <algorithm>", "body": "In `tensorflow/core/framework/op_gen_lib.cc` Add `#include <algorithm>` or else build with cmake fails at [line 415](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/op_gen_lib.cc#L415) \r\n```\r\n[ 1098s] /home/abuild/rpmbuild/BUILD/tensorflow-1.8.0/tensorflow/core/framework/op_gen_lib.cc:418:10: error: 'is_permutation' is not a member of 'std'\r\n[ 1098s]      if (!std::is_permutation(new_api_def.arg_order().begin(),\r\n\r\n```", "comments": []}, {"number": 19320, "title": "NULL for interpreter->typed_tensor<uint8_t>(input)", "body": "Hi there, for my iOS app i'm using tensorflow-lite, \r\nI'm facing this issue after initializing the interpreter\r\nIn,\r\n**int input =interpreter->inputs()[0];**\r\n**uint8_t \\*input_index = interpreter->typed_tensor<uint8_t>(input);**\r\n\r\ninput_index = NULL and input = 27\r\n\r\nMay I know what is **input** and what are the possibilities that **input_index** will be **NULL**\r\n\r\n\r\nHave I written custom code - No\r\nOS Platform and Distribution - MacOS\r\nTensorFlow installed from Pod\r\nTensorFlow version - r1.8\r\nTensorFlowLite version - 0.1.7\r\nBazel version - 0.11.0\r\nCUDA/cuDNN version  - N/A\r\nGPU model and memory - N/A\r\nExact command to reproduce - N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nTensorFlow installed from\nTensorFlow version", "Also, an nnapi error after running the app.\r\n**nnapi error: unable to open library libneuralnetworks.so**\r\n", "Nagging Assignee @aselle: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "The message \"nnapi error: unable to open library libneuralnetworks.so\" is ok if the device isn't running a recent version of android. That message has also been removed from TF Lite recently.\r\n\r\nAs for the null pointer:\r\n  - make sure you called AllocatedTensors\r\n  - make sure the data type (uint8) is appropriate for your model."]}, {"number": 19319, "title": "Can't import interpreter from tensorflow.contrib.lite.python ", "body": "![image](https://user-images.githubusercontent.com/22855898/40106717-0ef62cda-5929-11e8-806e-8e0ed2b14254.png)\r\ntensorflow 1.8.0\r\ntensorflow.contrib.lite has no attribute 'python', thus it's impossible to import interpreter from tensorflow.contrib.lite.python, which is in your tensorflow/tensorflow/contrib/lite/python/interpreter_test.py", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "This is intentional. Basically only what you see in `dir(tf.contrib.lite)` is supported. The reason that you can't import from the python subdirectory is that it has been intentionally sealed off from direct imports to prevent users from getting themselves into trouble by directly importing low level functions. (This is a very common pattern in TensorFlow.)\r\n\r\nWhy are you trying to import interpreter in the first place?", "I want to load my .tflite model and run the inference.\r\nHow can I do?", "I would start here. https://github.com/tensorflow/tensorflow/tree/master/tensorflow/docs_src/mobile/tflite", "model = tf.contrib.lite.Interpreter(model_path=\"test.tflite\")\r\nIt works now."]}, {"number": 19318, "title": "Simple Keras model does not compile in float64 mode", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS Sierra 10.12.6\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**: -\r\n- **GCC/Compiler version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: CPU only\r\n- **GPU model and memory**: CPU only\r\n- **Exact command to reproduce**: Run code below\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nWhen setting the backend to `float64`, a simple model does not compile anymore. I have debugged into the issue - the problem arises in the call to `weighted(y_true, y_pred, weights, mask=None)`, defined in `tensorflow.python.keras._impl.keras.engine.training_utils.py`. There, `weights` is\r\n\r\n`Tensor(\"dense_1_sample_weights:0\", shape=(?,), dtype=float32)`\r\n\r\nwhile `y_true` and `y_pred` have both the type `float64`. This causes the subsequent multiplication to error. The weights come from line `401`, the `compile` function in `tensorflow/python/keras/_impl/keras/engine/training.py`:\r\n\r\n```\r\nsample_weights.append(array_ops.placeholder_with_default(\r\n                [1.], shape=[None], name=name + '_sample_weights'))\r\n```\r\n\r\nBut I have not found a way to make this `placeholder_with_default` function return `float64` weights.\r\n\r\n**When using `keras` instead of `tensorflow.python.keras` it works** (for Keras version 2.1.6).\r\n\r\n### Source code / logs\r\nMinimum reproducible test case:\r\n```\r\nfrom tensorflow.python.keras import Model\r\nfrom tensorflow.python.keras import backend as K\r\nfrom tensorflow.python.keras.layers import Input, Dense\r\n\r\nK.set_floatx('float64')\r\n\r\ninput_ = Input((5,))\r\noutput = Dense(1)(input_)\r\nmodel = Model(input_, output)\r\nmodel.compile('rmsprop', 'mse')\r\n```\r\n\r\nStack trace:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/kilian/dev/tmp/src/test/float64.py\", line 13, in <module>\r\n    model.compile('rmsprop', 'mse')\r\n  File \"/Users/kilian/dev/tmp/venv/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/training.py\", line 428, in compile\r\n    output_loss = weighted_loss(y_true, y_pred, sample_weight, mask)\r\n  File \"/Users/kilian/dev/tmp/venv/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/training_utils.py\", line 454, in weighted\r\n    score_array *= weights\r\n  File \"/Users/kilian/dev/tmp/venv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 970, in binary_op_wrapper\r\n    y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=\"y\")\r\n  File \"/Users/kilian/dev/tmp/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1014, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"/Users/kilian/dev/tmp/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1104, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/Users/kilian/dev/tmp/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 947, in _TensorTensorConversionFunction\r\n    (dtype.name, t.dtype.name, str(t)))\r\nValueError: Tensor conversion requested dtype float64 for Tensor with dtype float32: 'Tensor(\"dense_1_sample_weights:0\", shape=(?,), dtype=float32)'\r\n```", "comments": ["The output dtype of `placeholder_with_default` is controlled by the input dtype. Converting `[1.]` to typed constant tensor should work I think.\r\n\r\nCreated a PR #19328 for the fix.", "Nagging Assignee @aselle: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 19317, "title": "TfLiteCameraDemo performance with and without NNAPI", "body": "Hi , \r\nI running TfLiteCameraDemo.apk on Google Pixel 2 8.1 Android.\r\nwhen NNAPI = false apk display 90-80 ms per frame \r\nwhen NNAPI = true apk display 170- 200 ms per frame.\r\n\r\nIs that NNAPI current performance ? match slower than Tflite path !?\r\n\r\nThanks.\r\n\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nSetUSENNAPI(true);\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nubuntu 16\r\n- **TensorFlow installed from (source or binary)**:\r\nsources\r\n- **TensorFlow version (use command below)**:\r\nmaster\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n0.10\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\nbazel build --cxxopt=--std=c++11 //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo --config=android_arm64 --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a\r\n\r\nThanks.", "comments": ["For Pixel 2 running Android 8.1, there is no GPU and DSP NN HAL drivers  (only CPU fallback). That is, when NNAPI is called, CPU fallback is used. The performance gap between tflite and nnap is caused by the number of threads used to run convolution.", "Thanks.", "Does Android 8.1 provide an libneuralnetworks.so (even there is no GPU/DSP support) ? If so, how is it implemented ? or such *.so is empty.  Any related docs?  ", "Yes, there should be libneuralnetworks.so that handles CPU fallback mentioned in NNAPI [doc](https://developer.android.com/ndk/guides/neuralnetworks/) and other NNAPI runtime stuff. For user-level code of NNAPI in Android 8.1, see \r\nhttps://android.googlesource.com/platform/frameworks/ml/+/oreo-mr1-release\r\n\r\n"]}, {"number": 19316, "title": "[TF-Lite] densenet.pb at /contrib/lite/g3doc/models.md cannot be loaded", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: r1.8\r\n- **Python version**:  2.7\r\n- **Bazel version (if compiling from source)**: 0.11.0\r\n- **GCC/Compiler version (if compiling from source)**: 4.9\r\n- **CUDA/cuDNN version**: no\r\n- **GPU model and memory**: no\r\n- **Exact command to reproduce**: \r\n\r\n### Describe the problem\r\nThe densenet.pb provided at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md\r\ncannot be converted by toco command line. It can also not be loaded by GFile API.\r\n\r\n### Source code / logs\r\n> bazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n--input_file=\"densenet.pb\" \\\r\n--output_file=\"quant_densenet.tflite\" \\\r\n--input_format=TENSORFLOW_GRAPHDEF \\\r\n--output_format=TFLITE \\\r\n--inference_input_type=QUANTIZED_UINT8 \\\r\n--inference_type=QUANTIZED_UINT8 \\\r\n--input_shape=1,224,224,3 \\\r\n--input_array=\"input\" \\\r\n--output_array=\"final_dense\" \\\r\n--allow_custom_ops \\\r\n--default_ranges_min=0.0 \\\r\n--default_ranges_max=6.0 \\\r\n--mean_value=127.5 \\\r\n--std_value=1.0 \r\n\r\n>[libprotobuf ERROR external/protobuf_archive/src/google/protobuf/text_format.cc:288] Error parsing text-format tensorflow.GraphDef: 1:1: Invalid control characters encountered in text.\r\n>[libprotobuf ERROR external/protobuf_archive/src/google/protobuf/text_format.cc:288] Error parsing text-format tensorflow.GraphDef: 1:2: Interpreting non ascii codepoint 139.\r\n>[libprotobuf ERROR external/protobuf_archive/src/google/protobuf/text_format.cc:288] Error parsing text-format tensorflow.GraphDef: 1:2: Expected identifier, got: \u2039\r\n>2018-05-16 15:19:01.232421: F tensorflow/contrib/lite/toco/import_tensorflow.cc:2230] Check failed: ParseFromStringEitherTextOrBinary(input_file_contents, tf_graph.get())\r\nAborted (core dumped)\r\n\r\nif using GFile python API to load the pb file:\r\n\r\n```\r\ndef load_pb_graph(frozen_graph_file):\r\n    with tf.gfile.GFile(frozen_graph_file, 'rb') as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n    with tf.Graph().as_default() as graph:\r\n        tf.import_graph_def(graph_def, name=\"prefix\")\r\n\r\n    return graph, graph_def\r\n```\r\n\r\n>Traceback (most recent call last):\r\n  File \"GraphCheckerPB.py\", line 24, in <module>\r\n    graph, graph_def = load_pb_graph(in_pb_file)\r\n  File \"GraphCheckerPB.py\", line 14, in load_pb_graph\r\n    graph_def.ParseFromString(f.read())\r\ngoogle.protobuf.message.DecodeError: Error parsing message\r\n\r\n", "comments": ["Try `codecs.getreader(\"utf-8\")` and note questions like these are better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!", "@jart I tried to add codecs.getreader(\"utf-8\"), but it does not work. \r\n`with codecs.getreader(\"utf-8\")(tf.gfile.GFile(frozen_graph_file, 'rb')) as f:`\r\n> UnicodeDecodeError: 'utf8' codec can't decode byte 0x8b in position 1: invalid start byte\r\n\r\nUsing the latest tf mobile demo to load the model is also failed. \r\n\r\nI opened the pb file by Notpad++. The structure of the content is totally different from that of the model created by tf.graph_utils.convert_variables_to_constants and tf.gfile.FastGFile API. \r\n\r\nThe purpose of parsing the pb model is to get the input and output name, and then to convert it to tf-lite quantified version. ", "Actually it's most likely the case that those model tarballs are in a new Flatbuffers model format, rather than tf.GraphDef protos. See https://www.tensorflow.org/mobile/tflite/"]}, {"number": 19315, "title": "AttributeError: 'module' object has no attribute 'Exporter'", "body": "i ran retrain.py \r\ni got the code from tensorflow.org\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"retrain.py\", line 133, in <module>\r\n    import tensorflow_hub as hub\r\n  File \"/home/pi/.local/lib/python2.7/site-packages/tensorflow_hub/__init__.py\", line 26, in <module>\r\n    from tensorflow_hub.estimator import LatestModuleExporter\r\n  File \"/home/pi/.local/lib/python2.7/site-packages/tensorflow_hub/estimator.py\", line 61, in <module>\r\n    class LatestModuleExporter(tf.estimator.Exporter):\r\nAttributeError: 'module' object has no attribute 'Exporter'\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 16 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 32 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I apologize, but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new). Please provide all the information it asks. Thank you.\r\n"]}, {"number": 19314, "title": "[INTEL MKL] upgrade mkldnn version to v0.14 ", "body": "1. Upgrade mkldnn version to v0.14 in tensorflow/workspace.bzl\r\n\r\n2.Latest MKL DNN v0.14 does more sanity check on EMPTY input tensors and throws \"invalid parameter\" exception.\r\nThe fix is to handle the empty tensor case at MKL Ops layer, specifically on\r\nMklConcat\r\nMklAvgPooling", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 19313, "title": "BUG: Arithmetic computations that should be identical are not?", "body": "### System information\r\nSystem information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.13.4\r\nTensorFlow installed from (source or binary): Binary\r\nTensorFlow version (use command below): v1.8.0-0-g93bc2e2072 1.8.0\r\nPython version: 3.5.4\r\nBazel version (if compiling from source): 0.11.1\r\nGCC/Compiler version (if compiling from source): 9.1.0\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A\r\n\r\n### Describe the problem\r\n\r\nI'm running two versions of a NN which I expect to produce identical results, but they don't.\r\n\r\nThe first version contains as part of the computation:\r\n\r\n`x = [0.0*a + 1.0*b, 1.0*a + 0.0*b]`\r\n\r\nThe second contains instead:\r\n\r\n`x = [b, a]`\r\n\r\nThe two runs produce different results. (a and b are both matrices).\r\n\r\nI'm using a constant seed for the random noise input, and I've verified that multiple runs of either of the two above cases produce identical results. Just changing from one case to the other produces different results. The results are not broken in either case (i.e. both cases produce reasonable output), just different. \r\n\r\nThis seems like a bug to me.\r\n\r\nThanks\r\n\r\nEDIT: \r\n\r\nI've also found that doing:\r\n\r\nx = 1.0 * a\r\nx = 1.0 * (1.0 * a)\r\nx = tf.identity(a)\r\n\r\nAll give different results.\r\n\r\nLooking at this: https://github.com/tensorflow/tensorflow/issues/14675\r\nApparently the problem is I've made a change to the computation graph. \r\n\r\nI still think this is a bug so I'm going to leave this here. It's very frustrating when obvious things don't work as expected. \r\n\r\n\r\n", "comments": ["Your code is a logic flaw in and of itself. The values you give are x= a/and or b. having to values. 0*x+1*x, 1*x+0. Your equation gives both the same self canceling value. X is a and b. Both are worth both 1x and 0x. You cannot define one thing as having two set values. Things can have a range of values, however you must provide a mechanism for defining these within the program. As you have given x the value of both 1 and nothing, a and b, I hardly think it would work with any code anywhere. 3 functions for one self defined term with 2 simultaneous values? This isn\u2019t a quantum computer ya know. \ud83e\udd2a\ud83d\ude02\ud83d\ude01 ", "@Negrodaumus \r\nx is an array with 2 values. \r\nIn one case, these two values are each linear interpolations of two tensors a & b. \r\nIn the second case, these two values are simply b then a respectively. \r\nThese two cases produce two separately constructed computations graphs, where one has a bunch of multiplication and addition nodes, which the second doesn't.  \r\n\r\n", "This is a duplicate of #14675, so I'm closing this issue.\r\n\r\n/CC @asimshankar, can you take a look at #14675?", "@dbarta : Do you see this problem only when random initialization is involved?\r\n(Want to confirm that you don't see this issue if `a` and `b` have the same value between runs - and indeed the issue is only when you expect two calls to a random initializer with the same seed to produce the same value)"]}, {"number": 19312, "title": "std::to_string issue when compile tensorflow lite with android ndk", "body": "I add some code in tflitedemo, and will use std::to_string, but i failed with error that\r\nerror: no member named 'to_string' in namespace 'std'\r\n\r\nthe compiling command of tflite indicates it will use gcc-4.9 include files\r\nexternal/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang .... -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/x86_64/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward\r\n\r\nthe android ndk i am using is android-ndk-r16b\r\nin fact it has to_string definition in /home/chao/Android/android-ndk-r16b/sources/cxx-stl/llvm-libc++/include/string\r\n\r\nhow can i make tflite to compile based on llvm-libc++ not gnu-libstdc++?\r\n\r\ni update with the system information\r\n\r\n**System information**\r\n    **Have I written custom code (as opposed to using a stock example script provided in TensorFlow):**\r\n    add std::to_string() in kernel/internal/conv.cc as:\r\n      _auto str = std::to_string(input->dims->size);\r\n       std::cout << str;_\r\n    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**\r\n    ubuntu 16\r\n    **TensorFlow installed from (source or binary):**\r\n    sources\r\n    **TensorFlow version (use command below):**\r\n    master\r\n    **Python version:**\r\n    2.7\r\n    **Bazel version (if compiling from source):**\r\n    0.11.1\r\n    **CUDA/cuDNN version:**\r\n    N/A\r\n    **GPU model and memory:**\r\n    N/A\r\n    **Exact command to reproduce:**\r\n    bazel build --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt='--std=c++11' //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo --force_pic --cpu=x86_64  --verbose_failures --fat_apk_cpu=x86_64\r\n    **Android NDK:**\r\n    android-ndk-r15c\r\n\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "The following is the system information\r\n\r\nSystem information\r\n    **Have I written custom code (as opposed to using a stock example script provided in TensorFlow):**\r\n    add std::to_string() in kernel/internal/conv.cc as:\r\n      _auto str = std::to_string(input->dims->size);\r\n       std::cout << str;_\r\n    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**\r\n    ubuntu 16\r\n    **TensorFlow installed from (source or binary):**\r\n    sources\r\n    **TensorFlow version (use command below):**\r\n    master\r\n    **Python version:**\r\n    2.7\r\n    **Bazel version (if compiling from source):**\r\n    0.11.1\r\n    **CUDA/cuDNN version:**\r\n    N/A\r\n    **GPU model and memory:**\r\n    N/A\r\n    **Exact command to reproduce:**\r\n    bazel build --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt='--std=c++11' //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo --force_pic --cpu=x86_64  --verbose_failures --fat_apk_cpu=x86_64\r\n    **Android NDK:**\r\n    android-ndk-r15c\r\n", "Nagging Assignee @aselle: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@xiechao4 : Sorry to ask but have you included '#include <string>' in the code that you wrote?", "Looks like issue is obsolete."]}, {"number": 19311, "title": "Branch 196777020", "body": "Merging internal changes.", "comments": []}, {"number": 19310, "title": "What packages I should use to build my Relationship advisor algorithm?", "body": "Hey guys,\r\n\r\nGood day!\r\n\r\nI'm trying to build a Relationship advisor using Machine Learning. \r\n\r\n**Project Description:** The system will be presented to the couples who are going to be in a relationship / going to apply divorce / going to marry. They will be asked a set of questions by the system about their habits, lifestyle, behaviour etc., Based on the answers provided by both of them, the system will evaluate them and tell them whether they are made for each other or not based on the percentage of matching between them.\r\n\r\nIn case, if the couples are going to apply divorce, the system will analyse them and also suggest some ways to the couples to solve their problem. If not, it will tell them to apply divorce so that they could lead a happier life separately.\r\n\r\n**Query:** So, the above description explains what I want to build. \r\nPlease suggest some packages inside tensorflow through which I can achieve the system I want.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hi @tensorflowbutler,\r\n\r\nThanks for the response.\r\n \r\nHave I written custom code: No.  I'm trying to explore the possibilities for now and then I'll write code.\r\nOS Platform and Distribution:  Ubuntu 17.04(mine) and Windows 10(friend)\r\nTensorFlow installed from: N/A\r\nTensorFlow version: N/A\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A", "Nagging Assignee @rohan100jain: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rohan100jain: It has been 31 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 19309, "title": "Generate Java graph building API from @Operator annotated classes", "body": "That completes the work started by @kbsriram to generate the `*Ops` API classes out of the operation wrappers annotated with `@Operator`. That includes core operations wrappers that are automatically generated at compile time or any user (or custom) operation that might be added to the source path.\r\n\r\nHere's an example of usage of the generated classes:\r\n```java\r\ntry (Graph g = new Graph()) {\r\n  Ops ops = new Ops(g);\r\n  // Operations are typed classes with convenience builders in Ops.\r\n  Constant three = ops.constant(3);\r\n  // Single-result operations implement the Operand interface, so this works too.\r\n  Operand four = ops.constant(4);\r\n  // Most builders are found within a group, and accept Operand types as operands\r\n  Operand nine = ops.math().add(four, ops.constant(5));\r\n  // Multi-result operations however offer methods to select a particular result for use.\r\n  Operand result = ops.math().add(ops.array().unique(s, a).y(), b);\r\n  // Optional attributes\r\n  ops.math().matMul(a, b, MatMul.transposeA(true));\r\n  // Naming operators\r\n  ops.withName(\u201cfoo\u201d).constant(5); // name \u201cfoo\u201d\r\n  // Names can exist in a hierarchy\r\n  Ops subOps = ops.withSubScope(\u201csub\u201d);\r\n  subOps.withName(\u201cbar\u201d).constant(4); // \u201csub/bar\u201d\r\n}\r\n```\r\nNote that the grouping of the operation wrappers in the `java_api` still need to be completed after this before releasing a first version of this API.\r\n\r\nCC: @asimshankar ", "comments": ["Nagging Assignee @asimshankar: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Woohoo :clap: awesome @karllessard for bringing things to this milestone! Thanks to both you and @asimshankar for all the ongoing work on the Java API.", "@karllessard : The tests are failing because of formatting sanity checks, specifically:\r\n\r\n```\r\nRunning do_buildifier on 341 files\r\ntensorflow/java/BUILD # listsort unsafesort sort:java_library.deps\r\nbuildifier took 1 s\r\nFAIL: buildifier found errors and/or warnings in above BUILD files.\r\nbuildifier suggested the following changes:\r\n60d59\r\n<         \"@com_squareup_javapoet\",\r\n61a61\r\n>         \"@com_squareup_javapoet\",\r\nPlease fix manually or run buildifier <file> to auto-fix.\r\n```\r\n\r\nCould you please fix? Thanks!", "Here @asimshankar , sorry for that!"]}, {"number": 19308, "title": "avoid magic number in AsyncService", "body": "", "comments": []}, {"number": 19307, "title": "Fix tf.matching_files issue with access denied subdirectory.", "body": "This fix tries to address the issue raised in #19274 where\r\ntf.matching_files will return an error if no permission for\r\nsubdirectories (Even if the subdirectories does not match).\r\n\r\nThis fix addresses the issue by ignore the case for `permisson denied`\r\nduring the directory traversal.\r\n\r\nThis fix fixes #19274.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang are you still working on this PR?", "@rohan100jain @yifeif Thanks for the help. The PR has been updated with test cases added. Also, when GetChildren encountered PERMISSION_DENIED will pass. Please take a look and let me know if there are any issues.", "Thanks @yongtang! ", "Thanks @rohan100jain  for the review. The PR has been updated. Please take a look.", "Nagging Assignee @rohan100jain: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@yifeif The `feedback/copybara` is failing but I could not see the details. Is there anyway to see what might be the error in `feedback/copybara`?", "Looks like the PR is merged successfully. Thanks all for the help \ud83d\udc4d \u2764\ufe0f !"]}, {"number": 19306, "title": "Fixing a buildifier issue.", "body": "", "comments": ["https://github.com/tensorflow/tensorflow/pull/19366 fixes GPU Python3 failure."]}, {"number": 19305, "title": "Inconsistent behaviour when `Dataset.repeats()` and `max_steps` differ", "body": "### System information\r\n(This issue is about an inconsistent behaviour between running TF with and without GPU. I will update the system information when it is clear to me which system that is not behaving. \r\n\r\n### Describe the problem\r\nI get inconsistent behaviour when I try to train a model with/without GPU. The problem can be summarized with these few lines of code:\r\n```\r\n    train_spec = tf.estimator.TrainSpec(\r\n        input_fn=lambda: make_dataset_iter(\r\n            n_repeats=2),  # *** THIS HAS PRECEDENCE ON GPU ***\r\n        max_steps=N_SAMPLES * 3)  # *** THIS HAS PRECEDENCE WITHOUT GPU ***\r\n```\r\nWith a `make_dataset_iter` function that produces 20 samples (`N_SAMPLES=20`) I will get a model that is trained for 60 global steps (data repeated 3 times) when I run locally without GPU, and 40 global steps (data repeated 2 times) when I run it in a Docker container on a GPU cluster. Not yet sure whether it is a GPU thing, or something else related to the way I run it in the cloud.\r\n\r\nNote that this also is a problem when either `max_steps` or `Dataset.repeats()` are set to the default value `None` and the other is set to something specific. This is probably the use-case where most people will experience this inconsistency.\r\n\r\nWhich of the two are the expected behaviours?\r\n\r\nFull code and logs can be found below.\r\n\r\n### Source code / logs\r\nMinimal code reproducing the issue:\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\nN_SAMPLES = 20\r\n\r\n\r\ndef lr_model_fn(features, labels, mode):\r\n    \"\"\"Simple logistic regression model\"\"\"\r\n    prediction = tf.layers.dense(features, units=1)\r\n    loss = tf.losses.mean_squared_error(prediction, labels)\r\n\r\n    return tf.estimator.EstimatorSpec(\r\n        mode=mode,\r\n        predictions=prediction,\r\n        loss=loss,\r\n        train_op=tf.train.AdamOptimizer().minimize(\r\n            loss=loss,\r\n            global_step=tf.train.get_global_step(),\r\n        )\r\n    )\r\n\r\n\r\ndef make_dataset_iter(n_repeats=1):\r\n    \"\"\"Turns an instance of a generator into a Dataset\"\"\"\r\n    dataset = tf.data.Dataset.from_generator(\r\n        generator=lambda: zip(range(N_SAMPLES), range(N_SAMPLES)),\r\n        output_types=(tf.float32))\r\n    dataset = dataset.repeat(n_repeats)\r\n    dataset = dataset.make_one_shot_iterator()\r\n    dataset = dataset.get_next()\r\n    features, labels = dataset[0], dataset[1]\r\n    return tf.reshape(features, [-1, 1]), tf.reshape(labels, [-1, 1])\r\n\r\n\r\ndef run_experiment():\r\n    # Create the estimator\r\n    estimator = tf.estimator.Estimator(\r\n        model_fn=lr_model_fn,\r\n    )\r\n    # Build train/eval specs\r\n    train_spec = tf.estimator.TrainSpec(\r\n        input_fn=lambda: make_dataset_iter(\r\n            n_repeats=2),  # *** THIS HAS PRECEDENCE ON GPU ***\r\n        max_steps=N_SAMPLES * 3)  # *** THIS HAS PRECEDENCE WITHOUT GPU ***\r\n    eval_spec = tf.estimator.EvalSpec(\r\n        input_fn=make_dataset_iter)\r\n    # Run training\r\n    tf.estimator.train_and_evaluate(\r\n        estimator,\r\n        train_spec,\r\n        eval_spec\r\n    )\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n    run_experiment()\r\n\r\n```\r\n[Logs from the model running on GPU cluster](https://github.com/tensorflow/tensorflow/files/2006677/with_gpu_log.txt)\r\n[Logs from my local machine](https://github.com/tensorflow/tensorflow/files/2006678/without_gpu_log.txt)\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Yes", "@mrry could you take a look?", "This sounds like a `tf.estimator.TrainSpec` issue, so reassigning to @xiejw.", "Sorry, I do not understand the problem here.  What's the inconsistent behavior here? The description and code are not very clear to me. For example\r\n\r\n- Where does the BATCH_SIZE have been used in the code? maybe I missed that part. \r\n- I do not understand these two lines \r\n\r\n            n_repeats=2),  # *** THIS HAS PRECEDENCE ON GPU ***\r\n        max_steps=N_SAMPLES * 3)  # *** THIS HAS PRECEDENCE WITHOUT GPU ***\r\n\r\nBut as I scanned the example code, I noticed you have a stopping condition in your input_fn, which is based on dataset repeat. \r\n\r\ntf.estimator.train_and_evaluate does not work with that in most of cases.   \r\n\r\nAs mentioned in the docstring (see [1]), \"Stop condition: In order to support both distributed and non-distributed configuration reliably, the only supported stop condition for model training is train_spec.max_steps. If train_spec.max_steps is None, the model is trained forever. \" \r\n\r\nIt is recommended, for training, make your input_fn returns data for ever, you can do that by using repeat(), i.e., no argument to repeat. And purely using the train_spec.max_steps to control the training length.\r\n \r\nHope this works for you.\r\n\r\n[1] https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate", "Thanks for getting back @xiejw . This is not about getting it to work for me - I have no problem with that. This is about reporting an issue to help improving TensorFlow and help other people not banging their heads agains the wall.\r\n\r\nPlease disregard `BATCH_SIZE`. It was removed in my process of simplifying the example, but apparently I forgot to move the the definition of this. I have removed this line from the example.\r\n\r\nI guess the root cause of the supposed inconsistency is whether training is stopped when based on the dataset repeat's `OutOfRangeError` or whether it is based on `train_spec.max_steps`. On one machine it stops when `OutOfRangeError` is reached, on another machine it re-initialises the dataset after getting the `OutOfRangeError` and continues to run until `train_spec.max_steps`.\r\n\r\nThe question is whether this is a feature or a bug?\r\n\r\nIt is true that the docstring recommends using only `train_spec.max_steps`, however, this is with distributed training in mind, which this issue is not about.\r\n\r\nIf you still think this is the desired behaviour, feel free to close the issue.\r\n\r\n", "@elgehelge I totally agree with you and share the same pain. \r\n\r\nI would like to share few things here\r\n\r\n1. We are improving the behavior for non-distributed case to avoid such confusion. It is not easy due to some special environments. But we are working on that. You should see some change soon in the master head (should be one or two weeks). With that, we hope this issue can be resolved. \r\n\r\n2. I have opened a team discussion to see whether we could detect the early stopping triggered by the input pipeline, i.e, dataset in your case. At least, a warning in the log should help user. We should give actionable, debuggable instructions. Hope you agree with this. \r\n\r\nI will close this issue once one of both is arrived. Hopefully both. \r\n ", "Nagging Assignees @xiejw, @drpngx: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Close this issue given the following commit.\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/3edb609926f2521c726737fc1efeae1572dc6581"]}, {"number": 19304, "title": "With TF C-API, got \"GPU device not registered\" and no-op for SessionRun.", "body": "Please go to Stack Overflow for help and support:\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.7.0/1.8.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:n/a\r\n- **GCC/Compiler version (if compiling from source)**:5.4.0\r\n- **CUDA/cuDNN version**:9.0/7.0\r\n- **GPU model and memory**:1080Ti\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\nOutput:\r\n```== cat /etc/issue ===============================================\r\nLinux yangl-contenttech 4.13.0-41-generic #46~16.04.1-Ubuntu SMP Thu May 3 10:06:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.4 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux yangl-contenttech 4.13.0-41-generic #46~16.04.1-Ubuntu SMP Thu May 3 10:06:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy                         1.14.3                \r\nprotobuf                      3.5.2.post1           \r\ntensorflow-gpu                1.8.0                 \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.8.0\r\ntf.GIT_VERSION = v1.8.0-0-g93bc2e2072\r\ntf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072\r\nSanity check: array([1], dtype=int32)\r\n/home/yangl/.local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda/lib64:/usr/local/cuda-9.0/extras/CUPTI/lib64/:\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nTue May 15 13:54:31 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 390.48                 Driver Version: 390.48                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |\r\n| 27%   48C    P0    72W / 250W |   1489MiB / 11177MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1168      G   /usr/lib/xorg/Xorg                           870MiB |\r\n|    0      2504      G   compiz                                       437MiB |\r\n|    0      3240      G   ...-token=F3F41E0025601B32775ADF60FE0AE568   178MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.252\r\n/usr/local/cuda-9.0/targets/aarch64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-9.0/targets/aarch64-linux/lib/libcudart.so.9.0.252\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7\r\n```\r\n\r\nYou can obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n```\r\n/home/yangl/.local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n('v1.8.0-0-g93bc2e2072', '1.8.0')\r\n```\r\n\r\n### Describe the problem\r\nI am trying to load and run graph model (mobilenet v2) with Tensorflow C-API. Python script downloaded from following link works on my desktop.\r\n`https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_v2.py`\r\nThe c-api library and framework library are downloaded via Bazel:\r\n```\r\nnew_http_archive(\r\n    name = \"libtensorflow\",\r\n    build_file = \"third_party/libtensorflow.BUILD\",\r\n    sha256 = \"05f5b543d5054f3c1ddc4f4caff7e3a6a96985579ef2d3dd9340ab94b1262f54\",\r\n    \r\n    url = \"https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-linux-x86_64-1.8.0.tar.gz\",\r\n    )\r\ncc_library(\r\n    name = \"libtensorflow\",\r\n    srcs = [\r\n        \"lib/libtensorflow.so\",\r\n        \"lib/libtensorflow_framework.so\",\r\n    ],\r\n    hdrs = [\r\n        \"include/tensorflow/c/c_api.h\",\r\n        \"include/tensorflow/c/c_api_experimental.h\",\r\n    ],\r\n    strip_include_prefix = \"include\",\r\n    visibility = [\"//visibility:public\"],\r\n)\r\n```\r\nWith following code calling into C-API I see non-changed output buffer after SessionRun() and dubious console log:\r\n```\r\n2018-05-15 15:42:11.572574: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-05-15 15:42:11.695012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 10.92GiB freeMemory: 9.26GiB\r\n2018-05-15 15:42:11.695035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\r\n2018-05-15 15:42:12.104753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-05-15 15:42:12.104773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 \r\n2018-05-15 15:42:12.104777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \r\n2018-05-15 15:42:12.104976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8958 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nHello from TensorFlow C library version 1.8.0\r\ninf starting\r\n2018-05-15 15:42:12.195162: E tensorflow/core/grappler/clusters/utils.cc:127] Not found: TF GPU device with id 0 was not registered\r\nMax value of 0 at 0\r\nMax value of 0 at 0\r\nMax value of 0 at 0\r\nMax value of 0 at 0\r\nMax value of 0 at 0\r\nMax value of 0 at 0\r\nMax value of 0 at 0\r\nMax value of 0 at 0\r\nMax value of 0 at 0\r\nMax value of 0 at 0\r\n\r\n```\r\n\r\nThe model is downloaded here: `https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet`\r\n`https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.4_224.tgz`\r\n\r\nThe image is downloaded here:\r\n`https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_example.ipynb`\r\nand processed with python code for loading in c++:\r\n```\r\nimport numpy as np\r\nfrom PIL import Image\r\nimg = np.array(Image.open('panda.jpeg').resize((224, 224))).astype(np.float) / 128 - 1\r\nreimg=np.reshape(img, [224,672])\r\nnp.savetxt('panda.ndarray',reimg)\r\n```\r\n\r\n### Source code / logs\r\n```#include <string.h>\r\n#include <fstream>\r\n#include <iostream>\r\n#include <memory>\r\n#include <string>\r\n\r\n#include \"gflags/gflags.h\"\r\n#include \"tensorflow/c/c_api.h\"\r\n\r\n#define ASSERT(expr, ...)      \\\r\n  if (!(expr)) {               \\\r\n    char buf[1024];            \\\r\n    sprintf(buf, __VA_ARGS__); \\\r\n    std::cerr << buf;          \\\r\n    std::abort();              \\\r\n  }\r\n\r\nDEFINE_string(model_filename,\r\n              \"mobilenet_v2_1.4_224_frozen.pb\",\r\n              \"Filename for model to load\");\r\n\r\nDEFINE_int32(num_repeat, 10, \"Run inference many times.\");\r\n\r\nDEFINE_string(np_filename, \"panda.ndarray\",\r\n              \"Filename for (text) image file to load\");\r\n\r\nvoid free_buffer(void* data, size_t length) { free(data); }\r\n\r\nvoid Deallocator(void* data, size_t length, void* arg) { free(data); }\r\n\r\nTF_Buffer* read_file(const char* file) {\r\n  FILE* f = fopen(file, \"rb\");\r\n  ASSERT(f != nullptr, \"Model File Not Ready\");\r\n  fseek(f, 0, SEEK_END);\r\n  long fsize = ftell(f);\r\n  fseek(f, 0, SEEK_SET);  // same as rewind(f);\r\n\r\n  void* data = malloc(fsize);\r\n  std::ignore = fread(data, fsize, 1, f);\r\n  fclose(f);\r\n\r\n  TF_Buffer* buf = TF_NewBuffer();\r\n  buf->data = data;\r\n  buf->length = fsize;\r\n  buf->data_deallocator = free_buffer;\r\n  return buf;\r\n}\r\n\r\nint main(int argc, char** argv) {\r\n  printf(\"Hello from TensorFlow C library version %s\\n\", TF_Version());\r\n\r\n  // Creates buffer for graph def by reading pb file\r\n  std::unique_ptr<TF_Buffer, std::function<void(TF_Buffer*)>> p_model_buffer(\r\n      read_file(FLAGS_model_filename.c_str()), [](TF_Buffer* p) { TF_DeleteBuffer(p); });\r\n  ASSERT(\r\n      p_model_buffer != nullptr && p_model_buffer->data != nullptr && p_model_buffer->length != 0,\r\n      \"Reading Graph Model pb file Failure\");\r\n\r\n  // Creates error status for checking\r\n  std::unique_ptr<TF_Status, std::function<void(TF_Status*)>> p_status(\r\n      TF_NewStatus(), [](TF_Status* p) { TF_DeleteStatus(p); });\r\n  ASSERT(p_status != nullptr, \"Status Creation Failure\");\r\n\r\n  // Creates graph instance\r\n  std::unique_ptr<TF_Graph, std::function<void(TF_Graph*)>> p_graph(\r\n      TF_NewGraph(), [](TF_Graph* p) { TF_DeleteGraph(p); });\r\n  ASSERT(p_graph != nullptr, \"Graph Creation Failure\");\r\n\r\n  // Creates session option\r\n  std::unique_ptr<TF_SessionOptions, std::function<void(TF_SessionOptions*)>> p_session_opts(\r\n      TF_NewSessionOptions(), [](TF_SessionOptions* p) { TF_DeleteSessionOptions(p); });\r\n  ASSERT(p_session_opts != nullptr, \"Create SessionOptions Failure\");\r\n\r\n  // Creates options for importing graph_def\r\n  std::unique_ptr<TF_ImportGraphDefOptions, std::function<void(TF_ImportGraphDefOptions*)>>\r\n      p_import_graph_def_options(TF_NewImportGraphDefOptions(), [](TF_ImportGraphDefOptions* p) {\r\n        TF_DeleteImportGraphDefOptions(p);\r\n      });\r\n  ASSERT(p_import_graph_def_options != nullptr, \"Create Import Graph Def Option Failure\");\r\n\r\n  // Imports graph def into graph\r\n  TF_GraphImportGraphDef(p_graph.get(), p_model_buffer.get(), p_import_graph_def_options.get(),\r\n                         p_status.get());\r\n  ASSERT(TF_GetCode(p_status.get()) == TF_Code::TF_OK, \"Import Failure %s\",\r\n         TF_Message(p_status.get()));\r\n\r\n  // Grab output ops from importing result\r\n  TF_Operation* ops_in = TF_GraphOperationByName(p_graph.get(), \"input\");\r\n  ASSERT(ops_in != nullptr, \"Getting Input Operation Failure\");\r\n  TF_Operation* ops_out =\r\n      TF_GraphOperationByName(p_graph.get(), \"MobilenetV2/Predictions/Reshape_1\");\r\n  ASSERT(ops_out != nullptr, \"Getting Output Operation Failure\");\r\n  TF_Output tf_output_in{.oper = ops_in, .index = 0};\r\n  TF_Output tf_output_out{.oper = ops_out, .index = 0};\r\n\r\n  // Create Tensors for input and output\r\n  std::vector<std::shared_ptr<TF_Tensor>> input_tensor_vector;\r\n  std::vector<std::shared_ptr<TF_Tensor>> output_tensor_vector;\r\n  // Input tensor is fp32 image of 224x224 with bgr channels, as\r\n  // img = np.array(PIL.Image.open('panda.jpeg').resize((224, 224))).astype(np.float) / 128 - 1\r\n  int64_t input_dims[] = {1, 224, 224, 3};\r\n  ASSERT(sizeof(float) == 4, \"Float32 should be 4 bytes.\");\r\n  size_t input_num_bytes = 224 * 224 * 3 * sizeof(float);\r\n  // Output tensor is fp32 1001x1 one-hot for classes of objects\r\n  int64_t output_dims[] = {1, 1001};\r\n  size_t output_num_bytes = 1001 * sizeof(float);\r\n\r\n  float* input_buffer = static_cast<float*>(malloc(input_num_bytes));\r\n  ASSERT(input_buffer != nullptr, \"Input Tensor Memory Allocation Failure.\");\r\n  TF_Tensor* input_tensor =\r\n      TF_NewTensor(TF_FLOAT, input_dims, 4, input_buffer, input_num_bytes, &Deallocator, 0);\r\n  ASSERT(input_tensor != nullptr, \"Input Tensor Creation Failure\");\r\n  input_tensor_vector.push_back(\r\n      std::shared_ptr<TF_Tensor>(input_tensor, [](TF_Tensor* p) { TF_DeleteTensor(p); }));\r\n\r\n  float* output_buffer = static_cast<float*>(malloc(output_num_bytes));\r\n  ASSERT(output_buffer != nullptr, \"Output Tensor Memory Allocation Failure.\");\r\n  memset(output_buffer, 0, output_num_bytes);\r\n  TF_Tensor* output_tensor =\r\n      TF_NewTensor(TF_FLOAT, output_dims, 2, output_buffer, output_num_bytes, &Deallocator, 0);\r\n  ASSERT(output_tensor != nullptr, \"Output Tensor Creation Failure\");\r\n  output_tensor_vector.push_back(\r\n      std::shared_ptr<TF_Tensor>(output_tensor, [](TF_Tensor* p) { TF_DeleteTensor(p); }));\r\n\r\n  // Prepare input image data\r\n  {\r\n    std::ifstream img(FLAGS_np_filename);\r\n    ASSERT(img, \"Read input image tensor failure.\");\r\n    float* p_data = static_cast<float*>(TF_TensorData(input_tensor_vector[0].get()));\r\n    for (int i = 0; i < 224; ++i) {\r\n      for (int j = 0; j < 224 * 3; ++j) {\r\n        int offset = i * 224 * 3 + j;\r\n        img >> p_data[offset];\r\n      }\r\n    }\r\n  }\r\n  {\r\n    // Initializes config proto with magic numbers got from following python code:\r\n    // config = tf.ConfigProto(allow_soft_placement = True)\r\n    // serialized = config.SerializeToString() # '8\\x01'\r\n    char config_proto[] = {'8', 0x01};\r\n    TF_SetConfig(p_session_opts.get(), config_proto, 2, p_status.get());\r\n    ASSERT(TF_GetCode(p_status.get()) == TF_Code::TF_OK, \"Set Config Failure %s\",\r\n           TF_Message(p_status.get()));\r\n\r\n    // Creates session instance for loaded graph model\r\n    std::unique_ptr<TF_Session, std::function<void(TF_Session*)>> p_session(\r\n        TF_NewSession(p_graph.get(), p_session_opts.get(), p_status.get()), [&](TF_Session* p) {\r\n          TF_CloseSession(p, p_status.get());\r\n          TF_DeleteSession(p, p_status.get());\r\n        });\r\n    ASSERT(p_session != nullptr, \"Session Creation Failure %s\", TF_Message(p_status.get()));\r\n\r\n    // Empty RunOptions for inference.\r\n    TF_Buffer* run_options = nullptr;\r\n    constexpr int ninputs = 1;\r\n    constexpr int noutputs = 1;\r\n    constexpr int ntargets = 0;\r\n    TF_Tensor* output_tensors[1] = {output_tensor_vector[0].get()};\r\n    TF_Tensor* const input_tensors[1] = {input_tensor_vector[0].get()};\r\n\r\n    std::cerr << \"inf starting\" << std::endl;\r\n    for (int i = 0; i < FLAGS_num_repeat; ++i) {\r\n      TF_SessionRun(p_session.get(), run_options,\r\n                    // Input tensors\r\n                    &tf_output_in, input_tensors, ninputs,\r\n                    // Output tensors\r\n                    &tf_output_out, output_tensors, noutputs,\r\n                    // Target operations\r\n                    nullptr, ntargets,\r\n                    // RunMetadata\r\n                    nullptr,  // only valid run_metadata value\r\n                    p_status.get());\r\n      ASSERT(p_status != nullptr, \"Inference Failure %s\", TF_Message(p_status.get()));\r\n\r\n      // print out max prob and corresponding idx\r\n      const float* p_data = static_cast<const float*>(TF_TensorData(output_tensor_vector[0].get()));\r\n      float v_max = p_data[0];\r\n      int i_max = 0;\r\n      for (int i = 1; i < 1001; ++i) {\r\n        if (p_data[i] > v_max) {\r\n          v_max = p_data[i];\r\n          i_max = i;\r\n        }\r\n      }\r\n      std::cout << \"Max value of \" << v_max << \" at \" << i_max << std::endl;\r\n    }\r\n  }\r\n\r\n  return 0;\r\n}\r\n```", "comments": ["How did you solve this?", "@thomasyoung @tobs Did you guys figure out what the zero problem was caused by? Getting the same issue.", "I have the same issue, have solutions?", "the config proto magic number is wrong, using this way to get the correct one in Python\r\nlist(map(hex, serialized)) \r\nthe output is ['0x38', '0x1']"]}, {"number": 19303, "title": "Model checkpoint issue: serialization error for tf.string", "body": "TF version: 1.8\r\nInstallation: tensorflow/tensorflow:latest-gpu\r\nOS details: \r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 16.04.4 LTS\r\nRelease:\t16.04\r\nCodename:\txenial\r\n\r\nAWS instance: p2.xlarge\r\nGPU details:\r\n```\r\n[name: \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 16086859869116902206\r\n, name: \"/device:GPU:0\"\r\ndevice_type: \"GPU\"\r\nmemory_limit: 11285974221\r\nlocality {\r\n  bus_id: 1\r\n  links {\r\n  }\r\n}\r\nincarnation: 13890740079777279899\r\nphysical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7\"\r\n]\r\n```\r\n\r\nCustom code:\r\n```python\r\nuse_model = hub.Module(\"http://tfhub.dev/google/universal-sentence-encoder/1\", trainable=True);\r\nsess.run(tf.global_variables_initializer());\r\nsess.run(tf.tables_initializer());\r\n\r\ndef USEEmbedding(x):\r\n    return use_model(tf.squeeze(tf.cast(x, tf.string)), \r\n                      signature=\"default\", as_dict=True)[\"default\"]\r\n \r\ninput_text = layers.Input(shape=(1,), dtype=tf.string)\r\nembedding = layers.Lambda(USEEmbedding, output_shape=(512,))(input_text)\r\ndense = layers.Dense(1024, activation='relu')(embedding)\r\nbnorm = layers.BatchNormalization()(dense)\r\npred = layers.Dense(2000, activation='softmax')(bnorm)\r\n\r\nmodel = Model(inputs=[input_text], outputs=pred)\r\n\r\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\nmodel.summary()\r\n\r\ncallbacks = [keras.callbacks.EarlyStopping(monitor='val_acc',\r\n                                               min_delta=1e-3,\r\n                                               patience=8,\r\n                                               verbose=0,\r\n                                               mode='auto'),\r\n             keras.callbacks.ModelCheckpoint('../models/best-weights.h5',\r\n                                                 monitor='val_acc',\r\n                                                 verbose=1,\r\n                                                 save_best_only=True,\r\n                                                 mode='auto'),\r\n             keras.callbacks.TensorBoard(log_dir='../tb-logs', histogram_freq=0,\r\n                                         write_graph=True, write_images=False)]\r\n\r\ntrain_text = [' '.join(t.split()[0:20]) for t in train_x.sentences.tolist()]\r\ntrain_text = np.array(train_text, dtype=object)[:, np.newaxis]\r\nvalid_text = [' '.join(t.split()[0:20]) for t in valid_x.sentences.tolist()]\r\nvalid_text = np.array(valid_text, dtype=object)[:, np.newaxis]\r\n\r\nmodel.fit(train_text , train['labels'].tolist(),\r\n          validation_data=(valid_text, valid['labels'].tolist()),\r\n          epochs=100, batch_size=256,\r\n          callbacks=callbacks, shuffle=True)\r\n```\r\n\r\n```\r\nError:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-81-e546b70750dc> in <module>()\r\n      2           validation_data=(valid_text, valid['labels'].tolist()),\r\n      3           epochs=100, batch_size=256,\r\n----> 4           callbacks=callbacks, shuffle=True)\r\n\r\n/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\r\n   1703                               initial_epoch=initial_epoch,\r\n   1704                               steps_per_epoch=steps_per_epoch,\r\n-> 1705                               validation_steps=validation_steps)\r\n   1706 \r\n   1707     def evaluate(self, x=None, y=None,\r\n\r\n/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc in _fit_loop(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\r\n   1254                             for l, o in zip(out_labels, val_outs):\r\n   1255                                 epoch_logs['val_' + l] = o\r\n-> 1256             callbacks.on_epoch_end(epoch, epoch_logs)\r\n   1257             if callback_model.stop_training:\r\n   1258                 break\r\n\r\n/usr/local/lib/python2.7/dist-packages/keras/callbacks.pyc in on_epoch_end(self, epoch, logs)\r\n     75         logs = logs or {}\r\n     76         for callback in self.callbacks:\r\n---> 77             callback.on_epoch_end(epoch, logs)\r\n     78 \r\n     79     def on_batch_begin(self, batch, logs=None):\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/_impl/keras/callbacks.pyc in on_epoch_end(self, epoch, logs)\r\n    466               self.model.save_weights(filepath, overwrite=True)\r\n    467             else:\r\n--> 468               self.model.save(filepath, overwrite=True)\r\n    469           else:\r\n    470             if self.verbose > 0:\r\n\r\n/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc in save(self, filepath, overwrite, include_optimizer)\r\n   2589         \"\"\"\r\n   2590         from ..models import save_model\r\n-> 2591         save_model(self, filepath, overwrite, include_optimizer)\r\n   2592 \r\n   2593     def save_weights(self, filepath, overwrite=True):\r\n\r\n/usr/local/lib/python2.7/dist-packages/keras/models.pyc in save_model(model, filepath, overwrite, include_optimizer)\r\n    125             'class_name': model.__class__.__name__,\r\n    126             'config': model.get_config()\r\n--> 127         }, default=get_json_type).encode('utf8')\r\n    128 \r\n    129         model_weights_group = f.create_group('model_weights')\r\n\r\n/usr/lib/python2.7/json/__init__.pyc in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, encoding, default, sort_keys, **kw)\r\n    249         check_circular=check_circular, allow_nan=allow_nan, indent=indent,\r\n    250         separators=separators, encoding=encoding, default=default,\r\n--> 251         sort_keys=sort_keys, **kw).encode(obj)\r\n    252 \r\n    253 \r\n\r\n/usr/lib/python2.7/json/encoder.pyc in encode(self, o)\r\n    205         # exceptions aren't as detailed.  The list call should be roughly\r\n    206         # equivalent to the PySequence_Fast that ''.join() would do.\r\n--> 207         chunks = self.iterencode(o, _one_shot=True)\r\n    208         if not isinstance(chunks, (list, tuple)):\r\n    209             chunks = list(chunks)\r\n\r\n/usr/lib/python2.7/json/encoder.pyc in iterencode(self, o, _one_shot)\r\n    268                 self.key_separator, self.item_separator, self.sort_keys,\r\n    269                 self.skipkeys, _one_shot)\r\n--> 270         return _iterencode(o, 0)\r\n    271 \r\n    272 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\r\n\r\n/usr/local/lib/python2.7/dist-packages/keras/models.pyc in get_json_type(obj)\r\n    102             return obj.__name__\r\n    103 \r\n--> 104         raise TypeError('Not JSON Serializable:', obj)\r\n    105 \r\n    106     from . import __version__ as keras_version\r\n\r\nTypeError: ('Not JSON Serializable:', tf.string)\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@fchollet looks like a keras issue", "I had the same issue and replacing \r\n```python\r\ninput_text = layers.Input(shape=(1,), dtype=tf.string)\r\n```\r\nby \r\n```python\r\ninput_text = layers.Input(shape=(1,), dtype=\"string\")\r\n```\r\nsolved it for me.", "This is fixed with latest tf-nightly build. Thanks!"]}, {"number": 19302, "title": "Image Classification: Usage with C++ API (python_api_gen failed)", "body": "### System information\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n**No custom code**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n**Ubuntu 16.04**\r\n- TensorFlow installed from (source or binary):\r\n**Installed from source**\r\n- TensorFlow version (use command below):\r\n**Tensorflow 1.8**\r\n- Python version: \r\n**3**\r\n- Bazel version (if compiling from source):\r\n**Build label: 0.13.0**\r\n- GCC/Compiler version (if compiling from source):\r\n**gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609**\r\n- CUDA/cuDNN version:\r\n**Cuda 9.0, cuDNN 7.0**\r\n- GPU model and memory:\r\n**GTX 1060 3gb**\r\n- Exact command to reproduce:\r\n----as taken from: https://www.tensorflow.org/tutorials/image_recognition\r\n\r\nbazel build tensorflow/examples/label_image/...\r\n\r\ncurl -L \"https://storage.googleapis.com/download.tensorflow.org/mode /inception_v3_2016_08_28_frozen.pb.tar.gz\" |\r\n  tar -C tensorflow/examples/label_image/data -xz\r\n\r\nbazel build tensorflow/examples/label_image/...\r\n\r\n\r\n### Describe the problem\r\n\r\nI am trying to run the image classification example with the c++ api; however, when I  do bazel build, the following error occurs. I had completed the python api example with no problems. Please help.\r\n\r\nAaron\r\n\r\n### Source code / logs\r\nERROR: /home/aaron/Software/tensorflow/tensorflow/tools/api/generator/BUILD:27:1: Executing genrule //tensorflow/tools/api/generator:python_api_gen failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/home/aaron/.cache/bazel/_bazel_aaron/f08049efc03facba46c4fd0cb426b554/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/tools/api/generator/create_python_api.py\", line 26, in <module>\r\n    from tensorflow.python.util import tf_decorator\r\n  File \"/home/aaron/.cache/bazel/_bazel_aaron/f08049efc03facba46c4fd0cb426b554/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 52, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"/home/aaron/.cache/bazel/_bazel_aaron/f08049efc03facba46c4fd0cb426b554/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"/usr/local/lib/python3.5/dist-packages/google/protobuf/__init__.py\", line 37, in <module>\r\n    __import__('pkg_resources').declare_namespace(__name__)\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2927, in <module>\r\n    @_call_aside\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2913, in _call_aside\r\n    f(*args, **kwargs)\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2952, in _initialize_master_working_set\r\n    add_activation_listener(lambda dist: dist.activate())\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 956, in subscribe\r\n    callback(dist)\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2952, in <lambda>\r\n    add_activation_listener(lambda dist: dist.activate())\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2515, in activate\r\n    declare_namespace(pkg)\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2097, in declare_namespace\r\n    _handle_ns(packageName, path_item)\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2047, in _handle_ns\r\n    _rebuild_mod_path(path, packageName, module)\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2066, in _rebuild_mod_path\r\n    orig_path.sort(key=position_in_sys_path)\r\nAttributeError: '_NamespacePath' object has no attribute 'sort'\r\nINFO: Elapsed time: 4009.117s, Critical Path: 120.96s\r\nINFO: 7023 processes, local.\r\n\r\n", "comments": ["Figured it out, running the following worked:\r\nbazel build tensorflow/examples/label_image:label_image\r\n\r\ninstead of \r\nbazel build tensorflow/examples/label_image/..."]}, {"number": 19301, "title": "Broken link on \"Simple text classifier with TF-Hub\" tutorial", "body": "In the Feature Columns part of the page, i.e.\r\n\r\n> Feature columns\r\n> \r\n> TF-Hub provides a feature column that applies a module on the given text feature and passes further the outputs of the module. In this tutorial we will be using the nnlm-en-dim128 module. For the purpose of this tutorial, the most important facts are:\r\n\r\nThe link in text \"feature column\" is broken.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This has been fixed internally, and should be updated in the docs once they next get pushed."]}, {"number": 19300, "title": "Branch 196691101", "body": "", "comments": ["cc: @erain", "@yifeif : Thanks a lot for the help!"]}, {"number": 19299, "title": "resource_mgr.h violates its own thread safety annotations", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS 10.13.4\r\n- **TensorFlow installed from (source or binary)**: Source (ff6be80a1ec3c353ebd0d17e2f0b46d9097310db)\r\n- **TensorFlow version (use command below)**: `b'v1.8.0-1663-gb2511764a7' 1.8.0`\r\n- **Python version**: 3.6.2\r\n- **Bazel version (if compiling from source)**: `0.13.0-homebrew`\r\n- **GCC/Compiler version (if compiling from source)**: `Apple LLVM version 9.1.0 (clang-902.0.39.1)`\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\n\r\nLine https://github.com/tensorflow/tensorflow/blob/de4a6e646be56ca59c78dd6f92f8f6bcc7196696/tensorflow/core/framework/resource_mgr.h#L527 accesses `resource_` without holding `mutex_`.  This is causing a warning for some custom op code of mine:\r\n\r\n```\r\nInstalling collected packages: debate\r\n  Found existing installation: debate 0.0.1\r\n    Uninstalling debate-0.0.1:\r\n      Successfully uninstalled debate-0.0.1\r\n  Running setup.py develop for debate\r\n    Complete output from command /Users/irving/anaconda/envs/openai/bin/python -c \"import setuptools, tokenize;__file__='/Users/irving/openai/debate/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" develop --no-deps:\r\n    running develop\r\n    running egg_info\r\n    writing debate.egg-info/PKG-INFO\r\n    writing dependency_links to debate.egg-info/dependency_links.txt\r\n    writing requirements to debate.egg-info/requires.txt\r\n    writing top-level names to debate.egg-info/top_level.txt\r\n    file debate.py (for module debate) not found\r\n    reading manifest file 'debate.egg-info/SOURCES.txt'\r\n    reading manifest template 'MANIFEST.in'\r\n    writing manifest file 'debate.egg-info/SOURCES.txt'\r\n    running build_ext\r\n    building 'debate_ops' extension\r\n    g++ -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/irving/anaconda/envs/openai/include -arch x86_64 -I/Users/irving/anaconda/envs/openai/include -arch x86_64 -I/Users/irving/anaconda/envs/openai/include/python3.6m -c debate/search.cc -o build/temp.macosx-10.7-x86_64-3.6/debate/search.o -std=c++1z -Wall -Werror -Wno-sign-compare -Wthread-safety -stdlib=libc++ -I/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\n    In file included from debate/search.cc:7:\r\n    /Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/include/tensorflow/core/framework/resource_mgr.h:527:22: error: passing variable 'resource_' by reference requires holding mutex 'mutex_' [-Werror,-Wthread-safety-reference]\r\n      ctx->set_output(0, resource_);\r\n                         ^\r\n    /Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/include/tensorflow/core/framework/resource_mgr.h:334:12: note: in instantiation of member function 'tensorflow::ResourceHandleOp<debate::(anonymous namespace)::Search>::Compute' requested here\r\n      explicit ResourceHandleOp(OpKernelConstruction* context);\r\n               ^\r\n    debate/search.cc:463:1: note: in instantiation of member function 'tensorflow::ResourceHandleOp<debate::(anonymous namespace)::Search>::ResourceHandleOp' requested here\r\n    REGISTER_RESOURCE_HANDLE_KERNEL(Search);\r\n    ^\r\n    /Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/include/tensorflow/core/framework/resource_mgr.h:350:27: note: expanded from macro 'REGISTER_RESOURCE_HANDLE_KERNEL'\r\n                              ResourceHandleOp<Type>)\r\n                              ^\r\n    1 error generated.\r\n    error: command 'g++' failed with exit status 1\r\n```\r\n\r\nThe code is at least *almost* correct, since\r\n\r\n1. The use of `resource_` outside the lock is read only\r\n2. `resource_` is only written to inside the lock.\r\n3. The unlocked read occurs only if `initialized_` is set to true.\r\n\r\nHowever, it looks like there is still a race condition:\r\n\r\n1. Two threads could both detect that `initialized_` is false at the same time.\r\n2. They both try to grab the lock.\r\n3. One of them succeeds, does some initialization, and allocates the resource.\r\n4. Once the first thread releases the lock, the second thread allocates the resource a second time.\r\n5. Possibly (I'm not sure) the two threads see different values when they call `set_output`.\r\n\r\nThe moral of this story is probably to not ignore thread safety warnings.", "comments": ["Cc @alextp, since https://github.com/tensorflow/tensorflow/commit/a72ee2f74061cdd72f1197eed4c90a8216d39d74.", "Also, does this use of `allocate_temp` violate the requirement to not use \"after kernel construction\" (not really sure what that means if used from a `Compute` call):\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/90fe7226a464983e72a0242d5a05e4acba309195/tensorflow/core/framework/op_kernel.h#L261", "It definitely violates that requirement, but so does a lot of other code\n(including the implementation of Variable and ResourceVariable). That\nrequirement is only meaningful for GPU tensors, though, and resource\ntensors are always on host so this does not matter.\n\nOn Tue, May 15, 2018 at 1:37 PM Geoffrey Irving <notifications@github.com>\nwrote:\n\n> Also, does this use of allocate_temp violate the requirement to not use\n> \"after kernel construction\" (not really sure what that means if used from a\n> Compute call):\n>\n>\n> https://github.com/tensorflow/tensorflow/blob/90fe7226a464983e72a0242d5a05e4acba309195/tensorflow/core/framework/op_kernel.h#L261\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19299#issuecomment-389303784>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxUFo4ivIzx47dO3PuSQkY1__monaks5tyzyOgaJpZM4UAAU2>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp That makes sense.\r\n\r\nFor the race condition, I think all that's required is checking `initialized_` again inside the critical section.  As for eliminating the warning, probably the only way is to remove the `GUARDED_BY` annotation.\r\n\r\nIf you concur I can send a patch.", "I've already submitted this patch internally, it should trickle out\nexternally soon :-)\n\nThanks, though\n\nOn Tue, May 15, 2018 at 3:26 PM Geoffrey Irving <notifications@github.com>\nwrote:\n\n> @alextp <https://github.com/alextp> That makes sense.\n>\n> For the race condition, I think all that's required is checking\n> initialized_ again inside the critical section. As for eliminating the\n> warning, probably the only way is to remove the GUARDED_BY annotation.\n>\n> If you concur I can send a patch.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19299#issuecomment-389332344>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxSHqBMrbX7IlLTFXGVL5V5cXFxeOks5ty1YBgaJpZM4UAAU2>\n> .\n>\n\n\n-- \n - Alex\n"]}, {"number": 19298, "title": "java/sbt. Cannot register 2 metrics with the same name on cross tests", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: osx 10.11.6\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: NA\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: https://github.com/ravwojdyla/minimal-repo-tf-crashing\r\n\r\n### Describe the problem\r\nWe have an issue open in scio (https://github.com/spotify/scio/issues/1137) which depends on java TF artifacts, please read full description in the link above. A TLDR: in cross test TF crushes with:\r\n\r\n```\r\nCannot register 2 metrics with the same name: /tensorflow/cc/saved_model/load_attempt_count\r\n```\r\n\r\nThis problem started to manifest itself from version 1.4.0 (previous versions were not affected), and continues to be a problem. The minimal reproduction repo is here: https://github.com/ravwojdyla/minimal-repo-tf-crashing and to reproduce you run `sbt +test`.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["/CC @asimshankar, can you take a look?", "Thanks for the report. I'm not familiar with Scala or sbt enough, so I apologize for any ignorance demonstrated below.\r\n\r\nIf I'm reading https://github.com/spotify/scio/issues/1137#issuecomment-387801270 right, it seems that the problem is that when the test is running in the same JVM as `sbt` (i.e., when `fork := true` is not set), then `org.tensorflow.NativeLibrary` is unable to correctly determine whether or not the shared library has already been loaded and ends up extracting the shared library to another location and loading it again, leading to the initialization conflict.\r\n\r\nResolving  #18397 would help with that, but it seems the underlying issue in this particular case is that `org.tensorflow.NativeLibrary.isLoaded()` isn't able to handle however `sbt` is managing class loading. \r\n\r\nI'll try to look into it, but any suggestions for a better implementation of [`isLoaded`](https://github.com/tensorflow/tensorflow/blob/3176ba990070cdde62b7cdf81747d70107d2e032/tensorflow/java/src/main/java/org/tensorflow/NativeLibrary.java#L119) will be greatly appreciated.\r\n", "Hmm...I'm not familiar with class loading/unloading in `sbt`, but I suspect the following:\r\n\r\n- The `NativeLibrary` class is being loaded multiple times, by different instances of a `ClassLoader` (does that sound like something `sbt +test` would do?)\r\n- The first time it is loaded it successfully loads the library (via `System.loadLibrary(...)`).\r\n- The second time around, I'm not clear on why the `isLoaded` check is failing, but it is (can that happen when a different `ClassLoader` is used?).\r\n\r\nI tried a hack to replace the [temporary directory](https://github.com/tensorflow/tensorflow/blob/2c5e94c075454bf23bfa5a8a83e2d05011f4758e/tensorflow/java/src/main/java/org/tensorflow/NativeLibrary.java#L87) with a fixed path, hoping that calling `System.loadLibrary` on the same path as before would be a no-op (as per [javadoc](https://docs.oracle.com/javase/7/docs/api/java/lang/Runtime.html#loadLibrary(java.lang.String))), but that didn't help either and I saw `java.lang.UnsatisfiedLinkError: Native Library /tmp/tensorflow_native_libraries-0/libtensorflow_jni.so already loaded in another classloader`. Which seems to suggest that there is some association between native libraries and class loaders?\r\n\r\nFor the record, what changed between version 1.3 and 1.4 was the the shared library was split into two (`libtensorflow_framework.so` and `libtensorflow_jni.so`) - with the latter depending on the former.\r\n\r\nIn the interim, I think adding `fork := true` in `build.sbt` would work around this problem.\r\n\r\nI'll think about a solution, ideas are welcome.", "I *think* sbt creates a different classloader to run each test class.\r\n\r\nMaybe `isLoaded` could list the currently loaded libraries and look for the presence of `libtensorflow` ?\r\n\r\nSomething along the line of:\r\n\r\n```scala\r\ndef isInClasspath: Boolean = {\r\n  import scala.collection.JavaConverters._\r\n  val libs = classOf[ClassLoader].getDeclaredField(\"loadedLibraryNames\")\r\n  libs.setAccessible(true)\r\n  val loadedLibs = \r\n      libs.get(ClassLoader.getSystemClassLoader())\r\n        .asInstanceOf[java.util.Vector[String]]\r\n        .asScala\r\n  loadedLibs.exists(_.contains(\"libtensorflow\"))\r\n}\r\n``` ", "JavaCPP keeps track of libraries already loaded from its cache, so using the [JavaCPP Presets for TensorFlow](https://github.com/bytedeco/javacpp-presets/tree/master/tensorflow) is one way to work around this. The official Java API is also bundled these days:\r\nhttps://github.com/bytedeco/javacpp-presets/tree/1.4.1/tensorflow#documentation\r\n\r\nHowever, if sbt uses multiple class loaders, the classes of TensorFlow should be loaded by the system class loader. That's a limitation of Java and JNI.", "I think I found the cause of the issue and I managed to workaround the bug for scio. See my comments on https://github.com/spotify/scio/issues/1137 and this PR: https://github.com/spotify/scio/pull/1168.\r\n\r\nThe problem is not SBT specific and anyone using a classloader hierarchy may experience it. ", "Thanks for the update @jto \r\nI'm still not 100% sure about what the appropriate fix would be.\r\n\r\n@saudet : How does JavaCPP keep track of libraries already loaded from its cache across class loaders (as in, how does it ensure that the class that keeps track of these libraries is managed by the system classloader?)", "It doesn't, class loaders isolate multiple versions of the same class from each other. We could do it through the file system I guess, but loading isn't the issue. The issue is that the life of the JNI library is associated with the class loader that loaded it, and it might get unloaded once that class loader is garbage collected. So the only real \"fix\" is to load any class that need JNI with the system class loader.\r\n\r\nIn a way though something like JavaCPP but standardized would make it easy to get all those details right in one place for as many libraries as we need, and then not worry about them at least. I wish the Panama team would understand the need for something like this, sigh...", "I'm facing the **Cannot register 2 metrics with the same name: /tensorflow/cc/saved_model/load_attempt_count** problem both in JUnit tests and in the context of application servers where different applications (i.e., different classloaders) try to load a tensorflow model (tf version 1.6.0).\r\nI'm trying to reproduce the issue programmatically, and what I experimented is:\r\nWhen I load a TF model in Java in a simple main class with SavedModelBundle.load() (as in the following code snippet) all is working, and in _/tmp_ only one _tensorflow_jni_ directory is created.\r\n\r\n```\r\npublic static void main(String[] args) throws InterruptedException {\r\n\tSavedModelBundle b = SavedModelBundle.load(mymodel_path, \"serve\");\r\n\tThread.sleep(10000);\t\r\n}\r\n```\r\nWhen using classloaders, i experience the Fatal error that is causing the JVM to crash, and in _/tmp_ i can find two different _tensorflow_jni_ directories.\r\n\r\nThis is the code:\r\n\r\n```\r\npublic class MyClassLoader extends ClassLoader {\r\n\r\n\tpublic MyClassLoader() {\r\n\t}\r\n\r\n\t@Override\r\n\tpublic Class<?> findClass(String name) {\r\n\t\tbyte[] bt = loadClassData(name);\r\n\t\treturn defineClass(name, bt, 0, bt.length);\r\n\t}\r\n\r\n\tprivate byte[] loadClassData(String className) {\r\n\t\t// read class\r\n\t\tInputStream is = getClass().getClassLoader().getResourceAsStream(className.replace(\".\", \"/\") + \".class\");\r\n\t\tByteArrayOutputStream byteSt = new ByteArrayOutputStream();\r\n\t\t// write into byte\r\n\t\tint len = 0;\r\n\t\ttry {\r\n\t\t\twhile ((len = is.read()) != -1) {\r\n\t\t\t\tbyteSt.write(len);\r\n\t\t\t}\r\n\t\t} catch (IOException e) {\r\n\t\t\te.printStackTrace();\r\n\t\t}\r\n\t\t// convert into byte array\r\n\t\treturn byteSt.toByteArray();\r\n\t}\r\n}\r\n```\r\n\r\nand the main:\r\n\r\n```\r\npublic class MultipleClassLoaderTest {\r\n\tpublic static void main(String[] args) throws NoSuchMethodException, SecurityException, IllegalArgumentException,\r\n\t\t\tInvocationTargetException, IllegalAccessException {\r\n\t\tMyClassLoader l1 = new MyClassLoader();\r\n\t\t\r\n\t\tString[] classesToLoad = new  String[] { \"org.tensorflow.NativeLibrary\", \"org.tensorflow.TensorFlow\" };\r\n\t\tfor (String cl : classesToLoad) {\r\n\t\t\tl1.findClass(cl);\t\t\t\r\n\t\t}\r\n\r\n\t\tClass<?> smb = l1.findClass(\"org.tensorflow.SavedModelBundle\");\r\n\t\tMethod loadModelMethod = smb.getMethod(\"load\", new Class<?>[] { String.class, String[].class });\r\n\t\tObject loadedModel = loadModelMethod.invoke(null, new Object[] { \"mymodel_path\", new String[] {\"serve\"}});\r\n\r\n\t\tSystem.out.println(\"Done.\");\r\n\t}\r\n\r\n}\r\n```\r\n\r\nThe strange thing is that in this last example I was expecting the fatal error when instantiating two MyClassLoader instances. Instead, the error is going to appear with only one instance of MyClassLoader. Is the \"system\" class loader also trying to load the library?\r\nHas anyone addressed this issue in the context of application servers?", "The system class loader won't load it unless you tell it to. There isn't any good solutions other than loading it as a shared JAR or something as per the instructions of the framework you're using...", "Nagging Assignee @asimshankar: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Marking as \"Contributions Welcome\" just so that we can keep this issue open for tracking, but as https://github.com/deeplearning4j/deeplearning4j/issues/6166 suggests it seems that a fix would require some JDK changes?\r\n\r\nIn the mean time, any contributions to update the documentation (updating https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/install/install_java.md) will be appreciated.", "On Linux and Mac, when we have one large JNI-only binary with no symbols or resources shared globally across the process, it's also possible to rename the library before loading it with another class loader: https://github.com/netty/netty/issues/7272", "Actually, we just need to rename the JNI library containing the wrappers, not TensorFlow itself. I think I've successfully worked around this limitation in JavaCPP with commit https://github.com/bytedeco/javacpp/commit/dd57c2cca04c937de8031f5dfc896f1c45b59379 and TensorFlow seems to work well when loaded from multiple class loaders with the JavaCPP Presets for TensorFlow, which comes bundled with the official Java API, but also maps the more complete C/C++ APIs: https://github.com/bytedeco/javacpp-presets/tree/master/tensorflow\r\nI've tested this a bit on Tomcat and it seems to work fine!\r\n\r\nThis might not work as well when TensorFlow is linked statically to the JNI wrappers, as is the case with the official binaries that are deployed by Google, because TensorFlow would get loaded multiple times in the same process, but maybe it's fine. @asimshankar Is this something supported by TensorFlow?", "> I'm facing the **Cannot register 2 metrics with the same name: /tensorflow/cc/saved_model/load_attempt_count** problem both in JUnit tests and in the context of application servers where different applications (i.e., different classloaders) try to load a tensorflow model (tf version 1.6.0).\r\n> I'm trying to reproduce the issue programmatically, and what I experimented is:\r\n> When I load a TF model in Java in a simple main class with SavedModelBundle.load() (as in the following code snippet) all is working, and in _/tmp_ only one _tensorflow_jni_ directory is created.\r\n> \r\n> ```\r\n> public static void main(String[] args) throws InterruptedException {\r\n> \tSavedModelBundle b = SavedModelBundle.load(mymodel_path, \"serve\");\r\n> \tThread.sleep(10000);\t\r\n> }\r\n> ```\r\n> \r\n> When using classloaders, i experience the Fatal error that is causing the JVM to crash, and in _/tmp_ i can find two different _tensorflow_jni_ directories.\r\n> \r\n> This is the code:\r\n> \r\n> ```\r\n> public class MyClassLoader extends ClassLoader {\r\n> \r\n> \tpublic MyClassLoader() {\r\n> \t}\r\n> \r\n> \t@Override\r\n> \tpublic Class<?> findClass(String name) {\r\n> \t\tbyte[] bt = loadClassData(name);\r\n> \t\treturn defineClass(name, bt, 0, bt.length);\r\n> \t}\r\n> \r\n> \tprivate byte[] loadClassData(String className) {\r\n> \t\t// read class\r\n> \t\tInputStream is = getClass().getClassLoader().getResourceAsStream(className.replace(\".\", \"/\") + \".class\");\r\n> \t\tByteArrayOutputStream byteSt = new ByteArrayOutputStream();\r\n> \t\t// write into byte\r\n> \t\tint len = 0;\r\n> \t\ttry {\r\n> \t\t\twhile ((len = is.read()) != -1) {\r\n> \t\t\t\tbyteSt.write(len);\r\n> \t\t\t}\r\n> \t\t} catch (IOException e) {\r\n> \t\t\te.printStackTrace();\r\n> \t\t}\r\n> \t\t// convert into byte array\r\n> \t\treturn byteSt.toByteArray();\r\n> \t}\r\n> }\r\n> ```\r\n> \r\n> and the main:\r\n> \r\n> ```\r\n> public class MultipleClassLoaderTest {\r\n> \tpublic static void main(String[] args) throws NoSuchMethodException, SecurityException, IllegalArgumentException,\r\n> \t\t\tInvocationTargetException, IllegalAccessException {\r\n> \t\tMyClassLoader l1 = new MyClassLoader();\r\n> \t\t\r\n> \t\tString[] classesToLoad = new  String[] { \"org.tensorflow.NativeLibrary\", \"org.tensorflow.TensorFlow\" };\r\n> \t\tfor (String cl : classesToLoad) {\r\n> \t\t\tl1.findClass(cl);\t\t\t\r\n> \t\t}\r\n> \r\n> \t\tClass<?> smb = l1.findClass(\"org.tensorflow.SavedModelBundle\");\r\n> \t\tMethod loadModelMethod = smb.getMethod(\"load\", new Class<?>[] { String.class, String[].class });\r\n> \t\tObject loadedModel = loadModelMethod.invoke(null, new Object[] { \"mymodel_path\", new String[] {\"serve\"}});\r\n> \r\n> \t\tSystem.out.println(\"Done.\");\r\n> \t}\r\n> \r\n> }\r\n> ```\r\n> \r\n> The strange thing is that in this last example I was expecting the fatal error when instantiating two MyClassLoader instances. Instead, the error is going to appear with only one instance of MyClassLoader. Is the \"system\" class loader also trying to load the library?\r\n> Has anyone addressed this issue in the context of application servers?\r\n\r\nImeet this issue now same as yours\uff0c have you solve this issue\uff1f", "What's the status of this? You might have more luck with SIG-JVM, which is taking over tensorflow's Java functionality.\r\n\r\ngithub.com/tensorflow/java", "We're using JavaCPP there now, so this can be considered \"fixed\"."]}, {"number": 19297, "title": "[Feature Request] GCS and S3 support on windows", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Will be required.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows, all versions\r\n- **TensorFlow installed from (source or binary)**: source and binary,\r\n- **TensorFlow version (use command below)**: master, and all releases\r\n- **Python version**: python 3+\r\n- **Bazel version (if compiling from source)**: - \r\n- **GCC/Compiler version (if compiling from source)**: - \r\n- **CUDA/cuDNN version**: - \r\n- **GPU model and memory**: - \r\n- **Exact command to reproduce**: Install tf on windows, and try anything that reads from GCS or S3\r\n\r\n### Describe the problem\r\nCurrently, TF has no support for GCS and S3 on windows. Because they depend on curl, and we have not worked to make curl build and work on windows. Someone needs to dive in and work through the problems, and the rest should be just removing the windows exceptions for GCS and S3 support in configure.py.\r\n\r\nThis issue is open for community contributions.", "comments": ["Here's an interesting fact I learned recently, which might help the person who implements this warn users about network egress fees, or misconfigured deployments. It's possible to automatically get the GCE host region with `curl -sfL metadata.google.internal/0.1/meta-data/zone | sed -n 's!.*/\\([^-/]*-[^-/]*\\).*!\\1!p'` and get the GCS $BUCKET region prefix with `curl -sfLH \"Authorization:Bearer $(curl -sfLH Metadata-Flavor:Google metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token | sed -n 's/.*\"access_token\": *\"\\([^\"]*\\)\".*/\\1/p')\" \"https://www.googleapis.com/storage/v1/b/$BUCKET\" | sed -n 's/.*\"location\": *\"\\([^\"]*\\)\".*/\\1/p' | tr A-Z a-z`. If the DNS lookup fails, or the first string doesn't start with the second, then GCS egress isn't free. This is especially helpful for TensorBoard users, since it does bandwidth intensive log reading.", "I am interested in this feature and I am working on my proposal for this feature. Could you please provide more detail. Thank you \ud83d\ude04 ", "@vnvo2409 I can help you on this issue with respect to S3. If I remember, there were several dependencies that needs to be resolved for S3. curl is one, boringssl could be another one. In [tensorflow/io](https://github.com/tensorflow/io) we get the curl working in windows so you may copy the BUILD file and give it a try.\r\n\r\nOn a side note, there are ongoing discussions about moving S3/GCS support to [tensorflow/io](https://github.com/tensorflow/io) once modular file system C API is in place, @vnvo2409 maybe we should start the effort from tensorflow/io? Once the build is in place, we could looking into moving to modular file system. /cc @mihaimaruseac @gunan for input.", "Let's keep this on hold for a while longer until Windows modular filesystem is released (if all goes well, at most 3-4 weeks?)", "@yongtang I wrote a small C++ program to list the content of a bucket on S3 ( by sending a curl http get request) and it works with openssl and the libcurl provided by Windows. However, I did not manage to build the curl by [tensorflow/io](https://github.com/tensorflow/io) on window. Could you please provide more instructions about building it on windows.\r\n\r\n@mihaimaruseac If the modular filesystem is released and goes well, will we continue working on this feature ( by resolving the missing dependencies ) or we will write plugins to provide S3 and GCS support. If there is any change, please let me know because the deadline to submit proposal for GSoC will be 31th March. \r\n\r\nAnd finally, beside sending my draft to webpaige@google.com, is there anyone could provide feedbacks for me. Thank you \ud83d\ude04 ", "@vnvo2409 Building [tensorflow/io](https://github.com/tensorflow/io) on windows is done through Bazel, we have GitHub Actions CI to check every commit: https://github.com/tensorflow/io/actions\r\n\r\nThe config file of GitHub actions is located: https://github.com/tensorflow/io/blob/master/.github/workflows/build.yml\r\n\r\nInside you can find related Windows part:\r\nhttps://github.com/tensorflow/io/blob/edd02b2cefd1dbf719990b42ea43de4c4eede2b5/.github/workflows/build.yml#L241-L270\r\n\r\nSetting up Bazel on Windows is not exactly straightforward, (that is also why the the issue of porting GCS/S3 to Windows is still open). But we are here to resolve it.\r\n\r\n@vnvo2409 Could you open an issue in [tensorflow/io](https://github.com/tensorflow/io)  if you encountered any issues in building it on Windows?\r\n", "@vnvo2409 once we have modular filesystem support for windows, we will start replacing the current ones with plugins for GCS, S3, Hadoop. Plan is for these plugins to reside under [tensorflow/io](https://github.com/tensorflow/io) and to work from the start on Linux, Mac and Windows.\r\n\r\nNote that there are also two other students that are supposed to start working on filesystems as part of [ROSEdu](http://www.rosedu.org/en/) CDL (open source development course)", "I built tensorflow on windows with s3 successfully by removing the line `clean_dep(\"//tensorflow:windows\"): [],` in `tensorflow/core/platform/default/build_config.bzl`. However, when I tried to test, the exceptions 'file scheme is not implemented' are remains. Could someone please tell me how to remove the exceptions ? Thank you :smile:", "Interesting. It shouldn't use the `tensorflow/core/platform/default` paths, but the `tensorflow/core/platform/windows` ones if on Windows.", "I ask @yongtang  and he told me to remove this line https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/default/build_config.bzl#L656-L660 and It trigger the s3 builds on windows. Anyway, I think that It is built succesfully. However, the exceptions \"File system S3 is not implemented\" are remains. Do you @mihaimaruseac know where to remove the exceptions ?\r\nIn addition, I am considering to change my proposal idea for Google Summer Of Code into write the plugins S3/GCS for your modular filesystem, could you please tell me which filesystems those 2 students work on ?\r\nThank you \ud83d\ude04 \r\n\r\nP/s: In my opinion, the `tensorflow/core/platform/windows` is an implementation for tensorflow can work on windows ( Low level operation ) rather then the tensorflow build config for windows", "@vnvo2409 The `\"File system S3 is not implemented\" ` error might be caused by the fact that `s3://` schema is not registered. Beside building successfully the schema also need to be registered. I haven't touched this part of the code for quite sometime so thing might have changed. On a side note I think write plugins for modular file system C API would be better.", "@yongtang When exploring the source code, I think that writing plugins for modular file system would be easier if we already had S3/GCS supports on Windows ( The plugins and the support are somehow similar, aren't they ?) So I will try to register `s3://` schema, could you tell me more detail how to register. Even if it is not so useful, it will be a great opportunity for me to learn more about tensorflow source code and bazel :smile:", "In addition, Could anyone please tell me who will be the mentor for the GCS and S3 supports on Windows ? It would be great if I could contact the mentor and have some feedbacks for my proposal. Thank you", "@vnvo2409 The registration code is in:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/s3/s3_file_system.cc#L1039\r\n\r\nSo it should have been registered if the build is successful. You may need to trace more to see where the issue comes from.", "> In addition, Could anyone please tell me who will be the mentor for the GCS and S3 supports on Windows ? It would be great if I could contact the mentor and have some feedbacks for my proposal.\r\n\r\nI don't know who was the original mentor that proposed S3/GCS on Windows. You can contact organizer and send email to tensorflow-developers (google group) email list. If no mentor, I could\r\nsync up with organizer to see if I can be mentor. (Served as GSoC mentor last year but that was a different project).", "I think the issue may come from what @gunan say at the open of issues. I did dive into the configure.py but I found nothing\r\n> and the rest should be just removing the windows exceptions for GCS and S3 support in configure.py.\r\n\r\n\r\n", "Sorry for late update, I went away for a while.\r\n\r\nI will probably be the mentor for this project since I proposed the refactoring in tensorflow/community#101\r\n\r\nThe only lines with `clean_dep(\"//tensorflow:windows\") : []` are [these 3](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/platform/default/build_config.bzl;l=638,647,656;drc=5473ad460d7f1831fac4b3882a37422bbc5c2aa9) and indeed, removing each one of them makes the specific filesystem be compiled on Windows. So, I was wrong above when I said that `platform/default/build_config.bzl` doesn't need to be changed. Apologies.\r\n\r\nFilesystem not registered is because the file that contains this line has not been compiled on Windows (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/s3/s3_file_system.cc#L1039 as suggested by @yongtang ) Can you try removing [this windows deps](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/platform/default/build_config.bzl;l=656;drc=5473ad460d7f1831fac4b3882a37422bbc5c2aa9) and compiling again? You can also add a print [at this line](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/platform/env.cc;l=73;drc=8f597046dc30c14b5413813d02c0e0aed399c177) and print the `scheme` argument. That should tell you which filesystems are registered and in what order? Or, in a C++ code linking to TensorFlow, you can call [`Env::GetRegisteredFileSystemSchemes`](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/platform/env.cc;l=129;drc=8f597046dc30c14b5413813d02c0e0aed399c177). See an example [here](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/c/experimental/filesystem/modular_filesystem_test.cc;l=1685-1687;drc=23ff63d546800cd7143172bb201d6bb8d6fd15fd)\r\n\r\nRegarding the modular API, we are still in flux now working on the Windows one. Every modular implementation should be at [tensorflow/c/experimental/filesystem/](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/c/experimental/filesystem/) Please look through the interface defined there and the implementation of the POSIX filesystem.\r\n\r\nMy other two students haven't yet started implementation of the modular filesystems, so we can start here instead, since I adhere to the philosophy of \"whoever writes the code first gets the PR merged\"", "@mihaimaruseac So I will start implementing the plugins right away. Could I send you my [proposal](https://docs.google.com/document/d/1PXNCkskQYDOJjtfZHMLJXcORRrg4-2r0GQ2LAh0_LzE/edit) and get some feedbacks about it please ?\r\nThank you :smile: ", "I try `std::cout` the `scheme` argument and here is what I found\r\n![Screenshot 2020-03-22 at 09 47 53](https://user-images.githubusercontent.com/17511625/77241298-9bfda280-6c22-11ea-95dd-4e824d6451f0.png)\r\nIt seems that the `s3://` file schema has already been implemented and the exceptions are hard-coded into some file ? I have to put the `#` to make it into a comment to prevent conflict with auto-generated file.\r\nI will try the `Env::GetRegisteredFileSystemSchemes` and come back later", "I will look over the proposal.\r\n\r\nThe output seems to come from compiling the code. Question is what is the output when you run the program", "the output when I import tensorflow is the same.\r\n![image](https://user-images.githubusercontent.com/17511625/77242298-9d819780-6c2f-11ea-9d12-7201becf158c.png)\r\nHowever, the compile output of `Env::GetRegisteredFileSystemSchemes` does not give me `s3`. I will dive into it.\r\n\r\nEdit: Output when import tensorflow to python", "That's interesting. Can you try again by doing `python -vvv -c \"import tensorflow\" &> log` and then sharing the entire `log` file?", "I redirection all the outputs and errors so it is a huge file ( 6mb ~ 68k line )\r\nThe output is the same as the screenshot\r\n[python_log.txt](https://github.com/tensorflow/tensorflow/files/4364664/python_log.txt)\r\n", "I think it might be useful. I put a static variable in the function `FileSystemRegistryImpl::Register` to count how many time it is called. It turn out that it is called by more than one process (?)\r\n<img width=\"366\" alt=\"image\" src=\"https://user-images.githubusercontent.com/17511625/77252723-64294600-6c88-11ea-9752-27ac16fb6bf3.png\">\r\nthe first process call it 3 times with \"s3\", \"\" and \"file\" ( the last two are for windows ). However, from the second process ( when the static variable is 1 ), \"s3\" is not registered anymore and therefore the exceptions happen.\r\n", "Yes, I was thinking to tell you to print the address of `registry_`, both in `FileSystemRegistryImpl::Register` and `FileSystemRegistryImpl::GetRegisteredFileSystemSchemes`", "Another thing to try: locate the dll of the s3 filesystem and then try to import it from python, via [`tf.load_library`](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/python/framework/load_library.py;l=123;drc=a02fe6c24a5a38c24d0330120c8426aeea28db9f)", "ok I will try. Thank you", "@mihaimaruseac There is no dll of s3 filesystem, there is only a s3_file_system.lo.lib which can not be loaded into tensorflow. I think that the problems might be come from `genrule` (you can see the call for `Register` is `from executing genrule`, as it needs to generate file, it call `FileSystemRegistryImpl::Register` for Windows and we lost the `s3://`. I think I will try to hard-code the `REGISTER_FILE_SYSTEM(\"s3\", RetryingS3FileSystem);` into this file https://github.com/tensorflow/tensorflow/blob/f7b6793c6611210405d066dde84cc67adca4097c/tensorflow/core/platform/windows/env.cc#L193\r\n<img width=\"810\" alt=\"image\" src=\"https://user-images.githubusercontent.com/17511625/77258524-b3ce3880-6cad-11ea-9a9d-efd702f10141.png\">\r\n", "Yeah, every genrule is a different process which does another filesystem registration. So this makes sense so far.\r\n\r\nIt is interesting that there is no dll. I think first step would be to make the necessary build changes to create one. Probably we need a `tf_cc_shared_object` rule, like [I did for the modular windows plugin](tensorflow/c/experimental/filesystem/plugins/windows/BUILD)", "I can not figure out how to deal with the genrule. I can not build the dll too. However, there is a good news that @gunan said that he has compiled successfully gcs support on Windows https://groups.google.com/a/tensorflow.org/forum/#!topic/developers/wYlMfe3KIYk . I think the best thing I can do now is waiting for his reply about the exceptions. I will also change my proposal to the second approach as the first approach is nearly done. Could you review my proposal for me when it is done @mihaimaruseac ?\r\nThank you \ud83d\ude04 ", "Sounds good to me", "Certainly a lot happened on this thread. I will try to summarize my responses.\r\nIn TFs codebase, now curl builds on windows. I have resolved all issues a couple weeks ago.\r\nI finally also have tensorflow/core/platform/cloud:gcs_file_system_test passing, too.\r\n\r\nabout the changes to platform/default/build_config.bzl. You cannot simply remove the `tensorflow:windows\": []` I think more than filesystems depend on that. You will need to look deeper into what build targets use these macros.\r\n\r\nAbout the exceptions, as @yongtang mentioned, `File system S3 is not implemented` means S3 module is just not built. When the module is built into the binary, it registers the file name schema for s3, and the error goes away.", "I think there may be an ODR violation, file system registry may be linked twice. Because even when I explicitly built GCS support, I seem to be missing it.", "So you have the same problems with me ? @gunan ", "@mihaimaruseac I finished my proposal. Thank you for review it one more times :smile:\r\n", "I can not build `//tensorflow/core/platform/s3:s3_file_system_test` becasue of this error\r\n<img width=\"1680\" alt=\"image\" src=\"https://user-images.githubusercontent.com/17511625/77523288-1dba2e00-6eb8-11ea-8193-ebea8d64ff72.png\">\r\nI think the root of it is there are two file [1](https://github.com/tensorflow/tensorflow/blob/0298dd6900d4278e1a3f8c4f6a6956dd9e7ca7f0/tensorflow/core/lib/bfloat16/BUILD#L13) and [2](https://github.com/tensorflow/tensorflow/blob/0298dd6900d4278e1a3f8c4f6a6956dd9e7ca7f0/tensorflow/core/framework/BUILD#L584) which both contain the rule to build `bfloat16.dll`\r\nCould anyone help me ? Thank you", "Interesting. But, afaik, the two rules should generate two different files in two different paths.\r\n\r\nI need to set-up a Windows environment and check", "That is a known issue.\nDue to path length restrictions, bazel has these name clashes on windows\nwhen running bazel test.\nYou need to add \"--dynamic_mode=off\" flag to your bazel test commands on\nwindows to be able to tests\n\nOn Wed, Mar 25, 2020 at 9:30 AM Mihai Maruseac <notifications@github.com>\nwrote:\n\n> Interesting. But, afaik, the two rules should generate two different files\n> in two different paths.\n>\n> I need to set-up a Windows environment and check\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19297#issuecomment-603944278>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AB4UEOLTHFS7QXYWKS5FDUDRJIWT7ANCNFSM4E77X7UA>\n> .\n>\n", "I run the test and not a single test pass. However, there is a good news that the registry_ only linked one times and the `s3://` works. I think we have to edit the path to make it works on windows\r\n<img width=\"1089\" alt=\"image\" src=\"https://user-images.githubusercontent.com/17511625/77599885-d9bb3d80-6f38-11ea-8b4c-58961202626d.png\">\r\nEdit: the problem is when `io::ParseURI` parse `s3:///tmp/RandomAccessFile`, scheme is `s3`, bucket is empty ( because `/` is root) and object is `tmp/RandomAccessFile`. However the [function](https://github.com/tensorflow/tensorflow/blob/6429fed042a8462e6502548945bed9fc99f29d2f/tensorflow/core/platform/s3/s3_file_system.cc#L175) `ParseS3Path` does not allow bucket to be empty.", "Another question, could anyone show me how to set up a build trigger process ? I see that `tensorflow` using cloud build API by GCP but I am not sure how to set up for my own", "Good news, S3 has passed all unit test except `HasAtomicMove` which does not compile because `NeedTempsLocation` is not belong to `s3fs`. I have to set up my own bucket on S3 sever to test. Is there a way to mock test it ?\r\nAnyway, the only thing left to do is solving the exceptions \ud83d\ude80 \r\n<img width=\"809\" alt=\"image\" src=\"https://user-images.githubusercontent.com/17511625/77730733-f6886b80-7033-11ea-94a0-f9331d6def21.png\">\r\n\r\n", "That's great.\r\n\r\nWe would prefer to not mock the filesystem tests because that could result in divergent behavior between actual use and the mock.\r\n\r\nRegarding the missing method, that might be because when it was added recently no one also ran the S3 tests.", "Ok. I am working on the exception and I will update when I find something ?\r\nDo you have any suggetion where to dig in for me ? @mihaimaruseac ", "@vnvo2409 In tensorflow-io we use [localstack](https://github.com/localstack/localstack), which is a AWS stack emulator, to test AWS related ops. It has been quite useful to us. This might not be suitable for tensorflow as tensorflow's code base and test running time is already quite long. But once s3 is migrated to tensorflow-io we could reuse localstack to setup the test for s3.", "![image](https://user-images.githubusercontent.com/17511625/77824815-ec4f9580-7137-11ea-80b9-0268f34c7227.png)\r\nI see that the `registry_` used by `Env::GetFileSystemForFile` is the fifth, not the very last one as I thought ( I've already tried many times )\r\n@mihaimaruseac I think that I need help now because it seems very werid to me as I think that the next `registry_` should override the last one. I could not get any further debug information. Could you explain the order of linking `registry_` for me \r\nThank you.", "In theory it should be just one global variable. But because it is used across multiple shared objects, there is a possibility that one shared object gets a `registry_` initialized at a different address. The keyword here is ODR violation (one definition rule). We need to debug the build and find out which DLLs provide the alternate definition", "Good news every one. I have made `s3` worked on Windows. I have to add to this rule `deps`\r\n```\r\nselect({\r\n        \"//tensorflow:windows\": [\"//tensorflow/core/platform/s3:s3_file_system\"],\r\n        \"//conditions:default\": [],\r\n})\r\n```\r\nhttps://github.com/tensorflow/tensorflow/blob/907a55ebadce3c78bef73148613b327ed84d92e4/tensorflow/python/BUILD#L6070-L6081\r\n<img width=\"560\" alt=\"image\" src=\"https://user-images.githubusercontent.com/17511625/78277615-00800200-753f-11ea-9b19-0e740a73ae40.png\">\r\n\r\n@mihaimaruseac Now I will make one PR for `s3` and another ones for `gcs` or I should make two filesystems in one PR ?\r\n", "This addition works, but it has a downside. Now the filesystem is linked always on Windows, even for users who might not want to use S3.\r\n\r\nPreferably we should provide a `tf_cc_shared_object` rule to build the DLL of the S3 filesystem on Windows (and an `.so` on POSIX) and then in Python we should use `load_library` to load the plugin on demand.", "I build tensorflow on Linux and `s3` is also loaded when we `import tensorflow`. So I think this addition is more suitatble for the tensorflow source code right now. About the idea of building the DLL of the S3 filesystem, it is more suitable when we finish the implementation of filesystem modular. Because right now, in the source code of tensorflow, we always register a filesystem by a macro even for users who do not use s3.\r\nhttps://github.com/tensorflow/tensorflow/blob/ca6e96effcd3d3bcb9eec96e09832519b1acda70/tensorflow/core/platform/s3/s3_file_system.cc#L1039\r\nI think also find the cause of ODR violation, it is because on windows `tensorflow` use 2 folder `core/platform/default` and `core/platform/windows` instead of only `core/platform/default` like on Linux. To solve the ODR violation, I think we have to re-organize the 2 folders.\r\n\r\n@mihaimaruseac  I would like to ask you, what should I do, trying to solve the ODR violation on Windows or begining to work on the plugins for modular filesystem ?", "I think I finally found the root cause of ODR violation. The problem is about the different of bazel config between Windows and Linux. This line is default on Windows but it is not on Linux\r\nhttps://github.com/tensorflow/tensorflow/blob/5daf0e00e9913a0f7bd262f5f37ea70d34b00811/.bazelrc#L119\r\n\r\nOn linux, we have a shared object named `libtensorflow_framework.so.` which hold a common instance of `Env::Default()` and is imported into all shared objects that require a static instance of `Env::Default()`. So If there is a need of accessing to filesystem, it can use the common `Env::Default()` rather then creating a new instance.\r\n\r\nOn Windows, that line is considered into the bazel build config, which means it does not build `libtensorflow_framework.dll`. The `_pywrap_tensorflow_internal.so` is imported into these shared objects, but it does not export the symbols `Env::Default()` so these shared objects must create a new instance of `Env::Default()`.", "On Linux, when I build with `--config=monolithic`, the behavior is extractly the same on Windows. `s3` is not linked into tensorflow and there are several ODR violations, just like windows", "I think the plugin has to be loaded manually via `load_library`. At least on Linux the shared object can be built.", "@mihaimaruseac Could you tell me more about the current implementation of C API Modular Filesystem. So that I can have a plan working on the plugin and not to put too much efforts here", "That sounds good.\r\n\r\nThe entire implementation is under [`c/experimental/filesystem`](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/c/experimental/filesystem/). There is a [test suite](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/c/experimental/filesystem/modular_filesystem_test.cc), the [API interface plugins needs to be compiled against](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/c/experimental/filesystem/filesystem_interface.h) (with a lot of comments) and some other glue files.\r\n\r\nThe plugins are under [`c/experimental/filesystem/plugins`](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/c/experimental/filesystem/plugins/). POSIX is fully implemented as of now, Windows is just the starting stub, but expect more changes to come there in the coming weeks.\r\n\r\nPlease read at the least the [API](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/c/experimental/filesystem/filesystem_interface.h) (takes precedence over tensorflow/community#101) and let me know if there are questions", "Everything you send me require me a MOMA Single Sign On and I do not have the account \ud83d\ude22 ", "@vnvo2409 The files are located on the master branch of the tensorflow repo. You can replace name mentioned by @mihaimaruseac with the repo, e.g., :\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/c/experimental/filesystem", "thank you", "Oh, sorry, I was using the links from my work computer. You can also replace the prefix of the links from `osscs.corp.google.com`, to `cs.opensource.google`", "@mihaimaruseac Can I use `AWS sdk for C++` for `s3` plugin or I should use as much `C` as possible ? A small note, I have to delete 2 lines to able to test `libposix_filesystem.so` \r\nhttps://github.com/tensorflow/tensorflow/blob/3364a141f2a7dc151154652e2e231516d6ee43cc/tensorflow/core/platform/default/env.cc#L215-L216", "Apologies for the delay in answering.\r\n\r\nRegarding the deletion of the two lines: yes, that is expected. Otherwise the filesystem in use is the old one. See #37734 for the latest updates w.r.t. testing. I should actually create a guide for how to test these but was planning on doing that after Windows support was ready.\r\n\r\nRegarding language usage: the plugin could use C++ and libraries but the interface must be valid C.", "@mihaimaruseac I have 2 more questions\r\n\r\n1. Could I reuse some part of tensorflow `core` or not ? I see the current implementation of s3 filesystem is heavily depended on the lib in `tensorflow/core/`. I think it is not fit to the principle of Modular Tensorflow. An example of the lib which is heavily used `tensorflow/core/platform/logging.h` and `tensorflow/core/platform/mutex.h`\r\n\r\n2. Three component `Aws::S3::S3Client`, `Aws::Transfer::TransferManager`, `Aws::Utils::Threading::PooledThreadExecutor` should be shared between data structures. I have something like\r\n```cpp\r\nnamespace tf_random_access_file {\r\n\r\ntypedef struct S3File {\r\n\tconst char* bucket_;\r\n\tconst char* object_;\r\n\tstd::shared_ptr<Aws::S3::S3Client> s3_client_;\r\n};\r\n}\r\n```\r\nA simple solution is\r\n```cpp\r\n// s3_helper.h\r\nnamespace tf_s3_filesystem {\r\n  std::shared_ptr<Aws::S3::S3Client> GetS3Client();\r\n  std::shared_ptr<Aws::Transfer::TransferManager> GetTransferManager();\r\n  std::shared_ptr<Aws::Utils::Threading::PooledThreadExecutor> GetExecutor();\r\n}  // namespace tf_s3_filesystem\r\n\r\n// s3_helper.cc  \r\n// use anonymous namespace to hide these pointers\r\nnamespace tf_s3_filesystem {\r\n  namespace {\r\n    std::shared_ptr<Aws::S3::S3Client> s3_client_;\r\n    std::shared_ptr<Aws::Transfer::TransferManager> transfer_manager_;\r\n    std::shared_ptr<Aws::Utils::Threading::PooledThreadExecutor> thread_executor_;\r\n  }\r\n\r\n  std::shared_ptr<Aws::S3::S3Client> GetS3Client() {\r\n    if(s3_client_.get() == nullptr ) {\r\n      //...\r\n      s3_client_ = std::shared_ptr<Aws::S3::S3Client>(...);\r\n    }\r\n    return s3_client_;\r\n  }\r\n}  // namespace tf_s3_filesystem\r\n\r\n// s3_filesystem.cc\r\n#include \"s3_helper.h\"\r\nnamespace tf_s3_filesystem {\r\nstatic void NewRandomAccessFile(const TF_Filesystem* filesystem,\r\n                                const char* path, TF_RandomAccessFile* file,\r\n                                TF_Status* status) {\r\n  // ...\r\n  file->plugin_file = new tf_random_access_file::S3File({bucket, object, GetS3Client()});\r\n  TF_SetStatus(status, TF_OK, \"\");\r\n}\r\n\r\n```\r\nIs this solution good enough ?\r\n", "Regarding 1: preferably not because then the plugin and the core of TensorFlow will still be entwined to the same compiler version and compile flags, breaking any guarantee that we want to achieve with the modularization. Instead, we should identify what are the common things and expose those to an interface, _if_ there are multiple users.\r\n\r\nRegarding 2: We would prefer to not initialize static global variables which are not trivially destructible. Do we need `shared_ptr` on the types? Keep in mind that the opaque structs in the API can be used to hide plugin specific implementation on the plugin side", "I think we need something to keep a single instance of variable of type `S3Client` to use when s3 plugins is loaeded because allocating a new `S3Client` each time we need would be a waste of resource. ", "Agree. But the plugin is only loaded a single time, it cannot be unloaded", "So I suggest using a `std::weak_ptr`\r\nSomething like\r\n```cpp\r\n// s3_filesystem.cc\r\nnamespace tf_random_access_file {\r\n\r\ntypedef struct S3File {\r\n\tconst char* bucket_;\r\n\tconst char* object_;\r\n\tstd::shared_ptr<Aws::S3::S3Client> s3_client_;\r\n};\r\n}\r\n\r\n// s3_helper.h\r\nnamespace tf_s3_filesystem {\r\n  std::shared_ptr<Aws::S3::S3Client> GetS3Client();\r\n  std::shared_ptr<Aws::Transfer::TransferManager> GetTransferManager();\r\n  std::shared_ptr<Aws::Utils::Threading::PooledThreadExecutor> GetExecutor();\r\n}  // namespace tf_s3_filesystem\r\n\r\n// s3_helper.cc  \r\n// use anonymous namespace to hide these pointers\r\nnamespace tf_s3_filesystem {\r\n  namespace {\r\n    std::weak_ptr<Aws::S3::S3Client> s3_client_;\r\n    std::weak_ptr<Aws::Transfer::TransferManager> transfer_manager_;\r\n    std::weak_ptr<Aws::Utils::Threading::PooledThreadExecutor> thread_executor_;\r\n  }\r\n\r\n  std::shared_ptr<Aws::S3::S3Client> GetS3Client() {\r\n    if(s3_client_.expired()) {\r\n      //...\r\n      s3_client_ = std::make_shared<Aws::S3::S3Client>(...);\r\n    }\r\n    return s3_client_.lock();\r\n  }\r\n}  // namespace tf_s3_filesystem\r\n```\r\nThe global static variables will be referenced by a `weak_ptr<Aws::S3::S3Client>`. If something need to use it, we could return a `shared_ptr<Aws::S3::S3Client>`. When it is no longer use anymore, It will be destructible right at that moment, and we don't have to keep it throughout the program as the plugin can not be unloaded. ", "This looks like something that should be done in the function passed to [`init`](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/c/experimental/filesystem/filesystem_interface.h;l=286-289;drc=12d516b9cd7a7954fa79cccfe645564837cc4d23).\r\n\r\nIn the plugin, you can have a private struct that keeps these pointers (naked pointer should be ok, I don't see a reason to have them be `std::weak_ptr` or other wrappers at the moment) and then you [attach that struct to the opaque `TF_Filesystem`](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/c/experimental/filesystem/filesystem_interface.h;l=77-79;drc=12d516b9cd7a7954fa79cccfe645564837cc4d23).\r\n\r\nFor example, [POSIX `init`](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/c/experimental/filesystem/plugins/posix/posix_filesystem.cc;l=191-193;drc=ac271534b897ea2bb78a865971823f814b0151e1) does nothing, but you could have something like\r\n\r\n```cpp\r\nnamespace tf_s3_filesystem {\r\n\r\ntypedef struct S3Data {\r\n    Aws::S3::S3Client *s3_client;\r\n} S3Data;\r\n\r\nstatic void Init(TF_Filesystem* filesystem, TF_Status* status) {\r\n  auto s3data = static_cast<S3Data*>(filesystem->plugin_filesystem);\r\n  s3data->s3_client = ...;\r\n  TF_SetStatus(status, TF_OK, \"\");\r\n}\r\n\r\n}  // namespace tf_s3_filesystem\r\n```\r\n\r\nThen, when you create a file:\r\n\r\n```cpp\r\n\r\nnamespace tf_s3_random_access_file {\r\n\r\ntypedef struct S3File {\r\n\tconst char* bucket;\r\n\tconst char* object;\r\n\tconst Aws::S3::S3Client *s3_client; // not owned\r\n};\r\n}  // namespace tf_s3_random_access_file\r\n\r\nnamespace tf_s3_filesystem {\r\n\r\nstatic void NewWritableFile(const TF_Filesystem* filesystem, const char* path,\r\n                            TF_WritableFile* file, TF_Status* status) {\r\n  auto s3data = static_cast<S3Data*>(filesystem->plugin_filesystem);\r\n  ...\r\n  file->plugin_file = new tf_random_access_file::S3File({bucket, object, s3data->s3_client})\r\n  TF_SetStatus(status, TF_OK, \"\");\r\n}\r\n\r\n}  // namespace tf_s3_filesystem\r\n```\r\n\r\nThis way you don't need globals.", "Thank you. I have misunderstood the usage of `filesystem->plugin_filesystem` as I do not see it anywhere in the implementation of `posix_filesystem`.\r\n\r\n1. The problems with naked pointer is https://sdk.amazonaws.com/cpp/api/LATEST/class_aws_1_1_transfer_1_1_transfer_manager.html#a0a035f323a5772cc769e651d1a1325bf\r\nWhen we call `Aws::Transfer::TransferManager::Create`, it return a `shared_ptr<Aws::Transfer::TransferManager>`. We can get a raw pointer by calling `get()` but the `shared_ptr` is still the owner of memory and `Aws::Transfer::TransferManager` will be deleted when this `shared_ptr` is deleted. I think we should stick with smart pointer to ensure the compatibility with AWS Sdk.\r\n\r\n2. I think Initializing `S3Client` in `Init` is not a good idea. Because we may not use all 3 shared components. I will do something like:\r\n```cpp\r\ntypedef S3Shared {\r\n  std::shared_ptr<Aws::S3::S3Client> s3_client;\r\n  ...\r\n} S3Shared;\r\n\r\nvoid GetS3Client(S3Shared* s3_shared) {\r\n  if(s3_shared->s3_client.get() == nullptr) {\r\n    s3_shared->s3_client = ...\r\n  }\r\n  return;\r\n}\r\nvoid GetTransferManager(S3Shared* s3_shared);\r\nvoid GetExecutor(S3Shared* s3_shared);\r\n```\r\nThis will save us a lot of resources. Take an example,`S3RandomAccessFile` does not need `Aws::Transfer::TransferManager` and `Aws::Utils::Threading::PooledThreadExecutor` at all.\r\n\r\nAnd when I create a file:\r\n```cpp\r\nnamespace tf_s3_random_access_file {\r\n\r\ntypedef struct S3File {\r\n\tconst char* bucket;\r\n\tconst char* object;\r\n\tconst std::shared_ptr<Aws::S3::S3Client>& s3_client; // not owned\r\n};\r\n}  // namespace tf_s3_random_access_file\r\n\r\nnamespace tf_s3_filesystem {\r\n\r\nstatic void NewRandomAccessFile(const TF_Filesystem* filesystem, const char* path,\r\n                            TF_WritableFile* file, TF_Status* status) {\r\n  auto s3shared = static_cast<S3Shared*>(filesystem->plugin_filesystem);\r\n  ...\r\n  GetS3Client(s3shared);\r\n  file->plugin_file = new tf_random_access_file::S3File({bucket, object, s3shared->s3_client});\r\n  TF_SetStatus(status, TF_OK, \"\");\r\n}\r\n\r\n}  // namespace tf_s3_filesystem\r\n```", "Regarding the pointers, yes, if the API already uses wrapped ones then we should incorporate them too. I was not checking the AWS API.\r\n\r\nRegarding the second point: that's great, I like the lazy loading approach you suggested.", "Here is what I have done with the s3 plugins\r\n\r\n1. I have created a `s3` filesystem plugins based on the current implementation of `s3` filesystem.\r\nHere is the test log. [attempt_1.log](https://github.com/tensorflow/tensorflow/files/4503596/attempt_1.log). As you can see. **94/103** test passed. [Here](http://bucketvnvo.s3.amazonaws.com) is the link to my test bucket on s3. I have removed some test: \r\n- `TestTranslateName` because the default `TranslateName` remove the bucket name ( `s3://bucket/tmp/test` -> `/tmp/test`) So I have to write my own `TranslateName`.\r\n- Any test related to `MemoryRegion`. `Seg fault` so I will work on it later.\r\n- `TestStatPathIsInvalid` I use `Stat` to check if path is invaild or not so this test can not run.\r\n\r\n2. Test time is very long. It is because most of the tests are not compatible with how `s3` filesystem work and they just add the complexity to the operation of `s3` plugins filesystem. `s3` does not have the `directory` concept. `s3` has a flat structure instead of a hierarchy like in a normal filesystem. Take an example, with only one bucket `bucketvnvo` without any \"directory\"\r\n```bash\r\naws s3api put-object --bucket bucketvnvo --key test/vnvo/file\r\naws s3api put-object --bucket bucketvnvo --key test/vnvo/file/dir\r\n```\r\nThese 2 command create 2 objects which have 2 keys `test/vnvo/file` and `test/vnvo/file/dir`. It does not care if the \"parent directory\" is exist or point to a file or something as there is no \"parent directory\". So `TestCreateFileNonExisting`, `TestCreateFileExistingDir`, `TestCreateFilePathIsInvalid`, etc are not so useful. I will remove some `path pre-checker` in the `s3` plugins filesystem and I think we should rewrite the modular filesystem test for `s3` \r\n\r\nA side note, I have some exams so I will be less active till the student announcement (May 5) of the Google Summer of Code.\r\n", "Since S3 does not support directories those operations should return the not implemented status. Then tests would be marked as passing and skipped fast.\r\n\r\nThe goal for the tests is to provide a checker that all plugins adhere to the specification of the API in the modular interface.\r\n\r\nDo you have a PR with the code so we can look through that?\r\n\r\nNo worries about the delay, good luck on the exams", "@yongtang I see you have fixed the `gcs_filesystem` linking problems on Windows by this PR https://github.com/tensorflow/tensorflow/pull/38959. Do you think I should make a PR for `s3_filesystem` on Windows as well ( it has already passed all tests ) and we can close this issue.", "That sounds good, @vnvo2409. Please assign/mention me in the PR.", "@mihaimaruseac I have made a PR, please review it.\r\nhttps://github.com/tensorflow/tensorflow/pull/39124", "Closed by #39124 "]}, {"number": 19296, "title": "tf.gfile.Open has different semantics from open", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.13\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\n\r\n`tf.gfile.Open` in `\"w\"` mode delays the creation of the file until the first write. This is different from the buitin [`open`](https://docs.python.org/3/library/functions.html#open), which creates a file right away. I am not sure if this is a bug, or the intended behaviour, but I think should be clarified in the documention of `tf.gfile.Open`.\r\n\r\n### Source code / logs\r\n\r\n```python\r\n>>> import tensorflow as tf\r\n>>> tf.gfile.Open(\"hdfs://root/tmp/user/test\", \"wb\").close()\r\n>>> tf.gfile.Exists(\"hdfs://root/tmp/user/test\")\r\n# ...\r\nFalse\r\n>>> tf.gfile.Open(\"/tmp/test\", \"wb\").close()\r\n>>> tf.gfile.Exists(\"/tmp/test\")\r\nFalse\r\n```", "comments": ["Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 31 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 46 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@superbobry, I believe this is expected behavior. We'd definitely appreciate a documentation improvement PR. The general story is that gfile can't and doesn't try to match semantics as it is an abstraction layer, so file systems vary on different platforms.", "@martinwicke, is there anybody who can provide better insight on gfile?", "@aselle just to be clear, the issue here is not the platform-specific differences (both HDFS and local FS behave consistently). The behaviour of GFile is different from Python (on all platforms, I assume).", "@superbobry This is (at this point) intended behavior, as it would be infeasible to change it. I agree with you though that this should be clarified in the documentation. Would you like to send a PR for this? Thanks!"]}, {"number": 19295, "title": "[FR] Add warm_start_from in model_to_estimator", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Docker nightly\r\n- **TensorFlow installed from (source or binary)**: Docker nightly\r\n- **TensorFlow version (use command below)**: Docker nightly\r\n- **Python version**: Docker nightly\r\n- **Bazel version (if compiling from source)**: Docker nightly\r\n- **GCC/Compiler version (if compiling from source)**: Docker nightly\r\n- **CUDA/cuDNN version**: Docker nightly\r\n- **GPU model and memory**:  N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nPlease add `warm_start_from` parameter in `tf.model_to_estimator`\r\n\r\n### Source code / logs\r\nN/A", "comments": ["@fchollet what are your thoughts on this?", "We have a duplicate at https://github.com/tensorflow/tensorflow/issues/20057", "@ispirmustafa adding you as well to this feature request if you have any take on this.", "See also https://github.com/tensorflow/tensorflow/issues/17950#issuecomment-404707663", "What's the use case here? (given that user created the model from keras, no checkpoint exists)", "@tanzhenyu One classical use case is to bootstrap a new experiment from another one created from keras and converted to estimator. That one have generated checkpoints.", "@tanzhenyu Similar use case to above is to train an estimator from a keras model, then later reload it to do predictions, i.e. `.predict`. \r\n\r\nI believe this is meant to be possible but does not seem to work correctly if you don't do the `.predict` immediately after training. I think that loading through warm start might make this possible. For more info on this use case see my issue #20057 (closed as duplicate to this one) and the stack overflow question it refers to - https://stackoverflow.com/questions/50855256/keras-estimator-model-to-estimator-cannot-warm-start-or-load-previous-checkpoi", "There is also a related problem on this API on how to handle models with BN. See https://github.com/tensorflow/tensorflow/issues/19903", "Hi,\r\nI don't see anything on tf.1.10 or master. Is there any plan to add the warm_start_from parameter?\r\n\r\nThanks.", "@tanzhenyu -- my understanding is that currently, we warm start from the keras model itself, is that correct? \r\n\r\n@bhack -- can you clarify how you imagine warm starting working in the context of model_to_estimator? The graph comes from the keras model, and the weights from another directory? Can you instead load weights into the Keras model, as they will travel through the warm estimator path in the current code?", "@karmel I.e. one user case:\r\n\r\n- Design a tf.keras model\r\n- Convert that to estimator\r\n- `train_and_evaluate` with automarted model saving from scratch\r\n- Than you want to bootstrap again a different fork of the model with quite the same code from some checkpoint of the previous one.", "In our case, we are leveraging tf.dataset for pipelining. So it's easier to train with estimator even if some users are more confortable with keras than estimator model_fn... But then you end up with tensorflow checkpoints.\r\nAnd yes, right now the workaround is to warm start your keras model before converting to an estimator. But it's possible to do it since tf.1.10 before Keras used to support only h5 file", "Our situation is \r\n\r\n- keras is our choice for building a model\r\n- estimator is our choice to train and evaluate the model\r\n- at some point in the future we wish to further train our estimator model with new data, or use the model in other ways, without having to retrain from scratch.\r\n\r\nProblem is that if we want to further train a model we cannot load up an estimator checkpoint. We would have to go back and start from scratch with an untrained model every time we wanted to use it. Hence wanting to warm start an estimator that has been converted from a keras model.\r\n\r\nIs @Silb78dg saying we can now at least warm up the keras model from a estimator checkpoint? That might help as we would then only have to reconvert to estimator rather than do previous training all over again. Would still prefer to warm start the estimator directly and not have to reconvert though.", "There are two options here:\r\n\r\n1. Warm start the keras model by calling [model.load_weights](https://www.tensorflow.org/api_docs/python/tf/keras/Model#load_weights) -- this will now work with hdf5 or checkpoints. Then, model_to_estimator. For cases where you want to reuse a keras architecture (which is what I think you are saying, @bhack ), you can wrap the model building in a function to build a new model with the same architecture for each new set of weights.\r\n\r\n2. Build the Keras model in an Estimator's model_fn, and use the outputs of the keras model to feed into an Estimator Head or an EstimatorSpec directly. An example of this strategy is the [Official MNIST](https://github.com/tensorflow/models/blob/master/official/mnist/mnist.py#L102), and another with Head briefly here:\r\n\r\n```\r\ndef model_fn(features, labels, mode):\r\n  model = tf.keras.Sequential([\r\n      tf.keras.layers.Dense(64),\r\n      tf.keras.layers.Dense(40, activation=tf.nn.softmax)\r\n  ])\r\n  \r\n  logits = model(features)\r\n  return tf.contrib.estimator.multi_class_head(...).create_estimator_spec(\r\n      mode=mode, features=features, labels=labels, logits=logits, optimizer=tf.train.AdamOptimizer())\r\n\r\n```\r\nUsing that model_fn in an Estimator will then allow you to use the Estimator API as-is with warm start settings.\r\n\r\nWill either of those work for the use cases above? If yes, and you're willing to share a little more-- which do you prefer, and why?", "@bobbyAtSperry we are on the situation. And yes I confirm that \r\n```\r\nmodel = KerasLoader.load_model_from_json(json_file)\r\nif warm_start_from is not None:\r\n  model.load_weights(warm_start_from)\r\nmodel.compile(loss=loss, optimizer=optimizer, metrics=metrics)\r\nreturn tf.keras.estimator.model_to_estimator(keras_model=model, config=run_config)\r\n```\r\nworks perfectly fine for me.\r\n\r\n@karmel I don't have any preferences because I'm not the one who write the model and I need to manage both cases (Keras model and model_fn). That's why I was looking for the warm_start parameter but after all this will not simplify my code.", "/cc @lenlen Any comment on this interface?\r\n", "@Silb78dg, @karmel, I'll try out your suggestions and report back. They look like they could provide what I need. May not be until next month when we get back onto developing again. ", "In the warm start case the option 1 allows us to avoid the model_fn implementation, but in option 2, as for summary, we need to implement it. For the option 2 read my comment https://github.com/tensorflow/tensorflow/issues/21983#issuecomment-418826230.", "@lenlen Will option 1 work for you?", "It should work, but we could be penalized by the lack of functionalities of `WarmStartSettings` as for example vars_to_warm_start to execute partial warm start. In the option 1 you need to replicate some these functionalities on your own.", "@tanzhenyu I've tried the option 1 with the nightly. I used  `model.load_weights`  with my trained model to use the saved tensorflow checkpoint, but I got the error\r\n\r\n```\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/checkpointable/util.py\", line 888, in streaming_restore\r\n    \"Streaming restore not supported from name-based checkpoints. File a \"\r\n```\r\nI think that `model.load_weights` only works if the checkpoint is saved with only weights and not for the whole model.\r\nTo  simulate the warm start I have to create the estimator from the old model, `model.save_weights` from it, then create a new model and `model.loads_weights` for weights saved before.\r\nIs there an option to load weights from a saved tensorflow checkpoints, without the double model instantiation?\r\n \r\n", "@lenlen Seems that it is supported only for object based checkpoints. So it is currently a limit cause you need to work in Eager mode for training. ", "@tanzhenyu Is it correct that option 1 could be used as \"a workaround\" only in Eager mode?", "@Silb78dg, @karmel, I can confirm that we have successfully implemented @karmel's option 1. So rather than warm starting we just save and load to checkpoints as we previously did but can now use the keras syntax to design our models which become standard TF models in the model_fn in the estimator.\r\n\r\nThe following is a schematic of how we use the various TF components to train_and_validate and to predict using keras, estimators and tf.data. This may be a useful outline for others:\r\n\r\nI specify the model with keras notation but then put it in a tensorflow model function which can then be loaded into and estimator. By taking this approach I can save to, and reload from, checkpoints as with any other tensorflow model. I think this gives the best combination of using the intuitive keras notation, while being able to take advantage of the tensorflow estimators and data tools. The following is an overview of my approach describing the setup of the various tensorflow calls:\r\n\r\n1.  Create an estimator:\r\n\r\n```\r\n|--estimator: tf.estimator.Estimator\r\n|--config: tf.estimator.RunConfig                         #checkpointPath and saving spec for training\r\n        |--model_fn: tf.estimator.EstimatorSpec\r\n            |--myKerasModel                               #specify model. Doesn't have to be keras.\r\n                    |--keras.models.Model\r\n            |--loss: myLossFunction                       #train_and_eval only\r\n            |--optimizer: myOptimizerFunction             #train_and_eval only\r\n            |--training_hooks:tf.train.SummarySaverHook   #train_and_eval only - for saved diagnostics\r\n            |--evaluation_hooks:tf.train.SummarySaverHook #train_and_eval only - for saved diagnostics\r\n            |--predictions: model(data, training=False)   #predict only\r\n```\r\n2.  Train_and_evaluate:\r\n```\r\n|--tf.estimator.train_and_evaluate\r\n        |--estimator: tf.estimator.Estimator                  #the estimator we created\r\n        |--train_spec: tf.estimator.TrainSpec\r\n                            |--input_fn: tf.data.Dataset      #specify the input function \r\n                                |--features, labels           #data to use\r\n                                |--batch, shuffle, num_epochs #controls for data\r\n        |--eval_spec: tf.estimator.EvalSpec\r\n                            |--input_fn: tf.data.Dataset      #specify the input function\r\n                                |--features, labels           #data to use\r\n                                |--batch                      #controls for data\r\n                            |--throttle and start_delay       #specify when to start evaluation\r\n```\r\nNote that we could use `tf.estimator.Estimator.train` and `tf.estimator.Estimator.evaluate` but that doesn't allow doing evaluations during the training so we use `tf.estimator.train_and_evaluate` instead.\r\n\r\n3.  Predict:\r\n```\r\n|--estimator: tf.estimator.Estimator.predict       #the estimator we created\r\n        |--input_fn: tf.data.Dataset               #specify the input function\r\n            |--features                            #data to use\r\n```", "I'm marking this as contribution welcome."]}, {"number": 19294, "title": "Add shape validation in shape function of MapAndBatchDataset", "body": "In MapAndBatchDataset, batch_size, num_parallel_batches, and drop_remainder are 0-D scalars. This fix adds the shape check to those Inputs.\r\n\r\nNote since the Input of `other_arguments` is a list and is before `batch_size`, this PR uses the index from the end of the inputs to retrieve the shape, without guessing the number of `other_arguments`.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}]