[{"number": 18556, "title": "The latest master branch fails to run the model on tensorflow lite", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:  source\r\n- **TensorFlow version (use command below)**: \r\n- **Python version**:  2.7.12\r\n- **Bazel version (if compiling from source)**:  0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI cloned the latest master branch code (commit id 63c6562df68ade3a03481874a71b536a4e02b6f5) in order to use the setNumThreads method on Android, built a tensorflow lite aar, but could not run my model. This model can be run in previous versions (including r1.8.0). How can I solve it?\r\n\r\n### Source code / logs\r\nno logs", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Do you have more details so we can help debug this?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 30 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 18555, "title": "[Feature Request] Inverse Functions: Auto-Solve similar to Auto-Grad?", "body": "Would it be possible to implement some type of automatic equation solving?\r\nE.g. f(x, y) = z => tf.solve(y) = f'(x, z)\r\n\r\nFunctions like tf.sigmoid have known inverse functions which could be used to solve functions if all other parameters are known.\r\n\r\nI'm thinking of something like simplified SymPy solvers:\r\nhttp://docs.sympy.org/latest/modules/solvers/solvers.html\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: None\r\n- **TensorFlow installed from (source or binary)**: None\r\n- **TensorFlow version (use command below)**: 1.7\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: None\r\n- **GCC/Compiler version (if compiling from source)**: None\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**: None\r\n\r\n", "comments": ["Nagging Assignee @shivaniag: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Apologies for late response, this seems like a reasonable request. Assigning it to @girving , do you have any comments on this?", "I don't work at Google anymore, so I'm out of normal bug fix rotation.  However, I also don't think this issue is a good fit for TensorFlow.  It might make sense in a separate library, but fundamentally nearly all graphs won't have closed form inverses, so one usually wants something iterative.", "Well, I wrote some reparametrisations for statistical distributions.\r\nIn theory, such an auto-solver could be used to automatically optimize conversions between different parametrization types.\r\nHowever, I simply had not the time and desire to add an additional library and figure out how to use it, only to tweak some calculations automatically."]}, {"number": 18554, "title": "importing tensorflow", "body": "Hello, \r\nI tried to install tensorflow using conda, but whenever I try to import it, I get this error:\r\n\r\n```\r\nImportError                               Traceback (most recent call last)\r\nC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     17         try:\r\n---> 18             return importlib.import_module(mname)\r\n     19         except ImportError:\r\n\r\nC:\\Anaconda\\lib\\importlib\\__init__.py in import_module(name, package)\r\n    125             level += 1\r\n--> 126     return _bootstrap._gcd_import(name[level:], package, level)\r\n    127 \r\n\r\nC:\\Anaconda\\lib\\importlib\\_bootstrap.py in _gcd_import(name, package, level)\r\n\r\nC:\\Anaconda\\lib\\importlib\\_bootstrap.py in _find_and_load(name, import_)\r\n\r\nC:\\Anaconda\\lib\\importlib\\_bootstrap.py in _find_and_load_unlocked(name, import_)\r\n\r\nC:\\Anaconda\\lib\\importlib\\_bootstrap.py in _load_unlocked(spec)\r\n\r\nC:\\Anaconda\\lib\\importlib\\_bootstrap.py in module_from_spec(spec)\r\n\r\nC:\\Anaconda\\lib\\importlib\\_bootstrap_external.py in create_module(self, spec)\r\n\r\nC:\\Anaconda\\lib\\importlib\\_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)\r\n\r\nImportError: DLL load failed: Une routine d\u2019initialisation d\u2019une biblioth\u00e8que de liens dynamiques (DLL) a \u00e9chou\u00e9.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nModuleNotFoundError                       Traceback (most recent call last)\r\nC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\nC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>()\r\n     20             return importlib.import_module('_pywrap_tensorflow_internal')\r\n---> 21     _pywrap_tensorflow_internal = swig_import_helper()\r\n     22     del swig_import_helper\r\n\r\nC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     19         except ImportError:\r\n---> 20             return importlib.import_module('_pywrap_tensorflow_internal')\r\n     21     _pywrap_tensorflow_internal = swig_import_helper()\r\n\r\nC:\\Anaconda\\lib\\importlib\\__init__.py in import_module(name, package)\r\n    125             level += 1\r\n--> 126     return _bootstrap._gcd_import(name[level:], package, level)\r\n    127 \r\n\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-3-90de3482487c> in <module>()\r\n      1 \r\n      2 \r\n----> 3 import tensorflow as tf\r\n      4 from skimage import transform\r\n      5 from skimage import data\r\n\r\nC:\\Anaconda\\lib\\site-packages\\tensorflow\\__init__.py in <module>()\r\n     22 \r\n     23 # pylint: disable=wildcard-import\r\n---> 24 from tensorflow.python import *\r\n     25 # pylint: enable=wildcard-import\r\n     26 \r\n\r\nC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>()\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 # Protocol buffers\r\n\r\nC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Anaconda\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: Une routine d\u2019initialisation d\u2019une biblioth\u00e8que de liens dynamiques (DLL) a \u00e9chou\u00e9.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Anaconda\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\n\r\n```\r\n\r\nWhenever I search for a solution, everyone recommend installing VCREDIST 2015, except I already have it.\r\n\r\nThank you for your time", "comments": ["Got the same issue, installed with anaconda, double-checked all my pathes, nothing helps!\r\n![2018-04-15_215717](https://user-images.githubusercontent.com/16338045/38817523-d83088b4-4198-11e8-847f-4ac77a50ca77.jpg)\r\n", "Ok, I was able to solve the problem, at least on my Windows 10:\r\nThis video finally helped me: https://www.youtube.com/watch?v=uIm3DMprk7M\r\n\r\nTensorflow 1.5.0 must be installed, to work with CUDA 9.0, you need to do this manually because almost every tutorial comes with commands that install the most recent version (1.7.0 for the moment).\r\n\r\nHope this helps!", "I'm not trying to install the GPU version, but the CPU version. \r\nThank you anyway !", "There was a stackoverflow post that says that TensorFlow 1.7 uses AVX for the CPU version which is causing the above issue on Windows(Same behaviour observed for 1.6 as well). The above case occurs even when the Visual C++ build tools are installed. Verified this on two desktops running Windows 10(They run the same processor). Also to add, Intel Virtualization is already enable in the BIOS.\r\n\r\nVersions <=1.5 works well without any issues.\r\n\r\nHappy to provide more detials.", "This is a duplicate of https://github.com/tensorflow/tensorflow/issues/17386. Please look there for discussion + assistance.", "Hi guys solved the above  importing tensor flow error  suing the CPU after i downloaded and install\r\nMicrosoft Visual C++ 2015-2019 Redistributable (x64) \r\nVC_redist.x64 version on my laptop.. hope this help someone out there\r\ni had initially installed Anaconda 3.7 python version, then i now using the conda command prompt\r\n installed the tensor flow successfully from Anaconda and it worked. But on importing tensorflow i ran into the above issues.\r\ni even installed msv140dll still did not work until i downloaded and run\r\nVC_redist.x64 version  everything worked after then \r\ncheers"]}, {"number": 18553, "title": "libtensorflow_cc.so on Android", "body": "Hello again team TF!\r\n\r\nAfter revisions, I made a more stable solution for building libtensorflow_cc.so on android that should avoid breaking common build scenarios. There may be issues if attempting to build for non-android armeabi-v7a or arm64-v8a, but I'm unable to confirm.\r\n\r\nI found issues with android NDK r15 and bazel 0.11.0, forcing an update to NDK r16 and bazel 0.12.0 to ensure a working android build. As part of this, I also made a switch to the new libc++ version of STL in android for the tensorflow build, which does not suffer from the near proximity to GPLv3 with gnustl_static/shared.\r\nI am not certain, but it seems like the default --config=android chain still uses gnustl libraries, which are not fully C++11-compatible and due to be deprecated in the android NDK.\r\n\r\nA terrifying build command is still being used:\r\nbazel build -c opt --config=monolithic --android_crosstool_top=@androidndk//:toolchain-libcpp --crosstool_top=@androidndk//:toolchain-libcpp --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=$ARCH //tensorflow:libtensorflow_cc.so --cxxopt=\"-std=c++11\" --copt=\"-DMDB_USE_ROBUST=0\" --cxxopt=\"-DTENSORFLOW_DISABLE_META\" --cxxopt=\"-DEIGEN_HAS_C99_MATH\" --cxxopt=\"-Wno-c++11-narrowing\" --copt=\"-DS_IREAD=00400\" --copt=\"-DS_IWRITE=00200\" --verbose_failures --copt=\"-DABSL_BASE_POLICY_CHECKS_H_\" --copt=\"-DPNG_ARM_NEON_OPT=0\"\r\n\r\nMost of these are to avoid build issues within libraries that do not have specific checks in place for android toolchains. I presume the build command could be shortened by introducing the commands into the android_cc flag and the specific libraries in question, but it's out of scope for my case right now. \r\nSome of these flags may also have been resolved by using @androidndk//:toolchain-libcpp, especially the need for TENSORFLOW_DISABLE_META, EIGEN_HAS_C99_MATH and MDB_USE_ROBUST, and DS_*, but I did not have time to test all variants.\r\n\r\nTo enable the C++ API, I created a new config, android_cc, which is checked for in all applicable places. The - beyond all others - most common fault is -lpthread being set in various files and libraries. This may be possible to resolve by instead using \"-pthread\", which is reportedly a flag for specifying pthread inclusion in a platform-compatible manner.\r\n\r\nFurther, I had to add build files for gemmlowp, protobuf_archive and boringssl, to correct similar flag issues. Ideally, these should be resolved in the respective packages.\r\n\r\nA remaining problem is that tensorflow is currently linking the static android STL library versions. I could not find a way to resolve this with bazel. Fortunately it does not seem to be visible externally, so other libraries linked with c++_shared/static still work.", "comments": ["Nagging Reviewer @angersson: It has been 20 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @angersson: It has been 36 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @angersson: It has been 51 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "@aselle @gunan can you look at this? I don't think I have the expertise to review appropriately.", "@linusmartensson thanks for your change!\r\nHowever, This is a PR to 1.8 branch, and old release branch.\r\nThis change will be lost in this old release.\r\nCould you recreate this change to the master branch?\r\nAs a policy, we do not accept changes to any release branches, except for critical bugfixes.", "Nagging Assignee @martinwicke: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @martinwicke: It has been 31 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'll close this. Please reopen against master."]}, {"number": 18552, "title": "parameterized_docker_build.sh fails to build", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n*NO*\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nDockerfile: nvidia/cuda:9.0-base-ubuntu16.04\r\nMy System: Fedora 27\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\nv1.8.0-rc0\r\n- **Python version**:\r\n3.6\r\n- **Bazel version**\r\nN/A\r\n- **CUDA/cuDNN version**\r\nN/A\r\n- **GPU model and memory**\r\nN/A\r\n- **Exact command to reproduce**\r\n`./parameterized_docker_build.sh`\r\n\r\n### Describe the problem\r\nBuild the Docker image from a fresh clone of Tensorflow:\r\n\r\n``` shell\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ngit fetch --all --tags --prune\r\ngit checkout -b v.1.8.0 v1.8.0-rc0\r\ncd tensorflow/tensorflow/tools/docker\r\nexport TF_DOCKER_BUILD_IS_DEVEL=NO\r\nexport TF_DOCKER_BUILD_TYPE=GPU\r\nexport TF_DOCKER_BUILD_PYTHON_VERSION=PYTHON3\r\n./parameterized_docker_build.sh\r\n```\r\n\r\nAfter some time the build fails with:\r\n\r\n```shell\r\nAttributeError: '_NamespacePath' object has no attribute 'sort'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n\r\n### Source code / logs\r\n```shell\r\nERROR: /workspace/tensorflow/tools/api/generator/BUILD:27:1: Executing genrule //tensorflow/tools/api/generator:python_api_gen failed (Exit 1): bash failed: error executing command \r\n  (cd /home/oroel/projects/Picsure/tensorflow/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_oroel/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda \\\r\n    CUDNN_INSTALL_PATH=/usr/local/cuda-9.0 \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\n    LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python3.5/dist-packages \\\r\n    TF_CUDA_CLANG=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=3.0 \\\r\n    TF_CUDA_VERSION=9.0 \\\r\n    TF_CUDNN_VERSION=7 \\\r\n    TF_NCCL_VERSION=1 \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/k8-py3-opt/bin/tensorflow/tools/api/generator/create_python_api bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/app/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/bitwise/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/compat/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/contrib/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/contrib/stat_summarizer/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/data/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/distributions/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/distributions/bijectors/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/errors/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/estimator/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/estimator/export/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/estimator/inputs/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/feature_column/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/gfile/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/graph_util/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/image/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/initializers/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/activations/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/densenet/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/inception_resnet_v2/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/inception_v3/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/mobilenet/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/nasnet/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/resnet50/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/vgg16/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/vgg19/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/applications/xception/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/backend/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/callbacks/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/constraints/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/datasets/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/datasets/boston_housing/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/datasets/cifar10/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/datasets/cifar100/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/datasets/fashion_mnist/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/datasets/imdb/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/datasets/mnist/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/datasets/reuters/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/estimator/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/initializers/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/layers/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/losses/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/metrics/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/models/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/optimizers/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/preprocessing/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/preprocessing/image/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/preprocessing/sequence/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/preprocessing/text/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/regularizers/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/utils/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/wrappers/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/keras/wrappers/scikit_learn/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/layers/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/linalg/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/logging/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/losses/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/manip/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/math/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/metrics/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/nn/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/nn/rnn_cell/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/profiler/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/python_io/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/resource_loader/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/builder/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/constants/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/loader/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/main_op/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/signature_constants/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/signature_def_utils/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/tag_constants/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/saved_model/utils/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/sets/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/spectral/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/summary/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/sysconfig/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/test/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/train/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/train/queue_runner/__init__.py bazel-out/k8-py3-opt/genfiles/tensorflow/tools/api/generator/api/user_ops/__init__.py')\r\nTraceback (most recent call last):\r\n  File \"/home/oroel/projects/Picsure/tensorflow/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_oroel/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/tools/api/generator/create_python_api.py\", line 26, in <module>\r\n    from tensorflow.python.util import tf_decorator\r\n  File \"/home/oroel/projects/Picsure/tensorflow/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_oroel/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 52, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"/home/oroel/projects/Picsure/tensorflow/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_oroel/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"/usr/local/lib/python3.5/dist-packages/google/protobuf/__init__.py\", line 37, in <module>\r\n    __import__('pkg_resources').declare_namespace(__name__)\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2927, in <module>\r\n    @_call_aside\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2913, in _call_aside\r\n    f(*args, **kwargs)\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2952, in _initialize_master_working_set\r\n    add_activation_listener(lambda dist: dist.activate())\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 956, in subscribe\r\n    callback(dist)\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2952, in <lambda>\r\n    add_activation_listener(lambda dist: dist.activate())\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2515, in activate\r\n    declare_namespace(pkg)\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2097, in declare_namespace\r\n    _handle_ns(packageName, path_item)\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2047, in _handle_ns\r\n    _rebuild_mod_path(path, packageName, module)\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2066, in _rebuild_mod_path\r\n    orig_path.sort(key=position_in_sys_path)\r\nAttributeError: '_NamespacePath' object has no attribute 'sort'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 2047.653s, Critical Path: 176.85s\r\nFAILED: Build did NOT complete successfully\r\nBuild failed.\r\n\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hi @OleRoel, guys\r\n \r\nI was stuck on this problem too, then I rolledback protobuf to version 3.5.1 and it worked for me.\r\n\r\nPreviously my system had protobuf==3.5.2,post1 and wasn't working then I tried with 3.5.2 and also didn't worked and finally after switch to 3.5.1 it worked.\r\n", "ya! I got exactly the same error & stacks when build on linux gpu version as @OleRoel . I was using v1.80 to build, hope to get some insights here. \r\n\r\n", "Hi,\r\n\r\nTry to Downgrade protobuf to 3.5.1 \r\n\r\n```\r\npip3 install protobuf==3.5.1\r\n```\r\n", "Thanks @cyberwillis , tried the workaround, it does fix the problem. just curious how the 3.5.2.post1 introduced the issue. ", "@cyberwillis and @simpeng Can you please elaborate in mode detail, what you did? I'd appreciate it very much. Makes me feel quite stupid that I don't see at the first glance, where to downgrade the protobuf package :-)", "I didn't make nothing either, I just thought.\r\n\r\nI saw the error tracing back to protobuf and I thought two things:\r\n1. I had compiled right after the release of tf 1.7 successfully  (right after march 30) and now I was compiling the  tf 1.8 and nothing appear to be right. \r\n\r\n2. I could not blame the entire repository of tf for this they would not let everybody down like this. Then may be that package or another one could be the problem.\r\n\r\nThen I tought... \"protobuf\" what was the time of my previous tensorflow build that actually get success and investigated at [pypi.org](https://pypi.org/project/protobuf/#history). Next I asked myself what was the protobuf that I could have on my machine on that time I get everything successfull?\r\n\r\nI just find out that the release date of the package was days after my last build on Tf 1.7. Then I relealised that could be it.", "Had the same issue  ( AttributeError: '_NamespacePath' object has no attribute 'sort' ) with  Tensorflow  1.9  Ubuntu 16.04, and Python 3.,5.2.   Seems to be gone after moving to Python 3.6.6 . \r\n", "I had the same issue `( AttributeError: '_NamespacePath' object has no attribute 'sort' )` with TF v1.9.0-rc2 and v1.8.0; on Ubuntu 16.04, Python 3.5.2, protobuf 3.6.\r\n\r\nFixed it by downgrading protobuff to 3.5.1.", "I am still seeing the issue even I moved back to protbuf 3.5.1. :( \r\n$ pip3 list | grep -i  protobuf\r\nprotobuf                      3.5.1\r\nBazel version is 0.15.2\r\ntf1.8\r\nOS: Ubuntu16.04.4\r\nPython : 3.5.2\r\n\r\nINFO: Found 1 target...\r\nERROR: /home/taccuser/.cache/bazel/_bazel_taccuser/cbc64142ddf66660b02ff669605f4d7f/external/protobuf_archive/BUILD:265:1: Executing genrule @protobuf_archive//:generate_js_well_known_types_embed failed (Aborted): bash failed: error executing command\r\n  (cd /home/taccuser/.cache/bazel/_bazel_taccuser/cbc64142ddf66660b02ff669605f4d7f/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/usr/local/lib:/usr/local/mpi/lib:/usr/local/lib:/usr/local/mpi/lib: \\\r\n    PATH=/opt/rocm/hcc/bin:/opt/rocm/hip/bin:/opt/rocm/hcc/bin:/opt/rocm/hip/bin:/home/taccuser/bin:/home/taccuser/.local/bin:/usr/local/mpi/bin:/home/taccuser/bin:/home/taccuser/.local/bin:/usr/local/mpi/bin:/home/taccuser/bin:/home/taccuser/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/external/protobuf_archive/js_embed external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js > bazel-out/host/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc'): bash failed: error executing command\r\n  (cd /home/taccuser/.cache/bazel/_bazel_taccuser/cbc64142ddf66660b02ff669605f4d7f/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/usr/local/lib:/usr/local/mpi/lib:/usr/local/lib:/usr/local/mpi/lib: \\\r\n    PATH=/opt/rocm/hcc/bin:/opt/rocm/hip/bin:/opt/rocm/hcc/bin:/opt/rocm/hip/bin:/home/taccuser/bin:/home/taccuser/.local/bin:/usr/local/mpi/bin:/home/taccuser/bin:/home/taccuser/.local/bin:/usr/local/mpi/bin:/home/taccuser/bin:/home/taccuser/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/external/protobuf_archive/js_embed external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js > bazel-out/host/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc')\r\n/bin/bash: line 1: 15045 Aborted                 (core dumped) bazel-out/host/bin/external/protobuf_archive/js_embed external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js > bazel-out/host/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 1.329s, Critical Path: 0.65s\r\nINFO: 12 processes: 12 local.\r\nFAILED: Build did NOT complete successfully\r\n", "I've recently proposed a change to TensorFlow that obsoletes parameterized_docker_build.sh, which may help alleviate this issue. If anyone following this thread is interested in making TensorFlow's Dockerfile story better for everyone, [please take a look at the RFC](https://github.com/tensorflow/community/pull/8).", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 18551, "title": "TypeError: Cannot convert a tensor of type float32 to an input of type float32_ref", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nTypeError: Cannot convert a tensor of type float32 to an input of type float32_ref\r\n\r\n\r\n\r\n### Source code / logs\r\nC:\\Users\\PC.000\\Desktop\\emoji_final\\asset\\train\\new>python afterpb.py\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\user.user-PC.000\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py\", line 667, in import_graph_de\r\n    op._add_input(source_tensor, dtype=input_type)\r\n  File \"C:\\Users\\user.user-PC.000\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1898, in _add_input\r\n    (tensor.dtype.name, dtype.name))\r\nTypeError: Cannot convert a tensor of type float32 to an input of type float32_ref\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"afterpb.py\", line 39, in <module>\r\n    tf.import_graph_def(graph_def, name='')\r\n  File \"C:\\Users\\user.user-PC.000\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 432, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\user.user-PC.000\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py\", line 671, in import_graph_de\r\n    node, 'Input tensor %r %s' % (input_name, te)))\r\nValueError: graph_def is invalid at node 'save/Assign': Input tensor 'conv1d_1/W:0' Cannot convert a tensor of type float32 to an input of type float32_ref.\r\n\r\n", "comments": ["any updates Mr. tatatodd ?", "Nagging Assignee @tatatodd: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 18550, "title": "compile tensorflow r1.8 occured error", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 17.10\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: r1.8\r\n- **Python version**:  python 3.6\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: GCC-6\uff0cG++-6\r\n- **CUDA/cuDNN version**: CUDA 9.1\uff08include patch1/2/3\uff09\uff0ccuDNN 7.1\r\n- **GPU model and memory**: gtx 1060ti\uff0c6GB\uff1b gtx 1080ti\uff0c11GB\r\n- **Exact command to reproduce**: \r\nbazel build --config=opt --config=cuda --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Describe the problem\r\nlately\uff0cI had try install tensorflow r1.8 from source but unsuccessful\uff0cthen I tryed to compile tf r1.6 it is normal, and my friend also try to install tensorflow r1.8 from source was successful, which difference enviroment between us are CUDA/cuDNN and gcc/g++, he use CUDA 8.0/cuDNN 5.0 and gcc-5/g++5.\r\n\r\ncompile tf r1.8 occured error log as below\uff1a\r\n```\r\n/usr/include/c++/6/tuple:484:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {std::tuple<int, int, int>}; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement\r\n     }\r\n ^\r\n/usr/include/c++/6/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {std::tuple<int, int, int>}; bool <anonymous> = true; _Elements = {int, int, int}]':\r\n/usr/include/c++/6/tuple:626:362:   required by substitution of 'template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {std::tuple<int, int, int>}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]'\r\n./tensorflow/stream_executor/dnn.h:891:91:   required from here\r\n/usr/include/c++/6/tuple:489:65: error: mismatched argument pack lengths while expanding 'std::is_convertible<_UElements&&, _Elements>'\r\n       return __and_<is_convertible<_UElements&&, _Elements>...>::value;\r\n                                                                 ^~~~~\r\n/usr/include/c++/6/tuple:490:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {std::tuple<int, int, int>}; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement\r\n     }\r\n ^\r\n/usr/include/c++/6/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = std::tuple<int, int, int>&&; bool <anonymous> = true; _Elements = {int, int, int}]':\r\n/usr/include/c++/6/tuple:686:422:   required by substitution of 'template<class ... _UElements, class _Dummy, typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), int, int, int>::_NonNestedTuple<tuple<_Elements ...>&&>()), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(std::tuple<_Args1 ...>&&) [with _UElements = {int, int, int}; _Dummy = void; typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), int, int, int>::_NonNestedTuple<tuple<_Elements ...>&&>()), bool>::type <anonymous> = <missing>]'\r\n./tensorflow/stream_executor/dnn.h:891:91:   required from here\r\n/usr/include/c++/6/tuple:495:244: error: wrong number of template arguments (4, should be 2)\r\n       return  __and_<__not_<is_same<tuple<_Elements...>,\r\n                                                                                                                                                                                                                                                    ^    \r\n/usr/include/c++/6/type_traits:1558:8: note: provided for 'template<class _From, class _To> struct std::is_convertible'\r\n     struct is_convertible\r\n        ^~~~~~~~~~~~~~\r\n/usr/include/c++/6/tuple:502:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = std::tuple<int, int, int>&&; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement\r\n     }\r\n ^\r\nERROR: /home/andy/TF/tensorflow/tensorflow/contrib/nccl/BUILD:23:1: output 'tensorflow/contrib/nccl/_objs/python/ops/_nccl_ops_gpu/tensorflow/contrib/nccl/kernels/nccl_ops.pic.o' was not created\r\nERROR: /home/andy/TF/tensorflow/tensorflow/contrib/nccl/BUILD:23:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1037.698s, Critical Path: 28.30s\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nmy configure of r1.7 and r1.8 is below:\r\n```\r\n(python3) andy@andy:~/TF/tensorflow$ ./configure \r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nYou have bazel 0.11.1 installed.\r\nPlease specify the location of python. [Default is /home/andy/python3/bin/python]: \r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nAttributeError: module 'site' has no attribute 'getsitepackages'\r\nFound possible Python library paths:\r\n  /home/andy/python3/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/andy/python3/lib/python3.6/site-packages]\r\n\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: \r\njemalloc as malloc support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n\r\nNo Google Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: n\r\nNo Hadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n\r\nNo Amazon S3 File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: n\r\nNo Apache Kafka Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: \r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: \r\nNo GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: \r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: \r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 9.0]: 9.1\r\n\r\n\r\nPlease specify the location where CUDA 9.1 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 7\r\n\r\n\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: \r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nPlease specify the NCCL version you want to use. [Leave empty to default to NCCL 1.3]: \r\n\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 6.1]\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: \r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/x86_64-linux-gnu-gcc-6]: \r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: \r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: \r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See tools/bazel.rc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\nConfiguration finished\r\n```\r\n\r\n", "comments": ["Edit: dupe of #18402 and #18522. Workaround is in #18434", "I believe this is resolved", "The error is still printed in v1.10.1. Ideal fix would be to redirect stderr in the respective `run_shell` command in `configure.py`. For myself, I added `sys.stderr.write('\\n*** The above error message can be safely ignored. ***\\n\\n')` in the `except` block."]}, {"number": 18549, "title": "non-gpu tensorflow cannot import and run ", "body": "1. Windows 10\r\n2. Anaconda 5.1.0 \uff08Python 3.6.4)\r\n3. **NO GPU version**  (my laptop doesn't have GPU... Poor )\r\n4. **MSVCP140.DLL** already installed and setted in PATH , it's everywhere\r\n\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: \u52a8\u6001\u94fe\u63a5\u5e93(DLL)\u521d\u59cb\u5316\u4f8b\u7a0b\u5931\u8d25\u3002\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *  # pylint: disable=redefined-builtin\r\n  File \"C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: \u52a8\u6001\u94fe\u63a5\u5e93(DLL)\u521d\u59cb\u5316\u4f8b\u7a0b\u5931\u8d25\u3002\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Q: Have I written custom code          \r\nAnswer: NO\r\n\r\nQ: OS Platform and Distribution       \r\nAnswer: What? (confused)  Windows 10 Education\r\n\r\nQ: TensorFlow installed from            \r\nAnswer:  **C:> conda create -n tensorflow pip python=3.6**  ([more](https://www.tensorflow.org/install/install_windows))\r\n\r\nQ: TensorFlow version\r\nAnswer:   1.7.0\r\n\r\nQ: Bazel version\r\nAnswer:  not install, Does **non-gpu tensorflow** version need it ?\r\n\r\nQ: CUDA/cuDNN version\r\n**Answer: My laptop does not have GPU**  \r\nDoes **non-gpu tensorflow** version need it ?\r\n\r\nQ: GPU model and memory\r\n**Answer: My laptop does not have GPU** \r\nDoes **non-gpu tensorflow** version need it ?\r\n\r\n\r\nQ: Exact command to reproduce\r\n**C:> conda create -n tensorflow pip python=3.6**\r\n\r\n**C:> activate tensorflow\r\n (tensorflow)C:>  # Your prompt should change**\r\n\r\n**(tensorflow)C:> pip install --ignore-installed --upgrade tensorflow** \r\n\r\n// Above three steps perfect ! However, start idle, and \r\n>>> import tensorflow as tf\r\nError (Details at top)\r\n\r\n**_I hope get your solution ASAP !_** ", "This problem is usually because your CPU not having AVX instruction set support. What is the model of your CPU?", "CPU : Intel(R) Pentium(R) CPU G4600  @ 3.60GHz    @gunan ", "I checked my CPU by using CPU-Z , it **does not** support AVX instruction..\r\nWhat should I do ?  \r\n@gunan  @gunan   ", "Nagging Assignee @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @gunan: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "duplicate of #19584 "]}, {"number": 18548, "title": "[r1.7][TensorRT] The passing parameter \"max_batch_size\" within function \"trt.create_inference_graph()\" is fixed to 1", "body": "What do I intended to do:\r\nI've already trained a model by TensorFlow, and then try to use the integrated TensorRT to import the frozen model and optimize it, then output the optimized graph_def and session run it by TensorFlow.\r\n\r\nScript:                                                 \r\n```\r\n      with tf.Graph().as_default():\r\n         with tf.device(\"/device:GPU:0\"):                                                                                 \r\n            output_graph_def = tf.GraphDef()\r\n            with open(\"./\"+frozen_model_name, \"rb\") as f:\r\n               output_graph_def.ParseFromString(f.read())\r\n               print (len(output_graph_def.node))\r\n               _ = tf.import_graph_def(output_graph_def, name=\"\")\r\n                                                                                   \r\n        f32_graph = trt.create_inference_graph(\r\n            input_graph_def=output_graph_def,                                                                                       \r\n            outputs=['InceptionV3/Logits/SpatialSqueeze'],\r\n            max_batch_size = 16,  #<<<<<<<<<<<<<<<HERE\r\n            max_workspace_size_bytes=1 << 20,\r\n            precision_mode=\"FP32\",  # TRT Engine precision \"FP32\",\"FP16\" or \"INT8\"                                        \r\n            minimum_segment_size=2  # minimum number of nodes in an engine                                                \r\n        )\r\n```\r\nLog:\r\n```\r\nhost/replica:0/task:0/device:GPU:0 with 4757 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:08.0, compute capability: 6.1)\r\nTraceback (most recent call last):\r\n  File \"extract_model_tf_trt_frozen.py\", line 118, in <module>\r\n    _main()\r\n  File \"extract_model_tf_trt_frozen.py\", line 91, in _main\r\n    val = sess.run(out, {inp: batch_input})\r\n  File \"/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 905, in run\r\n    run_metadata_ptr)\r\n  File \"/home/web_server/dlpy72/dlpy/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1116, in _run\r\n    str(subfeed_t.get_shape())))\r\nValueError: Cannot feed value of shape (16, 299, 299, 3) for Tensor u'import/Placeholder:0', which has shape '(1, 299, 299, 3)'\r\n```\r\n\r\nObservation:\r\nSeems like even though I set the passing parameter \"max_batch_size\" to 16, the generated new graph still only has batch_size == 1. I can't locate the source file of trt_convert(), which is relevant actually, probably it's a binary .o file so I can only post a new issue here for help.\r\n\r\nHas anyone encountered the same issue?\r\nAny idea will be welcome.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hi tensorflowbutler,\r\n\r\nSorry for ignoring the required information.\r\n\r\n- Have I written custom code: No\r\n- OS Platform and Distribution:  Red Hat 4.8.5-16, Linux version 3.10.0-693.5.2.el7.x86_64\r\n- TensorFlow installed from: pip (python 2.7)\r\n- TensorFlow version: tensorflow-gpu==1.7.0\r\n- Bazel version: N/A\r\n- CUDA/cuDNN version: CUDA9.0, cuDNN7.0.5\r\n- GPU model and memory: Tesla P4, 8GB\r\n- Exact command to reproduce: \r\nMy own script is coded based on the official test \"tensorflow/tensorflow/contrib/tensorrt/test/test_tftrt.py\", so I think you can reproduce this problem with this test. The point is \"max_batch_size\" within trt.create_inference_graph().\r\n\r\nThanks,", "I figured out the root-cause.\r\nIt's because the dimension of Placeholder of the frozen model is (1, 299, 299, 3) and it can' be modified once been generated.\r\nThe solution is to regenerate a binary graph.pbtxt of the model with input placeholder(?, 299, 299, 3), then freeze it and use tensorrt to optimize it.\r\n ", "@oscarriddle Good to hear that, and thanks for the information!", "@oscarriddle  @tensorflowbutler  I'm trying to execute tfrt.py but y have this problems:\r\n  File \"test_tftrt.py\", line 27, in <module>\r\n    from tensorflow.contrib import tensorrt as trt\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tensorrt/__init__.py\", line 25, in <module>\r\n    from tensorflow.contrib.tensorrt.python import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tensorrt/python/__init__.py\", line 22, in <module>\r\n    from tensorflow.contrib.tensorrt.python.ops import trt_engine_op\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tensorrt/python/ops/trt_engine_op.py\", line 25, in <module>\r\n    from tensorflow.contrib.tensorrt.ops.gen_trt_engine_op import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tensorrt/ops/gen_trt_engine_op.py\", line 11, in <module>\r\n    from tensorflow.python.eager import context as _context\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/context.py\", line 50, in <module>\r\n    DEVICE_PLACEMENT_EXPLICIT = pywrap_tensorflow.TFE_DEVICE_PLACEMENT_EXPLICIT\r\nAttributeError: module 'tensorflow.python.pywrap_tensorflow' has no attribute 'TFE_DEVICE_PLACEMENT_EXPLICIT'\r\n\r\n\r\nDo you know ?\r\n\r\nThanks \r\n"]}, {"number": 18547, "title": "Fix the doc strings of nn.sampled_softmax_loss since it was deprecated", "body": "This PR is to fix the doc strings of `nn.sampled_softmax_loss` within tensorflow/python related apis since it was deprecated and replaced with  `nn.sampled_softmax_loss_v2` according to [softmax_cross_entropy_with_logits](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits).\r\n\r\n", "comments": []}, {"number": 18546, "title": "call session.run() concurrency, latency boom", "body": "background: serving mnist model using libtensorflow_cc.so and libtensorflow_framework.so gpu(p40) version\r\n\r\nbelow: limit 3 concurrency to call `session.run()`, got the latency 508us, very close to serial run.\r\n![image](https://user-images.githubusercontent.com/3832082/38794426-a9a36dc4-4187-11e8-8aa6-57df4c61578f.png)\r\n\r\nHowever, expand  concurrency to 24 , `session.run()` got the latency 1256us\r\n![image](https://user-images.githubusercontent.com/3832082/38794599-3c0e1380-4188-11e8-9560-6dd4fd1e9750.png)\r\n\r\nanyone can help ?\r\n\r\nlogs\uff1a\r\n```shell\r\n[DDL-Serving]$ ./DDL_server --model_config_file=/home/luban/DDL-Serving/config.json\r\n2018-04-16 14:29:38.599280: I /home/luban/DDL-Serving/core/server_core.cc:91] Loading models:tensorflow\r\n2018-04-16 14:29:38.599490: I /home/luban/DDL-Serving/core/server_core.cc:98] name:/home/luban/DDL-Serving/mnist_model/3\r\nI0416 14:29:38.599814 23547 /home/luban/DDL-Serving/core/loader_helper.cc:59] Approving load for servable version { name:mnist ,version:3}\r\nI0416 14:29:38.599953 23549 /home/luban/DDL-Serving/core/loader_helper.cc:67] Loading servable version { name:mnist ,version:3}\r\nI0416 14:29:38.600110 23549 /home/luban/DDL-Serving/core/tf_bundle_factory.cc:230] Attempting to load native SavedModelBundle in loader.cc from: /home/luban/DDL-Serving/mnist_model/3\r\n2018-04-16 14:29:38.600163: I tensorflow/cc/saved_model/loader.cc:242] Loading SavedModel with tags: { serve }; from: /home/luban/DDL-Serving/mnist_model/3\r\n2018-04-16 14:29:38.910954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:\r\nname: Tesla P40 major: 6 minor: 1 memoryClockRate(GHz): 1.531\r\npciBusID: 0000:02:00.0\r\ntotalMemory: 22.38GiB freeMemory: 22.21GiB\r\n2018-04-16 14:29:38.911028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0\r\n2018-04-16 14:29:39.721881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-04-16 14:29:39.721955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0\r\n2018-04-16 14:29:39.721971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N\r\n2018-04-16 14:29:39.722897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21555 MB memory) -> physical GPU (device: 0, name: Tesla P40, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n2018-04-16 14:29:40.028849: I tensorflow/cc/saved_model/loader.cc:161] Restoring SavedModel bundle.\r\n2018-04-16 14:29:40.034555: I tensorflow/cc/saved_model/loader.cc:196] Running LegacyInitOp on SavedModel bundle.\r\n2018-04-16 14:29:40.040415: I tensorflow/cc/saved_model/loader.cc:291] SavedModel load for tags { serve }; Status: success. Took 1440252 microseconds.\r\nI0416 14:29:40.040555 23549 /home/luban/DDL-Serving/core/loader_helper.cc:77] Successfully loaded servable version { name:mnist ,version:3}\r\n2018-04-16 14:29:40.040656: I /home/luban/DDL-Serving/core/server_core.cc:98] name:/home/luban/DDL-Serving/mnist_model/1\r\nI0416 14:29:40.040829 23547 /home/luban/DDL-Serving/core/loader_helper.cc:59] Approving load for servable version { name:mnist ,version:1}\r\nI0416 14:29:40.040890 23549 /home/luban/DDL-Serving/core/loader_helper.cc:67] Loading servable version { name:mnist ,version:1}\r\nI0416 14:29:40.040937 23549 /home/luban/DDL-Serving/core/tf_bundle_factory.cc:230] Attempting to load native SavedModelBundle in loader.cc from: /home/luban/DDL-Serving/mnist_model/1\r\n2018-04-16 14:29:40.040966: I tensorflow/cc/saved_model/loader.cc:242] Loading SavedModel with tags: { serve }; from: /home/luban/DDL-Serving/mnist_model/1\r\n2018-04-16 14:29:40.041996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0\r\n2018-04-16 14:29:40.042044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-04-16 14:29:40.042066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0\r\n2018-04-16 14:29:40.042082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N\r\n2018-04-16 14:29:40.042318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21555 MB memory) -> physical GPU (device: 0, name: Tesla P40, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n2018-04-16 14:29:40.044106: I tensorflow/cc/saved_model/loader.cc:161] Restoring SavedModel bundle.\r\n2018-04-16 14:29:40.048062: I tensorflow/cc/saved_model/loader.cc:196] Running LegacyInitOp on SavedModel bundle.\r\n2018-04-16 14:29:40.053422: I tensorflow/cc/saved_model/loader.cc:291] SavedModel load for tags { serve }; Status: success. Took 12455 microseconds.\r\nI0416 14:29:40.053490 23549 /home/luban/DDL-Serving/core/loader_helper.cc:77] Successfully loaded servable version { name:mnist ,version:1}\r\n2018-04-16 14:29:40.053579: I /home/luban/DDL-Serving/core/server_core.cc:123] Load models Done.\r\nI0416 14:29:40.061132 23547 /home/luban/DDL-Serving/brpc/src/brpc/server.cpp:984] Server[DDL::serving::PredictionServiceImpl] is serving on port=8002.\r\nI0416 14:29:40.061608 23547 /home/luban/DDL-Serving/brpc/src/brpc/server.cpp:987] Check out http://dd80f54614c3:8002 in web browser.\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "- **OS Platform and Distribution (CentOS Linux release 7.3.1611)**:\r\n- **TensorFlow installed from (source r1.7)**:\r\n- **TensorFlow version (r1.7)**:\r\n- **Python version**: \r\n- **Bazel version (0.11.1-)**:\r\n- **GCC/Compiler version (gcc4.8.5)**:\r\n- **CUDA/cuDNN version**:(8.0/6.0.21)\r\n- **GPU model and memory**:( P40, 22.38GiB)\r\n- **Exact command to reproduce**:", "@aselle any idea ?", "Most likely,  when increase the number of concurrent, gpu kernels competition for resources has become frequent.\r\nMaybe you can reduce the number of concurrent to balance qps and latency.\r\nAlso, you can try binding thread to different gpu streams, if tensorflow support.", "This is more of a stackoverflow question, so please re-ask there. I would suspect that you are better off batching multiple requests together into a single sess.run(). (i.e. use a producer consumer model), rather than trying to do a lot of sess.run()'s. TensorFlow was not designed to have lots of concurrent sess.run()'s.  @mrry may have a better answer."]}, {"number": 18545, "title": "fix command line example package path", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it.", "CLAs look good, thanks!\n\n<!-- ok -->", "@andrehentz this is a bug in the sync scripts, probably should be fixed there.", "Nagging Assignee @andrehentz: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrehentz: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrehentz: It has been 31 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I came across this bug as well, the fix in the PR works for me. Could this be merged?"]}, {"number": 18544, "title": "Fix typo", "body": "fix typo", "comments": []}, {"number": 18543, "title": "saved_model.pb, saved_model.pbtxt missing google cloud", "body": "My model has finished training but i can not see the `saved_model.pb, saved_model.pbtxt` in the training directory. or any of my bucket folders", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: **NO**\r\nOS Platform and Distribution:  **Google clound ML Engine**\r\nTensorFlow installed from: I don't understand\r\nTensorFlow version: 1.5\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: I scheduled a training job on google cloud ml Engine using this command \r\n```\r\ngcloud ml-engine jobs submit training insectacon_frcnn_`date +%s` \\\r\n    --runtime-version 1.5 \\\r\n    --job-dir=${TRAIN_DIR} \\\r\n    --packages object_detection-0.1.tar.gz,slim-0.1.tar.gz \\\r\n    --module-name object_detection.train \\\r\n    --region us-central1 \\\r\n    --scale-tier BASIC_GPU \\\r\n    --config gcloud.yaml \\\r\n    -- \\\r\n    --train_dir=${TRAIN_DIR} \\\r\n    --pipeline_config_path=${PIPELINE_CONFIG_PATH}\r\n```\r\n\r\nhere is the `gcloud.yaml`\r\n\r\n```\r\ntrainingInput:\r\n  runtimeVersion: \"1.5\"\r\n  scaleTier: BASIC\r\n```\r\nhere is the pipeline\r\n\r\n```\r\n# Faster R-CNN with Resnet-101 (v1), configured for Pascal VOC Dataset.\r\n# Users should configure the fine_tune_checkpoint field in the train config as\r\n# well as the label_map_path and input_path fields in the train_input_reader and\r\n# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\r\n# should be configured.\r\n\r\nmodel {\r\n  faster_rcnn {\r\n    num_classes: 1\r\n    image_resizer {\r\n      keep_aspect_ratio_resizer {\r\n        min_dimension: 768\r\n        max_dimension: 1024\r\n      }\r\n    }\r\n    feature_extractor {\r\n      type: 'faster_rcnn_resnet101'\r\n      first_stage_features_stride: 16\r\n    }\r\n    first_stage_anchor_generator {\r\n      grid_anchor_generator {\r\n        scales: [0.25, 0.5, 1.0, 2.0]\r\n        aspect_ratios: [0.5, 1.0, 2.0]\r\n        height_stride: 16\r\n        width_stride: 16\r\n      }\r\n    }\r\n    first_stage_box_predictor_conv_hyperparams {\r\n      op: CONV\r\n      regularizer {\r\n        l2_regularizer {\r\n          weight: 0.0\r\n        }\r\n      }\r\n      initializer {\r\n        truncated_normal_initializer {\r\n          stddev: 0.01\r\n        }\r\n      }\r\n    }\r\n    first_stage_nms_score_threshold: 0.0\r\n    first_stage_nms_iou_threshold: 0.7\r\n    first_stage_max_proposals: 300\r\n    first_stage_localization_loss_weight: 2.0\r\n    first_stage_objectness_loss_weight: 1.0\r\n    initial_crop_size: 14\r\n    maxpool_kernel_size: 2\r\n    maxpool_stride: 2\r\n    second_stage_box_predictor {\r\n      mask_rcnn_box_predictor {\r\n        use_dropout: false\r\n        dropout_keep_probability: 1.0\r\n        fc_hyperparams {\r\n          op: FC\r\n          regularizer {\r\n            l2_regularizer {\r\n              weight: 0.0\r\n            }\r\n          }\r\n          initializer {\r\n            variance_scaling_initializer {\r\n              factor: 1.0\r\n              uniform: true\r\n              mode: FAN_AVG\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n    second_stage_post_processing {\r\n      batch_non_max_suppression {\r\n        score_threshold: 0.0\r\n        iou_threshold: 0.5\r\n        max_detections_per_class: 100\r\n        max_total_detections: 300\r\n      }\r\n      score_converter: SOFTMAX\r\n    }\r\n    second_stage_localization_loss_weight: 2.0\r\n    second_stage_classification_loss_weight: 1.0\r\n  }\r\n}\r\n\r\ntrain_config: {\r\n  batch_size: 1\r\n  optimizer {\r\n    momentum_optimizer: {\r\n      learning_rate: {\r\n        manual_step_learning_rate {\r\n          initial_learning_rate: 0.0003\r\n          schedule {\r\n            step: 50000\r\n            learning_rate: .00003\r\n          }\r\n          schedule {\r\n            step: 120000\r\n            learning_rate: .000003\r\n          }\r\n        }\r\n      }\r\n      momentum_optimizer_value: 0.9\r\n    }\r\n    use_moving_average: false\r\n  }\r\n  gradient_clipping_by_norm: 10.0\r\n  fine_tune_checkpoint: \"gs://insectacon-200911-mlengine/insectacon_v1/model.ckpt-20000\"\r\n  from_detection_checkpoint: true\r\n  num_steps: 20050\r\n  data_augmentation_options {\r\n    random_horizontal_flip {\r\n    }\r\n  }\r\n}\r\n\r\ntrain_input_reader: {\r\n  tf_record_input_reader {\r\n    input_path: \"gs://insectacon-200911-mlengine/data/pascal.train.record\"\r\n  }\r\n  label_map_path: \"gs://insectacon-200911-mlengine/data/map.pbtxt\"\r\n  queue_capacity: 500\r\n  min_after_dequeue: 250\r\n}\r\n\r\neval_config: {\r\n  num_examples: 53\r\n}\r\n\r\neval_input_reader: {\r\n  tf_record_input_reader {\r\n    input_path: \"gs://insectacon-200911-mlengine/data/pascal.eval.record\"\r\n  }\r\n  label_map_path: \"gs://insectacon-200911-mlengine/data/map.pbtxt\"\r\n  shuffle: false\r\n  num_readers: 1\r\n  queue_capacity: 500\r\n  min_after_dequeue: 250\r\n}\r\n```\r\n", "![screen shot 2018-04-16 at 22 28 00](https://user-images.githubusercontent.com/416610/38840050-5217979c-41e6-11e8-92b8-2e7d404e469d.png)\r\n![screen shot 2018-04-16 at 22 27 00](https://user-images.githubusercontent.com/416610/38840051-524aea84-41e6-11e8-8fdc-2f9956286b8a.png)\r\n", "I have the same problem. Any solution? I'm using GCP's public example.", "The same for me, in my case it happens in some job and not in others even if the trainer source code is almost identical (few changes in parameter tuning) ", "@saeta can you offer any hints about where these files end up in Cloud, or reassign appropriately, please?", "Looks like the latest version of the code defines a `--model-dir` flag. @cy89 are you setting that to a GCS directory? @mistaguy can you try the latest version of the code and use `--model-dir`? ", "@mistaguy,\r\nSorry for the delayed response. Can you please let us know if using  `--model-dir`, as per [this comment](https://github.com/tensorflow/tensorflow/issues/18543#issuecomment-427120830) has resolved your issue? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/18543\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/18543\">No</a>\n"]}, {"number": 18542, "title": "Why single-image inference on Android using tflite shows different answer everytime?", "body": "Hello, I have already asked a question on stackoverflow.`https://stackoverflow.com/questions/49833090/tflite-accuracy-on-a-device-is-not-consistent-with-that-of-origin-pb-file-on-pc`\r\nI am not sure about the mechanism of tensorflow lite, but I am now pretty sure that tflite run only a random part of the model to generate a result. I have thought of the toco operation being the cause and I used it to convert tflite back to pb which works fine same as the origin pb. So I have no idea what's going wrong here. Any advice? thanks a lot.", "comments": ["That StackOverflow page is not more available. Reopen the issue, please. "]}, {"number": 18541, "title": "How to get started with Tensorflow Lite?", "body": "How do you get started with Tensorflow Lite? For example, how do you download and/or build it? How do you use it? All I can find is statements that \"most of our documentation is on Github\", but that seems to be a dead end. \r\n\r\nThe only documentation I found was the stuff in the g3doc folder, which includes build instructions for iOS and Raspberry Pi, but it doesn't say anything about Android.", "comments": ["There used to be information in the [README.md of TF Lite](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/README.md). I think they are moved to [tensorflow/docs_src/mobile/tflite](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/docs_src/mobile/tflite) recently", "Thanks. I have another question: Are non-embedded platforms supported at all? E.g. could I use this on x64 Linux?", "It is possible to build TF Lite for non-embedded platforms. You can do that with Bazel, or you can play with the provided makefile until it works for you. This will be made easier in the future."]}, {"number": 18540, "title": "Tensorflow leaks 1280 bytes with each session opened and closed? (Python API)", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 17.10\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\n1.6 (also tested on 1.7)\r\n- **Python version**: \r\n3.6\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\n9.0/7.0\r\n- **GPU model and memory**:\r\nTitan XP, 12GB\r\n- **Exact command to reproduce**:\r\nTo reproduce, save the following python script as `memory_test.py`:\r\n```\r\n    import tensorflow as tf\r\n    import sys\r\n    n_Iterations=int(sys.argv[1])\r\n    def open_and_close_session():\r\n       with tf.Session() as sess:\r\n          pass\r\n    for _ in range(n_Iterations):\r\n       open_and_close_session()\r\n    with tf.Session() as sess:\r\n       print(\"bytes used=\",sess.run(tf.contrib.memory_stats.BytesInUse()))\r\n```\r\nThen run it from the command line using different number of iterations:\r\n`python memory_test.py 0`, `python memory_test.py 1`, `python memory_test.py 10`, and so on.\r\n\r\n### Describe the problem\r\nIt seems that each Tensorflow session I open and close consumes 1280 bytes from the GPU memory, which are not released until the python kernel is terminated. \r\n\r\nRunning the script given above, which simply opens and closes sessions without any further operation, yields these results:\r\n - `python memory_test.py 0` yields `bytes used= 1280`\r\n - `python memory_test.py 1` yields `bytes used= 2560`.\r\n - `python memory_test.py 10` yields `bytes used= 14080`.\r\n - `python memory_test.py 100` yields `bytes used= 129280`.\r\n - `python memory_test.py 1000` yields `bytes used= 1281280`.\r\n\r\nThe math is easy - each session opened and closed leaks 1280 bytes. I tested this script on two different ubuntu 17.10 workstations with tensorflow-gpu 1.6 and 1.7 and different NVIDIA GPUs.\r\n\r\n(here's a related [stackoverflow question](https://stackoverflow.com/q/49735217/1500585), at least one user was able to reproduce).", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nExact command to reproduce", "Thanks for the bug report.  I was able to reproduce and diagnose.  The missing memory belongs to scratch space allocated here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.cc#L292\r\n\r\nI'll start working through a change to deallocate this memory in ~BaseGPUDevice, but I'm not yet 100% sure it's safe to do so.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "The bug is still there, trying the most recent nightly build.", "A fix went into the internal version on April 26.\r\nI see it in the github repository source here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.cc#L269\r\n\r\nSo, I think it should be fixed, at least for a build from source.   "]}, {"number": 18539, "title": "Unable to convert MRCNN model to .tflite model", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra 10.13.4\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**:\r\n`python3 -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n/anaconda3/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nb'v1.7.0-1844-ga0edcf60f7' 1.7.0`\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**:\r\n`bazel version\r\nBuild label: 0.10.0-homebrew\r\nBuild target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Jan 10 02:02:06 +50057 (1517480013726)\r\nBuild timestamp: 1517480013726\r\nBuild timestamp as int: 1517480013726`\r\n- **GCC/Compiler version (if compiling from source)**:\r\n`gcc --version\r\nConfigured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.13.sdk/usr/include/c++/4.2.1\r\nApple LLVM version 9.0.0 (clang-900.0.39.2)\r\nTarget: x86_64-apple-darwin17.5.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin`\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\n`bazel-bin/tensorflow/contrib/lite/toco/toco   --input_file=./mobile_mrcnn.pb --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE --output_file=/tmp/mobilenet_v1_1.0_224.tflite --inference_type=FLOAT --input_arrays=input_image,input_image_meta --output_arrays=output_node0,output_node1,output_node2,output_node3,output_node4,output_node5,output_node6 --input_shapes=1,224,224,3:1,89`\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nWhen trying to convert an MRCNN from a frozen graph (.pb) file to (.tflite) using the tensorflow toco script, I get an `Abort trap: 6` error with no explanation. Any advice on how to debug/add unsupported ops/functionality or just what went wrong would be great.\r\n\r\n### Source code / logs\r\nError:\r\n`bazel-bin/tensorflow/contrib/lite/toco/toco   --input_file=./mobile_mrcnn.pb --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE --output_file=/tmp/mobilenet_v1_1.0_224.tflite --inference_type=FLOAT --input_arrays=input_image,input_image_meta --output_arrays=output_node0,output_node1,output_node2,output_node3,output_node4,output_node5,output_node6 --input_shapes=1,224,224,3:1,89 \r\n2018-04-15 15:48:31.302680: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: ResizeNearestNeighbor\r\n2018-04-15 15:48:31.303544: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: ResizeNearestNeighbor\r\n2018-04-15 15:48:31.303972: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: ResizeNearestNeighbor\r\n2018-04-15 15:48:31.319684: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: NonMaxSuppression\r\n2018-04-15 15:48:31.319847: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Round\r\n2018-04-15 15:48:31.319897: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Equal\r\n2018-04-15 15:48:31.319908: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Where\r\n2018-04-15 15:48:31.319921: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: GatherNd\r\n2018-04-15 15:48:31.319966: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: CropAndResize\r\n2018-04-15 15:48:31.319983: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Equal\r\n2018-04-15 15:48:31.319993: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Where\r\n2018-04-15 15:48:31.320006: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: GatherNd\r\n2018-04-15 15:48:31.320048: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: CropAndResize\r\n2018-04-15 15:48:31.320065: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Equal\r\n2018-04-15 15:48:31.320074: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Where\r\n2018-04-15 15:48:31.320087: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: GatherNd\r\n2018-04-15 15:48:31.320131: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: CropAndResize\r\n2018-04-15 15:48:31.320149: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Equal\r\n2018-04-15 15:48:31.320159: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Where\r\n2018-04-15 15:48:31.320180: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: GatherNd\r\n2018-04-15 15:48:31.320370: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: CropAndResize\r\n2018-04-15 15:48:31.416940: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: GatherNd\r\n2018-04-15 15:48:31.416977: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: GatherNd\r\n2018-04-15 15:48:31.417344: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Rint\r\n2018-04-15 15:48:31.417372: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Where\r\n2018-04-15 15:48:31.417419: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Unique\r\n2018-04-15 15:48:31.417470: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: TensorArrayV3\r\n2018-04-15 15:48:31.417531: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: TensorArrayScatterV3\r\n2018-04-15 15:48:31.417551: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: TensorArrayV3\r\n2018-04-15 15:48:31.417578: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Enter\r\n2018-04-15 15:48:31.417590: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Enter\r\n2018-04-15 15:48:31.417621: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Enter\r\n2018-04-15 15:48:31.417635: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: LoopCond\r\n2018-04-15 15:48:31.417673: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Enter\r\n2018-04-15 15:48:31.417685: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Enter\r\n2018-04-15 15:48:31.417709: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: TensorArrayReadV3\r\n2018-04-15 15:48:31.417719: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Enter\r\n2018-04-15 15:48:31.417729: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Equal\r\n2018-04-15 15:48:31.417738: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Where\r\n2018-04-15 15:48:31.417785: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Enter\r\n2018-04-15 15:48:31.417807: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Enter\r\n2018-04-15 15:48:31.417838: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: NonMaxSuppression\r\n2018-04-15 15:48:31.417853: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Enter\r\n2018-04-15 15:48:31.417942: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: PadV2\r\n2018-04-15 15:48:31.417955: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Enter\r\n2018-04-15 15:48:31.417969: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: TensorArrayWriteV3\r\n2018-04-15 15:48:31.418017: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Exit\r\n2018-04-15 15:48:31.418028: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: TensorArraySizeV3\r\n2018-04-15 15:48:31.418069: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: TensorArrayGatherV3\r\n2018-04-15 15:48:31.418957: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Where\r\n2018-04-15 15:48:31.419022: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: DenseToDenseSetOperation\r\n2018-04-15 15:48:31.419054: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: SparseToDense\r\n2018-04-15 15:48:31.419385: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Round\r\n2018-04-15 15:48:31.419425: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Equal\r\n2018-04-15 15:48:31.419434: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Where\r\n2018-04-15 15:48:31.419445: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: GatherNd\r\n2018-04-15 15:48:31.419498: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: CropAndResize\r\n2018-04-15 15:48:31.419525: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Equal\r\n2018-04-15 15:48:31.419545: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Where\r\n2018-04-15 15:48:31.419557: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: GatherNd\r\n2018-04-15 15:48:31.419597: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: CropAndResize\r\n2018-04-15 15:48:31.419612: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Equal\r\n2018-04-15 15:48:31.419620: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Where\r\n2018-04-15 15:48:31.419654: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: GatherNd\r\n2018-04-15 15:48:31.419725: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: CropAndResize\r\n2018-04-15 15:48:31.419756: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Equal\r\n2018-04-15 15:48:31.419766: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: Where\r\n2018-04-15 15:48:31.419793: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: GatherNd\r\n2018-04-15 15:48:31.419850: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: CropAndResize\r\n2018-04-15 15:48:31.445467: F tensorflow/contrib/lite/toco/tooling_util.cc:822] Check failed: d >= 1 (0 vs. 1)\r\nAbort trap: 6`\r\n\r\nSource:\r\nhttps://github.com/matterport/Mask_RCNN/blob/master/mrcnn/model.py", "comments": ["Hi @asimshankar,\r\n\r\nIt seems to be failing at `CHECK_GE(d, 1);`\r\nin\r\n\r\n`// Apply checks to arrays individually (for-each fashion).\r\n//\r\n// Check consistency of array fields, check name.\r\nvoid CheckEachArray(const Model& model) {\r\n  for (const auto& array_entry : model.GetArrayMap()) {\r\n    const auto& array = array_entry.second;\r\n    if (array->has_shape()) {\r\n      for (int d : array->shape().dims()) {\r\n        CHECK_GE(d, 1);\r\n      }\r\n    }\r\n    // It's OK to have a buffer or an alloc, but not both.\r\n    // (Since allocs are for transient arrays without a buffer).\r\n    CHECK(!array->buffer || !array->alloc);\r\n    if (array->buffer) {\r\n      // If there is a buffer, its type should be consistent with data_type.\r\n      CHECK(array->buffer->type == array->data_type);\r\n      // The presence of a fixed buffer should imply the presence of a fixed\r\n      // shape.\r\n      CHECK(array->has_shape());\r\n      // The shape flat-size should agree with the buffer length.\r\n      CHECK_EQ(array->buffer->Length(),\r\n               RequiredBufferSizeForShape(array->shape()));\r\n    }\r\n\r\n    // Check name.  Either \"name_with_suffix_8\", \"name_with_port:3\", but not\r\n    // \"name_with_both:3_8\".\r\n    const string& name = array_entry.first;\r\n    auto colon_pos = name.find_first_of(\":\");\r\n    if (colon_pos != string::npos) {\r\n      CHECK_EQ(name.substr(colon_pos + 1).find_first_not_of(\"0123456789\"),\r\n               string::npos)\r\n          << \"Array name must only have digits after colon\";\r\n    }\r\n    CHECK_GT(colon_pos, 0)\r\n        << \"First character of array name must not be a colon.\";\r\n  }\r\n}`\r\n\r\nIs the error produced when converting this instance of the CropAndResize operation? Also, are there any tools you recommend for narrowing down the error, ie. valgrind or some toco specific util?", "Using the order of operations, I'm pretty sure I've been able to narrow down to which crop_and_resize operation is causing the issue (assuming it is that function), and have seen that it might be the 4th tensor param that has 0 dim. Any suggestions on how to debugs? @andrehentz @aselle @asimshankar @suharshs ", "After some digging, I found that it was due to \r\n`\r\nAllocateTransientArrays(model, kDefaultTransientDataAlignment);` \r\nfrom\r\n```c\r\nif (SupportsPreallocatedWorkspace(output_format)) { \r\n   AllocateTransientArrays(model, kDefaultTransientDataAlignment);\r\n   LogDump(kLogLevelModelChanged, \"AFTER ALLOCATION\", *model); \r\n}\r\n```\r\nin the `void Transform(const TocoFlags& toco_flags, Model* model)` call in `ToolMain`\r\nand not from \r\n`Import(toco_flags, model_flags, graph_def_contents);` in `ToolMain`.\r\n\r\nHere's my output:\r\n\r\n...\r\ndimension 0size: 1\r\ndimension 0size: 4\r\n2018-05-22 21:55:46.128895: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 564 operators, 1155 arrays (0 quantized)\r\nfrom Transform\r\nfrom AllocateTransientArrays\r\nallocating mrcnn_detection/map/while/NextIteration\r\nfrom AllocateTransientArray\r\nfrom TransientArraySize\r\ndimension 0size: 1\r\ndimension 1size: 1\r\ndimension 2size: 1\r\ndimension 3size: 0\r\n2018-05-22 21:55:46.141249: F tensorflow/contrib/lite/toco/tooling_util.cc:664] Check failed: dim >= 1 (0 vs. 1)\r\nAbort trap: 6\r\n\r\n\r\nIs this expected with the `while` op since it is not implemented for lite yet and is it alright if I comment out the `SupportsPreallocatedWorkspace(output_format)` conditional block?", "@Cpruce \r\nHi Do you find any solution for this specific issues? it seems like I have a similar issue.\r\n\r\nF tensorflow/contrib/lite/toco/tooling_util.cc:664] Check failed: dim >= 1 (0 vs. 1)\r\n", "@offthewallace \r\nMy workaround was to comment out the `SupportsPreallocatedWorkspace(output_format)` if clause since this is likely an unintentional fault. However, I haven't continued further since my model would need several ops implemented for lite. ", "I apologize, but your link to the source code is now broken. Could you fix it, and I will take a look.", "Hi, I have similar problem with SSD_MobileNet trained with tensorflow object detection API.", "@aselle https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/model.py", "Thanks, I'll take a look when I can.", "Hi, I have same problem with SSD_MobileNet trained with tensorflow object detection API.", "I too am seeing this with ssd_mobilenet in tf od api. Same message, different location", "@xiaofansong try export_tf_lite_ssd_graph https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193", "+1 to this question \"Is this expected with the while op since it is not implemented for lite yet and is it alright if I comment out the SupportsPreallocatedWorkspace(output_format) conditional block?\" I'm running into the same thing with a custom model. Thanks!", "This might be because \"while\" is being used.\r\n\r\nWhat is happening is that the conversion is trying to calculate the size of a given tensor based on its shape, which is [1, 1, 1, 0]. That means the tensor has no elements and the memory planning code doesn't like that. It is possible that shape inference in the presence of \"while\" yields this type of zero-element tensor.\r\n\r\nWe are working to get some form of flow control working in TF Lite. Until then I'm afraid it will be quite hard to convert models that contain \"while\" ops.", "For CropAndResize op or other unsupported ops, please look at this [reference](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/using_select_tf_ops.md)\r\n\r\nAs we work toward fleshing out the builtin op library for TensorFlow Lite, we've been working on an experimental feature that allows using select TensorFlow ops from within the TensorFlow Lite runtime. The goal is to help reduce some of the friction for using models that rely on ops not yet natively supported by TensorFlow Lite (at the cost of increased binary size). This feature requires opting in during model conversion, as well as adding an additional dependency. More details can be found here.\r\n\r\nFeedback is very much appreciated (either via GitHub or directly via tflite@tensorflow.org), and we'll be adding and refining functionality over the coming weeks. Cheers.", "Thanks, @achowdhery . Did you mean for \"here\" in \"More details can be found here.\" to be a link we can look at?", "> @xiaofansong try export_tf_lite_ssd_graph https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193\r\n\r\n@Cpruce hi, have you convert MRCNN model to .tflite successfully?", "Closing due to lack of activity. There has been significant progress on improving conversion support over the past year."]}, {"number": 18538, "title": "AttributeError: module 'tensorflow' has no attribute 'Session'", "body": "When I call any function in python3.6, I get the error below; however, it works fine in python3.4. Any idea? \r\nimport tensorflow as tf \r\ntf.Session()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute 'Session'\r\n>>> \r\nHere is my  System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian 8.7 \r\n- **TensorFlow installed from (source or binary)**: by pip3\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**: 3.6.5\r\n- **CUDA/cuDNN version**: cuda 9.0 and cudnn 7.0\r\n- **GPU model and memory**: K80, 12 GB \r\n- **Exact command to reproduce**:\r\nimport tensorflow as tf \r\ntf.Session()", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nBazel version", "Have I written custom code: Yes. But I get the error in either of the codes in tensorflow models. \r\nBazel version: I did not install tensorflow from source, so I did not install Bazel. ", "Try reinstalling or upgrading tensorflow-gpu.\r\n\r\n`pip install --upgrade tensorflow-gpu`\r\n", "`pip3 install --upgrade --force-reinstall tensorflow-gpu` worked for me\r\n", "@oroojlooy is this still an issue for you? Could you check if there is tensorflow folder or tensorflow.py from where you were seeing the error? ", "@yifeif No, I installed a newer version of Python3.6, then installed tensorflow and it works.", "Nagging Assignee @yifeif: It has been 66 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I close the issue. ", "Can someone help me? I am getting the same issue.\r\nI have already tried both ways as told by 43061b4a and aznshodan but the problem still persists.\r\n", "Please open a new issue, filling in the template.\r\n\r\nNote that 1.7 is very old and outside of support window. You should consider using 2.0 in which case Session is no longer needed.", "thanks @mihaimaruseac \r\nI was encountering this on Tensorflow 2.0 only.\r\n\r\nAs i am enrolled in a bootcamp so they are teaching us using previous version but in my PC i was getting error as I didn't know that 2.0 don't support Session.\r\n\r\nNote:Sorry for grammatical mistakes.", "I solved this problem by unistall the tensorflow\uff0cand install tensorflow by this \"conda create -n tensorflow_env tensorflow\"\u3002And then use\"conda activate tensorflow_env\"", "If tf.Session() is deprecated, Please use **tf.compat.v1.Session()** instead", "> If tf.Session() is deprecated, Please use **tf.compat.v1.Session()** instead\r\n\r\nsess= tf.compat.v1.Session()\r\nsess.run()  is not working. its saying, \"The Session graph is empty.  Add operations to the graph before calling run()\"", "> sess= tf.compat.v1.Session()\r\n> sess.run() is not working. its saying, \"The Session graph is empty. Add operations to the graph before calling run()\"\r\n\r\n@rj1346 try the following, it worked for me\r\n\r\nimport tensorflow as tf\r\nwith tf.compat.v1.Session() as sess:\r\n    a = tf.constant(3.0)\r\n    b = tf.constant(4.0)\r\n    c = a+b\r\n    print(sess.run(c))\r\n    sess.close()\r\n\r\n\r\n\r\n\r\n", "> > If tf.Session() is deprecated, Please use **tf.compat.v1.Session()** instead\r\n> \r\n> sess= tf.compat.v1.Session()\r\n> sess.run() is not working. its saying, \"The Session graph is empty. Add operations to the graph before calling run()\"\r\n\r\nyou are getting this error because of the indentation \r\n", "Same error using ELMo. It seems solved using tensorflow==1.15, that includes CPU and GPU packages.", "Guys, this issue is solved and for a very old version of TF. Please open new issue, no one is really watching the closed issues", "Good thread for noobies like me. Was following some old doccos on how to use Tf and stumbled into this issue. Mihai, you helped a lot. Merci si sanatate!", "tensorflow 2.x \r\n\r\nimport tensorflow as tf\r\nhello = tf.constant('Hello, TensorFlow!')\r\nsess = tf.Session()\r\nprint(sess.run(hello))\r\n", "There is no session in TF 2.x.\r\n\r\nLocking conversation as this has been already answered multiple times."]}, {"number": 18537, "title": "Slow matrix multiplication using Tensorflow 1.7.0 on a GPU", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10 Home (64 bit) Version 1709 OS Build 16299.371\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary (Tensorflow GPU)\r\n- **TensorFlow version (use command below)**:\r\nPS C:\\Users\\gautam> python\r\nPython 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:54:40) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> print(tf.__version__)\r\n1.7.0\r\n- **Python version**: \r\n3.6.4\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n**CUDA:**\r\nPS C:\\Users\\gautam> nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2017 NVIDIA Corporation\r\nBuilt on Fri_Sep__1_21:08:32_Central_Daylight_Time_2017\r\nCuda compilation tools, release 9.0, V9.0.176\r\n**cuDNN:**\r\n#define CUDNN_MAJOR 7\r\n#define CUDNN_MINOR 0\r\n#define CUDNN_PATCHLEVEL 5\r\n\r\n- **GPU model and memory**:\r\nNvidia GTX 1050, 4GB\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI have tensorflow-gpu installed on my Dell XPS 15 laptop running Windows 10 (environment specified above). I tried to compare matrix multiplication performance between np.matmul() and tf.matmul() for the graph mode of execution. The numpy method returns the result in about 13.5 seconds whereas the tensorflow method takes a long time and eventually fails.\r\n\r\n### Source code / logs\r\nI generated input matrices in the following way:\r\nx = np.random.random((10000,10000))\r\ny = np.random.random((10000,10000))\r\n\r\nHere are the results:\r\n1. np.matmul(x,y) takes about 13.5 seconds\r\n2. The following takes a long time and eventually errors out:\r\nwith tf.Session() as sess:\r\n    z = tf.matmul(x,y).eval()\r\n2018-04-15 11:35:05.157088: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1344] Found device 0 with properties:\r\nname: GeForce GTX 1050 major: 6 minor: 1 memoryClockRate(GHz): 1.493\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 4.00GiB freeMemory: 3.30GiB\r\n2018-04-15 11:35:05.163665: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1423] Adding visible gpu devices: 0\r\n2018-04-15 11:35:07.338162: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-04-15 11:35:07.348029: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:917]      0\r\n2018-04-15 11:35:07.353379: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:930] 0:   N\r\n2018-04-15 11:35:07.363613: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3033 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-04-15 11:36:56.906874: E T:\\src\\github\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:1110] could not synchronize on CUDA context: CUDA_ERROR_LAUNCH_FAILED ::\r\n2018-04-15 11:36:56.906874: E T:\\src\\github\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED\r\n2018-04-15 11:36:56.915430: F T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_event_mgr.cc:203] Unexpected Event status: 1\r\n", "comments": ["Instead of generating the input matrices using numpy, have you checked to see that using `tf.random_uniform` to generate the input matrices produces the same result?\r\n", "Thanks @Benyuel! That works and is about 10x as fast as the np.matmul() and I do see spikes in the GPU compute usage. In this case the random numbers were generated in the GPU, which is fine for this toy example. For real tasks, the inputs will be generated by the CPU and I'm hoping that the computations will be way more than the rate of feeding inputs - will pay more with the tf.dataset API. The CUDA error might be a bug - I don't see it for smaller input shapes.", "@gskulkarni : Here's how you would generate on CPU but perform the matmul on GPU if you want to test:\r\n```\r\nwith tf.device('/device:CPU:0'):\r\n    x = tf.random_uniform...\r\n    y = ...\r\n\r\nwith tf.device('/device:GPU:0'):\r\n    z = tf.matmul(x, y)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(z)\r\n```\r\n\r\nI think you may be getting the CUDA_ERROR_LAUNCH_FAILED because you ran out of memory copying `np.random.random` matrices on your GPU.  The `np.random.random` matrix is by default `np.float64` which creates a subsequent graph bigger than 2GB and errors out for me.  If I do `np.random.random((10000, 10000)).astype(np.float32)`, I can fit it in memory and compute.  Typically, it is better to use tf.float32 for speed with the price of a possible negligible loss of precision.", "@gskulkarni, When benchmarking, you typically want to run several warmup iterations before starting the timer. This is because TensorFlow has to do more work for the first `sess.run()` calls, such as optimizing the graph. You should also run the session call for several iterations in a loop, to reduce variance.\r\n\r\nThe CUDA_ERROR_LAUNCH_FAILED error is a separate issue than performance. Can you post a stand-alone program to reproduce? @Benyuel, I think the Tensors are too small to cause an out of memory error, because a 10000 x 10000 float64 matrix is only 80 MB. But you are right it's typically better to use float32.", "@Benyuel Thanks for the suggestion - that works for me. However, I don't see the GPU under memory pressure. @reedwm - thanks for your pointers. Here's what you can do to reproduce the error:\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nx = np.random.random((10000,10000))\r\ny = np.random.random((10000,10000))\r\n\r\nwith tf.Session() as sess:\r\n    z = tf.matmul(x, y).eval()\r\n", "I cannot reproduce. @mrry, any ideas why the above code snippet is getting the error \"could not synchronize on CUDA context: CUDA_ERROR_LAUNCH_FAILED\"?", "No idea. Perhaps someone with more GPU familiarity could comment.", "Perhaps related: https://github.com/tensorflow/tensorflow/issues/1450\r\nhttps://github.com/tensorflow/tensorflow/issues/6783\r\nhttps://github.com/tensorflow/tensorflow/issues/10852", "/CC @zheng-xq any ideas?", "@gskulkarni Is this still an issue ?\r\nWe see that you are using old version of tensorflow 1.x which is not actively supported, We recommend that you upgrade to 2.4 or later version.Please have a look at the [migration](https://www.tensorflow.org/guide/migrate) guide for reference to migrate from TensorFlow 1.x to TensorFlow 2.Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 18536, "title": "DNNRegressor.predict issue", "body": "I have trained a Deep Neural Network Regressor on some weather data. When I tried classifier.predict(), it return a generator object. Usually what we do is to put list() over the object to get the prediction. But it is not working.\r\nI was unable to get the my code to turn into it's code form with the `` symbol by the way. So bare with me.\r\n\r\nimport numpy as np\r\nimport time\r\nimport itertools\r\nfrom onehotencode import load_single_data\r\nfrom onehotencode import training_week_data,training_week_price,\\\r\n    testing_week_data,testing_week_price,uber_price\r\nimport tensorflow as tf\r\n\r\nfeature_columns = [tf.feature_column.numeric_column(\"x\", shape=[163])]\r\nclassifier = tf.estimator.DNNRegressor(feature_columns=feature_columns,\r\n                                       hidden_units=[200, 175, 150, 125, 100, 75, 50, 25, 17, 10, 8, 5, 3],\r\n                                        model_dir='great_model/'\r\n                                        )\r\nonehot,price=load_single_data([[5,18,16,1],'Mostly Sunny','Mostly Sunny',46.5])\r\n\r\nmy_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={\"x\": np.array(onehot)},\r\n        y=None,\r\n        num_epochs=None,\r\n        shuffle=False)\r\ny = classifier.predict(input_fn=my_input_fn)\r\n#This line produced the error\r\nprint(list(y))\r\n\r\nThe error produced is :\r\n\r\n`ValueError: Dimension size must be evenly divisible by 163 but is 128 for 'dnn/input_from_feature_columns/input_layer/x/Reshape' (op: 'Reshape') with input shapes: [128,1], [2] and with input tensors computed as partial shapes: input[1] = [?,163].`\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "No, thanks for the support.", "Nagging Assignee @cy89: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 18535, "title": "Current Bazel version is 0.12.0, expected at least 0.4.2", "body": "Problem when building tensorflow from source. I have been following the guides from the tensorflow official website but apparently they are not in sync.\r\n\r\nTried to get the latest Bazel and build it myself , I found out the 0.12.0 was the latest on Bazel github. \r\n\r\nHave I written custom code: NO\r\nOS Platform and Distribution: Linux Ubuntu 16.04\r\nTensorFlow installed from: source\r\nTensorFlow version: 1.7\r\nCUDA/cuDNN version: No\r\nGPU model and memory: No\r\nExact command to reproduce: ./configure", "comments": ["Origin of the problem : The latest  Bazel `v0.12.0`.\r\n\r\nHow I solved it : Installed the` 0.11` version, everything is now fine.", " eddywm that doesn't work for me"]}, {"number": 18534, "title": "ValueError: An initializer for variable conv2d/kernel of <dtype: 'string'> is required", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nno\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 17.10\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.7 cpu only\r\n- **Python version**: \r\n3.6.5\r\n- **Bazel version (if compiling from source)**:\r\nno\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI want to train a CNN model classify images use keras model. Train data import from CSV files via pandas.  the dataset i frist want to change to float32 and split by ','but system return error, i change back to string  split by ' ', got error as below.\r\n\r\nWhen I call the train method, an error is thrown.\r\n\r\nHow can I fix this?\r\n\r\nimage size 48*48\r\n\r\n### Source code / logs\r\nmodel:\r\nimport numpy as np\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\nfrom PIL import Image\r\n#import csv\r\nfrom numpy import imag, float32\r\n\r\nimport tensorflow as tf\r\nfrom math import floor\r\nimport sys\r\n\r\n\r\nTRAIN_CSV_COLUMN_NAMES=['label','feature']\r\nTEST_CSV_COLUMN_NAMES=['id','feature']\r\ntrainEpochs = 40\r\nbatchSize = 100\r\nLEARNING_RATE = 1e-4\r\n\r\nclass Model(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self._input_shape = [-1,48,48,1]\r\n        self.conv1 = tf.layers.Conv2D(32,5,padding = 'same',data_format = 'channels_last',activation=tf.nn.relu)\r\n        self.conv2 = tf.layers.Conv2D(32,5,padding = 'same',data_format = 'channels_last',activation = tf.nn.relu)\r\n        self.conv3 = tf.layers.Conv2D(64,5,padding = 'same',data_format = 'channels_last',activation = tf.nn.relu)\r\n        self.max_pool2d = tf.layers.MaxPooling2D((2,2),(2,2),padding = 'same', data_format = 'channels_last')\r\n        self.fc1 = tf.layers.Dense(1600,activation=tf.nn.relu)\r\n        self.fc2 = tf.layers.Dense(7)\r\n        self.dropout = tf.layers.Dropout(0.4)\r\n    def __call__(self,input,training):\r\n        y=tf.reshape(input,self._input_shape)\r\n        y=self.conv1(y)#error appear here\r\n        y=self.max_pool2d(y)\r\n        y=self.conv2(y)\r\n        y=self.max_pool2d(y)\r\n        y=self.conv3(y)\r\n        \r\n        y=tf.layers.flatten(y)\r\n        y=self.fc1(y,activation = tf.nn.relu)\r\n        y=self.dropout(y,training=training)\r\n        return self.fc2(y)\r\n\r\n\r\n#load data from csv    \r\ndef dataLoad():\r\n    oragTrainData = pd.read_csv('train.csv',names = TRAIN_CSV_COLUMN_NAMES ,header = 0)\r\n    oragTestData = pd.read_csv('test.csv', names = TEST_CSV_COLUMN_NAMES, header = 0)\r\n    trainData,valiData =  splitValid(oragTrainData,0.2)\r\n    trainData,labels = trainData,trainData.pop('label')\r\n    valiData,vaLabels = valiData, valiData.pop('label')\r\n    testData,tId = oragTestData,oragTestData.pop('id')\r\n    return trainData,testData,valiData,labels,vaLabels,tId\r\n#input function\r\n def trainDataInputFn(): \r\n        features = tf.data.Dataset.from_tensor_slices(trainData)\r\n        labels = tf.data.Dataset.from_tensor_slices(trainLabels)\r\n        ds = tf.data.Dataset.zip((features,labels))\r\n        ds=ds.cache().shuffle(5000).batch(batchSize)\r\n        ds = ds.make_one_shot_iterator().get_next()\r\n        return ds\r\n#model_fn function\r\ndef model_fn(features,labels,mode,params):\r\n    model = Model()\r\n    images = features\r\n    if isinstance(images,dict):\r\n        images = images['feature']\r\n    \r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\r\n        logists = model(images, training=True)\r\n        loss = tf.losses.sparse_softmax_cross_entropy(labels = labels, logits=logists)\r\n        accuracy = tf.metrics.accuracy(labels=labels, predictions = logists)\r\n        \r\n        tf.identity(LEARNING_RATE, 'learning_rate')\r\n        tf.identity(loss,'loss')\r\n        tf.identity(accuracy,'accuracy')\r\n        \r\n        tf.summary.scalar(accuracy[1],name='training accuracy')\r\n        print('9999999999999999999999999999999999999999999999999999')\r\n        return tf.estimator.EstimatorSpec(mode = tf.estimator.ModeKeys.TRAIN,\r\n                                          loss = loss,\r\n                                          train_op = optimizer.minimize(loss, tf.train.get_or_create_global_step()))\r\npass to model data format:\r\n[[b'35 7 7 7 5 10 16 8 30 66 111.....48 54 56 53 67']\r\n...\r\n[b'249 249 249 249 247 254 ......143 148 97 98 59 57']]\r\n\r\nerror information\r\nTraceback (most recent call last):\r\n  File \"emontionClassfy.py\", line 218, in <module>\r\n    main(argv=sys.argv)  \r\n  File \"emontionClassfy.py\", line 206, in main\r\n    emontionClassify.train(input_fn=trainDataInputFn)\r\n  File \"/home/bruce/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 355, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/bruce/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 824, in _train_model\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"/home/bruce/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 805, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"emontionClassfy.py\", line 134, in model_fn\r\n    logists = model(images, training=True)\r\n  File \"emontionClassfy.py\", line 45, in __call__\r\n    y=self.conv1(y)\r\n  File \"/home/bruce/tensorflow/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 696, in __call__\r\n    self.build(input_shapes)\r\n  File \"/home/bruce/tensorflow/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py\", line 144, in build\r\n    dtype=self.dtype)\r\n  File \"/home/bruce/tensorflow/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 546, in add_variable\r\n    partitioner=partitioner)\r\n  File \"/home/bruce/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/checkpointable.py\", line 415, in _add_variable_with_custom_getter\r\n    **kwargs_for_getter)\r\n  File \"/home/bruce/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1297, in get_variable\r\n    constraint=constraint)\r\n  File \"/home/bruce/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1093, in get_variable\r\n    constraint=constraint)\r\n  File \"/home/bruce/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 439, in get_variable\r\n    constraint=constraint)\r\n  File \"/home/bruce/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 408, in _true_getter\r\n    use_resource=use_resource, constraint=constraint)\r\n  File \"/home/bruce/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 773, in _get_single_variable\r\n    name=name, shape=shape, dtype=dtype)\r\n  File \"/home/bruce/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 853, in _get_default_initializer\r\n    % (name, dtype.base_dtype))\r\nValueError: An initializer for variable conv2d/kernel of <dtype: 'string'> is required\r\n", "comments": ["as i analys the data format between MNIST and my model, i change the data structure , the problem solved."]}, {"number": 18533, "title": "Update pydoc for several tfdbg hooks", "body": "For class:\r\nSpecify that tfdbg.LocalCLIDebugHook can also be used to debug instances\r\nof tf.estimator.Estimator.\r\n\r\nFor __init__ method:\r\nClarify purpose of ui_type argument.", "comments": ["I did a commit --amend, as I think it will look better in the tensorflow history. Hope that is ok."]}, {"number": 18532, "title": "Error fetching values from variables - queue behavior semantics needs clarification", "body": "I am generating the following traceback when I try to feed data to the queue.\r\n\r\nHave I written custom code : NO\r\nOS Platform and Distribution : Ubuntu 16.04\r\nTensorFlow installed from : pip\r\nTensorFlow version: 1.4.1\r\nBazel version : N/A\r\nCUDA/cuDNN version : CUDA Version 8.0.61\r\nGPU model and memory : GTX 680 - 4 gb memory\r\nExact command to reproduce : detaliled in description\r\n\r\nThe error message is very long so let me try to break it down\r\n\r\nThe variable is declared as follows \r\n\r\n    a = tf.placeholder(tf.float32, shape=[1,2], name='a')\r\n\r\nThe variable is fed using a feed_dict as follows\r\n\r\n     _,X_hat_val,loss_val = sess.run([train,X_hat,loss],  \r\n                            feed_dict={X : np.array([[ x_ord[0,0],y_ord[0,0]]]), \r\n                                      a : np.array([[ x_acc[0,0], y_acc[0,0] ]]),\r\n                                      a_prev : np.array([[ out_xacc[0,0], out_yacc[0,0] ]]) })\r\n\r\nThe variable is printed before feeding to the queue:\r\n\r\n    [[ 0.  0.]] # the value of the variable\r\n    (1, 2) # the shape of the variable\r\n    float32 # the data type of the variable\r\n\r\nThe error is generated at \r\n\r\n    tensorflow.python.framework.errors_impl.InvalidArgumentError: \r\n    You must feed a value for placeholder tensor 'a' with dtype float and shape [1,2]\r\n\r\nHowever, when I remove an operation to fetch the variable value it works perfectly\r\n\r\nprint(sess.run(a))\r\n\r\nI am not sure if the queue mechanism is seeking out a new value when I run the operation, or if the semantics of the operation is not what I think it is\r\n\r\n    ==========      Training Model  ==========\r\n    \r\n    \r\n    /home/kiran/projects/Kalman/data/S10_05_19_2017_train_mouse_subjects.tfrecords\r\n    Running Model\r\n    \r\n    \r\n    \r\n    \r\n    **********Running Graph with a session**********\r\n    \r\n    \r\n    \r\n    \r\n    2018-04-15 09:07:11.227273: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\r\n    2018-04-15 09:07:11.263222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning\r\n     NUMA node zero\r\n    2018-04-15 09:07:11.263499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\r\n    name: GeForce GTX 680 major: 3 minor: 0 memoryClockRate(GHz): 1.0845\r\n    pciBusID: 0000:02:00.0\r\n    totalMemory: 3.93GiB freeMemory: 3.49GiB\r\n    2018-04-15 09:07:11.263522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 680, pci bus id: 0000:02:00.0, compute capa\r\n    bility: 3.0)\r\n    19999\r\n    [[ 0.  0.]]\r\n    (1, 2)\r\n    float32\r\n    Processing record :  0\r\n    \r\n    \r\n    2018-04-15 09:07:11.871973: W tensorflow/core/kernels/queue_base.cc:295] _0_input_producer: Skipping cancelled enqueue attempt with queue not closed\r\n    Traceback (most recent call last):\r\n      File \"/home/kiran/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\r\n        return fn(*args)\r\n      File \"/home/kiran/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\r\n        status, run_metadata)\r\n      File \"/home/kiran/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n        c_api.TF_GetCode(self.status.status))\r\n    tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'a' with dtype float and shape [1,2]\r\n             [[Node: a = Placeholder[dtype=DT_FLOAT, shape=[1,2], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n             [[Node: a/_69 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_4_a\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n    \r\n    During handling of the above exception, another exception occurred:\r\n    \r\n    Traceback (most recent call last):\r\n      File \"KalmanModel.py\", line 378, in <module>\r\n        training(subject)\r\n      File \"KalmanModel.py\", line 242, in training\r\n        print(sess.run(a))\r\n      File \"/home/kiran/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 889, in run\r\n        run_metadata_ptr)\r\n      File \"/home/kiran/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\r\n        feed_dict_tensor, options, run_metadata)\r\n      File \"/home/kiran/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n        options, run_metadata)\r\n      File \"/home/kiran/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\r\n        raise type(e)(node_def, op, message)\r\n    tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'a' with dtype float and shape [1,2]\r\n             [[Node: a = Placeholder[dtype=DT_FLOAT, shape=[1,2], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n             [[Node: a/_69 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_4_a\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n    \r\n    Caused by op 'a', defined at:\r\n      File \"KalmanModel.py\", line 378, in <module>\r\n        training(subject)\r\n      File \"KalmanModel.py\", line 159, in training\r\n        a = tf.placeholder(tf.float32, shape=[1,2], name='a')\r\n      File \"/home/kiran/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1599, in placeholder\r\n        return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\r\n      File \"/home/kiran/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3091, in _placeholder\r\n        \"Placeholder\", dtype=dtype, shape=shape, name=name)\r\n      File \"/home/kiran/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n        op_def=op_def)\r\n      File \"/home/kiran/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\r\n        op_def=op_def)\r\n      File \"/home/kiran/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\r\n        self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n    \r\n    InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'a' with dtype float and shape [1,2]\r\n             [[Node: a = Placeholder[dtype=DT_FLOAT, shape=[1,2], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n             [[Node: a/_69 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_4_a\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\n", "comments": ["In your post above I don't see the actual TF program where the queue is defined and used, so it's hard to say what might be going on.", "Hi, thanks for your reply. I am not sure what aspects define the queue internally but let me try to highlight the aspects\r\n\r\n```\r\n    # create a string input producer to read the tfrecords\r\n    filename_queue = tf.train.string_input_producer([filenames_train], shuffle=False)\r\n    ...decoded vars... = read_and_decode(filename_queue)  \r\n\r\n    # create a batch to train the model\r\n    batch = tf.train.batch([...decoded vars... ], \r\n                           batch_size=1, capacity=2000, num_threads=1) \r\n```\r\n\r\nI process this batch and feed the variables\r\n```\r\nx_ord,y_ord,x_acc,y_acc,out_x,out_y,out_xacc,out_yacc = sess.run(batch)\r\n                               \r\n_,X_hat_val,loss_val = sess.run([train,X_hat,loss],  \r\n                        feed_dict={X : np.array([[ x_ord[0,0],y_ord[0,0]]]), \r\n                                  a : np.array([[ x_acc[0,0], y_acc[0,0] ]]),\r\n                                  a_prev : np.array([[ out_xacc[0,0], out_yacc[0,0] ]]) })\r\n```", "Can you post a simple reproducible example?", "Hi, I will post a reproducible example soon. I have been busy dealing with other commitments. Sorry for the delay.", "Nagging Assignee @poxvoculi: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'm going to close this for now.  If it continues to be a problem and you can show a small reproducible example that suggests a bug, please reopen."]}, {"number": 18531, "title": "Issues running tensorflow-GPU: Couldn't open CUDA library libcuda.so.1.", "body": "Have I written custom code: No\r\nOS Platform and Distribution: Scientific Linux release 6.9 (Carbon)\r\nTensorFlow installed from: wheel\r\nTensorFlow version: 0.11.0rc0\r\nBazel version: N/A\r\nCUDA/cuDNN version: 7.5 / N/A\r\nGPU model and memory: Tesla K20m 5GB\r\nExact command to reproduce: Error occurs when importing tensorflow \r\n\r\nI am having issues running tensorflow-gpu on the cluster computer at my university. There are a number of Tesla K20m GPUs available. \r\n \r\nThe system only has CUDA 7.5 installed so I installed tensorflow in a conda environment using a wheel from version 0.11 which supports CUDA 7.5. I also needed to use the dirty trick posted [here](https://stackoverflow.com/questions/33655731/error-while-importing-tensorflow-in-python2-7-in-ubuntu-12-04-glibc-2-17-not-f?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa) to get this wheel to work, as we have GLIC_2.12 installed. However I don't think the above is causing the problem I am now facing. \r\n\r\nWhen I try and run Tensorflow I get the following error:\r\n\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:105] Couldn't open CUDA library libcudnn.so. LD_LIBRARY_PATH: /cm/shared/apps/cuda/7.5/lib64:/cm/shared/apps/cuda/7.5/targets/x86_64-linux/lib\r\nI tensorflow/stream_executor/cuda/cuda_dnn.cc:3448] Unable to load cuDNN DSO\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:105] Couldn't open CUDA library libcuda.so.1. LD_LIBRARY_PATH: /its/home/tjb32/new-tensorflow-workspace/glib-download/libc6_2.17/lib/x86_64-linux-gnu/:/its/home/tjb32/new-tensorflow-workspace/glib-download/libc6_2.17/usr/lib64/:/cm/shared/apps/cuda/7.5/lib64:/cm/shared/apps/cuda/7.5/targets/x86_64-linux/lib\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:160] hostname: node152\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:185] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] kernel reported version is: Permission denied: could not open driver version path for reading: /proc/driver/nvidia/version\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1080] LD_LIBRARY_PATH: /its/home/tjb32/new-tensorflow-workspace/glib-download/libc6_2.17/lib/x86_64-linux-gnu/:/its/home/tjb32/new-tensorflow-workspace/glib-download/libc6_2.17/usr/lib64/:/cm/shared/apps/cuda/7.5/lib64:/cm/shared/apps/cuda/7.5/targets/x86_64-linux/lib\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1081] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so.1; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:491] failed call to cuInit: CUDA_ERROR_NO_DEVICE\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:140] kernel driver does not appear to be running on this host (node152): /proc/driver/nvidia/version does not exist\r\n\r\n```\r\n\r\nI know cuDNN isn't installed on the system, but it seems the fatal error is the fact the tenorflow cannot find libcuda.so.1.\r\n\r\nRunning `locate libcuda*` on a login node returns the following: `/usr/lib64/nvidia/libcuda.so\r\n/usr/lib64/nvidia/libcuda.so.1\r\n/usr/lib64/nvidia/libcuda.so.384.98`\r\n\r\nI can also find libcuda.so if I navigate to the `/cm/shared/apps/conda/7.5/lib64/stubs`. \r\n\r\nHowever if I add `locate libcuda*` to the job I submit to the GPU queue it returns nothing. So adding the `/usr/lib64/nvidia` to my path does not solve the issue. It seems that the GPU nodes do not have access to llibcuda.so leading CUDA to believe there is no GPU device. \r\n\r\nAm I missing some additional configuration that I need to do to get Tensorflow working? Or is this more likely an issue with the underlying CUDA installation that I will need to get an system admin to help with?\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "This looks to me like something you'll need a sysadmin to help with. @gunan, any quick thoughts on this?", "Maybe you need to set your LD_LIBRARY_PATH to include the directory for libcuda?\r\nWe use that environment variable to find all cuda .so files.", "As this issue has invited community support, please remove the assignee. Otherwise, remove the `community support` label. Thank you.", "As this issue has invited community support, please remove the assignee. Otherwise, remove the `community support` label. Thank you."]}, {"number": 18530, "title": "DLL load failed on Windows 10 with tensorflow 1.7", "body": "Hello to all,\r\non my Windows 10, I installed tensorflow 1.5 and it works, but if I try to upgrade to tensorflow 1.7 it don't works, appeare the error:\r\n**ImportError: DLL load failed\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'**\r\n\r\nMicrosoft C++ Runtime is updated\r\n\r\nIf I downgrade to the version 1.5 it works again\r\nHow can I solve it ?\r\n\r\nThanks\r\nSergio\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Many thanks for your help\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\n1.7\r\n- **Python version**: \r\n3.6.5\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\nfrom command line: pip install --upgrade tensorflow\r\nfrom python: import tensorflow\r\n\r\n\r\n", "@gunan do you have ideas? Otherwise this will probably rely on community support.", "Just for reference, I'm experiencing very similar issue: https://github.com/tensorflow/tensorflow/issues/18503\r\n@sirjo66 posted the link for reference, but for some reason GitHub didn't detect it.", "What is your CPU model?\r\nThis is almost certainly an isue with an old CPU, that does not have AVX instruction set.", "I bought this PC one month ago\r\nIntel Celeron CPU J3355 @ 2.00 Ghz\r\nbut, from specification it don't have AVX  :-(\r\n\r\nsee it: [http://www.cpu-world.com/Compare/426/Intel_Celeron_J3355_vs_Intel_Core_i5_i5-7600.html](http://www.cpu-world.com/Compare/426/Intel_Celeron_J3355_vs_Intel_Core_i5_i5-7600.html) \r\n\r\n", "@sirjo66 \r\n\r\nYour link leads to wrong url.\r\n\r\n```\r\nWrong:   [http://www.example.com/](url)\r\nCorrect: [url](http://www.example.com/)\r\n```\r\n\r\nCorrect url is: [http://www.cpu-world.com/Compare/426/Intel_Celeron_J3355_vs_Intel_Core_i5_i5-7600.html](http://www.cpu-world.com/Compare/426/Intel_Celeron_J3355_vs_Intel_Core_i5_i5-7600.html)", "Nagging Assignee @gunan: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @gunan: It has been 32 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "duplicate of #19584", "This may help someone. \r\nI am a Windows 10 user with TF v1.13\r\n- After installing TF with pip, I experience this error.\r\n\r\nHere is what worked for me:\r\n- I already had Anaconda installed\r\n- I ran `conda install tensorflow` and it updated a few packages and installed some. And everything worked fine. \r\n\r\n**NOTE**: I did everything within a conda environment."]}, {"number": 18529, "title": "UnimplementedError: Cast uint8 to uint32 is not supported", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.7.0-0-g024aecf414\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.1/7.1.1\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nUnimplementedError: Cast uint8 to uint32 is not supported\r\n\r\nIt is unintuitive that tf.cast from uint8 to float32 works, but uint8 to uint32 does not.\r\nPlease document or implement unsupported pairs of numerical dtypes for tf.cast.\r\n\r\n### Source code / logs\r\n```\r\nx = tf.cast(uint8_tensor, dtype=tf.uint32)\r\n```\r\n```\r\n   return [tf.reduce_sum(tf.cast(orig, dtype=tf.uint32))]\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 782, in cast\r\n    return gen_math_ops.cast(x, base_type, name=name)\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1524, in cast\r\n    \"Cast\", x=x, DstT=DstT, name=name)\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nUnimplementedError (see above for traceback): Cast uint8 to uint32 is not supported\r\n\t [[Node: Cast_1 = Cast[DstT=DT_UINT32, SrcT=DT_UINT8, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_img_0_0)]]\r\n```\r\n", "comments": ["In TensorFlow, the following integer types are supported:\r\n`uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`. \r\n\r\n`uint32` and `uint64` are not supported so the `UnimplementedError` is the expected error.\r\n\r\nThere might be some enhancement that could be done in the docs. Added a PR #18561 for docs update.", "It seems that in newer versions of TensorFlow, the uint32 has been added. Do you know which is the version when this happened?"]}, {"number": 18528, "title": "Gradient Inconsistency ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: *No*\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: *Linux Ubuntu 14.04.5*\r\n- **TensorFlow installed from (source or binary)**: *pip install from binary*\r\n- **TensorFlow version (use command below)**: *v1.4.0-19-ga52c8d9 1.4.1*\r\n- **Python version**: *Python3.5 (Anaconda)*\r\n- **Bazel version (if compiling from source)**: *N/A*\r\n- **GCC/Compiler version (if compiling from source)**: *N/A*\r\n- **CUDA/cuDNN version**: *CUDA-8.0*\r\n- **GPU model and memory**: *GeForce GTX 1070(8GB) & GeForce GTX 770(4GB)*\r\n- **Exact command to reproduce**: *N/A*\r\n\r\n\r\n### Describe the problem\r\nWhen use operation `tf.transpose()`, `tf.gather()`, `tf.cholesky()` all together in a row over instance of `tf.Variable()`, the backward gradient computation may seems inconsistent. By using 'inconsistent', I mean that after run the same script multiple times with fixed random seeds, the computed gradient of **result of `tf.cholesky()`** over **input of `tf.transpose()`** are not always identical.\r\n\r\n### Source code / logs\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nnp.random.seed(1024)\r\ntf.set_random_seed(1024)\r\n\r\nN = 10\r\n\r\n# indices for `tf.gather()`\r\nindices_for_gather = tf.constant(np.concatenate((np.zeros(N), np.ones(N))).reshape(-1,).astype(np.int32))\r\n\r\n# build a 2-by-2 PSD matrix as `param` for 'tf.gather()'\r\nW = tf.constant(np.random.rand(2, 1), dtype=tf.float32)\r\nW = tf.Variable(W)\r\n\r\nPSD2x2 = tf.matmul(W, W, transpose_b=True)\r\n\r\n# do the `tf.gather()`\r\nM_temp = tf.gather(PSD2x2, indices_for_gather)\r\n\r\n# Then transpose the M_Temp and tf.gather() again (to ensure the result is a PSD for cholesky)\r\nM_temp_T = tf.transpose(M_temp)\r\n\r\nPSD = tf.gather(M_temp_T, indices_for_gather) + tf.eye(2*N, dtype=tf.float32) * 0.01\r\n\r\n# cholesky\r\nL = tf.cholesky(PSD)\r\n\r\n# compute the gradient of `L` over `M_temp`\r\ngrad = tf.gradients(L, M_temp)\r\n\r\n# build a session and compute the gradient\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nresults = []\r\nwith tf.Session(config=config) as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    for i in range(10):\r\n        result = sess.run(grad)\r\n        results.append(result[0]) \r\n        \r\n# check the results are whether identical or not\r\nfor i in range(len(results) - 1): \r\n    if isinstance(results[i], np.ndarray):\r\n        print(np.all(np.equal(results[i], results[i+1])))\r\n    else:\r\n        print(np.all(np.equal(results[i].values, results[i+1].values)))\r\n```\r\n\r\nThe output result is\r\n```\r\nFalse\r\nFalse\r\nFalse\r\nFalse\r\nFalse\r\nFalse\r\nFalse\r\nFalse\r\nFalse\r\n```\r\nIf I comment the line `W = tf.Variable(W)` making the `W` a constant tensor, the results are ideally all `True`. And I've also tried to compute the gradient of`L` over `M_temp_T` and `PSD`, both of them are all `True`. So I think the problem lies in the using of `tf.transpose()`, `tf.gather()`, `tf.cholesky()` all together over instance of `tf.Variable()`.", "comments": ["I suspect that reinitializing the variables will reinitialize them inconsistently. Is it consistent if you tf.reset_deafult_graph() and rebuidl the graph every iteration of checking?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 31 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 18527, "title": "New implementation of `tf.clip_by_value` changes behavior of the op.", "body": "FYI, @jart and @nfelt \r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian GNU/Linux (upgraded from: Ubuntu 14.04.5 LTS)\r\n- **TensorFlow installed from (source or binary)**: binary (pip package)\r\n- **TensorFlow version (use command below)**: Nightly 1.8.0 build 254 (`tf-nightly`)\r\n- **Python version**: Python 2.7.13\r\n- **Exact command to reproduce within TensorBoard's repository**:\r\nbazel test //tensorboard/plugins/pr_curve:pr_curves_plugin_test\r\n\r\n### Describe the problem\r\nEvery night, TensorBoard's python tests on travis installs the latest nightly version of TensorFlow. One day, I find that TensorBoard's `:pr_curves_plugin_test` is failing for python 2.\r\nhttps://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/pr_curve/pr_curves_plugin_test.py\r\nInterestingly, the test still passes for python 3.\r\n\r\nI confirmed that the test succeeds if I install `tf-nightly` build 253 and then `bazel test` the target within [TensorBoard's repo](https://github.com/tensorflow/tensorboard).\r\n```\r\npip install -I https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-linux/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/253/artifact/pip_test/whl/tf_nightly-1.8.0.dev20180409-cp27-cp27mu-linux_x86_64.whl\r\n```\r\n\r\nThe `:pr_curves_plugin_test` fails if I install `tf-nightly` build 254.\r\n```\r\npip install -I https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-linux/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/254/artifact/pip_test/whl/tf_nightly-1.8.0.dev20180410-cp27-cp27mu-linux_x86_64.whl\r\n```\r\n\r\nTherefore, a TensorFlow change introduced to build 254 seems to be a likely cause. During `setUp`, the `:pr_curves_plugin_test` runs a [demo script](https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/pr_curve/pr_curve_demo.py) that [seeds randomness](https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/pr_curve/pr_curve_demo.py#L65) and then [generates test data](https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/pr_curve/pr_curve_demo.py#L68) from a `tf.distributions.Normal` distribution.\r\n\r\nCould any changes to TensorFlow's seeding or `tf.distributions.Normal` logic (introduced to build 254) cause this breakage? Regardless of how we fix `:pr_curves_plugin_test` (Either TensorBoard's test logic could change, or we could partially roll back a TensorFlow change.), I'm curious about the root cause of the difference in behavior. Thanks! \r\n\r\n### Source code / logs\r\n\r\nHere are the logs for the failed test. They just describe how the floating point data derived from the PR curves plugin demo have changed.\r\n\r\n```\r\nTesting //.../plugins/pr_curve:pr_curves_plugin_test; 40s linux-sandbox\r\n...F....\r\n======================================================================\r\nFAIL: testPrCurvesDataCorrect (__main__.PrCurvesPluginTest)\r\nTests that responses for PR curves for run-tag combos are correct.\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/travis/.bazel-output-base/bazel-sandbox/2643229171274820909/execroot/org_tensorflow_tensorboard/bazel-out/k8-fastbuild/bin/tensorboard/plugins/pr_curve/pr_curves_plugin_test.runfiles/org_tensorflow_tensorboard/tensorboard/plugins/pr_curve/pr_curves_plugin_test.py\", line 217, in testPrCurvesDataCorrect\r\n    pr_curve_entry=entries[0])\r\n  File \"/home/travis/.bazel-output-base/bazel-sandbox/2643229171274820909/execroot/org_tensorflow_tensorboard/bazel-out/k8-fastbuild/bin/tensorboard/plugins/pr_curve/pr_curves_plugin_test.runfiles/org_tensorflow_tensorboard/tensorboard/plugins/pr_curve/pr_curves_plugin_test.py\", line 87, in validatePrCurveEntry\r\n    assert_allclose(expected_precision, pr_curve_entry['precision'])\r\n  File \"/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/numpy/testing/nose_tools/utils.py\", line 1396, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File \"/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/numpy/testing/nose_tools/utils.py\", line 779, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=0, atol=1e-07\r\n(mismatch 75.0%)\r\n x: array([0.333333, 0.385321, 0.542169, 0.75    ])\r\n y: array([0.333333, 0.391026, 0.630137, 0.666667])\r\n----------------------------------------------------------------------\r\nRan 8 tests in 38.439s\r\nFAILED (failures=1)\r\n================================================================================\r\n```\r\n", "comments": ["Huh, the [demo script](https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/pr_curve/pr_curve_demo.py) that `:pr_curves_plugin_test` relies on also makes use of `tf.clip_by_value`. I wonder if https://github.com/tensorflow/tensorflow/commit/083cf6b91a380641933457a4301f9b1efa13af92 or https://github.com/tensorflow/tensorflow/commit/daf0b206b5afde875a19270136ad22d9d2bb138c could be the culprit.\r\n\r\nYeah, I believe that's it. See my PR https://github.com/tensorflow/tensorboard/pull/1132.", "Thanks for reporting it. There was a bug clip_by_value. \r\nPlease try tf-1.8rc1 to see if the issue still exists.\r\n", "Thank you, @bignamehyp. The demo works again. See https://github.com/tensorflow/tensorboard/pull/1150. Could you please point us to what PR fixed the problem?", "It's an internal change and and get merged in a big PR."]}]