[{"number": 18526, "title": "Tensorflow is a big issue", "body": "I am using Windows 10 and Python 3.6 with jupyter notebook. I simply wanted to use tensorflow because of its hyped popularity but it seems like a real pain.\r\n\r\nfirst issue with installing tensorflow, then if installed then issue with importing tensorflow. Giving dumps like \r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\nLooked for this issue then found need to have CUDA and cuDNN from NVIDIA which doesn't allow if you are not an NVIDIA developer.\r\n\r\nI am not able to download cuDNN which seems \"very\" important for tensorflow to work.\r\n\r\nCan anyone please suggest how can I simply import tensorflow?\r\n", "comments": ["@Monalisa90 \r\n\r\nhttps://www.tensorflow.org/install/install_windows\r\n\r\nJust follow instructions to install the CPU version without GPU support, as recommended near the top of the article.", "Also, if you are interested in the GPU version you don't need to be an NVIDIA developer (i.e. work for NVIDIA), you just need to register an account with them.", "I have installed tensorflow as per the instructions provided. Then I installed CUDA 9.0. But still shows error while importing tensorflow.\r\n\r\nFor this particular line:\r\n\"In particular, the cuDNN version must match exactly: TensorFlow will not load if it cannot find cuDNN64_7.dll.\"\r\nI am trying to join https://developer.nvidia.com/cudnn to download cudnn...but here i am unable to join as it shows me the error \"Token expired\"", "Nagging Assignee @cy89: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@Monalisa90 did you figure out the NVIDIA/cuDNN issues? ", "I have figured out the issue.\n\nYou can close the ticket.\n\nApologies for the delay.\n\nThanks,\nMonalisa\nOn May 9, 2018 14:15, \"Cliff Young\" <notifications@github.com> wrote:\n\n> @Monalisa90 <https://github.com/Monalisa90> did you figure out the\n> NVIDIA/cuDNN issues?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/18526#issuecomment-387827402>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AhI3UTjDbWFMz70EnajG6_y81CKs2hFrks5twzIogaJpZM4TVS2V>\n> .\n>\n", "I have figured out the issue.\n\nYou can close the ticket.\n\nApologies for the delay.\n\nThanks,\nMonalisa\nOn May 9, 2018 14:39, \"Monalisa Mishra\" <mmishra@buffalo.edu> wrote:\n\n> I have figured out the issue.\n>\n> You can close the ticket.\n>\n> Apologies for the delay.\n>\n> Thanks,\n> Monalisa\n> On May 9, 2018 14:15, \"Cliff Young\" <notifications@github.com> wrote:\n>\n>> @Monalisa90 <https://github.com/Monalisa90> did you figure out the\n>> NVIDIA/cuDNN issues?\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/18526#issuecomment-387827402>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/AhI3UTjDbWFMz70EnajG6_y81CKs2hFrks5twzIogaJpZM4TVS2V>\n>> .\n>>\n>\n", "Thanks!"]}, {"number": 18525, "title": "Hyperparameter optimization and TensorFlow", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution:** Linux-Ubuntu (17.10)\r\n- **TensorFlow installed from**: Binary\r\n- **TensorFlow version**:  1.5.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nI know that this question has already been addressed here and in Stack Overflow, but until the moment I do not see many evolutions on the subject.\r\nSaw this approach here [A Scikit-learn compatible Deep Neural Network built with TensorFlow](https://medium.com/@williamkoehrsen/deep-neural-network-classifier-32c12ff46b6c),, that is, using TensorFlow (model build) and Scikit-learn to make the model's Hyperparameter Tuning.\r\nSo far this approach is the most correct?\r\nReally wanted to be able to contribute and work with the optimization of hyperparameters along with the TensorFlow models.\r\nI find this technique very important in Deep Learning...\r\nthanks in advance", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Take a look to the Kubeflow related project: https://github.com/kubeflow/hp-tuning", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Hello,\r\nI've not had any feedback nor here much less in stackoverflow ...\r\nI've seen several alternatives, one of which I have described here before, and another in this repository [here](https://github.com/charuj/multigraph_lstm_hyperparamsearch)\r\n\r\nConsequently I think the second most attractive alternative.\r\n\r\nIn my final paper (graduation in computer science) I approached the second alternative applying the automatic selection of hyperparameters in DNNClassfier ...\r\n\r\nIt is still crawling more I think it is possible to create an Estimator following this proposal ... I hope to spread my studies soon ...\r\n\r\nI just need to mature better ... For now is can create an example of how to use the optimization of hyperparameters along with Tensorflow ... Reinforcing I think this is very interesting ... And I see that this is still a limitation of Tensorflow", "Katib is trying to replicate Google vizier concepts in an opensource impl:\r\nhttps://github.com/kubeflow/katib", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Closing, as this capability is added in [Keras Tuner](https://github.com/keras-team/keras-tuner)."]}, {"number": 18524, "title": "Update sqlite version for cmake build", "body": "The sqlite has been updated in bazel, though\r\ncmake version was not updated. This fix updates\r\nsqlite in cmake so that cmake and bazel versions\r\nare synced.\r\n\r\nThe fix has been tested on Linux:\r\n```\r\ntensorflow/tools/ci_build/ci_build.sh CMAKE tensorflow/tools/ci_build/builds/cmake.sh\r\n```\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 18523, "title": "Unable to freeze graph for LinearClassifier", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: Anaconda\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: 3.5.4\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**:  N/A\r\n- **GPU model and memory**:  N/A\r\n- **Exact command to reproduce**: freeze_graph for LinearClassifier model\r\n\r\nI'm trying to freeze my model using the source https://www.tensorflow.org/mobile/prepare_models. So I could use it in Android app (using Tensorflow Mobile).\r\nI have a model created with `LinearClassifier`. Below is the example of creating it:\r\n\r\n```python\r\ngender = tf.feature_column.categorical_column_with_vocabulary_list(\"gender\", [\"f\", \"m\"])\r\ngoal = tf.feature_column.categorical_column_with_vocabulary_list(\"goal\", [\"fit\", \"lose\", \"muscle\"])\r\nlevel = tf.feature_column.categorical_column_with_vocabulary_list(\"level\", [\"begin\", \"middle\", \"advance\"])\r\nfeature_columns = [gender,goal,level]\r\n\r\ninput_fn = tf.estimator.inputs.pandas_input_fn(x=x_train, y=y_train, batch_size=100, num_epochs=None, shuffle=True)\r\nmodel = tf.estimator.LinearClassifier(feature_columns=feature_columns, n_classes=18, model_dir=\"export\")\r\n\r\nmodel.train(input_fn=input_fn, steps=100)\r\n```\r\nThe model is successfully created and saved. It works correctly when it's evaluated.\r\nHowever, I'm getting the following error, when I try to freeze the graph:\r\n```\r\nTypeError: names_to_saveables must be a dict mapping string names to Tensors/Variables. Not a variable: Tensor(\"linear/linear_model/bias_weights:0\", shape=(18,), dtype=float32)\r\n```\r\n\r\nI looked on [StackOverflow](https://stackoverflow.com/search?q=names_to_saveables+must+be+a+dict+) and seems this issue happens to many people and there is no answer to that.\r\n\r\nI would appreciate any help, because many people are having similar issue.", "comments": ["Hi @dkhmelenko \r\nCould you please tell us more about how do you freeze the graph? ", "I'm using the following command for freezing the graph\r\n```\r\n C:\\Users\\Dima\\Anaconda3\\envs\\tensorflow\\Lib\\site-packages\\tensorflow\\python\\tools>python freeze_graph.py --input_graph=export/graph.pbtxt --input_binary=false --input_checkpoint=export/model.ckpt-100 --output_graph=model/frozen_graph.pb --output_node_names=output_node\r\n```\r\nAnd the output is the following\r\n```\r\n2018-04-16 22:52:21.381508: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\35\\tensorflow\\core\\platform\\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX\r\nTraceback (most recent call last):\r\n  File \"freeze_graph.py\", line 350, in <module>\r\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"C:\\Users\\Dima\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"freeze_graph.py\", line 249, in main\r\n    FLAGS.saved_model_tags)\r\n  File \"freeze_graph.py\", line 239, in freeze_graph\r\n    input_meta_graph_def, input_saved_model_dir, saved_model_tags.split(\",\"))\r\n  File \"freeze_graph.py\", line 127, in freeze_graph_with_def_protos\r\n    saver = saver_lib.Saver(var_list=var_list)\r\n  File \"C:\\Users\\Dima\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1218, in __init__\r\n    self.build()\r\n  File \"C:\\Users\\Dima\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1227, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"C:\\Users\\Dima\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1263, in _build\r\n    build_save=build_save, build_restore=build_restore)\r\n  File \"C:\\Users\\Dima\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 729, in _build_internal\r\n    saveables = self._ValidateAndSliceInputs(names_to_saveables)\r\n  File \"C:\\Users\\Dima\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 632, in _ValidateAndSliceInputs\r\n    variable)\r\nTypeError: names_to_saveables must be a dict mapping string names to Tensors/Variables. Not a variable: Tensor(\"linear/linear_model/bias_weights:0\", shape=(18,), dtype=float32)\r\n\r\n(tensorflow) C:\\Users\\Dima\\Anaconda3\\envs\\tensorflow\\Lib\\site-packages\\tensorflow\\python\\tools>\r\n```\r\n\r\nThe files `graph.pbtxt` and `model.ckpt-100` are in attachment.\r\n[checkpoint.zip](https://github.com/tensorflow/tensorflow/files/1917237/checkpoint.zip)\r\n\r\n", "Pete, could you please take a look?", "@ispirmustafa @petewarden are there any updates on this issue?", "Nagging Assignee @petewarden: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Getting a similar error. Any updates? Thanks", "Nagging Assignee @petewarden: It has been 17 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I am facing a similar issue. I gave the following as command in Anaconda prompt: \r\nfreeze_graph --input_graph=C:\\Users\\msakthi\\Desktop\\Speech_rec\\speech_commands\\9wordsmodel\\conv.pb --input_checkpoint=C:\\Users\\msakthi\\Desktop\\Speech_rec\\speech_commands\\9wordsmodel\\conv.ckpt-18000 --output_graph=C:\\Users\\msakthi\\Desktop\\Speech_rec\\speech_commands\\9wordsmodel\\conv_graph.pb --output_node_names=MatMul --input_binary=true\r\n\r\nyet, I get an error saying:\r\nTypeError: names_to_saveables must be a dict mapping string names to Tensors/Variables. Not a variable: Tensor(\"Variable:0\", shape=(20, 8, 1, 64), dtype=float32).\r\n\r\nIf you could help me with this issue, great! \r\n\r\nThanks in advance.", "The issue still exists for Tensorflow 1.8.0", "This error indicates a general issue with trying to freeze a graph that doesn't match the checkpoint. I can't tell from the code how you've saved out the graph, but the most reliable way to freeze is to use a SavedModel folder where you can be sure the graph and checkpoints do match, and you only have to pass in a single folder location.", "@petewarden Thank you for your response.\r\n\r\nI've tried a couple of the approaches and none of them worked for me:\r\n1. I tried to freeze the graph using the function `model.export_savedmodel`. Then I've got a folder with the files `saved_model.pb`, `variables.data-00000-of-00001` and `variables.index`. But there is no checkpoint file in this case. What should be done in this case?\r\n\r\n2. I tried also to freeze the graph using the argument `model_dir` on creating `LinearClassifier`. In that case, I've got the files `graph.pbtxt`, `checkpoint` and a couple of `model.ckpt` files. But still no success.\r\n\r\nMy code is pretty straightforward and can be found here https://github.com/dkhmelenko/fitness-ml/blob/master/simple/Fitness%20Classification.ipynb\r\n\r\nI would appreciate any kind of help from your side. Thank you in advance.", "@dkhmelenko  did you find any solution to the mentioned error?", "@dkhmelenko did you find any solution to the mentioned error?", "I have the same issue in v1.9. In my case, I am trying to get a .tflite model to serve from Firebase.\r\nThe other method I tried (which has been mentioned above) is creating a SavedModel then attempting to convert using \r\n`tf.contrib.lite.TocoConverter.from_saved_model(path_to_savedModel)`\r\nbut then I get a different error:\r\n**ValueError: Tensors input_example_tensor:0 not known type tf.string**", "@harsh-agar @zhangjf23 sorry, I didnt find any solution so far, I'm still facing the issue", "@OzzieFZI The issue you listed has been fixed in head. To use the nightly build either use \"pip install --upgrade tf_nightly\" or follow the instructions [here](https://www.tensorflow.org/install/install_sources).", "@gargn Thank you. I followed the instructions and ran the same code for converting the SavedModel but ended up with a \"_Some of the operators in the model are not supported by the standard TensorFlow Lite runtime_...\" error for ParseExample and AsString.", "@OzzieFZI The list of supported ops are available [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/tf_ops_compatibility.md). The two recommended solutions are:\r\n1. Add a custom op as described [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/custom_operators.md).\r\n2. Only use the ops that are currently supported by TFLite in your model.\r\n\r\nFor any additional issues with your model, please make another bug so we can track issues separately.\r\n\r\n@dkhmelenko Could you attach a SavedModel for your model?", "@gargn \r\nHere is SavedModel as a zip. It contains `saved_model.pbtxt` and `variables` directory.\r\n[SavedModel.zip](https://github.com/tensorflow/tensorflow/files/2229710/SavedModel.zip)\r\n", "@dkhmelenko I looked into the issue further and there seem to be some alternative ways that you can run the code using your SavedModel.\r\n\r\n1. If you intend to run your code through TensorFlow Lite then you can use either [`TocoConverter`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/python_api.md#basic-savedmodel) (Python API) or [`tflite_convert`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md#savedmodel) (command line tool) with your SavedModel. This should be working starting from TensorFlow 1.9. If not, it works on the nightly build. However, this results in the error mentioned above indicating there are some operators that aren't supported. The solution to that error is mentioned in the comment below.\r\n\r\n2. If you just want to run `freeze_graph.py` then you should be able to run it with a SavedModel. The current error you are getting is coming from the code that loads the checkpoints. The command line below should work with your code. The values for `output_node_names` came from the SignatureDef within your SavedModel.\r\n\r\n```\r\npython freeze_graph.py \\\r\n--input_saved_model_dir=SAVED_MODEL_DIR \\\r\n--output_node_names=linear/head/Tile,linear/head/predictions/probabilities \\\r\n--output_graph=/tmp/frozen_graph.pb\r\n```\r\n\r\nLet me know if one of these solutions suffices. In the meantime I'm looking for a solution to fix the problem in `freeze_graph.py`.", "After some investigation, it appears the only way to freeze models with partitioned variables is using SavedModels. The error message has been [updated](https://github.com/tensorflow/tensorflow/commit/37594fd0945061aee9f4d5f6ba2aa8c4b360697c) to state this clearly.\r\n\r\nI am closing the issue since there is working solution and an improved error message. Please reopen if there are any additional issues relating to freezing your model.", "I'm having the same problem. When I tried to use the SavedModels option, I got the following error: `IOError: SavedModel file does not exist at: /path/I/supplied/as/argument`\r\n\r\nMaybe I am misunderstanding what this argument should be. I thought it would be the path to the folder which has the ascii graph and trios of ckpt files (index, meta, data) in it? The same one that I provide as the `--logdir` argument to tensorboard?", "@chrisrapson Please file a separate issue with the command you used to run `freeze_graph.py`.", "ok, new issue submitted: https://github.com/tensorflow/tensorflow/issues/22069"]}, {"number": 18522, "title": "Error building 1.8 RC-0 'eye_functor_gpu.cu.pic.o was not created'", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Fedora 27 x64\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.8.0 RC-0 (zip)\r\n- **Python version**: 3.6.4\r\n- **Bazel version (if compiling from source)**: 0.12.0-1.fc27\r\n- **GCC/Compiler version (if compiling from source)**: 6.4.0-6\r\n- **CUDA/cuDNN version**: 9.1 / 7.0.5.15-3.fc27\r\n- **GPU model and memory**: GTX 1060\r\n- **Exact command to reproduce**: `$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package `\r\n\r\n### Describe the problem\r\nI get nearly 6k files into the build and it fails with:\r\n```\r\nINFO: From Compiling tensorflow/core/kernels/eye_functor_gpu.cu.cc [for host]:\r\n/usr/include/bits/floatn.h(61): error: invalid argument to attribute \"__mode__\"\r\n/usr/include/bits/floatn.h(73): error: identifier \"__float128\" is undefined\r\n```\r\nAdd `\"#define _BITS_FLOATN_H\"` to `cuda/host_defines.h`, and build again.\r\nGets further but fails at:\r\n```\r\nINFO: From Compiling tensorflow/core/kernels/eye_functor_gpu.cu.cc:\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {const std::tuple<int, int, int>&}; bool <anonymous> = true; _Elements = {int, int, int}]':\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:626:248:   required by substitution of 'template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {const std::tuple<int, int, int>&}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]'\r\n./tensorflow/stream_executor/dnn.h:891:91:   required from here\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:483:67: error: mismatched argument pack lengths while expanding 'std::is_constructible<_Elements, _UElements&&>'\r\n       return __and_<is_constructible<_Elements, _UElements&&>...>::value;\r\n                                                                   ^~~~~\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:484:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {const std::tuple<int, int, int>&}; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement\r\n     }\r\n ^\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {const std::tuple<int, int, int>&}; bool <anonymous> = true; _Elements = {int, int, int}]':\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:626:362:   required by substitution of 'template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {const std::tuple<int, int, int>&}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]'\r\n./tensorflow/stream_executor/dnn.h:891:91:   required from here\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:489:65: error: mismatched argument pack lengths while expanding 'std::is_convertible<_UElements&&, _Elements>'\r\n       return __and_<is_convertible<_UElements&&, _Elements>...>::value;\r\n                                                                 ^~~~~\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:490:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {const std::tuple<int, int, int>&}; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement\r\n     }\r\n ^\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = const std::tuple<int, int, int>&; bool <anonymous> = true; _Elements = {int, int, int}]':\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:662:419:   required by substitution of 'template<class ... _UElements, class _Dummy, typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), int, int, int>::_NonNestedTuple<const tuple<_Elements ...>&>()), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(const std::tuple<_Args1 ...>&) [with _UElements = {int, int, int}; _Dummy = void; typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), int, int, int>::_NonNestedTuple<const tuple<_Elements ...>&>()), bool>::type <anonymous> = <missing>]'\r\n./tensorflow/stream_executor/dnn.h:891:91:   required from here\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:495:244: error: wrong number of template arguments (4, should be 2)\r\n       return  __and_<__not_<is_same<tuple<_Elements...>,\r\n                                                                                                                                                                                                                                                    ^    \r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/type_traits:1558:8: note: provided for 'template<class _From, class _To> struct std::is_convertible'\r\n     struct is_convertible\r\n        ^~~~~~~~~~~~~~\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:502:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = const std::tuple<int, int, int>&; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement\r\n     }\r\n ^\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {std::tuple<int, int, int>}; bool <anonymous> = true; _Elements = {int, int, int}]':\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:626:248:   required by substitution of 'template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {std::tuple<int, int, int>}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]'\r\n./tensorflow/stream_executor/dnn.h:891:91:   required from here\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:483:67: error: mismatched argument pack lengths while expanding 'std::is_constructible<_Elements, _UElements&&>'\r\n       return __and_<is_constructible<_Elements, _UElements&&>...>::value;\r\n                                                                   ^~~~~\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:484:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {std::tuple<int, int, int>}; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement\r\n     }\r\n ^\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {std::tuple<int, int, int>}; bool <anonymous> = true; _Elements = {int, int, int}]':\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:626:362:   required by substitution of 'template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {std::tuple<int, int, int>}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]'\r\n./tensorflow/stream_executor/dnn.h:891:91:   required from here\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:489:65: error: mismatched argument pack lengths while expanding 'std::is_convertible<_UElements&&, _Elements>'\r\n       return __and_<is_convertible<_UElements&&, _Elements>...>::value;\r\n                                                                 ^~~~~\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:490:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {std::tuple<int, int, int>}; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement\r\n     }\r\n ^\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = std::tuple<int, int, int>&&; bool <anonymous> = true; _Elements = {int, int, int}]':\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:686:422:   required by substitution of 'template<class ... _UElements, class _Dummy, typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), int, int, int>::_NonNestedTuple<tuple<_Elements ...>&&>()), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(std::tuple<_Args1 ...>&&) [with _UElements = {int, int, int}; _Dummy = void; typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), int, int, int>::_NonNestedTuple<tuple<_Elements ...>&&>()), bool>::type <anonymous> = <missing>]'\r\n./tensorflow/stream_executor/dnn.h:891:91:   required from here\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:495:244: error: wrong number of template arguments (4, should be 2)\r\n       return  __and_<__not_<is_same<tuple<_Elements...>,\r\n                                                                                                                                                                                                                                                    ^    \r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/type_traits:1558:8: note: provided for 'template<class _From, class _To> struct std::is_convertible'\r\n     struct is_convertible\r\n        ^~~~~~~~~~~~~~\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:502:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = std::tuple<int, int, int>&&; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement\r\n     }\r\n ^\r\nERROR: /home/torstein/progs/tensorflow-1.8.0-rc0/tensorflow/core/kernels/BUILD:1864:1: output 'tensorflow/core/kernels/_objs/eye_functor_gpu/tensorflow/core/kernels/eye_functor_gpu.cu.o' was not created\r\nERROR: /home/torstein/progs/tensorflow-1.8.0-rc0/tensorflow/core/kernels/BUILD:1864:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 48.261s, Critical Path: 33.18s\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nBuild conf:\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/bin/python3\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/local/lib64/python3.6/site-packages\"\r\nbuild --force_python=py3\r\nbuild --host_force_python=py3\r\nbuild --python_path=\"/usr/bin/python3\"\r\nbuild --define with_jemalloc=true\r\nbuild:gcp --define with_gcp_support=true\r\nbuild:hdfs --define with_hdfs_support=true\r\nbuild:s3 --define with_s3_support=true\r\nbuild:kafka --define with_kafka_support=true\r\nbuild:xla --define with_xla_support=true\r\nbuild:gdr --define with_gdr_support=true\r\nbuild:verbs --define with_verbs_support=true\r\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\nbuild --action_env TF_NEED_CUDA=\"1\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr\"\r\nbuild --action_env TF_CUDA_VERSION=\"9.1\"\r\nbuild --action_env CUDNN_INSTALL_PATH=\"/usr\"\r\nbuild --action_env TF_CUDNN_VERSION=\"7\"\r\nbuild --action_env TF_NCCL_VERSION=\"2\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"6.1\"\r\nbuild --action_env LD_LIBRARY_PATH=\"/usr/include/cuda:/usr/include:/usr:/usr/lib64:/usr/include/cuda:/usr:/usr/include\"\r\nbuild --action_env TF_CUDA_CLANG=\"0\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/cuda-gcc\"\r\nbuild --config=cuda\r\ntest --config=cuda\r\nbuild --define grpc_no_ares=true\r\nbuild:opt --copt=-march=native\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\nbuild --copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\r\nbuild --host_copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\r\n```\r\nSame error happens on master branch.\r\n\r\nIf I add the build flag `--cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"`, it fails at different place:\r\n```\r\nINFO: From Compiling tensorflow/core/kernels/spacetodepth_op_gpu.cu.cc:\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {const std::tuple<int, int, int>&}; bool <anonymous> = true; _Elements = {int, int, int}]':\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:626:248:   required by substitution of 'template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {const std::tuple<int, int, int>&}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]'\r\n./tensorflow/stream_executor/dnn.h:891:91:   required from here\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:483:67: error: mismatched argument pack lengths while expanding 'std::is_constructible<_Elements, _UElements&&>'\r\n       return __and_<is_constructible<_Elements, _UElements&&>...>::value;\r\n                                                                   ^~~~~\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:484:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {const std::tuple<int, int, int>&}; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement\r\n     }\r\n ^\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {const std::tuple<int, int, int>&}; bool <anonymous> = true; _Elements = {int, int, int}]':\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:626:362:   required by substitution of 'template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {const std::tuple<int, int, int>&}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]'\r\n./tensorflow/stream_executor/dnn.h:891:91:   required from here\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:489:65: error: mismatched argument pack lengths while expanding 'std::is_convertible<_UElements&&, _Elements>'\r\n       return __and_<is_convertible<_UElements&&, _Elements>...>::value;\r\n                                                                 ^~~~~\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:490:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {const std::tuple<int, int, int>&}; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement\r\n     }\r\n ^\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = const std::tuple<int, int, int>&; bool <anonymous> = true; _Elements = {int, int, int}]':\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:662:419:   required by substitution of 'template<class ... _UElements, class _Dummy, typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), int, int, int>::_NonNestedTuple<const tuple<_Elements ...>&>()), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(const std::tuple<_Args1 ...>&) [with _UElements = {int, int, int}; _Dummy = void; typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), int, int, int>::_NonNestedTuple<const tuple<_Elements ...>&>()), bool>::type <anonymous> = <missing>]'\r\n./tensorflow/stream_executor/dnn.h:891:91:   required from here\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:495:244: error: wrong number of template arguments (4, should be 2)\r\n       return  __and_<__not_<is_same<tuple<_Elements...>,\r\n                                                                                                                                                                                                                                                    ^    \r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/type_traits:1558:8: note: provided for 'template<class _From, class _To> struct std::is_convertible'\r\n     struct is_convertible\r\n        ^~~~~~~~~~~~~~\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:502:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = const std::tuple<int, int, int>&; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement\r\n     }\r\n ^\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {std::tuple<int, int, int>}; bool <anonymous> = true; _Elements = {int, int, int}]':\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:626:248:   required by substitution of 'template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {std::tuple<int, int, int>}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]'\r\n./tensorflow/stream_executor/dnn.h:891:91:   required from here\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:483:67: error: mismatched argument pack lengths while expanding 'std::is_constructible<_Elements, _UElements&&>'\r\n       return __and_<is_constructible<_Elements, _UElements&&>...>::value;\r\n                                                                   ^~~~~\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:484:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {std::tuple<int, int, int>}; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement\r\n     }\r\n ^\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {std::tuple<int, int, int>}; bool <anonymous> = true; _Elements = {int, int, int}]':\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:626:362:   required by substitution of 'template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {std::tuple<int, int, int>}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]'\r\n./tensorflow/stream_executor/dnn.h:891:91:   required from here\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:489:65: error: mismatched argument pack lengths while expanding 'std::is_convertible<_UElements&&, _Elements>'\r\n       return __and_<is_convertible<_UElements&&, _Elements>...>::value;\r\n                                                                 ^~~~~\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:490:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {std::tuple<int, int, int>}; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement\r\n     }\r\n ^\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = std::tuple<int, int, int>&&; bool <anonymous> = true; _Elements = {int, int, int}]':\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:686:422:   required by substitution of 'template<class ... _UElements, class _Dummy, typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), int, int, int>::_NonNestedTuple<tuple<_Elements ...>&&>()), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(std::tuple<_Args1 ...>&&) [with _UElements = {int, int, int}; _Dummy = void; typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), int, int, int>::_NonNestedTuple<tuple<_Elements ...>&&>()), bool>::type <anonymous> = <missing>]'\r\n./tensorflow/stream_executor/dnn.h:891:91:   required from here\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:495:244: error: wrong number of template arguments (4, should be 2)\r\n       return  __and_<__not_<is_same<tuple<_Elements...>,\r\n                                                                                                                                                                                                                                                    ^    \r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/type_traits:1558:8: note: provided for 'template<class _From, class _To> struct std::is_convertible'\r\n     struct is_convertible\r\n        ^~~~~~~~~~~~~~\r\n/usr/lib64/gcc/x86_64-redhat-linux/6.4.0/include/c++/tuple:502:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = std::tuple<int, int, int>&&; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement\r\n     }\r\n ^\r\nERROR: /home/torstein/progs/tensorflow-1.8.0-rc0/tensorflow/core/kernels/BUILD:3659:1: output 'tensorflow/core/kernels/_objs/depth_space_ops_gpu/tensorflow/core/kernels/spacetodepth_op_gpu.cu.o' was not created\r\nERROR: /home/torstein/progs/tensorflow-1.8.0-rc0/tensorflow/core/kernels/BUILD:3659:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 111.410s, Critical Path: 35.53s\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "comments": ["Building with gcc-5.4 on master branch did work without issues. Still, someone might take a look at the errors above. Is not gcc 6.4 supposed to be supported by CUDA 9.1 and tensorflow alike?", "Nagging Assignee @tatatodd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#system-requirements\r\n\r\nAccording to this, gcc 6.4 is not supported by CUDA.\r\nAs we are building on top of CUDA, there is not much we can do on TF side until CUDA supports GCC 6.4."]}, {"number": 18521, "title": "Update gemmlowp version for cmake build", "body": "The gemmlowp has been updated in bazel, though\r\ncmake version was not updated. This fix updates\r\ngemmlowp in cmake so that cmake and bazel versions\r\nare synced.\r\n\r\nThe fix has been tested on Linux:\r\n```\r\ntensorflow/tools/ci_build/ci_build.sh CMAKE tensorflow/tools/ci_build/builds/cmake.sh\r\n```\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Looks like the Windows make build is failing, closing for now."]}, {"number": 18520, "title": "Fix embedding_ops doc formatting", "body": "Currently the doc looks very messy: https://www.tensorflow.org/versions/master/api_docs/python/tf/nn/embedding_lookup_sparse", "comments": []}, {"number": 18519, "title": "Tensorflow profiler problem, maybe bug.", "body": "### Describe the problem\r\nThe implementation of topk kernel operation seems faster on my cpu than on gpu(for example below profiler outputs 6us for cpu and 345us for gpu, and result reapeats for other inputs). But that is not the case for this issue, I started some testing on that which lead me to this:\r\n\r\nI extracted the kernel implementation from tf source. Put some time measuring code in Compute around functor call, for real(using std::chrono::time_steady) and cpu(clock() from ctime) time. Compiled it without optimizations, loaded in python as tensorflow library. For cpu the result was much bigger than what profiler printed, 6us(profiler) vs 1432us(clock()), 473us(using std::chrono::time_steady).\r\nThen I added in functor code thread sleep, only real time duration extended, then I added long for loop and both real and cpu duration got bigger but again profiler output stayed the same as in code without modifications ~6us, what is not proper in my understanding.\r\n### Source code / logs\r\nHere is the python3 code I used:\r\n```\r\nimport tensorflow as tf\r\nimport random\r\n\r\narr=[[int(100000*random.random())\r\n    for i in range(100)] for j in range(100)]\r\n\r\na = tf.convert_to_tensor(arr)\r\ntopk_module = tf.load_op_library(\"topk_op.so\")\r\nb = topk_module.matrix_top_k(a, 10)\r\n\r\nbuilder = tf.profiler.ProfileOptionBuilder\r\nopts = builder(builder.time_and_memory()).order_by('micros').build()\r\n\r\n\r\nwith tf.contrib.tfprof.ProfileContext('/tmp/train_dir',\r\n                                    trace_steps=[],\r\n                                    dump_steps=[]) as pctx:\r\n    with tf.Session() as sess:\r\n        pctx.trace_next_step()\r\n        pctx.dump_next_step()\r\n        x = sess.run(b)\r\n        pctx.profiler.profile_operations(options=opts)\r\n```\r\nand snippet from topk_op.cc\r\n```\r\n    std::chrono::steady_clock::time_point time_begin_real =\r\n        std::chrono::steady_clock::now();\r\n    clock_t time_begin_cpu = clock();\r\n\r\n\r\n    Status s = functor::TopKFunctor<Device, T>::Compute(\r\n        context, sorted_, k, input, num_rows, num_cols, values, indices);\r\n    \r\n    clock_t time_end_cpu = clock();\r\n    std::chrono::steady_clock::time_point time_end_real =\r\n        std::chrono::steady_clock::now();\r\n    //stdout printing\r\n```\r\n------------------------\r\n### System information\r\nHave I written custom code: yes, can post modified topk op kernel code and makefile if you want\r\nubuntu 16.04\r\ntensorflow installed from pip package manager.\r\ntensorflow 1.7.0\r\nbazel: n/a, used gcc 4.9.3\r\ncuda 9.0 cudnn 7.0\r\ngtx 1050 ti 4GB\r\nn/a\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler I've updated the info.", "Nagging Assignee @zhangyaobit: It has been 155 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 18518, "title": "Semantic Similarity with TF-Hub doesn't work in the provided colab notebook", "body": "### System information\r\ncolab.research.google.com using Python3 and GPU runtime. \r\n\r\n### Describe the problem\r\nGetting the module from the Hub doesn't work.\r\n\r\n```\r\nKeyError: \"The name 'global_step:0' refers to a Tensor which does not exist. The operation, 'global_step', does not exist in the graph.\"\r\n```\r\n\r\n### Source code / logs\r\nhttps://colab.research.google.com/github/tensorflow/hub/blob/r0.1/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb\r\n\r\n```\r\nembed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/1\")\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "# 1. System information\r\nTo add onto @several27's earlier post.\r\nThe example of the **Universal Sentence Encoder Colaboratory** notebook does not work. This notebook can be found using this link:\r\n ```https://colab.research.google.com/github/tensorflow/hub/blob/r0.1/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb ```\r\n# 2. Describe the problem\r\nThe following error is thrown when all cells are run on the notebook using the ```Ctrl+F9``` command.\r\n\r\n``` KeyError: \"The name 'global_step:0' refers to a Tensor which does not exist. The operation, 'global_step', does not exist in the graph.\" ```\r\n\r\n# 3. Source code / logs:\r\n\r\n```  INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\r\nWARNING:tensorflow:cannot use a string pattern on a bytes-like object\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-4-e136a80b7068> in <module>()\r\n----> 1 embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/1\")\r\n      2 \r\n      3 # Compute a representation for each message, showing various lengths supported.\r\n      4 word = \"Elephant\"\r\n      5 sentence = \"I am a sentence for which I would like to get its embedding.\"\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_hub/module.py in __init__(self, spec, trainable, name, tags)\r\n    124           name=self._name,\r\n    125           trainable=self._trainable,\r\n--> 126           tags=self._tags)\r\n    127       # pylint: enable=protected-access\r\n    128 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_hub/native_module.py in _create_impl(self, name, trainable, tags)\r\n    280         trainable=trainable,\r\n    281         checkpoint_path=self._checkpoint_variables_path,\r\n--> 282         name=name)\r\n    283 \r\n    284   def _export(self, path, variables_saver):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_hub/native_module.py in __init__(self, spec, meta_graph, trainable, checkpoint_path, name)\r\n    331     # TPU training code.\r\n    332     with tf.control_dependencies(None):\r\n--> 333       variable_tensor_map, self._state_map = self._create_state_graph(name)\r\n    334       self._variable_map = recover_partitioned_variable_map(\r\n    335           get_node_map_from_tensor_map(variable_tensor_map)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_hub/native_module.py in _create_state_graph(self, name)\r\n    398     state_op_names = list_registered_stateful_ops_without_inputs()\r\n    399     state_map = get_state_map(self._meta_graph, state_op_names, set(),\r\n--> 400                               _get_tensor)\r\n    401 \r\n    402     return variables_tensor_map, state_map\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_hub/native_module.py in get_state_map(meta_graph, state_ops, unsupported_state_ops, get_tensor_by_name)\r\n    533     if node.op in state_ops:\r\n    534       tensor_name = node.name + \":0\"\r\n--> 535       state_map[tensor_name] = get_tensor_by_name(tensor_name)\r\n    536     if node.op in unsupported_state_ops:\r\n    537       raise ValueError(\"Unsupported stateful op: %s\" % node.op)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_hub/native_module.py in _get_tensor(tensor_name)\r\n    394     def _get_tensor(tensor_name):\r\n    395       return self._graph.get_tensor_by_name(\r\n--> 396           prepend_name_scope(tensor_name, import_scope=absolute_scope_name))\r\n    397 \r\n    398     state_op_names = list_registered_stateful_ops_without_inputs()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_tensor_by_name(self, name)\r\n   3764       raise TypeError(\"Tensor names are strings (or similar), not %s.\" %\r\n   3765                       type(name).__name__)\r\n-> 3766     return self.as_graph_element(name, allow_tensor=True, allow_operation=False)\r\n   3767 \r\n   3768   def _get_tensor_by_tf_output(self, tf_output):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in as_graph_element(self, obj, allow_tensor, allow_operation)\r\n   3588 \r\n   3589     with self._lock:\r\n-> 3590       return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\r\n   3591 \r\n   3592   def _as_graph_element_locked(self, obj, allow_tensor, allow_operation):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _as_graph_element_locked(self, obj, allow_tensor, allow_operation)\r\n   3630           raise KeyError(\"The name %s refers to a Tensor which does not \"\r\n   3631                          \"exist. The operation, %s, does not exist in the \"\r\n-> 3632                          \"graph.\" % (repr(name), repr(op_name)))\r\n   3633         try:\r\n   3634           return op.outputs[out_n]\r\n\r\nKeyError: \"The name 'global_step:0' refers to a Tensor which does not exist. The operation, 'global_step', does not exist in the graph.\"\r\n```\r\n# 2 Have I written custom code\r\nNo custom code written or added to original Colaboratory notebook.\r\n# 3. OS Platform and Distribution\r\nUnkown.\r\nColab notebook and environment provided by this link ```https://colab.research.google.com/github/tensorflow/hub/blob/r0.1/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb```. \r\n# 4. TensorFlow installed from:\r\nPip3\r\n# 5. TensorFlow version: \r\nVersion 1.7 + nightly build\r\n# 6. Bazel version: \r\nNot installed from Bazel\r\n# 7 CUDA/cuDNN version: \r\nStandard version on Colab notebook - unchanged\r\n# 8.  GPU model and memory: \r\nk80 GPU and 13GB RAM.\r\n# 9. Exact command to reproduce:\r\nGo onto link below:\r\n``` https://colab.research.google.com/github/tensorflow/hub/blob/r0.1/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb``` and then press ```Ctrl+F9```", "@kiariendegwa thanks for present your bug. i meet with the same issue exactlly hours ago. \r\n\r\nwhich i just fix this issue by install the latest tensorflow version. ", "There seems to be some bugs/inconsistencies with the get_state_map function in native_module.py, where the variable scope was not merged in with the variable names.\r\n\r\nIt is also possible that the protobuf was updated with a newer version of Saver. Need to look into the details.", "I found that this happens with tensorflow v1.8 (installed by default with `!pip3 install --quiet --upgrade --pre tensorflow`)\r\n\r\nChanging to \r\n`!pip3 install --quiet tensorflow==1.7` \r\nallows \r\n`embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/1\")` \r\nto execute successfully.", "Exact same problem here, installing from pip 1.7.0 (not from sources) made things work, thanks @onedarkflame ", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 30 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 18517, "title": "Fix rendering of documentation of tf.contrib.summary.", "body": "Add backticks for code formatting and python highlighting in the documentation.", "comments": ["Nagging Assignee @jhseu: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jhseu: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 18516, "title": "How to become a voluntary translator of tensorflow's official website in China\uff1f", "body": "I am a postgraduate student in computer science from China and have a special passion for deep learning. Due to its high degree of flexibility and powerful computing ability, tensorflow is my most commonly used deep learning framework. I have used it to do a lot of interesting things during my graduate studies. In China, tensorflow is one of the most commonly used deep learning frameworks for developers. However, due to some reasons, tensorflow cannot be normally accessed in China, which restricts the development of tensorflow to some extent, and at the same time it widens the distance between ordinary developers and deep learning. Recently, tensorflow China's official website (tensorflow.google.cn) went online, which is a very good news for Chinese developers. Developers no longer need to use some extra ways to learn to use tensorflow.For most experienced developers in China, reading English documents is not a hindrance. However, for beginners, whether they are new to programming or planning to get started with deep learning, due to the limitation of English reading ability, they are somewhat excluded from official documents, especially English official documents.Many people can only obtain the knowledge of debris through the Baidu search enginer and cannot have a systematic understanding of tensorflow.Due to the special love for google and the worship for tensorflow's open source spirit, I wish to be a tensorflow Chinese volunteer translator.I hope that my efforts will enable Chinese developers to use tensorflow more easily and efficiently. I also hope to have the privilege of expanding tensorflow's influence in China.For the tensorflow official documentation (tensorflow.org), I basically read all the documents and I feel competent to perform this honorable duty.The only question now is how to become a voluntary translator of tensorflow's official website in China. Thanks for your reading and hope to get a reply.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 18515, "title": "How to become a voluntary translator of tensorflow's official website in China\uff1f", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 18514, "title": "Build error from source", "body": "I encountered an error while building from source.some system infomation as follows:\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution**: Ubuntu 16.04(x64)\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: 0.10\r\n- **GCC/Compiler version (if compiling from source)**: 4.8\r\n- **CUDA/cuDNN version**: 9.0/7.1\r\n- **GPU model and memory**: Tesla K80\r\n- **Exact command to reproduce**: bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Source code / logs\r\nWARNING: /home/zhangzw/.cache/bazel/_bazel_root/0b0b28318d2a7ef283e46c49f1b108b6/external/protobuf_archive/WORKSPACE:1: Workspace name in /home/zhangzw/.cache/bazel/_bazel_root/0b0b28318d2a7ef283e46c49f1b108b6/external/protobuf_archive/WORKSPACE (@com_google_protobuf) does not match the name given in the repository's definition (@protobuf_archive); this will cause a build error in future versions\r\nWARNING: /home/zhangzw/.cache/bazel/_bazel_root/0b0b28318d2a7ef283e46c49f1b108b6/external/grpc/WORKSPACE:1: Workspace name in /home/zhangzw/.cache/bazel/_bazel_root/0b0b28318d2a7ef283e46c49f1b108b6/external/grpc/WORKSPACE (@com_github_grpc_grpc) does not match the name given in the repository's definition (@grpc); this will cause a build error in future versions\r\nWARNING: /home/zhangzw/.cache/bazel/_bazel_root/0b0b28318d2a7ef283e46c49f1b108b6/external/grpc/BUILD:1943:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/zhangzw/.cache/bazel/_bazel_root/0b0b28318d2a7ef283e46c49f1b108b6/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /home/zhangzw/.cache/bazel/_bazel_root/0b0b28318d2a7ef283e46c49f1b108b6/external/grpc/BUILD:1943:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/zhangzw/.cache/bazel/_bazel_root/0b0b28318d2a7ef283e46c49f1b108b6/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /home/zhangzw/.cache/bazel/_bazel_root/0b0b28318d2a7ef283e46c49f1b108b6/external/grpc/BUILD:1943:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/zhangzw/.cache/bazel/_bazel_root/0b0b28318d2a7ef283e46c49f1b108b6/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /home/zhangzw/tensorflow-1.8.0-rc0/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/zhangzw/tensorflow-1.8.0-rc0/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\n...\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 288.567s, Critical Path: 58.33s\r\nFAILED: Build did NOT complete successfully", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 18513, "title": "[INTEL MK] Updating MKL CPU CI build and test.", "body": "Setting  KMP_BLOCKTIME to 0  to  workaround the effects of an oversubscription of OpenMP threads caused by executing multiple tests concurrently. ", "comments": ["@gunan I added comments as requested. Thanks."]}, {"number": 18512, "title": "NoOp replacement in DependencyOptimizer misses some control dependency conversions", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nDebian testing (buster)\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n('v1.7.0-3-g024aecf414', '1.7.0')\r\n- **Python version**: \r\n2.7.14\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\nRun Python code below with environment variable `TF_CPP_MIN_VLOG_LEVEL=1`\r\n\r\n### Describe the problem\r\nWhen `DependencyOptimizer::OptimizeNode()` converts a node to a NoOp, it only converts the first input from each input node to a control dependencies. Multiple inputs from the same node can cause a NoOp to be created with non-control inputs. In the VLOG output below, the IdentityN node has inputs \"^Placeholder\" and \"Placeholder\". I haven't tried it but perhaps when [this condition](https://github.com/tensorflow/tensorflow/blob/92e6c3e4f5c1cabfda1e61547a6a1b268ef95fa5/tensorflow/core/grappler/optimizers/dependency_optimizer.cc#L214) is false, the input should be deleted (like in the [control input case](https://github.com/tensorflow/tensorflow/blob/92e6c3e4f5c1cabfda1e61547a6a1b268ef95fa5/tensorflow/core/grappler/optimizers/dependency_optimizer.cc#L204)). Note that the bad NoOp gets deleted in the small example here (because that was the easiest way to display the problem) but the real code I was running, the NoOp remains in the graph to cause trouble later.\r\n\r\nThis bug is pretty sneaky: a NoOp with non-control inputs can cause `GraphConstructor` methods to fail, which in turn (silently!) prevents graph optimization ([see here](https://github.com/tensorflow/tensorflow/blob/92e6c3e4f5c1cabfda1e61547a6a1b268ef95fa5/tensorflow/core/common_runtime/graph_execution_state.cc#L396)). The fallback original graph runs fine. I only went investigating because I saw output like this:\r\n\r\n```\r\n2018-04-12 13:09:26.465603: E tensorflow/core/framework/types.cc:102] Unrecognized DataType enum value 425390044\r\n```\r\n\r\nThe cause was an error message from `GraphConstructor::MakeEdge()`, called [here](https://github.com/tensorflow/tensorflow/blob/92e6c3e4f5c1cabfda1e61547a6a1b268ef95fa5/tensorflow/core/graph/graph_constructor.cc#L1008). It appears `node` and `node_def` did not agree on how many inputs there were to the node! `node` thought zero, which is where that strange DataType enum value came from: reading uninitialized memory [here](https://github.com/tensorflow/tensorflow/blob/92e6c3e4f5c1cabfda1e61547a6a1b268ef95fa5/tensorflow/core/graph/graph_constructor.cc#L1156). I wasn't able to reproduce that exact failure in a tiny example; I believe the example here causes optimization to fail [here](https://github.com/tensorflow/tensorflow/blob/92e6c3e4f5c1cabfda1e61547a6a1b268ef95fa5/tensorflow/core/graph/graph_constructor.cc#L442).\r\n\r\nIt would also be nice to see a warning when graph optimization fails.\r\n\r\n### Source code / logs\r\n\r\n#### Code to reproduce the problem\r\n```python\r\nimport tensorflow as tf\r\n\r\nwith tf.device('/cpu:0'):\r\n    x = tf.placeholder(dtype=tf.int32, shape=[])\r\n    id2 = tf.identity_n([x, x])\r\n    with tf.control_dependencies(id2):\r\n        y = x + 1\r\n\r\nwith tf.Session() as sess:\r\n    print sess.run(y, feed_dict={x: 5})\r\n```\r\n\r\n#### Relevant log output\r\n```\r\n2018-04-13 15:58:27.870639: I tensorflow/core/grappler/optimizers/dependency_optimizer.cc:194] ***** Replacing  IdentityN (IdentityN) with NoOp.\r\n2018-04-13 15:58:27.870678: I tensorflow/core/grappler/optimizers/dependency_optimizer.cc:332] ***** Rerouting input around\r\nname: \"IdentityN\"\r\nop: \"NoOp\"\r\ninput: \"^Placeholder\"\r\ninput: \"Placeholder\"\r\ndevice: \"/job:localhost/replica:0/task:0/device:CPU:0\"\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory", "@rmlarsen Can you take a look at this?", "This is fixed with latest version of TF '1.15.0-dev20190821'. Feel free to reopen if have any issues. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=18512\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=18512\">No</a>\n"]}, {"number": 18511, "title": "Branch 192842670", "body": "", "comments": []}, {"number": 18510, "title": "1.8.0-rc1 cherry-pick request: some fixes for boosted_trees", "body": "After the fixes:\r\n * Stopping training at the right moment, without extra layers.\r\n * BoostedTreesClassifier and BoostedTreesRegressors work well with batched Dataset now.\r\n * Dependency between resource serialization and prediction is set properly.\r\n\r\n@annarev This is the list of cherrypicks that I wish to merge. Please let me know if I need to do anything else. Thanks!\r\n", "comments": ["Thanks @yk5! Could you take a look at the build failure?", "Looks like the build failure is irrelevant.", "Hmm. Let me look at it. What's the timeline for Cherrypicks for RC1?\n\nOn Mon, Apr 16, 2018 at 10:51 AM Yifei Feng <notifications@github.com>\nwrote:\n\n> Thanks @yk5 <https://github.com/yk5>! Could you take a look at the build\n> failure?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/18510#issuecomment-381689571>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AXihKivmuCaWwbh5gWQOGd5MsJowPnv1ks5tpNoRgaJpZM4TUto1>\n> .\n>\n", "By EOD Tuesday 04/17. I will start building the binaries on Wednesday.", "Thanks!\n\nOn Tue, Apr 17, 2018 at 10:45 AM Yifei Feng <notifications@github.com>\nwrote:\n\n> Merged #18510 <https://github.com/tensorflow/tensorflow/pull/18510>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/18510#event-1579460244>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AXihKqc2nfxDPVD85krSPS_qmV7I86g9ks5tpiowgaJpZM4TUto1>\n> .\n>\n"]}, {"number": 18509, "title": "Add testing for TF-TRT", "body": "This pr adds a py_test with a trivial class to contrib/tensorrt. Tagging @aaroey ", "comments": ["@aaroey It should be done now. Thanks for the review."]}, {"number": 18508, "title": "[INTEL MKLDNN]: Upgrade mkldnn version to v13", "body": "this PR upgrades MKLDNN/ MKL versions on three platforms (linux, windows and mac) to V13.", "comments": ["@jhseu  the failing check seems not relate to the new MKLDNN version. It is a timeout error.", "@jhseu the failing check seems not relate to the new MKLDNN version. It is a timeout error. would be really appreciate if the test can be re-trigger again? thanks", "The error is a known failure. Merging.", "@jhseu thanks!"]}, {"number": 18507, "title": "Branch 192821482", "body": "Fixing XLA test", "comments": ["@yifeif the windows build seems broken, which might related to cl/192784701, can u take a look?", "`ERROR: 'remote' is an invalid value for spawn strategy`\r\n@meteorcloudy is this related to the new remote_cache flag or bazel version upgrade?", "Its already Friday midnight in Germany. Do u want to do a rollback for that change?", "Also, seems that the OS presubmit test is bit different from the internal test, which did not catch the error.", "Let's revert. I'll send you a change.", "Closing because the other pull request has been merged."]}, {"number": 18506, "title": "Error when building Tensorflow-GPU from source on windows", "body": "\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n No.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 8.1\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\nr1.7\r\n- **Python version**: \r\n3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n9.0\r\n- **GPU model and memory**:\r\nNVIDIA Geforce GT 740M\r\n- **Exact command to reproduce**:\r\nFirst cmake works fine with:\r\n\r\n> \r\n> cmake .. -A x64  -DCMAKE_BUILD_TYPE=Release ^\r\n> -DSWIG_EXECUTABLE=C:/swigwin-3.0.12/swig.exe  ^\r\n> -DPYTHON_EXECUTABLE=C:/Users/M/Anaconda3/python.exe  ^\r\n> -DPYTHON_LIBRARIES=C:/Users/M/Anaconda3/libs/python35.lib  ^\r\n> -Dtensorflow_ENABLE_GPU=ON  ^\r\n> -DCUDNN_HOME=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\"  ^\r\n> -DCUDA_HOST_COMPILER=\"C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\bin\\cl.exe\" \r\n\r\nBut when I try to build it using\r\n\r\n> \"C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/MSBuild/15.0/Bin/MSBuild.exe\" \r\n> /p:Configuration=Release tf_tutorials_example_trainer.vcxproj\r\n\r\nthe errors appear (I'm not really familiar with C++, MSBuild , so pardon my naivity)\r\n\r\n\r\n### Describe the problem\r\nWhen building tensorflow using above command I get a lot of syntax errors and the process fails.\r\n\r\n### Source code / logs\r\n(Logs are in german, but they are basically warning about missing semicolons,brackets etc.)\r\nAn excerpt:\r\n\r\n>  \"C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\build\\ALL_BUILD\r\n>  .vcxproj\" (Standardziel) (1) ->\r\n>  \"C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\build\\_beam_sea\r\n>  rch_ops.vcxproj\" (Standardziel) (2) ->\r\n>  \"C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_te\r\n>  nsorflow_internal.vcxproj\" (Standardziel) (3) ->\r\n>  \"C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_te\r\n>  nsorflow_internal_static.vcxproj\" (Standardziel) (4) ->\r\n>  \"C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_c.vcxp\r\n>  roj\" (Standardziel) (6) ->\r\n>  \"C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_cc_fra\r\n>  mework.vcxproj\" (Standardziel) (7) ->\r\n>  \"C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_f\r\n>  ramework.vcxproj\" (Standardziel) (9) ->\r\n>  \"C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\build\\proto_tex\r\n>  t.vcxproj\" (Standardziel) (10) ->\r\n>  \"C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\build\\grpc.vcxp\r\n>  roj\" (Standardziel) (11) ->\r\n>  (CustomBuild Ziel) ->\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(508): error C2146: Syntaxfehler: Fehlendes \")\" vor Bezeichner \"Stati\r\n>  stics\" [C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\build\\gr\r\n>  pc\\src\\grpc\\third_party\\cares\\cares\\c-ares.vcxproj] [C:\\tesnorflow_build\r\n>  \\tensorflow\\tensorflow\\contrib\\cmake\\build\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(508): error C2061: Syntaxfehler: Bezeichner \"Statistics\" [C:\\tesnorf\r\n>  low_build\\tensorflow\\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_\r\n>  party\\cares\\cares\\c-ares.vcxproj] [C:\\tesnorflow_build\\tensorflow\\tensor\r\n>  flow\\contrib\\cmake\\build\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(508): error C2059: Syntaxfehler: \";\" [C:\\tesnorflow_build\\tensorflow\r\n>  \\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_party\\cares\\cares\\c-\r\n>  ares.vcxproj] [C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\b\r\n>  uild\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(508): error C2059: Syntaxfehler: \",\" [C:\\tesnorflow_build\\tensorflow\r\n>  \\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_party\\cares\\cares\\c-\r\n>  ares.vcxproj] [C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\b\r\n>  uild\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(510): error C2059: Syntaxfehler: \")\" [C:\\tesnorflow_build\\tensorflow\r\n>  \\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_party\\cares\\cares\\c-\r\n>  ares.vcxproj] [C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\b\r\n>  uild\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(515): error C2146: Syntaxfehler: Fehlendes \")\" vor Bezeichner \"Stati\r\n>  stics\" [C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\build\\gr\r\n>  pc\\src\\grpc\\third_party\\cares\\cares\\c-ares.vcxproj] [C:\\tesnorflow_build\r\n>  \\tensorflow\\tensorflow\\contrib\\cmake\\build\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(515): error C2061: Syntaxfehler: Bezeichner \"Statistics\" [C:\\tesnorf\r\n>  low_build\\tensorflow\\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_\r\n>  party\\cares\\cares\\c-ares.vcxproj] [C:\\tesnorflow_build\\tensorflow\\tensor\r\n>  flow\\contrib\\cmake\\build\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(515): error C2059: Syntaxfehler: \";\" [C:\\tesnorflow_build\\tensorflow\r\n>  \\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_party\\cares\\cares\\c-\r\n>  ares.vcxproj] [C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\b\r\n>  uild\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(515): error C2059: Syntaxfehler: \",\" [C:\\tesnorflow_build\\tensorflow\r\n>  \\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_party\\cares\\cares\\c-\r\n>  ares.vcxproj] [C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\b\r\n>  uild\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(517): error C2059: Syntaxfehler: \")\" [C:\\tesnorflow_build\\tensorflow\r\n>  \\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_party\\cares\\cares\\c-\r\n>  ares.vcxproj] [C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\b\r\n>  uild\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(508): error C2146: Syntaxfehler: Fehlendes \")\" vor Bezeichner \"Stati\r\n>  stics\" [C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\build\\gr\r\n>  pc\\src\\grpc\\third_party\\cares\\cares\\c-ares.vcxproj] [C:\\tesnorflow_build\r\n>  \\tensorflow\\tensorflow\\contrib\\cmake\\build\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(508): error C2061: Syntaxfehler: Bezeichner \"Statistics\" [C:\\tesnorf\r\n>  low_build\\tensorflow\\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_\r\n>  party\\cares\\cares\\c-ares.vcxproj] [C:\\tesnorflow_build\\tensorflow\\tensor\r\n>  flow\\contrib\\cmake\\build\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(508): error C2059: Syntaxfehler: \";\" [C:\\tesnorflow_build\\tensorflow\r\n>  \\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_party\\cares\\cares\\c-\r\n>  ares.vcxproj] [C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\b\r\n>  uild\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(508): error C2059: Syntaxfehler: \",\" [C:\\tesnorflow_build\\tensorflow\r\n>  \\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_party\\cares\\cares\\c-\r\n>  ares.vcxproj] [C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\b\r\n>  uild\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(510): error C2059: Syntaxfehler: \")\" [C:\\tesnorflow_build\\tensorflow\r\n>  \\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_party\\cares\\cares\\c-\r\n>  ares.vcxproj] [C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\b\r\n>  uild\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(515): error C2146: Syntaxfehler: Fehlendes \")\" vor Bezeichner \"Stati\r\n>  stics\" [C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\build\\gr\r\n>  pc\\src\\grpc\\third_party\\cares\\cares\\c-ares.vcxproj] [C:\\tesnorflow_build\r\n>  \\tensorflow\\tensorflow\\contrib\\cmake\\build\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(515): error C2061: Syntaxfehler: Bezeichner \"Statistics\" [C:\\tesnorf\r\n>  low_build\\tensorflow\\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_\r\n>  party\\cares\\cares\\c-ares.vcxproj] [C:\\tesnorflow_build\\tensorflow\\tensor\r\n>  flow\\contrib\\cmake\\build\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(515): error C2059: Syntaxfehler: \";\" [C:\\tesnorflow_build\\tensorflow\r\n>  \\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_party\\cares\\cares\\c-\r\n>  ares.vcxproj] [C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\b\r\n>  uild\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(515): error C2059: Syntaxfehler: \",\" [C:\\tesnorflow_build\\tensorflow\r\n>  \\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_party\\cares\\cares\\c-\r\n>  ares.vcxproj] [C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\b\r\n>  uild\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(517): error C2059: Syntaxfehler: \")\" [C:\\tesnorflow_build\\tensorflow\r\n>  \\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_party\\cares\\cares\\c-\r\n>  ares.vcxproj] [C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\b\r\n>  uild\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(508): error C2146: Syntaxfehler: Fehlendes \")\" vor Bezeichner \"Stati\r\n>  stics\" [C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\build\\gr\r\n>  pc\\src\\grpc\\third_party\\cares\\cares\\c-ares.vcxproj] [C:\\tesnorflow_build\r\n>  \\tensorflow\\tensorflow\\contrib\\cmake\\build\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(508): error C2061: Syntaxfehler: Bezeichner \"Statistics\" [C:\\tesnorf\r\n>  low_build\\tensorflow\\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_\r\n>  party\\cares\\cares\\c-ares.vcxproj] [C:\\tesnorflow_build\\tensorflow\\tensor\r\n>  flow\\contrib\\cmake\\build\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(508): error C2059: Syntaxfehler: \";\" [C:\\tesnorflow_build\\tensorflow\r\n>  \\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_party\\cares\\cares\\c-\r\n>  ares.vcxproj] [C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\b\r\n>  uild\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(508): error C2059: Syntaxfehler: \",\" [C:\\tesnorflow_build\\tensorflow\r\n>  \\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_party\\cares\\cares\\c-\r\n>  ares.vcxproj] [C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\b\r\n>  uild\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(510): error C2059: Syntaxfehler: \")\" [C:\\tesnorflow_build\\tensorflow\r\n>  \\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_party\\cares\\cares\\c-\r\n>  ares.vcxproj] [C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\b\r\n>  uild\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(515): error C2146: Syntaxfehler: Fehlendes \")\" vor Bezeichner \"Stati\r\n>  stics\" [C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\build\\gr\r\n>  pc\\src\\grpc\\third_party\\cares\\cares\\c-ares.vcxproj] [C:\\tesnorflow_build\r\n>  \\tensorflow\\tensorflow\\contrib\\cmake\\build\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(515): error C2061: Syntaxfehler: Bezeichner \"Statistics\" [C:\\tesnorf\r\n>  low_build\\tensorflow\\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_\r\n>  party\\cares\\cares\\c-ares.vcxproj] [C:\\tesnorflow_build\\tensorflow\\tensor\r\n>  flow\\contrib\\cmake\\build\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(515): error C2059: Syntaxfehler: \";\" [C:\\tesnorflow_build\\tensorflow\r\n>  \\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_party\\cares\\cares\\c-\r\n>  ares.vcxproj] [C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\b\r\n>  uild\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(515): error C2059: Syntaxfehler: \",\" [C:\\tesnorflow_build\\tensorflow\r\n>  \\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_party\\cares\\cares\\c-\r\n>  ares.vcxproj] [C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\b\r\n>  uild\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(517): error C2059: Syntaxfehler: \")\" [C:\\tesnorflow_build\\tensorflow\r\n>  \\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_party\\cares\\cares\\c-\r\n>  ares.vcxproj] [C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\b\r\n>  uild\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(508): error C2146: Syntaxfehler: Fehlendes \")\" vor Bezeichner \"Stati\r\n>  stics\" [C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\build\\gr\r\n>  pc\\src\\grpc\\third_party\\cares\\cares\\c-ares.vcxproj] [C:\\tesnorflow_build\r\n>  \\tensorflow\\tensorflow\\contrib\\cmake\\build\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(508): error C2061: Syntaxfehler: Bezeichner \"Statistics\" [C:\\tesnorf\r\n>  low_build\\tensorflow\\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_\r\n>  party\\cares\\cares\\c-ares.vcxproj] [C:\\tesnorflow_build\\tensorflow\\tensor\r\n>  flow\\contrib\\cmake\\build\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(508): error C2059: Syntaxfehler: \";\" [C:\\tesnorflow_build\\tensorflow\r\n>  \\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_party\\cares\\cares\\c-\r\n>  ares.vcxproj] [C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\b\r\n>  uild\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(508): error C2059: Syntaxfehler: \",\" [C:\\tesnorflow_build\\tensorflow\r\n>  \\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_party\\cares\\cares\\c-\r\n>  ares.vcxproj] [C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\b\r\n>  uild\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(510): error C2059: Syntaxfehler: \")\" [C:\\tesnorflow_build\\tensorflow\r\n>  \\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_party\\cares\\cares\\c-\r\n>  ares.vcxproj] [C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\b\r\n>  uild\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(515): error C2146: Syntaxfehler: Fehlendes \")\" vor Bezeichner \"Stati\r\n>  stics\" [C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\build\\gr\r\n>  pc\\src\\grpc\\third_party\\cares\\cares\\c-ares.vcxproj] [C:\\tesnorflow_build\r\n>  \\tensorflow\\tensorflow\\contrib\\cmake\\build\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(515): error C2061: Syntaxfehler: Bezeichner \"Statistics\" [C:\\tesnorf\r\n>  low_build\\tensorflow\\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_\r\n>  party\\cares\\cares\\c-ares.vcxproj] [C:\\tesnorflow_build\\tensorflow\\tensor\r\n>  flow\\contrib\\cmake\\build\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(515): error C2059: Syntaxfehler: \";\" [C:\\tesnorflow_build\\tensorflow\r\n>  \\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_party\\cares\\cares\\c-\r\n>  ares.vcxproj] [C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\b\r\n>  uild\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(515): error C2059: Syntaxfehler: \",\" [C:\\tesnorflow_build\\tensorflow\r\n>  \\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_party\\cares\\cares\\c-\r\n>  ares.vcxproj] [C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\b\r\n>  uild\\grpc.vcxproj]\r\n>    C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\include\\iphlpap\r\n>  i.h(517): error C2059: Syntaxfehler: \")\" [C:\\tesnorflow_build\\tensorflow\r\n>  \\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\third_party\\cares\\cares\\c-\r\n>  ares.vcxproj] [C:\\tesnorflow_build\\tensorflow\\tensorflow\\contrib\\cmake\\b\r\n>  uild\\grpc.vcxproj]\r\n", "comments": ["For Windows, we use `cmake` to build the release binaries and it seems that is working well for you?\r\n\r\nWe're not familiar enough with builds via `MSBuild` or Visual Studio, and rely on community support for that.\r\n", "Closing as the current build supports VS"]}, {"number": 18505, "title": "Add extra code owner for tensor_forest", "body": "Add myself for that", "comments": ["@martinwicke "]}, {"number": 18504, "title": "Use eager compatible wrappers in load_library for custom ops", "body": "", "comments": []}, {"number": 18503, "title": "[Install] Failed to load the native TensorFlow runtime.", "body": "### System information\r\n- **Have I written custom code**: Yes\r\n- **OS Platform and Distribution**: windows 8.1 x64\r\n- **TensorFlow installed from**: have no idea, I suppose binary\r\n- **TensorFlow version**: 1.7\r\n- **Python version**: 3.6.5\r\n- **Bazel version**: ???\r\n- **GCC/Compiler version**: ?\r\n- **CUDA/cuDNN version**: cuDNN 7.0 for CUDA 9.0\r\n- **GPU model and memory**: tensorflow-gpu, NVIDIA 4GHZ dualcore, 8GB RAM\r\n- **Exact command to reproduce**: `python test.py`\r\n\r\n### Describe the problem\r\n\r\nI downloaded tensorflow and installed everything properly and getting error (click on it to expand):\r\n\r\n<details>\r\n<summary>\r\nError\r\n</summary>\r\n\r\n```console\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\abcdef\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\r\n\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\abcdef\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\r\n\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\abcdef\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\r\n\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in swig_import_helpe\r\nr\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\abcdef\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__i\r\nnit__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routin\r\ne failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\abcdef\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\r\n\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *  # pylint: disable=redefined-builtin\r\n  File \"C:\\Users\\abcdef\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\r\n\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\abcdef\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\r\n\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\abcdef\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\r\n\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\abcdef\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\r\n\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\abcdef\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\r\n\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in swig_import_helpe\r\nr\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\abcdef\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__i\r\nnit__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routin\r\ne failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_probl\r\nems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\n</details><br/>\r\n\r\nAsked on stack overflow and they suggested me to report an issue here because they dont know the answer. I also was thinking that I did something wrong, but maybe there is a bug somewhere. I installed everything needed, including cuda, cudnn, python latest, pip and I used pip to install tensorflow gpu\r\n\r\n### Source code\r\n\r\n```python\r\nimport tensorflow as tf\r\nhello = tf.constant('Hello, TensorFlow!')\r\nsess = tf.Session()\r\nprint(sess.run(hello))\r\n```\r\n\r\n### Edit\r\n\r\nHere is my `%PATH%` environment variable:\r\n\r\n<details>\r\n<summary>\r\nPath\r\n</summary>\r\n\r\n```console\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin;C:\\Program Files\\NVI\r\nDIA GPU Computing Toolkit\\CUDA\\v9.0\\libnvvp;C:\\Program Files (x86)\\Common Files\\\r\nIntel\\Shared Files\\cpp\\bin\\Intel64;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Window\r\ns\\System32\\WindowsPowerShell\\v1.0;C:\\Program Files (x86)\\ATI Technologies\\ATI.AC\r\nE\\Core-Static;C:\\Program Files (x86)\\Windows Kits\\10\\Windows Performance Toolkit\r\n;C:\\MinGW\\bin;C:\\Program Files (x86)\\Windows Kits\\8.0\\Windows Performance Toolki\r\nt;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\Makefi\r\nle;C:\\windows\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC;C:\r\n\\Users\\abcdef\\AppData\\Local\\Programs\\Python\\Python36\\Scripts;C:\\Users\\abcdef\\App\r\nData\\Local\\Programs\\Python\\Python36;C:\\Program Files (x86)\\Microsoft Visual Stud\r\nio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.10.25017\\bin\\HostX64\\x64\r\n```\r\n</details>", "comments": ["try to install tensorflow version 1.5:\r\n\r\npip install --upgrade --ignore-installed tensorflow-gpu==1.5\r\n", "@sirjo66 \r\n\r\nI just tried it and it works. Here is the output:\r\n\r\n<details><summary>Output</summary>\r\n\r\n```console\r\n> pip install --upgrade --ignore-installed tensorflow-gpu==1.5\r\nCollecting tensorflow-gpu==1.5\r\n  Downloading tensorflow_gpu-1.5.0-cp36-cp36m-win_amd64.whl (82.1MB)\r\nCollecting protobuf>=3.4.0 (from tensorflow-gpu==1.5)\r\n  Using cached protobuf-3.5.2.post1-cp36-cp36m-win_amd64.whl\r\nCollecting six>=1.10.0 (from tensorflow-gpu==1.5)\r\n  Using cached six-1.11.0-py2.py3-none-any.whl\r\nCollecting tensorflow-tensorboard<1.6.0,>=1.5.0 (from tensorflow-gpu==1.5)\r\n  Downloading tensorflow_tensorboard-1.5.1-py3-none-any.whl (3.0MB)\r\nCollecting wheel>=0.26 (from tensorflow-gpu==1.5)\r\n  Using cached wheel-0.31.0-py2.py3-none-any.whl\r\nCollecting numpy>=1.12.1 (from tensorflow-gpu==1.5)\r\n  Using cached numpy-1.14.2-cp36-none-win_amd64.whl\r\nCollecting absl-py>=0.1.6 (from tensorflow-gpu==1.5)\r\n  Using cached absl-py-0.1.13.tar.gz\r\nCollecting setuptools (from protobuf>=3.4.0->tensorflow-gpu==1.5)\r\n  Downloading setuptools-39.0.1-py2.py3-none-any.whl (569kB)\r\nCollecting html5lib==0.9999999 (from tensorflow-tensorboard<1.6.0,>=1.5.0->tenso\r\nrflow-gpu==1.5)\r\n  Using cached html5lib-0.9999999.tar.gz\r\nCollecting markdown>=2.6.8 (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflo\r\nw-gpu==1.5)\r\n  Using cached Markdown-2.6.11-py2.py3-none-any.whl\r\nCollecting bleach==1.5.0 (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow-\r\ngpu==1.5)\r\n  Using cached bleach-1.5.0-py2.py3-none-any.whl\r\nCollecting werkzeug>=0.11.10 (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorf\r\nlow-gpu==1.5)\r\n  Using cached Werkzeug-0.14.1-py2.py3-none-any.whl\r\nBuilding wheels for collected packages: absl-py, html5lib\r\n  Running setup.py bdist_wheel for absl-py: started\r\n  Running setup.py bdist_wheel for absl-py: finished with status 'done'\r\n  Stored in directory: C:\\Users\\abcdef\\AppData\\Local\\pip\\Cache\\wheels\\76\\f7\\0c\\8\r\n  8796d7212af59bb2f496b12267e0605f205170781e9b86479\r\n  Running setup.py bdist_wheel for html5lib: started\r\n  Running setup.py bdist_wheel for html5lib: finished with status 'done'\r\n  Stored in directory: C:\\Users\\abcdef\\AppData\\Local\\pip\\Cache\\wheels\\6f\\85\\6c\\5\r\n  6b8e1292c6214c4eb73b9dda50f53e8e977bf65989373c962\r\nSuccessfully built absl-py html5lib\r\nInstalling collected packages: six, setuptools, protobuf, html5lib, wheel, markd\r\nown, bleach, werkzeug, numpy, tensorflow-tensorboard, absl-py, tensorflow-gpu\r\nSuccessfully installed absl-py-0.1.13 bleach-1.5.0 html5lib-0.9999999 markdown-2\r\n.6.11 numpy-1.14.2 protobuf-3.5.2.post1 setuptools-39.0.1 six-1.11.0 tensorflow-\r\ngpu-1.7.0 tensorflow-tensorboard-1.5.1 werkzeug-0.14.1 wheel-0.31.0\r\n\r\n> call python main.py\r\nb'Hello, TensorFlow!'\r\n```\r\n\r\n</details><br/>\r\n\r\nThank you for help. But, I'm still wondering why the latest version doesn't work... ?", "I don't know, \r\nI have the same problem: \r\n[https://github.com/tensorflow/tensorflow/issues/18530](https://github.com/tensorflow/tensorflow/issues/18530)\r\n", "Nagging Assignee @tatatodd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Is anyone reviewing these issues? Here is the @tatatodd 's activity on github in the last year (he/she is assigned to this issue, but as it can be seen from their activity, I don't think they are going to review this issue soon):\r\n\r\n![](https://user-images.githubusercontent.com/38354752/39446946-771eb658-4cc0-11e8-903e-7abe65cc68af.png)\r\n\r\nCan assignee be changed? I mean, I'm waiting more than two weeks for this issue to be resolved while I'm still using old Tensorflow.\r\n\r\n@gunan ?", "I think that the reason is because your CPU doesn't have AVX feature.\r\nTensorflow from version 1.6 needs this feature on CPU\r\nWhat is your CPU ???\r\n\r\n\r\n", "@sirjo66 \r\n\r\nMy CPU is `Intel Pentium CPU G3260 @ 3.30GHz`\r\nI have no idea if it supports avx or not, but as I can see, your and mine error message are different, so I'm not sure if our issues are the same.", "yes, your cpu don't support AVX feature, so for me this is the problem", "@sirjo66 \r\n\r\nCan you please tell me where have you found that information?", "You can use CPU-Z program, here is a screenshot for your CPU:\r\n[http://cdn.cpu-world.com/Images/uploaded/0001/11/L_00011144.jpg](http://cdn.cpu-world.com/Images/uploaded/0001/11/L_00011144.jpg)\r\nAs you can see there isn't AVX in \"instructions\"section\r\n\r\nYou can also see this page:\r\n[http://cpuboss.com/cpus/Intel-Pentium-G3260-vs-Intel-Core-i3-4160](http://cpuboss.com/cpus/Intel-Pentium-G3260-vs-Intel-Core-i3-4160)\r\n\r\nAs you can see at \"Features\" section, AVX is \"no\"\r\n\r\n", "@sirjo66 \r\n\r\nI asked on the official Intel website and the Intel developer said that this type of processor **does** support AVX... I'm wondering who is right...\r\n\r\nRef: https://communities.intel.com/message/540453\r\n\r\nHere is their response:\r\n\r\n> Yes, the Pentium G3260 supports the AVX instruction set. Per ark.intel.com: Intel Pentium Processor G3260 (3M Cache, 3.30 GHz), this processor is based upon the Haswell microarchitecture. Per Wikipedia: Advanced Vector Extensions, the Haswell microarchitecture supports the AVX instruction set.", "Unfortunately, this CPU while formerly haswell, does not seem to have AVX. You can see \"Instruction set Extensions\" line in \"Advanced Technologies\" section in Intel ARK website:\r\nhttps://ark.intel.com/products/87356/Intel-Pentium-Processor-G3260-3M-Cache-3_30-GHz\r\n\r\nCompare that to this core i7 processor, that has ivy bridge architecture:\r\nhttps://ark.intel.com/products/64891/Intel-Core-i7-3720QM-Processor-6M-Cache-up-to-3_60-GHz", "This is hilarious \ud83d\ude02 \ud83d\ude02 \ud83d\ude02\r\n\r\nEvery article on the internet I've found so far says that G3260 doesn't support AVX. And I also executed one of AVX instructions from C++ code and got system exception: `Illegal Instruction`. But Intel developers are still strongly claiming that my cpu supports AVX.", "yes, very hilarious \ud83d\ude02\r\n\r\nbut are you sure that on the \"official Intel website\" the person that have answer to you is an \"Intel developer\" ?\r\nOr he has a normal \"forum user\"  ?\r\n", "Anyways, I think the situation is clear. Nothing can be done by TensorFlow team, I need to buy new CPU. So, I'm closing this issue. Thanks gunan and sirjo66 for help.", "I have the same problem. I tried everything but it is not solved.\r\nplease help\r\n", "same here i guess the problem is with CPU\r\n#confused", "For me, I just needed to use 3.6 instead of 3.7", "> try to install tensorflow version 1.5:\r\n> \r\n> pip install --upgrade --ignore-installed tensorflow-gpu==1.5\r\n\r\nThis worked for me. Thank you.", "I had same problem, probably becouse of CPU. Best sollution - Anaconda. xD  Seriously it will solve everything +without downgrading anything.\r\n-Just uninstall everything(python,Cuda,cuDNN) clear PATHS.. Conda will get everything. \r\n-Download Annaconda it will come with python 3.7(and will work with tensorflow^_^) \r\n-when install make mark so it sets up everything in  PATH or set PATH yourself\r\n-run in cmd: \u201cconda install tensorflow-gpu\u201d\r\nwoullaa I got CUDA-10.0.13/cuDNN-7.3.1 and Tensorflow-gpu 1.13.1\r\nand when time comes -to update every package \"conda update --all\"\r\nBest of Luck!", "If you encounter this problem with TensorFlow2 (i.e. tensorflow version 2.0.0a0), try:\r\npip install tensorflow-gpu==2.0.0-alpha0\r\n\r\nFor me it solved the problem when none of the other suggestions here did.. ", "import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\KARAN AGGARWAL\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\KARAN AGGARWAL\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\KARAN AGGARWAL\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\KARAN AGGARWAL\\AppData\\Local\\Programs\\Python\\Python35\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\KARAN AGGARWAL\\AppData\\Local\\Programs\\Python\\Python35\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<pyshell#0>\", line 1, in <module>\r\n    import tensorflow\r\n  File \"C:\\Users\\KARAN AGGARWAL\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\KARAN AGGARWAL\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\KARAN AGGARWAL\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\KARAN AGGARWAL\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\KARAN AGGARWAL\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\KARAN AGGARWAL\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\KARAN AGGARWAL\\AppData\\Local\\Programs\\Python\\Python35\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\KARAN AGGARWAL\\AppData\\Local\\Programs\\Python\\Python35\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "what is this error", "Hey, guys. I tell a story about me, last time I import TensorFlow in its source file directory, it happened this fault, and I use \"cd ..\" (then import TensorFlow)solve this problem. Maybe it's not one problem, but may this help.\r\n![source directory  import tensorflow](https://user-images.githubusercontent.com/50257188/61941358-43620100-afd2-11e9-95b7-a1ec98015c79.png)\r\n", "I experienced this problem when I tried to use tensorflow in jupyter notebook after I installed tensorflow-gpu in one of my conda env. I ran my codes well when I activate the env, and use command line (conda activate 'env name' --> python --> import tensorflow as tf ). Then I realized I lauch the jupyter notebook outside the 'env'. My solution is install jupyter notebook in 'env' and then launch jupyter notebook inside the 'env'. When I use tensorflow again in jypnb, the problem of 'failed to load the tensorflow runtime' is gone! --Just in case some one has the same problem.", "@xiaoyanzhuo  I am wondering if I could use your method when I am running the program in command line? My error comes from the command line", "> @xiaoyanzhuo I am wondering if I could use your method when I am running the program in command line? My error comes from the command line\r\n\r\nYou can try. Make sure you use tensorflow in the right installation location (like my case is jupyter notebook and tensorflow are both installed in same conda env then it works). I believe it works for both jupyter notebook or command line as long as you did have installed tensorflow when you call it.", "my processor is i5-650@ 3.20GHz and i have the same issue ,and it works fine with tensorflow 1.5 version,can somone tell me even this processor doesn,t support avx feature?", "For people still facing this issue and know their CPU is compatible, try updating your Visual C++ redistributable; Can be found here. https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads  "]}, {"number": 18502, "title": "Use eager compatible wrappers in load_library for custom ops", "body": "", "comments": []}, {"number": 18501, "title": "Enable mobile libtensorflow_cc.so", "body": "Hey team TF!\r\nI'm not expecting a merge on this, but wanted to submit it for posterity. I've been working for a few years on a project where we use tensorflow as part of a C++ application running simultaneously on armv7, arm64, and ubuntu x64, additionally utilizing ceres, protobuf, opencv and eigen in our pipeline.\r\n\r\nThere seems to be a throng of different versions of tensorflow within the same build system, all with different capabilities, APIs and build instructions; so my recurring torment has been attempting to update tensorflow and consolidate those versions and dependencies into something that works seamlessly for our application. I had hope TF Lite would help solve this, but since upsampling ops seem to be missing still, its impossible for me to use as of yet, though I do have hope for the future. :) \r\n\r\nDuring my previous endavour, I ended up customizing //tensorflow/contrib/makefile, which combined with android-ndk-r11 and a bunch of skitty rules was made to work for all three platforms. Unfortunately, this meant a mess of a build system that wasn't always up to date with the bazel core, and no chance of reliable optimized builds.\r\n\r\nSo for this time around, I decided to update to NDKr15 and look into all the various errors preventing the full libtensorflow_cc.so from building, resolving them to the best of my efforts, while caring little for breaking the lackluster libtensorflow_inference.so adaptation, which to my despair seemed to have no C++ API and zero documentation for native usage.\r\n\r\nThis PR resolves all build- and link-preventing issues in libtensorflow_cc.so and all of its dependencies in r1.7, whilst breaking and removing all android build graph branches to force a full build as close to ubuntu libtensorflow_cc.so as possible.\r\n\r\nThe majority of all incompatibilities with android seem to be related to android STL library issues, especially with functions from STL math libraries like std::asinh, which for android are placed in a global namespace (::asinh). I tried to resolve these with solutions that would work equally well on all platforms.\r\n\r\nThe remainder are discrepancies with ubuntu, e.g. pthread which is included implicitly in android, missing string functions and lack of bfloat16-support. These were resolved while making an effort to maintain multi-platform support. \r\n\r\nI'll also add that I have not yet had a chance to test this on a physical mobile device, but it seems promising and compiles on armv7 thus far. \r\n\r\nThe build command I'm using contains additional flags necessary for android compatibility, and may be somewhat daunting:\r\nbazel build --config=monolithic --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=TARGET_PLATFORM //tensorflow:libtensorflow_cc.so --cxxopt=\"-std=c++11\" --copt=\"-DMDB_USE_ROBUST=0\" --cxxopt=\"-DTENSORFLOW_DISABLE_META\" --cxxopt=\"-DEIGEN_HAS_C99_MATH\" --cxxopt=\"-Wno-c++11-narrowing\"\r\n\r\nNotes on this command:\r\nlmdb attempts to use an (on android) undefined PTHREAD_MUTEX_ROBUST, so disable that.\r\nTENSORFLOW_DISABLE_META seems to resolve some build issues for android inside tensorflow and was cut from a filed issue.\r\nauto detecting EIGEN_HAS_C99_MATH seems to fail on android, so enable it manually to overcome static asserts for missing functions.\r\nDisabling the warning for c++11-narrowing is required for \"Eigen::DSizes<Eigen::DenseIndex, 2> slice_indices{position, 0};\" to compile in batch_kernels.cc - since the Concat operation calling it is templated with long long, and Eigen seems to expect a signed int in some circumstances. This may be a cause for concern.\r\n\r\nI'm hoping some parts of this can be useful either for team TF, or for external developers who like me need to integrate on android within a modern C++ environment, and for various reasons cannot use TF Lite.", "comments": ["We're not accepting anymore changes to the 1.7 branch. Did you want to move this to master instead?", "@jhseu I considered it, but right now this version is not directly compatible with master or v1.8-rc0, and as I describe in the commit, it contains breaking changes and is not suitable for a merge, so I would say its better to leave it here for any TF user who needs full C++ Tensorflow on Android and has reasons to avoid the makefile contrib.\r\nIf you take a guick gander at the patch and read through my comment above, is there anything you'd have a use for in master and would like me to provide a separate patch for?", "If building for c++_shared using ndk r14b+, consider this:\r\n\r\nsetup workspace\r\n./configure\r\nbazel build\r\ncd bazel-tensorflow/external/androidndk/ndk/sources/cxx-stl/\r\ncp llvm-libc++ libcxx && mv libcxx llvm-libc++/libcxx\r\ncp llvm-libc++abi libcxxabi && mv libcxxabi llvm-libc++/libcxxabi\r\n\r\nThis is because llvm-libc++ include paths have changed, leaving bazel-tensorflow/external/androidndk/BUILD.bazel in a bad state. \r\nInforming bazel devs as well, but this is a sufficient workaround.\r\n\r\nThen modify your build command:\r\nbazel build --config=monolithic --crosstool_top=@androidndk//:toolchain-libcpp --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a //tensorflow:libtensorflow_cc.so --cxxopt=\"-std=c++11\" --copt=\"-DMDB_USE_ROBUST=0\" --cxxopt=\"-DTENSORFLOW_DISABLE_META\" --cxxopt=\"-DEIGEN_HAS_C99_MATH\" --cxxopt=\"-Wno-c++11-narrowing\" --verbose_failures --copt=\"-DS_IREAD=00400\" --copt=\"-DS_IWRITE=00200\"\r\n\r\nS_IREAD and S_IWRITE are used by the gif library, but not included in c++_shared, and flags __HAS_BSD or __HAS_GNU to override may have other side effects.\r\n\r\nSome other changes in the PR may be superfluous as c++_shared is a more complete c++11 implementation compared to the bazel default.", "Added @aselle to comment whether there's something here we could use. We're unlikely to merge it as-is because the r1.7 branch is closed..", "After considerations and some bazel exercises, I'm updating to 1.8, bazel 0.12 and NDK r16 for the resolved compatibility issues, so I'll be modifying this shortly.", "Nagging Assignee @jhseu: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jhseu: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jhseu: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jhseu: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jhseu: It has been 60 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I don't have enough context to know whether to accept similar changes for master, but feel free to make a new pull request against master instead.", "@jhseu I'm not sure if it still merges on master, but I updated with #18553 for r1.8 ages ago, feel free to follow up there. :) "]}, {"number": 18500, "title": "closure proto library for example protos", "body": "Build TF example protos using closure_proto_library so they can be used by TesnsorBoard TS/JS code in the future.", "comments": []}, {"number": 18499, "title": "Cherry-picking PR #18444 into r1.8", "body": "This PR is a cherry-pick of #18444 and fixes a static linkage issue that breaks int8 path in Tensorflow-TensorRT integration.", "comments": []}, {"number": 18498, "title": "Fixed the bug in mkl_input_conversion_op when reorder is not needed", "body": "In mkl_input_conversion, if one tensor is mkl tensor an another one tf tensor, we check if broadcast is needed. If broadcast is not needed, tf tensor is converted to mkl tensor. However, when tf tensor and mkl tensor has the shape, mkldnn reorder is not run, the output buffer is not updated, so the output buffer is random data.\r\n\r\nThe fix is to copy the memory if reorder is not needed.", "comments": ["@shengfuintel Thanks for the fix. Please confirm that the corresponding unit tests passes on your end.", "I do not think my change will break so many XLA tests. Can you check? thanks.", "Looks like there are something wrong in your llvm environment. in //tensorflow/compiler/tests:adagrad_test_cpu, the compile error is:\r\n\r\ntensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:37:41: fatal error: llvm/CodeGen/CommandFlags.inc: No such file or directory\r\n1705\r\n #include \"llvm/CodeGen/CommandFlags.inc\"", "Is it ready to merge? thanks."]}, {"number": 18497, "title": "Branch 192771889", "body": "Third attempt to push to public.\r\n\r\nManually merged \r\n\r\n\ttensorflow/contrib/data/python/ops/BUILD\r\n\ttensorflow/python/framework/dtypes_test.py", "comments": ["XLA test is broken by internal env change, which has nothing to do with this PR. I think this is good to go now."]}]