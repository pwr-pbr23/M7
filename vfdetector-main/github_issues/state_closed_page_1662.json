[{"number": 3045, "title": "Fix some typos", "body": "Fix some typos\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n"]}, {"number": 3044, "title": "Deep MNIST Tutorial would benefit from some diagrams of the neural network architecture ", "body": "The basic version of this tutorial has a [nice diagram](https://www.tensorflow.org/versions/r0.9/images/softmax-regression-scalargraph.png) that makes it really clear what model the code is attempting to reproduce.\n\nThe _Deep MNIST for Experts_ tutorial however doesn't have such an image, especially for the Multilayer Convolutional Network that is the heart of the article. I had a hard time (and still do, a bit) visualising what exactly the model code is attempting to do. I think the tutorial would benefit from some diagrams.\n\nI am willing to contribute this if someone could handhold me a bit as to how the graphic above was generated and what the bar is for contributing to the docs.\n", "comments": ["I think that's reasonable to have. Just submit a PR, and we'll handle it from there. Images are a bit tricky for us to sync, so we might remove them from github and push them separately to the documentation site. Feel free to open a PR and CC me on it."]}, {"number": 3043, "title": "NameError: Name MNIST is not defined", "body": "I am following the [Deep MNIST](https://www.tensorflow.org/versions/r0.9/tutorials/mnist/pros/index.html#start-tensorflow-interactivesession) tutorial. After copy-pasting code till [here](https://www.tensorflow.org/versions/r0.9/tutorials/mnist/pros/index.html#evaluate-the-model), I get a NameError. The exact output is\n\n```\nTraceback (most recent call last):\n  File \"Deep_MNIST.py\", line 29, in <module>\n    batch = mnist.train.next_batch(50)\nNameError: name 'mnist' is not defined\n```\n\nThis is my file-\n\n```\nx = tf.placeholder(tf.float32, shape=[None, 784])\ny_ = tf.placeholder(tf.float32, shape=[None, 10])\n\nW = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))\n\nsess.run(tf.initialize_all_variables())\n\ny = tf.nn.softmax(tf.matmul(x,W) + b)\n\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n\ntrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n\nfor i in range(1000):\n  batch = mnist.train.next_batch(50)\n  train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n\ncorrect_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nprint(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n```\n\nThanks!\n", "comments": ["Hi! Look a little earlier at the instructions, the first section is about loading the MNIST data. And you need to run the following before your snippet of code above:\n\n```\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n```\n", "@hnarayanan I already have the MNIST_data directory(i had done it earlier)\n", "Yes, but you need to load it in the context of the current python script that you're attempting to run. (In either case, this is a general support issue that I think will soon be moved to StackOverflow.)\n", "@hnarayanan thanks for your help\n", "had the same error, thanks.\n", "Same problem, thanks!", "Results:\r\nrunfile('C:/Users/3/Documents/Python Scripts/TensorFlow/mnist_basic1.py', wdir='C:/Users/3/Documents/Python Scripts/TensorFlow')\r\nExtracting MNIST_data\\train-images-idx3-ubyte.gz\r\nExtracting MNIST_data\\train-labels-idx1-ubyte.gz\r\nExtracting MNIST_data\\t10k-images-idx3-ubyte.gz\r\nExtracting MNIST_data\\t10k-labels-idx1-ubyte.gz\r\n0.9199\r\n\r\nCut and Paste this:\r\n\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)\r\n\r\nimport tensorflow as tf\r\nx = tf.placeholder(tf.float32, [None, 784])\r\nW = tf.Variable(tf.zeros([784, 10]))\r\nb = tf.Variable(tf.zeros([10]))\r\ny = tf.nn.softmax(tf.matmul(x, W) + b)\r\ny_ = tf.placeholder(tf.float32, [None, 10])\r\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\r\ntrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\r\ninit = tf.global_variables_initializer()\r\nsess = tf.Session()\r\nsess.run(init)\r\nfor i in range(1000):\r\n  batch_xs, batch_ys = mnist.train.next_batch(100)\r\n  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\r\ncorrect_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\r\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\nprint(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))", "i am getting this now\r\n No module named 'tensorflow.examples.tutorials"]}, {"number": 3042, "title": "Inconsistent naming of the getter functions in the C API", "body": "The naming scheme of the getter functions in the [C API](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/public/tensor_c_api.h) seems to be a bit inconsistent. For instance, there are the following declarations:\n\n``` c\n// With \u201cGet\u201d and without the type name.\nextern TF_Code TF_GetCode(const TF_Status*);\n\n// Without \u201cGet\u201d and without the type name.\nextern const char* TF_Message(const TF_Status*);\n\n// Without \u201cGet\u201d and with the type name (\u201cTensor\u201d).\nextern TF_DataType TF_TensorType(const TF_Tensor*);\n```\n\nThe first two are even concerned with the same type, `TF_Status`. I\u2019m curious if there\u2019s any particular reason behind this naming. Thanks!\n\nRegards,\nIvan\n", "comments": ["I'm sure there was some reasoning, for example since all of these functions are in the global namespace, TF_Type wouldn't be very descriptive, and might end up overloading another TF_Type in the future, etc.  In any case, we'll try to be thoughtful about these interface names in the future.  Thanks!\n"]}, {"number": 3041, "title": "docs: Change \"valued\" to \"values\"", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "docs: Change word \"valued\" to \"values\" in 2_fullyconnected.ipynb.\n", "I signed it.\n\nOn Sat, Jun 25, 2016, 3:54 PM Tensorflow Jenkins notifications@github.com\nwrote:\n\n> Can one of the admins verify this patch?\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/3041#issuecomment-228529711,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AFek0OS4YQlR3dv7F8JQN5NHcrP-3BBrks5qPQHhgaJpZM4I-UNE\n> .\n", "CLAs look good, thanks!\n\n<!-- ok -->\n"]}, {"number": 3040, "title": "R0.9", "body": "have a try on this project\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I'm assuming this is an accident (like most of these branch merge PRs are).\n"]}, {"number": 3039, "title": "convert_to_records.py has maybe_download and other functions that are not found.", "body": "When I ran convert_to_records.py without changing the file, it would complain maybe_download and other functions that are not found. If you look at the input_data that was imported, it doesn't include the maybe_download as well even with recursive imports. \n\nWhen I fixed that by copying and pasting from the mnist/convolution.py the download and extraction of the images and label, I ran the fully_connected_reader.py, then it would complain:\nW tensorflow/core/framework/op_kernel.cc:909] Invalid argument: Shape mismatch in tuple component 0. Expected [784], got [3136]\nTraceback (most recent call last):\n  File \"fully_connected_reader.py\", line 203, in <module>\n    tf.app.run()\n  File \"/Users/user/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"fully_connected_reader.py\", line 199, in main\n    run_training()\n  File \"fully_connected_reader.py\", line 194, in run_training\n    coord.join(threads)\n  File \"/Users/user/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 322, in join\n    six.reraise(*self._exc_info_to_raise)\n  File \"/Users/user/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/queue_runner.py\", line 185, in _run\n    sess.run(enqueue_op)\n  File \"/Users/user/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 372, in run\n    run_metadata_ptr)\n  File \"/Users/user/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 636, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/Users/user/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 708, in _do_run\n    target_list, options, run_metadata)\n  File \"/Users/user/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 728, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: Shape mismatch in tuple component 0. Expected [784], got [3136]\n     [[Node: input/shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], _class=[\"loc:@input/shuffle_batch/random_shuffle_queue\"], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input/shuffle_batch/random_shuffle_queue, input/sub, input/Cast_1)]]\nCaused by op u'input/shuffle_batch/random_shuffle_queue_enqueue', defined at:\n  File \"fully_connected_reader.py\", line 203, in <module>\n    tf.app.run()\n  File \"/Users/user/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"fully_connected_reader.py\", line 199, in main\n    run_training()\n  File \"fully_connected_reader.py\", line 140, in run_training\n    num_epochs=FLAGS.num_epochs)\n  File \"fully_connected_reader.py\", line 126, in inputs\n    min_after_dequeue=1000)\n  File \"/Users/user/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/input.py\", line 768, in shuffle_batch\n    _enqueue(queue, tensor_list, num_threads, enqueue_many)\n  File \"/Users/user/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/input.py\", line 490, in _enqueue\n    enqueue_ops = [queue.enqueue(tensor_list)] \\* threads\n  File \"/Users/user/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py\", line 296, in enqueue\n    return gen_data_flow_ops._queue_enqueue(self._queue_ref, vals, name=scope)\n  File \"/Users/user/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 542, in _queue_enqueue\n    name=name)\n  File \"/Users/user/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 711, in apply_op\n    op_def=op_def)\n  File \"/Users/user/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2260, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/user/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1230, in __init__\n    self._traceback = _extract_stack()\n\nNot sure if my fix to convert_to_records.py was wrong or the fully_connected_reader.py was off for some other reason.\n", "comments": ["**In future, could you please try to follow the issues submission template.**  \n\nIt is very difficult to reproduce errors when you do not provide the following basic information:\n- What platform / OS are your running on?\n- How did you install TensorFlow?  (i.e. what version / package / or source revision)\n- Exactly what command lines did you use to build and execute the program?\n", "Sorry about not specifying my situation clearly.\n\nI am running on local OSX El Capitan and I installed Tensorflow using Anaconda with python 2.7 only on CPU mode so far, following https://www.tensorflow.org/versions/r0.9/get_started/os_setup.html#anaconda-installation, \nand downloaded the tensorflow wheel from https://storage.googleapis.com/tensorflow/mac/tensorflow-0.9.0-py2-none-any.whl (I'm guessing that's version 0.9.0, but I can be very wrong).\n\nI have tried running in the mnist directory :\n\n> source activate tensorflow\n> python convolution.py \n> This works perfectly!\n\nThen I cloned the current tensorflow repository (with the latest commit of commit 84225a2b612fe748c9c923f0c1cb8471911c3b77), trying out the convert_to_records.py from https://github.com/tensorflow/tensorflow/blob/r0.9/tensorflow/examples/how_tos/reading_data/convert_to_records.py\n\nI executed the following commands in tensorflow_tools/tensorflow/tensorflow/examples/how_tos/reading_data:\n\n> source activate tensorflow\n> python convert_to_records.py\n> python convert_to_records.py \n> Traceback (most recent call last):\n>   File \"convert_to_records.py\", line 104, in <module>\n>     tf.app.run()\n>   File \"/Users/chungyuwang/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n>     sys.exit(main(sys.argv))\n>   File \"convert_to_records.py\", line 76, in main\n>     train_images_filename = input_data.maybe_download(\n> AttributeError: 'module' object has no attribute 'maybe_download'\n\nTo fix that, I updated the convert_to_records.py's main method to be similar to mnist/convolution.py's beginning part of the method\n'''Get the data.'''  \ntrain_data_filename = maybe_download('train-images-idx3-ubyte.gz')  \ntrain_labels_filename = maybe_download('train-labels-idx1-ubyte.gz')  \ntest_data_filename = maybe_download('t10k-images-idx3-ubyte.gz')  \ntest_labels_filename = maybe_download('t10k-labels-idx1-ubyte.gz')  \n'''Extract it into numpy arrays.'''  \ntrain_images = extract_data(train_data_filename, 60000)  \ntrain_labels = extract_labels(train_labels_filename, 60000)  \ntest_images = extract_data(test_data_filename, 10000)  \ntest_labels = extract_labels(test_labels_filename, 10000)  \nand copied the maybe_download and extract_data directly from mnist/convolution.py\n\nThis change allowed the convert_to_records.py to run without problem, but when I tried to run\n(tensorflow)> python fully_connected_reader.py, then the original post's stack trace/error occurred.\n\nIf there is anything else that I'm missing to help identify the issue, please definitely let me know, I'm really hoping to help out and learn from tensorflow! Thanks!\n", "Looking at this, it seems like in 5c9bc51857bc0c330d3ab976871ee3509647d1e7 the the maybe_download and extract_data methods (among others) were removed from input_data.py without checking that they were used outside the file. \n\nNot sure where else this affects but as @jackywang529 points out, models/image/mnist/convolutional.py has similar methods, so one solution might be to adapt maybe_download in convolutional.py and import those methods in input data. Another solution could be just to reinstate those methods even though we'd have two very similar methods in input_data and convolutional.py\n", "Ya as @mnuke mentioned, those methods were removed from input_data.py. However, those methods are not exactly the same as the ones defined in the mnist/convolution.py, as the ones in mnist/convolution.py sets the data type to be float32 which shouldn't happen in this case, as the fully_connected_reader.py's read_and_decode method changes it to float32 again, thinking that it received int8 type. That's why I was getting the matrix mismatch issue reported earlier.\n\nAlthough not highest priority, but it would definitely be kind to new users to update the input_data.py or other files soon. Thanks!\n", "This references code that might have changed substantially. Feel free to open a new issue if the problem persists with new code."]}, {"number": 3038, "title": "Incorrect tf.truncated_normal results on gpu", "body": "I've narrowed the behavior I'm seeing to this small example\n\n```\nwith tf.Session() as s : \n    with tf.device(DEVICE):\n      v = tf.Variable(tf.truncated_normal([2, 2], stddev=1.0, dtype=tf.float32))\n    with tf.device('/gpu:0'):\n      p = tf.placeholder( tf.float32, shape=(2,2))\n      product = tf.matmul(p,v)\n      s.run(tf.initialize_all_variables())\n      feed = {p: np.eye(2,2)}\n      product_res,v_res = s.run([product,v], feed_dict=feed)\n      print(v_res)\n      print(product_res)\n```\n\nWhen DEVICE is '/cpu:0' all is as expected.  With DEVICE as '/gpu:0', the output will be as expected for one execution, but output two identity matrices for all subsequent executions.  Running again with DEVICE='/cpu:0' resets the gpu behavior.\n### Environment info\n\nOperating System: Fedora 23\n\nInstalled version of CUDA and cuDNN: \nI'm using CUDA 7.5 and cuDNN v5 with Driver Version: 367.27 on a gtx780\n-rw-r--r--. 1 root    root      322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx. 1 root    root          16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx. 1 root    root          19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x. 1 root    root      383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r--. 1 root    root      720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a\nlrwxrwxrwx. 1 jlovitt jlovitt       13 Apr 22 20:52 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5\nlrwxrwxrwx. 1 jlovitt jlovitt       17 Apr 22 20:52 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.0.5\n-rwxrwxr-x. 1 jlovitt jlovitt 59909104 Apr 22 18:15 /usr/local/cuda/lib64/libcudnn.so.5.0.5\n-rw-rw-r--. 1 jlovitt jlovitt 58775484 Apr 22 18:15 /usr/local/cuda/lib64/libcudnn_static.a\n\nIf installed from sources, provide the commit hash: 84225a2b612fe748c9c923f0c1cb8471911c3b77\nI've added the following lines to third_party/gpus/crosstool/CROSSTOOL to compile with gcc 4.9.3\n\n```\ncxx_builtin_include_directory: \"/home/jlovitt/opt/gcc-4.9.3/include\"\ncxx_builtin_include_directory: \"/home/jlovitt/opt/gcc-4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3/include\"\ncxx_builtin_include_directory: \"/home/jlovitt/opt/gcc-4.9.3/lib/gcc/x86_64-unknown-linux-gnu/4.9.3/include-fixed\"\n```\n### Output from execution producing incorrect results\n\n```\n$ipython3 util/tensorflowtest.py \nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 780\nmajor: 3 minor: 5 memoryClockRate (GHz) 1.0195\npciBusID 0000:01:00.0\nTotal memory: 2.95GiB\nFree memory: 2.60GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:844] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 780, pci bus id: 0000:01:00.0)\n[[ 1.  0.]\n [ 0.  1.]]\n[[ 1.  0.]\n [ 0.  1.]]\n```\n## Update:\n\nRecompiling with CUDA 7.0 and cuDNN v4 fixes the problem.\n", "comments": ["@lovi9573, I tried to reproduce your issue with your problem a number of times. The result is as expected. \n\nCould you try the binary install from TensorFlow.org and see if that has the same problem? \n\nhttps://www.tensorflow.org/versions/r0.9/get_started/os_setup.html#pip-installation\n", "The pip install at \nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0-cp34-cp34m-linux_x86_64.whl\nproduces the correct behavior loading the same libcudart.so.7.5.18 and libcudnn.so.5.0.5 that weren't behaving correctly before.\nI must have done something wrong setting up my build.\n", "I was also unable to reproduce this issue. Closing this issue, please re-open if the problem persists.\n"]}, {"number": 3037, "title": "Stuck at prepare_or_wait_for_session for workers when running on kubernetes", "body": "### Environment info\n\nOperating System: Kubernetes/Ubuntu 14.04\n### Steps to reproduce\n1. start tensorflow server on kubernetes using yaml generated by [the python code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/scripts/k8s_tensorflow.py)\n2. Start tensorflow code:\n\n```\npython mnist_dnn.py --worker_grpc_url=grpc://180.101.191.78:30001 --worker_index=0 --workers=180.101.191.78:30001,180.101.191.78:30002,180.101.191.78:30003 --parameter_servers=tf-ps0:2222,tf-ps1:2222\n```\n\n```\npython mnist_dnn.py --worker_grpc_url=grpc://180.101.191.78:30002 --worker_index=1 --workers=180.101.191.78:30001,180.101.191.78:30002,180.101.191.78:30003 --parameter_servers=tf-ps0:2222,tf-ps1:2222\n```\n### What have you tried?\n1. The worker with index 0 (chief) can execute normally.\n2. It was able to execute well (using the same yaml and code)\n3. I tried to restart the servers, but it didn't work.\n4. All other workers stuck at `prepare_or_wait_for_session`. However, it seems that logs suggest other workers are actually executing. \n\nLog is [here](https://github.com/tensorflow/tensorflow/files/332895/log.txt) and the code is here:\n\n```\nimport sys\nimport time\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport datetime\n\nflags = tf.app.flags\nflags.DEFINE_string(\"data_dir\", \"/tmp/data\",\n                    \"Directory for storing mnist data\")\n\nflags.DEFINE_boolean(\"download_only\", False,\n                     \"Only perform downloading of data; Do not proceed to \"\n                     \"session preparation, model definition or training\")\n\nflags.DEFINE_integer(\"worker_index\", 0,\n                     \"Worker task index, should be >= 0. worker_index=0 is \"\n                     \"the master worker task the performs the variable \"\n                     \"initialization \")\n\nflags.DEFINE_string(\"workers\", None,\n                    \"The worker url list, separated by comma (e.g. tf-worker1:2222,1.2.3.4:2222)\")\n\nflags.DEFINE_string(\"parameter_servers\", None,\n                    \"The ps url list, separated by comma (e.g. tf-ps2:2222,1.2.3.5:2222)\")\n\nflags.DEFINE_integer(\"grpc_port\", 2222,\n                     \"TensorFlow GRPC port\")\n\nflags.DEFINE_integer(\"train_steps\", 200000,\n                     \"Number of (global) training steps to perform\")\n\nflags.DEFINE_string(\"worker_grpc_url\", None,\n                    \"Worker GRPC URL (e.g., grpc://1.2.3.4:2222, or \"\n                    \"grpc://tf-worker0:2222)\")\nFLAGS = flags.FLAGS\n\ncur_time = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n\ndef nn_layer(input_tensor, input_dim, output_dim, act=tf.nn.relu):\n    with tf.name_scope(cur_time):\n        weights = tf.Variable(tf.truncated_normal([input_dim, output_dim], stddev=0.1))\n        biases = tf.Variable(tf.constant(0.1, shape=[output_dim]))\n    activations = act(tf.matmul(input_tensor, weights) + biases)\n    return activations\n\ndef model(x, y_, global_step):\n    hidden_nodes = 500\n    hidden1 = nn_layer(x, 784, hidden_nodes)\n    y = nn_layer(hidden1, hidden_nodes, 10, act=tf.nn.softmax)\n\n    cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n    train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy, global_step=global_step)\n\n    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n    return train_step, accuracy\n\nprint(\"Loading data from worker index = %d\" % FLAGS.worker_index)\n\nmnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\nprint(\"Testing set size: %d\" % len(mnist.test.images))\nprint(\"Training set size: %d\" % len(mnist.train.images))\nif FLAGS.download_only: sys.exit(0)\n\nprint(\"Worker GRPC URL: %s\" % FLAGS.worker_grpc_url)\nprint(\"Workers = %s\" % FLAGS.workers)\nprint(\"Using time = %s\" % cur_time)\n\nis_chief = (FLAGS.worker_index == 0)\ncluster = tf.train.ClusterSpec({\"ps\": FLAGS.parameter_servers.split(\",\"), \"worker\": FLAGS.workers.split(\",\")})\n# Construct device setter object\ndevice_setter = tf.train.replica_device_setter(cluster=cluster)\n\n# The device setter will automatically place Variables ops on separate\n# parameter servers (ps). The non-Variable ops will be placed on the workers.\nwith tf.device(device_setter):\n    with tf.name_scope(cur_time):\n        global_step = tf.Variable(0, trainable=False)\n\n    x = tf.placeholder(tf.float32, [None, 784])\n    y_ = tf.placeholder(tf.float32, [None, 10])\n    val_feed = {x: mnist.test.images, y_: mnist.test.labels}\n\n    train_step, accuracy = model(x, y_, global_step)\n\n    sv = tf.train.Supervisor(is_chief=is_chief,\n                             logdir=\"/tmp/dist-mnist-log/train\",\n                             saver=tf.train.Saver(),\n                             init_op=tf.initialize_all_variables(),\n                             recovery_wait_secs=1,\n                             global_step=global_step)\n    sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True,\n                                 device_filters=[\"/job:ps\", \"/job:worker/task:%d\" % FLAGS.worker_index])\n\n    # The chief worker (worker_index==0) session will prepare the session,\n    # while the remaining workers will wait for the preparation to complete.\n    if is_chief:\n        print(\"Worker %d: Initializing session...\" % FLAGS.worker_index)\n    else:\n        print(\"Worker %d: Waiting for session to be initialized...\" % FLAGS.worker_index)\n\n    with sv.prepare_or_wait_for_session(FLAGS.worker_grpc_url, config=sess_config) as sess:\n        print(\"Worker %d: Session initialization complete.\" % FLAGS.worker_index)\n\n        # Perform training\n        time_begin = time.time()\n        print(\"Training begins @ %f\" % time_begin)\n\n        local_step = 0\n        while True:\n            # Training feed\n            batch_xs, batch_ys = mnist.train.next_batch(100)\n            train_feed = {x: batch_xs, y_: batch_ys}\n\n            _, step = sess.run([train_step, global_step], feed_dict=train_feed)\n            local_step += 1\n            if local_step % 100 == 0:\n                print(\"Worker %d: training step %d done (global step: %d); Accuracy: %g\" % \n                      (FLAGS.worker_index, local_step, step, sess.run(accuracy, feed_dict=val_feed)))\n            if step >= FLAGS.train_steps: break\n\n        time_end = time.time()\n        print(\"Training ends @ %f\" % time_end)\n        training_time = time_end - time_begin\n        print(\"Training elapsed time: %f s\" % training_time)\n\n        # Accuracy on test data\n        print(\"Final test accuracy = %g\" % (sess.run(accuracy, feed_dict=val_feed)))\n```\n", "comments": ["by the way, some times (not always). some worker may report\n\n```\nE0625 12:21:16.511111376    3825 tcp_client_posix.c:191]     failed to connect to 'ipv4:180.101.191.78:30002': timeout occurred\n```\n\nbut if it failed to connect the server, why the server log still shows the log above? And most of the workers (or most of the time) the code just stuck at the `prepare_or_wait_for_session` step and output nothing,\n", "Seems to be duplicate of #2472\n"]}, {"number": 3036, "title": "rnn language model example https://www.tensorflow.org/versions/r0.8/tutorials/recurrent/index.html isnt working", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\nUbuntu 14.04\nInstalled version of CUDA and cuDNN: cuda 7.5, cudnn 5.0.5\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n-rw-r--r-- 1 root root 189170 Jun 12 02:51 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Jun 12 02:51 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Jun 12 02:51 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Jun 12 02:51 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 Jun 12 02:51 /usr/local/cuda/lib/libcudart_static.a\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so.7.5 locally\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so.5.0.5 locally\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so.7.5 locally\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so.7.5 locally\n   0.8.0\n   If installed from sources, provide the commit hash:\n### Steps to reproduce\n1. example from https://www.tensorflow.org/versions/r0.8/tutorials/recurrent/index.html github in: tensorflow/tensorflow/models/rnn/ptb/ \n2. wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz and extract it\n3. python ptb_word_lm.py --data_path=simple-examples/data/ after you get \n### What have you tried?\n1. Running it - i'm new to TF, however the cost is nan and doesn't make sense that iters is nan as well. Tried both TF 0.9 and 0.8 , same output \n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n0.004 perplexity: 1.000 speed: 9537 wps\n0.104 perplexity: nan speed: 11236 wps\n0.204 perplexity: nan speed: 11232 wps\n0.304 perplexity: nan speed: 11206 wps\n0.404 perplexity: nan speed: 11231 wps\n\n0.504 perplexity: nan speed: 11220 wps\n0.604 perplexity: nan speed: 11239 wps\n0.703 perplexity: nan speed: 11250 wps\n0.803 perplexity: nan speed: 11259 wps\n0.903 perplexity: nan speed: 11261 wps\nEpoch: 4 Train Perplexity: nan\nEpoch: 4 Valid Perplexity: nan\nEpoch: 5 Learning rate: 1.000\n0.004 perplexity: 1.000 speed: 9563 wps\n0.104 perplexity: nan speed: 11171 wps\n0.204 perplexity: nan speed: 11219 wps\n0.304 perplexity: nan speed: 11245 wps\n0.404 perplexity: nan speed: 11264 wps\n0.504 perplexity: nan speed: 11269 wps\n0.604 perplexity: nan speed: 11265 wps\n0.703 perplexity: nan speed: 11269 wps\n", "comments": ["Possibly related to #2037 ?\n\n@dexter4455 Could you please let me know your GPU hardware / driver config.\nAlso, it might be useful if you could provide the output of:\nls -lR simple-examples/data\n", "@prb12 thanks for replying!\nRegarding #2037 I'm not sure, it works fine for my models. I would say that i've installed from source of course and i don't recall any issues during installation.\nGPU: Amazon EC2 K520 GRID or laptop: Mac book pro\n\noutput:\n\n```\n-rw-r--r-- 1 ubuntu ubuntu   884846 Feb 23  2011 ptb.char.test.txt\n-rw-r--r-- 1 ubuntu ubuntu 10034964 Feb 23  2011 ptb.char.train.txt\n-rw-r--r-- 1 ubuntu ubuntu   786084 Feb 23  2011 ptb.char.valid.txt\n-rw-r--r-- 1 ubuntu ubuntu   449945 Aug  6  2010 ptb.test.txt\n-rw-r--r-- 1 ubuntu ubuntu  5101618 Aug  6  2010 ptb.train.txt\n-rw-r--r-- 1 ubuntu ubuntu   399782 Aug  6  2010 ptb.valid.txt\n-rw-r--r-- 1 ubuntu ubuntu      609 Sep 13  2011 README\n```\n", "@prb12 any update?\nor anyone else? what to do next... \n", "switched to 0.9 and things worked\n", "@zheng-xq  Very difficult to repro - but possibly related to some issues you mentioned.  Given it appears fixed by 0.9, please close if there is no additional information here.\n", "This should be a different issue from another failure that we were investigating. Closing for now. \n"]}, {"number": 3035, "title": "UnboundLocalError when TF_NewSessionOptions fails in session.py", "body": "The following fails with UnboundLocalError after https://github.com/tensorflow/tensorflow/commit/b80a4a8732dfcbbeb348b9f8470aa420bab3931b\n\n```\nsession = tf.Session(tf.ConfigProto(log_device_placement=True))\n\nTraceback (most recent call last):\n  File \"opts_error.py\", line 3, in <module>\n    session = tf.Session(tf.ConfigProto(log_device_placement=True))\n  File \"/Users/yaroslavvb/tfimmediate_hood/tensorflow/_python_build/tensorflow/python/client/session.py\", line 880, in __init__\n    super(Session, self).__init__(target, graph, config=config)\n  File \"/Users/yaroslavvb/tfimmediate_hood/tensorflow/_python_build/tensorflow/python/client/session.py\", line 147, in __init__\n    tf_session.TF_DeleteSessionOptions(opts)\n\nUnboundLocalError: local variable 'opts' referenced before assignment\n\n```\n\nIt's an incorrect usage of `Session`  I think it used to fail with a more informative message like ``*** TypeError: Expected binary or unicode string, got log_device_placement: true` before https://github.com/tensorflow/tensorflow/commit/b80a4a8732dfcbbeb348b9f8470aa420bab3931b\n\nThis commit moved creation inside the \"try/finally\" block, so real exception is swallowed, and this exception happens because \"opts\" right hand side doesn't get evaluated\n", "comments": ["This has been fixed.  It will take a little while to make it out into the GitHub repo.\n"]}, {"number": 3034, "title": "Tensorflow import error on OS X 10.8.5", "body": "I have installed tensorflow using acadonda virtual envoronment. When I import it is giving following error.\n\n```\nImportError: dlopen(/Users/SummerREU/anaconda/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so, 2): Symbol not found: ___sincos_stret\n  Referenced from: /Users/SummerREU/anaconda/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n  Expected in: /usr/lib/libSystem.B.dylib\n in /Users/SummerREU/anaconda/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n```\n", "comments": ["Our binaries are built with OS X 10.10. Can you upgrade or try building from source?\n", "Automatically closing due to lack of recent activity. Please reopen when further information becomes available.\n"]}, {"number": 3033, "title": "Udacity Readme.md updates to docker command", "body": "Change to docker command to ensure changes to notebooks are not lost between restarts\n", "comments": ["Can one of the admins verify this patch?\n", "Sorry, found an issue with this.\n"]}, {"number": 3032, "title": "Optimizer classes do not support complex variables", "body": "Using TF 0.8.0.\n\n`````` ---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-4-871aef3761c0> in <module>()\n----> 1 train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n      2 init = tf.initialize_all_variables()\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.pyc in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name)\n    188         loss, var_list=var_list, gate_gradients=gate_gradients,\n    189         aggregation_method=aggregation_method,\n--> 190         colocate_gradients_with_ops=colocate_gradients_with_ops)\n    191     return self.apply_gradients(grads_and_vars, global_step=global_step,\n    192                                 name=name)\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.pyc in compute_gradients(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops)\n    243       grads = control_flow_ops.tuple(grads)\n    244     grads_and_vars = list(zip(grads, var_list))\n--> 245     self._assert_valid_dtypes([v for g, v in grads_and_vars if g is not None])\n    246     return grads_and_vars\n    247 \n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.pyc in _assert_valid_dtypes(self, tensors)\n    354         raise ValueError(\n    355             \"Invalid type %r for %s, expected: %s.\" % (\n--> 356                 dtype, t.name, [v for v in valid_dtypes]))\n    357 \n    358   # --------------\n\nValueError: Invalid type tf.complex64 for complex_weight_variable:0, expected: [tf.float32].```\n``````\n", "comments": ["Maybe it would be enough to throw an error specific to complex numbers that says\n\n```\nValueError: Can't optimize complex variables.\nExpress complex variables using tf.complex(a, b), where a and b are real.\n```\n\nA solution that works with complex numbers would involve creating two real variables, and replacing every occurence of the old one with `tf.complex(a, b)`.\nWouldn't that also involve copying the graph?\n", "Is this still an open issue? What do you mean by \"replacing every occurence of the old one with tf.complex(a, b)\" ? Is it possible to optimize a given complex-valued output of a layer? ", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.", "Indeed if you want to optimize over a complex variable you need to instead define two float variables which take the place of real and imaginary parts, and then use var_list=[real_var, imag_var].", "@Joshuaalbert Would it take care of doing back propagation the way it is explained in the following paper? \r\n\r\nhttps://openreview.net/forum?id=H1S8UE-Rb\r\n\r\nThat still needs to be implemented somehow, right? Do you know what the best way to do so would be?\r\n", "The paper you mention uses projection kernels of the form `W = W_r + i W_i` and input tensors of the form `x= x_r + i x_i`. The standard perceptron complex output is then \r\n```\r\ny_r + i y_i = f(Real(W x)) + i f(Imag(W x)) \r\n= f( W_r x_r - W_i x_i) + i f(W_r x_i + W_i x_r)\r\n= f(   (W_r, W_i)^T . (x_r, -x_i)^T ) + i f(   (W_r, W_i)^T . (x_i, x_r)^T ) \r\n```\r\nIn the last line you see that you can represent the complex weight kernel as `W_r` and `W_i` stacked on top of each other.\r\nTherefore you can represent this complex forward projection as the following set of operations:\r\n```\r\nAssume N = output size of each perceptron\r\n  x_r = tf.real(x)\r\n  x_i = tf.imag(x)\r\n  x_1 = tf.concat([x_r, -x_i], axis=0)\r\n  x_2 = tf.concat([x_i, x_r], axis=0)\r\n  layer_1 = tf.keras.layers.Dense(N)\r\n  # use the same layer twice and the kernel will represent (W_r, W_i)^T\r\n  y_r = layer_1(x_1)\r\n  y_i = layer_1(x_2)\r\n  # to add more layers again for x_1 and x_2 from y_r and y_i\r\n  x_1 = tf.concat([y_r, -y_i], axis=0)\r\n  x_2 = tf.concat([y_i, y_r], axis=0)\r\n  layer_2 = tf.keras.layers.Dense(N)\r\n  z_r = layer_2(z_1)\r\n  z_i = layer_2(z_2)\r\n  ...\r\nTo combine outputs into complex you can always do tf.complex(real_part, imag_part)\r\nThe loss function must be real and can be loss(real_part) + loss(imag_part)\r\n```\r\n\r\nMy observations are that since complex numbers are merely a particular container of two real numbers you are actually restricting yourself by choosing to use complex numbers. In general if you wanted to allow more complex patterns you should form an input `input = tf.concat([tf.real(x), tf.imag(x)], axis=-1)`.", "Thanks for clarfying this. This might be a bit of off-topic, but what could be the best way to define a complex loss-function? Just imagine you compare imag/imag with real/real in your loss function or you do it with abs and phase of your complex number. There is a wrap-around problem with the phase at 2p, but maybe it's a better metric if it comes to stability. \r\n\r\nMaybe it's faster to optimize first for the magnitude 10 iterations and then for the phase, eventually you approach to you final result faster? I've just started to see these challenges. Are you familiar with this? \r\n\r\n", "Generally, doing it in phase and amplitude space is better because there are far fewer local minima, however then yes you have phase wrapping. If you do it in phase/amp space then you should solve phase first (mostly) before doing pase+amplitude. Phase errors are usually the most important. I'm curious what problem you are trying to solve?", "The main use right now is for using inverse problems on a GPU. Especially everything which is related to light fields in optics. ", "I see, a colleague of mine is doing adaptive optics and predicting phase screens.", "Sounds cool. Did you publish anything on that already? I\u2019m a bit curious :)\n\n \n\nVon: Joshua George Albert <notifications@github.com>\nAntworten an: tensorflow/tensorflow <reply@reply.github.com>\nDatum: Donnerstag, 14. Dezember 2017 um 23:05\nAn: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: beniroquai <bene.d@gmx.de>, Comment <comment@noreply.github.com>\nBetreff: Re: [tensorflow/tensorflow] Optimizer classes do not support complex variables (#3032)\n\n \n\nI see, a colleague of mine is doing adaptive optics and predicting phase screens.\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n\n"]}, {"number": 3031, "title": "Add sparse_transpose wrapper", "body": "Attempt to add `transpose` function for SparseTensor as mentioned in #2180.\n\nThe commented code is an attempt to use `tf.reverse` to reverse `indices` and `shape`. But right now it raises error since `reverse` doesn't support `int64`.\n\nI did test the changes locally. This is my first attempt to contribute. Please let me know if I have to do something here. Thanks.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for the contribution. A couple of issues with the current code:\n1. We need to support tensors with more than 2 dimensions.\n2. tf.transpose is more general than simply reversing the indices, but takes an optional argument with a permutation of [0...n-1]. I think we should support this functionality for sparse tensors as well. I think your can use tf.gather to reorder the columns of the index tensor. \n", "Thank you for the suggestions. I will look into the code for transpose and try to extend it to n dimensions for sparse tensor too. If I understand `tf.gather` correctly, it can permute along the axis=0, but here it needs to be done along axis=1. I couldn't think of any approach other than transposing the indices matrix and gather and transpose back. Could you please let me know how to do this ? Sorry for my questions but I am new to contributing here and am trying to understand the codebase better. Thanks.\n", "Hi, sorry for disturbing again but was hoping someone can give insight on how to use `gather` to reorder the columns (along axis=1). Thanks!\n", "Sorry for the delay.\n\nI don't quite understand why `tf.gather()` comes into place here, given the two new requirements?\n\nHere's one potential solution -- I haven't thought through the details! -- recall `indicies : [NNZ, NDIM]` and `values : [NNZ]`.  \n\nWe first unpack/split `indices` into `[NNZ, 1, ..., 1]` with `NDIM` ones in the end.  We concatenate it with `values`, obtaining a tensor with shape `[NNZ] + [1]*NDIM + [1]`.  \n\nNow we call `tf.transpose()` on this concatenated tensor, with `perm = [0] + user_perm + [NDIM + 1]`, namely we permute the middle indices.  Finally, transform the shape back and split again.  \n\nIf this doesn't work, perhaps we'll need a C++ native kernel.\n", "Sorry if I am understanding it wrong, but @rmlarsen has mentioned that probably `tf.gather` can be used to reorder the columns in terms of `perm` but AFAIU it seemed that `tf.gather` can only reorder only along axis=0. That was the reason I asked for the help.\n\nThanks for suggesting the approach. Sorry but I am unable to understand it correctly here.\n\n> We first unpack/split indices into [NNZ, 1, ..., 1]\n\nIndices has shape `[NNZ, ndim]`. So how does it unpack into `[NNZ] + [1]*ndim` ? Unpack reduces the rank by at most 1 right. Could you clarify it here please ? Thanks a lot.\n", "@rmlarsen gentle ping, we might need a small clarification here ;)\n", "@rmlarsen Sorry to bother you again but it would be really helpful to have your input here. Thanks a lot !\n", "ping @rmlarsen ?\n", "Yes, tf.gather only operates on the first dimension, so what I had in mind was:\n\n...make sure that perm is a permutation of [0:ndims-1]...\ntransposed_indices = tf.transpose(tf.gather(tf.transpose(indices), perm))\n\ninstead of the less general\n\ntransposed_indices = array_ops.concat(1, [indices[:, 1:2], indices[:, 0:1]])\n", "Sorry about the delay BTW.\n", "No problem! Thanks a lot for the clarification. I will try to make the changes today and get back to you. \n", "Hi, I have used `array_ops.transpose(array_ops.gather(array_ops.transpose(indices), perm))` instead of `array_ops.concat(1, [indices[:, 1:2], indices[:, 0:1]])` according to your suggestion. I have run the tests locally and are passing. I have added a simple test, so please do have a look at it and let me know if there are any better tests. Thanks!\n", "@rmlarsen please re-review at your convenience\n", "@tensorflow-jenkins test this please\n", "@maniteja123 looking into your question...\n", "Hi I have added generalised tests. Can someone please have a look at the tests and run jenkins if they seem appropriate ? Thanks!\n", "@tensorflow-jenkins test this please\n", "@maniteja123 Thanks for your contribution!\n", "@rmlarsen @concretevitamin Thanks a lot for patiently guiding me throughout the PR. Sorry for failing to address the nitpicks. Is that okay ?\n", "No worries!\n\nOn Friday, August 26, 2016, Maniteja Nandana notifications@github.com\nwrote:\n\n> @rmlarsen https://github.com/rmlarsen @concretevitamin\n> https://github.com/concretevitamin Thanks a lot for patiently guiding\n> me throughout the PR. Sorry for failing to address the nitpicks. Is that\n> okay ?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/3031#issuecomment-242889872,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAkLHqukqXJ_Apqk1XbOFgFzpHgUQzBRks5qj54WgaJpZM4I-CR0\n> .\n", "Hi @maniteja123,\n\nCan we now transpose sparse tensors (dtype float64 and/or complex128) of order/rank greater than 2 ? I just wanted to make sure before I download the most recent version of tensorflow library. Thanks.\n", "Hi @amrit-poudel Yeah, I believe it should be possible to transpose sparse tensors of rank till the maximum, which I suppose is 5.\n", "Hi @maniteja123, \n\nThanks for the quick note and for your input in developing this feature. I appreciate it.\n\nI was just wondering if there was any specific reason why it won't work beyond 5th order.\n\nThanks.\n", "Hi @amrit-poudel, even I am not aware of the exact reason but I remember reading that it is because higher ranks don't have common use case. Probably someone experienced can give a better explanation.\n", "Hi @maniteja123, \nI am currently working with sparse tensors of rank 7 and was wondering if it is too difficult, in your experience, to extend transpose function to tensors of higher ranks.  Thanks.\n", "Hi, I get these warnings when using the recently added sparse_transpose function:\n\nWARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\nWARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n\nIs there a way to update sparse_transpose so that these warnings are avoided?\n", "Hi @maniteja123,\n\nI just checked that spares_transpose function seems to work fine without any error even for sparse tensor of order/rank >5, unlike what you previously stated.\n\nI tested this on build  # 234 (last stable build on cpu only macosx)\n\nCould you or someone please clarify if sparse_transpose function is correctly implemented for sparse tensors of rank greater than 5? Thanks.\n", "@amrit-poudel sparse_transpose works correctly for high-rank tensors. You can ignore the deprecation messages about op_scope. They do not affect the result at all.\n", "Thanks for confirming.\n", "How do I suppress this warning:\n\nWARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values) \n\nThis warning originated from recently updated sparse_transpose function in tensorflow. \n\nHere is a testprogram.py file:\n\nimport tensorflow as tf\n\na = tf.SparseTensor(indices=[[0, 0, 0]], values=[1.0+2j], shape=[3, 4, 2])\n\nb = tf.sparse_transpose(a)\n\nprint 'Done!'\n\nI tried python -W ignore testprogram.py . However, this warning did not go away.\n\nI also tried export PYTHONWARNINGS=\"ignore\". This did not work either.\n\nThen I tried:\n\nimport warnings\n warnings.filterwarnings('ignore')\n\nDid not work. This certainly is one-of-a-kind warming!\n\n My output file is polluted by this warning message. I absolutely want to get rid of it. Please suggest. Thanks!\n\nEdit:\n\ntf.logging.set_verbosity(tf.logging.ERROR)\n\nThat solves it.\n"]}, {"number": 3030, "title": "Branch 125795153", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}, {"number": 3029, "title": "Linking Error building master from sources on 16.04", "body": "Hey all I am getting the following error building tensorflow from sources:\n\n```\nERROR: /home/william/tensorflow/tensorflow/tools/proto_text/BUILD:31:1: Linking of rule '//tensorflow/tools/proto_text:gen_proto_text_functions' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command \n  (cd /home/william/.cache/bazel/_bazel_william/1c01fb06292961944db1914ac48e6a52/execroot/tensorflow && \\\n  exec env - \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/host/bin/tensorflow/tools/proto_text/gen_proto_text_functions bazel-out/host/bin/tensorflow/tools/proto_text/_objs/gen_proto_text_functions/tensorflow/tools/proto_text/gen_proto_text_functions.o bazel-out/host/bin/tensorflow/tools/proto_text/libgen_proto_text_functions_lib.a bazel-out/host/bin/tensorflow/core/liblib_internal.a bazel-out/host/bin/external/farmhash_archive/libfarmhash.a bazel-out/host/bin/external/jpeg_archive/libjpeg.a bazel-out/host/bin/external/png_archive/libpng.a bazel-out/host/bin/external/highwayhash/libsip_hash.a bazel-out/host/bin/external/re2/libre2.a bazel-out/host/bin/tensorflow/core/libprotos_all_cc.a bazel-out/host/bin/external/protobuf/libprotobuf.a bazel-out/host/bin/external/protobuf/libprotobuf_lite.a bazel-out/host/bin/external/zlib_archive/libzlib.a -ldl -lz -pthread -lpthread -lstdc++ -B/usr/bin/ -pie -Wl,-z,relro,-z,now -no-canonical-prefixes -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,-S -Wl,--gc-sections): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n/usr/bin/ld: bazel-out/host/bin/tensorflow/core/liblib_internal.a(numbers.o): undefined reference to symbol 'ceil@@GLIBC_2.2.5'\n//lib/x86_64-linux-gnu/libm.so.6: error adding symbols: DSO missing from command line\ncollect2: error: ld returned 1 exit status\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\n```\n\nThis occurs when I run `bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer --verbose_failures`\n### Environment info\n\nOperating System: Ubuntu 16.04\n\nInstalled version of CUDA and cuDNN: 8.0, 5\n\n```\nls -l /usr/local/cuda-8.0/lib64/libcud*\n-rw-r--r-- 1 root root   560184 May 18 17:44 /usr/local/cuda-8.0/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 May 18 17:47 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root       19 May 18 17:47 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\n-rw-r--r-- 1 root root   394472 May 18 17:44 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.27\n-rw-r--r-- 1 root root   737516 May 18 17:44 /usr/local/cuda-8.0/lib64/libcudart_static.a\n-rwxr-xr-x 1 root root 78065952 Jun 23 18:54 /usr/local/cuda-8.0/lib64/libcudnn.so\n-rwxr-xr-x 1 root root 78065952 Jun 23 18:54 /usr/local/cuda-8.0/lib64/libcudnn.so.5\n-rwxr-xr-x 1 root root 78065952 Jun 23 18:54 /usr/local/cuda-8.0/lib64/libcudnn.so.5.0.5\n-rw-r--r-- 1 root root 68709594 Jun 23 18:54 /usr/local/cuda-8.0/lib64/libcudnn_static.a\n```\n\nIf installed from sources, provide the commit hash: `84225a2b612fe748c9c923f0c1cb8471911c3b77`\n### Reproduce It\n\nFollow the documnentation exactly for building from sources:\n1. Install Cuda and Cudnn from nvidia\n2. Install python/build dependencies\n3. Install bazel from apt-get:\n- `sudo apt-get install openjdk-8-jdk`\n- `sudo apt-get install pkg-config zip g++ zlib1g-dev unzip`\n  -  `echo \"deb http://storage.googleapis.com/bazel-apt stable jdk1.8\" | sudo tee /etc/apt/sources.list.d/bazel.list`\n- `curl https://storage.googleapis.com/bazel-apt/doc/apt-key.pub.gpg | sudo apt-key add -`\n-  `sudo apt-get update && sudo apt-get install bazel`\n- Then run the first install command using bazel as mentioned before.\n", "comments": ["Solved by add -lm to the proto_text/BUILD file line 31.\n", "How exactly did you edit it? Editing it into line 31 does not seem trivial to me:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/proto_text/BUILD\n\nThanks!\n", "Edit \n\n> \\tensorflow\\tools\\proto_text\n\nAdd `linkopts = [\"-lm\"]` to cc_binary\n\nSo, you should have\n\n```\ncc_binary(\n    name = \"gen_proto_text_functions\",\n    srcs = [\"gen_proto_text_functions.cc\"],\n    visibility = [\"//tensorflow:internal\"],\n    deps = [\n        \":gen_proto_text_functions_lib\",\n        \"//tensorflow/core:lib\",\n    ],\n    linkopts = [\"-lm\"],\n)\n```\n\nAfter modified just build tensorflow normally.\n", "This solution doesn't seem to work with the current master branch. Any ideas?! \n", "This change resolved the issue on Ubuntu 15.10 (current master branch) as well.\n"]}, {"number": 3028, "title": "iterative pruning", "body": "It would be very advantageous to have support (tools) for iterative pruning in TF as in \nDeep Compression paper\nhttps://arxiv.org/abs/1510.00149\nSince in many cases it can give up to 2x speedup and 80% weight reduction practically for free. \nPS: thanks for the excellent support of the framework !\n", "comments": ["I've read the paper, mobile app will benefit from it. We need a normal training first, then apply this algorithm to continue training.\n", "this [article](https://www.tensorflow.org/versions/master/how_tos/quantization/index.html#how-to-quantize-neural-networks-with-tensorflow) mentioned this paper, it seems tensorflow has already done something.\n", "This [work](https://github.com/garion9013/impl-pruning-TF) actually implements iterative pruning (part of Deep compression) on TF.\n_embedding_lookup_sparse_ operation is used to do sparse matrix multiplication.\nI've got some of speedups with this prototype, but it's somewhat mediocre.\nI think it's because of a small size of the model and sparse matrix format that TF uses as a default (COO).\n", "What did you compare to ? Did you compare to original dense multiplication\n(without sparsity flags) ?\nPS: btw, there is matmultiplication of dense on sparse tensors instead of\nembedding_lookup\ntf.sparse_tensor_dense_matmul\nhttps://www.tensorflow.org/versions/r0.10/api_docs/python/sparse_ops.html#sparse_tensor_dense_matmul\n\nOn 27 September 2016 at 01:41, Young H. Oh notifications@github.com wrote:\n\n> This work https://github.com/garion9013/impl-pruning-TF actually\n> implements iterative pruning (part of Deep compression) on TF.\n> _embedding_lookup_sparse_ operation is used to do sparse matrix\n> multiplication.\n> I've got some of speedups with this prototype, but it's somewhat mediocre.\n> I think it's because small size of the model and sparse matrix format that\n> TF uses as a default (COO).\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3028#issuecomment-249802651,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AI_RNags9isvjZuEr84HNccCwXSbNbO8ks5quNbHgaJpZM4I94Gy\n> .\n\n## \n\nBest regards,\nArtem Molchanov.\n", "Yes, baselines are from dense matrix multiplication without iterative pruning. Also, you're right. _sparse_tensor_dense_matmul_ would be more appropriate just for [this example](https://github.com/garion9013/impl-pruning-TF), especially for CPU.\nBut I think bigger models like AlexNet will have more benefits from _embedding_lookup_ than _sparse_tensor_dense_matmul_. Because it truly reduces app's memory requirements, while the latter doesn't.\n", "Is there any progress on this issue?", "I just post my [work](https://github.com/Connor323/Convolution-with-sparse-kernel-in-TF) implementing convolution with sparse kernel. As I mentioned in the TODO list, there are still plenty of things need to improve, but overall performance is much faster than the dense convolution in TF, especially with a large size input. Please feel free to check it out and let me know if there is any issues. Thanks! "]}, {"number": 3027, "title": "cp: cannot stat 'bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/tensorflow': No such file or directory", "body": "### Steps to reproduce\n1. `docker build ./tensorflow/tools/docker/Dockerfile.devel-gpu`\n### Logs or other output that would be helpful\n\n```\ncp: cannot stat 'bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/tensorflow': No such file or directory\ncp: cannot stat 'bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/external': No such file or directory\nThe command '/bin/sh -c ./configure &&     bazel build --local_resources 3072,3.0,1.0 -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package &&     bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip &&     pip install --upgrade /tmp/pip/tensorflow-*.whl' returned a non-zero code: 1\n```\n", "comments": ["Could you please provide more details of the TensorFlow version you are using?\n", "I am using the Dockerfile from [here](https://github.com/tensorflow/tensorflow/blob/dcbc4701845b6bad57b6183514d64d0eeabad57a/tensorflow/tools/docker/Dockerfile.devel-gpu) which checks out `r0.8`. I am also setting:\n\n```\nENV TF_UNOFFICIAL_SETTING=1 \\\n    TF_CUDA_COMPUTE_CAPABILITIES=3.0\n```\n\nand passing to bazel:\n\n```\n--local_resources 3072,3.0,1.0\n```\n", "We probably need to update the Dockerfile to check out r0.9, bazel 0.3.0 doesn't work with r0.8.\n", "I want to get rid of this dockerfile. As a stopgap, can you use the Dockerfile we use for testing? \n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/Dockerfile.gpu\n", "I am using the following Dockerfile with the relevant changes highlighted and it now builds: https://github.com/4Catalyzer/tensorflow/blob/ce67e52461f9e6b05ec8cd35d0736dd0c68f07eb/tensorflow/tools/docker/Dockerfile.devel-gpu#L82-L85\n", "Cool, thanks! Do you want to make a PR with that?\n\nOn Thu, Jun 30, 2016 at 4:12 PM Alex Rothberg notifications@github.com\nwrote:\n\n> I am using the following Dockerfile with the relevant changes highlighted\n> and it now builds:\n> https://github.com/4Catalyzer/tensorflow/blob/ce67e52461f9e6b05ec8cd35d0736dd0c68f07eb/tensorflow/tools/docker/Dockerfile.devel-gpu#L82-L85\n> \n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3027#issuecomment-229814498,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAjO_WOYt09balMQjMCcdEtA48dnH0PQks5qRE15gaJpZM4I9z49\n> .\n", "https://github.com/tensorflow/tensorflow/pull/3143\n", "Seems like this is resolved. Closing for now.\n", "@cancan101 Hi \uff0cI have met the same problem, can you tell more details about how you solved this problem. Thank you very much"]}, {"number": 3026, "title": "Accessing ALL the states in rnn.py", "body": "Dear all,\n\nWould it be possible to return all the states when we call the `rnn` function in `tensorflow/tensorflow/python/ops/rnn.py` (and not only the final state):\n\n```\ndef rnn(cell, inputs, initial_state=None, dtype=None,\n        sequence_length=None, scope=None):\n```\n\nI posted the code of the latest version as an illustration:\n\n```\n   for time, input_ in enumerate(inputs):\n      if time > 0: varscope.reuse_variables()\n      # pylint: disable=cell-var-from-loop\n      call_cell = lambda: cell(input_, state)\n      # pylint: enable=cell-var-from-loop\n      if sequence_length is not None:\n        (output, state) = _rnn_step(\n            time=time,\n            sequence_length=sequence_length,\n            min_sequence_length=min_sequence_length,\n            max_sequence_length=max_sequence_length,\n            zero_output=zero_output,\n            state=state,\n            call_cell=call_cell,\n            state_size=cell.state_size)\n      else:\n        (output, state) = call_cell()\n\n      outputs.append(output)\n\n    return (outputs, state)\n```\n\nThanks!\n", "comments": []}, {"number": 3025, "title": "python 3.5 mnist implementation error", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System: ubuntu 16.04 / 14.04\n\nInstalled version of CUDA and cuDNN: NA\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide: NA\n1. Which pip package you installed.\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`. 0.8.0\n   If installed from sources, provide the commit hash:\n### Steps to reproduce\n1. Just implemented the mnist code provided by tensorflow. \n   2.\n   3.\n### What have you tried?\n1. While starting training, I have got an error as below. I have tried with ubuntu 14.04 and ubuntu 16.04, tensorflow 0.8 and tensorflow 0.9, both had the same error. However, when I tried it with python 2.7, it worked. Now with python 3.5, not working.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n\nTypeError                                 Traceback (most recent call last)\n<ipython-input-9-80fac15278ee> in <module>()\n----> 1 training()\n\n<ipython-input-8-d1198db7bf1d> in training()\n     11         loss = mnist.loss(logits, labels_placeholder)\n     12         train_op = mnist.training(loss, FLAGS.learning_rate)\n---> 13         eval_correct = mnist.evaluation(logits, labels_placeholder)\n     14         # Build the summary operation based on the TF collections of Summaries.\n     15         summary_op = tf.merge_all_summaries()\n\n/opt/conda/lib/python3.5/site-packages/tensorflow/examples/tutorials/mnist/mnist.py in evaluation(logits, labels)\n    146   # the examples where the label is in the top k (here k=1)\n    147   # of all logits for that example.\n--> 148   correct = tf.nn.in_top_k(logits, labels, 1)\n    149   # Return the number of true entries.\n    150   return tf.reduce_sum(tf.cast(correct, tf.int32))\n\n/opt/conda/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py in in_top_k(predictions, targets, k, name)\n    547   \"\"\"\n    548   return _op_def_lib.apply_op(\"InTopK\", predictions=predictions,\n--> 549                               targets=targets, k=k, name=name)\n    550 \n    551 \n\n/opt/conda/lib/python3.5/site-packages/tensorflow/python/ops/op_def_library.py in apply_op(self, op_type_name, name, **keywords)\n    484             for base_type in base_types:\n    485               _SatisfiesTypeConstraint(base_type,\n--> 486                                        _Attr(op_def, input_arg.type_attr))\n    487             attrs[input_arg.type_attr] = attr_value\n    488             inferred_from[input_arg.type_attr] = input_name\n\n/opt/conda/lib/python3.5/site-packages/tensorflow/python/ops/op_def_library.py in _SatisfiesTypeConstraint(dtype, attr_def)\n     57           \"DataType %s for attr '%s' not in list of allowed values: %s\" %\n     58           (dtypes.as_dtype(dtype).name, attr_def.name,\n---> 59            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n     60 \n     61 \n\nTypeError: DataType float32 for attr 'T' not in list of allowed values: int32, int64\n", "comments": ["> Just implemented the mnist code provided by tensorflow. 2. 3.\n\nCould you please be a bit more precise.\nIs this code you 'implemented' based on a tutorial? If so, could you please \na) check carefully for any transcription mistakes\nb) if you believe there are none, please provide a link to the 'provided code' and a copy of the code you are executing.\n\nAlternatively, if this an unmodified example program from the TensorFlow distribution could you please provide the _exact_ sequence of command lines used to build and execute the program.\n\nThanks! \n", "The code I had was obsolete and now it is working fine. Thanks!\n"]}, {"number": 3024, "title": "The speed of reading images is unstable.", "body": "The speed of reading image is very unstable. This problem troubled me for a long time. Can anyone help me to check the problem. \n\nThe function of the following code is just reading images from several files with tf_recored pattern in the batch method.\n\n```\ndef test_tf_decode_jpeg():\n  with tf.device('/cpu:0'): \n    tf_record_pattern = os.path.join('../anno/train_data_with_history', '*')\n    data_files = tf.gfile.Glob(tf_record_pattern)\n    filename_queue = tf.train.string_input_producer(data_files, shuffle=False, capacity=10)\n    images = []\n    num_preprocess_threads = 4\n    for thread_id in range(num_preprocess_threads):\n      reader = tf.TFRecordReader()\n      _, image = reader.read(filename_queue)\n      image = parse_example_proto(image)\n      image = tf.image.decode_jpeg(image, channels=3)\n      image.set_shape([256,256,3])\n      images.append([image])\n    batch_size = 256\n    image_batch = tf.train.batch_join(images, batch_size=batch_size,capacity=2 * num_preprocess_threads * batch_size)\n  sess = tf.Session()\n  coord = tf.train.Coordinator()\n  threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n  for i in xrange(1000000):\n    if coord.should_stop():\n      break\n    start = time.time()\n    output_images = sess.run(image_batch) \n    print time.time() - start, output_images.shape\n  coord.request_stop()\n  coord.join(threads)\n```\n\nThe speed of reading images is very unstable. Here is the running log.\n0.876242160797 (256, 256, 256, 3)\n0.165473937988 (256, 256, 256, 3)\n0.458849906921 (256, 256, 256, 3)\n0.172335863113 (256, 256, 256, 3)\n0.483803033829 (256, 256, 256, 3)\n0.517580032349 (256, 256, 256, 3)\n0.174633979797 (256, 256, 256, 3)\n0.944914102554 (256, 256, 256, 3)\n0.169141054153 (256, 256, 256, 3)\n0.447196006775 (256, 256, 256, 3)\n0.512051820755 (256, 256, 256, 3)\n0.179712057114 (256, 256, 256, 3)\n0.53243303299 (256, 256, 256, 3)\n0.646076917648 (256, 256, 256, 3)\n0.175096988678 (256, 256, 256, 3)\n0.656931877136 (256, 256, 256, 3)\n0.500935077667 (256, 256, 256, 3)\n0.456654071808 (256, 256, 256, 3)\n0.41271686554 (256, 256, 256, 3)\n0.169769048691 (256, 256, 256, 3)\n0.632494926453 (256, 256, 256, 3)\n0.369579076767 (256, 256, 256, 3)\n0.160307168961 (256, 256, 256, 3)\n0.47083902359 (256, 256, 256, 3)\n0.440319061279 (256, 256, 256, 3)\n0.534809827805 (256, 256, 256, 3)\n0.464602947235 (256, 256, 256, 3)\n0.183679103851 (256, 256, 256, 3)\n0.605715990067 (256, 256, 256, 3)\n0.463223934174 (256, 256, 256, 3)\n0.329600095749 (256, 256, 256, 3)\n0.181442975998 (256, 256, 256, 3)\n0.551628112793 (256, 256, 256, 3)\n0.52916097641 (256, 256, 256, 3)\n0.172710180283 (256, 256, 256, 3)\n0.577185153961 (256, 256, 256, 3)\n\nThe utilization of cpu and io is quite low.  \n\n\"cat /proc/cpu\" outputs:\nprocessor       : 39\nvendor_id       : GenuineIntel\ncpu family      : 6\nmodel           : 63\nmodel name      : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz\n", "comments": ["@FangxiangFeng I'm pretty new in Tensorflow and I've met this kind of unstable speed problem, which I think is about reading images. I trained on ImageNet with tf_recored pattern too but the time spent on each iteration is quite different. Especially after several thousand iterations, it becomes slower and slower. \n\nI don't know if you solved your problem and could you share? Thanks.\n", "One source of instability can come from Python thread scheduler. If pre-fetching thread (started by `start_queue_runners`) are not scheduled by Python thread scheduler sufficiently often, the queue will run out of examples. This can happen when `session.run` call in main thread runs too fast for Python thread scheduler to pre-empt the thread. You can diagnose this by looking at queue fullness summaries in TensorBoard, and the solution is to wait in the beginning to let the examples load.\n\nGetting \"slower and slower\" with time could be caused by memory-leak or inefficent use of memory (which can happen if you don't run tf.get_default_graph().finalize() and modify the graph). To diagnose, see if memory usage grows over time. Another approach is to drop caches and see if that improves throughput -- \n`free && sync && echo 3 > /proc/sys/vm/drop_caches && free`\n", "@yaroslavvb Thanks. I think I get the first point of yours and I finally understand the \"queue\" of tensorflow. \n\nAs for the second point, I don't modify the graph and I use nvidia-smi for checking GPU memory, it doesn't change at all. When I said \"slower and slower\", maybe it's a little exaggerated. After several thousand iterations, it takes around 1.5 times more than at begging but it doesn't become slower after that. Although it's acceptable, it takes less time for the first thousand iterations. I think it's strange.\n", "GPU memory shouldn't change, but I've seen people report slow-down because of main RAM usage/fragmentation. Dropping caches or using tcmalloc (through LD_PRELOAD) fixes the latter\n", "@yaroslavvb GPU memory didn't change in my training process. Dropping caches trick has solved my problem.\n", "Thank you both. I got it.\n"]}, {"number": 3023, "title": "Update copyright dates", "body": "The copyright dates are changed to 2016.\n", "comments": ["Can one of the admins verify this patch?\n", "I believe the copyright reflects the date of the first appearance of the content.  New files generally are getting 2016 on them, so this is fine for now.  Thanks though!\n"]}, {"number": 3022, "title": "README: fix shell snippet markdown", "body": "Fixes shell snippet markdown, by ensuring that\nthe shell command `python` and the actual Python code\nare separated in different code blocks. This is to prevent\na formatting nit because $ not in string form, is not\na valid Python token.\n### Before\n\n<img width=\"682\" alt=\"screen shot 2016-06-23 at 11 38 15 pm\" src=\"https://cloud.githubusercontent.com/assets/4898263/16330389/ac42f926-399d-11e6-84d6-4110fc3293b2.png\">\n### After\n\n<img width=\"501\" alt=\"screen shot 2016-06-23 at 11 54 38 pm\" src=\"https://cloud.githubusercontent.com/assets/4898263/16330423/e8645ad0-399d-11e6-981a-9c2fa8756a42.png\">\n", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 3021, "title": "\"Could not find platform independent libraries\"  && \"Missing input file\"", "body": "I am installing Tensorflow on a server without internet access.  I can now build tutorials_example_trainer without GPU support and successfully execute it, but can't move forward to complete the installation. Can you please help me to diagnosis ? Thanks!  \n### Environment info\n\nOperating System:\n   Red Hat Enterprise Linux Server release 6.6 (Santiago)\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n-rw-r--r-- 1 root root  28585480 Aug 15  2015 libcublas_device.a\nlrwxrwxrwx 1 root root        16 Jun 15 19:52 libcublas.so -> libcublas.so.7.5\nlrwxrwxrwx 1 root root        19 Jun 15 19:52 libcublas.so.7.5 -> libcublas.so.7.5.18\n-rwxr-xr-x 1 root root  23938736 Aug 15  2015 libcublas.so.7.5.18\n-rw-r--r-- 1 root root  28220076 Aug 15  2015 libcublas_static.a\n-rw-r--r-- 1 root root    322936 Aug 15  2015 libcudadevrt.a\nlrwxrwxrwx 1 root root        16 Jun 15 19:52 libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root        19 Jun 22 03:12 libcudart.so.7.0 -> libcudart.so.7.5.18\nlrwxrwxrwx 1 root root        19 Jun 15 19:52 libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root    383336 Aug 15  2015 libcudart.so.7.5.18\n-rw-r--r-- 1 root root    720192 Aug 15  2015 libcudart_static.a\n-rwxr-xr-x 1 root root  59909104 Jun 15 20:09 libcudnn.so\n-rwxr-xr-x 1 root root  59909104 Jun 15 20:09 libcudnn.so.5\n-rwxr-xr-x 1 root root  59909104 Jun 15 20:09 libcudnn.so.5.0.5\nlrwxrwxrwx 1 root root        17 Jun 22 03:13 libcudnn.so.6.5 -> libcudnn.so.5.0.5\n-rw-r--r-- 1 root root  58775484 Jun 15 20:09 libcudnn_static.a\nlrwxrwxrwx 1 root root        15 Jun 15 19:52 libcufft.so -> libcufft.so.7.5\nlrwxrwxrwx 1 root root        18 Jun 15 19:52 libcufft.so.7.5 -> libcufft.so.7.5.18\n-rwxr-xr-x 1 root root 111231960 Aug 15  2015 libcufft.so.7.5.18\n-rw-r--r-- 1 root root 115104400 Aug 15  2015 libcufft_static.a\nlrwxrwxrwx 1 root root        16 Jun 15 19:52 libcufftw.so -> libcufftw.so.7.5\nlrwxrwxrwx 1 root root        19 Jun 15 19:52 libcufftw.so.7.5 -> libcufftw.so.7.5.18\n-rwxr-xr-x 1 root root    447664 Aug 15  2015 libcufftw.so.7.5.18\n-rw-r--r-- 1 root root     42206 Aug 15  2015 libcufftw_static.a\nlrwxrwxrwx 1 root root        17 Jun 15 19:52 libcuinj64.so -> libcuinj64.so.7.5\nlrwxrwxrwx 1 root root        20 Jun 15 19:52 libcuinj64.so.7.5 -> libcuinj64.so.7.5.18\n-rwxr-xr-x 1 root root   5751400 Aug 15  2015 libcuinj64.so.7.5.18\n-rw-r--r-- 1 root root   1649726 Aug 15  2015 libculibos.a\nlrwxrwxrwx 1 root root        16 Jun 15 19:52 libcurand.so -> libcurand.so.7.5\nlrwxrwxrwx 1 root root        19 Jun 15 19:52 libcurand.so.7.5 -> libcurand.so.7.5.18\n-rwxr-xr-x 1 root root  51765952 Aug 15  2015 libcurand.so.7.5.18\n-rw-r--r-- 1 root root  51992564 Aug 15  2015 libcurand_static.a\nlrwxrwxrwx 1 root root        18 Jun 15 19:52 libcusolver.so -> libcusolver.so.7.5\nlrwxrwxrwx 1 root root        21 Jun 15 19:52 libcusolver.so.7.5 -> libcusolver.so.7.5.18\n-rwxr-xr-x 1 root root  37034328 Aug 15  2015 libcusolver.so.7.5.18\n-rw-r--r-- 1 root root  16613348 Aug 15  2015 libcusolver_static.a\nlrwxrwxrwx 1 root root        18 Jun 15 19:52 libcusparse.so -> libcusparse.so.7.5\nlrwxrwxrwx 1 root root        21 Jun 15 19:52 libcusparse.so.7.5 -> libcusparse.so.7.5.18\n-rwxr-xr-x 1 root root  36816424 Aug 15  2015 libcusparse.so.7.5.18\n-rw-r--r-- 1 root root  44445334 Aug 15  2015 libcusparse_static.a\nlrwxrwxrwx 1 root root        14 Jun 15 19:52 libnppc.so -> libnppc.so.7.5\nlrwxrwxrwx 1 root root        17 Jun 15 19:52 libnppc.so.7.5 -> libnppc.so.7.5.18\n-rwxr-xr-x 1 root root    427168 Aug 15  2015 libnppc.so.7.5.18\n-rw-r--r-- 1 root root     20664 Aug 15  2015 libnppc_static.a\nlrwxrwxrwx 1 root root        14 Jun 15 19:52 libnppi.so -> libnppi.so.7.5\nlrwxrwxrwx 1 root root        17 Jun 15 19:52 libnppi.so.7.5 -> libnppi.so.7.5.18\n-rwxr-xr-x 1 root root  63516808 Aug 15  2015 libnppi.so.7.5.18\n-rw-r--r-- 1 root root  90106200 Aug 15  2015 libnppi_static.a\nlrwxrwxrwx 1 root root        14 Jun 15 19:52 libnpps.so -> libnpps.so.7.5\nlrwxrwxrwx 1 root root        17 Jun 15 19:52 libnpps.so.7.5 -> libnpps.so.7.5.18\n-rwxr-xr-x 1 root root   6047400 Aug 15  2015 libnpps.so.7.5.18\n-rw-r--r-- 1 root root   8647292 Aug 15  2015 libnpps_static.a\nlrwxrwxrwx 1 root root        16 Jun 15 19:52 libnvblas.so -> libnvblas.so.7.5\nlrwxrwxrwx 1 root root        19 Jun 15 19:52 libnvblas.so.7.5 -> libnvblas.so.7.5.18\n-rwxr-xr-x 1 root root    456112 Aug 15  2015 libnvblas.so.7.5.18\nlrwxrwxrwx 1 root root        24 Jun 15 19:52 libnvrtc-builtins.so -> libnvrtc-builtins.so.7.5\nlrwxrwxrwx 1 root root        27 Jun 15 19:52 libnvrtc-builtins.so.7.5 -> libnvrtc-builtins.so.7.5.18\n-rwxr-xr-x 1 root root  22408512 Aug 15  2015 libnvrtc-builtins.so.7.5.18\nlrwxrwxrwx 1 root root        15 Jun 15 19:52 libnvrtc.so -> libnvrtc.so.7.5\nlrwxrwxrwx 1 root root        18 Jun 15 19:52 libnvrtc.so.7.5 -> libnvrtc.so.7.5.17\n-rwxr-xr-x 1 root root  18199288 Aug 15  2015 libnvrtc.so.7.5.17\nlrwxrwxrwx 1 root root        18 Jun 15 19:52 libnvToolsExt.so -> libnvToolsExt.so.1\nlrwxrwxrwx 1 root root        22 Jun 15 19:52 libnvToolsExt.so.1 -> libnvToolsExt.so.1.0.0\n-rwxr-xr-x 1 root root     37936 Aug 15  2015 libnvToolsExt.so.1.0.0\n-rw-r--r-- 1 root root     25840 Aug 15  2015 libOpenCL.so\ndrwxr-xr-x 2 root root      4096 Jun 15 19:52 stubs\n\nIf installed from sources, provide the commit hash:\n  r0.9 master\n### What have you tried?\n1. Transferred the tensorflow repo and the dependency repos to server, modified WORKSPACE and tensorflow/workspace.bzl\n2. Built bazel-real from source code and gcc-4.9.0 from source because version of the gcc library with the server isn't compatible. \n3. Manually copied anaconda3 (with python 3.5), \n4. set customized PATH and LD_LIBRARY_PATH to use the new bazel, gcc, python etc.\n5. eventually successfully build tutorials_example_trainer without GPU support\n        bazel build -c opt //tensorflow/cc:tutorials_example_trainer\n   and can run it without use_gpu\n        bazel-bin/tensorflow/cc/tutorials_example_trainer \n6. But failed on building tutorials_example_trainer with cuda. Seems need to set PYTHONHOME. but after I didn't that, no change at all.\n   \n   ```\n   bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\n   ```\n\nERROR: /home/wzhan/.cache/bazel/_bazel_wzhan/c2f4a381d5d3be272ee3353b5d3cb1aa/external/protobuf/BUILD:331:1: Linking of rule '@protobuf//:protoc' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command\n  (cd /home/wzhan/.cache/bazel/_bazel_wzhan/c2f4a381d5d3be272ee3353b5d3cb1aa/execroot/tensorflow && \\\n  exec env - \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/host/bin/external/protobuf/protoc bazel-out/host/bin/external/protobuf/_objs/protoc/external/protobuf/src/google/protobuf/compiler/main.o bazel-out/host/bin/external/protobuf/libprotoc_lib.a bazel-out/host/bin/external/protobuf/libprotobuf.a bazel-out/host/bin/external/protobuf/libprotobuf_lite.a -lpthread -lstdc++ -B/usr/bin/ -pie -Wl,-z,relro,-z,now -no-canonical-prefixes -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,-S -Wl,--gc-sections): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nCould not find platform independent libraries <prefix>\nCould not find platform dependent libraries <exec_prefix>\nConsider setting $PYTHONHOME to <prefix>[:<exec_prefix>]\nImportError: No module named site\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nINFO: Elapsed time: 22.901s, Critical Path: 17.57s\n1. Failed to build :build_pip_package (without gpu support). It reported different file missed each time \n\n$ bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n/home/wzhan/.bazel/bin/bazel-real\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nWARNING: /home/wzhan/.cache/bazel/_bazel_wzhan/c2f4a381d5d3be272ee3353b5d3cb1aa/external/protobuf/WORKSPACE:1: Workspace name in /home/wzhan/.cache/bazel/_bazel_wzhan/c2f4a381d5d3be272ee3353b5d3cb1aa/external/protobuf/WORKSPACE (@__main__) does not match the name given in the repository's definition (@protobuf); this will cause a build error in future versions.\nWARNING: /usr/local/tf/tensorflow/util/python/BUILD:11:16: in includes attribute of cc_library rule //util/python:python_headers: 'python_include' resolves to 'util/python/python_include' not in 'third_party'. This will be an error in the future.\nWARNING: /home/wzhan/.cache/bazel/_bazel_wzhan/c2f4a381d5d3be272ee3353b5d3cb1aa/external/re2/WORKSPACE:1: Workspace name in /home/wzhan/.cache/bazel/_bazel_wzhan/c2f4a381d5d3be272ee3353b5d3cb1aa/external/re2/WORKSPACE (@__main__) does not match the name given in the repository's definition (@re2); this will cause a build error in future versions.\nINFO: Found 1 target...\nERROR: missing input file ## ### **'**@webcomponentsjs//:ShadowDOM.min.js**'.**\nERROR: /usr/local/tf/tensorflow/tensorflow/tools/pip_package/BUILD:23:1: Creating runfiles tree bazel-out/local-py3-opt/bin/tensorflow/tools/pip_package/build_pip_package.runfiles failed: build-runfiles failed: error executing command /home/wzhan/.cache/bazel/_bazel_wzhan/c2f4a381d5d3be272ee3353b5d3cb1aa/execroot/tensorflow/_bin/build-runfiles ... (remaining 2 argument(s) skipped): com.google.devtools.build.lib.shell.AbnormalTerminationException: Process terminated by signal 15.\nERROR: /usr/local/tf/tensorflow/tensorflow/tools/pip_package/BUILD:23:1: //tensorflow/tools/pip_package:build_pip_package: missing input file '@webcomponentsjs//:ShadowDOM.min.js'.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nERROR: /usr/local/tf/tensorflow/tensorflow/tools/pip_package/BUILD:23:1 1 input file(s) do not exist.\nINFO: Elapsed time: 3.593s, Critical Path: 0.08s\n\n8 build :build_pip_package with GPU support also reports \"missing file\"\n\n$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n/home/wzhan/.bazel/bin/bazel-real\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nWARNING: /home/wzhan/.cache/bazel/_bazel_wzhan/c2f4a381d5d3be272ee3353b5d3cb1aa/external/protobuf/WORKSPACE:1: Workspace name in /home/wzhan/.cache/bazel/_bazel_wzhan/c2f4a381d5d3be272ee3353b5d3cb1aa/external/protobuf/WORKSPACE (@__main__) does not match the name given in the repository's definition (@protobuf); this will cause a build error in future versions.\nWARNING: /usr/local/tf/tensorflow/util/python/BUILD:11:16: in includes attribute of cc_library rule //util/python:python_headers: 'python_include' resolves to 'util/python/python_include' not in 'third_party'. This will be an error in the future.\nWARNING: /home/wzhan/.cache/bazel/_bazel_wzhan/c2f4a381d5d3be272ee3353b5d3cb1aa/external/re2/WORKSPACE:1: Workspace name in /home/wzhan/.cache/bazel/_bazel_wzhan/c2f4a381d5d3be272ee3353b5d3cb1aa/external/re2/WORKSPACE (@__main__) does not match the name given in the repository's definition (@re2); this will cause a build error in future versions.\nINFO: Found 1 target...\nERROR: missing input file '@grpc//:src/core/lib/support/tmpfile_win32.c'.\nERROR: /home/wzhan/.cache/bazel/_bazel_wzhan/c2f4a381d5d3be272ee3353b5d3cb1aa/external/protobuf/BUILD:71:1: C++ compilation of rule '@protobuf//:protobuf_lite' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object ... (remaining 43 argument(s) skipped): com.google.devtools.build.lib.shell.AbnormalTerminationException: Process terminated by signal 15.\nERROR: /home/wzhan/.cache/bazel/_bazel_wzhan/c2f4a381d5d3be272ee3353b5d3cb1aa/external/grpc/BUILD:38:1: @grpc//:gpr: missing input file '@grpc//:src/core/lib/support/tmpfile_win32.c'.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nERROR: /home/wzhan/.cache/bazel/_bazel_wzhan/c2f4a381d5d3be272ee3353b5d3cb1aa/external/grpc/BUILD:38:1 1 input file(s) do not exist.\nINFO: Elapsed time: 4.185s, Critical Path: 2.34s\n", "comments": ["from the error message, it seems python version problem, but the problem still there after I changed a couple of different versions. all version can \"import site\" correctly when it runs alone\n\nCould not find platform independent libraries <prefix>\nCould not find platform dependent libraries <exec_prefix>\nConsider setting $PYTHONHOME to <prefix>[:<exec_prefix>]\nImportError: No module named site\n", "After struggling with the installation on the silly \"secured\" server for two weeks, the team started to discuss switching to torch as it already works. Can anybody please help?\n", "Which version of bazel have you compiled? We're testing with 0.3 (not head), it would probably be good to rule that out as a source of trouble.\n\n@damienmg can you see something in this?  \n", "Release 0.3.0 (2016-06-10), baseline: a9301fa\n\nIs it the correct one? \n", "Building errors keep changing. Now when I execute \" bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer --verbose_failures\",  the error messages are:\n\nERROR: /root/.cache/bazel/_bazel_root/48b0697df5815d95b49fd98c3083955b/external/protobuf/BUILD:331:1: Linking of rule '@protobuf//:protoc' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command\n  (cd /root/.cache/bazel/_bazel_root/48b0697df5815d95b49fd98c3083955b/execroot/tensorflow && \\\n  exec env - \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/host/bin/external/protobuf/protoc bazel-out/host/bin/external/protobuf/_objs/protoc/external/protobuf/src/google/protobuf/compiler/main.o bazel-out/host/bin/external/protobuf/libprotoc_lib.a bazel-out/host/bin/external/protobuf/libprotobuf.a bazel-out/host/bin/external/protobuf/libprotobuf_lite.a -lpthread -lstdc++ -B/usr/bin/ -pie -Wl,-z,relro,-z,now -no-canonical-prefixes -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,-S -Wl,--gc-sections): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nCould not find platform independent libraries <prefix>\nCould not find platform dependent libraries <exec_prefix>\nConsider setting $PYTHONHOME to <prefix>[:<exec_prefix>]\nImportError: No module named site\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\n\n### when I manually change to the tmp directory and execute the command, no error at all.\n", "> Transferred the tensorflow repo and the dependency repos to server, modified WORKSPACE and tensorflow/workspace.bzl\n\nHow did you do that?\n", "download the repos from another machine, transferred to the server and modify new_git_repository -> new_local_repository, new_http_archive -> new_local_repository\n", "What does your new workspace.bzl looks like?\n\nCan you try to `bazel build @webcomponentsjs//:ShadowDOM.min.js`?\n", "### workspace.bzl looks like below\n\ndef tf_workspace(path_prefix = \"\", tf_repo_name = \"\"):\n  native.new_local_repository(\n    name = \"eigen_archive\",\n    path = \"/usr/local/tf/tensorflow/local_repos/eigen-eigen-5f86b31739cd\",\n    build_file = path_prefix + \"eigen.BUILD\",\n  )\n\n  native.local_repository(\n    name = \"re2\",\n    path = \"/usr/local/tf/tensorflow/local_repos/re2\",\n  )\n\n  native.local_repository(\n    name = \"gemmlowp\",\n    path = \"/usr/local/tf/tensorflow/local_repos/gemmlowp\",\n  )\n\n  native.new_local_repository(\n    name = \"farmhash_archive\",\n    path = \"/usr/local/tf/tensorflow/local_repos/farmhash-34c13ddfab0e35422f4c3979f360635a8c050260\",\n    build_file = path_prefix + \"farmhash.BUILD\",\n  )\n  ...\n\n### webcomponentjs settings in WORKSPACE\n\nnew_local_repository(\n  name = \"webcomponentsjs\",\n  build_file = \"/usr/local/tf/tensorflow/bower.BUILD\",\n  path = \"/usr/local/tf/tensorflow/local_repos/webcomponentsjs\",\n)\n\n### ls local_repos/webcomponentsjs/ -l\n\ntotal 68\n-rw-r--r--  1 wzhan root  564 Jun 23 01:17 banner.txt\n-rw-r--r--  1 wzhan root  423 Jun 23 01:17 bower.json\n-rw-r--r--  1 wzhan root 2507 Jun 23 01:17 CONTRIBUTING.md\n-rw-r--r--  1 wzhan root 3781 Jun 23 01:17 gulpfile.js\n-rw-r--r--  1 wzhan root 1559 Jun 23 01:17 LICENSE.md\n-rw-r--r--  1 wzhan root  737 Jun 23 01:17 package.json\n-rw-r--r--  1 wzhan root 9095 Jun 23 01:17 README.md\ndrwxr-xr-x 11 wzhan root 4096 Jun 23 01:17 src\ndrwxr-xr-x 10 wzhan root 4096 Jun 23 01:17 tests\n-rw-r--r--  1 wzhan root   93 Jun 23 01:17 wct.conf.json\n-rw-r--r--  1 wzhan root 2916 Jun 23 01:17 webcomponents.js\n-rw-r--r--  1 wzhan root 2167 Jun 23 01:17 webcomponents-lite.js\n-rw-r--r--  1 wzhan root 2167 Jun 24 03:26 webcomponents-lite.min.js\n-rw-r--r--  1 root  root 2916 Jun 25 15:46 webcomponents.min.js\ndrwxr-xr-x  2 wzhan root 4096 Jun 23 01:17 workbench\n\n### LOG of bazel build @webcomponentsjs//:ShadowDOM.min.js --verbose_failures\n\nINFO: Found 1 target...\nERROR: missing input file '@webcomponentsjs//:ShadowDOM.min.js'.\nERROR: /root/.cache/bazel/_bazel_root/48b0697df5815d95b49fd98c3083955b/external/webcomponentsjs/BUILD:577:12: @webcomponentsjs//:ShadowDOM.min.js: missing input file '@webcomponentsjs//:ShadowDOM.min.js'.\nERROR: /root/.cache/bazel/_bazel_root/48b0697df5815d95b49fd98c3083955b/external/webcomponentsjs/BUILD:577:12 1 input file(s) do not exist.\nINFO: Elapsed time: 1.799s, Critical Path: 0.00s\n", "You cloned the wrong version of webcomponentjs, see https://github.com/webcomponents/webcomponentsjs/tree/v0.7.22 for the good one (you should clone tag the specific tag)\n", "Thanks Damien. I'm closing this. @zhangwei0119 if you have indications for a bug in TensorFlow, you can comment to reopen.\n"]}, {"number": 3020, "title": "Support GIF image decode ops by giflib library", "body": "Refactor the code from the former PR #2578, to use giflib instead of FreeImage. \n@martinwicke Please review this. Thanks\n", "comments": ["Can one of the admins verify this patch?\n", "Cool. While I look at the code in detail, can you remove earth.gif? If you need a gif for tests (I'm assuming you do), can you make or use one that is <20k? We're trying to keep the repo of manageable size.\n", "The gif file for test case has been replaced. Thanks\n", "Jenkins, test this please.\n", "Hi @buaaliyi \n\nIf only single frame can be decoded, what's the point of using GIF? It will be really helpful, if this can be extended to load all frames. In that case, you may need to take care of the \"coalesce\". I am happy to help in this effort.\n", "Hi @raingo ,\nYou are right. However, I'm not quite familiar with the 'Tensor' representation to store multiple images in just one Tensor, which compatible with current image decoding method such as jpeg and png. So the code just decode the first frame at this moment, and to be fixed. I'll try in the future. Furthermore, thank you for your help.\n", "@martinwicke do you have comments on how to feed in variable length images? Examples from existing operations can help. Thanks!\n", "If we add reading all frames (which I think would be great, it's the main use of the gif format these days after all), the output should simply be a 4D tensor `[frames, height, width, channels]`. In case the gif has only a single image, the output would be `[1, height, width, channels]`. This is different from the `decode_jpg`, and `decode_png` functions, but we can provide a Python convenience function `decode_single_frame_gif` or similar which collapses the first dimension.\n", "Jenkins, test this please.\n", "Since this is included in #3264, I'd rather accept that (it also fixes the test failures). I'll then close this PR. @buaaliyi could you reply to #3264 to make our CLAbot happy?\n"]}, {"number": 3019, "title": "R0.9", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}, {"number": 3018, "title": "no such package '@webcomponentsjs//': Error cloning repository", "body": "I am trying to build the r0.8 `Dockerfile.devel-gpu` image:\n\n```\nERROR: /tensorflow/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@webcomponentsjs//': Error cloning repository: https://github.com/polymer/webcomponentsjs.git: cannot open git-upload-pack caused by https://github.com/polymer/webcomponentsjs.git: cannot open git-upload-pack caused by Remote host closed connection during handshake caused by SSL peer shut down incorrectly and referenced by '//tensorflow/tensorboard/bower:bower'.\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\n```\n", "comments": ["This is a bazel issue (https://github.com/bazelbuild/bazel/issues/1194). It happens intermittently and surfaces because bazel doesn't retry when the git clone fails. Please try building again.\n"]}, {"number": 3017, "title": "how to generate image_ops.h with makefile?", "body": "As TensorFlow v0.9.0 RC0 add makefile for better cross-platform build support (C API only), i try to compile [Image Recognition](https://www.tensorflow.org/versions/r0.9/tutorials/image_recognition/index.html) with makefile (which is compiled with bazel originally). However, it raise error that \n\n```\nimage_ops.h: No such file...\n```\n\nAnd I cannot find `image_ops.h` in `tensorflow/cc/ops/`. But when I compile it with Bazel, the error is gone.\n### what you have tried?\n1. I analysed `tensorflow/tensorflow.bzl` and `BUILD` files related and found that image_ops.h(or many others) was generated by `tensorflow/cc/ops/cc_op_gen.cc` automatically.\n2. I found files which generated by`tensorflow/contrib/makefile/gen_file_lists.sh` didn't contain cc_ops_gen.cc etc. Is it not supported yet with makefile?\n### My Questions:\n1. how to generate image_ops.h with makefile so as to compile [Image Recongnition](https://www.tensorflow.org/versions/r0.9/tutorials/image_recognition/index.html) successfully.\n2. Has image_ops any other lib dependencies?\n", "comments": ["A simple workaround is: run Bazel to generate the ops files, then use them.\n\nFYI, I asked if these files could be generted by Makefile before:\nhttps://github.com/tensorflow/tensorflow/pull/2440#issuecomment-221466416\n", "@cg31 Thank you for your reply. The idea, using the ops files generated by Bazel, seems to NOT work for me.\nActually, I wanna to use ops such as DecodePng, DecodeJpeg, ReadFile, etc. However, these ops are not compiled into libtensorflow-core.a and link errors below are raised.\n\n```\nmain.cc:(.text+0x386): undefined reference to `tensorflow::ops::Const(tensorflow::StringPiece, tensorflow::GraphDefBuilder::Options const&)'\nmain.cc:(.text+0x3b9): undefined reference to `tensorflow::ops::ReadFile(tensorflow::NodeBuilder::NodeOut, tensorflow::GraphDefBuilder::Options const&)'\nmain.cc:(.text+0x4ff): undefined reference to `tensorflow::ops::DecodePng(tensorflow::NodeBuilder::NodeOut, tensorflow::GraphDefBuilder::Options const&)'\nmain.cc:(.text+0x5fa): undefined reference to `tensorflow::ops::DecodeJpeg(tensorflow::NodeBuilder::NodeOut, tensorflow::GraphDefBuilder::Options const&)'\nmain.cc:(.text+0x6ad): undefined reference to `tensorflow::ops::Cast(tensorflow::NodeBuilder::NodeOut, tensorflow::DataType, tensorflow::GraphDefBuilder::Options const&)'\nmain.cc:(.text+0x700): undefined reference to `tensorflow::ops::Const(int, tensorflow::GraphDefBuilder::Options const&)'\nmain.cc:(.text+0x754): undefined reference to `tensorflow::ops::ExpandDims(tensorflow::NodeBuilder::NodeOut, tensorflow::NodeBuilder::NodeOut, tensorflow::GraphDefBuilder::Options const&)'\nmain.cc:(.text+0x87f): undefined reference to `tensorflow::ops::ResizeBilinear(tensorflow::NodeBuilder::NodeOut, tensorflow::NodeBuilder::NodeOut, tensorflow::GraphDefBuilder::Options const&)'\nmain.cc:(.text+0xa2d): undefined reference to `tensorflow::ops::Sub(tensorflow::NodeBuilder::NodeOut, tensorflow::NodeBuilder::NodeOut, tensorflow::GraphDefBuilder::Options const&)'\nmain.cc:(.text+0xa67): undefined reference to `tensorflow::ops::Div(tensorflow::NodeBuilder::NodeOut, tensorflow::NodeBuilder::NodeOut, tensorflow::GraphDefBuilder::Options const&)'\nmain.o: In function `LoadGraph(std::string, std::unique_ptr<tensorflow::Session, std::default_delete<tensorflow::Session> >*)':\nmain.cc:(.text+0x1170): undefined reference to `tensorflow::ReadBinaryProto(tensorflow::Env*, std::string const&, google::protobuf::MessageLite*)'\nmain.o: In function `GetTopLabels(std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> > const&, int, tensorflow::Tensor*, tensorflow::Tensor*)':\nmain.cc:(.text+0x1445): undefined reference to `tensorflow::ops::Const(int, tensorflow::GraphDefBuilder::Options const&)'\nmain.cc:(.text+0x1490): undefined reference to `tensorflow::ops::Const(tensorflow::Tensor const&, tensorflow::GraphDefBuilder::Options const&)'\nmain.cc:(.text+0x14ca): undefined reference to `tensorflow::ops::TopKV2(tensorflow::NodeBuilder::NodeOut, tensorflow::NodeBuilder::NodeOut, tensorflow::GraphDefBuilder::Options const&)'\nmain.o: In function `main':\nmain.cc:(.text+0x22ed): undefined reference to `tensorflow::Flag::Flag(char const*, std::string*)'\nmain.cc:(.text+0x230f): undefined reference to `tensorflow::Flag::Flag(char const*, std::string*)'\nmain.cc:(.text+0x2331): undefined reference to `tensorflow::Flag::Flag(char const*, std::string*)'\nmain.cc:(.text+0x23e7): undefined reference to `tensorflow::Flag::Flag(char const*, std::string*)'\nmain.cc:(.text+0x240c): undefined reference to `tensorflow::Flag::Flag(char const*, std::string*)'\nmain.o:main.cc:(.text+0x2456): more undefined references to `tensorflow::Flag::Flag(char const*, std::string*)' follow\nmain.o: In function `main':\nmain.cc:(.text+0x267d): undefined reference to `tensorflow::io::JoinPath(tensorflow::StringPiece, tensorflow::StringPiece)'\nmain.cc:(.text+0x279f): undefined reference to `tensorflow::io::JoinPath(tensorflow::StringPiece, tensorflow::StringPiece)'\nmain.o: In function `tensorflow::ops::Const(std::initializer_list<float>, tensorflow::GraphDefBuilder::Options const&)':\n```\n", "Hi, \n\nI think I have the same issue; I cannot find the many .h files needed to compile via C/C++ api and apparently generated by cc_op_gen (the first of which is array_ops.h).\n\n@cg31: how can I run Bazel to generate the ops files? What command should I use?\nI've tried \"bazel build cc_ops\" (while inside the cc folder), but that didn't seem work.\n\nI'm sorry if the question is silly - I'm a tensorflow/bazel newbie...\n\nDaniele\n", "@danieleghisi you can run bazel in the tensorflow root directory\n`bazel build tensorflow/cc:cc_ops`. But, the ops files generated stay in the `bazel-genfiles/tensorflow/cc/ops`\n\n`bazel build cc_ops` don't work because of some relative path in the BUILD file.\n\neg.\n\n```\ngit clone https://github.com/tensorflow/tensorflow\ncd tensorflow\nbazel build tensorflow/cc:cc_ops\n```\n", "Thanks, I had managed to compile, but I didn't know that generated files were in bazel-genfiles/tensorflow/cc/ops and for some reason I couldn't find them. That works!\n", "@danieleghisi, did this work for you? I am still finding I need to link against the gif_archive / jpeg_archive / png_archive to get the functions that underly the DecodeImage ops...\n", "@tomasreimers I still have some issues but I can't fully remember which ones; I ended up removing everything and reinstalling, yet I gave up on the C api thing; just using python...\n", "I just ran into the same missing file on Windows when building.  Bazel isn't one of the Windows requirements, so how do I generate the file on that platform?  CMAKE/SWIG don't return any errors, and only when building with MSBuild do I see image_ops.h and array_ops.h as missing.\n", "On windows, you can use either CMAKE or Bazel.\r\nWe use makefile primarily for iOS support.\r\n@kakashidan It is possible some of the libraries you are trying to use are excluded in makefile build?\r\nYou can see this piece for details:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/Makefile#L418\r\n\r\nPlease see the README file in makefile folder for more details.\r\n", "Assuming @gunan 's instructions worked.", "@gunan So what are the steps to include for example \"tensorflow/cc/ops/array_ops.h\" to the result build? Do I need firstly build with Bazel and then with makefile or do I need to add functions I want to use to makefile/tf_*_files.sh and then just build with makefile? I'm trying to build for iOS using build_all_ios.sh so could you please describe the steps? I would like to use examples from https://www.tensorflow.org/api_guides/cc/guide but I need use Bazel functionality. It is good to reduce the lib size to exclude some functions but how could I run different small examples on iOS?  "]}, {"number": 3016, "title": "WIP: Reorganizes tutorial navigation", "body": "Reorganizes tutorial navigation\n", "comments": ["Can one of the admins verify this patch?\n"]}]