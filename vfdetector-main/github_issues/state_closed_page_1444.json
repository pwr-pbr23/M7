[{"number": 9640, "title": "Java API to get the size of output list operations", "body": "This opens up access to the TF_OperationOutputListLength C API from java, for operations that return output lists.", "comments": ["Can one of the admins verify this patch?", "great suggestions @asimshankar - thanks, and I've incorporated them wholesale.", "@tensorflow-jenkins test this please"]}, {"number": 9639, "title": "Keep auditwheel at 1.5.0.", "body": "Nightly is failing with the newest auditwheel release.", "comments": []}, {"number": 9638, "title": "Seg fault on session run when built with CMake and optimize for native arch enabled", "body": "**System info: Ubuntu 16.04 64 bit, gcc 5.4.0, Intel i5 CPU**\r\n\r\nSegmentation fault occurs in Eigen when a certain AVX instruction is performed (see stack trace below). This occurs during session run of several convolutional neural network graphs.\r\n\r\nTensorflow (checked out from the master branch today) is built using CMake with tensorflow_BUILD_SHARED_LIB enabled which generates a libtensorflow.so library file. This library file is linked to another C++ application which simply loads a graph and executes it.\r\n\r\nDisabling the CMake option tensorflow_OPTIMIZE_FOR_NATIVE_ARCH removes the error, but probably also reduce performance.\r\n\r\nBelow is a nasty long stack trace, if you need any other info please let me know.\r\n\r\nStack trace:\r\n```\r\n#0 0x00007fffee19021b in _mm256_store_ps (__A=..., __P=0x7fff7c110cd0) at /usr/lib/gcc/x86_64-linux-gnu/5/include/avxintrin.h:854\r\n#1 Eigen::internal::pstore<float, float __vector(8)>(float*, float __vector(8) const&) (to=0x7fff7c110cd0, from=...)\r\n  at /home/smistad/workspace/FAST/build_Release/external/tensorflow/external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/AVX/PacketMath.h:260\r\n#2 0x00007fffefd255b5 in Eigen::internal::gemm_pack_lhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, 24, 8, 0, false, false>::operator() (\r\n  this=0x7fff98fd4a67, blockA=0x7fff7c110cd0, lhs=..., depth=9, rows=8, stride=0, offset=0)\r\n  at /home/smistad/workspace/FAST/build_Release/external/tensorflow/external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/products/GeneralBlockPanelKernel.h:1767\r\n#3 0x00007fffeff65692 in Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>::Context<Eigen::internal::gemm_pack_lhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, 24, 8, 0, false, false>, Eigen::internal::gemm_pack_rhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, 4, 0, false, false>, Eigen::internal::gebp_kernel<float, float, long, Eigen::internal::blas_data_mapper<float, long, 0, 0>, 24, 4, false, false>, Eigen::internal::TensorContractionInputMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, Eigen::internal::blas_data_mapper<float, long, 0, 0> >::pack_lhs (\r\n  this=0x7fff937fbbd0, m=1, k=0) at /home/smistad/workspace/FAST/build_Release/external/tensorflow/external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContractionThreadPool.h:495\r\n#4 0x00007fffeff63758 in Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>::Context<Eigen::internal::gemm_pack_lhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, 24, 8, 0, false, false>, Eigen::internal::gemm_pack_rhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, 4, 0, false, false>, Eigen::internal::gebp_kernel<float, float, long, Eigen::internal::blas_data_mapper<float, long, 0, 0>, 24, 4, false, false>, Eigen::internal::TensorContractionInputMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, Eigen::internal::blas_data_mapper<float, long, 0, 0> >::enqueue_packing_helper (this=0x7fff937fbbd0, start=1, end=2, k=0, rhs=false) at /home/smistad/workspace/FAST/build_Release/external/tensorflow/external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContractionThreadPool.h:624\r\n#5 0x00007fffeff6369d in Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>::Context<Eigen::internal::gemm_pack_lhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, 24, 8, 0, false, false>, Eigen::internal::gemm_pack_rhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, 4, 0, false, false>, Eigen::internal::gebp_kernel<float, float, long, Eigen::internal::blas_data_mapper<float, long, 0, 0>, 24, 4, false, false>, Eigen::internal::TensorContractionInputMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, Eigen::internal::blas_data_mapper<float, long, 0, 0> >::enqueue_packing_helper(long, long, long, bool)::{lambda()#1}::operator()() const (__closure=0x7fff7c1aaca0)\r\n  at /home/smistad/workspace/FAST/build_Release/external/tensorflow/external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContractionThreadPool.h:628\r\n#6 0x00007fffeff73cca in std::_Bind<Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>::Context<Eigen::internal::gemm_pack_lhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, 24, 8, 0, false, false>, Eigen::internal::gemm_pack_rhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, 4, 0, false, false>, Eigen::internal::gebp_kernel<float, float, long, Eigen::internal::blas_data_mapper<float, long, 0, 0>, 24, 4, false, false>, Eigen::internal::TensorContractionInputMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, Eigen::internal::blas_data_mapper<float, long, 0, 0> >::enqueue_packing_helper(long, long, long, bool)::{lambda()#1} ()>::__call<void>(std::tuple<>&&, std::_Index_tuple<>) (this=0x7fff7c1aaca0,\r\n  __args=<unknown type in /home/smistad/workspace/FAST/build_Release/lib/libtensorflow.so, CU 0xf277508, DIE 0xf39a603>) at /usr/include/c++/5/functional:1074\r\n#7 0x00007fffeff71a5d in std::_Bind<Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>::Context<Eigen::internal::gemm_pack_lhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, 24, 8, 0, false, false>, Eigen::internal::gemm_pack_rhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, 4, 0, false, false>, Eigen::internal::gebp_kernel<float, float, long, Eigen::internal::blas_data_mapper<float, long, 0, 0>, 24, 4, false, false>, Eigen::internal::TensorContractionInputMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, Eigen::internal::blas_data_mapper<float, long, 0, 0> >::enqueue_packing_helper(long, long, long, bool)::{lambda()#1} ()>::operator()<, void>() (this=0x7fff7c1aaca0) at /usr/include/c++/5/functional:1133\r\n#8 0x00007fffeff6d3ac in std::_Function_handler<void (), std::_Bind<Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>::Context<Eigen::internal::gemm_pack_lhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::M---Type <return> to continue, or q <return> to quit---\r\nakePointer>, 24, 8, 0, false, false>, Eigen::internal::gemm_pack_rhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, 4, 0, false, false>, Eigen::internal::gebp_kernel<float, float, long, Eigen::internal::blas_data_mapper<float, long, 0, 0>, 24, 4, false, false>, Eigen::internal::TensorContractionInputMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 8, true, false, 0, Eigen::MakePointer>, Eigen::internal::blas_data_mapper<float, long, 0, 0> >::enqueue_packing_helper(long, long, long, bool)::{lambda()#1} ()> >::_M_invoke(std::_Any_data const&) (__functor=...) at /usr/include/c++/5/functional:1871\r\n#9 0x00007fffedeba4c8 in std::function<void ()>::operator()() const (this=0x7fff7c1aace0) at /usr/include/c++/5/functional:2267\r\n#10 0x00007fffedeb9da7 in tensorflow::thread::EigenEnvironment::ExecuteTask (this=0x21bd7a8, t=...) at /home/smistad/workspace/FAST/build_Release/external/tensorflow/src/tensorflow/tensorflow/core/lib/core/threadpool.cc:81\r\n#11 0x00007fffedebca71 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop (this=0x21bd7a0, thread_id=3)\r\n  at /home/smistad/workspace/FAST/build_Release/external/tensorflow/external/eigen_archive/unsupported/Eigen/CXX11/src/ThreadPool/NonBlockingThreadPool.h:232\r\n#12 0x00007fffedebade0 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::NonBlockingThreadPoolTempl(int, bool, tensorflow::thread::EigenEnvironment)::{lambda()#1}::operator()() const ()\r\n  at /home/smistad/workspace/FAST/build_Release/external/tensorflow/external/eigen_archive/unsupported/Eigen/CXX11/src/ThreadPool/NonBlockingThreadPool.h:65\r\n#13 0x00007fffedebe462 in std::_Function_handler<void (), Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::NonBlockingThreadPoolTempl(int, bool, tensorflow::thread::EigenEnvironment)::{lambda()#1}>::_M_invoke(std::_Any_data const&) (__functor=...) at /usr/include/c++/5/functional:1871\r\n#14 0x00007fffedeba4c8 in std::function<void ()>::operator()() const (this=0x3333350) at /usr/include/c++/5/functional:2267\r\n#15 0x00007fffedeb9aec in tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}::operator()() const (__closure=0x3333350)\r\n  at /home/smistad/workspace/FAST/build_Release/external/tensorflow/src/tensorflow/tensorflow/core/lib/core/threadpool.cc:56\r\n#16 0x00007fffedebbcb1 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) (__functor=...)\r\n  at /usr/include/c++/5/functional:1871\r\n#17 0x00007fffedeba4c8 in std::function<void ()>::operator()() const (this=0x3346398) at /usr/include/c++/5/functional:2267\r\n#18 0x00007fffedef50ca in std::_Bind_simple<std::function<void ()> ()>::_M_invoke<>(std::_Index_tuple<>) (this=0x3346398) at /usr/include/c++/5/functional:1531\r\n#19 0x00007fffedef5020 in std::_Bind_simple<std::function<void ()> ()>::operator()() (this=0x3346398) at /usr/include/c++/5/functional:1520\r\n#20 0x00007fffedef4fb0 in std::thread::_Impl<std::_Bind_simple<std::function<void ()> ()> >::_M_run() (this=0x3346380) at /usr/include/c++/5/thread:115\r\n#21 0x00007ffff6201c80 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#22 0x00007ffff5d1d6ba in start_thread (arg=0x7fff98fd5700) at pthread_create.c:333\r\n#23 0x00007ffff5a5382d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109\r\n```\r\n", "comments": ["I can confirm this issue. I just build tensorflow on Ubuntu 16.04.3 LTS with gcc (Ubuntu 5.4.0-6ubuntu1~16.04.4). I had to build against Eigen master (034fba127699) for a successful build using tensorflow master and 1.3.0 with bazel.  My Python version is 3.5.2.\r\n\r\nThe simple hello world is working fine. I can reproduce the error by using the mnist deep example (mnist_deep.py).\r\n\r\nThis only happens with march=native (My CPU: Intel(R) Core(TM) i7-7800X CPU).\r\n\r\nThe error I could extract using gdb on python was:\r\n`\r\nThread 20 \"python3\" received signal SIGSEGV, Segmentation fault.\r\n[Switching to Thread 0x7fffcf879700 (LWP 68916)]\r\n0x00007fffd47bc355 in Eigen::internal::gemm_pack_lhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 16, true, false, 0, Eigen::MakePointer>, 48, 16, 0, false, false>::operator()(float*, Eigen::internal::TensorContractionSubMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 16, true, false, 0, Eigen::MakePointer> const&, long, long, long, long) () from /projects/tensorflow/venv/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n`\r\n  ", "Same issue with me. I ended up installing pre built version from PIP.", "I can confirm same happens on Windows (experimental MINGW build)\r\nI can load Graph, load Tensor, but when i call \"session->Run()\" i get same sigsegv.\r\nCompiled only with -msse4.2 (OPTIMIZE_FOR_NATIVE_ARCH DISABLED)", "@gunan can comment more, but I believe we suggest building with bazel instead of cmake in all cases now.", "Nagging Assignee @skye: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Correct. Please see the new guide to build from sources on windows:\r\nhttps://www.tensorflow.org/install/source_windows"]}, {"number": 9637, "title": "Convolutional Neural Networks Tutorial problem", "body": "I am learning about Convolutional Neural Networks from the tutorial:\r\nhttps://www.tensorflow.org/tutorials/deep_cnn\r\n \r\nInside it there is a link for getting the code but it is not working:\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow_models/tutorials/image/cifar10/\r\n\r\nhow can I get the code?\r\n\r\n\r\n", "comments": ["Most tensorflow models are now in their own [models repo](https://github.com/tensorflow/models).\r\n\r\nHere's what you're looking for: https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10", "I think you're referring to the link here, right?\r\n\r\n```\r\nThe CIFAR-10 network is largely contained in cifar10.py. \r\n```\r\n\r\nFeel free to send a PR correcting the [md file](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/tutorials/deep_cnn.md#L99)\r\n/CC: @nealwu ", "It looks like the .md file on GitHub is correct, but the TensorFlow site documentation points to the wrong place. Seems like a documentation bug -- talked to @wolffg and he will look into it.", "Another bug in the same document is the last sentence in the \"Pooling Layer #1\" section.  It says \"has a shape of [batch_size, 14, 14, 1]\" but it should read \"has a shape of [batch_size, 14, 14, **32**]\"", "Hi @ddurham2, thanks for reporting this. We fixed this in https://github.com/tensorflow/tensorflow/issues/8301, but unfortunately it looks like it still hasn't been pushed to the documentation site. Once r1.2 becomes the standard version however, the site will be correct (see https://www.tensorflow.org/versions/r1.2/tutorials/layers#pooling_layer_1).", "Closing this issue since it has been fixed in https://www.tensorflow.org/versions/r1.2/tutorials/deep_cnn and will be the default once r1.2 is the standard."]}, {"number": 9636, "title": "`reload(tensorflow)` fails", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.1.0\r\n- **Bazel version (if compiling from source)**:\r\n- **Exact command to reproduce**:\r\n\r\n``` python\r\nfrom importlib import reload\r\nimport tensorflow\r\nreload(tensorflow)\r\n```\r\n\r\nfails with error\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"...\\lib\\importlib\\__init__.py\", line 166, in reload\r\n    _bootstrap._exec(spec, module)\r\n  File \"<frozen importlib._bootstrap>\", line 626, in _exec\r\n  File \"<frozen importlib._bootstrap_external>\", line 673, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\n  File \"...\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in <module>\r\n    del python\r\nNameError: name 'python' is not defined\r\n```", "comments": ["Hi @drasmuss while it's easy to fix that one problem, the library was not designed to do that. It will have issues later with gradient registration and the like.", "Closing as not feasible given importance."]}, {"number": 9635, "title": "Implement of attention mechanisms", "body": "Here is a question(maybe bug) about implements of different attention mechanisms, i.e. LuongAttention & BahdanauAttention, in contrib/seq2seq/python/ops/attention_wrapper.py. It seems that the only difference is their score functions(alignment models).\r\n\r\n In BahdanauAttention, the alignment model is as described in https://arxiv.org/abs/1412.7449\r\n\r\n```\r\n score = math_ops.reduce_sum(v * math_ops.tanh(keys + processed_query),\r\n                                    [2])\r\n```\r\n\r\nIn LuongAttention, the alignment model is a dot function:\r\n`score = math_ops.matmul(query, self.keys, transpose_b=True)`\r\n\r\nThen, the context vector is used all in same way. However, there should be more differences between these two attention mechanisms. Is this a modified version of Bahdanau attention proposed in https://arxiv.org/abs/1409.0473?\r\n\r\nAny response would be appreciated. Thanks!", "comments": ["@ebrevdo can you comment?", "LuongAttention implements the following paper:\r\n\r\nMinh-Thang Luong, Hieu Pham, Christopher D. Manning.\r\n  \"Effective Approaches to Attention-based Neural Machine Translation.\"\r\n  EMNLP 2015.  https://arxiv.org/abs/1508.04025\r\n\r\nboth attentions have a scaling option (Luong it's called \"scale\" and Bahdanau it's called \"normalize\").  The Luong scaling version is unpublished; the Bahdanau normalize option is based on a recent paper by colin raffel.\r\n\r\nEither way; that really is the only difference between the two attention mechanisms.  Everything else is the same.  Is there a specific term that you feel should be different between the two?", "(other differences have been factored out and are controlled elsewhere; for example, the AttentionWrapper object has an argument `output_attention`; the Luong and Bahdanau papers used different versions of this.  but from an API perspective, it's better to keep it configurable elsewhere).", "I guess another main difference is where computation begins. In Bahdanau papers, the path is h(t-1)->a(t)->c(t)->h(t), while in Luong it should be h(t)->a(t)->c(t)->h\u02dc(t).\r\n\r\nIn current implements, both Bahdanau and Luong use current hidden state h(t) as \"query\" to attend on memories. I am not sure whether this matters a lot or not.", "i believe that @Songweiping is correct and that is specified on the Luong's paper if i remember correctly. \r\nfor your information, i'd implemented both and they actually had no significant difference in final performance for NMT tasks(in both log-perplexity and BLEU).", "@yhg0112 That makes sense to me. Using h(t-1) or h(t) to determine the context vector seems not a big issue.\r\nBTW, I'm curious about how these two TF implements of Luong and Bahdanau perform on NMT tasks. Have you tested this? @yhg0112  \r\n", "Hi guys,\r\n\r\nThanks for discussing. I developed the early version of attention_wrapper.py. To me, we replicate most details in Bahdanau and Luong's attention mechanisms, except from the fact that Songweiping mentioned \"In Bahdanau papers, the path is h(t-1)->a(t)->c(t)->h(t), while in Luong it should be h(t)->a(t)->c(t)->h\u02dc(t).\" I believe similar to yhg0112 that there're no significant difference. Besides, I'm gonna release detailed results on which attentions work best and under which settings this month, so stay tune! [ but give scaled LuongAttention a try ;) ] \r\nps: For the scaled version of LuongAttention and the normalized version of BahdanauAttention, see this paper https://arxiv.org/abs/1704.00784. We simplified scaled LuongAttention a bit when implementing though.\r\n", "@lmthang That sounds great! Please let us know once you release those results.", "Hi @ebrevdo @lmthang ,\r\n\r\nI am wondering if it is desirable to support personalized activation function in attention layer. In current version, no activation is applied after concatenating context vector and target hidden value, while there is a _tanh_ in Luong's paper https://arxiv.org/abs/1508.04025.\r\nI have tested this on some tasks, it seems that it does make some difference, but not significant. Therefore, this is just a kind suggestion.\r\n\r\nThanks!", "In this paper\r\nhttps://arxiv.org/abs/1607.01628\r\nFrom chapter 3.1 Decoder Cost,\r\nThey observe that BahdanauAttention works better than concatenation in their experiments.\r\n", "@Songweiping do you mean like adding an `attention_layer_activation` in addition to `attention_layer_size`?", "BTW I'm going to close this issue soon as it seems thang answered the main question.", "@ebrevdo That's what I mean. Thanks!", "Yes, adding attention_layer_activation makes sense to me :)", "I'm sorry for commenting on a closed issue, but @lmthang did you end up benchmarking those different attention mechanisms?\r\n\r\nAdditionally, I'd like to note that scaled Luong attention is now described in https://arxiv.org/abs/1706.03762.", "Hi @rubenvereecken,\r\n\r\nI did try to benchmark but didn't have time to publish the results ... I'd suggest trying it on your own problem to see if there's a difference. In many cases, we only see minor differences. "]}, {"number": 9634, "title": "cuda_configure.bzl makes bad symlink for: cuda/include/cudnn.h --> cuda/include/include/cudnn.h", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS release 6.7 (Final) x86_64\r\n- **TensorFlow installed from (source or binary)**:  source\r\n- **TensorFlow version (use command below)**: master: git version 550df413158b32645ca5df4dcaabc67f1a48964d (checked out May 3rd 2017)\r\n- **Bazel version (if compiling from source)**: bazel-0.4.5\r\n- **CUDA/cuDNN version**: 7.5/5.1.3\r\n- **GPU model and memory**: Quadro K600 1GB DDR3\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nsetenv CC '/mnt/nfs/home/momeara/opt/bin/gcc'\r\nsetenv CXX '/mnt/nfs/home/momeara/opt/bin/g++'\r\nsetenv EXTRA_BAZEL_ARGS '--verbose_failures --jobs=1'\r\nsetenv CPLUS_INCLUDE_PATH '/mnt/nfs/home/momeara/opt/include'\r\nsetenv C_INCLUDE_PATH '/mnt/nfs/home/momeara/opt/include'\r\nsetenv LIBRARY_PATH '/mnt/nfs/home/momeara/opt/lib:/mnt/nfs/home/momeara/opt/lib64'\r\nsetenv LD_LIBRARY_PATH /mnt/nfs/work/momeara/sea/DeepSEA/cuda/lib64:/usr/local/cuda-7.5/lib64:/mnt/nfs/home/momeara/opt/lib:/mnt/nfs/home/momeara/opt/lib64:/usr/local/cuda-7.5/extras/CUPTI/lib64:$LD_LIBR\\\r\nARY_PATH\r\nsetenv PATH /usr/local/cuda-7.5/bin:/mnt/nfs/work/momeara/sea/DeepSEA/tensorflow/bazel-0.4.5/output:$PATH\r\n\r\n./configure\r\nPlease specify the location of python. [Default is /mnt/nfs/work/momeara/tools/anaconda2/envs/sea16/bin/python]:\r\nFound possible Python library paths:\r\n  /mnt/nfs/work/momeara/tools/anaconda2/envs/sea16/lib/python2.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/mnt/nfs/work/momeara/tools/anaconda2/envs/sea16/lib/python2.7/site-packages]\r\n\r\nUsing python library path: /mnt/nfs/work/momeara/tools/anaconda2/envs/sea16/lib/python2.7/site-packages\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\nDo you wish to use jemalloc as the malloc implementation? [Y/n] n\r\njemalloc disabled\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] n\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] n\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] y\r\nXLA JIT support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with VERBS support? [y/N] y\r\nVERBS support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] n\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nDo you want to use clang as CUDA compiler? [y/N] n\r\nnvcc will be used as CUDA compiler\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 7.5\r\nPlease specify the location where CUDA 7.5 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /mnt/nfs/home/momeara/opt/bin/gcc]:\r\nPlease specify the cuDNN version you want to use. [Leave empty to use system default]: 5.1.3\r\nPlease specify the location where cuDNN 5.1.3 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /mnt/nfs/work/momeara/sea/DeepSEA/cuda\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 5.2\r\nWARNING: Output base '/mnt/nfs/home/momeara/.cache/bazel/_bazel_momeara/ef8339021629a8146b3e301bb7dc3099' is on NFS. This may lead to surprising failures and undetermined behavior.\r\n................................................................................\r\n____Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.\r\nConfiguration finished\r\n\r\nbazel --output_user_root=/scratch/momeara/.cache/baze build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures --jobs=1\r\n```\r\n\r\n### Describe the problem\r\nDuring configure, it tries to make a symlink\r\n\r\n    ln -s /mnt/nfs/work/momeara/sea/DeepSEA/cuda/include/cudnn.h /scratch/momeara/.cache/baze/ef8339021629a8146b3e301bb7dc3099/execroot/tensorflow/bazel-out/host/genfiles/external/local_config_cuda/cuda/include/include/cudnn.h\r\n\r\nbut this fails because the directory `/scratch/momeara/.cache/baze/ef8339021629a8146b3e301bb7dc3099/execroot/tensorflow/bazel-out/host/genfiles/external/local_config_cuda/cuda/include/include` does not exist\r\n\r\nnotice that it has `include/include` at the end.\r\n\r\nIf I change this line: https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/cuda_configure.bzl#L877\r\n\r\n    genrules.append(_symlink_genrule_for_dir(repository_ctx, None, \"\",\r\n            \"cudnn-include\", [cudnn_header_dir + \"/cudnn.h\"], [\"include/cudnn.h\"]))\r\n\r\nto \r\n\r\n    genrules.append(_symlink_genrule_for_dir(repository_ctx, None, \"\",\r\n            \"cudnn-include\", [cudnn_header_dir + \"/cudnn.h\"], [\"cudnn.h\"]))\r\n\r\nthe build proceeds without error\r\n\r\n\r\n### Source code / logs\r\n\r\n____[107 / 393] Writing script external/local_config_cuda/cuda/cuda-include.genrule_script.sh [for host]\r\nERROR: /scratch/momeara/.cache/baze/ef8339021629a8146b3e301bb7dc3099/external/local_config_cuda/cuda/BUILD:1309:1: Executing genrule @local_config_cuda//cuda:cudnn-include failed: bash failed: error exec\\\r\nuting command\r\n  (cd /scratch/momeara/.cache/baze/ef8339021629a8146b3e301bb7dc3099/execroot/tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/mnt/nfs/work/momeara/sea/DeepSEA/cuda/lib64:/usr/local/cuda-7.5/lib64:/mnt/nfs/home/momeara/opt/lib:/mnt/nfs/home/momeara/opt/lib64:/usr/local/cuda-7.5/extras/CUPTI/lib64:/mnt/nfs/ho\\\r\nme/momeara/opt/lib:/mnt/nfs/home/momeara/opt/lib64 \\\r\n    PATH=/usr/local/cuda-7.5/bin:/mnt/nfs/work/momeara/sea/DeepSEA/tensorflow/tensorflow/bazel-0.4.5/output:/mnt/nfs/work/momeara/tools/anaconda2/envs/sea16/bin:/mnt/nfs/work/momeara/tools/anaconda2/bin:\\\r\n/mnt/nfs/home/momeara/.local/bin:/mnt/nfs/home/momeara/opt/node-v4.5.0-linux-x64/bin:/mnt/nfs/home/momeara/opt/bin:/mnt/nfs/work/momeara/tools/anaconda2/envs/sea16/bin:/mnt/nfs/work/momeara/tools/anacond\\\r\na2/bin:/mnt/nfs/home/momeara/.local/bin:/mnt/nfs/home/momeara/opt/node-v4.5.0-linux-x64/bin:/mnt/nfs/home/momeara/opt/bin:/mnt/nfs/work/momeara/tools/anaconda2/envs/sea16/bin:/mnt/nfs/work/momeara/tools/\\\r\nanaconda2/bin:/mnt/nfs/home/momeara/.local/bin:/mnt/nfs/home/momeara/opt/node-v4.5.0-linux-x64/bin:/mnt/nfs/home/momeara/opt/bin:/usr/lib64/qt-3.3/bin:/usr/kerberos/sbin:/usr/kerberos/bin:/usr/local/bin:\\\r\n/bin:/usr/bin \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh;\r\nln -s /mnt/nfs/work/momeara/sea/DeepSEA/cuda/include/cudnn.h bazel-out/host/genfiles/external/local_config_cuda/cuda/include/include/cudnn.h    '): com.google.devtools.build.lib.shell.BadExitStatusExcept\\\r\nion: Process exited with status 1.\r\nln: creating symbolic link `bazel-out/host/genfiles/external/local_config_cuda/cuda/include/include/cudnn.h': No such file or directory\r\nblaze: Leaving directory `/scratch/momeara/.cache/baze/ef8339021629a8146b3e301bb7dc3099/execroot/tensorflow/'\r\n____Building complete.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n____Elapsed time: 75.364s, Critical Path: 3.99s\r\n\r\n", "comments": ["@rmlarsen or @jart what do you think?", "I ran into this same problem and had to make an additional change apart from the one mentioned by @momeara to get it to build:\r\n\r\nIn the file   tensorflow/stream_executor/cuda/cuda_dnn.cc\r\nChange:\r\n#include \"cuda/include/cudnn.h\"\r\ninto:\r\n#include \"cuda/cudnn.h\"\r\n\r\n\r\n", "I had also to change these lines in this file https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/cuda_configure.bzl#L858:\r\n\r\n```\r\n858  genrules.append(_symlink_genrule_for_dir(repository_ctx,\r\n859       cuda_toolkit_path + extras/CUPTI/include\",\r\n860       \"extras/CUPTI/include\", \"cuda-extras\"))\r\n```\r\nto these:\r\n\r\n```\r\n858  genrules.append(_symlink_genrule_for_dir(repository_ctx,\r\n859       cuda_toolkit_path,\r\n860       \"extras/CUPTI/include\", \"cuda-extras\"))\r\n\r\n```", "I confirm that the issue is solved for me with the modifications suggested in https://github.com/tensorflow/tensorflow/issues/9634#issue-226035782 and https://github.com/tensorflow/tensorflow/issues/9634#issuecomment-299503115, namely:\r\n\r\n1. In `/tensorflow/blob/master/third_party/gpus/cuda_configure.bzl#L877`\r\n    change:\r\n    ```python\r\n    genrules.append(_symlink_genrule_for_dir(repository_ctx, None, \"\",\r\n            \"cudnn-include\", [cudnn_header_dir + \"/cudnn.h\"], [\"include/cudnn.h\"]))\r\n    ```\r\n    to\r\n    ``` python\r\n    genrules.append(_symlink_genrule_for_dir(repository_ctx, None, \"\",\r\n            \"cudnn-include\", [cudnn_header_dir + \"/cudnn.h\"], [\"cudnn.h\"]))\r\n    ```\r\n2. In `tensorflow/stream_executor/cuda/cuda_dnn.cc`\r\n    change:\r\n    `#include \"cuda/include/cudnn.h\"`\r\n    to:\r\n    `#include \"cuda/cudnn.h\"`", "I'm also experiencing this on my work computer: I have CUDA and libcudnn5 installed from the Ubuntu .deb packages provided by NVidia. It seems like this is a configuration that really ought to work?\r\n\r\nPR #9751 did make things build for me again, but it doesn't seem like the right general fix.", "@rmlarsen  With reference to PR #9751, I  may have a hunch as to why this is happening.\r\n\r\nIf one untars `cudnn-8.0-linux-x64-v5.1.tar.gz` it produces the following directory tree\r\n\r\n```bash\r\ncuda/lib64/libcudnn.so\r\ncuda/lib64/libcudnn.so.5\r\ncuda/lib64/libcudnn.so.5.1.5\r\ncuda/lib64/libcudnn_static.a\r\ncuda/include/cudnn.h\r\n```\r\n\r\nThis is probably designed to be unzipped into `/usr/local/cuda`. Now, if we untar it into some non-standard location, say `/usr/local/cudnn-5.1-cuda-8.0`, users are probably passing the directory to `configure` as `/usr/local/cudnn-5.1-cuda-8.0/cuda`. \r\n\r\nBazel accepts `/usr/local/cudnn-5.1-cuda-8.0/cuda` as it finds the right headers and libs under this path. But I think the tensorflow `#includes` actually want `/usr/local/cudnn-5.1-cuda-8.0/` in this case.\r\n\r\nTo summarise, I suspect the problem is that bazel accepts `/usr/local/cudnn-5.1-cuda-8.0`, but the headers are expecting `/usr/local/cudnn-5.1-cuda-8.0/cuda`. I haven't found time to test this myself, but perhaps others in this thread could confirm my suspicions.\r\n\r\n(In my case I copied everything in `/usr/local/cudnn-5.1-cuda-8.0/cuda` to `/usr/local/cudnn-5.1-cuda-8.0` and passed `/usr/local/cudnn-5.1-cuda-8.0` as the cuDNN path)", "I could not get #9751 to work, but was able to fix this a different way without changing the relative include path in `tensorflow/stream_executor/cuda/cuda_dnn.cc`:\r\n```diff --git a/third_party/gpus/cuda_configure.bzl b/third_party/gpus/cuda_configure.bzl\r\nindex 6994db0..dad0d83 100644\r\n--- a/third_party/gpus/cuda_configure.bzl\r\n+++ b/third_party/gpus/cuda_configure.bzl\r\n@@ -875,8 +875,8 @@ def _create_cuda_repository(repository_ctx):\r\n   included_files = _read_dir(repository_ctx, cuda_include_path).replace(\r\n       cuda_include_path, '').splitlines()\r\n   if '/cudnn.h' not in included_files:\r\n-    genrules.append(_symlink_genrule_for_dir(repository_ctx, None, \"\",\r\n-        \"cudnn-include\", [cudnn_header_dir + \"/cudnn.h\"], [\"include/cudnn.h\"]))\r\n+    genrules.append(_symlink_genrule_for_dir(repository_ctx, cudnn_header_dir,\r\n+        \"include\", \"cudnn-include\"))\r\n   else:\r\n     genrules.append(\r\n             'filegroup(\\n' +\r\n```\r\n\r\nWill continue testing to decide whether to open an alternative PR\r\n\r\nEDIT: Tested and PR created here: #10126", "@jart What's the status of this issue?", "I am facing the similar issue\r\n\r\nERROR: /private/var/tmp/_bazel_azain/45a5c55226309e7282643bd8440e7750/external/local_config_cuda/cuda/BUILD:1302:1: Executing genrule @local_config_cuda//cuda:cudnn-include failed: bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nln: bazel-out/local-py3-opt/genfiles/external/local_config_cuda/cuda/includeinclude/cudnn.h: No such file or directory\r\nTarget //tensorflow/tools/graph_transforms:summarize_graph failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n\r\nin the path it says incluceinclude/cudnn.h. I assume something is going wrong with the path..  Any help please?\r\n", "@azainab, it looks like there is a missing `/` in this path\r\n\r\n    bazel-out/local-py3-opt/genfiles/external/local_config_cuda/cuda/includeinclude/cudnn.h\r\n\r\ncan you figure out where that path is coming from and fix it?\r\n\r\n\r\n", "@momeara I know I looked at that but I am not able to realise from where it is picking this path :(", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Closing due to inactivity. Please comment with new information and I will reopen."]}, {"number": 9633, "title": "SIGSEGV with sparse_add and broadcasting", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes, enclosed below\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary via pip\r\n- **TensorFlow version (use command below)**:\r\n('v1.0.0-65-g4763edf-dirty', '1.0.1')\r\n- **Bazel version (if compiling from source)**:\r\nN/A, using pip installation\r\n- **CUDA/cuDNN version**:\r\nN/A, CPU-only\r\n- **GPU model and memory**:\r\nnone\r\n- **Exact command to reproduce**:\r\n```\r\nfrom __future__ import print_function\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndense_sz = [1, 1000, 1000]\r\ndense = tf.constant(1.0, shape=dense_sz, dtype=tf.float32)\r\n\r\nsparse_sz = [10, 1000, 1000]\r\nnnz = 100\r\nnz_ind = np.random.choice(np.prod(sparse_sz), size=nnz, replace=False)\r\nnz_ind = np.unravel_index(nz_ind, dims=sparse_sz)\r\nnz_ind = np.array(nz_ind).T\r\nassert np.all(nz_ind < np.array(sparse_sz)[None, :])\r\n# Ensure canonical ordering.\r\nind = np.lexsort([nz_ind[:, i].flatten() for i in reversed(range(nz_ind.shape[1]))])\r\nnz_ind = nz_ind[ind, :]\r\nprint('nz_ind\\n', nz_ind)\r\n\r\nsparse_plc = tf.sparse_placeholder(tf.float32)\r\nsparse_sum = tf.sparse_add(dense, sparse_plc)\r\ninit = tf.global_variables_initializer()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    print('after init')\r\n    res = sess.run(sparse_sum, feed_dict={sparse_plc: tf.SparseTensorValue(nz_ind, np.ones((nnz,)), sparse_sz)})\r\n    print('sum\\n', res)\r\n```\r\n\r\n### Describe the problem\r\nRunning the code above results in\r\n```\r\n[...]\r\nafter init\r\n\r\nProcess finished with exit code 139 (interrupted by signal 11: SIGSEGV)\r\n```\r\nFor lower values of nnz, (nnz = 1) it finishes fine quite often.\r\n```\r\n[...]\r\nafter init\r\nsum\r\n [[[ 1.  1.  1. ...,  1.  1.  1.]\r\n  [ 1.  1.  1. ...,  1.  1.  1.]\r\n  [ 1.  1.  1. ...,  1.  1.  1.]\r\n  ..., \r\n  [ 1.  1.  1. ...,  1.  1.  1.]\r\n  [ 1.  1.  1. ...,  1.  1.  1.]\r\n  [ 1.  1.  1. ...,  1.  1.  1.]]]\r\n\r\nProcess finished with exit code 0\r\n```\r\n\r\n### Source code / logs\r\nSee above.", "comments": ["@concretevitamin can you take a look at this?\r\n\r\nHere's the stacktrace:\r\n```\r\nPC: @     0x7f37662d027d  (unknown)  raise\r\n    @         0x243417e6       1120  FailureSignalHandler()\r\n    @     0x7f37662d03d0       1472  (unknown)\r\n    @     0x7f3764db3191        128  faulthandler_fatal_error\r\n    @     0x7f37662d03d0  (unknown)  (unknown)\r\n    @         0x2217d353       1664  tensorflow::SparseTensorDenseAddOp<>::Compute()\r\n    @         0x22f76586         96  tensorflow::ThreadPoolDevice::Compute()\r\n    @         0x22f0764b       2464  tensorflow::(anonymous namespace)::ExecutorState::Process()\r\n    @         0x22f13cb1        160  std::_Mem_fn<>::operator()<>()\r\n    @         0x22f13bc3         96  std::_Bind<>::__call<>()\r\n    @         0x22f13b06         64  std::_Bind<>::operator()<>()\r\n    @         0x22f136fd         32  std::_Function_handler<>::_M_invoke()\r\n    @         0x13172ede         32  std::function<>::operator()()\r\n    @         0x2325fcc8        128  tensorflow::thread::EigenEnvironment::ExecuteTask()\r\n    @         0x2325f019        208  Eigen::ThreadPoolTempl<>::WorkerLoop()\r\n    @         0x2325ec5e         32  Eigen::ThreadPoolTempl<>::ThreadPoolTempl()::{lambda()#1}::operator()()\r\n    @         0x2325eacd         32  std::_Function_handler<>::_M_invoke()\r\n    @         0x13172ede         32  std::function<>::operator()()\r\n    @         0x2325e934         48  tensorflow::thread::EigenEnvironment::CreateThread()::{lambda()#1}::operator()()\r\n    @         0x2325e76d         32  std::_Function_handler<>::_M_invoke()\r\n    @         0x13172ede         32  std::function<>::operator()()\r\n    @         0x23296f0c         32  tensorflow::(anonymous namespace)::GoogleThread::FuncThread::Run()\r\n    @         0x23d41427        448  Thread::ThreadBody()\r\n    @     0x7f37662c6890        176  start_thread\r\n    @     0x7f3765d2237d  (unknown)  clone\r\n```", "Did you modify anything else in the code? The version shows `dirty`.\r\n\r\n@yifeif @av8ramit are the pip installs built from a dirty git repo?\r\n", "I'm taking a look.", "The results look the same with tf pip-upgraded to ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0').", "Thanks for the report @tpet.\r\n\r\nI am submitting a \"fix\" that instead of segfaulting, return a proper error status that requires both operands have matching shapes.  This has always been the assumption in the `SparseTensorDenseAddOp`, but was [not enforced](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/sparse_tensor_dense_add_op.cc#L56).  The patch should show up in master within a day or two.\r\n\r\nDo you exactly require the functionality of \"sparse + dense -> dense, with dense-to-sparse broadcast\"?  If so, I'd like to mark this as contributions welcome (the current kernels do not support this broadcast pattern).\r\n\r\nHowever, if you can get away with \"sparse + dense -> sparse, with dense-to-sparse broadcast\", we already have [`sparse_dense_cwise_add()`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/sparse_ops.py#L316) that does this.  Let us know, and we can expose this function as a public method.", "@concretevitamin I agree raising an exception is much better. The \"sparse + dense -> dense, with dense-to-sparse broadcast\" really seems not that much useful, compared to \"sparse + dense -> sparse, with dense-to-sparse broadcast\". Now I actually don't need this particular thing.\r\n\r\nMy initial use case was a bit different. In the process of trying to get some reasonable behavior I happened to find the segfault and created this example.\r\nMy use case is this:\r\nD + reduce_sum(a * S)\r\nwhere D and the result is dense [1 n2 n3 n4]\r\nS is sparse [n1 n2 n3 n4]\r\na is dense [n1 1 1 1] and broadcasts to S.\r\nSo far I hasn't been able to get to some reasonable performance with this.", "I'm using a pip installation so it will take some time until it propagates down to me so feel free to close the issue if you think it is resolved. Thanks.", "Okay, closing for now.  For the reduce, take a look at `tf.sparse_reduce_sum()` and/or `tf.sparse_reduce_sum_reduce()`."]}, {"number": 9632, "title": "building error, tensorflow r1.0, with bazel 0.4.5, Ubuntu 16.04.1 LTS    armv7 board", "body": "I try to use TensorFlow r1.0 on an Odroid-XU3 board (armv7).\r\nThe distribution is an Ubuntu 16.04.1 LTS (Xenial Xerus)\r\n\r\nBazel is succesfully installed from bazel-0.4.5-dist.zip\r\n\r\nDuring the configuration, I get multiple errors:\r\n\r\n````\r\nodroid@odroid:~/local_DT_project/tensorflow_git$ python --version\r\nPython 2.7.12\r\n\r\nodroid@odroid:~/local_DT_project/tensorflow_git$ git status\r\nOn branch r1.0\r\nYour branch is up-to-date with 'origin/r1.0'.\r\nnothing to commit, working directory clean\r\n\r\nodroid@odroid:~/local_DT_project/tensorflow_git$ ./configure\r\nPlease specify the location of python. [Default is /usr/bin/python]:\r\nPlease specify optimization flags to use during compilation [Default is -march=native]:\r\nDo you wish to use jemalloc as the malloc implementation? (Linux only) [Y/n]\r\njemalloc enabled on Linux\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N]\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N]\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N]\r\nNo XLA support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\nUsing python library path: /usr/local/lib/python2.7/dist-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N]\r\nNo OpenCL support will be enabled for TensorFlow \r\nDo you wish to build TensorFlow with CUDA support? [y/N]\r\nNo CUDA support will be enabled for TensorFlow   \r\nConfiguration finished\r\n.........................................................................................................\r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\n....................................................................................................\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/workspace.bzl:403:3: no such package '@junit_jar//jar': Error downloading [https://gith\r\nub.com/junit-team/junit4/releases/download/r4.12/junit-4.12.jar] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/\r\njunit_jar/junit-4.12.jar: java.lang.IllegalStateException and referenced by '//external:junit'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/contrib/nccl/BUILD:23:1: no such package '@nccl_archive//': Error downloading [https://\r\ngithub.com/nvidia/nccl/archive/024d1e267845f2ed06f3e2e42476d50f04a00ee6.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d\r\n2dddc/external/nccl_archive/024d1e267845f2ed06f3e2e42476d50f04a00ee6.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/contrib/n\r\nccl:python/ops/_nccl_ops.so'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/contrib/nccl/BUILD:23:1: no such package '@nccl_archive//': Error downloading [https://\r\ngithub.com/nvidia/nccl/archive/024d1e267845f2ed06f3e2e42476d50f04a00ee6.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d\r\n2dddc/external/nccl_archive/024d1e267845f2ed06f3e2e42476d50f04a00ee6.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/contrib/n\r\nccl:python/ops/_nccl_ops.so_check_deps'.\r\nERROR: Evaluation of query \"deps((//tensorflow/... - //tensorflow/examples/android/...))\" failed: errors were encountered while computing transitive c\r\nlosure.\r\n````\r\n\r\nAny suggestion to finish the installation?\r\n\r\nCheers,\r\nWilly", "comments": ["Try upgrading to 1.1, and reopen if the issues persists.", " @andydavis1 suggest upgrading to 1.1, but it seems to be worse:\r\n\r\n````\r\nodroid@odroid:~/local_DT_project/tensorflow_git$ git checkout r1.1\r\nBranch r1.1 set up to track remote branch r1.1 from origin.\r\nSwitched to a new branch 'r1.1'\r\nodroid@odroid:~/local_DT_project/tensorflow_git$ ./configure\r\nPlease specify the location of python. [Default is /usr/bin/python]:\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\nDo you wish to use jemalloc as the malloc implementation? [Y/n]\r\njemalloc enabled\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N]\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N]\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N]\r\nNo XLA support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\nUsing python library path: /usr/local/lib/python2.7/dist-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N]\r\nNo OpenCL support will be enabled for TensorFlow \r\nDo you wish to build TensorFlow with CUDA support? [y/N]\r\nNo CUDA support will be enabled for TensorFlow   \r\nConfiguration finished\r\n......................................................................................................\r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\n.....................................................................................................\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_dialog//': Error downloading [https://github.com/polymerelements/paper-dialog/archive/v1.0.4.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_dialog/v1.0.4.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_form_element_behavior//': Error downloading [https://github.com/polymerelements/iron-form-element-behavior/archive/v1.0.6.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_form_element_behavior/v1.0.6.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_collapse//': Error downloading [https://github.com/polymerelements/iron-collapse/archive/v1.0.8.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_collapse/v1.0.8.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_fit_behavior//': Error downloading [https://github.com/polymerelements/iron-fit-behavior/archive/v1.2.5.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_fit_behavior/v1.2.5.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_flex_layout//': Error downloading [https://github.com/polymerelements/iron-flex-layout/archive/v1.3.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_flex_layout/v1.3.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_dropdown//': Error downloading [https://github.com/polymerelements/iron-dropdown/archive/v1.4.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_dropdown/v1.4.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_checked_element_behavior//': Error downloading [https://github.com/polymerelements/iron-checked-element-behavior/archive/v1.0.4.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_checked_element_behavior/v1.0.4.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_behaviors//': Error downloading [https://github.com/polymerelements/iron-behaviors/archive/v1.0.17.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_behaviors/v1.0.17.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_autogrow_textarea//': Error downloading [https://github.com/polymerelements/iron-autogrow-textarea/archive/v1.0.12.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_autogrow_textarea/v1.0.12.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.  \r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_ajax//': Error downloading [https://github.com/polymerelements/iron-ajax/archive/v1.2.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_ajax/v1.2.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@dagre//': Error downloading [https://github.com/cpettitt/dagre/archive/v0.7.4.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/dagre/v0.7.4.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@font_roboto//': Error downloading [https://github.com/polymerelements/font-roboto/archive/v1.0.1.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/font_roboto/v1.0.1.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_a11y_keys_behavior//': Error downloading [https://github.com/polymerelements/iron-a11y-keys-behavior/archive/v1.1.8.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_a11y_keys_behavior/v1.1.8.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'. \r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_a11y_announcer//': Error downloading [https://github.com/polymerelements/iron-a11y-announcer/archive/v1.0.5.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_a11y_announcer/v1.0.5.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@graphlib//': Error downloading [https://github.com/cpettitt/graphlib/archive/v1.0.7.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/graphlib/v1.0.7.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@es6_promise//': Error downloading [https://github.com/components/es6-promise/archive/v2.1.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/es6_promise/v2.1.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@d3//': Error downloading [https://github.com/mbostock-bower/d3-bower/archive/v3.5.15.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/d3/v3.5.15.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@three_js_orbitcontrols_js//file': Error downloading [https://raw.githubusercontent.com/mrdoob/three.js/r77/examples/js/controls/OrbitControls.js] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/three_js_orbitcontrols_js/OrbitControls.js: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@web_animations_js//': Error downloading [https://github.com/web-animations/web-animations-js/archive/2.2.1.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/web_animations_js/2.2.1.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@weblas_weblas_js//file': Error downloading [https://raw.githubusercontent.com/waylonflinn/weblas/v0.9.0/dist/weblas.js] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/weblas_weblas_js/weblas.js: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@webcomponentsjs//': Error downloading [https://github.com/webcomponents/webcomponentsjs/archive/v0.7.22.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/webcomponentsjs/v0.7.22.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@three_js_three_min_js//file': Error downloading [https://raw.githubusercontent.com/mrdoob/three.js/r77/build/three.min.js] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/three_js_three_min_js/three.min.js: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@promise_polyfill//': Error downloading [https://github.com/polymerlabs/promise-polyfill/archive/v1.0.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/promise_polyfill/v1.0.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@polymer//': Error downloading [https://github.com/polymer/polymer/archive/v1.7.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/polymer/v1.7.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@plottable//': Error downloading [https://github.com/palantir/plottable/archive/v1.16.1.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/plottable/v1.16.1.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_tooltip//': Error downloading [https://github.com/polymerelements/paper-tooltip/archive/v1.1.2.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_tooltip/v1.1.2.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_toolbar//': Error downloading [https://github.com/polymerelements/paper-toolbar/archive/v1.1.4.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_toolbar/v1.1.4.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_toggle_button//': Error downloading [https://github.com/polymerelements/paper-toggle-button/archive/v1.2.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_toggle_button/v1.2.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_toast//': Error downloading [https://github.com/polymerelements/paper-toast/archive/v1.3.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_toast/v1.3.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_tabs//': Error downloading [https://github.com/polymerelements/paper-tabs/archive/v1.7.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_tabs/v1.7.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_styles//': Error downloading [https://github.com/polymerelements/paper-styles/archive/v1.1.4.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_styles/v1.1.4.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_toggle_button//': Error downloading [https://github.com/polymerelements/paper-toggle-button/archive/v1.2.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_toggle_button/v1.2.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_toast//': Error downloading [https://github.com/polymerelements/paper-toast/archive/v1.3.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_toast/v1.3.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_tabs//': Error downloading [https://github.com/polymerelements/paper-tabs/archive/v1.7.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_tabs/v1.7.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_styles//': Error downloading [https://github.com/polymerelements/paper-styles/archive/v1.1.4.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_styles/v1.1.4.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_spinner//': Error downloading [https://github.com/polymerelements/paper-spinner/archive/v1.1.1.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_spinner/v1.1.1.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_slider//': Error downloading [https://github.com/polymerelements/paper-slider/archive/v1.0.10.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_slider/v1.0.10.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_ripple//': Error downloading [https://github.com/polymerelements/paper-ripple/archive/v1.0.5.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_ripple/v1.0.5.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_radio_group//': Error downloading [https://github.com/polymerelements/paper-radio-group/archive/v1.0.9.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_radio_group/v1.0.9.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_radio_button//': Error downloading [https://github.com/polymerelements/paper-radio-button/archive/v1.1.2.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_radio_button/v1.1.2.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_progress//': Error downloading [https://github.com/polymerelements/paper-progress/archive/v1.0.9.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_progress/v1.0.9.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_menu_button//': Error downloading [https://github.com/polymerelements/paper-menu-button/archive/v1.5.1.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_menu_button/v1.5.1.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_menu//': Error downloading [https://github.com/polymerelements/paper-menu/archive/v1.2.2.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_menu/v1.2.2.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_material//': Error downloading [https://github.com/polymerelements/paper-material/archive/v1.0.6.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_material/v1.0.6.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_listbox//': Error downloading [https://github.com/polymerelements/paper-listbox/archive/v1.1.2.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_listbox/v1.1.2.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_item//': Error downloading [https://github.com/polymerelements/paper-item/archive/v1.1.4.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_item/v1.1.4.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_input//': Error downloading [https://github.com/polymerelements/paper-input/archive/v1.1.18.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_input/v1.1.18.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_icon_button//': Error downloading [https://github.com/polymerelements/paper-icon-button/archive/v1.1.3.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_icon_button/v1.1.3.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_header_panel//': Error downloading [https://github.com/polymerelements/paper-header-panel/archive/v1.1.4.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_header_panel/v1.1.4.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_dropdown_menu//': Error downloading [https://github.com/polymerelements/paper-dropdown-menu/archive/v1.4.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_dropdown_menu/v1.4.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_dialog_behavior//': Error downloading [https://github.com/polymerelements/paper-dialog-behavior/archive/v1.2.5.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_dialog_behavior/v1.2.5.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_dialog//': Error downloading [https://github.com/polymerelements/paper-dialog/archive/v1.0.4.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_dialog/v1.0.4.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_checkbox//': Error downloading [https://github.com/polymerelements/paper-checkbox/archive/v1.4.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_checkbox/v1.4.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_button//': Error downloading [https://github.com/polymerelements/paper-button/archive/v1.0.11.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_button/v1.0.11.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_behaviors//': Error downloading [https://github.com/polymerelements/paper-behaviors/archive/v1.0.12.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/paper_behaviors/v1.0.12.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@numericjs_numeric_min_js//file': Error downloading [https://cdnjs.cloudflare.com/ajax/libs/numeric/1.2.6/numeric.min.js] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/numericjs_numeric_min_js/numeric.min.js: sun.security.validator.ValidatorException: PKIX path validation failed: java.security.cert.CertPathValidatorException: signature check failed and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@neon_animation//': Error downloading [https://github.com/polymerelements/neon-animation/archive/v1.2.2.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/neon_animation/v1.2.2.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@lodash//': Error downloading [https://github.com/lodash/lodash/archive/3.8.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/lodash/3.8.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_validatable_behavior//': Error downloading [https://github.com/polymerelements/iron-validatable-behavior/archive/v1.1.1.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_validatable_behavior/v1.1.1.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_selector//': Error downloading [https://github.com/polymerelements/iron-selector/archive/v1.5.2.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_selector/v1.5.2.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_scroll_target_behavior//': Error downloading [https://github.com/polymerelements/iron-scroll-target-behavior/archive/v1.0.3.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_scroll_target_behavior/v1.0.3.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_resizable_behavior//': Error downloading [https://github.com/polymerelements/iron-resizable-behavior/archive/v1.0.3.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_resizable_behavior/v1.0.3.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'. \r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_range_behavior//': Error downloading [https://github.com/polymerelements/iron-range-behavior/archive/v1.0.4.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_range_behavior/v1.0.4.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_overlay_behavior//': Error downloading [https://github.com/polymerelements/iron-overlay-behavior/archive/v1.10.1.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_overlay_behavior/v1.10.1.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_meta//': Error downloading [https://github.com/polymerelements/iron-meta/archive/v1.1.1.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_meta/v1.1.1.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_menu_behavior//': Error downloading [https://github.com/polymerelements/iron-menu-behavior/archive/v1.1.10.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_menu_behavior/v1.1.10.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_list//': Error downloading [https://github.com/polymerelements/iron-list/archive/v1.3.9.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_list/v1.3.9.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_input//': Error downloading [https://github.com/polymerelements/iron-input/archive/1.0.10.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_input/1.0.10.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_iconset_svg//': Error downloading [https://github.com/polymerelements/iron-iconset-svg/archive/v1.1.0.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_iconset_svg/v1.1.0.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_icons//': Error downloading [https://github.com/polymerelements/iron-icons/archive/v1.1.3.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_icons/v1.1.3.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/odroid/local_DT_project/tensorflow_git/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_icon//': Error downloading [https://github.com/polymerelements/iron-icon/archive/v1.0.11.tar.gz] to /home/odroid/.cache/bazel/_bazel_odroid/d841288b9ad4d0fda5b62853b4d2dddc/external/iron_icon/v1.0.11.tar.gz: java.lang.IllegalStateException and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: Evaluation of query \"deps(((//tensorflow/... - //tensorflow/contrib/nccl/...) - //tensorflow/examples/android/...))\" failed: errors were encountered while computing transitive closure.\r\n````\r\n\r\nAny help will be appreciated."]}, {"number": 9631, "title": "Make link clickable", "body": "A quick fix on [issue 7750](https://github.com/tensorflow/tensorflow/issues/7750#issuecomment-298915389).", "comments": ["Can one of the admins verify this patch?"]}, {"number": 9630, "title": "separable_conv2d() got an unexpected keyword argument 'rate'", "body": "When running : \r\n```\r\nmodel = Xception(include_top=True, weights='imagenet')\r\n```\r\nI'm getting this error : \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/username/PycharmProjects/segmentation/data/xception.py\", line 273, in <module>\r\n    model = Xception(include_top=True, weights='imagenet')\r\n  File \"/home/username/PycharmProjects/segmentation/data/xception.py\", line 155, in Xception\r\n    x = SeparableConv2D(128, (3, 3), padding='same', use_bias=False, name='block2_sepconv1')(x)\r\n  File \"/home/username/.virtualenvs/cv/lib/python3.5/site-packages/keras/engine/topology.py\", line 578, in __call__\r\n    output = self.call(inputs, **kwargs)\r\n  File \"/home/username/.virtualenvs/cv/lib/python3.5/site-packages/keras/layers/convolutional.py\", line 986, in call\r\n    padding=self.padding)\r\n  File \"/home/username/.virtualenvs/cv/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 2967, in separable_conv2d\r\n    rate=dilation_rate)\r\nTypeError: separable_conv2d() got an unexpected keyword argument 'rate'\r\n```", "comments": ["It looks like you are using an outdated version of TensorFlow where `tf.nn.separable_conv2d` did not yet support the `rate` argument. Update to the latest version."]}, {"number": 9629, "title": "Add 8 comments for event_multiplexer tests.", "body": "I added some comments on event_multiplexer_test.py.\r\nIf I had any mistake, please tell me, I'll fix it :)", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 9628, "title": " tf.cond doesn't work as expected", "body": "### System information\r\n- **Windows 10**:\r\n- **TensorFlow installed from pip install**:\r\n- **TensorFlow version 1.0**:\r\n\r\n### Describe the problem\r\nWhen I use tf.cond, both f1 and f2 are executed.\r\n\r\n### Source code / logs\r\n\r\n    sess = tf.Session()\r\n    x = tf.constant(7)\r\n    y = tf.constant(5)\r\n    lst = []\r\n    def f1():\r\n        global lst\r\n        lst.append(x)\r\n        return x\r\n    def f2(): \r\n        global lst\r\n        lst.append(y)\r\n        return y\r\n    r = tf.cond(tf.less(x, y), f1, f2)\r\n    print(sess.run(lst))  #return [7,5], which means lst.append(x) and lst.append(y) are both executed\r\n", "comments": ["print(lst) \r\n<< [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'Const_1:0' shape=() dtype=int32>]\r\nWhen you run lst ,you are not actually using tf.cond. Instead, you will fetch both x and y directly.\r\n", "@mocique Thanks for your reply, but shouldn't lst be 1) [<tf.Tensor 'Const:0' shape=() dtype=int32>] or 2)[<tf.Tensor 'Const_1:0' shape=() dtype=int32>]?", "Yes, both are executed. f1 and f2 are creating a graph, so they are executed. If you want to construct a list, use `tf.concat`.\r\n\r\nRemember that tensorflow is building a symbolic graph and then executing it."]}, {"number": 9627, "title": "Out of memory when running small network with mnist?", "body": "I work in Ubuntu14.04 with 1080Ti(12GB memory).\r\nWhen I run a small network using mnist data set, at the beginning, everything is OK. But after nearly 3000\r\n iterations with batchsize 128, tensorflow returns out of memory error. It looks something is accumulated into memory, but I don't know what it is. Here is the error log:\r\n```\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 2492 Chunks of size 802816 totalling 1.86GiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 2492 Chunks of size 3211264 totalling 7.45GiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 3212544 totalling 3.06MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 2 Chunks of size 3686400 totalling 7.03MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 3937280 totalling 3.75MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 6422528 totalling 6.12MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 12845056 totalling 12.25MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 21105152 totalling 20.13MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use chunks: 9.86GiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats: \r\nLimit:                 10584624333\r\nInUse:                 10582817024\r\nMaxInUse:              10582817280\r\nNumAllocs:                 1438980\r\nMaxAllocSize:            115605504\r\n\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:274] ****************************************************************************************************\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:275] Ran out of memory trying to allocate 3.06MiB.  See logs for memory state.\r\nW tensorflow/core/framework/op_kernel.cc:993] Resource exhausted: OOM when allocating tensor with shape[128,32,14,14]\r\n```\r\nAnd here is part of my code:\r\n```\r\ndef iterate_minibatches(inputs, batchsize, shuffle=False):\r\n    if shuffle:\r\n        indices = np.arange(len(inputs))\r\n        np.random.shuffle(indices)\r\n    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\r\n        if shuffle:\r\n            excerpt = indices[start_idx:start_idx + batchsize]\r\n        else:\r\n            excerpt = slice(start_idx, start_idx + batchsize)\r\n        yield inputs[excerpt]\r\n\r\nX = np.load('mnist.npz')['x_train']\r\nX = np.reshape(X,(-1,28,28,1))\r\nepoch = 200\r\nnum = 0 \r\nfor i in range(epoch):\r\n    for batch in iterate_minibatches(X, batchsize, shuffle=True):\r\n        Rng = np.random.rand(batchsize,100).astype('float32')\r\n        if  num%6!=0:\r\n            _, p_d, err_d = sess.run([train_d,p_real,d_loss/batchsize],{real:batch,rng:Rng})\r\n        else:\r\n            _, p_g, err_g = sess.run([train_g,p_fake,g_loss/batchsize],{rng:Rng})\r\n            del _\r\n        if (num+1)%200==0:\r\n            _img = sess.run([fake],{rng:Rng})[0]\r\n            img = np.reshape(_img,(-1,28,28))\r\n            img = np.array(255*img,dtype='uint8')\r\n            save_images(img,str(num+1)+'.png')\r\n```\r\nWhat is the problem? ( No other programs occupy GPU memory\uff09\r\n\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since this is likely a problem with your code and not a TensorFlow bug. Please reopen this issue if further investigation makes you think it is a bug after all. Thanks!"]}, {"number": 9626, "title": "Fix typo in LINE#500", "body": "cell_input_fn -> probability_fn", "comments": ["Can one of the admins verify this patch?"]}, {"number": 9625, "title": "embedding failed on gpu", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_word2vec_basic.py\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04.5\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.1.0\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8.0/5\r\n- **GPU model and memory**: titan x (pascal) * 4\r\n- **Exact command to reproduce**: python tutorial_word2vec_basic.py\r\n\r\n### Describe the problem\r\nembedding failed on gpu\r\n1. The code works well on CPU. \r\n2. The code works well with GPU disabled: \r\nsess = tf.InteractiveSession(config=tf.ConfigProto(device_count={'GPU':0}))\r\n3. The code failed with GPU:\r\nsess = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True))\r\n\r\n### Source code / logs\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\r\nname: TITAN X (Pascal)\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.531\r\npciBusID 0000:02:00.0\r\nTotal memory: 11.90GiB\r\nFree memory: 11.76GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x3a47a50\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties:\r\nname: TITAN X (Pascal)\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.531\r\npciBusID 0000:03:00.0\r\nTotal memory: 11.90GiB\r\nFree memory: 11.76GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x3a4b3d0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 2 with properties:\r\nname: TITAN X (Pascal)\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.531\r\npciBusID 0000:82:00.0\r\nTotal memory: 11.90GiB\r\nFree memory: 11.76GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x3a4ed50\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 3 with properties:\r\nname: TITAN X (Pascal)\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.531\r\npciBusID 0000:83:00.0\r\nTotal memory: 11.90GiB\r\nFree memory: 11.76GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 2\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 3\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 2\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 3\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 2 and 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 2 and 1\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 3 and 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 3 and 1\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 2 3\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0: Y Y N N\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1: Y Y N N\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 2: N N Y Y\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 3: N N Y Y\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: TITAN X (Pascal), pci bus id: 0000:03:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:2) -> (device: 2, name: TITAN X (Pascal), pci bus id: 0000:82:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:3) -> (device: 3, name: TITAN X (Pascal), pci bus id: 0000:83:00.0)\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0\r\n/job:localhost/replica:0/task:0/gpu:1 -> device: 1, name: TITAN X (Pascal), pci bus id: 0000:03:00.0\r\n/job:localhost/replica:0/task:0/gpu:2 -> device: 2, name: TITAN X (Pascal), pci bus id: 0000:82:00.0\r\n/job:localhost/replica:0/task:0/gpu:3 -> device: 3, name: TITAN X (Pascal), pci bus id: 0000:83:00.0\r\nI tensorflow/core/common_runtime/direct_session.cc:257] Device mapping:\r\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0\r\n/job:localhost/replica:0/task:0/gpu:1 -> device: 1, name: TITAN X (Pascal), pci bus id: 0000:03:00.0\r\n/job:localhost/replica:0/task:0/gpu:2 -> device: 2, name: TITAN X (Pascal), pci bus id: 0000:82:00.0\r\n/job:localhost/replica:0/task:0/gpu:3 -> device: 3, name: TITAN X (Pascal), pci bus id: 0000:83:00.0\r\n\r\nLoad or Download matt_mahoney_text8 Dataset> data/mm_test8/\r\n('Data size', 17005207)\r\n132853 Steps a Epoch, total Epochs 20\r\nlearning_rate: 1.000000\r\nbatch_size: 128\r\n()\r\nReal vocabulary size 253854\r\nLimited vocabulary size 50000\r\n('Most 5 common words (+UNK)', [['_UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)])\r\n('Sample data', [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156], ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against'])\r\n()\r\n(12, 'as', '->', 195, 'term')\r\n(12, 'as', '->', 5239, 'anarchism')\r\n(12, 'as', '->', 6, 'a')\r\n(12, 'as', '->', 3084, 'originated')\r\n(6, 'a', '->', 2, 'of')\r\n(6, 'a', '->', 3084, 'originated')\r\n(6, 'a', '->', 195, 'term')\r\n(6, 'a', '->', 12, 'as')\r\n(195, 'term', '->', 12, 'as')\r\n(195, 'term', '->', 2, 'of')\r\n(195, 'term', '->', 6, 'a')\r\n(195, 'term', '->', 3137, 'abuse')\r\n(2, 'of', '->', 3137, 'abuse')\r\n(2, 'of', '->', 195, 'term')\r\n(2, 'of', '->', 46, 'first')\r\n(2, 'of', '->', 6, 'a')\r\n(3137, 'abuse', '->', 195, 'term')\r\n(3137, 'abuse', '->', 2, 'of')\r\n(3137, 'abuse', '->', 59, 'used')\r\n(3137, 'abuse', '->', 46, 'first')\r\n()\r\n[TL] Word2vecEmbeddingInputlayer word2vec_layer: (50000, 128)\r\n()\r\nword2vec_layer/nce_biases/Adagrad: (VariableV2): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_biases/Adagrad: (VariableV2)/job:localhost/replica:0/task:0/gpu:0\r\nword2vec_layer/nce_biases/Adagrad/Assign: (Assign): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_biases/Adagrad/Assign: (Assign)/job:localhost/replica:0/task:0/gpu:0\r\nword2vec_layer/nce_weights/Adagrad: (VariableV2): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_weights/Adagrad: (VariableV2)/job:localhost/replica:0/task:0/gpu:0\r\nword2vec_layer/nce_weights/Adagrad/Assign: (Assign): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_weights/Adagrad/Assign: (Assign)/job:localhost/replica:0/task:0/gpu:0\r\nword2vec_layer/embeddings/Adagrad: (VariableV2): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/embeddings/Adagrad: (VariableV2)/job:localhost/replica:0/task:0/gpu:0\r\nword2vec_layer/embeddings/Adagrad/Assign: (Assign): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/embeddings/Adagrad/Assign: (Assign)/job:localhost/replica:0/task:0/gpu:0\r\nword2vec_layer/nce_biases: (VariableV2): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_biases: (VariableV2)/job:localhost/replica:0/task:0/gpu:0\r\nword2vec_layer/nce_biases/Assign: (Assign): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_biases/Assign: (Assign)/job:localhost/replica:0/task:0/gpu:0\r\nword2vec_layer/nce_weights: (VariableV2): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_weights: (VariableV2)/job:localhost/replica:0/task:0/gpu:0\r\nword2vec_layer/nce_weights/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_weights/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal)/job:localhost/replica:0/task:0/gpu:0\r\nword2vec_layer/nce_weights/Initializer/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_weights/Initializer/truncated_normal/mul: (Mul)/job:localhost/replica:0/task:0/gpu:0\r\nword2vec_layer/nce_weights/Initializer/truncated_normal: (Add): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_weights/Initializer/truncated_normal: (Add)/job:localhost/replica:0/task:0/gpu:0\r\nword2vec_layer/nce_weights/Assign: (Assign): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_weights/Assign: (Assign)/job:localhost/replica:0/task:0/gpu:0\r\nword2vec_layer/embeddings: (VariableV2): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/embeddings: (VariableV2)/job:localhost/replica:0/task:0/gpu:0\r\nword2vec_layer/embeddings/Initializer/random_uniform/sub: (Sub): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/embeddings/Initializer/random_uniform/sub: (Sub)/job:localhost/replica:0/task:0/gpu:0\r\nword2vec_layer/embeddings/Initializer/random_uniform/RandomUniform: (RandomUniform): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/embeddings/Initializer/random_uniform/RandomUniform: (RandomUniform)/job:localhost/replica:0/task:0/gpu:0\r\nword2vec_layer/embeddings/Initializer/random_uniform/mul: (Mul): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/embeddings/Initializer/random_uniform/mul: (Mul)/job:localhost/replica:0/task:0/gpu:0\r\nword2vec_layer/embeddings/Initializer/random_uniform: (Add): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/embeddings/Initializer/random_uniform: (Add)/job:localhost/replica:0/task:0/gpu:0\r\nword2vec_layer/embeddings/Assign: (Assign): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/embeddings/Assign: (Assign)/job:localhost/replica:0/task:0/gpu:0\r\ninit: (NoOp): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] init: (NoOp)/job:localhost/replica:0/task:0/gpu:0\r\nConst_4: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Const_4: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nConst_3: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Const_3: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nConst_2: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Const_2: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nword2vec_layer/nce_biases/Initializer/Const: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_biases/Initializer/Const: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nword2vec_layer/nce_weights/Initializer/truncated_normal/stddev: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_weights/Initializer/truncated_normal/stddev: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nword2vec_layer/nce_weights/Initializer/truncated_normal/mean: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_weights/Initializer/truncated_normal/mean: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nword2vec_layer/nce_weights/Initializer/truncated_normal/shape: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_weights/Initializer/truncated_normal/shape: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nword2vec_layer/embeddings/Initializer/random_uniform/max: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/embeddings/Initializer/random_uniform/max: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nword2vec_layer/embeddings/Initializer/random_uniform/min: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/embeddings/Initializer/random_uniform/min: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nword2vec_layer/embeddings/Initializer/random_uniform/shape: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/embeddings/Initializer/random_uniform/shape: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nword2vec_layer/embeddings: (VariableV2): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/embeddings: (VariableV2)/job:localhost/replica:0/task:0/gpu:0\r\n_send_word2vec_layer/embeddings_0: (_Send): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] _send_word2vec_layer/embeddings_0: (_Send)/job:localhost/replica:0/task:0/cpu:0\r\nparam 0: (50000, 128) (mean: -0.000195725995582, median: -0.000534415245056, std: 0.577156186104 ) word2vec_layer/embeddings:0\r\nword2vec_layer/nce_weights: (VariableV2): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_weights: (VariableV2)/job:localhost/replica:0/task:0/gpu:0\r\n_send_word2vec_layer/nce_weights_0: (_Send): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] _send_word2vec_layer/nce_weights_0: (_Send)/job:localhost/replica:0/task:0/cpu:0\r\nparam 1: (50000, 128) (mean: -9.69215670921e-06, median: 2.75921775028e-05 , std: 0.0777502208948 ) word2vec_layer/nce_weights:0\r\nword2vec_layer/nce_biases: (VariableV2): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_biases: (VariableV2)/job:localhost/replica:0/task:0/gpu:0\r\n_send_word2vec_layer/nce_biases_0: (_Send): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] _send_word2vec_layer/nce_biases_0: (_Send)/job:localhost/replica:0/task:0/cpu:0\r\nparam 2: (50000,) (mean: 0.0 , median: 0.0 , std: 0.0 ) word2vec_layer/nce_biases:0\r\nnum of params: 12850000\r\nlayer 0: Tensor(\"word2vec_layer/embedding_lookup:0\", shape=(128, 128), dtype=float32)\r\n50000 vocab saved to vocab_text8.txt in /home/omnisky/app/sourceCode/tensorlayer/example\r\nword2vec_layer/nce_biases/Adagrad: (VariableV2): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_biases/Adagrad: (VariableV2)/job:localhost/replica:0/task:0/gpu:0\r\nword2vec_layer/nce_weights/Adagrad: (VariableV2): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_weights/Adagrad: (VariableV2)/job:localhost/replica:0/task:0/gpu:0\r\nword2vec_layer/embeddings/Adagrad: (VariableV2): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/embeddings/Adagrad: (VariableV2)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/embedding_lookup_grad/ExpandDims: (ExpandDims): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_grad/ExpandDims: (ExpandDims)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/embedding_lookup_grad/strided_slice: (StridedSlice): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_grad/strided_slice: (StridedSlice)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/embedding_lookup_grad/concat: (ConcatV2): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_grad/concat: (ConcatV2)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_grad/stack: (Pack): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_grad/stack: (Pack)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/word2vec_layer/embedding_lookup_grad/ExpandDims: (ExpandDims): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/word2vec_layer/embedding_lookup_grad/ExpandDims: (ExpandDims)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/word2vec_layer/embedding_lookup_grad/strided_slice: (StridedSlice): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/word2vec_layer/embedding_lookup_grad/strided_slice: (StridedSlice)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/word2vec_layer/embedding_lookup_grad/concat: (ConcatV2): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/word2vec_layer/embedding_lookup_grad/concat: (ConcatV2)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/embedding_lookup_1_grad/ExpandDims: (ExpandDims): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_1_grad/ExpandDims: (ExpandDims)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/embedding_lookup_1_grad/strided_slice: (StridedSlice): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_1_grad/strided_slice: (StridedSlice)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/embedding_lookup_1_grad/concat: (ConcatV2): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_1_grad/concat: (ConcatV2)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_2_grad/stack: (Pack): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_2_grad/stack: (Pack)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_1_grad/sub: (Sub): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_1_grad/sub: (Sub)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_1_grad/stack: (Pack): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_1_grad/stack: (Pack)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_3_grad/sub: (Sub): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_3_grad/sub: (Sub)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_3_grad/stack: (Pack): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_3_grad/stack: (Pack)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/sub_1_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_1_grad/BroadcastGradientArgs: (BroadcastGradientArgs)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/sub_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_grad/BroadcastGradientArgs: (BroadcastGradientArgs)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/mul_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/mul_grad/BroadcastGradientArgs: (BroadcastGradientArgs)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/sub_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/sub_grad/BroadcastGradientArgs: (BroadcastGradientArgs)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses_grad/BroadcastGradientArgs: (BroadcastGradientArgs)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/Mean_grad/Prod_1: (Prod): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Mean_grad/Prod_1: (Prod)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/Mean_grad/Maximum: (Maximum): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Mean_grad/Maximum: (Maximum)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/Mean_grad/Prod: (Prod): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Mean_grad/Prod: (Prod)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/Mean_grad/floordiv: (FloorDiv): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Mean_grad/floordiv: (FloorDiv)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/Mean_grad/Cast: (Cast): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Mean_grad/Cast: (Cast)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/Fill: (Fill): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Fill: (Fill)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/Mean_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Mean_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/Mean_grad/Tile: (Tile): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Mean_grad/Tile: (Tile)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/Mean_grad/truediv: (RealDiv): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Mean_grad/truediv: (RealDiv)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/Reshape_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Reshape_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\nstrided_slice: (StridedSlice): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] strided_slice: (StridedSlice)/job:localhost/replica:0/task:0/gpu:0\r\nstack: (Pack): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] stack: (Pack)/job:localhost/replica:0/task:0/gpu:0\r\nones: (Fill): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] ones: (Fill)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/MatMul_grad/MatMul: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/MatMul_grad/MatMul: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/ones_like: (Fill): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/ones_like: (Fill)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/truediv: (RealDiv): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/truediv: (RealDiv)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/concat_3_grad/mod: (FloorMod): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/concat_3_grad/mod: (FloorMod)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/concat_3_grad/ConcatOffset: (ConcatOffset): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/concat_3_grad/ConcatOffset: (ConcatOffset)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_3_grad/sub_1: (Sub): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_3_grad/sub_1: (Sub)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_3_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_3_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_3_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_3_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_3_grad/concat: (ConcatV2): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_3_grad/concat: (ConcatV2)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/strided_slice_3: (StridedSlice): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/strided_slice_3: (StridedSlice)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/stack_2: (Pack): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/stack_2: (Pack)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_2_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_2_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_1_grad/sub_1: (Sub): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_1_grad/sub_1: (Sub)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_1_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_1_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_1_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_1_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_1_grad/concat: (ConcatV2): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_1_grad/concat: (ConcatV2)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/strided_slice: (StridedSlice): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/strided_slice: (StridedSlice)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/stack: (Pack): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/stack: (Pack)/job:localhost/replica:0/task:0/gpu:0\r\nword2vec_layer/nce_biases: (VariableV2): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_biases: (VariableV2)/job:localhost/replica:0/task:0/gpu:0\r\nword2vec_layer/nce_biases/read: (Identity): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_biases/read: (Identity)/job:localhost/replica:0/task:0/cpu:0\r\nword2vec_layer/nce_weights: (VariableV2): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_weights: (VariableV2)/job:localhost/replica:0/task:0/gpu:0\r\nword2vec_layer/nce_weights/read: (Identity): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_weights/read: (Identity)/job:localhost/replica:0/task:0/cpu:0\r\nword2vec_layer/embeddings: (VariableV2): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/embeddings: (VariableV2)/job:localhost/replica:0/task:0/gpu:0\r\nword2vec_layer/embeddings/read: (Identity): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/embeddings/read: (Identity)/job:localhost/replica:0/task:0/cpu:0\r\nnce_loss/Cast: (Cast): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Cast: (Cast)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/LogUniformCandidateSampler: (LogUniformCandidateSampler): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/LogUniformCandidateSampler: (LogUniformCandidateSampler)/job:localhost/replica:0/task:0/cpu:0\r\nnce_loss/Log_1: (Log): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Log_1: (Log)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/Log: (Log): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Log: (Log)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/concat: (ConcatV2): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/concat: (ConcatV2)/job:localhost/replica:0/task:0/cpu:0\r\ngradients/nce_loss/embedding_lookup_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\nAdagrad/update_word2vec_layer/nce_weights/Unique: (Unique): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/nce_weights/Unique: (Unique)/job:localhost/replica:0/task:0/cpu:0\r\nAdagrad/update_word2vec_layer/nce_weights/Shape: (Shape): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/nce_weights/Shape: (Shape)/job:localhost/replica:0/task:0/cpu:0\r\nAdagrad/update_word2vec_layer/nce_weights/strided_slice: (StridedSlice): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/nce_weights/strided_slice: (StridedSlice)/job:localhost/replica:0/task:0/cpu:0\r\ngradients/nce_loss/embedding_lookup_1_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_1_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\nAdagrad/update_word2vec_layer/nce_biases/Unique: (Unique): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/nce_biases/Unique: (Unique)/job:localhost/replica:0/task:0/cpu:0\r\nAdagrad/update_word2vec_layer/nce_biases/Shape: (Shape): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/nce_biases/Shape: (Shape)/job:localhost/replica:0/task:0/cpu:0\r\nAdagrad/update_word2vec_layer/nce_biases/strided_slice: (StridedSlice): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/nce_biases/strided_slice: (StridedSlice)/job:localhost/replica:0/task:0/cpu:0\r\nnce_loss/embedding_lookup_1: (Gather): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/embedding_lookup_1: (Gather)/job:localhost/replica:0/task:0/cpu:0\r\nnce_loss/Slice_3: (Slice): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Slice_3: (Slice)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/Slice_1: (Slice): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Slice_1: (Slice)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/Reshape_5: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Reshape_5: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/embedding_lookup: (Gather): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/embedding_lookup: (Gather)/job:localhost/replica:0/task:0/cpu:0\r\nnce_loss/Slice_2: (Slice): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Slice_2: (Slice)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_2_grad/Shape: (Shape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_2_grad/Shape: (Shape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_2_grad/sub: (Sub): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_2_grad/sub: (Sub)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_2_grad/sub_1: (Sub): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_2_grad/sub_1: (Sub)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_2_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_2_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_2_grad/concat: (ConcatV2): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_2_grad/concat: (ConcatV2)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/Slice: (Slice): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Slice: (Slice)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_grad/Shape: (Shape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_grad/Shape: (Shape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_grad/sub: (Sub): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_grad/sub: (Sub)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_grad/sub_1: (Sub): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_grad/sub_1: (Sub)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_grad/concat: (ConcatV2): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_grad/concat: (ConcatV2)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Reshape_1_grad/Shape: (Shape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Reshape_1_grad/Shape: (Shape)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/Shape_2: (Shape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Shape_2: (Shape)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/strided_slice_1: (StridedSlice): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/strided_slice_1: (StridedSlice)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/concat_2: (ConcatV2): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/concat_2: (ConcatV2)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/concat_1: (ConcatV2): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/concat_1: (ConcatV2)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Mul_grad/Shape_1: (Shape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Mul_grad/Shape_1: (Shape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Mul_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Mul_grad/BroadcastGradientArgs: (BroadcastGradientArgs)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/word2vec_layer/embedding_lookup_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/word2vec_layer/embedding_lookup_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\nAdagrad/update_word2vec_layer/embeddings/Unique: (Unique): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/embeddings/Unique: (Unique)/job:localhost/replica:0/task:0/cpu:0\r\nAdagrad/update_word2vec_layer/embeddings/Shape: (Shape): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/embeddings/Shape: (Shape)/job:localhost/replica:0/task:0/cpu:0\r\nAdagrad/update_word2vec_layer/embeddings/strided_slice: (StridedSlice): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/embeddings/strided_slice: (StridedSlice)/job:localhost/replica:0/task:0/cpu:0\r\nword2vec_layer/embedding_lookup: (Gather): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/embedding_lookup: (Gather)/job:localhost/replica:0/task:0/cpu:0\r\nnce_loss/MatMul_1: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/MatMul_1: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/add_1_grad/Shape: (Shape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_1_grad/Shape: (Shape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/add_1_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_1_grad/BroadcastGradientArgs: (BroadcastGradientArgs)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/add_1: (Add): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/add_1: (Add)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/sub_1: (Sub): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/sub_1: (Sub)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/zeros_like: (ZerosLike): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/zeros_like: (ZerosLike)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/concat_4: (ConcatV2): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/concat_4: (ConcatV2)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/ExpandDims: (ExpandDims): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/ExpandDims: (ExpandDims)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/Mul: (Mul): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Mul: (Mul)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/Reshape_2: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Reshape_2: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/Shape_3: (Shape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Shape_3: (Shape)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/strided_slice_2: (StridedSlice): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/strided_slice_2: (StridedSlice)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/stack_1: (Pack): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/stack_1: (Pack)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/ones: (Fill): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/ones: (Fill)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/MatMul: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/MatMul: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Reshape_3_grad/Shape: (Shape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Reshape_3_grad/Shape: (Shape)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/Reshape_3: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Reshape_3: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Reshape_4_grad/Shape: (Shape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Reshape_4_grad/Shape: (Shape)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/Reshape_4: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Reshape_4: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/add_grad/Shape: (Shape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_grad/Shape: (Shape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/add_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_grad/BroadcastGradientArgs: (BroadcastGradientArgs)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/add: (Add): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/add: (Add)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/sub: (Sub): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/sub: (Sub)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/concat_3: (ConcatV2): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/concat_3: (ConcatV2)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/Select_grad/zeros_like: (ZerosLike): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Select_grad/zeros_like: (ZerosLike)/job:localhost/replica:0/task:0/gpu:0\r\nsampled_losses/mul: (Mul): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] sampled_losses/mul: (Mul)/job:localhost/replica:0/task:0/gpu:0\r\nsampled_losses/Neg: (Neg): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] sampled_losses/Neg: (Neg)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/Select_1_grad/zeros_like: (ZerosLike): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Select_1_grad/zeros_like: (ZerosLike)/job:localhost/replica:0/task:0/gpu:0\r\nsampled_losses/zeros_like: (ZerosLike): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] sampled_losses/zeros_like: (ZerosLike)/job:localhost/replica:0/task:0/gpu:0\r\nsampled_losses/GreaterEqual: (GreaterEqual): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] sampled_losses/GreaterEqual: (GreaterEqual)/job:localhost/replica:0/task:0/gpu:0\r\nsampled_losses/Select_1: (Select): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] sampled_losses/Select_1: (Select)/job:localhost/replica:0/task:0/gpu:0\r\nsampled_losses/Exp: (Exp): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] sampled_losses/Exp: (Exp)/job:localhost/replica:0/task:0/gpu:0\r\nsampled_losses/Log1p: (Log1p): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] sampled_losses/Log1p: (Log1p)/job:localhost/replica:0/task:0/gpu:0\r\nsampled_losses/Select: (Select): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] sampled_losses/Select: (Select)/job:localhost/replica:0/task:0/gpu:0\r\nsampled_losses/sub: (Sub): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] sampled_losses/sub: (Sub)/job:localhost/replica:0/task:0/gpu:0\r\nsampled_losses: (Add): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] sampled_losses: (Add)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/MatMul_grad/MatMul_1: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/MatMul_grad/MatMul_1: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/MatMul_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/MatMul_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/MatMul_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/MatMul_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses_grad/Sum_1: (Sum)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses_grad/Sum: (Sum): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses_grad/Sum: (Sum)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/Log1p_grad/add: (Add): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Log1p_grad/add: (Add)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/Log1p_grad/Reciprocal: (Reciprocal): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Log1p_grad/Reciprocal: (Reciprocal)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/Log1p_grad/mul: (Mul): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Log1p_grad/mul: (Mul)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/Exp_grad/mul: (Mul): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Exp_grad/mul: (Mul)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/Select_1_grad/Select_1: (Select): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Select_1_grad/Select_1: (Select)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/Select_1_grad/Select: (Select): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Select_1_grad/Select: (Select)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/Select_1_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Select_1_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/Select_1_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Select_1_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/Select_1_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Select_1_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/Neg_grad/Neg: (Neg): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Neg_grad/Neg: (Neg)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/sub_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/sub_grad/Sum_1: (Sum)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/sub_grad/Neg: (Neg): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/sub_grad/Neg: (Neg)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/sub_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/sub_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/sub_grad/Sum: (Sum): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/sub_grad/Sum: (Sum)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/sub_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/sub_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/sub_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/sub_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/sub_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/sub_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/mul_grad/mul_1: (Mul): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/mul_grad/mul_1: (Mul)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/mul_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/mul_grad/Sum_1: (Sum)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/mul_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/mul_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/mul_grad/mul: (Mul): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/mul_grad/mul: (Mul)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/mul_grad/Sum: (Sum): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/mul_grad/Sum: (Sum)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/mul_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/mul_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/mul_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/mul_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/mul_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/mul_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/sub_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/sub_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/Select_grad/Select_1: (Select): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Select_grad/Select_1: (Select)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/Select_grad/Select: (Select): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Select_grad/Select: (Select)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/Select_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Select_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/Select_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Select_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/AddN: (AddN): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/AddN: (AddN)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/concat_3_grad/Slice_1: (Slice): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/concat_3_grad/Slice_1: (Slice)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/concat_3_grad/Slice: (Slice): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/concat_3_grad/Slice: (Slice)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/concat_3_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/concat_3_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/concat_3_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/concat_3_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/sub_1_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_1_grad/Sum_1: (Sum)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/sub_1_grad/Neg: (Neg): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_1_grad/Neg: (Neg)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/sub_1_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_1_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/sub_1_grad/Sum: (Sum): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_1_grad/Sum: (Sum)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/sub_1_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_1_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/sub_1_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_1_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/sub_1_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_1_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/add_1_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_1_grad/Sum_1: (Sum)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/add_1_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_1_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/add_1_grad/Sum: (Sum): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_1_grad/Sum: (Sum)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/add_1_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_1_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/add_1_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_1_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/add_1_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_1_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_3_grad/Pad: (Pad): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_3_grad/Pad: (Pad)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/add_1_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_1_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/MatMul_1_grad/MatMul_1: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/MatMul_1_grad/MatMul_1: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/MatMul_1_grad/MatMul: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/MatMul_1_grad/MatMul: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/MatMul_1_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/MatMul_1_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/MatMul_1_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/MatMul_1_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_2_grad/Pad: (Pad): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_2_grad/Pad: (Pad)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/MatMul_1_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/MatMul_1_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/concat_3_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/concat_3_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/sub_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_grad/Sum_1: (Sum)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/sub_grad/Neg: (Neg): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_grad/Neg: (Neg)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/sub_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/sub_grad/Sum: (Sum): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_grad/Sum: (Sum)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/sub_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/sub_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/sub_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/add_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_grad/Sum_1: (Sum)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/add_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/add_grad/Sum: (Sum): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_grad/Sum: (Sum)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/add_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/add_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/add_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Reshape_5_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Reshape_5_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_1_grad/Pad: (Pad): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_1_grad/Pad: (Pad)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/AddN_1: (AddN): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/AddN_1: (AddN)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/embedding_lookup_1_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_1_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\nAdagrad/update_word2vec_layer/nce_biases/UnsortedSegmentSum: (UnsortedSegmentSum): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/nce_biases/UnsortedSegmentSum: (UnsortedSegmentSum)/job:localhost/replica:0/task:0/cpu:0\r\nAdagrad/update_word2vec_layer/nce_biases/SparseApplyAdagrad: (SparseApplyAdagrad): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/nce_biases/SparseApplyAdagrad: (SparseApplyAdagrad)/job:localhost/replica:0/task:0/cpu:0\r\ngradients/nce_loss/add_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Reshape_4_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Reshape_4_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Reshape_3_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Reshape_3_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/MatMul_grad/MatMul_1: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/MatMul_grad/MatMul_1: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/MatMul_grad/MatMul: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/MatMul_grad/MatMul: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/MatMul_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/MatMul_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/MatMul_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/MatMul_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Reshape_2_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Reshape_2_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Mul_grad/mul_1: (Mul): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Mul_grad/mul_1: (Mul)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Mul_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Mul_grad/Sum_1: (Sum)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Mul_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Mul_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Mul_grad/mul: (Mul): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Mul_grad/mul: (Mul)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Mul_grad/Sum: (Sum): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Mul_grad/Sum: (Sum)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Mul_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Mul_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Mul_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Mul_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Mul_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Mul_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Reshape_1_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Reshape_1_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_grad/Pad: (Pad): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_grad/Pad: (Pad)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/AddN_3: (AddN): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/AddN_3: (AddN)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/embedding_lookup_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\nAdagrad/update_word2vec_layer/nce_weights/UnsortedSegmentSum: (UnsortedSegmentSum): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/nce_weights/UnsortedSegmentSum: (UnsortedSegmentSum)/job:localhost/replica:0/task:0/cpu:0\r\nAdagrad/update_word2vec_layer/nce_weights/SparseApplyAdagrad: (SparseApplyAdagrad): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/nce_weights/SparseApplyAdagrad: (SparseApplyAdagrad)/job:localhost/replica:0/task:0/cpu:0\r\ngradients/nce_loss/Mul_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Mul_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/ExpandDims_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/ExpandDims_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/AddN_2: (AddN): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/AddN_2: (AddN)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/word2vec_layer/embedding_lookup_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/word2vec_layer/embedding_lookup_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\nAdagrad/update_word2vec_layer/embeddings/UnsortedSegmentSum: (UnsortedSegmentSum): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/embeddings/UnsortedSegmentSum: (UnsortedSegmentSum)/job:localhost/replica:0/task:0/cpu:0\r\nAdagrad/update_word2vec_layer/embeddings/SparseApplyAdagrad: (SparseApplyAdagrad): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/embeddings/SparseApplyAdagrad: (SparseApplyAdagrad)/job:localhost/replica:0/task:0/cpu:0\r\nAdagrad: (NoOp): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad: (NoOp)/job:localhost/replica:0/task:0/gpu:0\r\nMatMul: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] MatMul: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\nReshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0\r\nMean: (Mean): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Mean: (Mean)/job:localhost/replica:0/task:0/gpu:0\r\n_recv_Placeholder_0: (_Recv): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] _recv_Placeholder_0: (_Recv)/job:localhost/replica:0/task:0/cpu:0\r\n_recv_Placeholder_1_0: (_Recv): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] _recv_Placeholder_1_0: (_Recv)/job:localhost/replica:0/task:0/cpu:0\r\n_send_Mean_0: (_Send): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] _send_Mean_0: (_Send)/job:localhost/replica:0/task:0/cpu:0\r\nAdagrad/update_word2vec_layer/nce_biases/strided_slice/stack_2: (Const): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/nce_biases/strided_slice/stack_2: (Const)/job:localhost/replica:0/task:0/cpu:0\r\nAdagrad/update_word2vec_layer/nce_biases/strided_slice/stack_1: (Const): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/nce_biases/strided_slice/stack_1: (Const)/job:localhost/replica:0/task:0/cpu:0\r\nAdagrad/update_word2vec_layer/nce_biases/strided_slice/stack: (Const): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/nce_biases/strided_slice/stack: (Const)/job:localhost/replica:0/task:0/cpu:0\r\nAdagrad/update_word2vec_layer/nce_weights/strided_slice/stack_2: (Const): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/nce_weights/strided_slice/stack_2: (Const)/job:localhost/replica:0/task:0/cpu:0\r\nAdagrad/update_word2vec_layer/nce_weights/strided_slice/stack_1: (Const): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/nce_weights/strided_slice/stack_1: (Const)/job:localhost/replica:0/task:0/cpu:0\r\nAdagrad/update_word2vec_layer/nce_weights/strided_slice/stack: (Const): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/nce_weights/strided_slice/stack: (Const)/job:localhost/replica:0/task:0/cpu:0\r\nAdagrad/update_word2vec_layer/embeddings/strided_slice/stack_2: (Const): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/embeddings/strided_slice/stack_2: (Const)/job:localhost/replica:0/task:0/cpu:0\r\nAdagrad/update_word2vec_layer/embeddings/strided_slice/stack_1: (Const): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/embeddings/strided_slice/stack_1: (Const)/job:localhost/replica:0/task:0/cpu:0\r\nAdagrad/update_word2vec_layer/embeddings/strided_slice/stack: (Const): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/embeddings/strided_slice/stack: (Const)/job:localhost/replica:0/task:0/cpu:0\r\nAdagrad/learning_rate: (Const): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/learning_rate: (Const)/job:localhost/replica:0/task:0/cpu:0\r\ngradients/nce_loss/embedding_lookup_grad/concat/axis: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_grad/concat/axis: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/embedding_lookup_grad/strided_slice/stack_2: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_grad/strided_slice/stack_2: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/embedding_lookup_grad/strided_slice/stack_1: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_grad/strided_slice/stack_1: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/embedding_lookup_grad/strided_slice/stack: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_grad/strided_slice/stack: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/embedding_lookup_grad/ExpandDims/dim: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_grad/ExpandDims/dim: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/embedding_lookup_grad/Size: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_grad/Size: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/embedding_lookup_grad/Shape: (Const): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_grad/Shape: (Const)/job:localhost/replica:0/task:0/cpu:0\r\ngradients/nce_loss/Slice_grad/concat/axis: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_grad/concat/axis: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_grad/stack/1: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_grad/stack/1: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_grad/Rank: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_grad/Rank: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/word2vec_layer/embedding_lookup_grad/concat/axis: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/word2vec_layer/embedding_lookup_grad/concat/axis: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/word2vec_layer/embedding_lookup_grad/strided_slice/stack_2: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/word2vec_layer/embedding_lookup_grad/strided_slice/stack_2: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/word2vec_layer/embedding_lookup_grad/strided_slice/stack_1: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/word2vec_layer/embedding_lookup_grad/strided_slice/stack_1: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/word2vec_layer/embedding_lookup_grad/strided_slice/stack: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/word2vec_layer/embedding_lookup_grad/strided_slice/stack: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/word2vec_layer/embedding_lookup_grad/ExpandDims/dim: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/word2vec_layer/embedding_lookup_grad/ExpandDims/dim: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/word2vec_layer/embedding_lookup_grad/Size: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/word2vec_layer/embedding_lookup_grad/Size: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/word2vec_layer/embedding_lookup_grad/Shape: (Const): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/word2vec_layer/embedding_lookup_grad/Shape: (Const)/job:localhost/replica:0/task:0/cpu:0\r\ngradients/nce_loss/ExpandDims_grad/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/ExpandDims_grad/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Mul_grad/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Mul_grad/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Reshape_2_grad/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Reshape_2_grad/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/embedding_lookup_1_grad/concat/axis: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_1_grad/concat/axis: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/embedding_lookup_1_grad/strided_slice/stack_2: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_1_grad/strided_slice/stack_2: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/embedding_lookup_1_grad/strided_slice/stack_1: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_1_grad/strided_slice/stack_1: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/embedding_lookup_1_grad/strided_slice/stack: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_1_grad/strided_slice/stack: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/embedding_lookup_1_grad/ExpandDims/dim: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_1_grad/ExpandDims/dim: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/embedding_lookup_1_grad/Size: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_1_grad/Size: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/embedding_lookup_1_grad/Shape: (Const): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_1_grad/Shape: (Const)/job:localhost/replica:0/task:0/cpu:0\r\ngradients/nce_loss/Slice_2_grad/concat/axis: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_2_grad/concat/axis: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_2_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_2_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_2_grad/stack/1: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_2_grad/stack/1: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_2_grad/Rank: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_2_grad/Rank: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_1_grad/concat/axis: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_1_grad/concat/axis: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_1_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_1_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_1_grad/stack/1: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_1_grad/stack/1: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_1_grad/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_1_grad/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_1_grad/Rank: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_1_grad/Rank: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_3_grad/concat/axis: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_3_grad/concat/axis: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_3_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_3_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_3_grad/stack/1: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_3_grad/stack/1: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_3_grad/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_3_grad/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Slice_3_grad/Rank: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_3_grad/Rank: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/Reshape_5_grad/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Reshape_5_grad/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/add_1_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_1_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/add_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/sub_1_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_1_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/sub_1_grad/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_1_grad/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/sub_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/sub_grad/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_grad/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/concat_3_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/concat_3_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/concat_3_grad/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/concat_3_grad/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/nce_loss/concat_3_grad/Rank: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/concat_3_grad/Rank: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/mul_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/mul_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/mul_grad/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/mul_grad/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/sub_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/sub_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/sub_grad/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/sub_grad/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses_grad/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses_grad/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/Reshape_grad/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Reshape_grad/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/Mean_grad/Maximum/y: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Mean_grad/Maximum/y: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/Mean_grad/Const_1: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Mean_grad/Const_1: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/Mean_grad/Const: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Mean_grad/Const: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/Mean_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Mean_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/Mean_grad/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Mean_grad/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/Mean_grad/Tile/multiples: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Mean_grad/Tile/multiples: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/Mean_grad/Reshape/shape: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Mean_grad/Reshape/shape: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/Const: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Const: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nConst_1: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Const_1: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nReshape/shape: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Reshape/shape: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nones/Const: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] ones/Const: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nstack/1: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] stack/1: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nstrided_slice/stack_2: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] strided_slice/stack_2: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nstrided_slice/stack_1: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] strided_slice/stack_1: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nstrided_slice/stack: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] strided_slice/stack: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nShape: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Shape: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/concat_4/axis: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/concat_4/axis: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/truediv/y: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/truediv/y: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/ones_like/Const: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/ones_like/Const: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/ones_like/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/ones_like/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/concat_3/axis: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/concat_3/axis: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/Slice_3/size: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Slice_3/size: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/Shape_5: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Shape_5: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/Slice_2/size: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Slice_2/size: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/stack_2/1: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/stack_2/1: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/strided_slice_3/stack_2: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/strided_slice_3/stack_2: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/strided_slice_3/stack_1: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/strided_slice_3/stack_1: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/strided_slice_3/stack: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/strided_slice_3/stack: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/Shape_4: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Shape_4: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/Reshape_5/shape: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Reshape_5/shape: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/Reshape_4/shape: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Reshape_4/shape: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/Reshape_3/shape: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Reshape_3/shape: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/ones/Const: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/ones/Const: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/stack_1/1: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/stack_1/1: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/strided_slice_2/stack_2: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/strided_slice_2/stack_2: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/strided_slice_2/stack_1: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/strided_slice_2/stack_1: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/strided_slice_2/stack: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/strided_slice_2/stack: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/concat_2/axis: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/concat_2/axis: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/concat_2/values_0: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/concat_2/values_0: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/ExpandDims/dim: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/ExpandDims/dim: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/concat_1/axis: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/concat_1/axis: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/concat_1/values_0: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/concat_1/values_0: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/strided_slice_1/stack_2: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/strided_slice_1/stack_2: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/strided_slice_1/stack_1: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/strided_slice_1/stack_1: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/strided_slice_1/stack: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/strided_slice_1/stack: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/Slice_1/begin: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Slice_1/begin: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/Shape_1: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Shape_1: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/Slice/begin: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Slice/begin: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/stack/1: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/stack/1: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/strided_slice/stack_2: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/strided_slice/stack_2: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/strided_slice/stack_1: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/strided_slice/stack_1: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/strided_slice/stack: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/strided_slice/stack: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nnce_loss/concat/axis: (Const): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/concat/axis: (Const)/job:localhost/replica:0/task:0/cpu:0\r\nnce_loss/Reshape/shape: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Reshape/shape: (Const)/job:localhost/replica:0/task:0/gpu:0\r\ngradients/sampled_losses/Log1p_grad/add/x: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Log1p_grad/add/x: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nE tensorflow/core/framework/op_segment.cc:53] Create kernel failed: Invalid argument: AttrValue must not have reference type value of float_ref\r\nfor attr 'tensor_type'\r\n; NodeDef: word2vec_layer/embeddings/Adagrad/_71 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_100_word2vec_layer/embeddings/Adagrad\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](^Adagrad/learning_rate, ^Adagrad/update_word2vec_layer/embeddings/UnsortedSegmentSum, ^Adagrad/update_word2vec_layer/embeddings/Unique); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\nE tensorflow/core/common_runtime/executor.cc:594] Executor failed to create kernel. Invalid argument: AttrValue must not have reference type value of float_ref\r\nfor attr 'tensor_type'\r\n; NodeDef: word2vec_layer/embeddings/Adagrad/_71 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_100_word2vec_layer/embeddings/Adagrad\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](^Adagrad/learning_rate, ^Adagrad/update_word2vec_layer/embeddings/UnsortedSegmentSum, ^Adagrad/update_word2vec_layer/embeddings/Unique); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n[[Node: word2vec_layer/embeddings/Adagrad/_71 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_100_word2vec_layer/embeddings/Adagrad\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](^Adagrad/learning_rate, ^Adagrad/update_word2vec_layer/embeddings/UnsortedSegmentSum, ^Adagrad/update_word2vec_layer/embeddings/Unique)]]\r\nTraceback (most recent call last):\r\nFile \"tutorial_word2vec_basic.py\", line 369, in \r\nmain_word2vec_basic()\r\nFile \"tutorial_word2vec_basic.py\", line 246, in main_word2vec_basic\r\n_, loss_val = sess.run([train_op, cost], feed_dict=feed_dict)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 767, in run\r\nrun_metadata_ptr)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 965, in _run\r\nfeed_dict_string, options, run_metadata)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\r\ntarget_list, options, run_metadata)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\r\nraise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: AttrValue must not have reference type value of float_ref\r\nfor attr 'tensor_type'\r\n; NodeDef: word2vec_layer/embeddings/Adagrad/_71 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_100_word2vec_layer/embeddings/Adagrad\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](^Adagrad/learning_rate, ^Adagrad/update_word2vec_layer/embeddings/UnsortedSegmentSum, ^Adagrad/update_word2vec_layer/embeddings/Unique); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n[[Node: word2vec_layer/embeddings/Adagrad/_71 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_100_word2vec_layer/embeddings/Adagrad\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](^Adagrad/learning_rate, ^Adagrad/update_word2vec_layer/embeddings/UnsortedSegmentSum, ^Adagrad/update_word2vec_layer/embeddings/Unique)]]\r\n", "comments": ["Unfortunately this is too much code for us to debug. I see you've also opened an issue in the TensorLayer project (https://github.com/zsdonghao/tensorlayer/issues/135). I'm gonna close this for now, but please reopen if further investigation yields a smaller code sample that repros a TensorFlow bug (this might turn out to be a bug in the TensorLayer code)."]}, {"number": 9624, "title": "Is batch_norm_param argument missing in the depthwise convolution 2d layer implementation?", "body": "This is with reference to: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L1850\r\n\r\nThe line says \"if `batch_norm_params` is None\" but there is no batch_norm_params argument included in the function, and it doesn't seem that batch_norm is implemented within the function as an option. Is the batch_norm function included in the regularizer function or has it not been implemented by default within the function?\r\n\r\nThanks for your help.", "comments": ["It looks like it's a remnant of a copy-paste. Do you want to submit a PR fixing the doc?", "@drpngx I have submitted a PR fixing the description error and including `batch_norm` by default as the `normalizer_fn`. \r\n\r\nLink: https://github.com/tensorflow/tensorflow/pull/9652/files", "Thanks! Adding cross-link: #9652.", "Merged. Thanks!"]}, {"number": 9623, "title": "What is the best way to test speed of TF", "body": "I am trying to test the speed of TF but seems TF is using python and it is gonna be quite slow. What is the rrecommended way to test the speed of a model in TF?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9622, "title": "Making the quantizedMatmul for FC layers multi-thread", "body": "For the \u201cm<=4\u201d GEMM that would call into Gemmlowp/meta lib, change the single thread invoking to multi-threads invoking. Because for the quantized full connection layers, multi-thread would improve the memory accessing of the relatively large weights matrix.", "comments": ["Can one of the admins verify this patch?", "Maybe Andrew Harp andrewharp@google.com<mailto:andrewharp@google.com>  can help on it.\r\nThank you Andrew!\r\n\r\nJi\r\n\r\n-------------------------------------------------------------------------------------------------------------------------------------\r\nTHIS E-MAIL INCLUDING ANY ATTACHMENTS CONTAINS SPREADTRUM\u2019S PROPRIETARY CONFIDENTIAL INFORMATION THAT IS HIGHLY CONFIDENTIAL AND PRIVILEGED.\r\n\r\nFrom: Tensorflow Jenkins [mailto:notifications@github.com]\r\nSent: Wednesday, May 03, 2017 4:50 PM\r\nTo: tensorflow/tensorflow\r\nCc: Ji Qiu (\u90b1\u5409); Author\r\nSubject: Re: [tensorflow/tensorflow] Making the quantizedMatmul for FC layers multi-thread (#9622)\r\n\r\n\r\nCan one of the admins verify this patch?\r\n\r\n\u2014\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/pull/9622#issuecomment-298854909>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AXR6ukgHd2_EZW9sXTMMCgGSkT0rVJYVks5r2D-vgaJpZM4NPHAN>.\r\n", "I've asked @andrewharp to run our internal benchmarks with this change, to make sure that we don't see performance regressions on any devices or models.", "Hi, thanks for the PR, but do you have the numbers that support it. IIRC I have decided not to multithread, exactly because of the memory controller. When the GEMM is close to being a GEMV, the computation is memory constrained, and multithreading does not help much. Can you please run some benchmarks and point out the cases when this actually improves performance?", "Here they are.\r\nTest results are showed in the following table. The performance of the inference of the \u201cquantized both node and weights VGG16\u201d on 4 target phones become better. \r\n\r\n\r\nPhone              chip-name               test-condition                        before   after    diff-ratio\r\nSamsung C7\tSnapdragon625\ton core4~7 @2.0GHz\t       3.55\t    2.58\t27%\r\nSPRD-proto1\tSPRD-SOC1\t        on big core4~7 @2.0GHz\t7.05\t    3.36\t52%\r\nLG Nexus5X6P\tSnapdragon808\ton little core0~3@1.44Ghz    4.63\t    3.41\t26%\r\nSPRD-proto2\tSPRD-SOC2\ton core0~3 @1.4Ghz\t               3.73\t    3.15\t16%\r\n\r\nPlatform and test condition description:\r\nPhone\t        chip name \t       CPU and DDR configuration\t                   \r\nSamsung C7\tSnapdragon625\t8*CA53, not big/little, LPDDR3 933Mhz\t\r\nSPRD-proto1\tSPRD-SOC1\t        8*CA53,big.LITTLE,LPDDR4 667Mhz\t\r\nLG Nexus5X6P\tSnapdragon808\t4*CA53,2*CA57, big.LITTLE, LPDDR3 933Mhz\t\r\nSPRD-proto2\tSPRD-SOC2\t        4*CA7, LPDDR3 667Mhz\t\r\n\r\nTo my knowledge, the FC layers of VGG16 has much more weights matrix and much less FLOPS than the convolution layers, that means the computation/memory access ratio is quite smaller. \r\nFor the CA53 likewise in-ordered core, stalls would happen every time a load instruction is not get data back. For those out-of-order cores like CA57, although the pipeline has some tolerant of long latency load instruction, due to the limited depth of the re-order queue, the core may still stall to wait the load instructions. If only one core are assigned the whole FC task, the bandwidth usage would be quite low because the single core cannot emit enough DDR requests. If we divide the task into more cores, the requests would be more intensive then the total memory bandwidth would be much better than before. Meanwhile, the latency of the single core stall for load would be amortized between cores.\r\n\r\n\r\n\r\n", "Mixed results, with a couple of both significant improvements and regressions. The majority of it could basically be noise.\r\n\r\n```\r\nBaseline [baseline_155022362]:\r\n\r\nAbsolute time per iteration (s):\r\n                              \t  nexus-5\t nexus-5x\t  nexus-6\t nexus-6p\t    pixel\r\n           facenet-v8-stripped\t  0.32138\t  0.54841\t  0.29269\t  0.63117\t  0.20970\r\n                  inception-v1\t  0.18819\t  0.27600\t  0.14769\t  0.39691\t  0.12089\r\n                  inception-v3\t  1.09949\t  1.67107\t  0.94116\t  1.87631\t  0.66315\r\n               multibox-person\t  0.24984\t  0.35329\t  0.36708\t  0.30667\t  0.17588\r\n       ssd-mobilenet-v1-person\t  0.13788\t  0.23684\t  0.13987\t  0.23274\t  0.09382\r\n                      wordlens\t  0.00028\t  0.00020\t  0.00029\t  0.00018\t  0.00017\r\n\r\n------------------------\r\n\r\nDelta [meta3_155022362]:\r\n\r\nAbsolute time per iteration (s):\r\n                              \t  nexus-5\t nexus-5x\t  nexus-6\t nexus-6p\t    pixel\r\n           facenet-v8-stripped\t  0.32849\t  0.54276\t  0.28696\t  0.63264\t  0.21397\r\n                  inception-v1\t  0.20234\t  0.27980\t  0.15618\t  0.38028\t  0.12036\r\n                  inception-v3\t  1.20894\t  1.69124\t  0.94312\t  1.83578\t  0.69452\r\n               multibox-person\t  0.22295\t  0.37751\t  0.33506\t  0.31456\t  0.20516\r\n       ssd-mobilenet-v1-person\t  0.13643\t  0.27929\t  0.14091\t  0.23362\t  0.09218\r\n                      wordlens\t  0.00029\t  0.00020\t  0.00027\t  0.00018\t  0.00016\r\n\r\nRelative change in percent:\r\n                              \t  nexus-5\t nexus-5x\t  nexus-6\t nexus-6p\t    pixel\r\n           facenet-v8-stripped\t  2.21340\t -1.02976\t -1.95770\t  0.23317\t  2.03569\r\n                  inception-v1\t  7.51770\t  1.37644\t  5.75014\t -4.18876\t -0.43913\r\n                  inception-v3\t  9.95425\t  1.20700\t  0.20764\t -2.15987\t  4.72982\r\n               multibox-person\t-10.76515\t  6.85537\t -8.72257\t  2.57241\t 16.64816\r\n       ssd-mobilenet-v1-person\t -1.05368\t 17.92013\t  0.73927\t  0.37705\t -1.74032\r\n                      wordlens\t  2.17376\t -0.59429\t -6.38588\t -1.21792\t -7.01046\r\n```", "Hi Andrew,\r\nGreat thanks to your data! Yes, it's seems the result is not quite agreed with what I get on the quantized VGG16 model.\r\nBut could I get to know more about the benchmark models you had used? I mean could I get access to the \u201cfacenet-v8-stripped\u201d/\u201cinception-v1\u201d and so on models you used to do test?  Because I'm quite interested to look into the regression reason. \r\nAs the parallelization for the FC layers can only bring benefits to those with big weights matrix, it's reasonable that models don't has such attribute would go worse with this patch.\r\n\r\nAt the moment, I only have two models, one is VGG16, the other is  from https://storage.googleapis.com/download.tensorflow.org/models/inception_dec_2015.zip -O tensorflow/examples/label_image/data/inception_dec_2015.zip\r\n\r\nThe performance of this inception model is not enhanced by the patch because the FC layers are so small comparing to VGG16. One is (m:1, n:1024, k:2048) and the other is (m:1, n\uff1a1008, k\uff1a1024)\u3002 \r\n\r\nHow about the idea of adding some decision code here to judge whether a FC layers should be parallelized? \r\n\r\nThanks again!\r\n\r\nJi", "Can one of the admins verify this patch?", "Hi, sorry for the delay,\r\nAdding some additional logic to decide whether to multithread would be great. Metagemm has some basic code to do that, but looking at the mixed results Andrew got, this code does not work well. We might try a weight matrix size threshold. What are the shapes of the vector x matrix multiplications in VGG16 ?", "Hi Maciekcc\uff0c\r\nThanks for your reply\uff01\r\nVGG16 has 3 vector \u00d7matrix layers\uff0c they are FC6/FC7/FC8 layers.\r\nFC6\uff1a LHS\uff1a 1\u00d725088 RHS\uff1a25088\u00d74096\r\nFC7\uff1a LHS: 1x4096 RHS:4096x4096\r\nFC8: LHS: 1x4096 RHS: 4096x1000\r\n\r\nhere is my test result on Nexus5X6P, 4 little core of Cortex-A53 @1.44Ghz, for both single thread and 4 thread multi-thread for m always 1, n from 512 to 32768, k from 1024 to 16384. The result shows that parallelization always brings good to these shapes.\r\nhttp://paste.ubuntu.com/24559715/\r\n ", "Any update on this? @petewarden @andrewharp do you think we should proceed with this?", "@rmlarsen I ran the benchmarks again two more times and got much better results (make sure to scroll over if the pixel results are hidden):\r\n\r\nRun 1:\r\n```\r\nRelative change in percent:\r\n                              \t  nexus-5\t nexus-5x\t  nexus-6\t nexus-6p\t    pixel\r\n           facenet-v8-stripped\t-14.49978\t -1.32524\t -3.33963\t -2.77208\t -6.87099\r\n                  inception-v1\t 58.87876\t  5.66264\t  3.67781\t -2.78782\t-16.60963\r\n                  inception-v3\t 27.62776\t -0.03850\t  4.21516\t  1.79008\t  0.39579\r\n               multibox-person\t -7.51846\t 17.31707\t 32.65962\t-10.62376\t -8.44006\r\n       ssd-mobilenet-v1-person\t-29.54936\t -0.88066\t  6.12411\t  3.71565\t -4.09103\r\n                       stylize\t-28.55930\t -7.68450\t -6.32689\t 19.17275\t -6.02302\r\n             stylize-quantized\t-15.68737\t -2.59237\t  4.23402\t 18.88487\t -6.51874\r\n                      wordlens\t 67.26827\t202.31065\t -0.73515\t  0.69556\t -0.38815\r\n```\r\n\r\nRun 2:\r\n```\r\nRelative change in percent:\r\n                              \t  nexus-5\t nexus-5x\t  nexus-6\t nexus-6p\t    pixel\r\n           facenet-v8-stripped\t 35.86868\t -5.92455\t -4.43560\t  4.27805\t -3.76654\r\n                  inception-v1\t -5.30476\t  2.79366\t-13.61444\t  1.77188\t-17.61543\r\n                  inception-v3\t -5.92321\t -0.65455\t -2.57162\t -1.41027\t  0.92794\r\n               multibox-person\t-23.50986\t -0.06107\t  5.29944\t -2.18944\t-11.09237\r\n       ssd-mobilenet-v1-person\t 36.09077\t-18.27276\t -0.10371\t  1.09788\t -2.32616\r\n                       stylize\t -6.52473\t -8.29781\t -0.80654\t 18.24873\t -8.23492\r\n             stylize-quantized\t 28.76460\t  1.34093\t -0.11584\t 13.73769\t -5.18219\r\n                      wordlens\t-28.47009\t -0.14812\t-11.14184\t 41.61584\t  6.06806\r\n```\r\nNot sure what the difference was from the initial test, maybe I had some conflating factor previously -- made extra sure to isolate the change this time around. Seems worth commiting.\r\n\r\nThe couple of spikes in timing could be down to getting warm devices that hit thermal throttling during the tests, as most of them only exist in a single run. The only device which got consistently worse was the 6p, everything else got some decent gains.", "Jenkins, test this please.\r\n\r\n@andrewharp so then we should merge this?", "@drpngx Yes, I think performance gains merit merging it in"]}, {"number": 9621, "title": "How can I load data from a data file just like http://projector.tensorflow.org/ does when using Tensorboard embedding projector", "body": "At http://projector.tensorflow.org/  we can choose a data file from our own computer by the `Load data` button(see the red rectangle below) and visualize the high-dimensional data\r\n![image](https://cloud.githubusercontent.com/assets/7208819/25652477/fbc9675e-301a-11e7-931f-c7bb22fbdea2.png)\r\n![image](https://cloud.githubusercontent.com/assets/7208819/25652234/ce6c2158-3019-11e7-9c10-47fe2a1e2224.png)\r\n\r\nBut when I tried to using Tensorboard embedding on my own computer (follow the instruction at https://www.tensorflow.org/get_started/embedding_viz and start a server on localhost:6006 by command `tensorboard --logdir=LOG_DIR` )\r\nI just find the `Load data` button doesn't exist (see below)\r\n\r\n![image](https://cloud.githubusercontent.com/assets/7208819/25652904/0f4ff566-301d-11e7-9a1e-4d097d108349.png)\r\n\r\nSo I want to ask when I run Tensorboard on localhost, how can I enable the load data button like http://projector.tensorflow.org/ does, thus I can visualize the high-dimensional data by uploading my file rather than writing code ?\r\n\r\nthanks a lot :)\r\n\r\n", "comments": ["@dandelionmane @jart can one of you comment?", "The load data is a feature on the TensorFlow.org version. It probably wouldn't be too hard to enable it in TensorBoard too, but this doesn't feel like a priority. If you want to make a pull request that enables it in TensorBoard be happy to review it. Also pinging @dsmilkov. Marking tensorboard/feature request/contributions welcome. ", "ok, thx anyway :)", "Today I found this:\r\nhttps://github.com/tensorflow/tensorflow/commit/8c8ac705ec4cbae71b56fa37990e26890b576a77?diff=split\r\n\r\nfrom \r\ntensorflow/tensorboard/components/vz_projector/vz-projector-data-panel.html\r\n<pre><code>#demo-data-buttons-container {\r\ndisplay: none;\r\n}\r\n</code></pre>\r\nand \r\ntensorflow/tensorboard/components/vz_projector/vz-projector-data-panel.ts\r\n<pre><code> if (this.projector.servingMode === 'demo') {\r\n       (this.$$('#demo-data-buttons-container') as HTMLElement).style.display = 'block';\r\n       // Fill out the projector config.\r\n      ...\r\n</code></pre>\r\nwe can know only when the `projector.servingMode` is 'demo', the upload button can be displayed\r\nbut I don't know how to set the `projector.servingMode` into 'demo' mode\r\n\r\nso I just modified \r\n<pre><code>#demo-data-buttons-container {\r\ndisplay: none;\r\n}\r\n</code></pre>\r\ninto\r\n<pre><code>#demo-data-buttons-container {\r\ndisplay: block;\r\n}\r\n</code></pre>\r\nand it works!\r\n![image](https://cloud.githubusercontent.com/assets/7208819/25985450/21f74366-371d-11e7-8e4e-25b8c0459478.png)\r\n\r\nHowever, can anyone tell me how to set the `projector.servingMode` into 'demo' mode?", "This issue has been migrated to https://github.com/tensorflow/tensorboard/issues/94."]}, {"number": 9620, "title": "inception/imagenet_distributed_train running is faild", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Not - Use last TF master and last Models\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:   Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: from source ( dramatically less compilation number of files, less in 100 files.)\r\n- **TensorFlow version (use command below)**: 'v1.1.0-rc2-607-g550df41', '1.1.0-rc2' \r\n\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: 8.0, 6\r\n- **GPU model and memory**: Nvidia P100 PCI - 16 GB\r\n- **Exact command to reproduce**:\r\nOn ps - bazel-bin/inception/imagenet_distributed_train --job_name='ps' --task_id=0 --ps_hosts='11.11.11.31:2222' --worker_hosts='11.11.11.41:2222,11.11.11.41:2223'\r\n\r\nAll good \r\n\r\nINFO:tensorflow:PS hosts are: ['11.11.11.31:2222']\r\nINFO:tensorflow:Worker hosts are: ['11.11.11.41:2222', '11.11.11.41:2223']\r\n2017-05-03 11:12:54.447399: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2222}\r\n2017-05-03 11:12:54.447459: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> 11.11.11.41:2222, 1 -> 11.11.11.41:2223}\r\n2017-05-03 11:12:54.456646: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:296] Started server with target: grpc://localhost:2222\r\n\r\n### Describe the problem\r\n\r\nOn Worker \r\n\r\nCUDA_VISIBLE_DEVICES='1' bazel-bin/inception/imagenet_distributed_train --batch_size=128 --job_name='worker' --ps_hosts='11.11.11.31:2222' --worker_hosts='11.11.11.41:2222,11.11.11.41:2223' --data_dir=/data/imagenet_data/ --train_dir=/data/imagenet_train/ --task_id=1\r\n\r\nIt's failed with\r\n INFO:tensorflow:PS hosts are: ['11.11.11.31:2222']                                                                \r\nINFO:tensorflow:Worker hosts are: ['11.11.11.41:2222', '11.11.11.41:2223']                                        \r\n2017-05-03 11:24:17.640127: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> 11.11.11.31:2222}                                                                        \r\n2017-05-03 11:24:17.640169: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> 11.11.11.41:2222, 1 -> localhost:2223}                                               \r\n2017-05-03 11:24:17.648767: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:296] Started server with target: grpc://localhost:2223                                                                                     \r\nTraceback (most recent call last):                                                                                \r\n  File \"/root/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception/imagenet_distributed_train.py\", line 66, in <module>                                                                     \r\n    tf.app.run()                                                                                                  \r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 48, in run                \r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))                                                            \r\n  File \"/root/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception/imagenet_distributed_train.py\", line 62, in main\r\n    inception_distributed_train.train(server.target, dataset, cluster_spec)\r\n  File \"/root/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception/inception_distributed_train.py\", line 157, in train\r\n    inception.loss(logits, labels)\r\n  File \"/root/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception/inception_model.py\", line 128, in loss\r\n    weight=1.0)\r\n  File \"/root/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/inception/inception/slim/losses.py\", line 166, in cross_entropy_loss\r\n    cross_entropy = tf.contrib.nn.deprecated_flipped_softmax_cross_entropy_with_logits(\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/lazy_loader.py\", line 53, in __getattr__\r\n    module = self._load()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/lazy_loader.py\", line 42, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/usr/lib/python2.7/importlib/__init__.py\", line 37, in import_module\r\n    __import__(name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/__init__.py\", line 34, in <module>\r\n    from tensorflow.contrib import image\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/image/__init__.py\", line 39, in <module>\r\n    from tensorflow.contrib.image.python.ops.single_image_random_dot_stereograms import single_image_random_dot_stereograms\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/image/python/ops/single_image_random_dot_stereograms.py\", line 26, in <module>\r\n    \"_single_image_random_dot_stereograms.so\"))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/util/loader.py\", line 55, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/load_library.py\", line 64, in load_op_library\r\n    None, None, error_msg, error_code)\r\ntensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/image/python/ops/_single_image_random_dot_stereograms.so: undefined symbol: _ZN6google8protobuf8internal10LogMessageC1ENS0_8LogLevelEPKci\r\n\r\nThanks,\r\n\r\nBoris\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["More simple;\r\n\r\nbazel-bin/inception/imagenet_train --batch_size=128 --train_dir=/data/imagenet_train/ --num_gpus=1 --data_dir=/data/imagenet_data/ \r\n\r\nTraceback (most recent call last):                   \r\n  File \"/root/models/inception/bazel-bin/inception/imagenet_train.runfiles/inception/inception/imagenet_train.py\", line 41, in <module>                                                                                             \r\n    tf.app.run()                                                                                                  \r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 48, in run                \r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))                                                            \r\n  File \"/root/models/inception/bazel-bin/inception/imagenet_train.runfiles/inception/inception/imagenet_train.py\", line 37, in main                                                                                                 \r\n    inception_train.train(dataset)                                                                                \r\n  File \"/root/models/inception/bazel-bin/inception/imagenet_train.runfiles/inception/inception/inception_train.py\", line 241, in train                                                                                              \r\n    scope, reuse_variables)                                                                                       \r\n  File \"/root/models/inception/bazel-bin/inception/imagenet_train.runfiles/inception/inception/inception_train.py\", line 114, in _tower_loss                                                                                        \r\n    inception.loss(logits, labels, batch_size=split_batch_size)                                                   \r\n  File \"/root/models/inception/bazel-bin/inception/imagenet_train.runfiles/inception/inception/inception_model.py\", line 128, in loss                                                                                               \r\n    weight=1.0)                                                                                                   \r\n  File \"/root/models/inception/bazel-bin/inception/imagenet_train.runfiles/inception/inception/slim/losses.py\", line 166, in cross_entropy_loss                                                                                     \r\n    cross_entropy = tf.contrib.nn.deprecated_flipped_softmax_cross_entropy_with_logits(                           \r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/lazy_loader.py\", line 53, in __getattr__    \r\n    module = self._load()                                                                                         \r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/lazy_loader.py\", line 42, in _load          \r\n    module = importlib.import_module(self.__name__)                                                               \r\n  File \"/usr/lib/python2.7/importlib/__init__.py\", line 37, in import_module                                      \r\n    __import__(name)                                                                                              \r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/__init__.py\", line 34, in <module>              \r\n    from tensorflow.contrib import image                                                                          \r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/image/__init__.py\", line 39, in <module>        \r\n    from tensorflow.contrib.image.python.ops.single_image_random_dot_stereograms import single_image_random_dot_stereograms                                                                                                         \r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/image/python/ops/single_image_random_dot_stereograms.py\", line 26, in <module>                                                                                    \r\n    \"_single_image_random_dot_stereograms.so\"))                                                                   \r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/util/loader.py\", line 55, in load_op_library    \r\n    ret = load_library.load_op_library(path)                                                                      \r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/load_library.py\", line 64, in load_op_library                                                                                                            \r\n    None, None, error_msg, error_code)                                                                            \r\ntensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/image/python/ops/_single_image_random_dot_stereograms.so: undefined symbol: _ZN6google8protobuf8internal10LogMessageC1ENS0_8LogLevelEPKci                                                       \r\n\r\n", "Here's the error:\r\n```\r\ntensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/image/python/ops/_single_image_random_dot_stereograms.so: undefined symbol: _ZN6google8protobuf8internal10LogMessageC1ENS0_8LogLevelEPKci\r\n(google::protobuf::internal::LogMessage::LogMessage(google::protobuf::LogLevel, char const*, int)\r\n)\r\n```\r\nIt looks like something is wrong in your installation. Perhaps tensorflow is not loaded or linking properly there. You could try to install the protobuf library on all slaves, which might result in some instability.\r\n/CC: @skye \r\n", "Happened to me when I was building latest tensorflow/serving.", "Is there someone fix this? \r\nBy the way,I used oracle jdk for my env.", "It happens randomly. I've encountered the similar issue. Just tried again and it worked!\r\n\r\ntensorflow.python.framework.errors_impl.NotFoundError: /home/serving/bazel-bin/tensorflow_serving/example/mnist_saved_model.runfiles/org_tensorflow/tensorflow/contrib/image/python/ops/_single_image_random_dot_stereograms.so: undefined symbol: _ZN6google8protobuf8internal10LogMessageC1ENS0_8LogLevelEPKci\r\n", "Any news?", "I find some reason:there is a env is not setted correct by the script.\n\n_Sent from my Xiaomi MIX using [FastHub](https://play.google.com/store/apps/details?id=com.fastaccess.github)_", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 9619, "title": "feed input node (which is a sentence) using Tensor Flow Inference Interface", "body": "hi all,\r\n\r\nunfortunately there is little stuff when we are going to link the tensorflow model to android\r\nthis is the thing I am doing\r\nI freezed and optimized tensorflow model\r\nIt is a RNN lstm model , getting a sentence then saying if that positive or negative\r\nI successfully added all the things in android studio\r\nNow I have a editbox to enter my sentence , then touch Run button, then this run button should fill the label if this sentence was positive or negative,\r\n\r\nbut dont know how to feed input node when that is sentence, \r\neven didnt find any document to do so,\r\n\r\nhope someone kindly help me\r\n\r\nI did  something like this but is incorrect\r\n`protected void onCreate(Bundle savedInstanceState) {\r\n    super.onCreate(savedInstanceState);\r\n    setContentView(R.layout.activity_main);\r\n\r\n    inferenceInterface = new TensorFlowInferenceInterface();\r\n    inferenceInterface.initializeTensorFlow(getAssets(), MODEL_FILE);\r\n\r\n\r\n    final Button button = (Button) findViewById(R.id.button);\r\n\r\n    button.setOnClickListener(new View.OnClickListener() {\r\n        public void onClick(View v) {\r\n\r\n            final EditText editNum1 = (EditText) findViewById(R.id.editNum1);\r\n\r\n            inferenceInterface.fillNodeInt(INPUT_NODE, INPUT_SIZE, editNum1);\r\n\r\n            inferenceInterface.runInference(new String[] {OUTPUT_NODE});\r\n\r\n            int[] resu = {0, 0};\r\n            inferenceInterface.readNodeInt(OUTPUT_NODE, resu);\r\n\r\n            final TextView textViewR = (TextView) findViewById(R.id.txtViewResult);\r\n            textViewR.setText(Float.toString(resu[0]) + \", \" + Float.toString(resu[1]));\r\n        }\r\n    });\r\n\r\n}`\r\nthe problem is I couldnt find any method in TensorFlowInferenceInterface that gives the string as input they were int or float or byte\r\n\r\nappreciate you help\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "It does not make sense when there is no documentation for something limiting us to tell it in one community.\r\n\r\nanyway I had already send in stackover\r\n\r\nhttps://stackoverflow.com/questions/43752756/how-to-feed-input-node-which-is-a-sentence-in-android-studio-using-tensorflowi\r\n\r\nno one answered, In this case what should I do?\r\nplease let me know", "There are a little differences in using android interface API between tensorflow version.\r\nIf you go with old version (build num. 44 on Feb. 17, 2017) the code for feeding, running will be exactly as you did\r\nBut you have new version (build num 164 on June 15, 2017) the code should be like this:\r\nRefer this videos for comprehensive info.\r\nhttps://www.youtube.com/watch?v=SSGp06uFhhU&t=25s\r\nhttps://www.youtube.com/watch?v=_gVn0vpkHYg&t=25s\r\n\r\nbtnProcess.setOnClickListener(new View.OnClickListener() {\r\n            @Override\r\n            public void onClick(View v) {\r\n                float[] outputs = new float[2];\r\n                String str;\r\n                float[] pixels = getPixelData(bitmap);\r\n                // feed\r\n                inferenceInterface.feed(INPUT_NODE, pixels, 1, IMAGE_SIZE, IMAGE_SIZE, 3);\r\n                //run\r\n                inferenceInterface.run(new String[] {OUTPUT_NODE});\r\n                // fetch\r\n                inferenceInterface.fetch(OUTPUT_NODE, outputs);\r\n                if (outputs[0] > outputs[1])\r\n                    str = \"fake\";\r\n                else\r\n                    str = \"live\";\r\n                String str1 = \"fake: \" + Float.toString(outputs[0]*100) + \"  live: \" + Float.toString(outputs[1]*100);\r\n                result1.setTextColor(Color.BLUE);\r\n                result2.setTextColor(Color.BLUE);\r\n                result1.setText(str1);\r\n                result2.setText(\"This is \" + str + \" fingerprint.\");\r\n            }\r\n        });", "@saria85  Did you get any solution to this? I am trying to do the same thing as yours.\r\n", "Did you find the solution for this problem? I have a same problem but I can't find any solution", "I'm having the same problem as well. \r\n", "This works for me to feed the string using inferenceInterface.feedString with the string converted to byte array. If the output is String as well, it can be fetched through ByteBuffer and converted to String afterwards. \r\n\r\n", "@Nothing9x @longdt219 @saria85 \r\n\r\nIt worked for me:\r\nimport android.content.res.AssetManager;\r\nimport org.tensorflow.contrib.android.TensorFlowInferenceInterface;\r\nprivate AssetManager assetManager;\r\nprivate TensorFlowInferenceInterface inferenceInterface;\r\nprivate static final String MODEL_FILE = \"**name of the pb file along with extension**\"\r\ninferenceInterface = new TensorFlowInferenceInterface(getAssets(), MODEL_FILE);\r\n\r\nAfter this you can use the feed run and fetch commands:\r\n\r\ninferenceInterface.feed(.....)\r\ninferenceInterface.run(.....)\r\ninferenceInterface.fetch(........)"]}, {"number": 9618, "title": "Can't load the model file from model_dir in CNN text classifier", "body": "Dear all,\r\nI am working on NLP using tensor flow, i am using thhe text classifier using cnn from the tensor flow git hub example, where i stored my trained model by passing \"model_dir\" argument successfully , but when i try to reload the model it says an error\r\nError occurring part of my code is, \r\n\r\n\"\r\nclassifier = learn.Estimator(model_fn=cnn_model,model_dir = \"/home/local/models/\")\r\n#classifier.fit(x_train, y_train, steps=1000)\r\ny_predicted = [p['class'] for p in classifier.predict(x_test, as_iterable=True)]\r\nprint(y_predicted)\r\n\"\r\nthis is a code when i try to reload the model file files from the \"model_dir\" which is already stored in it when train.\r\n\r\nThe error is ,\r\n\r\n\"\r\nTraceback (most recent call last):\r\n  File \"CNN_Text.py\", line 114, in <module>\r\n    y_predicted = [p['class'] for p in classifier.predict(x_test, as_iterable=True)]\r\n  File \"/home/local/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py\", line 281, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/local/tensorflow/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 565, in predict\r\n    as_iterable=as_iterable)\r\n  File \"/home/local/tensorflow/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 857, in _infer_model\r\n    infer_ops = self._get_predict_ops(features)\r\n  File \"/home/local/tensorflow/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1188, in _get_predict_ops\r\n    return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.INFER)\r\n  File \"/home/local/tensorflow/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1103, in _call_model_fn\r\n    model_fn_results = self._model_fn(features, labels, **kwargs)\r\n  File \"CNN_Text.py\", line 31, in cnn_model\r\n    target = tf.one_hot(target, 2, 1, 0)\r\n  File \"/home/local/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 2156, in one_hot\r\n    name)\r\n  File \"/home/local/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1848, in _one_hot\r\n    axis=axis, name=name)\r\n  File \"/home/local/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 509, in apply_op\r\n    (input_name, err))\r\nValueError: Tried to convert 'indices' to a tensor and failed. Error: None values not supported.\r\n\"\r\n\r\nPlease help me to solve this problem.\r\nThank you in advance.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9617, "title": "Add input function for training and testing", "body": "Estimator is decoupled from Scikit Learn interface by moving into separate class SKCompat. Arguments x, y and batch_size are only available in the SKCompat class, Estimator will only accept input_fn", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please", "Seems like an unrelated failure, so merging.", "I should have checked but this got pushed to r1.1 -- you may want to send the PR to the master branch if you want this to persist in the future.", "Thanks for going ahead and also merging into master, @vrv!"]}, {"number": 9616, "title": "NameError: name' core' is not defined !", "body": "When I try to : import tensorflow as tf\r\nthis error appears:\r\n\r\nNameError: name' core' is not defined !\r\n\r\nWhat should I do?", "comments": ["Please fill in the template with version, how you installed, etc.", "Closing due to lack of activity. Please reopen if this is still an issue."]}, {"number": 9615, "title": "Added options for linking to libraries that are present on system", "body": "Added options for linking dynamically to libraries that are present on local system, instead of downloading them (such as libpng, libjpeg, zlib, ...).\r\nThis reduces the file size of the shared library libtensorflow.so and adds some security since the libraries such as libpng, libjpeg may be updated independently, without having to rebuild them as part of a tensorflow build from source.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Can one of the admins verify this patch?", "@martinwicke Do we know who the best person would be to review a nontrivial cmake build change? If I remember correctly, our friends at Microsoft are big stakeholders in this build?", "By default, @mrry could take a look.\n", "Ping for @jzuern  !", "@jzuern when do you think you'll be able to finish this?", "Hi @jzuern, this pull request hasn't been updated in a while. Closing it out as per our policy. Feel free to reopen if you'd like to update the change."]}, {"number": 9614, "title": "Tensorboard not fully functional behind nginx proxy", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes. The ngnix configuration to proxy tensorboard from port 6006 to port 80. Details below.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\n('v1.0.1-2-g250e72c-dirty', '1.0.1')\r\n- **Bazel version (if compiling from source)**:\r\nBuild label: 0.4.5\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Mar 16 12:19:38 2017 (1489666778)\r\nBuild timestamp: 1489666778\r\nBuild timestamp as int: 1489666778\r\n- **CUDA/cuDNN version**:\r\n8.0.61/cudnn-8.0-linux-x64-v5.1\r\n- **GPU model and memory**:\r\n\"TITAN X (Pascal) and 12187 MBytes\r\n- **Exact command to reproduce**:\r\ntensorboard  --logdir /tmp/retrain_logs/\r\n\r\n### Describe the problem\r\nI follow the image retraining tutorial  [https://www.tensorflow.org/tutorials/image_retraining]. As we have different applications providing a webinterface and a firewall that blocks all ports except 22(ssh) and 80(http)/443(https) we want to put all of them behind a nginx proxy. \r\nWith the config below the tensorboard shows up, however no data is displayed.\r\nWhen disabling all other applications / services that run on port 80 and forcing tensorboard to deliver on port 80 with `sudo tensorboard --port 80 --logdir /tmp/retrain_logs` the data graphs (ie. accuracy_1, cross_entropy_1, etc.) are displayed. \r\n\r\n### Source code / logs\r\n`server {\r\n        listen 80 default_server;\r\n        listen [::]:80 default_server;\r\n\r\n        root /var/www/html;\r\n\r\n        # Add index.php to the list if you are using PHP\r\n        index index.html index.htm index.nginx-debian.html;\r\n\r\n        server_name _;\r\n\r\n        location / {\r\n                # First attempt to serve request as file, then\r\n                # as directory, then fall back to displaying a 404.\r\n                try_files $uri $uri/ =404;\r\n        }\r\n        location /tensorflow {\r\n                rewrite ^/tensorflow(.*) /$1 break;\r\n                proxy_pass http://127.0.0.1:6006/;\r\n        }\r\n}\r\n`\r\n\r\n\r\n", "comments": ["It's worked for me in the past when I configured it as follows:\r\n\r\n```nginx\r\nserver {\r\n  listen 80 default_server;\r\n  listen [::]:80 default_server ipv6only=on;\r\n\r\n  location / {\r\n    proxy_http_version 1.1;\r\n    proxy_pass http://127.0.0.1:6006;\r\n  }\r\n}\r\n```\r\n\r\nSince you're chrooting the app, you need to take into consideration that asset and XHR requests get sent to a whole bunch of other endpoints (e.g. /data, /runs, etc.) using absolute paths.\r\n\r\nRefactoring TensorBoard to make all asset requests relative, and then maybe rooting XHRs under a single prefix like `/_/tensorboard/` would be nice (CC: @dandelionmane) but also a nontrivial undertaking.\r\n\r\nI would recommend creating a separate hostname for TensorBoard. For example, say `http://tensorboard.you.com` rather than `http://you.com/tensorboard`. Then configure a virtual host in nginx.\r\n\r\nYou wouldn't by any chance be building a productionized service for TensorBoard? Because if that's something you're interested in having, we should talk more.", "@jart, your nginx configuration worked for me.", "Closing since it sounds like Justine's solution works.", "@decentralion I'd like to re-open this issue since we have a use case where it is not possible to deploy tensorboard on `/`. \r\n\r\nWe are deploying several services behind a single DNS name due to certificate signing and other restrictions. We want to be able to add tensorboard on this proxy at some sub-path like `<my-host>/tensorboard` but the static paths break this. Seems like many other apps already support this such as Jupyter's baseUrl: https://jupyter-notebook.readthedocs.io/en/stable/config.html", "@decentralion  we also have similar issue since we try to setup a tensorflow lab environment on kubernetes. \r\nOur use case is use kubernetes Ingress for user to access jupyter/tensorboard from a url having sub-path.  For Jupyter, we use baseUrl mentioned by @jlzhao27 to do this, but we can't do this for tensorboard. ", "Since this is a top Google result for \"tensorboard reverse proxy\", let it be known that tensorboard now has a `--path_prefix=/whatever/you/want/tensorboard` argument for serving at a non-root URL.", "Also important to note - that I was using apache reverse  proxy to send `/tensorboard` to the (tensorboard at port) 6006. I passed `--path_prefix=/tensorboard` to tensorboard to run it, but I ultimatley had to connect to `http://1.2.3.4/tensorboard/tensorboard/` to actually get it to work. (Notice that \"tensorboard\" is specified *twice* and there is a trailing slash on the end of the URL!)\r\n\r\nMy Apache config was as folllows:\r\n\r\n```\r\nProxyPass \"/tensorboard\"  \"http://127.0.0.1:6006/\"\r\nProxyPassReverse \"/tensorboard\"  \"http://127.0.0.1:6006/\"\r\n```\r\n"]}, {"number": 9613, "title": "ValueError:tensorflow", "body": "when I'm trying to run the tensorflow odel with my data generating an error as:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/tensorflow/models/summarization/bazel-bin/textsum/seq2seq_attention.runfiles/__main__/textsum/seq2seq_attention.py\", line 214, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/home/ubuntu/tensorflow/models/summarization/bazel-bin/textsum/seq2seq_attention.runfiles/__main__/textsum/seq2seq_attention.py\", line 209, in main\r\n    decoder = seq2seq_attention_decode.BSDecoder(model, batcher, hps, vocab)\r\n  File \"/home/ubuntu/tensorflow/models/summarization/textsum/seq2seq_attention_decode.py\", line 95, in __init__\r\n    self._model.build_graph()\r\n  File \"/home/ubuntu/tensorflow/models/summarization/textsum/seq2seq_attention_model.py\", line 296, in build_graph\r\n    self._add_seq2seq()\r\n  File \"/home/ubuntu/tensorflow/models/summarization/textsum/seq2seq_attention_model.py\", line 224, in _add_seq2seq\r\n    tf.log(tf.nn.softmax(model_outputs[-1])), hps.batch_size*2)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py\", line 1970, in top_k\r\n    return gen_nn_ops._top_kv2(input, k=k, sorted=sorted, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2462, in _top_kv2\r\n    name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2397, in create_op\r\n    set_shapes_for_outputs(ret)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1757, in set_shapes_for_outputs\r\n    shapes = shape_func(op)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1707, in call_with_requiring\r\n    return call_cpp_shape_fn(op, require_shape_fn=True)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py\", line 610, in call_cpp_shape_fn\r\n    debug_python_shape_fn, require_shape_fn)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py\", line 675, in _call_cpp_shape_fn_impl\r\n    raise ValueError(err.message)\r\nValueError: input must have last dimension >= k = 128 but is 102 for 'seq2seq/decode_output/TopKV2' (op: 'TopKV2') with input shapes: [64,102], [].\r\n", "comments": ["Please fill out [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new), as this isn't enough information to help you. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 9612, "title": "Problems on the doc of install cudnn on Ubuntu and OS X", "body": "The [docs on install CUDA on Ubuntu as well as OS X](https://www.tensorflow.org/install/install_linux) says that we need to set CUDA_HOME as described in the NVIDIA documentation as following, however, I cannot find any NVIDIA official documentation on how to set CUDA_HOME, could anyone provide this doc so that we can make the doc better?\r\n\r\n>  Ensure that you create the CUDA_HOME environment variable as described in the NVIDIA documentation.\r\n\r\n\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Have you tried installing CUDA via the Nvidia provided installers and just proceeded in the installation? I never had issues with this.", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9611, "title": "Update Link", "body": "Very similar as [PR 9598](https://github.com/tensorflow/tensorflow/pull/9598), fix all outdated links of https://www.tensorflow.org/code/tensorflow/contrib/deprecated/__init__.py.", "comments": ["Can one of the admins verify this patch?"]}]