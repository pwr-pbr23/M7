[{"number": 38199, "title": "tf.keras.losses.categorical_hinge mentions [-1, 1] values while it works with one-hot-encoded tensor", "body": "Hi!\r\n\r\nPlease see:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/losses/categorical_hinge  \r\nhttps://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/losses.py#L866-L882\r\n\r\nBoth mention:\r\n> y_true: The ground truth values. y_true values are expected to be -1 or 1. If binary (0 or 1) labels are provided they will be converted to -1 or 1.\r\n\r\nWhile the code is --as expected-- a transcription of keras' one:\r\n```\r\n# ...\r\ny_pred = ops.convert_to_tensor(y_pred)\r\ny_true = math_ops.cast(y_true, y_pred.dtype)\r\npos = math_ops.reduce_sum(y_true * y_pred, axis=-1)\r\nneg = math_ops.reduce_max((1. - y_true) * y_pred, axis=-1)\r\nreturn math_ops.maximum(0., neg - pos + 1.)\r\n```\r\n\r\nAnd this code is meant to work with one-hot-encoded tensors. See the original discussion here: https://github.com/keras-team/keras/issues/2830", "comments": ["I'll start working on this, Thanks!", "@FrankwaP The nightly version of docs mention \r\n`y_true : The ground truth values. y_true values are expected to be 0 or 1.`", "Maybe \"y_true : The ground truth values in one-hot-encoded format.\" would add some clarity?", "@FrankwaP CAn you be clear on what you are describing here. Thanks!", "Oh sorry!\r\n\r\nThe original/binary hinge loss expects a (n_samples,1) size vector for y_true, with -1 or 1 values. So it's a form of label encoding.\r\n\r\nThe categorical hinge loss has been adapted for N>2 classes, and now use a one-hot-encoded (n_samples,n_classes) size tensor for y_true.\r\n\r\nSo I think that specifying that we need a one-hot-encoded tensor is more explicit that specifying that the values should be 0 or 1, especially for people who also knows the binary hinge loss."]}, {"number": 38198, "title": "TF 2.1: inserting into MutableHashTable results into error", "body": "**System information** \r\n- Have written custom code\r\n- OS Platform and Distribution: Linux Ubuntu 18.04: \r\n- TensorFlow installed from binary: `pip install tensorflow-gpu==2.1.0` \r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 10/7.0\r\n- GPU model and memory: GTX1070 and 6GB\r\n\r\n**Describe the current behavior**\r\nI am trying to insert some key:value pairs into a MutableHashTable\r\n\r\n**Describe the expected behavior**\r\nUsing the contrib equivalent of MutableHashTable does not produce this error.\r\n\r\n\r\n**Standalone code to reproduce the issue** \r\n```import tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\n\r\nCHARMAP = ['', '', ''] + list('0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ')\r\n\r\nwith tf.device('/gpu:0'):\r\n    table = tf.raw_ops.MutableHashTable(\r\n            key_dtype=tf.int64,\r\n            value_dtype=tf.string,\r\n    )\r\n\r\ninsert = table.insert(tf.constant(list(range(len(CHARMAP))), dtype=tf.int64),\r\n                     tf.constant(CHARMAP)\r\n                     )\r\n```\r\n\r\n**Other info / logs** \r\nHere is the output:\r\n\r\n```Traceback (most recent call last):\r\n File \"test.py\", line 12, in <module>\r\n    insert = table.insert(tf.constant(list(range(len(CHARMAP))), dtype=tf.int64),\r\nAttributeError: 'Tensor' object has no attribute 'insert'\r\n```", "comments": ["Was able to reproduce the issue with Tf2.1.\r\nPleas find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/fddfec4fe89b58d863090f06a6e6691a/untitled490.ipynb). Thanks", "tf.raw_ops.MutableHashTable returns a tensor handle corresponding to the table that then needs to be used in an init_op / insert op etc.\r\n\r\nI think what you want is tf.lookup.experimental.DenseHashTable https://www.tensorflow.org/api_docs/python/tf/lookup/experimental/DenseHashTable\r\n\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\n\r\nCHARMAP = ['', '', ''] + list('0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ')\r\n\r\nwith tf.device('/gpu:0'):\r\n    table = tf.lookup.experimental.DenseHashTable(\r\n            key_dtype=tf.int64,\r\n            value_dtype=tf.string,\r\n            default_value='_',\r\n            empty_key=0,\r\n            deleted_key=-1,\r\n    )\r\n\r\ntable.insert(tf.constant(list(range(len(CHARMAP))), dtype=tf.int64),\r\n                     tf.constant(CHARMAP))\r\n\r\nPlease let me know if you have any issues.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38198\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38198\">No</a>\n"]}, {"number": 38197, "title": "Model not deterministic, even though os.environ['TF_DETERMINISTIC_OPS'] = '1' is set", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow):  Pretty much the MirroredStrategy fmnist example\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): tensorflow/tensorflow:2.2.0rc2-gpu-py3\r\n- TensorFlow installed from (source or\r\nbinary): tensorflow/tensorflow:2.2.0rc2-gpu-py3\r\n- TensorFlow version (use command below): tensorflow/tensorflow:2.2.0rc2-gpu-py3\r\n- Python version: tensorflow/tensorflow:2.2.0rc2-gpu-py3\r\n- CUDA/cuDNN version: tensorflow/tensorflow:2.2.0rc2-gpu-py3\r\n- GPU model and memory: 1050M\r\n\r\n**Describe the current behavior**\r\nModel is not deterministic/reproducible.\r\nTwo runs:\r\n```\r\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\r\n11493376/11490434 [==============================] - 2s 0us/step\r\nEpoch 1, Loss: 0.17844311892986298, Accuracy: 0.9466999769210815,Test Loss: 0.057941436767578125, Test Accuracy: 0.9815000295639038\r\nEpoch 2, Loss: 0.05286668613553047, Accuracy: 0.9836500287055969,Test Loss: 0.044471099972724915, Test Accuracy: 0.9853000044822693\r\nEpoch 3, Loss: 0.03694676235318184, Accuracy: 0.9883000254631042,Test Loss: 0.034947194159030914, Test Accuracy: 0.9897000193595886\r\nEpoch 4, Loss: 0.028592929244041443, Accuracy: 0.9910500049591064,Test Loss: 0.027234185487031937, Test Accuracy: 0.9907000064849854\r\nEpoch 5, Loss: 0.022629836574196815, Accuracy: 0.9927666783332825,Test Loss: 0.029115190729498863, Test Accuracy: 0.9904000163078308\r\nEpoch 6, Loss: 0.0172086451202631, Accuracy: 0.9944999814033508,Test Loss: 0.027797872200608253, Test Accuracy: 0.9902999997138977\r\nEpoch 7, Loss: 0.013981950469315052, Accuracy: 0.9956499934196472,Test Loss: 0.02764272689819336, Test Accuracy: 0.9909999966621399\r\nEpoch 8, Loss: 0.01210874691605568, Accuracy: 0.9961333274841309,Test Loss: 0.035009630024433136, Test Accuracy: 0.9896000027656555\r\nEpoch 9, Loss: 0.008961305022239685, Accuracy: 0.9971666932106018,Test Loss: 0.034057389944791794, Test Accuracy: 0.9905999898910522\r\nEpoch 10, Loss: 0.00800476036965847, Accuracy: 0.9972166419029236,Test Loss: 0.033878158777952194, Test Accuracy: 0.9900000095367432\r\nGPU Run Time: 70.80781483650208 seconds\r\n```\r\n\r\n```\r\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\r\n11493376/11490434 [==============================] - 2s 0us/step\r\nEpoch 1, Loss: 0.1761329025030136, Accuracy: 0.9478499889373779,Test Loss: 0.05224931612610817, Test Accuracy: 0.9835000038146973\r\nEpoch 2, Loss: 0.05251472815871239, Accuracy: 0.9836666584014893,Test Loss: 0.04059470072388649, Test Accuracy: 0.9860000014305115\r\nEpoch 3, Loss: 0.03771379590034485, Accuracy: 0.98785001039505,Test Loss: 0.03189479187130928, Test Accuracy: 0.9894000291824341\r\nEpoch 4, Loss: 0.027971116825938225, Accuracy: 0.9912333488464355,Test Loss: 0.03176414594054222, Test Accuracy: 0.9890000224113464\r\nEpoch 5, Loss: 0.022653400897979736, Accuracy: 0.9925000071525574,Test Loss: 0.03643624112010002, Test Accuracy: 0.9876999855041504\r\nEpoch 6, Loss: 0.01727919466793537, Accuracy: 0.9942166805267334,Test Loss: 0.02887595444917679, Test Accuracy: 0.9901000261306763\r\nEpoch 7, Loss: 0.01397143118083477, Accuracy: 0.9957500100135803,Test Loss: 0.03118096850812435, Test Accuracy: 0.9905999898910522\r\nEpoch 8, Loss: 0.01202292088419199, Accuracy: 0.9961333274841309,Test Loss: 0.03164077177643776, Test Accuracy: 0.9909999966621399\r\nEpoch 9, Loss: 0.008715414442121983, Accuracy: 0.9971333146095276,Test Loss: 0.04146642982959747, Test Accuracy: 0.9896000027656555\r\nEpoch 10, Loss: 0.008586470037698746, Accuracy: 0.9969000220298767,Test Loss: 0.033046264201402664, Test Accuracy: 0.9902999997138977\r\nGPU Run Time: 72.08828902244568 seconds\r\n```\r\n\r\n**Describe the expected behavior**\r\nI expect the model to be reproducible with the same loss, accuracy etc\r\n**Standalone code to reproduce the issue** \r\n```\r\n#!/usr/bin/env python \r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport argparse\r\nimport time\r\nimport random\r\nimport os\r\n\r\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D\r\nfrom tensorflow.keras import Model\r\n\r\n\r\ndef random_seed(seed):\r\n    os.environ['PYTHONHASHSEED'] = str(seed) # Python general\r\n    np.random.seed(seed)\r\n    random.seed(seed) # Python random\r\n    tf.random.set_seed(seed)\r\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\r\n\r\n# Not yet using click due to Docker issues\r\nparser = argparse.ArgumentParser(description='Tensorflow entry point')\r\nparser.add_argument('--epochs', type=int, default=10)\r\nparser.add_argument('--seed', type=int, default=0)\r\nargs = parser.parse_args()\r\n\r\n# Detect GPUs\r\nprint(f'Num GPUs Available: {len(tf.config.experimental.list_physical_devices(\"GPU\"))}')\r\n\r\n# Load MNIST\r\nmnist = tf.keras.datasets.mnist\r\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\r\n\r\n# Adding a dimension to the array -> new shape == (28, 28, 1), since the first layer in our model is a convolutional\r\n# layer and it requires a 4D input (batch_size, height, width, channels).\r\n# batch_size dimension will be added later on.\r\ntrain_images = train_images[..., None]\r\ntest_images = test_images[..., None]\r\n\r\n# Normalizing the images to [0, 1] range.\r\ntrain_images = train_images / np.float32(255)\r\ntest_images = test_images / np.float32(255)\r\n\r\n# Use MirroredStrategy for multi GPU support\r\n# If the list of devices is not specified in the`tf.distribute.MirroredStrategy` constructor, it will be auto-detected.\r\nstrategy = tf.distribute.MirroredStrategy()\r\n\r\nBUFFER_SIZE = len(train_images)\r\nBATCH_SIZE_PER_REPLICA = 64\r\nGLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\r\n\r\n# Batch and distribute data\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE) \r\ntest_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE) \r\ntrain_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\r\ntest_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)\r\n\r\n# Fix seeds\r\nrandom_seed(0)\r\n\r\n# Define model\r\ndef create_model():\r\n    model = tf.keras.Sequential([\r\n    tf.keras.layers.Conv2D(32, 3, activation='relu'),\r\n    tf.keras.layers.MaxPooling2D(),\r\n    tf.keras.layers.Conv2D(64, 3, activation='relu'),\r\n    tf.keras.layers.MaxPooling2D(),\r\n    tf.keras.layers.Flatten(),\r\n    tf.keras.layers.Dense(64, activation='relu'),\r\n    tf.keras.layers.Dense(10)\r\n    ])\r\n\r\n    return model\r\n\r\n# Define Loss and accuracyc metrics\r\nwith strategy.scope():\r\n    # Set reduction to `none` so reduction can be done afterwards and divide by global batch size.\r\n    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\r\n        from_logits=True,\r\n        reduction=tf.keras.losses.Reduction.NONE)\r\n    def compute_loss(labels, predictions):\r\n        per_example_loss = loss_object(labels, predictions)\r\n\r\n        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\r\n\r\n    test_loss = tf.keras.metrics.Mean(name='test_loss')\r\n\r\n    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\r\n    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\r\n\r\n\r\n# Define model, optimizer, training- and test step\r\nwith strategy.scope():\r\n  model = create_model()\r\n  optimizer = tf.keras.optimizers.Adam()\r\n\r\n  def train_step(inputs):\r\n    images, labels = inputs\r\n\r\n    with tf.GradientTape() as tape:\r\n        predictions = model(images, training=True)\r\n        loss = compute_loss(labels, predictions)\r\n\r\n    gradients = tape.gradient(loss, model.trainable_variables)\r\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n    train_accuracy.update_state(labels, predictions)\r\n\r\n    return loss \r\n\r\n  def test_step(inputs):\r\n    images, labels = inputs\r\n\r\n    predictions = model(images, training=False)\r\n    t_loss = loss_object(labels, predictions)\r\n    test_loss.update_state(t_loss)\r\n    test_accuracy.update_state(labels, predictions)\r\n\r\n\r\nwith strategy.scope():\r\n  # `run` replicates the provided computation and runs it with the distributed input.\r\n  @tf.function\r\n  def distributed_train_step(dataset_inputs):\r\n    per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))\r\n    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\r\n \r\n  @tf.function\r\n  def distributed_test_step(dataset_inputs):\r\n    return strategy.run(test_step, args=(dataset_inputs,))\r\n\r\n  gpu_runtime = time.time()\r\n  for epoch in range(args.epochs):\r\n    # TRAIN LOOP\r\n    total_loss = 0.0\r\n    num_batches = 0\r\n    for dist_dataset in train_dist_dataset:\r\n      total_loss += distributed_train_step(dist_dataset)\r\n      num_batches += 1\r\n    train_loss = total_loss / num_batches\r\n\r\n    # TEST LOOP\r\n    for dist_dataset in test_dist_dataset:\r\n      distributed_test_step(dist_dataset)\r\n\r\n    print(f'Epoch {epoch + 1}, Loss: {train_loss}, Accuracy: {train_accuracy.result()},'\r\n          f'Test Loss: {test_loss.result()}, Test Accuracy: {test_accuracy.result()}')\r\n\r\n    # Reset states\r\n    test_loss.reset_states()\r\n    train_accuracy.reset_states()\r\n    test_accuracy.reset_states()\r\n\r\n  print(f'GPU Run Time: {str(time.time() - gpu_runtime)} seconds')\r\n```\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\ndef random_seed(seed):\r\n    os.environ['PYTHONHASHSEED'] = str(seed) # Python general\r\n    np.random.seed(seed)\r\n    random.seed(seed) # Python random\r\n    tf.random.set_seed(seed)\r\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\r\n```\r\nI guess this should cover everything?\r\n\r\nThe code is currently running on a SINGLE GPU, even though I'm planning to run it on several GPUs.", "comments": ["@Zethson \r\ni ran the code shared by you and face the error in [this gist](https://colab.sandbox.google.com/gist/Saduf2019/2877c126568248d87c939427a38cf752/untitled128.ipynb)", "```\r\nipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-cafa3e81-09ee-4d77-8548-81b49eb3128e.json\r\n\r\nAn exception has occurred, use %tb to see the full traceback.\r\n\r\nSystemExit: 2\r\n```\r\n\r\nI don't see how this error is related to the code? Seems to be a jupyter notebook kernel issue, no?\r\n", "@duncanriach Any ideas what could be going wrong here?", "> @duncanriach Any ideas what could be going wrong here?\r\n\r\nWill take a look at this, hopefully today. Feel free to assign it to me, @sanjoy.", "I'm now actively working on this issue ...", "Hey @Zethson, I repo'd your issue and found a solution. To get determinism, you need to do the following:\r\n\r\nIn both calls to `shuffle`, you should:\r\n\r\n1. set `seed=123` (any integer)\r\n2. set `reshuffle_each_iteration=False`\r\n\r\n`tf.data.Dataset` re-shuffling (which is the `shuffle` default and causes a re-shuffle before each epoch, including the first epoch) is currently not reproducible when used in conjunction with `tf.distribute.MirroredStrategy` (or I suspect any `tf.distribute` strategy). This is a bug that I came across recently in another context, but I have not yet had a chance to dig in and root-cause it. @sanjoy, I'll try to create a simple, direct test that demonstrates that the re-shuffle is not reproducible in the context of `tf.distribute.MirroredStrategy` (even with a single GPU) and possibly open a new issue for that.\r\n\r\nAlso, note that the code given in the original comment is almost the same as what's provided for [Custom training with tf.distribute.Strategy](https://www.tensorflow.org/tutorials/distribute/custom_training) except that everything from `def train_step()` onwards is, unnecessarily, in the strategy scope.", "@duncanriach \r\nThanks!\r\nI will try the approach in the next ~ 10 days and report back.\r\nI also had reproduciblity issues with the CPU, do you expect them to be related to the dataset shuffling? ", "> I also had reproducibility issues with the CPU, do you expect them to be related to the dataset shuffling?\r\n\r\nYou're welcome. Yes, with these changes you should see the CPU training become reproducible as well. (Let me know the outcome of that.) The sources of non-determinism that we are addressing here are not related to the ops and therefore not related to which type of processor the ops are running on. Therefore, `TF_DETERMINISTIC_OPS` was necessary but not sufficient for determinism.", "@duncanriach \r\nThanks! This does indeed solve the reproducibility issue.\r\nHowever, it looks to me like it is only reproducible on the same system. \r\nI am running the whole code inside a Docker container and I get reproducible results on two of my systems. However, the results inbetween the two systems is not the same.\r\n\r\nIs this to be expected? If yes, what is the reason for this behavior?", "To add some numbers:\r\n\r\nSystem 1:\r\nRun 1:\r\n```\r\nEpoch 1/5\r\n938/938 [==============================] - 13s 14ms/step - accuracy: 0.9492 - loss: 0.1709\r\nEpoch 2/5\r\n938/938 [==============================] - 11s 12ms/step - accuracy: 0.9727 - loss: 0.0899\r\nEpoch 3/5\r\n938/938 [==============================] - 11s 12ms/step - accuracy: 0.9768 - loss: 0.0816\r\nEpoch 4/5\r\n938/938 [==============================] - 11s 11ms/step - accuracy: 0.9785 - loss: 0.0729\r\nEpoch 5/5\r\n938/938 [==============================] - 10s 11ms/step - accuracy: 0.9797 - loss: 0.0731\r\n157/157 [==============================] - 2s 12ms/step - accuracy: 0.9861 - loss: 0.0562\r\nTest loss: 0.0561639703810215, Test Accuracy: 0.9861000180244446\r\n\r\n```\r\nRun 2:\r\n```\r\nEpoch 1/5\r\n938/938 [==============================] - 13s 14ms/step - accuracy: 0.9492 - loss: 0.1709\r\nEpoch 2/5\r\n938/938 [==============================] - 10s 11ms/step - accuracy: 0.9727 - loss: 0.0899\r\nEpoch 3/5\r\n938/938 [==============================] - 11s 11ms/step - accuracy: 0.9768 - loss: 0.0816\r\nEpoch 4/5\r\n938/938 [==============================] - 10s 11ms/step - accuracy: 0.9785 - loss: 0.0729\r\nEpoch 5/5\r\n938/938 [==============================] - 10s 11ms/step - accuracy: 0.9797 - loss: 0.0731\r\n157/157 [==============================] - 2s 12ms/step - accuracy: 0.9861 - loss: 0.0562\r\nTest loss: 0.0561639703810215, Test Accuracy: 0.9861000180244446\r\n```\r\n\r\nSystem 2:\r\nRun 1: \r\n```\r\nEpoch 1/5\r\n938/938 [==============================] - 17s 18ms/step - accuracy: 0.9484 - loss: 0.1709\r\nEpoch 2/5\r\n938/938 [==============================] - 13s 14ms/step - accuracy: 0.9738 - loss: 0.0882\r\nEpoch 3/5\r\n938/938 [==============================] - 13s 14ms/step - accuracy: 0.9771 - loss: 0.0787\r\nEpoch 4/5\r\n938/938 [==============================] - 13s 14ms/step - accuracy: 0.9789 - loss: 0.0764\r\nEpoch 5/5\r\n938/938 [==============================] - 13s 14ms/step - accuracy: 0.9793 - loss: 0.0707\r\n157/157 [==============================] - 3s 20ms/step - accuracy: 0.9848 - loss: 0.0672\r\nTest loss: 0.06723744422197342, Test Accuracy: 0.9847999811172485\r\n```\r\n\r\n\r\nRun 2:\r\n```\r\nEpoch 1/5\r\n938/938 [==============================] - 17s 18ms/step - accuracy: 0.9484 - loss: 0.1709\r\nEpoch 2/5\r\n938/938 [==============================] - 13s 14ms/step - accuracy: 0.9738 - loss: 0.0882\r\nEpoch 3/5\r\n938/938 [==============================] - 13s 14ms/step - accuracy: 0.9771 - loss: 0.0787\r\nEpoch 4/5\r\n938/938 [==============================] - 13s 14ms/step - accuracy: 0.9789 - loss: 0.0764\r\nEpoch 5/5\r\n938/938 [==============================] - 13s 14ms/step - accuracy: 0.9793 - loss: 0.0707\r\n157/157 [==============================] - 4s 22ms/step - accuracy: 0.9848 - loss: 0.0672\r\nTest loss: 0.06723744422197342, Test Accuracy: 0.9847999811172485\r\n\r\n\r\n```", "> Is this to be expected? If yes, what is the reason for this behavior?\r\n\r\nHey @Zethson, from the GPU standpoint, bit-exact reproducibility between two systems is only guaranteed if the hardware-software stack is the same. Any changes in the stack could lead to differences in the way the computation workload is partitioned for (massively) parallel processing. The change in this partitioning will inevitably lead to differences in the accumulation of floating-point rounding errors in the computations. You can learn more about this by watching my [GTC talk](http://bit.ly/determinism-in-deep-learning) on the topic.\r\n\r\nWhile a different version of anything in the hardware-software stack (e.g. different CUDA driver versions) could lead to slightly different results, you're most likely to see a difference if the GPU architecture is different, if the cuDNN version is different, or if the TensorFlow version is different. Since you're using the same container, I can infer that you're using the same versions of both cuDNN and TensorFlow on both machines. That leaves the GPU architecture. Does one of these machines contain a Pascal GPU and the other a Volta GPU perhaps? Please share the output from `nvidia-smi` on each of the machines.\r\n\r\nIt's also possible that there are hardware-software differences in the CPU-related stack that are introducing slightly different floating-point rounding errors, but that's less likely due to much less (or no) parallel computation on the CPU.", "Thank you very much for your detailed response. \r\nYour talk was already on my shortlist and I will absolutely be watching it. I'm highly interested in reproducible ML and I am sure that your talk will improve my understanding of the broader challenges.\r\n\r\nYes, the GPU architecture is very likely to be different. I am working on my own Laptop (1050M) and a VM (2 K80s).\r\nSystem 1: \r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.82       Driver Version: 440.82       CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1050    Off  | 00000000:01:00.0 Off |                  N/A |\r\n| N/A   45C    P0    N/A /  N/A |      0MiB /  4040MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n```\r\n\r\nSystem 2: \r\n```\r\nubuntu@mlflow ~> nvidia-smi\r\nTue Jun  2 08:57:48 2020       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           On   | 00000000:00:05.0 Off |                    0 |\r\n| N/A   27C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla K80           On   | 00000000:00:06.0 Off |                    0 |\r\n| N/A   24C    P8    26W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nSo judging from your answer I determine that for full reproducibility, the same GPU architecture has to be used. Hence, for a ML model (Say, a 'terrorist detection model' or a 'cancer detection model') to be verifiable and reproducible, we would not only need the same code (solved by git), the same environment (solved by containers), but also the same hardware (solved as long as the hardware exists)?\r\n\r\nNaive question: would it technically be possible to improve reproducibility (at the cost of precise model training) by decreasing the floating point precision and introducing a more eager rounding procedure?\r\n\r\nI am a bioinformatician and currently, the state of reproducibility of data analysis has dramatically improved with the introduction of [https://anaconda.org/bioconda/](containers) and workflow languages such as [Nextflow](https://www.nextflow.io/), which facilitate their usage. As a result, the results are not only fully reproducible, but also portable. Therefore, researchers can very easily verify the results, which is very important for the peer review process.\r\n\r\nNevertheless, I am aware of Nvidias efforts of speeding up the very computationally expensive bioinformatics analyses (and support that!) and fear that we may lose the portability, since the very same GPU architecture would be required.\r\n\r\nIf my last two paragraphs are off topic, then please tell me and I will remove them and would be happy to move the discussion elsewhere (if you are interested).", "I'm happy to discuss this here. I suspect that our discussion may be helpful to others. Thanks for all the additional information.\r\n\r\nThe GeForce GTX 1050 contains a GPU that is based on the Pascal architecture and the Tesla K80s contain GPUs that are based on the Kepler architecture. So bit-exact reproducibility is not guaranteed, and in fact unlikely, between those two machines based solely on the GPU architecture they use.\r\n\r\nHowever, a more significant factor is that you're doing multi-GPU training and on your laptop you have only a single GPU (one Kepler) while on the remote machine you have two GPUs (two Pascals). Because of the different number of GPUs, even if all those GPUs were from same architecture, you would definitely not get bit-exact reproducibility between the laptop and the remote machine.\r\n\r\nThe reason for this (again) is that the extensive floating-point computations are parallelized by being distributed in different ways on these two systems. This distribution in necessary and inherent in the process of maximally parallelizing (and therefore maximally accelerating) these computations. Computations that involve reducing the partial results from these compute partitions will include slightly different rounding errors depending on the way that the computation was partitioned. In the case of data-parallel multi-GPU (or multi-node) training, there is always going to be a reduction of the partial gradients produced on each of the GPUs (or nodes).\r\n\r\n--------\r\n\r\n> Would it technically be possible to improve reproducibility (at the cost of precise model training) by decreasing the floating point precision and introducing a more eager rounding procedure?\r\n\r\nYou're on the right track. Theoretically, there are four different possible ways around this that I can think of:\r\n\r\n### 1. Use integers\r\n\r\nWhile floating-point operations are not perfectly associative (rounding errors differ based on the order of operations), integers _are_ perfectly associative. Integers (e.g. [INT8](http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf)) can be used for inference, and they often are used because they result in increased performance and reduced memory footprint. However, integers cannot (currently) easily be used for training because both range and precision are required, especially in the gradients.\r\n\r\n### 2. Use double-precision floating-point (i.e. 64-bit floating point)\r\n\r\nThis will reduce the amount of floating-point rounding error that accumulates but there would still be a difference between GPU architectures and/or number of GPUs. This will also reduce performance a lot (at least 4x ?) and will at least double memory footprint. I've never trained a model with 64-bit integers through, and I don't know if it's possible in TensorFlow and whether the precision propagates all the way through, including through the back-prop. Based on my experience with TensorFlow's source code, I think it's very unlikely for typical cases.\r\n\r\n### 3. Quantize after training\r\n\r\nIt's not possible to train all the way through in regular floating-point and then convert to integer or a reduced-precision floating point format at the end to get (probably reduced-accuracy) between-stack reproducible training results (i.e. trainable variables) because it's fundamentally not possible to reproducibly quantize-away the accumulated error differences. This is challenging concept to understand or explain in text form, sorry.\r\n\r\n### 4. Final-train on CPU\r\n\r\nAnother option to think about is to report results from running on a single thread on a CPU. You would do all your development using the massive amount of acceleration provided by GPUs (or other accelerators) and then run once to get values to report. However, it's going to take a long time for that final run. Also, since the exact implementation of the underlying math can change on different CPUs (especially when using MKL), even when only using a single thread, you should still really include the CPU architecture that you used along with your results. Someone could run the same container and git repo hashes on a different CPU architecture and theoretically get slightly different results (just as with GPUs).\r\n\r\nI imagine that none of the above are feasible for your needs.\r\n\r\n--------\r\n\r\nWe now have run-to-run training reproducibility on GPUs in TensorFlow. This is a relatively new achievement. I, and others, are now working on extending this support.\r\n\r\nIn reality, I think it's going to be totally practical for you to provide bit-exact results for one or more GPU architectures (e.g. Pascal or Volta). In terms of reproducibility, as far as I am aware, this goes way further than the current state-of-the-art in the ML/DL research community. I recommend that you qualify your bit-exact results as being achieved on a given hardware-software version stack, including the type and number of GPUs used. Given the underlying technical constraints, this is a reasonable compromise.\r\n\r\nAnd with all of that said, it's important to remember that in most SGD-DL applications the amount of variance in the final result (e.g. test-set accuracy) is relatively small due to these differences in floating-point rounding error propagation. The advantage of a particular git hash running in a particular container image hash will, and should, attain most of the reproducibility demanded by peer-review. You can step-up the game even further by specifying the GPU architecture that the results were produced on.", "@duncanriach \r\nThank you very much for your amazing in depth answer!\r\nLearning a lot!\r\n\r\n> However, a more significant factor is that you're doing multi-GPU training and on your laptop you have only a single GPU (one Kepler) while on the remote machine you have two GPUs (two Pascals).\r\n\r\nYes, I was aware. Hence, I restricted my Docker container on my multi-GPU machine to only make a single of those 2 GPUs available. Both do of course show up when running `nvidia-smi`.\r\n\r\nI will now ensure that any of my pipelines will output the CPU and GPU architecture and will advocate this whenever appropriate.\r\n\r\n> And with all of that said, it's important to remember that in most SGD-DL applications the amount of variance in the final result (e.g. test-set accuracy) is relatively small due to these differences in floating-point rounding error propagation. \r\n\r\nAre you aware of any studies related to this? Any 'hard' numbers?\r\nIt would be interesting to know the fluctuation between the different network architectures and how close the found minima/maxima actually are.\r\nI could image that the overall performance will always be relatively similar, but it might be possible that (imagine a manifold) we end up in completely different optimal solutions with more or less the same loss, but quite different models.\r\n\r\nIt would be nice to able to have an expected variance between different GPU architectures.\r\nSay: Between Pascal and Kepler empirical studies suggest that a variance between 0.01% of the loss is to be expected, even when all reproducibility settings are used.\r\n\r\nCheers!", "> Hence, I restricted my Docker container on my multi-GPU machine to only make a single of those 2 GPUs available.\r\n\r\nGood job.\r\n\r\n> I will now ensure that any of my pipelines will output the CPU and GPU architecture and will advocate this whenever appropriate.\r\n\r\nGreat. Thanks.\r\n\r\n> Are you aware of any studies related to this? Any 'hard' numbers?\r\n\r\nNo, but it's on my roadmap to do this.\r\n\r\n> I could image that the overall performance will always be relatively similar, but it might be possible that (imagine a manifold) we end up in completely different optimal solutions with more or less the same loss, but quite different models.\r\n\r\nYes. When there is non-determinism, this can result in training randomly and non-reproducibly failing (or not doing as well). Luckily, mini-batch training has the effect of avoiding local minima and finding the global minimum. If there is non-reproducible gradient explosion or disappearance on one of the effectively infinite paths to that global minimum, however, then that can make debugging almost impossible.\r\n\r\nIn training regimes in which there is no negative feedback (or where there is actually positive feedback, as with reinforcement learning), non-determinism will lead to completely different results on every run.\r\n\r\nNote, and remember, that any system that does not have an end-to-end negative feedback loop can, and often will, amplify small differences in input to produce large differences in output.\r\n\r\nThese concepts apply, of course, to changing the hardware-software stack versioning and thereby potentially changing bit-accurate results, but it's less critical than run-to-run reproducibility (what we call determinism).\r\n\r\n> It would be nice to able to have an expected variance between different GPU architectures.\r\n\r\nSomething we plan to do sooner is to characterize the variance due to non-determinism for different model architectures on a given GPU architecture. This variance will have a similar order of magnitude to the variance between GPU architectures for that model architecture (it should be small).\r\n\r\nThe goal of deterministic operation of TensorFlow on GPUs is run-to-run determinism. What this then gives us, as a side-effect, is the ability to characterize the effect of changes to hardware-software stack versioning on model accuracy. Right now, however, there is a lot of work to be done to consolidate and broaden run-to-run reproducibility (in all DL frameworks).", "@duncanriach \r\nGreat. Thank you again! Learned a lot in this thread.\r\n\r\nI am looking forward to reading/hearing about your results of the variance of non-deterministic models. Very interested in this matter.\r\n\r\n> tf.data.Dataset re-shuffling (which is the shuffle default and causes a re-shuffle before each epoch, including the first epoch) is currently not reproducible when used in conjunction with tf.distribute.MirroredStrategy (or I suspect any tf.distribute strategy). This is a bug that I came across recently in another context, but I have not yet had a chance to dig in and root-cause it. @sanjoy, I'll try to create a simple, direct test that demonstrates that the re-shuffle is not reproducible in the context of tf.distribute.MirroredStrategy (even with a single GPU) and possibly open a new issue for that.\r\n\r\nI consider this issue solved, but we could also keep it open for your mentioned bug.\r\nWhatever you prefer.\r\n\r\n", "> Great. Thank you again! Learned a lot in this thread.\r\n\r\nYou're welcome, @Zethson. It's been a pleasure.\r\n\r\n> I am looking forward to reading/hearing about your results of the variance of non-deterministic models. Very interested in this matter.\r\n\r\nYou might want to star or watch https://github.com/NVIDIA/tensorflow-determinism because progress will be reported there first.\r\n\r\n> I consider this issue solved, but we could also keep it open for your mentioned bug.\r\n\r\nLet's keep this current issue open for now. Once I've opened a new issue, with minimal repro code for the re-shuffle problem, then I'll inform you. Then you can close this current issue.\r\n", "Update: TensorFlow version 2.3.0 no longer exhibits the nondeterminism associated using `tf.data.Dataset.shuffle(reshuffle_each_iteration=True)` with `tf.distribute.MirroredStrategy`. Therefore, another issue does not need to be created. @Zethson, please will you now close this current issue.", "@duncanriach \r\nThank you very much for the update!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38197\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38197\">No</a>\n"]}, {"number": 38196, "title": "TESLA T4 Graphics Card", "body": "I am planning to move to Nvidia Tesla T4 16gb from RTX 2080 8gb.\r\nI am working on yolo model with tensorflow.\r\nI am running around 8 parallel video detection simultaneously.\r\nCurrently,my present graphics card RTX 2080 8 gb supporting 8 parallel instances successfully.\r\nSo what through Tesla 12gb is supposed to give? How many parallel instances can be run on Tesla 12gb VRAM?\r\n\r\n\r\nPlease help me with this.", "comments": ["@SouradipBh \r\n\r\nThis question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!", "OK. But can u tell me how many instances will work on this GPU?\n\nOn Fri 3 Apr, 2020, 6:34 PM ravikyram, <notifications@github.com> wrote:\n\n> @SouradipBh <https://github.com/SouradipBh>\n>\n> This question is better asked on StackOverflow since it is not a bug or\n> feature request. There is also a larger community that reads questions\n> there and provide better and faster support for such issues. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/38196#issuecomment-608421656>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AL6QMGYTUV2WPELFLO2XTWTRKXNHRANCNFSM4L36KIOA>\n> .\n>\n", "> OK. But can u tell me how many instances will work on this GPU?\r\n\r\nIt is hard to know for sure, but usually these things do scale linearly with GPU memory as you'd expect (so you'd run 12 instances in parallel if you had 12G of memory).", "Is there any way to run minimum 30 instances in limited GPU Memory.\r\nBasically i want to run  parallel real time object detection simultaneously.\r\nCan u help me with this? I would be very grateful if you could help me.", "> Can u help me with this? I would be very grateful if you could help me.\r\n\r\nI personally cannot commit to giving one on one help on system design issues like this, and github issues are not a good forum for this either.  Please consider asking on StackOverflow or other such forums.", "Ok. Thanks"]}, {"number": 38195, "title": "GPU accelerated LSTM model crashes", "body": "**System information** \r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution (e.g.,Linux Ubuntu 16.04): Windows 10 pro 64 bit \r\n- Mobile device if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): pip \r\n- TensorFlow version (use command below): 2.1.0 \r\n- Python version: - 3.7.3\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: GeForce GTX 1050ti 4GB OC, driver version 441.22\r\n\r\n**Describe the current behavior**\r\n\r\nHi, I have this issue since yesterday. Before that, everything was working fine. I have not updated either the tensorflow version or the gpu driver or anything else for that matter in the last couple of days. This issue suddenly appeared just yesterday on its own. Below is my model,\r\n\r\n```\r\ndef neural_network(vocab_size, embedding_dim, max_length, train_padded, train_labels, validation_frac, num_epochs):\r\n    model = Sequential()\r\n    model.add(Embedding(vocab_size, embedding_dim, input_length = max_length))\r\n    model.add(Bidirectional(LSTM(64, return_sequences = True)))\r\n    model.add(GlobalAveragePooling1D())\r\n    model.add(Dropout(0.2))\r\n    model.add(Dense(50, activation = 'relu'))\r\n    model.add(Dropout(0.1))\r\n    model.add(Dense(1, activation = 'sigmoid'))\r\n    model.summary()\r\n    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\r\n    history = model.fit(train_padded, train_labels, epochs = num_epochs, verbose = 2, validation_split = validation_frac)\r\n    return model, history\r\n```\r\n\r\nI am pretty sure I am not running out of memory as previously I have trained the very same model with 10 times the parameters (~25M) compared to the parameters it has right now (~2M). Even the GPU usage barely exceeds 5%. I have GTX 1050ti 4GB. I have successfully run the above model, with varying number of total parameters, plenty of times before but only since yesterday this issue is coming up. Now the model runs fine only if I omit just the LSTM layer.\r\n\r\n```\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nembedding (Embedding)        (None, 200, 128)          1920000   \r\n_________________________________________________________________\r\nbidirectional (Bidirectional (None, 200, 128)          98816     \r\n_________________________________________________________________\r\nglobal_average_pooling1d (Gl (None, 128)               0         \r\n_________________________________________________________________\r\ndropout (Dropout)            (None, 128)               0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 50)                6450      \r\n_________________________________________________________________\r\ndropout_1 (Dropout)          (None, 50)                0         \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 1)                 51        \r\n=================================================================\r\nTotal params: 2,025,317\r\nTrainable params: 2,025,317\r\nNon-trainable params: 0\r\n```\r\n\r\nBelow is the exact error. \r\n\r\n```\r\nTrain on 143613 samples, validate on 15958 samples\r\nEpoch 1/5\r\nTraceback (most recent call last):\r\n\r\n  File \"C:\\Users\\admin\\Documents\\Machine Learning\\Projects\\Classification\\jigsaw-toxic-comment-classification-challenge\\toxic_classifier.py\", line 122, in <module>\r\n    model, history = neural_network(vocab_size, embedding_dim, max_length, train_padded, toxicity[col], validation_frac, num_epochs)\r\n\r\n  File \"C:\\Users\\admin\\Documents\\Machine Learning\\Projects\\Classification\\jigsaw-toxic-comment-classification-challenge\\toxic_classifier.py\", line 87, in neural_network\r\n    history = model.fit(train_padded, train_labels, epochs = num_epochs, verbose = 2, validation_split = validation_frac)\r\n\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 819, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 342, in fit\r\n    total_epochs=epochs)\r\n\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 128, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 98, in execution_function\r\n    distributed_function(input_fn))\r\n\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 599, in _call\r\n    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2363, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1611, in _filtered_call\r\n    self.captured_inputs)\r\n\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1692, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 545, in call\r\n    ctx=ctx)\r\n\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n\r\n  File \"<string>\", line 3, in raise_from\r\n\r\nInternalError:  [_Derived_]  Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] \r\n\t [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n\t [[StatefulPartitionedCall_1]]\r\n\t [[Reshape_14/_46]] [Op:__inference_distributed_function_5894]\r\n\r\nFunction call stack:\r\ndistributed_function -> distributed_function -> distributed_function\r\n```\r\n\r\nAlso, once this issue occurs, the kernel keeps crashing on its own _even if I am not compiling anything_. I am using Spyder IDE and even restarting the kernel does not help; it simply crashes after a few seconds. Below is the log for that (it is written on a red background)\r\n\r\n```\r\nAn error ocurred while starting the kernel\r\n2020\udae1\udea8\udae1\udea7 09:10:48.429373: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:24.630422: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:24.654232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:26:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\ncoreClock: 1.392GHz coreCount: 6 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020\udae1\udea8\udae1\udea7 10:25:24.655330: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:24.660386: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:24.665212: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:24.667487: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:24.672356: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:24.675255: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:24.685248: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:24.686442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020\udae1\udea8\udae1\udea7 10:25:24.687128: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2020\udae1\udea8\udae1\udea7 10:25:24.689769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:26:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\ncoreClock: 1.392GHz coreCount: 6 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020\udae1\udea8\udae1\udea7 10:25:24.690853: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:24.691406: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:24.691954: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:24.692497: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:24.693046: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:24.693600: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:24.694155: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:24.695292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020\udae1\udea8\udae1\udea7 10:25:25.261369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020\udae1\udea8\udae1\udea7 10:25:25.261990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] 0 \r\n2020\udae1\udea8\udae1\udea7 10:25:25.262349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0: N \r\n2020\udae1\udea8\udae1\udea7 10:25:25.263472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2990 MB memory) \u2011> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:26:00.0, compute capability: 6.1)\r\n2020\udae1\udea8\udae1\udea7 10:25:27.808120: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:28.068884: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020\udae1\udea8\udae1\udea7 10:26:06.218534: E tensorflow/stream_executor/dnn.cc:596] CUDNN_STATUS_INTERNAL_ERROR\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1921): 'cudnnRNNBackwardData( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, output_desc.handles(), output_data.opaque(), output_desc.handles(), output_backprop_data.opaque(), output_h_desc.handle(), output_h_backprop_data.opaque(), output_c_desc.handle(), output_c_backprop_data.opaque(), rnn_desc.params_handle(), params.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), input_desc.handles(), input_backprop_data\u2011>opaque(), input_h_desc.handle(), input_h_backprop_data\u2011>opaque(), input_c_desc.handle(), input_c_backprop_data\u2011>opaque(), workspace.opaque(), workspace.size(), reserve_space_data\u2011>opaque(), reserve_space_data\u2011>size())'\r\n2020\udae1\udea8\udae1\udea7 10:26:06.221888: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at cudnn_rnn_ops.cc:1922 : Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] \r\n2020\udae1\udea8\udae1\udea7 10:26:06.223371: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] \r\n[[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n2020\udae1\udea8\udae1\udea7 10:26:06.225063: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: {{function_node __inference___backward_cudnn_lstm_with_fallback_4410_4588_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_5894}} {{function_node __inference___backward_cudnn_lstm_with_fallback_4410_4588_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_5894}} Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] \r\n[[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n[[StatefulPartitionedCall_1]]\r\n[[Reshape_14/_46]]\r\n2020\udae1\udea8\udae1\udea7 10:26:06.228126: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: {{function_node __inference___backward_cudnn_lstm_with_fallback_4410_4588_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_5894}} {{function_node __inference___backward_cudnn_lstm_with_fallback_4410_4588_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_5894}} Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] \r\n[[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n[[StatefulPartitionedCall_1]]\r\n2020\udae1\udea8\udae1\udea7 10:35:24.183417: F .\\tensorflow/core/kernels/random_op_gpu.h:232] Non\u2011OK\u2011status: GpuLaunchKernel(FillPhiloxRandomKernelLaunch, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: unspecified launch failure\r\n```\r\n\r\nTo get rid of the recurrent kernel crashes I have to restart Spyder every time. None of this had ever occurred before yesterday and I can say for sure I have not updated anything in the last 1 month at least. my TF version is 2.1 and the GPU driver version is 441.22. \r\n\r\n\r\n**Describe the expected behavior**\r\nshould compile to completion\r\n\r\n**Standalone code to reproduce the issue** \r\n[Code](https://github.com/diggee/toxic-comment-classification/blob/master/toxic_classifier.py)\r\n\r\n**Other info / logs** \r\nAlready included the logs above. \r\n", "comments": ["@diggee,\r\nWhile reproducing the issue, I ran into an error stating `FileNotFoundError: [Errno 2] File train.csv does not exist: 'train.csv'`\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide all the supporting files required to reproduce the issue reported here. Thanks!", "> @diggee,\r\n> While reproducing the issue, I ran into an error stating `FileNotFoundError: [Errno 2] File train.csv does not exist: 'train.csv'`\r\n> \r\n> In order to expedite the trouble-shooting process, could you please provide all the supporting files required to reproduce the issue reported here. Thanks!\r\n\r\nHi @amahendrakar , the data was bigger than Github's allowed size limit. Can you please download it from [here](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data)", "@diggee,\r\nI was able to run the code without any issues. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/8ed714c3cf849fea5c8d8a9376b465d5/38195-2-1.ipynb).\r\n\r\nCould you please check if you are able to reproduce the error in a virtual environment. Thanks!", "hey @amahendrakar , after some testing I have realized that this issue randomly pops up on its own after running LSTM models for sometime. Restarting my computer is the only way to stop it from crashing the kernel again and again. Just restarting Spyder does not help; I have to restart the PC itself. Maybe this issue is related to how long the kernel is continuously used for, I am not sure. Since I have figured out a (temporary) solution for the time being, I am closing this issue. Thanks for your help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38195\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38195\">No</a>\n", "I'm having a similar problem. Happens at random times. Totally freezes the computer, even the mouse cursor. Only solution is a hard reset. \r\nUsing tf-nightly-gpu 2.4.0.dev20201012."]}, {"number": 38194, "title": "Will TensorFlow 2.2.0 support CUDA 10.2?", "body": "Hi :) I am going to use neural networking and TensorFlow.\r\nI'm trying to install different versions of tensorflow and tensorflow-gpu using pip (for example, 2.1.0 both tensorflow and tensorflow-gpu, 2.2.0-rc0 both tensorflow and tensorflow-gpu) and in Python (3.7) I get error about loading `cudart64_101.dll`, like this:\r\n`>>> import tensorflow as tf`\r\n`2020-03-31 03:30:42.120394: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-03-31 03:30:42.134395: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.`\r\nI copied cuDNN files, also I set `CUDA_HOME` env. to value of `CUDA_PATH` env. My hardware meets the requirements.\r\nAs far as I understand, TensorFlow 2.1.0 should work fine with CUDA 10.1. But I don't want to use CUDA 10.1 unless emergency, I just install 10.2 and don't want to reinstall it to reinstall back to 10.2 again in future.\r\nI ready to wait for 2.2.0 release, if that makes sense in my case. So my question is: Will TensorFlow 2.2.0 support CUDA 10.2?\r\n", "comments": ["@Farxial, To use CUDA 10.2 with Tensorflow 2.2. Please build the Tensorflow from source. \r\nFollow the instructions mentioned [here](https://www.tensorflow.org/install/source). Thanks", "CUDA 10.2 should be compatible with CUDA 10.1. We are building the official pips with CUDA 10.1 as we already changed infrastructure a lot to enable Python3.8 pips. Next release will have infrastructure changed for newer CUDA versions.\r\n\r\nUntil then, you can try compiling from source, or symlinking the libraries.", "Symlinking works.\r\nNice :)\r\nThanks for answers :)", "@Farxial, Closing since the issue is resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38194\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38194\">No</a>\n", "UPDATE: WARNING in https://github.com/tensorflow/tensorflow/issues/34759#issuecomment-633819017\r\n\r\nThe symlink works for me too, details below (installed on Ubuntu 20.04):\r\n\r\n* actual 10.2 libcudart code is in `/usr/local/cuda-10.2/`\r\n* the tensorflow 2.2 code looks in a number of places (and fails to find it in all of them)\r\n```\r\nstrace -o test1.log /usr/bin/python .../quick_tour.py\r\n...\r\nopenat(AT_FDCWD, \"/home/peter_v/.local/lib/python3.8/site-packages/tensorflow/python/../libcudart.so.10.1\", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)\r\nopenat(AT_FDCWD, \"/home/peter_v/.local/lib/python3.8/site-packages/tensorflow/python/libcudart.so.10.1\", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)\r\nopenat(AT_FDCWD, \"/home/peter_v/.local/lib/python3.8/site-packages/tensorflow/python/../libcudart.so.10.1\", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)\r\nopenat(AT_FDCWD, \"/etc/ld.so.cache\", O_RDONLY|O_CLOEXEC) = 20\r\nfstat(20, {st_mode=S_IFREG|0644, st_size=83403, ...}) = 0\r\nmmap(NULL, 83403, PROT_READ, MAP_PRIVATE, 20, 0) = 0x7fb8ad602000\r\nclose(20)                               = 0\r\nopenat(AT_FDCWD, \"/lib/x86_64-linux-gnu/tls/libcudart.so.10.1\", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)\r\nopenat(AT_FDCWD, \"/lib/x86_64-linux-gnu/libcudart.so.10.1\", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)\r\nopenat(AT_FDCWD, \"/usr/lib/x86_64-linux-gnu/tls/libcudart.so.10.1\", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)\r\nopenat(AT_FDCWD, \"/usr/lib/x86_64-linux-gnu/libcudart.so.10.1\", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)\r\nopenat(AT_FDCWD, \"/lib/libcudart.so.10.1\", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)\r\nopenat(AT_FDCWD, \"/usr/lib/libcudart.so.10.1\", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)\r\n```\r\nSomewhat at random, I decided to symlink from `/usr/lib/x86_64-linux-gnu/` to the libcudart.so.10.2 file.\r\n\r\n```\r\nsudo ln -s /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudart.so.10.2 /usr/lib/x86_64-linux-gnu/libcudart.so.10.1\r\n```\r\n\r\nI am actually using mostly the CPU (my 8 core CPU seems faster than a smallish laptop GPU and also the GPU runs easily into OOM for real work-loads). ", "Just to confirm, symlink idea works on Windows too. I symlinked C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2\\bin\\cudart64_102.dll as cudart64_101.dll in the same folder.", "In `Ubuntu 20.04` you don't have to symlink, nor build from source. You just need to modify the installation steps in the TensorFlow documentation at https://www.tensorflow.org/install/gpu to match the new Cuda 10-2 package names.\r\n\r\nHere are the modifications to the https://www.tensorflow.org/install/gpu instructions that worked for me:\r\n\r\n```\r\n# Download the 10-2 packages\r\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.2.89-1_amd64.deb\r\nsudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\r\nsudo dpkg -i cuda-repo-ubuntu1804_10.2.89-1_amd64.deb\r\nsudo apt-get update\r\nwget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\r\nsudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\r\nsudo apt-get update\r\n\r\n# Install the ubuntu drivers, if not done so already\r\nsudo ubuntu-drivers autoinstall\r\n\r\n# Install the 10-2 versions of packages\r\napt-get install -y --no-install-recommends \\\r\ncuda-10-2 \\ \r\nlibcudnn7=7.6.5.32-1+cuda10.2  \\\r\nlibcudnn7-dev=7.6.5.32-1+cuda10.2 \\\r\nlibnvinfer7=7.0.0-1+cuda10.2 \\\r\nlibnvinfer-dev=7.0.0-1+cuda10.2 \\\r\nlibnvinfer-plugin7=7.0.0-1+cuda10.2\\\r\ncuda-cudart-10-1\r\n```\r\nThis works for a clean install. \r\n\r\nFor pre-existing configurations you may need to uninstall previous Cuda 10-1 packages beforehand.\r\n", "Just to confirm solution by @palisadoes works.\r\nJust make sure you have installed everything (developer version) and then you can run:\r\n`sudo apt-get install cuda-cudart-10-1`", "I had to install libnvinfer-plugin-dev to fix /usr/include/x86_64-linux-gnu/NvInferPlugin.h file not found\r\n\r\ndpkg -l | grep libnvinfer\r\nii  libnvinfer-dev                                   7.0.0-1+cuda10.2                                     amd64        TensorRT development libraries and headers\r\nii  libnvinfer-plugin-dev                            7.0.0-1+cuda10.2                                     amd64        TensorRT plugin libraries\r\nii  libnvinfer-plugin7                               7.0.0-1+cuda10.2                                     amd64        TensorRT plugin libraries\r\nii  libnvinfer7                                      7.0.0-1+cuda10.2                                     amd64        TensorRT runtime libraries\r\n", "Expanding on the Windows fix for people who aren't familiar (like myself) with symlinks and just want it to work.\r\nAs admin, in cmd, paste:\r\n\r\n```shell\r\nmklink /H \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2\\bin\\cudart64_101.dll\" \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2\\bin\\cudart64_102.dll\"\r\n```\r\n\r\nAlternatively and more clearly written, navigate to the directory and do the same thing:\r\n```shell\r\ncd \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2\\bin\"\r\nmklink /H cudart64_101.dll cudart64_102.dll\r\n```\r\n\r\nI'm quite surprised that there doesn't exist out-of-the-box support for CUDA 10.2 yet. I mean, CUDA 11 is out.", "after trying most every solution I could find  for windows even with @thomasaarholt fix,\r\nturned out tensor flow could not find any dll's .even with setting the system path \r\n\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2\\bin.\"\r\nthe solution that worked for me with python 3.8 was \r\n>>> import os\r\n>>> os.add_dll_directory(\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2/bin\")\r\n\r\nall the tensorflow dlls could be imported and everything works \r\n\r\ngot the hint to try this here ,  https://stackoverflow.com/questions/59330863/cant-import-dll-module-in-python\r\n\r\n\r\n", "After many months, CUDA 10.2 still cannot work with TF 2.2? ", "TF2.2 won't be patched to support newer CUDA versions. We can only bring new versions of CUDA with newer versions of TF (likely 2.4)", "I am *desperate* for 10.2 support! - my company has bought me a graphics card and I can't get it to play with Cuda desite all the above suggestions. I have tried the nightly build of Tensorflow (which is 2.4) - but seems it still looks for 10.1.\r\n\r\nHas anybody produced a build that supports 10.2 ?\r\n\r\n", "Please try `conda install -c anaconda tensorflow-gpu=1.15.0`. Anaconda built TF under CUDA 10.2", "> \r\n> \r\n> Please try `conda install -c anaconda tensorflow-gpu=1.15.0`. Anaconda built TF under CUDA 10.2\r\n\r\nthanks very much. \r\nI did give this a shot, but no luck so far. For one thing it seems the max conda version of tensorflow is 2.1 - actually I need at least 2.2 for what I'm doing (Tensorflow.Net). Also - I am seeing the following error which I can't make much sense of: \r\n\r\n```bash\r\nThe following specifications were found to be incompatible with your CUDA driver:\r\n\r\n  - feature:/win-64::__cuda==10.2=0\r\n  - feature:|@/win-64::__cuda==10.2=0\r\n\r\nYour installed CUDA driver is: 10.2\r\n```\r\n\r\n", "I think that in this case the best solution is to try building on the target machine with the 10.2 CUDA headers. NVidia claims compatibility between 10.1 and 10.2 so it should be possible to compile from source and have something working", "on my windows machine with RTX2060, symlink works again for the cudnn.\r\n\r\ncd \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2\\bin\"\r\nmklink /H cudnn64_7.dll cudnn64_8.dll\r\n", "Compiled v2.3.1 for Cuda 10.2 in my fork:\r\n[v2.3.1-cuda10.2](https://github.com/alexshvid/tensorflow/releases/tag/v2.3.1-cuda10.2)", "> In `Ubuntu 20.04` you don't have to symlink, nor build from source. You just need to modify the installation steps in the TensorFlow documentation at https://www.tensorflow.org/install/gpu to match the new Cuda 10-2 package names.\r\n> \r\n> Here are the modifications to the https://www.tensorflow.org/install/gpu instructions that worked for me:\r\n> \r\n> ```\r\n> # Download the 10-2 packages\r\n> wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.2.89-1_amd64.deb\r\n> sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\r\n> sudo dpkg -i cuda-repo-ubuntu1804_10.2.89-1_amd64.deb\r\n> sudo apt-get update\r\n> wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\r\n> sudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\r\n> sudo apt-get update\r\n> \r\n> # Install the ubuntu drivers, if not done so already\r\n> sudo ubuntu-drivers autoinstall\r\n> \r\n> # Install the 10-2 versions of packages\r\n> apt-get install -y --no-install-recommends \\\r\n> cuda-10-2 \\ \r\n> libcudnn7=7.6.5.32-1+cuda10.2  \\\r\n> libcudnn7-dev=7.6.5.32-1+cuda10.2 \\\r\n> libnvinfer7=7.0.0-1+cuda10.2 \\\r\n> libnvinfer-dev=7.0.0-1+cuda10.2 \\\r\n> libnvinfer-plugin7=7.0.0-1+cuda10.2\\\r\n> cuda-cudart-10-1\r\n> ```\r\n> \r\n> This works for a clean install.\r\n> \r\n> For pre-existing configurations you may need to uninstall previous Cuda 10-1 packages beforehand.\r\n\r\nIs this valid only on `Ubuntu 20.04`? I'm curious if it works for `Ubuntu 18.04` as well. "]}, {"number": 38193, "title": "R0.12", "body": "Cmakelists", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38193) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 38192, "title": "tf.data.Dataset should support attr classes instances as values", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.1.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently Datasets only support nested structures of tuples, named tuples, and dictionaries, however attr classes are also supported transparently by tf.function. Ideally, to be consistent, these should also be supported by datasets as they are the primary entry point for data in autographed functions.\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nAnyone using structured data.\r\n\r\n**Any Other info.**\r\n- Code for attr support in tf.function:\r\nhttps://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/core/utils/computation_utils.py#L47\r\n\r\n- Code for supported types in Dataset:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/util/structure.py#L407\r\n\r\n- A usage example that should be legit but is not:\r\n\r\n```python\r\nimport attr\r\nfrom collections import namedtuple\r\nimport tensorflow as tf\r\n\r\n\r\n@attr.s(frozen=True)\r\nclass DataContainer:\r\n    data = attr.ib()\r\n    \r\nNamedDataContainer = namedtuple('NamedDataContainer', ['data'])\r\n        \r\n@tf.function\r\ndef f(x):\r\n    return x\r\n\r\nx = tf.constant(0.)\r\nf(DataContainer(x)) # legit\r\nf(NamedDataContainer(x)) # legit\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices([0., 1., 2., 3.])\r\n\r\ndataset.map(NamedDataContainer) # legit\r\ndataset.map(DataContainer) # not legit\r\n```", "comments": ["Shouldn't this issue be closed? Or is it up for grabs still?\r\n"]}, {"number": 38191, "title": "Error while passing initial_state to Bidirectional LSTM", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow):  yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): MacOS Mojave (10.14.6)\r\n- TensorFlow installed from (source or\r\nbinary): binary (Anaconda) \r\n- Python version: 3.6.10\r\n- Tensorflow version: 2.1.0\r\n\r\n**Describe the current behavior**\r\nError while passing `initial_state` to `tf.keras.layers.Bidirectional` with `tf.keras.layers.LSTM` as cell.\r\n\r\n**Describe the expected behavior**\r\nShould be able to pass the initial state\r\n\r\n**Standalone code to reproduce the issue** \r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras.layers as layers\r\n\r\n\r\nencoder_units = 100\r\nbatch_size = 5\r\nembedding_dim = 300\r\n\r\nlstm =  layers.Bidirectional(layers.LSTM(encoder_units,\r\n                                                  return_sequences=True,\r\n                                                    return_state=True,\r\n                                                     time_major=False))\r\ninitial_state = [tf.zeros((batch_size, encoder_units)), tf.zeros((batch_size, encoder_units))]\r\nembedding_inp =  tf.zeros((batch_size, encoder_units, embedding_dim))\r\nencoder_op, state_h, state_c = lstm(embedding_inp, initial_state= initial_state)\r\n```\r\n\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n/anaconda3/envs/tf2/bin/python /Users/jshah02/Library/Preferences/PyCharmCE2019.2/scratches/scratch_8.py\r\n2020-04-03 16:02:31.592278: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-04-03 16:02:31.606727: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fe10d5c24e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-04-03 16:02:31.606745: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nTraceback (most recent call last):\r\n  File \"/Users/jshah02/Library/Preferences/PyCharmCE2019.2/scratches/scratch_8.py\", line 15, in <module>\r\n    encoder_op, state_h, state_c = lstm(embedding_inp, initial_state= initial_state)\r\n  File \"/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/wrappers.py\", line 605, in __call__\r\n    return super(Bidirectional, self).__call__(inputs, **kwargs)\r\n  File \"/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 818, in __call__\r\n    self._maybe_build(inputs)\r\n  File \"/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 2116, in _maybe_build\r\n    self.build(input_shapes)\r\n  File \"/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/wrappers.py\", line 697, in build\r\n    self.forward_layer.build(input_shape)\r\n  File \"/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent.py\", line 574, in build\r\n    self._validate_state_spec(state_size, self.state_spec)\r\n  File \"/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent.py\", line 605, in _validate_state_spec\r\n    raise validation_error\r\nValueError: An `initial_state` was passed that is not compatible with `cell.state_size`. Received `state_spec`=ListWrapper([InputSpec(shape=(5, 100), ndim=2)]); however `cell.state_size` is [100, 100]\r\n\r\nProcess finished with exit code 1\r\n", "comments": ["I have tried on colab with TF version 2.1.0 , 2.2.0-rc2 and was able to reproduce the issue. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/56a61e5c22d9c78fc49eea3b9434590d/untitled764.ipynb#scrollTo=5-mB1Y5GqzbP). Thanks!", "Hi,\r\n\r\nI found the correct approach.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras.layers as layers\r\n\r\n\r\nencoder_units = 100\r\nbatch_size = 5\r\nembedding_dim = 300\r\n\r\nlstm =  layers.Bidirectional(layers.LSTM(encoder_units,\r\n                                                  return_sequences=True,\r\n                                                    return_state=True,\r\n                                                     time_major=False),\r\n                             merge_mode='concat')\r\ninitial_state = [tf.zeros((batch_size, encoder_units)) for _ in range(4)]\r\nembedding_inp =  tf.zeros((batch_size, encoder_units, embedding_dim))\r\nencoder_op, state_hf, state_cf, state_hb, state_cb = lstm(embedding_inp, initial_state= initial_state)\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38191\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38191\">No</a>\n"]}, {"number": 38190, "title": "RPC failed when using TPU", "body": "I have no idea how this error is triggered, so far it happens more than 10 times to me in the past 2 weeks, sometimes it works by just restart the node or wait for another day. But it seems not the case this time. \r\n\r\nBelow is the full message (it can repeat forever and the process won't terminate)\r\n```I0403 09:00:11.077040 140542806840704 multi_init.py:172] Computing metrics...\r\n2020-04-03 09:00:45.480838: W tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:157] RPC failed with status = \"Unavailable: Socket closed\" and grpc_error_string = \"{\"created\":\"@1585904445.480666714\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Socket closed\",\"grpc_status\":14}\", maybe retrying the RPC\r\n2020-04-03 09:00:45.480839: W tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:157] RPC failed with status = \"Unavailable: Socket closed\" and grpc_error_string = \"{\"created\":\"@1585904445.480690662\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Socket closed\",\"grpc_status\":14}\", maybe retrying the RPC\r\nWARNING:tensorflow:An error occurred when attempting to close the session. This may be due to a preemption in a connected worker or parameter server. Error: Session ef9fc537f782c0ae is not found. Possibly, this master has restarted.\r\nW0403 09:01:06.161614 140542806840704 monitored_session.py:1171] An error occurred when attempting to close the session. This may be due to a preemption in a connected worker or parameter server. Error: Session ef9fc537f782c0ae is not found. Possibly, this master has restarted.\r\n```\r\nAnd besides these lines, there is no other error or exception message. I've also checked all the path on gs bucket and they look normal.", "comments": ["@crystina-z,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the TensorFlow version you are using. Thanks!", "Hi thanks for reply. I was trying to provide a snippet of code yet it just failed to reproduce the problem with the toy code and dataset.. then I decreased the size of gs files and now it seems working. ", "@crystina-z,\r\nIs this still an issue? Please feel free to close the issue if resolved. Thanks!"]}, {"number": 38189, "title": "request to add DepthToSpace Spilt and conv2d_transpose operation in post training quantization(full integer quantization))", "body": "****System information****\r\n-  Linux Ubuntu 16.04:\r\n- TensorFlow installed from binary:\r\n- TensorFlow version 1.15.0:\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nRuntimeError: Quantization not yet supported for op: SPLIT\r\n\r\n```\r\n", "comments": ["@wujiexiaowugui, Could you share the standalone code to reproduce the reported issue. Thanks!", "> @wujiexiaowugui, Could you share the standalone code to reproduce the reported issue. Thanks!\r\n\r\nimport tensorflow.compat.v1 as tf\r\n#Eager Mode is essential!\r\ntf.enable_eager_execution()\r\n\r\nimport sys\r\nimport glob\r\nif sys.version_info.major >= 3:\r\n    import pathlib\r\nelse:\r\n    import pathlib2 as pathlib\r\nimport random\r\nimport cv2\r\nimport numpy as np\r\n\r\ntf.logging.set_verbosity(tf.logging.DEBUG)\r\npb_file='model.pb'\r\ninput_arrays=['net_input']\r\noutput_arrays=['net_output']\r\ninput_shapes=[1, 128, 128, 3]\r\n\r\nsample_img_dir='/tmp'\r\n\r\n\r\nconverter =  tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file=pb_file,\r\n                                                       input_arrays=input_arrays,\r\n                                                       output_arrays=output_arrays,\r\n                                                       input_shapes={input_arrays[0]:input_shapes})\r\nconverter.allow_custom_ops=True\r\ntflite_model = converter.convert()\r\n\r\ntflite_models_dir = pathlib.Path(\"./tmp/tflite_models/\")\r\ntflite_models_dir.mkdir(exist_ok=True, parents=True)\r\n\r\ntflite_model_file = tflite_models_dir/\"model.tflite\"\r\ntflite_model_file.write_bytes(tflite_model)\r\n\r\ntf.logging.set_verbosity(tf.logging.DEBUG)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\ndef preprocess_img(img):\r\n    # preprocess\r\n    img = cv2.resize(img, (128, 128))\r\n    # FLOAT32\r\n    img = img.astype(np.float32)\r\n    # img=(img/128.)-1.\r\n    img = img - 128.\r\n    return img\r\n\r\ndef create_datastream_from_imgs(img_dir):\r\n\r\n    img_path_list=glob.glob(img_dir+'/*.jpg')\r\n    random.shuffle(img_path_list)\r\n    img_path_list=img_path_list[:200]\r\n    imgs_list=[]\r\n    for path in img_path_list:\r\n        img=cv2.imread(path)\r\n        img=preprocess_img(img)\r\n        imgs_list.append(img)\r\n    imgs=np.stack(imgs_list,axis=0)\r\n    return tf.data.Dataset.from_tensor_slices((imgs)).batch(1)\r\n\r\n\r\nds=create_datastream_from_imgs(sample_img_dir)\r\n\r\ndef representative_data_gen():\r\n  for input_value in ds.take(20):\r\n    yield [input_value]\r\n\r\nconverter.representative_dataset = representative_data_gen\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\ntflite_quant_model = converter.convert()\r\ntflite_model_quant_file = tflite_models_dir/\"model_quant.tflite\"\r\ntflite_model_quant_file.write_bytes(tflite_quant_model)", "@wujiexiaowugui, I tried reproducing the error but looks like some files are missing.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/dd4525cf55c01d33ac2313923ad38461/untitled491.ipynb) and provide the more information to reproduce the issue. Thanks", "> @wujiexiaowugui, I tried reproducing the error but looks like some files are missing.\r\n> Please find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/dd4525cf55c01d33ac2313923ad38461/untitled491.ipynb) and provide the more information to reproduce the issue. Thanks\r\n\r\nYou can download the model in under link to replace model.pb.\r\nhttp://mcml.yonsei.ac.kr/files/srzoo/eusr_x2.pb\r\n\r\nThe error is :\r\nRuntimeError: Quantization not yet supported for op: **DepthToSpace**\r\n\r\n", "@wujiexiaowugui, Please take a look at [gist](https://colab.sandbox.google.com/gist/gadagashwini/e131278e0fbf20345d4c026bf155a28b/untitled504.ipynb) and confirm the issue. Thanks", "> @wujiexiaowugui, Please take a look at [gist](https://colab.sandbox.google.com/gist/gadagashwini/e131278e0fbf20345d4c026bf155a28b/untitled504.ipynb) and confirm the issue. Thanks\r\n\r\nif you use the \"eusr_x2.pb\" , the input_arrays=['sr_input']  and  output_arrays=['sr_output']\r\n", "@wujiexiaowugui, \r\nas per above update, i ran the code shared, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/aa894a3b5b648efecd07e71c06cd738e/38189.ipynb) error on 1.15 is as per [this gist](https://colab.sandbox.google.com/gist/Saduf2019/2f8887a229943d86a70863b2f64a4c3f/untitled152.ipynb)", "@wujiexiaowugui,\r\nCould you please add the below two lines to your code before converting the model.\r\n```\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops =[tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS] \r\n```\r\nI was able to convert the model without any issues after adding these lines. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/83cb6168fd3eff26bb8f92ef6d2685b4/38189.ipynb). Thanks!", "@wujiexiaowugui,\r\nAny updates regarding this issue? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "> @wujiexiaowugui,\r\n> Could you please add the below two lines to your code before converting the model.\r\n> \r\n> ```\r\n> converter.experimental_new_converter = True\r\n> converter.target_spec.supported_ops =[tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS] \r\n> ```\r\n> \r\n> I was able to convert the model without any issues after adding these lines. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/83cb6168fd3eff26bb8f92ef6d2685b4/38189.ipynb). Thanks!\r\n\r\n> Any updates regarding this issue? Thanks!\r\n\r\nWhen I added two lines to my code , there is another error occurred:\r\n**RuntimeError: Unsupported output type UINT8 for output tensor 90 of type FLOAT32.**\r\n\r\nYou can set \"output_arrays=['DepthToSpace']\". Then you will get the same error.\r\nI think it is because that \"DepthToSpace\" layer still can't be quantized to int8.", "> You can set \"output_arrays=['DepthToSpace']\". Then you will get the same error.\r\n> I think it is because that \"DepthToSpace\" layer still can't be quantized to int8.\r\n\r\nWas able to reproduce the issue with [TF v1.5](https://colab.research.google.com/gist/amahendrakar/e67f099245873bd8fdb117893c538942/38189.ipynb), [TF v2.2](https://colab.research.google.com/gist/amahendrakar/baa57e83ea6d1f881bf74445f2099944/38189-2-2.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/bbbd505f1f3687850022ee74b04f37ee/38189-tf-nightly.ipynb). Please find the attached gist. Thanks!", "> > You can set \"output_arrays=['DepthToSpace']\". Then you will get the same error.\r\n> > I think it is because that \"DepthToSpace\" layer still can't be quantized to int8.\r\n> \r\n> Was able to reproduce the issue with [TF v1.5](https://colab.research.google.com/gist/amahendrakar/e67f099245873bd8fdb117893c538942/38189.ipynb), [TF v2.2](https://colab.research.google.com/gist/amahendrakar/baa57e83ea6d1f881bf74445f2099944/38189-2-2.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/bbbd505f1f3687850022ee74b04f37ee/38189-tf-nightly.ipynb). Please find the attached gist. Thanks!\r\n\r\nYeah, I met the same issues when using TFv1.5, TFv2.2 and TF-nightly.\r\nSo, will you support this \"DepthToSpace\" op in later tensorflow version?", "@wujiexiaowugui One of the attribute(`block_size`) of `DepthToSpace` has dtype of `int64` which is not supported. Can you please change it to `int32` and create the .pb? [here](https://colab.research.google.com/gist/jvishnuvardhan/400b6fe933154506ea1183c5fba59051/38189-tf-nightly.ipynb) is a gist for our reference. Thanks!", "> @wujiexiaowugui One of the attribute(`block_size`) of `DepthToSpace` has dtype of `int64` which is not supported. Can you please change it to `int32` and create the .pb? [here](https://colab.research.google.com/gist/jvishnuvardhan/400b6fe933154506ea1183c5fba59051/38189-tf-nightly.ipynb) is a gist for our reference. Thanks!\r\n\r\n@jvishnuvardhan  I changed my code in upsampling section:\r\n`scale = 2\r\nscale = np.asarray(scale, dtype=np.int32)\r\nout = tf.depth_to_space(input, scale)`\r\nBut, I still met the error as before.", "@wujiexiaowugui \r\n\r\n> One of the attribute(block_size) of DepthToSpace has dtype of int64 which is not supported. Can you please change it to int32 and create the .pb?\r\ndid you check `block_size`? Can you do `tf.cast` to cast it to int32? Please let us know if you have any doubts?  Can you share the updated *.pb after `tf.cast` of `block_size` to `tf.int32`. Thanks!", "> > @wujiexiaowugui\r\n> > > One of the attribute(block_size) of DepthToSpace has dtype of int64 which is not supported. Can you please change it to int32 and create the .pb?\r\n> > > did you check `block_size`? Can you do `tf.cast` to cast it to int32? Please let us know if you have any doubts?  Can you share the updated *.pb after `tf.cast` of `block_size` to `tf.int32`. Thanks!\r\n> \r\n> @jvishnuvardhan When I set:\r\n> `scale = tf.cast(scale, tf.int32)`\r\n> There is an error occurred:\r\n> **TypeError: Expected int for argument 'block_size' not <tf.Tensor 'generator/WDSR/Cast/x:0' shape=() dtype=int32>.**\r\n\r\n@jvishnuvardhan \r\n`block_size = 2;\r\nblock_size = tf.cast(block_size, tf.int32);\r\nout = tf.depth_to_space(input, block_size)`\r\nAn error occurred:\r\n**TypeError: Expected int for argument 'block_size' not <tf.Tensor 'generator/WDSR/Cast/x:0' shape=() dtype=int32>.**", "Hey! Is the DepthToSpace support added now?", "> Hey! Is the DepthToSpace support added now?\r\n\r\nI think DepthToSpace is still not suppored.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38189\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38189\">No</a>\n"]}, {"number": 38188, "title": "tflite ssbo buffer dump", "body": "I`m using tflite gpu ssbo way to inference but the output is not the same as without ssbo\r\nI have referenced this example\r\nhttps://github.com/gnsmrky/tensorflow-lite-ssbo.git\r\nSo I want to inpect the input with ssbo is or not same as the input without ssbo but have no idea how to get ssbo content to evaluate ?\r\n\r\nThe way to get input without ssbo\r\n![image](https://user-images.githubusercontent.com/17869361/78337233-3f579b80-75c3-11ea-910a-29e67cd7dcdf.png)\r\n\r\nBellow is ssbo way by using compute shader in opengl es 3.1\r\nI have tried to use glMapBufferRange to map the buf to cpu for evaluting but all result is zero.\r\nI am new to opengles 3.1 and googled but not found a solution  \r\n![image](https://user-images.githubusercontent.com/17869361/78337436-89d91800-75c3-11ea-91cc-ae4a8a35fcf2.png)\r\n\r\n\r\n", "comments": ["@weinixuehao \r\nplease let us know the tensorflow version on which issue is faced.", "tensorflow 2.0.1", "@Saduf2019 \r\nIt`s inference result is wrong with ssbo", "I think this is a duplicate of https://github.com/tensorflow/tensorflow/issues/26297", "@impjdi you can close it"]}, {"number": 38187, "title": "No module named 'tensorflow.keras'", "body": "i use : \r\nwin10 os\r\nanaconda 3(python 3.6)\r\ntensorflow 1.5.0\r\nkeras 2.1.6\r\ni run \r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, ReLU, SeparableConv2D\r\ni have error :\r\n    from tensorflow.keras.layers import Conv2D, BatchNormalization, ReLU, SeparableConv2D\r\nModuleNotFoundError: No module named 'tensorflow.keras'", "comments": ["@DucLong06 \r\nTF 1.5.0 version is very old. Please, upgrade to latest TF versions `1.15,  2.X( 2.0, 2.1.0, 2.2rc1) `and you will not have any issues with these versions.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 38186, "title": "[MLIR/XLA] Add LhloLegalizeToLoopPass and InputInlineFusionPass to support kLoop\u2026", "body": "Move kLoop/kInput style fusion of XLA to MLIR. This is part of work from:\r\n[RFC: mlir dynamic shape codegen](https://groups.google.com/a/tensorflow.org/forum/#!topic/mlir/_X48poNcbDI).\r\nAnd is intended to work with the fusion/shape inference pass in HLO layer:\r\n[add a pass to outline kLoop/kInput fusion pattern in xla_hlo dialect](https://github.com/tensorflow/tensorflow/pull/37682)\r\n\r\nThe fusion codegen is seperated into two phases:\r\n1, in LhloLegalizeToLoopPass, the (multiple) root ops is lowered into a set of nested loops.\r\n      (1) find out all the root ops and choose a legal schedule (kColReduction/kRowReduction/kLoop etc);\r\n      (2) Lower the root ops into nested loops with the selected schedule.\r\n2, in InputInlineFusionPass, greedily perform input inline fusion on LHLO_Op->Loops patterns. The LHLO_Op is supported to be any fusible Ops in XLA codegen.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38186) for more info**.\n\n<!-- need_sender_cla -->", "> @googlebot I signed it!\r\n\r\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38186) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I signed it!", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38186) for more info**.\n\n<!-- ok -->", "@linearhit Can you please check @yangjunpro's comments and keep us posted. Thanks!", "> @linearhit Can you please check @yangjunpro's comments and keep us posted. Thanks!\r\n\r\nI made some major revision on the schedule implementation during later works. I'll update this merge request in recent days.\r\n", "@linearhit Can you please resolve conflicts? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@linearhit Any update on this PR? and please resolve conflicts Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 38185, "title": "Add GPU-deterministic back-prop for fused softmax/cross-entropy ops", "body": "**SYSTEM INFORMATION**\r\n\r\n- TensorFlow version (you are using): `2.2.0-rc2`\r\n- Are you willing to contribute it: Yes (please assign it to me)\r\n\r\n**CURRENT BEHAVIOR**\r\n\r\nThe back-prop of `tf.nn.softmax_cross_entropy_with_logits` and `tf.nn.sparse_softmax_cross_entropy_with_logits` is non-deterministic on GPUs.\r\n\r\n**WILL THIS CHANGE THE CURRENT API?**\r\n\r\nNo. Assuming the deterministic back-prop kernels are slower than the current non-deterministic ones, then the deterministic operation will be selectable using the preferred mechanism at the time. At the time of writing, that mechanism is to set the environment variable `TF_DETERMINISTIC_OPS` to \"1\" or \"true\".\r\n\r\n**WHO WILL BENEFIT FROM THIS FEATURE?**\r\n\r\nDeterminism, for both training and inference, is becoming increasingly important as deep learning systems are moved into production, not only because of regulatory requirements of some markets but also, more broadly, because of the massive potential performance advantages of training with determinism; determinism accelerates and facilitates debug, experimentation (including hyper-parameter tuning and active learning), and regression testing.\r\n\r\nWith `TF_DETERMINISTIC_OPS` now enabling deterministic functionality of cuDNN convolution, bias addition, max-pooling, and CTC loss, many deep learning models will train deterministically on GPUs. Softmax and cross entropy are both foundational functions in deep learning, and are combined [to ensure performance and numerical stability](https://github.com/tensorflow/tensorflow/issues/2462\r\n). Enabling these fused ops to function deterministically will enhance the ability for TensorFlow to be used for various production systems.\r\n\r\nThis current issue comes out of [Issue 9](https://github.com/NVIDIA/tensorflow-determinism/issues/9) in the [tensorflow-determinism](https://github.com/NVIDIA/tensorflow-determinism) project, which is focused on making the `SequenceToSequece` model of [OpenNMT-tf](https://github.com/OpenNMT/OpenNMT-tf) train deterministically.\r\n\r\n**UNIT TESTS**\r\n\r\nWhat follows are essentially production-ready TensorFlow unit tests that currently fail, but that will pass when this feature is implemented correctly. Depending on the implementation, the parameter-space covered by the tests (e.g. \"batch\" dimensionality) may need to be broadened for coverage. The tests can be seen running (and failing) on TensorFlow version `2.2.0-rc2` in [this colab](https://colab.research.google.com/drive/1syj32Jl7dS6mBa-GhNrq_LLIxvfumIOz).\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass DeterministicTest(tf.test.TestCase):\r\n\r\n  def _randomInts(self, shape, high, dtype):\r\n    return tf.constant(\r\n        np.random.randint(low=0, high=high, size=shape).astype(dtype))\r\n\r\n  def _randomFloats(self, shape, dtype, normalized_rows=False):\r\n    a = (2 * np.random.random_sample(shape) - 1).astype(dtype)\r\n\r\n    if normalized_rows:\r\n\r\n      def normalize(row):\r\n        return row / row.sum()\r\n\r\n      a = np.apply_along_axis(normalize, 1, a)\r\n\r\n    return tf.constant(a)\r\n    \r\n  def _testDeterministicGradients(self, exclusive_labels):\r\n    with self.session(force_gpu=True):\r\n      batch_size = 1024\r\n      classes_count = 1000\r\n      logits_shape = (batch_size, classes_count)\r\n      logits_dtype = np.float32\r\n      logits = self._randomFloats(logits_shape, logits_dtype)\r\n      if exclusive_labels:\r\n        labels_shape = (batch_size)\r\n        labels_dtype = np.int32\r\n        labels = self._randomInts(labels_shape, classes_count, labels_dtype)\r\n      else:\r\n        labels_shape = logits_shape\r\n        labels_dtype = logits_dtype\r\n        labels = self._randomFloats(labels_shape, labels_dtype,\r\n                                    normalized_rows=True)\r\n      output_shape = (batch_size)\r\n      output_dtype = logits_dtype\r\n\r\n      def gradients(local_seed):\r\n        np.random.seed(local_seed)\r\n        upstream_gradients = self._randomFloats(output_shape, output_dtype)\r\n        with tf.GradientTape(persistent=True) as tape:\r\n          tape.watch(logits)\r\n          if exclusive_labels:\r\n            tested_op = tf.nn.sparse_softmax_cross_entropy_with_logits\r\n          else:\r\n            tested_op = tf.nn.softmax_cross_entropy_with_logits\r\n          op_output = tested_op(labels=labels, logits=logits)\r\n          gradient_injector_output = op_output * upstream_gradients\r\n        return tape.gradient(gradient_injector_output, logits)\r\n\r\n      repeat_count = 5\r\n      for seed in range(repeat_count):\r\n        result_a = gradients(seed)\r\n        result_b = gradients(seed)\r\n        self.assertAllEqual(result_a, result_b)\r\n\r\n  def testExclusiveLabelsDeterministicGradients(self):\r\n    self._testDeterministicGradients(exclusive_labels=True)\r\n\r\n  def testDistributionLabelsDeterministicGradients(self):\r\n    self._testDeterministicGradients(exclusive_labels=False)\r\n\r\nif __name__ == '__main__':\r\n  tf.test.main()\r\n```\r\n\r\n**POSSIBLE SOLUTION APPROACHES**\r\n\r\nThe ultimate solution will be to fix this at the CUDA level by creating deterministic kernels that are exactly functionally equivalent to the existing non-deterministic kernels. I'll soon be upstreaming a solution like this for `tf.image.resize` with `method=ResizeMethod.BILINEAR`; see [PR 39243](https://github.com/tensorflow/tensorflow/pull/39243).\r\n\r\nIt could also be possible to create a temporary solution at the Python level by combining existing deterministic ops. See deterministic [tf.nn.bias_add](https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/ops/nn_ops.py#L2741-L2755) as an example. Assuming that `tf.nn.softmax` operates deterministically (I have not tested that), then this could be followed by an arithmetically robust implementation of cross-entropy (e.g. [streaming log-sum-exp](http://www.nowozin.net/sebastian/blog/streaming-log-sum-exp-computation.html)). There would, of course, be a performance cost (1/2 speed) compared to a fused softmax and cross-entopy.\r\n\r\n**ASSIGNMENT**\r\n\r\nI'm willing and able to implement this feature, but I might not be able to get to it for quite a while. I'm happy for someone else to implement it, but please let me know if you start actively working on it (and I will do the same), so that we can avoid duplicating effort.", "comments": ["I just added two sections to my original/initial comment: (1) **possible solution approaches** and (2) **assignment**.", "Work-around info: use non-fused softmax and cross-entropy. For example, assuming you're using `tf.keras`, specify the `activation` on the final `Dense` layer to be 'softmax' and then select `tf.keras.losses.categorical_crossentropy` for the loss function.", "@duncanriach Thanks for helping me resolve my issue (yesterday on the `tensorflow-determinism` repo) that revolved around this very problem. \r\n\r\nThe (cleanest) CUDA solution approach you outline is probably out of scope for me. \r\nHowever, I believe I am able (and would be happy) to implement a temporary python-side solution.\r\n\r\nChecking some assumptions before jumping out there and starting to work on this:\r\n* would you be fine with me taking this on?\r\n* this would essentially be a special case in the body of `tf.nn.softmax_cross_entropy_with_logits` and `tf.nn.sparse_softmax_cross_entropy_with_logits` that is active if and only if `TF_DETERMINISTIC_OPS` environment variable is set?\r\n* to work around this, it should be possible to follow what `tf.keras` does in the non-fused case you mention as workaround above. Seems to come down to computing `tf.nn.softmax` and then following implementation of `tf.keras.backend.categorical_crossentropy(from_logits=False)` ([lines 4636 - 4641](https://github.com/tensorflow/tensorflow/blob/d5b3ec27d1d6bb157588ff3033a3d9bd2e46711f/tensorflow/python/keras/backend.py#L4636)) Does that sound correct? I'll make sure functionality is retained as per the unit tests.", "@MFreidank,\r\n\r\n> would you be fine with me taking this on?\r\n\r\nAbsolutely. I expect that you'll do a great job of it.\r\n\r\n> this would essentially be a special case in the body of `tf.nn.softmax_cross_entropy_with_logits` and `tf.nn.sparse_softmax_cross_entropy_with_logits` that is active if and only if `TF_DETERMINISTIC_OPS` environment variable is set?\r\n\r\nYes, please refer to how I did this for [`tf.nn.bias_add`](https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/ops/nn_ops.py#L2741-L2755) and follow that patten.\r\n\r\n> Seems to come down to computing `tf.nn.softmax`.\r\n\r\nI think you can/should just use `tf.nn.softmax` directly.\r\n\r\n> and then following implementation of tf.keras.backend.categorical_crossentropy(from_logits=False)\r\n\r\nI think you can/should just use `tf.keras.losses.categorical_crossentropy(from_logits=False)` and `tf.keras.losses.sparse_categorical_crossentropy(from_logits=False)` directly also.\r\n\r\nI don't fully remember what the issue with arithmetic robustness was (and the need to consider [streaming log-sum-exp](http://www.nowozin.net/sebastian/blog/streaming-log-sum-exp-computation.html)). It's related to exponentiation of un-scaled floating-point numbers (as in the softmax) leading to arithmetic overflow. That might only be something to consider when implementing the deterministic fused solution in a single CUDA kernel.\r\n\r\n> I'll make sure functionality is retained as per the unit tests.\r\n\r\nPlease use the test code in my original comment on this issue.\r\n\r\nReference how I organized the determinism tests for `tf.nn.bias_add` in [tensorflow/python/kernel_tests/](https://github.com/tensorflow/tensorflow/tree/d5b3ec27d1d6bb157588ff3033a3d9bd2e46711f/tensorflow/python/kernel_tests): the existing test moved into [bias_op_base.py](https://github.com/tensorflow/tensorflow/blob/d5b3ec27d1d6bb157588ff3033a3d9bd2e46711f/tensorflow/python/kernel_tests/bias_op_base.py) and was then included in [bias_op_test.py](https://github.com/tensorflow/tensorflow/blob/d5b3ec27d1d6bb157588ff3033a3d9bd2e46711f/tensorflow/python/kernel_tests/bias_op_test.py) and then the deterministic test cases were added in [bias_op_deterministic_test.py](https://github.com/tensorflow/tensorflow/blob/d5b3ec27d1d6bb157588ff3033a3d9bd2e46711f/tensorflow/python/kernel_tests/bias_op_deterministic_test.py).\r\n\r\nYou'll need to find the existing op test and where the gradient is tested (for some ops it's in a different file) and then appropriately add-in the deterministic tests.\r\n\r\nReview the changes in [PR 31465](https://github.com/tensorflow/tensorflow/pull/31465), including the change to `tensorflow/tools/pip_package/BUILD`.\r\n\r\n---------\r\n\r\nAnother option is for you to begin by adding a patch to tensorflow-determinism (applied with tfd.patch()). This could be released almost immediately (in version 0.4) for application to stock TensorFlow 2.2 (and even earlier versions). Then either you or I could take that solution and upstream it into stock TensorFlow for release in version 2.3 or 2.4 (depending on how long the review process takes).\r\n\r\nThe advantages of this approach is that it allows almost immediate release (and utility) along with the ability to focus on getting the implementation correct before dealing with the process of getting it correctly integrated into, and released via, stock TensorFlow.", "@duncanriach I like the suggested option to start from `tensorflow-determinism` as it would get this shipped to users at earliest. \r\nI'll start on it there and once we have confidence the fix works upstreaming it should be much more straightforward.\r\n\r\nThanks for the guidance, really appreciate it!\r\n\r\n_Update_: [Started PR #21 on tensorflow-determinism](https://github.com/NVIDIA/tensorflow-determinism/pull/21) (still WIP).", "Note that [PR 47925](https://github.com/tensorflow/tensorflow/pull/47925), which has been merged and will be present in TensorFlow version 2.6 onwards, causes TensorFlow to throw `tf.errors.UnimplementedError` when `TF_DETERMINISTIC_OPS` is set to `\"true\"` or `\"1\"` (when determinism is expected) and the fused softmax/cross-entropy ops are used in such a way that truly random noise will be injected into the model's functionality.\r\n\r\nThis exception-throwing functionality can currently be disabled by setting `TF_DISABLE_SPARSE_SOFTMAX_XENT_WITH_LOGITS_OP_DETERMINISM_EXCEPTIONS` to `\"true\"` or `\"1\"` in case you're using a work-around (such as the one [described here](https://github.com/NVIDIA/framework-determinism/blob/master/tensorflow_status.md#fused-softmaxcross-entropy)) that relies on the existing GPU-op functionality being exposed and usable.\r\n\r\nThis move to implement d9m-unimplemented exception throwing is orchestrated under this [determinism RFC](https://github.com/tensorflow/community/blob/master/rfcs/20210119-determinism.md).\r\n", "Note that [PR 49178](https://github.com/tensorflow/tensorflow/pull/49178), which has been merged and will be present in TensorFlow 2.6 onwards, adds GPU-determinism for `tf.nn.softmax_cross_entropy_with_logits` when `TF_DETERMINISTIC_OPS` is set to `\"true\"` or `\"1\"`.\r\n\r\n[PR 50070](https://github.com/tensorflow/tensorflow/pull/50070) still needs to be merged before this issue can be closed.\r\n\r\nA thorough status of GPU-determinism for these ops can be found [here](https://github.com/NVIDIA/framework-determinism/blob/master/tensorflow_status.md#softmax-xent). This work is orchestrated under this [determinism RFC](https://github.com/tensorflow/community/blob/master/rfcs/20210119-determinism.md).", "Note that PR [50070](https://github.com/tensorflow/tensorflow/pull/50070) has been merged and will be present in TF 2.7.  Deterministic functionality for both `tf.nn.softmax_cross_entropy_with_logits` and `tf.nn.sparse_softmax_cross_entropy_with_logits` (and the Keras layers that utilize them) can be enabled using `tf.config.experimental.enable_deterministic_ops(True)`."]}, {"number": 38184, "title": "Error on confliction between custom op argument name and python builtin name", "body": "**System information** \r\n- Have I written custom code: yes \r\n- OS Platform and Distribution: Linux \r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 1.13.1 \r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\nIf we define custom op as\r\n```\r\nREGISTER_OP(\"MyCustomOp\")\r\n    .Input(\"list: list({int32, float32})\")\r\n    .Attr(\"my_list_attr: list(int)\")\r\n    // other settings ...\r\n```\r\nAnd invoke in python as\r\n```\r\nlib = load_library.load_op_library('my_custom_op.so')\r\noutput = lib.my_custom_op(arguments)\r\n```\r\nThe error raises `TypeError: isinstance() arg 2 must be a type or tuple of types`\r\n\r\n**Other info / logs** \r\nRefer to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/python_op_gen.cc#L484\r\nI find python side wrapper will be created as\r\n`def my_custom_op(list, ...)` if we define input name as \"list\"\r\n\r\nAnd for attribute \"my_list_attr\", check code is generated as\r\n`if not isinstance(my_list_attr, (list, tuple)):`\r\n\r\nBut \"list\" do not refer to builtin type anymore :(, which results to python type errors. Can some name checking get conducted to avoid such kind of confusing errors?\r\n", "comments": ["Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38184\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38184\">No</a>\n"]}, {"number": 38182, "title": "bulid tensorflow failed ", "body": "I follow the instrctons in the \"https://tensorflow.google.cn/federated/install\".\r\n(1)Install TensorFlow Federated using pip: When I run the command \"python -c \"import tensorflow_federated as tff; print(tff.federated_computation(lambda: 'Hello World')())\"\" , there is a SyntaxError: invalid syntax os\r\n(2)Using Docker: When I run the command \"docker build .  --tag tensorflow_federated\", it always fails.\r\nThese two errors are shown in the shapshot.\r\n<img width=\"771\" alt=\"\u65e0\u6807\u989811\" src=\"https://user-images.githubusercontent.com/35478590/78322895-a0bb4280-75a2-11ea-9886-20e256eae28c.png\">\r\n", "comments": ["This issue more suitable on [TensorFlow Federated](https://github.com/tensorflow/federated/issues) repository. Thanks!\r\n", "Closing as not an issue of main TensorFlow. Please reopen on TensorFlow Federated", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38182\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38182\">No</a>\n"]}, {"number": 38181, "title": "UnimplementedError: Cast string to float is not supported \t [[{{node Cast_4}}]]", "body": "I don't know why I am getting error for steps_per_epoch. I tried to give values in integer directly but still not working.\r\n\r\n**Traceback:**\r\n\r\n`---------------------------------------------------------------------------\r\nUnimplementedError                        Traceback (most recent call last)\r\n<ipython-input-57-941407f460bb> in <module>\r\n      5     epochs=EPOCHS,\r\n      6     callbacks=[lr_callback],\r\n----> 7     steps_per_epoch=float(186.0),\r\n      8     #validation_data=valid_dataset\r\n      9 )\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    725         max_queue_size=max_queue_size,\r\n    726         workers=workers,\r\n--> 727         use_multiprocessing=use_multiprocessing)\r\n    728 \r\n    729   def evaluate(self,\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n    683         validation_steps=validation_steps,\r\n    684         validation_freq=validation_freq,\r\n--> 685         steps_name='steps_per_epoch')\r\n    686 \r\n    687   def evaluate(self,\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\r\n    298           else:\r\n    299             actual_inputs = ins()\r\n--> 300           batch_outs = f(actual_inputs)\r\n    301         except errors.OutOfRangeError:\r\n    302           if is_dataset:\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py in __call__(self, inputs)\r\n   3471         feed_symbols != self._feed_symbols or self.fetches != self._fetches or\r\n   3472         session != self._session):\r\n-> 3473       self._make_callable(feed_arrays, feed_symbols, symbol_vals, session)\r\n   3474 \r\n   3475     fetched = self._callable_fn(*array_vals,\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py in _make_callable(self, feed_arrays, feed_symbols, symbol_vals, session)\r\n   3408       callable_opts.run_options.CopyFrom(self.run_options)\r\n   3409     # Create callable.\r\n-> 3410     callable_fn = session._make_callable_from_options(callable_opts)\r\n   3411     # Cache parameters corresponding to the generated callable, so that\r\n   3412     # we can detect future mismatches and refresh the callable.\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py in _make_callable_from_options(self, callable_options)\r\n   1503     \"\"\"\r\n   1504     self._extend_graph()\r\n-> 1505     return BaseSession._Callable(self, callable_options)\r\n   1506 \r\n   1507 \r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py in __init__(self, session, callable_options)\r\n   1458       try:\r\n   1459         self._handle = tf_session.TF_SessionMakeCallable(\r\n-> 1460             session._session, options_ptr)\r\n   1461       finally:\r\n   1462         tf_session.TF_DeleteBuffer(options_ptr)\r\n\r\nUnimplementedError: Cast string to float is `", "comments": ["@ninjakx,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the TensorFlow version you are using. Thanks!", "Any updates regarding this issue? Thanks!", "I have lost the code. I was running a SO code.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38181\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38181\">No</a>\n"]}, {"number": 38180, "title": "example person_detection_test  can not make form source code of tensorflow", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nthird party's down shell file should update?\r\n![image](https://user-images.githubusercontent.com/24219045/78319275-6600dc80-7599-11ea-8150-b05a7c33eb6e.png)\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@MrZoloft, Can you Provide the exact sequence of commands or standalone code that you executed before running into the problem and also share the Tensorflow version. Thanks", "@gadagashwini  hello , i just git clone the latest tensorflow source code base on master branch \r\nThen  commad follow\uff1a\r\nmake -f tensorflow/lite/micro/tools/make/Makefile\r\nmake -f tensorflow/lite/micro/tools/make/Makefile test_person_detection_test \r\njust like what has been written on tensorflow/lite/micro/person_detection -- READEME.MD .\r\nmy machine is ubuntu 16.04\r\nwhether should i install bazel firstly? or do other pre -jobs ", "@ymodak Is there any solution for this issue?  i changed another new machine env to use above command, but still useness.\r\ni git pull --rebase origin master \r\nand then \r\nmake -f tensorflow/lite/micro/tools/make/Makefile \r\nmake -f tensorflow/lite/micro/tools/make/Makefile test_person_detection_test  \r\nbut still failed\r\n![image](https://user-images.githubusercontent.com/24219045/78623942-15e88980-78bb-11ea-959d-9abbed2f8401.png)\r\n", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38180\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38180\">No</a>\n"]}, {"number": 38179, "title": "[r2.2:CherryPick] Check for None spec in module_util.py.", "body": "PiperOrigin-RevId: 303904448\nChange-Id: Ib4a8cd50819aef30b1172b8b03aa461b48d88bca", "comments": []}, {"number": 38178, "title": "[r2.2:Cherrypick] Add include dir", "body": "PiperOrigin-RevId: 303463403\nChange-Id: Id55a29e598a9b88d64b238efe9c25bc83120caf3", "comments": []}, {"number": 38177, "title": "[r2.2:Cherrypick] Bump LLVM rev", "body": null, "comments": ["This is no longer needed, we cherry-picked other changes to not need this intrusive change"]}, {"number": 38176, "title": "Update jsoncpp from 1.8.4 to 1.9.2", "body": "The jsoncpp library used by tensorflow is currently 1.8.4 which\r\nis more than 2 and half years old. For this reason it makes\r\nsense to update the verson to the latest verson.\r\nThis PR updates jsoncpp library from 1.8.4 to 1.9.2.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n", "comments": []}, {"number": 38175, "title": "[TFMicro]  Adds helper function to enable make project generation that will compile", "body": "Adds a PARSE_THIRD_PARTY flag that will pull out all third party includes, copy those files and add the correct flags to the makefile in the generated folder.\r\n\r\n make -f tensorflow/lite/micro/tools/make/Makefile PARSE_THIRD_PARTY=true TARGET=apollo3evb  TENSORFLOW_ROOT=<PATH_TO_TENSORFLOW> generate_hello_world_make_project\r\n\r\n* tested with allow appolo3evb, apollo3evb_makefile.inc needed minor changes. Probably other makefiles will as well. ", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38175) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38175) for more info**.\n\n<!-- ok -->", "@petewarden opening a pull request to add a flag for parsing projects. ", "This PR introduces a compilation error that's caught by our internal presubmits:\r\n\r\n```\r\n/bin/sh: 1: APOLLO3_SDK: not found\r\n/bin/sh: 1: MAKEFILE_DIR: not found\r\n/workspace/tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin/../lib/gcc/arm-none-eabi/7.3.1/../../../../arm-none-eabi/bin/ld: cannot open linker script file /boards/apollo3_evb/examples/hello_world/gcc_patched/apollo3evb.ld: No such file or directory\r\ncollect2: error: ld returned 1 exit status\r\ntensorflow/lite/micro/examples/micro_speech/Makefile.inc:277: recipe for target 'tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/bin/micro_speech' failed\r\nmake: *** [tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/bin/micro_speech] Error 1\r\n```\r\n\r\nCould you take a look at what's going wrong? The command to reproduce this is:\r\n\r\n```\r\ntensorflow/tools/ci_build/ci_build.sh MICRO tensorflow/lite/micro/tools/ci_build/test_all.sh\r\n```", "Sure I'll take a look.\n\n-Chris\n\nOn Wed, Jun 3, 2020 at 10:21 AM Pete Warden <notifications@github.com>\nwrote:\n\n> This PR introduces a compilation error that's caught by our internal\n> presubmits:\n>\n> /bin/sh: 1: APOLLO3_SDK: not found\n> /bin/sh: 1: MAKEFILE_DIR: not found\n> /workspace/tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin/../lib/gcc/arm-none-eabi/7.3.1/../../../../arm-none-eabi/bin/ld: cannot open linker script file /boards/apollo3_evb/examples/hello_world/gcc_patched/apollo3evb.ld: No such file or directory\n> collect2: error: ld returned 1 exit status\n> tensorflow/lite/micro/examples/micro_speech/Makefile.inc:277: recipe for target 'tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/bin/micro_speech' failed\n> make: *** [tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/bin/micro_speech] Error 1\n>\n> Could you take a look at what's going wrong? The command to reproduce this\n> is:\n>\n> tensorflow/tools/ci_build/ci_build.sh MICRO tensorflow/lite/micro/tools/ci_build/test_all.sh\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/38175#issuecomment-638338254>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAGX5SDCAQQXZTEBATNRLZDRU2BBJANCNFSM4L2ZU2UQ>\n> .\n>\n"]}, {"number": 38174, "title": "TF looking for CUDA 10.1", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 (Latest Update)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version: 2.2.0rc2\r\n- Python version:  Python 3.8.2\r\n- Installed using virtualenv? pip? conda?: venv + pip + conda\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA: 10.2 cuDNN: 7.6.5.32 \r\n- GPU model and memory:  GeForce GTX 1060 6GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n\r\n\r\nWhile following this tutorial: https://www.wandb.com/tutorial/build-a-neural-network and trying to run this file: https://github.com/lukas/ml-class/blob/master/videos/intro/perceptron-single.py\r\n\r\nAfter following all instructions at https://www.tensorflow.org/install/gpu#software_requirements, I get the the following error: \r\n```\r\n python perceptron-single.py\r\nUsing TensorFlow backend.\r\n2020-04-02 14:09:01.203876: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-04-02 14:09:01.241792: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n\r\n2020-04-02 14:09:08.194348: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-04-02 14:09:08.236234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1060 6GB computeCapability: 6.1\r\ncoreClock: 1.7085GHz coreCount: 10 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 178.99GiB/s\r\n2020-04-02 14:09:08.259510: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-04-02 14:09:08.310824: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-04-02 14:09:08.345234: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-04-02 14:09:08.373030: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-04-02 14:09:08.424593: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-04-02 14:09:08.462421: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-04-02 14:09:08.483778: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-04-02 14:09:08.493898: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n```\r\n\r\nI have CUDA 10.2 installed but TF appears to be looking for 10.1. It proceeds to fall back to the CPU. \r\nHow can I configure TF so that this works properly?\r\n\r\n\r\n", "comments": ["@eScribMac \r\n\r\nCan you downgrade to CUDA 10.1 and check whether the issue still persists.Please, follow the guidelines from [here](https://www.tensorflow.org/install/gpu#windows_setup).Please, let us know if the problem still persists. Thanks!", "I'm having the same issue\r\n\r\n```\r\n2020-04-03 15:47:30.610616: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-04-03 15:47:32.728673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-03 15:47:32.729831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:00:1b.0 name: Tesla T4 computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\r\n2020-04-03 15:47:32.729940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-03 15:47:32.731020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: \r\npciBusID: 0000:00:1c.0 name: Tesla T4 computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\r\n2020-04-03 15:47:32.731105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-03 15:47:32.732204: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 2 with properties: \r\npciBusID: 0000:00:1d.0 name: Tesla T4 computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\r\n2020-04-03 15:47:32.732283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-03 15:47:32.733386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 3 with properties: \r\npciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\r\n2020-04-03 15:47:32.733811: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:\r\n2020-04-03 15:47:32.735654: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-04-03 15:47:32.737341: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-04-03 15:47:32.737683: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-04-03 15:47:32.739523: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-04-03 15:47:32.740580: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-04-03 15:47:32.744448: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-03 15:47:32.744474: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\n```\r\n\r\nThe problem is that the [Profiler](https://github.com/tensorflow/profiler) needs Cuda 10.2 to profile multiple GPUs like I need", "> @eScribMac\r\n> \r\n> Can you downgrade to CUDA 10.1 and check whether the issue still persists.Please, follow the guidelines from [here](https://www.tensorflow.org/install/gpu#windows_setup).Please, let us know if the problem still persists. Thanks!\r\n\r\nI downgraded to confirm my issue was solved to execute tensorflow-gpu. But the [Profiler](https://github.com/tensorflow/profiler) does not show what I need since it needs Cuda 10.2 to profile multiple GPUs", "@eScribMac, I have had some help [here](https://github.com/tensorflow/profiler/issues/3#issuecomment-608955663) if you want to try. I'm still having another issue, but tell me if you are able to solve your problem", "@eScribMac \r\n\r\nAny update on this issue please. Thanks!", "@eScribMac for CUDA 10.2 TF has to be built with CUDA 10.2. As it's officially not supported, must be built by yourself. v2.2rc3 builds fine for both Windows and Ubuntu (used python 3.8 and cuda 10.2).\r\n\r\n**Building tensorflow v2.2rc3 with CUDA 10.2 and python 3.8 from source**\r\nFew notes to get it done more easily:\r\nAdjust ~/.bazelrc to match your cpu core count for faster build\r\n```\r\nstartup --batch_cpu_scheduling --io_nice_level 7\r\nbuild --jobs 7 --ram_utilization_factor 80\r\ntest --jobs 7\r\n```\r\n\r\nSnippets to get the build running, adjust.\r\n```\r\npip install six numpy wheel keras_applications keras_preprocessing --no-deps\r\n\r\nSET TF_CUDA_COMPUTE_CAPABILITIES=7.5\r\nSET TF_NEED_CUDA=1\r\nSET BAZEL_VC=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\r\nSET BAZEL_VS=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\r\nSET CUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2\r\nSET CUDA_TOOLKIT_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2\r\nSET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2\\libnvvp;%PATH%\r\n\r\n# SUPER IMPORTANT TO KEEP BUILD TIME REASONABLE\r\nSET TF_VC_VERSION=16.4\r\n\r\nbazel clean --expunge\r\npython configure.py\r\n\r\nbazel build --config=opt --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\nbazel-bin\\tensorflow\\tools\\pip_package\\build_pip_package C:/tmp/tensorflow_cuda_10_2\r\n```\r\n\r\n**Using existing wheel and CUDA 10.1**\r\nAnother (easier) option is to additionally install CUDA 10.1 (while keeping CUDA 10.2 and *not* installing old drivers). CUDA 10.1 works fine with the latest drivers, CUDA installation does not do anything else than populating CUDA dir and adjusting the env variables. To allow proper libs lookup, adjust the env. Not sure anymore if all the following vars are required but feel free to experiment:\r\n```\r\nSET CUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\r\nSET CUDA_TOOLKIT_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\r\nSET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\libnvvp;%PATH%\r\n```\r\n\r\nValidate the wheel by installing it to your env and running `python -c \"import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([10000, 10000])))\"`", "@eScribMac\r\n\r\nAny update on this issue please. Thanks!", "You should be able to symlink CUDA 10.2 libs to CUDA 10.1 and TF would pick them up.\r\n\r\nAlternatively, you will have to build from source.", "@ravikyram thank you for the help and apologies for the delay, I've been busy with other projects. I will give that a shot this weekend. \r\n\r\n@mihaimaruseac what would that entail exactly? just symlinking each dll individually?", "Yes. It has been reported in other issues that this works", "@eScribMac \r\n\r\nAny update on this issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38174\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38174\">No</a>\n"]}, {"number": 38173, "title": "tf.nn.bias_add does not work right after tf.map_fn", "body": "tf-nightly-gpu 2.2.0.dev20200402\r\nUbuntu 18.04.4 LTS\r\n\r\nBug:\r\nFile \".../anaconda3/envs/tf-nightly-gpu/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 672, in bias_add\r\n    tld.op_callbacks, value, bias, \"data_format\", data_format)\r\ntensorflow.python.eager.core._FallbackException: This function does not handle the case of the path where all inputs are not already EagerTensors.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n...\r\n x = tf.nn.bias_add(x, self.bias)  # Where self.bias is a tf.Variable\r\n  File \".../anaconda3/envs/tf-nightly-gpu/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 3018, in bias_add\r\n    value, bias, data_format=data_format, name=name)\r\n  File \".../anaconda3/envs/tf-nightly-gpu/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 677, in bias_add\r\n    value, bias, data_format=data_format, name=name, ctx=_ctx)\r\n  File \".../anaconda3/envs/tf-nightly-gpu/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 710, in bias_add_eager_fallback\r\n    ctx=ctx, name=name)\r\n  File \".../anaconda3/envs/tf-nightly-gpu/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 75, in quick_execute\r\n    raise e\r\n  File \".../anaconda3/envs/tf-nightly-gpu/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\nTypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  @tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\nThe graph tensor has name: .../StatefulPartitionedCall_1:0\r\n\r\n\r\nHotfix:\r\nx = tf.map_fn(...)\r\nLambda(lambda x: x)(x) // or any other keras layer\r\ntf.nn.bias_add(x)\r\n\r\n", "comments": ["@fingerblister,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "Any updates regarding this issue? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38173\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38173\">No</a>\n"]}, {"number": 38172, "title": "fftshift is failing for negative axes", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.6.8\r\n\r\n**Describe the current behavior**\r\nWhen using the `fftshift` op, I would like to specify the shift axes using negative indexes. Right now, the op fails if I specify negative axes.\r\n\r\n**Describe the expected behavior**\r\nI would like the op not to fail.\r\n\r\n**Standalone code to reproduce the issue** \r\n```python\r\nimport tensorflow as tf \r\ntf.signal.fftshift(tf.ones([1, 32, 32]), axes=[-2, -1])\r\n```\r\n\r\n**Other info / logs** \r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-3-11929d1809ec> in <module>\r\n----> 1 tf.signal.fftshift(tf.ones([1, 32, 32]), axes=[-2, -1])\r\n\r\n~/workspace/fastmri-reproducible-benchmark/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/signal/fft_ops.py in fftshift(x, axes, name)\r\n    389       shift = _array_ops.shape(x)[axes] // 2\r\n    390     else:\r\n--> 391       shift = _array_ops.gather(_array_ops.shape(x), axes) // 2\r\n    392 \r\n    393     return manip_ops.roll(x, shift, axes, name)\r\n\r\n~/workspace/fastmri-reproducible-benchmark/venv/lib/python3.6/site-packages/tensorflow_core/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    178     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    179     try:\r\n--> 180       return target(*args, **kwargs)\r\n    181     except (TypeError, ValueError):\r\n    182       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n~/workspace/fastmri-reproducible-benchmark/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py in gather(***failed resolving arguments***)\r\n   4106     return params.sparse_read(indices, name=name)\r\n   4107   except AttributeError:\r\n-> 4108     return gen_array_ops.gather_v2(params, indices, axis, name=name)\r\n   4109 \r\n   4110 \r\n\r\n~/workspace/fastmri-reproducible-benchmark/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py in gather_v2(params, indices, axis, batch_dims, name)\r\n   3677       try:\r\n   3678         return gather_v2_eager_fallback(\r\n-> 3679             params, indices, axis, batch_dims=batch_dims, name=name, ctx=_ctx)\r\n   3680       except _core._SymbolicException:\r\n   3681         pass  # Add nodes to the TensorFlow graph.\r\n\r\n~/workspace/fastmri-reproducible-benchmark/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py in gather_v2_eager_fallback(params, indices, axis, batch_dims, name, ctx)\r\n   3715   _attr_Tindices, \"Taxis\", _attr_Taxis)\r\n   3716   _result = _execute.execute(b\"GatherV2\", 1, inputs=_inputs_flat,\r\n-> 3717                              attrs=_attrs, ctx=ctx, name=name)\r\n   3718   if _execute.must_record_gradient():\r\n   3719     _execute.record_gradient(\r\n\r\n~/workspace/fastmri-reproducible-benchmark/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     65     else:\r\n     66       message = e.message\r\n---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     68   except TypeError as e:\r\n     69     keras_symbolic_tensors = [\r\n\r\n~/workspace/fastmri-reproducible-benchmark/venv/lib/python3.6/site-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: indices[0] = -2 is not in [0, 3) [Op:GatherV2]\r\n```\r\n", "comments": ["I could replicate the issue with Tf 2.1.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/e8c8b2522f2b3c5839880fdf65247537/38172.ipynb). Thanks!", "I tested above case on my Windows system Python 3.7.7. I did not see any error as shown in below image. I will check on Ubuntu and get back.\r\n\r\n![image](https://user-images.githubusercontent.com/5499416/78391106-5d44f080-7603-11ea-866b-329c1e5e3a76.png)\r\n", "Added a PR #38209 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38172\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38172\">No</a>\n"]}, {"number": 38171, "title": "[REGRESSION] Bazel build failure ", "body": "commit 2082d706a5ed3bbe07706cfb694b431b2fe50f89 included 2 days ago in #34218 break bazel tests:\r\n\r\n```\r\n$bazel test --config=noaws --config=nogcp --config=nohdfs --verbose_failures -c dbg --copt -DNDEBUG  //tensorflow/compiler/xla/service/gpu:all\r\n\r\n...\r\nExecution platform: @local_config_platform//:host\r\ngcc: error: missing argument to '-B'\r\n```\r\n\r\nI cannot just comment the offending line: `cuda_defines[\"%{linker_bin_path}\"] = \"\"` as elsewhere it expect it to be present. But if I set the old value \"host_compiler_prefix\", it fix my problems.\r\n\r\nCan we just revert this commit until a good version is available?\r\n\r\nI tried with bazel 2.0.0 and 2.1.0 and 2.2.0. So bumping the bazel version doesn't fix this problem.", "comments": ["> commit 2082d70 included 2 days ago in #34218 break bazel tests:\r\n\r\nI'm missing something, isn't the commit/PR back from Nov 2019?\r\n\r\nCC @mihaimaruseac ", "The commit is very old, but it got merged 2 days ago.", "> The commit is very old, but it got merged 2 days ago.\r\n\r\nI see.  Then we should probably revert this commit.\r\n\r\n@mihaimaruseac @gunan WDYT?", "Here is a dockerfile that reproduce this in case it help identify the root cause:\r\n```\r\nFROM nvcr.io/nvidia/tensorflow:20.02-tf1-py3\r\nRUN cd /workspace/ && git clone https://github.com/tensorflow/tensorflow.git && cd /opt/tensorflow && ./nvbuild.sh --python3.6 --configonly && cd /workspace/tensorflow && cp /opt/tensorflow/tensorflow-source/.tf_configure.bazelrc .\r\nRUN cd /workspace && wget -q https://github.com/bazelbuild/bazel/releases/download/2.0.0/bazel-2.0.0-installer-linux-x86_64.sh && bash bazel-2.0.0-installer-linux-x86_64.sh\r\nRUN cd /workspace/tensorflow && bazel test -s --config=noaws --config=nogcp --config=nohdfs --verbose_failures -c dbg --copt -DNDEBUG  //tensorflow/compiler/xla/service/gpu:all //tensorflow/compiler/xla/client/lib:arithmetic_test\r\n```\r\n\r\nI build it with this command: `docker build --tag tf_error:1.0 .`", "Agree to reverting the commit. Will do so at once", "Well, @gunan was faster", "Thanks this one is fixed, but there is another regression. I made a new issues for it.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/38205", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38171\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38171\">No</a>\n"]}, {"number": 38170, "title": "With .experimental_run_functions_eagerly(True), tf.functions run by Dataset doesn't get eager tensors", "body": "**System information** \r\n- Have I written custom code: Yes \r\n- OS Platform and Distribution: Ubuntu 18.04 \r\n- Mobile device: N/A \r\n- TensorFlow installed from: binary \r\n - TensorFlow version (use command below): 2.1.0 / 2.2.0rc2\r\n- Python version: 3.7.6\r\n\r\nThe manual suggests switching eager computation on for  `tf.function`s if debugging is desired, via `tf.config.experimental_run_functions_eagerly(True)`. However, this is not possible in situations like shown below.\r\n\r\nTo me it seems, that the in case of running eagerly, the function passed to `.map` would need to be executed with `tensorflow.python.framework.ops.EagerTensor` tensor arguments, not a regular `tensorflow.python.framework.ops.Tensor`.\r\n\r\n**Describe the current behavior**\r\nThe example below outputs the following, failing with an exception:\r\n```\r\n[<tf.Tensor: shape=(), dtype=float32, numpy=1.0>, <tf.Tensor: shape=(), dtype=float32, numpy=0.0>]\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 14, in <module>\r\n    perform_test()\r\n  File \"test.py\", line 8, in perform_test\r\n    print(list(tf.data.Dataset.from_tensor_slices([1.0,-1.0]).map(non_negative)))\r\n  File \"\u2026tensorflow/python/data/ops/dataset_ops.py\", line 1621, in map\r\n    return MapDataset(self, map_func, preserve_cardinality=True)\r\n  File \"\u2026tensorflow/python/data/ops/dataset_ops.py\", line 3974, in __init__\r\n    use_legacy_function=use_legacy_function)\r\n  File \"\u2026tensorflow/python/data/ops/dataset_ops.py\", line 3221, in __init__\r\n    self._function = wrapper_fn.get_concrete_function()\r\n  File \"\u2026tensorflow/python/eager/function.py\", line 2532, in get_concrete_function\r\n    *args, **kwargs)\r\n  File \"\u2026tensorflow/python/eager/function.py\", line 2496, in _get_concrete_function_garbage_collected\r\n    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n  File \"\u2026tensorflow/python/eager/function.py\", line 2777, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"\u2026tensorflow/python/eager/function.py\", line 2667, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"\u2026tensorflow/python/framework/func_graph.py\", line 981, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"\u2026tensorflow/python/data/ops/dataset_ops.py\", line 3214, in wrapper_fn\r\n    ret = _wrapper_helper(*args)\r\n  File \"\u2026tensorflow/python/data/ops/dataset_ops.py\", line 3156, in _wrapper_helper\r\n    ret = autograph.tf_convert(func, ag_ctx)(*nested_args)\r\n  File \"\u2026tensorflow/python/eager/def_function.py\", line 564, in __call__\r\n    return self._python_function(*args, **kwds)\r\n  File \"test.py\", line 5, in non_negative\r\n    return 1.0 if value > 0.0 else 0.0\r\n  File \"\u2026tensorflow/python/framework/ops.py\", line 778, in __bool__\r\n    self._disallow_bool_casting()\r\n  File \"\u2026tensorflow/python/framework/ops.py\", line 542, in _disallow_bool_casting\r\n    \"using a `tf.Tensor` as a Python `bool`\")\r\n  File \"\u2026tensorflow/python/framework/ops.py\", line 527, in _disallow_when_autograph_disabled\r\n    \" Try decorating it directly with @tf.function.\".format(task))\r\ntensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph is disabled in this function. Try decorating it directly with @tf.function.\r\n``` \r\n**Describe the expected behavior**\r\nThe example below working as desired, outputting: \r\n``` \r\n[<tf.Tensor: shape=(), dtype=float32, numpy=1.0>, <tf.Tensor: shape=(), dtype=float32, numpy=0.0>]\r\n[<tf.Tensor: shape=(), dtype=float32, numpy=1.0>, <tf.Tensor: shape=(), dtype=float32, numpy=0.0>]\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\n```python\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef non_negative(value):\r\n    return 1.0 if value > 0.0 else 0.0\r\n\r\ndef perform_test():\r\n    print(list(tf.data.Dataset.from_tensor_slices([1.0,-1.0]).map(non_negative)))\r\n\r\nperform_test()\r\n\r\ntf.config.experimental_run_functions_eagerly(True)\r\n\r\nperform_test()\r\n``` ", "comments": ["Was able to reproduce the issue with [TF v2.2.0-rc2](https://colab.research.google.com/gist/amahendrakar/03bf4471653ba67f0387aebd9469f68a/38170-2-2.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/f7a7583cf73c4500517d2bea2feddf94/38170-tf-nightly.ipynb). Please find the attached gist. Thanks!", "@jsimsa \r\nThis is a known limitation: tf.data operations only support graph execution. We plan to document this more clearly.", "This should not work, but the documentation wasn't clear. The new documentation should make this explicit: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/def_function.py#L352 and https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/ops/dataset_ops.py#L3273", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38170\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38170\">No</a>\n"]}, {"number": 38169, "title": "Tensor load wrong CUDNN", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.15\r\n- Python version: 3.7.6\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 7.6.0\r\n- GPU model and memory: 940mx 2GB\r\n\r\n\r\nSo i want to run some project from [this repo](https://github.com/wcwowwwww/Real-Time-Object-Detection-and-Tracking), but i'm having a problem, tensorflow showing error like this:\r\n\r\n> E tensorflow/stream_executor/cuda/cuda_dnn.cc:319] Loaded runtime CuDNN library: 7.5.1 but source was compiled with: 7.6.0.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\r\n\r\ni have installed CUDNN 7.6.5, 7.6.0, 7.5.1 but the error still appears\r\n", "comments": ["@ijalalfrz\r\nCan you please check with cuda 10 and cudnn 7.4, and let us know if that helps resolve your issue.\r\nis there any specific reason for using 1.15 version and not a later updated version of tensorflow\r\n\r\n#35376 #23715 ", "> @ijalalfrz\r\n> Can you please check with cuda 10 and cudnn 7.4, and let us know if that helps resolve your issue.\r\n> is there any specific reason for using 1.15 version and not a later updated version of tensorflow\r\n> \r\n> #35376 #23715\r\n\r\nbecause some code of the repo using tensorflow under 2.0 which is some codes are deprecated, i will try to install cudnn 7.4", "@Saduf2019 \r\nI'am still getting same error after install CUDNN 7.4.2\r\n\r\n> E tensorflow/stream_executor/cuda/cuda_dnn.cc:319] Loaded runtime CuDNN library: 7.5.1 but source was compiled with: 7.6.0.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\r\n\r\ni don't know why tensor always pointing to cudnn 7.5.1, i just install single CUDA 10 in my drive, is there any step to check if there are 2 CUDAs ", "@ijalalfrz\r\nIn that case you will have to uninstall what cudas you have, do a fresh installation of just the required.\r\n\r\nor please refer to these links [link1](https://blog.kovalevskyi.com/multiple-version-of-cuda-libraries-on-the-same-machine-b9502d50ae77) , [link2](https://stackoverflow.com/questions/40517083/multiple-cuda-versions-on-machine-nvcc-v-confusion)\r\n\r\n\r\n\r\n", "@Saduf2019 ok. i will try to reinstall it and post a progress after", "@Saduf2019 still getting same error, i have installed cuda 10 and cudnn 7.6.5", "@ijalalfrz \r\nplease add cudnn and cuda path. follow the instructions https://www.tensorflow.org/install/gpu#linux_setup\r\n", "@ijalalfrz\r\nplease update as per above comment", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38169\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38169\">No</a>\n"]}]