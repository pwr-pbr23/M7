[{"number": 25299, "title": "Dictionary as model targets for subclassed Keras models", "body": "It is currently possible to build subclassed Keras models with a dict of tensors as inputs, but not as outputs, e.g.,\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\nclass MyModel(tf.keras.Model):\r\n  def call(self, inputs):\r\n    return inputs\r\n  \r\nmodel = MyModel()\r\nmodel.compile(tf.keras.optimizers.Adam(), 'mean_squared_error')\r\ninputs = labels = {'a': tf.range(5.0)}\r\nmodel.fit(x=inputs, y=labels)\r\n```\r\nThis results in the error:\r\n```python-stacktrace\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-4-21d4cae99b87> in <module>()\r\n     10 model.compile(tf.keras.optimizers.Adam(), 'mean_squared_error')\r\n     11 inputs = labels = {'a': tf.range(5.0)}\r\n---> 12 model.fit(x=inputs, y=labels)\r\n\r\n/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    900         steps=steps_per_epoch,\r\n    901         validation_split=validation_split,\r\n--> 902         shuffle=shuffle)\r\n    903 \r\n    904     # Prepare validation data.\r\n\r\n/tensorflow/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\r\n   2458           all_inputs += list(y_input)\r\n   2459         elif isinstance(y_input, dict):\r\n-> 2460           raise ValueError('You cannot pass a dictionary as model targets.')\r\n   2461         else:\r\n   2462           if (not isinstance(y_input, np.ndarray) and\r\n\r\nValueError: You cannot pass a dictionary as model targets.\r\n```\r\n\r\nI'd like to build Keras models that receive and output tensors as dictionaries -- would it be reasonable to add support for this? It currently works to specify inputs as a dict, but not labels for training moels.\r\n\r\n**System information**\r\n- TensorFlow version (you are using): development version\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?**\r\nNo, this currently results in an error.\r\n\r\n**Who will benefit with this feature?**\r\nUsers building complex models that want to keep track of multiple tensors by name instead of by position.\r\n\r\n**Any Other info.**\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the fields in the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. Thanks!", "@jvishnuvardhan I filled out the \"Feature request\" template: https://github.com/tensorflow/tensorflow/issues/new?template=30-feature-request.md", "I confirm the same behavior on the latest TF 2.0 nightly. `tf.keras.Model` can use dict as inputs (training with `model.fit()`) but does not accept dict as outputs.\r\n\r\nWhen using `tf.keras.Sequential`, both inputs and outputs can be dict (you need to set the name of the layers according to the keys in your inputs/outputs).", "The current paradigm requires that model outputs are array-like. Why do you need to output dicts? Would [multiple outputs](https://keras.io/getting-started/functional-api-guide/#multi-input-and-multi-output-models) work for your use-case instead?", "It is perhaps worth noting that it is already supported to pass multiple outputs into a subclassed model as a list:\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\nclass MyModel(tf.keras.Model):\r\n  def call(self, inputs):\r\n    return [inputs['a'], 2 * inputs['b']]\r\n  \r\nmodel = MyModel()\r\nmodel.compile(tf.train.AdamOptimizer(), 'mean_squared_error')\r\ninputs = {'a': tf.range(5.0), 'b': tf.range(5.0, 10.0)}\r\nlabels = [tf.range(5, 10), tf.range(5, 10)]\r\nmodel.fit(x=inputs, y=labels)\r\n```\r\nOutputs:\r\n```\r\nWARNING:tensorflow:The list of trainable weights is empty. Make sure that you are not setting model.trainable to False before compiling the model.\r\n5/5 [==============================] - 0s 1ms/sample - loss: 76.0000 - output_1_loss: 25.0000 - output_2_loss: 51.0000\r\n```\r\n\r\nSee, e.g., these lines in `keras/engine/training.py`:\r\nhttps://github.com/tensorflow/tensorflow/blob/38264f1bf2b69e241323b575738091f701b883e1/tensorflow/python/keras/engine/training.py#L2432-L2440\r\n\r\n\"Multiple outputs\" by name appear to currently only be supported with the functional API. In principle, I could use the functional API for this, insofar as any subclassing model could be written functionally. But I much prefer writing a subclass: my model has complex domain specific logic, and I vastly prefer being between able to debug it in eager mode.", "My use case for a dict is to manage a configurable model, where the outputs are not always the same.\r\nIt is possible to do it via a list, but it is quite cumbersome to know what's the output order depending on the configuration. I also find the code much more readable inside the model:\r\n\r\n```\r\n   def call(self, inputs):\r\n       image = inputs[Inputs.IMAGE]\r\n       bounding_box = inputs[Inputs.BOUNDING_BOX]\r\n       ...\r\n       outputs = {Outputs.CLASS_LOGITS: class_logits} \r\n       if self.mask:\r\n            outputs[Outputs.OBJECT_MASK]=  masks\r\n       return outputs\r\n```\r\n\r\n\r\n", "> The current paradigm requires that model outputs are array-like. Why do you need to output dicts? Would [multiple outputs](https://keras.io/getting-started/functional-api-guide/#multi-input-and-multi-output-models) work for your use-case instead?\r\n\r\nWhen we need to apply the loss to a specific output and target, we can send a dict to `loss` .\r\n```python\r\nloss = {\"MyOutput1\": \"MyLoss\"}\r\ntargets = {\"MyOutput1\": y1, \"MyOutput2\": y2}\r\noutputs = {\"MyOutput1\": out1, \"MyOutput2\": out2}\r\n```\r\n\r\nNow, we have to send loss by keras default output name:\r\n\r\n```python\r\nloss = {\"output_1\": \"MyLoss\"}\r\ntargets = y1\r\noutputs = [out1, out2]\r\n```", "In my case, it's more a general question than for a specific use-case.\r\n\r\nI thought it was natural to have the same API for inputs/outputs for both `tf.keras.Sequential` and `tf.keras.Model`. And also the same API between inputs and outputs. What is the reason why dict could be used for inputs and not for outputs?\r\n\r\n", "In the same spirit, it should be possible to specify inputs shape from  dict of shapes using `tf.function` or `model._set_inputs()`.\r\n\r\nConsider the following code:\r\n\r\n```python\r\nimport tempfile\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nclass MyModel(tf.keras.Model):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = tf.keras.layers.Dense(64)\r\n\r\n    @tf.function(input_signature=[tf.TensorSpec((None, 128, 128), tf.float32)])\r\n    def call(self, inputs):\r\n        return self.layer(inputs)\r\n\r\nmodel = MyModel()\r\n\r\nbatch_size = 16\r\ninputs = np.random.random((batch_size, 128, 128)).astype('float32')\r\noutputs = model(inputs)\r\n\r\nprint(inputs.shape, outputs.shape)\r\n\r\nmodel_path = tempfile.mkdtemp()\r\ntf.saved_model.save(model, model_path)\r\n```\r\n\r\nSaving the model will fail without `tf.function` because the model has not been fitted. Since such a model can be trained with a dict as inputs:\r\n\r\n```python\r\ninputs = dict(input1=np.random.random(28, 28), input2=(16, 16, 3))\r\nmodel.fit(inputs)\r\n```\r\n\r\nThen we should be able to do the following that currently fails:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nclass MyModel(tf.keras.Model):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = tf.keras.layers.Dense(64)\r\n\r\n    @tf.function(input_signature=dict(input1=tf.TensorSpec((None, 128, 128), tf.float32),\r\n                                      input2=tf.TensorSpec((None, 28, 28), tf.float32)))\r\n    def call(self, inputs):\r\n        return self.layer(inputs)\r\n\r\nmodel = MyModel()\r\n\r\nbatch_size = 16\r\ninputs = np.random.random((batch_size, 128, 128)).astype('float32')\r\noutputs = model(inputs)\r\n```\r\n\r\n(again the above will also fail if using `model._set_inputs()` instead of `tf.function`)\r\n\r\nLet me know if I should open a new issue about it.", "I've noticed this too. Basically the functional API model(inputs=..., outputs=...) appears to work with outputs=dict but subclass API, while it *runs*, returns a list. \r\n\r\nIt would be good to either kill the dict input/output feature across all model APIs or enable it. There should be one true way of doing things across the API. It would be interesting to understand why if this is tricky or not possible for some reason.\r\n\r\n", "this not work on tensorflow 2.0:\r\n\r\n```python\r\n dataset, _ = tfds.load('tf_flowers', with_info=True)\r\n    train_dataset = dataset['train']\r\n    train_dataset = train_dataset.shuffle(100).map(preprocess).batch(12).repeat()\r\n    print(train_dataset)\r\n\r\n    # init model\r\n    model = build_model()\r\n    logging.info('model loaded.')\r\n\r\n    if use_keras_fit:\r\n        # train_images = train_dataset['image']\r\n        # train_labels = train_dataset['label']\r\n        model.compile(\r\n            optimizer='adam',\r\n            loss='sparse_categorical_crossentropy',\r\n            metrics=['accuracy'])\r\n        model.fit(train_dataset.take(1), epochs=50)\r\n```", "@karmel any chance to see that feature implemented? Nested data (dict) are supported in a lot of places (model's input and `tf.data.Dataset` for example), it would be really nice to see it supported in model's outputs.\r\n\r\nIt's semantically stronger than a simple list where you need to \"remember\" what each position is.", "Is there any progress on this? I have models with lots of outputs, which were no problem in TF1.X but are now a big hassle. Not having labels for the outputs affects more than how the data is fed. As an example, the loss names now need to be rather cryptic 'output_x'. The metric callbacks are filled with 'output_1: accuracy', 'output_2: accuracy', etc, making them rather useless for visual inspection. Tensorboard is similarly difficult to work with.", "@ghannum I think this now works. Always use the subclass API at the moment to begin with and I think it will save hassle. And just write a custom train loop as it will be more explicit when things fail.\r\n\r\nimport tensorflow as tf\r\n\r\n```\r\nclass Model(tf.keras.models.Model):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n    @tf.function\r\n    def call(self, inputs):\r\n        return dict(a=tf.convert_to_tensor(1), b=tf.convert_to_tensor(2))\r\n\r\n```\r\n", "@cottrell Thanks for the feedback. I've tried the subclass approach as you've shown, but the fit_generator is very explicit about not taking dicts. 'ValueError: You cannot pass a dictionary as model target.'\r\n\r\nI am considering writing a custom loop, but the fit_generator encompasses a ton of useful code, including handling of metrics, callbacks, validation, progress bars, workers, etc. That's a lot of work and things that can wrong to fix a simple oversight of the API. It's a shame, since dicts are supported in 1.X, but have been dropped for some reason in 2.X (only for fit_generator). Named outputs are an important part of developing complex / multi-task models. Hopefully they get support again in the near future.", "I think you can just have a non-tf-function call and tf function _call under the hood to swizzle the args to what you need. They'll probably fix it eventually.", "Thanks for the advice. I actually managed to find a solution by not using the subclass approach. Returning a dict to fit_generator seems to be supported for functional models.", "This is something my company would really like to see implemented as well. We've implemented a number of custom model architectures via the subclass API, and due to the nature of the problems we're solving, a dictionary of labels is necessary (the multiple targets are different ranks, and our custom loss layers know how to handle different targets). Is there any plan for this to be implemented in future versions?", "Bump. This issue makes training on multiple objectives / multiple loss functions rather difficult when y is multiple targets managed by python dictionaries. ", "It seems it is supported now. I tried the following code without any error:\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nclass DictModel(tf.keras.models.Model):\r\n\r\n    def __init__(self, **kwargs):\r\n        super(DictModel, self).__init__(**kwargs)\r\n\r\n    def call(self, inputs, training=None, mask=None):\r\n        out1 = tf.add(inputs, tf.convert_to_tensor(1.))\r\n        out2 = tf.add(inputs, tf.convert_to_tensor(2.))\r\n        return {\"output_1\": out1, \"output_2\": out2}\r\n\r\ndef main():\r\n    model = DictModel()\r\n\r\n    model.compile(tf.keras.optimizers.Adam(),\r\n                  loss={\"output_1\": tf.keras.losses.MeanSquaredError(),\r\n                        \"output_2\": tf.keras.losses.MeanSquaredError()})\r\n\r\n    inputs = tf.range(5.0)\r\n    labels = {'output_1': tf.range(5.0), 'output_2': tf.range(5.0)}\r\n\r\n    model.fit(x=inputs, y=labels)\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n", "Dictionary mapping is supported now with `model.fit`. Thanks!\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/Model#fit", "Are there any plans to make dict inputs supported for `model.predict`?"]}, {"number": 25298, "title": "Update the link for MKL optimized wheels", "body": "Please remove the `WIP` and merge only after we at `Intel` post the `MKL optimized` wheels. `v1.13`.\r\nThanks.\r\nI'll post here when that's done.", "comments": ["@ashahba thank you for the PR , please submit your PR to Tensorflow:master ", "@rthadur the PR for master is here: #25407 ", "Nagging Reviewer @drpngx: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "Hi @drpngx this is ready to be merged and packages for v1.13.1 have been uploaded."]}, {"number": 25297, "title": "TFTRT: Capture new logging level introduced by TRT 5.1", "body": "TRT 5.1 added a new logging severity level, kVERBOSE. We will now capture this level and report it as VLOG(2) like with kINFO, instead of the default LOG(FATAL).", "comments": []}, {"number": 25296, "title": "TensorFlow build for raspberry pi, Python 3.6", "body": "Hello,\r\n\r\n**System information**\r\n- TensorFlow version (you are using): HEAD\r\n- Are you willing to contribute it? Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, [this page](https://www.tensorflow.org/install/source_rpi) provides instructions on how to build a TF pip package for raspberry pi. However, it limits Python 3 versions to `3.4`, which is not officially supported by some popular packages (including `pandas`), which makes using `2.7` the only reasonable solution.\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nRaspberry Pi developers\r\n\r\n", "comments": ["FYR. I didn't try Python 3.6, but I guess it works. I use the following to build tensorflow pip wheel with clang on RPI 3 B+.\r\n```\r\nCC=clang CXX=clang+= bazel build --config opt \\\r\n--config=noaws --config=nogcp --config=nohdfs --config=noignite --config=nokafka \\\r\n--local_resources 2048.0,1,1 \\\r\n--copt=-march=armv7-a --copt=-mfpu=neon-vfpv4 \\\r\n--copt=-ftree-vectorize --copt=-funsafe-math-optimizations \\\r\n--copt=-fomit-frame-pointer --copt=-DRASPBERRY_PI --cxxopt=-Wno-c++11-narrowing \\\r\n--host_copt=-DRASPBERRY_PI --host_copt=-Wno-c++11-narrowing \\\r\n--host_copt=-march=armv7-a --host_copt=-mfpu=neon-vfpv4 \\\r\n--host_copt=-fomit-frame-pointer \\\r\n//tensorflow/tools/pip_package:build_pip_package --linkopt=-Wl,--no-keep-memory\r\n```", "The Python 3.5 wheel is simply the 3.4 wheel renamed. Have you tried downloading the wheel and renaming it?\r\n\r\n```bash\r\nwget https://www.piwheels.org/simple/tensorflow/tensorflow-1.11.0-cp35-none-linux_armv7l.whl\r\nmv tensorflow-1.11.0-cp35-none-linux_armv7l.whl tensorflow-1.11.0-cp36-none-linux_armv7l.whl\r\nsudo pip3 install tensorflow-1.11.0-cp36-none-linux_armv7l.whl\r\n```\r\n\r\nI have a feeling this will work. If you can confirm, I'll add these copies to the piwheels index.", "> The Python 3.5 wheel is simply the 3.4 wheel renamed. Have you tried downloading the wheel and renaming it?\r\n> \r\n> ```shell\r\n> wget https://www.piwheels.org/simple/tensorflow/tensorflow-1.11.0-cp35-none-linux_armv7l.whl\r\n> mv tensorflow-1.11.0-cp35-none-linux_armv7l.whl tensorflow-1.11.0-cp36-none-linux_armv7l.whl\r\n> sudo pip3 install tensorflow-1.11.0-cp36-none-linux_armv7l.whl\r\n> ```\r\n> \r\n> I have a feeling this will work. If you can confirm, I'll add these copies to the piwheels index.\r\n\r\nIt works! \r\nAs expected, it outputs a runtime warning regarding a mismatch between compiletime version (3.4) and runtime version (3.6). This warning also appears when running Python 3.5 version.\r\n\r\n`python -c \"import tensorflow as tf\"`\r\n\r\n> /home/pi/.virtualenvs/keras/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.4 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\nreturn f(*args, **kwds)\r\n/home/pi/.virtualenvs/keras/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: builtins.type size changed, may indicate binary incompatibility. Expected 432, got 412\r\nreturn f(*args, **kwds)\r\n\r\n\r\n", "That's good to know, thanks for the investigation! I'll close this one for now, and aim to create official 3.6 binaries using this approach soon.", "Tensorflow 1.12.0 is now available from piwheels.org for Python 2.7, 3.4, 3.5, 3.6 and 3.7 for Arm v6 and v7.", "Installing tensorflow 1.12.0 didn't work for me. I have always the runtime error (see issue https://github.com/tensorflow/tensorflow/issues/23840) \r\n\r\n```sh\r\npython3 --version\r\n3.5.3\r\n\r\npip3 install tensorflow==1.12.0\r\n\r\npython3 -c \"import tensorflow as tf\"\r\n/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.4 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\r\n  return f(*args, **kwds)\r\n/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: builtins.type size changed, may indicate binary incompatibility. Expected 432, got 412\r\n  return f(*args, **kwds)\r\n\r\n\r\ncat /etc/os-release\r\nPRETTY_NAME=\"Raspbian GNU/Linux 9 (stretch)\"\r\nNAME=\"Raspbian GNU/Linux\"\r\nVERSION_ID=\"9\"\r\nVERSION=\"9 (stretch)\"\r\nID=raspbian\r\nID_LIKE=debian\r\nHOME_URL=\"http://www.raspbian.org/\"\r\nSUPPORT_URL=\"http://www.raspbian.org/RaspbianForums\"\r\nBUG_REPORT_URL=\"http://www.raspbian.org/RaspbianBugs\"\r\n\r\n``` \r\n\r\nin the end I need a working openCV, tensorflow, python3 enviroment....but I'm unable to get this up and running.\r\n\r\nI wonder why there is no OS image available for download for this purpose. This combination is used so often but I could only find Rasberry images for media centers, schools or gaming. For OpenCV/Tensorflow on a normal Raspberry I was not successful....yes there is Coral....but a special camera is required - not the official Rasberry Camera v2 is supported.\r\n", "> Installing tensorflow 1.12.0 didn't work for me. I have always the runtime error\r\n\r\nIt's not a runtime error. It's a RuntimeWarning. Tensorflow still works."]}, {"number": 25294, "title": "Image Retraining: maybe reference retrain.py?", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version:\r\n1.12\r\n- Doc Link:\r\nhttps://www.tensorflow.org/hub/tutorials/image_retraining\r\n\r\nThe Image Retraining document is nice because it talks about the behaviors you need around the retraining code. However, it doesn't go into much detail of how someone would produce that code to retrain the model. There's a 1.3K line `retain.py` linked from the tutorial takes a while to dig through. It might make things clearer if the tutorial referenced or displayed parts of `retrain.py` or something similar. \r\n\r\nFor instance, other venues talk about \"removing the output layer\" in these models and it was unclear what that meant in the tensorflow context. The bottleneck discussion also mentioned \"before the output layer\". I was unsure if I'd have to take a Module's Graph and strip something off the end. I think showing the code involved might clear that up? \r\n\r\nSimilarly, I think the bottleneck discussion might benefit from referencing the code that actually writes data down because, while it's probably obvious to others, I wasn't sure what all was going into those files. (I was also curious if there was some special ordering or collation being done.)", "comments": ["Thanks for the feedback on the image_retraining tutorial. However its more suitable on the [TensorFlow Hub repo](https://github.com/tensorflow/hub/issues). Please post it on tensorflow/hub. Thanks!"]}, {"number": 25293, "title": "error: futures 3.0.5 is installed but futures>=3.1.1 is required by set(['tensorboard'])", "body": "Hello, I am using UBuntu 16.04 LTS. I have use this https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/install/install_pip_packages.sh#L45\r\n\r\nbut when i run under:\r\n/tensorflow/tensorflow/tools/pip_package$\r\n\r\nsudo python setup.py install\r\n\r\nits return error: futures 3.0.5 is installed but futures>=3.1.1 is required by set(['tensorboard'])\r\n![screenshot from 2019-01-30 04-25-56](https://user-images.githubusercontent.com/1058663/51944781-272c3f80-2447-11e9-9be1-b50723d5a8b0.png)\r\n\r\npls help ", "comments": ["It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n"]}, {"number": 25292, "title": "Error in configure.py, MPI_HOME", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu (4.15.0-44-generic)\r\n- TensorFlow installed from: Source\r\n- TensorFlow version: r1.13 (e7f2979fc7bbbd491a5c1db2268d4ee67cc46f88)\r\n- Python version: 3.5\r\n- Installed using virtualenv? pip? conda?: Conda\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source) : gcc-7 (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\n- CUDA/cuDNN version: 10.0/7.4.2.24-1+cuda10.0\r\n- GPU model and memory: K1100M\r\n\r\n\r\n\r\n**Describe the problem**\r\nIf you specify a location for the MPI Toolkit, or use the default location the `configure.py` script will throw and error instead of printing a help message.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\n#!/bin/bash\r\ngit clone git@github.com:tensorflow/tensorflow.git\r\ncd tensorflow\r\ngit checkout r1.13\r\nPYTHON_BIN=\"${HOME}/anaconda3/envs/tf_dev_3.5/bin/python\"\r\nexport PYTHON_BIN_PATH=\"${PYTHON_BIN}\"\r\nexport PYTHON_LIB_PATH=\"${HOME}/anaconda3/envs/tf_dev_3.5/lib/python3.5/site-packages\"\r\nexport TF_ENABLE_XLA=1\r\nexport TF_NEED_OPENCL_SYCL=0\r\nexport TF_NEED_ROCM=0\r\nexport TF_NEED_CUDA=1\r\nexport TF_CUDA_VERSION=\"10.0\"\r\nexport CUDA_TOOLKIT_PATH=\"/usr/local/cuda-10.0\"\r\nexport TF_CUDNN_VERSION=\"7\"\r\nexport CUDNN_INSTALL_PATH=\"/usr/local/cuda-10.0\"\r\nexport TF_NEED_TENSORRT=1\r\nexport TENSORRT_INSTALL_PATH=\"/usr/lib/x86_64-linux-gnu\"\r\nexport TF_NCCL_VERSION=\"2.3\" # 2.3.7\r\nexport TF_CUDA_COMPUTE_CAPABILITIES=\"3.0\"\r\nexport TF_CUDA_CLANG=0\r\nexport GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc-7\" #\"/usr/bin/gcc\" #\"$(which gcc)\"\r\n\r\nexport TF_NEED_MPI=1\r\n./configure\r\n```\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nINFO: Invocation ID: c5371cf2-ec36-4f56-b7f9-c2355305c07e\r\nYou have bazel 0.21.0 installed.\r\nNCCL libraries found in /usr/lib/x86_64-linux-gnu/libnccl.so\r\nThis looks like a system path.\r\nAssuming NCCL header path is /usr/include\r\n\r\nPlease specify the MPI toolkit folder. [Default is /usr]: \r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"./configure.py\", line 1701, in <module>\r\n    main()\r\n  File \"./configure.py\", line 1663, in main\r\n    set_other_mpi_vars(environ_cp)\r\n  File \"./configure.py\", line 1483, in set_other_mpi_vars\r\n    mpi_home, mpi_home, mpi_home)\r\nTypeError: not enough arguments for format string\r\n```\r\n", "comments": ["Thank you for the report. I have submitted a change to fix this, should land soon.\r\n\r\nIn the meanwhile, you can replace the `mpi_home, mpi_home, mpi_home)` line with `(mpi_home, mpi_home, mpi_home))` to not get the `TypeError` error from Python.\r\n\r\nI think you are getting to that exceptional case because MPI toolkit is not found in the configured path.", "Thanks for the quick reply. I applied the patch you suggested<sup>[1](#patch1)</sup>, it now gives me this error:\r\n\r\n```\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nINFO: Invocation ID: db6d283a-8268-4090-97ab-5f610d611862\r\nYou have bazel 0.21.0 installed.\r\nNCCL libraries found in /usr/lib/x86_64-linux-gnu/libnccl.so\r\nThis looks like a system path.\r\nAssuming NCCL header path is /usr/include\r\nPlease specify the MPI toolkit folder. [Default is /usr]: \r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"./configure.py\", line 1701, in <module>\r\n    main()\r\n  File \"./configure.py\", line 1663, in main\r\n    set_other_mpi_vars(environ_cp)\r\n  File \"./configure.py\", line 1483, in set_other_mpi_vars\r\n    (mpi_home, mpi_home, mpi_home))\r\nValueError: Cannot find the MPI library file in /usr/lib or /usr/lib64 or /usr/lib32\r\n```\r\n\r\nWhich is the expected behaviour AFAK. I'll close this issue.\r\n\r\n<a name=\"patch1\"><sup>1</sup>patch</a>:\r\n```\r\ndiff --git a/configure.py b/configure.py\r\nindex 4f8cae2c57..32ecd18ec2 100644\r\n--- a/configure.py\r\n+++ b/configure.py\r\n@@ -1480,7 +1480,7 @@ def set_other_mpi_vars(environ_cp):\r\n   else:\r\n     raise ValueError(\r\n         'Cannot find the MPI library file in %s/lib or %s/lib64 or %s/lib32' %\r\n-        mpi_home, mpi_home, mpi_home)\r\n+        (mpi_home, mpi_home, mpi_home))\r\n \r\n \r\n def set_system_libs_flag(environ_cp):\r\n```"]}, {"number": 25291, "title": "fix for the compiler error with --config=rocm", "body": "@timshen91 \r\n\r\nThis PR is to fix the compile failure when building TF with `--config=rocm`.  \r\nThis is a trivial change and am hoping it can be merged quickly.\r\n\r\nThanks\r\n\r\ndeven", "comments": ["@whchung "]}, {"number": 25290, "title": "Merge ignite fixes for 1.13", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Requested by original author to be merged into 1.13 here: https://github.com/tensorflow/tensorflow/pull/24899/commits", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->"]}, {"number": 25289, "title": "tf-nightly-2.0-preview installation issue", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Professinal 64-bit (10.0, Build 17763) (17763.rs5_release.180914-1434)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary (via Pypi, both origin host and mirror)\r\n- TensorFlow version: 2.0-preview (CPU)\r\n- Python version: 3.5&&3.7(could not find any available distribution) || 3.6(can't find `TensorContractionThreadPool.h`)\r\n- Installed using virtualenv? pip? conda?:  pip 19.0.1 & conda 4.5.12\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: CUDA 9 && cudnn 7.3.1\r\n- GPU model and memory: NVIDIA GeForce 940MX 2010 MB\r\n\r\n\r\n\r\n**Describe the problem**\r\nI'm a GDE in ML and taking part in TF 2.0 test plan. however, I could not install tf-nightly-2.0-preview on my laptop, even I installed eigen_archive separately or reinstall my OS.\r\n```bash\r\nC:\\Users\\kurileo\r\n(tf2) \u03bb pip install tf-nightly-2.0-preview\r\nCollecting tf-nightly-2.0-preview\r\n  Using cached https://files.pythonhosted.org/packages/e1/c6/1221e21c46031f2b71b688e575263c287e76284a0214b2395bc9498910ec/tf_nightly_2.0_preview-1.13.0.dev20190115-cp36-cp36m-win_amd64.whl\r\nCould not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\kurileo\\\\AppData\\\\Local\\\\Temp\\\\pip-install-4nwb1mwi\\\\tf-nightly-2.0-preview\\\\tf_nightly_2.0_preview-1.13.0.dev20190115.data/purelib/tensorflow/include/tensorflow/include/external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContractionThreadPool.h'\r\n```\r\nin Python 3.5/3.7, pip could not find any available distribution:\r\n```bash\r\nC:\\Users\\kurileo\r\n(tf2) \u03bb pip install tf-nightly-2.0-preview\r\nCollecting tf-nightly-2.0-preview\r\n  Could not find a version that satisfies the requirement tf-nightly-2.0-preview (from versions: )\r\nNo matching distribution found for tf-nightly-2.0-preview\r\n```\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nIn build 1213(or 1203... I've forgotten) tf-2.0-nightly-preview worked fine, but when I updated it, it crashed with log `Could not install packages due to an EnvironmentError` and pointed that `TensorContractionThreadPool.h` is missing.\r\nI've once set a [pypi mirror](https://mirrors.tuna.tsinghua.edu.cn/help/pypi/), but when I switched back\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nFor the first issue, it may be caused by Win10 path limitation as #24835 && #24886 pointed.\r\nBut for the second one, I'm not sure if it's an issue.", "comments": ["@kuri-leo For the first issue, could you try this [solution](https://github.com/tensorflow/tensorflow/issues/24886) that worked for another user and let us know. Thanks!", "@jvishnuvardhan thank you very much. it works. But I think this solution is hard for other common developers, so if there's any better way to solve this?\r\n\r\nalso here comes successful installation log, it seems that everything goes fine:\r\n```bash\r\nC:\\Users\\kurileo\r\n(tf2) \u03bb pip install tf-nightly-2.0-preview\r\nCollecting tf-nightly-2.0-preview\r\n  Downloading https://files.pythonhosted.org/packages/3f/e8/819cfaa6de9fd88eea68568f3c1314fb29cd0a6240449905a2131a24cc26/tf_nightly_2.0_preview-2.0.0.dev20190129-cp36-cp36m-win_amd64.whl (41.8MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 41.9MB 473kB/s\r\nCollecting numpy>=1.13.3 (from tf-nightly-2.0-preview)\r\n  Downloading https://files.pythonhosted.org/packages/31/7e/8905636f7e4f9b9d7078aa0e701500634f832f145855a11beb098d3b0fb1/numpy-1.16.0-cp36-cp36m-win_amd64.whl (11.9MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11.9MB 1.0MB/s\r\nRequirement already satisfied: wheel>=0.26 in c:\\programdata\\anaconda3\\envs\\tf2\\lib\\site-packages (from tf-nightly-2.0-preview) (0.32.3)\r\nCollecting keras-applications>=1.0.6 (from tf-nightly-2.0-preview)\r\n  Downloading https://files.pythonhosted.org/packages/3f/c4/2ff40221029f7098d58f8d7fb99b97e8100f3293f9856f0fb5834bef100b/Keras_Applications-1.0.6-py2.py3-none-any.whl (44kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 1.5MB/s\r\nCollecting absl-py>=0.1.6 (from tf-nightly-2.0-preview)\r\n  Downloading https://files.pythonhosted.org/packages/31/bc/ab68120d1d89ae23b694a55fe2aece2f91194313b71f9b05a80b32d3c24b/absl-py-0.7.0.tar.gz (96kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102kB 1.1MB/s\r\nCollecting six>=1.10.0 (from tf-nightly-2.0-preview)\r\n  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\r\nCollecting termcolor>=1.1.0 (from tf-nightly-2.0-preview)\r\n  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\r\nCollecting astor>=0.6.0 (from tf-nightly-2.0-preview)\r\n  Downloading https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl\r\nCollecting google-pasta>=0.1.1 (from tf-nightly-2.0-preview)\r\n  Downloading https://files.pythonhosted.org/packages/86/6c/9eabce1c1cdaa657751a802f94d71ca29b8f82e10cac97c3fd5f8c82736c/google_pasta-0.1.1-py3-none-any.whl (51kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 1.9MB/s\r\nCollecting protobuf>=3.6.1 (from tf-nightly-2.0-preview)\r\n  Downloading https://files.pythonhosted.org/packages/e8/df/d606d07cff0fc8d22abcc54006c0247002d11a7f2d218eb008d48e76851d/protobuf-3.6.1-cp36-cp36m-win_amd64.whl (1.1MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.1MB 1.3MB/s\r\nCollecting gast>=0.2.0 (from tf-nightly-2.0-preview)\r\n  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\r\nCollecting tb-nightly<1.14.0a0,>=1.13.0a0 (from tf-nightly-2.0-preview)\r\n  Downloading https://files.pythonhosted.org/packages/af/5d/9a86478cd0a4e39779552e3d99d3b25eba42ec6b10e2ab7391dd9fa96c88/tb_nightly-1.13.0a20190129-py3-none-any.whl (3.2MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.2MB 1.1MB/s\r\nCollecting grpcio>=1.8.6 (from tf-nightly-2.0-preview)\r\n  Downloading https://files.pythonhosted.org/packages/bb/a7/fa027f411616f6e7c5899a8e3e1ab2e101808f8d62b6ee8b645411ed270b/grpcio-1.18.0-cp36-cp36m-win_amd64.whl (1.5MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.5MB 1.6MB/s\r\nCollecting tensorflow-estimator-2.0-preview (from tf-nightly-2.0-preview)\r\n  Downloading https://files.pythonhosted.org/packages/35/c1/5a32c6093e96421cb0fe64d25a0e9c5e7bba9d87c7e431fb375b12437e73/tensorflow_estimator_2.0_preview-1.13.0.dev2019012800-py2.py3-none-any.whl (347kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 348kB 4.9MB/s\r\nCollecting keras-preprocessing>=1.0.5 (from tf-nightly-2.0-preview)\r\n  Downloading https://files.pythonhosted.org/packages/fc/94/74e0fa783d3fc07e41715973435dd051ca89c550881b3454233c39c73e69/Keras_Preprocessing-1.0.5-py2.py3-none-any.whl\r\nCollecting h5py (from keras-applications>=1.0.6->tf-nightly-2.0-preview)\r\n  Downloading https://files.pythonhosted.org/packages/01/1e/115c4403544a91001d9c618748b2e8786db45544e36b8a6cf3c525e9b57f/h5py-2.9.0-cp36-cp36m-win_amd64.whl (2.4MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.4MB 2.2MB/s\r\nRequirement already satisfied: setuptools in c:\\programdata\\anaconda3\\envs\\tf2\\lib\\site-packages (from protobuf>=3.6.1->tf-nightly-2.0-preview) (40.7.1)\r\nCollecting werkzeug>=0.11.15 (from tb-nightly<1.14.0a0,>=1.13.0a0->tf-nightly-2.0-preview)\r\n  Downloading https://files.pythonhosted.org/packages/20/c4/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243/Werkzeug-0.14.1-py2.py3-none-any.whl (322kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 327kB 1.4MB/s\r\nCollecting markdown>=2.6.8 (from tb-nightly<1.14.0a0,>=1.13.0a0->tf-nightly-2.0-preview)\r\n  Downloading https://files.pythonhosted.org/packages/7a/6b/5600647404ba15545ec37d2f7f58844d690baf2f81f3a60b862e48f29287/Markdown-3.0.1-py2.py3-none-any.whl (89kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 1.4MB/s\r\nBuilding wheels for collected packages: absl-py, termcolor, gast\r\n  Building wheel for absl-py (setup.py) ... done\r\n  Stored in directory: C:\\Users\\kurileo\\AppData\\Local\\pip\\Cache\\wheels\\90\\db\\f8\\2c3101f72ef1ad434e4662853174126ce30201a3e163dcbeca\r\n  Building wheel for termcolor (setup.py) ... done\r\n  Stored in directory: C:\\Users\\kurileo\\AppData\\Local\\pip\\Cache\\wheels\\7c\\06\\54\\bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\r\n  Building wheel for gast (setup.py) ... done\r\n  Stored in directory: C:\\Users\\kurileo\\AppData\\Local\\pip\\Cache\\wheels\\5c\\2e\\7e\\a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\r\nSuccessfully built absl-py termcolor gast\r\nInstalling collected packages: numpy, six, h5py, keras-applications, absl-py, termcolor, astor, google-pasta, protobuf, gast, grpcio, werkzeug, markdown, tb-nightly, tensorflow-estimator-2.0-preview, keras-preprocessing, tf-nightly-2.0-preview\r\nSuccessfully installed absl-py-0.7.0 astor-0.7.1 gast-0.2.2 google-pasta-0.1.1 grpcio-1.18.0 h5py-2.9.0 keras-applications-1.0.6 keras-preprocessing-1.0.5 markdown-3.0.1 numpy-1.16.0 protobuf-3.6.1 six-1.12.0 tb-nightly-1.13.0a20190129 tensorflow-estimator-2.0-preview-1.13.0.dev2019012800 termcolor-1.1.0 tf-nightly-2.0-preview-2.0.0.dev20190129 werkzeug-0.14.1\r\n```\r\n\r\n", "@kuri-leo Thanks! I am closing the issue. Please open new ticket if you see similar issue again. Thanks!"]}, {"number": 25288, "title": "Models produced by SavedModel.simple_save can't be loaded with saved_model.loader", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows\r\n- TensorFlow installed from (source or binary):\r\nPIP\r\n- TensorFlow version (use command below):\r\n1.12.0/'v1.12.0-rc2-3-ga6d8ffae09'\r\n- Python version:\r\n3.6.2\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nTF.saved_model.simple_save can't be loaded by tf.saved_model_loader  - it states\r\n```\r\nRuntimeError: MetaGraphDef associated with tags serve could not be found in SavedModel. To inspect available tag-sets in the SavedModel, please use the SavedModel CLI: `saved_model_cli`\r\n```\r\n**Describe the expected behavior**\r\na model created with tf.saved_model_simple_save can be loaded by using \r\ntf.saved_model.loader.load(sess, tf.saved_model.tag_constants.SERVING, \"./model\")\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy\r\n\r\nlearning_rate = 0.01\r\ntraining_epochs = 2\r\n\r\n# Training Data\r\ntrain_X = numpy.asarray([3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59, 2.167,\r\n\t\t\t\t\t\t 7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1])\r\ntrain_Y = numpy.asarray([1.7, 2.76, 2.09, 3.19, 1.694, 1.573, 3.366, 2.596, 2.53, 1.221,\r\n\t\t\t\t\t\t 2.827, 3.465, 1.65, 2.904, 2.42, 2.94, 1.3])\r\nn_samples = train_X.shape[0]\r\n\r\nwith tf.Session() as sess:\r\n\tX = tf.placeholder(\"float\")\r\n\tY = tf.placeholder(\"float\")\r\n\r\n\tW = tf.Variable(1.0, name=\"weight\")\r\n\tb = tf.Variable(2.0, name=\"bias\")\r\n\r\n\tpred = tf.add(tf.multiply(X, W), b)\r\n\tcost = tf.reduce_sum(tf.pow(pred - Y, 2)) / (2 * n_samples)\r\n\tsess.run(tf.global_variables_initializer())\r\n\r\n\toptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\r\n\tfor epoch in range(training_epochs):\r\n\t\tfor (x, y) in zip(train_X, train_Y):\r\n\t\t\tsess.run(optimizer, feed_dict={X: x, Y: y})\r\n\r\n\ttf.saved_model.simple_save(sess, \"./model\", inputs={\"x\": X, \"y\": Y},\r\n\t\t\t\t\t\t\t   outputs={\"c\": pred})\r\n\t\t\t\t\t\t\t  \r\nwith tf.Session() as sess:\r\n\ttf.saved_model.loader.load(sess, tf.saved_model.tag_constants.SERVING, \"./model\")\r\n```\r\n\r\n**Other info / logs**\r\nWhen I go to check the model with 'saved_model_cli' - show lists the tag 'serve':\r\n```\r\nsaved_model_cli.py show --dir . --all\r\n\r\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n\r\nsignature_def['serving_default']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n    inputs['x'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: unknown_rank\r\n        name: Placeholder:0\r\n    inputs['y'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: unknown_rank\r\n        name: Placeholder_1:0\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['c'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: unknown_rank\r\n        name: Add:0\r\n  Method name is: tensorflow/serving/predict\r\n```\r\n\r\nbut when I try to run it, I encounter the same issue:\r\n```\r\nsaved_model_cli.py run --dir . --tag_set 'serve' --signature_def 'serving_default' --input_exprs x=[1];y=[2]\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\coverste\\AppData\\Local\\Continuum\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\tools\\saved_model_cli.py\", line 827, in <module>\r\n    sys.exit(main())\r\n  File \"C:\\Users\\coverste\\AppData\\Local\\Continuum\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\tools\\saved_model_cli.py\", line 823, in main\r\n    args.func(args)\r\n  File \"C:\\Users\\coverste\\AppData\\Local\\Continuum\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\tools\\saved_model_cli.py\", line 644, in run\r\n    init_tpu=args.init_tpu, tf_debug=args.tf_debug)\r\n  File \"C:\\Users\\coverste\\AppData\\Local\\Continuum\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\tools\\saved_model_cli.py\", line 304, in run_saved_model_with_feed_dict\r\n    tag_set)\r\n  File \"C:\\Users\\coverste\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\tools\\saved_model_utils.py\", line 49, in get_meta_graph_def\r\n    ' could not be found in SavedModel')\r\nRuntimeError: MetaGraphDef associated with tag-set 'serve' could not be found in SavedModel\r\n```", "comments": ["The tags to load should be a list in your last line:\r\n\r\n```\r\n\ttf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], \"./model\")\r\n```\r\n\r\nWhen I run with that, everything works as expected. Closing this issue; please reopen if I have misunderstood.", "Thanks - that resolved my issue. It would be nice to have an example using simple saver in the API docs or type annotations or something."]}, {"number": 25287, "title": "[TF C++] Added WhileContext to all while loop nodes", "body": "Per discussions in #24842 and [here](https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!topic/developers/IjuGQNcD6Kw), this adds the while context to all the nodes created by the while loop using the method Skye suggested. It also updates the one test that checks the while context.\r\n\r\ncc @skye ", "comments": ["Sorry! I think I made the const changes like minutes after it was marked ready to pull. I only added const though and the C++ tests are still passing", "No worries! To be honest I don't really understand how our PR magic works... I just re-approved the changes, so hopefully it'll run the tests and merge now.", "Sorry for the delay. I'm seeing internally that tensorflow/core:graph_control_flow_test (not sure why this didn't show up in the PR tests here, maybe I'm missing something). @samdow can you take a look?", "Thanks for checking! I tried to run it locally but it seemed to pass for me:\r\n![screen shot 2019-02-07 at 2 47 01 pm](https://user-images.githubusercontent.com/17888388/52448014-5e55bd00-2ae7-11e9-88d1-b03fb45969b0.png)\r\nWould you be able to tell me what error it was getting? I'll also try rebasing in case the test changed", "Just FYI, I'm holding off on moving this forward until we resolve the discussion about the original email.", "@samdow do we still need this change?", "Sorry for the delay! I just wanted to check it, but we do not need this change because @melissagrueter's PR will fix the AddGradients problem we were having", "No worries, I was just checking as well. Thanks!"]}, {"number": 25286, "title": "[CP request] Fix GFile tf_export for v2", "body": "GFile was not properly exposed under io.gfile.GFile.\r\nWe also expose the tf.compat.v1.io.gfile.GFile.\r\n\r\nPiperOrigin-RevId: 226959951", "comments": []}, {"number": 25285, "title": "Training with multiple datasets using the estimator", "body": "\r\n**System information**\r\n- TensorFlow version (you are using): 1.12\r\n- Are you willing to contribute it (Yes/No): Not in the near future\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nI'll start with what i'm trying to achieve.\r\nI want to train a model (a language model) with Multitask Learning and Transfer Learning.\r\nI want to train with such techniques while using 3 different tasks.\r\nEach task has it's own loss function, optimizer method training hooks, evaluation hooks and different data input.\r\nI'm using the tf.data TFRecord dataset.\r\nI've read a lot about the head API which seems to solve my problem just as described in this image:\r\n\r\n![1_jcoaugzhqn3_7oyvu58t4q](https://user-images.githubusercontent.com/28672915/51889143-512b2680-23a1-11e9-8756-f03c3522adbd.png)\r\n\r\nbut my problem is the features - the input data, which changes according to what task i'm training on.\r\nI want to be able to control the data input with a single call to estimator.train or estimator.eval\r\n\r\n**Current Behavior**\r\n\r\nSuppose the following:\r\n| Tasks  | Task's Dataset |\r\n|--------|----------------|\r\n| Task A | Database A     |\r\n| Task B | Database B     |\r\n| Task C | Database C     |\r\n\r\nand i want to train one task for x steps and then perform a \"context switch\" - switch to a different task and train it for `x` steps as well.\r\nSo I train Task A using `estimator.train(input_fn=Dataset_A, steps=x)` and then i train Task B using `estimator.train(input_fn=Dataset_B, steps=x)`\r\n\r\nThe problem starts when i want to train Task A again for x steps. Task A will be trained be first x records from it's input_fn (Dataset_A), this occurs since that estimator recreates the input_fn (and model_fn). I know the estimator does it this by design but this causes issues when performing Multitask learning while training with different datasets.\r\n\r\nThe tf.data api supports this kind of functionality. Using the [feedable](https://www.tensorflow.org/guide/datasets#creating_an_iterator) iterator, i can switch between datasets by invoking a different string handles of each dataset i got. **But how can i make the estimator to manage different string handles of multiple datasets?**\r\nI would like to invoke estimator.train once and have a control over the flow of the data during the training (maybe be using session run hooks?)\r\n\r\n**Will this change the current api? How?**\r\n\r\nIn order to support feedable iterators the estimator should be able to expected an input_fn that returns multiple tf.data.Dataset objects or iterators of those datasets.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone that will want to train and experiment models using multiple datasets.\r\n", "comments": ["@jvishnuvardhan @karmel I'd love to hear your opinion on this. Did i provide enough info to describe the problem?\r\n\r\nThis might partly be related to #16087 and #18016. But i would like to emphasize that here I'm describing a use case that is needed for the training process. I would also like to control the \"context switch\" of datasets with estimator hooks(tf.train.SessionRunHook).\r\n\r\nThank you.", "Thanks, @amitlevy21 . I'm not totally sure I understand the use-case above, so let me repeat back to you what I hear: you want to switch between datasets, performing the same train operation on batches from each, but interleaving the steps. Is that correct? If so, Estimator doesn't interleave particularly well, but you can instead represent one input/model_fn step as a pass through each of the datasets-- ie, the input fn loads A, B, and C, and the model fn runs A, B, and C in one go. Each \"step\" is really a three-fold pass over the batch. Alternatively, the tf.keras subclassing API may be a better fit here, as it is not forced to be hermetic with each step in the way Estimator is.", "@karmel Thank you. You understand exactly what i'm trying to achieve.\r\nI'll try my a go with the suggestions you made.", "Hello @amitlevy21. I am facing similar problem and want to ask you a question. You wrote \r\n\r\n> So I train Task A using estimator.train(input_fn=Dataset_A, steps=x) and then i train Task B using estimator.train(input_fn=Dataset_B, steps=x)\r\n\r\nbut you also mentioned that your tasks have different loss functions. How do you switch between them? As far as i understand, in this code snippet you only change your input_fn, but not model_fn and hence, model loss stays the same. I am facing similar issue as i want to implement a multi-task learning model, where different inputs from different datasets are processed through shared BERT encoder, on top of which there are task-specific layers and different task-specific losses. I am playing with the original BERT implementation at [https://github.com/google-research/bert.](https://github.com/google-research/bert.) BERT instance is created inside my model_fn, where it gets called, and i only want one instance to be created. How do i make model_fn use the appropriate loss for current task, if i can not change the computational graph dynamically(e.g. pass task_id to model_fn to make it choose appropriate task loss by this id)? Maybe i should have as many model_fn's as there are tasks, but in this case, how do i share the same BERT instance between them? @karmel Your answer will also be greatly appreciated. \r\nThank you", "@svboeing I'm implementing an infrastructure for conducting experiments on language models using multitask and transfer learning, my client have asked me to not use Keras implementation. The use of Keras, Especially with the upcoming Tensorflow 2.0 should have decrease the complexity of my code, So i suggest to only considerate the following if you must not use Keras in your solution. \r\n\r\nI used the following function to create the train, validation and test data of each task:\r\n```python\r\ndef _create_input_fn(tf_record_path, hyperparams):\r\n    def input_fn():\r\n        \"\"\"\r\n        This method is invoke each time we call estimator.train\r\n        or estimator.eval or estimator.predict, the estimator recreates\r\n        the tf.data.Dataset that we return here. For each dataset we train,\r\n        we want to remember how many records we trained it so we can iterate\r\n        different datasets each epoch or batch. Therefore, we create a dataset\r\n        step counter for each new dataset that we load, and skip records from\r\n        reloaded dataset according to it's step.\r\n        Returns:\r\n            tf.data.Dataset object representing the dataset\r\n        \"\"\"\r\n        global dataset_step_counter\r\n\r\n        logger.debug(\"TF records path=%s\", tf_record_path)\r\n        logger.debug(\"dataset_steps=%s\", dataset_step_counter[tf_record_path])\r\n\r\n        logger.info(\"training with dataset=%s\", tf_record_path)\r\n\r\n        shuffle = hyperparams.train.get_or_default(key=\"shuffle\", default=False)\r\n        shuffle_buffer_size = hyperparams.train.get_or_default(key=\"shuffle_buffer_size\", default=10000)\r\n        dataset = load_dataset(abs_tf_record_path=tf_record_path,\r\n                               batch_size=hyperparams.train.batch_size,\r\n                               feature_sample_size=hyperparams.data.shape_size_features,\r\n                               label_sample_size=hyperparams.data.shape_size_labels,\r\n                               skip_first_n=dataset_step_counter[tf_record_path],\r\n                               shuffle=shuffle,\r\n                               shuffle_buffer_size=shuffle_buffer_size)\r\n        return dataset\r\n\r\n    return input_fn\r\n\r\ntrain_dataset = _create_input_fn(tf_record_path=train_tf_record_path,\r\n                                     hyperparams=hyperparams)\r\nvalidation_dataset = _create_input_fn(tf_record_path=valid_tf_record_path,\r\n                                          hyperparams=hyperparams)\r\ntest_dataset = _create_input_fn(tf_record_path=test_tf_record_path,\r\n                                    hyperparams=hyperparams)\r\n```\r\n\r\nI have a similar function, that returns a model_fn function each time its called:\r\n```python\r\n estimator_spec = _create_tf_estimator_spec(create_model=create_model,\r\n                                               create_loss=create_loss,\r\n                                               create_optimizer=create_optimizer,\r\n                                               shared_hyperparams=shared_hyperparams,\r\n                                               training_hooks=training_hooks,\r\n                                               evaluation_hooks=evaluation_hooks)\r\n```\r\n\r\nWhere create_model, create_loss and create_optimizer are **functions**.\r\nMy code was inspired by this [talk](https://www.youtube.com/watch?v=Fudxo6WZHO0&t=1s) which i recommend.\r\n\r\nSo basically, i am able to train different tasks where each has different a different dataset, loss and optimizer by invoking ```_create_input_fn``` and ```_create_tf_estimator_spec``` for each task.\r\nthe create_model method must be the same for all tasks, which means it contains a logits layer for each of the tasks, which are used in the create_loss function.\r\n\r\nThere are some issues with this approach, which are derived from the fact that the estimator recreates by design the input_fn and model_fn each time you call ```estimator.train```. Which is why i had to use ```dataset_step_counter``` global variable that recalls how many records the model has seen from dataset X and then skip those records the next time ```estimator.train``` is called.\r\n\r\nAnother main issue  is the use of train and evaluation hooks which will not preserve state between calls of ```estimator.train```.\r\n\r\nAgain, the use of Keras will simplify a lot of the code and reduce the issues you will have if you will only use the estimator.", "> @svboeing I'm implementing an infrastructure for conducting experiments on language models using multitask and transfer learning, my client have asked me to not use Keras implementation. The use of Keras, Especially with the upcoming Tensorflow 2.0 should have decrease the complexity of my code, So i suggest to only considerate the following if you must not use Keras in your solution.\r\n> \r\n> I used the following function to create the train, validation and test data of each task:\r\n> \r\n> ```python\r\n> def _create_input_fn(tf_record_path, hyperparams):\r\n>     def input_fn():\r\n>         \"\"\"\r\n>         This method is invoke each time we call estimator.train\r\n>         or estimator.eval or estimator.predict, the estimator recreates\r\n>         the tf.data.Dataset that we return here. For each dataset we train,\r\n>         we want to remember how many records we trained it so we can iterate\r\n>         different datasets each epoch or batch. Therefore, we create a dataset\r\n>         step counter for each new dataset that we load, and skip records from\r\n>         reloaded dataset according to it's step.\r\n>         Returns:\r\n>             tf.data.Dataset object representing the dataset\r\n>         \"\"\"\r\n>         global dataset_step_counter\r\n> \r\n>         logger.debug(\"TF records path=%s\", tf_record_path)\r\n>         logger.debug(\"dataset_steps=%s\", dataset_step_counter[tf_record_path])\r\n> \r\n>         logger.info(\"training with dataset=%s\", tf_record_path)\r\n> \r\n>         shuffle = hyperparams.train.get_or_default(key=\"shuffle\", default=False)\r\n>         shuffle_buffer_size = hyperparams.train.get_or_default(key=\"shuffle_buffer_size\", default=10000)\r\n>         dataset = load_dataset(abs_tf_record_path=tf_record_path,\r\n>                                batch_size=hyperparams.train.batch_size,\r\n>                                feature_sample_size=hyperparams.data.shape_size_features,\r\n>                                label_sample_size=hyperparams.data.shape_size_labels,\r\n>                                skip_first_n=dataset_step_counter[tf_record_path],\r\n>                                shuffle=shuffle,\r\n>                                shuffle_buffer_size=shuffle_buffer_size)\r\n>         return dataset\r\n> \r\n>     return input_fn\r\n> \r\n> train_dataset = _create_input_fn(tf_record_path=train_tf_record_path,\r\n>                                      hyperparams=hyperparams)\r\n> validation_dataset = _create_input_fn(tf_record_path=valid_tf_record_path,\r\n>                                           hyperparams=hyperparams)\r\n> test_dataset = _create_input_fn(tf_record_path=test_tf_record_path,\r\n>                                     hyperparams=hyperparams)\r\n> ```\r\n> \r\n> I have a similar function, that returns a model_fn function each time its called:\r\n> \r\n> ```python\r\n>  estimator_spec = _create_tf_estimator_spec(create_model=create_model,\r\n>                                                create_loss=create_loss,\r\n>                                                create_optimizer=create_optimizer,\r\n>                                                shared_hyperparams=shared_hyperparams,\r\n>                                                training_hooks=training_hooks,\r\n>                                                evaluation_hooks=evaluation_hooks)\r\n> ```\r\n> \r\n> Where create_model, create_loss and create_optimizer are **functions**.\r\n> My code was inspired by this [talk](https://www.youtube.com/watch?v=Fudxo6WZHO0&t=1s) which i recommend.\r\n> \r\n> So basically, i am able to train different tasks where each has different a different dataset, loss and optimizer by invoking `_create_input_fn` and `_create_tf_estimator_spec` for each task.\r\n> the create_model method must be the same for all tasks, which means it contains a logits layer for each of the tasks, which are used in the create_loss function.\r\n> \r\n> There are some issues with this approach, which are derived from the fact that the estimator recreates by design the input_fn and model_fn each time you call `estimator.train`. Which is why i had to use `dataset_step_counter` global variable that recalls how many records the model has seen from dataset X and then skip those records the next time `estimator.train` is called.\r\n> \r\n> Another main issue is the use of train and evaluation hooks which will not preserve state between calls of `estimator.train`.\r\n> \r\n> Again, the use of Keras will simplify a lot of the code and reduce the issues you will have if you will only use the estimator.\r\n\r\nI am also facing similar problem and want to ask question.\r\n\r\nIn your implementation, how to share specific module between different task?\r\n\r\nthank you", "@zjh-nudger Did you mean a python module?\r\nEach task is a module that has a file for each of the following functions: create_loss, create_model, create_optimizer.\r\nThen i can simply import that of the those functions anywhere.\r\n", "> There are some issues with this approach, which are derived from the fact that the estimator recreates by design the input_fn and model_fn each time you call estimator.train. Which is why i had to use dataset_step_counter global variable that recalls how many records the model has seen from dataset X and then skip those records the next time estimator.train is called.\r\n\r\n@amitlevy21 It looks like you solved the issue of recreation of input_fn by using a global data_step_counter variable. How did you solve the issue of recreating graph when model_fn gets called every time estimator.train is called? ", "@agupta74 which issue exactly? I'll guess you're talking about preserving the trained weights?\r\n```python\r\n    config = tf.estimator.RunConfig(model_dir=checkpoint_path,\r\n                                    save_summary_steps=summary_steps,\r\n                                    save_checkpoints_steps=save_checkpoint_steps,\r\n                                    keep_checkpoint_max=keep_checkpoints_max)\r\n\r\n```\r\nDefine a run config pointing the same model_dir. The same model_dir gets called each time i call train on the estimator so it does not matter to me that the graph is recreated again and again.", "@amitlevy21 Thanks for your quick response !  Yes, I meant how do you preserve the trained weights for multiple estimator.train calls. So it looks like we would have to save checkpoints on disk at the end of every estimator.train call before we can call estimator.train on the next task. I guess this approach would be very slow for use cases with multi-task learning where the task type changes after every k steps where k = 1.", "@agupta74 actually I have performance issues when k > 1. Since calling estimator.train also looks for available GPU devices. Not sure why, but the training speed decreases over time when i conduct more than 4 experiments, each of them performing multitask training on 2-3 tasks. \r\nAgain, i think there might be a better design for multitask learning purposes using Keras. ", "It seems like [tf.data.experimental.choose_from_datasets](https://www.tensorflow.org/api_docs/python/tf/data/experimental/choose_from_datasets) might now provide this functionality?", "@strubell I have tried using it for my use case, but i needed a much more flexible way to interleave between the datasets. I also took a look at [tf.data.Dataset.interleave](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#interleave) but struggled a lot for two main reasons:\r\n1. How can i identify which task am i going to train and load its dataset accordingly?\r\n2. How can i interleave between datasets themselves and tasks themselves and apply 1? \r\n\r\nUsing tf.data.experimental.choose_from_dataset and tf.data.Dataset.interleave may solve many use cases. But i could not make it to fit my use case.", "choose_from_datasets takes two parameters: a list of datasets (in your case this would be a list of datasets corresponding to all the tasks you want to train on) and a list of indices into that list (this determines, for a given training step, which task/dataset the batch comes from). The list of indices achieves interleaving. If the problem is knowing which data you're dealing with in the model, I imagine you could pass some indicator in with the data that tells you which task it's for, then use `tf.cond` to execute different tf code depending on the value of that indicator. I could be missing something, though."]}, {"number": 25284, "title": "TensorFlow1.11 estimator train_and_evaluate distributed\uff08MirroredStrategy\uff09\uff0c the sum eval metric \uff08false_negatives\uff0cfalse_positives\uff0ctrue_negatives\uff0c true_positives  \uff09at different GPUs is wrong", "body": "The test set is 1000\r\nwhen I use one GPU\uff0cthe sum of test set is true, 1000\r\nINFO:tensorflow:Saving dict for global step 102: eval_accuracy = 0.762, eval_loss = 0.5891396, **false_negatives = 27.0, false_positives = 211.0**, global_step = 102, loss = 0.5891396, negative_f1_score = 0.715311, negative_precision = 0.9171779, negative_recall = 0.5862745, positive_f1_score = 0.7955326, positive_precision = 0.6869436, positive_recall = 0.94489795, **true_negatives = 299.0, true_positives = 463.0**\r\n\r\nwhen I use two GPU\uff0cthe sum of test set is wrong, 992\r\nINFO:tensorflow:Saving dict for global step 102: eval_accuracy = 0.68245965, eval_loss = 0.6352085, **false_negatives = 134.0, false_positives = 181.0**, global_step = 102, loss = 0.6352085, negative_f1_score = 0.6764509, negative_precision = 0.70869565, negative_recall = 0.64299804, positive_f1_score = 0.6902654, positive_precision = 0.6597744, positive_recall = 0.7237113, **true_negatives = 326.0, true_positives = 351.0**\r\n\r\nwhen I use three GPU\uff0cthe sum of test set is wrong, 984\r\nINFO:tensorflow:Saving dict for global step 102: eval_accuracy = 0.8231707, eval_loss = 0.40057757, **false_negatives = 99.0, false_positives = 75.0**, global_step = 102, loss = 0.40057757, negative_f1_score = 0.83139527, negative_precision = 0.8125, negative_recall = 0.85119045, positive_f1_score = 0.81410253, positive_precision = 0.8355263, positive_recall = 0.79375, **true_negatives = 429.0, true_positives = 381.0**", "comments": ["https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/contrib/distribute/python/values.py#L1130\r\n```\r\n      # TODO(priyag): If dropping remainder is not appropriate, find another\r\n      # approach to distributing the dataset when not possible to divide evenly.\r\n      # Possibly not an issue when we start using PartitionedDataset.\r\n      self._dataset = dataset.batch(len(devices), drop_remainder=True)\r\n```", "@xhyandwyy Could you provide a code to reproduce the bug? Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 25283, "title": "Tensorflow wheel install fails on s390x ", "body": "**System information**\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04, 18.04 s390x\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version: v1.12.0\r\nPython version: 2.7.x\r\nInstalled using virtualenv? pip? conda?: Building from source\r\nBazel version (if compiling from source): v0.15.0\r\nGCC/Compiler version (if compiling from source): 7.3.0 (Ubuntu 18.04), 5.4.0 (Ubuntu 16.04)\r\nCUDA/cuDNN version: NA\r\nGPU model and memory: NA\r\n\r\n\r\n\r\n**Describe the problem**\r\nWe have build TensorFlow v1.12.0 from source on Ubuntu 16.04, 18.04 on s390x platform. Observed below failure on installing wheel ( pip install tensorflow_wheel/tensorflow-1.12.0-cp27-cp27mu-linux_s390x.whl )\r\n```\r\nthird_party/boringssl/include/openssl/base.h:118:2: error: #error \"Unknown target CPU\"\r\n     #error \"Unknown target CPU\"\r\n CompileError: command 's390x-linux-gnu-gcc' failed with exit status 1\r\n```\r\n\r\n**Any other info / logs**\r\nLooks like it initiates grpcio installation which is throwing an error on s390x.\r\n\r\n", "comments": ["For now, I have used export GRPC_PYTHON_BUILD_SYSTEM_OPENSSL=True to use Openssl instead of Boringssl.  \r\nWill it affect performance? Will it affect any functionality of TensorFlow?", "@gunan Any inputs on this?", "looks like we now have another way that enabled boring ssl.\r\nWe are running into this quite frequently now.\r\nMaybe instead of disabling all features that require ssl, we should look into substituting opensll for boringssl  for at least s390x? Or maybe use system openssl installation following examples contributed by @perfinion \r\n\r\nUnless you use distributed training, you should not be affected by GRPC.", "Yeah, in this case unbundling and using openssl is probably much easier.\r\n\r\n`export TF_SYSTEM_LIBS=\"boringssl\"; ./configure`\r\n\r\nthen do the build like normal and it will swap boringssl out and link to regular libssl on your system.\r\nYou can also unbundle and use your system libraries for both grpc and boringssl by doing `export TF_SYSTEM_LIBS=\"boringssl,grpc\"`. I doubt even for distributed training you'd have any performance impact. If you do, your build of grpc/openssl probably just needs building with more optimizations.\r\n\r\nAnother thing to keep in mind is that protobuf gets pretty unhappy if you build with one version and have another at runtime. I've had issues with this because grpc uses protobuf. If you end up hitting this too, just unbundle protobuf as well (or just unbundle everything, look in third_party/systemlibs/syslibs_configure.bzl).", "Thanks for comments @gunan @perfinion \r\nwe will see if we can build TensorFlow with unbundling and using system openssl libs for s390x. \r\nRight now , due to boringssl we have to skip few modules like `core/platform/cloud , contrib/lite , contrib/cloud/ and grpcio `\r\n\r\n", "@Nayana-ibm \r\nPlease let us know if this is still an issue else move this to closed status.", "@Saduf2019 Its still an issue as grpc uses boringssl and boringssl is not supported on s39ox. \r\nHowever we could install TensorFlow wheel with workaround to use Openssl instead of Boringssl.\r\nClosing the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25283\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25283\">No</a>\n"]}, {"number": 25282, "title": "ImportError: cannot import name 'mock' durring import tensorflow", "body": "**System information**\r\n- CentOS Linux release 7.4.1708\r\n- Source, hash: bf4767c6bafb077fc591107691199a6981c29304\r\n- TensorFlow version: 1.12\r\n- Python version: Python 3.4.9\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 0.19.2 (bazel from centos repos)\r\n- GCC/Compiler version (if compiling from source): 4.8.5 (gcc from centos repos)\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n**Describe the problem**\r\n\r\nCannot  import tensorflow due to  import 'mock' library from tensorflow.python.platform.googletest  failling.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nSuccessfully built and installed TensorFlow with the following commands:\r\n\r\n`bazel build --incompatible_remove_native_http_archive=false --incompatible_package_name_is_a_function=false --config=opt //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n`./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg`\r\n\r\n`pip3.4 install --user /tmp/tensorflow_pkg/tensorflow-1.12.0-cp34-cp34m-linux_x86_64.whl`\r\n\r\n> See pip log below for found dependencies\r\n\r\nAttempting to import tensorflow i get this:\r\n\r\n`Python 3.4.9 (default, Aug 14 2018, 21:28:57) `\r\n`[GCC 4.8.5 20150623 (Red Hat 4.8.5-28)] on linux`\r\n`Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.`\r\n`>>> import tensorflow`\r\n`/usr/lib64/python3.4/importlib/_bootstrap.py:321: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as 'np.float64 == np.dtype(float).type'.`\r\n`  return f(*args, **kwds)`\r\n`Traceback (most recent call last):`\r\n`  File \"<stdin>\", line 1, in <module>`\r\n`  File \"/home1/private/mavridis/.local/lib/python3.4/site-packages/tensorflow/__init__.py\", line 34, in <module>`\r\n`    from tensorflow._api.v1 import compat`\r\n`  File \"/home1/private/mavridis/.local/lib/python3.4/site-packages/tensorflow/_api/v1/compat/__init__.py\", line 21, in <module>`\r\n`    from tensorflow._api.v1.compat import v1`\r\n`  File \"/home1/private/mavridis/.local/lib/python3.4/site-packages/tensorflow/_api/v1/compat/v1/__init__.py\", line 69, in <module>`\r\n`    from tensorflow._api.v1.compat.v1 import test`\r\n`  File \"/home1/private/mavridis/.local/lib/python3.4/site-packages/tensorflow/_api/v1/compat/v1/test/__init__.py\", line 24, in <module>`\r\n`    from tensorflow.python.platform.googletest import mock`\r\n`ImportError: cannot import name 'mock'`\r\n\r\n\r\n**Any other info / logs**\r\n\r\npip install output:\r\n\r\n`Processing /tmp/tensorflow_pkg/tensorflow-1.12.0-cp34-cp34m-linux_x86_64.whl`\r\n`Requirement already satisfied: grpcio>=1.8.6 in /usr/lib64/python3.4/site-packages (from tensorflow==1.12.0) (1.10.0)`\r\n`Requirement already satisfied: keras-preprocessing>=1.0.5 in /home1/private/mavridis/.local/lib/python3.4/site-packages (from tensorflow==1.12.0) (1.0.5)`\r\n`Requirement already satisfied: six>=1.10.0 in /home1/private/mavridis/.local/lib/python3.4/site-packages (from tensorflow==1.12.0) (1.12.0)`\r\n`Requirement already satisfied: absl-py>=0.1.6 in /usr/lib/python3.4/site-packages (from tensorflow==1.12.0) (0.1.11)`\r\n`Requirement already satisfied: google-pasta>=0.1.1 in /home1/private/mavridis/.local/lib/python3.4/site-packages (from tensorflow==1.12.0) (0.1.1)`\r\n`Requirement already satisfied: numpy>=1.13.3 in /usr/lib64/python3.4/site-packages (from tensorflow==1.12.0) (1.14.1)`\r\n`Requirement already satisfied: protobuf>=3.6.1 in /home1/private/mavridis/.local/lib/python3.4/site-packages (from tensorflow==1.12.0) (3.6.1)`\r\n`Requirement already satisfied: wheel>=0.26 in /home1/private/mavridis/.local/lib/python3.4/site-packages (from tensorflow==1.12.0) (0.32.3)`\r\n`Requirement already satisfied: tensorboard<1.13.0,>=1.12.0 in /home1/private/mavridis/.local/lib/python3.4/site-packages (from tensorflow==1.12.0) (1.12.2)`\r\n`Requirement already satisfied: astor>=0.6.0 in /usr/lib/python3.4/site-packages (from tensorflow==1.12.0) (0.6.2)`\r\n`Requirement already satisfied: keras-applications>=1.0.6 in /home1/private/mavridis/.local/lib/python3.4/site-packages (from tensorflow==1.12.0) (1.0.6)`\r\n`Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0rc0 in /home1/private/mavridis/.local/lib/python3.4/site-packages (from tensorflow==1.12.0) (1.13.0rc0)`\r\n`Requirement already satisfied: gast>=0.2.0 in /usr/lib/python3.4/site-packages (from tensorflow==1.12.0) (0.2.0)`\r\n`Requirement already satisfied: termcolor>=1.1.0 in /usr/lib/python3.4/site-packages (from tensorflow==1.12.0) (1.1.0)`\r\n`Requirement already satisfied: setuptools in /home1/private/mavridis/.local/lib/python3.4/site-packages (from protobuf>=3.6.1->tensorflow==1.12.0) (40.6.3)`\r\n`Requirement already satisfied: werkzeug>=0.11.10 in /usr/lib64/python3.4/site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0) (0.14.1)`\r\n`Requirement already satisfied: markdown>=2.6.8 in /usr/lib64/python3.4/site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0) (2.6.11)`\r\n`Requirement already satisfied: h5py in /usr/lib64/python3.4/site-packages (from keras-applications>=1.0.6->tensorflow==1.12.0) (2.7.1)`\r\n`Requirement already satisfied: mock>=2.0.0 in /home1/private/mavridis/.local/lib/python3.4/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0rc0->tensorflow==1.12.0) (2.0.0)`\r\n`Requirement already satisfied: pbr>=0.11 in /home1/private/mavridis/.local/lib/python3.4/site-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0rc0->tensorflow==1.12.0) (5.1.1)`\r\n`Installing collected packages: tensorflow`\r\n`Successfully installed tensorflow-1.12.0`\r\n\r\n", "comments": ["By importing mock prior to tensorflow the previous error is replaced by this:\r\n\r\n`\r\n>>> import mock\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home1/private/mavridis/.local/lib/python3.4/site-packages/tensorflow/__init__.py\", line 34, in <module>\r\n    from tensorflow._api.v1 import compat\r\n  File \"/home1/private/mavridis/.local/lib/python3.4/site-packages/tensorflow/_api/v1/compat/__init__.py\", line 21, in <module>\r\n    from tensorflow._api.v1.compat import v1\r\n  File \"/home1/private/mavridis/.local/lib/python3.4/site-packages/tensorflow/_api/v1/compat/v1/__init__.py\", line 26, in <module>\r\n    from tensorflow._api.v1.compat.v1 import app\r\nImportError: cannot import name 'app'\r\n`\r\n\r\nSo i would guess something went horribly wrong with the pip/build ?", "@dmcbeing Could you follow the instructions [here](https://github.com/rnreich/ubuntu-tensorflow-gpu-all-versions) and let us know how it works. I recently used those steps and successfully installed tensorflow-gpu. Thanks!", "I am having the same issue; however, I'm using python 2.7.15 and using bazel 0.21.0.", "I have the same issue, using `pip install tf-nightly` with python 3.6.7 (Anaconda)", "> @dmcbeing Could you follow the instructions [here](https://github.com/rnreich/ubuntu-tensorflow-gpu-all-versions) and let us know how it works. I recently used those steps and successfully installed tensorflow-gpu. Thanks!\r\n\r\nThanks for the guide, i had do change several things:\r\n- You are using Ubuntu, i am using CentOS, so the packages are different. That said, given that the build is successful, i would guess i am not missing any dependencies.\r\n- You seem to use version 1.5, which is ~1 year old, i am trying 1.12, i am not really willing to try with such an old version as it will most likely break existing workloads/networks and also code i am adding( a new platform)\r\n-  I do not need CUDA, so i also said no to the relevant config option.\r\n\r\nGiven all that, i am still getting the same error.\r\n\r\nThanks again", "@dmcbeing In my case I followed that approach till \"git clean -xdf\" command and then \"sudo pip3 install numpy\", \"sudo pip3 install --upgrade tensorflow-gpu\". I now have TF 1.12.\r\n\r\nFor Centos, there are lot of resources online. I found [this](https://nuxnix.wordpress.com/2018/04/11/setup-tensorflow-on-centos-7/) helpful. Thanks!\r\n\r\nWhen you have a right solution, please post it here or in Stackoverflow so that community will benefit. Thanks!", "> @dmcbeing In my case I followed that approach till \"git clean -xdf\" command and then \"sudo pip3 install numpy\", \"sudo pip3 install --upgrade tensorflow-gpu\". I now have TF 1.12.\r\n> \r\n> For Centos, there are lot of resources online. I found [this](https://nuxnix.wordpress.com/2018/04/11/setup-tensorflow-on-centos-7/) helpful. Thanks!\r\n\r\nThanks for the link, i will try it tomorow (the virtualenv trick seems it might help things, as i am on a shared server, so i dont have full controll over the installed packages)\r\n> \r\n> When you have a right solution, please post it here or in Stackoverflow so that community will benefit. Thanks!\r\n\r\nWill do, thank you for the assistance.", "So turns out that with a clean environment (virtualenv) i can build install and then import tensorflow successfully.\r\nUnfortunately i can not diagnose why it would not build successfully without virtualenv and what library causes the issue.\r\nThanks again.\r\n\r\nps:I wont close the issue as others seem to experience the same with the release", "@dmcbeing Thanks! I also see build issues with CentOS. Please check for a solution on GitHub and Stackoverflow. If you find any solution, please post it here also so that community will get benefited. Thanks! Also, please note that at some point we need to close the issue.", "Temporary solution is: afrer install edit two files where importing mock\r\n```\r\n/your-python-path/site-packages/tensorflow/_api/v1/test/__init__.py\r\n/your-python-path/site-packages/tensorflow/_api/v1/compat/v1/test/__init__.py\r\n```\r\nComment this line: from tensorflow.python.platform.googletest import mock\r\nand remove pyc.\r\n", "Met the same issue.\r\nWith today's `pip install tf-nightly-gpu`, archlinux, python 3.6.", "> Temporary solution is: afrer install edit two files where importing mock\r\n> \r\n> ```\r\n> /your-python-path/site-packages/tensorflow/_api/v1/test/__init__.py\r\n> /your-python-path/site-packages/tensorflow/_api/v1/compat/v1/test/__init__.py\r\n> ```\r\n> Comment this line: from tensorflow.python.platform.googletest import mock\r\n> and remove pyc.\r\n\r\nRunning python 3.6.4, tf-nightly-gpu (1.13.0-dev20190214) and CUDA 10.0 on Windows 10 \r\nThis solution worked for me.  ", "actually ,tensorflow using absl for this `mock`,\r\n```\r\nfrom absl.testing.absltest import *\r\n```\r\n\r\nso using `pip install -U absl-py`,when I install absl-py==0.7.0, there is no error", "\r\n\r\n\r\n> \r\n> \r\n> Temporary solution is: afrer install edit two files where importing mock\r\n> \r\n> ```\r\n> /your-python-path/site-packages/tensorflow/_api/v1/test/__init__.py\r\n> /your-python-path/site-packages/tensorflow/_api/v1/compat/v1/test/__init__.py\r\n> ```\r\n> \r\n> Comment this line: from tensorflow.python.platform.googletest import mock\r\n> and remove pyc.\r\n\r\nThis solution does not work for me: Window10, Python 3.6.7, tensorflow 1.12", "> \r\n> \r\n> actually ,tensorflow using absl for this `mock`,\r\n> \r\n> ```\r\n> from absl.testing.absltest import *\r\n> ```\r\n> \r\n> so using `pip install -U absl-py`,when I install absl-py==0.7.0, there is no error\r\n\r\nThanks, this worked", "Seems like this is resolved. Closing this out since I understand it to be resolved, but please let me know if I'm mistaken. Thanks!\r\n", "For me simply doing `pip install mock` worked...", "I had the same issue. What happened to me was a duplicate installation of `absl` under\r\n```\r\n$HOME/.local/lib/python3.5/site-packages\r\n```\r\nwhile there is another installation under\r\n```\r\n/usr/local/lib/python3.5/dist-packages\r\n```\r\nLikely I used `pip3 install` with and without `--user` in the past, which caused the confusion. Deleting `$HOME/.local/lib/python3.5` solved the problem for me (there may be a better way).", "I have the same issue within conda environment after I upgraded tf from 1.13.1 to 1.14.0:\r\n\r\n```\r\n>>>import tensorflow as tf\r\n\r\n... from tensorflow.python.platform.googletest import mock\r\n\r\nImportError: cannot import name 'mock'\r\n```\r\n\r\nMy system is Windows 7, Python 3.6.8 :: Anaconda, Inc.\r\n\r\nI tried `conda install mock` but id didn't help. \r\n\r\nAs i understand form the thread, there is no one solutions of the problem, is it? So maybe it should be reopen, @jvishnuvardhan?", "I'm getting this error with a new checkout of 1.15.0, obtained when installing magenta via \"pip install -e .\" \r\nSeparately installing mock has no effect, because it's looking for it in \"from tensorflow.python.platform.googletest\"\r\n\r\nFor now my workaround is to delete \"from tensorflow.python.platform.googletest\" and just leave the \"import mock\" in each file in which this error arises. \r\n", "> > actually ,tensorflow using absl for this `mock`,\r\n> > ```\r\n> > from absl.testing.absltest import *\r\n> > ```\r\n> > \r\n> > \r\n> > so using `pip install -U absl-py`,when I install absl-py==0.7.0, there is no error\r\n> \r\n> Thanks, this worked\r\n\r\nthis works for me too.  env : running  in docker env, tf==1.14.0, python==3.6.8 "]}, {"number": 25281, "title": "TensorFlow 2.0 Preview - TypeError: 'Attribute' object is not iterable when using tf.function", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, the code is attached below\r\n- OS Platform and Distribution: Arch Linux\r\n- TensorFlow installed from (source or binary): PyPI\r\n- TensorFlow version (use command below): `tf-nightly-gpu-2.0-preview==2.0.0.dev20190129` AKA the latest nightly\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: GTX 1080Ti\r\n\r\n**Describe the current behavior**\r\nRunning the `train()` procedure provided below breaks while using the `@tf.function` decorator.\r\n\r\n**Describe the expected behavior**\r\nNot encountering any errors as per the [\"Effective TensorFlow 2.0 Guide\"](https://github.com/tensorflow/docs/blob/7c9d49ee188c67a315deaf92ebd41fd0f3b15c4a/site/en/r2/guide/effective_tf2.md#combine-tfdatadatasets-and-tffunction)\r\n\r\n**Code to reproduce the issue**\r\n```python\r\n\"\"\"\r\nImplement DCGAN using the new TF 2.0 API.\r\n\r\nAlso test tensorflow-datasets.\r\n\r\nCeleb-A dataset.\r\n\"\"\"\r\n\r\nfrom typing import Dict\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\nfrom tensorflow import keras as k\r\n\r\n\r\ndef bce(x: tf.Tensor, label: tf.Tensor, label_smoothing: float = 0.0) -> tf.Tensor:\r\n    \"\"\"Returns the discrete binary cross entropy between x and the discrete label\r\n    Args:\r\n        x: a 2D tensor\r\n        label: the discrite label, aka, the distribution to match\r\n        label_smoothing: if greater than zero, smooth the labels\r\n\r\n    Returns:\r\n        The binary cros entropy\r\n    \"\"\"\r\n    # FIXME: Fix the warning\r\n    # assert len(x.shape) == 2 and len(label.shape) == 0\r\n\r\n    return k.losses.BinaryCrossentropy()(tf.ones_like(x) * label, x)\r\n\r\n\r\ndef min_max(\r\n    positive: tf.Tensor, negative: tf.Tensor, label_smoothing: float = 0.0\r\n) -> tf.Tensor:\r\n    \"\"\"Returns the discriminator (min max) loss\r\n    Args:\r\n        positive: the discriminator output for the positive class: 2D tensor\r\n        negative: the discriminator output for the negative class: 2D tensor\r\n        smooth: if greater than zero, appiles one-sided label smoothing\r\n    Returns:\r\n        The sum of 2 BCE\r\n    \"\"\"\r\n\r\n    one = tf.constant(1.0)\r\n    zero = tf.constant(0.0)\r\n    d_loss = bce(positive, one, label_smoothing) + bce(negative, zero)\r\n    return d_loss\r\n\r\n\r\nclass Generator(k.Model):\r\n    def __init__(self) -> None:\r\n        super(Generator, self).__init__()\r\n        self.fc1 = k.layers.Dense(4 * 4 * 1024)\r\n        self.batchnorm1 = k.layers.BatchNormalization()\r\n\r\n        self.conv2 = k.layers.Conv2DTranspose(\r\n            filters=512,\r\n            kernel_size=(5, 5),\r\n            strides=(2, 2),\r\n            padding=\"same\",\r\n            use_bias=False,\r\n        )\r\n        self.batchnorm2 = k.layers.BatchNormalization()\r\n\r\n        self.conv3 = k.layers.Conv2DTranspose(\r\n            filters=256,\r\n            kernel_size=(5, 5),\r\n            strides=(2, 2),\r\n            padding=\"same\",\r\n            use_bias=False,\r\n        )\r\n        self.batchnorm3 = k.layers.BatchNormalization()\r\n\r\n        self.conv4 = k.layers.Conv2DTranspose(\r\n            filters=128,\r\n            kernel_size=(5, 5),\r\n            strides=(2, 2),\r\n            padding=\"same\",\r\n            use_bias=False,\r\n        )\r\n        self.batchnorm4 = k.layers.BatchNormalization()\r\n\r\n        self.conv5 = k.layers.Conv2DTranspose(\r\n            filters=3,\r\n            kernel_size=(5, 5),\r\n            strides=(2, 2),\r\n            padding=\"same\",\r\n            use_bias=False,\r\n        )\r\n        self.batchnorm5 = k.layers.BatchNormalization()\r\n\r\n    def call(self, x: tf.Tensor, training: bool = True) -> tf.Tensor:\r\n        x = self.fc1(x)\r\n        x = self.batchnorm1(x, training=training)\r\n        x = tf.nn.relu(x)\r\n        x = tf.reshape(x, shape=(-1, 4, 4, 1024))\r\n\r\n        x = self.conv2(x)\r\n        x = self.batchnorm2(x, training=training)\r\n        x = tf.nn.relu(x)\r\n\r\n        x = self.conv3(x)\r\n        x = self.batchnorm3(x, training=training)\r\n        x = tf.nn.relu(x)\r\n\r\n        x = self.conv4(x)\r\n        x = self.batchnorm4(x, training=training)\r\n        x = tf.nn.relu(x)\r\n\r\n        x = self.conv5(x)\r\n        x = self.batchnorm5(x, training=training)\r\n\r\n        x = tf.nn.tanh(x)\r\n        return x\r\n\r\n\r\nclass Discriminator(k.Model):\r\n    def __init__(self):\r\n        super(Discriminator, self).__init__()\r\n        self.conv1 = k.layers.Conv2D(128, (5, 5), strides=(2, 2), padding=\"same\")\r\n        self.conv2 = k.layers.Conv2D(256, (5, 5), strides=(2, 2), padding=\"same\")\r\n        self.batchnorm2 = k.layers.BatchNormalization()\r\n        self.conv3 = k.layers.Conv2D(512, (5, 5), strides=(2, 2), padding=\"same\")\r\n        self.batchnorm3 = k.layers.BatchNormalization()\r\n        self.conv4 = k.layers.Conv2D(1024, (5, 5), strides=(2, 2), padding=\"same\")\r\n        self.batchnorm4 = k.layers.BatchNormalization()\r\n        self.flatten = k.layers.Flatten()\r\n        self.fc5 = k.layers.Dense(1)\r\n\r\n    def call(self, x, training=True):\r\n        x = self.conv1(x)\r\n        x = tf.nn.leaky_relu(x)\r\n\r\n        x = self.conv2(x)\r\n        x = self.batchnorm2(x)\r\n        x = tf.nn.leaky_relu(x)\r\n\r\n        x = self.conv3(x)\r\n        x = self.batchnorm3(x)\r\n        x = tf.nn.leaky_relu(x)\r\n\r\n        x = self.conv4(x)\r\n        x = self.batchnorm4(x)\r\n        x = tf.nn.leaky_relu(x)\r\n\r\n        x = self.flatten(x)\r\n        x = self.fc5(x)\r\n        return x\r\n\r\n\r\nclass GAN:\r\n    def __init__(self, generator, discriminator, encoder=None):\r\n        \"\"\"\r\n        GAN initializer.\r\n\r\n        Args:\r\n            generator: A ``tensorflow.keras.Model`` to use as Generator.\r\n            discriminator: A ``tensorflow.keras.Model`` to use as Discriminator.\r\n            encoder: A ``tensorflow.keras.Model`` to use as Encoder.\r\n\r\n        Returns:\r\n            Trained GAN model (?).\r\n\r\n        \"\"\"\r\n        self.G = generator()\r\n        self.D = discriminator()\r\n        self.E = encoder() if encoder is not None else None\r\n        self.latent_vector_dims = 100\r\n\r\n        self.G_opt = k.optimizers.Adam(learning_rate=1e-5, beta_1=0.5)\r\n        self.D_opt = k.optimizers.Adam(learning_rate=1e-5, beta_1=0.5)\r\n\r\n    @tf.function()\r\n    def train(self, dataset: tf.data.Dataset):\r\n        \"\"\"\r\n        Train.\r\n        \"\"\"\r\n        for step, features in enumerate(dataset, start=1):\r\n            x = features[\"image\"]\r\n            z = tf.random.normal((x.shape[0], self.latent_vector_dims))\r\n\r\n            # We record all the operations in the tape\r\n            with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\r\n                G_z = self.G(z, training=True)\r\n\r\n                D_x = self.D(x, training=True)\r\n                D_Gz = self.D(G_z, training=True)\r\n\r\n                g_loss = bce(D_Gz, tf.constant(1.0))\r\n                d_loss = min_max(D_x, D_Gz, label_smoothing=0.0)\r\n\r\n            # We retrieve the gradients from our records\r\n            G_grads = gen_tape.gradient(g_loss, self.G.trainable_variables)\r\n            D_grads = disc_tape.gradient(d_loss, self.D.trainable_variables)\r\n\r\n            # Optimize and apply the gradients\r\n            self.G_opt.apply_gradients(zip(G_grads, self.G.trainable_variables))\r\n            self.D_opt.apply_gradients(zip(D_grads, self.D.trainable_variables))\r\n\r\n            if step % 10 == 0:\r\n                print(f\"--------------------------\")\r\n                print(f\"STEP: {step}\")\r\n                print(f\"D_LOSS: {d_loss}\")\r\n                print(f\"G_LOSS: {g_loss}\")\r\n\r\n\r\nclass InputPipeline:\r\n    def __init__(\r\n        self, dataset, batch_size, epochs, shuffle_buffer, prefetched_items, size\r\n    ):\r\n        self.batch_size = batch_size\r\n        self.dataset_name = dataset\r\n        self.epochs = epochs\r\n        self.prefetched_items = prefetched_items\r\n        self.shuffle_buffer = shuffle_buffer\r\n        self.size = size\r\n\r\n    def get_input_fn(self) -> tf.data.Dataset:\r\n        \"\"\"Input fn.\"\"\"\r\n        return self.input_fn\r\n\r\n    def load_public_dataset(self):\r\n        \"\"\"\r\n        Load one of the publicly available datasets, will merge together all the splits.\r\n\r\n        Args:\r\n            chosen_dataset: dataset to use.\r\n\r\n        Return:\r\n            The chosen dataset as a ``tf.data.Dataset``\r\n\r\n        \"\"\"\r\n        # Construct a tf.data.Dataset\r\n        datasets = tfds.load(name=self.dataset_name, split=tfds.Split.ALL)\r\n        return datasets\r\n\r\n    def resize_images(self, features: Dict) -> Dict:\r\n        \"\"\"\r\n        Overwrite the \\\"image\\\" feature in order to resize them.\r\n\r\n        Args:\r\n            features: features dictionary.\r\n            size: desired target size.\r\n\r\n        Returns:\r\n            Features with \\\"image\\\" resized to the correct shape.\r\n\r\n        \"\"\"\r\n        features[\"image\"] = tf.image.resize(features[\"image\"], self.size)\r\n        return features\r\n\r\n    def input_fn(self):\r\n        dataset = self.load_public_dataset()\r\n        dataset = (\r\n            dataset.map(self.resize_images)\r\n            .shuffle(self.shuffle_buffer)\r\n            .batch(self.batch_size)\r\n            .prefetch(self.prefetched_items)\r\n            .repeat(self.epochs)\r\n        )\r\n        return dataset\r\n\r\n\r\ndef main():\r\n\r\n    # TODO: replace with CLI\r\n    CHOICE = \"celeb_a\"\r\n    EPOCHS = 10\r\n    BATCH_SIZE = 64\r\n    PREFETCH = 10\r\n    SHUFFLE_BUFFER = 10000\r\n\r\n    # See available datasets\r\n    public_datasets = tfds.list_builders()\r\n\r\n    gan = GAN(Generator, Discriminator)\r\n    input_pipeline = InputPipeline(\r\n        dataset=CHOICE,\r\n        batch_size=BATCH_SIZE,\r\n        epochs=EPOCHS,\r\n        prefetched_items=PREFETCH,\r\n        shuffle_buffer=SHUFFLE_BUFFER,\r\n        size=(64, 64),\r\n    )\r\n    dataset = input_pipeline.input_fn()\r\n    gan.train(dataset=dataset)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n**Other info / logs**\r\n\r\n**Full Traceback**\r\n```\r\nTraceback (most recent call last):\r\n  File \"dcgan-tf2.py\", line 289, in <module>\r\n    main()\r\n  File \"dcgan-tf2.py\", line 285, in main\r\n    gan.train(dataset=dataset)\r\n  File \"/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 383, in __call__\r\n    self._initialize(args, kwds)\r\n  File \"/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 355, in _initialize\r\n    *args, **kwds))\r\n  File \"/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1097, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1322, in _maybe_define_function\r\n    arg_names=arg_names), self._function_attributes)\r\n  File \"/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\", line 540, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 298, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1803, in bound_method_wrapper\r\n    return wrapped_fn(weak_instance(), *args, **kwargs)\r\n  File \"/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\", line 533, in wrapper\r\n    ), *args, **kwargs)\r\n  File \"/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 293, in converted_call\r\n    experimental_partial_types=partial_types)\r\n  File \"/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 415, in to_graph\r\n    arg_values, arg_types)\r\n  File \"/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py\", line 222, in entity_to_graph\r\n    entity_to_graph(candidate, program_ctx, {}, {})\r\n  File \"/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py\", line 175, in entity_to_graph\r\n    node, name, ns = function_to_graph(o, program_ctx, arg_values, arg_types)\r\n  File \"/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py\", line 376, in function_to_graph\r\n    node = node_to_graph(node, context)\r\n  File \"/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py\", line 435, in node_to_graph\r\n    node = converter.apply_(node, context, call_trees)\r\n  File \"/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/autograph/core/converter.py\", line 507, in apply_\r\n    node = converter_module.transform(node, context)\r\n  File \"/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/autograph/converters/call_trees.py\", line 350, in transform\r\n    return CallTreeTransformer(ctx).visit(node)\r\n  File \"/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/autograph/core/converter.py\", line 440, in visit\r\n    return super(Base, self).visit(node)\r\n  File \"/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/transformer.py\", line 484, in visit\r\n    result = super(Base, self).visit(node)\r\n  File \"/usr/lib64/python3.6/ast.py\", line 253, in visit\r\n    return visitor(node)\r\n  File \"/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/autograph/converters/call_trees.py\", line 282, in visit_FunctionDef\r\n    node.returns = self.visit_block(node.returns)\r\n  File \"/home/ubik/.python_envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/transformer.py\", line 368, in visit_block\r\n    for node in nodes:\r\nTypeError: 'Attribute' object is not iterable\r\n```\r\n\r\nI had previously opened a [question on SO](https://stackoverflow.com/questions/54369539/tensorflow-2-0-preview-typeerror-attribute-object-is-not-iterable-when-usin).\r\n\r\nEDIT: even writing a simpler input pipeline, dropping `tensorflow-datasets` and using the builtin Keras datasets the error persists.\r\n\r\nCC @galeone", "comments": ["Actually we have hunted down the issue to the type annotations in the two losses.\r\n\r\nIf we remove the return type we get `tf is undefined`, if we remove all the annotations the train under `tf.function` works properly.", "@mr-ubik Is there any issue still or is it okay to be closed? Thanks!", "@jvishnuvardhan I guess you could close it, though the issue with type hints remains.", "This should be open.\r\n\r\n@alexbw we should support type annotations. With Py3 these will be increasingly pervasive and not supporting them will cause trouble. ", "@mr-ubik we should at least say clearly that type annotations are not supported in autograph. Better, throw a meaningful error. \r\n\r\nA real fix may take longer. ", "I agree that type annotations should not blow us up. Noted.\r\n\r\nAlso, actually *using* type information will be important in the future, as it will help us catch \"easy\" cases when type information has been user-provided. We have avoided that strategy so far because we can't unambiguously determine type in all cases for Python, so focused on the purely runtime-type-discovery strategy. Adding even a little bit of ahead-of-time type information will help, and we'll get there.\r\n\r\n@mdanatg @aaandrewww @brilee ", "Thank you for the detailed investigation!\r\n\r\nThankfully, this is a simpler bug in autograph and only incidentally caused by annotations (could have been any other field). We have a change in progress that will land over the next few days, and that should fix this particular error.\r\n\r\nWe didn't run extensive tests with type-annotated code, which is why this bug slipped though (so there may be more), but I'm not aware of any reasons why annotations should not be supported, in the sense that autograph should just let them pass through. Anything that doesn't do that should be a bug, please let us know about it!\r\n\r\nWill update this thread as the fix lands.\r\n\r\nCheers,\r\nDan", "Quick update - The TypeError has now been fixed, but a second fix is needed to make sure the symbols that the type annotations refer to resolve during the conversion process. The fix is largely a refactoring, and should land over the coming weeks - will post an update here.", "Just double checked this against tf-nightly and it runs. BTW, I had to make a few changes to get it to run well with tf.function, mainly in the train function. See below:\r\n\r\n```\r\nfrom typing import Dict\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\nfrom tensorflow import keras as k\r\n\r\n\r\ndef bce(x: tf.Tensor, label: tf.Tensor, label_smoothing: float = 0.0) -> tf.Tensor:\r\n    \"\"\"Returns the discrete binary cross entropy between x and the discrete label\r\n    Args:\r\n        x: a 2D tensor\r\n        label: the discrite label, aka, the distribution to match\r\n        label_smoothing: if greater than zero, smooth the labels\r\n\r\n    Returns:\r\n        The binary cros entropy\r\n    \"\"\"\r\n    # FIXME: Fix the warning\r\n    # assert len(x.shape) == 2 and len(label.shape) == 0\r\n\r\n    return k.losses.BinaryCrossentropy()(tf.ones_like(x) * label, x)\r\n\r\n\r\ndef min_max(\r\n    positive: tf.Tensor, negative: tf.Tensor, label_smoothing: float = 0.0\r\n) -> tf.Tensor:\r\n    \"\"\"Returns the discriminator (min max) loss\r\n    Args:\r\n        positive: the discriminator output for the positive class: 2D tensor\r\n        negative: the discriminator output for the negative class: 2D tensor\r\n        smooth: if greater than zero, appiles one-sided label smoothing\r\n    Returns:\r\n        The sum of 2 BCE\r\n    \"\"\"\r\n\r\n    one = tf.constant(1.0)\r\n    zero = tf.constant(0.0)\r\n    d_loss = bce(positive, one, label_smoothing) + bce(negative, zero)\r\n    return d_loss\r\n\r\n\r\nclass Generator(k.Model):\r\n    def __init__(self) -> None:\r\n        super(Generator, self).__init__()\r\n        self.fc1 = k.layers.Dense(4 * 4 * 1024)\r\n        self.batchnorm1 = k.layers.BatchNormalization()\r\n\r\n        self.conv2 = k.layers.Conv2DTranspose(\r\n            filters=512,\r\n            kernel_size=(5, 5),\r\n            strides=(2, 2),\r\n            padding=\"same\",\r\n            use_bias=False,\r\n        )\r\n        self.batchnorm2 = k.layers.BatchNormalization()\r\n\r\n        self.conv3 = k.layers.Conv2DTranspose(\r\n            filters=256,\r\n            kernel_size=(5, 5),\r\n            strides=(2, 2),\r\n            padding=\"same\",\r\n            use_bias=False,\r\n        )\r\n        self.batchnorm3 = k.layers.BatchNormalization()\r\n\r\n        self.conv4 = k.layers.Conv2DTranspose(\r\n            filters=128,\r\n            kernel_size=(5, 5),\r\n            strides=(2, 2),\r\n            padding=\"same\",\r\n            use_bias=False,\r\n        )\r\n        self.batchnorm4 = k.layers.BatchNormalization()\r\n\r\n        self.conv5 = k.layers.Conv2DTranspose(\r\n            filters=3,\r\n            kernel_size=(5, 5),\r\n            strides=(2, 2),\r\n            padding=\"same\",\r\n            use_bias=False,\r\n        )\r\n        self.batchnorm5 = k.layers.BatchNormalization()\r\n\r\n    def call(self, x: tf.Tensor, training: bool = True) -> tf.Tensor:\r\n        x = self.fc1(x)\r\n        x = self.batchnorm1(x, training=training)\r\n        x = tf.nn.relu(x)\r\n        x = tf.reshape(x, shape=(-1, 4, 4, 1024))\r\n\r\n        x = self.conv2(x)\r\n        x = self.batchnorm2(x, training=training)\r\n        x = tf.nn.relu(x)\r\n\r\n        x = self.conv3(x)\r\n        x = self.batchnorm3(x, training=training)\r\n        x = tf.nn.relu(x)\r\n\r\n        x = self.conv4(x)\r\n        x = self.batchnorm4(x, training=training)\r\n        x = tf.nn.relu(x)\r\n\r\n        x = self.conv5(x)\r\n        x = self.batchnorm5(x, training=training)\r\n\r\n        x = tf.nn.tanh(x)\r\n        return x\r\n\r\n\r\nclass Discriminator(k.Model):\r\n    def __init__(self):\r\n        super(Discriminator, self).__init__()\r\n        self.conv1 = k.layers.Conv2D(128, (5, 5), strides=(2, 2), padding=\"same\")\r\n        self.conv2 = k.layers.Conv2D(256, (5, 5), strides=(2, 2), padding=\"same\")\r\n        self.batchnorm2 = k.layers.BatchNormalization()\r\n        self.conv3 = k.layers.Conv2D(512, (5, 5), strides=(2, 2), padding=\"same\")\r\n        self.batchnorm3 = k.layers.BatchNormalization()\r\n        self.conv4 = k.layers.Conv2D(1024, (5, 5), strides=(2, 2), padding=\"same\")\r\n        self.batchnorm4 = k.layers.BatchNormalization()\r\n        self.flatten = k.layers.Flatten()\r\n        self.fc5 = k.layers.Dense(1)\r\n\r\n    def call(self, x, training=True):\r\n        x = self.conv1(x)\r\n        x = tf.nn.leaky_relu(x)\r\n\r\n        x = self.conv2(x)\r\n        x = self.batchnorm2(x)\r\n        x = tf.nn.leaky_relu(x)\r\n\r\n        x = self.conv3(x)\r\n        x = self.batchnorm3(x)\r\n        x = tf.nn.leaky_relu(x)\r\n\r\n        x = self.conv4(x)\r\n        x = self.batchnorm4(x)\r\n        x = tf.nn.leaky_relu(x)\r\n\r\n        x = self.flatten(x)\r\n        x = self.fc5(x)\r\n        return x\r\n\r\n\r\nclass GAN:\r\n    def __init__(self, generator, discriminator, encoder=None):\r\n        \"\"\"\r\n        GAN initializer.\r\n\r\n        Args:\r\n            generator: A ``tensorflow.keras.Model`` to use as Generator.\r\n            discriminator: A ``tensorflow.keras.Model`` to use as Discriminator.\r\n            encoder: A ``tensorflow.keras.Model`` to use as Encoder.\r\n\r\n        Returns:\r\n            Trained GAN model (?).\r\n\r\n        \"\"\"\r\n        self.G = generator()\r\n        self.D = discriminator()\r\n        self.E = encoder() if encoder is not None else None\r\n        self.latent_vector_dims = 100\r\n\r\n        self.G_opt = k.optimizers.Adam(learning_rate=1e-5, beta_1=0.5)\r\n        self.D_opt = k.optimizers.Adam(learning_rate=1e-5, beta_1=0.5)\r\n\r\n    @tf.function()\r\n    def train(self, dataset: tf.data.Dataset):\r\n        \"\"\"\r\n        Train.\r\n        \"\"\"\r\n        step = tf.constant(0)\r\n        for features in dataset:\r\n            step += 1\r\n\r\n            x = features[\"image\"]\r\n            z = tf.random.normal((tf.shape(x)[0], self.latent_vector_dims))\r\n\r\n            # We record all the operations in the tape\r\n            with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\r\n                G_z = self.G(z, training=True)\r\n\r\n                D_x = self.D(x, training=True)\r\n                D_Gz = self.D(G_z, training=True)\r\n\r\n                g_loss = bce(D_Gz, tf.constant(1.0))\r\n                d_loss = min_max(D_x, D_Gz, label_smoothing=0.0)\r\n\r\n            # We retrieve the gradients from our records\r\n            G_grads = gen_tape.gradient(g_loss, self.G.trainable_variables)\r\n            D_grads = disc_tape.gradient(d_loss, self.D.trainable_variables)\r\n\r\n            # Optimize and apply the gradients\r\n            self.G_opt.apply_gradients(zip(G_grads, self.G.trainable_variables))\r\n            self.D_opt.apply_gradients(zip(D_grads, self.D.trainable_variables))\r\n\r\n            if tf.equal(step % 10, 0):\r\n                tf.print(\"--------------------------\")\r\n                tf.print(\"STEP:\", step)\r\n                tf.print(\"D_LOSS:\", d_loss)\r\n                tf.print(\"G_LOSS:\", g_loss)\r\n\r\n\r\nclass InputPipeline:\r\n    def __init__(\r\n        self, dataset, batch_size, epochs, shuffle_buffer, prefetched_items, size\r\n    ):\r\n        self.batch_size = batch_size\r\n        self.dataset_name = dataset\r\n        self.epochs = epochs\r\n        self.prefetched_items = prefetched_items\r\n        self.shuffle_buffer = shuffle_buffer\r\n        self.size = size\r\n\r\n    def get_input_fn(self) -> tf.data.Dataset:\r\n        \"\"\"Input fn.\"\"\"\r\n        return self.input_fn\r\n\r\n    def load_public_dataset(self):\r\n        \"\"\"\r\n        Load one of the publicly available datasets, will merge together all the splits.\r\n\r\n        Args:\r\n            chosen_dataset: dataset to use.\r\n\r\n        Return:\r\n            The chosen dataset as a ``tf.data.Dataset``\r\n\r\n        \"\"\"\r\n        # Construct a tf.data.Dataset\r\n        datasets = tfds.load(name=self.dataset_name, split=tfds.Split.ALL)\r\n        return datasets\r\n\r\n    def resize_images(self, features: Dict) -> Dict:\r\n        \"\"\"\r\n        Overwrite the \\\"image\\\" feature in order to resize them.\r\n\r\n        Args:\r\n            features: features dictionary.\r\n            size: desired target size.\r\n\r\n        Returns:\r\n            Features with \\\"image\\\" resized to the correct shape.\r\n\r\n        \"\"\"\r\n        features[\"image\"] = tf.image.resize(features[\"image\"], self.size)\r\n        return features\r\n\r\n    def input_fn(self):\r\n        dataset = self.load_public_dataset()\r\n        dataset = (\r\n            dataset.map(self.resize_images)\r\n            .take(10 * self.batch_size)\r\n            .shuffle(self.shuffle_buffer)\r\n            .batch(self.batch_size)\r\n            .prefetch(self.prefetched_items)\r\n            .repeat(self.epochs)\r\n        )\r\n        return dataset\r\n\r\n\r\ndef main():\r\n\r\n    tf.autograph.set_verbosity(0, True)\r\n\r\n    # TODO: replace with CLI\r\n    CHOICE = \"celeb_a\"\r\n    EPOCHS = 10\r\n    BATCH_SIZE = 64\r\n    PREFETCH = 10\r\n    SHUFFLE_BUFFER = 10000\r\n\r\n    # See available datasets\r\n    public_datasets = tfds.list_builders()\r\n\r\n    gan = GAN(Generator, Discriminator)\r\n    input_pipeline = InputPipeline(\r\n        dataset=CHOICE,\r\n        batch_size=BATCH_SIZE,\r\n        epochs=EPOCHS,\r\n        prefetched_items=PREFETCH,\r\n        shuffle_buffer=SHUFFLE_BUFFER,\r\n        size=(64, 64),\r\n    )\r\n    dataset = input_pipeline.input_fn()\r\n    gan.train(dataset=dataset)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```"]}, {"number": 25280, "title": "Added New Google Activation function Swish", "body": "New activation function performance better than ReLU", "comments": ["@tanzhenyu & @rthadur , can you pls review the PR", "@pavithrasv & @rthadur , can you pls review the PR", "Nagging Reviewer @pavithrasv: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 46 days with no activity and the `awaiting review` label has been applied.", "I think we should add this to add-ons for now and and merge into core based on how widely it is used. https://github.com/tensorflow/addons/tree/master/tensorflow_addons", "I'm going to go ahead and close this PR, If you're still interested in pursing this (and responding to above comments), please feel free to reopen!\r\n"]}, {"number": 25279, "title": "tf.contrib.opt.ScipyOptimizerInterface error", "body": "Hello.\r\n\r\nI tried to use tf.contrib.opt.ScipyOptimizerInterface with an example code from your documentation.\r\n`vector = tf.Variable([7., 7.], 'vector')`\r\n`loss = tf.reduce_sum(tf.square(vector))`\r\n`optimizer = tf.contrib.opt.ScipyOptimizerInterface(loss, options={'maxiter': 100})`\r\n`with tf.Session() as session:`\r\n`---->optimizer.minimize(session)`.\r\n\r\nI am getting the following error: `Tensors in list passed to 'values' of 'ConcatV2' Op have types [int32, float32, float32, float32, float32, float32, float32, float32, float32, float32, int32, float32, float32, float32, float32, int32, int32, float32, float32, float32, float32] that don't all match.`\r\n\r\nOS Platform and Distribution: Ubuntu 18.04.1\r\nTensorFlow installed from: source\r\nTensorFlow version: 1.12.0\r\nPython version: 3.6\r\nBazel version: 0.21.0\r\nGCC version: 7.3.0\r\nCUDA and GPU: no running on GPU, because I don't have GPU.\r\n\r\nThank you!", "comments": ["ScipyOptimizer is moving from contrib to tensorflow/addons in https://github.com/tensorflow/addons/issues/3  . I'm afraid that it would be a while, at least, before we could look into the issue. cc @seanpmorgan", "@facaiy is correct it will take some time before we're in a position to accept questions for addons ExternalOptimizers. As part of the moving and upgrading code from contrib to addons, we also need to step through the code and make sure we understand exactly how it works. \r\n\r\nIn the mean time, I'll try to do some troubleshooting -- but can not give any concrete time estimates.", "Contrib has been depreciated here in Tensorflow repo and moved to tensorflow/addons , please reopen the issue in  https://github.com/tensorflow/addons/issues if this exists in latest version. Thank you "]}, {"number": 25278, "title": "Reducing the binary size of tflite library for android", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: all\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version:1.10\r\n- Python version:.3.6\r\n- Installed using virtualenv? pip? conda?:conda\r\n- Bazel version (if compiling from source):1.15\r\n- GCC/Compiler version (if compiling from source):5.4\r\n- CUDA/cuDNN version:8.0\r\n- GPU model and memory:16GB\r\n\r\nI build the libtensorflowLite.so following the https://github.com/tensorflow/tensorflow/issues/19642#issuecomment-422697028, and i can use it in the android project . But the .so of the  libtensorflowLite.so is about 3.7M. From the document ,it should be hundreds of KB.\r\nHere is my build commit:\r\n```\r\nbazel build -c opt //tensorflow/contrib/lite:libtensorflowLite.so \r\n --crosstool_top=//external:android/crosstool \r\n --cpu=arm64-v8a\r\n --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\n --cxxopt=\"-std=c++11\" \r\n --verbose_failures\r\n\r\n```\r\nSo, could someone helps me, how can I reduce the size of tflite library?", "comments": ["libtensorflowLite.so memory size significantly contributed by the Kernels for various operators, the simplest way to reduce the size is to remove unnecessary ops kernels from compiling into the lib, which are not required for your model. So you can just compile libtensorflowLite.so with the required OPs for your model only.\r\n\r\nAlso suggest if it is not mandatory, try to use latest tensorflow version, in that way you can get benefit of recent tools/customization added into it. ", "@ANSHUMAN87 Tanks for reply. I just want keep all the ops as a platform . May the configuration and compile setting can help reduce the size.? I think 3.7M is too big even with all the ops. Do you have some ideas. Looking forward to your reply.\r\n\r\n", "Hi @holyhao, I'm landing a change which adds a proper C++ shared library target for TensorFlow Lite (tensorflow/lite:libtensorflowlite.so). The (Android) binary size for arm should be ~1MB, and arm64 ~1.5MB (this includes the full set of builtin ops). You likely won't be able to get much smaller than that unless you use selective op registration as noted by @ANSHUMAN87.", "The main reason you're seeing a much larger binary size is that the suggested build target you referenced includes *all* symbols in the shared library, not just the symbols that are part of the public C/C++ API. Please follow issue #20905 for further updates.", "Duplicate of #20905 ", "@ANSHUMAN87 Is there documentation on how to do the selective registration? The tflite documentation points to a script that says it has been deprecated.", "@lenaevans : You can follow steps mentioned in #20905. Now the size has reduced much.\r\n\r\nSelective Registration is in future plan, not yet supported i guess. \r\nRefer: https://www.tensorflow.org/lite/guide/ops_select\r\n\r\nBut you can always do it manually.", ">\r\n\r\n\r\n\r\n> \r\n> \r\n> Hi @holyhao, I'm landing a change which adds a proper C++ shared library target for TensorFlow Lite (tensorflow/lite:libtensorflowlite.so). The (Android) binary size for arm should be ~1MB, and arm64 ~1.5MB (this includes the full set of builtin ops). You likely won't be able to get much smaller than that unless you use selective op registration as noted by @ANSHUMAN87.\r\n\r\nbuild cmd\r\n\r\n>   bazel build --config monolithic --config=android_arm64 --cxxopt=-std=c++11 -c opt  \\\r\n  --crosstool_top=//external:android/crosstool \\\r\n  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n --cpu=arm64-v8a --verbose_failures \\\r\n  //tensorflow/lite/examples/l17:libProphet.so\r\n\r\nget ~3.6m , 2.8m after strip ops\u3002\r\n what\u2018s wrong with me ?", "What other dependencies/code do you have in `libProphet.so`? ", "> \r\n> \r\n> What other dependencies/code do you have in `libProphet.so`?\r\n\r\n> cc_binary(\r\n    name = \"libProphet.so\",\r\n    srcs = [\r\n        \"Prophet.cc\",\r\n        \"Prophet.h\",\r\n    ],\r\n    copts = tflite_copts(),\r\n    defines = [\r\n        \"TFLITE_CUSTOM_OPS_HEADER\",\r\n        \"TFLITE_WITH_RUY\",\r\n    ],\r\n    linkopts = tflite_linkopts() + select({\r\n        \"//tensorflow:android\": [\r\n            \"-pie\",  # Android 5.0 and later supports only PIE\r\n            \"-lm\",  # some builtin ops, e.g., tanh, need -lm\r\n        ],\r\n        #        \"//conditions:default\": [],\r\n    }),\r\n    linkshared = True,\r\n    deps = [\r\n        #        \":custom_ops\",\r\n        \"//tensorflow/lite:framework\",\r\n        #        \"//tensorflow/lite/c:c_api_internal\",\r\n        \"//tensorflow/lite/kernels:builtin_ops\",\r\n        #        \"//tensorflow/lite/kernels:builtin_op_kernels\",\r\n        #        \"//tensorflow/lite:builtin_op_data\",\r\n        #        \"//tensorflow/lite/schema:schema_fbs\",\r\n        #        \"//tensorflow/lite:util\",\r\n    ],\r\n)\r\n", "Which NDK version are you using? "]}, {"number": 25277, "title": "build tensorflow-r1.13 fails", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Red Hat 4.8.2-16\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):source \r\n- TensorFlow version:r1.13\r\n- Python version:python2.7\r\n- Installed using virtualenv? pip? conda?:bazel\r\n- Bazel version (if compiling from source):0.22.0, 0.19.1, 0.18.0\r\n- GCC/Compiler version (if compiling from source):4.8.2\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:CPU\r\n\r\n\r\n\r\n**Describe the problem**\r\ncannot fetch @bazel_toolchains when building tensorflow-r1.13\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\nStarting local Bazel server and connecting to it...\r\nINFO: Invocation ID: 8c0b66e8-db86-4ac7-a5db-052c331926db\r\nERROR: error loading package '': Encountered error while reading extension file 'repositories/repositories.bzl': no such package '@bazel_toolchains//repositories': java.io.IOException: thread interrupted\r\nERROR: error loading package '': Encountered error while reading extension file 'repositories/repositories.bzl': no such package '@bazel_toolchains//repositories': java.io.IOException: thread interrupted\r\nINFO: Elapsed time: 142.844s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    Fetching @bazel_toolchains; fetching 140s\r\n", "comments": ["my fault\uff0chttp proxy error", "How do you solve proxy error?(From Tencent...)"]}, {"number": 25276, "title": "build failstensorflow-r1.13", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["misoperation"]}, {"number": 25275, "title": "bazel build aot armeabi-v7a so failed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nmacOS 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version:\r\n1.12.0\r\n- Python version:\r\n3.6.2\r\n- Installed using virtualenv? pip? conda?:\r\nconda\r\n- Bazel version (if compiling from source):\r\n0.21.0\r\n- ndk version\r\n14b\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\nno\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI have tried to perform a simple demo in //tensorflow/compiler/aot/test/test_graph_tfmatmul.pb by using this command:\r\nbazel build //tensorflow/compiler/aot/tests:my_library\r\nand have succeeded in the building procedure.\r\n\r\nHowever, when I tried to generate a .so file for an android project I got failed. The command I used is as below:\r\n```text\r\nbazel build --crosstool_top=@androidndk//:default_crosstool  --cpu=armeabi-v7a  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=\"-std=c++11\" //tensorflow/compiler/aot/tests:my_library\r\n```\r\nI don't know how to fix the problem. Please help.  The error message is as below:\r\n\r\n\r\n```text\r\n/private/var/tmp/_bazel_sunday/7a01d1437f77ea8b21ffc614971e2517/external/mkl_dnn/BUILD.bazel:71:1: C++ compilation of rule '@mkl_dnn//:mkldnn_single_threaded' failed (Exit 1)\r\nIn file included from external/mkl_dnn/src/common/utils.cpp:22:\r\nIn file included from external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/xmmintrin.h:27:\r\nexternal/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:47:5: error: use of undeclared identifier '__builtin_ia32_emms'; did you mean '__builtin_isless'?\r\n    __builtin_ia32_emms();\r\n    ^\r\nexternal/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:47:5: note: '__builtin_isless' declared here\r\nexternal/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:47:25: error: too few arguments to function call, expected 2, have 0\r\n    __builtin_ia32_emms();\r\n                        ^\r\nexternal/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:64:19: error: use of undeclared identifier '__builtin_ia32_vec_init_v2si'\r\n    return (__m64)__builtin_ia32_vec_init_v2si(__i, 0);\r\n                  ^\r\nexternal/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:81:12: error: use of undeclared identifier '__builtin_ia32_vec_ext_v2si'\r\n    return __builtin_ia32_vec_ext_v2si((__v2si)__m, 0);\r\n           ^\r\nexternal/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:143:19: error: use of undeclared identifier '__builtin_ia32_packsswb'\r\n    return (__m64)__builtin_ia32_packsswb((__v4hi)__m1, (__v4hi)__m2);\r\n                  ^\r\nexternal/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:173:19: error: use of undeclared identifier '__builtin_ia32_packssdw'\r\n    return (__m64)__builtin_ia32_packssdw((__v2si)__m1, (__v2si)__m2);\r\n                  ^\r\nexternal/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:203:19: error: use of undeclared identifier '__builtin_ia32_packuswb'\r\n    return (__m64)__builtin_ia32_packuswb((__v4hi)__m1, (__v4hi)__m2);\r\n                  ^\r\nexternal/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:230:19: error: use of undeclared identifier '__builtin_ia32_punpckhbw'\r\n    return (__m64)__builtin_ia32_punpckhbw((__v8qi)__m1, (__v8qi)__m2);\r\n                  ^\r\nexternal/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:253:19: error: use of undeclared identifier '__builtin_ia32_punpckhwd'\r\n    return (__m64)__builtin_ia32_punpckhwd((__v4hi)__m1, (__v4hi)__m2);\r\n                  ^\r\nexternal/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:274:19: error: use of undeclared identifier '__builtin_ia32_punpckhdq'\r\n    return (__m64)__builtin_ia32_punpckhdq((__v2si)__m1, (__v2si)__m2);\r\n                  ^\r\nexternal/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:301:19: error: use of undeclared identifier '__builtin_ia32_punpcklbw'\r\n    return (__m64)__builtin_ia32_punpcklbw((__v8qi)__m1, (__v8qi)__m2);\r\n                  ^\r\nexternal/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:324:19: error: use of undeclared identifier '__builtin_ia32_punpcklwd'\r\n    return (__m64)__builtin_ia32_punpcklwd((__v4hi)__m1, (__v4hi)__m2);\r\n                  ^\r\nexternal/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:345:19: error: use of undeclared identifier '__builtin_ia32_punpckldq'\r\n    return (__m64)__builtin_ia32_punpckldq((__v2si)__m1, (__v2si)__m2);\r\n                  ^\r\nexternal/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:366:19: error: use of undeclared identifier '__builtin_ia32_paddb'; did you mean '__builtin_arm_qadd'?\r\n    return (__m64)__builtin_ia32_paddb((__v8qi)__m1, (__v8qi)__m2);\r\n                  ^\r\nexternal/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:366:19: note: '__builtin_arm_qadd' declared here\r\nexternal/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:366:40: error: cannot initialize a parameter of type 'int' with an rvalue of type '__v8qi' (vector of 8 'char' values)\r\n    return (__m64)__builtin_ia32_paddb((__v8qi)__m1, (__v8qi)__m2);\r\n                                       ^~~~~~~~~~~~\r\nexternal/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:387:19: error: use of undeclared identifier '__builtin_ia32_paddw'; did you mean '__builtin_arm_qadd'?\r\n    return (__m64)__builtin_ia32_paddw((__v4hi)__m1, (__v4hi)__m2);\r\n                  ^\r\nexternal/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:366:19: note: '__builtin_arm_qadd' declared here\r\n    return (__m64)__builtin_ia32_paddb((__v8qi)__m1, (__v8qi)__m2);\r\n                  ^\r\nexternal/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:387:40: error: cannot initialize a parameter of type 'int' with an rvalue of type '__v4hi' (vector of 4 'short' values)\r\n    return (__m64)__builtin_ia32_paddw((__v4hi)__m1, (__v4hi)__m2);\r\n                                       ^~~~~~~~~~~~\r\nexternal/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:408:19: error: use of undeclared identifier '__builtin_ia32_paddd'; did you mean '__builtin_arm_qadd'?\r\n    return (__m64)__builtin_ia32_paddd((__v2si)__m1, (__v2si)__m2);\r\n                  ^\r\nexternal/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:366:19: note: '__builtin_arm_qadd' declared here\r\n    return (__m64)__builtin_ia32_paddb((__v8qi)__m1, (__v8qi)__m2);\r\n                  ^\r\nexternal/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/../lib64/clang/3.8.275480/include/mmintrin.h:408:40: error: cannot initialize a parameter of type 'int' with an rvalue of type '__v2si' (vector of 2 'int' values)\r\n    return (__m64)__builtin_ia32_paddd((__v2si)__m1, (__v2si)__m2);\r\n                                       ^~~~~~~~~~~~\r\nfatal error: too many errors emitted, stopping now [-ferror-limit=]\r\n20 errors generated.\r\nTarget //tensorflow/compiler/aot/tests:my_library failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1.407s, Critical Path: 0.28s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n```\r\nthe content of the bazel BUILD file:\r\n```text\r\nload(\"//tensorflow/compiler/aot:tfcompile.bzl\", \"tf_library\")\r\n\r\ntf_library(\r\n    name = \"test_graph_tfmatmul\",\r\n    cpp_class = \"foo::bar::MatMulComp\",\r\n    graph = \"test_graph_tfmatmul.pb\",\r\n    config = \"test_graph_tfmatmul.config.pbtxt\",\r\n)\r\n\r\ncc_library(\r\n    name = \"my_library\",\r\n    srcs = [\r\n        \"my_code.cc\",  # include test_graph_tfmatmul.h to access the generated header\r\n    ],\r\n    hdrs = [\"my_code.h\"],\r\n    deps = [\r\n        \":test_graph_tfmatmul\",  # link in the generated object file\r\n        \"//third_party/eigen3\",\r\n    ],\r\n    linkopts = [\r\n        \"-lpthread\",\r\n    ]\r\n)\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@sundayxcn Could you provide more details on the context and the issue? Thanks!", "> @sundayxcn Could you provide more details on the context and the issue? Thanks!\r\n\r\n\r\n@jvishnuvardhan \r\n\r\nAt first I pulled the tensorflow source code from the branch master\r\nThe demo file is from the path of  /tensorflow/compiler/aot/tests\r\n\r\nStep 1:  Run python make_simple.py and generate the .pb file\r\n```python\r\nimport argparse\r\nimport os\r\nimport sys\r\n\r\nfrom tensorflow.core.protobuf import saver_pb2\r\nfrom tensorflow.python.client import session\r\nfrom tensorflow.python.framework import constant_op\r\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.framework import function\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.ops import array_ops\r\nfrom tensorflow.python.ops import control_flow_ops\r\nfrom tensorflow.python.ops import math_ops\r\nfrom tensorflow.python.ops import variables\r\nfrom tensorflow.python.platform import app\r\nfrom tensorflow.python.training import saver as saver_lib\r\n\r\nFLAGS = None\r\n\r\ndef tfmatmul(_):\r\n  x = array_ops.placeholder(dtypes.float32, name='x_hold')\r\n  y = array_ops.placeholder(dtypes.float32, name='y_hold')\r\n  math_ops.matmul(x, y, name='x_y_prod')\r\n\r\ndef tfmatmulandadd(_):\r\n  \r\n  x = array_ops.placeholder(dtypes.float32, name='x_hold')\r\n  y = array_ops.placeholder(dtypes.float32, name='y_hold')\r\n  math_ops.matmul(x, y, name='x_y_prod')\r\n  math_ops.add(x, y, name='x_y_sum')\r\n\r\ndef write_graph(build_graph, out_dir):\r\n  \"\"\"Build a graph using build_graph and write it out.\"\"\"\r\n  g = ops.Graph()\r\n  with g.as_default():\r\n    build_graph(out_dir)\r\n    filename = os.path.join(out_dir, 'test_graph_%s.pb' % build_graph.__name__)\r\n    with open(filename, 'wb') as f:\r\n      f.write(g.as_graph_def().SerializeToString())\r\n\r\ndef main(_):\r\n  write_graph(tfmatmul, FLAGS.out_dir)\r\n\r\nif __name__ == '__main__':\r\n  parser = argparse.ArgumentParser()\r\n  parser.register('type', 'bool', lambda v: v.lower() == 'true')\r\n  parser.add_argument(\r\n      '--out_dir',\r\n      type=str,\r\n      default='',\r\n      help='Output directory for graphs, checkpoints and savers.')\r\n  FLAGS, unparsed = parser.parse_known_args()\r\n  app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n```\r\n\r\nstep 2:( After running step1 the test_graph_tfmatmul.pb file is obtained)\r\n\r\n Step 2.1 \r\nI create files of my_code.cc and my_code.h\r\nmy_code.cc\r\n```C++\r\n#define EIGEN_USE_THREADS\r\n#define EIGEN_USE_CUSTOM_THREAD_POOL\r\n\r\n#include <iostream>\r\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\r\n#include \"tensorflow/compiler/aot/tests/test_graph_tfmatmul.h\" // generated\r\n#include \"my_code.h\"\r\n\r\nint my_code::compare(){\r\n    Eigen::ThreadPool tp(2);  // Size the thread pool as appropriate.\r\n    Eigen::ThreadPoolDevice device(&tp, tp.NumThreads());\r\n\r\n    foo::bar::MatMulComp matmul;\r\n    matmul.set_thread_pool(&device);\r\n\r\n    // Set up args and run the computation.\r\n    const float args[15] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,13,14,15};\r\n    std::copy(args + 0, args + 6, matmul.arg0_data());\r\n    std::copy(args + 6, args + 12, matmul.arg1_data());\r\n    matmul.Run();\r\n\r\n    // Check result\r\n    if (matmul.result0(0, 0) == 58) {\r\n        std::cout << \"Success\" << std::endl;\r\n    } else {\r\n        std::cout << \"Failed. Expected value 58 at 0,0. Got:\"\r\n                    << matmul.result0(0, 0) << std::endl;\r\n    }\r\n\r\n    return 0;\r\n}\r\n```\r\n\r\nmy_code.h\r\n```C++\r\nclass my_code\r\n{\r\npublic:\r\n    int compare();\r\n}\r\n```\r\n\r\nstep 2.2:\r\n I changed the BUILD File as follows:\r\n```text\r\nload(\"//tensorflow/compiler/aot:tfcompile.bzl\", \"tf_library\")\r\n\r\ntf_library(\r\nname = \"test_graph_tfmatmul\",\r\ncpp_class = \"foo::bar::MatMulComp\",\r\ngraph = \"test_graph_tfmatmul.pb\",\r\nconfig = \"test_graph_tfmatmul.config.pbtxt\",\r\n)\r\n\r\ncc_library(\r\nname = \"my_library\",\r\nsrcs = [\r\n\"my_code.cc\", # include test_graph_tfmatmul.h to access the generated header\r\n],\r\nhdrs = [\"my_code.h\"],\r\ndeps = [\r\n\":test_graph_tfmatmul\", # link in the generated object file\r\n\"//third_party/eigen3\",\r\n],\r\nlinkopts = [\r\n\"-lpthread\",\r\n]\r\n)\r\n```\r\n\r\nStep3: Run the bazel build to generate the .so file\r\n```text\r\nbazel build --crosstool_top=@androidndk//:default_crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=\"-std=c++11\" //tensorflow/compiler/aot/tests:my_library\r\n```\r\nthe error message hints about mkl_dnn as you can see in my first post. I added --config=mkl in the command \r\n\r\n```text\r\nbazel build --crosstool_top=@androidndk//:default_crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=\"-std=c++11\" --config=mkl //tensorflow/compiler/aot/tests:my_library \r\n```\r\nbut got failed again:\r\n\r\n```text\r\nINFO: Invocation ID: c9afb568-3d69-4fb3-bae5-27dd141e2357\r\nINFO: Build options have changed, discarding analysis cache.\r\nINFO: Analysed target //tensorflow/compiler/aot/tests:my_library (96 packages loaded, 11819 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /Users/apple/tmp/tensorflow/tensorflow/compiler/xla/service/cpu/BUILD:525:1: C++ compilation of rule '//tensorflow/compiler/xla/service/cpu:runtime_conv2d' failed (Exit 1)\r\nIn file included from tensorflow/compiler/xla/service/cpu/runtime_conv2d.cc:21:\r\nIn file included from ./tensorflow/compiler/xla/service/cpu/runtime_conv2d_impl.h:18:\r\nIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:111:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDimensions.h:340:167: error: non-constant-expression cannot be narrowed from type 'long long' to 'int' in initializer list [-Wc++11-narrowing]\r\n  EIGEN_STRONG_INLINE explicit DSizes(DenseIndex firstDimension, DenseIndex secondDimension, IndexTypes... otherDimensions) : Base({{firstDimension, secondDimension, otherDimensions...}}) {\r\n                                                                                                                                                                      ^~~~~~~~~~~~~~~\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorMap.h:69:131: note: in instantiation of function template specialization 'Eigen::DSizes<int, 4>::DSizes<long long, long long>' requested here\r\n    EIGEN_STRONG_INLINE TensorMap(PointerArgType dataPtr, Index firstDimension, IndexTypes... otherDimensions) : m_data(dataPtr), m_dimensions(firstDimension, otherDimensions...) {\r\n                                                                                                                                  ^\r\n./tensorflow/compiler/xla/service/cpu/runtime_conv2d_impl.h:40:7: note: in instantiation of function template specialization 'Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, int>, 16, MakePointer>::TensorMap<long long, long long, long long>' requested here\r\n      input(lhs, input_batch, input_rows, input_cols, input_channels);\r\n      ^\r\ntensorflow/compiler/xla/service/cpu/runtime_conv2d.cc:37:20: note: in instantiation of function template specialization 'tensorflow::xla::EigenConvImpl<Eigen::ThreadPoolDevice, float>' requested here\r\n  tensorflow::xla::EigenConvImpl(\r\n                   ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDimensions.h:340:167: note: insert an explicit cast to silence this issue\r\n  EIGEN_STRONG_INLINE explicit DSizes(DenseIndex firstDimension, DenseIndex secondDimension, IndexTypes... otherDimensions) : Base({{firstDimension, secondDimension, otherDimensions...}}) {\r\n                                                                                                                                                                      ^~~~~~~~~~~~~~~\r\n                                                                                                                                                                      static_cast<int>( )\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDimensions.h:340:167: error: non-constant-expression cannot be narrowed from type 'long long' to 'int' in initializer list [-Wc++11-narrowing]\r\n  EIGEN_STRONG_INLINE explicit DSizes(DenseIndex firstDimension, DenseIndex secondDimension, IndexTypes... otherDimensions) : Base({{firstDimension, secondDimension, otherDimensions...}}) {\r\n                                                                                                                                                                      ^~~~~~~~~~~~~~~\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDimensions.h:340:167: note: insert an explicit cast to silence this issue\r\n  EIGEN_STRONG_INLINE explicit DSizes(DenseIndex firstDimension, DenseIndex secondDimension, IndexTypes... otherDimensions) : Base({{firstDimension, secondDimension, otherDimensions...}}) {\r\n                                                                                                                                                                      ^~~~~~~~~~~~~~~\r\n                                                                                                                                                                      static_cast<int>( )\r\n2 errors generated.\r\nTarget //tensorflow/compiler/aot/tests:my_library failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 51.010s, Critical Path: 12.17s\r\nINFO: 20 processes: 20 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n@jvishnuvardhan  \r\nI have't found any document on how to build an android project to support .so file (containing a pre-trained model ) on tensorflow aot guide.  The command I found is from [bazel guide](\r\nhttps://docs.bazel.build/versions/master/android-ndk.html)\r\n\r\nand I tried to implement a helloword.cc (just return \"hellowolrd\") which was successful. However, when I tried to embed the pre-trained model test_graph_tfmatmul I got failed. I guess the tensorflow configure should be changed in some way but I can't figure it out.\r\n\r\nThank you very much!          \r\n     ", "@sundayxcn Try\r\n```\r\nbazel build --config android_arm \\\r\n--cxxopt=-std=c++11 --cxxopt=-Wno-c++11-narrowing \\\r\n//tensorflow/compiler/aot/tests:my_library\r\n```\r\nHoweverr, `cc_library()` is for creating static libraries (.a), to create .so you should use [`cc_binary()`](https://docs.bazel.build/versions/master/be/c-cpp.html#cc_binary), it should be something like,\r\n```\r\ncc_binary(\r\n    name = \"libmylib.so\",\r\n    linkshared=True,\r\n    srcs = [\r\n        \"my_code.cc\", \r\n        \"my_code.h\", \r\n    ],\r\n    deps = [\r\n        \":test_graph_tfmatmul\",\r\n        \"//third_party/eigen3\",\r\n    ],\r\n)\r\n```", "\r\n@freedomtan , Hi, thanks for your quick reply. I have adopted your suggestions and built the .so library successfully a moment ago. However, while I am trying calling the **compare** method mentioned, see above **step 2.1** in before attached script. The Android Studio gives the error traceback like following:\r\n```text\r\n  Process: com.example.sunday.ndkdemo, PID: 28571\r\n    java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol \"ceilf\" referenced by \"/data/app/com.example.sunday.ndkdemo-B05bPAF3wQDe9L865kV88Q==/lib/arm/libmylib.so\"...\r\n        at java.lang.Runtime.loadLibrary0(Runtime.java:1016)\r\n        at java.lang.System.loadLibrary(System.java:1672)\r\n        at com.example.sunday.ndkdemo.MainActivity.<clinit>(MainActivity.java:11)\r\n        at java.lang.Class.newInstance(Native Method)\r\n        at android.app.AppComponentFactory.instantiateActivity(AppComponentFactory.java:69)\r\n```\r\nI believe this is caused by the lack of **ceilf** method in my .so library.  But in the **compare** method, I only easy imported ''foo::bar::MatMulComp matmul'' and did some simple calculations. Now, I am confused why there were concerned about the **ceilf** in my library.  PS: Also, the resulted .so library is around 30mb+, after imported and built in Android APK, it has been converted to 600KB only in APK!!?? \r\nCould you help me to check this problem. please? Hope to hear from you soon! Thanks in advance!", "`ceilf()` is a standard C library function, should be in `libm`. Add something like `linkopts=\"-lm\"` to your `cc_binary(....)` could help. Add `-s` to your bazel build command:\r\n```\r\nbazel build --config android_arm \\\r\n--cxxopt=-std=c++11 --cxxopt=-Wno-c++11-narrowing \\\r\n//tensorflow/compiler/aot/tests:my_library -s\r\n```\r\nThen you can learn why you got a huge .so.", "> `ceilf()` is a standard C library function, should be in `libm`. Add something like `linkopts=\"-lm\"` to your `cc_binary(....)` could help. Add `-s` to your bazel build command:\r\n> \r\n> ```\r\n> bazel build --config android_arm \\\r\n> --cxxopt=-std=c++11 --cxxopt=-Wno-c++11-narrowing \\\r\n> //tensorflow/compiler/aot/tests:my_library -s\r\n> ```\r\n> Then you can learn why you got a huge .so.\r\n@freedomtan  According to your advice, i change the cc_binary,\r\n```text\r\ncc_binary(\r\n    name = \"libmy_library.so\",\r\n    linkshared=True,\r\n    srcs = [\r\n        \"my_code.cc\", \r\n        \"my_code.h\", \r\n    ],\r\n    deps = [\r\n        \":test_graph_tfmatmul\",\r\n        \"//third_party/eigen3\",\r\n    ],\r\n    linkopts=[\"-lm\"],\r\n)\r\n```\r\n\r\nThere are different error:\r\n```text\r\njava.lang.UnsatisfiedLinkError: dlopen failed: library \"../../../../src/main/jniLibs/armeabi/libmy_library.so\" not found\r\n        at java.lang.Runtime.loadLibrary0(Runtime.java:989)\r\n        at java.lang.System.loadLibrary(System.java:1562)\r\n        at com.example.sunday.ndkdemo.MainActivity.<clinit>(MainActivity.java:12)\r\n        at java.lang.Class.newInstance(Native Method)\r\n```\r\nbut my android project  configuration files still unchanged,only change the libmy_library.so . \r\n( i check the apk. the package include libmy_library.so.)\r\nI tested it several times. if i remove linkopts=[\"-lm\"], \r\nerrmsg:\r\n```text\r\njava.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol \"ceilf\" referenced by \"/data/app/com.example.sunday.ndkdemo-B05bPAF3wQDe9L865kV88Q==/lib/arm/libmylib.so\"...\r\n```\r\nif i restore linkopts=[\"-lm\"], \r\nerrmsg:\r\n```text\r\njava.lang.UnsatisfiedLinkError: dlopen failed: library \"../../../../src/main/jniLibs/armeabi/libmy_library.so\" not found\r\n```\r\nso I'm sure there's still a problem about so. Please check it for me. thank you...\r\n\r\n\r\n\r\n", "Are there any additional logcat messages surrounding the dlopen failure?", "@jdduke @freedomtan \r\nthe dlopen message has no more message. and i change the bazel command :\r\n```text\r\nbazel build --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=-std=c++11 --cxxopt=-Wno-c++11-narrowing //tensorflow/compiler/aot/tests:my_library\r\n```\r\nshow linking error:\r\n```text\r\nnote: 'CFURLCreateDataAndPropertiesFromResource' has been explicitly marked deprecated here\r\nBoolean CFURLCreateDataAndPropertiesFromResource(CFAllocatorRef alloc, CFURLRef url, CFDataRef *resourceData, CFDictionaryRef *properties, CFArrayRef desiredProperties, SInt32 *errorCode) API_DEPRECATED(\"For resource data, use the CFReadStream API. For file resource properties, use CFURLCopyResourcePropertiesForKeys.\", macos(10.0,10.9), ios(2.0,7.0), watchos(2.0,2.0), tvos(9.0,9.0));\r\n        ^\r\n1 warning generated.\r\nINFO: From Linking external/curl/libcurl.a [for host]:\r\n/Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/curl/_objs/curl/asyn-ares.o has no symbols\r\n/Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/curl/_objs/curl/curl_multibyte.o has no symbols\r\n/Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/curl/_objs/curl/curl_rtmp.o has no symbols\r\n/Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/curl/_objs/curl/curl_sspi.o has no symbols\r\n/Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/curl/_objs/curl/curl_threads.o has no symbols\r\n/Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/curl/_objs/curl/hostasyn.o has no symbols\r\n/Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/curl/_objs/curl/hostcheck.o has no symbols\r\n/Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/curl/_objs/curl/hostip4.o has no symbols\r\n/Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/curl/_objs/curl/inet_pton.o has no symbols\r\n/Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/curl/_objs/curl/krb5.o has no symbols\r\n/Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/curl/_objs/curl/md4.o has no symbols\r\n/Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/curl/_objs/curl/memdebug.o has no symbols\r\n/Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/curl/_objs/curl/nwlib.o has no symbols\r\n/Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/curl/_objs/curl/nwos.o has no symbols\r\n/Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/curl/_objs/curl/security.o has no symbols\r\n/Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/curl/_objs/curl/strtok.o has no symbols\r\nINFO: From Compiling tensorflow/core/grappler/optimizers/pin_to_host_optimizer.cc [for host]:\r\nIn file included from tensorflow/core/grappler/optimizers/pin_to_host_optimizer.cc:16:\r\n./tensorflow/core/grappler/optimizers/pin_to_host_optimizer.h:56:26: warning: private field 'opt_level_' is not used [-Wunused-private-field]\r\n  RewriterConfig::Toggle opt_level_;\r\n                         ^\r\n1 warning generated.\r\nINFO: From Linking tensorflow/compiler/aot/tfcompile [for host]:\r\nld: warning: text-based stub file /System/Library/Frameworks//IOKit.framework/IOKit.tbd and library file /System/Library/Frameworks//IOKit.framework/IOKit are out of sync. Falling back to library file for linking.\r\nld: warning: text-based stub file /System/Library/Frameworks//CoreFoundation.framework/CoreFoundation.tbd and library file /System/Library/Frameworks//CoreFoundation.framework/CoreFoundation are out of sync. Falling back to library file for linking.\r\nld: warning: text-based stub file /System/Library/Frameworks//Security.framework/Security.tbd and library file /System/Library/Frameworks//Security.framework/Security are out of sync. Falling back to library file for linking.\r\nINFO: From Executing genrule //tensorflow/compiler/aot/tests:gen_test_graph_tfmatmul:\r\n2019-02-18 13:19:30.705553: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX\r\nERROR: /Users/sunday/tensorflow/tensorflow/compiler/aot/tests/BUILD:15:1: Linking of rule '//tensorflow/compiler/aot/tests:my_library' failed (Exit 1)\r\nclang: error: no such file or directory: 'libmy_library.so'\r\nTarget //tensorflow/compiler/aot/tests:my_library failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1399.023s, Critical Path: 231.09s\r\nINFO: 2563 processes: 2563 local.\r\n\r\n```\r\nbut the tensorflow binary is from resource code compile \r\n```text\r\nbazel build  --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-mavx2 //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n", "apparently, it failed because of this line\r\n```\r\njava.lang.UnsatisfiedLinkError: dlopen failed: library \"../../../../src/main/jniLibs/armeabi/libmy_library.so\" not found\r\n```\r\nThis probably is caused by some link flag which wrote `../../../../src/main/jniLibs/armeabi/libmy_library.so` into your `libmy_library.so`. You may want to check what link flags are used (with `-s` in bazel command) and inspect your `libmy_library.so` (by something like `readelf -a`) before using it. If you don't how to do these, you may want to check with your local compile / tool chain expert to get quick response.", "@freedomtan  @jdduke please help my check my latest action,thank you !!\r\n\r\nthe message  \r\n```text\r\n2019-02-18 13:19:30.705553: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX \r\n```\r\nis that mean the aot cant support arm-v7a?\r\n\r\n> ```\r\n> bazel build --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=-std=c++11 --cxxopt=-Wno-c++11-narrowing //tensorflow/compiler/aot/tests:my_library\r\n> ```\r\n> show linking error:\r\n> \r\n> ```\r\n> note: 'CFURLCreateDataAndPropertiesFromResource' has been explicitly marked deprecated here\r\n> Boolean CFURLCreateDataAndPropertiesFromResource(CFAllocatorRef alloc, CFURLRef url, CFDataRef *resourceData, CFDictionaryRef *properties, CFArrayRef desiredProperties, SInt32 *errorCode) API_DEPRECATED(\"For resource data, use the CFReadStream API. For file resource properties, use CFURLCopyResourcePropertiesForKeys.\", macos(10.0,10.9), ios(2.0,7.0), watchos(2.0,2.0), tvos(9.0,9.0));\r\n>         ^\r\n> 1 warning generated.\r\n> INFO: From Linking external/curl/libcurl.a [for host]:\r\n> /Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/curl/_objs/curl/asyn-ares.o has no symbols\r\n> /Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/curl/_objs/curl/curl_multibyte.o has no symbols\r\n> /Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/curl/_objs/curl/curl_rtmp.o has no symbols\r\n> /Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/curl/_objs/curl/curl_sspi.o has no symbols\r\n> /Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/curl/_objs/curl/curl_threads.o has no symbols\r\n> /Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/curl/_objs/curl/hostasyn.o has no symbols\r\n> /Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/curl/_objs/curl/hostcheck.o has no symbols\r\n> /Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/curl/_objs/curl/hostip4.o has no symbols\r\n> /Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/curl/_objs/curl/inet_pton.o has no symbols\r\n> /Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/curl/_objs/curl/krb5.o has no symbols\r\n> /Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/curl/_objs/curl/md4.o has no symbols\r\n> /Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/curl/_objs/curl/memdebug.o has no symbols\r\n> /Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/curl/_objs/curl/nwlib.o has no symbols\r\n> /Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/curl/_objs/curl/nwos.o has no symbols\r\n> /Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/curl/_objs/curl/security.o has no symbols\r\n> /Library/Developer/CommandLineTools/usr/bin/libtool: file: bazel-out/host/bin/external/curl/_objs/curl/strtok.o has no symbols\r\n> INFO: From Compiling tensorflow/core/grappler/optimizers/pin_to_host_optimizer.cc [for host]:\r\n> In file included from tensorflow/core/grappler/optimizers/pin_to_host_optimizer.cc:16:\r\n> ./tensorflow/core/grappler/optimizers/pin_to_host_optimizer.h:56:26: warning: private field 'opt_level_' is not used [-Wunused-private-field]\r\n>   RewriterConfig::Toggle opt_level_;\r\n>                          ^\r\n> 1 warning generated.\r\n> INFO: From Linking tensorflow/compiler/aot/tfcompile [for host]:\r\n> ld: warning: text-based stub file /System/Library/Frameworks//IOKit.framework/IOKit.tbd and library file /System/Library/Frameworks//IOKit.framework/IOKit are out of sync. Falling back to library file for linking.\r\n> ld: warning: text-based stub file /System/Library/Frameworks//CoreFoundation.framework/CoreFoundation.tbd and library file /System/Library/Frameworks//CoreFoundation.framework/CoreFoundation are out of sync. Falling back to library file for linking.\r\n> ld: warning: text-based stub file /System/Library/Frameworks//Security.framework/Security.tbd and library file /System/Library/Frameworks//Security.framework/Security are out of sync. Falling back to library file for linking.\r\n> INFO: From Executing genrule //tensorflow/compiler/aot/tests:gen_test_graph_tfmatmul:\r\n> 2019-02-18 13:19:30.705553: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX\r\n> ERROR: /Users/sunday/tensorflow/tensorflow/compiler/aot/tests/BUILD:15:1: Linking of rule '//tensorflow/compiler/aot/tests:my_library' failed (Exit 1)\r\n> clang: error: no such file or directory: 'libmy_library.so'\r\n> Target //tensorflow/compiler/aot/tests:my_library failed to build\r\n> Use --verbose_failures to see the command lines of failed build steps.\r\n> INFO: Elapsed time: 1399.023s, Critical Path: 231.09s\r\n> INFO: 2563 processes: 2563 local.\r\n> ```\r\n> but the tensorflow binary is from resource code compile\r\n> \r\n> ```\r\n> bazel build  --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-mavx2 //tensorflow/tools/pip_package:build_pip_package\r\n> ```\r\n\r\n", "> bazel build --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=-std=c++11 --cxxopt=-Wno-c++11-narrowing //tensorflow/compiler/aot/tests:my_library\r\n\r\nHave you run `./configure` and located the Android toolchain from your root checkout directory?\r\n\r\n> clang: error: no such file or directory: 'libmy_library.so'\r\n\r\nThis looks like an issue with your build rule, are you referring to the .so directly or indirectly from the .so target itself?\r\n\r\n\r\n", "@sundayxcn I checked output .so of bazel, it won't cause\r\n```\r\njava.lang.UnsatisfiedLinkError: dlopen failed: library \"../../../../src/main/jniLibs/armeabi/libmy_library.so\" not found\r\n\r\n```\r\nShould be caused by your settings when you built your .so in Android Studio. Surely you won't find ``` \"../../../../src/main/jniLibs/armeabi/libmy_library.so\"``` on your Android devices. The path is supposed to be used during linking time on your host machine only."]}, {"number": 25274, "title": "tensorflow.python.framework.errors_impl.InternalError: tensorflow/core/kernels/cuda_solvers.cc:803: cuBlas call failed status = 13", "body": "Hello ,I met a mistake when using tensorflow 1.10.0 as follows. I install tensorflow 1.10.0 with command `pip install tensorflow-gpu ==1.10.0` \r\nMore information:\r\nsystem:ubuntu 16.04\r\nGPU\uff1aRTX2080\r\nCUDA: 9.0\r\ncudnn:7.4.1\r\npython: 2.7/3.6\r\nWhen run the code ,we get a mistake about cuBlas and the status is 13. We know little about  this and we try using docker to run this but failed.Can you help me? Thanks!  \r\nCode:\r\n```\r\ndef input_fn_train():\r\n        return input_function(True, flags.data_dir, flags.batch_size,\r\n                              flags.epochs_per_eval, flags.num_parallel_calls,\r\n                              flags.multi_gpu)\r\n    if flags.mode == tf.estimator.ModeKeys.EVAL:\r\n        eval_results = estimator.evaluate(input_fn=input_fn_eval,steps=flags.max_train_steps)\r\n        print(eval_results)\r\n\r\n    if flags.mode == tf.estimator.ModeKeys.TRAIN:\r\n        for _ in range(flags.train_epochs // flags.epochs_per_eval):\r\n            train_hooks = hooks_helper.get_train_hooks([\"LoggingTensorHook\"], batch_size=flags.batch_size)\r\n\r\n            print('Starting a training cycle.')\r\n            estimator.train(input_fn=input_fn_train,\r\n                            max_steps=flags.max_train_steps)\r\n\r\n            print('Starting to evaluate.')\r\n            eval_results = estimator.evaluate(input_fn=input_fn_eval,\r\n                                                steps=flags.max_train_steps)\r\n            print(eval_results)\r\n```\r\n\r\nMistake:\r\n```\r\n2019-01-29 12:12:50.442014: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x55bd7c128050\r\n2019-01-29 12:12:53.677220: W tensorflow/core/framework/op_kernel.cc:1275] OP_REQUIRES failed at matrix_inverse_op.cc:191 : Internal: tensorflow/core/kernels/cuda_solvers.cc:803: cuBlas call failed status = 13\r\nTraceback (most recent call last):\r\n  File \"/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1278, in _do_call\r\n    return fn(*args)\r\n  File \"/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1263, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1350, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InternalError: tensorflow/core/kernels/cuda_solvers.cc:803: cuBlas call failed status = 13\r\n\t [[Node: s2/MatrixInverse = MatrixInverse[T=DT_FLOAT, adjoint=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](s2/Reshape_2)]]\r\n\t [[Node: s2/transform/ImageProjectiveTransform/_2343 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1510_s2/transform/ImageProjectiveTransform\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"DAN_V2.py\", line 141, in <module>\r\n    tf.app.run(argv=sys.argv)\r\n  File \"/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"DAN_V2.py\", line 137, in main\r\n    dan_run_loop.dan_main(flags,vgg16_model_fn,input_function)\r\n  File \"/home/qiujia/DAN/DAN_V2/dan_run_loop.py\", line 203, in dan_main\r\n    max_steps=flags.max_train_steps)\r\n  File \"/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 376, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1145, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1173, in _train_model_default\r\n    saving_listeners)\r\n  File \"/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1451, in _train_with_estimator_spec\r\n    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n  File \"/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 583, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1059, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1150, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/six.py\", line 686, in reraise\r\n    raise value\r\n  File \"/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1135, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1207, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 987, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 877, in run\r\n    run_metadata_ptr)\r\n  File \"/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1100, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1272, in _do_run\r\n    run_metadata)\r\n  File \"/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1291, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: tensorflow/core/kernels/cuda_solvers.cc:803: cuBlas call failed status = 13\r\n\t [[Node: s2/MatrixInverse = MatrixInverse[T=DT_FLOAT, adjoint=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](s2/Reshape_2)]]\r\n\t [[Node: s2/transform/ImageProjectiveTransform/_2343 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1510_s2/transform/ImageProjectiveTransform\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nCaused by op 's2/MatrixInverse', defined at:\r\n  File \"DAN_V2.py\", line 141, in <module>\r\n    tf.app.run(argv=sys.argv)\r\n  File \"/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"DAN_V2.py\", line 137, in main\r\n    dan_run_loop.dan_main(flags,vgg16_model_fn,input_function)\r\n  File \"/home/qiujia/DAN/DAN_V2/dan_run_loop.py\", line 203, in dan_main\r\n    max_steps=flags.max_train_steps)\r\n  File \"/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 376, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1145, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1170, in _train_model_default\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1133, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"DAN_V2.py\", line 131, in vgg16_model_fn\r\n    multi_gpu=params['multi_gpu'])\r\n  File \"/home/qiujia/DAN/DAN_V2/dan_run_loop.py\", line 102, in dan_model_fn\r\n    mean_shape,imgs_mean,imgs_std)\r\n  File \"/home/qiujia/DAN/DAN_V2/dan_model.py\", line 152, in __call__\r\n    inputs = self.__affine_image(inputs_imgs,r,t)\r\n  File \"/home/qiujia/DAN/DAN_V2/dan_model.py\", line 78, in __affine_image\r\n    r = tf.matrix_inverse(r)\r\n  File \"/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/ops/gen_linalg_ops.py\", line 1049, in matrix_inverse\r\n    \"MatrixInverse\", input=input, adjoint=adjoint, name=name)\r\n  File \"/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\r\n    op_def=op_def)\r\n  File \"/home/qiujia/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInternalError (see above for traceback): tensorflow/core/kernels/cuda_solvers.cc:803: cuBlas call failed status = 13\r\n\t [[Node: s2/MatrixInverse = MatrixInverse[T=DT_FLOAT, adjoint=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](s2/Reshape_2)]]\r\n\t [[Node: s2/transform/ImageProjectiveTransform/_2343 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1510_s2/transform/ImageProjectiveTransform\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n```\r\n\r\n\r\n", "comments": ["@OxInsky Could you follow the instructions [here](https://github.com/rnreich/ubuntu-tensorflow-gpu-all-versions) and let us know how it works. I recently used those steps and successfully installed tensorflow-gpu. If you had already resolved the issue, please close the issue or let us know. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Hello,have you solved this problem?I also meet the same problem.how do you solve this problem?", "Hi, I meet the same issue, did you solve this issue?", "I met this problem about three moths ago. My trouble code is:\r\n`tf.matrix_inverse(view_mat_model)`\r\nTensorflow will create a cublas solver for this, and it works well on 1080ti, but crashes on 2080ti.\r\nI didn't find a good way to solve this problem, say, finding a good combination of cudnn and cuda version or installing a cudnn patch.\r\nBut I finally found a way to avoid that by:\r\n```\r\nwith tf.device(\"/cpu:0\"):\r\n    view_mat_for_normal =tf.matrix_inverse(view_mat_model)\r\n```\r\nwhich move the computation  to cpu.\r\nThis works fine with my code and there was no obvious efficiency reduction.\r\n\r\nIf you got others computation code crashed with cublas, you can let them computed on cpu like above. \r\n\r\nHope this may help you.\r\n@Donald-Su @longmalongma @OxInsky", "I discussed this problem with carlkulseng [here](https://devtalk.nvidia.com/default/topic/1049490/container-tensorflow/-op_requires-failed-at-matrix_inverse_op-cc-191-internal-tensorflow-core-kernels-cuda_solvers-cc-803-cublas-call-failed-status-13/?offset=6#5380467)\r\n\r\nAnd the solution given by him works fine with me.\r\n\r\n@Donald-Su @longmalongma @OxInsky"]}, {"number": 25273, "title": "Add decode_wav api endpoint", "body": "As discussed here: https://groups.google.com/a/tensorflow.org/forum/#!topic/testing/_9AfKgzldf8", "comments": ["Regarding your question: there are tests in tensorflow/core/kernels/decode_wav_op_test.cc.", "This looks good! I am assuming there's no need to really test the python API part since it eventually comes down to the C++ API anyway. Or maybe there's one API test that covers all endpoints.\r\n\r\n@martinwicke Are there _good-to-read_ docs around how tests are designed/setup?", "I added the API review label. Let's wait for that to disappear then we're\ngood.\n"]}, {"number": 25272, "title": "estimator saving eval metrics to /eval_0 not /eval", "body": "I decided to give early stopping a shot with [stop_if_no_decrease_hook](https://www.tensorflow.org/api_docs/python/tf/contrib/estimator/stop_if_no_decrease_hook) but after tweaking the params and making sure the hook was actually being triggered, discovered an oddity with my estimator. \r\n\r\nThe hook reads eval data from the directory[ specified by estimator.eval_dir()](https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/early_stopping.py#L391) which in this case pops out \"trained_models/rcnn_inception_v2/eval\". This looks all good until I check my actual directory structure and find that /eval doesn't exist. Instead, I have eval_0/ (containing the proper logs) with no other directories in the rcnn_inception_v2/ folder.\r\n\r\nI'm curious why eval_0/ is being created instead of eval/ when I've done nothing special to specify this directory. And importantly on top of that, why does estimator.eval_dir() return the wrong directory?", "comments": ["Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. Github is mainly for addressing bugs in installation and performance. Thanks!", "ok will do, I thought this might be a bug with estimator.eval_dir, so thought it might be of interest here as well. go ahead and close out if you think that's appropriate", "@amm385 Could you post a code to reproduce the bug? Also, could you fill the fields in the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, It would be great if you can provide a small code to reproduce the error. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 25271, "title": "How Java calls the estimator model?", "body": "Hello, I would like to ask how Java calls the estimator model. Is there any sample code to see?", "comments": ["@mrchor Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. Github is mainly for addressing bugs in installation and performance. Thanks!", "Closing the issue as it is not related to Bug/Performance or Build/Installation. Thanks!"]}, {"number": 25270, "title": "Training accuracy of TF official tutorial is very low with Ubuntu 18.04, CUDA 10.0 and cuDNN 7.4", "body": "**System information**\r\n- Ubuntu 18.04\r\n- Python 3.7.1 (Anaconda 3)\r\n- CUDA 10.0\r\n- cuDNN 7.4.2\r\n- GTX 1070 mobile\r\n- Core i7 8950H, 6C12T @2.2G Hz\r\n\r\n**Describe the current behavior**\r\n    I run the code from the TensorFlow tutorial from https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/basic_classification.ipynb, but the training accuracy is very low (~0.1). \r\n\r\nNo matter I installed the TensorFlow from 1) compiling source or from 2) `pip install tf-nightly-gpu`, the accuracies are both around 0.1. The stable release of tensorflow is not available for Python 3.7. Besides, I have tested running the code on `GPU` and `CPU`, the results are same.\r\n\r\n**Describe the expected behavior**\r\nIn the tutorial, the expected accuracy is around 0.8.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport os\r\n\r\nUSE_CPU = 1\r\nif USE_CPU == 1:\r\n    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \r\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\r\n\r\nfashion_mnist = keras.datasets.fashion_mnist\r\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\r\n\r\ntrain_images = train_images / 255.0\r\ntest_images = test_images / 255.0\r\n\r\nmodel = keras.Sequential([\r\n    keras.layers.Flatten(input_shape=(28, 28)),\r\n    keras.layers.Dense(128, activation=tf.nn.relu),\r\n    keras.layers.Dense(10, activation=tf.nn.softmax)\r\n])\r\n\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy']\r\n              )\r\nmodel.fit(train_images, train_labels, epochs=5)\r\ntest_loss, test_acc = model.evaluate(test_images, test_labels)\r\nprint('Test accuracy:', test_acc)\r\n```\r\n\r\nThe output of the above code:\r\n```\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0128 17:35:44.705833 140133976348480 deprecation.py:506] From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1253: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nW0128 17:35:44.735007 140133976348480 deprecation.py:506] From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_utils.py:123: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\n2019-01-28 17:35:44.862411: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-01-28 17:35:44.866879: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz\r\n2019-01-28 17:35:44.867464: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x5574ea7aa690 executing computations on platform Host. Devices:\r\n2019-01-28 17:35:44.867481: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-01-28 17:35:44.868907: I tensorflow/stream_executor/platform/default/dso_loader.cc:154] successfully opened CUDA library libcuda.so.1 locally\r\n2019-01-28 17:35:44.871695: E tensorflow/stream_executor/cuda/cuda_driver.cc:303] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2019-01-28 17:35:44.871714: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:161] retrieving CUDA diagnostic information for host: toughlife-AW17R5\r\n2019-01-28 17:35:44.871719: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:168] hostname: toughlife-AW17R5\r\n2019-01-28 17:35:44.871794: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:192] libcuda reported version is: 410.78.0\r\n2019-01-28 17:35:44.871831: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:196] kernel reported version is: 410.78.0\r\n2019-01-28 17:35:44.871836: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:303] kernel version seems to match DSO: 410.78.0\r\nEpoch 1/5\r\n60000/60000==============================] - 1s 22us/sample - loss: 0.5021 - acc: 0.1024\r\nEpoch 2/5\r\n60000/60000==============================] - 1s 21us/sample - loss: 0.3748 - acc: 0.1025\r\nEpoch 3/5\r\n60000/60000==============================] - 1s 21us/sample - loss: 0.3365 - acc: 0.1025\r\nEpoch 4/5\r\n60000/60000==============================] - 1s 21us/sample - loss: 0.3132 - acc: 0.1031\r\nEpoch 5/5\r\n60000/60000==============================] - 1s 21us/sample - loss: 0.2931 - acc: 0.1023\r\n60000/60000==============================] - 1s 9us/sample - loss: 0.2689 - acc: 0.1015\r\n```\r\n\r\n**Other info / logs**\r\n", "comments": ["@ToughLife Please check your system and size of files. There is something wrong with your system or installation. I ran it on my desktop and got the following results.\r\n\r\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\r\n32768/29515 [=================================] - 0s 6us/step\r\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\r\n26427392/26421880 [==============================] - 2s 0us/step\r\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\r\n8192/5148 [===============================================] - 0s 2us/step\r\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\r\n4423680/4422102 [==============================] - 1s 0us/step\r\nEpoch 1/5\r\n60000/60000 [==============================] - 7s 118us/step - loss: 0.4985 - acc: 0.8259\r\nEpoch 2/5\r\n60000/60000 [==============================] - 7s 115us/step - loss: 0.3767 - acc: 0.8636\r\nEpoch 3/5\r\n60000/60000 [==============================] - 7s 118us/step - loss: 0.3354 - acc: 0.8769\r\nEpoch 4/5\r\n60000/60000 [==============================] - 7s 117us/step - loss: 0.3134 - acc: 0.8847\r\nEpoch 5/5\r\n60000/60000 [==============================] - 7s 117us/step - loss: 0.2949 - acc: 0.8916\r\n10000/10000 [==============================] - 1s 63us/step\r\nTest accuracy: 0.8635\r\nThanks!", "I am closing this issue as the issue was not due to TF. Thanks!"]}, {"number": 25269, "title": "Add deterministic cuDNN max pooling", "body": "When the TF_CUDNN_DETERMINISTIC environment variable is set to 1/true, cuDNN max pooling is performed using a deterministic algorithm.\r\n\r\nThis current pull request follows-on from pull request [24747](https://github.com/tensorflow/tensorflow/pull/24747) which dealt only with the convolution algorithms.\r\n\r\nThis current pull request completes the functionality controlled by TF_CUDNN_DETERMINISTIC, so the TODO from the previous pull request is also removed.", "comments": ["Adding @timshen91 as reviewer for comments in https://github.com/tensorflow/tensorflow/pull/24747#issuecomment-452181624.\r\nThanks!"]}]