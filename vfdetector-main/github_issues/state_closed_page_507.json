[{"number": 38546, "title": "FTRL maths displayed messy", "body": "In this doc https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Ftrl the math is messed up\r\n\r\nsee https://snipboard.io/my0uSb.jpg\r\n\r\nusing chrome browser", "comments": ["@dirknbr,\r\nI have tried different browsers and did not observe any difference in the page. Could you please point out what needs to be corrected? Thanks! ", "as per the screenshot, the \\abs bit is in red and not rendered correctly", "This error is due to mistake in parentheses, it should be `abs(z_{i})` instead of `abs{z_{i}}`. I'd like to fix this but I can't get that file version correctly since it's on different tag. Could you plz tell how can I get the correct file version in my local repository.", "@dirknbr,\r\nClosing this issue as it is resolved in the [nightly version](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Ftrl?version=nightly).\r\n\r\nPlease feel free to re-open if mistaken. Thanks!"]}, {"number": 38545, "title": "tf.image.extract_glimpse does not work as it should (tensorflow 2.1)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 64 bit\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: NVIDIA (https://docs.nvidia.com/deeplearning/frameworks/tensorflow-release-notes)\r\n- **TensorFlow version (use command below)**: 2.1\r\n- **Python version**: 3.6.9\r\n\r\n\r\n\r\n### Describe the problem\r\nHello, I am trying to use tf.image.extract_glimpse and I realised that It does not work as it should.\r\nThe issue was the same than 4 year ago #7681 and it was solved. But It seems like that in the new version of tensorflow 2.1 it is not fixed.\r\n\r\n### Source code / logs\r\nYou can try to reproduce this code:\r\n\r\n\r\nBATCH_SIZE = 1\r\nIMAGE_HEIGHT = 7\r\nIMAGE_WIDTH = 7\r\nCHANNELS = 1\r\nGLIMPSE_SIZE = (3,3)\r\n\r\nimage = tf.reshape(tf.range(49, delta=1, dtype=tf.float32),\r\n  shape=(BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WIDTH, CHANNELS))\r\n\r\noutput1 = tf.image.extract_glimpse(image, size=GLIMPSE_SIZE,\r\n  offsets=[[1,1]], centered=False, normalized=False)\r\n\r\noutput2 = tf.image.extract_glimpse(image, size=GLIMPSE_SIZE,\r\n  offsets=[[2,2]], centered=False, normalized=False)\r\n\r\noutput1 = [[  0.   1.   2.]\r\n                 [  5.   6.   7.]\r\n                 [ 10.  11.  12.]]\r\n\r\noutput2 =  [[  0.   1.   2.]\r\n                 [  5.   6.   7.]\r\n                 [ 10.  11.  12.]]\r\n\r\nThe results are the same, that we got in issue #7681.", "comments": ["@patriciacs1994 I think you are referring to PR #12829?", "Yes, I am referring to that issue that was solved 4 years ago. But it seems like that for tf 2.1 the error still persist. Am I wrong?", "@patriciacs1994 The PR was merged but seems to have been reverted at one point. Just re-submitted the PR in #38549. It should fix the issue if merged.", "Perfect. So, to solve this issue in my machine, how should I proceed? I installed tf through NVIDIA containers (https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow/tags). Is this change applied automatically? Thank you. ", "@patriciacs1994 After the PR is approved and merged, it will show up in tf-nightly automatically (and will show up in the next version of the release, likely TF 2.3.0+ as TF 2.2.0 release is already imminent).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38545\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38545\">No</a>\n"]}, {"number": 38544, "title": "Add XLA kernel for tf.ensure_shape", "body": "This PR is part of the effort to address #34363 and a prerequisite for PR #34399 which will resolve #34363. \r\n\r\nIn order for the PR #34399 to pass all tests, an XLA kernel for tf.ensure_shape need\r\nto be added. See https://github.com/tensorflow/tensorflow/pull/34399#issuecomment-563283239 for more details.\r\n\r\nOnce this PR is merged, PR #34399 will be re-opened to address #34363.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 38543, "title": "fix readability (newlines) on github issue templates", "body": "The bug-issue and performance-issue templates currently have some issues that make them difficult to work with when submitting new github issues. Mainly the usage (or lack thereof) of newlines. There are several places in the templates where newlines should obviously be present and are not. And several other places where newlines should not be present but are (wrapping *way* before 80 chars).\r\n\r\nWhile I don't know that wrapping at 80 chars makes much sense, as these files are primarily rendered in a browser which will both auto-wrap for you, as well as has a wider viewport, I mostly kept them wrapped at 80. The exception is for the 2 version commands, which I felt made more sense keeping each on a whole line (which they already were before, just on the same line). But I can wrap these too if desired.\r\n\r\nI would also like to point out that while these are markdown files, the primary audience (the author) is consuming them in their text form. Thus things like that `<em>` at the top don't work, and the formatting needs to be clean in text form.\r\n\r\n![image](https://user-images.githubusercontent.com/1826947/79249301-0e8d2700-7e4b-11ea-87da-7d543b3c87a6.png)\r\n", "comments": []}, {"number": 38542, "title": "Macro pollution (Is it called like this) on windows", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nwindows 10 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version:\r\n2.1\r\n- Python version:\r\n3.7\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n0.27.1\r\n- GCC/Compiler version (if compiling from source):\r\nvisual studio 2019\r\n- CUDA/cuDNN version:\r\n10.2/7\r\n- GPU model and memory:\r\nTuring\r\n\r\n\r\n**Describe the problem**\r\nUse compiled tensorflow to compile an empty project like this:\r\n`#include <tensorflow/core/framework/tensor.h>\r\n\r\nint main(int argc, char const* argv[])\r\n{\r\n    return 0;\r\n}`\r\nI got some errors like this:\r\nwarning C4003: not enough arguments for function-like macro invocation 'min'\r\nerror C2589: '(': illegal token on right side of '::'\r\nerror C2062: type 'unknown-type' unexpected\r\nerror C3805: 'type': unexpected token, expected either '}' or a ','.\r\nThen I write `#undef max\r\n#undef min`  on third_party\\eigen3\\unsupported\\Eigen\\CXX11\\Tensor,it passed compile.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nCompile tensorflow to get .dll .lib and header files.\r\nCreate an empty vs project,config and use those files.\r\nWrite \"#include <tensorflow/core/framework/tensor.h>\" and compile it.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@yonghenghuanmie Can you please provide more details on your issue? If this is not a bug, then please post this question in stackoverflow where there is a large community to support. If this is more of a bug, then please provide more details. Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38542\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38542\">No</a>\n", "I solved a similar problem.\r\nI got some errors like this: warning C4003: not enough arguments for function-like macro invocation 'min'. \r\nThis is because the macro definition of min in windows.h requires two parameters, so there may be no problem with Linux. This is a namespace conflict, you can use\r\n#undef min\r\n#undef max\r\nto close the macro definition in window.h.\r\nMay be useful", "@jvishnuvardhan This is a bug. The simplest possible project linking to tensorflow won't build without a manual workaround. @yonghenghuanmie provided sufficient details to replicate the issue, what more do you need?"]}, {"number": 38541, "title": "Issues of Adam Optimizer on Complex domain", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow):  Yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04):  Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): `conda install -c conda-forge tensorflow-gpu=2.1`\r\n- TensorFlow version (use command below):  2.1.0\r\n- Python version: - 3.7.7\r\nversion (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory: 10.1.243/7.6.5\r\n\r\n**Describe the current behavior**\r\n\r\nI'm building a custom model that does computation in Complex value domain, we find the performance in accuracy sense by using Adam is worse than that by translating the whole model into a real domain counterpart (2 times network parameters, real and imag parts ), and use same Adam optimizer.\r\n\r\nI dig into the sources codes of [Adam](https://github.com/tensorflow/tensorflow/blob/f270180a6caa8693f2b2888ac7e6b8e69c4feaa8/tensorflow/core/kernels/training_ops_gpu.cu.cc#L57), line 57: \r\n\r\n`v_i += one_minus_beta2 * (g_i * g_i - v_i);`\r\n\r\nfor real numbers, it is correct as g_i * g_i standards for 2nd order moments, while g_i * g_i in the Complex domain is pseudo-variance. Andy M. Sarrof also discuss this behavior in his [thesis](https://andysarroff.com/papers/sarroff2018a.pdf) (page 35, equation 3.56)\r\n\r\nHowever, it is expected that this behavior does not throw errors because it's valid operation.\r\n\r\n**Describe the expected behavior**\r\n\r\nI suppose in [line 57](https://github.com/tensorflow/tensorflow/blob/f270180a6caa8693f2b2888ac7e6b8e69c4feaa8/tensorflow/core/kernels/training_ops_gpu.cu.cc#L57),  `g_i * g_i` should be replace by `g_i * {conjugate function}(g_i)` as pointed out by Andy M. Sarrof in this thesis [https://andysarroff.com/papers/sarroff2018a.pdf] (page 35, equation 3.56)\r\n\r\n", "comments": ["@remifan \r\n\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@remifan \r\n\r\nAny update on this issue please. Thanks!", "@ravikyram Sorry for being inactive so long, I'm preparing a simple test case to reproduce the issue, I'll update soon, thanks!!", "@ravikyram I've made a simple demo to illustrate the concept, please check the this [notebook](https://github.com/remifan/complex_adam/blob/master/complex_adam_example.ipynb)\r\n`myadam.py` is my modified Adam using `g * conj(g)` as \"variance\".\r\n\r\nIn this simple demo, the `x` trace of TF2's Adam is biased from the obviously optimal trace that theory predicts. I suppose the 2nd-moment term in Adam is intended to regularize the step size in \"intensity\" sense for reasonable heuristics, rather than touching the direction of the gradients based on g * g whose angle is not quite related to the intuitive \"variance\".\r\n\r\nAnother possible issue of native Adam is that it does not implement the `amsgrad=True` (it'd raise an error`No registered 'ResourceApplyAdamWithAmsgrad' OpKernel ....`) version on complex numbers. This issue is expected because 2nd-moment `v_t` is complex (since g*g is complex), comparing complex numbers makes no sense, While using `g*conj(g)` as \"variance\", `v_t` becomes real numbers, amsgrad is then valid by replacing maximum function with function like `cxmax = lambda a,b: math_ops.cast(math_ops.maximum(math_ops.real(a), math_ops.real(b)), var_dtype)` to be valid in complex dtype.", "@remifan \r\n\r\nI tried in colab with TF-GPU 2.1.0 and i am seeing below error (`ModuleNotFoundError: No module named 'myadam'`).Request you to provide colab link or reproducible code in our environment.It helps us in localizing the issue faster.Thanks!", "@ravikyram I have updated that [notebook ](https://github.com/remifan/complex_adam/blob/master/complex_adam_example.ipynb)to be working inside Colab, or please check this [shared colab](https://colab.research.google.com/github/remifan/complex_adam/blob/master/complex_adam_example.ipynb) for your convenience, thanks!", "@gowthamkpr @ravikyram Hey, any updates on this issue? Is this issue confirmed from your side? ", "Thank you for this feature request. Because we don't have widespread support for complex numbers in Keras, we can't fix this in one place and ignore the maintenance implications across the rest of the codebase. That said, an Adam Optimizer that handles complex numbers might be of interest to the TF Addons community repo, where special subclasses of Optimizers can live: https://github.com/tensorflow/addons", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38541\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38541\">No</a>\n"]}, {"number": 38540, "title": "Building a vocabulary from text data for classification", "body": "I am following this [tutorial](https://www.tensorflow.org/tutorials/load_data/text) and [here](https://www.tensorflow.org/tutorials/load_data/text#build_vocabulary) the documentation says `There are a few ways to do this in both TensorFlow and Python.`. I am wondering what could be other ways of building a vocabulary. The way defined after this is slow for me since it is a loop and my dataset consists over 900,000 text docs.", "comments": ["@rishabhkabra other method include as below \r\n\r\nhttps://www.tensorflow.org/versions/r1.15/api_docs/python/tf/lookup\r\n\r\nSample code \r\n\r\n```\r\ntableLookup= tf.contrib.lookup.index_table_from_file(\"<path_to_vobab_file>\", num_oov_buckets=0, default_value=0)\r\nindex = tableLookup.lookup(<tensor_of_Strings>)\r\n```\r\nYou can do the preprocessing in python once and store tokens in file and read tokens from that file using above or you can store it in .tFRecord  file for better perfomance  \r\n\r\nAnother good resource is https://cs230.stanford.edu/blog/datapipeline/#building-a-text-data-pipeline", "@rishabhsahrawat,\r\nCould you please check @17patelumang's comment and let us know if it helps? Thanks!", "@amahendrakar is there any way i can be assigned few tickets ?", "@17patelumang thanks for suggestion. On your provided link they are also using a for loop to read each line of the data. \r\n````\r\n for i, line in enumerate(f):\r\n        vocab.update(line.strip().split(' '))\r\n````\r\nand the approach mentioned in my question is following:\r\n\r\n`````\r\n\r\nfor text_tensor, _ in all_labeled_data:\r\n  some_tokens = tokenizer.tokenize(text_tensor.numpy())\r\n  vocabulary_set.update(some_tokens)\r\n`````\r\n\r\nIn my understanding they both will take the same time. Since my dataset has a lot of lines so it is very time taking and I want a solution that is faster.\r\n\r\nMaybe @amahendrakar , you can help. \r\n\r\nThanks:)", "@17patelumang or do you think `set()` and `Counter()` can make speed differences between the two approaches?", "Okay, so I tested both `set()` and `Counter()` and in my tests, `set()` is already faster than `Counter()`.\r\nSo, my original question is still valid. @17patelumang @amahendrakar ", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n Thanks!\r\n", "My question was about the alternative ways mentioned in the TF documenatation, but OK. \ud83d\udc4d"]}, {"number": 38538, "title": "kaggle tpu kernel(tf 2.2.0) \"Socket closed\" when using subclassing api ", "body": "When doing model.fit, will always got socket lost/closed error using tpu kernel V3-8 if using subclassing api while functional api is ok. code like below\r\n\r\n\r\n    class Model(keras.Model):\r\n        def __init__(self):\r\n            super(Model, self).__init__() \r\n\r\n            bert_layer = hub.load(BERT_GCS_PATH_SAVEDMODEL)\r\n            bert_layer = hub.KerasLayer(bert_layer, trainable=True)\r\n            self.bert_layer = bert_layer\r\n            self.dense = keras.layers.Dense(1)\r\n\r\n       def call(self, input):\r\n            input_word_ids = input['input_word_ids']\r\n            input_mask = input['input_mask']\r\n            segment_ids = input['all_segment_id']\r\n  \r\n            x, _ = self.bert_layer([input_word_ids, input_mask, segment_ids])\r\n            x = self.dense(x)\r\n            self.logit = x\r\n            x = tf.math.sigmoid(x)\r\n            return x", "comments": ["@chenghuige \r\ni ran the code shared by you on nightly and do not face any issues, please let me know if i am missing anything here, [gist for the same](https://colab.sandbox.google.com/gist/Saduf2019/cdfde9f1b4be72b9335696450e1d3ec7/38538.ipynb)\r\nplease share the error faced by you.", "@Saduf2019  I tried to use latest tf in kaggle kernel, following [https://www.kaggle.com/kivlichangoogle/jigsaw-multilingual-getting-started](jigsaw-multilingual-getting-started) but with latest tf, tpu setup will fail, I have modified the colab file you provide.\r\n\r\n--> 11     tf.tpu.experimental.initialize_tpu_system(tpu)\r\n     12     strategy = tf.distribute.experimental.TPUStrategy(tpu)\r\n     13 else:\r\n\r\n3 frames\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: NodeDef expected inputs 'string' do not match 0 inputs specified; Op<name=_Send; signature=tensor:T -> ; attr=T:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>; NodeDef: {{node _Send}}", "@chenghuige I would recommend you to post this in [Kaggle/kaggle-api](https://github.com/Kaggle/kaggle-api/issues) as this platform is only for tensorflow issues. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing this issue as its not related to tensorflow. Please post it in the kaggle platform. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38538\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38538\">No</a>\n"]}, {"number": 38537, "title": "can not restore adagrad Variable", "body": "for fixed_size_partitioner trainningVariable, adagrad Variable in  tf.GraphKeys.GLOBAL_VARIABLES is\uff1a<tf.Variable 'title_rank/title_rank/concat_projection/part_2/Adagrad:0' shape=(15, 1) dtype=float32_ref>\r\nbut in saved checkpoint: 'title_rank/concat_projection/ion/part_0/Adagrad', [75, 1]\r\nshape is not right and restore adagrad Variable failed", "comments": ["@yanghzcc \r\n\r\nRequest you to fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\nRequest you to provide colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster.Thanks!", "@ravikyram \r\nthanks\r\nhttps://stackoverflow.com/questions/56930685/pywrap-tensorflow-checkpoint-reader-fails-for-ftrl-states-of-partitioned-variabl\r\nthe same question", "I have tried on colab with TF version 1.12, 1.15 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/5717d6bd77d0807da3a2c694df303d60/untitled791.ipynb).Thanks!", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38537\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38537\">No</a>\n"]}, {"number": 38535, "title": "installation issue-ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NO\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version:2.0\r\n- Python version:3.8.2\r\n- Installed using virtualenv? pip? conda?:pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Abhishek Sharma\\anaconda3\\envs\\tfp3.8\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Abhishek Sharma\\anaconda3\\envs\\tfp3.8\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Abhishek Sharma\\anaconda3\\envs\\tfp3.8\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Abhishek Sharma\\anaconda3\\envs\\tfp3.8\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Abhishek Sharma\\anaconda3\\envs\\tfp3.8\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "@abhisharma048 ,\r\n\r\n[#36167 (comment)](https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156)\r\n\r\n[#36167](https://github.com/tensorflow/tensorflow/issues/36167) [#36151](https://github.com/tensorflow/tensorflow/issues/36151) [#36138](https://github.com/tensorflow/tensorflow/issues/36138) [#36054](https://github.com/tensorflow/tensorflow/issues/36054) [#36045](https://github.com/tensorflow/tensorflow/issues/36045) [#36020](https://github.com/tensorflow/tensorflow/issues/36020) [#36003](https://github.com/tensorflow/tensorflow/issues/36003) [#35988](https://github.com/tensorflow/tensorflow/issues/35988) [#35903](https://github.com/tensorflow/tensorflow/issues/35903) [#35880](https://github.com/tensorflow/tensorflow/issues/35880) [#35865](https://github.com/tensorflow/tensorflow/issues/35865) ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I have the same problem using PyCharm.\r\nTensorFlow version 2.2.0rc3\r\nI tried reinstall new CUDA driver version and  adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\r\nI have Python version 3.8.2., maybe I have to install the previous version (3.7)?", "@abhisharma048 \r\nis this still an issue ", "I have the same issue with all version > 2.0.0 (not with 2.0.0) for the prebuilt CPU package and python 3.7\r\nFor python 3.8 I only found the 2.2.0rc versions and same issue.", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38535\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38535\">No</a>\n", "I have the same issue:\r\n\r\n* Windows 10 1909\r\n* Python 3.8.2\r\n* Tensorflow-GPU 2.2.0\r\n* NVIDIA CUDA Toolkit 10.2\r\n* cuDNN 10.2 for Windows 10 x64 v7.6.5.32 (`cudnn64_7.dll`)\r\n\r\n> ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found."]}, {"number": 38534, "title": "Longer latency after both post quantization and aware-quantization training ", "body": "**System information**\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.4\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version: 1.15.2\r\nPython version: 3.7.7\r\nInstalled using virtualenv? pip? conda?: pip\r\nBazel version (if compiling from source) : 0.26.1\r\nGCC/Compiler version (if compiling from source): None\r\nCUDA/cuDNN version: None\r\nGPU model and memory: None\r\n**Describe the problem\r\n\r\nProvide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI try to evaluate my TensorFlow Lite model using the tool in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks/coco_object_detection I evaluate 3 models: original model, model after full integer quantization and model after aware-quantization training. The model I use is SSD MobileNetV2 in object detection TensorFlow API and trained with another dataset. \r\n\r\nFor the data I get, the original model has latency 151954, full interger quantized model's latency: 2970549, aware quantized model's: 945065\r\n\r\nMy code for running :\r\n  -- \\\r\n\r\n  //tensorflow/lite/tools/evaluation/tasks/coco_object_detection:run_eval \\\r\n\r\n  --model_file=/home/sicong/Documents/model_zoo/ssd_mobilenet_v2_coco_2018_03_29/Udacity/Udacity_300_247705/full_integer_quantization_247705_200_dev01_with_input_inference.tflite \\\r\n\r\n  --ground_truth_images_path=/home/sicong/Documents/datasets/udacity/for_evaluation/images \\\r\n\r\n  --ground_truth_proto=/home/sicong/Documents/datasets/udacity/for_evaluation/ground_truth_change_id.pbtxt \\\r\n\r\n  --model_output_labels=/home/sicong/Documents/datasets/udacity/for_evaluation/udacity_label_map_edge_TPU.pbtxt \\\r\n\r\n  --output_file_path=/home/sicong/Documents/datasets/udacity/for_evaluation/full_integer_quantization_247705_200_dev01_with_input_inference.txt\r\n\r\nThank you for help!", "comments": ["@Sicongzai \r\nplease share simple stand alone code for us to replicate the issue faced.", "Hi, thank you for your response.\r\n\r\nFor aware-quantization training, I convert and quantize the model following with command line:\r\nexport CONFIG_FILE=/home/sicong/Documents/datasets/udacity/ssd_mobilenet_v2_300x300_udacity.config\r\nexport CHECKPOINT_PATH=/home/sicong/Documents/model_zoo/ssd_mobilenet_v2_coco_2018_03_29/Udacity/Udacity_300_315109_001/model.ckpt-27404\r\nexport OUTPUT_DIR=/home/sicong/Documents/model_zoo/ssd_mobilenet_v2_coco_2018_03_29/Udacity/Udacity_300_315109_001\r\npython object_detection/export_tflite_ssd_graph.py \\\r\n--pipeline_config_path=$CONFIG_FILE \\\r\n--trained_checkpoint_prefix=$CHECKPOINT_PATH \\\r\n--output_directory=$OUTPUT_DIR \\\r\n--add_postprocessing_op=true\r\n\r\nFor full integer quantization, I use the code below:\r\nconverter.representative_dataset = representative_data_gen\r\nconverter.allow_custom_ops=True\r\nconverter.inference_input_type = tf.uint8\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_quant_model = converter.convert()", "Did you run eval on a desktop? If so it's expected that the latency of the quantized model is larger than the float one. Currently we only have optimized kernel on mobile devices.\r\nCan you follow the instructions and run eval on an android phone again?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38534\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38534\">No</a>\n"]}, {"number": 38533, "title": "TFLu compile option in msys under Win64", "body": "Users that need a build option of TFLu under msys on Win64 need to install the Win executables of the gcc tool chain. The PR adds this option to the TFLu installation to automatically download and unpack.", "comments": ["Please open against master and then cherry-pick on the branch", "Re-opened against master as #38744 "]}, {"number": 38532, "title": "TensorFlow Object Detection API with Keras model", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): partially\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device:  Desktop\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): 1.15\r\n- Python version: - Bazel\r\nversion (if compiling from source): 3.6.9\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: 10.0/7.6.5\r\n- GPU model and memory: GTX970 4GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI feed custom trained model with a video stream. I would like to use output's bounding boxes to feed 6 different models trained in for example Keras. I use webcam tutorial from Object Detection API. Only way it works for now is to set session by Keras and load model from the beggining. It ruins the performance very much. \r\n\r\n**Describe the expected behavior**\r\nI would like to have more than 1 fps with passing the detected objects to CNN Keras model.\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n[Google Colab code](https://colab.research.google.com/drive/1O-FkQg6PGJ6EtCYzBE4oZeMKS5gbJSsa)\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@SMatusik \r\n\r\nI am not able to access the colab code. Request  you to provide access to colab link to reproduce the issue. Thanks!", "It's accessable right now, sorry for mistake", "@SMatusik \r\n\r\nI tried reproducing the issue and i am seeing the below error message `(ImportError: cannot import name 'string_int_label_map_pb2'`).Request you to help me with the reproducible code which helps me in localizing the issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@SMatusik \r\n\r\nAny update on this issue please. Thanks!", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 38531, "title": "TFLu compile option in msys under Win64", "body": "Users that need a build option of TFLu under msys on Win64 need to install the Win executables of the gcc tool chain. The PR adds this option to the TFLu installation to automatically download and unpack.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38531) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 38530, "title": "Feature/build under msys on win64", "body": "Users that need a build option of TFLu under msys on Win64 need to install the Win executables of the gcc tool chain. The PR adds this option to the TFLu installation to automatically download and unpack. ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38530) for more info**.\n\n<!-- need_sender_cla -->", "Sorry, Incorrect target branch for the PR. Should go back to tensorflow/r2.2. Closed this and re-started the PR in #38533."]}, {"number": 38529, "title": "TFLite_Convertor cannot convert atrous_conv PB to tflite file", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Linux Redhat 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: Desktop\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below):  1.13.1\r\n- Python version: - Bazel\r\nversion (if compiling from source): 3.6.5\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory: CPU\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nwrite a simple artous_conv model to convert like below:\r\n\r\ninput = tf.compat.v1.placeholder(tf.float32, shape=(1, 244, 244, 3))\r\n    filter = [11, 11, 3, 64]\r\n\r\n    filter_weight = tf.compat.v1.get_variable(\r\n        \"weights\", filter,\r\n        initializer=tf.truncated_normal_initializer(stddev=0.1)\r\n    )\r\n    tf.nn.atrous_conv2d(value=input, filters=filter_weight, rate=2,\r\n                        padding='VALID')\r\n\r\nConvert this PB to tflite by TFlite_convertor with below cmd:\r\n\r\ntflite_convert --output_file=test.tflite --graph_def_file=frozen.pb --input_arrays=Placeholder --output_arrays=convolution/BatchToSpaceND\r\n\r\nError happened:\r\n\r\n \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n2020-04-14 18:35:43.409903: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 5 operators, 11 arrays (0 quantized)\r\n2020-04-14 18:35:43.410080: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 5 operators, 11 arrays (0 quantized)\r\n\r\n\r\n**Describe the expected behavior**\r\nShould be converted without error\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@RunnerZhong,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "Hi, you can reproduce this issue with below code easily..\r\n\r\n`\r\ngraph = tf.Graph()\r\n\r\nwith graph.as_default():\r\n    input = tf.compat.v1.placeholder(tf.float32, shape=(1, 244, 244, 3))\r\n    filter = [11, 11, 3, 64]\r\n\r\n    filter_weight = tf.compat.v1.get_variable(\r\n        \"weights\", filter,\r\n        initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.1)\r\n    )\r\n    tf.nn.atrous_conv2d(value=input, filters=filter_weight, rate=2,\r\n                        padding='VALID')\r\n\r\nwith tf.compat.v1.Session(graph=graph) as sess:\r\n    sess.run(tf.compat.v1.global_variables_initializer())\r\n   \r\n    frozen_graph_def = tf.compat.v1.graph_util.convert_variables_to_constants(sess, sess.graph_def, [\"convolution/BatchToSpaceND\"])\r\n\r\n    with open(\"/home/runner/PycharmProjects/tf/frozen.pb\", 'wb') as f:\r\n        f.write(frozen_graph_def.SerializeToString())\r\n\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(\"/home/runner/PycharmProjects/tf/frozen.pb\",\r\n                                                      [\"Placeholder\"], [\"convolution/BatchToSpaceND\"])\r\n\r\ntflite_model = converter.convert()\r\nopen(\"/home/runner/PycharmProjects/tf/test.tflite\", \"wb\").write(tflite_model)`", "Same issue existed in TF\uff1a 1.15", "@RunnerZhong can you please try TF 2.2 / nightly and let us know if it still exists.\r\n\r\nThanks", "Issue cannot reproduce in tf_nightly ..", "Closed as this issue disappeared in latest tf version"]}, {"number": 38528, "title": "tf.function does not transform nested class methods", "body": "System information\r\n\r\nHave I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): true\r\nOS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): OSX 10.14.5\r\nTensorFlow installed from (source or\r\nbinary): pip\r\nTensorFlow version (use command below): v2.1.0\r\nDescribe the current behavior\r\nWhen trying to  call a tf.function decorated class method, it raises an OperatorNotAllowedError:\r\n\r\nOperatorNotAllowedInGraphError: using a tf.Tensor as a Python bool is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.\r\n\r\nDescribe the expected behavior\r\nIt should work.\r\n\r\ncode structure\r\n\r\n```python\r\nclass Foo:\r\n        def __init__:\r\n              .....\r\n        @tf.function\r\n         def myfun(y):\r\n               # some computation\r\n               t = 0\r\n               while tf.math.reduce_max(pinf_t) > 1e-8:\r\n                       # some computation\r\n                       if t==0:\r\n                          # some computation\r\n                       else:\r\n                          # some computation\r\n               # do some computation\r\n       return value\r\n``` \r\n\r\n\r\n\r\nFull traceback:\r\n\r\n ---------------------------------------------------------------------------\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/mac/Documents/bishe/DeepStateCount/DeepStateCount/countdssm.py\", line 168, in <module>\r\n    dssm.run()\r\n  File \"/Users/mac/Documents/bishe/DeepStateCount/DeepStateCount/countdssm.py\", line 142, in run\r\n    epochs=epochs)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 819, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 235, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 593, in _process_training_inputs\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 706, in _process_inputs\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\", line 702, in __init__\r\n    x = standardize_function(x)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 660, in standardize_function\r\n    standardize(dataset, extract_tensors_from_dataset=False)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 2360, in _standardize_user_data\r\n    self._compile_from_inputs(all_inputs, y_input, x, y)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 2618, in _compile_from_inputs\r\n    experimental_run_tf_function=self._experimental_run_tf_function)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 446, in compile\r\n    self._compile_weights_loss_and_weighted_metrics()\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 1592, in _compile_weights_loss_and_weighted_metrics\r\n    self.total_loss = self._prepare_total_loss(masks)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 1652, in _prepare_total_loss\r\n    per_sample_losses = loss_fn.call(y_true, y_pred)\r\n  File \"/Users/mac/Documents/bishe/DeepStateCount/DeepStateCount/countdssm.py\", line 24, in call\r\n    likes = tf.map_fn(self.loss_fun, tf.concat([y_true, y_pred], axis=2))\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/map_fn.py\", line 268, in map_fn\r\n    maximum_iterations=n)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/control_flow_ops.py\", line 2675, in while_loop\r\n    back_prop=back_prop)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/while_v2.py\", line 194, in while_loop\r\n    add_control_dependencies=add_control_dependencies)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/while_v2.py\", line 172, in wrapped_body\r\n    outputs = body(*_pack_sequence_as(orig_loop_vars, args))\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/map_fn.py\", line 257, in compute\r\n    packed_fn_values = fn(packed_values)\r\n  File \"/Users/mac/Documents/bishe/DeepStateCount/DeepStateCount/countdssm.py\", line 31, in loss_fun\r\n    like = count_ssm.loglike()\r\n  File \"/Users/mac/Documents/bishe/DeepStateCount/DeepStateCount/NLSSMforDeep.py\", line 33, in loglike\r\n    linear_ssm = self.approx_gaussian(max_iter=50)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 615, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 497, in _initialize\r\n    *args, **kwds))\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2389, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 3211, in bound_method_wrapper\r\n    return wrapped_fn(*args, **kwargs)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 964, in wrapper\r\n    user_requested=True,\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 560, in converted_call\r\n    return _call_unconverted(f, args, kwargs, options)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 326, in _call_unconverted\r\n    return f.__self__.call(args, kwargs)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 3173, in call\r\n    return wrapped_fn(self.weakrefself_target__(), *args, **kwargs)\r\n  File \"/Users/mac/Documents/bishe/DeepStateCount/DeepStateCount/NLSSMforDeep.py\", line 123, in approx_gaussian\r\n    var=False)['alpha']\r\n  File \"/Users/mac/Documents/bishe/DeepStateCount/DeepStateCount/LSSMforDeep.py\", line 317, in exact_kalman_smoothing\r\n    filtered = self.exact_kalman_filtering()\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 615, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 497, in _initialize\r\n    *args, **kwds))\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2389, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 3211, in bound_method_wrapper\r\n    return wrapped_fn(*args, **kwargs)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 964, in wrapper\r\n    user_requested=True,\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 560, in converted_call\r\n    return _call_unconverted(f, args, kwargs, options)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 326, in _call_unconverted\r\n    return f.__self__.call(args, kwargs)\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 3173, in call\r\n    return wrapped_fn(self.weakrefself_target__(), *args, **kwargs)\r\n  File \"/Users/mac/Documents/bishe/DeepStateCount/DeepStateCount/LSSMforDeep.py\", line 139, in exact_kalman_filtering\r\n    while tf.math.reduce_max(pinf_t) > 1e-8:\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 757, in __bool__\r\n    self._disallow_bool_casting()\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 523, in _disallow_bool_casting\r\n    \"using a `tf.Tensor` as a Python `bool`\")\r\n  File \"/Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 510, in _disallow_when_autograph_enabled\r\n    \" decorating it directly with @tf.function.\".format(task))\r\ntensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.functio\r\n```", "comments": ["@AngelPone, Can you provide the standalone code to reproduce the reported issue. Thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38528\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38528\">No</a>\n"]}, {"number": 38527, "title": "pass name to GraphExecutionFunction", "body": "", "comments": []}, {"number": 38526, "title": "ModuleNotFoundError: No module named 'tensorflow.python._pywrap_tfe',how to due with it?", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["@A12-RUI \r\ncan you please share the tensorflow version and simple stand alone code for us to replicate the issue faced.\r\n\r\nAs per error, refer to these issues: [link1](https://github.com/tensorflow/tensorflow/issues/9469) [link2](https://github.com/tensorflow/tensorflow/issues/7623#issuecomment-280704672) [link3](https://github.com/tensorflow/tensorflow/issues/8385) [link4](https://github.com/tensorflow/tensorflow/issues/7705)  [link5](https://stackoverflow.com/questions/42011070/on-windows-running-import-tensorflow-generates-no-module-named-pywrap-tenso) [link6](https://python-forum.io/Thread-ModuleNotFoundError-No-module-named-pywrap-tensorflow-internal) [link7](https://www.reddit.com/r/learnprogramming/comments/9385ds/i_keep_getting_the_same_error_when_importing/)  and let us know if it helps resolve the issue.", "My tensorflow version is gpu-1.10.0 and the python version is 3.6 . The error is :ModuleNotFoundError: No module named 'tensorflow.python._pywrap_tfe', I have tried to solve this problem ,but I can not find this file from your github and I can not find someone have the solution.", "@A12-RUI\r\nis there any particular reason for not using later versions of tensorflow. \r\nplease share the code for which this error is faced for us to replicate it.", "Yes , I need to run a modle using this version of tensorflow .And code is:\r\n\r\n# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ==============================================================================\r\n\"\"\"Python module for Session ops, vars, and functions exported by pybind11.\"\"\"\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\n# pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import\r\nfrom tensorflow.python import pywrap_tensorflow\r\nfrom tensorflow.python._pywrap_tf_session import *\r\nfrom tensorflow.python._pywrap_tf_session import _TF_SetTarget\r\nfrom tensorflow.python._pywrap_tf_session import _TF_SetConfig\r\nfrom tensorflow.python._pywrap_tf_session import _TF_NewSessionOptions", "@a12-rui\r\ni ran the code shared, it works fine on 1.4 and above, please find the result on [1.4 ](https://colab.sandbox.google.com/gist/Saduf2019/7ad40ecc04c8164b3382494e7059b73a/untitled140.ipynb)and [nightly](https://colab.sandbox.google.com/gist/Saduf2019/294176cf5f322c2bde08fea72527d888/38369.ipynb#scrollTo=ozQa1m-TOcgi) the [version used by you](https://colab.sandbox.google.com/gist/Saduf2019/f4d306aa5e75cd048858ce80f48bcbc7/your_version.ipynb).\r\n\r\ncould you please refer the the links share by me above and let us know. please try to upgrade the tensorflow version and try.", "Thank you  for your kind annswer , I have upgraded the tensorflow version to 2.x , and solved the problem , thank you very much.", "@A12-RUI \r\nglad was able to help, moving this to closed status as issue is resolved"]}, {"number": 38525, "title": "ERROR: C:/users/alex/_bazel_alex/xv6zejqw/external/flatbuffers/src/BUILD:40:1: C++ compilation of rule '@flatbuffers//src:flatc' failed (Exit -1).", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r2.1\r\n- Python version: 3.6.0\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.29.1\r\n- GCC/Compiler version (if compiling from source): VS2019\r\n- CUDA/cuDNN version: n\r\n- GPU model and memory: n\r\n\r\n\r\n\r\n**Describe the problem**\r\nI'm trying to compile tensorflow lite for android use. But I failed.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. setup environment according to\r\nhttps://www.tensorflow.org/install/source_windows\r\n\r\n2. git checkout r2.1 (I tried the master but same error)\r\n\r\n3. configure\r\nC:\\tensorflow>configure\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nWARNING: Waiting for server process to terminate (waited 5 seconds, waiting at most 60)\r\nWARNING: Waiting for server process to terminate (waited 10 seconds, waiting at most 60)\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.29.1 installed.\r\nPlease specify the location of python. [Default is D:\\Program Files\\Python\\Python36\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  D:\\Program Files\\Python\\Python36\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [D:\\Program Files\\Python\\Python36\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: y\r\nEigen strong inline overridden.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n\r\n4. Build with\r\nbazel build -c opt --cxxopt=--std=c++11 --config=android_arm //tensorflow/lite/c:libtensorflowlite_c.so\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nC:\\tensorflow>bazel build -c opt --cxxopt=--std=c++11 --config=android_arm //tensorflow/lite/c:libtensorflowlite_c.so\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=D:/Program Files/Python/Python36/python.exe\r\nINFO: Reading rc options for 'build' from c:\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --config=v2\r\nINFO: Reading rc options for 'build' from c:\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=D:/Program Files/Python/Python36/python.exe --action_env PYTHON_LIB_PATH=D:/Program Files/Python/Python36/lib/site-packages --python_path=D:/Program Files/Python/Python36/python.exe --config monolithic --copt=-w --host_copt=-w --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=-D_USE_MATH_DEFINES --verbose_failures --distinct_host_configuration=false --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file c:\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:monolithic in file c:\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Found applicable config definition build:android_arm in file c:\\tensorflow\\.bazelrc: --config=android --cpu=armeabi-v7a --fat_apk_cpu=armeabi-v7a\r\nINFO: Found applicable config definition build:android in file c:\\tensorflow\\.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Call stack for the definition of repository 'io_bazel_rules_docker' which is a git_repository (rule definition at C:/users/alex/_bazel_alex/xv6zejqw/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18):\r\n - C:/users/alex/_bazel_alex/xv6zejqw/external/bazel_toolchains/repositories/repositories.bzl:37:9\r\n - C:/tensorflow/WORKSPACE:37:1\r\nERROR: Skipping '//tensorflow/lite/c:libtensorflowlite_c.so': no such target '//tensorflow/lite/c:libtensorflowlite_c.so': target 'libtensorflowlite_c.so' not declared in package 'tensorflow/lite/c' defined by C:/tensorflow/tensorflow/lite/c/BUILD\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such target '//tensorflow/lite/c:libtensorflowlite_c.so': target 'libtensorflowlite_c.so' not declared in package 'tensorflow/lite/c' defined by C:/tensorflow/tensorflow/lite/c/BUILD\r\nINFO: Elapsed time: 13.103s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (1 packages loaded)\r\n\r\nThanks in advance.", "comments": ["Can you try with TF 2.2? You should be able to build with:\r\n\r\n```\r\nbazel build -c opt --cxxopt=--std=c++14 --config=android_arm //tensorflow/lite/c:tensorflowlite_c\r\n```\r\n\r\nAnd it should produce a binary with the appropriate suffix.", "I tried the TF r2.2 with bazel 2.0.0. It failed with another error.\r\n\r\n```\r\nC:\\tensorflow>bazel build -c opt --cxxopt=--std=c++14 --config=android_arm //tensorflow/lite/c:tensorflowlite_c\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Reading rc options for 'build' from c:\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=D:/Program Files/Python/Python36/python.exe\r\nINFO: Reading rc options for 'build' from c:\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from c:\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=D:/Program Files/Python/Python36/python.exe --action_env PYTHON_LIB_PATH=D:/Program Files/Python/Python36/lib/site-packages --python_path=D:/Program Files/Python/Python36/python.exe --config=xla --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file c:\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file c:\\tensorflow\\.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:android_arm in file c:\\tensorflow\\.bazelrc: --config=android --cpu=armeabi-v7a --fat_apk_cpu=armeabi-v7a\r\nINFO: Found applicable config definition build:android in file c:\\tensorflow\\.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\nINFO: Found applicable config definition build:windows in file c:\\tensorflow\\.bazelrc: --copt=/w --copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Call stack for the definition of repository 'io_bazel_rules_docker' which is a git_repository (rule definition at C:/users/alex/_bazel_alex/xv6zejqw/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18):\r\n - C:/users/alex/_bazel_alex/xv6zejqw/external/bazel_toolchains/repositories/repositories.bzl:37:9\r\n - C:/tensorflow/WORKSPACE:37:1\r\nINFO: Analyzed target //tensorflow/lite/c:tensorflowlite_c (58 packages loaded, 1345 targets configured).\r\nINFO: Found 1 target...\r\nINFO: Deleting stale sandbox base C:/users/alex/_bazel_alex/xv6zejqw/sandbox\r\nERROR: C:/tensorflow/tensorflow/lite/kernels/internal/BUILD:666:1: C++ compilation of rule '//tensorflow/lite/kernels/internal:audio_utils' failed (Exit -1). Note: Remote connection/protocol failed with: execution failed\r\nAction failed to execute: java.io.IOException: ERROR: src/main/native/windows/process.cc(199): CreateProcessW(\"C:\\users\\alex\\_bazel_alex\\xv6zejqw\\execroot\\org_tensorflow\\bin\\false\" -MD -MF bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/internal/_objs/audio_utils/spectrogram.pic.d -frandom-seed=bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/internal/_objs/audio_utils/spectrogram.pic.o -fPIC -iquote . -iquote bazel-out/armeabi-v7a-opt/bin -iquote external/fft2d -iquote bazel-out/armeabi-v7a-opt/bin/external/fft2d /w /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /std:c++14 --std=c++1(...)): The system cannot find the file specified.\r\n (error: 2)\r\nTarget //tensorflow/lite/c:tensorflowlite_c failed to build\r\nINFO: Elapsed time: 85.905s, Critical Path: 0.23s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n\r\n\r\n", "I built the TF r2.2 again. The error is different\r\n```\r\nC:\\tensorflow>bazel build -c opt --cxxopt=--std=c++14 --config=android_arm //tensorflow/lite/c:tensorflowlite_c\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Reading rc options for 'build' from c:\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=D:/Program Files/Python/Python36/python.exe\r\nINFO: Reading rc options for 'build' from c:\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from c:\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=D:/Program Files/Python/Python36/python.exe --action_env PYTHON_LIB_PATH=D:/Program Files/Python/Python36/lib/site-packages --python_path=D:/Program Files/Python/Python36/python.exe --config=xla --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file c:\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file c:\\tensorflow\\.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:android_arm in file c:\\tensorflow\\.bazelrc: --config=android --cpu=armeabi-v7a --fat_apk_cpu=armeabi-v7a\r\nINFO: Found applicable config definition build:android in file c:\\tensorflow\\.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\nINFO: Found applicable config definition build:windows in file c:\\tensorflow\\.bazelrc: --copt=/w --copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Analyzed target //tensorflow/lite/c:tensorflowlite_c (58 packages loaded, 1345 targets configured).\r\nINFO: Found 1 target...\r\nINFO: Deleting stale sandbox base C:/users/alex/_bazel_alex/xv6zejqw/sandbox\r\nERROR: C:/users/alex/_bazel_alex/xv6zejqw/external/flatbuffers/src/BUILD:8:1: C++ compilation of rule '@flatbuffers//src:flatbuffers' failed (Exit -1). Note: Remote connection/protocol failed with: execution failed\r\nAction failed to execute: java.io.IOException: ERROR: src/main/native/windows/process.cc(199): CreateProcessW(\"C:\\users\\alex\\_bazel_alex\\xv6zejqw\\execroot\\org_tensorflow\\bin\\false\" -MD -MF bazel-out/armeabi-v7a-opt/bin/external/flatbuffers/src/_objs/flatbuffers/idl_parser.d -frandom-seed=bazel-out/armeabi-v7a-opt/bin/external/flatbuffers/src/_objs/flatbuffers/idl_parser.o -iquote external/flatbuffers -iquote bazel-out/armeabi-v7a-opt/bin/external/flatbuffers -Ibazel-out/armeabi-v7a-opt/bin/external/flatbuffers/src/_virtual_includes/flatbuffers /w /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /s(...)): The system cannot find the file specified.\r\n (error: 2)\r\nTarget //tensorflow/lite/c:tensorflowlite_c failed to build\r\nINFO: Elapsed time: 4.497s, Critical Path: 0.05s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n```", "I think there may be something wrong in your config? Did you run `/.configure` from the root checkout directory and feed it the path to your Android SDK/NDK?\r\n\r\nIn particular, `-DWIN32_LEAN_AND_MEAN` should not be present in the build command when building with `--config=android_arm`. It's possible this is an issue with the flatbuffer build config, but let's confirm that your configuration is correct first. Can you attach the `.tf_configure_bazelrc` file which should be present in the root of your tensorflow source tree?", "I did run the /.configure, but it didn't ask for path to android SDK/NDK.\r\nThere is the .tf_configure.bazelrc file.\r\n\r\n[tf_configure.bazelrc.txt](https://github.com/tensorflow/tensorflow/files/4485225/tf_configure.bazelrc.txt)\r\n\r\n", "Hmm, looks like the Android config option is disabled by default on Windows. Can you try removing [this line](https://github.com/tensorflow/tensorflow/blob/master/configure.py#L1386) and see if that lets you configure the paths properly for Android builds?", "Yes, now the configure ask me for path of android NDK.\r\nNow, the error is\r\n\r\n```\r\nC:\\tensorflow>bazel build -c opt --cxxopt=--std=c++11 --config=android_arm //tensorflow/lite/c:libtensorflowlite_c.so\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Reading rc options for 'build' from c:\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=D:/Program Files/Python/Python36/python.exe\r\nINFO: Reading rc options for 'build' from c:\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from c:\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=D:/Program Files/Python/Python36/python.exe --action_env PYTHON_LIB_PATH=D:/Program Files/Python/Python36/lib/site-packages --python_path=D:/Program Files/Python/Python36/python.exe --config=xla --define=override_eigen_strong_inline=true --action_env ANDROID_NDK_HOME=D:/library/android_SDK/ndk/android-ndk-r19 --action_env ANDROID_NDK_API_LEVEL=21 --action_env ANDROID_BUILD_TOOLS_VERSION=29.0.3 --action_env ANDROID_SDK_API_LEVEL=29 --action_env ANDROID_SDK_HOME=D:libraryandroid_SDK --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file c:\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file c:\\tensorflow\\.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:android_arm in file c:\\tensorflow\\.bazelrc: --config=android --cpu=armeabi-v7a --fat_apk_cpu=armeabi-v7a\r\nINFO: Found applicable config definition build:android in file c:\\tensorflow\\.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\nINFO: Found applicable config definition build:windows in file c:\\tensorflow\\.bazelrc: --copt=/w --copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Analyzed target //tensorflow/lite/c:libtensorflowlite_c.so (59 packages loaded, 7376 targets configured).\r\nINFO: Found 1 target...\r\nINFO: Deleting stale sandbox base C:/users/alex/_bazel_alex/xv6zejqw/sandbox\r\nERROR: C:/users/alex/_bazel_alex/xv6zejqw/external/flatbuffers/src/BUILD:24:1: C++ compilation of rule '@flatbuffers//src:flatc_library' failed (Exit 1)\r\nclang.exe: error: no such file or directory: '/w'\r\nclang.exe: error: no such file or directory: '/D_USE_MATH_DEFINES'\r\nclang.exe: error: no such file or directory: '/std:c++14'\r\nTarget //tensorflow/lite/c:libtensorflowlite_c.so failed to build\r\nINFO: Elapsed time: 48.965s, Critical Path: 1.75s\r\nINFO: 1 process: 1 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "Interesting, looks like building for Android still pulls in some of the Windows build flags despite your setting `--config=android_arm`. I'll see if I can repro.", "Looks like this is caused by the following .bazelrc entry:\r\n```\r\nbuild --enable_platform_specific_config\r\n```\r\n\r\nCan you try building with that line commented out or removed in the `.bazelrc` file in your root checkout? If that works, I'll see how we can make the appropriate changes in the default build scripts.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38525\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38525\">No</a>\n", "This *should* work now if you re-sync, run `/.configure` again and then build with\r\n```\r\nbazel build -c opt --config=android_arm //tensorflow/lite/c:libtensorflowlite_c.so\r\n```", "I checkout the  master, run configure and it got this\r\n\r\n```\r\nC:\\tensorflow>bazel build -c opt --config=android_arm //tensorflow/lite/c:libtensorflowlite_c.so\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=118\r\nINFO: Reading rc options for 'build' from c:\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=D:/Program Files/Python/Python36/python.exe\r\nINFO: Reading rc options for 'build' from c:\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from c:\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=D:/Program Files/Python/Python36/python.exe --action_env PYTHON_LIB_PATH=D:/Program Files/Python/Python36/lib/site-packages --python_path=D:/Program Files/Python/Python36/python.exe --config=xla --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file c:\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file c:\\tensorflow\\.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:android_arm in file c:\\tensorflow\\.bazelrc: --config=android --cpu=armeabi-v7a --fat_apk_cpu=armeabi-v7a\r\nINFO: Found applicable config definition build:android in file c:\\tensorflow\\.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\nINFO: Found applicable config definition build:windows in file c:\\tensorflow\\.bazelrc: --copt=/w --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Build options --copt, --cxxopt, --define, and 6 more have changed, discarding analysis cache.\r\nINFO: Analyzed target //tensorflow/lite/c:libtensorflowlite_c.so (0 packages loaded, 1405 targets configured).\r\nINFO: Found 1 target...\r\nERROR: C:/users/alex/_bazel_alex/xv6zejqw/external/flatbuffers/src/BUILD:24:1: C++ compilation of rule '@flatbuffers//src:flatc_library' failed (Exit -1). Note: Remote connection/protocol failed with: execution failed\r\nAction failed to execute: java.io.IOException: ERROR: src/main/native/windows/process.cc(199): CreateProcessW(\"C:\\users\\alex\\_bazel_alex\\xv6zejqw\\execroot\\org_tensorflow\\bin\\false\" -MD -MF bazel-out/armeabi-v7a-opt/bin/external/flatbuffers/src/_objs/flatc_library/flatc.d -frandom-seed=bazel-out/armeabi-v7a-opt/bin/external/flatbuffers/src/_objs/flatc_library/flatc.o -iquote external/flatbuffers -iquote bazel-out/armeabi-v7a-opt/bin/external/flatbuffers -Ibazel-out/armeabi-v7a-opt/bin/external/flatbuffers/src/_virtual_includes/flatc_library -Ibazel-out/armeabi-v7a-opt/bin/external/flatbuffers/src/_v(...)): The system cannot find the file specified.\r\n (error: 2)\r\nTarget //tensorflow/lite/c:libtensorflowlite_c.so failed to build\r\nINFO: Elapsed time: 0.796s, Critical Path: 0.05s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n", "I commented out\r\nbuild --enable_platform_specific_config\r\nin .bazelrc\r\n\r\nand run  configure, compile and got\r\n\r\n```\r\nC:\\tensorflow>bazel build -c opt --config=android_arm //tensorflow/lite/c:libtensorflowlite_c.so\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=118\r\nINFO: Reading rc options for 'build' from c:\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=D:/Program Files/Python/Python36/python.exe\r\nINFO: Reading rc options for 'build' from c:\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --config=v2\r\nINFO: Reading rc options for 'build' from c:\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=D:/Program Files/Python/Python36/python.exe --action_env PYTHON_LIB_PATH=D:/Program Files/Python/Python36/lib/site-packages --python_path=D:/Program Files/Python/Python36/python.exe --config=xla --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file c:\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file c:\\tensorflow\\.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:android_arm in file c:\\tensorflow\\.bazelrc: --config=android --cpu=armeabi-v7a --fat_apk_cpu=armeabi-v7a\r\nINFO: Found applicable config definition build:android in file c:\\tensorflow\\.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\nINFO: Analyzed target //tensorflow/lite/c:libtensorflowlite_c.so (58 packages loaded, 1450 targets configured).\r\nINFO: Found 1 target...\r\nINFO: Deleting stale sandbox base C:/users/alex/_bazel_alex/xv6zejqw/sandbox\r\nERROR: C:/users/alex/_bazel_alex/xv6zejqw/external/flatbuffers/src/BUILD:8:1: C++ compilation of rule '@flatbuffers//src:flatbuffers' failed (Exit 1)\r\n\r\nThe target you are compiling requires Visual C++ build tools.\r\nBazel couldn't find a valid Visual C++ build tools installation on your machine.\r\n\r\nVisual C++ build tools seems to be installed at C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\r\nBut Bazel can't find the following tools:\r\n    cl.exe, link.exe, lib.exe, ml64.exe\r\n\r\nPlease check your installation following https://docs.bazel.build/versions/master/windows.html#using\r\n\r\nTarget //tensorflow/lite/c:libtensorflowlite_c.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 4.621s, Critical Path: 0.14s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n```", "The master still does not ask for my android sdk configuration when running ./configure.", "For those who have problem on build under windows like me, you could try to build under linux. It works.", "> The target you are compiling requires Visual C++ build tools.\r\n> Bazel couldn't find a valid Visual C++ build tools installation on your machine.\r\n\r\nDoes this build command work for you?\r\n```\r\nbazel build -c opt //tensorflow/lite/c:libtensorflowlite_c.so\r\n```\r\n\r\nIt's possible that some of the intermediate build targets require some amount of host (MSVC) compilation, in which case you would need to configure bazel accordingly.", "I would like to build for Android. Will this work?\n\nOn Wed, May 20, 2020 at 4:03 AM Jared Duke <notifications@github.com> wrote:\n\n> The target you are compiling requires Visual C++ build tools.\n> Bazel couldn't find a valid Visual C++ build tools installation on your\n> machine.\n>\n> Does this build command work for you?\n>\n> bazel build -c opt //tensorflow/lite/c:libtensorflowlite_c.so\n>\n> It's possible that some of the intermediate build targets require some\n> amount of host (MSVC) compilation, in which case you would need to\n> configure bazel accordingly.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/38525#issuecomment-631051251>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/APCOC7YS22TVOVUKRSF6VY3RSLQXXANCNFSM4MHSUICQ>\n> .\n>\n", "> \r\n> \r\n> Hmm, looks like the Android config option is disabled by default on Windows. Can you try removing [this line](https://github.com/tensorflow/tensorflow/blob/master/configure.py#L1386) and see if that lets you configure the paths properly for Android builds?\r\n\r\n\r\nCurrently the line you pointed to  refers to a statement which gets the current bazel version required to build. Can you specify exactly what is present in that line so it will be clear for any future references.\r\n"]}, {"number": 38524, "title": "problem with nested tf.function in tensorflow 2", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\n\r\n- OS Platform and Distribution \r\n\r\nNAME=\"Ubuntu\"\r\nVERSION=\"18.04.3 LTS (Bionic Beaver)\"\r\nID=ubuntu\r\nID_LIKE=debian\r\nPRETTY_NAME=\"Ubuntu 18.04.3 LTS\"\r\nVERSION_ID=\"18.04\"\r\nHOME_URL=\"https://www.ubuntu.com/\"\r\nSUPPORT_URL=\"https://help.ubuntu.com/\"\r\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\r\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\r\nVERSION_CODENAME=bionic\r\nUBUNTU_CODENAME=bionic\r\n\r\n- Mobile device\r\n\r\nNot tested on mobile\r\n\r\n- TensorFlow installed from (source or\r\nbinary): \r\n\r\ntensorflow and tf-nightly installed with pip\r\n\r\n- TensorFlow version (use command below): \r\n\r\nTensorflow version: treid with 2.1.0 and tf-nigthly (tf 2.2.0)\r\n\r\n- Python version: \r\n\r\npython3.6\r\n\r\n - Bazel\r\nversion (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n\r\n- CUDA/cuDNN version: - GPU model and memory:\r\n\r\nProblem persists on CPU and GPU, \r\n\r\nCPU:\r\nprocessor       : 3                                                                                                                                                                            \r\nvendor_id       : GenuineIntel                                                                                                                                                                 \r\ncpu family      : 6                                                                                                                                                                            \r\nmodel           : 142                                                                                                                                                                          \r\nmodel name      : Intel(R) Core(TM) i7-8565U CPU @ 1.80GHz                                                                                                                                     \r\n\r\n\r\nGPU:\r\n\r\ndescription: VGA compatible controller\r\nproduct: GP102 [GeForce GTX 1080 Ti]\r\nvendor: NVIDIA Corporation\r\n\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nIt throws a ValueError \r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nDoes not throw ValueError\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\nI made a small example that reproduces the error here\r\n\r\nhttps://colab.research.google.com/drive/1yWowuoHCjuBHckHnS9sRBwnx9i8NtIxW\r\n\r\nIf running that code as is it throws an error, see the \"constraint\" function. If i just copy the contents of the fem function into constraint then it works (as described in the comment in the constraint function).\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nHere's the traceback of the error\r\n\r\n```bash\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_attr(self, name)\r\n   2327       with c_api_util.tf_buffer() as buf:\r\n-> 2328         pywrap_tf_session.TF_OperationGetAttrValueProto(self._c_op, name, buf)\r\n   2329         data = pywrap_tf_session.TF_GetBuffer(buf)\r\n\r\nInvalidArgumentError: Operation 'StatefulPartitionedCall' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n15 frames\r\nValueError: Operation 'StatefulPartitionedCall' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1736         raise ValueError(\"All inputs to `ConcreteFunction`s must be Tensors; \"\r\n   1737                          \"on invocation of %s, the %d-th input (%s) was not a \"\r\n-> 1738                          \"Tensor.\" % (self._func_graph.name, i, str(arg)))\r\n   1739     args = tensor_inputs + captured_inputs\r\n   1740     possible_gradient_type = (\r\n\r\nValueError: All inputs to `ConcreteFunction`s must be Tensors; on invocation of __backward_fem_664, the 0-th input (IndexedSlices(indices=Tensor(\"gradients/PartitionedCall_grad/PartitionedCall_4:1\", shape=(3200,), dtype=int64), values=Tensor(\"gradients/PartitionedCall_grad/PartitionedCall_4:0\", shape=(3200,), dtype=float64), dense_shape=Tensor(\"gradients/PartitionedCall_grad/PartitionedCall_4:2\", shape=(1,), dtype=int32))) was not a Tensor.\r\n```\r\n", "comments": ["Was able to reproduce the issue with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/4217a09da938c934035865d96a6464f5/38524-2-1.ipynb), [TF v2.2.0rc3](https://colab.research.google.com/gist/amahendrakar/2439a4df8967e50bcab92c063330ad2a/38524-2-2.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/06082758377bb6a10d642a4f65e92943/38524-tf-nightly.ipynb). Please find the attached gist. Thanks!", "Note that this has most likely been fixed in b0e08f68d. See https://colab.research.google.com/drive/1wtFY0GIUWwKBN14gLOyzWIeUMDHV42VC?usp=sharing for working TF-nightly gist.", "I can confirm that it fails on 2.2 but passes on tf-nightly. Closing bug. Please feel free to open if there are any other issues.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38524\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38524\">No</a>\n"]}, {"number": 38523, "title": "https://www.tensorflow.org/api_docs/python/tf/saved_model/tag_constants is broken", "body": "## URL(s) with the issue: \r\nhttps://www.tensorflow.org/api_docs/python/tf/saved_model/tag_constants\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/tfx/serving/serving_basic#train_and_export_tensorflow_model\r\n\r\n## Description of issue (what needs changing):\r\nIn the explanation corresponding to **`tags`** in [this link](https://www.tensorflow.org/tfx/serving/serving_basic#train_and_export_tensorflow_model), the Hyperlink corresponding to the Text, related [TensorFlow API documentation](https://www.tensorflow.org/api_docs/python/tf/saved_model/tag_constants) is broken.\r\n\r\n### Clear description\r\n\r\nThis link is useful for the community to understand the purpose of different Tags used while Saving a Model.\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly? : Yes\r\n\r\n### Returns defined\r\n\r\nAre return values defined? : N/A", "comments": ["Thanks, for the report.\r\nOn page, search for \"For more details, see tag_constants.py and related TensorFlow API documentation.\"\r\n"]}, {"number": 38522, "title": "When imports tensorflow, there is an error.", "body": "```python\r\n>>> import tensorflow as tf\r\n2020-04-14 16:10:32.366842: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-04-14 16:10:32.385198: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 84, in <module>\r\n    from tensorflow.python import keras\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\__init__.py\", line 27, in <module>\r\n    from tensorflow.python.keras import models\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\models.py\", line 24, in <module>\r\n    from tensorflow.python.keras import metrics as metrics_module\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py\", line 37, in <module>\r\n    from tensorflow.python.keras.engine import base_layer\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 51, in <module>\r\n    from tensorflow.python.keras import initializers\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\initializers\\__init__.py\", line 127, in <module>\r\n    populate_deserializable_objects()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\initializers\\__init__.py\", line 85, in populate_deserializable_objects\r\n    generic_utils.populate_dict_with_module_objects(\r\nAttributeError: module 'tensorflow.python.keras.utils.generic_utils' has no attribute 'populate_dict_with_module_objects'\r\n```\r\nHow can I fix the ploblem? Can you fix it?I'm using the newest version of tensorflow, 2.2.0rc3.", "comments": ["@happypycode \r\ncan please tell us know how did you install the tensorflow, did you follow [this link](https://www.tensorflow.org/install/source_windows)", "> @happypycode\r\n> can please tell us know how did you install the tensorflow, did you follow [this link](https://www.tensorflow.org/install/source_windows)\r\n\r\nYes. I install tensorflow using pip:\r\n```\r\npip install tensorflow==2.2.0rc3\r\n```\r\nand got this error. I do this:\r\n```\r\npip install tensorflow==2.2.0rc2 # this version that I didn't get error\r\n```\r\nBut when I imports tensorflow, the same! How can I do?", "> @happypycode\r\n> can please tell us know how did you install the tensorflow, did you follow [this link](https://www.tensorflow.org/install/source_windows)\r\n\r\nHey! Just at this moment, I fixed up how to do it:\r\n```\r\npip install tensorflow==2.1.0\r\n```\r\nHowever, this error was solved, but I still wonder about how does this error raise?\r\n\r\n\r\nP.S: I think I'm the first one install tensorflow 2.2.0rc3, when I recive the email, I installed quickly \ud83d\ude06", "@happypycode \r\nas the issue is resolved can we move this to resolved status.", "@happypycode\r\nplease update as per above comment", "Thank you for warning.\r\n```python\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'2.2.0-rc3'\r\n```", "Is the issue still happening in 2.2? Can you post output of `pip list` please?", "@happypycode\r\nplease update if this is still an issue", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 38521, "title": "Installation on Android with armv8", "body": "**System information**\r\n- Xiaomi Redmi 5 plus, Redmi Note 8 Pro, Samsung Galaxy A50\r\n\r\n\r\ni m clone just now project from https://github.com/tensorflow/examples/tree/master/lite/examples/speech_commands/android\r\nbuild and tried install to the phones and at the end of insall i m getting error - \"App not Installed\"", "comments": ["Unassign myself as I am not the expert on this.", "so if tflite do not supported armv8 you may close it", "@B0yma could you share logcat output when you installed the app?", "in logs there is nothing...\r\n\r\nguys i found problem, i get one of this devices...\r\n\r\nso problem was that when i connect my telephone and install via studio app on my device it was installed correctly, but when i transfered apk from my telephone to other devices application wont started. \r\n\r\nThe solution is to generate realese apk build and thats all"]}, {"number": 38520, "title": "MonitoredTrainingSession/Parameter server with large embedding table", "body": "i want to train a model with MonitoredTrainingSession, and here is a large embedding table(K*D, K is almost 100 millions, D is 16), and i have to put the tensor(embedding_table) to one parameter server(ps.0), which will make a bottleneck.\r\n\r\nSo, i want to know is there any method to solve this problem. For example, put the large embedding table to be parted and each PS have one of them. ", "comments": ["@colourful-tree As this issue is not related to bug/performance, build/install, feature request or doc related issues, Please post this issue in stack overflow where there is a wider community to respond. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 38519, "title": "ClusterSpec propagation propagates \"localhost\" to remote", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): CentOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source orbinary): source\r\n- TensorFlow version (use command below): latest master\r\n- Python version: python3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): gcc8\r\n- CUDA/cuDNN version: - GPU model and memory:\r\n\r\nTensorFlow propagates \"localhost\" instead of real ip address to remote.\r\n\r\nDemo code:\r\none is ps\r\n```python\r\nimport tensorflow.compat.v1 as tf                                                                                                                                       \r\ntf.disable_v2_behavior()\r\nserver = tf.distribute.Server(tf.train.ClusterSpec({\"ps\" : [\"ps_ip_address:5333\"]}), job_name=\"ps\", task_index=0, protocol='grpc')                                          \r\nprint(\"start ps\")\r\nserver.join() \r\n```\r\n\r\none worker\r\n```python\r\nimport tensorflow.compat.v1 as tf\r\nfrom tensorflow.core.protobuf import config_pb2\r\nfrom tensorflow.python.training import server_lib\r\nfrom tensorflow.core.protobuf import cluster_pb2\r\nimport time\r\ntf.disable_v2_behavior()\r\n\r\nwith tf.device(\"/job:ps/replica:0/task:0\"):\r\n    a = tf.get_variable(\"param\", [10], tf.float32, initializer=tf.zeros_initializer)\r\n\r\nwith tf.device(\"/job:worker/replica:0/task:0\"):\r\n    update = tf.get_variable(\"update\", [10], tf.float32, initializer=tf.ones_initializer)\r\n    add_op = a.assign_add(update)\r\n\r\ninit_op = tf.initialize_all_variables()\r\n\r\nserver = tf.distribute.Server({\"localhost\": [\"worker_ip_address:0\"]}, protocol=\"grpc\")\r\ncluster_def = cluster_pb2.ClusterDef()\r\nworker_job = cluster_def.job.add()\r\nworker_job.name = 'worker'\r\nworker_job.tasks[0] = server.target[len('grpc://'):]\r\nps_job = cluster_def.job.add()\r\nps_job.name = \"ps\"\r\nps_job.tasks[0] = \"ps_ip_address:5333\"\r\nconfig = config_pb2.ConfigProto(cluster_def=cluster_def, \r\n experimental=config_pb2.ConfigProto.Experimental(share_session_state_in_clusterspec_propagation=True))\r\n\r\nwith tf.Session(server.target, config=config) as sess:\r\n    sess.run(init_op)\r\n    print(sess.run(add_op))\r\n\r\n```\r\nps and server starts on different machines. The ps starts without worker device information and relies cluster spec propagation to propagates worker device information to ps.\r\nHowever, from ps log, worker device is propagated as \"localhost\" to ps.\r\n```console\r\n2020-04-14 13:30:21.673766: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job ps -> {0 -> localhost:5333}\r\n2020-04-14 13:30:21.676047: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:390] Started server with target: grpc://localhost:5333\r\nstart ps\r\n2020-04-14 13:36:33.582439: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job worker -> {0 -> localhost:51798}\r\n2020-04-14 13:36:33.582471: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job ps -> {0 -> localhost:5333}\r\n```\r\nSo ps server tries to create grpc channel to the wrong worker device `localhost:51798` and the session run hangs forever.\r\n\r\nI tried to replace `worker_job.tasks[0] = server.target[len('grpc://'):]` with `worker_job.tasks[0] = server.target[len('grpc://'):].replace(\"localhost\", \"worker_ip_address\")`, but TF failed to create session with following error:\r\n```console\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: The master (current machine) is not included in the provided cluster_def.  job {\r\n  name: \"worker\"\r\n  tasks {\r\n    key: 0\r\n    value: \"worker_ip_address:43479\"\r\n  }\r\n}\r\njob {\r\n  name: \"ps\"\r\n  tasks {\r\n    key: 0\r\n    value: \"ps_ip_address:5333\"\r\n  } \r\n}\r\n```\r\n\r\nI changed the code of https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/master_session.cc#L1355 and replaced all localhost with real ip address, it works. I'm not sure if this change will cause other issues.\r\n\r\nAny idea how to fix this generally?\r\n\r\n", "comments": ["Gently ping @guptapriya; Priya, mind to take a look here?", "cc @saeta who seems to implemented this feature in the first place.", "Close this as the PR has been merged.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38519\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38519\">No</a>\n"]}, {"number": 38518, "title": "InvalidArgumentError: assertion failed: [0] [Op:Assert] name: EagerVariableNameReuse", "body": "when i use tensorflow2.1 .I trained my model custom,yesterday\uff0cthe code can run correctly. But torday is error, I debug my code , I found this code is error ,\r\n```python\r\nself.train_accuracy = tf.keras.metrics.CategoricalAccuracy('train_accuracy')\r\n```\r\nbut this code is actually correct,\r\nNow I assert a variable using this in jupyter notebook, it's wrong !\r\nthe error is :\r\n```\r\nInvalidArgumentError: assertion failed: [0] [Op:Assert] name: EagerVariableNameReuse\r\n```\r\n who can tell me the reason and some solutions,thanks.\r\n\r\nthe whole code is:\r\n```python\r\nimport os\r\nimport numpy as np\r\nimport cv2\r\nimport tensorflow as tf\r\n\r\n\r\n\r\nclass ModelTrain():\r\n    def __init__(self):\r\n        self.loss_object = tf.keras.losses.CategoricalCrossentropy()\r\n        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\r\n        self.train_loss = tf.keras.metrics.CategoricalCrossentropy('train_loss', dtype=tf.float32)\r\n        self.train_accuracy = tf.keras.metrics.CategoricalAccuracy('train_accuracy')\r\n        self.validation_loss = tf.keras.metrics.CategoricalCrossentropy('validation_loss', dtype=tf.float32)\r\n        self.validation_accuracy = tf.keras.metrics.CategoricalAccuracy('validation_accuracy')\r\n        \r\nif __name__ == \"__main__\":\r\n    model_train = ModelTrain()\r\n```\r\n\r\nthe error is :\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/media/huaxin/tcl3/facepro/hand-gesture-recognition/jester-data-preprocessing_v0.2/test.py\", line 18, in <module>\r\n    model_train = ModelTrain()\r\n  File \"/media/huaxin/tcl3/facepro/hand-gesture-recognition/jester-data-preprocessing_v0.2/test.py\", line 12, in __init__\r\n    self.train_loss = tf.keras.metrics.CategoricalCrossentropy('train_loss', dtype=tf.float32)\r\n  File \"/media/huaxin/tcl3/facepro/anaconda3/envs/python3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/metrics.py\", line 2818, in __init__\r\n    label_smoothing=label_smoothing)\r\n  File \"/media/huaxin/tcl3/facepro/anaconda3/envs/python3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/metrics.py\", line 560, in __init__\r\n    super(MeanMetricWrapper, self).__init__(name=name, dtype=dtype)\r\n  File \"/media/huaxin/tcl3/facepro/anaconda3/envs/python3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/metrics.py\", line 460, in __init__\r\n    reduction=metrics_utils.Reduction.WEIGHTED_MEAN, name=name, dtype=dtype)\r\n  File \"/media/huaxin/tcl3/facepro/anaconda3/envs/python3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/metrics.py\", line 296, in __init__\r\n    'total', initializer=init_ops.zeros_initializer)\r\n  File \"/media/huaxin/tcl3/facepro/anaconda3/envs/python3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/metrics.py\", line 276, in add_weight\r\n    aggregation=aggregation)\r\n  File \"/media/huaxin/tcl3/facepro/anaconda3/envs/python3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 446, in add_weight\r\n    caching_device=caching_device)\r\n  File \"/media/huaxin/tcl3/facepro/anaconda3/envs/python3.7.4/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\", line 744, in _add_variable_with_custom_getter\r\n    **kwargs_for_getter)\r\n  File \"/media/huaxin/tcl3/facepro/anaconda3/envs/python3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\", line 142, in make_variable\r\n    shape=variable_shape if variable_shape else None)\r\n  File \"/media/huaxin/tcl3/facepro/anaconda3/envs/python3.7.4/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 258, in __call__\r\n    return cls._variable_v1_call(*args, **kwargs)\r\n  File \"/media/huaxin/tcl3/facepro/anaconda3/envs/python3.7.4/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 219, in _variable_v1_call\r\n    shape=shape)\r\n  File \"/media/huaxin/tcl3/facepro/anaconda3/envs/python3.7.4/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 197, in <lambda>\r\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\r\n  File \"/media/huaxin/tcl3/facepro/anaconda3/envs/python3.7.4/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py\", line 2596, in default_variable_creator\r\n    shape=shape)\r\n  File \"/media/huaxin/tcl3/facepro/anaconda3/envs/python3.7.4/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 262, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"/media/huaxin/tcl3/facepro/anaconda3/envs/python3.7.4/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\", line 1411, in __init__\r\n    distribute_strategy=distribute_strategy)\r\n  File \"/media/huaxin/tcl3/facepro/anaconda3/envs/python3.7.4/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\", line 1557, in _init_from_args\r\n    graph_mode=self._in_graph_mode)\r\n  File \"/media/huaxin/tcl3/facepro/anaconda3/envs/python3.7.4/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\", line 232, in eager_safe_variable_handle\r\n    shape, dtype, shared_name, name, graph_mode, initial_value)\r\n  File \"/media/huaxin/tcl3/facepro/anaconda3/envs/python3.7.4/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\", line 164, in _variable_handle_from_shape_and_dtype\r\n    math_ops.logical_not(exists), [exists], name=\"EagerVariableNameReuse\")\r\n  File \"/media/huaxin/tcl3/facepro/anaconda3/envs/python3.7.4/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_logging_ops.py\", line 55, in _assert\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/media/huaxin/tcl3/facepro/anaconda3/envs/python3.7.4/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 6606, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [0] [Op:Assert] name: EagerVariableNameReuse\r\n```\r\n\r\nthe same situation existed yesterday,I uninstall tensorflow2.1 and re-install,the problemis solved,but today same problem exist again,what's the reason, and how can solve this.\r\n\r\n", "comments": ["@LoveMIssY \r\nplease share standalone code for us to replicate the issue faced", "Facing the same issue when trying to define a tf variable.\r\n\r\n`learning_rate = tf.Variable(initial_value=0.0001, trainable=False, dtype=tf.float32)`", "@matheushent \r\nplease share standalone code for us to replicate the issue faced", "\r\n@Saduf2019  standalone code available on the [gist](https://gist.github.com/matheushent/e5ac809c96bb1ab276514e7ed46517bb).\r\n\r\nAfter instantiating ConvDiscriminator class the same error raises.", "@matheushent \r\ni have replicate the code shared, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/785d66091e5768d24307145dffa70245/38518.ipynb)", "@Saduf2019 my bad, sorry. I fixed the error. Here follows the [fixed gist](https://gist.github.com/matheushent/e5ac809c96bb1ab276514e7ed46517bb).\r\n\r\nBut this is a very strange problem since sometimes it happens and sometimes everything goes very well.\r\n\r\nNote you need to install tensorflow addons.", "@matheushent\r\nplease let us know as the error is fixed can we move this to closed status", "> @matheushent\r\n> please let us know as the error is fixed can we move this to closed status\r\n\r\nI am not sure the error is fixed. As I said, sometimes it happens and sometimes everything goes very well.\r\n\r\nActually I made a test and I realized that the error just appears for me if I use GPU. When using CPU the error doesn't appear.\r\n\r\nMy GPU setup is a RTX 2070 8GB GDDR6.", "@matheushent\r\nas this is one of a kind and not able to replicate it, can we move this to closed status, in case it appears again please share the error and code for us to replicate the same", "> @matheushent\r\n> as this is one of a kind and not able to replicate it, can we move this to closed status, in case it appears again please share the error and code for us to replicate the same\r\n\r\nRight.", "In case somebody runs into this issue, I had a very similar error and it ended up that I had another background process running that was using Tensorflow (I was running a model serving process while trying to a run a model training process). It looks like trying to run two pythong programs that use TF at the same time triggers this error. ", "i have similar error and fixed it by close other python session using tensorflow. Use 'nvidia-smi' to find running process. ", "same here, another process running in background triggered this error. ", "@osushkov Yes, I got the same error.\r\nIn the jupyter lab, I used two notebook.\r\nFinally with closing all notebooks, I solved the issue after open with one notebook. ", "this problem have been solved\uff0cthis problem happens when there are many GPU in your server\uff0cbut some GPU have been used by others\uff0cso when you create some variables or objects, there is no GPU memory for you,the solution is that you shuold  choose one GPU that have not been used.", "> this problem have been solved\uff0cthis problem happens when there are many GPU in your server\uff0cbut some GPU have been used by others\uff0cso when you create some variables or objects, there is no GPU memory for you,the solution is that you shuold choose one GPU that have not been used.\r\n\r\nUm, this problem certainly has **not** been solved. Many of us only have one GPU.", "Ran into this issue as well. 1 GPU. This problem goes away if I disable GPU.", "For me, the issue was caused by the Chrome GPU process.  Apparently, something in Chrome is using the name \"train_loss\".  Once I killed the chrome gpu process, my program started to work.\r\n\r\n\r\nBy encapsulating all tensors into a variable scope (like '__chrome' or something), Chrome would do the ML community a huge favor. I've just filed a bug with Chrome to fix that. https://bugs.chromium.org/p/chromium/issues/detail?id=1086032", "Restarting and clearing the output of jupyter notebook worked for me. ", "Regardless of other underlying circumstances (running on 0/1/many gpus, et.c.) this problem hasn't been solved as long as the error message is totally opaque and unhelpful to anyone not actually developing tensorflow. An addition of 'different gpu processes might be in conflict' would already be a lot better and some more \"forensic\" information wouldn't come amiss either.", "Two or more processes running tensorflow (2.2.0), one of them running a code as simple as `var = tf.Variable([3, 3])` will trigger this error. \r\nSolution: kill the other processes.", "Honestly, killing other processes is not a solution it self. In my opinion this issue must be open since the problem isn't solved yet.", "> \r\n> \r\n> Honestly, killing other processes is not a solution it self. In my opinion this issue must be open since the problem isn't solved yet.\r\n\r\nCompletly agree..  I just wrote \"solution\" for those googling it.. ", "I had the same problem when working with PyCharm. Tensorflow was working with all my python files but I couldn't run any Ipynb files from Pycharm. I always got that error when defining and TF model.\r\nMy tensorflow version in the Jupyter notebook was an unstable version, I uninstalled it and reinstalled Tensorflow 2.2.0 and it became normal. \r\n\r\nAnother factor that might have affected was that initially, the notebook kernel didn't match project interpreter. I had a python3.6 notebook kernel while I used to run all my Tensorflow python codes using python 3.7.7. So I configured the Python 3 kernel to python3.7.7 in jupyter notebook.", "This issue should probably be re-opened, as it exists with only one GPU, with all other services killed (excluding Windows service processes). The issue is resolved by adding something like\r\n`tf.config.experimental.set_visible_devices([], 'GPU')`\r\nto disable the GPU, but that means that you can't train networks using your GPU.", "For me, running unitests while also using my library from jupyter on the same machine also triggers the same error, unless I fall back to tensorflow-cpu or use @AndrewKhans 's solution. This can't be the intended behavior - this should be reopened.", "I got the the same issue when I run the code at https://www.tensorflow.org/xla/tutorials/compile. I run the code on databricks/GPU (p2.xlarge) with one driver and one worker. \r\n\r\nI installed Tensorflow 2.2.0 from \r\n          https://docs.databricks.com/applications/deep-learning/single-node-training/tensorflow.html#install-tensorflow-22-on-databricks-runtime-66-ml&language-GPU. \r\n\r\n\r\n         Tensorflow version : 2.2.0\r\n         Num XLA_GPUs Available:  1\r\n         Num GPUs Available:  1\r\n         phy devices Available:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'), PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\n\r\nMy databricks environment:\r\n       \r\n        6.6 ML, spark 2.4.5, GPU, Scala 2.11  \r\n        Keras version : 2.2.5\r\n\r\n         nvidia-smi\r\n        NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2    \r\n\r\nThe error popped at:\r\n\r\n      for images, labels in train_ds:\r\n      **if optimizer.iterations > TRAIN_STEPS:  # error popped here !**\r\n          break\r\n       train_mnist(images, labels)\r\n\r\nI do not have other Tensorflow or python process running on the same GPU. ", "I had the same problem. Please reopen the bug. It's a shame to put this under the carpet...\r\n\r\nThe issue happens when another process is already using - even partially - the same GPU.\r\n\r\nLooks like Tensorflow tries to occupy the same namespace across processes - the type of bug that gets people fired on most companies and promoted at Google Brain...\r\n", "I am having the same issue with no other processes running. I started to get this error after updating my driver to 418.87. In another node with same GPU series, but earlier driver (396.82), the code works just fine..\r\n\r\nAfter analyzing my code, I discovered the problem and it is funny. In my code, I am using torch and tensorflow and if I import torch first tensorflow will break\r\n\r\n`import torch`\r\n`import tensorflow as tf`\r\n`train_loss = tf.keras.metrics.Mean(name=\u201ctrain_loss\u201d)`\r\n\r\nThe above code will give InvalidArgumentError assertion failed: [0] [Op:Assert] name: EagerVariableNameReuse", "> I am having the same issue with no other processes running. I started to get this error after updating my driver to 418.87. In another node with same GPU series, but earlier driver (396.82), the code works just fine..\r\n> \r\n> After analyzing my code, I discovered the problem and it is funny. In my code, I am using torch and tensorflow and if I import torch first tensorflow will break\r\n> \r\n> `import torch`\r\n> `import tensorflow as tf`\r\n> `train_loss = tf.keras.metrics.Mean(name=\u201ctrain_loss\u201d)`\r\n> \r\n> The above code will give InvalidArgumentError assertion failed: [0] [Op:Assert] name: EagerVariableNameReuse\r\n\r\nSo, basically you're saying that is not possible to run torch and tf models at the same time, right? If it is, this is completely insane and just reaffirms the need of reopening the issue", "Any news on this issue? \r\n\r\nI'm training multiple models at the same time. I should be able to maximize the usage of my GPUs by training concurrently.", "I have the same problem. The only way it works is by disabling GPU, but I want to train the neural network using the GPU. Any update on how to solve this issue? This issue should be reopened", "> I have the same problem. The only way it works is by disabling GPU, but I want to train the neural network using the GPU. Any update on how to solve this issue? This issue should be reopened\r\n\r\nWhat fixed it for me was switching my TensorFlow version (I moved to some kind of older release). You can do this by uninstalling and reinstalling a specific version.", "Just got this error when trying to do \r\n`tensorflow.keras.models.load_model(path_to_file)`\r\nAlready tried to close google and all the things mentioned before, but nothing worked", "I just had a similar problem with model.compile() without using metric while running multiple processes. But it seems to be fixed with tf-2.3 and memory restricted GPU.", "I have same issue too. I have 2 laptop. One has rtx2060 mobile and other gtx860M.\r\nI installed linux a while of time ago. before that it is worked smoothly but after i install windows 10 back its doesn't working anymore\r\nI m facing with this error on gtx860m. rtx one just working fine with these versions. but gtx one not working whatever i do\r\nvisual studio 2019 community with c++\r\nCuda: 10.1 update2\r\nCudnn: 7.6.5\r\nWindows10\r\n( two laptop has same installation versions) \r\nI tried all possiblities.\r\nI tried python 3.7, python 3.8, anaconda versions, i tried driver version wich comes with cuda, i tried updating driver version to 451. I tried tensorflow 2.2.0 and 2.3.0.\r\nand always i installed windows 10 from zero when trying different combination. \r\n\r\n```\r\nimport tensorflow as tf\r\ntrain_loss = tf.keras.metrics.Mean(name=\u201ctrain_loss\u201d)\r\n```\r\n\r\n```\r\nimport tensorflow as tf\r\nm = tf.keras.Sequential()\r\n```\r\n\r\n**or trying to train a model it s not working and giving this error**\r\n\r\n``` File \"C:\\Users\\mehmet\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 1507, in __init__\r\n\tself._init_from_args(\r\n  File \"C:\\Users\\mehmet\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 1661, in _init_from_args\r\n\thandle = eager_safe_variable_handle(\r\n  File \"C:\\Users\\mehmet\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 242, in eager_safe_variable_handle\r\n\treturn _variable_handle_from_shape_and_dtype(\r\n  File \"C:\\Users\\mehmet\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 174, in _variable_handle_from_shape_and_dtype\r\n\tgen_logging_ops._assert(  # pylint: disable=protected-access\r\n  File \"C:\\Users\\mehmet\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_logging_ops.py\", line 49, in _assert\r\n\t_ops.raise_from_not_ok_status(e, name)\r\n  File \"C:\\Users\\mehmet\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 6843, in raise_from_not_ok_status\r\n\tsix.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\nInvalidArgumentError: assertion failed: [0] [Op:Assert] name: EagerVariableNameReuse\r\n```\r\n\r\n**or stoping like this.**\r\n\r\n```2020-08-05 17:36:46.242249: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-08-05 17:36:46.242249: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-08-05 17:36:48.741475: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n2020-08-05 17:36:46.242249: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-08-05 17:36:48.741475: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n2020-08-05 17:36:49.608631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 860M computeCapability: 5.0\r\ncoreClock: 1.0195GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 74.65GiB/s\r\n2020-08-05 17:36:49.608692: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-08-05 17:36:49.613753: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-08-05 17:36:49.618772: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-08-05 17:36:49.620362: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-08-05 17:36:49.626329: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-08-05 17:36:49.629463: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-08-05 17:36:49.641028: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-08-05 17:36:49.641171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-08-05 17:36:49.641781: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-08-05 17:36:49.659086: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x18a99c5aff0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-08-05 17:36:49.659142: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-08-05 17:36:49.659427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 860M computeCapability: 5.0\r\ncoreClock: 1.0195GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 74.65GiB/s\r\n2020-08-05 17:36:49.659465: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-08-05 17:36:49.659487: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-08-05 17:36:49.659506: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-08-05 17:36:49.659523: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-08-05 17:36:49.659539: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-08-05 17:36:49.659555: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-08-05 17:36:49.659576: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-08-05 17:36:49.659646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-08-05 17:36:49.751107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-08-05 17:36:49.751144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]  \t0\r\n2020-08-05 17:36:49.751154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N\r\n2020-08-05 17:36:49.751380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3121 MB memory) -> physical GPU (device: 0, name: GeForce GTX 860M, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n2020-08-05 17:36:49.755399: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x18a9c483ba0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-08-05 17:36:49.755435: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 860M, Compute Capability 5.0\r\n```\r\n", "I got this error when I upgraded from tf 2.2 to 2.3. I downgraded back to 2.2 and I am no longer getting the error.", "I am training a model with tf 2.3 and is the only python running in cpu and I am getting the same error every time.", "I got this error message in a new 2.3 environment (both tensorflow and tensorflow-gpu). When I created a new environment with version 2.2 for both, the problem went away. So this bug shouldn't be closed.", "And just that quickly, when I tried to install it on another computer with 2.2.0, the error message came back. Tried it both with and without tensorflow-gpu, and had the same problem.", "I'm getting the same error. If I force it to use CPU, code executes fine, when using GPU I get the error:\r\n\"InvalidArgumentError: assertion failed: [0] [Op:Assert] name: EagerVariableNameReuse\" \r\n\r\nRunning Win10 Conda setup with GF 930M (Cc 5)\r\n\r\nAll tests indicate that the GPU is available and tests okay. \r\n\r\nDefinitely not a 'Closed' issue\r\n", "I am stuck with the same problem from days, no other processes are running on my GPU. Did someone find a solution?", "> I am stuck with the same problem from days, no other processes are running on my GPU. Did someone find a solution?\r\n\r\ntry to downgrade to 2.2.0 if you havent alredy, for me the problem was Paint 3D, which is running in a background process", "> > I am stuck with the same problem from days, no other processes are running on my GPU. Did someone find a solution?\r\n> \r\n> try to downgrade to 2.2.0 if you havent alredy, it solved my problem\r\n\r\nDidn't work for me...", "@Saduf2019, sorry to ping you directly, but considering how many people are blocked by this issue, would you please consider reopening?  ", "> > I am stuck with the same problem from days, no other processes are running on my GPU. Did someone find a solution?\r\n> \r\n> try to downgrade to 2.2.0 if you havent alredy, for me the problem was Paint 3D, which is running in a background process\r\n\r\nYes, thank you. I actually tried to downgrade to 2.2.0 after posting my reply and it resolved the problem for me. ", "This issue is still persistent and there is no proper cause and solution relationship. Please re-open this issue.", "Same problem here! Downgrading to 2.2.0 didn't solve the problem for me ...", "It seems that the problem is in Python versions.\r\nI'm trying to repeat tutorial https://www.tensorflow.org/tutorials/text/nmt_with_attention\r\nIn py -3.6, tf.2.30 (Windows 7, 64 bit) line\r\nencoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\r\ncreates the same error.\r\nI switched to Windows 10, py -3.7 - no problems.\r\nIn both computers tf  uses GPU, cudart64_101.dll\r\nThis kind of problems are with tf very common, thus I have installed Pyhton 3.5 (works best with tf 1.*), 3.6, 3.7 (currently seems the best) and 3.8 (already 64b)   ", "> It seems that the problem is in Python versions.\r\n> I'm trying to repeat tutorial https://www.tensorflow.org/tutorials/text/nmt_with_attention\r\n> In py -3.6, tf.2.30 (Windows 7, 64 bit) line\r\n> encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\r\n> creates the same error.\r\n> I switched to Windows 10, py -3.7 - no problems.\r\n> In both computers tf uses GPU, cudart64_101.dll\r\n> This kind of problems are with tf very common, thus I have installed Pyhton 3.5 (works best with tf 1.*), 3.6, 3.7 (currently seems the best) and 3.8 (already 64b)\r\n\r\nI don't think that Pyton version is the problem. I am running tf in Win10 with Python 3.7.\r\nI would like to point out that there isn't a single specific line of code that generates the problem. \r\nEven a simple line like `model = tf.keras.Sequential()` causes the assertion to fail.", "Correction to my earlier:\r\nproblem seems to be Microsoft:\r\nunder Windows 10 both python 3.6 and python 3.7 do not create error;\r\nunder Windows 7 (64 bit) 3.6 creates error (python 3.7 is not installed)", "Hi! I have 2 laptops with Windows 10, CUDA 10.1, cudnn 7.6.5:\r\n1. I tested on version 1909, then update it to version 2004 and repeated the test\r\n- on Windows 10 Pro, Python 3.7, Tensorflow 2.3 raises the error.\r\n- on Windows 10 Pro, Python 3.7, Tensorflow 2.2 everything is working.\r\n\r\n2. On corporate version 1809 with Python 3.7 and Tensorflow 2.3 no this error\r\n\r\nMay be the problem is in Windows?", "Like many comments above, I confirm the problem on tensorflow 2.3. Downgrading to tensorflow 2.2 is working. But it is a workaround but it is definitly not solving the problem on 2.3.\r\n@Saduf2019, we should reopen the issue.", "I'm a newb here.  First time trying to get TF working with my Quadro M500M card.  Not sure if it's even relevant, but considering any bit of information could be useful information...\r\n\r\nTF 2.3 + CUDA 10.1 + cudnn 10.1 - 8.0.3.33 + python 3.8 kept telling me it couldn't find cudnn64_7.dll.  After looking around I found with cudnn 10.1 came cudnn64_8.dll.  I'm new to this so I don't know much, but that just struck me as odd that it was asking for a file from a version of cudnn not meant for CUDA 10.1.  That aside, after pointing my path at cudnn 10.0 - 7.6.5.32, which had the correct file, I got past the previous stumbling block, but then to this error.\r\n\r\n`tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [0] [Op:Assert] name: EagerVariableNameReuse`\r\n\r\nFrom this line (from the beginner tutorial)\r\n\r\nmodel = keras.Sequential([\r\n    keras.layers.Flatten(input_shape=(28,28)),\r\n    keras.layers.Dense(128, activation='relu'),\r\n    keras.layers.Dense(10)\r\n])\r\n\r\n", "I am having same problem. I can downgrade to 2.2 but then i cant use tf.keras.preprocessing.text_dataset_from_directory like tensorflow tutorial says, and \"solution\" to that is to use 2.3 but i cant because of problem above. Any suggestions?\r\n\r\nEdit: I had multiple tensorflow installed so i uninstalled both 2.2 and 2.3 and installed tf-nightly which is 2.4. Now everything works fine. Dont know if its because i had multiple versions installed or they fixed it in nightly.\r\nEdit2: Not solved since 2.4 version wasnt using GPU, only CPU. It seems I need CUDA 11 for tf-gpu 2.4 but for some reason CUDA update restarts my computer. I guess i ll downgrade to 2.2\r\n", "> I got this error when I upgraded from tf 2.2 to 2.3. I downgraded back to 2.2 and I am no longer getting the error.\r\n\r\ni am having same problem. it's really work for me. i uninstall tensorflow2.3 and  i install tensorflow 2.2 .", "Hey everyone. After struggling with this issue I finally got tensor flow to work with windows and python. I was getting the eager variable name reuse error. Really was annoying because if you downgrade you can't use functions to import image sets. The other alternative is to not use the GPU...which defeats the purpose if you want train models and use them to make meaningful predictions. \r\n\r\nStep 1: Uninstall tensorflow with pip\r\nStep 2: Go into your site-packages folder within your python installation. Delete ever folder having to do with tensorflow. This is very important. Even after you uninstall a python package it still leaves the tensor flow folders. You must remove these. This is the thing that made it work for me.\r\nStep3: Use pip to install the tf-nightly package\r\nStep4: Now tensorflow complains about tensorboard not being present. Go ahead and pip install tensorboard\r\nStep5: You will see it working but tensorflow complains about cudnn64_8.dll not being available.\r\nStep6: Go to nvdia, create an account and download cuDNN8 for v10.1\r\nStep7: Extract the contents of the zip files into your CUDA directory.\r\nStep8: Attempt to run and it should work now \ud83d\udc4d \r\nHope this helps anyone who has been stuck! Onward to your AI dreams!!!", "Any other ideas to make it works with tensorflow 2.3.1 please ?", "> \r\n> \r\n> Any other ideas to make it works with tensorflow 2.3.1 please ?\r\n\r\nUse Google Colab if you can. I don't think that the problem will be fixed before tf2.4 release.", "Same issue, any suggestions?\r\n`tf 2.3.1`", "You guys need to use the tf-nightly build package....not the official TF one...and before you install it you need to remove all TF stuff from your python site packages directory. If you don't delete TF out of site packages, tf-nightly install won't work like you think.", "> You guys need to use the tf-nightly build package....not the official TF one...and before you install it you need to remove all TF stuff from your python site packages directory. If you don't delete TF out of site packages, tf-nightly install won't work like you think.\r\n\r\nThanks, I will try it.", "> You guys need to use the tf-nightly build package....not the official TF one...and before you install it you need to remove all TF stuff from your python site packages directory. If you don't delete TF out of site packages, tf-nightly install won't work like you think.\r\n\r\nNow `tf-nightly` is `2.5.0.dev20201030`, it seems not supports GPU? \r\n\r\n![image](https://user-images.githubusercontent.com/4510984/97718708-3733f000-1b01-11eb-9728-601f0a3c1bdc.png)\r\n", "@jsudlow, Thank you, indeed, the bug does not seem to appear with tf-nightly. But in my case, I don't have the choise to use tensorflow 2.3.1 because autokeras is only compatible with this version... With tf-nightly, I have another problem (certainly because it is not compatible)", "Same problem here with TF 2.3.1:\r\n\r\n```\r\n>>> var = tf.Variable([3, 3])\r\n(...)\r\n2020-11-06 13:28:09.651195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1750 MB memory) -> physical GPU (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n2020-11-06 13:28:09.651826: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\nTraceback (most recent call last):\r\nFile \"<stdin>\", line 1, in <module>\r\nFile \"/usr/lib/python3.8/site-packages/tensorflow/python/ops/variables.py\", line 262, in __call__\r\nreturn cls._variable_v2_call(*args, **kwargs)\r\nFile \"/usr/lib/python3.8/site-packages/tensorflow/python/ops/variables.py\", line 244, in _variable_v2_call\r\nreturn previous_getter(\r\nFile \"/usr/lib/python3.8/site-packages/tensorflow/python/ops/variables.py\", line 237, in <lambda>\r\nprevious_getter = lambda **kws: default_variable_creator_v2(None, **kws)\r\nFile \"/usr/lib/python3.8/site-packages/tensorflow/python/ops/variable_scope.py\", line 2633, in default_variable_creator_v2\r\nreturn resource_variable_ops.ResourceVariable(\r\nFile \"/usr/lib/python3.8/site-packages/tensorflow/python/ops/variables.py\", line 264, in __call__\r\nreturn super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\nFile \"/usr/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1507, in __init__\r\nself._init_from_args(\r\nFile \"/usr/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1661, in _init_from_args\r\nhandle = eager_safe_variable_handle(\r\nFile \"/usr/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 242, in eager_safe_variable_handle\r\nreturn _variable_handle_from_shape_and_dtype(\r\nFile \"/usr/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 174, in _variable_handle_from_shape_and_dtype\r\ngen_logging_ops._assert(  # pylint: disable=protected-access\r\nFile \"/usr/lib/python3.8/site-packages/tensorflow/python/ops/gen_logging_ops.py\", line 49, in _assert\r\n_ops.raise_from_not_ok_status(e, name)\r\nFile \"/usr/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 6843, in raise_from_not_ok_status\r\nsix.raise_from(core._status_to_exception(e.code, message), None)\r\nFile \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [0] [Op:Assert] name: EagerVariableNameReuse\r\n```\r\n\r\n[**UPDATE**]\r\n\r\nFixed with:\r\n\r\n- cudnn 7.6.5.32-4\r\n- cuda 10.2.89-5\r\n- tensowflow 2.2.0", "I had the same issue today. Using \r\n```\r\npip install tensorflow-gpu==2.2.0\r\n```\r\n \"fixed\" the problem.  \r\nHowever, possibly unrelated, it now takes a long time (like a minute or more) to go from attaching GPU 0 to start training.  Unclear what is taking so long.", "> I had the same issue today. Using\r\n> \r\n> ```\r\n> pip install tensorflow-gpu==2.2.0\r\n> ```\r\n> \r\n> \"fixed\" the problem.\r\n> However, possibly unrelated, it now takes a long time (like a minute or more) to go from attaching GPU 0 to start training. Unclear what is taking so long.\r\n\r\nHi eafpres, I got this error first on tf-gpu-2.3.0 then I switched to tf-gpu-2.2.0 and had the same issue with you.", "> I switched to tf-gpu-2.2.0 and had the same issue with you.\r\n\r\nToday sometimes it\u2019s very fast to start. I cannot find any background process.  But it\u2019s working so I can continue my work!", "> Hey everyone. After struggling with this issue I finally got tensor flow to work with windows and python. I was getting the eager variable name reuse error. Really was annoying because if you downgrade you can't use functions to import image sets. The other alternative is to not use the GPU...which defeats the purpose if you want train models and use them to make meaningful predictions.\r\n> \r\n> Step 1: Uninstall tensorflow with pip\r\n> Step 2: Go into your site-packages folder within your python installation. Delete ever folder having to do with tensorflow. This is very important. Even after you uninstall a python package it still leaves the tensor flow folders. You must remove these. This is the thing that made it work for me.\r\n> Step3: Use pip to install the tf-nightly package\r\n> Step4: Now tensorflow complains about tensorboard not being present. Go ahead and pip install tensorboard\r\n> Step5: You will see it working but tensorflow complains about cudnn64_8.dll not being available.\r\n> Step6: Go to nvdia, create an account and download cuDNN8 for v10.1\r\n> Step7: Extract the contents of the zip files into your CUDA directory.\r\n> Step8: Attempt to run and it should work now \ud83d\udc4d\r\n> Hope this helps anyone who has been stuck! Onward to your AI dreams!!!\r\n\r\n\r\n\r\nDid until step3 and now getting RuntimeError: The current Numpy installation fails to pass a sanity check due to a bug in the windows runtime error :(", "Thank you very much for this post!!! I was having this same issue with Tensorflow v2.3 on a Windows 10 pc with the latest python, conda and pycharm versions, just downgraded to v2.2 and it did the trick. I only needed to go to my Conda prompt, activate my Pycharm environment, uninstall, install and voila! Basically this:\r\n\r\n(base) `conda activate Pycharm`\r\n(Pycharm) `conda uninstall tensorflow`\r\n...\r\n(Pycharm) `conda install tensorflow==2.2`", "Just wanted to drop a line to say I faced the same issue today - trying to run the https://www.tensorflow.org/tutorials/text/text_classification_rnn tutorial.\r\n\r\nOS: Windows 10 Pro x64\r\nGPU: GeForce 940M\r\nDrivers:\r\n- CUDA 10.1\r\n- cudNN 7\r\n- Nvidia 457.30\r\n\r\nWould crash if `tensorflow==2.3`\r\n\r\nDowngrading to `tensorflow==2.2` 'fixed' the issue but causes other bugs within above tutorial (such as inability to evaluate encoder e.g. `encoded_example = encoder(example)[:3].numpy()` as it is missing patch [a92ff929b818c7dbca2d0c2648ae17e8d6ae3a40](https://github.com/tensorflow/tensorflow/commit/a92ff929b818c7dbca2d0c2648ae17e8d6ae3a40). ", "> ror afte\r\n\r\nplace torch's import behind tf's import works for me.\r\n```\r\nimport tensorflow as tf\r\nimport torch\r\n...\r\n```"]}, {"number": 38517, "title": "mkl-dnn sha256sum not match since two available link has different sha256sum", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Utuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): 5.5.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nBazel build fails since mkl-dnn sha256sum doesn't match.\r\n\r\nIn tensorflow/workspace.bzl, there are two links:\r\n\r\nurls = [\r\n            \"https://storage.googleapis.com/mirror.tensorflow.org/github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz\",\r\n            \"https://github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz\",\r\n        ],\r\n\r\nHowever the sha256sum of the two links is not the same. The sha256sum of the first url is given in workspace. Due to some connect issue in China, we don't have access to the first url and can only use the second. Then sha256sum not match is met.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@tz301 \r\n\r\nRequest you to provide the exact sequence of commands / steps that you executed before running into the problem.Thanks!", "@ravikyram\r\n\r\nOur project using tensorflow as dependency. When we use \"bazel build //project_name:module\", it fails on mkl-dnn sha256sum check. We have solved this problem by adding mkl-dnn lines the same as that in tensorflow/workspace.bzl but manually change the strip-prefix and sha256sum in our workspace. \r\n\r\nBut I think it should be a bug, since the two urls for mkl-dnn have different sha256sum, it should be the same, right? Thanks.", "It seems that mkl-dnn redid their release and have different hash now than the one we mirrored. Unfortunately, the mirror is read-only by design, so all you can do is locally change the hash", "This is unintended result of repository migration from `intel/mkl-dnn` to `oneapi-src/oneDNN`. The source code blob includes metadata that include repo name.", "Understood. I have done it locally. \r\nBut I just find that the master branch also has this issue, hope it can be fixed in master branch.", "We cannot fix until there is a new release for mkl-dnn. The things stored at `mirror.tensorflow` are read-only, we cannot rewrite them. So we have to use that hash always.\r\n\r\nWhen mkl-dnn makes a new release we can mirror from the new repo and then both links will have the same hash.", "[oneDNN v1.4](https://github.com/oneapi-src/oneDNN/releases/tag/v1.4) is released.", "We need a PR to update to the new version and then we're can close this. Thank you", "> We need a PR to update to the new version and then we're can close this. Thank you\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/38671   This will fix it  for mkl build (config=mkl).  Compiling TF without config=mkl will still use old 0.2x release (at least for now). \r\n@tz301 how are you building TF?", "@agramesh1  Our project is also a bazel project, and include tensorflow's github url in workspace. Due to this issue, I have to include mkl-dnn url locally (the same version as that in tensorflow's workspace) and copy tensorflow's mkl-dnn.BUILD file. \r\nIt's worked now in our project, though not very elegant. If there are more elegant ways to fix this in old branch, please advise, thank you.\r\n", "@agramesh1,\r\n \r\nIf this would help we can do a patch release from the branch Tensorflow uses. Just let me know which release you need.\r\n", "@vpirogov @agramesh1 \r\nWe are using tf-1.15.0, \"https://github.com/tensorflow/tensorflow/archive/v1.15.0.tar.gz\" is included in our workspace now. Thanks!", "There will be a 1.15.3 patch release soon. We can cherry-pick this on the branch and release with that included too.", "I posted [v0.21.5](https://github.com/oneapi-src/oneDNN/releases/tag/v0.21.5). Updating branches or configurations still relying on v0.x to this version will address the problem for these as well.", "The issue should have been fixed in PR #39725 now. ", "Closing as fixed. Please reopen if that's not the case.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38517\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38517\">No</a>\n", "@mihaimaruseac Seems fixed in master branch. Is this branch also fix as you mentioned before?  \"There will be a 1.15.3 patch release soon. We can cherry-pick this on the branch and release with that included too.\" ", "Oh, there was no PR to cherry-pick for 1.15.3 so it didn't get included.\r\n\r\nWe should cherry-pick the #39725 PR on the `r1.15` branch in the events of there being a 1.15.4 patch release though that would not be before TF 2.3 release at least."]}, {"number": 38516, "title": "Cannot use set_visible_devices with mixed_precision", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): kind of. Combination of 2 example scripts\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux - Fedora 31\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): source\r\n- TensorFlow version (use command below): v2.2.0-rc3-0-gaad398b5e9 2.2.0-rc3\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source): gcc (GCC) 9.2.1 20190827 (Red Hat 9.2.1-1)\r\n- CUDA/cuDNN version: CUDA 10.2, cuDNN 7.6.5.33\r\n- GPU model and memory: 2x GeForce RTX 2080 Ti 12gb\r\n\r\n**Describe the current behavior**\r\nWhen attempting to use `tf.config.set_visible_devices()` in conjunction with `tf.python.keras.mixed_precision.experimental.policy.set_policy()`, the Tensorflow errors with:\r\n```\r\nRuntimeError: TensorFlow device (GPU:0) is being mapped to multiple CUDA devices (0 now, and 1 previously), which is not supported. This may be the result of providing different GPU configurations (ConfigProto.gpu_options, for example different visible_device_list) when creating multiple Sessions in the same process. This is not  currently supported, see https://github.com/tensorflow/tensorflow/issues/19083\r\n```\r\n\r\n**Describe the expected behavior**\r\nNo error\r\n\r\n**Standalone code to reproduce the issue** \r\n```\r\nimport tensorflow as tf\r\ndevices = tf.config.list_physical_devices('GPU')\r\ntf.config.set_visible_devices(devices[1:], 'GPU')\r\n\r\nfrom tensorflow.python.keras.mixed_precision.experimental import policy as mixed_precision\r\nmixed_precision.set_policy(mixed_precision.Policy('mixed_float16'))\r\n```", "comments": ["@phemmer, I have run this code on google colab GPU but i don't found any issue. For your reference link of gist is [here](https://gist.github.com/khimraj/262dbd66a71c266c81f17249f7ba67fe).", "And google collab gives you multiple GPUs? I don't know what this will do if you only have 1 GPU", "Ugh I thought I fixed this in 10e5748ddfcf0c60e5ef0a90bb72a34bc55190ec but didn't. I still call `list_local_devices` even though I intended not to call it.\r\n\r\nI will fix, but I'm unsure if this can be cherrypicked into TF 2.2 this late in the process.\r\n\r\nAs a hacky workaround, you can fix by adding the following lines of code anywhere before setting the policy.\r\n\r\n```\r\nif tf.__version__.startswith('2.2.'):\r\n  from tensorflow.python.keras.mixed_precision.experimental import device_compatibility_check\r\n  device_compatibility_check.log_device_compatibility_check = lambda policy_name, skip_local: None\r\n```\r\nThis replaces the problematic function with one that does nothing. The function normally just logs some info. Due to this bug, the function fails if `list_physical_devices` is called.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38516\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38516\">No</a>\n"]}, {"number": 38515, "title": "macosx py38 invalid wheel", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macosx 10.15.4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.2.0-rc.3 py38\r\n- Python version: 3.8.1\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\ninvalid wheel for tensorflow 2.2.0-rc.3 for macosx python 3.8\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\npip install tensorflow-cpu\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\nERROR: tensorflow-cpu has an invalid wheel, .dist-info directory 'tensorflow-2.2.0rc3.dist-info' does not start with 'tensorflow-cpu'\r\n```", "comments": ["Apologies. We needed to release macos py38 on a different CI infrastructure and we had a bug in the renaming of the pip.\r\n\r\nFortunately, for mac both `tensorflow` and `tensorflow-cpu` pips are identical. Can you use `tensorflow` instead?", "@mihaimaruseac I see, thx", "We will definitely fix this by the final release", "2.2-rc4 should have the issue solved. Can you check please?", "solved, thx", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38515\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38515\">No</a>\n"]}]