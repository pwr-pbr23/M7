[{"number": 16840, "title": "Point Tensorflow To My Local Protobuf Installation", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: sorta\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.4.1\r\n- **Python version**: 2.7 and 3.0\r\n- **Bazel version (if compiling from source)**: 0.6.0\r\n- **GCC/Compiler version (if compiling from source)**: 5\r\n- **CUDA/cuDNN version**: 8 and 6\r\n- **GPU model and memory**: GTX 1080\r\n- **Exact command to reproduce**: make\r\n-**PROTOBUF VERSION**: 3.4.1\r\n\r\nFolks, I am using tensorflow C++, and I added it's path to my CMakeLists.txt.\r\nEverything was working fine, but I had to change the protobuf installation to a different path because CAFFE needs a different version of protobuf other than 3.4.1\r\nNow, Tensorflow libraries are complaining they cannot find common.h from protobuf:\r\n\r\n/usr/local/include/google/tensorflow/tensorflow/core/framework/tensor.pb.h:9:42: fatal error: google/protobuf/stubs/common.h: No such file or directory\r\n\r\nI installed protobuf 3.4.1 here:\r\n/usr/local/include/google\r\n\r\nHow do I make Tensorflow realize that protobuf is installed in a different path path?", "comments": ["See tensorflow/contrib/cmake/external/protobuf.cmake for how tf cmake imports the proto compiler"]}, {"number": 16838, "title": "Fixes issue when linking of rule '//tensorflow/contrib/lite/toco:toco\u2026", "body": "Fixes issue when linking of rule '//tensorflow/contrib/lite/toco:toco' fails because LD_LIBRARY_PATH is not configured. The workaround is to add `--action_env=\"LD_LIBRARY_PATH=${LD_LIBRARY_PATH}\"` to `bazel build` as described here https://stackoverflow.com/questions/47080760/tensorflow-fails-to-compile/47295278#47295278.\r\n\r\nRelated to issue: https://github.com/tensorflow/tensorflow/issues/15142#issuecomment-352562394", "comments": ["The issue happens when building TensorFlow with CUDA support when running `./configure` and then running `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`", "Hi, is this fix planned to be merged in to r1.5?", "As we are very close to 1.6 final, we probably wont backport this fix into 1.5.\r\n", "still broken in both master and tf 1.8 "]}, {"number": 16837, "title": "Remove warnings in tf.losses.softmax_cross_entropy", "body": "This fix tries to address the issue raised in #16534 where tf.losses.softmax_cross_entropy causes warnings due to the calling of tf.nn.softmax_cross_entropy_with_logits.\r\n\r\nThis fix switches to tf.nn.softmax_cross_entropy_with_logits_v2 to remove the warning.\r\n\r\nThis fix fixes #16534.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@martinwicke Would it make sense to make a tf.losses.softmax_cross_entropy_v2 to match the behavior?", "@ebrevdo may know more. It may be useful, I'm not sure.", "The current change LGTM.  It makes sense to have a _v2 in losses as well;\nbut that could be in a separate PR.\n\nOn Wed, Feb 7, 2018 at 1:24 PM, Jonathan Hseu <notifications@github.com>\nwrote:\n\n> Assigned #16837 <https://github.com/tensorflow/tensorflow/pull/16837> to\n> @ebrevdo <https://github.com/ebrevdo>.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/16837#event-1463323497>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimxMUzrOi9DPrzBR21kx2h9CFD4n-ks5tShSXgaJpZM4R8_rM>\n> .\n>\n"]}, {"number": 16836, "title": "TypeError when trying to import tensorflow ", "body": "Hello. I have install TF 1.6 from source and getting next error when trying to import it from python.\r\n```\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 63, in <module>\r\n    from tensorflow.python.framework.framework_lib import *\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/framework_lib.py\", line 76, in <module>\r\n    from tensorflow.python.framework.ops import Graph\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 871, in <module>\r\n    EagerTensor = c_api.TFE_Py_InitEagerTensor(_EagerTensorBase)\r\nTypeError: Unable to create subclass EagerTensor from base class type. Need its size to be <= 32\r\n>>>\r\n```\r\n------------------------\r\n\r\n### System information\r\n- **Linux Ubuntu 16.04**\r\n- **Tensorflow 1.6.0-rc0 built from source**\r\n- **Python 3.6**\r\n- **Bazel version 0.10.0**\r\n- **GCC version 5.4.0**\r\n- **CUDA 9.1. cuDNN 7.0**\r\n- **Nvidia 1050 Ti 4GB**\r\n", "comments": ["Hello again, updates on my issue.\r\nI recompiled tensorflow with the exact same settings but with python 3.5 from Ubuntu repositories and everything work fine. \r\nAfter some examination of source code, i find out that \r\non python 3.5\r\n`sys.getsizeof(type) == 400`\r\nbut on my python 3.6 \r\n`sys.getsizeof(type) == 416`\r\nand because of that `_EagerTensorBase` was bigger then expected.\r\nAnd looks like this was the cause of issue", "hello. I have encountered the same problem when installing tensorflow from source in mac. When I import tensorflow in python, I had the same issue. But I tryed different python versions including \"3.4.0,3.5.0 and 3.6.0\" and none of them works. ", "As per #17530 we need to increase kMaxEagerTensorParentSize to fix this error."]}, {"number": 16834, "title": "Tensorflow not supporting 3D convolution with None input size", "body": "System information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nPartly\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows7\r\nTensorFlow installed from (source or binary):\r\nppip/python\r\nTensorFlow version (use command below):\r\n1.5.0\r\nPython version:\r\npython3.4\r\nCUDA/cuDNN version:\r\n9.0 / 7.0\r\nGPU model and memory:\r\nGTX 1080 Ti\r\n\r\nHello,\r\n\r\nI am trying to build a 3D fully convolutional network with input of variable size. To do that I define x with 3 None dimensions and then apply a conv3d layers:\r\n\r\nexact command to reproduce:\r\n```\r\nx = T.placeholder(shape=(1,nbrChannel,None,None,None),name=\"input\")\r\nconv=T.layers.conv3d(inputs=x,filters=num_filters,kernel_size=filter_size,strides=strides,padding=\"same\",data_format='channels_first')\r\n\r\n```\r\nI get the following error. Please note that I tried to do exactly the same thing with 2D image input (with 2 \"None\" dimensions) and 2D convolution and it worked fine. Therefore, I think this is a problem specific to the conv3d layer function.\r\n\r\n> \r\n> c:\\programdata\\miniconda3\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py in conv3d(inputs, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)\r\n>     817       _reuse=reuse,\r\n>     818       _scope=name)\r\n> --> 819   return layer.apply(inputs)\r\n>     820 \r\n>     821 \r\n> \r\n> c:\\programdata\\miniconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py in apply(self, inputs, *args, **kwargs)\r\n>     760       Output tensor(s).\r\n>     761     \"\"\"\r\n> --> 762     return self.__call__(inputs, *args, **kwargs)\r\n>     763 \r\n>     764   def _add_inbound_node(self,\r\n> \r\n> c:\\programdata\\miniconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py in __call__(self, inputs, *args, **kwargs)\r\n>     650 \r\n>     651         if not in_deferred_mode:\r\n> --> 652           outputs = self.call(inputs, *args, **kwargs)\r\n>     653           if outputs is None:\r\n>     654             raise ValueError('A layer\\'s `call` method should return a Tensor '\r\n> \r\n> c:\\programdata\\miniconda3\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py in call(self, inputs)\r\n>     182           outputs_4d = array_ops.reshape(outputs,\r\n>     183                                          [outputs_shape[0], outputs_shape[1],\r\n> --> 184                                           outputs_shape[2] * outputs_shape[3],\r\n>     185                                           outputs_shape[4]])\r\n>     186           outputs_4d = nn.bias_add(outputs_4d, self.bias, data_format='NCHW')\r\n> \r\n> TypeError: unsupported operand type(s) for *: 'NoneType' and 'NoneType'\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "done!", "I have the same issue...", "I think this issue might be similar to #20527", "Related to #22127"]}, {"number": 16833, "title": "Tensorflow Lite demo app for Android: add support for floating point models as Inception-v3", "body": "Although the new Lite interface does support float models as well, the current Android demo app does only support quantized models. Furthermore, it isn't obvious to transfer the code from the quantized version to the floating point model. [Based on this discussion](https://github.com/tensorflow/tensorflow/issues/14719) I integrated the [Inception-v3 slim model](https://storage.googleapis.com/download.tensorflow.org/models/tflite/inception_v3_slim_2016_android_2017_11_10.zip) as an alternative to the existing MobileNet.\r\n\r\n_Remaining TODO_:\r\nThe confidence scores returned by the inception net are not in [0,1] yet. Besides that, the inference itself seems to work. So the correct results are listed on top, but the confidence score isn't normalized. Maybe the given model doesn't include a Softmax layer and ends with the logits? I'm not sure about this. Any help is appreciated.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLA problem should be solved.", "CLAs look good, thanks!\n\n<!-- ok -->", "For the output of Inception V3 slim 2016 tflite model, see https://github.com/tensorflow/tensorflow/issues/16851"]}, {"number": 16832, "title": " tensorflow/tensorflow/contrib/factorization/python/ops/gmm_ops_test.py  FAILED", "body": "### System information\r\n- **Code** :  tensorflow/tensorflow/contrib/factorization/python/ops/gmm_ops_test.py\r\n- **OS Platform and Distribution** : Linux Ubuntu 16.04)\r\n- **TensorFlow installed from** : Binary\r\n- **TensorFlow version** : '1.4.0'\r\n- **Python version**:  2.7.12\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: CUDA 8.0 & cuDNN 6.0\r\n- **GPU model and memory**: Nvidia 1080Ti\r\n- **Exact command to reproduce**: python gmm_ops_test.py\r\n\r\n### Issue\r\ngmm test isn't working, either gmm_ops_test.py \r\n\r\n### Error message\r\n```\r\n`======================================================================\r\nERROR: testParams (__main__.GmmOpsTest)\r\nTests that the params work as intended.\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"gmm_ops_test.py\", line 147, in testParams\r\n    sess.run(training_ops)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\nInvalidArgumentError: Incompatible shapes: [1,1000,2] vs. [0]\r\n\t [[Node: sub_2 = Sub[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ExpandDims_2, clusters/read)]]\r\n\r\nCaused by op u'sub_2', defined at:\r\n  File \"gmm_ops_test.py\", line 200, in <module>\r\n    tf.test.main()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/test.py\", line 73, in main\r\n    return _googletest.main(argv)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/googletest.py\", line 99, in main\r\n    benchmark.benchmarks_main(true_main=main_wrapper)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/benchmark.py\", line 341, in benchmarks_main\r\n    true_main()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/googletest.py\", line 98, in main_wrapper\r\n    return app.run(main=g_main, argv=args)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/googletest.py\", line 69, in g_main\r\n    return unittest_main(argv=argv)\r\n  File \"/usr/lib/python2.7/unittest/main.py\", line 95, in __init__\r\n    self.runTests()\r\n  File \"/usr/lib/python2.7/unittest/main.py\", line 232, in runTests\r\n    self.result = testRunner.run(self.test)\r\n  File \"/usr/lib/python2.7/unittest/runner.py\", line 151, in run\r\n    test(result)\r\n  File \"/usr/lib/python2.7/unittest/suite.py\", line 70, in __call__\r\n    return self.run(*args, **kwds)\r\n  File \"/usr/lib/python2.7/unittest/suite.py\", line 108, in run\r\n    test(result)\r\n  File \"/usr/lib/python2.7/unittest/suite.py\", line 70, in __call__\r\n    return self.run(*args, **kwds)\r\n  File \"/usr/lib/python2.7/unittest/suite.py\", line 108, in run\r\n    test(result)\r\n  File \"/usr/lib/python2.7/unittest/case.py\", line 393, in __call__\r\n    return self.run(*args, **kwds)\r\n  File \"/usr/lib/python2.7/unittest/case.py\", line 329, in run\r\n    testMethod()\r\n  File \"gmm_ops_test.py\", line 143, in testParams\r\n    [[3.0, 3.0], [0.0, 0.0]], 'w')\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/factorization/python/ops/gmm_ops.py\", line 154, in __init__\r\n    self._define_graph(data)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/factorization/python/ops/gmm_ops.py\", line 274, in _define_graph\r\n    self._define_log_prob_operation(shard_id, shard)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/factorization/python/ops/gmm_ops.py\", line 338, in _define_log_prob_operation\r\n    self._define_full_covariance_probs(shard_id, shard)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/factorization/python/ops/gmm_ops.py\", line 290, in _define_full_covariance_probs\r\n    diff = shard - self._means\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\", line 894, in binary_op_wrapper\r\n    return func(x, y, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 4636, in _sub\r\n    \"Sub\", x=x, y=y, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): Incompatible shapes: [1,1000,2] vs. [0]\r\n\t [[Node: sub_2 = Sub[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ExpandDims_2, clusters/read)]]\r\n\r\n\r\n======================================================================\r\nERROR: test_simple_cluster (__main__.GmmOpsTest)\r\nTests that the clusters are correct.\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"gmm_ops_test.py\", line 125, in test_simple_cluster\r\n    random_seed=self.seed)\r\nValueError: too many values to unpack\r\n\r\n----------------------------------------------------------------------\r\nRan 4 tests in 0.992s\r\n\r\nFAILED (errors=2`\r\n```", "comments": ["Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This is part of tensorflow's [bazel](https://www.tensorflow.org/install/install_sources) packages. So this is supposed to be run like this:\r\n\r\n    cd tensorflow/tensorflow/contrib/factorization\r\n    bazel test :gmm_ops_test :gmm_test\r\n\r\nor just do:\r\n\r\n    bazel test tensorflow/contrib/factorization:all\r\n\r\nfor all packages related to factorization (i.e. kmeans and gmm so far).", "Nagging Assignee @agarwal-ashish: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing as per comment from @xavigonzalvo"]}, {"number": 16831, "title": "Feature Request: Dynamic Convolution Kernels", "body": "I was wondering if it is possible to allow the kernel in conv2d and conv3d to have an additional batch dimension, e.g. to allow the filter shape to be:\r\n`[batch, filter_depth, filter_height, filter_width, in_channels, out_channels]` \r\nHence, the convolution kernel can depend on the input data.\r\n\r\nIn doing so, the convolution kernels can be 'dynamic' and use prior information. \r\nCurrently, the kernels are 'static' and therefore always look for the same patterns in the input data.\r\nHowever, it would be helpful for the kernel to be a function of some input. \r\nThat way a local transformation (learned through back propagation) can be applied to the kernels, in order to look for patterns unique for that image/ event/ data sample.\r\n\r\nI implemented a 3d version of this in python, however, all the indexing and slicing make it rather slow. For the conv2d, I guess one could use tf.extract_image_patches to speed things up, but something equivalent does not exist for the 3d case (unless I'm missing something?).\r\nI tried looking into the code of conv3d and conv2d to see how much effort it would be to implement this. Unfortunately, I'm neither a cuda expert, nor familiar with the way ops and kernels are implemented in tensorflow.\r\n\r\nA feature like this would be greatly appreciated.\r\n\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A\r\n- **TensorFlow installed from (source or binary)**: N/A\r\n- **TensorFlow version (use command below)**: N/A\r\n- **Python version**:  N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A", "comments": ["I guess something like this shouldn't be too much of a slowdown:\r\n\r\n```\r\ndef dynamic_conv(\r\n                input, \r\n                filter, \r\n                strides=[1,1,1], \r\n                padding='SAME',\r\n                dilation_rate=None,\r\n                ):\r\n    '''\r\n    Equivalent to tf.nn.convolution, but filter has additional\r\n    batch dimension. This allows the filter to be a function\r\n    of some input, hence, enabling dynamic convolutions.\r\n\r\n    Parameters\r\n    ----------\r\n    input:  A Tensor. Must be one of the following types: float32, float64, int64, int32, \r\n            uint8, uint16, int16, int8, complex64, complex128, qint8, quint8, qint32, half. \r\n            2d case:\r\n            Shape [batch, in_depth, in_height, in_channels].\r\n            3d case:\r\n            Shape [batch, in_depth, in_height, in_width, in_channels].\r\n\r\n    filter: A Tensor. Must have the same type as input. \r\n            in_channels must match between input and filter.\r\n            2d case:\r\n            Shape [batch, filter_x, filter_y, in_ch, out_ch].\r\n            3d case:\r\n            Shape [batch, filter_x, filter_y, filter_z, in_ch, out_ch] . \r\n\r\n    strides:    A list of ints that has length >= 5. 1-D tensor of length 5. \r\n                The stride of the sliding window for each dimension of input.\r\n                Must have strides[0] = strides[4] = 1.\r\n    padding:    A string from: \"SAME\", \"VALID\". The type of padding algorithm to use.\r\n\r\n    dilation_rate: Optional. \r\n                Sequence of N ints >= 1. Specifies the filter upsampling/input downsampling rate. \r\n                In the literature, the same parameter is sometimes called input stride or dilation. \r\n                The effective filter size used for the convolution will be\r\n                spatial_filter_shape + (spatial_filter_shape - 1) * (rate - 1), \r\n                obtained by inserting (dilation_rate[i]-1) zeros between consecutive elements of \r\n                the original filter in each spatial dimension i. \r\n                If any value of dilation_rate is > 1, then all values of strides must be 1.\r\n\r\n    Returns\r\n    -------\r\n            A Tensor. Has the same type as input.\r\n    '''\r\n\r\n    assert len(filter.get_shape()) == len(input.get_shape()) + 1\r\n    assert filter.get_shape()[0] == input.get_shape()[0]\r\n\r\n    split_inputs = tf.split(input, \r\n                            input.get_shape().as_list()[0], \r\n                            axis=0)\r\n    split_filters = tf.unstack(filter, \r\n                            input.get_shape().as_list()[0], \r\n                            axis=0)\r\n\r\n    output_list = []\r\n    for split_input, split_filter in zip(split_inputs,split_filters):\r\n        output_list.append(\r\n              tf.nn.convolution( split_input,\r\n                            split_filter, \r\n                            strides=strides, \r\n                            padding=padding,\r\n                            dilation_rate=dilation_rate,\r\n                            )\r\n          )\r\n    output = tf.concat(output_list, axis=0)\r\n    return output\r\n```", "@yzhwang can you comment or redirect? Thanks!", "This sounds like a higher level API that first splits both input and filter then run regular convolution before finally concat. @ekelsen can you comment? Thank you!", "Along those lines:\r\nI'm currently investigating how additional symmetries and a priori knowledge can be exploited apart from translational invariance. It would be helpful to have something like a tf.map_fn for patches of a conv2d or conv3d. So instead of applying a convolution on the extracted 2D or 3D input patches, a function fn would be applied.\r\nFor the 2D case a combination of tf.extract_image_patches and tf.map_fn can be used. But nothing equivalent exists for the 3D case as far as I've seen. I've tried plugging some stuff together in python, however, these get extremely memory intensive in addition to being slow.\r\nBut I guess this should go into a separate issue?\r\nAnd thanks for looking into this! :)\r\n ", "We appreciate your input and suggest. We don't have any near plan to support this. Stackoverflow might be a better place to host this discussion."]}, {"number": 16830, "title": "[tflite] fixed label_image resize bilinear problems", "body": "1. Interpreter does not need delete anymore, so cannot use `std::unique_ptr<>`, otherwise there will be double free\r\n2. ResizeBilinear need the `align_corners` parameter after https://github.com/tensorflow/tensorflow/commit/1a0b637df8d082301118dd0f85ec63704f862aeb", "comments": ["Close this, since it's solved by google's internal patch https://github.com/tensorflow/tensorflow/commit/8b00f9867422e89b04988a52f2b3f25870dbbbd7"]}, {"number": 16829, "title": "tf.contrib.estimator.replicate_model_fn fails when a trainable variable doesn't have gradient", "body": "tf.contrib.estimator.replicate_model_fn fails when the gradient of a trainable variable is None. The error messages are:\r\n\r\n```\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 302, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 711, in _train_model\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 694, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/estimator/python/estimator/replicate_model_fn.py\", line 235, in replicated_model_fn\r\n    local_ps_devices=ps_devices)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/estimator/python/estimator/replicate_model_fn.py\", line 558, in _get_loss_towers\r\n    **optional_params)\r\n  File \"model-60m-1280-2gpus-16-32-64-128-bn50000/net.py\", line 38, in model_fn\r\n    train_op = optimizer.minimize(model.total_loss, global_step)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 353, in minimize\r\n    name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/estimator/python/estimator/replicate_model_fn.py\", line 317, in apply_gradients\r\n    with ops_lib.control_dependencies(_extract_tensors(grads_and_vars)):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 4304, in control_dependencies\r\n    return get_default_graph().control_dependencies(control_inputs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 4017, in control_dependencies\r\n    c = self.as_graph_element(c)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3035, in as_graph_element\r\n    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3124, in _as_graph_element_locked\r\n    types_str))\r\nTypeError: Can not convert a NoneType into a Tensor or Operation.\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@isaprykin ", "@x10000year Hi!  I fixed this yesterday and the fix is coming.  Thanks for reporting.\r\nI'll link the commit when it's available and then close the issue.  I hope you can re-built or take the nightly build to take advantage of the fix.", "Thank you!"]}, {"number": 16828, "title": "Code size with XLA AOT", "body": "I currently research about xla using AOT compilation and use Cifar10 as benchmark\r\n\r\nAfter using AOT compilation, I got a binary file that size is 5.3MB, but the original tensorflow graph's size is about 4.2MB. Does it means a using AOT compilation can't promise always reduce code size efficiently?\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "The AOT generated binary contains machine code for the CPU you're targeting (for instance x86) whereas the TF graph is a serialized representation of the TensorFlow graph (very different from machine code).  Everything else remaining equal, you can expect that a larger TF graph will generate a larger binary; but other than that there is no correlation between these two things.", "@sanjoy Thanks for your help very much.\r\nAccording to speech at TensorFlow Dev Summit 2017(https://youtu.be/kAOanJczHA0?t=2831),\r\nCode size can be evaluated by Original(Runtime + Graph) compare with Compiled(Code+Weigh).\r\nAs I know, Graph = TF graph, what does the rumtime means? Does it mean all libraries that graph need to use?\r\nAnd does Code+Weight mean the binary file which compiled by AOT?", "> what does the rumtime means? Does it mean all libraries that graph need to use?\r\n\r\nYes, it is everything that you need to actually execute the graph.\r\n\r\n> And does Code+Weight mean the binary file which compiled by AOT?\r\n\r\nIt depends on the context, but most likely yes.", "It probably makes sense to move any further discussion on this topic to stackoverflow, since it isn't a bug report or feature request. Thanks!"]}, {"number": 16827, "title": "how to build and install cplusplus library and header file to /usr/local?", "body": "how to build and install cplusplus library and header file to /usr/local?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "how to build and install cplusplus library and header file to /usr/local?\r\n\r\nHave I written custom code\r\nNo\r\n\r\nOS Platform and Distribution\r\nCentos 7.0\r\n\r\nTensorFlow installed from\r\nSource\r\n\r\nTensorFlow version\r\n1.5\r\n\r\nBazel version\r\n0.9.0\r\n\r\nCUDA/cuDNN version\r\nNone\r\n\r\nGPU model and memory\r\nNone\r\n\r\nExact command to reproduce", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 16826, "title": "first session.Run() when inference too slow on android and mac with c++ api", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Android-armeabi-v7a, macOS\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.4.0 (for building .so file), 1.3.0 (for model train)\r\n- **Python version**: 3.5.2 (Just used for train)\r\n- **Bazel version (if compiling from source)**:0.8.0\r\n- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.0.0\r\n- **CUDA/cuDNN version**: V8.0.61 (Just used for train)\r\n- **GPU model and memory**: 11GB (Just used for train)\r\n- **Exact command to reproduce**:\r\n\r\n***In a word, after I loaded the model, the cost time of first use session.Run() is longer(4x) than the second use of session.Run() and the third... when coding with C++ API.***\r\n\r\nFirst, I used tensorflow-1.3.0 (python) to define and train a rnn model. Then, I used `freeze_graph ` and `transform_graph ` to make the model files to be one file and shrink the model file size. After setting the `<WORKSPACE>` by adding the following lines, I build the benchmark tool to test the performance.\r\n```\r\nandroid_sdk_repository(\r\n    name = \"androidsdk\",\r\n    api_level = 23,\r\n    build_tools_version = \"27.0.1\",\r\n    # Replace with path to Android SDK on your system\r\n    path = \"/Users/XXX/Library/Android/sdk/\",\r\n)\r\nandroid_ndk_repository(\r\n    name=\"androidndk\",\r\n    path=\"/Users/XXX/Downloads/android-ndk-r12b/\",\r\n    api_level=14)\r\n```\r\nbuilding benchmark tool like this:\r\n\r\n`bazel build -c opt --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --config monolithic   tensorflow/tools/benchmark:benchmark_model`\r\n\r\ntest model performance like this:\r\n\r\n`./bechmark_model --graph=\"frozen_model.pb\" --input_layer=\"Placeholder_data/inputs_placeholder:0,Placeholder_data/length_placeholder:0\" --input_layer_shape=\"1,10:1\" --input_layer_type=\"int32,int32\" --input_layer_values=\"6,13:2\" --output_layer=\"Top_ids/topk:1\"`\r\n\r\nand the performance on Oneplus3T(android 8.0):\r\n\r\n```\r\nnative : stat_summarizer.cc:468 Timings (microseconds): count=300 first=19337 curr=24105 min=19337 max=39753 avg=26515.4 std=4461\r\nnative : stat_summarizer.cc:468 Memory (bytes): count=300 curr=1382834(all same)\r\nnative : stat_summarizer.cc:468 188 nodes observed\r\nnative : stat_summarizer.cc:468\r\n```\r\n\r\nand then I tried to use C++ API to use this model, some major code as following:\r\n\r\n**model load:**\r\n```\r\nStatus RNNInference::InitializeSession(\r\n    int num_threads, const std::string& graph,\r\n    std::unique_ptr<Session>* session,\r\n    std::unique_ptr<GraphDef>* graph_def) {\r\n  tensorflow::SessionOptions options;\r\n  tensorflow::ConfigProto& config = options.config;\r\n // here num_threads = -1\r\n  if (num_threads > 0) {\r\n    config.set_intra_op_parallelism_threads(num_threads); \r\n  }\r\n  LOG(INFO) << \" Got config, \" << config.device_count_size() << \" devices\";\r\n  session->reset(tensorflow::NewSession(options));\r\n  graph_def->reset(new GraphDef());\r\n  tensorflow::GraphDef tensorflow_graph;\r\n\r\n  Status s = ReadBinaryProto(Env::Default(), graph, graph_def->get());\r\n  if (!s.ok()) {\r\n    LOG(ERROR) << \"Could not create TensorFlow Graph: \" << s;\r\n    return s;\r\n  }\r\n\r\n  s = (*session)->Create(*(graph_def->get()));\r\n  if (!s.ok()) {\r\n    LOG(ERROR) << \"Could not create TensorFlow Session: \" << s;\r\n    return s;\r\n  }\r\n\r\n  return Status::OK();\r\n}\r\n```\r\n\r\n**first inference:**\r\n```\r\nint RNNInference::InitializePredict() {\r\n  int ram_size = GetRamKB();\r\n  // \u5185\u5b58\u9650\u5236\r\n  if (ram_size < MIN_MEMORY_SIZE) {\r\n    return -1;\r\n  }\r\n\r\n  tensorflow::Status s;\r\n  const int64 start_time = Env::Default()->NowMicros();\r\n  s = session_.get()->Run(model_inputs_, {\"Top_ids/topk:1\"}, {}, {});\r\n  if (!s.ok()) {\r\n    // return s;\r\n    LOG(ERROR) << \" rnn_inference -- session error : \" << s;\r\n    return -1;\r\n  }\r\n  const int64 end_time = Env::Default()->NowMicros();\r\n  int64 use_time = end_time - start_time;\r\n  LOG(INFO) << \" rnn_inference -- init session use time is \" << use_time;\r\n\r\n  // \u901f\u5ea6\u6d4b\u8bd5 + \u9650\u5236 1 - 20\r\n  if (use_time < 1000000) {\r\n    return ceil(use_time / 50000);\r\n  }\r\n\r\n  // default\r\n  return 0;\r\n}\r\n```\r\n\r\n**second inference and so on:**\r\n\r\n```\r\n  // Keep output results\r\n  std::vector<tensorflow::Tensor> output_tensors;\r\n  // Assign new data to model input\r\n  int history_count = input_words_index.size();\r\n  AssignVaulesFromWordHistroy(input_words_index, history_count);\r\n  tensorflow::Status s;\r\n  const int64 start_time = Env::Default()->NowMicros();\r\n  s = session->Run(model_inputs_, {\"Top_ids/topk:1\"}, {}, &output_tensors);\r\n  const int64 end_time = Env::Default()->NowMicros();\r\n  int64 use_time = end_time - start_time;\r\n  LOG(INFO) << \" input inference use \" << use_time;\r\n  if (!s.ok()) {\r\n    return s;\r\n  }\r\n  auto output_matrix = output_tensors[0].matrix<int32>();\r\n  // dim_size is int64, which is long long\r\n  // int first_dim = (int)(output_tensors[0].dim_size(0) - 1);\r\n  int first_dim = history_count - 1;\r\n  int second_dim = (int)(output_tensors[0].dim_size(1));\r\n  for(int n = 0; n < second_dim; ++n){\r\n    // Pass pad and unk\r\n    if (output_matrix(first_dim, n) > 1) { \r\n      // int is equal to int32\r\n      output_words_index->push_back(output_matrix(first_dim, n));\r\n    }\r\n  }\r\n```\r\n\r\nAnd I build these file or say my app like this and `adb push` it to my android device:\r\n\r\n`bazel build -c opt --copt=\"-DSELECTIVE_REGISTRATION\" --copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\" --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain tensorflow/contrib/xxx/myapp:demo`\r\n> here I used `print_selective_registration_header ` to get `ops_to_register .h`\r\n\r\nLast, I run it on android and test it performance about real application.\r\n**Output:(only show time cost)**\r\n```\r\nnative : inference.cc:126  rnn_inference -- init session use time is 198629 (first)\r\nnative : inference.cc:165  input inference use 53291 (second)\r\nnative : inference.cc:165  input inference use 50341 (third)\r\nnative : inference.cc:165  input inference use 60115 (fourth)\r\nnative : inference.cc:165  input inference use 44707 (fifth)\r\n```\r\n\r\n***If you need other information, please let me know!***\r\n", "comments": ["Also I find a same issue [#11711](https://github.com/tensorflow/tensorflow/issues/11711)", "The first session.run inference is always slow as a lot of one-time computation and initialization happens then.\r\n\r\nThis is a known issue and unlikely to be fixed in tensorflow (probably tflite will be the right answer to this)."]}, {"number": 16825, "title": "Bug of tf.contrib.estimator.replicate_model_fn", "body": "I have used tf.contrib.estimator.replicate_model_fn() recently. I encountered a bug of the implementation (on master version). When there are some global trainable variables created in model_fn() and if the variables are never used (or if they are only used inside tf.cond()), it will fail with the following error messages:\r\n\r\n```\r\n\r\n```", "comments": []}, {"number": 16824, "title": "CMake option for C API", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7 x64\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: branch r1.6\r\n- **Python version**: 3.4\r\n- **Bazel version (if compiling from source)**: 1.7.4 (https://mirror.bazel.build/github.com/NVlabs/cub/archive/1.7.4.zip)\r\n- **GCC/Compiler version (if compiling from source)**: Visual Studio 2015(MSVC 14)\r\n- **CUDA/cuDNN version**: Only CPU\r\n- **GPU model and memory**: Only CPU\r\n- **Exact command to reproduce**: cd tensorflow/contrib/cmake && cmake-gui .\r\n\r\nHi,\r\n\r\nI'm using CMake to build the library on Windows, everything works well, but I'd like to make a very tiny library with C API only, is that possible to do this from CMake ?\r\n\r\nThanks very much", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hi, I modified the first post with more info. Thanks.", "Yes, the `tensorflow` target defined in [`tf_shared_lib.cmake`](https://github.com/tensorflow/tensorflow/blob/b9f548d041ba8d66102c6d195e645051f1bee52f/tensorflow/contrib/cmake/tf_shared_lib.cmake#L64) should build a DLL that exports the C API.", "Nagging Assignee @mrry: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing due to lack of response. If `tf_shared_lib.cmake` doesn't work, please feel free to reopen the issue."]}, {"number": 16823, "title": "Centos 7, installed bazel version is 0.10 yet ./configure complain about atleast 0.4.2 version", "body": "I was trying to build tensorflow (branch r1.0) on centos7.\r\n./configure exited with following error\r\n`Current Bazel version is 0.10.0- (@non-git), expected at least 0.4.2\r\nERROR: Error evaluating WORKSPACE file\r\nERROR: error loading package 'external': Package 'external' contains errors\r\nBuilding: no action`\r\n\r\nSteps involved in installing bazel\r\n1. Go to bazel website\r\n2. Download repo file and copy the same to /etc/repo.d\r\n3. Execute `yum install bazel` \r\n\r\nHave I written custom code: NO\r\nOS Platform and Distribution: Centos 7\r\nTensorFlow installed from: source\r\nTensorFlow version: 0.10\r\nCUDA/cuDNN version: NA\r\nGPU model and memory: NA\r\nExact command to reproduce: ./configure", "comments": ["[updated] \r\nSame problem here. Ubuntu 16.04. Following instructions from https://www.tensorflow.org/install/install_sources  Installed bazel based on \"Using Bazel custom APT repository (recommended)\" (https://docs.bazel.build/versions/master/install-ubuntu.html):\r\n\r\n```\r\nsudo apt-get install openjdk-8-jdk\r\nsudo add-apt-repository ppa:webupd8team/java\r\nsudo apt-get update && sudo apt-get install oracle-java8-installer\r\necho \"deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8\" | sudo tee /etc/apt/sources.list.d/bazel.list\r\ncurl https://bazel.build/bazel-release.pub.gpg | sudo apt-key add -\r\nsudo apt-get update && sudo apt-get install bazel\r\n```\r\n\r\nHave I written custom code: no\r\nOS Platform and Distribution: Ubuntu 16.04\r\nTensorFlow installed from: source\r\nTensorFlow version: r1.0 tag\r\nCUDA/cuDNN version: 8.0/5\r\nGPU model and memory: NVIDIA GeForce GTX 1060\r\nExact command to reproduce: \r\n\r\n```\r\n~/projects/tensorflow$ ./configure \r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python2.7\r\nPlease specify optimization flags to use during compilation [Default is -march=native]: \r\nDo you wish to use jemalloc as the malloc implementation? (Linux only) [Y/n] \r\njemalloc enabled on Linux\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] \r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] \r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] \r\nNo XLA support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\nUsing python library path: /usr/local/lib/python2.7/dist-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] \r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5\r\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 6.1\r\nExtracting Bazel installation...\r\n..............\r\nINFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.\r\n.........\r\nERROR: /home/vvirag/projects/tensorflow/WORKSPACE:8:1: Traceback (most recent call last):\r\n\tFile \"/home/vvirag/projects/tensorflow/WORKSPACE\", line 8\r\n\t\tcheck_version(\"0.4.2\")\r\n\tFile \"/home/vvirag/projects/tensorflow/tensorflow/workspace.bzl\", line 33, in check_version\r\n\t\tfail(\"\\nCurrent Bazel version is {}, ...))\r\n\r\nCurrent Bazel version is 0.10.0, expected at least 0.4.2\r\nERROR: Error evaluating WORKSPACE file\r\nERROR: error loading package 'external': Package 'external' contains errors\r\nBuilding: no action\r\n\r\n```", "Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Switched to tensorflow r1.6, now it works. If you wanna fix the original problem, just checkout the relevant bazel version checking function from the r1.6 in the workspace.bzl.", "I can confirm the issue with Bazel 0.10.1 and Tensorflow 1.5.\r\n\r\nHave I written custom code: NO\r\nOS Platform and Distribution: Ubuntu 17.10\r\nTensorFlow installed from: Source\r\nTensorFlow version: 1.5\r\nCUDA/cuDNN version: NA\r\nGPU model and memory: NA\r\nExact command to reproduce: bazel build --jobs 15 --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\nsteven@desktop:~/opt/git/tensorflow$ ./configure\r\nExtracting Bazel installation...\r\nYou have bazel 0.10.1 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3\r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/lib/python3.6/dist-packages\r\n  /usr/lib/python3/dist-packages\r\n  /usr/local/lib/python3.6/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/lib/python3.6/dist-packages]\r\n/usr/lib/python3/dist-packages\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: Y\r\njemalloc as malloc support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n\r\nNo Google Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: n\r\nNo Hadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n\r\nNo Amazon S3 File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: N\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: N\r\nNo GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: N\r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: N\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: N\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: N\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\n\r\n\r\nAdd \"--config=mkl\" to your bazel command to build with MKL support.\r\nPlease note that MKL on MacOS or windows is still not supported.\r\nIf you would like to use a local MKL instead of downloading, please set the environment variable \"TF_MKL_ROOT\" every time before build.\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nConfiguration finished\r\n\r\n\r\nsteven@desktop:~/opt/git/tensorflow$ bazel build --jobs 15 --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nERROR: /home/steven/opt/git/tensorflow/WORKSPACE:15:1: Traceback (most recent call last):\r\n\tFile \"/home/steven/opt/git/tensorflow/WORKSPACE\", line 15\r\n\t\tclosure_repositories()\r\n\tFile \"/home/steven/.cache/bazel/_bazel_steven/50a548f93783a225d8430a085c4d5aab/external/io_bazel_rules_closure/closure/repositories.bzl\", line 69, in closure_repositories\r\n\t\t_check_bazel_version(\"Closure Rules\", \"0.4.5\")\r\n\tFile \"/home/steven/.cache/bazel/_bazel_steven/50a548f93783a225d8430a085c4d5aab/external/io_bazel_rules_closure/closure/repositories.bzl\", line 172, in _check_bazel_version\r\n\t\tfail((\"%s requires Bazel >=%s but was...)))\r\nClosure Rules requires Bazel >=0.4.5 but was 0.10.1\r\nERROR: Error evaluating WORKSPACE file\r\nERROR: /home/steven/opt/git/tensorflow/WORKSPACE:41:1: Traceback (most recent call last):\r\n\tFile \"/home/steven/opt/git/tensorflow/WORKSPACE\", line 41\r\n\t\ttf_workspace()\r\n\tFile \"/home/steven/opt/git/tensorflow/tensorflow/workspace.bzl\", line 48, in tf_workspace\r\n\t\tcheck_version(\"0.5.4\")\r\n\tFile \"/home/steven/opt/git/tensorflow/tensorflow/workspace.bzl\", line 38, in check_version\r\n\t\tfail(\"\\nCurrent Bazel version is {}, ...))\r\n\r\nCurrent Bazel version is 0.10.1, expected at least 0.5.4\r\nERROR: Error evaluating WORKSPACE file\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'external': Package 'external' contains errors\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'external': Package 'external' contains errors\r\nINFO: Elapsed time: 0.187s\r\nFAILED: Build did NOT complete successfully (0 packages loaded)", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Have I written custom code: NO\r\nOS Platform and Distribution: Centos 7\r\nTensorFlow installed from: source\r\nTensorFlow version: 0.10\r\nCUDA/cuDNN version: NA\r\nGPU model and memory: NA\r\nExact command to reproduce: ./configure", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 16822, "title": "Fix typo", "body": "", "comments": []}, {"number": 16821, "title": "Access to C++ APIs for tfprof", "body": "Hi\r\n\r\nHow can I get access to the C++ APIs for tfprof?\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code - Yes\r\nOS Platform and Distribution - Linux Ubuntu 16.04\r\nTensorFlow installed from - Github (built from source)\r\nTensorFlow version - 1.4.1\r\nBazel version - 0.8.1\r\nCUDA/cuDNN version - 8/6\r\nGPU model and memory - NVidia M40 / 24GB\r\nExact command to reproduce - N/A\r\n\r\nI want to use tfprof in my TF code in C++", "@ChrisAntaki can you please take a look?", "Sorry, I just have experience using the Python API", "@shlens: looks like this question was invited from this documentation: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/tfprof", "I just have experience with the Python API. Reassigning.", "@panyx0718 Could you comment on it?", "@tvkpz  Hi, sorry to bother, did you have solution for this?", "Nagging Assignee @zhangyaobit: It has been 164 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @zhangyaobit: It has been 179 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@gunan Hi, is there any procedure for an external user to request access to C++ API for tfprof ?", "@harshini-gadige no idea.\r\n@shlens looks like you are the only remaining owner of this component. If you have no idea about its C++ interface, I am inclined to say that C++ API for this module does not exist. Could you submit a change to remove the C++ interface from the README?\r\n\r\n@martinwicke FYI\r\n", "This looks like an ownerless contrib module to me. I will close this issue. If someone wants to fix the readme, great. "]}, {"number": 16820, "title": "why tfdbg dumps so much things out", "body": "I was running a very small network(only 1 million parameters), everything is right with the training process, then I tried `tfdbg`, when I get a `tfdbg -> run`, it dumps so many things out, then root directory is filled, before run, there is `11G` left, with a run, quickly fills all the space and prompts no space left:\r\n```\r\n2018-02-07 10:23:07.433343: E ./tensorflow/core/kernels/debug_ops.h:186] Debug node of watch key tower1/gradients/tower1/group2/block3/add_grad/Shape:0:DebugIdentityfailed to publish debug tensor data to all URLs file:///var/tmp/tfdbg_2DnmZj, due to: Publishing to 1 of 1 debug target URLs failed, due to the following errors: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/add_grad, due to: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3;\r\n2018-02-07 10:23:07.443635: E ./tensorflow/core/kernels/debug_ops.h:186] Debug node of watch key tower1/gradients/tower1/group2/block3/conv1/Conv2D_2_grad/ShapeN:0:DebugIdentityfailed to publish debug tensor data to all URLs file:///var/tmp/tfdbg_2DnmZj, due to: Publishing to 1 of 1 debug target URLs failed, due to the following errors: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1/Conv2D_2_grad, due to: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1;\r\n2018-02-07 10:23:07.443804: E ./tensorflow/core/kernels/debug_ops.h:186] Debug node of watch key tower1/gradients/tower1/group2/block3/conv1/Conv2D_2_grad/ShapeN:1:DebugIdentityfailed to publish debug tensor data to all URLs file:///var/tmp/tfdbg_2DnmZj, due to: Publishing to 1 of 1 debug target URLs failed, due to the following errors: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1/Conv2D_2_grad, due to: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1;\r\n2018-02-07 10:23:07.445778: E ./tensorflow/core/kernels/debug_ops.h:186] Debug node of watch key tower1/gradients/tower1/group2/block3/conv1/Conv2D_1_grad/ShapeN:0:DebugIdentityfailed to publish debug tensor data to all URLs file:///var/tmp/tfdbg_2DnmZj, due to: Publishing to 1 of 1 debug target URLs failed, due to the following errors: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1/Conv2D_1_grad, due to: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1;\r\n2018-02-07 10:23:07.445894: E ./tensorflow/core/kernels/debug_ops.h:186] Debug node of watch key tower1/gradients/tower1/group2/block3/conv1/Conv2D_1_grad/ShapeN:1:DebugIdentityfailed to publish debug tensor data to all URLs file:///var/tmp/tfdbg_2DnmZj, due to: Publishing to 1 of 1 debug target URLs failed, due to the following errors: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1/Conv2D_1_grad, due to: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1;\r\n2018-02-07 10:23:07.446192: E ./tensorflow/core/kernels/debug_ops.h:186] Debug node of watch key tower1/gradients/tower1/group2/block3/conv1/Conv2D_grad/ShapeN:0:DebugIdentityfailed to publish debug tensor data to all URLs file:///var/tmp/tfdbg_2DnmZj, due to: Publishing to 1 of 1 debug target URLs failed, due to the following errors: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1/Conv2D_grad, due to: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1;\r\n```\r\nHow to make it work?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 16819, "title": "Branch 184768730", "body": "Pushing from Google Internal to Public", "comments": []}, {"number": 16818, "title": "Fix the tfcompile_tests", "body": "\r\nWe can remove the mock import as its not being used. Also some of the\r\ncheckpoints are V1 and V2 versions and we need to handle both the scenarios\r\n\r\nBUILD: bazel build //tensorflow/compiler/aot/tests:tfcompile_test\r\nTEST: ./bazel-bin/tensorflow/compiler/aot/tests/tfcompile_test\r\n\r\nSigned-off-by: Subash Patel <subash@nod-labs.com>", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "Both changes are not quite right. We intentionally reexport mock from the test module. The second condition is due to the fact that v1 uses files an v2 uses dirs.", "How do you suggest I get past the build failures for XLA tests in master? I can rework and submit you that change."]}, {"number": 16817, "title": "Python3 support of docs generation", "body": "1. Replace codegen by astor.\r\n2. Use tuple._asdict() to replace tuple.__dict__.\r\n\r\nFix #9437", "comments": ["Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Sure I can give it a try. However `bazel test //tensorflow/tools/docs:build_docs_test` runs fine locally. Perhaps because I'm using Python 3.6?", "Nagging Assignee @MarkDaoust: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Where did the commits go?", "Everything's fine, I've got them all here: https://github.com/markdaoust/tensorflow/tree/pr16817\r\n\r\nI was trying to fix the test errors and issued the wrong push command and pushed tensorflow/master to your master instead of the pr branch. That killed the PR, and then locked me out of your branch so I can't fix it.\r\n\r\nThe commits are all intact, if you pull from my branch and reopen this (or a new pr) everything should be right back where it was.\r\n\r\nOr I can push my branch as a new PR (they're still all marked as your commits).\r\n\r\nSorry for the confusion. How should we go about fixing this?", "Cool! Either way works for me. I don't have access to a computer now so you can go with a new PR to keep the tests going", "> so you can go with a new PR to keep the tests going.\r\n\r\nWill do. Thanks again for all your work on this one."]}, {"number": 16816, "title": "Move some ndlstm functions to contrib", "body": "Moved `images_to_sequence` and `sequence_to_images` to contrib.layers since the ndlstm module might be removed in the future tensorflow versions. Addresses this issue: https://github.com/tensorflow/tensorflow/issues/16794", "comments": ["Once this is pushed I can remove `tf.contrib.ndlstm`.", "Test please", "Mind fixing `//tensorflow/contrib/layers:layers_test`?", "Working on that one.", "Fixed it. Forgot to remove the extra argument that was causing the error. My bad.", "I checked the Sanity Check logs and some parts of the code I modified seem to exceed the line length limit. Do I push the fixes now or do I wait for the tests to finish?", "Fix them now and restart tests.\n\nOn Wed, Feb 7, 2018 at 7:16 PM, Jerome <notifications@github.com> wrote:\n\n> I checked the Sanity Check logs and some parts of the code I modified seem\n> to exceed the line length limit. Do I push the fixes now or do I wait for\n> the tests to finish?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/16816#issuecomment-363989585>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimyrwQvwtiw_Ny-ApqiGEUS_4Blmpks5tSmcBgaJpZM4R7_WP>\n> .\n>\n", "Fixed. Test please."]}, {"number": 16815, "title": "Update tensorboard dependency to 1.6.0+ and new name", "body": "TensorBoard for versions 1.6.0+ will use the `tensorboard` name on PyPI rather than the previous `tensorflow-tensorboard` name.  Unfortunately PyPI/pip have no notion of package \"renames\" so it will look like an unrelated dependency, i.e. users doing an upgrade will wind up with both names installed, although the new `tensorboard` package will overwrite the old one functionally speaking.\r\n\r\nLet me know if you think it's worth mentioning this in the release notes.\r\n\r\nNote that the new name currently just has [TB 1.6.0-rc0 on PyPI](https://pypi.python.org/pypi/tensorboard/1.6.0rc0) but we should have the 1.6.0 final release out in a couple days, before TF 1.6.0 is published.\r\n\r\ncc @jhseu \r\n", "comments": ["Change LGTM, but I would like to investigate the pip test failures first.", "Okay, the pip test failure is because it's failing to install the `tensorflow` pip package due to not being able to satisfy the `tensorboard` dependency.  This is a mistake on my part, I thought that the `>=1.6.0` specifier would match `1.6.0-rc0` if there were no other higher versions, but it turns out that actually `1.6.0-rc0` is always ordered as < `1.6.0` no matter what, so it doesn't match.\r\n\r\nWe'll need to publish our TensorBoard 1.6.0 release first and then hopefully on a re-run the test will pass.\r\n\r\nFWIW though it would probably be good to make the pip test smarter and abort early if the pip package fails to install, rather than continuing to attempt to run all the tests.", "This is a rare kind of failure, so we did not need to check this before.\r\nBut the scripts are also checked into git:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/builds/pip.sh#L318\r\nIt should already crash if pip fails but somehow, it is moving forward to running tests.\r\nI am happy to review any PRs to fix it.", "PTAL - TensorBoard 1.6.0 is released to PyPI now so hopefully this time the pip test will pass.\r\n\r\nPer advice from @martinwicke I've also changed RELEASE.md to add a note under Known Bugs about the tensorboard name change since in certain upgrade situations it might result in the tensorboard command and module being inadvertently overwritten (when old, conflicting packages get uninstalled).\r\n"]}, {"number": 16814, "title": "Fix incorrect links in CONTRIBUTING.md", "body": "This fix fixes two incorrect links in `CONTRIBUTING.md` about license examples. The reason for broken links is because tensorboard is in another repo.\r\n\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 16813, "title": "TensorFlow Lite for Inception v3 - Squeeze operator isn't supported", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo, I haven't used any custom code for this.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\nr1.5 (6c5063a3f099c302412fcefa17edb2efa9921f01)\r\n- **Python version**: \r\n3\r\n- **Bazel version (if compiling from source)**:\r\n0.9.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc4.4\r\n- **CUDA/cuDNN version**:\r\n9.0/7.0\r\n- **GPU model and memory**:\r\nGTX 1080 8GB\r\n- **Exact command to reproduce**:\r\n```\r\nbazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n  --input_file=inception-retrained-graph_freezed.pb \\\r\n  --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE \\\r\n  --output_file=inception-retrained-graph.tflite --inference_type=FLOAT \\\r\n  --input_type=FLOAT --input_arrays=input \\\r\n  --output_arrays=InceptionV3/Predictions/Reshape_1 --input_shapes=1,299,299,3\r\n```\r\n\r\n### Describe the problem\r\nI tried to use the above command to convert a retrained inception v3 model into the new tflite format. The input graph has already been freezed. The general process is described in [this tutorial](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite#freeze-graph). However, I'm not able to finish the conversion as the tool complains that the Squeeze operator isn't supported. [According to this guide](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/tf_ops_compatibility.md) the script should be able to remove the squeeze operation. [This comment](https://github.com/tensorflow/tensorflow/issues/16001#issuecomment-356635651) suggests that it may not always be possible to remove it. [The inception v3 architecture is guaranteed to work though](https://www.tensorflow.org/mobile/tflite/). Am I safe using the `--allow_custom_ops` flag?\r\n\r\nSome final note:\r\nI retrained the inception net using the [slim model](https://github.com/tensorflow/models/tree/master/research/inception/inception/slim) rather than just retraining the last layer using the freezed graph version. May this be the cause of the additional squeeze problem?\r\n\r\n### Source code / logs\r\nThis is the complete log as returned by the toco script:\r\n```\r\nW tensorflow/contrib/lite/toco/toco_cmdline_flags.cc:178] --input_type is deprecated. It was an ambiguous flag that set both --input_data_types and --inference_input_type. If you are trying to complement the input file with information about the type of input arrays, use --input_data_type. If you are trying to control the quantization/dequantization of real-numbers input arrays in the output file, use --inference_input_type.\r\n2018-02-07 01:23:50.754337: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1074 operators, 1657 arrays (0 quantized)\r\n2018-02-07 01:23:50.826283: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 126 operators, 317 arrays (0 quantized)\r\n2018-02-07 01:23:50.827683: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 126 operators, 317 arrays (0 quantized)\r\n2018-02-07 01:23:50.828908: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:313] Total transient array allocated size: 11139584 bytes, theoretical optimal value: 8297856 bytes.\r\n2018-02-07 01:23:50.829206: I tensorflow/contrib/lite/toco/toco_tooling.cc:269] Estimated count of arithmetic ops: 11.4574 billion (note that a multiply-add is counted as 2 ops).\r\n2018-02-07 01:23:50.829493: F tensorflow/contrib/lite/toco/tflite/export.cc:303] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: Squeeze.\r\n[1]    16187 abort (core dumped)  bazel-bin/tensorflow/contrib/lite/toco/toco   --output_format=TFLITE\r\n```\r\n", "comments": ["I just figured out that the [commit which is required for the squeeze support](ec652809b15b9308d3f9e864cd8d0b97a532b64a) was actually added after the v1.5 release. Using the latest master commit works just fine. However, it should be considered to backport this feature as the [1.5 docs](https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/contrib/lite/g3doc/tf_ops_compatibility.md) already list the squeeze operator as supported (more precisely it's listed as removable).", "Unfortunately, I still can't use any inception model that I retrained on my own. Using the master branch fixes the above error and allows using the [precompiled inception-v3 slim model](https://storage.googleapis.com/download.tensorflow.org/models/tflite/inception_v3_slim_2016_android_2017_11_10.zip). The latter implies that there shouldn't be any problem with the slim version in general. It doesn't work with mine though. The tflite conversion seems to be completed without any errors now, but if I want to use the graph in the Android example app, I always get the following crash log:\r\n```\r\n02-07 17:32:06.651 18636-18651/android.example.com.tflitecamerademo E/AndroidRuntime: FATAL EXCEPTION: CameraBackground\r\n                                                                                      Process: android.example.com.tflitecamerademo, PID: 18636\r\n                                                                                      java.lang.IllegalArgumentException: Invalid handle to Interpreter.\r\n                                                                                          at org.tensorflow.lite.NativeInterpreterWrapper.getInputDims(Native Method)\r\n                                                                                          at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:82)\r\n                                                                                          at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:112)\r\n                                                                                          at org.tensorflow.lite.Interpreter.run(Interpreter.java:93)\r\n                                                                                          at com.example.android.tflitecamerademo.ImageClassifierFloatInception.runInference(ImageClassifierFloatInception.java:103)\r\n                                                                                          at com.example.android.tflitecamerademo.ImageClassifier.classifyFrame(ImageClassifier.java:107)\r\n                                                                                          at com.example.android.tflitecamerademo.Camera2BasicFragment.classifyFrame(Camera2BasicFragment.java:664)\r\n                                                                                          at com.example.android.tflitecamerademo.Camera2BasicFragment.-wrap0(Camera2BasicFragment.java)\r\n                                                                                          at com.example.android.tflitecamerademo.Camera2BasicFragment$4.run(Camera2BasicFragment.java:559)\r\n                                                                                          at android.os.Handler.handleCallback(Handler.java:739)\r\n                                                                                          at android.os.Handler.dispatchMessage(Handler.java:95)\r\n                                                                                          at android.os.Looper.loop(Looper.java:158)\r\n                                                                                          at android.os.HandlerThread.run(HandlerThread.java:61)\r\n```\r\n\r\nIt doesn't help to recreate the input graph using the current TensorFlow version. I also tried to start the conversion using python instead of the command line, but it doesn't help either.\r\n\r\nAny thoughts?", "The java exception seems to imply that file you are trying to read is invalid. I'd say loading failed at https://github.com/tensorflow/tensorflow/blob/d4ad9c73969c45d1a224ebfc43eb645b9860216b/tensorflow/contrib/lite/java/src/main/native/nativeinterpreterwrapper_jni.cc#L296 but I can't tell exactly why.", "Thanks for your answer. Yes, it states that my model file seems to be invalid, but I don't see a reason why. Unfortunately, I'm not really familiar with the native code parts. Can someone give me some hints how to debug this appropriately?", "Nagging Assignee @andrehentz: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrehentz: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrehentz: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrehentz: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi @Johnson145 any chance you can try this again? I'm hoping the new versions will give you a better error message and help debug.", "Nagging Assignee @andrehentz: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrehentz: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrehentz: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Could you try this again? Can you confirm that toco succeeds and a non-zero file is created?", "Nagging Assignee @andrehentz: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrehentz: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing it, please reopen if you still see this issue "]}, {"number": 16812, "title": "Improve TensorFlow Lite description", "body": "I just started working with the new TensorFlow Lite interface and noticed a few errors in the [description](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite).\r\n- First of all, there is a small typo / copy and paste error that is already fixed in this pr.\r\n- Furthermore, the new file extension isn't described consistently. Sometimes it's _.lite_ and sometimes it's _.tflite_. This inconsistency may be improved at other places as well.\r\n- Finally, the given [link](https://www.tensorflow.org/mobile/android_build) that shall describe the Android integration isn't up-to-date. Instead of the new `compile 'org.tensorflow:tensorflow-lite:+'` resource, the old one (`compile 'org.tensorflow:tensorflow-android:+'`) is given.", "comments": ["Thanks for the patch and your comments. \r\n\r\nI'll talk with other people to unify the filename. "]}, {"number": 16811, "title": "spelling fixes for contrib docs", "body": "", "comments": []}, {"number": 16810, "title": "reshuffle_each_iteration args now default to True", "body": "The [documentation for the tf.Dataset.data.shuffle function](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle) states the following:\r\n\r\n> + **reshuffle_each_iteration**: (Optional.) A boolean, which if true indicates that the dataset should be pseudorandomly reshuffled each time it is iterated over. (Defaults to True.)\r\n\r\nHowever, the default value in the function is None:\r\n\r\n    def shuffle(self, buffer_size, seed=None, reshuffle_each_iteration=None):\r\n\r\nThe function calls the ShuffleDataset class, whose `__init__` function also sets the same argument to None by default, and uses the following logic to set the default value of the argument to True:\r\n\r\n    if reshuffle_each_iteration is None:\r\n      self._reshuffle_each_iteration = True\r\n    else:\r\n      self._reshuffle_each_iteration = reshuffle_each_iteration\r\n\r\nThis commit sets the argument to True by default in both the function and the class, making the above code block redundant and replacing it with only `self._reshuffle_each_iteration = reshuffle_each_iteration`.\r\n", "comments": ["Thanks for the contribution, but unfortunately I am not going to accept this PR for a few reasons:\r\n\r\n* It would be a breaking change for any existing code that calls `Dataset.shuffle(..., reshuffle_each_iteration=None)`.\r\n* In general I prefer `None` for default arguments, because it makes it easier to write wrappers around an API. If the default argument is set to `True`, it's difficult for a wrapper to use the default of the wrapped function, without making a copy of the default at each layer of wrapping. Using `None` and implementing the default at the innermost level is simpler to maintain.\r\n* Using `None` is also consistent with the necessary style when the default argument can be a mutable type (like a list) and it is unsafe to put the default value in the argument list.", "Thank you for reviewing this! I appreciate the details- using `None` makes sense to me now. Thanks again!"]}, {"number": 16809, "title": "  tf.extract_image_patches ", "body": "The documentation of the above function is available on the official Tensorflow page.\r\nhttps://www.tensorflow.org/api_docs/python/tf/extract_image_patches\r\n\r\nCan somebody explain me the meaning of the argument 'rates' and also what do these lines mean, preferably with an example with value of 'rates' >1 ?\r\n\r\n**_This is the input stride, specifying how far two consecutive patch samples are in the input. Equivalent to extracting patches with patch_sizes_eff = patch_sizes + (patch_sizes - 1) * (rates - 1), followed by subsampling them spatially by a factor of rates. This is equivalent to rate in dilated (a.k.a. Atrous) convolutions._**", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}]