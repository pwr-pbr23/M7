[{"number": 8420, "title": "set _interactive in logging utilities", "body": "this update fixes #8401", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I just signed the CLA!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please"]}, {"number": 8419, "title": "Error in compiling TensorFlow from source on Ubuntu: bazel build fails.", "body": "I am compiling TensorFlow from source on an Ubuntu machine. This is a verbose output of the error log generated in the ```bazel build //tensorflow/tools/pip_package:build_pip_package``` execution. \r\n\r\nSystem details:\r\n```uname -a ```: Linux - 3.13.0-107-generic\r\n\r\n\r\n\r\n    ERROR: /home/annanay/.cache/bazel/_bazel_root/9b62c7240de3d9136528cdded945b550/external/llvm/BUILD:418:5: Generating code from table: lib/Target/AArch64/AArch64.td @llvm//:aarch64_target_gen__gen_fast_isel_genrule failed: bash failed: \r\n    error executing command \r\n    (cd /home/annanay/.cache/bazel/_bazel_root/9b62c7240de3d9136528cdded945b550/execroot/tensorflow && \\\r\n    exec env - \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \\\r\n    /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; \r\n    bazel-out/host/bin/external/llvm/llvm-tblgen -I external/llvm/include -I external/llvm/tools/clang/include -I    \r\n    $(dirname external/llvm/lib/Target/AArch64/AArch64.td) -gen-fast-isel external/llvm/lib/Target/AArch64/AArch64.td -o bazel-out/local-opt/genfiles/external/llvm/lib/Target/AArch64/AArch64GenFastISel.inc'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 127.\r\n    bazel-out/host/bin/external/llvm/llvm-tblgen: relocation error: bazel-out/host/bin/external/llvm/llvm-tblgen: symbol _ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE9_M_createERmm, version GLIBCXX_3.4.21 not defined in file libstdc++.so.6 with link time reference\r\n    Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n", "comments": ["There is no information about TensorFlow version, there is not information on compiler version, or your linux distribution. We cannot provide any help just by looking at your linux kernel version.\r\n\r\nI will close this issue now.\r\nPlease file again by filling out all of the issue template."]}, {"number": 8418, "title": "bazel can not quantize image net inception models", "body": "when i run command : `bazel build tensorflow/tools/quantization:quantize_graph --verbose_failures` from this tutorial : https://www.tensorflow.org/performance/quantization, i get this error, and still can not fix it : \r\n    ERROR: /tensorflow/tensorflow/core/kernels/BUILD:1315:1: C++ compilation of rule '//tensorflow    /core/kernels:svd_op' failed: gcc failed: error executing command \r\n    virtual memory exhausted: Cannot allocate memory    \r\n\r\nI run this command on docker with single CPU. I have 1.5 GB RAM in docker.\r\nSo will i need more RAM to run it ?\r\ncan i download a quantize image net inception models without using bazel , like as downloading inception models without compiling it by bazel ?\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\n(And yes, the error suggests just your VM's RAM is not enough.)"]}, {"number": 8417, "title": "Have mnist.py be controlled by tf.set_random_seed() for testing purposes.", "body": "It might be helpful to make the internal random generator within mnist.py be connected to tf.set_random_seed().  Since calling this function is part of the first tutorials to tensorflow, it can be confusing if users try to obtain reproducible results by setting tf.set_random_seed() and seeing that it has no effect (because mnist.py does not comprehend this).", "comments": ["Created pull request with code changes to supersede this issue."]}, {"number": 8416, "title": "CPU resources of Tensorflow's docker containers could not be controlled by --cup-shares  ", "body": "### Environment info\r\nOS:Ubuntu14.04LTS\r\nGPU:Nvidia Pascal TITUN X\r\nCUDA8.0\r\ncuDNN CUDA8.0 V5.1\r\nDocker Verison:17.03.0-ce\r\nNvidia docker : 1.0.1\r\nCPU Intel Core i7 6900K , Hyper-THreading off , Turbo boost off\r\n\r\n\r\n####Docker file\r\nI installed Tensorflow by Dockerfile as below;\r\n\r\n================================\r\n FROM nvidia/cuda:8.0-cudnn5-devel\r\n\r\nENV http_proxy http://mycompany.proxy:8080\r\nENV https_proxy http://mycompany.proxyp:8080\r\n\r\n\r\nRUN     apt-get update  &&  apt-get install -y \\\r\n        python-dev \\\r\n        python-pip \\\r\n        nano \\\r\n        git \r\n    \r\nRUN    rm -rf /var/lib/apt/lists/* /var/cache/apt/archieves/* \r\n\r\nRUN    pip install --upgrade --user https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.0-cp27-none-linux_x86_64.whl\r\n\r\nWORKDIR /root/.local/lib/python2.7/site-packages/tensorflow\r\n\r\nRUN git clone https://github.com/tensorflow/models.git\r\n\r\nWORKDIR /root/.local/lib/python2.7/site-packages/tensorflow/models/tutorials/image/cifar10\r\n\r\n####My Procedure[a]\r\n(1) At first ,I build Dockerfile \r\n     $ nvidia-docker build -t cpu:tensorflow0.\r\n\r\n     Tensorflow docker image was build with no-problem and no-erros.\r\n\r\n(2)Next  I run two docker container and log in two container via Bash\r\n  $ nvidia-docker run --cpuset-cpus=0-5 --cpu-shares=2048 -it cpu:tensorflow0\r\n  $nvidia-docker run --cpuset-cpus=0-5 --cpu-shares=1024 -it cpu:tensorflow0\r\n\r\n    I used all CPU cores(6 cores).\r\n\r\n(3) I did \"python cifar10_train.py\" on two containers ,and check cpu-resources by docker stats.\r\n     By the way, tow python examples\"cifar10_train.py\" were same code.  \r\n     CPU Resource of one container was  339.86%\r\n     CPU Resources of another on another container was 230.19% \r\n   \r\n      CPU Resouce Rate 339.86 : 230.19 was not different from --cpu-shares Rate 2048:1024 \r\n\r\n  ####My Procedure[b]\r\nTo make sure, I did same procedure by using CPU 5cores.   \r\n\r\n(1)Next  I run two docker container and log in two container via Bash\r\n  $ nvidia-docker run --cpuset-cpus=0-4 --cpu-shares=2048 -it cpu:tensorflow0\r\n  $nvidia-docker run --cpuset-cpus=0-4 --cpu-shares=1024 -it cpu:tensorflow0\r\n\r\n    I used only 5 CPU cores. because --cpuset-cpus=0-4\r\n\r\n(2) I did \"python cifar10_train.py\" on two containers ,and check cpu-resources by docker stats.\r\n     By the way, tow python examples\"cifar10_train.py\" were same code.  \r\n     CPU Resource of one container was  278.13%\r\n     CPU Resources of another on another container was 195.80% \r\n   \r\n      CPU Resources Rate 278.13 :195.80 was not different from --cpu-shares Rate 2048:1024 \r\n\r\n ####My Procedure[c]\r\n I did same process by Chainer Containers :Chainer Version 1.21.0\r\n\r\n (1)6cores :Two Chainer containers ,--cpusets-cpu=0-5,--cpu-shares=2048 and --cpu-shares=1024\r\n\r\n     CPU resources   394.23 : 195.41 nealy 2:1 , It is as same as 2048:1024\r\n\r\n(2) 5cores :Two Chainer containers ,--cpusets-cpu=0-4,--cpu-shares=2048 and --cpu-shares=1024\r\n\r\n     CPU resources   330 .82: 166.13 nealy 2:1 , It is as same as 2048:1024\r\n\r\n ####My additional Procedure[d]\r\nI run 4 docker container at --cpu-shares setting at 2048,1024,1024 ,and 512.\r\n\r\n*Tensoflow 4 Container CPU resources : 205.21 : 135.68 : 142.71 : 97.94\r\n  They were not rates of --cpu-shares ; 2.09 : 1.39 : 1.46 : 1.0\r\n\r\n*Chainer 4 Containers CPU resources : 267.44 : 133.93 :129.59 : 59.98\r\n  They ware nearly equal rate of --cpu-shares ; 4.45 : 2.23 : 2.26 : 1.0   \r\n\r\n####Result\r\n\r\nTwo Chainer's docker containers operated according with the setting of --cpu-shares.\r\nBut  Two Tensorflow's docker containers did not operate according with the setting of --cpu-shares.\r\n\r\n ####My Question\r\nDo you know the reason why two Tensorflow's containers did not operate according with the setting of --cpu-shares?\r\n\r\n\r\n\r\n", "comments": ["Caisq, could you please take a look.", "TensorFlow's Dockerfiles don't have any special settings on cpu-shares. I suspect this has to do with the way low-level dependencies of TF do multi-threading. This may involve eigen and stream_executor.\r\n\r\ncc @benoitsteiner @rmlarsen", "@yoshihingis Is this issue resolved? Please update here If it was not resolved already. Thanks!", "Please check with the latest version of TensorFlow. Feel free to reopen if the issues still persists.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=8416\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=8416\">No</a>\n"]}, {"number": 8415, "title": "Cannot decode csv using int32, int64 or float32 in windows when csv is encoded in utf-8 BOM", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttp://stackoverflow.com/questions/38879779/tensorflow-python-framework-errors-invalidargumenterror-field-0-in-record-0-is\r\nhttp://stackoverflow.com/questions/33808368/how-do-i-change-the-dtype-in-tensorflow-for-a-csv-file\r\n\r\n\r\n### Environment info\r\nOperating System: windows10\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n1.0.1\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nimport os\r\nimport sys\r\n\r\nimport tensorflow as tf\r\n\r\nfilename_list = tf.train.match_filenames_once(\"D:/Source/Repos/alerte-source/Alertedh/Python/Alerte.Test.PythonTestingApplication/Alerte.Test.PythonTestingApplication/mnist_data/*.csv\", name='filename_list')\r\n\r\nfilename_queue = tf.train.string_input_producer(filename_list,num_epochs=1,name='filename_queue')\r\n\r\nreader = tf.TextLineReader()\r\nkey, value = reader.read(filename_queue)\r\n\r\nrecord_defaults = [tf.constant([], dtype=tf.int32) for row in range(785)]\r\n\r\ndata = tf.decode_csv(value, record_defaults=record_defaults)\r\n\r\nprint(data[0].dtype)\r\n\r\nfeatures = tf.stack(data[:-1])\r\n\r\nlabel = tf.stack(data[-1])\r\n\r\nmin_after_dequeue = 10000\r\nbatch_size = 2\r\ncapacity = min_after_dequeue + 3 * batch_size\r\n\r\nexamples, labels = tf.train.shuffle_batch([features, label],batch_size=batch_size, capacity=capacity,\r\n      min_after_dequeue=min_after_dequeue)\r\n\r\n\r\nwith tf.Session() as sess:\r\n  tf.initialize_all_variables().run()\r\n  tf.local_variables_initializer().run()\r\n  tf.global_variables_initializer().run()\r\n\r\n  # start populating filename queue\r\n  coord = tf.train.Coordinator()\r\n  threads = tf.train.start_queue_runners(coord=coord)\r\n\r\n  try:\r\n    while not coord.should_stop():\r\n      example_batch, label_batch = sess.run([examples, labels])\r\n      print(example_batch)\r\n  except tf.errors.OutOfRangeError:\r\n    print('Done training, epoch reached')\r\n  finally:\r\n    coord.request_stop()\r\n\r\n  coord.join(threads) \r\n\r\n\r\nexample csv:\r\n2,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17\r\n\r\n### What other attempted solutions have you tried?\r\nWorks when the dtype is string as it doesnt have to try to parse the values...\r\n\r\n### Logs or other output that would be helpful\r\nnone\r\n\r\n\r\nError: tensorflow.python.framework.errors.InvalidArgumentError: Field 0 in record 0 is not a valid int32: 2 \r\n\r\n(this is likely a windows only error)\r\nBecause of the encoding prefix / (\\xef\\xbb\\xbf) the string decoders cannot read the first element of the file... resiliency to this can help cross platform errors. the error doesnt pick up the prefix either, so it was a bit of a mystery to me for a while... \r\n\r\nIf this is not to be supported then feel free to close\r\n\r\nregardless, Ill put this here for other people having similar issues. you can check by setting your dtype to string and checking the first element for the prefix... go to notepad++ or similar and convert your file to UTF-8 rathter than UTF-8 BOM which is default.\r\n\r\n", "comments": ["To do that, we maybe be able to plumb through a file mode via the [`TextLineReader`](https://github.com/tensorflow/tensorflow/blob/b07791f6e9b306937eb58f7bb6c3300cd26583af/tensorflow/core/kernels/text_line_reader_op.cc#L38) to the [windows file open](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/windows/windows_file_system.cc#L242) with [`_O_UTF8`](https://msdn.microsoft.com/en-us/library/yeby3zcb.aspx). If you feel strongly about this and want to implement this, let us know.\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 8414, "title": "softmax_cross_entropy_with_logits() behaves differently with it's comment", "body": "version: 1.0.1 CPU compiled by cmake from master branch.\r\n\r\nAccording to [the document](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits), `softmax_cross_entropy_with_logits()` accept inputs with any shape. However, the comments in the source code says:\r\n\r\n`logits` and `labels` must have the same shape `[batch_size, num_classes]`  and the same dtype (either `float16`, `float32`, or `float64`).\r\n\r\nmeaning that only rank-2 tensors are allowed.\r\n\r\nI tried the following code to construct a net which performs binary-classify on each pixel of an image :\r\n\r\n```\r\n    final_two_channel = tf.layers.conv2d(\r\n        inputs=res_out_iter,\r\n        filters=2,\r\n        kernel_size=[1, 1],\r\n        padding='same',\r\n        activation=tf.nn.sigmoid\r\n    )\r\n    # logits = tf.reshape(tensor=final_two_channel, shape=[-1, 2], name='flatten_net_out')\r\n    label_one_hot = tf.one_hot(\r\n        indices=output_field,\r\n        depth=2,\r\n        on_value=1, off_value=0,\r\n        name='label_one_hot'\r\n    )\r\n    crs_etp_loss = tf.nn.softmax_cross_entropy_with_logits(labels=label_one_hot, logits=final_two_channel)\r\n    print(crs_etp_loss.shape)\r\n    crs_etp_loss = tf.reshape(tensor=crs_etp_loss, shape=[-1, field_width * field_width])\r\n    crs_etp_loss = tf.reduce_mean(input_tensor=crs_etp_loss, axis=1)\r\n\r\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss=crs_etp_loss)\r\n```\r\n\r\nThe program runs without error and `print(crs_etp_loss.shape)` gives `(?, 512, 128)`, but I am not sure if it's safe/correct to do like this.\r\n\r\nI posted a question on [stackoverflow](http://stackoverflow.com/questions/42799167/is-it-safe-to-use-softmax-cross-entropy-with-logits-when-each-instance-has-mul) about this.", "comments": ["I'm gonna close this issue as it's not a bug or feature request. Thanks for posting to StackOverflow, that's the preferred forum for usage questions like this."]}, {"number": 8413, "title": "configure: Bugfix", "body": "commit 429d14d435259899ff0436014187fe35c6cdd1d7 introduced\r\nTF_BAZEL_TARGETS. However, it turns out that if:\r\n\r\n`TF_BAZEL_TARGETS=\"//tensorflow/... -//tensorflow/contrib/nccl/...\"`\r\nthen\r\n`bazel fetch \"$TF_BAZEL_TARGETS\"`\r\nworks out fine, but\r\n`bazel fetch $TF_BAZEL_TARGETS`\r\nfails with\r\n`\"Invalid options syntax: -//tensorflow/contrib/nccl/...`\r\n\r\nHowever, if\r\n`TF_BAZEL_TARGETS=\"//tensorflow:libtensorflow.so //tensorflow/tools/lib_packages:clicences_generate\"`\r\nthen\r\n`bazel fetch \"$TF_BAZEL_TARGETS\"`\r\nfails with:\r\n`  ERROR: Error while parsing 'deps(//tensorflow:libtensorflow.so  //tensorflow/tools/lib_packages:clicences_generate)'`\r\nwhile\r\n`bazel fetch $TF_BAZEL_TARGETS`\r\nsucceeds.\r\n\r\nThe latter is used in the Windows libtensorflow release scripts.\r\nFor now, a quick fix to make the intended usage of TF_BAZEL_TARGETS\r\nwork. Revisit the appropriate fix later.", "comments": []}, {"number": 8412, "title": "Fix: Use TestCase.get_temp_dir in dataframe_test.", "body": "Addresses nightly failure here:\r\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=NO_OPT,TF_BUILD_IS_PIP=NO_PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/419/consoleFull", "comments": []}, {"number": 8411, "title": "nn_ops.py docs clarify atrous output dimensions", "body": "Clarifies documentation based on discussion in #4742, but does not solve that original issue.", "comments": ["Can one of the admins verify this patch?", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Jenkins, test this please."]}, {"number": 8410, "title": "tfdbg strange things about detecting NAN weights in network", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nHi all, I just meet the NAN weight in the training for my network, I have check the loss and see whether there is like the form of 0/0, or log(0) and changing the learning rate but it still meet the problem when I try in the 200 steps. So i use tfdbg to debug, it is really nice tool. But when I use it, it running only several steps and stop. And I print this node out, this node has so many numbers. when I use pt node, and there is so many numbers to detect, I am not find the inf and nan. Besides, the weights i even not change it, so what cause this problem\r\n### Environment info\r\nOperating System:\r\nUbuntu 14:04\r\nCuda 8.0\r\ncuDNN5.1\r\nInstalled version of CUDA and cuDNN: \r\n\r\nthis is output in the screen--- run-end: run #103: fetch: t_DQN/dqn_q/BiasAdd:0; 2 feeds ----------------------------------------------------\r\nTensor \"encoder/conv0/Conv2D:0:DebugIdentity\":\r\n  dtype: float32\r\n  shape: (8, 149, 149, 32)\r\n\r\narray([[[[ -2.31261123e-02,  -2.28441790e-01,  -1.15194634e-01,   1.87772080e-01,   1.52234808e-01,\r\n            9.16329771e-02,  -1.40365764e-01,   8.70720018e-04,   1.98106512e-01,  -3.14010084e-02,\r\n            5.08706868e-02,  -1.47755712e-01,   1.76122814e-01,   4.00081761e-02,  -1.88322872e-01,\r\n           -1.56596974e-01,  -1.78046972e-01,  -1.91862926e-01,  -1.90227181e-01,  -6.63982928e-02,\r\n            1.77524462e-01,  -7.36486465e-02,  -6.05005510e-02,   1.09997056e-01,   9.24435183e-02,\r\n            4.22568992e-05,  -1.06559973e-03,   9.99473780e-02,  -1.73573643e-02,   1.16514668e-01,\r\n           -1.51743174e-01,   2.23651499e-01],\r\n         [ -2.14941762e-02,  -2.30754048e-01,  -1.18553132e-01,   1.89629942e-01,   1.53854966e-01,\r\n\r\n\r\n", "comments": ["@caisq , thanks, if you have time to check it.", "@Lan1991Xu,\r\n\r\ntfdbg CLI has two features that might be useful to you here.\r\n1) Regular expression search, similar to the Linux `less` UI. For example, if you want to search for `inf` and `nan`, you can scroll to the top with the `Home` key and then do:\r\n```\r\ntfdbg> /(inf|nan)\r\n```\r\nThe CLI will try to find `inf` or `nan` and if any is found, scroll to the first instance. If any match is found, you can continue to the next ones by using `tfdbg> /`, just like in `less`.\r\n\r\n2) You can slice large tensors by using numpy style slicing, e.g.,\r\n```\r\ntfdbg> pt encoder/conv0/Conv2D:0:DebugIdentity[0, :, :, :]\r\n```\r\n\r\nIf you have more questions, let me know.\r\n", "There is actually another potentially useful feature: the `-r` option of the `pt` command. It allows you to count how many elements a Tensor falls into a certain range. The range can be a real number or `inf`. If you do:\r\n```\r\ntfdbg> pt encoder/conv0/Conv2D:0:DebugIdentity -r [-inf,inf]\r\n```\r\nthe CLI will count how many elements are >=-inf and <inf. So if you there is one NaN, the element will not be counted. You'll see in the first line on the top something like:\r\n999 of 1000 elements (99.90%)\r\n\r\nIn addition, please beware that if you use the command `lt`, the CLI will list all the dumped tensors from the current `Session.run()` call. However, if you do `lt -f has_inf_or_nan`, it should list only the dumped tensors that contain `inf`s or `nan`s.", "Hi Cai\r\nThanks for your very useful suggestion, it seems in this pt encoder/conv0/Conv2D:0 -r [-inf,inf] really have NAN, the output of the number is small than predefined number.  However, the strange things is that I use this to extract my features and test, the performance is really good. Actually, the weight NAN really influence my performance is another network(DQN network). I use the first network to extract feature(the weight is fixed). Can i directly check the NAN in my DQN network, ignoring the NAN number in the feature extraction?", "By the way, you means using the command lt. I guess if I directly use run -f has_inf_or_nan, the output of dumped tensors are all related inf and nan problems, so I have no need to do that?", "@Lan1991Xu \r\n\r\nRegarding your first question, if the nodes in your DQN network have a common naming pattern, maybe you can use the `-n` option of `lt`, such as:\r\n```\r\ntfdbg> lt -f has_inf_or_nan -n \"^DQN/.*\"\r\n```\r\n\r\nThis will ignore all the Tensors with NaNs and Infs whose name do not begin with DQN.\r\n\r\nIf you want to write a tensor_filter (i.e., conditional breakpoint between runs) similar to tf_debug.has_inf_or_nan, but checks only the \"^DQN/.*\" notes, you can do:\r\n```python\r\ndef has_inf_or_nan_in_dqn(datum, tensor):\r\n  return datum.node_name.startswith(\"DQN\") and tf_debug.has_inf_or_nan(datum, tensor)\r\n\r\nsess = tf_debug.LocalCLIDebugWrapperSession(sess)\r\nsess.add_tensor_filter(\"has_inf_or_nan_in_dqn\", has_inf_or_nan_in_dqn)\r\n```\r\n\r\nThen when you do `run -f has_inf_or_nan_in_dqn`, tfdbg will only break if the DQN network part of your model contains NaNs or Infs.\r\n\r\nRegarding your second question, yes, if you use the `run -f ...` command, the initial screen automatically does `lt -f ...` for you.", "Thanks caisq!I just find the bug, it takes me about two days. tfdbg is really powerful tools, I find it is indeed cause by my feature extraction when i move action on images!  This tools really amazing for us to debug tensorflow. By the way, when I use this tfdbg, it seems when I need to save the model and also using tfdbg, it will report: ufunc\"is nan\" not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule\"safe\". I guess it is  the filter problem, not about the tfdbg itself. Any suggestions about define this filter to avoid this problems?", "@Lan1991Xu Glad it worked for you.\r\n\r\nRegarding the ufunc error. Can you copy and paste the entire error message and stack trace if it is not too inconvenient for you? Thanks.", "Hi caisq\r\nThe report error is :   ufunc\"is nan\" not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule\"safe\"\r\nWhen I use filter \"has inf or nan\", I guess it is some conflict about input file or save the data. So when I temporary remove  the save data \"  self.saver.save(self.sess, self.model_dir + 'snapshot', global_step = step) \" and load weights before I add \"tf_debug.LocalCLIDebugWrapperSession(self.sess)\", it works. ", "However, for some code, like test on Inception V3 for slim, I can not simply change the load weights or data. It is difficult for me to debug on inception V3 model on NAN. The problem I guess is related to the specific filter.  After use this tool, it is really convenient for me to debug tensorflow, and I also find how to place \"self.sess= tf_debug.LocalCLIDebugWrapperSession(self.sess)\r\n\t\t#self.sess.add_tensor_filter(\"has_inf_or_nan\", tf_debug.has_inf_or_nan)\" is also important.\r\nAny suggestions about this. Cause i find I also meet the problem \" Error in tensorflow debugger (tfdbg) while executing session run call in child thread\", but only change the place of this line, it works.", "@caisq any update on this? @Lan1991Xu are you still experiencing this problem?", "Solve it. It is nice to use this.", "Thanks for checking, @skye. Thanks for confirming, @Lan1991Xu ."]}, {"number": 8409, "title": "Windows: is Python 2.7 support planned?", "body": "Good evening! Are you planning to support Python 2.7 on Windows? It may be very useful.", "comments": ["@gunan and @mmry can correct me if I'm wrong - but we don't have any immediate plans to add Python 2 support for Windows.", "CC @mrry (corrected username)\r\nAFAIK, unless we build python 2.7 from sources using Visual studio 2015 or 2017, it is not possible to build tensorflow with python 2.7.\r\nTherefore, we will not support python 2.7 on windows.", "Any custom Python 2.7 libraries can easily be built with VS 2015, even if Python 2.7 is built with VS2008.\r\n\r\nYou need just to set VS90COMNTOOLS to your VS2015 directory.", "Even if that might work (and I have been able to run simple TensorFlow programs using Python 2.7 on Windows), I don't think we'd want to support such a configuration. Some of the issues discussed here are relevant:\r\n\r\nhttp://siomsystems.com/mixing-visual-studio-versions/"]}, {"number": 8408, "title": "Fix build_config.bzl broken by recent push", "body": "", "comments": ["Ah, jart@ is OOO today."]}, {"number": 8407, "title": "Support for 'paged' tensor ", "body": "I'm curious if it is technically feasible to support 'paged' tensors - by this I mean a tensor that seamlessly spills over into main memory when it overflows GPU memory and intelligently pages in and out from main memory. \r\n\r\nMy interest is in processing very large volumes of microscopy data - on the order of a couple thousand voxels to each axis. They don't even come close to fitting into GPU memory, one of the smaller volumes takes a full 250G to run on CPU.  My model in this case is fully spatially convolutional so a divide and conquer strategy into blocks works fine, however there's a lot of wasted computation in recalculating overlapping activations. \r\n\r\nThis particular use case may be somewhat rare, but I think this would be a really really useful thing to have around. Very curious if this is at all feasible and if automatic device allocation/management stuff like this in the works. ", "comments": ["@hawkinsp does TF have support for this already?", "I think what you referred to is much similar to what CUDA refers to as [Unified Memory](https://devblogs.nvidia.com/parallelforall/unified-memory-in-cuda-6/). However, I think TF team decided not to support it. See the discussion #3678 #4779.", "Thanks @byronyi for digging up #3678.  As such, @rueberger please refer to that issue."]}, {"number": 8406, "title": "Android example: symbolic link error when running on Android Studio", "body": "I am trying to run android example app on Android studio. It builds fine with Bazel and Android Studio. But it gives this error when I run the app using Android Studio.\r\n\r\n```\r\nError:Could not list contents of '/home/a/.cache/bazel/_bazel_root/6c98a6c54aeed71fa731445a9f51836b/execroot/tensorflow/external/local_jdk/src.zip'. Couldn't follow symbolic link.\r\n\r\n", "comments": ["What version of bazel are you using? Are there any more logs around the error to give context?\r\n\r\nCan you find the built APK and manually push it to your device and run?", "Bazel version: Build label: 0.4.4\r\nI was able to push the apk by following the readme:\r\n`adb install -r bazel-bin/tensorflow/examples/android/tensorflow_demo.apk`\r\n\r\nBut when I open the project in Android Studio it gives me only that error.\r\nEvent log gives only: \r\n`Error during Sync: Remote object doesn't exist!`", "I just tried running the project in AS on a fresh sync and am unable to reproduce -- I'd double check your Android Studio install (problematic plugins maybe?) and search for possible workaround that people with similarly errors are getting. Possibly a [device issue](https://code.google.com/p/android/issues/detail?id=57024)?\r\n\r\nIn any case I don't think this doesn't seem to be a TensorFlow issue, closing."]}, {"number": 8405, "title": "NADAM Optimizer", "body": "Implements Nadam - a published variant of Adam Optimizer with Nestov momentum\r\n\r\nSame basic algorithm used by the Nadam Optimizer in Keras (https://keras.io/optimizers/#nadam) but implemented from scratch by me for TF's dense / gpu kernels\r\n\r\nCites academic references for source algorithms", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I'm not the right person to review this.  Maybe @lukaszkaiser?", "Please do not add any C++ kernels. We're happy to add NAdam, but please make a python-only implementation and put it in contrib/opt, preferably subclassing AdamOptimizer. For an example, see the LazyAdam implemented in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/opt/python/training/lazy_adam_optimizer.py", "To be clear: the reason for requesting a different implementation is maintenance. We cannot commit to maintaining and testing a separate C++ kernel for every paper that comes out -- that's why modular ops written in python are preferable, except if it's really not practical (e.g., much slower, but I doubt it will be the case here). Putting things in contrib first is necessary because we commit to core APIs so things need to get some use first, before they get moved to contrib.", "Thanks Lukasz! Can do.", "This closes: https://github.com/tensorflow/tensorflow/issues/7715", "@lukaszkaiser I re-implemented Nadam in pure python but it's a lot slower. Wish there was a way to accept the same amount of C++ that's used in all the other optimizers.", "This is interesting. Could we pinpoint why it's slow? Maybe we can change the existing adam C++ kernels to accept some flags to make them more general? If we knew the exact reason, maybe we could make implementation of other optimizers fast as well. For example, there is LazyAdam in tf.contrib.opt, is it slow too?", "Yes, LazyAdam is slow as well.", "We should make both fast, best if it can be done somehow together. Do you have an idea how that could be done?", "A flag seems reasonable for Nadam. MomentumOptimizer also has a `use_nesterov` flag to use Nesterov Momentum.\r\n\r\nBut it would of course be good to know why the python implementation is slower. Can you profile it?", "The performance gap disappears when I enable XLA. I currently profile my code with XLA traces. What's a way I could detect this issue? TFDBG?\r\n", "@louiehelm Both AdamOptimizer and LazyAdamOptimizer implement dense updates using C++ op kernels, and implement sparse updates using standard TensorFlow ops like gather() and scatter_update(). Furthermore, the code for handling dense updates is shared between both optimizers. So if you're seeing a slow-down going from Adam to LazyAdam, I suspect that it's for other reasons.\r\n\r\nWhile LazyAdamOptimizer is several times faster than AdamOptimizer for my use-case (which involves training embedding matrices with sparse updates), I've noticed that my model training jobs get lower throughput with LazyAdam than they do with RMSProp or Adagrad. At peak throughput, my training jobs get around 1600 steps/second with Adagrad, 1550 steps/second with RMSProp, and 1250 steps/second with LazyAdam. Even with lower throughput, LazyAdam consumes more CPU on the parameter servers than Adagrad/RMSProp. It's possible that this is just because Adam is more compute-intensive than the other two algorithms (especially Adagrad). But the fact that Adagrad and RMSProp use C++ op kernels to handle sparse as well as dense updates might also have something to do with it.\r\n\r\nI also suspect that the current implementation of Nadam is likely to get poor throughput for jobs that apply sparse updates to embedding matrices, for exactly the same reason that AdamOptimizer gets poor throughput. This part of the _apply_sparse() function is likely to be a bottleneck:\r\n```\r\nm_t = state_ops.assign(m, m * beta1_t,\r\n                       use_locking=self._use_locking)\r\nm_t = state_ops.scatter_add(m_t, grad.indices, (tp1 / tp) * m_scaled_g_values,\r\n                            use_locking=self._use_locking)\r\n```\r\nThe `assign` op will touch every single row of the embedding matrix, even if not all of the rows appear in the current batch of training examples. It's always possible to add a LazyNadamOptimizer subclass in the future, as we did with LazyAdam, but I wonder if there's a better solution.", "@louiehelm could you sign the CLA? We cannot otherwise proceed.", "@martinwicke are you gonna pull my code? :D\r\n\r\nI can sign a CLA, but umich.edu's Google service is down right now. Don't suppose you go across campus and fix that for me??", "@googlebot I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@louiehelm could you respond to @gmbender concerns?\r\n\r\n@lukaszkaiser other than that, does that look good?", "I'd like to be sure that it implements sparse updates similar to LazyAdam, it's much faster for large vocabulary embedding. Other that it looks fine to me, happy to get this in.", "Patched m_t calculations as @gmbender suggested. Also patched v_t calculation and variable update below it since all 3 areas in LazyAdam are much more careful about not touching the whole matrix.\r\n\r\nShould be ready to go.", "Feels good to me. Someone with more C++ experience could review, or maybe it's fine.", "Thanks! Please address the rest of comments.", "@drpngx Thanks for your feedback.\r\n\r\nAs you see, I implemented all the proper formatting changes you suggested.\r\n\r\nHowever the optimizers in TF (i.e., Adam, Adagrad, RMSProp, etc) all have the exact same \"code-related\" issues you're flagging my code for:\r\n\r\n- argument ordering\r\n- consts best practices\r\n- argument indexing\r\n- code-reuses + quote misuse in validation sections\r\n- and GPU section formatting\r\n\r\nIf I implement any of your remaining suggestions in just my code, Nadam will be the only optimizer following these new conventions. I'd prefer not to make Nadam's conventions deviate from all the other optimizers.\r\n\r\nAre you open to me re-factoring and modifying all the optimizers in Tensorflow to address your complaints with them?\r\n\r\nHow about we build, test, and pull this PR first with all the optimizers having uniform conventions for the moment.\r\n\r\nI'm happy to help you address these remaining issues separately though. I'm also dissatisfied with how this C++ is organized. Let's flag all these deficiencies you've found (with all the C++ optimizers) in a separate Issue. Give me a shout on that thread and I'll come over and start working on a new branch to refactor all of TF's optimizers. Then I can submit a second PR for that.", "OK, sounds good to me.\r\n\r\nJenkins, test this please.", "There are [lint errors](https://ci.tensorflow.org/job/tensorflow-pull-requests-sanity/3883/console):\r\n\r\n```\r\ntensorflow/python/training/adam.py:241: [E1003(bad-super-call), NadamOptimizer.__init__] Bad first argument 'AdamOptimizer' given to super()\r\n\r\ntensorflow/python/training/adam.py:349: [E1003(bad-super-call), RadamOptimizer.__init__] Bad first argument 'AdamOptimizer' given to super()\r\n```", "@drpngx Lint errors fixed", "Jenkins, test this please.", "Looks like there are some build errors.\r\n\r\n```\r\n11:18:41 tensorflow/core/kernels/training_ops.cc: In member function 'void tensorflow::functor::ApplyNadamNonCuda<Device, T>::operator()(const Device&, typename tensorflow::TTypes<T>::Flat, typename tensorflow::TTypes<T>::Flat, typename tensorflow::TTypes<T>::Flat, typename tensorflow::TTypes<T>::ConstScalar, typename tensorflow::TTypes<T>::ConstScalar, typename tensorflow::TTypes<T>::ConstScalar, typename tensorflow::TTypes<T>::ConstScalar, typename tensorflow::TTypes<T>::ConstScalar, typename tensorflow::TTypes<T>::ConstScalar, typename tensorflow::TTypes<T>::ConstFlat)':\r\n11:18:41 tensorflow/core/kernels/training_ops.cc:305:34: error: 'alpha' was not declared in this scope\r\n11:18:41      var.device(d) -= (m * lr() * alpha) / (v.sqrt() + epsilon());\r\n```", "Jenkins, test this please.", "This change seems to have caused api_compatibility_test failure. https://ci.tensorflow.org/view/Tensorflow%20Jenkins%20Monitored%20builds/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=NO_PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/456/consoleFull\r\n\r\n```\r\nERROR:tensorflow:3 differences found between API and golden.\r\nERROR:tensorflow:Issue 1\t: New object tensorflow.train.RadamOptimizer found (added).\r\nERROR:tensorflow:Issue 2\t: Change detected in python object: tensorflow.train.\r\nERROR:tensorflow:Issue 3\t: New object tensorflow.train.NadamOptimizer found (added).\r\nF.\r\n```\r\n\r\ncc @martinwicke, @josh11b on how to proceed.", "We should revert this. I thought this was supposed to go into contrib/opt, which I think is the right spot, unless there's a strong reason to put it directly into core.", "SG. Yes I missed this too \ud83d\ude2f\n\nOn Apr 13, 2017 8:54 AM, \"Martin Wicke\" <notifications@github.com> wrote:\n\nWe should revert this. I thought this was supposed to go into contrib/opt,\nwhich I think is the right spot, unless there's a strong reason to put it\ndirectly into core.\n\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\n<https://github.com/tensorflow/tensorflow/pull/8405#issuecomment-293936122>,\nor mute the thread\n<https://github.com/notifications/unsubscribe-auth/AT_SbdHyX6G5dTsaaQKuahCtAnT9I8iNks5rvkUogaJpZM4Mc-v0>\n.\n"]}, {"number": 8404, "title": "Wrong order of dependencies after running freeze_graph and/or optimize_for_inference", "body": "I haven't found any mention of this anywhere online.\r\nIt makes the graph serializations completely useless for inference.\r\n\r\nSteps to reproduce:\r\n- create graph that contains tf.contrib.layers.batch_norm with tf.bool tensor as is_training argument (to force use of Switch node\r\n- run freeze_graph.freeze_graph and optimize_for_inference_lib.optimize_for_inference\r\n- load resulting graph on Android via TensorFlowInferenceInterface\r\n\r\nWhat happened:\r\nADB Logcat shows error message\r\n`E/TensorFlowInferenceInterface: Failed to load model from 'file:///android_asset/optimized_model.pb': java.io.IOException: Not a valid TensorFlow Graph serialization: Node 'conv1/bn1/BatchNorm/cond/AssignMovingAvg/BatchNorm/moving_mean/sub_1/x': Control dependencies must come after regular dependencies`\r\n\r\nWhy did this happen:\r\nI found out that the order of dependencies was inconsistent after the processing.\r\n\r\nDependencies before processing:\r\n```\r\ninput: \"^conv1/bn1/BatchNorm/cond/AssignMovingAvg/BatchNorm/moving_mean/BatchNorm/BatchNorm/moving_mean\"\r\ninput: \"^conv1/bn1/BatchNorm/cond/AssignMovingAvg/BatchNorm/moving_mean/AssignAdd\"\r\ninput: \"^conv1/bn1/BatchNorm/cond/switch_t\"\r\n```\r\n\r\nDependencies after processing:\r\n```\r\ninput: \"^conv1/bn1/BatchNorm/cond/AssignMovingAvg/BatchNorm/moving_mean/BatchNorm/BatchNorm/moving_mean\"\r\ninput: \"^conv1/bn1/BatchNorm/cond/AssignMovingAvg/BatchNorm/moving_mean/AssignAdd\"\r\ninput: \"conv1/bn1/BatchNorm/cond/Switch:1\"\r\n```\r\n\r\nWhat is wrong:\r\nThe control dependencies (starting with '^') should be after the regular dependencies.\r\n\r\nExpected behaviour:\r\nReordering of dependencies to ensure ordering consistency.\r\n\r\nExpected order of dependencies:\r\n```\r\ninput: \"conv1/bn1/BatchNorm/cond/Switch:1\"\r\ninput: \"^conv1/bn1/BatchNorm/cond/AssignMovingAvg/BatchNorm/moving_mean/BatchNorm/BatchNorm/moving_mean\"\r\ninput: \"^conv1/bn1/BatchNorm/cond/AssignMovingAvg/BatchNorm/moving_mean/AssignAdd\"\r\n```", "comments": ["@petewarden ", "Can you try using the new Graph Transform Tool approach to optimizing for inference?\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms/#optimizing-for-deployment\r\n\r\nI'm hoping to deprecate the old optimize_for_inference Python script soon, so it would be helpful to know if this works better.", "@petewarden \r\nOkay, I have used the new Graph Transform Tool with arguments:\r\n\r\n`--inputs='x' --outputs='y_conv'  --transforms='strip_unused_nodes(type=float, shape=\"1,49,257,1\") remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms'`\r\n\r\nand all of the previous error messages are gone.\r\nThe serialization works just fine now, but when trying to run the model:\r\n\r\n```\r\nInference exception: java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Switch' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n        device='CPU'; T in [DT_FLOAT]\r\n        device='CPU'; T in [DT_INT32]\r\n        device='GPU'; T in [DT_STRING]\r\n        device='GPU'; T in [DT_BOOL]\r\n        device='GPU'; T in [DT_INT32]\r\n        device='GPU'; T in [DT_FLOAT]\r\n        \r\n        [[Node: drop/cond/Switch = Switch[T=DT_BOOL](is_training/_0__cf__0, is_training/_0__cf__0)]]\r\n```", "I had to manually turn Switch ops into Identity ops.\r\nEffectively, `is_training` is now permanently `False`.\r\nSeems to be issue #6124, maybe related to #5919", "@Androbin I have the exact same issue - would you be so kind as to explain how to convert the Switch ops into Identity ops?\r\n\r\nThanks a lot!", "@ronny3050\r\nThis arguably simple python script should work for you:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom google.protobuf import text_format\r\n\r\ngd = tf.GraphDef()\r\n\r\nwith tf.gfile.FastGFile(\"model.pb\", \"r\") as f:\r\n    text_format.Merge(f.read(), gd)\r\n\r\nfor node in gd.node:\r\n    if node.op == \"Switch\":\r\n        node.op = \"Identity\"\r\n        del node.input[1]\r\n\r\ntf.train.write_graph(gd, \".\", \"fixed_model.pb\")\r\n```\r\n\r\nFeel free to ask if there are further issues!", "@Androbin thanks a ton for this code, it's an excellent starting point. Apologies for further questions:\r\n1. del node.input[2] fails in scenarios where I have `node.input = ['phase_train','phase_train']`. Why is this important?\r\n2. If I comment out `del node.input[2]`, your code converts Switch ops to Identity ops and removes the second redundant input. However, I end up with errors such as `Input 0 of node Bottleneck/BatchNorm/cond/Switch_2 was passed bool from Bottleneck/BatchNorm/cond/pred_id:0 incompatible with expected float.`\r\n\r\nAny help is greatly appreciated!", "@Androbin just wanted to thank you again for your help. I got it by freezing the graph without phase_train. \ud83d\udc4d ", "@ronny3050 I am sorry. I got confused because of a special case of mine. I have already updated the script. For reference: [`Switch`](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/switch) and [`Identity`](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/identity)", "I happened to encounter the exact same issue (with DT_BOOL) crashing my app at run time.\r\nBeen trying to work around that for 2 days now (trying to remove keras_learning_phase Switch node branch)... this is frustrating", "@Lakedaemon Do any of the provided solutions work for you?", "I'm investigating. I don't have a definitive answer yet.\r\n\r\nI have setup tensorflow through the virutal env method and python, so some solutions are harder to try than others (I had to get a look into the optimize_for_inference_lib.py file to find how to use it with python as I couldn't/didn't know how to use the optimize_for_inference.py app...I don't have any binary for that)\r\n\r\nI have tried some variants of your  solution that modifies the graphdef nodes  and then I applied optimize_for_inference but it corrupts it (maybee I'm doing something wrong).\r\n\r\nI'm going to pruduce an image to visualize my graph next, to see what nodes I should prune.\r\nI'm also going to try to use the transform graph api (when I find out how, I'll probably have to build tensorflow from source).\r\n\r\nI'll tell you if I succeed and ask for help if I really can't make it work\r\n(I can also build the android lib with more ops but I would rather avoid that)", "I tried tinkering with this quite a bit and made progress until I had to account for a ton of edge cases. At the end, I just retrained the network with the Boolean.", "Where is the tensorflow/tensorflow/tools/graph_transforms/ stuff in the virtual env installation ? \r\nIs it shipped with the tenserflow release ? or not ?\r\n\r\nI searched on my ubuntu, but couldn't find anything\r\n(and I see in the source code for tensorflow 1.1 that there should be a python wrapper for the c code)", "I tried replacing a PlaceHolder node holding a boolean, namely\r\n`node {\r\n  name: \"dropout_1/keras_learning_phase\"\r\n  op: \"Placeholder\"\r\n  attr {\r\n    key: \"dtype\"\r\n    value {\r\n      type: DT_BOOL\r\n    }\r\n  }\r\n  attr {\r\n    key: \"shape\"\r\n    value {\r\n      shape {\r\n      }\r\n    }\r\n  }\r\n}`\r\n\r\nwith this node (not sure if it is correct)\r\n`node {\r\n  name: \"dropout_1/keras_learning_phase\"\r\n  op: \"Const\"\r\n  attr {\r\n    key: \"dtype\"\r\n    value {\r\n      type: DT_BOOL\r\n    }\r\n  }\r\n  attr {\r\n    key: \"value\"\r\n    value {\r\n      tensor {\r\n        dtype: DT_BOOL\r\n        tensor_shape {\r\n        }\r\n        bool_val: false\r\n      }\r\n    }\r\n  }\r\n}`\r\n\r\n \r\nand then running it through\r\n\r\n`gd = tf.GraphDef()\r\n\r\nfrom google.protobuf import text_format\r\n      with tf.gfile.FastGFile(self.exportPath+self.modelName+\"Constant.pbtxt\", \"r\") as f:\r\n        text_format.Merge(f.read(), gd)\r\n\r\n      tf.train.write_graph(gd,  self.exportPath,  self.modelName + \"Test.pbtxt\") \r\n      optimized_graph_def = optimize_for_inference(input_graph_def= gd,\r\n                         input_node_names=\"conv2d_1_input\".split(\",\"),# \\\r\n                        output_node_names=\"activation_6/Softmax\".split(\",\"),\r\n                            placeholder_type_enum=dtypes.float32.as_datatype_enum)\r\n      optimizedGraphPath = self.exportPath +  self.modelName + \"ConstantOptimized.pbtxt\"\r\n      tf.train.write_graph(optimized_graph_def,  self.exportPath,  self.modelName + \"ConstantOptimized.pbtxt\") \r\n      self.freeze(self.modelName + \"ConstantOptimized\", self.modelName + \"-\"+str(self.step))`\r\n\r\nBut, when freezing, I ended up with \r\nValueError: graph_def is invalid at node u'dropout_1/cond/mul/y': More inputs specified ('dropout_1/cond/Switch:1') than the op expects..\r\n\r\n\r\nwas my attempt correct ? \r\n\r\nNext I'm going to try to build tenserflow with basel...I really need those graph transform tools", "ok, built tensorflow from sources (30 minutes O.O)\r\nretrained my model (took mostly as long for 1 epoch as the prebuilt tensorflow binary, despite the added cpu instructions ? I didn't add any optimisation flag (there is no documentation for that anyway))\r\nbuilt summarize_graph (it takes sooooooooooooooooooooo long to compile C++ :/, like 8 minutes for a simple utility)\r\n\r\nAlso, tried this command `bazel build tensorflow/tools/graph_transforms:transform_graph\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=tensorflow_inception_graph.pb \\\r\n--out_graph=optimized_inception_graph.pb \\\r\n--inputs='Mul' \\\r\n--outputs='softmax' \\\r\n--transforms='\r\n  strip_unused_nodes(type=float, shape=\"1,299,299,3\")\r\n  remove_nodes(op=Identity, op=CheckNumerics)\r\n  fold_constants(ignore_errors=true)\r\n  fold_batch_norms\r\n  fold_old_batch_norms'`\r\n\r\nloaded the .pb file, exported as .pbtext to see the differences (went from 370kb to 29kb, nice).\r\nThere should have been quite a lot of optimizations in there...\r\n\r\nYet, the DT_BOOL & keras leraning-phase stuff is still in there (obviously, because the keras_learning_phase placeholder hasn't been replaced by a constant op)\r\n\r\nAnd... looking at the doc for the transform_graph tools, I don't see any transformation that allows one to replace a placeholder op (with a single bool) by a const op... :/\r\nsigh.... Am I supposed to write a custom transform function for that ?\r\n\r\nAnd if I do, will the switch op disappear with an opmtimising phase ?\r\n\r\n/me begins to think that he'll throw the towel and just build tensorflow for android with CPU:BOOL kernel\r\n(why is there support for GPU:BOOL and not for CPU:BOOL anyway ?)", "manually replaced the placeholder with a constant op (potentially in a bad way), rerun through transform_graph -> no change to the Switch ops that are fed with a constant false.\r\n\r\nThat's it... let's build CPU:DT_BOOL for android.. :/ ", "@Lakedaemon Totally agree on Android lacking default support for important -.- stuff #10254", "Well, tried compiling the android demo example with basel (on a fresh git clone of the repo, after having nuked the .cache for bazel on my box) : fail\r\n\r\nERROR: /home/lakedaemon/.cache/bazel/_bazel_lakedaemon/6532aa38fb8c8ce96c07312dcb04db38/external/protobuf/BUILD:113:1: C++ compilation of rule '@protobuf//:protobuf' failed: false failed: error executing command \r\n  (cd /home/lakedaemon/.cache/bazel/_bazel_lakedaemon/6532aa38fb8c8ce96c07312dcb04db38/execroot/tensorflow && \\\r\n  exec env - \\\r\n    PWD=/proc/self/cwd \\\r\n  /bin/false -MD -MF bazel-out/stub_armeabi-v7a-opt/bin/external/protobuf/_objs/protobuf/external/protobuf/src/google/protobuf/util/internal/type_info_test_helper.pic.d '-frandom-seed=bazel-out/stub_armeabi-v7a-opt/bin/external/protobuf/_objs/protobuf/external/protobuf/src/google/protobuf/util/internal/type_info_test_helper.pic.o' -fPIC -iquote external/protobuf -iquote bazel-out/stub_armeabi-v7a-opt/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/stub_armeabi-v7a-opt/genfiles/external/bazel_tools -isystem external/protobuf/src -isystem bazel-out/stub_armeabi-v7a-opt/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -DHAVE_PTHREAD -Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare -Wno-unused-function -c external/protobuf/src/google/protobuf/util/internal/type_info_test_helper.cc -o bazel-out/stub_armeabi-v7a-opt/bin/external/protobuf/_objs/protobuf/external/protobuf/src/google/protobuf/util/internal/type_info_test_helper.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nTarget //tensorflow/examples/android:tensorflow_native_libs failed to build\r\n____Elapsed time: 67.998s, Critical Path: 7.19s\r\n FAILED\r\n\r\nSo, I managed to build tenserflow natively for my linux box with basel.... but it won't crosse compile for the android demo app with the 'basel' build tool....  :/", "mmh managed to build the android app with make  and found why it wouldn't build with basel (used a wrong git clone command missing recurse subprojects).\r\n\r\nWill now try to build the android app with Bazel and then with custom ops (selective) and kernels", "I managed to crosscompile tensorFlow with Basel and to build an android app with the native library. \r\nOf course, the app crashed at runtime as it was still missing types (DT_BOOL) and probably ops too.\r\n\r\nToday, I have been trying to cross-compile libtensorflow_inference.so  for android with more types/ops without success : \r\nI created a \"ops_to_register.h\" header file that I put in tenserflow/core/framework with the print_selective_registration_header.py utility\r\n\r\nBut I could not find the bazel command I should use or the files I should modify to build  libtensorflow_inference.so with selective_registration/ SUPPORT_SELECTIVE_REGISTRATION or with __ANDROID_FULL_TYPE__\r\n\r\nThe documentations is severely lacking concerning this. Lost one more day looking for a solution (losing 10/30 minutes each time I was trying a bazel build)\r\n\r\nCould someone please tell me how to use selective_registration, pretty please ? \r\nsee issue #10299", "@Lakedaemon The following should work:\r\n`bazel build -c opt --copt=\"-DSELECTIVE_REGISTRATION\" //tensorflow/contrib/android:libtensorflow_inference.so --config=android_arm`", "I have tried a few commands : \r\n1) bazel build -c opt --copt=\"-DSELECTIVE_REGISTRATION\" tensorflow/contrib/android:libtensorflow_inference.so --config=android_arm\r\n\r\n-> I get a 37 MB libtensorflow_inference. \r\n\r\nWhen compiling in an android apk, I get \r\n:app:transformNativeLibsWithStripDebugSymbolForDebug/home/lakedaemon/Android/Sdk/ndk-bundle/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-strip:/home/lakedaemon/IdeaProjects/Handwriter/app/build/intermediates/transforms/mergeJniLibs/debug/folders/2000/1f/main/lib/armeabi-v7a/libtensorflow_inference.so: File format not recognized\r\n\r\nSo, the file format might be wrong\r\n\r\nWhen trying my app on an arm device, I get \r\ndlopen(\"/data/app-lib/org.lakedaemon.handwriter-5/libtensorflow_inference.so\") failed: dlopen failed: \"/data/app-lib/org.lakedaemon.handwriter-5/libtensorflow_inference.so\" not 32-bit: 2\r\nand then a java.lang.RuntimeException: Native TF methods not found; check that the correct native libraries are present in the APK\r\n\r\n2) bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --config=android_arm\r\n\r\n-> I get a 91.2 MB libtensorflow_inference. Which means that the \"-DSELECTIVE_REGISTRATION\" looks like it is working.\r\n\r\nBut I get the same errors on android...\r\n\r\ninvestigating further", "3) bazel build -c opt --copt=\"-DSELECTIVE_REGISTRATION\" //tensorflow/contrib/android:libtensorflow_inference.so    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a\r\n\r\n-> I get a 3.7 MB libtensorflow_inference.\r\n\r\nno android error at compile time/when loading the .so code in Java but the infamous\r\njava.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Switch' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n                                                                             device='CPU'; T in [DT_FLOAT]\r\n                                                                             device='CPU'; T in [DT_INT32]\r\n                                                                             device='GPU'; T in [DT_STRING]\r\n                                                                             device='GPU'; T in [DT_BOOL]\r\n                                                                             device='GPU'; T in [DT_INT32]\r\n                                                                             device='GPU'; T in [DT_FLOAT]\r\n                                                                           \r\n                                                                           \t [[Node: dropout_1/cond/Switch = Switch[T=DT_BOOL](dropout_1/keras_learning_phase, dropout_1/keras_learning_phase)]]\r\n\r\nso, either selective_registration doesn't work or \r\nit doesn't load the DT_BOOL type which might be possible since register.types.h uses this condition\r\n\r\n#if !defined(IS_MOBILE_PLATFORM) || defined(SUPPORT_SELECTIVE_REGISTRATION)\r\n\r\nand I couldn't find any place where SUPPORT_SELECTIVE_REGISTRATION was defined\r\nso, next I'm trying to define it also in the copt arguments\r\n", "and the winner is.....\r\nbazel build -c opt --copt=\"-DSELECTIVE_REGISTRATION\" --copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\" //tensorflow/contrib/android:libtensorflow_inference.so    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a\r\n\r\nI get a 3.9 MB libtensorflow_inference.so that doesn't crash the app because of tensorflow code \r\n(well, it crashes later on, but because of a bug in MY code, which is much less frustrating as I'm going to be able to fix it quick)\r\n\r\nSo, either the tenserflow developers intended to have 2 different flags : \r\nSELECTIVE_REGISTRATION for ops and SUPPORT_SELECTIVE_REGISTRATION for types and this is not documented either it is a bug in the tenserflow code\r\n\r\nPlease fix ! @petewarden @andrewharp", "@Lakedaemon Look what I found in [tensorflow/core/BUILD#L1004](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/BUILD#L1004)\r\nOne more question: What is `proto_rtti` about?", "Seems that SUPPORT_SELECTIVE_REGISTRATION is used in core and all BUILD files while SELECTIVE_REGISTRATION appears further away from core (python tools). These may be outdated references:\r\n[tensorflow/python/tools/selective_registration_header_lib.py](../blob/master/tensorflow/python/tools/selective_registration_header_lib.py)\r\n[tensorflow/python/tools/print_selective_registration_header.py](../blob/master/tensorflow/python/tools/print_selective_registration_header.py)\r\n[tensorflow/core/framework/selective_registration.h](../blob/master/tensorflow/core/framework/selective_registration.h)\r\n", "yes, I read that stuff in tensorflow/core/BUILD#L1004 while greping for SELECTIVE_REGISTRATION but I could not find a way to build lib_tensorflow_inference.so with this target. \r\n\r\nBazel is really new to me (and to any non-googler I guess), learned a lot of things about it while trying to find a solution to my issue this past feaw days (the BUILD language, the if_android \"pythonic\"-functions/macro, ..)\r\n\r\nfound about android_arm while greping for it, but surprisingly, it doesn't produce valid android lib for me. probably because it doesn't set --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\n\r\na complete guess : proto_rtti might be a version of the protocol buffer lib without support for run type information", "@Lakedaemon I share your disorientation.\r\nInterestingly, wherever `--config=android_arm` or its expansion is used, it adds `--host_crosstool_top=@bazel_tools//tools/cpp:toolchain` to it.\r\nMaybe @andrewharp can help clarifying things here.", "Hey guys! I've been having the same problem trying to make this work on android and I have used some of the solutions from above but none seem to work...here are some of the errors I get back from the various methods of trying to solve this problem...Any Help would be appreciated!\r\n\r\nThe models work when I run it on python but doesn't when I run them on android.\r\n\r\nIm using the SSD_Mobilenet model from tensorflow object detection api.\r\n\r\nThis is from the optimize for inference function:\r\nNodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: Preprocessor/map/while/add/y = Constdtype=DT_INT32, value=Tensor<type: int32 shape: [] values: 1>\r\n\r\nThis is from renaming Switch to Identify:\r\nNot a valid TensorFlow Graph serialization: Node 'Preprocessor/map/while/add/y': Connecting to invalid output 1 of source node Preprocessor/map/while/Switch which has 1 outputs\r\n\r\nThis last one loads the model but crashes when processing the inputs:\r\njava.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Switch' with these attrs. Registered devices: [CPU], Registered kernels:\r\ndevice='GPU'; T in [DT_STRING]\r\ndevice='GPU'; T in [DT_BOOL]\r\ndevice='GPU'; T in [DT_INT32]\r\ndevice='GPU'; T in [DT_FLOAT]\r\ndevice='CPU'; T in [DT_FLOAT]\r\ndevice='CPU'; T in [DT_INT32]\r\n\r\n                                                              \t [[Node: Postprocessor/BatchMultiClassNonMaxSuppression/PadOrClipBoxList/cond/Switch = Switch[T=DT_BOOL](Postprocessor/BatchMultiClassNonMaxSuppression/PadOrClipBoxList/Greater, Postprocessor/BatchMultiClassNonMaxSuppression/PadOrClipBoxList/Greater)]]\r\nAny ideas guys? I've also tried to recompile tensorflow with the changes from [https://stackoverflow.com/questions/40855271/no-opkernel-was-registered-to-support-op-switch-with-these-attrs-on-ios/43627334#43627334](url) but bazel crashes while compiling :(", "If you are using keras, don't rename node to identity to strip switches from learning to inference.\r\nIn keras, you train your model, you save weigths and when you want to export your frozen model to tenserflow, you : \r\n1) launch a new python script FROM SCRATCH that\r\n\r\n `K.set_learning_phase(0)\r\n      inputShape = (32, 32, 1)\r\n      model = self.buildModel(inputShape, self.nbClasses)# I'm building the model from scratch here\r\n      model.load_weights('weights-improvement-12-0.96.hdf5')\r\n      saver = tf.train.Saver()\r\n      saver.save(K.get_session(), self.exportPath + self.modelName, global_step=self.step)\r\n      self.frozenOptimizedQuantizied()# I optimize things here\r\n`\r\n\r\n2) load your weigths (with learning phase set to false) -> the useless switches will be stripped\r\n3) then you save your model to tenserflow\r\n4) after that, you apply a python method to freeze your graph\r\n5) afterwards, you use the transfrom tools (you need bazel) to optimize your model for android\r\n6) then it works really great\r\nI have a 5MB model that classify 3000 japanese characters on android blazingly fast : \r\nThe guys who developped NN and tensorflow are geniuses. This is one of the best tools ever...\r\n\r\n\r\n", "I am trying to do something exactly similar. I was able to save the model to a graph.pb file and use it in android studio. \r\n\r\nThe nodes are as such:\r\n[print(n.name) for n in tf.get_default_graph().as_graph_def().node]\r\n\r\n_>>>[print(n.name) for n in tf.get_default_graph().as_graph_def().node]\r\nkeras_learning_phase\r\nSigmoid\r\n[None, None]_\r\n\r\nI am trying to read the input and output node but it doesnt recognize keras_learning_phase. I tried using the summarize_graph to read the summary of the nodes but it throws an error.\r\n\r\n(tensorflow) azainab:tensorflow-master_2 azain$ _bazel-bin/tensorflow/tools/graph_transforms/summarize_graph --in_graph=graph.pb\r\n2017-10-16 16:07:48.147635: E tensorflow/tools/graph_transforms/summarize_graph_main.cc:283] Loading graph 'graph.pb' failed with graph.pb\r\n\t (for file graph.pb)\r\n2017-10-16 16:07:48.148816: E tensorflow/tools/graph_transforms/summarize_graph_main.cc:285] usage: bazel-bin/tensorflow/tools/graph_transforms/summarize_graph\r\nFlags:\r\n\t--in_graph=\"\"                    \tstring\tinput graph file name\r\n\t--print_structure=false          \tbool\twhether to print the network connections of the graph_\r\n\r\n\r\nCan somebody help please\r\n\r\n@Lakedaemon  @andrewharp ", "In my experience, if you still have keras_learning_phase node in your model after you have trained it and frozen/optimized it in python for use in java/android, you have been doing something wrong when using the keras api.\r\n\r\nYou have to reload your model after training and before optimizing in a completely new python process (i;e. you launch your python file on another line of your console)\r\n\r\n\r\n", "Also, there aren't many nodes in your model. This feel weirds (my model had like 60+ nodes before optimizing and maybee half after optimizing, but I still had like 20 or 30 nodes....)"]}, {"number": 8403, "title": "C++ libtensorflow.so segfault on exit on Ubuntu 14.04 (no segfault on OSX)", "body": "### Summary\r\nWhen using libtensorflow.so in a C++ app a segfault occurs when exiting the app on Ubuntu 14.04 while on OSX Sierra this does not get thrown (both using Protobuf 3.2.0). The actual execution of the app is fine, successfully running data through the graph. Only the mentioned segfault when exiting. From the stacktrace you can see it occurs while a protobuf hastable is cleared by libtensorflow.so.\r\n\r\n### Environment info\r\nUbuntu 14.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\nls -l /usr/local/cuda/lib64/libcud*  \r\n-rw-r--r-- 1 root    546K Sep 15 00:02 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root      16 Sep 15 00:05 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root      19 Sep 15 00:05 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rw-r--r-- 1 root    406K Sep 15 00:02 /usr/local/cuda/lib64/libcudart.so.8.0.44\r\n-rw-r--r-- 1 root    757K Sep 15 00:02 /usr/local/cuda/lib64/libcudart_static.a\r\nlrwxrwxrwx 1 mtanner   13 Jul 27  2016 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5*\r\nlrwxrwxrwx 1 mtanner   17 Jul 27  2016 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5*\r\n-rwxrwxr-x 1 mtanner  76M Jul 27  2016 /usr/local/cuda/lib64/libcudnn.so.5.1.5*\r\n-rw-rw-r-- 1 mtanner  67M Jul 27  2016 /usr/local/cuda/lib64/libcudnn_static.a\r\n\r\n### Installed from source\r\n1. The commit hash - `e895d5ca395c2362df4f5c8f08b68501b41f8a98` (from the r1.0 branch)\r\n2. The output of `bazel version` - Build label: 0.4.4 Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar Build time: Wed Feb 1 18:54:21 2017 (1485975261) Build timestamp: 1485975261 Build timestamp as int: 1485975261\r\n3. Workspace.bzl simply updated to point to a fixed protobuf 3.2.0 to match our system version\r\n```\r\n...\r\n  native.http_archive(\r\n      name = \"protobuf\",\r\n      urls = [\r\n          \"https://github.com/google/protobuf/archive/v3.2.0.tar.gz\",\r\n      ],\r\n      sha256 = \"2a25c2b71c707c5552ec9afdfb22532a93a339e1ca5d38f163fe4107af08c54c\",\r\n      strip_prefix = \"protobuf-3.2.0\",\r\n  )\r\n...\r\n```\r\n\r\n### Logs or other output that would be helpful\r\n\r\n#### protoc --version\r\n```\r\nlibprotoc 3.2.0\r\n```\r\n\r\n#### gdb stacktrace (at the end of app execution)\r\n```\r\n[Thread 0x7fffdcdc0700 (LWP 2193) exited]\r\nProgram received signal SIGSEGV, Segmentation fault.\r\n0x00007fffefed8ea3 in std::_Hashtable<std::string, std::pair<std::string const, google::protobuf::FieldDescriptorProto_Type>, std::allocator<std::pair<std::string const, google::protobuf::FieldDescriptorProto_Type> >, std::__detail::_Select1st, std::equal_to<std::string>, google::protobuf::hash<std::string>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::clear() () from /home/dbarnes/code/third_party/tensorflow/bazel-bin/tensorflow/libtensorflow.so\r\n(gdb)\r\n```\r\n\r\n#### ldd libtensorflow.so\r\n```\r\nldd /home/dbarnes/code/third_party/tensorflow/bazel-bin/tensorflow/libtensorflow.so\r\n        linux-vdso.so.1 =>  (0x00007ffe19bf6000)\r\n        libcudart.so.8.0 => /home/dbarnes/code/third_party/tensorflow/bazel-bin/tensorflow/../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Slib/libcudart.so.8.0 (0x00007f92440ef000)\r\n        libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f9243eeb000)\r\n        libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f9243be5000)\r\n        libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f92439c7000)\r\n        libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f92436c3000)\r\n        libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f92434ad000)\r\n        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f92430e8000)\r\n        /lib64/ld-linux-x86-64.so.2 (0x00007f924ef6a000)\r\n        librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f9242ee0000)\r\n```\r\n\r\nI tried to make this brief but if any other info is helpful let me know.\r\n\r\nCheers\r\n", "comments": ["How are you linking your app? Are you linking against tensorflow for protobufs or are you using your own?\r\n\r\nThanks for the description! Please also include the ldd for your app.", "From my understanding, Tensorflow is built against the protobuf bazel pulls down (3.2.0 after the modifications above) which matches the system protobuf version we have installed (shown by protoc --version = `libprotoc 3.2.0`). Then at run time the app will be linking against our system protobuf.\r\n\r\n\r\n### ldd of our app\r\n```\r\n  linux-vdso.so.1 =>  (0x00007ffd9e2e9000)\r\n  libboost_unit_test_framework.so.1.60.0 => /usr/local/lib/libboost_unit_test_framework.so.1.60.0 (0x00007fc89f4da000)\r\n  libglog.so.0 => /usr/local/lib/libglog.so.0 (0x00007fc89f074000)\r\n  libgflags.so.2 => /usr/local/lib/libgflags.so.2 (0x00007fc89ee58000)\r\n  libopencv_videostab.so.2.4 => /usr/local/lib/libopencv_videostab.so.2.4 (0x00007fc89ec25000)\r\n  libopencv_superres.so.2.4 => /usr/local/lib/libopencv_superres.so.2.4 (0x00007fc89e9e4000)\r\n  libopencv_stitching.so.2.4 => /usr/local/lib/libopencv_stitching.so.2.4 (0x00007fc89e764000)\r\n  libopencv_contrib.so.2.4 => /usr/local/lib/libopencv_contrib.so.2.4 (0x00007fc89e467000)\r\n  libtensorflow.so => /home/dbarnes/code/third_party/tensorflow/bazel-bin/tensorflow/libtensorflow.so (0x00007fc893852000)\r\n  libprotobuf.so.12 => /usr/local/lib/libprotobuf.so.12 (0x00007fc8933f8000)\r\n  libpng12.so.0 => /lib/x86_64-linux-gnu/libpng12.so.0 (0x00007fc8931d2000)\r\n  libz.so.1 => /lib/x86_64-linux-gnu/libz.so.1 (0x00007fc892fb9000)\r\n  libPocoFoundation.so.11 => /usr/local/lib/libPocoFoundation.so.11 (0x00007fc892c3c000)\r\n  libboost_filesystem.so.1.60.0 => /usr/local/lib/libboost_filesystem.so.1.60.0 (0x00007fc8927ea000)\r\n  libboost_system.so.1.60.0 => /usr/local/lib/libboost_system.so.1.60.0 (0x00007fc8925e6000)\r\n  libboost_chrono.so.1.60.0 => /usr/local/lib/libboost_chrono.so.1.60.0 (0x00007fc8921bd000)\r\n  libboost_iostreams.so.1.60.0 => /usr/local/lib/libboost_iostreams.so.1.60.0 (0x00007fc891d38000)\r\n  libboost_regex.so.1.60.0 => /usr/local/lib/libboost_regex.so.1.60.0 (0x00007fc891a46000)\r\n  libPocoUtil.so.11 => /usr/local/lib/libPocoUtil.so.11 (0x00007fc8917ee000)\r\n  libboost_thread.so.1.60.0 => /usr/local/lib/libboost_thread.so.1.60.0 (0x00007fc8913c0000)\r\n  libboost_date_time.so.1.60.0 => /usr/local/lib/libboost_date_time.so.1.60.0 (0x00007fc8911af000)\r\n  libboost_atomic.so.1.60.0 => /usr/local/lib/libboost_atomic.so.1.60.0 (0x00007fc890fad000)\r\n  libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fc890d8f000)\r\n  libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fc890316000)\r\n  libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fc890010000)\r\n  librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007fc88fe08000)\r\n  libopencv_nonfree.so.2.4 => /usr/local/lib/libopencv_nonfree.so.2.4 (0x00007fc88fbd0000)\r\n  libopencv_ocl.so.2.4 => /usr/local/lib/libopencv_ocl.so.2.4 (0x00007fc88f7d8000)\r\n  libopencv_gpu.so.2.4 => /usr/local/lib/libopencv_gpu.so.2.4 (0x00007fc88f5b8000)\r\n  libopencv_photo.so.2.4 => /usr/local/lib/libopencv_photo.so.2.4 (0x00007fc88f397000)\r\n  libopencv_objdetect.so.2.4 => /usr/local/lib/libopencv_objdetect.so.2.4 (0x00007fc88f115000)\r\n  libopencv_legacy.so.2.4 => /usr/local/lib/libopencv_legacy.so.2.4 (0x00007fc88ee0d000)\r\n  libopencv_video.so.2.4 => /usr/local/lib/libopencv_video.so.2.4 (0x00007fc88ebbb000)\r\n  libopencv_ml.so.2.4 => /usr/local/lib/libopencv_ml.so.2.4 (0x00007fc88e941000)\r\n  libopencv_calib3d.so.2.4 => /usr/local/lib/libopencv_calib3d.so.2.4 (0x00007fc88e698000)\r\n  libopencv_features2d.so.2.4 => /usr/local/lib/libopencv_features2d.so.2.4 (0x00007fc88e3e3000)\r\n  libopencv_highgui.so.2.4 => /usr/local/lib/libopencv_highgui.so.2.4 (0x00007fc88dff8000)\r\n  libopencv_imgproc.so.2.4 => /usr/local/lib/libopencv_imgproc.so.2.4 (0x00007fc88db39000)\r\n  libopencv_flann.so.2.4 => /usr/local/lib/libopencv_flann.so.2.4 (0x00007fc88d8c9000)\r\n  libopencv_core.so.2.4 => /usr/local/lib/libopencv_core.so.2.4 (0x00007fc88d45b000)\r\n  libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007fc88c38b000)\r\n  libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fc88c175000)\r\n  libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fc88bdb0000)\r\n  libcudart.so.8.0 => /home/dbarnes/code/third_party/tensorflow/bazel-bin/tensorflow/../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Slib/libcudart.so.8.0 (0x00007fc88bb4a000)\r\n  /lib64/ld-linux-x86-64.so.2 (0x00007fc89f78e000)\r\n  libbz2.so.1.0 => /lib/x86_64-linux-gnu/libbz2.so.1.0 (0x00007fc88b93a000)\r\n  libPocoXML.so.11 => /usr/local/lib/libPocoXML.so.11 (0x00007fc88b6ac000)\r\n  libjpeg.so.8 => /usr/lib/x86_64-linux-gnu/libjpeg.so.8 (0x00007fc88b457000)\r\n  libgtk-x11-2.0.so.0 => /usr/lib/x86_64-linux-gnu/libgtk-x11-2.0.so.0 (0x00007fc88ae1a000)\r\n  libgdk-x11-2.0.so.0 => /usr/lib/x86_64-linux-gnu/libgdk-x11-2.0.so.0 (0x00007fc88ab67000)\r\n  libgobject-2.0.so.0 => /usr/lib/x86_64-linux-gnu/libgobject-2.0.so.0 (0x00007fc88a916000)\r\n  libglib-2.0.so.0 => /lib/x86_64-linux-gnu/libglib-2.0.so.0 (0x00007fc88a60e000)\r\n  libdc1394.so.22 => /usr/lib/x86_64-linux-gnu/libdc1394.so.22 (0x00007fc88a39a000)\r\n  libgmodule-2.0.so.0 => /usr/lib/x86_64-linux-gnu/libgmodule-2.0.so.0 (0x00007fc88a196000)\r\n  libpangocairo-1.0.so.0 => /usr/lib/x86_64-linux-gnu/libpangocairo-1.0.so.0 (0x00007fc889f89000)\r\n  libX11.so.6 => /usr/lib/x86_64-linux-gnu/libX11.so.6 (0x00007fc889c54000)\r\n  libXfixes.so.3 => /usr/lib/x86_64-linux-gnu/libXfixes.so.3 (0x00007fc889a4e000)\r\n  libatk-1.0.so.0 => /usr/lib/x86_64-linux-gnu/libatk-1.0.so.0 (0x00007fc88982c000)\r\n  libcairo.so.2 => /usr/lib/x86_64-linux-gnu/libcairo.so.2 (0x00007fc889521000)\r\n  libgdk_pixbuf-2.0.so.0 => /usr/lib/x86_64-linux-gnu/libgdk_pixbuf-2.0.so.0 (0x00007fc889300000)\r\n  libgio-2.0.so.0 => /usr/lib/x86_64-linux-gnu/libgio-2.0.so.0 (0x00007fc888f8d000)\r\n  libpangoft2-1.0.so.0 => /usr/lib/x86_64-linux-gnu/libpangoft2-1.0.so.0 (0x00007fc888d78000)\r\n  libpango-1.0.so.0 => /usr/lib/x86_64-linux-gnu/libpango-1.0.so.0 (0x00007fc888b2b000)\r\n  libfontconfig.so.1 => /usr/lib/x86_64-linux-gnu/libfontconfig.so.1 (0x00007fc8888ef000)\r\n  libXrender.so.1 => /usr/lib/x86_64-linux-gnu/libXrender.so.1 (0x00007fc8886e5000)\r\n  libXinerama.so.1 => /usr/lib/x86_64-linux-gnu/libXinerama.so.1 (0x00007fc8884e2000)\r\n  libXi.so.6 => /usr/lib/x86_64-linux-gnu/libXi.so.6 (0x00007fc8882d2000)\r\n  libXrandr.so.2 => /usr/lib/x86_64-linux-gnu/libXrandr.so.2 (0x00007fc8880c8000)\r\n  libXcursor.so.1 => /usr/lib/x86_64-linux-gnu/libXcursor.so.1 (0x00007fc887ebe000)\r\n  libXcomposite.so.1 => /usr/lib/x86_64-linux-gnu/libXcomposite.so.1 (0x00007fc887cbb000)\r\n  libXdamage.so.1 => /usr/lib/x86_64-linux-gnu/libXdamage.so.1 (0x00007fc887ab8000)\r\n  libXext.so.6 => /usr/lib/x86_64-linux-gnu/libXext.so.6 (0x00007fc8878a6000)\r\n  libffi.so.6 => /usr/lib/x86_64-linux-gnu/libffi.so.6 (0x00007fc88769e000)\r\n  libpcre.so.3 => /lib/x86_64-linux-gnu/libpcre.so.3 (0x00007fc887460000)\r\n  libraw1394.so.11 => /usr/lib/x86_64-linux-gnu/libraw1394.so.11 (0x00007fc887252000)\r\n  libusb-1.0.so.0 => /lib/x86_64-linux-gnu/libusb-1.0.so.0 (0x00007fc88703b000)\r\n  libfreetype.so.6 => /usr/lib/x86_64-linux-gnu/libfreetype.so.6 (0x00007fc886d98000)\r\n  libxcb.so.1 => /usr/lib/x86_64-linux-gnu/libxcb.so.1 (0x00007fc886b79000)\r\n  libpixman-1.so.0 => /usr/lib/x86_64-linux-gnu/libpixman-1.so.0 (0x00007fc8868d0000)\r\n  libxcb-shm.so.0 => /usr/lib/x86_64-linux-gnu/libxcb-shm.so.0 (0x00007fc8866cd000)\r\n  libxcb-render.so.0 => /usr/lib/x86_64-linux-gnu/libxcb-render.so.0 (0x00007fc8864c4000)\r\n  libselinux.so.1 => /lib/x86_64-linux-gnu/libselinux.so.1 (0x00007fc8862a1000)\r\n  libresolv.so.2 => /lib/x86_64-linux-gnu/libresolv.so.2 (0x00007fc886086000)\r\n  libharfbuzz.so.0 => /usr/lib/x86_64-linux-gnu/libharfbuzz.so.0 (0x00007fc885e31000)\r\n  libthai.so.0 => /usr/lib/x86_64-linux-gnu/libthai.so.0 (0x00007fc885c28000)\r\n  libexpat.so.1 => /lib/x86_64-linux-gnu/libexpat.so.1 (0x00007fc8859fe000)\r\n  libudev.so.1 => /lib/x86_64-linux-gnu/libudev.so.1 (0x00007fc8857ed000)\r\n  libXau.so.6 => /usr/lib/x86_64-linux-gnu/libXau.so.6 (0x00007fc8855e9000)\r\n  libXdmcp.so.6 => /usr/lib/x86_64-linux-gnu/libXdmcp.so.6 (0x00007fc8853e3000)\r\n  libgraphite2.so.3 => /usr/lib/x86_64-linux-gnu/libgraphite2.so.3 (0x00007fc8851c7000)\r\n  libdatrie.so.1 => /usr/lib/x86_64-linux-gnu/libdatrie.so.1 (0x00007fc884fc0000)\r\n  libcgmanager.so.0 => /lib/x86_64-linux-gnu/libcgmanager.so.0 (0x00007fc884da5000)\r\n  libnih.so.1 => /lib/x86_64-linux-gnu/libnih.so.1 (0x00007fc884b8d000)\r\n  libnih-dbus.so.1 => /lib/x86_64-linux-gnu/libnih-dbus.so.1 (0x00007fc884983000)\r\n  libdbus-1.so.3 => /lib/x86_64-linux-gnu/libdbus-1.so.3 (0x00007fc88473e000)\r\n```", "This is essentially a variation of https://github.com/tensorflow/tensorflow/issues/8394.\r\n\r\nWhat happens is that you have this\r\n```\r\n  libprotobuf.so.12 => /usr/local/lib/libprotobuf.so.12 (0x00007fc8933f8000)\r\n```\r\n\r\nEven though it's the same version, each SO library has its own `.data` segment, where static variables are stored. Protobuf uses the static store for registration, default values, and various caching mechanism. When you first start up the program, you are using your own protobuf, with its own static store. Any function that you call will refer to your app's `.data` segment. When you load `_pywrap_tensorflow_internal`, it will clobber these functions, referring to the tensorflow, statically-linked protobuf, therefore trying to free things that don't exist etc.\r\n\r\nThis is not a problem with your usage, rather, with our library.\r\n\r\nFor now, a workaround is to link against `_pywrap_tensorflow` to get the protobuf symbols. Make sure you *do not have* any static initializers, or if you do, try using `__attribute__((constructor))`.", "hi @drpngx. Thanks for your reply.\r\n\r\nI see what your saying. I linked to `libtensorflow.so` (not `_pywrap_tensorflow`) and rebuilt all dependencies pointing at `libtensorflow.so` and the error went away. \r\n\r\nHowever this proves quite a challenge for our buildsystem as all the dependencies pulled in for a particular C++ app by CMake also have to be rebuilt pointing at `libtensorflow.so`. This can be quite a lot of dependencies.\r\n\r\nIs there any specific bazel `BUILD` file I could modify to stop tensorflow statically linking against protobuf  and make it dynamic instead? \r\n\r\nSpecifically it feels like I could modify something in [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/BUILD](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/BUILD). However I have not used bazel before and am not sure where to start.\r\n\r\nThanks", "You could try linkshared = 1 while building the protobuf.\n\nOn Thu, Mar 16, 2017 at 10:26 AM, Dan Barnes <notifications@github.com>\nwrote:\n\n> hi @drpngx <https://github.com/drpngx>. Thanks for your reply.\n>\n> I see what your saying. I linked to libtensorflow.so (not\n> _pywrap_tensorflow) and rebuilt all dependencies pointing at\n> libtensorflow.so and the error went away.\n>\n> However this proves quite a challenge for our buildsystem as all the\n> dependencies pulled in for a particular C++ app by CMake also have to be\n> rebuilt pointing at libtensorflow.so. This can be quite a lot of\n> dependencies.\n>\n> Is there any specific bazel BUILD file I could modify to stop tensorflow\n> statically linking against protobuf and make it dynamic instead?\n>\n> Specifically it feels like I could modify something in\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/BUILD.\n> However I have not used bazel before and am not sure where to start.\n>\n> Thanks\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8403#issuecomment-287130932>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbfnSjnbgPJTHsa398He84eXWsI4jks5rmXC_gaJpZM4Mc-QB>\n> .\n>\n", "We are having trouble finding the right place to insert linkshared in the BUILD files for this to be successful. Is there any chance you could point us to the right place to set \"linkshared = 1\" to load protobuf as a shared library?\r\n\r\nThanks for your help", "Hi Dan, it's in the protobuf library in third_party/protobuf. I'll try to\ntake a look later and give you the full path\n\nOn Fri, Mar 17, 2017 at 6:45 AM, Dan Barnes <notifications@github.com>\nwrote:\n\n> We are having trouble finding the right place to insert linkshared in the\n> BUILD files for this to be successful. Is there any chance you could point\n> us to the right place to set \"linkshared = 1\" to load protobuf as a shared\n> library?\n>\n> Thanks for your help\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8403#issuecomment-287357021>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_Sbb_QyEtgt52qIfU5AWYpZm2TJbnOks5rmo50gaJpZM4Mc-QB>\n> .\n>\n", "It's in $(bazel info output_base)/external/protobuf/BUILD, the\n\"protobuf\" rule. I'm not sure you can actually modify that. Another\nway might be to create a cc_library rule, which depends on that, and\nthen making that a linkshare = 1 rule.\n\n\nOn Fri, Mar 17, 2017 at 10:30 AM, Patrick Nguyen <drpng@google.com> wrote:\n\n> Hi Dan, it's in the protobuf library in third_party/protobuf. I'll try to\n> take a look later and give you the full path\n>\n> On Fri, Mar 17, 2017 at 6:45 AM, Dan Barnes <notifications@github.com>\n> wrote:\n>\n>> We are having trouble finding the right place to insert linkshared in the\n>> BUILD files for this to be successful. Is there any chance you could point\n>> us to the right place to set \"linkshared = 1\" to load protobuf as a shared\n>> library?\n>>\n>> Thanks for your help\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/8403#issuecomment-287357021>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/AT_Sbb_QyEtgt52qIfU5AWYpZm2TJbnOks5rmo50gaJpZM4Mc-QB>\n>> .\n>>\n>\n>\n", "Assigning to myself since I'm working on a stable, more user-friendly C++ API. Perhaps future TF libraries can dynamically link protobuf (or some other fancier solution) to avoid this problem moving forward.", "@drpngx Thanks for the suggestion. We tried the methods you mentioned but unfortunately we still cant get the symbols out of libtensorflow.so\r\n\r\n@skye great! If you need any help with testing please let me know. Namely the option to build dynamically against user selected dependancies would would be ideal (without digging into bazel). \r\n\r\nWe also came across the [rename_protobuf.sh](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/rename_protobuf.sh) script but we couldnt get this to work either. ", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "We have since split the library and it is no longer monolithic. I think it should be solved.\r\n\r\n/CC @allenlavoie "]}, {"number": 8402, "title": "Branch 150085723", "body": "", "comments": ["@gunan btw, one idea of making some of these flaky numeric tests less flaky is to always run nodes in the same order. We improved determinism of our stuff by adding control deps that force computation to use deterministic execution order (https://github.com/yaroslavvb/stuff/tree/master/linearize)", "@yaroslavvb Thanks for the suggestion.\r\nWhen we previously looked into this there were some issues we ran into when making the run order deterministic.\r\n@aselle I think you took a look at this, right?\r\nMaybe we should reconsider this using the code @yaroslavvb shared as a starting point?"]}, {"number": 8401, "title": "Tensorflow not working in Zeppelin 0.7.0", "body": "There seems to be some issue using Tensorflow in Zeppelin 0.7.0 and it throws this error:\r\nNameError: name `_interactive` is not defined\r\n\r\nAnother user reported the [same issue](http://stackoverflow.com/questions/42757433/tensorflow-can-not-work-with-zeppelin) on SO. The fix suggested there seems to be really hacky.\r\n\r\nThe issue seems to be that in tf_logging.py file [.\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\platform\\tf_logging.py] the _interactive variable is not triggering for Zeppelin.\r\n\r\n```\r\n# Determine whether we are in an interactive environment\r\ntry:\r\n  # This is only defined in interactive shells\r\n  if _sys.ps1: _interactive = True\r\nexcept AttributeError:\r\n  # Even now, we may be in an interactive shell with `python -i`.\r\n  _interactive = _sys.flags.interactive\r\n```", "comments": ["It looks the code meant to have `_interactive = False` before the [block](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/tf_logging.py#L40). Would you be willing to file a PR?", "Sure thing!", "Great, thanks!"]}, {"number": 8400, "title": "Update Java API test for size and missing asserts", "body": "Corrected issues where unit tests tested too many things in a single\r\ntest, and where they didn't perform any effective tests. Its best\r\npractice to keep the size of test to as close to one assert as\r\npossible, and to not have tests that never test anything.", "comments": ["Can one of the admins verify this patch?", "ping @asimshankar ", "@craykg any chance to address the comments?", "@drpngx Haven't had a chance yet - planning on making the changes on Friday/Saturday.", "ping @craykg ", "Another ping for @craykg ", "(Closing this for now to keep our backlog low since it's been a while since we've heard back -- once you are back and capable of moving forward we can make progress again!)"]}, {"number": 8399, "title": "mnist_softmax.py has \"TimeoutError: [WinError 10060]\" ...", "body": "The code https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/mnist_softmax.py\r\ndoes not run on TensorFlow (Windows installation)\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nQuite similar problem reported here: http://stackoverflow.com/questions/40467893/running-mnist-softmax-py-on-tensorflow-installed-with-docker However, no solution there...\r\n\r\n### Environment info\r\nOperating System: Windows 10 pro (Intel(R) Core(TM) i7 6500U CPU) \r\nTensorFlow installed: pip install -U tensorflow; Python 3.5.2 :: Anaconda custom (64-bit)\r\n------------------------------------------------------------------------------------------\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\natlun\\AppData\\Local\\Continuum\\Anaconda3\\lib\\urllib\\request.py\", line 1254, in do_open\r\n    h.request(req.get_method(), req.selector, req.data, headers)\r\n  File \"C:\\Users\\natlun\\AppData\\Local\\Continuum\\Anaconda3\\lib\\http\\client.py\", line 1106, in request\r\n    self._send_request(method, url, body, headers)\r\n  File \"C:\\Users\\natlun\\AppData\\Local\\Continuum\\Anaconda3\\lib\\http\\client.py\", line 1151, in _send_request\r\n    self.endheaders(body)\r\n  File \"C:\\Users\\natlun\\AppData\\Local\\Continuum\\Anaconda3\\lib\\http\\client.py\", line 1102, in endheaders\r\n    self._send_output(message_body)\r\n  File \"C:\\Users\\natlun\\AppData\\Local\\Continuum\\Anaconda3\\lib\\http\\client.py\", line 934, in _send_output\r\n    self.send(msg)\r\n  File \"C:\\Users\\natlun\\AppData\\Local\\Continuum\\Anaconda3\\lib\\http\\client.py\", line 877, in send\r\n    self.connect()\r\n  File \"C:\\Users\\natlun\\AppData\\Local\\Continuum\\Anaconda3\\lib\\http\\client.py\", line 849, in connect\r\n    (self.host,self.port), self.timeout, self.source_address)\r\n  File \"C:\\Users\\natlun\\AppData\\Local\\Continuum\\Anaconda3\\lib\\socket.py\", line 711, in create_connection\r\n    raise err\r\n  File \"C:\\Users\\natlun\\AppData\\Local\\Continuum\\Anaconda3\\lib\\socket.py\", line 702, in create_connection\r\n    sock.connect(sa)\r\nTimeoutError: [WinError 10060] \r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\natlun\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\spyderlib\\widgets\\externalshell\\sitecustomize.py\", line 714, in runfile\r\n    execfile(filename, namespace)\r\n  File \"C:\\Users\\natlun\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\spyderlib\\widgets\\externalshell\\sitecustomize.py\", line 89, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n  File \"C:/Users/natlun/Documents/Python Scripts/mnist_softmax03.py\", line 92, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"C:\\Users\\natlun\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"C:/Users/natlun/Documents/Python Scripts/mnist_softmax03.py\", line 20, in main\r\n    mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\r\n  File \"C:\\Users\\natlun\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py\", line 211, in read_data_sets\r\n    SOURCE_URL + TRAIN_IMAGES)\r\n  File \"C:\\Users\\natlun\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py\", line 208, in maybe_download\r\n    temp_file_name, _ = urlretrieve_with_retry(source_url)\r\n  File \"C:\\Users\\natlun\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py\", line 165, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"C:\\Users\\natlun\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py\", line 190, in urlretrieve_with_retry\r\n    return urllib.request.urlretrieve(url, filename)\r\n  File \"C:\\Users\\natlun\\AppData\\Local\\Continuum\\Anaconda3\\lib\\urllib\\request.py\", line 188, in urlretrieve\r\n    with contextlib.closing(urlopen(url, data)) as fp:\r\n  File \"C:\\Users\\natlun\\AppData\\Local\\Continuum\\Anaconda3\\lib\\urllib\\request.py\", line 163, in urlopen\r\n    return opener.open(url, data, timeout)\r\n  File \"C:\\Users\\natlun\\AppData\\Local\\Continuum\\Anaconda3\\lib\\urllib\\request.py\", line 466, in open\r\n    response = self._open(req, data)\r\n  File \"C:\\Users\\natlun\\AppData\\Local\\Continuum\\Anaconda3\\lib\\urllib\\request.py\", line 484, in _open\r\n    '_open', req)\r\n  File \"C:\\Users\\natlun\\AppData\\Local\\Continuum\\Anaconda3\\lib\\urllib\\request.py\", line 444, in _call_chain\r\n    result = func(*args)\r\n  File \"C:\\Users\\natlun\\AppData\\Local\\Continuum\\Anaconda3\\lib\\urllib\\request.py\", line 1282, in http_open\r\n    return self.do_open(http.client.HTTPConnection, req)\r\n  File \"C:\\Users\\natlun\\AppData\\Local\\Continuum\\Anaconda3\\lib\\urllib\\request.py\", line 1256, in do_open\r\n    raise URLError(err)\r\nurllib.error.URLError: <urlopen error [WinError 10060]>", "comments": ["This issue is already solved with PR #8171.\r\n\r\nRelated to issues #8159, #8134, #8126, #8116, #8113 and #8009.\r\n\r\nNext TF release will bring the fix.", "hello, I just meet the same issue and looking forward the solution.\r\nIt looks like a server relevant issue that too many worldwide download/pull data request at the same time, crashed the mnist data server, which is on Yann LeCun's website (a personal website)?", "> This issue is already solved with PR #8171.\r\n\r\nThey actually rejected the PR because:\r\n\r\n> We do not have the permission from Yann Lecun to mirror these files.\r\n>\r\n> Therefore, we cannot accept this PR. We are looking into modifying our tutorials to not depend on mnist dataset.\r\n\r\nSo I think this will continue to be an issue with no resolution except manually downloading the MNIST files and placing them in the right directory. You should be able to download them from https://web.archive.org/web/20160828233817/http://yann.lecun.com/exdb/mnist/.", "Yes, it is really unfortunate that this happens. We're looking for a solution...", "I can download it from [http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz](http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz) with browser. But when running code, WinError 10060 occurred. ", "This error indicates that a timeout occurred while trying to download the files. It's not something we can fix unfortunately. The intermittent availability of MNIST is covered in other issues, so I will close this one as a duplicate.", "You could fix that by using tensorflow command \r\n\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\n\r\nafter that :+1: \r\nmnist = input_data.read_data_sets(\"path/to/your/data/\", one_hot=True)\r\n\r\n\r\n"]}, {"number": 8398, "title": "How to apply dropout for getting prediction from the last layer of a trained model?", "body": "I have used transfer learning to retrain/fine-tune Inception-V3 model for image classification on my own data in Tensorflow. However, when I test the image classification after training, I get the following error:\r\n\r\n> \"Invalid argument: You must feed a value for placeholder tensor 'final_layer/dropout/Placeholder' with dtype float\"\r\n\r\nI am using the following code for getting the predictions from the final layer:\r\n\r\n`softmax_tensor = sess.graph.get_tensor_by_name('final_layer/final_result/Softmax:0')\r\n\t\t\tpredictions = sess.run(softmax_tensor, {'DecodeJpeg/contents:0': image_data})`\r\n\r\nI guess the error is caused by not applying the dropout placeholder in the second statement, as the training was performed with dropout layer. How do I add a dropout placeholder here?\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "(Without looking at the code, I am guessing you can try\r\n```predictions = sess.run(softmax_tensor, \r\n         {'DecodeJpeg/contents:0': image_data, 'final_layer/dropout/Placeholder': 0.0})\r\n```\r\nor something like that.  Regardless, please ask usage questions on Stackoverflow; thanks!)"]}, {"number": 8397, "title": "32-bit Windows exe generation using cmake", "body": "Hi\r\n\r\nI have compiled my code using the procedure outlined in the documentation to generate 64-bit exe on windows using cmake. Now I want to create 32-bit binary but cmake does not run for the option -A x86.\r\nCan somebody how can I generate a 32-bit binary still using 64-bit VS tools?", "comments": ["Perhaps this [stackoverflow post](http://stackoverflow.com/questions/28350214/how-to-build-x86-and-or-x64-on-windows-from-command-line-with-cmake) helps.\r\n\r\nI am not sure that TF works on 32 bits. We might have done that work for Android.", "Ok, I tried the procedure mentioned in that post. But it does not compile and exits with \"error MSB6006, cmd.exe exited with code 9009\". No further information about error. I searched for it and there is no concrete reason for this error. ", "Could you post the full log, maybe somewhere on pastebin? I'm not sure what's wrong.\r\n\r\nI know that VS 32bit can cross compile into amd64, but I don't know if the converse is true. It should be.", "Here is the attached log file. I rebuilt so that's why time is less, else it took 53 minutes last time for error to pop up. Anyways, what I do is:\r\n\r\n(1) Set up Visual Studio 14 to be used as Generator:\r\ncmake -G \"Visual Studio 14 2015\" /path_to_source\r\n\r\n(2) Then I do \r\ncmake --build build_32 --config Release\r\n\r\nIf I use \"Visual Studio 14 2015 Win64\", then it generates targets for Platform x64.\r\n\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/846735/log.txt)\r\n", "(1) If I use the process outlined in the original ReadMe at Github, [https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake](url)\r\n\r\nThen using 64 bit tools, and using cmake \r\ncmake .. -A x64 -DCMAKE_BUILD_TYPE=Release ^ ... (rest of command)\r\n\r\nThe process runs fine, and 64bit exe is generated for sample code. I even changed the sample code and got it to read from already built graph and it works fine. \r\n\r\n(2) If I repeat the process, that is setup 64-bit tools of VS and then run the same cmake command but without mentioning architecture, i.e. -A FLAG\r\n\r\ncmake ..  -DCMAKE_BUILD_TYPE=Release ^ ... (rest of command)\r\n\r\nThen VS projects are for Win32 Platform and the process again halts with error, but this time error is of linking 64bit library \"libprotobuf.lib\" with the target exe for x86. Here is the attached error log:\r\n\r\n[log_2.txt](https://github.com/tensorflow/tensorflow/files/846812/log_2.txt)\r\n\r\nI guess that this library is downloaded when repo of protobuf is cloned when building the projects. So does this mean that exe for 32-bit platform can't be generated?\r\n", "Maybe this?\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/external/protobuf.cmake#L10\r\n\r\nBut since 32 bit on windows is not officially supported, you can get better help on stackoverflow.", "ok, still not working. Another dependency on some 64bit identifier is popping up now. The error is:\r\n\r\n**c:\\users\\aneek\\documents\\tf_x86_build\\tensorflow\\tensorflow\\core\\common_runtime\\bfc_allocator.h(356): error C3861: '_BitScanReverse64': identifier not found [C:\\Users\\ANeeK\\Documents\\TF_x86_Build\\tensorflow\\tensorflow\\contrib\\cmake\\build_32\\tf_core_cpu.vcxproj]**\r\n\r\nI think it probably won't compile for 32-bit. ", "@ANeeK181  I came into the same question as you, and I fixed it by change _BitScanReverse64 function to _BitScanReverse, and then built tf_core_cpu.lib successfully.", "Hi Guys,\r\n\r\nCould anybody post clear steps how to build 32-bit exe using cmake?\r\n\r\nThanks", "We (TensorFlow team) never tried building TF on a 32 bit OS.\r\nSo we have no official support for TF on a 32 bit desktop system."]}, {"number": 8396, "title": "Retrain Inception Model Error In Android", "body": "I am following [Tensorflow for poet](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0) instruction for retrain model. I have successfully create retrained_graph.pb and retrained_labels.txt. While I use imagenet_comp_graph_label_strings.txt and tensorflow_inception_graph.pb then application run without any error. But use my created file then I get error that:\r\n\r\n`Caused by: java.lang.UnsupportedOperationException: Op BatchNormWithGlobalNormalization is not available in GraphDef version 21. It has been removed in version 9. Use tf.nn.batch_normalization().\r\n                                                                         at org.tensorflow.Graph.importGraphDef(Native Method)\r\n                                                                         at org.tensorflow.Graph.importGraphDef(Graph.java:118)\r\n                                                                         at org.tensorflow.Graph.importGraphDef(Graph.java:102)\r\n                                                                         at org.tensorflow.contrib.android.TensorFlowInferenceInterface.load(TensorFlowInferenceInterface.java:402)\r\n                                                                         at org.tensorflow.contrib.android.TensorFlowInferenceInterface.initializeTensorFlow(TensorFlowInferenceInterface.java:91)\r\n                                                                     \tat org.tensorflow.demo.TensorFlowImageClass`", "comments": ["As the error message says, you want to use `tf.nn.batch_normalization` instead. You may have to retrain from scratch.\r\n\r\n@nealwu may have some suggestion.", "I used [this](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/image_retraining)(image retraining sample - inception v3) to learn my own classification.\r\nWhen I used my 'pb' and 'txt' files in Android, I got the same error.\r\n", "Thanks for the bug report, I believe there is an unexpected mismatch in how we import graphs in Python and the C API, looking into it.", "@drpngx, As per your suggestion I try to retrain model from scratch followed [TensorFlow guides ](https://www.tensorflow.org/versions/r0.11/how_tos/image_retraining/) but I get same error.\r\nI also try to strip graph and change in [ClassifierActivity](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/ClassifierActivity.java). But still I get same error.", "I get one blog [Tensorflow for mobile](https://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/).Using That create **optimized_graph.pb, rounded_graph.pb, mmapped_graph.pb**  files. \r\n\r\noptimized_graph.pb and rounded_graph.pb file work in android application without any error.\r\nWhile use mmapped_graph.pb I get error that  **Failed to initialize: java.io.IOException: Not a valid TensorFlow Graph serialization: Invalid GraphDef**\r\n\r\nPerformance of application is not good while use optimized_graph.pb and rounded_graph.pb file.While application camera screen not contain any flower photos otherwise random flower name show with high confidence rate. Any reason behind these. \r\n"]}, {"number": 8395, "title": "R0.12", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I'll assume this is a mistake."]}, {"number": 8394, "title": "Protobufs are into multiple shared libraries loaded from python", "body": "### Issue description\r\nTensorflow currently fails with the following error if compiled using `clang` in `-c opt` mode when trying to import `tensorflow.contrib` package in Python .\r\n\r\nPython code reproducing the problem is very simple: \r\n```\r\nimport tensorflow.contrib\r\n```\r\n\r\nProgram output:\r\n```\r\n[libprotobuf ERROR external/protobuf/src/google/protobuf/descriptor_database.cc:57] File already exists in database: tensorflow/core/example/example.proto\r\n[libprotobuf FATAL external/protobuf/src/google/protobuf/descriptor.cc:1275] CHECK failed: generated_database_->Add(encoded_file_descriptor, size): \r\nterminate called after throwing an instance of 'google::protobuf::FatalException'\r\n  what():  CHECK failed: generated_database_->Add(encoded_file_descriptor, size): \r\n```\r\n\r\nThe short story is that protobufs are getting statically linked into two shared libraries, both of which get loaded at runtime and that causes the error.\r\n\r\nHere's the full breakdown of what happens:\r\n1. Protobufs (`//tensorflow/core:protos_all_cc`) get compiled as a static library.\r\n1. Protobufs (`//tensorflow/core:protos_all_cc`) get statically linked into two separate shared libraries: `_pywrap_tensorflow_internal.so` and `_pywrap_tensorflow_print_model_analysis_lib.so`.\r\n2. While compiling those clang inlines the protobuf initialization code(`AddDescriptors`) inside `example.pb.cc`(look for it in `bazel-genfiles`)  to the global initialization code of both shared libraries.\r\n3. `python run.py` starts running. While processing python's import statement, dynamic linker gets called to load `_pywrap_tensorflow_internal.so`. Static initialization code inside `example.pb.cc` is run, registering it to the protobuf database of `_pywrap_tensorflow_internal.so`\r\n4. At a later point `_pywrap_tensorflow_print_model_analysis_lib.so` gets loaded. Since python calls `dlopen` with `RTLD_GLOBAL` dynamic linker finds an existing symbols for `AddDescriptorsImpl` in `_pywrap_tensorflow_internal.so` and uses that for all calls to that function later(for calls coming from `_pywrap_tensorflow_print_model_analysis_lib.so` too). \r\n5. Static initialization code inside `example.pb.cc` is run again (for `_pywrap_tensorflow_print_model_analysis_lib.so`), it calls `AddDescriptorsImpl` and gets into the function from `_pywrap_tensorflow_internal.so`, which tries to registers the same file again in the protobuf database of `_pywrap_tensorflow_internal.so` leading to the specified error.\r\n\r\nHere are a few observations that may be interesting:\r\n1. It works with `gcc`, because `gcc` doesn't inline `AddDescriptors` to the global initialization code of libraries, then dynamic linker merges those two functions into one, and that function has a proper check for being called multiple times(`AddDescriptorsImpl`, which is getting called after inlining doesn't). But note that it may break too if `gcc` will start inlining `AddDescriptors` in a newer version.\r\n2. It works on Mac, because dynamic linker there doesn't merge corresponding functions into one. Note that it means we get multiple protobuf databases(one for each loaded shared library that has protobufs in it) and can probably lead to other problems.\r\n\r\n### Environment info\r\nOperating System: ubuntu 14.04\r\nInstalled version of CUDA and cuDNN: none\r\n1. The commit hash (`git rev-parse HEAD`)\r\n`ff9682b5f493ae7ad912da29789668dbf50d5e1f`\r\n2. The output of `bazel version`\r\n`Build label: 0.4.4\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Feb 1 18:54:21 2017 (1485975261)\r\nBuild timestamp: 1485975261\r\nBuild timestamp as int: 1485975261\r\n`\r\n\r\n### Repro\r\n\r\n1. Extract [test.zip](https://github.com/tensorflow/tensorflow/files/841040/test.zip) to the repository root(it's a sample python target that fails)\r\n1. Make sure `clang` is installed. My version is `3.8.0-2ubuntu3~trusty4`, but that shouldn't matter.\r\n2. Configure with \r\n```\r\nexport CC=/usr/bin/clang\r\nTF_NEED_JEMALLOC=1 TF_NEED_GCP=0 TF_NEED_HDFS=0 \\\r\nTF_ENABLE_XLA=0 TF_NEED_OPENCL=0 TF_NEED_CUDA=0 \\\r\nyes \"\"  | ./configure\r\n```\r\n3. Build and run with opt\r\n```\r\nbazel run -c opt //test:run\r\n```\r\n", "comments": ["Thank you for reporting!\r\n\r\nWe saw something similar a couple of weeks ago. Currently we don't have safeguards for this.\r\n`print_model_analysis` should link against `_pywrap_tensorflow_internal`, rather directly to corelib internal.\r\n\r\n@keveman @martinwicke @jhseu @gunan \r\n@skye -- another set of symbols with external linkage, depending on what we do there.\r\n", "@allenlavoie fyi\r\n\r\n@gunan Perhaps we should run non-mac clang builds in our CI?", "Running clang builds is in the plans, but there are still a few higher priority items on the list.\r\nMaybe we should bump the priority of this item.", "Clang build is now running nightly. Closing this issue.", "@gunan, the issue is still there. There a patch applied to workaround it, but I'm just saying that you may want to keep the issue open until a proper solution is possible with bazel.\r\nSee [a line in workspace.bzl which applies the patch](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl#L314)  and [the patch itself](https://github.com/tensorflow/tensorflow/blob/master/third_party/protobuf/add_noinlines.patch).\r\n", "We have some longer term projects which may triage this on the side.\r\nBut from your comment I feel like most work needed for this is on bazel side?", "You are right, it hard to fix without changes to bazel.", "@ilya-biryukov: Could you kindly recheck if the patch is still necessary? I've not managed to reproduce the problem with Clang 3.9.", "@tkoeppe , sorry for the late response. AFAIK there were changes to tensorflow's build files that should have fixed that problem. It should be fine now, I'll double-check that it's the case and will report if there are any issues left.", "@ilya-biryukov: Thank you very much! I've already removed the patch internally; I'm not sure if this has landed yet."]}, {"number": 8393, "title": "Add bounds to tf.contrib.opt.ScipyOptimizerInterface", "body": "For style transfer it is common the use L-BFGS-B, and the ScipyOptimizerInterface greatly simplifies its application in TF. However, to increase the quality of the synthesised images one typically restricts the range of the values to standard int8 [0, 255] by providing box constraints. \r\n\r\nHence my feature request: Please add the bounds keyword to the interface of ScipyOptimizerInterface.", "comments": ["cc @joshburkart who wrote the wrapper", "I believe this is already possible -- `ScipyOptimizerInterface` takes an optional parameter `optimizer_kwargs`, into which you can pass things like `{'bounds': [[0, 1]]}` etc.\r\n\r\nThe deficiency, as I see it, is that you don't directly control how the `Variable`s you are optimizing over get packed into a single vector for SciPy to do its thing with... LMK if this is a problem/if you have any ideas on how to improve this.", "I tried `optimizer_kwargs`, but those get inserted as dictionary argument for `options` in [scipy.minimize](https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.optimize.minimize.html) and so cannot be used for bounds.", "Hmm, I'm looking [here](https://github.com/tensorflow/tensorflow/blob/cb17d1b0e2b581b5da1d9597b7929ba764749d38/tensorflow/contrib/opt/python/training/external_optimizer.py#L317) and it seems like `optimizer_kwargs` get sent as kwargs straight to `scipy.optimize.minimize`. So why couldn't a `bounds` kwarg be added this way...?", "Sorry, I was blind - @joshburkart is correct. Thanks!", "I have a problem when I try to use the bounds argument to the ScipyOptimizerInterface function. I have this error message \"TypeError: _minimize_lbfgsb() got multiple values for argument 'bounds'\r\n\" with the following code. :\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nvector = tf.Variable([7., 7.], 'vector')\r\n# Make vector norm as small as possible.\r\nloss = tf.reduce_sum(tf.square(vector))\r\nbounds = [[-10,10],[-10,10]]\r\noptimizer = tf.contrib.opt.ScipyOptimizerInterface(loss, options={'maxiter': 100,\"bounds\" : bounds})\r\nwith tf.Session() as session:\r\n\tsession.run(tf.global_variables_initializer())\r\n\toptimizer.minimize(session)\r\n# The value of vector should now be [0., 0.].\r\nprint(vector[0],vector[1])\r\n```\r\n\r\nIt seems to be the same kind of problem than on an other conversation : [https://github.com/scipy/scipy/issues/4608](url)\r\nAnd the solution is to pass bounds as a primary key of minimizer_kwargs of the function which use scipy.optimize.minimize but it is not possible to do that with the ScipyOptimizerInterface.\r\n\r\nDo you have an idea ?\r\n", "This is a SciPy thing... Change to\r\n```python\r\noptimizer = tf.contrib.opt.ScipyOptimizerInterface(loss, bounds=bounds, options={'maxiter': 100})\r\n```", "Thanks a lot.", "FYI that we've now added a `var_to_bounds` argument to ScipyOptimizerInterface that allows specifying per-`Variable` bounds. It's submitted in the internal Google repository so should be available in GitHub/PyPi soon. Also be aware that once the update is rolled out, supplying the `bounds` keyword explicitly as I suggested above will raise an exception..."]}, {"number": 8392, "title": "How to add a new op which is similar with matmul", "body": "I want to add a new op which is similar with matmul, for example anothermatmul. \r\n1. add REGISTER_OP(\"AnotherMatMul\") to tensorflow/core/ops/math_ops.cc like MatMul\r\n2. add anothermatmul_op.h anothermatmul_op.cc to tensorflow/core/kernels/\r\n3. bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nthen I can see  bazel-bin/tensorflow/core/libmath_ops_op_lib.lo is update, I think the new op has been compiled successfully.\r\n4. install \r\n\r\nI find util/python/python_lib/tensorflow/python/ops/gen_math_ops.py is not updated, and cannot use AnotherMatMul in python.\r\n\r\nhow to update gen_math_ops.py?\r\nor how to generate python wrap?\r\n", "comments": ["Please be a bit clearer about \"how to use\".\r\n\r\nThere are a few steps:\r\n\r\n1. Make sure you add `anothermatmul_op` in the [`BUILD` file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/BUILD#L2136)\r\n2. Make sure you follow the instructions if you want to run in-place (or install the pip)\r\n\r\nIf the `gen_math_ops.py` is not generated, then it means that you didn't rebuild properly. Try `bazel build --explain=/tmp/blaze_debug.log --subcommands --config=opt //tensorflow/python:math_ops`\r\nIt should show that the file is updated.", "Thanks. I add anothermatmul_op to BUILD,  bazel build and install. But the  gen_math_ops.py cannot be updated. \r\n1. run bazel build tensorflow/python:math_ops_gen, and a new gen_math_ops.py is generated in bazel-genfiles/tensorflow/python/ops/, which can find the new op another_mat_mul.\r\n2. cp bazel-genfiles/tensorflow/python/ops/gen_math_ops.py util/python/python_lib/tensorflow/python/ops/gen_math_ops.py\r\n3. add a function def another_matmul: gen_math_ops.another_mat_mul to util/python/python_lib/tensorflow/python/ops/math_ops.py\r\n4. when call tf.another_matmul, get error:tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'AnotherMatMul'\r\n\r\nhowever, I have add REGISTER_OP(\"AnotherMatMul\") to tensorflow/core/ops/math_ops.cc\r\n\r\nhow to add a new op?\r\nThanks in advance.\r\n", "OK, that's great. The build/install should have worked. I never had to do this before, but try to uninstall first before installing. And maybe remove /tmp/tensorflow_pkg before you run the pip building script.\r\n\r\nThe error you are seeing is because the `_pywrap_tensorflow_internal.so` has not been updated. If you figure out how to rebuild the PIP install then it should work.", "Thanks! uninstall and install again, that solved!"]}, {"number": 8391, "title": "tf.lbeta() error when fed with placeholder", "body": "Hi, there seems to be a bug in `tf.lbeta`.\r\n\r\n```python\r\ntf.lbeta(tf.placeholder(tf.float32, [3, 2, 3, None]))\r\n# => ValueError: Can not squeeze dim[0], expected a dimension of 1, got 3 for 'lbeta_1/cond/Squeeze' (op: 'Squeeze') with input shapes: [3,2,3,?].\r\n```\r\n", "comments": ["@langmore are you the right person to take a look?", "Yes, I'm the right person for this.\r\n\r\n@concretevitamin  I have an internal fix for this.  I'll send to you once ready.\r\n\r\n@thjashin  Thanks for reporting!", "A fix by @langmore will be pushed upstream soon.  Closing this.", "Thanks! @concretevitamin @langmore \r\nJust a bit more, there seems to be no static shape inference in `tf.lbeta()`", "What in particular doesn't work?  The old code had some static inference (see the tests).  After this recent fix, the same tests are still in place, and the code is much simpler, so I suspect there won't be any problems."]}]