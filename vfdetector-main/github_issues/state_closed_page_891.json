[{"number": 26752, "title": "Bump abseil-py dep and min Bazel version", "body": "This makes builds not fail with --incompatible_remove_old_python_version_api enabled, which will become the default in Bazel 0.25. Fixes #26616.", "comments": ["@gunan is a better person to review this. He is also already in the conversation in https://github.com/tensorflow/tensorflow/issues/26616.", "Rebased to include ed297347fdba844f636d980c3a5e0729b043cc65.", "Let me double check how bazel version is picked in the CI.\r\nI will try to get back to this PR tomorrow.", "FYI I'll flip the flag in Bazel on Thursday, at which point it may break CI for building Tensorflow with Bazel-at-head. (Though I think that's already broken due to the hardcoded version check.)"]}, {"number": 26751, "title": "[ROCm] add ROCm support for const related ops", "body": "add ROCm support for Const Fill ZerosLike OnesLike Placeholder PlaceholderV2 op\r\n\r\nThese ops are fundamental to TensorFlow and it has been running for more than 1 year on ROCm platform. We have published docker images at:\r\n\r\nhttps://hub.docker.com/r/rocm/tensorflow/tags\r\n\r\nand also PyPI packages:\r\n\r\nhttps://pypi.org/project/tensorflow-rocm/\r\n\r\nfor a sample public test run you can refer to:\r\n\r\nhttp://ml-ci.amd.com:21096/job/tensorflow-upstream-unit-tests/721/console\r\n\r\nyou can see:\r\n\r\n```\r\n//tensorflow/python/kernel_tests:constant_op_test                        \u001b[0m\u001b[32mPASSED\u001b[0m in 3.1s\r\n//tensorflow/python/kernel_tests:constant_op_test_gpu                    \u001b[0m\u001b[32mPASSED\u001b[0m in 3.1s\r\n```", "comments": ["Do you have a passing test from any public continuous integration run - so we can check that this code actually compiles and runs?  If so, please add it to the PR message.\r\n\r\nSame for your other ROCm PRs.", "@ebrevdo \"Const\" ops are fundamental to TensorFlow and it has been running for more than 1 year on ROCm platform. We have published docker images at:\r\n\r\nhttps://hub.docker.com/r/rocm/tensorflow/tags\r\n\r\nand also PyPI packages:\r\n\r\nhttps://pypi.org/project/tensorflow-rocm/\r\n\r\nfor a sample public test run you can refer to:\r\n\r\nhttp://ml-ci.amd.com:21096/job/tensorflow-upstream-unit-tests/721/console\r\n\r\nyou can see:\r\n\r\n```\r\n//tensorflow/python/kernel_tests:constant_op_test                        \u001b[0m\u001b[32mPASSED\u001b[0m in 3.1s\r\n//tensorflow/python/kernel_tests:constant_op_test_gpu                    \u001b[0m\u001b[32mPASSED\u001b[0m in 3.1s\r\n```\r\n", "Great!  Add your message to the PR description please.\n", "@ebrevdo it'd done. Also the 3 failures `GPU Python3` , `Windows Bazel` , `XLA` have nothing to do with this PR.\r\n\r\n- `GPU Python3` shows `TF_CUDA_VERSION` not configured which should be an environment issue in the CI system\r\n- `Windows Bazel` has no public link but to my knowledge the target has been broken for some time\r\n- `XLA` failed case has nothing to do with this PR"]}, {"number": 26750, "title": "[docs] Remove duplicate paragraph", "body": "Removes a dupe paragraph & button RE: iOS + Android.", "comments": []}, {"number": 26749, "title": "TfLite on object detection with QUANTIZED_UINT8.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04.\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: XiaoMi 8.\r\n- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu==1.9.0\r\n- **TensorFlow version (use command below)**: 1.9.0\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0/7.1\r\n- **GPU model and memory**: enough.\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI train a model with only multi-layer conv+bn+relu/sigmoid using `tf.contrib.quantize.create_training_graph()` and test it with `tf.contrib.quantize.create_eval_graph()`, every seems ok.\r\n\r\nThen I freeze the *.ckpt file to *.pb file with `tensorflow.python.tools.freeze_graph.freeze_graph`.\r\nFinally, I convert the *.pb file to *.tflite file with the following code, and again every is ok.\r\n\r\n```\r\n    lite8 = tf.contrib.lite.toco_convert(\r\n        input_tensors=[inT],\r\n        output_tensors=[outT],\r\n        input_data=graph.as_graph_def(),\r\n        # default_ranges_stats=(-6.0, 6.0),\r\n        quantized_input_stats=[(127.5, 128.0)],\r\n        # inference_type=tf.contrib.lite.constants.FLOAT,\r\n        inference_type=tf.contrib.lite.constants.QUANTIZED_UINT8,\r\n    )\r\n\r\n    with open(lite8Path, 'wb') as txt:\r\n        txt.write(lite8)\r\n```\r\n\r\n**Here is the question:**\r\nThe output of my model is the concat of 2 tensors, the first is the output of `cnn+bn`, the second one is the output of `cnn+bn+sigmoid`, **both should be type of float32**. However, when I run the quantized model on android phone with the following code, I am force to get output tensor of type of `uint8` rather than 'float32'.\r\n\r\n```\r\n        byte outB[][][] = new byte[1][NUM * NUM][2 + 4]; # The output is a tensor of rank 3.\r\n        org.tensorflow.lite.Interpreter.run(input_value, outB); # The outB must be uint8 (byte in android) here.\r\n\r\n        float outF[][][] = new Float[1][NUM * NUM][2 + 4];\r\n        org.tensorflow.lite.Interpreter.run(input_value, outF); # The application crash in this line, saying the output dtype is not match.\r\n```\r\n\r\n**I wonder how I can get `float32` output rather than `uint8` when using quantized .tflite file ?**\r\n**Thank you very much !!!**\r\n\r\n**In short, I have convert a pb file to tflite with uint8 quantization. I would like to know how to convert the output from uint8 back to float so that the previous code (that work on float type output) can still work.**\r\n\r\n### Source code / logs\r\nClear enough, need not to code.\r\n", "comments": ["@jdduke \r\n@andrehentz  \r\n@shashishekhar \r\n\r\nHelp me, please !!! \r\n", "@jiarenyf : \r\nRight now you can only convert the entire graph to uint8. One way you can avoid quantizing a particular layer would be changing the output_tensors specified in toco_convert, but then you may need to add some custom post processing for the model. Since the graph will only be converted till the output tensor.\r\nAdding @suharshs who may be able to give a definitive answer.", "@shashishekhar Thank you.\r\n\r\nIn short, I have convert a pb file to tflite with uint8 quantization. I would like to know how to convert the output from uint8 back to float so that the previous code (that work on float type output) can still work.\r\n\r\n@suharshs Could you please help me ?\r\n", "@shafi-dayatar Finally I have struggled to find the solution:\r\n\r\n**First, read the quantized_min/max value from the ckpt file with the output tensor names:**\r\n```\r\nimport tensorflow as tf\r\n\r\nfolder = './model8'\r\niCheckpoint = tf.train.latest_checkpoint(folder)\r\nreader = tf.train.NewCheckpointReader(iCheckpoint)\r\n\r\nvs_map = reader.get_variable_to_shape_map()\r\nprint(reader.get_tensor('conv2d_11/act_quant/min')) # As `MIN_1` ...\r\nprint(reader.get_tensor('conv2d_11/act_quant/max')) # As `MAX_1` ...\r\nprint(reader.get_tensor('conv2d_12/act_quant/min')) # As `MIN_2` ...\r\nprint(reader.get_tensor('conv2d_12/act_quant/max')) # As `MAX_2` ...\r\n```\r\n**And remember not to package the `sigmoid` op in the pb/tflite files, since it bring slide effect in the tflite interpreter.**\r\n\r\n**Second, calculate the `float32` using `unit8` with min/max value:**\r\n```\r\n    private static final float MAX_1 = 28.67623f;\r\n    private static final float MIN_1 = -28.629274f;\r\n    private static final float SCALE_1 = (float) ((MAX_1 - MIN_1) / 255.0);\r\n\r\n    private static final float MAX_2 = 4.380291f;\r\n    private static final float MIN_2 = -5.4539404f;\r\n    private static final float SCALE_2 = (float) ((MAX_2 - MIN_2) / 255.0);\r\n\r\n    Bitmap nBitmap = Bitmap.createScaledBitmap(\r\n                bitmap, IMG_WIDTH, IMG_HEIGHT, true);\r\n        int intValues[] = new int[IMG_WIDTH * IMG_HEIGHT];\r\n        nBitmap.getPixels(intValues, 0, IMG_WIDTH, 0, 0, IMG_WIDTH, IMG_HEIGHT);\r\n\r\n        byte[][][][] bValues = new byte[1][IMG_HEIGHT][IMG_WIDTH][1];\r\n        for (int i = 0; i < IMG_HEIGHT; ++i) {\r\n            for (int k = 0; k < IMG_WIDTH; ++k) {\r\n                int val = intValues[i * IMG_WIDTH + k];\r\n                int b = val & 0xFF;\r\n                int g = (val >> 8) & 0xFF;\r\n                int r = (val >> 16) & 0xFF;\r\n\r\n                bValues[0][i][k][0] = (byte) (r * 0.30 + g * 0.59 + b * 0.11);\r\n            }\r\n        }\r\n        byte outB1[][][] = new byte[1][NUM * NUM][2];\r\n        byte outB2[][][] = new byte[1][NUM * NUM][4];\r\n\r\n        Object[] inputArray = {bValues};\r\n        @SuppressLint(\"UseSparseArrays\")\r\n        Map<Integer, Object> outputMap = new HashMap<>();\r\n        outputMap.put(0, outB1);\r\n        outputMap.put(1, outB2);\r\n        lite.runForMultipleInputsOutputs(inputArray, outputMap);\r\n\r\n        float out1[][][] = new float[1][NUM * NUM][2];\r\n        float out2[][][] = new float[1][NUM * NUM][4];\r\n        for (int i = 0; i < NUM * NUM; ++i) {\r\n            for (int k = 0; k < 2; ++k) {\r\n                out1[0][i][k] = (outB1[0][i][k] & 0xFF) * SCALE_1 + MIN_1;\r\n            }\r\n            for (int k = 0; k < 4; ++k) {\r\n                out2[0][i][k] = (outB2[0][i][k] & 0xFF) * SCALE_2 + MIN_2;\r\n                out2[0][i][k] = (float) (1 / (1 + Math.exp(-out2[0][i][k])));\r\n            }\r\n        }\r\n```\r\n**We store the `uint8` values in `outB1` and `outB2` arrays, and calculate the corresponding `float32` values using the `MIN/MAX` values we got in Step 1.**\r\n\r\n**The key point is that I used to cast `byte` to `int` with `int = byte + 128` which is wrong and the right answer is `int = byte & 0xFF` in JAVA.**\r\n\r\nThat's all~\r\n"]}, {"number": 26748, "title": "Lite: Strided_slice 4-D bottleneck removed from kernel", "body": "1:> Removed hard coding or bottleneck to 4-Dim\r\n2:> Add assumption, that begin size and input dim are equal\r\n3:> Add assumptions, that begin, end, stride should be of same size\r\n4:> Error message updated", "comments": ["@haozha111 : Would you please help conclude this PR, thanks!", "@haozha111 : gentle reminder for review, TIA!", "Do you have an example model which exercises this? ", "@jdduke : As per i can recollect i have encountered this Op in PTB & HDRNET model. Please let me know if any other info required, TIA!", "> This needs some unit tests, and an additional test case in lite/testing/generate_examples.py\r\n\r\n@jdduke : I have checked generate_examples.py test cases, it seems all the cases are already covered, would you please share any specific case which you have in mind, i might have missed. Thanks!", "@jdduke : I have handled your comments, please review, thanks!", "> As per i can recollect i have encountered this Op in PTB & HDRNET model. \r\n\r\nRight, but do those models require >4D strided slice? Do you have a link to the models?", "> > As per i can recollect i have encountered this Op in PTB & HDRNET model.\r\n> \r\n> Right, but do those models require >4D strided slice? Do you have a link to the models?\r\n\r\n@jdduke : Those models don't require >4D strided_slice. But this PR is not only to address > 4D case, it is to address < 4D case as well. Mostly i see strided_slice used for < 4-D case, in this case it used to mandatory convert input to 4-D, which i feel is extra overhead and restrict the kernel to only 1-D to 4-D. So i generalized it. Thanks!\r\n\r\nHDRNet Model: https://groups.csail.mit.edu/graphics/hdrnet/", "> it is to address < 4D case as well. \r\n\r\nDo you have a model for this case?", "> > it is to address < 4D case as well.\r\n> \r\n> Do you have a model for this case?\r\n\r\nHDRNET & PTB has this case, as i remember.", "Can one of the admins verify this patch?", "@ANSHUMAN87 Could you please resolve the conflicts? Thanks!", "Resolved!", "@ANSHUMAN87 Can you please check reviewer comments and keep us posted. Thanks!", "> @ANSHUMAN87 Can you please check reviewer comments and keep us posted. Thanks!\r\n\r\nReplied, please check Thanks!", "@ANSHUMAN87 Can you please resolve conflicts? Thanks!", "> @ANSHUMAN87 Can you please resolve conflicts? Thanks!\r\n\r\n@gbaned : Resolved! Thanks!", "@ANSHUMAN87 Can you please resolve conflicts? Thanks!", "> @ANSHUMAN87 Can you please resolve conflicts? Thanks!\r\n\r\n@gbaned : Resolved!", "Thanks! In general looks fine. Have you been able to compare performance of the models you mentioned with/without this CL?", "> Thanks! In general looks fine. Have you been able to compare performance of the models you mentioned with/without this CL?\r\n\r\n@jdduke : Thanks for your feedback! I have not yet checked for the models i mentioned. But similar changes i had bench-marked for slice op(#27113), which has good results. So i feel the result would be same here as well. However if you feel need to be done for this op as well. Then i can do it. Please let me know your valuable opinion. Thanks!", "@jdduke gentle ping , please let us know if this PR is valid ?", "@ANSHUMAN87 could you provide a performance result about the strided slice op in your model, especially for input dim > 4?", "Closing as I believe this PR will not be merged as per @jdduke comment!\r\nhttps://github.com/tensorflow/tensorflow/pull/28179#issuecomment-642834327"]}, {"number": 26747, "title": "[ROCm] add ROCm support for RGBToHSV and HSVToRGB op", "body": "add ROCm support for RGBToHSV and HSVToRGB op", "comments": []}, {"number": 26746, "title": "[ROCm] add ROCm support for Cast op", "body": "add ROCm support for Cast op", "comments": [":frowning_face: Sorry, but only Googlers may change the label `cla: yes`."]}, {"number": 26745, "title": "[ROCm] Enable ROCm support for \"BroadcastTo\" op", "body": "This PR enables ROCm support for \"BroadcastTo\" op.", "comments": ["Not too familiar with ROCm, but if this PR does not impact CPU/CUDA build (I think it doe not impact) then it looks good to me."]}, {"number": 26744, "title": "[ROCm] Enable ROCm support for \"Betainc\" op", "body": "This PR enables ROCm support for \"Betainc\" op.", "comments": []}, {"number": 26743, "title": "[ROCm] Enable ROCm support for \"BatchToSpace BatchToSpaceND\" op", "body": "This PR enables ROCm support for \"BatchToSpace BatchToSpaceND\" op.", "comments": ["@whchung can you please check build failures ", "@rthadur I couldn't really check as there is no URLs for those failed targets. The best I can do is to restart another kokoro run."]}, {"number": 26742, "title": "[ROCm] Enable ROCm support for \"ArgMax ArgMin\" op", "body": "This PR enables ROCm support for \"ArgMax ArgMin\" op.", "comments": []}, {"number": 26741, "title": "Added Median Filtering with 2D filter (User's choice in shape of the filter)", "body": "Added Median Filtering with 2D filter. \r\nAdded Median Filtering with 1D filter in pull request id #26328\r\n\r\nTest Code -\r\n\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot\r\nimport matplotlib.pyplot as plt\r\nfrom tensorflow.python.ops import array_ops\r\nfrom tensorflow.python.util.tf_export import tf_export\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.client import session\r\n\r\n\r\nfname = 'index3.jpeg'\r\nimg = matplotlib.pyplot.imread(fname)\r\nimport numpy as np\r\n\r\n\r\ntf_img = tf.convert_to_tensor(img)\r\n\r\n\r\n@tf_export('image.median_filtering_2D')\r\ndef median_filtering_2D(tf_img,filter_shapex=3,filter_shapey=3):\r\n    #Default Filter Size = 3\r\n    try:\r\n        m,no = int(tf_img.shape[0]),int(tf_img.shape[1])\r\n    except:\r\n        raise Exception(\"Input Tensor is not an image\")\r\n    try :\r\n        ch = int(tf_img.shape[2])\r\n    except:\r\n        ch = 1\r\n        tf_img = array_ops.reshape(tf_img, [m,no,ch])\r\n    if m < filter_shapex or no < filter_shapey:\r\n        raise Exception('No of Pixels in the image should be more than the filter size')\r\n    if filter_shapex % 2 == 0 or filter_shapey % 2 == 0:\r\n        raise Exception(\"Filter size should be odd\")\r\n    sess = session.InteractiveSession()\r\n    tf_img = tf_img.eval()\r\n    tf_img = tf_img.astype('float64')\r\n    tf_i = tf_img.reshape(m*no*ch)\r\n    maxi = max(tf_i)\r\n    if maxi == 1:\r\n        tf_img /= maxi\r\n    else :\r\n        tf_img /= 255\r\n    #k is the Zero-padding size\r\n    res = np.empty((m,no,ch))\r\n    for a in range(ch):\r\n        img = tf_img[:,:,a:a+1]\r\n        img = img.reshape(m,no)\r\n        k = (filter_shapex - 1)\r\n        l = filter_shapey - 1\r\n        img = tf.convert_to_tensor(img)\r\n        img  = tf.pad(img,tf.constant([[k / 2, k / 2], [l / 2,l / 2]]),'CONSTANT')\r\n        img = img.eval()\r\n        res1 = np.empty((m,no))\r\n        for i in range(img.shape[0] - k) :\r\n            for j in range(img.shape[1] - l) :\r\n                li = []\r\n                for b in range(i, i + filter_shapex):\r\n                    for d in range(j, j + filter_shapey):\r\n                        li.append(img[b][d])\r\n                li.sort()\r\n                res1[i][j] = li[len(li)/2]\r\n        res1 = res1.reshape(m,no,1)\r\n        res[:,:,a:a+1] = res1\r\n\r\n    res *= 255\r\n    res = res.astype('int')\r\n    res = ops.convert_to_tensor(res)\r\n    sess.close()\r\n    return res\r\n\r\nmimage = median_filtering_2D(tf_img,5,9)\r\nsess = tf.InteractiveSession()\r\nfig = plt.figure()\r\nfig.add_subplot()\r\nplt.imshow(img,cmap='gray')\r\nplt.show()\r\nmimage = mimage.eval()\r\nfig.add_subplot()\r\nif mimage.shape[2] == 1:\r\n    mimage = mimage.reshape(mimage.shape[0],mimage.shape[1])\r\nplt.imshow(mimage,cmap = 'gray')\r\nplt.show()", "comments": ["@seanpmorgan: do you think this belongs in addons?\r\n\r\nAlso, I'd like to see a test, regardless of whether we put this in core tf or in addons first.", "Apologies for the delayed answer. Yes I do think that this could have a home in TF-Addons, as we've already started to move some [image processing contributions from tf.contrib](https://github.com/tensorflow/addons/tree/master/tensorflow_addons/image).\r\n\r\nJust some things to note:\r\n- As Alex mentioned, good test coverage would be essential\r\n- We're going to publish an RFC for Maintainership of subpackages very shortly. Addons-Image is one of the subpackages we're looking to find a set of maintainers for so it'll ultimately be up to those maintainers to decide the significance of the contribution (my first impression is that it's certainly useful though)", "*@alextp  *Can you suggest a method such that  i can get the values of the\ntensor without using tf.eval which requires session or without using eager\nexecution. We need values of the tensors to get the median value.\n\nOn Mon 25 Mar, 2019, 11:15 PM Alexandre Passos, <notifications@github.com>\nwrote:\n\n> *@alextp* requested changes on this pull request.\n>\n> Thanks for the contribution!\n>\n> As it is I do not think this is appropriate for tensorflow, for the\n> reasons I outline in the comments below.\n>\n> That said, if you want to clean it up and reopen the pull request after\n> removing sessions from op code, it should be fine.\n> ------------------------------\n>\n> In tensorflow/python/ops/image_ops_impl.py\n> <https://github.com/tensorflow/tensorflow/pull/26741#discussion_r268761832>\n> :\n>\n> > @@ -40,6 +40,7 @@\n>  from tensorflow.python.ops import variables\n>  from tensorflow.python.util import deprecation\n>  from tensorflow.python.util.tf_export import tf_export\n> +from tensorflow.python.client import session\n>\n> Implementations of TensorFlow operations are not allowed to depend on\n> sessions.\n>\n> Operations need to support eager execution, function building, and a few\n> other contexts in which a session does not exist.\n>\n> Also the goal of operation code is to build a graph that will be executed\n> later; using a session here violates this and can have weird unforeseen\n> consequences.\n> ------------------------------\n>\n> In tensorflow/python/ops/image_ops_impl.py\n> <https://github.com/tensorflow/tensorflow/pull/26741#discussion_r268762341>\n> :\n>\n> > @@ -3541,3 +3542,63 @@ def combined_non_max_suppression(boxes,\n>      return gen_image_ops.combined_non_max_suppression(\n>          boxes, scores, max_output_size_per_class, max_total_size, iou_threshold,\n>          score_threshold, pad_per_class)\n> +\n> +\n> +@tf_export('image.median_filtering_2D')\n> +def median_filtering_2D(tf_img,filter_shapex=3,filter_shapey=3):\n>\n> The argument names should probably be (input, filter_shape) where\n> filter_shape is a 2-element tuple. The function should be named\n> median_filter_2d to match conv_2d and other things in the tf namespace.\n> ------------------------------\n>\n> In tensorflow/python/ops/image_ops_impl.py\n> <https://github.com/tensorflow/tensorflow/pull/26741#discussion_r268762488>\n> :\n>\n> > @@ -3541,3 +3542,63 @@ def combined_non_max_suppression(boxes,\n>      return gen_image_ops.combined_non_max_suppression(\n>          boxes, scores, max_output_size_per_class, max_total_size, iou_threshold,\n>          score_threshold, pad_per_class)\n> +\n> +\n> +@tf_export('image.median_filtering_2D')\n> +def median_filtering_2D(tf_img,filter_shapex=3,filter_shapey=3):\n> +    # This methods takes both 2D Tensor as well as 3D Tensor Images\n>\n> This comment should be a docstring so it's available in TF's generated\n> documentation.\n> ------------------------------\n>\n> In tensorflow/python/ops/image_ops_impl.py\n> <https://github.com/tensorflow/tensorflow/pull/26741#discussion_r268762987>\n> :\n>\n> > @@ -3541,3 +3542,63 @@ def combined_non_max_suppression(boxes,\n>      return gen_image_ops.combined_non_max_suppression(\n>          boxes, scores, max_output_size_per_class, max_total_size, iou_threshold,\n>          score_threshold, pad_per_class)\n> +\n> +\n> +@tf_export('image.median_filtering_2D')\n> +def median_filtering_2D(tf_img,filter_shapex=3,filter_shapey=3):\n> +    # This methods takes both 2D Tensor as well as 3D Tensor Images\n> +    # Other than Tensor it takes optional parameter filter_Size\n> +    # Default Filter Size = 3 , 3\n> +    # This Median Filtering is done by using 2D filters of user's choice\n> +    # Filter_size should be odd\n> +    # This method takes both kind of images where pixel values lie between 0 to 255 and where it lies between 0.0 and 1.0\n> +\n> +    try:\n> +        m,no = int(tf_img.shape[0]),int(tf_img.shape[1])\n> +    except:\n>\n> A naked try-except is really bad as it can mask errors unforeseen by the\n> original author, specially as code evolves.\n>\n> The message you suggest below, for example, does not match reality. The\n> input tensor can be an image of partially unknown shape and the exception\n> will fire; similarly it can be not an image and the exception will not fire.\n> ------------------------------\n>\n> In tensorflow/python/ops/image_ops_impl.py\n> <https://github.com/tensorflow/tensorflow/pull/26741#discussion_r268763430>\n> :\n>\n> > +\n> +@tf_export('image.median_filtering_2D')\n> +def median_filtering_2D(tf_img,filter_shapex=3,filter_shapey=3):\n> +    # This methods takes both 2D Tensor as well as 3D Tensor Images\n> +    # Other than Tensor it takes optional parameter filter_Size\n> +    # Default Filter Size = 3 , 3\n> +    # This Median Filtering is done by using 2D filters of user's choice\n> +    # Filter_size should be odd\n> +    # This method takes both kind of images where pixel values lie between 0 to 255 and where it lies between 0.0 and 1.0\n> +\n> +    try:\n> +        m,no = int(tf_img.shape[0]),int(tf_img.shape[1])\n> +    except:\n> +        raise Exception(\"Input Tensor is not an image\")\n> +    try :\n> +        ch = int(tf_img.shape[2])\n>\n> Most tensorflow operations have to support two data layouts, NHWC\n> (channels-last) and NCHW (channels-first).\n>\n> Ideally all image-manipulating operations would support both.\n>\n> Also the batch dimension is important for performance.\n> ------------------------------\n>\n> In tensorflow/python/ops/image_ops_impl.py\n> <https://github.com/tensorflow/tensorflow/pull/26741#discussion_r268767082>\n> :\n>\n> > +    # Default Filter Size = 3 , 3\n> +    # This Median Filtering is done by using 2D filters of user's choice\n> +    # Filter_size should be odd\n> +    # This method takes both kind of images where pixel values lie between 0 to 255 and where it lies between 0.0 and 1.0\n> +\n> +    try:\n> +        m,no = int(tf_img.shape[0]),int(tf_img.shape[1])\n> +    except:\n> +        raise Exception(\"Input Tensor is not an image\")\n> +    try :\n> +        ch = int(tf_img.shape[2])\n> +    except:\n> +        ch = 1\n> +        tf_img = array_ops.reshape(tf_img, [m,no,ch])\n> +    if m < filter_shapex or no < filter_shapey:\n> +        raise Exception('No of Pixels in the image should be more than the filter size')\n>\n> We use ValueError in TensorFlow for invalid inputs to operations.\n> ------------------------------\n>\n> In tensorflow/python/ops/image_ops_impl.py\n> <https://github.com/tensorflow/tensorflow/pull/26741#discussion_r268767605>\n> :\n>\n> > +\n> +    try:\n> +        m,no = int(tf_img.shape[0]),int(tf_img.shape[1])\n> +    except:\n> +        raise Exception(\"Input Tensor is not an image\")\n> +    try :\n> +        ch = int(tf_img.shape[2])\n> +    except:\n> +        ch = 1\n> +        tf_img = array_ops.reshape(tf_img, [m,no,ch])\n> +    if m < filter_shapex or no < filter_shapey:\n> +        raise Exception('No of Pixels in the image should be more than the filter size')\n> +    if filter_shapex % 2 == 0 or filter_shapey % 2 == 0:\n> +        raise Exception(\"Filter size should be odd\")\n> +    sess = session.InteractiveSession()\n> +    tf_img = tf_img.eval()\n>\n> So this is not actually building a tensorflow graph to do anything useful\n> but instead it's escaping to numpy and doing the computation there.\n>\n> You can achieve this as part of a tf graph by using tf.py_func /\n> tf.numpy_function, but I encourage you instead to try to rewrite this in\n> terms of graph operations.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/26741#pullrequestreview-218475109>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AiI2Qun_OQZx3Bzbor2jCw6mdDZ3C2Vuks5vaQscgaJpZM4b2WMV>\n> .\n>\n", "@Mainak431 One option is to use tf.py_func which does not require eval.\r\n\r\nAnother option is to write the median computation code in tf proper; a naive and slow implementation is possible using tf.top_k; you can also use [tfp.stats.percentile](https://www.tensorflow.org/probability/api_docs/python/tfp/stats/percentile) for a bit more idiomatic code.", "@alextp . Please review pull request #27140 "]}, {"number": 26740, "title": "run bidirectional_sequence_lstm_test.py error", "body": "clone code from git and cd tensorflow/tensorflow/lite/experimental/examples/lstm dir \r\nrun python bidirectional_sequence_lstm_test.py get errors:\r\n\r\nTraceback (most recent call last):\r\n  File \"bidirectional_sequence_lstm_test.py\", line 23, in <module>\r\n    from tensorflow.lite.experimental.examples.lstm.rnn import bidirectional_dynamic_rnn\r\nImportError: No module named lite.experimental.examples.lstm.rnn", "comments": ["bazel run tensorflow/lite/experimental/examples/lstm:bidirectional_sequence_lstm_test"]}, {"number": 26739, "title": "[doc] use links in deprecation notice", "body": "Many deprecated functions currently contain a notice such as\r\n```\r\nWarning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.data.experimental.ignore_errors().\r\n```\r\nIt would be really nice if the suggested replacement would actually link to the corresponding documentation.", "comments": ["@ngc92 I think this was resolved. As you requested, the deprecation warning includes link to the corresponding documentation. For example, please check [this page](https://www.tensorflow.org/api_docs/python/tf/compat/v1/data/Dataset) on TF website. Thanks!\r\n\r\nI am closing this issue as this was resolved. Feel free to reopen if I am mistaken. Thanks!"]}, {"number": 26738, "title": "TF 2.0 Keras model utilizing another model with metrics cannot fit/evaluate in graph mode (mistakenly using the metrics of the inner model)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **YES**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Debian Stable**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **CPU, both TF-2.0.0a0 and tf-nightly-2.0-preview-2.0.0.dev20190315**\r\n- Python version: **3.5.3**\r\n- Bazel version (if compiling from source): **N/A**\r\n- GCC/Compiler version (if compiling from source): **N/A**\r\n- CUDA/cuDNN version: **N/A**\r\n- GPU model and memory: **N/A**\r\n\r\n**Describe the current behavior**\r\nWhen running the script below, it fails during execution with error\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'dense_target' with dtype float and shape [?,?]\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe code should not crash.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ninputs = np.arange(10)\r\noutputs = 2 * inputs\r\n\r\ninner_model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=[1])])\r\ninner_model.compile(optimizer=tf.keras.optimizers.Adam(),\r\n                    loss=tf.keras.losses.MeanSquaredError(),\r\n                    metrics=[tf.keras.metrics.MeanSquaredError()])\r\n\r\nouter_inputs = tf.keras.layers.Input(shape=[1])\r\nouter_outputs = inner_model(outer_inputs)\r\nouter_model = tf.keras.Model(inputs=outer_inputs, outputs=outer_outputs)\r\nouter_model.compile(optimizer=tf.keras.optimizers.Adam(),\r\n                    loss=tf.keras.losses.MeanSquaredError(),\r\n                    metrics=[tf.keras.metrics.MeanSquaredError()])\r\nouter_model.evaluate(inputs, outputs) # crashes\r\nouter_model.fit(inputs, outputs) # also crashes\r\n```\r\n\r\n\r\n**Other info / logs**\r\n- `outer_model.predict` works\r\n- when `outer_model.run_eagerly=True`, then `outer_model.{evaluate/fit}` works\r\n- when the inner model has no metrics (`metrics=[]` in `inner_model.compile`), then `outer_model.{evaluate/fit}` works\r\n\r\nI have trace the problem to an incorrect FuncGraph generated -- it mistakenly uses the metrics from the _inner graph_ (i.e., the metrics calculations utilize the _original_ placeholders of the inner graph).\r\n", "comments": ["The name conflict is further supported by the fact that naming the metric in the `outer_model` differently, as in\r\n```python\r\nouter_model.compile(optimizer=tf.keras.optimizers.Adam(),\r\n                    loss=tf.keras.losses.MeanSquaredError(),\r\n                    metrics=[tf.keras.metrics.MeanSquaredError(name=\"outer_model\")])\r\n```\r\nalso makes `outer_model.{evaluate/fit}` work.", "Is there a workaround for this? (Preferably without using Sequential() )", "https://github.com/tensorflow/tensorflow/commit/659c981a3556c6424237eacd0bf4cdc86f228f16 should fix this issue. Please give it a try in the next nightly and let me know if it works as expected.\r\n\r\nThank you!", "I can confirm that `tf-nightly==1.14.1.dev190623` fixes the issue, and that `tf-nightly==1.14.1.dev190620` was still broken.", "Great, thank you for verifying!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26738\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26738\">No</a>\n"]}, {"number": 26737, "title": "Added Activation Scenarios in the file.", "body": "This was one of the TODOs in the file.", "comments": []}, {"number": 26736, "title": "tflite's TRANSPOSE_CONV is much slower than tfmobile ...", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:Zenfone 5Z\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (use command below): nightly build 20190314\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):0.21.0\r\n- GCC/Compiler version (if compiling from source):5.4\r\n- CUDA/cuDNN version:not used\r\n- GPU model and memory:not used\r\n\r\n**Describe the current behavior**\r\n\r\nI measured performance of tf-mobile, tf-lite on Zenfone 5Z  / Snapdragon 845 / Android 8.0 by C++ benchmark model tool (arm64 build), and I find that the speed of tf-lite's TRANSPOSE_CONV is much slower than tf-mobile's one.\r\n\r\nI used the attached custom model [models.zip](https://github.com/tensorflow/tensorflow/files/2970561/models.zip)  for benchmark .\r\nThe attached tflite is converted by toco_convert from the attached pb file.\r\n\r\n**Summary**\r\n\r\nThe following table shows average computing time of 50 times predict.\r\n\r\n|              | Threads|Conv2D     | TransposeConv2D | All        |\r\n|:-------------|--------:|-----------:|----------------:|-----------:|\r\n| TFMobile     | 1|251.561 ms |       35.585 ms | 310.380 ms |\r\n| TFMobile     | 4|190.228 ms |       78.047 ms | 295.469 ms |\r\n| TFMobile     | 16|87.586 ms |       20.264 ms | 122.102 ms |\r\n| TFLite       | 1|294.214 ms |      562.609 ms | 880.674 ms |\r\n| TFLite       | 4|75.783 ms |      560.368 ms | 659.156 ms |\r\n| TFLite       | 16|55.597 ms |      561.441 ms | 641.541 ms |\r\n\r\n**TFMobile 1 threads In:1x256x256x3 Performance**\r\n`\r\nbenchmark_model --graph=/data/local/tmp/UpResnet5-X2-H256-W256-C3.pb --max_num_runs=50 --num_threads=1 --input_layer_shape=\"1,256,256,3\"\r\n`\r\n```\r\nnative : benchmark_model.cc:469 Graph: [/data/local/tmp/UpResnet5-X2-H256-W256-C3.pb]\r\nnative : benchmark_model.cc:470 Init ops:\r\nnative : benchmark_model.cc:471 Input layers: [input:0]\r\nnative : benchmark_model.cc:472 Input shapes: [1,256,256,3]\r\nnative : benchmark_model.cc:473 Input types: [float]\r\nnative : benchmark_model.cc:474 Output layers: [output:0]\r\nnative : benchmark_model.cc:475 Target layers: []\r\nnative : benchmark_model.cc:476 Num runs: [50]\r\nnative : benchmark_model.cc:477 Inter-inference delay (seconds): [-1.0]\r\nnative : benchmark_model.cc:478 Inter-benchmark delay (seconds): [-1.0]\r\nnative : benchmark_model.cc:480 Num threads: [1]\r\nnative : benchmark_model.cc:481 Benchmark name: []\r\nnative : benchmark_model.cc:482 Output prefix: []\r\nnative : benchmark_model.cc:483 Show sizes: [0]\r\nnative : benchmark_model.cc:484 Warmup runs: [1]\r\nnative : benchmark_model.cc:251 Loading TensorFlow.\r\nnative : benchmark_model.cc:258 Got config, 0 devices\r\ncan't determine number of CPU cores: assuming 4\r\nnative : benchmark_model.cc:496 Initialized session in 0.03605s\r\nnative : benchmark_model.cc:327 Running benchmark for max 1 iterations, max -1 seconds without detailed stat logging, with -1s sleep between inferences\r\nnative : benchmark_model.cc:361 count=1 curr=637203\r\n\r\nnative : benchmark_model.cc:327 Running benchmark for max 50 iterations, max 10 seconds without detailed stat logging, with -1s sleep between inferences\r\nnative : benchmark_model.cc:361 count=30 first=616452 curr=309802 min=309536 max=618552 avg=342551 std=92067\r\n\r\nnative : benchmark_model.cc:327 Running benchmark for max 50 iterations, max 10 seconds with detailed stat logging, with -1s sleep between inferences\r\nnative : benchmark_model.cc:361 count=33 first=310795 curr=311214 min=310428 max=311479 avg=310885 std=265\r\n\r\nnative : benchmark_model.cc:600 Average inference timings in us: Warmup: 637203, no stats: 342550, with stats: 310885\r\nnative : stat_summarizer.cc:85 ============================== Run Order ==============================\r\nnative : stat_summarizer.cc:85 \t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\nnative : stat_summarizer.cc:85 \t                    NoOp\t            0.000\t    0.014\t    0.010\t  0.003%\t  0.003%\t     0.000\t        1\t_SOURCE\r\nnative : stat_summarizer.cc:85 \t                   Const\t            0.017\t    0.016\t    0.009\t  0.003%\t  0.006%\t     0.000\t        1\tConst\r\nnative : stat_summarizer.cc:85 \t                   Const\t            0.029\t    0.005\t    0.004\t  0.001%\t  0.007%\t     0.000\t        1\tconv2d_transpose/strided_slice/stack\r\nnative : stat_summarizer.cc:85 \t                   Const\t            0.034\t    0.004\t    0.004\t  0.001%\t  0.009%\t     0.000\t        1\tconv2d_transpose/strided_slice/stack_1\r\nnative : stat_summarizer.cc:85 \t                   Const\t            0.040\t    0.004\t    0.004\t  0.001%\t  0.010%\t     0.000\t        1\tconv2d_transpose/strided_slice/stack_2\r\nnative : stat_summarizer.cc:85 \t                   Const\t            0.046\t    0.003\t    0.003\t  0.001%\t  0.011%\t     0.000\t        1\tconv2d_transpose/strided_slice_1/stack\r\nnative : stat_summarizer.cc:85 \t                   Const\t            0.051\t    0.003\t    0.003\t  0.001%\t  0.012%\t     0.000\t        1\tconv2d_transpose/strided_slice_1/stack_1\r\nnative : stat_summarizer.cc:85 \t                   Const\t            0.055\t    0.004\t    0.003\t  0.001%\t  0.013%\t     0.000\t        1\tconv2d_transpose/strided_slice_1/stack_2\r\nnative : stat_summarizer.cc:85 \t                   Const\t            0.059\t    0.003\t    0.003\t  0.001%\t  0.014%\t     0.000\t        1\tconv2d_transpose/strided_slice_2/stack\r\nnative : stat_summarizer.cc:85 \t                   Const\t            0.064\t    0.004\t    0.003\t  0.001%\t  0.015%\t     0.000\t        1\tconv2d_transpose/strided_slice_2/stack_1\r\nnative : stat_summarizer.cc:85 \t                   Const\t            0.069\t    0.003\t    0.003\t  0.001%\t  0.016%\t     0.000\t        1\tconv2d_transpose/strided_slice_2/stack_2\r\nnative : stat_summarizer.cc:85 \t                   Const\t            0.074\t    0.003\t    0.003\t  0.001%\t  0.017%\t     0.000\t        1\tconv2d_transpose/mul/y\r\nnative : stat_summarizer.cc:85 \t                   Const\t            0.078\t    0.004\t    0.003\t  0.001%\t  0.018%\t     0.000\t        1\tconv2d_transpose/mul_1/y\r\nnative : stat_summarizer.cc:85 \t                   Const\t            0.083\t    0.004\t    0.003\t  0.001%\t  0.019%\t     0.000\t        1\tconv2d_transpose/stack/3\r\nnative : stat_summarizer.cc:85 \t                   Const\t            0.087\t    0.003\t    0.003\t  0.001%\t  0.020%\t     0.000\t        1\toutput/Minimum/y\r\nnative : stat_summarizer.cc:85 \t                   Const\t            0.092\t    0.003\t    0.003\t  0.001%\t  0.021%\t     0.000\t        1\toutput/y\r\nnative : stat_summarizer.cc:85 \t                    _Arg\t            0.096\t    0.005\t    0.003\t  0.001%\t  0.022%\t     0.000\t        1\t_arg_input_0_0\r\nnative : stat_summarizer.cc:85 \t               MirrorPad\t            0.103\t    2.747\t    2.678\t  0.863%\t  0.884%\t   811.200\t        1\tMirrorPad\r\nnative : stat_summarizer.cc:85 \t                   Const\t            2.802\t    0.007\t    0.006\t  0.002%\t  0.886%\t     0.000\t        1\tbatch_normalization/beta/read/_0__cf__0\r\nnative : stat_summarizer.cc:85 \t                   Const\t            2.810\t    0.004\t    0.004\t  0.001%\t  0.887%\t     0.000\t        1\tbatch_normalization/gamma/read/_1__cf__1\r\nnative : stat_summarizer.cc:85 \t                   Const\t            2.815\t    0.004\t    0.004\t  0.001%\t  0.889%\t     0.000\t        1\tbatch_normalization/moving_mean/read/_2__cf__2\r\nnative : stat_summarizer.cc:85 \t                   Const\t            2.820\t    0.004\t    0.003\t  0.001%\t  0.890%\t     0.000\t        1\tbatch_normalization/moving_variance/read/_3__cf__3\r\nnative : stat_summarizer.cc:85 \t                   Const\t            2.825\t    0.005\t    0.003\t  0.001%\t  0.891%\t     0.000\t        1\tbatch_normalization_1/beta/read/_4__cf__4\r\nnative : stat_summarizer.cc:85 \t                   Const\t            2.830\t    0.003\t    0.003\t  0.001%\t  0.892%\t     0.000\t        1\tbatch_normalization_1/gamma/read/_5__cf__5\r\nnative : stat_summarizer.cc:85 \t                   Const\t            2.835\t    0.004\t    0.004\t  0.001%\t  0.893%\t     0.000\t        1\tbatch_normalization_1/moving_mean/read/_6__cf__6\r\nnative : stat_summarizer.cc:85 \t                   Const\t            2.840\t    0.004\t    0.003\t  0.001%\t  0.894%\t     0.000\t        1\tbatch_normalization_1/moving_variance/read/_7__cf__7\r\nnative : stat_summarizer.cc:85 \t                   Const\t            2.845\t    0.006\t    0.005\t  0.002%\t  0.896%\t     0.000\t        1\tconv2d/Conv2D/ReadVariableOp/_8__cf__8\r\nnative : stat_summarizer.cc:85 \t                  Conv2D\t            2.852\t   31.104\t   31.094\t 10.018%\t 10.914%\t  6390.144\t        1\tconv2d/Conv2D\r\nnative : stat_summarizer.cc:85 \t                    Relu\t           33.975\t    0.855\t    0.864\t  0.279%\t 11.192%\t     0.000\t        1\tactivation/Relu\r\nnative : stat_summarizer.cc:85 \t          FusedBatchNorm\t           34.849\t    8.088\t    8.000\t  2.577%\t 13.770%\t  6390.528\t        1\tbatch_normalization/FusedBatchNorm\r\nnative : stat_summarizer.cc:85 \t                   Const\t           42.882\t    0.012\t    0.012\t  0.004%\t 13.774%\t     0.000\t        1\tconv2d_1/Conv2D/ReadVariableOp/_9__cf__9\r\nnative : stat_summarizer.cc:85 \t                  Conv2D\t           42.897\t   57.605\t   57.749\t 18.606%\t 32.379%\t  6390.144\t        1\tconv2d_1/Conv2D\r\nnative : stat_summarizer.cc:85 \t          FusedBatchNorm\t          100.677\t    6.175\t    6.276\t  2.022%\t 34.401%\t     0.384\t        1\tbatch_normalization_1/FusedBatchNorm\r\nnative : stat_summarizer.cc:85 \t                    Relu\t          106.967\t    0.854\t    0.866\t  0.279%\t 34.680%\t     0.000\t        1\tactivation_1/Relu\r\nnative : stat_summarizer.cc:85 \t                   Const\t          107.846\t    0.016\t    0.015\t  0.005%\t 34.685%\t     0.000\t        1\tconv2d_2/Conv2D/ReadVariableOp/_10__cf__10\r\nnative : stat_summarizer.cc:85 \t                  Conv2D\t          107.865\t   57.759\t   57.751\t 18.607%\t 53.291%\t  6390.144\t        1\tconv2d_2/Conv2D\r\nnative : stat_summarizer.cc:85 \t                     Add\t          165.648\t    1.811\t    1.839\t  0.592%\t 53.884%\t     0.000\t        1\tadd\r\nnative : stat_summarizer.cc:85 \t                   Const\t          167.501\t    0.013\t    0.014\t  0.005%\t 53.889%\t     0.000\t        1\tconv2d_3/Conv2D/ReadVariableOp/_11__cf__11\r\nnative : stat_summarizer.cc:85 \t                  Conv2D\t          167.521\t  104.822\t  104.968\t 33.819%\t 87.708%\t 12582.912\t        1\tconv2d_3/Conv2D\r\nnative : stat_summarizer.cc:85 \t                    Relu\t          272.521\t    1.732\t    1.741\t  0.561%\t 88.269%\t     0.000\t        1\tactivation_2/Relu\r\nnative : stat_summarizer.cc:85 \t                   Shape\t          274.272\t    0.026\t    0.024\t  0.008%\t 88.276%\t     0.016\t        1\tconv2d_transpose/Shape\r\nnative : stat_summarizer.cc:85 \t            StridedSlice\t          274.303\t    0.022\t    0.024\t  0.008%\t 88.284%\t     0.004\t        1\tconv2d_transpose/strided_slice\r\nnative : stat_summarizer.cc:85 \t            StridedSlice\t          274.335\t    0.021\t    0.009\t  0.003%\t 88.287%\t     0.004\t        1\tconv2d_transpose/strided_slice_1\r\nnative : stat_summarizer.cc:85 \t                     Mul\t          274.349\t    0.012\t    0.011\t  0.003%\t 88.291%\t     0.000\t        1\tconv2d_transpose/mul\r\nnative : stat_summarizer.cc:85 \t            StridedSlice\t          274.363\t    0.008\t    0.011\t  0.003%\t 88.294%\t     0.004\t        1\tconv2d_transpose/strided_slice_2\r\nnative : stat_summarizer.cc:85 \t                     Mul\t          274.378\t    0.005\t    0.004\t  0.001%\t 88.295%\t     0.000\t        1\tconv2d_transpose/mul_1\r\nnative : stat_summarizer.cc:85 \t                    Pack\t          274.385\t    0.015\t    0.016\t  0.005%\t 88.301%\t     0.016\t        1\tconv2d_transpose/stack\r\nnative : stat_summarizer.cc:85 \t                   Const\t          274.407\t    0.010\t    0.009\t  0.003%\t 88.304%\t     0.000\t        1\tconv2d_transpose/conv2d_transpose/ReadVariableOp/_12__cf__12\r\nnative : stat_summarizer.cc:85 \t     Conv2DBackpropInput\t          274.419\t   35.577\t   35.586\t 11.465%\t 99.769%\t 15728.640\t        1\tconv2d_transpose/conv2d_transpose\r\nnative : stat_summarizer.cc:85 \t                 Minimum\t          310.038\t    0.359\t    0.370\t  0.119%\t 99.888%\t     0.000\t        1\toutput/Minimum\r\nnative : stat_summarizer.cc:85 \t                 Maximum\t          310.415\t    0.424\t    0.338\t  0.109%\t 99.997%\t     0.000\t        1\toutput\r\nnative : stat_summarizer.cc:85 \t                 _Retval\t          310.759\t    0.010\t    0.010\t  0.003%\t100.000%\t     0.000\t        1\t_retval_output_0_0\r\nnative : stat_summarizer.cc:85 \r\nnative : stat_summarizer.cc:85 ============================== Top by Computation Time ==============================\r\nnative : stat_summarizer.cc:85 \t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\nnative : stat_summarizer.cc:85 \t                  Conv2D\t          167.521\t  104.822\t  104.968\t 33.819%\t 33.819%\t 12582.912\t        1\tconv2d_3/Conv2D\r\nnative : stat_summarizer.cc:85 \t                  Conv2D\t          107.865\t   57.759\t   57.751\t 18.607%\t 52.426%\t  6390.144\t        1\tconv2d_2/Conv2D\r\nnative : stat_summarizer.cc:85 \t                  Conv2D\t           42.897\t   57.605\t   57.749\t 18.606%\t 71.032%\t  6390.144\t        1\tconv2d_1/Conv2D\r\nnative : stat_summarizer.cc:85 \t     Conv2DBackpropInput\t          274.419\t   35.577\t   35.586\t 11.465%\t 82.497%\t 15728.640\t        1\tconv2d_transpose/conv2d_transpose\r\nnative : stat_summarizer.cc:85 \t                  Conv2D\t            2.852\t   31.104\t   31.094\t 10.018%\t 92.515%\t  6390.144\t        1\tconv2d/Conv2D\r\nnative : stat_summarizer.cc:85 \t          FusedBatchNorm\t           34.849\t    8.088\t    8.000\t  2.577%\t 95.092%\t  6390.528\t        1\tbatch_normalization/FusedBatchNorm\r\nnative : stat_summarizer.cc:85 \t          FusedBatchNorm\t          100.677\t    6.175\t    6.276\t  2.022%\t 97.114%\t     0.384\t        1\tbatch_normalization_1/FusedBatchNorm\r\nnative : stat_summarizer.cc:85 \t               MirrorPad\t            0.103\t    2.747\t    2.678\t  0.863%\t 97.977%\t   811.200\t        1\tMirrorPad\r\nnative : stat_summarizer.cc:85 \t                     Add\t          165.648\t    1.811\t    1.839\t  0.592%\t 98.570%\t     0.000\t        1\tadd\r\nnative : stat_summarizer.cc:85 \t                    Relu\t          272.521\t    1.732\t    1.741\t  0.561%\t 99.131%\t     0.000\t        1\tactivation_2/Relu\r\nnative : stat_summarizer.cc:85 \r\nnative : stat_summarizer.cc:85 ============================== Top by Memory Use ==============================\r\nnative : stat_summarizer.cc:85 \t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\nnative : stat_summarizer.cc:85 \t     Conv2DBackpropInput\t          274.419\t   35.577\t   35.586\t 11.465%\t 11.465%\t 15728.640\t        1\tconv2d_transpose/conv2d_transpose\r\nnative : stat_summarizer.cc:85 \t                  Conv2D\t          167.521\t  104.822\t  104.968\t 33.819%\t 45.285%\t 12582.912\t        1\tconv2d_3/Conv2D\r\nnative : stat_summarizer.cc:85 \t          FusedBatchNorm\t           34.849\t    8.088\t    8.000\t  2.577%\t 47.862%\t  6390.528\t        1\tbatch_normalization/FusedBatchNorm\r\nnative : stat_summarizer.cc:85 \t                  Conv2D\t          107.865\t   57.759\t   57.751\t 18.607%\t 66.469%\t  6390.144\t        1\tconv2d_2/Conv2D\r\nnative : stat_summarizer.cc:85 \t                  Conv2D\t           42.897\t   57.605\t   57.749\t 18.606%\t 85.074%\t  6390.144\t        1\tconv2d_1/Conv2D\r\nnative : stat_summarizer.cc:85 \t                  Conv2D\t            2.852\t   31.104\t   31.094\t 10.018%\t 95.092%\t  6390.144\t        1\tconv2d/Conv2D\r\nnative : stat_summarizer.cc:85 \t               MirrorPad\t            0.103\t    2.747\t    2.678\t  0.863%\t 95.955%\t   811.200\t        1\tMirrorPad\r\nnative : stat_summarizer.cc:85 \t          FusedBatchNorm\t          100.677\t    6.175\t    6.276\t  2.022%\t 97.977%\t     0.384\t        1\tbatch_normalization_1/FusedBatchNorm\r\nnative : stat_summarizer.cc:85 \t                   Shape\t          274.272\t    0.026\t    0.024\t  0.008%\t 97.985%\t     0.016\t        1\tconv2d_transpose/Shape\r\nnative : stat_summarizer.cc:85 \t                    Pack\t          274.385\t    0.015\t    0.016\t  0.005%\t 97.990%\t     0.016\t        1\tconv2d_transpose/stack\r\nnative : stat_summarizer.cc:85 \r\nnative : stat_summarizer.cc:85 Number of nodes executed: 52\r\nnative : stat_summarizer.cc:85 ============================== Summary by node type ==============================\r\nnative : stat_summarizer.cc:85 \t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\nnative : stat_summarizer.cc:85 \t                  Conv2D\t        4\t   251.561\t    81.056%\t    81.056%\t 31753.344\t        4\r\nnative : stat_summarizer.cc:85 \t     Conv2DBackpropInput\t        1\t    35.585\t    11.466%\t    92.522%\t 15728.640\t        1\r\nnative : stat_summarizer.cc:85 \t          FusedBatchNorm\t        2\t    14.275\t     4.600%\t    97.121%\t  6390.912\t        2\r\nnative : stat_summarizer.cc:85 \t                    Relu\t        3\t     3.469\t     1.118%\t    98.239%\t     0.000\t        3\r\nnative : stat_summarizer.cc:85 \t               MirrorPad\t        1\t     2.677\t     0.863%\t    99.102%\t   811.200\t        1\r\nnative : stat_summarizer.cc:85 \t                     Add\t        1\t     1.839\t     0.593%\t    99.694%\t     0.000\t        1\r\nnative : stat_summarizer.cc:85 \t                 Minimum\t        1\t     0.369\t     0.119%\t    99.813%\t     0.000\t        1\r\nnative : stat_summarizer.cc:85 \t                 Maximum\t        1\t     0.337\t     0.109%\t    99.922%\t     0.000\t        1\r\nnative : stat_summarizer.cc:85 \t                   Const\t       28\t     0.126\t     0.041%\t    99.962%\t     0.000\t       28\r\nnative : stat_summarizer.cc:85 \t            StridedSlice\t        3\t     0.041\t     0.013%\t    99.976%\t     0.012\t        3\r\nnative : stat_summarizer.cc:85 \t                   Shape\t        1\t     0.024\t     0.008%\t    99.983%\t     0.016\t        1\r\nnative : stat_summarizer.cc:85 \t                    Pack\t        1\t     0.016\t     0.005%\t    99.988%\t     0.016\t        1\r\nnative : stat_summarizer.cc:85 \t                     Mul\t        2\t     0.014\t     0.005%\t    99.993%\t     0.000\t        2\r\nnative : stat_summarizer.cc:85 \t                 _Retval\t        1\t     0.010\t     0.003%\t    99.996%\t     0.000\t        1\r\nnative : stat_summarizer.cc:85 \t                    NoOp\t        1\t     0.009\t     0.003%\t    99.999%\t     0.000\t        1\r\nnative : stat_summarizer.cc:85 \t                    _Arg\t        1\t     0.003\t     0.001%\t   100.000%\t     0.000\t        1\r\nnative : stat_summarizer.cc:85 \r\nnative : stat_summarizer.cc:85 Timings (microseconds): count=33 first=310208 curr=310701 min=309938 max=310969 avg=310380 std=264\r\nnative : stat_summarizer.cc:85 Memory (bytes): count=33 curr=54684140(all same)\r\nnative : stat_summarizer.cc:85 52 nodes observed\r\nnative : stat_summarizer.cc:85 \r\n```\r\n\r\n**TFMobile 16 threads In:1x256x256x3 Performance**\r\n`\r\nbenchmark_model --graph=/data/local/tmp/UpResnet5-X2-H256-W256-C3.pb --max_num_runs=50 --num_threads=16 --input_layer_shape=\"1,256,256,3\"\r\n`\r\n```\r\nnative : benchmark_model.cc:469 Graph: [/data/local/tmp/UpResnet5-X2-H256-W256-C3.pb]\r\nnative : benchmark_model.cc:470 Init ops:\r\nnative : benchmark_model.cc:471 Input layers: [input:0]\r\nnative : benchmark_model.cc:472 Input shapes: [1,256,256,3]\r\nnative : benchmark_model.cc:473 Input types: [float]\r\nnative : benchmark_model.cc:474 Output layers: [output:0]\r\nnative : benchmark_model.cc:475 Target layers: []\r\nnative : benchmark_model.cc:476 Num runs: [50]\r\nnative : benchmark_model.cc:477 Inter-inference delay (seconds): [-1.0]\r\nnative : benchmark_model.cc:478 Inter-benchmark delay (seconds): [-1.0]\r\nnative : benchmark_model.cc:480 Num threads: [16]\r\nnative : benchmark_model.cc:481 Benchmark name: []\r\nnative : benchmark_model.cc:482 Output prefix: []\r\nnative : benchmark_model.cc:483 Show sizes: [0]\r\nnative : benchmark_model.cc:484 Warmup runs: [1]\r\nnative : benchmark_model.cc:251 Loading TensorFlow.\r\nnative : benchmark_model.cc:258 Got config, 0 devices\r\ncan't determine number of CPU cores: assuming 4\r\nnative : benchmark_model.cc:496 Initialized session in 0.036933s\r\nnative : benchmark_model.cc:327 Running benchmark for max 1 iterations, max -1 seconds without detailed stat logging, with -1s sleep between inferences\r\nnative : benchmark_model.cc:361 count=1 curr=144572\r\n\r\nnative : benchmark_model.cc:327 Running benchmark for max 50 iterations, max 10 seconds without detailed stat logging, with -1s sleep between inferences\r\nnative : benchmark_model.cc:361 count=50 first=121095 curr=120424 min=118295 max=159110 avg=123218 std=7596\r\n\r\nnative : benchmark_model.cc:327 Running benchmark for max 50 iterations, max 10 seconds with detailed stat logging, with -1s sleep between inferences\r\nnative : benchmark_model.cc:361 count=50 first=120234 curr=123678 min=117632 max=144145 avg=122647 std=4286\r\n\r\nnative : benchmark_model.cc:600 Average inference timings in us: Warmup: 144572, no stats: 123217, with stats: 122646\r\nnative : stat_summarizer.cc:85 ============================== Run Order ==============================\r\nnative : stat_summarizer.cc:85 \t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\nnative : stat_summarizer.cc:85 \t                    NoOp\t            0.000\t    0.012\t    0.008\t  0.006%\t  0.006%\t     0.000\t        1\t_SOURCE\r\nnative : stat_summarizer.cc:85 \t                   Const\t            0.015\t    0.016\t    0.009\t  0.007%\t  0.014%\t     0.000\t        1\tConst\r\nnative : stat_summarizer.cc:85 \t                   Const\t            0.026\t    0.004\t    0.003\t  0.003%\t  0.016%\t     0.000\t        1\tconv2d_transpose/strided_slice/stack\r\nnative : stat_summarizer.cc:85 \t                   Const\t            0.030\t    0.004\t    0.004\t  0.003%\t  0.019%\t     0.000\t        1\tconv2d_transpose/strided_slice/stack_1\r\nnative : stat_summarizer.cc:85 \t                   Const\t            0.035\t    0.006\t    0.004\t  0.003%\t  0.022%\t     0.000\t        1\tconv2d_transpose/strided_slice/stack_2\r\nnative : stat_summarizer.cc:85 \t                   Const\t            0.040\t    0.005\t    0.003\t  0.003%\t  0.025%\t     0.000\t        1\tconv2d_transpose/strided_slice_1/stack\r\nnative : stat_summarizer.cc:85 \t                   Const\t            0.045\t    0.004\t    0.003\t  0.003%\t  0.027%\t     0.000\t        1\tconv2d_transpose/strided_slice_1/stack_1\r\nnative : stat_summarizer.cc:85 \t                   Const\t            0.049\t    0.003\t    0.003\t  0.003%\t  0.030%\t     0.000\t        1\tconv2d_transpose/strided_slice_1/stack_2\r\nnative : stat_summarizer.cc:85 \t                   Const\t            0.055\t    0.003\t    0.003\t  0.002%\t  0.033%\t     0.000\t        1\tconv2d_transpose/strided_slice_2/stack\r\nnative : stat_summarizer.cc:85 \t                   Const\t            0.059\t    0.004\t    0.003\t  0.002%\t  0.035%\t     0.000\t        1\tconv2d_transpose/strided_slice_2/stack_1\r\nnative : stat_summarizer.cc:85 \t                   Const\t            0.064\t    0.004\t    0.003\t  0.002%\t  0.037%\t     0.000\t        1\tconv2d_transpose/strided_slice_2/stack_2\r\nnative : stat_summarizer.cc:85 \t                   Const\t            0.068\t    0.004\t    0.003\t  0.002%\t  0.040%\t     0.000\t        1\tconv2d_transpose/mul/y\r\nnative : stat_summarizer.cc:85 \t                   Const\t            0.072\t    0.003\t    0.003\t  0.002%\t  0.042%\t     0.000\t        1\tconv2d_transpose/mul_1/y\r\nnative : stat_summarizer.cc:85 \t                   Const\t            0.077\t    0.003\t    0.003\t  0.002%\t  0.044%\t     0.000\t        1\tconv2d_transpose/stack/3\r\nnative : stat_summarizer.cc:85 \t                   Const\t            0.081\t    0.004\t    0.003\t  0.002%\t  0.046%\t     0.000\t        1\toutput/Minimum/y\r\nnative : stat_summarizer.cc:85 \t                   Const\t            0.085\t    0.004\t    0.003\t  0.002%\t  0.049%\t     0.000\t        1\toutput/y\r\nnative : stat_summarizer.cc:85 \t                    _Arg\t            0.089\t    0.004\t    0.003\t  0.003%\t  0.051%\t     0.000\t        1\t_arg_input_0_0\r\nnative : stat_summarizer.cc:85 \t               MirrorPad\t            0.095\t    1.112\t    1.351\t  1.107%\t  1.158%\t   811.200\t        1\tMirrorPad\r\nnative : stat_summarizer.cc:85 \t                   Const\t            1.471\t    0.006\t    0.006\t  0.005%\t  1.163%\t     0.000\t        1\tbatch_normalization/beta/read/_0__cf__0\r\nnative : stat_summarizer.cc:85 \t                   Const\t            1.479\t    0.005\t    0.004\t  0.003%\t  1.166%\t     0.000\t        1\tbatch_normalization/gamma/read/_1__cf__1\r\nnative : stat_summarizer.cc:85 \t                   Const\t            1.485\t    0.005\t    0.004\t  0.003%\t  1.170%\t     0.000\t        1\tbatch_normalization/moving_mean/read/_2__cf__2\r\nnative : stat_summarizer.cc:85 \t                   Const\t            1.491\t    0.005\t    0.003\t  0.003%\t  1.172%\t     0.000\t        1\tbatch_normalization/moving_variance/read/_3__cf__3\r\nnative : stat_summarizer.cc:85 \t                   Const\t            1.496\t    0.006\t    0.004\t  0.003%\t  1.175%\t     0.000\t        1\tbatch_normalization_1/beta/read/_4__cf__4\r\nnative : stat_summarizer.cc:85 \t                   Const\t            1.501\t    0.008\t    0.004\t  0.003%\t  1.178%\t     0.000\t        1\tbatch_normalization_1/gamma/read/_5__cf__5\r\nnative : stat_summarizer.cc:85 \t                   Const\t            1.505\t    0.007\t    0.004\t  0.003%\t  1.181%\t     0.000\t        1\tbatch_normalization_1/moving_mean/read/_6__cf__6\r\nnative : stat_summarizer.cc:85 \t                   Const\t            1.511\t    0.006\t    0.003\t  0.003%\t  1.184%\t     0.000\t        1\tbatch_normalization_1/moving_variance/read/_7__cf__7\r\nnative : stat_summarizer.cc:85 \t                   Const\t            1.516\t    0.006\t    0.004\t  0.003%\t  1.187%\t     0.000\t        1\tconv2d/Conv2D/ReadVariableOp/_8__cf__8\r\nnative : stat_summarizer.cc:85 \t                  Conv2D\t            1.522\t    8.478\t    8.902\t  7.290%\t  8.477%\t  6390.144\t        1\tconv2d/Conv2D\r\nnative : stat_summarizer.cc:85 \t                    Relu\t           10.455\t    1.142\t    1.071\t  0.878%\t  9.355%\t     0.000\t        1\tactivation/Relu\r\nnative : stat_summarizer.cc:85 \t          FusedBatchNorm\t           11.540\t    3.001\t    2.754\t  2.256%\t 11.611%\t  6390.528\t        1\tbatch_normalization/FusedBatchNorm\r\nnative : stat_summarizer.cc:85 \t                   Const\t           14.332\t    0.011\t    0.012\t  0.010%\t 11.620%\t     0.000\t        1\tconv2d_1/Conv2D/ReadVariableOp/_9__cf__9\r\nnative : stat_summarizer.cc:85 \t                  Conv2D\t           14.348\t   23.520\t   23.289\t 19.073%\t 30.694%\t  6390.144\t        1\tconv2d_1/Conv2D\r\nnative : stat_summarizer.cc:85 \t          FusedBatchNorm\t           37.672\t    2.232\t    2.291\t  1.877%\t 32.570%\t     0.384\t        1\tbatch_normalization_1/FusedBatchNorm\r\nnative : stat_summarizer.cc:85 \t                    Relu\t           39.983\t    0.692\t    0.830\t  0.680%\t 33.250%\t     0.000\t        1\tactivation_1/Relu\r\nnative : stat_summarizer.cc:85 \t                   Const\t           40.828\t    0.012\t    0.012\t  0.010%\t 33.260%\t     0.000\t        1\tconv2d_2/Conv2D/ReadVariableOp/_10__cf__10\r\nnative : stat_summarizer.cc:85 \t                  Conv2D\t           40.844\t   23.039\t   23.440\t 19.197%\t 52.457%\t  6390.144\t        1\tconv2d_2/Conv2D\r\nnative : stat_summarizer.cc:85 \t                     Add\t           64.319\t    2.242\t    2.501\t  2.048%\t 54.505%\t     0.000\t        1\tadd\r\nnative : stat_summarizer.cc:85 \t                   Const\t           66.836\t    0.012\t    0.012\t  0.010%\t 54.515%\t     0.000\t        1\tconv2d_3/Conv2D/ReadVariableOp/_11__cf__11\r\nnative : stat_summarizer.cc:85 \t                  Conv2D\t           66.854\t   31.680\t   31.959\t 26.174%\t 80.689%\t 12582.912\t        1\tconv2d_3/Conv2D\r\nnative : stat_summarizer.cc:85 \t                    Relu\t           98.846\t    1.627\t    1.682\t  1.377%\t 82.066%\t     0.000\t        1\tactivation_2/Relu\r\nnative : stat_summarizer.cc:85 \t                   Shape\t          100.543\t    0.027\t    0.026\t  0.022%\t 82.088%\t     0.016\t        1\tconv2d_transpose/Shape\r\nnative : stat_summarizer.cc:85 \t            StridedSlice\t          100.576\t    0.023\t    0.021\t  0.017%\t 82.105%\t     0.004\t        1\tconv2d_transpose/strided_slice\r\nnative : stat_summarizer.cc:85 \t            StridedSlice\t          100.605\t    0.018\t    0.007\t  0.006%\t 82.111%\t     0.004\t        1\tconv2d_transpose/strided_slice_1\r\nnative : stat_summarizer.cc:85 \t                     Mul\t          100.617\t    0.010\t    0.012\t  0.010%\t 82.121%\t     0.000\t        1\tconv2d_transpose/mul\r\nnative : stat_summarizer.cc:85 \t            StridedSlice\t          100.632\t    0.007\t    0.008\t  0.007%\t 82.127%\t     0.004\t        1\tconv2d_transpose/strided_slice_2\r\nnative : stat_summarizer.cc:85 \t                     Mul\t          100.644\t    0.003\t    0.004\t  0.003%\t 82.131%\t     0.000\t        1\tconv2d_transpose/mul_1\r\nnative : stat_summarizer.cc:85 \t                    Pack\t          100.651\t    0.013\t    0.015\t  0.012%\t 82.143%\t     0.016\t        1\tconv2d_transpose/stack\r\nnative : stat_summarizer.cc:85 \t                   Const\t          100.672\t    0.008\t    0.009\t  0.007%\t 82.150%\t     0.000\t        1\tconv2d_transpose/conv2d_transpose/ReadVariableOp/_12__cf__12\r\nnative : stat_summarizer.cc:85 \t     Conv2DBackpropInput\t          100.684\t   19.043\t   20.264\t 16.596%\t 98.746%\t 15728.640\t        1\tconv2d_transpose/conv2d_transpose\r\nnative : stat_summarizer.cc:85 \t                 Minimum\t          120.986\t    1.088\t    0.878\t  0.719%\t 99.465%\t     0.000\t        1\toutput/Minimum\r\nnative : stat_summarizer.cc:85 \t                 Maximum\t          121.874\t    0.465\t    0.634\t  0.519%\t 99.985%\t     0.000\t        1\toutput\r\nnative : stat_summarizer.cc:85 \t                 _Retval\t          122.517\t    0.010\t    0.019\t  0.015%\t100.000%\t     0.000\t        1\t_retval_output_0_0\r\nnative : stat_summarizer.cc:85 \r\nnative : stat_summarizer.cc:85 ============================== Top by Computation Time ==============================\r\nnative : stat_summarizer.cc:85 \t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\nnative : stat_summarizer.cc:85 \t                  Conv2D\t           66.854\t   31.680\t   31.959\t 26.174%\t 26.174%\t 12582.912\t        1\tconv2d_3/Conv2D\r\nnative : stat_summarizer.cc:85 \t                  Conv2D\t           40.844\t   23.039\t   23.440\t 19.197%\t 45.371%\t  6390.144\t        1\tconv2d_2/Conv2D\r\nnative : stat_summarizer.cc:85 \t                  Conv2D\t           14.348\t   23.520\t   23.289\t 19.073%\t 64.444%\t  6390.144\t        1\tconv2d_1/Conv2D\r\nnative : stat_summarizer.cc:85 \t     Conv2DBackpropInput\t          100.684\t   19.043\t   20.264\t 16.596%\t 81.041%\t 15728.640\t        1\tconv2d_transpose/conv2d_transpose\r\nnative : stat_summarizer.cc:85 \t                  Conv2D\t            1.522\t    8.478\t    8.902\t  7.290%\t 88.331%\t  6390.144\t        1\tconv2d/Conv2D\r\nnative : stat_summarizer.cc:85 \t          FusedBatchNorm\t           11.540\t    3.001\t    2.754\t  2.256%\t 90.587%\t  6390.528\t        1\tbatch_normalization/FusedBatchNorm\r\nnative : stat_summarizer.cc:85 \t                     Add\t           64.319\t    2.242\t    2.501\t  2.048%\t 92.635%\t     0.000\t        1\tadd\r\nnative : stat_summarizer.cc:85 \t          FusedBatchNorm\t           37.672\t    2.232\t    2.291\t  1.877%\t 94.511%\t     0.384\t        1\tbatch_normalization_1/FusedBatchNorm\r\nnative : stat_summarizer.cc:85 \t                    Relu\t           98.846\t    1.627\t    1.682\t  1.377%\t 95.888%\t     0.000\t        1\tactivation_2/Relu\r\nnative : stat_summarizer.cc:85 \t               MirrorPad\t            0.095\t    1.112\t    1.351\t  1.107%\t 96.995%\t   811.200\t        1\tMirrorPad\r\nnative : stat_summarizer.cc:85 \r\nnative : stat_summarizer.cc:85 ============================== Top by Memory Use ==============================\r\nnative : stat_summarizer.cc:85 \t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\nnative : stat_summarizer.cc:85 \t     Conv2DBackpropInput\t          100.684\t   19.043\t   20.264\t 16.596%\t 16.596%\t 15728.640\t        1\tconv2d_transpose/conv2d_transpose\r\nnative : stat_summarizer.cc:85 \t                  Conv2D\t           66.854\t   31.680\t   31.959\t 26.174%\t 42.770%\t 12582.912\t        1\tconv2d_3/Conv2D\r\nnative : stat_summarizer.cc:85 \t          FusedBatchNorm\t           11.540\t    3.001\t    2.754\t  2.256%\t 45.026%\t  6390.528\t        1\tbatch_normalization/FusedBatchNorm\r\nnative : stat_summarizer.cc:85 \t                  Conv2D\t           40.844\t   23.039\t   23.440\t 19.197%\t 64.223%\t  6390.144\t        1\tconv2d_2/Conv2D\r\nnative : stat_summarizer.cc:85 \t                  Conv2D\t           14.348\t   23.520\t   23.289\t 19.073%\t 83.296%\t  6390.144\t        1\tconv2d_1/Conv2D\r\nnative : stat_summarizer.cc:85 \t                  Conv2D\t            1.522\t    8.478\t    8.902\t  7.290%\t 90.587%\t  6390.144\t        1\tconv2d/Conv2D\r\nnative : stat_summarizer.cc:85 \t               MirrorPad\t            0.095\t    1.112\t    1.351\t  1.107%\t 91.693%\t   811.200\t        1\tMirrorPad\r\nnative : stat_summarizer.cc:85 \t          FusedBatchNorm\t           37.672\t    2.232\t    2.291\t  1.877%\t 93.570%\t     0.384\t        1\tbatch_normalization_1/FusedBatchNorm\r\nnative : stat_summarizer.cc:85 \t                    Pack\t          100.651\t    0.013\t    0.015\t  0.012%\t 93.582%\t     0.016\t        1\tconv2d_transpose/stack\r\nnative : stat_summarizer.cc:85 \t                   Shape\t          100.543\t    0.027\t    0.026\t  0.022%\t 93.604%\t     0.016\t        1\tconv2d_transpose/Shape\r\nnative : stat_summarizer.cc:85 \r\nnative : stat_summarizer.cc:85 Number of nodes executed: 52\r\nnative : stat_summarizer.cc:85 ============================== Summary by node type ==============================\r\nnative : stat_summarizer.cc:85 \t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\nnative : stat_summarizer.cc:85 \t                  Conv2D\t        4\t    87.586\t    71.748%\t    71.748%\t 31753.344\t        4\r\nnative : stat_summarizer.cc:85 \t     Conv2DBackpropInput\t        1\t    20.264\t    16.600%\t    88.348%\t 15728.640\t        1\r\nnative : stat_summarizer.cc:85 \t          FusedBatchNorm\t        2\t     5.045\t     4.133%\t    92.481%\t  6390.912\t        2\r\nnative : stat_summarizer.cc:85 \t                    Relu\t        3\t     3.582\t     2.934%\t    95.415%\t     0.000\t        3\r\nnative : stat_summarizer.cc:85 \t                     Add\t        1\t     2.500\t     2.048%\t    97.463%\t     0.000\t        1\r\nnative : stat_summarizer.cc:85 \t               MirrorPad\t        1\t     1.351\t     1.107%\t    98.570%\t   811.200\t        1\r\nnative : stat_summarizer.cc:85 \t                 Minimum\t        1\t     0.877\t     0.718%\t    99.288%\t     0.000\t        1\r\nnative : stat_summarizer.cc:85 \t                 Maximum\t        1\t     0.634\t     0.519%\t    99.807%\t     0.000\t        1\r\nnative : stat_summarizer.cc:85 \t                   Const\t       28\t     0.116\t     0.095%\t    99.903%\t     0.000\t       28\r\nnative : stat_summarizer.cc:85 \t            StridedSlice\t        3\t     0.036\t     0.029%\t    99.932%\t     0.012\t        3\r\nnative : stat_summarizer.cc:85 \t                   Shape\t        1\t     0.026\t     0.021%\t    99.953%\t     0.016\t        1\r\nnative : stat_summarizer.cc:85 \t                 _Retval\t        1\t     0.018\t     0.015%\t    99.968%\t     0.000\t        1\r\nnative : stat_summarizer.cc:85 \t                     Mul\t        2\t     0.015\t     0.012%\t    99.980%\t     0.000\t        2\r\nnative : stat_summarizer.cc:85 \t                    Pack\t        1\t     0.014\t     0.011%\t    99.992%\t     0.016\t        1\r\nnative : stat_summarizer.cc:85 \t                    NoOp\t        1\t     0.007\t     0.006%\t    99.998%\t     0.000\t        1\r\nnative : stat_summarizer.cc:85 \t                    _Arg\t        1\t     0.003\t     0.002%\t   100.000%\t     0.000\t        1\r\nnative : stat_summarizer.cc:85 \r\nnative : stat_summarizer.cc:85 Timings (microseconds): count=50 first=119656 curr=123144 min=117106 max=143566 avg=122102 std=4282\r\nnative : stat_summarizer.cc:85 Memory (bytes): count=50 curr=54684140(all same)\r\nnative : stat_summarizer.cc:85 52 nodes observed\r\nnative : stat_summarizer.cc:85 \r\n```\r\n\r\n**TFLite 1 threads In:1x256x256x3 Performance w/o NNAPI**\r\n`\r\nbenchmark_model --graph=/data/local/tmp/UpResnet5-X2-H256-W256-C3.tflite --num_threads=1\r\n`\r\n```\r\nSTARTING!\r\nMin num runs: [50]\r\nMin runs duration (seconds): [1]\r\nInter-run delay (seconds): [-1]\r\nNum threads: [1]\r\nBenchmark name: []\r\nOutput prefix: []\r\nMin warmup runs: [1]\r\nMin warmup runs duration (seconds): [0.5]\r\nGraph: [/data/local/tmp/UpResnet5-X2-H256-W256-C3.tflite]\r\nInput layers: []\r\nInput shapes: []\r\nUse nnapi : [0]\r\nAllow fp16 : [0]\r\nnnapi error: requires android sdk version to be at least 27\r\nLoaded model /data/local/tmp/UpResnet5-X2-H256-W256-C3.tflite\r\nresolved reporter\r\nInitialized session in 17.527ms\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds\r\ncount=1 curr=1058871\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds\r\ncount=50 first=885931 curr=881142 min=851668 max=903937 avg=880688 std=12426\r\n\r\n============================== Run Order ==============================\r\n\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n\t              MIRROR_PAD\t            0.000\t   11.161\t   11.156\t  1.267%\t  1.267%\t     0.000\t        1\t[MirrorPad]\r\n\t                 CONV_2D\t           11.158\t    9.939\t   10.067\t  1.143%\t  2.410%\t     0.000\t        1\t[activation/Relu]\r\n\t                     MUL\t           21.226\t    1.432\t    1.521\t  0.173%\t  2.583%\t     0.000\t        1\t[batch_normalization/FusedBatchNorm_mul_0]\r\n\t                     ADD\t           22.748\t    1.491\t    1.520\t  0.173%\t  2.755%\t     0.000\t        1\t[batch_normalization/FusedBatchNorm]\r\n\t                 CONV_2D\t           24.269\t   82.282\t   82.912\t  9.415%\t 12.170%\t     0.000\t        1\t[activation_1/Relu]\r\n\t                 CONV_2D\t          107.182\t   82.992\t   82.822\t  9.404%\t 21.574%\t     0.000\t        1\t[conv2d_2/Conv2D]\r\n\t                     ADD\t          190.005\t    1.450\t    1.574\t  0.179%\t 21.753%\t     0.000\t        1\t[add]\r\n\t                 CONV_2D\t          191.579\t  117.950\t  118.416\t 13.446%\t 35.199%\t     0.000\t        1\t[activation_2/Relu]\r\n\t          TRANSPOSE_CONV\t          309.996\t  569.173\t  562.609\t 63.884%\t 99.083%\t     0.000\t        1\t[conv2d_transpose/conv2d_transpose]\r\n\t                 MINIMUM\t          872.607\t    4.092\t    4.072\t  0.462%\t 99.545%\t     0.000\t        1\t[output/Minimum]\r\n\t                 MAXIMUM\t          876.680\t    3.955\t    4.004\t  0.455%\t100.000%\t     0.000\t        1\t[output]\r\n\r\n============================== Top by Computation Time ==============================\r\n\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n\t          TRANSPOSE_CONV\t          309.996\t  569.173\t  562.609\t 63.884%\t 63.884%\t     0.000\t        1\t[conv2d_transpose/conv2d_transpose]\r\n\t                 CONV_2D\t          191.579\t  117.950\t  118.416\t 13.446%\t 77.330%\t     0.000\t        1\t[activation_2/Relu]\r\n\t                 CONV_2D\t           24.269\t   82.282\t   82.912\t  9.415%\t 86.745%\t     0.000\t        1\t[activation_1/Relu]\r\n\t                 CONV_2D\t          107.182\t   82.992\t   82.822\t  9.404%\t 96.149%\t     0.000\t        1\t[conv2d_2/Conv2D]\r\n\t              MIRROR_PAD\t            0.000\t   11.161\t   11.156\t  1.267%\t 97.416%\t     0.000\t        1\t[MirrorPad]\r\n\t                 CONV_2D\t           11.158\t    9.939\t   10.067\t  1.143%\t 98.559%\t     0.000\t        1\t[activation/Relu]\r\n\t                 MINIMUM\t          872.607\t    4.092\t    4.072\t  0.462%\t 99.021%\t     0.000\t        1\t[output/Minimum]\r\n\t                 MAXIMUM\t          876.680\t    3.955\t    4.004\t  0.455%\t 99.476%\t     0.000\t        1\t[output]\r\n\t                     ADD\t          190.005\t    1.450\t    1.574\t  0.179%\t 99.655%\t     0.000\t        1\t[add]\r\n\t                     MUL\t           21.226\t    1.432\t    1.521\t  0.173%\t 99.827%\t     0.000\t        1\t[batch_normalization/FusedBatchNorm_mul_0]\r\n\r\nNumber of nodes executed: 11\r\n============================== Summary by node type ==============================\r\n\t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n\t          TRANSPOSE_CONV\t        1\t   562.609\t    63.884%\t    63.884%\t     0.000\t        1\r\n\t                 CONV_2D\t        4\t   294.214\t    33.408%\t    97.292%\t     0.000\t        4\r\n\t              MIRROR_PAD\t        1\t    11.156\t     1.267%\t    98.559%\t     0.000\t        1\r\n\t                 MINIMUM\t        1\t     4.072\t     0.462%\t    99.021%\t     0.000\t        1\r\n\t                 MAXIMUM\t        1\t     4.004\t     0.455%\t    99.476%\t     0.000\t        1\r\n\t                     ADD\t        2\t     3.093\t     0.351%\t    99.827%\t     0.000\t        2\r\n\t                     MUL\t        1\t     1.521\t     0.173%\t   100.000%\t     0.000\t        1\r\n\r\nTimings (microseconds): count=50 first=885917 curr=881128 min=851653 max=903921 avg=880674 std=12426\r\nMemory (bytes): count=0\r\n11 nodes observed\r\n\r\n\r\nAverage inference timings in us: Warmup: 1.05887e+06, Init: 17527, no stats: 880688\r\n```\r\n\r\n**TFLite 16 threads In:1x256x256x3 Performance w/o NNAPI**\r\n`\r\nbenchmark_model --graph=/data/local/tmp/UpResnet5-X2-H256-W256-C3.tflite --num_threads=16\r\n`\r\n```\r\nSTARTING!\r\nMin num runs: [50]\r\nMin runs duration (seconds): [1]\r\nInter-run delay (seconds): [-1]\r\nNum threads: [16]\r\nBenchmark name: []\r\nOutput prefix: []\r\nMin warmup runs: [1]\r\nMin warmup runs duration (seconds): [0.5]\r\nGraph: [/data/local/tmp/UpResnet5-X2-H256-W256-C3.tflite]\r\nInput layers: []\r\nInput shapes: []\r\nUse nnapi : [0]\r\nAllow fp16 : [0]\r\nnnapi error: requires android sdk version to be at least 27\r\nLoaded model /data/local/tmp/UpResnet5-X2-H256-W256-C3.tflite\r\nresolved reporter\r\nInitialized session in 17.382ms\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds\r\ncount=1 curr=814662\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds\r\ncount=50 first=632324 curr=652388 min=617231 max=683660 avg=641555 std=13138\r\n\r\n============================== Run Order ==============================\r\n\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n\t              MIRROR_PAD\t            0.000\t   11.208\t   11.343\t  1.768%\t  1.768%\t     0.000\t        1\t[MirrorPad]\r\n\t                 CONV_2D\t           11.344\t   11.457\t   11.713\t  1.826%\t  3.594%\t     0.000\t        1\t[activation/Relu]\r\n\t                     MUL\t           23.059\t    2.024\t    1.853\t  0.289%\t  3.883%\t     0.000\t        1\t[batch_normalization/FusedBatchNorm_mul_0]\r\n\t                     ADD\t           24.912\t    1.510\t    1.803\t  0.281%\t  4.164%\t     0.000\t        1\t[batch_normalization/FusedBatchNorm]\r\n\t                 CONV_2D\t           26.716\t   11.487\t   12.328\t  1.922%\t  6.085%\t     0.000\t        1\t[activation_1/Relu]\r\n\t                 CONV_2D\t           39.044\t   11.615\t   11.867\t  1.850%\t  7.935%\t     0.000\t        1\t[conv2d_2/Conv2D]\r\n\t                     ADD\t           50.912\t    1.470\t    1.421\t  0.222%\t  8.157%\t     0.000\t        1\t[add]\r\n\t                 CONV_2D\t           52.334\t   19.418\t   19.692\t  3.069%\t 11.226%\t     0.000\t        1\t[activation_2/Relu]\r\n\t          TRANSPOSE_CONV\t           72.027\t  554.071\t  561.442\t 87.515%\t 98.741%\t     0.000\t        1\t[conv2d_transpose/conv2d_transpose]\r\n\t                 MINIMUM\t          633.470\t    4.022\t    4.066\t  0.634%\t 99.374%\t     0.000\t        1\t[output/Minimum]\r\n\t                 MAXIMUM\t          637.537\t    4.028\t    4.014\t  0.626%\t100.000%\t     0.000\t        1\t[output]\r\n\r\n============================== Top by Computation Time ==============================\r\n\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n\t          TRANSPOSE_CONV\t           72.027\t  554.071\t  561.442\t 87.515%\t 87.515%\t     0.000\t        1\t[conv2d_transpose/conv2d_transpose]\r\n\t                 CONV_2D\t           52.334\t   19.418\t   19.692\t  3.069%\t 90.584%\t     0.000\t        1\t[activation_2/Relu]\r\n\t                 CONV_2D\t           26.716\t   11.487\t   12.328\t  1.922%\t 92.506%\t     0.000\t        1\t[activation_1/Relu]\r\n\t                 CONV_2D\t           39.044\t   11.615\t   11.867\t  1.850%\t 94.355%\t     0.000\t        1\t[conv2d_2/Conv2D]\r\n\t                 CONV_2D\t           11.344\t   11.457\t   11.713\t  1.826%\t 96.181%\t     0.000\t        1\t[activation/Relu]\r\n\t              MIRROR_PAD\t            0.000\t   11.208\t   11.343\t  1.768%\t 97.949%\t     0.000\t        1\t[MirrorPad]\r\n\t                 MINIMUM\t          633.470\t    4.022\t    4.066\t  0.634%\t 98.583%\t     0.000\t        1\t[output/Minimum]\r\n\t                 MAXIMUM\t          637.537\t    4.028\t    4.014\t  0.626%\t 99.209%\t     0.000\t        1\t[output]\r\n\t                     MUL\t           23.059\t    2.024\t    1.853\t  0.289%\t 99.498%\t     0.000\t        1\t[batch_normalization/FusedBatchNorm_mul_0]\r\n\t                     ADD\t           24.912\t    1.510\t    1.803\t  0.281%\t 99.778%\t     0.000\t        1\t[batch_normalization/FusedBatchNorm]\r\n\r\nNumber of nodes executed: 11\r\n============================== Summary by node type ==============================\r\n\t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n\t          TRANSPOSE_CONV\t        1\t   561.441\t    87.515%\t    87.515%\t     0.000\t        1\r\n\t                 CONV_2D\t        4\t    55.597\t     8.666%\t    96.182%\t     0.000\t        4\r\n\t              MIRROR_PAD\t        1\t    11.343\t     1.768%\t    97.950%\t     0.000\t        1\r\n\t                 MINIMUM\t        1\t     4.065\t     0.634%\t    98.583%\t     0.000\t        1\r\n\t                 MAXIMUM\t        1\t     4.013\t     0.626%\t    99.209%\t     0.000\t        1\r\n\t                     ADD\t        2\t     3.223\t     0.502%\t    99.711%\t     0.000\t        2\r\n\t                     MUL\t        1\t     1.852\t     0.289%\t   100.000%\t     0.000\t        1\r\n\r\nTimings (microseconds): count=50 first=632310 curr=652370 min=617213 max=683643 avg=641541 std=13137\r\nMemory (bytes): count=0\r\n11 nodes observed\r\n\r\n\r\nAverage inference timings in us: Warmup: 814662, Init: 17382, no stats: 641555\r\n```", "comments": ["Why are you running with 16 threads on a 4-core device? Can you run the numbers with just 1 thread?", "@jdduke Sorry for replying late. \r\nI added the benchmark result with 1 and 4 threads.\r\nThe result shows that 16 threads is faster than 4 threads.", "Thanks @stakemura, we're actively investigating the performance discrepancy internally. Stay tuned.", "A [fix](https://github.com/tensorflow/tensorflow/commit/5e52b70188ceffc28e4c6a92f2366245dd6be159#diff-2b45693b554369bde8c98e9a76b80036) for this just landed, and TransposeConv performance in TFLite (single-threaded) should now be comparable to TFMobile. Thanks for your patience.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26736\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26736\">No</a>\n"]}, {"number": 26735, "title": "[2.0] tf.numpy_function logs deprecation warning", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Ubuntu 16.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: `2.0.0.dev20190311`\r\n- Python version: 3.6.6\r\n- CUDA/cuDNN version: 10.0\r\n\r\n**Describe the current behavior**\r\n\r\nWhen using `tf.numpy_function`, a warning is logged about `tf.py_func` being deprecated.\r\n\r\n**Describe the expected behavior**\r\n\r\nAs a V2 symbol, `tf.numpy_function` should not produce a deprecation warning.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.numpy_function(lambda x: x, [tf.zeros([5])], [tf.float32])\r\n```\r\n\r\n```text\r\nW0315 11:05:55.860109 139695358637824 deprecation.py:323] From /home/klein/dev/OpenNMT-tf/envv2/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py:476: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\ntf.py_func is deprecated in TF V2. Instead, there are two\r\n    options available in V2.\r\n    - tf.py_function takes a python function which manipulates tf eager\r\n    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\r\n    an ndarray (just call tensor.numpy()) but having access to eager tensors\r\n    means `tf.py_function`s can use accelerators such as GPUs as well as\r\n    being differentiable using a gradient tape.\r\n    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\r\n    (it is not differentiable, and manipulates numpy arrays). It drops the\r\n    stateful argument making all functions stateful.\r\n```", "comments": ["Added PR #26770 for the fix."]}, {"number": 26734, "title": "Update oauth_client.cc", "body": "Warning fixed.", "comments": []}, {"number": 26733, "title": "Expected to see 2 array(s), but instead got the following list of 1 arrays", "body": "I want to create a model that receive one image and compute the image by two Softmax(two output). The code is:\r\n\r\n-------------------------------------------------\r\n`base_model = InceptionV3(include_top=False)`\r\n`x = base_model.output`\r\n`x = GlobalAveragePooling2D()(x)`\r\n\r\n# first Softmax\r\n`x_1 = Dense(1024, activation='relu')(x)`\r\n`predictions_1 = Dense(4, activation='softmax')(x_1)`\r\n\r\n# second Softmax\r\n`x_2 = Dense(1024, activation='relu')(x)`\r\n`predictions_2 = Dense(4, activation='softmax')(x_2)`\r\n\r\n`my_model = Model(inputs=base_model.input, outputs=[predictions_1,predictions_2])`\r\n\r\n# train\r\n`my_model.compile(...)`\r\n`my_model.fit_generator(...)`\r\n-------------------------------------------------\r\n\r\nWhen training, I got error:\r\n\r\nValueError: \r\nExpected to see 2 array(s), but instead got the following list of 1 arrays: [array([[0., 0., 0., 1.],\r\n       [0., 0., 0., 1.],\r\n       [0., 0., 0., 1.],\r\n       [0., 1., 0., 0.],\r\n       [1., 0., 0., 0.],\r\n       [0., 1., 0., 0.],\r\n       [0., 1., 0., 0.],\r\n       [0., 1., 0., 0.],...", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 26732, "title": "[TF2.0] Error from Tensorboard with keras callback ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):win10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:no\r\n- TensorFlow installed from (source or binary): pip install tensorflow-gpu==2.0.0-alpha0\r\n- TensorFlow version (use command below):conda list 2.0.0a0\r\n- Python version:Python 3.6.8 :: Anaconda custom (64-bit)\r\n- CUDA/cuDNN version:V10.0.130/7.5.0\r\n- GPU model and memory:GTX 1050 TI\r\n\r\n**Describe the current behavior**\r\nI want to try tensorboard from keras, it work if tensorflow-gpu=1.13.1 & tensorboard=1.13.1,\r\nbut get the error if tensorflow-gpu=2.0.0a0 & tb-nightly=1.14.0a20190301 as below:\r\n```\r\nEpoch 1/50\r\n   32/60000 [..............................] - ETA: 11:31 - loss: 2.3852 - acc: 0.1562\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-4-aadf56b04ffa> in <module>\r\n----> 1 model.fit(x_train, y_train, epochs=50, callbacks=[tensorboard_callback])\r\n      2 \r\n      3 model.evaluate(x_test, y_test)\r\n\r\n~\\Anaconda3\\envs\\lab\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    863           validation_steps=validation_steps,\r\n    864           validation_freq=validation_freq,\r\n--> 865           steps_name='steps_per_epoch')\r\n    866 \r\n    867   def evaluate(self,\r\n\r\n~\\Anaconda3\\envs\\lab\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\r\n    361         # Callbacks batch end.\r\n    362         batch_logs = cbks.make_logs(model, batch_logs, batch_outs, mode)\r\n--> 363         callbacks._call_batch_hook(mode, 'end', batch_index, batch_logs)\r\n    364         progbar.on_batch_end(batch_index, batch_logs)\r\n    365 \r\n\r\n~\\Anaconda3\\envs\\lab\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py in _call_batch_hook(self, mode, hook, batch, logs)\r\n    225     for callback in self.callbacks:\r\n    226       batch_hook = getattr(callback, hook_name)\r\n--> 227       batch_hook(batch, logs)\r\n    228     self._delta_ts[hook_name].append(time.time() - t_before_callbacks)\r\n    229 \r\n\r\n~\\Anaconda3\\envs\\lab\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py in on_train_batch_end(self, batch, logs)\r\n    507     \"\"\"\r\n    508     # For backwards compatibility.\r\n--> 509     self.on_batch_end(batch, logs=logs)\r\n    510 \r\n    511   def on_test_batch_begin(self, batch, logs=None):\r\n\r\n~\\Anaconda3\\envs\\lab\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks_v1.py in on_batch_end(self, batch, logs)\r\n    360     self._total_batches_seen += 1\r\n    361     if self._is_profiling:\r\n--> 362       profiler.save(self.log_dir, profiler.stop())\r\n    363       self._is_profiling = False\r\n    364     elif (not self._is_profiling and\r\n\r\n~\\Anaconda3\\envs\\lab\\lib\\site-packages\\tensorflow\\python\\eager\\profiler.py in save(logdir, result)\r\n    141       logdir, 'plugins', 'profile',\r\n    142       datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\r\n--> 143   gfile.MakeDirs(plugin_dir)\r\n    144   maybe_create_event_file(logdir)\r\n    145   with gfile.Open(os.path.join(plugin_dir, 'local.trace'), 'wb') as f:\r\n\r\n~\\Anaconda3\\envs\\lab\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py in recursive_create_dir(dirname)\r\n    446     errors.OpError: If the operation fails.\r\n    447   \"\"\"\r\n--> 448   recursive_create_dir_v2(dirname)\r\n    449 \r\n    450 \r\n\r\n~\\Anaconda3\\envs\\lab\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py in recursive_create_dir_v2(path)\r\n    462   \"\"\"\r\n    463   with errors.raise_exception_on_not_ok_status() as status:\r\n--> 464     pywrap_tensorflow.RecursivelyCreateDir(compat.as_bytes(path), status)\r\n    465 \r\n    466 \r\n\r\n~\\Anaconda3\\envs\\lab\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)\r\n    546             None, None,\r\n    547             compat.as_text(c_api.TF_Message(self.status.status)),\r\n--> 548             c_api.TF_GetCode(self.status.status))\r\n    549     # Delete the underlying status object from memory otherwise it stays alive\r\n    550     # as there is a reference to status from this from the traceback due to\r\n\r\nNotFoundError: Failed to create a directory: logs/fit/20190315-164851\\plugins\\profile\\2019-03-15_16-48-53; No such file or directory\r\n```\r\nif reinstall tensorflow==2.0.0-alpha0 and tf-nightly-gpu got another error:\r\n```\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-6-aadf56b04ffa> in <module>\r\n----> 1 model.fit(x_train, y_train, epochs=50, callbacks=[tensorboard_callback])\r\n      2 \r\n      3 model.evaluate(x_test, y_test)\r\n\r\n~\\Anaconda3\\envs\\lab\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    871           validation_steps=validation_steps,\r\n    872           validation_freq=validation_freq,\r\n--> 873           steps_name='steps_per_epoch')\r\n    874 \r\n    875   def evaluate(self,\r\n\r\n~\\Anaconda3\\envs\\lab\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\r\n    202       samples=num_samples_or_steps,\r\n    203       verbose=0,  # Handle ProgBarLogger separately in this loop.\r\n--> 204       mode=mode)\r\n    205   # TODO(omalleyt): Handle ProgBar as part of Callbacks once hooks are ready.\r\n    206   progbar = training_utils.get_progbar(model, count_mode)\r\n\r\n~\\Anaconda3\\envs\\lab\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py in configure_callbacks(callbacks, model, do_validation, batch_size, epochs, steps_per_epoch, samples, verbose, count_mode, mode)\r\n     94   # Set callback model\r\n     95   callback_model = model._get_callback_model()  # pylint: disable=protected-access\r\n---> 96   callback_list.set_model(callback_model)\r\n     97 \r\n     98   set_callback_parameters(\r\n\r\n~\\Anaconda3\\envs\\lab\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py in set_model(self, model)\r\n    208     self.model = model\r\n    209     for callback in self.callbacks:\r\n--> 210       callback.set_model(model)\r\n    211 \r\n    212   def _call_batch_hook(self, mode, hook, batch, logs=None):\r\n\r\n~\\Anaconda3\\envs\\lab\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py in set_model(self, model)\r\n   1213     self.model = model\r\n   1214     with context.eager_mode():\r\n-> 1215       self._initialize_writers()\r\n   1216       if self.write_graph:\r\n   1217         if model.run_eagerly:\r\n\r\n~\\Anaconda3\\envs\\lab\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py in _initialize_writers(self)\r\n   1251       return summary_ops_v2.create_file_writer_v2(path)\r\n   1252 \r\n-> 1253     self._train_writer = create_writer('train')\r\n   1254     self._writers.append(self._train_writer)\r\n   1255     self._validation_writer = create_writer('validation')\r\n\r\n~\\Anaconda3\\envs\\lab\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py in create_writer(subdir)\r\n   1249     def create_writer(subdir):\r\n   1250       path = os.path.join(self.log_dir, subdir)\r\n-> 1251       return summary_ops_v2.create_file_writer_v2(path)\r\n   1252 \r\n   1253     self._train_writer = create_writer('train')\r\n\r\n~\\Anaconda3\\envs\\lab\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py in create_file_writer_v2(logdir, max_queue, flush_millis, filename_suffix, name)\r\n    377               filename_suffix=filename_suffix),\r\n    378           name=name,\r\n--> 379           v2=True)\r\n    380 \r\n    381 \r\n\r\n~\\Anaconda3\\envs\\lab\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py in __init__(self, shared_name, init_op_fn, name, v2)\r\n    197     # TODO(nickfelt): cache other constructed ops in graph mode\r\n    198     self._init_op_fn = init_op_fn\r\n--> 199     self._init_op = init_op_fn(self._resource)\r\n    200     self._v2 = v2\r\n    201     self._closed = False\r\n\r\n~\\Anaconda3\\envs\\lab\\lib\\site-packages\\tensorflow\\python\\ops\\gen_summary_ops.py in create_summary_file_writer(writer, logdir, max_queue, flush_millis, filename_suffix, name)\r\n    190       else:\r\n    191         message = e.message\r\n--> 192       _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n    193   # Add nodes to the TensorFlow graph.\r\n    194   _, _, _op = _op_def_lib._apply_op_helper(\r\n\r\n~\\Anaconda3\\envs\\lab\\lib\\site-packages\\six.py in raise_from(value, from_value)\r\n\r\nNotFoundError: Failed to create a directory: logs/fit/20190315-171835\\train; No such file or directory [Op:CreateSummaryFileWriter]\r\n```\r\n\r\nAny suggestion to fix? Thanks.\r\n\r\n**Describe the expected behavior**\r\nshould be trained and logged\r\n\r\n**Code to reproduce the issue**\r\n```\r\nfrom __future__ import absolute_import, division, print_function\r\nimport tensorflow as tf\r\nimport datetime\r\nfrom tensorflow.keras.callbacks import TensorBoard\r\n\r\nmnist = tf.keras.datasets.mnist\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10, activation='softmax')\r\n])\r\n\r\nlog_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\ntensorboard_callback = TensorBoard(log_dir)\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\nmodel.fit(x_train, y_train, epochs=50, callbacks=[tensorboard_callback])\r\nmodel.evaluate(x_test, y_test)\r\n```\r\n\r\n**Other info / logs**", "comments": ["I've had the same problem.\r\n\r\nSystem information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):no\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):win10\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:no\r\nTensorFlow installed from (source or binary): pip install tensorflow-gpu==2.0.0-alpha0\r\nTensorFlow version (use command below):conda list 2.0.0a0\r\nPython version:Python 3.6.8 :: Anaconda custom (64-bit)\r\nCUDA/cuDNN version:V10.0.130/7.5.0\r\nGPU model and memory:GTX 1050 TI\r\n", "@peterhsu2018 and @Suger131 Could you post the issue in Tensorboard repo [here](https://github.com/tensorflow/tensorboard/issues). The TBoard Group is actively responds to the queries related TB. Thanks.", "mkdir those dir: \r\n```bash\r\nmkdir -p logs/fit\r\n```\r\n\r\nI have encountered the same dilemma as you. Suddenly, I asked myself why I do not create this folder, then I create those dir, things works fine.", "> mkdir those dir:\r\n> mkdir -p logs/fit\r\n> I have encountered the same dilemma as you. Suddenly, I asked myself why I do not create this folder, then I create those dir, things works fine.\r\n\r\nIf you want to see how this problem is solved, you can take a look [here(#2023).](https://github.com/tensorflow/tensorboard/issues/2023)", "> > mkdir those dir:\r\n> > mkdir -p logs/fit\r\n> > I have encountered the same dilemma as you. Suddenly, I asked myself why I do not create this folder, then I create those dir, things works fine.\r\n> \r\n> If you want to see how this problem is solved, you can take a look [here(#2023).](https://github.com/tensorflow/tensorboard/issues/2023)\r\n\r\nThanks.", "If you use Tensorflow in windows,you should for sure your path, for exsample  log_dir=\"logs/fit/\" , you should change it to log_dir=\"logs\\\\\\fit\\\\\\\\\"", "What helped me, is wrapping the path with Path object (from pathlib module) and converting back to str"]}, {"number": 26731, "title": "Error building Tensorflow Lite on AARCH64", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): N/A\r\n- TensorFlow version: the latest\r\n- Python version: N/A\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source):  gcc version 5.4.0 20160609 \r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the problem**\r\nI am trying to build Tensorflow Lite for ARM64 boards.\r\nI followed the instructions on  https://tensorflow.google.cn/lite/guide/build_arm64 and executed the following commands:\r\n\r\n    sudo apt-get update\r\n    sudo apt-get install crossbuild-essential-arm64\r\n    ./tensorflow/lite/tools/make/download_dependencies.sh\r\n    ./tensorflow/lite/tools/make/build_aarch64_lib.sh\r\n\r\nBut at the last step got lots of errors such as: \r\n\r\n\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3823:22: note: use -flax-vector-conversions to permit conversions between vectors with differing element types or numbers of subparts\r\n       filter_reg_0_b = vdupq_n_u8(kSignBit);\r\n                      ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3823:22: error: cannot convert \u2018uint8x16_t {aka __vector(16) unsigned char}\u2019 to \u2018int8x16_t {aka __vector(16) signed char}\u2019 in assignment\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3824:22: error: cannot convert \u2018uint8x16_t {aka __vector(16) unsigned char}\u2019 to \u2018int8x16_t {aka __vector(16) signed char}\u2019 in assignment\r\n       filter_reg_1_b = vdupq_n_u8(kSignBit);\r\n                      ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3825:22: error: cannot convert \u2018uint8x16_t {aka __vector(16) unsigned char}\u2019 to \u2018int8x16_t {aka __vector(16) signed char}\u2019 in assignment\r\n       filter_reg_2_b = vdupq_n_u8(kSignBit);\r\n                      ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:21:0,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:25:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:50:71: error: cannot convert \u2018int8x16_t {aka __vector(16) signed char}\u2019 to \u2018uint64x2_t {aka __vector(2) long unsigned int}\u2019 for argument \u20182\u2019 to \u2018uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)\u2019\r\n   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)\r\n                                                                       ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3828:24: note: in expansion of macro \u2018vld1q_lane_s8x8\u2019\r\n       filter_reg_0_a = vld1q_lane_s8x8(filter_block_ptr, filter_reg_0_a, 0);\r\n                        ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:50:71: error: cannot convert \u2018int8x16_t {aka __vector(16) signed char}\u2019 to \u2018uint64x2_t {aka __vector(2) long unsigned int}\u2019 for argument \u20182\u2019 to \u2018uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)\u2019\r\n   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)\r\n                                                                       ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3830:24: note: in expansion of macro \u2018vld1q_lane_s8x8\u2019\r\n       filter_reg_0_b = vld1q_lane_s8x8(filter_block_ptr, filter_reg_0_b, 0);\r\n                        ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:50:71: error: cannot convert \u2018int8x16_t {aka __vector(16) signed char}\u2019 to \u2018uint64x2_t {aka __vector(2) long unsigned int}\u2019 for argument \u20182\u2019 to \u2018uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)\u2019\r\n   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)\r\n                                                                       ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3832:24: note: in expansion of macro \u2018vld1q_lane_s8x8\u2019\r\n       filter_reg_0_a = vld1q_lane_s8x8(filter_block_ptr, filter_reg_0_a, 1);\r\n                        ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:50:71: error: cannot convert \u2018int8x16_t {aka __vector(16) signed char}\u2019 to \u2018uint64x2_t {aka __vector(2) long unsigned int}\u2019 for argument \u20182\u2019 to \u2018uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)\u2019\r\n   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)\r\n                                                                       ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3834:24: note: in expansion of macro \u2018vld1q_lane_s8x8\u2019\r\n       filter_reg_1_a = vld1q_lane_s8x8(filter_block_ptr, filter_reg_1_a, 0);\r\n                        ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:50:71: error: cannot convert \u2018int8x16_t {aka __vector(16) signed char}\u2019 to \u2018uint64x2_t {aka __vector(2) long unsigned int}\u2019 for argument \u20182\u2019 to \u2018uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)\u2019\r\n   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)\r\n                                                                       ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3836:24: note: in expansion of macro \u2018vld1q_lane_s8x8\u2019\r\n       filter_reg_1_b = vld1q_lane_s8x8(filter_block_ptr, filter_reg_1_b, 0);\r\n                        ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:50:71: error: cannot convert \u2018int8x16_t {aka __vector(16) signed char}\u2019 to \u2018uint64x2_t {aka __vector(2) long unsigned int}\u2019 for argument \u20182\u2019 to \u2018uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)\u2019\r\n   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)\r\n                                                                       ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3838:24: note: in expansion of macro \u2018vld1q_lane_s8x8\u2019\r\n       filter_reg_1_a = vld1q_lane_s8x8(filter_block_ptr, filter_reg_1_a, 1);\r\n                        ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:50:71: error: cannot convert \u2018int8x16_t {aka __vector(16) signed char}\u2019 to \u2018uint64x2_t {aka __vector(2) long unsigned int}\u2019 for argument \u20182\u2019 to \u2018uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)\u2019\r\n   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)\r\n                                                                       ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3840:24: note: in expansion of macro \u2018vld1q_lane_s8x8\u2019\r\n       filter_reg_2_a = vld1q_lane_s8x8(filter_block_ptr, filter_reg_2_a, 0);\r\n                        ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:50:71: error: cannot convert \u2018int8x16_t {aka __vector(16) signed char}\u2019 to \u2018uint64x2_t {aka __vector(2) long unsigned int}\u2019 for argument \u20182\u2019 to \u2018uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)\u2019\r\n   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)\r\n                                                                       ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3842:24: note: in expansion of macro \u2018vld1q_lane_s8x8\u2019\r\n       filter_reg_2_b = vld1q_lane_s8x8(filter_block_ptr, filter_reg_2_b, 0);\r\n                        ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:50:71: error: cannot convert \u2018int8x16_t {aka __vector(16) signed char}\u2019 to \u2018uint64x2_t {aka __vector(2) long unsigned int}\u2019 for argument \u20182\u2019 to \u2018uint64x2_t vld1q_lane_u64(const uint64_t*, uint64x2_t, int)\u2019\r\n   vld1q_lane_u64(reinterpret_cast<const uint64_t*>(src), reg, lane_num)\r\n                                                                       ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3844:24: note: in expansion of macro \u2018vld1q_lane_s8x8\u2019\r\n       filter_reg_2_a = vld1q_lane_s8x8(filter_block_ptr, filter_reg_2_a, 1);\r\n                        ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:21:0,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:25:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3846:57: error: cannot convert \u2018const uint8x16_t {aka const __vector(16) unsigned char}\u2019 to \u2018int8x16_t {aka __vector(16) signed char}\u2019 for argument \u20182\u2019 to \u2018int8x16_t veorq_s8(int8x16_t, int8x16_t)\u2019\r\n       filter_reg_0_a = veorq_s8(filter_reg_0_a, sign_bit);\r\n                                                         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3847:57: error: cannot convert \u2018const uint8x16_t {aka const __vector(16) unsigned char}\u2019 to \u2018int8x16_t {aka __vector(16) signed char}\u2019 for argument \u20182\u2019 to \u2018int8x16_t veorq_s8(int8x16_t, int8x16_t)\u2019\r\n       filter_reg_0_b = veorq_s8(filter_reg_0_b, sign_bit);\r\n                                                         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3848:57: error: cannot convert \u2018const uint8x16_t {aka const __vector(16) unsigned char}\u2019 to \u2018int8x16_t {aka __vector(16) signed char}\u2019 for argument \u20182\u2019 to \u2018int8x16_t veorq_s8(int8x16_t, int8x16_t)\u2019\r\n       filter_reg_1_a = veorq_s8(filter_reg_1_a, sign_bit);\r\n                                                         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3849:57: error: cannot convert \u2018const uint8x16_t {aka const __vector(16) unsigned char}\u2019 to \u2018int8x16_t {aka __vector(16) signed char}\u2019 for argument \u20182\u2019 to \u2018int8x16_t veorq_s8(int8x16_t, int8x16_t)\u2019\r\n       filter_reg_1_b = veorq_s8(filter_reg_1_b, sign_bit);\r\n                                                         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3850:57: error: cannot convert \u2018const uint8x16_t {aka const __vector(16) unsigned char}\u2019 to \u2018int8x16_t {aka __vector(16) signed char}\u2019 for argument \u20182\u2019 to \u2018int8x16_t veorq_s8(int8x16_t, int8x16_t)\u2019\r\n       filter_reg_2_a = veorq_s8(filter_reg_2_a, sign_bit);\r\n                                                         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3851:57: error: cannot convert \u2018const uint8x16_t {aka const __vector(16) unsigned char}\u2019 to \u2018int8x16_t {aka __vector(16) signed char}\u2019 for argument \u20182\u2019 to \u2018int8x16_t veorq_s8(int8x16_t, int8x16_t)\u2019\r\n       filter_reg_2_b = veorq_s8(filter_reg_2_b, sign_bit);\r\n                                                         ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h: In static member function \u2018static void tflite::optimized_ops::depthwise_conv::PackMacroBlock<(tflite::DepthwiseConvImplementation)3, (tflite::DepthwiseConvDepthMultiplication)0, 0>::PackMacroBlockNeon(const uint8*, int8*, const tflite::optimized_ops::depthwise_conv::DepthwiseConvDotProdParams*)\u2019:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3954:53: error: cannot convert \u2018uint8x16_t {aka __vector(16) unsigned char}\u2019 to \u2018const int8x16_t {aka const __vector(16) signed char}\u2019 in initialization\r\n     const int8x16_t perm_data_0 = vld1q_u8(perm_data);\r\n                                                     ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3955:58: error: cannot convert \u2018uint8x16_t {aka __vector(16) unsigned char}\u2019 to \u2018const int8x16_t {aka const __vector(16) signed char}\u2019 in initialization\r\n     const int8x16_t perm_data_1 = vld1q_u8(perm_data + 16);\r\n                                                          ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3956:58: error: cannot convert \u2018uint8x16_t {aka __vector(16) unsigned char}\u2019 to \u2018const int8x16_t {aka const __vector(16) signed char}\u2019 in initialization\r\n     const int8x16_t perm_data_2 = vld1q_u8(perm_data + 32);\r\n                                                          ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:3957:58: error: cannot convert \u2018uint8x16_t {aka __vector(16) unsigned char}\u2019 to \u2018const int8x16_t {aka const __vector(16) signed char}\u2019 in initialization\r\n     const int8x16_t perm_data_3 = vld1q_u8(perm_data + 48);\r\n\r\n\r\n\r\nHow can I fix it?", "comments": ["I build tensorflow-lite on raspberry-3B+\uff0cand met this problem\u3002", "It also happened to me when cross-compiling for Raspberry Pi\r\n\r\n`./tensorflow/lite/tools/make/build_rpi_lib.sh`", "I have been doing some testing, I share here this in case it helps.\r\n\r\nI get the error during the compilation when executing.\r\n\r\n`arm-linux-gnueabihf-g++ -O3 -DNDEBUG -fPIC  --std=c++11 -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/javi/Qt/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/javi/Qt/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/javi/Qt/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/javi/Qt/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/javi/Qt/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/javi/Qt/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/javi/Qt/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/javi/Qt/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/javi/Qt/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/kernels/depthwise_conv.cc -o /home/javi/Qt/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/depthwise_conv.o`\r\n\r\nBut, if I remove `mfpu=neon-vfpv4` from the previous command, it works.\r\n\r\nI  also noticed than when executing `./tensorflow/lite/tools/make/download_dependencies.sh`, I get this warning.\r\n\r\n`cat: /home/javi/Qt/tensorflow/tensorflow/lite/tools/make/../../../../third_party/eigen3/gebp_neon.patch: No such file or directory\r\n`", "I had the exact same error on a Pine64 A64+ Board. \r\n", "Same problom with master branch of tensorflow for ARMv8 platform", "It also happened to me when cross-compiling for ARMv8 platform", "Same for me.", "I think the code was not tested with newer gcc. If you build it with **gcc** or **clang** for `android-arm64`, e.g.,\r\n```\r\nbazel build  --config android_arm64 --cxxopt=-std=c++11 \\\r\n//tensorflow/lite/examples/label_image:label_image --config monolithic\r\n````\r\nIt goes well. \r\n\r\nTo build it for aarch64 machines running Linux with gcc, either natively or cross-compiling, as suggested in the error message `use -flax-vector-conversions` is one of the possible answers. Other possible solutions include adding explicit type casting to make gcc happy. Or you can use clang instead of gcc. Tested on an internal dev board and Google's Coral Dev Board. \r\n\r\nI'll submit a PR for this issue later.", "it seem the problem is gone after 152095e319a0b6b79a13e689ea8decee819dbfcd. Those who met the problem may want to `git pull` and try again.", "@freedomtan I tried it again in my Pine64 A64+ with the process described in [the guide](https://www.tensorflow.org/lite/guide/build_arm64#native_compiling) and this time it ended with a different error:\r\n\r\n``` bash\r\nnnapi_delegate.cc:(.text+0x28): undefined reference to `NnApiImplementation()'\r\n/home/pine/Downloads/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a(nnapi_delegate.o): In function `tflite::NNAPIAllocation::NNAPIAllocation(char const*, tflite::ErrorReporter*)':\r\nnnapi_delegate.cc:(.text+0x18c): undefined reference to `NnApiImplementation()'\r\n/home/pine/Downloads/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a(nnapi_delegate.o): In function `tflite::NNAPIDelegate::~NNAPIDelegate()':\r\nnnapi_delegate.cc:(.text+0x200): undefined reference to `NnApiImplementation()'\r\nnnapi_delegate.cc:(.text+0x21c): undefined reference to `NnApiImplementation()'\r\n/home/pine/Downloads/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a(nnapi_delegate.o): In function `tflite::addTensorOperands(tflite::Subgraph*, ANeuralNetworksModel*, unsigned int*, std::vector<long, std::allocator<long> >*)':\r\nnnapi_delegate.cc:(.text+0x298): undefined reference to `NnApiImplementation()'\r\n/home/pine/Downloads/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a(nnapi_delegate.o):nnapi_delegate.cc:(.text+0x578): more undefined references to `NnApiImplementation()' follow\r\ncollect2: error: ld returned 1 exit status\r\ntensorflow/lite/tools/make/Makefile:227: recipe for target '/home/pine/Downloads/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/bin/minimal' failed\r\nmake: *** [/home/pine/Downloads/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/bin/minimal] Error 1\r\n```\r\n**Edit 1:** Maybe this is the same as [Issue 25120](https://github.com/tensorflow/tensorflow/issues/25120)?\r\n\r\n**Edit 2:** Indeed this appears to be issue 25120. I did as suggested there, modified the Makefile to set `BUILD_WITH_NNAPI=true` -> `BUILD_WITH_NNAPI=false`, deleted the `/gen` folder and re ran the build script and it finished without errors. Now I guess it's time to run some example to try it out. I'll do that next. ", "I have a travis-ci build running to build tflite and can reproduce the error on the latest commit: https://travis-ci.org/kmader/tflite_lib_builder/jobs/529685375", "Hi, I still have the same error under both the latest master branch and `r1.14` tag. Any progress on a solution to it?", "@freedomtan \r\nChanged to clang, the mentioned error goes away, but the following error occurs at the end of the compilation. \r\nClang info: `clang version 6.0.0-1ubuntu2 (tags/RELEASE_600/final) Target: aarch64-unknown-linux-gnu`\r\n``` c++\r\n/usr/bin/llvm-ar-6.0: creating /home/pi/code/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a\r\n/usr/bin/clang++ -O3 -DNDEBUG -fPIC  --std=c++11 -march=armv8-a -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I -I/usr/local/include \\\r\n-o /home/pi/code/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/bin/minimal /home/pi/code/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/obj/tensorflow/lite/examples/minimal/minimal.o \\\r\n /home/pi/code/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a -Wl,--no-export-dynamic -Wl,--exclude-libs,ALL -Wl,--gc-sections -Wl,--as-needed -lrt -lstdc++ -lpthread -lm -ldl/home/pi/code/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a(audio_spectrogram.o): In function `flexbuffers::Reference::AsUInt64() const':\r\naudio_spectrogram.cc:(.text._ZNK11flexbuffers9Reference8AsUInt64Ev[_ZNK11flexbuffers9Reference8AsUInt64Ev]+0x2f0): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\naudio_spectrogram.cc:(.text._ZNK11flexbuffers9Reference8AsUInt64Ev[_ZNK11flexbuffers9Reference8AsUInt64Ev]+0x2f4): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\n/home/pi/code/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a(while.o): In function `flexbuffers::Reference::AsInt64() const':\r\nwhile.cc:(.text._ZNK11flexbuffers9Reference7AsInt64Ev[_ZNK11flexbuffers9Reference7AsInt64Ev]+0x2f0): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\nwhile.cc:(.text._ZNK11flexbuffers9Reference7AsInt64Ev[_ZNK11flexbuffers9Reference7AsInt64Ev]+0x2f4): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\ntensorflow/lite/tools/make/Makefile_clang:264: recipe for target '/home/pi/code/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/bin/minimal' failed\r\nmake: *** [/home/pi/code/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/bin/minimal] Error 1\r\nmake: *** Waiting for unfinished jobs....\r\n```\r\n\r\nAnd adding `-flax-vector-conversions` flag end up with this error. gcc version `gcc (Ubuntu/Linaro 7.4.0-1ubuntu1~18.04) 7.4.0`\r\n``` c++\r\naarch64-linux-gnu-g++ -O3 -DNDEBUG -fPIC -flax-vector-conversions  --std=c++11 -march=armv8-a -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/kernels/dequantize.cc -o /home/pi/code/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/obj/tensorflow/lite/kernels/dequantize.o\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:22,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h: In static member function \u2018static void tflite::optimized_ops::depthwise_conv::KernelMacroBlock<(tflite::DepthwiseConvImplementation)3, (tflite::DepthwiseConvDepthMultiplication)0, 2>::Run(const int8*, const int8*, const int32*, uint8*, const tflite::optimized_ops::depthwise_conv::DepthwiseConvDotProdParams*)\u2019:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:8255:3: error: x29 cannot be used in asm here\r\n```", "@PhilipXue  try adding `-fomit-frame-pointer` to see if it can make inline assembly happy", "@freedomtan \r\nHi, thanks for reply, I add the suggested flag and the previous error is gone, but end up with the following error at the end, which is the same error as the clang compilation.\r\n\r\n```\r\naarch64-linux-gnu-ar: creating /home/pi/code/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a\r\naarch64-linux-gnu-g++ -O3 -DNDEBUG -fPIC -flax-vector-conversions -fomit-frame-pointer  --std=c++11 -march=armv8-a -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include \\\r\n-o /home/pi/code/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/bin/minimal /home/pi/code/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/obj/tensorflow/lite/examples/minimal/minimal.o \\\r\n /home/pi/code/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a -Wl,--no-export-dynamic -Wl,--exclude-libs,ALL -Wl,--gc-sections -Wl,--as-needed -lrt -lstdc++ -lpthread -lm -ldl/home/pi/code/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a(while.o): In function `tflite::ops::custom::while_kernel::Init(TfLiteContext*, char const*, unsigned long)':\r\nwhile.cc:(.text+0x1a6c): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\nwhile.cc:(.text+0x1a7c): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\nwhile.cc:(.text+0x1db8): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\nwhile.cc:(.text+0x1dc8): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\n/home/pi/code/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a(audio_spectrogram.o): In function `tflite::ops::custom::audio_spectrogram::Init(TfLiteContext*, char const*, unsigned long)':\r\naudio_spectrogram.cc:(.text+0xd94): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\n/home/pi/code/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a(audio_spectrogram.o):audio_spectrogram.cc:(.text+0xda4): more undefined references to `flatbuffers::ClassicLocale::instance_' follow\r\ncollect2: error: ld returned 1 exit status\r\ntensorflow/lite/tools/make/Makefile:267: recipe for target '/home/pi/code/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/bin/minimal' failed\r\n```\r\nIt seems that flatbuffer is to blame, I am trying both lasted github flatbuffer and one download by `dowload_denpendencies.sh`, \r\nAlso, I am trying a fresh reinstall of the OS.", "I turned in a pull request at https://github.com/tensorflow/tensorflow/pull/29515 that can hopefully resolve the two issues.", "> @freedomtan\r\n> Hi, thanks for reply, I add the suggested flag and the previous error is gone, but end up with the following error at the end, which is the same error as the clang compilation.\r\n> \r\n> ```\r\n> aarch64-linux-gnu-ar: creating /home/pi/code/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a\r\n> aarch64-linux-gnu-g++ -O3 -DNDEBUG -fPIC -flax-vector-conversions -fomit-frame-pointer  --std=c++11 -march=armv8-a -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/pi/code/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include \\\r\n> -o /home/pi/code/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/bin/minimal /home/pi/code/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/obj/tensorflow/lite/examples/minimal/minimal.o \\\r\n>  /home/pi/code/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a -Wl,--no-export-dynamic -Wl,--exclude-libs,ALL -Wl,--gc-sections -Wl,--as-needed -lrt -lstdc++ -lpthread -lm -ldl/home/pi/code/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a(while.o): In function `tflite::ops::custom::while_kernel::Init(TfLiteContext*, char const*, unsigned long)':\r\n> while.cc:(.text+0x1a6c): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\n> while.cc:(.text+0x1a7c): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\n> while.cc:(.text+0x1db8): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\n> while.cc:(.text+0x1dc8): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\n> /home/pi/code/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a(audio_spectrogram.o): In function `tflite::ops::custom::audio_spectrogram::Init(TfLiteContext*, char const*, unsigned long)':\r\n> audio_spectrogram.cc:(.text+0xd94): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\n> /home/pi/code/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a(audio_spectrogram.o):audio_spectrogram.cc:(.text+0xda4): more undefined references to `flatbuffers::ClassicLocale::instance_' follow\r\n> collect2: error: ld returned 1 exit status\r\n> tensorflow/lite/tools/make/Makefile:267: recipe for target '/home/pi/code/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/bin/minimal' failed\r\n> ```\r\n> \r\n> It seems that flatbuffer is to blame, I am trying both lasted github flatbuffer and one download by `dowload_denpendencies.sh`,\r\n> Also, I am trying a fresh reinstall of the OS.\r\n\r\nI have the same issue. Any solutions yet?", "Anyone has a workaround to the error with flatbuffer? Have also that problem.", "I don't have the flatbuffer issue when applying the patch at #29515 and using gcc. Do you guys have to use llvm?", "Haven't really checked what happened to cmake, `bazel build` works reliably for me with either gcc or clang :-)", "we're working on that internally", "> \r\n> \r\n> Anyone has a workaround to the error with flatbuffer? Have also that problem.\r\n\r\nHi @vizero1 ...\r\n I was able to solve the problem by following the workaround mentioned in issue [29806](https://github.com/tensorflow/tensorflow/issues/29806). \r\n\r\nI have added those changes along with the above mentioned changes, and made a new Makefile (for aarch64 architecture).\r\n\r\n[Gist of the new makefile](https://gist.github.com/mohan-barathi/538c45a77cd8531fb9f4367dd2e0cd1a)", "Hi, can you sync to the head and try again? thanks!", "> Hi, can you sync to the head and try again? thanks!\r\n\r\nI successfully built head: https://github.com/tensorflow/tensorflow/tree/fc7bce9b4ada6ef123b899ed88889923c9fafae6\r\non aarch64 without needing the r1.14.0 build-fix (setting -flax-vector-conversions).\r\n\r\nThanks\r\n", "mark this issue as fixed, plz reopen if any other issue occurs."]}, {"number": 26730, "title": "tflite GPU only support  one batch input?if not,how can I set my mult-dim inputs like [16,256,256,3]", "body": "here is my log in java:\r\njava.lang.IllegalArgumentException: Internal error: Failed to apply delegate: GpuDelegate Prepare: First dimension is supposed to be BATCH and always equal to 1.Node number 19 (GpuDelegate) failed to prepare.\r\n", "comments": ["@sunzhe09 \r\n\r\nThanks for trying out the GPU delegate.  Unfortunately, batching was not fully supported at the time of building the dev preview.  Since the release of the dev preview, we made some progress in that direction and you will most likely be able to use the batch dimension in our open source release.", "@sunzhe09 \r\n\r\nYou might be able to use batch > 1 if you set `dynamic_batching_enabled` to `1`.  That applies to C++ interface though.  I don't think we did the plumbing for Java API side yet...", "Thx\uff01"]}, {"number": 26729, "title": "Reorganised the code for FloorMod.", "body": "This was one of the TODO items.", "comments": ["@haozha111 , thanks for the  review, the original one also has this. Kindly check and approve.", "@haozha111 , thanks for the review, i have shifted the 'struct FloatMod' local to function 'FloorMod'. I tried other methods as well but they were increasing the code size. Hope this is ok. Kindly check and approve.\r\n\r\nRegards\r\nAmit", "@amitsrivastava78 can you please rebase your branch", "@rthadur , i have re-based the branch and up-steamed the code.\r\n@haozha111 , can you please approve it again.\r\n\r\nRegards\r\nAmit", "@haozha111 , thanks for the review i have updated the code as per your comments, kindly check and approve.\r\n\r\nRegards\r\nAmit"]}, {"number": 26728, "title": "libc.so.6 : GLIBC_2.14 not found. Instruct Tensorflow to look for glibc in custom path.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.6.2\r\n- Installed using virtualenv? pip? conda?: Still at build phase\r\n- Bazel version (if compiling from source): 0.20.0\r\n- GCC/Compiler version (if compiling from source): gcc-5.5.0 (installed from Linuxbrew)\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: 16GB RAM\r\n\r\n\r\n**Describe the problem**\r\n\r\nMy computer has a rather old `glibc-2.12` and since I'm not an admin, there's nothing I can do to change that. Upon trying to build Tensorflow, I'd get an error along the lines of `$SECRET_PATH/lib64/libc.so.6 : GLIBC_2.14 not found`, which implies that the Tensorflow build system looks for `glibc` in the standard system paths.\r\n\r\nHowever, I have been able to install `glibc-2.23` into `$HOME/.linuxbrew`. I cannot simply do `LD_LIBRARY_PATH=$HOME/.linuxbrew/lib:$LD_LIBRARY_PATH <tensorflow build commands>` because it would segfault all the other utilities.\r\n\r\nHow do I instruct the Tensorflow build system to look for `glibc` within `$HOME/.linuxbrew`? Which files should I edit?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\n$ cd ~\r\n$ mkdir build-bazel\r\n$ cd build-bazel\r\n$ wget https://github.com/bazelbuild/bazel/releases/download/0.20.0/bazel-0.20.0-dist.zip\r\n$ unzip bazel-0.20.0-dist.zip\r\n$ env EXTRA_BAZEL_ARGS=\"--host_javabase=@local_jdk//:jdk\" bash ./compile.sh\r\n$ cd ~\r\n$ mkdir build-tensorflow\r\n$ cd build-tensorflow\r\n$ wget https://codeload.github.com/tensorflow/tensorflow/zip/v1.13.1 -O tensorflow-1.13.1.zip\r\n$ unzip tensorflow-1.13.1.zip\r\n$ cd tensorflow-1.13.1\r\n$ PATH=$HOME/build-bazel/output:$PATH ./configure\r\n$ PATH=$HOME/build-bazel/output:$PATH bazel build --config=monolithic //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nIn another issue #26432, I had succeeded in building Tensorflow with Python 3.7.2 (installed from Linuxbrew, and likely linked with the Linuxbrew `glibc-2.23`). In this issue, however, I am building it with Python 3.6.2 (official office Python), which I suspect may have been linked with the old `glibc-2.12`.\r\n\r\nFor various reasons I need to build with Python 3.6.2.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I think you can use these options of bazel to direct it to use different toolchains.\r\nBefore trying each environment change, you may need to run `bazel clean --expunge`\r\nhttps://docs.bazel.build/versions/master/be/make-variables.html#custom_variables", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26728\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26728\">No</a>\n"]}, {"number": 26727, "title": "[Feature] Support RISCV with tfcompile --target_tuple", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.13.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nthird_party/llvm/llvm.autogenerated.BUILD supports AArch64, ARM, PowerPC, and X86 in llvm_target_list, so I can use 'tfcompile --target_triple=aarch64-none-android' to generate  AArch64 ELF object.\r\nIt seems the tool that generates llvm.autogenerated.BUILD is not released yet, I suggest adding a RISCV target in llvm.autogenerated.BUILD.\r\n\r\n**Will this change the current api? How?**\r\nNo.\r\n**Who will benefit with this feature?**\r\nPeople who use RISCV cpus.\r\n**Any Other info.**\r\n", "comments": ["@jpienaar do you think we can add this?\r\nI am guessing there may be other build bugs we can encounter, but after llvm.autogenerated.BUILD is updated, the community can take over and fix those.", "Yes it could be added. As Gunan mentioned a caveat is that it would be best effort.\r\n\r\n@bixia1 for visibility as she knows this area better than me.", "@arcbbb Could you please check with the latest TF v2.6.0 ? Please refer to the similar [issue  ](https://github.com/tensorflow/tensorflow/issues/47622) and let us know if it helps? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26727\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26727\">No</a>\n"]}, {"number": 26726, "title": "densetnet.pb not correct result", "body": "Hi, \r\nI download to model from https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/densenet_2018_04_27.tgz\r\nThere are:\r\n* densenet.pb\r\n* label.txt\r\n* readme.md (said that densenet.pb use vgg_preprocess)\r\ninside `densenet_2018_04_27`:\r\n\r\n I try one random picture `bike.jpg`, the result top1 is `828`. However, the 828-th category in `label.txt` is not something like bike.\r\n\r\nMy proprocess is:\r\n```\r\n img=cv2.imread(img_path)\r\n img=cv2.resize(img_path,(224,224))\r\n img=img.astype(np.float32)[...,::-1]\r\n img-=[123.68,116.78,103.94]\r\n img/=128.\r\n```\r\n", "comments": ["@lyuchuny3 \r\nThis is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. If you think this could be a bug then please fill the template and provide a code (GitHub gist) to reproduce the bug. Thanks!"]}, {"number": 26725, "title": "BaseCollectiveExecutor::StartAbort Invalid argument: Upper bound check fail is reported with CollectiveAllReduceStrategy", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  CentOS Linux release 7.4.1708 (Core)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): ('v1.13.1-0-g6612da8951', '1.13.1')\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: Tesla P100, 16G\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nWhen running transformer within the tensor2tensor library with CollectiveAllReduceStrategy via little customized change for distributed training in t2t only, it reported below error:\r\n\r\nINFO:tensorflow:Graph was finalized.\r\nWARNING:tensorflow:From /home/xh/tf-1.13.1/lib/python2.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\nINFO:tensorflow:Restoring parameters from /home/xh/datasets/t2t_train_enzh_multi-gpu-ok/translate_enzh_wmt32k/transformer-transformer_base/model.ckpt-179000\r\n2019-03-14 15:30:36.585213: I tensorflow/core/distributed_runtime/master_session.cc:1192] Start master session 4c2e055e28f41c2f with config: device_filters: \"/job:worker/task:0\" device_filters: \"/job:worker/task:0\" gpu_options { per_process_gpu_memory_fraction: 0.95 } allow_soft_placement: true graph_options { optimizer_options { global_jit_level: OFF } rewrite_options { scoped_allocator_optimization: ON scoped_allocator_opts { enable_op: \"CollectiveReduce\" enable_op: \"CollectiveReduce\" enable_op: \"CollectiveReduce\" enable_op: \"CollectiveReduce\" } } } experimental { collective_group_leader: \"/job:worker/replica:0/task:0\" }\r\nWARNING:tensorflow:From /home/xh/tf-1.13.1/lib/python2.7/site-packages/tensorflow/python/training/saver.py:1070: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file utilities to get mtimes.\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Initialize strategy\r\nINFO:tensorflow:Saving checkpoints for 179000 into /home/xh/datasets/t2t_train_enzh_multi-gpu-ok/translate_enzh_wmt32k/transformer-transformer_base/model.ckpt.\r\n2019-03-14 15:32:18.009875: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\r\n2019-03-14 15:32:28.447121: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:101] Filling up shuffle buffer (this may take a while): 387 of 512\r\n2019-03-14 15:32:31.537519: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:140] Shuffle buffer filled.\r\n2019-03-14 15:32:31.695275: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Invalid argument: Upper bound check fail for input 1 from node training/gradients/transformer/parallel_0_5/transformer/transformer/symbol_modality_32610_512_1/softmax/concat_grad/Slice_7 to node scoped_allocator_concat_232 input bounds = [0x7fb060a84f00, 0x7fb060e7ff00] backing_tensor bounds = [0x7faf8da62700, 0x7faf8fe35700]\r\n\t [[{{node scoped_allocator_concat_232}}]]\r\n2019-03-14 15:32:31.708208: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Invalid argument: Upper bound check fail for input 1 from node training/gradients/transformer/parallel_0_5/transformer/transformer/symbol_modality_32610_512_1/softmax/concat_grad/Slice_7 to node scoped_allocator_concat_232 input bounds = [0x7fb060a84f00, 0x7fb060e7ff00] backing_tensor bounds = [0x7faf8da62700, 0x7faf8fe35700]\r\n\t [[{{node scoped_allocator_concat_232}}]]\r\n\t [[{{node _send_add_1_0}}]]\r\nInvalidArgumentError: InvalidA...ntError()\r\n\r\n**Describe the expected behavior**\r\nWhat does above error log mean, and is there any way to fix it? thanks.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Also seeing this bug when using tf.contrib.distribute.CollectiveAllReduceStrategy, any information on what it means?", "Any update on this issue? Even I am facing the exact same error. @isaacnoble @jvishnuvardhan ", "Ping @poxvoculi; looks like a ScopeAllocator issue.", "@poxvoculi by any chance did you get time to look into this issue? @byronyi any workaround you can suggest for this? ", "Pinging @dubey too", "Even I'm facing the same issue. Do update us as soon as you find a solution.", "Even I'm facing the same with tensorflow version=1.11 Python=3.6", "Thanks for filing the issue.  [`589fb0dfdb`](https://github.com/tensorflow/tensorflow/commit/589fb0dfdb694d6318784e1523672fc2833b736d) fixed a similar issue in `ScopedAllocator`.  Can you try running with a nightly build and let us know if you still see this bug?  If yes, please also post instructions on how to reproduce.", "> Thanks for filing the issue. [`589fb0dfdb`](https://github.com/tensorflow/tensorflow/commit/589fb0dfdb694d6318784e1523672fc2833b736d) fixed a similar issue in `ScopedAllocator`. Can you try running with a nightly build and let us know if you still see this bug? If yes, please also post instructions on how to reproduce.\r\n\r\nHi @dubey ,\r\n\r\nI have a constraint where I need to work on conda tensorflow v1.11 \r\n\r\nFollowing are the steps to reproduce the error with tensorflow v1.11\r\n1. Run the worker1.ipynb on node 1 \r\n[worker1.zip](https://github.com/tensorflow/tensorflow/files/3062532/worker1.zip)\r\n\r\n2. Run the worker2.ipynb on node 2 \r\n[worker2.zip](https://github.com/tensorflow/tensorflow/files/3062533/worker2.zip)\r\n\r\nNote: Do modify the TF_Config variable according to your cluster specs in both the jupyter files\r\n\r\nYou will get errors [https://gist.github.com/vishald527/5c464addba66188625ea0088853db970](url) and [https://gist.github.com/vishald527/4c06f908651cae57b8a9f72996ea834d](url) on node 1 and 2 respectively.\r\n\r\nAfter making the changes as suggested in [https://github.com/tensorflow/tensorflow/commit/f10b00558de87020554c9c0512537dab96dba918](url) re-run the jupyter files.\r\n\r\nNow you will get this [https://gist.github.com/vishald527/616c1dc32f20d8226d712f4552b025fa](url) error on node 1 and this [https://gist.github.com/vishald527/d47696e61c3f31f4d3063ff48dffaea7](url) log on node 2.\r\n\r\nPost making the changes suggested here [https://github.com/tensorflow/tensorflow/commit/e692dda4c8b199555e2fa32132a7784e0893c870](url) re-run the jupyter files.\r\n\r\nNow you will get this [https://gist.github.com/vishald527/b1a0d7cb436f70902594dbd742701794](url) error on node 1 and this [https://gist.github.com/vishald527/6dc8b8719b4a67e0ee6dabd3a61ad779](url) on node 2.\r\n\r\nWe even tried building tensorflow v1.11 after making the changes you had suggested here [https://github.com/tensorflow/tensorflow/commit/589fb0dfdb694d6318784e1523672fc2833b736d](url), but we are again getting the same Upper/Lower bound check fail error on both nodes.\r\n\r\nRequest you to please suggest what fixes are to be done in tensorflow v1.11 to get rid of this error and get CollectiveAllReduceStrategy up and running.", "@vishald527 I am not a member of the TF team but I believe they do not accept bugfix PR on release branches for experimental features, a.k.a. those in the contrib folder.\r\n\r\n`CollectiveAllReduceStrategy` was an experimental feature in r1.11, so you probably need to cherry pick the bugfix patches, and backport them to r1.11. I am not sure how hard it is though.\r\n\r\nPlease correct me if I am wrong.", "@byronyi got your point but how to get a patch for this particular error? i am getting this error in v1.11 and the person who raised this issue got the same error in v1.13.\r\nThe patch for this error which @dubey had mentioned above didn't work.\r\n\r\nAlso, do we say that currently tensorflow does not support a stable multi node gpu based synchronous distributed training strategy at all?", "> @byronyi got your point but how to get a patch for this particular error? i am getting this error in v1.11 and the person who raised this issue got the same error in v1.13.\r\n> The patch for this error which @dubey had mentioned above didn't work.\r\n\r\nI do not know. I would suggest you to upgrade your TF version.\r\n\r\n> Also, do we say that currently tensorflow does not support a stable multi node gpu based synchronous distributed training strategy at all?\r\n\r\nIt depends on your definition to \"stable\". It was an experimental feature in r1.11, it is still an experimental feature right now. Use it at your own risk. ", "> I do not know. I would suggest you to upgrade your TF version.\r\n\r\nTF v1.13 already has the same error and TF v2.0 does not have this strategy..so not sure which TF version you are suggesting to upgrade.\r\n\r\n> It depends on your definition to \"stable\". It was an experimental feature in r1.11, it is still an experimental feature right now. Use it at your own risk.\r\n\r\nBy stable I meant a strategy which would at least start synchronous distributed training on a multiple node gpu based system. Currently we are in a situation where we don't have a single strategy which even starts distributed training on multiple nodes synchronously. Please do correct me if I am wrong.\r\n\r\nAlso is there any method you are aware of by which we can run distributed training on gpu based multiple node system?", "> TF v1.13 already has the same error and TF v2.0 does not have this strategy..so not sure which TF version you are suggesting to upgrade.\r\n\r\nCollectiveAllReduce is exposed as `tf.distribute.experimental.MultiWorkerMirroredStrategy` in TF 2.0. See [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/distribute/collective_all_reduce_strategy.py#L48) for details.\r\n\r\n> Also is there any method you are aware of by which we can run distributed training on gpu based multiple node system?\r\n\r\nApart from `MultiWorkerMirroredStrategy`, you could try `ParameterServerStrategy` as well. They are both in the `tf.distribute.experimental` subpackage.\r\n", "> CollectiveAllReduce is exposed as tf.distribute.experimental.MultiWorkerMirroredStrategy in TF 2.0. See here for details.\r\n\r\nTF 2.0 i guess requires CUDA 10 and I have CUDA 9.2 installed. Upgrading CUDA is not an option for me right now and that is why I need help on fixing this reported issue for TF 1.11/1.13. Any pointers on who might be able to help resolving this issue?\r\n\r\n> Apart from MultiWorkerMirroredStrategy, you could try ParameterServerStrategy as well. They are both in the tf.distribute.experimental subpackage.\r\n\r\nParameterServerStrategy is an asynchronous technique. I am looking for a synchronous one. ", "> TF 2.0 i guess requires CUDA 10 and I have CUDA 9.2 installed. Upgrading CUDA is not an option for me right now and that is why I need help on fixing this reported issue for TF 1.11/1.13. Any pointers on who might be able to help resolving this issue?\r\n\r\nYou could recompile your TF using CUDA 9.2.", "> You could recompile your TF using CUDA 9.2.\r\n\r\nWill try that too.\r\n\r\n@dubey any luck on finding the cause and fix for this issue?", "Apologies for the delay, this was a bit tricky to debug and fix.  I just submitted [`a3d2ee6`](https://github.com/tensorflow/tensorflow/commit/a3d2ee6ada53fdfde96a76d635c721629acb9582) which should fix the bounds check failure.  Let us know if it works for you.", "@dubey do I need to make both the code changes you had given and then try or only the latest one?", "I'd recommend patching both changes.", "@dubey \r\nI used a recent built(6 days ago) `nightly-gpu` docker image, this error still exists", "@royxue thanks for the update.  Can you post the error message you get with the recent image, as well as instructions to reproduce the error?", "I will try run again with the most recent docker image, and get back to you the result", "@dubey Here is the logs.\r\nI slightly modified the TF object detection. And run the collective allreduce strategy with 4 workers (1 gpu per worker) on k8s.\r\n\r\n```\r\n2019-04-24 03:09:04.405555: W tensorflow/core/common_runtime/base_collective_executor.cc:215] BaseCollectiveExecutor::StartAbort Invalid argument: Upper bound check fail for input 1 from node gradients/AddN_20 to node scoped_allocator_concat_100_16 input bounds = [0x7fdbfcfb0f00, 0x7fdbfcff8f00] backing_tensor bounds = [0x7fdbea25fd00, 0x7fdbea2f7900]\r\n\t [[{{node scoped_allocator_concat_100_16}}]]\r\n\t [[global_norm/global_norm_G6542]]\r\n2019-04-24 03:09:04.405608: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Invalid argument: Upper bound check fail for input 1 from node gradients/AddN_20 to node scoped_allocator_concat_100_16 input bounds = [0x7fdbfcfb0f00, 0x7fdbfcff8f00] backing_tensor bounds = [0x7fdbea25fd00, 0x7fdbea2f7900]\r\n\t [[{{node scoped_allocator_concat_100_16}}]]\r\n\t [[global_norm/global_norm_G6542]]\r\n2019-04-24 03:09:04.405617: W tensorflow/core/common_runtime/base_collective_executor.cc:215] BaseCollectiveExecutor::StartAbort Invalid argument: Upper bound check fail for input 1 from node gradients/AddN_20 to node scoped_allocator_concat_100_16 input bounds = [0x7fdbfcfb0f00, 0x7fdbfcff8f00] backing_tensor bounds = [0x7fdbea25fd00, 0x7fdbea2f7900]\r\n\t [[{{node scoped_allocator_concat_100_16}}]]\r\n\t [[global_norm/global_norm_G6542]]\r\n2019-04-24 03:09:04.405728: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Invalid argument: Upper bound check fail for input 1 from node gradients/AddN_20 to node scoped_allocator_concat_100_16 input bounds = [0x7fdbfcfb0f00, 0x7fdbfcff8f00] backing_tensor bounds = [0x7fdbea25fd00, 0x7fdbea2f7900]\r\n\t [[{{node scoped_allocator_concat_100_16}}]]\r\n\t [[global_norm/global_norm_G6542]]\r\n2019-04-24 03:09:04.405741: W tensorflow/core/common_runtime/base_collective_executor.cc:215] BaseCollectiveExecutor::StartAbort Invalid argument: Upper bound check fail for input 1 from node gradients/AddN_20 to node scoped_allocator_concat_100_16 input bounds = [0x7fdbfcfb0f00, 0x7fdbfcff8f00] backing_tensor bounds = [0x7fdbea25fd00, 0x7fdbea2f7900]\r\n\t [[{{node scoped_allocator_concat_100_16}}]]\r\n\t [[global_norm/global_norm_G6542]]\r\n2019-04-24 03:09:04.405823: W tensorflow/core/framework/op_kernel.cc:1455] OP_REQUIRES failed at collective_ops.cc:223 : Invalid argument: Upper bound check fail for input 1 from node gradients/AddN_20 to node scoped_allocator_concat_100_16 input bounds = [0x7fdbfcfb0f00, 0x7fdbfcff8f00] backing_tensor bounds = [0x7fdbea25fd00, 0x7fdbea2f7900]\r\n\t [[{{node scoped_allocator_concat_100_16}}]]\r\n\t [[global_norm/global_norm_G6542]]\r\n2019-04-24 03:09:04.405885: W tensorflow/core/framework/op_kernel.cc:1455] OP_REQUIRES failed at collective_ops.cc:223 : Invalid argument: Upper bound check fail for input 1 from node gradients/AddN_20 to node scoped_allocator_concat_100_16 input bounds = [0x7fdbfcfb0f00, 0x7fdbfcff8f00] backing_tensor bounds = [0x7fdbea25fd00, 0x7fdbea2f7900]\r\n\t [[{{node scoped_allocator_concat_100_16}}]]\r\n\t [[global_norm/global_norm_G6542]]\r\n2019-04-24 03:09:04.408029: W tensorflow/core/common_runtime/base_collective_executor.cc:215] BaseCollectiveExecutor::StartAbort Invalid argument: Upper bound check fail for input 1 from node gradients/AddN_20 to node scoped_allocator_concat_100_16 input bounds = [0x7fdbfcfb0f00, 0x7fdbfcff8f00] backing_tensor bounds = [0x7fdbea25fd00, 0x7fdbea2f7900]\r\n\t [[{{node scoped_allocator_concat_100_16}}]]\r\nTraceback (most recent call last):\r\n  File \"/models/object_detection/model_main.py\", line 127, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/models/object_detection/model_main.py\", line 123, in main\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 464, in train_and_evaluate\r\n    estimator, train_spec, eval_spec, _TrainingExecutor)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/estimator_training.py\", line 290, in train_and_evaluate\r\n    session_config=run_config.session_config)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 360, in _run_single_worker\r\n    return worker_fn(strategy)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/estimator_training.py\", line 252, in _worker_fn\r\n    hooks=hooks)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 362, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1152, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1215, in _train_model_distributed\r\n    self._config._train_distribute, input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1325, in _actual_train_model_distributed\r\n    saving_listeners)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1402, in _train_with_estimator_spec\r\n    estimator_spec, worker_hooks, saving_listeners)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1348, in _train_with_estimator_spec_distributed\r\n    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 754, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1252, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1353, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1338, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1411, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1169, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 948, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1171, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1348, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1368, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Upper bound check fail for input 1 from node gradients/AddN_20 to node scoped_allocator_concat_100_16 input bounds = [0x7fdbfcfb0f00, 0x7fdbfcff8f00] backing_tensor bounds = [0x7fdbea25fd00, 0x7fdbea2f7900]\r\n\t [[{{node scoped_allocator_concat_100_16}}]]\r\n\t [[global_norm/global_norm_G6542]]\r\n```", "Can you provide a bare minimum test case that reproduces this error?\r\n\r\nFor some context, the scoped allocator is a static optimization that converts multiple collective all-reduce ops in to a single all-reduce.  In this optimization, we pre-allocate a buffer that is large enough to hold all the inputs to the all reduce.  We add an attribute to each OpKernel whose output takes part in this all-reduce indicating that the output should be assigned from a slice of this pre-allocated buffer.  The bounds check failure is a runtime error that indicates that the OpKernel somehow circumvented this process and used a different buffer.\r\n\r\n[`a3d2ee6`](https://github.com/tensorflow/tensorflow/commit/a3d2ee6ada53fdfde96a76d635c721629acb9582) fixed the case when an OpKernel calls `set_output` instead of `allocate_output`.\r\n\r\nIn this example, it looks like the input to the all-reduce comes from an AddN op.  I tried a small test case which fuses 2 ops using the scoped allocator whose inputs are both AddN, but my test case passed.", "@dubey \r\nI was checking the solution yesterdays. If I started training from a checkpoint, this error happened. But is I started training from scratch this error will not happen. However the training process will hang up after the graph is initialized. Im thinking this is may caused by the conflicts between `slim.argscope`(object detection api used) and `distributed strategy`. As mentioned, the strategy do not support deprecated `slim` api.", "Is this resolved or is it still an issue? ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26725\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26725\">No</a>\n"]}, {"number": 26724, "title": "[ROCm] Enable ROCm support for \"snapshot\" op", "body": "This PR enables ROCm support for the \"snapshot\" op.\r\n\r\nPR #26722  is a pre-req for this PR, and hence this PR includes commits from that PR.\r\nOnly the last commit in this PR should be reviewed here (as all others will be reviewed as part of PR #26722  )\r\n\r\n--------------------------\r\n\r\n@tatianashp , @whchung : just FYI", "comments": ["rebased to remove merge conflicts", "@deven-amd Please do not add Eigen patches to TensorFlow. Submit these to Eigen instead and add me as a reviewer. The same goes for all PRs.", "> @deven-amd Please do not add Eigen patches to TensorFlow. Submit these to Eigen instead and add me as a reviewer. The same goes for all PRs.\r\n\r\n@rmlarsen , cc'd you on the Eigen PR \r\n(PR No 546 : https://bitbucket.org/eigen/eigen/pull-requests/546/rocm-hip-specfic-fixes-updates/diff )", "@deven-amd Thanks!", "@rmlarsen \r\n\r\nAlso please advise on how to proceed with PR #26722  (the PR that is tracking the edit to the eigen patch file. this PR requires that PR and hence includes its commits)\r\n\r\nNot including the eigen patch update will break the `--config=rocm` build, which will effect all PRs for which PR #26722 is a pre-requisite.\r\nAll of them will need to wait until \r\n* the Eigen PR I mentioned above gets merged and \r\n* the TF eigen pointer moves to/beyond the commit containing the Eigen PR merge\r\n...that could take a while!\r\n\r\nCan we come up with a solution that will allow for minor + temporary updates to the eigen patch file (both for now and going forward?\r\nThis is required for future too because:\r\n* Sometimes new additions to the TF code break Eigen compilation under HIP, and sometimes new commits to the Eigen codebase do the same.\r\n* Since we can only find out about it after the fact, it necessitates the need for temporary updates to the patch file (in the ROCm TF fork) to keep the ROCm TF fork working.  \r\n  * Typically we file a PR to upstream the fix to Eigen, and then remove the patch file once the TF eigen pointer moves to a point, where it is pulling in the fix\r\n* Now that we are upstreaming ROCm support, we need to continue to have a similar capability to keep the `--config=rocm` build working on the tip (when Eigen+HIP compilation gets broken due to TF or eigen updates)\r\n\r\nWe do run nightly Eigen unit tests (see `prj47-rack-50` results in the Eigen dashboard : http://manao.inria.fr/CDash/index.php?project=Eigen), \r\n\r\n", "@deven-amd Yeah, we need to upstream the last few bits in that patch file. I'll do it today or Monday.", "rebased to remove merge conflicts", "rebased PR to account for updates to PR #26722 ", "rebased PR to resolve merge-conflicts post the merge for PR #26722 .\r\n\r\nNote that now PR #28116 is a pre-requisite for this PR. This PR includes commits from PR #28116", "rebased PR to the tip of master (since all the pre-reqs are now merged).\r\n\r\nthe changes in this PR are now trivial...please approve and merge.\r\n\r\nthanks\r\n\r\ndeven"]}, {"number": 26723, "title": "BUG: fix, layer_test doesn't accept layer_cls key", "body": "The bug is reported on https://github.com/tensorflow/addons/pull/92", "comments": ["@fchollet Thank you, I think unit test has been added, could you take a look again?", "Can one of the admins verify this patch?", "@facaiy Can you please resolve conflicts? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}]