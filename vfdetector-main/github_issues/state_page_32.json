[{"number": 48484, "title": "einsum with ellipsis is slow", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: No\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16-20\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:N/A\r\n-   **TensorFlow installed from (source or binary)**: N/A\r\n-   **TensorFlow version (use command below)**: 2.4.1\r\n-   **Python version**: 3.5-3.8\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**: see https://colab.research.google.com/drive/14pGREWEo2zd4j5jDJIxEERfNcpLBGuvv?usp=sharing\r\n\r\n### Describe the problem\r\nRuntime of einsum heavily depends on the usage of an ellipsis (Both on GPU and CPU)\r\n```python\r\na = tf.random.normal([100,32,32,32,3])\r\ne = tf.random.normal([100,3,3])\r\n\r\nx=timer()\r\nR = tf.einsum('b...l,blr->b...r',a,e)\r\nprint(timer()-x)\r\n# output: 0.28\r\n\r\nx=timer()\r\nR = tf.einsum('bijkl,blr->bijkr',a,e)\r\nprint(timer()-x)\r\n# output: 0.0008\r\n```\r\n\r\n### Source code / logs\r\nhttps://colab.research.google.com/drive/14pGREWEo2zd4j5jDJIxEERfNcpLBGuvv?usp=sharing\r\n\r\n\r\n", "comments": ["Thanks for filing this.  FYI here is the implementation `third_party/tensorflow/core/kernels/linalg/einsum_op_impl.h`.  Marking contribution welcome."]}, {"number": 48474, "title": "Possible race condition in nccl_manager_test", "body": "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/nccl/nccl_manager_test.cc#L157\r\n\r\n``in_cpu`` is not referenced and goes out of scope (line 159) right after the ``stream->ThenMemcpy`` is called\r\n\r\nThe source cpu buffer may become invalid before the actual H2D memory transfer start.\r\n\r\nThis test failed in our in-house platform. I'm not sure why this test seems fine on tensorflow's CI.  That piece of code looks buggy to me though.\r\n\r\nThanks\r\nKevin", "comments": ["@kevint324 \r\nCould you please fill the issue template\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the following details\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version:\r\nPython version:\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory\r\n\r\nand the exact sequence of commands / steps that you executed before running into the problem. Thanks!\r\n", "@kevint324 \r\n\r\nHi, Can you provide the above requested details.Thanks\r\n\r\n\r\n\r\n", "OS Platform and Distribution (e.g., Linux Ubuntu 16.04): UBUNTU18\r\nTensorFlow installed from (source or binary): from source\r\nTensorFlow version: 2.4.1\r\nPython version: 3.6\r\nInstalled using virtualenv? pip? conda?: virtualenv\r\nBazel version (if compiling from source): 3.1\r\nGCC/Compiler version (if compiling from source): 7.5\r\nCUDA/cuDNN version:n/a\r\nGPU model and memory: n/a\r\n\r\nThe test failed on our in house platform.\r\nI'd be happy to submit a PR to fix it but maybe someone  could confirm whether it's a valid issue first.\r\nIt is pretty straight forward just by reviewing the code.\r\n", "Hi @kevint324,\r\n\r\nThis is a valid issue, please send a PR.\r\n\r\nThis is passing on our internal CI likely because in this case the NVIDIA memcpy implementation is coping the CPU memory into a staging area at memcpy enqueue time, hiding this problem."]}, {"number": 48469, "title": "tf.keras.layers.Conv2DTranspose raises error when dilation rates are different in two dimensions", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nConv2DTranspose raises AssertionError with diifferent dilation_rate for each dimension.\r\n\r\n**Describe the expected behavior**\r\nExpect no error.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nx = tf.keras.Input([None, None, 16])\r\ntf.keras.layers.Conv2DTranspose(filters=1, kernel_size=32, dilation_rate=(1,2))(x)\r\n```\r\n\r\nOutput:\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"...\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3437, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-32-469167c62d25>\", line 1, in <module>\r\n    tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=32, dilation_rate=(1,2))(x)\r\n  File \"...\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 952, in __call__\r\n    input_list)\r\n  File \"...\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 1091, in _functional_construction_call\r\n    inputs, input_masks, args, kwargs)\r\n  File \"...\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 822, in _keras_tensor_symbolic_call\r\n    return self._infer_output_signature(inputs, args, kwargs, input_masks)\r\n  File \"...\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 863, in _infer_output_signature\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"...\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\", line 1315, in call\r\n    dilation_rate=self.dilation_rate)\r\n  File \"...\\venv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"...\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 5355, in conv2d_transpose\r\n    assert dilation_rate[0] == dilation_rate[1]\r\nAssertionError\r\n", "comments": ["@lugalUrim \r\nAs per the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose?version=nightly#expandable-1) stride and dilation_rate should be same,if strides=1,dilation_rate=(1,1) or if strides=2,dilation_rate=(2,2)\r\nI [reproduced](https://colab.research.google.com/gist/UsharaniPagadala/37d997f5b9e66e26e75bacc5d323cdbe/untitled13.ipynb#scrollTo=mGOQcl_1pNK-) it by changing stride and dilation_rate as mentioned above.\r\nCould you please check and let us know if it helps.Thanks\r\n", "Thanks @UsharaniPagadala . I suppose you are suggesting that `dilation_rate` and `strides` should be the same for all spatial dimension? \r\n\r\nI was trying `dilation_rate=(1,2)` (without specifying strides) in order to have different dilation rate for width and height dimension. \r\n\r\nFor example pytorch's Conv2d supports `dilation=(3, 1)`.\r\nhttps://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d", "I believe it's possible to have different dilation rates for width and height dimension. For example, in conv3d `dilation_rate=(1,2,1)` works:\r\n```\r\nimport tensorflow as tf\r\nx = tf.keras.Input([None, None, None, 16])\r\ntf.keras.layers.Conv3DTranspose(filters=1, kernel_size=32, dilation_rate=(1,2,1))(x)\r\n```", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hi everyone, any update on this issue?"]}, {"number": 48454, "title": "Support for convolutional ops for complex variable inputs", "body": "**System information**\r\n- TensorFlow version (you are using): 2.4\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI hope to add support for complex variable optimization to Tensorflow. I have made a custom optimizer that is capable of C->C domain convergence. Currently, when I pass a complex tensor to tf.nn.convolution, I get a NotOkStatusException (complex64 not supported), but it doesn't say where the code is failing. According to the documentation, the Convolution class is simply an implementation of Cross-Correlation rather than a convolution, but even NumPy's correlate (and convolve) supports complex inputs. I do not see where the code is failing exactly and why?\r\n\r\n**Will this change the current api? How?**\r\nThis will add support for complex64 and complex128 datatypes to some ops involved in convolution operations\r\n\r\n**Who will benefit with this feature?**\r\nAny signal processing specialist would be able to use complex inputs directly rather than separating and stacking the real and imaginary components separately. Pytorch supports complex domain optimizations, so doing the same for Tensorflow would be beneficial for the community.\r\n", "comments": ["Thanks for working on this! I just skimmed and it seems like the kernel implemenations are at `third_party/tensorflow/core/kernels/conv_ops*` and we need to add complex type support for those kernels.", "@kkimdev Yes, those are the kernels but I'm not sure where exactly the support should be added. Does [this](https://github.com/tensorflow/tensorflow/blob/c0abef1d844a93d0c2d4adbed8e5dd8158e6f77c/tensorflow/core/ops/nn_ops.cc#L335) list define all the supported types? What are the dependencies for these ops? Is there another file that specifies the input types for Convolutional operations? I've been through ```third_party/tensorflow/core/kernels/conv ops*```, but I'm not sure if I fully understand everything in those files", "You can read about general TensorFlow op implementations here https://www.tensorflow.org/guide/create_op (the doc is for custom ops, but pretty similar), and here are some complex op support examples:\r\n- https://github.com/petewarden/tensorflow_makefile/blob/64380a0/tensorflow/core/ops/array_ops.cc#L167-L171\r\n- https://github.com/tensorflow/tensorflow/blob/6e9220c/tensorflow/core/ops/math_ops.cc#L140-L155"]}, {"number": 48452, "title": "Manipulate Input/Output of TF model ", "body": "### 1. System information\r\n\r\n- Mac OS 11.2.3\r\n- TensorFlow 2.4.1 from COLAB\r\n- Android 10\r\n- MLKit by Google object-detection-custom:16.3.1\r\n\r\n### 2. The problem \r\n\r\nI need to manipulate TensorFlow models to make them compatible with MLKit by Google for my Android Application.\r\nA TFlite model is compatible with MLKit if the Tensors are like that:\r\nINPUT\r\n- The model must have only one input tensor with the following constraints:\r\n- The data is in RGB pixel format.\r\n- The data is UINT8 or FLOAT32 type. If the input tensor type is FLOAT32, it must specify the NormalizationOptions by attaching Metadata.\r\n- The tensor has 4 dimensions : BxHxWxC, where: B is the batch size. It must be 1 (inference on larger batches is not supported). W and H are the input width and height. C is the number of expected channels. It must be 3.\r\n\r\nOUTPUT\r\n- The model must have at least one output tensor with N classes and either 2 or 4 dimensions: (1xN) , (1x1x1xN)\r\n\r\nWith this TFlite model [ssd_mobilenet_v1](https://tfhub.dev/tensorflow/lite-model/ssd_mobilenet_v1/1/metadata/2)\r\nCurrently when I use a model with different predicted output (The problem is mainly in the output), I have the this error:\r\n` com.google.android.gms.tasks.RuntimeExecutionException: com.google.mlkit.common.MlKitException: Failed to initialize detector. Unexpected number of dimensions for output index 0: got 3D, expected either 2D (BxN with B=1) or 4D (BxHxWxN with B=1, W=1, H=1).\r\n` \r\n\r\nI'm new to TensorFlow but I know that TF models can be converted using the TFlite converter but I don't know how to use for manipulating the Input/Ouput. Can someone explain me how to interpret the commands to give to the converter?\r\n\r\n#### Other Infos\r\n\r\nTF model to convert: [SSD MobileNetV2](https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2). From all the outputs I need only this: \"detection_scores: a tf.float32 tensor of shape [N] containing detection scores.\"\r\n", "comments": ["@mrjoelc  Please share simple stand alone code such that we can replicate the issue faced or if possible share a colab gist.", "Well currently I'm working with Android and MLKit so for me is difficult to give you something to replicare the issue.\r\nThe only thing I can provide you is this [colab](https://colab.research.google.com/drive/125ifeqPU4hHo9On7mDiZkkC48Zi_h3PR?usp=sharing)\r\nHere I try to convert the model from format TF2 to TFlite.\r\n\r\nHow can I set the output tensor to be like 2D (BxN with B=1) or 4D (BxHxWxN with B=1, W=1, H=1)?\r\nI know is something related with the **converter.optimizations** where currently I'm using **tf.lite.Optimize.DEFAULT**, but I don't know how to customize for having the Input tensor to be like  [B=1xHxWxC=3] and output Tensor to be like said before.\r\n\r\nI know this is possible but I don't know who to do it.", "You can set the batch size by wrapping the model with the concrete function, e.g.,\r\n\r\n```\r\ntf.function(input_signature=(tf.TensorSpec(shape=[1, None,None,3]))\r\ndef batch_size_one_model_func(input):\r\n  return original_model(input)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([batch_size_one_model_func.get_concrete_function()])\r\n...\r\n```\r\n\r\nFor the output tensor, you can add additional TF operators to expand dims, e.g., tf.ExpandDims, in the above tf.function.", "I'm sorry, I may not have been able to express myself.\r\n\r\nWhat I need is to take an SSD type model and convert it into an image classifier. To do that I need to change the output tensor. In general the output tensor of a SSD consist [bounding boxes][scores of bounding boxes][the categories of the detected boxes][#detections] I only need the [scores of bounding boxes] (in my case will be only 1 score for the single image). So I need to save the model in TFlite format with an output [1xN] or [1x1x1xN]\r\n\r\nit is possible?"]}, {"number": 48431, "title": "Didn't find op for builtin opcode 'SUM' version '1'.", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution: Linux Ubuntu 18.04:\r\n- TensorFlow installed from binary\r\n- Tensorflow version : 2.3\r\n- Target platform: ARM 64\r\n\r\n**Describe the problem**\r\nIn the following lines I proceed to describe the steps followed:\r\n\r\n1. Adapt MobileNetv2 model. Loaded for training with Keras API. Take out the last layer by setting include_top = false, and adding a customized last Conv2D layer with the Functional API. This is done in order to apply the convolutional sliding window approach. (Bigger input image, than the images used for training).\r\n2. Training done with success. Translation of the model from TF to TF Lite and it runs and provides a sensible matrix of results.\r\n3. Translation of TFLite model to TF Tiny model (model.cc) by using the xxd -i ... command provided in the documentation.\r\n4. When allocating the model, we encounter the following failure:\r\n\r\n> libraries ready\r\n> STM32 Tensorflow Lite test\r\n> Model working\r\n> Interpreter working\r\n> Didn't find op for builtin opcode 'SUM' version '1'. An older version of this builtin might be supported. Are you using an old TFLite binary with a newer model?\r\n> \r\n> Failed to get registration from op code SUM\r\n>  \r\n> Failed starting model allocation.\r\n> \r\n> Allocate working\r\n> Type input: 1\r\n> Bytes input: 1310720\r\n> Size input: 4\r\n> Dim input 0: 1\r\n> Dim input 1: 512\r\n> Dim input 2: 640\r\n> Dim input 3: 1\r\n\r\nAs seen in the message, dims are correct, type input is correct (Float) and everything else seems to be working fine.\r\n\r\nPrior to this issue, a similar fault appeared as it requested to have the op code EXP. This one is already available in Tensorflow Git and it has been already implemented. (exp.cc, exp_test.cc, exp.h, AddEpx() included in AllOps file and make file modified).\r\n\r\nThe same procedure has been handled for the MobileNetV2 with no modifications and it worked fine. \r\n\r\nI have two main questions: \r\n\r\n- Why do we need extra operators, if we only added one extra Conv2D layer?\r\n- Is SUM implemented, and will we have further operators needed?\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\nModel configuration in python (Tensorflow + Keras API):\r\n`            model = MobileNetV2(include_top=False,weights=None,input_tensor=Input(shape = (512,640,channels) ,dtype = 'float32'),pooling = None,classes = len(class_labels))`\r\n`            last = Conv2D(filters = 5,kernel_size = 3, padding= 'valid',strides=(1,1),activation='softmax', input_shape = (model.layers[-1].output.shape))(model.layers[-1].output)`\r\n `           model = Model(model.input, last)`\r\n\r\nModel Conversion from Tensorflow to Tensorflow Lite:\r\n`    converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)`\r\n `   converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]`\r\n`    model_no_quant_tflite = converter.convert()`\r\nModel Conversion from Tensorflow Lite to Tensorflow Tiny:\r\n`xxd -i model_mobilenet_sliding2.tflite > model.cc\r\n`\r\n\r\nModel loading in Tensorflow Tiny (C++):\r\n\r\n`namespace{\r\n  tflite::ErrorReporter* error_reporter = nullptr;`\r\n  `const tflite::Model* model = nullptr;`\r\n  `// This pulls in all the operation implementations we need`\r\n  `tflite::AllOpsResolver resolver;`\r\n  `constexpr int kTensorArenaSize = 4 * 1024 * 1024;`\r\n  `uint8_t tensor_arena[kTensorArenaSize];` \r\n   `uint8_t* img = nullptr;`\r\n  `uint64_t timeImage;`\r\n  `uint64_t timeInvoke;`\r\n  `TfLiteTensor* model_input = nullptr;`\r\n  `TfLiteTensor* model_output = nullptr;\r\n}`\r\n\r\n`// Set up logging (modify tensorflow/lite/micro/debug_log.cc)\r\n  static tflite::MicroErrorReporter micro_error_reporter;\r\n  error_reporter = &micro_error_reporter;`\r\n\r\n`  // Say something to test error reporter\r\n  error_reporter->Report(\"STM32 Tensorflow Lite test\");`\r\n\r\n`  model = ::tflite::GetModel(model_mobilenet_sliding2_tflite);\r\n  cout << \"Model working\"<<endl;`\r\n\r\n`  tflite::MicroInterpreter interpreter(model, resolver, tensor_arena,\r\n                                      kTensorArenaSize, &micro_error_reporter);`\r\n\r\n  `cout << \"Interpreter working\" << endl;`\r\n\r\n ` interpreter.AllocateTensors();`\r\n\r\n ` cout << \"Allocate working\" << endl;`\r\n\r\n`model_input = interpreter.input(0);`\r\n\r\n`// Get image from provider.`\r\n    `cout << \"Type input: \" << model_input->type << endl;`\r\n   ` cout << \"Bytes input: \" << model_input->bytes << endl;`\r\n    `cout << \"Size input: \" << model_input->dims->size << endl;`\r\n    `cout << \"Dim input 0: \" << model_input->dims->data[0] <<  endl;`\r\n    `cout << \"Dim input 1: \" <<model_input->dims->data[1] <<  endl;`\r\n    `cout << \"Dim input 2: \" <<model_input->dims->data[2] <<  endl;`\r\n    `cout << \"Dim input 3: \" <<model_input->dims->data[3] <<  endl;`\r\n", "comments": ["@javierFerreroM \r\nCould you please try on tf 2.4.1 or 2.5rc1 and let us know if you still face the issue.", "Good morning @Saduf2019 ,\r\n\r\nI just tested both versions and in both we encounter the same issue. The project is generated out of the hello_world example using the following command:\r\n\r\n`make -f tensorflow/lite/micro/tools/make/Makefile TARGET_ARCH=x86_64 generate_hello_world_make_project`\r\n\r\nI used the same model.cc file for the three TF Tiny versions ( I am assuming that the model is not the issue, but the missing libraries).\r\n\r\nAn interesting point I'd like to highlight is that I have checked that in version 2.5rc1, exp.cc is already present in the kernels folder, and it is already integrated in the make file. The only missing part is the AddExp(); that I added by hand to the All_Ops_resolver.cc\r\n\r\nThus, I integrated it and so, the error of missing operand EXP disappears, but we arrive to the same point with the missing SUM operand.\r\n\r\n> libraries ready\r\n> STM32 Tensorflow Lite test\r\n> Model working\r\n> Interpreter working\r\n> Didn't find op for builtin opcode 'SUM' version '1'. An older version of this builtin might be supported. Are you using an old TFLite binary with a newer model?\r\n> \r\n> Failed to get registration from op code SUM\r\n>  \r\n> Failed starting model allocation.\r\n\r\nTherefore, I guess that the same procedure should be followed, to include the SUM operand, and I am not sure whether new faults of the same characteristics may rise.\r\n\r\nThank you very much, I stand by awainting your response.\r\n"]}, {"number": 48421, "title": "Exploring a baseline Action build", "body": "With this I want to explore a new testing baseline with Github Action and our official CPU `tensorflow/tensorflow:devel` image.\r\n\r\nThe idea is to test in the CI the (more or less) Episodic contributor journey to contribute code to Tensorflow at least on CPU.\r\n\r\nThis is the proposed list of steps:\r\n\r\n*  `tensorflow/tensorflow:devel` image rebuid build (or Dockerhub pull?)\r\n* Code checkout\r\n* `ci_sanity.sh` selected steps (--pylint, --<whatever you want> see https://github.com/tensorflow/tensorflow/pull/48294)\r\n* TF bazel `./configure`\r\n* `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`\r\n* `bazel test //tensorflow/`\r\n\r\nAs the average user it is already experiencing, this will probably require a bazel cache (on [GCS like for TF/IO](https://github.com/tensorflow/io/pull/1294)?) to achieve reasonable compilation times.\r\n\r\nI think that reproducibility and the timing of these build steps will let us to monitor the experience of a Tensorflow episodic contribution.\r\n\r\n/cc @angerson @mihaimaruseac  @theadactyl @joanafilipa", "comments": ["As expected we had a Github Action timeout on the TensorFlow build step after `5h 56m 42s` with only `11,286` compiled target on an estimate of `33563 targets configured`.\r\n\r\nGithub Actions are currently running on a [Standard_DS2_v2](https://docs.microsoft.com/it-it/azure/virtual-machines/dv2-dsv2-series#dsv2-series) machine. \r\n\r\nAs we  already know this is really a bottleneck for an average TF external (Episodic or not) contributor as we ask to reproduce these steps on its own local machine just for preparing an occasional code PR.\r\n\r\nI think that it is important to continuously monitor this Action over time to expect that we could execute it in the expected time that it seems to us reasonable for an Episodic/Average TF contributor.\r\n\r\nSome proposed solutions to enable this action in order of preference:\r\n\r\n- Use a Google hosted [Action runner](https://docs.github.com/en/actions/hosting-your-own-runners) and produce a GCS cache that could be re-used by the action itself and (read-only) by every contributor on his own local-machine when he is working with the official `tensorflow/tensorflow:devel`. A sort of an improvement over https://github.com/tensorflow/io/pull/1294\r\n- Still use Github hosted Action consuming an usable GCS cache produced elsewhere (Where?)\r\n- Let use a bazel python only build and test commands in this Action using all the c/c++ component from a system `pip install ft-nightly`. **This is risky** cause as we build nightly once a day we could have misalignment against current master c/c++ features.\r\n", "Just in the case we want to explore the first option with the self hosted `Github Action runner` on GKE:\r\nhttps://github.com/summerwind/actions-runner-controller\r\nhttps://github.com/evryfs/github-actions-runner-operator/\r\n\r\n", "There is also a Terraform Github Self Hosted Runners on GKE repo maintained by Google Cloud members (/cc @bharathkkb) at https://github.com/terraform-google-modules/terraform-google-github-actions-runners", "/cc @perfinion If we can do some steps together on this.", "Update: We discussed a pilot plan with @perfinion yesterday on [SIG-Build Gitter](https://gitter.im/tensorflow/sig-build).", "I would add one more difficulty: Even with local cache, it seems to be invalidated each time I pull the commits from upstream. ( I think LLVM-related commits like https://github.com/tensorflow/tensorflow/commit/17e6dc2492e7556dd6c2107f14b1e2ba76a8ed34 are the culprits ).", "> I would add one more difficulty: Even with local cache, it seems to be invalidated each time I pull the commits from upstream. ( I think LLVM-related commits like [17e6dc2](https://github.com/tensorflow/tensorflow/commit/17e6dc2492e7556dd6c2107f14b1e2ba76a8ed34) are the culprits ).\r\n\r\nWhat cache command are you using?", "I am using [`--disk_cache`](https://docs.bazel.build/versions/master/remote-caching.html#disk-cache). I noice that I have a much longer build ( around 8~10 hours ) everytime there is one LLVM-related commit ( which is pretty much daily but I don't pull upstream that often ).", "I found that in https://github.com/tensorflow/tensorflow/issues/40505#issuecomment-644841643, @mihaimaruseac said the same thing. Do you have any problem regarding this issue @bhack ?", "> I found that in [#40505 (comment)](https://github.com/tensorflow/tensorflow/issues/40505#issuecomment-644841643), @mihaimaruseac said the same thing. Do you have any problem regarding this issue @bhack ?\r\n\r\nWe are waiting to have a bootstrapped GCS cache for this action produced with a fresh master build in `tensorflow/tensorflow:devel`", "If the llvm sync will totally invalidate the remote bazel cache we cannot use Github Action but we need to use self hosted github Actions as suggested in https://github.com/tensorflow/tensorflow/pull/48421#issuecomment-818335926.", "@bhack  This PR is in draft, any update on this? Please. Thanks!", "@gbaned It is a draft cause as you can see the introduced action go in Timeout on Github. \r\nI am waiting on an agreement on how I could use a GCS readonly cache with the infra/build team /cc @mihaimaruseac @angerson ", "Just for reference, it is going in timeout on this kind of HW resources:\r\n\r\nhttps://docs.github.com/en/actions/using-github-hosted-runners/about-github-hosted-runners#supported-runners-and-hardware-resources"]}, {"number": 48397, "title": "TopK GPU slower than TopK CPU", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: MX150 4GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nTopK GPU slower than TopK CPU\r\n**Describe the expected behavior**\r\nTopK GPU faster than TopK CPU\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n``` python\r\nimport tensorflow as tf\r\n\r\ncls_outputs_reshape = tf.random.uniform((1, 4419360), -30 ,30)\r\ndef top_k_func(*args, **kwargs):\r\n    return tf.math.top_k(*args, **kwargs)\r\ntop_k = tf.function(top_k_func)\r\nwith tf.device('/CPU:0'):\r\n    _, cls_topk_indices = top_k(cls_outputs_reshape, k=5000, sorted=False)\r\nwith tf.device('/GPU:0'):\r\n    _, cls_topk_indices = top_k(cls_outputs_reshape, k=5000, sorted=False)\r\ntf.profiler.experimental.start('./profile_dir')\r\nwith tf.device('/CPU:0'):\r\n    _, cls_topk_indices = top_k(cls_outputs_reshape, k=5000, sorted=False)\r\nwith tf.device('/GPU:0'):\r\n    _, cls_topk_indices = top_k(cls_outputs_reshape, k=5000, sorted=False)\r\ntf.profiler.experimental.stop()\r\n```\r\n``` bash\r\ntensorboard --logdir=./profile_dir\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n![Screenshot from 2021-04-08 16-21-30](https://user-images.githubusercontent.com/17592563/113993013-9226fb80-9886-11eb-912f-c25bc2927295.png)\r\n", "comments": ["@fsx950223 \r\nI ran the code on gpu and do not face any delay, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/ae8068949605e40337134b5178116f7c/untitled.ipynb).", "I modified the [gist](https://colab.research.google.com/drive/1Mmp6qTEWkpAhKq6XjRvMMWpaae4jVdV3?usp=sharing)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Any update? @Saduf2019 ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I am able to replicate the issue reported, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/8d0fff98cbcb5ff6e91242597db5dcae/untitled592.ipynb).", "@ebrevdo (since you wrote the TopK GPU implementation originally) Is this unexpected?  I don't really know if the GPU implementation is expected to work well with k = 5000.", "Some comments here:\r\n\r\n* I'm seeing `Avg. Time` for GPU-based TopKV2 is 11k us, and for CPU-based is 13k us - so it looks like GPU is actually faster?\r\n* For a measurement of total runtime (excludiing profiler) I'm not sure (from that code) whether a copy from/to CPU is happening.  May want to verify this by ensuring the `uniform` call is happening on GPU and remove the `tf.function` AND use %%timeit instead of the profiler.\r\n* We do an extra copy in the GPU codepath this is using [here](https://github.com/tensorflow/tensorflow/blob/5dcfc51118817f27fad5246812d83e5dccdc5f72/tensorflow/core/kernels/topk_op_gpu.h#L450) and it's likely that gpuprim now has support for iterators for ValueT, which would allow us to get rid of an extra device allocation and copy - probably speeding up the sort kernel.  I haven't checked since I wrote this code.", "> Some comments here:\r\n> \r\n> * I'm seeing `Avg. Time` for GPU-based TopKV2 is 11k us, and for CPU-based is 13k us - so it looks like GPU is actually faster?\r\n> * For a measurement of total runtime (excludiing profiler) I'm not sure (from that code) whether a copy from/to CPU is happening.  May want to verify this by ensuring the `uniform` call is happening on GPU and remove the `tf.function` AND use %%timeit instead of the profiler.\r\n> * We do an extra copy in the GPU codepath this is using [here](https://github.com/tensorflow/tensorflow/blob/5dcfc51118817f27fad5246812d83e5dccdc5f72/tensorflow/core/kernels/topk_op_gpu.h#L450) and it's likely that gpuprim now has support for iterators for ValueT, which would allow us to get rid of an extra device allocation and copy - probably speeding up the sort kernel.  I haven't checked since I wrote this code.\r\n\r\nIt's a sort algorithm instead of a topk algorithm and its time complexity is O(nlog(n)) instead of O(nlog(k)). Implement [gpu-topk](https://anilshanbhag.in/static/papers/gputopk_sigmod18.pdf) should enhance the performance and gpu-topk has been integrated with onnx-runtime.", "While I'm not sure the implementation is actually O(nlog(n)) given it's a radix sort (iirc), the fact that it's not a pure top-k algorithm is clearly limiting its performance when `n >> k`.  So it would be great to implement `gpu-topk` or some variant.  I'll mark this as contributions welcome for now.", "Keep in mind we would need a segmented top-k algorithm since we perform multiple parallel top-k operations in general.", "Hey, I am interested in contributing to this project.  Could someone point me to where in the codebase I can find the the current topk function?  Am I correct in assuming that the gpu-topk function is only different from top_k in that its the hash map implementation vs radix Sort?"]}, {"number": 48395, "title": "Allowing strides with dilated convolutions in tf.keras.layers.Conv2D", "body": "**System information**\r\n- TensorFlow version (you are using): v2.4.1\r\n- Are you willing to contribute it (Yes/No): Depends on acceptable solution\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nAt this point in time `tf.keras.layers.Conv2D` doesn't support dilated convolutions with strides > 1, which would be a nice feature to have. This limitation seems to be a limitation of Keras Conv2D, not a Tensorflow. `tf.nn.conv2d` supports dilated convolutions with strides, but all Keras Conv layers use generic `tf.nn.convolution`, which doesn't support dilated convolutions with strides.\r\n\r\nThe most natural solution would probably be creating something like `_conv_implementation` method in base Conv layer and override it using `tf.nn.conv1d`, `tf.nn.conv2d`, `tf.nn.conv3d` in corresponding Keras layers. \r\n\r\n**Will this change the current api? How?**\r\nIt seems that `tf.nn.convolution` and `tf.nn.conv1d`, `tf.nn.conv2d`, `tf.nn.conv3d` behave identically, so it shouldn't change API or break back-compatibility (except now dilated convolutions will be able to use strides).\r\n\r\n**Who will benefit with this feature?**\r\nAnybody who wants to use convolutional neural networks with large receptive field but little computational overhead.\r\n", "comments": ["I found that check here \r\nhttps://github.com/tensorflow/tensorflow/blob/9a2545aecc498e7a5e8bf5d63c47af59e1550111/tensorflow/python/ops/nn_ops.py#L1135\r\nSeems that nn.conv2d supports gradient for dilations for GPU, not only TPU devices"]}, {"number": 48389, "title": "Tensorflow support for advance models like VAE-GAN, GAN ", "body": "Hello,\r\nI was wondering if you guys are planning to make functional APIs for advanced research-based models like VAE, VAEGAN, GANS, etc.\r\nIf so then it will be very easier for the TensorFlow community to explore applications of these models in an array of real-world problems. Since currently the generative model uses multiple losses therefore training them is not intuitive and often ends up in unstable training. \r\nGradientTape alleviates the problem a little but still, it's a major bottleneck in the widespread adoption of advance deep learning models in business problems.\r\nLink: https://arxiv.org/pdf/1512.09300.pdf\r\nRegards,\r\nMrinal", "comments": []}, {"number": 48372, "title": "Using same group key in CollectiveBcastSend/RecvV2 and CollectiveReduceV2 raises an error", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.1-54425-g18a6a9fc87f 2.6.0-dev20210407\r\n- Python version: 3.8.8\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: 11.0.3 / 7.6.5\r\n- GPU model and memory: TITAN Xp (12GB)\r\n\r\n**Describe the current behavior**\r\n\r\nIf I use 2 consecutive CollectiveCommunication Operations (CollectiveBcastSend / Recv V2 => CollectiveReduceV2) on the same devices, the program is aborted by InternalError\r\n\r\n**Describe the expected behavior**\r\n\r\nThe program should not be aborted and should execute two collective operations\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.core.protobuf import config_pb2\r\nfrom tensorflow.python.ops import collective_ops\r\n\r\ndef main():\r\n  g = tf.Graph()\r\n  with g.as_default():\r\n    c = tf.random.normal(shape=(10, 20))\r\n    with tf.device(\"/GPU:0\"):\r\n      b0 = collective_ops.broadcast_send_v2(c, group_size=2, group_key=1, instance_key=1)\r\n      r0 = collective_ops.all_reduce_v2(b0, group_size=2, group_key=1, instance_key=2)\r\n    with tf.device(\"/GPU:1\"):\r\n      b1 = collective_ops.broadcast_recv_v2(c.shape, c.dtype, group_size=2, group_key=1, instance_key=1)\r\n      r1 = collective_ops.all_reduce_v2(b1, group_size=2, group_key=1, instance_key=2)\r\n\r\n    with tf.compat.v1.Session(graph=g) as sess:\r\n        options = config_pb2.RunOptions()\r\n        options.experimental.collective_graph_key = 1\r\n        sess.run([r0, r1], options=options)\r\n\r\nif __name__ == \"__main__\":\r\n  main()\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n\r\nThe error logs are\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/ktaebum/Program/anaconda3/envs/temp/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1375, in _do_call\r\n    return fn(*args)\r\n  File \"/home/ktaebum/Program/anaconda3/envs/temp/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1359, in _run_fn\r\n    return self._call_tf_sessionrun(options, feed_dict, fetch_list,\r\n  File \"/home/ktaebum/Program/anaconda3/envs/temp/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1451, in _call_tf_sessionrun\r\n    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\r\ntensorflow.python.framework.errors_impl.InternalError: [_Derived_]Collective ops is aborted by: Collective Op CollectiveReduceV2: ReduceV2(Add,Id) is assigned to device /job:localhost/replica:0/task:0/device:GPU:0 with type GPU and group_key 1 but that group has type DEFAULT\r\nThe error could be from a previous operation. Restart your program to reset.\r\n         [[{{node CollectiveReduceV2_1}}]]\r\n         [[CollectiveReduceV2/_1]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"collective.py\", line 31, in <module>\r\n    main()\r\n  File \"collective.py\", line 28, in main\r\n    print(sess.run([r0, r1], options=options))\r\n  File \"/home/ktaebum/Program/anaconda3/envs/temp/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 967, in run\r\n    result = self._run(None, fetches, feed_dict, options_ptr,\r\n  File \"/home/ktaebum/Program/anaconda3/envs/temp/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1190, in _run\r\n    results = self._do_run(handle, final_targets, final_fetches,\r\n  File \"/home/ktaebum/Program/anaconda3/envs/temp/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1368, in _do_run\r\n    return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n  File \"/home/ktaebum/Program/anaconda3/envs/temp/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1394, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: [_Derived_]Collective ops is aborted by: Collective Op CollectiveReduceV2: ReduceV2(Add,Id) is assigned to device /job:localhost/replica:0/task:0/device:GPU:0 with type GPU and group_key 1 but that group has type DEFAULT\r\nThe error could be from a previous operation. Restart your program to reset.\r\n         [[node CollectiveReduceV2_1 (defined at collective.py:21) ]]\r\n         [[CollectiveReduceV2/_1]]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node CollectiveReduceV2_1:\r\n CollectiveBcastRecvV2 (defined at collective.py:20)\r\n\r\nOriginal stack trace for 'CollectiveReduceV2_1':\r\n  File \"collective.py\", line 31, in <module>\r\n    main()\r\n  File \"collective.py\", line 21, in main\r\n    r1 = collective_ops.all_reduce_v2(b1, 2, 1, 2)\r\n  File \"/home/ktaebum/Program/anaconda3/envs/temp/lib/python3.8/site-packages/tensorflow/python/ops/collective_ops.py\", line 111, in all_reduce_v2\r\n    return gen_collective_ops.collective_reduce_v2(\r\n  File \"/home/ktaebum/Program/anaconda3/envs/temp/lib/python3.8/site-packages/tensorflow/python/ops/gen_collective_ops.py\", line 779, in collective_reduce_v2\r\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\r\n  File \"/home/ktaebum/Program/anaconda3/envs/temp/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\", line 748, in _apply_op_helper\r\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\r\n  File \"/home/ktaebum/Program/anaconda3/envs/temp/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 3561, in _create_op_internal\r\n    ret = Operation(\r\n  File \"/home/ktaebum/Program/anaconda3/envs/temp/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 2049, in __init__\r\n    self._traceback = tf_stack.extract_stack_for_node(self._c_op)\r\n```\r\n\r\nInterestingly, if I give a different `group_key` to collective reduce as\r\n```\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.core.protobuf import config_pb2\r\nfrom tensorflow.python.ops import collective_ops\r\n\r\ndef main():\r\n  g = tf.Graph()\r\n  with g.as_default():\r\n    c = tf.random.normal(shape=(10, 20))\r\n    with tf.device(\"/GPU:0\"):\r\n      b0 = collective_ops.broadcast_send_v2(c, group_size=2, group_key=1, instance_key=1)\r\n      r0 = collective_ops.all_reduce_v2(b0, group_size=2, group_key=2, instance_key=2)\r\n    with tf.device(\"/GPU:1\"):\r\n      b1 = collective_ops.broadcast_recv_v2(c.shape, c.dtype, group_size=2, group_key=1, instance_key=1)\r\n      r1 = collective_ops.all_reduce_v2(b1, group_size=2, group_key=2, instance_key=2)\r\n\r\n    with tf.compat.v1.Session(graph=g) as sess:\r\n        options = config_pb2.RunOptions()\r\n        options.experimental.collective_graph_key = 1\r\n        sess.run([r0, r1], options=options)\r\n\r\nif __name__ == \"__main__\":\r\n  main()\r\n```\r\n, the program runs without an error.\r\nHowever, from my understanding, I can use the same `group_key` for different collective operations if the participant devices are same (only `instance_key` should be different) because CollectiveReduce => CollectiveGather works well if I run\r\n```\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.core.protobuf import config_pb2\r\nfrom tensorflow.python.ops import collective_ops\r\n\r\ndef main():\r\n  g = tf.Graph()\r\n  with g.as_default():\r\n    c = tf.random.normal(shape=(10, 20))\r\n    with tf.device(\"/GPU:0\"):\r\n      r0 = collective_ops.all_reduce_v2(c, group_size=2, group_key=2, instance_key=2)\r\n      r0 = collective_ops.all_gather_v2(r0, group_size=2, group_key=2, instance_key=3)\r\n    with tf.device(\"/GPU:1\"):\r\n      r1 = collective_ops.all_reduce_v2(c, group_size=2, group_key=2, instance_key=2)\r\n      r1 = collective_ops.all_gather_v2(r1, group_size=2, group_key=2, instance_key=3)\r\n\r\n    with tf.compat.v1.Session(graph=g) as sess:\r\n        options = config_pb2.RunOptions()\r\n        options.experimental.collective_graph_key = 1\r\n        sess.run([r0, r1], options=options)\r\n\r\nif __name__ == \"__main__\":\r\n  main()\r\n```", "comments": ["@ymodak,\r\nOn running the code with TF v2.4.1, I am facing an error stating `AttributeError: module 'tensorflow.python.ops.collective_ops' has no attribute 'broadcast_send_v2'`\r\n\r\nWhereas with TF v2.5.0rc0 and TF-nightly, I was able to reproduce the issue. Please check the attached screenshot for reference. \r\n![Screenshot 2021-04-10 12 13 43 AM](https://user-images.githubusercontent.com/57165142/114226968-f97da200-9991-11eb-97ec-9d779907ed95.png)\r\nThanks!"]}, {"number": 48365, "title": "Segmentation fault in tf-opt while running a tf dialect mlir file ", "body": " tf-opt segfaults on running the  tf.mlir file attached below \r\ntensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt --tf-shape-inference  -xla-legalize-tf=allow-partial-conversion --hlo-legalize-to-linalg -linalg-fusion-for-tensor-ops --linalg-bufferize --print-ir-before-all --print-ir-after-all resnet50_v1_tf_trimmed.mlir \r\nGDB log is attached \r\nThe error seems to come from /home/ubuntu/.cache/bazel/_bazel_ubuntu/f3ca1101791d1383bd78d7eef31c6279/execroot/org_tensorflow/bazel-out/k8-dbg/bin/external/llvm-project/mlirnclude/mlir/Dialect/Linalg/IR/LinalgOps.cpp.inc \r\n\r\nprintOperandsOrIntegersSizesList\r\n\r\nvoid InitTensorOp::print(::mlir::OpAsmPrinter &p) {\r\n  p << \"linalg.init_tensor\";\r\n  p << ' ';\r\n  printOperandsOrIntegersSizesList(p, *this, sizes(), static_sizesAttr());\r\n  p.printOptionalAttrDict((*this)->getAttrs(), /*elidedAttrs=*/{\"static_sizes\"});\r\n  p << ' ' << \":\";\r\n  p << ' ';\r\n  p << ::llvm::ArrayRef<::mlir::Type>(result().getType());\r\n}\r\nOn changing the batchsize to a known value like from ? to  the segfault goes away. \r\n\r\n %2 = \"tf.Placeholder\"() {device = \"\", shape = #tf.shape<?x224x224x3>} : () -> tensor<?x224x224x3xf32>\r\n \r\n %2 = \"tf.Placeholder\"() {device = \"\", shape = #tf.shape<4x224x224x3>} : () -> tensor<4x224x224x3xf32>\r\n\r\nLooks like it's due to an unknown dim of the input tensor .\r\n\r\n  %cst_9 =  constant dense<0xFF800000> : tensor<f32>\r\n  %454 =  linalg.init_tensor [3, 3] : tensor<3x3xf32>\r\n  %455 =  linalg.init_tensor [TensorFlow crashed, please file a bug on https://github.com/tensorflow/tensorflow/issues with the trace below.\r\nStack dump:\r\n #0 0x000055e21d04e433 llvm::sys::PrintStackTrace(llvm::raw_ostream&, int) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb2af433)\r\n #1 0x000055e21d04bcc2 llvm::sys::RunSignalHandlers() (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb2accc2)\r\n #2 0x000055e21d04c9d8 SignalHandler(int) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb2ad9d8)\r\n #3 0x00007fad38f618a0 __restore_rt (/lib/x86_64-linux-gnu/libpthread.so.0+0x128a0)\r\n #4 0x000055e21cfd8620 (anonymous namespace)::SSANameState::printValueID(mlir::Value, bool, llvm::raw_ostream&) const (.constprop.646) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb239620)\r\n #5 0x000055e21cfd9b4d (anonymous namespace)::OperationPrinter::printOperand(mlir::Value) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb23ab4d)\r\n #6 0x000055e21cf52ea7 mlir::printOperandsOrIntegersSizesList(mlir::OpAsmPrinter&, mlir::Operation*, mlir::OperandRange, mlir::ArrayAttr) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb1b3ea7)\r\n #7 0x000055e21ccf4ad8 mlir::linalg::InitTensorOp::print(mlir::OpAsmPrinter&) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xaf55ad8)\r\n #8 0x000055e21cd3e6c5 _ZN4mlir2OpINS_6linalg12InitTensorOpEINS_7OpTrait10ZeroRegionENS3_9OneResultENS3_14OneTypedResultINS_10TensorTypeEE4ImplENS3_13ZeroSuccessorENS3_16VariadicOperandsENS_23MemoryEffectOpInterface5TraitEEE13printAssemblyEPNS_9OperationERNS_12OpAsmPrinterE (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xaf9f6c5)\r\n #9 0x000055e21cfe4b18 (anonymous namespace)::OperationPrinter::print(mlir::Operation*) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb245b18)\r\n#10 0x000055e21cfe4c43 (anonymous namespace)::OperationPrinter::print(mlir::Block*, bool, bool) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb245c43)\r\n#11 0x000055e21cfe512f (anonymous namespace)::OperationPrinter::printRegion(mlir::Region&, bool, bool) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb24612f)\r\n#12 0x000055e21cf957b6 mlir::impl::printFunctionLikeOp(mlir::OpAsmPrinter&, mlir::Operation*, llvm::ArrayRef<mlir::Type>, bool, llvm::ArrayRef<mlir::Type>) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb1f67b6)\r\n#13 0x000055e21cfbc8c2 print(mlir::FuncOp, mlir::OpAsmPrinter&) (.constprop.300) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb21d8c2)\r\n#14 0x000055e21cfbc8e9 _ZN4mlir2OpINS_6FuncOpEINS_7OpTrait9OneRegionENS2_10ZeroResultENS2_13ZeroSuccessorENS2_12ZeroOperandsENS2_11AffineScopeENS2_24AutomaticAllocationScopeENS_19CallableOpInterface5TraitENS2_12FunctionLikeENS2_19IsIsolatedFromAboveENS_17SymbolOpInterface5TraitEEE13printAssemblyEPNS_9OperationERNS_12OpAsmPrinterE (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb21d8e9)\r\n#15 0x000055e21cfe4b18 (anonymous namespace)::OperationPrinter::print(mlir::Operation*) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb245b18)\r\n#16 0x000055e21cfe541c mlir::Operation::print(llvm::raw_ostream&, mlir::AsmState&, mlir::OpPrintingFlags) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb24641c)\r\n#17 0x000055e21cfe54d4 mlir::Operation::print(llvm::raw_ostream&, mlir::OpPrintingFlags) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb2464d4)\r\n#18 0x000055e21cedfec6 printIR(mlir::Operation*, bool, llvm::raw_ostream&, mlir::OpPrintingFlags) (.constprop.117) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb140ec6)\r\n#19 0x000055e21cee02b3 void llvm::function_ref<void (llvm::raw_ostream&)>::callback_fn<(anonymous namespace)::IRPrinterInstrumentation::runAfterPass(mlir::Pass*, mlir::Operation*)::'lambda'(llvm::raw_ostream&)>(long, llvm::raw_ostream&) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb1412b3)\r\n#20 0x000055e21cedfa12 (anonymous namespace)::BasicIRPrinterConfig::printAfterIfEnabled(mlir::Pass*, mlir::Operation*, llvm::function_ref<void (llvm::raw_ostream&)>) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb140a12)\r\n#21 0x000055e21cee075f (anonymous namespace)::IRPrinterInstrumentation::runAfterPass(mlir::Pass*, mlir::Operation*) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb14175f)\r\n#22 0x000055e21ceffd71 mlir::PassInstrumentor::runAfterPass(mlir::Pass*, mlir::Operation*) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb160d71)\r\n#23 0x000055e21cf062ed mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb1672ed)\r\n#24 0x000055e21cf06611 mlir::detail::OpToOpPassAdaptor::runPipeline(llvm::iterator_range<llvm::pointee_iterator<std::unique_ptr<mlir::Pass, std::default_delete<mlir::Pass> >*, mlir::Pass> >, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int, mlir::PassInstrumentor*, mlir::PassInstrumentation::PipelineParentInfo const*) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb167611)\r\n#25 0x000055e21cf07791 mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::'lambda'(llvm::MutableArrayRef<mlir::OpPassManager>) std::for_each<llvm::SmallVector<mlir::OpPassManager, 1u>*, mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::'lambda'(llvm::MutableArrayRef<mlir::OpPassManager>)>(llvm::SmallVector<mlir::OpPassManager, 1u>*, llvm::SmallVector<mlir::OpPassManager, 1u>*, mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::'lambda'(llvm::MutableArrayRef<mlir::OpPassManager>)) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb168791)\r\n#26 0x000055e21cf058c9 mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb1668c9)\r\n#27 0x000055e21cf062df mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb1672df)\r\n#28 0x000055e21cf06611 mlir::detail::OpToOpPassAdaptor::runPipeline(llvm::iterator_range<llvm::pointee_iterator<std::unique_ptr<mlir::Pass, std::default_delete<mlir::Pass> >*, mlir::Pass> >, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int, mlir::PassInstrumentor*, mlir::PassInstrumentation::PipelineParentInfo const*) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb167611)\r\n#29 0x000055e21cf07d24 mlir::PassManager::run(mlir::Operation*) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb168d24)\r\n#30 0x000055e218f746a7 performActions(llvm::raw_ostream&, bool, bool, llvm::SourceMgr&, mlir::MLIRContext*, mlir::PassPipelineCLParser const&) (.constprop.155) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0x71d56a7)\r\n#31 0x000055e218f74a4d processBuffer(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer> >, bool, bool, bool, bool, mlir::PassPipelineCLParser const&, mlir::DialectRegistry&) (.constprop.154) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0x71d5a4d)\r\n#32 0x000055e218f74d51 mlir::MlirOptMain(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer> >, mlir::PassPipelineCLParser const&, mlir::DialectRegistry&, bool, bool, bool, bool, bool) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0x71d5d51)\r\n#33 0x000055e218f7563f mlir::MlirOptMain(int, char**, llvm::StringRef, mlir::DialectRegistry&, bool) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0x71d663f)\r\n#34 0x000055e2128ce58c main (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb2f58c)\r\n#35 0x00007fad38967b97 __libc_start_main /build/glibc-2ORdQG/glibc-2.27/csu/../csu/libc-start.c:344:0\r\n#36 0x000055e2129bf4da _start (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xc204da)\r\n #0 0x000055e21d04e433 llvm::sys::PrintStackTrace(llvm::raw_ostream&, int) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb2af433)\r\n #1 0x000055e21d04bcc2 llvm::sys::RunSignalHandlers() (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb2accc2)\r\n #2 0x000055e21d04c9d8 SignalHandler(int) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb2ad9d8)\r\n #3 0x00007fad38f618a0 __restore_rt (/lib/x86_64-linux-gnu/libpthread.so.0+0x128a0)\r\n #4 0x000055e21cfd8620 (anonymous namespace)::SSANameState::printValueID(mlir::Value, bool, llvm::raw_ostream&) const (.constprop.646) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb239620)\r\n #5 0x000055e21cfd9b4d (anonymous namespace)::OperationPrinter::printOperand(mlir::Value) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb23ab4d)\r\n #6 0x000055e21cf52ea7 mlir::printOperandsOrIntegersSizesList(mlir::OpAsmPrinter&, mlir::Operation*, mlir::OperandRange, mlir::ArrayAttr) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb1b3ea7)\r\n #7 0x000055e21ccf4ad8 mlir::linalg::InitTensorOp::print(mlir::OpAsmPrinter&) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xaf55ad8)\r\n #8 0x000055e21cd3e6c5 _ZN4mlir2OpINS_6linalg12InitTensorOpEINS_7OpTrait10ZeroRegionENS3_9OneResultENS3_14OneTypedResultINS_10TensorTypeEE4ImplENS3_13ZeroSuccessorENS3_16VariadicOperandsENS_23MemoryEffectOpInterface5TraitEEE13printAssemblyEPNS_9OperationERNS_12OpAsmPrinterE (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xaf9f6c5)\r\n #9 0x000055e21cfe4b18 (anonymous namespace)::OperationPrinter::print(mlir::Operation*) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb245b18)\r\n#10 0x000055e21cfe4c43 (anonymous namespace)::OperationPrinter::print(mlir::Block*, bool, bool) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb245c43)\r\n#11 0x000055e21cfe512f (anonymous namespace)::OperationPrinter::printRegion(mlir::Region&, bool, bool) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb24612f)\r\n#12 0x000055e21cf957b6 mlir::impl::printFunctionLikeOp(mlir::OpAsmPrinter&, mlir::Operation*, llvm::ArrayRef<mlir::Type>, bool, llvm::ArrayRef<mlir::Type>) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb1f67b6)\r\n#13 0x000055e21cfbc8c2 print(mlir::FuncOp, mlir::OpAsmPrinter&) (.constprop.300) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb21d8c2)\r\n#14 0x000055e21cfbc8e9 _ZN4mlir2OpINS_6FuncOpEINS_7OpTrait9OneRegionENS2_10ZeroResultENS2_13ZeroSuccessorENS2_12ZeroOperandsENS2_11AffineScopeENS2_24AutomaticAllocationScopeENS_19CallableOpInterface5TraitENS2_12FunctionLikeENS2_19IsIsolatedFromAboveENS_17SymbolOpInterface5TraitEEE13printAssemblyEPNS_9OperationERNS_12OpAsmPrinterE (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb21d8e9)\r\n#15 0x000055e21cfe4b18 (anonymous namespace)::OperationPrinter::print(mlir::Operation*) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb245b18)\r\n#16 0x000055e21cfe541c mlir::Operation::print(llvm::raw_ostream&, mlir::AsmState&, mlir::OpPrintingFlags) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb24641c)\r\n#17 0x000055e21cfe54d4 mlir::Operation::print(llvm::raw_ostream&, mlir::OpPrintingFlags) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb2464d4)\r\n#18 0x000055e21cedfec6 printIR(mlir::Operation*, bool, llvm::raw_ostream&, mlir::OpPrintingFlags) (.constprop.117) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb140ec6)\r\n#19 0x000055e21cee02b3 void llvm::function_ref<void (llvm::raw_ostream&)>::callback_fn<(anonymous namespace)::IRPrinterInstrumentation::runAfterPass(mlir::Pass*, mlir::Operation*)::'lambda'(llvm::raw_ostream&)>(long, llvm::raw_ostream&) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb1412b3)\r\n#20 0x000055e21cedfa12 (anonymous namespace)::BasicIRPrinterConfig::printAfterIfEnabled(mlir::Pass*, mlir::Operation*, llvm::function_ref<void (llvm::raw_ostream&)>) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb140a12)\r\n#21 0x000055e21cee075f (anonymous namespace)::IRPrinterInstrumentation::runAfterPass(mlir::Pass*, mlir::Operation*) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb14175f)\r\n#22 0x000055e21ceffd71 mlir::PassInstrumentor::runAfterPass(mlir::Pass*, mlir::Operation*) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb160d71)\r\n#23 0x000055e21cf062ed mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb1672ed)\r\n#24 0x000055e21cf06611 mlir::detail::OpToOpPassAdaptor::runPipeline(llvm::iterator_range<llvm::pointee_iterator<std::unique_ptr<mlir::Pass, std::default_delete<mlir::Pass> >*, mlir::Pass> >, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int, mlir::PassInstrumentor*, mlir::PassInstrumentation::PipelineParentInfo const*) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb167611)\r\n#25 0x000055e21cf07791 mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::'lambda'(llvm::MutableArrayRef<mlir::OpPassManager>) std::for_each<llvm::SmallVector<mlir::OpPassManager, 1u>*, mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::'lambda'(llvm::MutableArrayRef<mlir::OpPassManager>)>(llvm::SmallVector<mlir::OpPassManager, 1u>*, llvm::SmallVector<mlir::OpPassManager, 1u>*, mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::'lambda'(llvm::MutableArrayRef<mlir::OpPassManager>)) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb168791)\r\n#26 0x000055e21cf058c9 mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb1668c9)\r\n#27 0x000055e21cf062df mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb1672df)\r\n#28 0x000055e21cf06611 mlir::detail::OpToOpPassAdaptor::runPipeline(llvm::iterator_range<llvm::pointee_iterator<std::unique_ptr<mlir::Pass, std::default_delete<mlir::Pass> >*, mlir::Pass> >, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int, mlir::PassInstrumentor*, mlir::PassInstrumentation::PipelineParentInfo const*) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb167611)\r\n#29 0x000055e21cf07d24 mlir::PassManager::run(mlir::Operation*) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb168d24)\r\n#30 0x000055e218f746a7 performActions(llvm::raw_ostream&, bool, bool, llvm::SourceMgr&, mlir::MLIRContext*, mlir::PassPipelineCLParser const&) (.constprop.155) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0x71d56a7)\r\n#31 0x000055e218f74a4d processBuffer(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer> >, bool, bool, bool, bool, mlir::PassPipelineCLParser const&, mlir::DialectRegistry&) (.constprop.154) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0x71d5a4d)\r\n#32 0x000055e218f74d51 mlir::MlirOptMain(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer> >, mlir::PassPipelineCLParser const&, mlir::DialectRegistry&, bool, bool, bool, bool, bool) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0x71d5d51)\r\n#33 0x000055e218f7563f mlir::MlirOptMain(int, char**, llvm::StringRef, mlir::DialectRegistry&, bool) (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0x71d663f)\r\n#34 0x000055e2128ce58c main (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xb2f58c)\r\n#35 0x00007fad38967b97 __libc_start_main /build/glibc-2ORdQG/glibc-2.27/csu/../csu/libc-start.c:344:0\r\n#36 0x000055e2129bf4da _start (/data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt+0xc204da)\r\n\r\n-------------------- GDB log ------\r\n%454 =  linalg.init_tensor [3, 3] : tensor<3x3xf32>\r\n  %455 =  linalg.init_tensor [tf-opt: external/llvm-project/llvm/include/llvm/ADT/STLExtras.h:1118: ReferenceT llvm::detail::indexed_accessor_range_base<DerivedT, BaseT, T, PointerT, ReferenceT>::operator[](size_t) const [with DerivedT = mlir::ValueRange; BaseT = llvm::PointerUnion<const mlir::Value*, mlir::OpOperand*, mlir::detail::OpResultImpl*>; T = mlir::Value; PointerT = mlir::Value; ReferenceT = mlir::Value; size_t = long unsigned int]: Assertion `Index < size() && \"invalid index for value range\"' failed.\r\nProgram received signal SIGABRT, Aborted.\r\n__GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51\r\n51\t../sysdeps/unix/sysv/linux/raise.c: No such file or directory.\r\n(gdb) bt\r\n#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51\r\n#1  0x00007ffff0d1b8b1 in __GI_abort () at abort.c:79\r\n#2  0x00007ffff0d0b42a in __assert_fail_base (fmt=0x7ffff0e92a38 \"%s%s%s:%u: %s%sAssertion `%s' failed.\\n%n\", assertion=assertion@entry=0x555572d2ca38 \"Index < size() && \\\"invalid index for value range\\\"\",\r\n    file=file@entry=0x555572d2c8a0 \"external/llvm-project/llvm/include/llvm/ADT/STLExtras.h\", line=line@entry=1118,\r\n    function=function@entry=0x555572d3d6a0 <llvm::detail::indexed_accessor_range_base<mlir::ValueRange, llvm::PointerUnion<mlir::Value const*, mlir::OpOperand*, mlir::detail::OpResultImpl*>, mlir::Value, mlir::Value, mlir::Value>::operator[](unsigned long) const::__PRETTY_FUNCTION__> \"ReferenceT llvm::detail::indexed_accessor_range_base<DerivedT, BaseT, T, PointerT, ReferenceT>::operator[](size_t) const [with DerivedT = mlir::ValueRange; BaseT = llvm::PointerUnion<const mlir::Value\"...) at assert.c:92\r\n#3  0x00007ffff0d0b4a2 in __GI___assert_fail (assertion=0x555572d2ca38 \"Index < size() && \\\"invalid index for value range\\\"\", file=0x555572d2c8a0 \"external/llvm-project/llvm/include/llvm/ADT/STLExtras.h\",\r\n    line=1118,\r\n    function=0x555572d3d6a0 <llvm::detail::indexed_accessor_range_base<mlir::ValueRange, llvm::PointerUnion<mlir::Value const*, mlir::OpOperand*, mlir::detail::OpResultImpl*>, mlir::Value, mlir::Value, mlir::Value>::operator[](unsigned long) const::__PRETTY_FUNCTION__> \"ReferenceT llvm::detail::indexed_accessor_range_base<DerivedT, BaseT, T, PointerT, ReferenceT>::operator[](size_t) const [with DerivedT = mlir::ValueRange; BaseT = llvm::PointerUnion<const mlir::Value\"...) at assert.c:101\r\n#4  0x00005555628b36c1 in llvm::detail::indexed_accessor_range_base<mlir::ValueRange, llvm::PointerUnion<mlir::Value const*, mlir::OpOperand*, mlir::detail::OpResultImpl*>, mlir::Value, mlir::Value, mlir::Value>::operator[] (this=0x7fffffffc450, Index=0) at external/llvm-project/llvm/include/llvm/ADT/STLExtras.h:1118\r\n#5  0x0000555568efb182 in <lambda(mlir::Attribute)>::operator()(mlir::Attribute) const (__closure=0x7fffffffc370, a=...) at external/llvm-project/mlir/lib/Interfaces/ViewLikeInterface.cpp:84\r\n#6  0x0000555568efbf3a in llvm::interleave<const mlir::Attribute*, printOperandsOrIntegersListImpl(mlir::OpAsmPrinter&, mlir::ValueRange, mlir::ArrayAttr) [with long int dynVal = -1l]::<lambda(mlir::Attribute)>, llvm::interleave(const Container&, StreamT&, UnaryFunctor, const llvm::StringRef&) [with Container = mlir::ArrayAttr; UnaryFunctor = printOperandsOrIntegersListImpl(mlir::OpAsmPrinter&, mlir::ValueRange, mlir::ArrayAttr) [with long int dynVal = -1l]::<lambda(mlir::Attribute)>; StreamT = mlir::OpAsmPrinter; T = const mlir::Attribute]::<lambda()>, void>(const mlir::Attribute *, const mlir::Attribute *, <lambda(mlir::Attribute)>, llvm::<lambda()>) (begin=0x55557c087c08, end=0x55557c087c28, each_fn=..., between_fn=...) at external/llvm-project/llvm/include/llvm/ADT/STLExtras.h:1740\r\n#7  0x0000555568efbe91 in llvm::interleave<mlir::ArrayAttr, printOperandsOrIntegersListImpl(mlir::OpAsmPrinter&, mlir::ValueRange, mlir::ArrayAttr) [with long int dynVal = -1l]::<lambda(mlir::Attribute)>, mlir::OpAsmPrinter, const mlir::Attribute>(const mlir::ArrayAttr &, mlir::OpAsmPrinter &, <lambda(mlir::Attribute)>, const llvm::StringRef &) (c=..., os=..., each_fn=..., separator=...)\r\n    at external/llvm-project/llvm/include/llvm/ADT/STLExtras.h:1762\r\n#8  0x0000555568efbd33 in llvm::interleaveComma<mlir::ArrayAttr, printOperandsOrIntegersListImpl(mlir::OpAsmPrinter&, mlir::ValueRange, mlir::ArrayAttr) [with long int dynVal = -1l]::<lambda(mlir::Attribute)>, mlir::OpAsmPrinter, const mlir::Attribute>(const mlir::ArrayAttr &, mlir::OpAsmPrinter &, <lambda(mlir::Attribute)>) (c=..., os=..., each_fn=...)\r\n    at external/llvm-project/llvm/include/llvm/ADT/STLExtras.h:1776\r\n#9  0x0000555568efb24e in printOperandsOrIntegersListImpl<-1l> (p=..., values=..., arrayAttr=...) at external/llvm-project/mlir/lib/Interfaces/ViewLikeInterface.cpp:81\r\n#10 0x0000555568efaf40 in mlir::printOperandsOrIntegersSizesList (p=..., op=0x55557b4738b0, values=..., integers=...) at external/llvm-project/mlir/lib/Interfaces/ViewLikeInterface.cpp:103\r\n#11 0x0000555568952ad0 in mlir::linalg::InitTensorOp::print (this=0x7fffffffc588, p=...)\r\n    at bazel-out/k8-dbg/bin/external/llvm-project/mlir/_virtual_includes/LinalgOpsIncGen/mlir/Dialect/Linalg/IR/LinalgOps.cpp.inc:313\r\n#12 0x0000555568a60d2b in mlir::Op<mlir::linalg::InitTensorOp, mlir::OpTrait::ZeroRegion, mlir::OpTrait::OneResult, mlir::OpTrait::OneTypedResult<mlir::TensorType>::Impl, mlir::OpTrait::ZeroSuccessor, mlir::OpTrait::VariadicOperands, mlir::MemoryEffectOpInterface::Trait>::printAssembly (op=0x55557b4738b0, p=...) at external/llvm-project/mlir/include/mlir/IR/OpDefinition.h:1709\r\n#13 0x0000555569018294 in mlir::AbstractOperation::printAssembly (this=0x55557bf662f8, op=0x55557b4738b0, p=...) at external/llvm-project/mlir/include/mlir/IR/OperationSupport.h:93\r\n#14 0x000055556900f789 in (anonymous namespace)::OperationPrinter::printOperation (this=0x7fffffffca60, op=0x55557b4738b0) at external/llvm-project/mlir/lib/IR/AsmPrinter.cpp:2434\r\n#15 0x000055556900f4c0 in (anonymous namespace)::OperationPrinter::print (this=0x7fffffffca60, op=0x55557b4738b0) at external/llvm-project/mlir/lib/IR/AsmPrinter.cpp:2397\r\n#16 0x0000555569010060 in (anonymous namespace)::OperationPrinter::print (this=0x7fffffffca60, block=0x55557c03d9b0, printBlockArgs=false, printBlockTerminator=true)\r\n    at external/llvm-project/mlir/lib/IR/AsmPrinter.cpp:2534\r\n \r\n[resnet50_v1_tf_trimmed.tgz.txt](https://github.com/tensorflow/tensorflow/files/6270791/resnet50_v1_tf_trimmed.tgz.txt)\r\n\r\n", "comments": ["@dinkdeep,\r\nCould you please share the TensorFlow version you are using?\r\n\r\nAlso, please let us know if this issue is related to issues [#48338](https://github.com/tensorflow/tensorflow/issues/48338) [#48362](https://github.com/tensorflow/tensorflow/issues/48362) and [#48363](https://github.com/tensorflow/tensorflow/issues/48363)? Thanks!", "@amahendrakar \r\nPlease find the info \r\n1) I had built the TensorFlow from source code  \r\nTensorFlow installed from (source or binary): Source\r\ncommit ac5f2a1 (HEAD -> master, origin/nightly, origin/master, origin/HEAD\r\n(base) ubuntu@HYD1PNF01:/data/dkd/tf/tensorflow$ git log\r\n**commit ac5f2a1 (HEAD -> master, origin/nightly, origin/master, origin/HEAD)**\r\nAuthor: Pankaj Kanwar pkanwar@google.com\r\nDate: Fri Mar 19 21:01:15 2021 -0700\r\n\r\n2) Build the tf-opt with bazel in tf folder\r\n#time bazel build --jobs 100 -c opt tensorflow/compiler/mlir:tf-opt\r\n\r\n3) Please use the attached \"resnet50_v1_tf_trimmed.tgz.txt\" file .mlir file, Download the above txt file run it with the tf-opt with following mlir passes\r\n**# /data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt --tf-shape-inference -xla-legalize-tf=allow-partial-conversion --hlo-legalize-to-linalg -linalg-fusion-for-tensor-ops --linalg-bufferize --print-ir-before-all --print-ir-after-all  resnet50_v1_tf_trimmed.mlir\r\n\r\nBasically, it is a mlir file with tf Dialect which is being converted to HLO Dialect and then to linalg Dialect.\r\n\r\n #48338 is a pretty minor issue not a bug just an unused variable .\r\n #48362 is case where  mhlo.conv operator of HLO dialect is not get getting converted to linalg.con when the tf.Conv operator has padding specified, the hlo-legalize-to-linalg fails and in the linalg dialect we see the mhlo.convolution operator .\r\n  \r\n  #48363 is something a feature request where conv operator of linalg Dialects are not placed inside Generic region operator, Some thing can be done to get them inside the regions.  \r\n\r\n\r\n\r\n"]}, {"number": 48363, "title": "In the \"LinalgFusionOfTensorOps\" node fusion pass linalg.conv Operator does not seems to be inside the Generic Operator Region unlike other pointwise operators add/mul .. ", "body": " Looks like the linalg.conv operator  are not placed inside the Generic Operator Region\r\n For our custom requirement we would like to use and leverage on the \"Node fusion capability of Regions\" offered by the \"LinalgFusionOfTensorOps\"  , The pass  seems to work point wise operators but not for the  operators like linalg.conv/MaxPool/BatchNorm/AVvgPool etc  \r\n \r\nWe have our own customized HW accelerated kernels for the above operator sets , We can do away with the affine_map<(...)>  information also.\r\n\r\nWhat does it need to be done in any reference sample or pointer would be appreciated?\r\n \r\n   %2 =  linalg.init_tensor [2, 112, 112, 1, 64] : tensor<2x112x112x1x64xf32>\r\n  %cst =  constant 0.000000e+00 : f32\r\n  %3 =  linalg.fill(%2, %cst) : tensor<2x112x112x1x64xf32>, f32 -> tensor<2x112x112x1x64xf32>\r\n  **%4 =  linalg.depthwise_conv_2d_input_nhwc_filter_hwcf {strides =  dense<2> : tensor<2xi64>} ins(%arg0, %arg1 : tensor<2x224x224x3xf32>, tensor<7x7x3x64xf32>) outs(%3 : tensor<2x112x112x1x64xf32>) -> tensor<2x112x112x1x64xf32>**\r\n  %5 =  linalg.tensor_reshape %4 [affine_map<(d0, d1, d2, d3, d4) -> (d0)>, affine_map<(d0, d1, d2, d3, d4) -> (d1)>, affine_map<(d0, d1, d2, d3, d4) -> (d2)>, affine_map<(d0, d1, d2, d3, d4) -> (d3, d4)>] : tensor<2x112x112x1x64xf32> into tensor<2x112x112x64xf32>\r\n  %6 =  shape.shape_of %5 : tensor<2x112x112x64xf32> -> tensor<?xindex>\r\n  %7 =  shape.to_extent_tensor %6 : tensor<?xindex> -> tensor<4xindex>\r\n  %8 =  linalg.init_tensor [2, 112, 112, 64] : tensor<2x112x112x64xf32>\r\n  %9 =  linalg.generic {indexing_maps =  [affine_map<(d0, d1, d2, d3) -> (d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types =  [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%arg2 : tensor<64xf32>) outs(%8 : tensor<2x112x112x64xf32>) {\r\n  ^bb0(%arg321: f32, %arg322: f32):  // no predecessors\r\n    linalg.yield %arg321 : f32\r\n  } -> tensor<2x112x112x64xf32>\r\n  %10 =  linalg.init_tensor [2, 112, 112, 64] : tensor<2x112x112x64xf32>\r\n  %11 =  linalg.generic {indexing_maps =  [affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>], iterator_types =  [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%5, %9 : tensor<2x112x112x64xf32>, tensor<2x112x112x64xf32>) outs(%10 : tensor<2x112x112x64xf32>) {\r\n  ^bb0(%arg321: f32, %arg322: f32, %arg323: f32):  // no predecessors\r\n    **%58 =  addf %arg321, %arg322 : f32**\r\n    linalg.yield %58 : f32\r\n  } -> tensor<2x112x112x64xf32>\r\n", "comments": ["@dinkdeep \r\n\r\nKindly open a [stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow) issue for this as it is not a bug or feature request, Please post this kind of support questions at Stackoverflow. There is a big community to support and learn from your questions.\r\nThanks!", "This behavior observed is something specific to MLIR and HloLegalizeToLinalgPass in TensorFlow .  A corresponding stackoverflow issue might not be of much help."]}, {"number": 48338, "title": "Unused variable body_arg_types in  class PointwiseToLinalgConverter   hlo/lib/Dialect/mhlo/transforms/legalize_to_linalg.cc", "body": "File  hlo/lib/Dialect/mhlo/transforms/legalize_to_linalg.cc\r\nFunction : template <typename OpTy, bool isLHLO = true>\r\nclass PointwiseToLinalgConverter : public OpConversionPattern<OpTy> { }\r\n----\r\n SmallVector<Type, 4> body_arg_types, body_result_types, op_result_types;\r\n ValueRange inputs(args.take_front(num_inputs));\r\n for (Value in : inputs)\r\n      body_arg_types.emplace_back(getElementTypeOrSelf(in.getType()));\r\n \r\nseems body_arg_types is populated but not used elsewhere in the function. \r\n \r\ncommit ac5f2a1f0db1ba1e585559ad4e593fdb3e73713a (HEAD -> master, origin/nightly, origin/master, origin/HEAD\r\n", "comments": ["@dinkdeep \r\n\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the following information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nand the the exact sequence of commands / steps that you executed before running into the problem\r\n\r\n\r\nThanks!", "OS Platform and Distribution (e.g., Linux Ubuntu 16.04):   cat /etc/lsb-release \r\nDISTRIB_ID=Ubuntu\r\nDISTRIB_RELEASE=18.04\r\nDISTRIB_CODENAME=bionic\r\nDISTRIB_DESCRIPTION=\"Ubuntu 18.04.5 LTS\r\n\r\nTensorFlow installed from (source or binary):  Source \r\ncommit ac5f2a1 (HEAD -> master, origin/nightly, origin/master, origin/HEAD\r\n(base) ubuntu@HYD1PNF01:/data/dkd/tf/tensorflow$ git log\r\ncommit ac5f2a1f0db1ba1e585559ad4e593fdb3e73713a (HEAD -> master, origin/nightly, origin/master, origin/HEAD)\r\nAuthor: Pankaj Kanwar <pkanwar@google.com>\r\nDate:   Fri Mar 19 21:01:15 2021 -0700\r\n\r\n**Build the tf-opt with bazel in tf folder** \r\n#time bazel build --jobs 100 -c opt tensorflow/compiler/mlir:tf-opt\r\n\r\nCopy the below mlir module to a text file and run the tf-opt with following mlir passes \r\n**# /data/dkd/tf/tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt --tf-shape-inference  -xla-legalize-tf=allow-partial-conversion --hlo-legalize-to-linalg -linalg-fusion-for-tensor-ops --linalg-bufferize --print-ir-before-all --print-ir-after-all resnet50_tf_small_readVar.mlir\r\n\r\nThe Mlir has been generated from native resnet50 benchmark present in tf folder through python it contains the tf dialect  .\r\n-------------------------- mlir file -----------------------\r\nmodule  {\r\n  func @main(%arg0: tensor<2x224x224x3xf32> {tf._user_specified_name = \"inputs\", tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg1: tensor<7x7x3x64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg2: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg3: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg4: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg5: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg6: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg7: tensor<1x1x64x64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg8: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg9: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg10: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg11: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg12: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg13: tensor<3x3x64x64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg14: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg15: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg16: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg17: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg18: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg19: tensor<1x1x64x256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg20: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg21: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg22: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg23: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg24: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg25: tensor<1x1x64x256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg26: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg27: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg28: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg29: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg30: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg31: tensor<1x1x256x64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg32: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg33: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg34: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg35: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg36: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg37: tensor<3x3x64x64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg38: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg39: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg40: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg41: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg42: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg43: tensor<1x1x64x256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg44: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg45: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg46: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg47: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg48: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg49: tensor<1x1x256x64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg50: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg51: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg52: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg53: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg54: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg55: tensor<3x3x64x64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg56: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg57: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg58: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg59: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg60: tensor<64xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg61: tensor<1x1x64x256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg62: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg63: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg64: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg65: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg66: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg67: tensor<1x1x256x128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg68: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg69: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg70: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg71: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg72: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg73: tensor<3x3x128x128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg74: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg75: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg76: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg77: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg78: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg79: tensor<1x1x128x512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg80: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg81: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg82: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg83: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg84: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg85: tensor<1x1x256x512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg86: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg87: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg88: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg89: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg90: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg91: tensor<1x1x512x128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg92: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg93: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg94: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg95: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg96: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg97: tensor<3x3x128x128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg98: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg99: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg100: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg101: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg102: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg103: tensor<1x1x128x512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg104: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg105: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg106: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg107: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg108: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg109: tensor<1x1x512x128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg110: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg111: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg112: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg113: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg114: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg115: tensor<3x3x128x128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg116: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg117: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg118: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg119: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg120: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg121: tensor<1x1x128x512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg122: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg123: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg124: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg125: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg126: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg127: tensor<1x1x512x128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg128: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg129: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg130: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg131: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg132: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg133: tensor<3x3x128x128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg134: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg135: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg136: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg137: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg138: tensor<128xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg139: tensor<1x1x128x512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg140: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg141: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg142: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg143: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg144: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg145: tensor<1x1x512x256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg146: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg147: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg148: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg149: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg150: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg151: tensor<3x3x256x256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg152: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg153: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg154: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg155: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg156: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg157: tensor<1x1x256x1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg158: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg159: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg160: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg161: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg162: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg163: tensor<1x1x512x1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg164: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg165: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg166: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg167: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg168: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg169: tensor<1x1x1024x256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg170: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg171: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg172: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg173: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg174: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg175: tensor<3x3x256x256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg176: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg177: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg178: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg179: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg180: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg181: tensor<1x1x256x1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg182: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg183: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg184: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg185: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg186: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg187: tensor<1x1x1024x256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg188: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg189: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg190: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg191: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg192: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg193: tensor<3x3x256x256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg194: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg195: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg196: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg197: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg198: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg199: tensor<1x1x256x1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg200: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg201: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg202: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg203: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg204: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg205: tensor<1x1x1024x256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg206: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg207: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg208: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg209: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg210: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg211: tensor<3x3x256x256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg212: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg213: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg214: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg215: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg216: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg217: tensor<1x1x256x1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg218: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg219: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg220: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg221: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg222: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg223: tensor<1x1x1024x256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg224: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg225: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg226: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg227: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg228: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg229: tensor<3x3x256x256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg230: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg231: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg232: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg233: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg234: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg235: tensor<1x1x256x1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg236: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg237: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg238: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg239: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg240: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg241: tensor<1x1x1024x256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg242: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg243: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg244: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg245: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg246: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg247: tensor<3x3x256x256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg248: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg249: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg250: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg251: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg252: tensor<256xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg253: tensor<1x1x256x1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg254: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg255: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg256: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg257: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg258: tensor<1024xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg259: tensor<1x1x1024x512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg260: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg261: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg262: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg263: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg264: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg265: tensor<3x3x512x512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg266: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg267: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg268: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg269: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg270: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg271: tensor<1x1x512x2048xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg272: tensor<2048xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg273: tensor<2048xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg274: tensor<2048xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg275: tensor<2048xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg276: tensor<2048xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg277: tensor<1x1x1024x2048xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg278: tensor<2048xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg279: tensor<2048xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg280: tensor<2048xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg281: tensor<2048xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg282: tensor<2048xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg283: tensor<1x1x2048x512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg284: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg285: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg286: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg287: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg288: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg289: tensor<3x3x512x512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg290: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg291: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg292: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg293: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg294: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg295: tensor<1x1x512x2048xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg296: tensor<2048xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg297: tensor<2048xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg298: tensor<2048xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg299: tensor<2048xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg300: tensor<2048xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg301: tensor<1x1x2048x512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg302: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg303: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg304: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg305: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg306: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg307: tensor<3x3x512x512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg308: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg309: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg310: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg311: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg312: tensor<512xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg313: tensor<1x1x512x2048xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg314: tensor<2048xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg315: tensor<2048xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg316: tensor<2048xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg317: tensor<2048xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg318: tensor<2048xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg319: tensor<2048x1000xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}, %arg320: tensor<1000xf32> {tf.device = \"/job:localhost/replica:0/task:0/device:CPU:0\"}) -> tensor<2x1000xf32> attributes {tf.entry_function = {control_outputs = \"conv1/Conv2D/ReadVariableOp,conv1/BiasAdd/ReadVariableOp,bn_conv1/ReadVariableOp,bn_conv1/ReadVariableOp_1,bn_conv1/FusedBatchNormV3/ReadVariableOp,bn_conv1/FusedBatchNormV3/ReadVariableOp_1,private__conv_block/res2a_branch2a/Conv2D/ReadVariableOp,private__conv_block/res2a_branch2a/BiasAdd/ReadVariableOp,private__conv_block/bn2a_branch2a/ReadVariableOp,private__conv_block/bn2a_branch2a/ReadVariableOp_1,private__conv_block/bn2a_branch2a/FusedBatchNormV3/ReadVariableOp,private__conv_block/bn2a_branch2a/FusedBatchNormV3/ReadVariableOp_1,private__conv_block/res2a_branch2b/Conv2D/ReadVariableOp,private__conv_block/res2a_branch2b/BiasAdd/ReadVariableOp,private__conv_block/bn2a_branch2b/ReadVariableOp,private__conv_block/bn2a_branch2b/ReadVariableOp_1,private__conv_block/bn2a_branch2b/FusedBatchNormV3/ReadVariableOp,private__conv_block/bn2a_branch2b/FusedBatchNormV3/ReadVariableOp_1,private__conv_block/res2a_branch2c/Conv2D/ReadVariableOp,private__conv_block/res2a_branch2c/BiasAdd/ReadVariableOp,private__conv_block/bn2a_branch2c/ReadVariableOp,private__conv_block/bn2a_branch2c/ReadVariableOp_1,private__conv_block/bn2a_branch2c/FusedBatchNormV3/ReadVariableOp,private__conv_block/bn2a_branch2c/FusedBatchNormV3/ReadVariableOp_1,private__conv_block/res2a_branch1/Conv2D/ReadVariableOp,private__conv_block/res2a_branch1/BiasAdd/ReadVariableOp,private__conv_block/bn2a_branch1/ReadVariableOp,private__conv_block/bn2a_branch1/ReadVariableOp_1,private__conv_block/bn2a_branch1/FusedBatchNormV3/ReadVariableOp,private__conv_block/bn2a_branch1/FusedBatchNormV3/ReadVariableOp_1,private__identity_block/res2b_branch2a/Conv2D/ReadVariableOp,private__identity_block/res2b_branch2a/BiasAdd/ReadVariableOp,private__identity_block/bn2b_branch2a/ReadVariableOp,private__identity_block/bn2b_branch2a/ReadVariableOp_1,private__identity_block/bn2b_branch2a/FusedBatchNormV3/ReadVariableOp,private__identity_block/bn2b_branch2a/FusedBatchNormV3/ReadVariableOp_1,private__identity_block/res2b_branch2b/Conv2D/ReadVariableOp,private__identity_block/res2b_branch2b/BiasAdd/ReadVariableOp,private__identity_block/bn2b_branch2b/ReadVariableOp,private__identity_block/bn2b_branch2b/ReadVariableOp_1,private__identity_block/bn2b_branch2b/FusedBatchNormV3/ReadVariableOp,private__identity_block/bn2b_branch2b/FusedBatchNormV3/ReadVariableOp_1,private__identity_block/res2b_branch2c/Conv2D/ReadVariableOp,private__identity_block/res2b_branch2c/BiasAdd/ReadVariableOp,private__identity_block/bn2b_branch2c/ReadVariableOp,private__identity_block/bn2b_branch2c/ReadVariableOp_1,private__identity_block/bn2b_branch2c/FusedBatchNormV3/ReadVariableOp,private__identity_block/bn2b_branch2c/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_1/res2c_branch2a/Conv2D/ReadVariableOp,private__identity_block_1/res2c_branch2a/BiasAdd/ReadVariableOp,private__identity_block_1/bn2c_branch2a/ReadVariableOp,private__identity_block_1/bn2c_branch2a/ReadVariableOp_1,private__identity_block_1/bn2c_branch2a/FusedBatchNormV3/ReadVariableOp,private__identity_block_1/bn2c_branch2a/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_1/res2c_branch2b/Conv2D/ReadVariableOp,private__identity_block_1/res2c_branch2b/BiasAdd/ReadVariableOp,private__identity_block_1/bn2c_branch2b/ReadVariableOp,private__identity_block_1/bn2c_branch2b/ReadVariableOp_1,private__identity_block_1/bn2c_branch2b/FusedBatchNormV3/ReadVariableOp,private__identity_block_1/bn2c_branch2b/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_1/res2c_branch2c/Conv2D/ReadVariableOp,private__identity_block_1/res2c_branch2c/BiasAdd/ReadVariableOp,private__identity_block_1/bn2c_branch2c/ReadVariableOp,private__identity_block_1/bn2c_branch2c/ReadVariableOp_1,private__identity_block_1/bn2c_branch2c/FusedBatchNormV3/ReadVariableOp,private__identity_block_1/bn2c_branch2c/FusedBatchNormV3/ReadVariableOp_1,private__conv_block_1/res3a_branch2a/Conv2D/ReadVariableOp,private__conv_block_1/res3a_branch2a/BiasAdd/ReadVariableOp,private__conv_block_1/bn3a_branch2a/ReadVariableOp,private__conv_block_1/bn3a_branch2a/ReadVariableOp_1,private__conv_block_1/bn3a_branch2a/FusedBatchNormV3/ReadVariableOp,private__conv_block_1/bn3a_branch2a/FusedBatchNormV3/ReadVariableOp_1,private__conv_block_1/res3a_branch2b/Conv2D/ReadVariableOp,private__conv_block_1/res3a_branch2b/BiasAdd/ReadVariableOp,private__conv_block_1/bn3a_branch2b/ReadVariableOp,private__conv_block_1/bn3a_branch2b/ReadVariableOp_1,private__conv_block_1/bn3a_branch2b/FusedBatchNormV3/ReadVariableOp,private__conv_block_1/bn3a_branch2b/FusedBatchNormV3/ReadVariableOp_1,private__conv_block_1/res3a_branch2c/Conv2D/ReadVariableOp,private__conv_block_1/res3a_branch2c/BiasAdd/ReadVariableOp,private__conv_block_1/bn3a_branch2c/ReadVariableOp,private__conv_block_1/bn3a_branch2c/ReadVariableOp_1,private__conv_block_1/bn3a_branch2c/FusedBatchNormV3/ReadVariableOp,private__conv_block_1/bn3a_branch2c/FusedBatchNormV3/ReadVariableOp_1,private__conv_block_1/res3a_branch1/Conv2D/ReadVariableOp,private__conv_block_1/res3a_branch1/BiasAdd/ReadVariableOp,private__conv_block_1/bn3a_branch1/ReadVariableOp,private__conv_block_1/bn3a_branch1/ReadVariableOp_1,private__conv_block_1/bn3a_branch1/FusedBatchNormV3/ReadVariableOp,private__conv_block_1/bn3a_branch1/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_2/res3b_branch2a/Conv2D/ReadVariableOp,private__identity_block_2/res3b_branch2a/BiasAdd/ReadVariableOp,private__identity_block_2/bn3b_branch2a/ReadVariableOp,private__identity_block_2/bn3b_branch2a/ReadVariableOp_1,private__identity_block_2/bn3b_branch2a/FusedBatchNormV3/ReadVariableOp,private__identity_block_2/bn3b_branch2a/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_2/res3b_branch2b/Conv2D/ReadVariableOp,private__identity_block_2/res3b_branch2b/BiasAdd/ReadVariableOp,private__identity_block_2/bn3b_branch2b/ReadVariableOp,private__identity_block_2/bn3b_branch2b/ReadVariableOp_1,private__identity_block_2/bn3b_branch2b/FusedBatchNormV3/ReadVariableOp,private__identity_block_2/bn3b_branch2b/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_2/res3b_branch2c/Conv2D/ReadVariableOp,private__identity_block_2/res3b_branch2c/BiasAdd/ReadVariableOp,private__identity_block_2/bn3b_branch2c/ReadVariableOp,private__identity_block_2/bn3b_branch2c/ReadVariableOp_1,private__identity_block_2/bn3b_branch2c/FusedBatchNormV3/ReadVariableOp,private__identity_block_2/bn3b_branch2c/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_3/res3c_branch2a/Conv2D/ReadVariableOp,private__identity_block_3/res3c_branch2a/BiasAdd/ReadVariableOp,private__identity_block_3/bn3c_branch2a/ReadVariableOp,private__identity_block_3/bn3c_branch2a/ReadVariableOp_1,private__identity_block_3/bn3c_branch2a/FusedBatchNormV3/ReadVariableOp,private__identity_block_3/bn3c_branch2a/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_3/res3c_branch2b/Conv2D/ReadVariableOp,private__identity_block_3/res3c_branch2b/BiasAdd/ReadVariableOp,private__identity_block_3/bn3c_branch2b/ReadVariableOp,private__identity_block_3/bn3c_branch2b/ReadVariableOp_1,private__identity_block_3/bn3c_branch2b/FusedBatchNormV3/ReadVariableOp,private__identity_block_3/bn3c_branch2b/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_3/res3c_branch2c/Conv2D/ReadVariableOp,private__identity_block_3/res3c_branch2c/BiasAdd/ReadVariableOp,private__identity_block_3/bn3c_branch2c/ReadVariableOp,private__identity_block_3/bn3c_branch2c/ReadVariableOp_1,private__identity_block_3/bn3c_branch2c/FusedBatchNormV3/ReadVariableOp,private__identity_block_3/bn3c_branch2c/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_4/res3d_branch2a/Conv2D/ReadVariableOp,private__identity_block_4/res3d_branch2a/BiasAdd/ReadVariableOp,private__identity_block_4/bn3d_branch2a/ReadVariableOp,private__identity_block_4/bn3d_branch2a/ReadVariableOp_1,private__identity_block_4/bn3d_branch2a/FusedBatchNormV3/ReadVariableOp,private__identity_block_4/bn3d_branch2a/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_4/res3d_branch2b/Conv2D/ReadVariableOp,private__identity_block_4/res3d_branch2b/BiasAdd/ReadVariableOp,private__identity_block_4/bn3d_branch2b/ReadVariableOp,private__identity_block_4/bn3d_branch2b/ReadVariableOp_1,private__identity_block_4/bn3d_branch2b/FusedBatchNormV3/ReadVariableOp,private__identity_block_4/bn3d_branch2b/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_4/res3d_branch2c/Conv2D/ReadVariableOp,private__identity_block_4/res3d_branch2c/BiasAdd/ReadVariableOp,private__identity_block_4/bn3d_branch2c/ReadVariableOp,private__identity_block_4/bn3d_branch2c/ReadVariableOp_1,private__identity_block_4/bn3d_branch2c/FusedBatchNormV3/ReadVariableOp,private__identity_block_4/bn3d_branch2c/FusedBatchNormV3/ReadVariableOp_1,private__conv_block_2/res4a_branch2a/Conv2D/ReadVariableOp,private__conv_block_2/res4a_branch2a/BiasAdd/ReadVariableOp,private__conv_block_2/bn4a_branch2a/ReadVariableOp,private__conv_block_2/bn4a_branch2a/ReadVariableOp_1,private__conv_block_2/bn4a_branch2a/FusedBatchNormV3/ReadVariableOp,private__conv_block_2/bn4a_branch2a/FusedBatchNormV3/ReadVariableOp_1,private__conv_block_2/res4a_branch2b/Conv2D/ReadVariableOp,private__conv_block_2/res4a_branch2b/BiasAdd/ReadVariableOp,private__conv_block_2/bn4a_branch2b/ReadVariableOp,private__conv_block_2/bn4a_branch2b/ReadVariableOp_1,private__conv_block_2/bn4a_branch2b/FusedBatchNormV3/ReadVariableOp,private__conv_block_2/bn4a_branch2b/FusedBatchNormV3/ReadVariableOp_1,private__conv_block_2/res4a_branch2c/Conv2D/ReadVariableOp,private__conv_block_2/res4a_branch2c/BiasAdd/ReadVariableOp,private__conv_block_2/bn4a_branch2c/ReadVariableOp,private__conv_block_2/bn4a_branch2c/ReadVariableOp_1,private__conv_block_2/bn4a_branch2c/FusedBatchNormV3/ReadVariableOp,private__conv_block_2/bn4a_branch2c/FusedBatchNormV3/ReadVariableOp_1,private__conv_block_2/res4a_branch1/Conv2D/ReadVariableOp,private__conv_block_2/res4a_branch1/BiasAdd/ReadVariableOp,private__conv_block_2/bn4a_branch1/ReadVariableOp,private__conv_block_2/bn4a_branch1/ReadVariableOp_1,private__conv_block_2/bn4a_branch1/FusedBatchNormV3/ReadVariableOp,private__conv_block_2/bn4a_branch1/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_5/res4b_branch2a/Conv2D/ReadVariableOp,private__identity_block_5/res4b_branch2a/BiasAdd/ReadVariableOp,private__identity_block_5/bn4b_branch2a/ReadVariableOp,private__identity_block_5/bn4b_branch2a/ReadVariableOp_1,private__identity_block_5/bn4b_branch2a/FusedBatchNormV3/ReadVariableOp,private__identity_block_5/bn4b_branch2a/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_5/res4b_branch2b/Conv2D/ReadVariableOp,private__identity_block_5/res4b_branch2b/BiasAdd/ReadVariableOp,private__identity_block_5/bn4b_branch2b/ReadVariableOp,private__identity_block_5/bn4b_branch2b/ReadVariableOp_1,private__identity_block_5/bn4b_branch2b/FusedBatchNormV3/ReadVariableOp,private__identity_block_5/bn4b_branch2b/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_5/res4b_branch2c/Conv2D/ReadVariableOp,private__identity_block_5/res4b_branch2c/BiasAdd/ReadVariableOp,private__identity_block_5/bn4b_branch2c/ReadVariableOp,private__identity_block_5/bn4b_branch2c/ReadVariableOp_1,private__identity_block_5/bn4b_branch2c/FusedBatchNormV3/ReadVariableOp,private__identity_block_5/bn4b_branch2c/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_6/res4c_branch2a/Conv2D/ReadVariableOp,private__identity_block_6/res4c_branch2a/BiasAdd/ReadVariableOp,private__identity_block_6/bn4c_branch2a/ReadVariableOp,private__identity_block_6/bn4c_branch2a/ReadVariableOp_1,private__identity_block_6/bn4c_branch2a/FusedBatchNormV3/ReadVariableOp,private__identity_block_6/bn4c_branch2a/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_6/res4c_branch2b/Conv2D/ReadVariableOp,private__identity_block_6/res4c_branch2b/BiasAdd/ReadVariableOp,private__identity_block_6/bn4c_branch2b/ReadVariableOp,private__identity_block_6/bn4c_branch2b/ReadVariableOp_1,private__identity_block_6/bn4c_branch2b/FusedBatchNormV3/ReadVariableOp,private__identity_block_6/bn4c_branch2b/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_6/res4c_branch2c/Conv2D/ReadVariableOp,private__identity_block_6/res4c_branch2c/BiasAdd/ReadVariableOp,private__identity_block_6/bn4c_branch2c/ReadVariableOp,private__identity_block_6/bn4c_branch2c/ReadVariableOp_1,private__identity_block_6/bn4c_branch2c/FusedBatchNormV3/ReadVariableOp,private__identity_block_6/bn4c_branch2c/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_7/res4d_branch2a/Conv2D/ReadVariableOp,private__identity_block_7/res4d_branch2a/BiasAdd/ReadVariableOp,private__identity_block_7/bn4d_branch2a/ReadVariableOp,private__identity_block_7/bn4d_branch2a/ReadVariableOp_1,private__identity_block_7/bn4d_branch2a/FusedBatchNormV3/ReadVariableOp,private__identity_block_7/bn4d_branch2a/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_7/res4d_branch2b/Conv2D/ReadVariableOp,private__identity_block_7/res4d_branch2b/BiasAdd/ReadVariableOp,private__identity_block_7/bn4d_branch2b/ReadVariableOp,private__identity_block_7/bn4d_branch2b/ReadVariableOp_1,private__identity_block_7/bn4d_branch2b/FusedBatchNormV3/ReadVariableOp,private__identity_block_7/bn4d_branch2b/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_7/res4d_branch2c/Conv2D/ReadVariableOp,private__identity_block_7/res4d_branch2c/BiasAdd/ReadVariableOp,private__identity_block_7/bn4d_branch2c/ReadVariableOp,private__identity_block_7/bn4d_branch2c/ReadVariableOp_1,private__identity_block_7/bn4d_branch2c/FusedBatchNormV3/ReadVariableOp,private__identity_block_7/bn4d_branch2c/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_8/res4e_branch2a/Conv2D/ReadVariableOp,private__identity_block_8/res4e_branch2a/BiasAdd/ReadVariableOp,private__identity_block_8/bn4e_branch2a/ReadVariableOp,private__identity_block_8/bn4e_branch2a/ReadVariableOp_1,private__identity_block_8/bn4e_branch2a/FusedBatchNormV3/ReadVariableOp,private__identity_block_8/bn4e_branch2a/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_8/res4e_branch2b/Conv2D/ReadVariableOp,private__identity_block_8/res4e_branch2b/BiasAdd/ReadVariableOp,private__identity_block_8/bn4e_branch2b/ReadVariableOp,private__identity_block_8/bn4e_branch2b/ReadVariableOp_1,private__identity_block_8/bn4e_branch2b/FusedBatchNormV3/ReadVariableOp,private__identity_block_8/bn4e_branch2b/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_8/res4e_branch2c/Conv2D/ReadVariableOp,private__identity_block_8/res4e_branch2c/BiasAdd/ReadVariableOp,private__identity_block_8/bn4e_branch2c/ReadVariableOp,private__identity_block_8/bn4e_branch2c/ReadVariableOp_1,private__identity_block_8/bn4e_branch2c/FusedBatchNormV3/ReadVariableOp,private__identity_block_8/bn4e_branch2c/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_9/res4f_branch2a/Conv2D/ReadVariableOp,private__identity_block_9/res4f_branch2a/BiasAdd/ReadVariableOp,private__identity_block_9/bn4f_branch2a/ReadVariableOp,private__identity_block_9/bn4f_branch2a/ReadVariableOp_1,private__identity_block_9/bn4f_branch2a/FusedBatchNormV3/ReadVariableOp,private__identity_block_9/bn4f_branch2a/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_9/res4f_branch2b/Conv2D/ReadVariableOp,private__identity_block_9/res4f_branch2b/BiasAdd/ReadVariableOp,private__identity_block_9/bn4f_branch2b/ReadVariableOp,private__identity_block_9/bn4f_branch2b/ReadVariableOp_1,private__identity_block_9/bn4f_branch2b/FusedBatchNormV3/ReadVariableOp,private__identity_block_9/bn4f_branch2b/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_9/res4f_branch2c/Conv2D/ReadVariableOp,private__identity_block_9/res4f_branch2c/BiasAdd/ReadVariableOp,private__identity_block_9/bn4f_branch2c/ReadVariableOp,private__identity_block_9/bn4f_branch2c/ReadVariableOp_1,private__identity_block_9/bn4f_branch2c/FusedBatchNormV3/ReadVariableOp,private__identity_block_9/bn4f_branch2c/FusedBatchNormV3/ReadVariableOp_1,private__conv_block_3/res5a_branch2a/Conv2D/ReadVariableOp,private__conv_block_3/res5a_branch2a/BiasAdd/ReadVariableOp,private__conv_block_3/bn5a_branch2a/ReadVariableOp,private__conv_block_3/bn5a_branch2a/ReadVariableOp_1,private__conv_block_3/bn5a_branch2a/FusedBatchNormV3/ReadVariableOp,private__conv_block_3/bn5a_branch2a/FusedBatchNormV3/ReadVariableOp_1,private__conv_block_3/res5a_branch2b/Conv2D/ReadVariableOp,private__conv_block_3/res5a_branch2b/BiasAdd/ReadVariableOp,private__conv_block_3/bn5a_branch2b/ReadVariableOp,private__conv_block_3/bn5a_branch2b/ReadVariableOp_1,private__conv_block_3/bn5a_branch2b/FusedBatchNormV3/ReadVariableOp,private__conv_block_3/bn5a_branch2b/FusedBatchNormV3/ReadVariableOp_1,private__conv_block_3/res5a_branch2c/Conv2D/ReadVariableOp,private__conv_block_3/res5a_branch2c/BiasAdd/ReadVariableOp,private__conv_block_3/bn5a_branch2c/ReadVariableOp,private__conv_block_3/bn5a_branch2c/ReadVariableOp_1,private__conv_block_3/bn5a_branch2c/FusedBatchNormV3/ReadVariableOp,private__conv_block_3/bn5a_branch2c/FusedBatchNormV3/ReadVariableOp_1,private__conv_block_3/res5a_branch1/Conv2D/ReadVariableOp,private__conv_block_3/res5a_branch1/BiasAdd/ReadVariableOp,private__conv_block_3/bn5a_branch1/ReadVariableOp,private__conv_block_3/bn5a_branch1/ReadVariableOp_1,private__conv_block_3/bn5a_branch1/FusedBatchNormV3/ReadVariableOp,private__conv_block_3/bn5a_branch1/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_10/res5b_branch2a/Conv2D/ReadVariableOp,private__identity_block_10/res5b_branch2a/BiasAdd/ReadVariableOp,private__identity_block_10/bn5b_branch2a/ReadVariableOp,private__identity_block_10/bn5b_branch2a/ReadVariableOp_1,private__identity_block_10/bn5b_branch2a/FusedBatchNormV3/ReadVariableOp,private__identity_block_10/bn5b_branch2a/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_10/res5b_branch2b/Conv2D/ReadVariableOp,private__identity_block_10/res5b_branch2b/BiasAdd/ReadVariableOp,private__identity_block_10/bn5b_branch2b/ReadVariableOp,private__identity_block_10/bn5b_branch2b/ReadVariableOp_1,private__identity_block_10/bn5b_branch2b/FusedBatchNormV3/ReadVariableOp,private__identity_block_10/bn5b_branch2b/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_10/res5b_branch2c/Conv2D/ReadVariableOp,private__identity_block_10/res5b_branch2c/BiasAdd/ReadVariableOp,private__identity_block_10/bn5b_branch2c/ReadVariableOp,private__identity_block_10/bn5b_branch2c/ReadVariableOp_1,private__identity_block_10/bn5b_branch2c/FusedBatchNormV3/ReadVariableOp,private__identity_block_10/bn5b_branch2c/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_11/res5c_branch2a/Conv2D/ReadVariableOp,private__identity_block_11/res5c_branch2a/BiasAdd/ReadVariableOp,private__identity_block_11/bn5c_branch2a/ReadVariableOp,private__identity_block_11/bn5c_branch2a/ReadVariableOp_1,private__identity_block_11/bn5c_branch2a/FusedBatchNormV3/ReadVariableOp,private__identity_block_11/bn5c_branch2a/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_11/res5c_branch2b/Conv2D/ReadVariableOp,private__identity_block_11/res5c_branch2b/BiasAdd/ReadVariableOp,private__identity_block_11/bn5c_branch2b/ReadVariableOp,private__identity_block_11/bn5c_branch2b/ReadVariableOp_1,private__identity_block_11/bn5c_branch2b/FusedBatchNormV3/ReadVariableOp,private__identity_block_11/bn5c_branch2b/FusedBatchNormV3/ReadVariableOp_1,private__identity_block_11/res5c_branch2c/Conv2D/ReadVariableOp,private__identity_block_11/res5c_branch2c/BiasAdd/ReadVariableOp,private__identity_block_11/bn5c_branch2c/ReadVariableOp,private__identity_block_11/bn5c_branch2c/ReadVariableOp_1,private__identity_block_11/bn5c_branch2c/FusedBatchNormV3/ReadVariableOp,private__identity_block_11/bn5c_branch2c/FusedBatchNormV3/ReadVariableOp_1,fc1000/MatMul/ReadVariableOp,fc1000/BiasAdd/ReadVariableOp\", inputs = \"inputs,conv1_conv2d_readvariableop_resource,conv1_biasadd_readvariableop_resource,bn_conv1_readvariableop_resource,bn_conv1_readvariableop_1_resource,bn_conv1_fusedbatchnormv3_readvariableop_resource,bn_conv1_fusedbatchnormv3_readvariableop_1_resource,private__conv_block_res2a_branch2a_conv2d_readvariableop_resource,private__conv_block_res2a_branch2a_biasadd_readvariableop_resource,private__conv_block_bn2a_branch2a_readvariableop_resource,private__conv_block_bn2a_branch2a_readvariableop_1_resource,private__conv_block_bn2a_branch2a_fusedbatchnormv3_readvariableop_resource,private__conv_block_bn2a_branch2a_fusedbatchnormv3_readvariableop_1_resource,private__conv_block_res2a_branch2b_conv2d_readvariableop_resource,private__conv_block_res2a_branch2b_biasadd_readvariableop_resource,private__conv_block_bn2a_branch2b_readvariableop_resource,private__conv_block_bn2a_branch2b_readvariableop_1_resource,private__conv_block_bn2a_branch2b_fusedbatchnormv3_readvariableop_resource,private__conv_block_bn2a_branch2b_fusedbatchnormv3_readvariableop_1_resource,private__conv_block_res2a_branch2c_conv2d_readvariableop_resource,private__conv_block_res2a_branch2c_biasadd_readvariableop_resource,private__conv_block_bn2a_branch2c_readvariableop_resource,private__conv_block_bn2a_branch2c_readvariableop_1_resource,private__conv_block_bn2a_branch2c_fusedbatchnormv3_readvariableop_resource,private__conv_block_bn2a_branch2c_fusedbatchnormv3_readvariableop_1_resource,private__conv_block_res2a_branch1_conv2d_readvariableop_resource,private__conv_block_res2a_branch1_biasadd_readvariableop_resource,private__conv_block_bn2a_branch1_readvariableop_resource,private__conv_block_bn2a_branch1_readvariableop_1_resource,private__conv_block_bn2a_branch1_fusedbatchnormv3_readvariableop_resource,private__conv_block_bn2a_branch1_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_res2b_branch2a_conv2d_readvariableop_resource,private__identity_block_res2b_branch2a_biasadd_readvariableop_resource,private__identity_block_bn2b_branch2a_readvariableop_resource,private__identity_block_bn2b_branch2a_readvariableop_1_resource,private__identity_block_bn2b_branch2a_fusedbatchnormv3_readvariableop_resource,private__identity_block_bn2b_branch2a_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_res2b_branch2b_conv2d_readvariableop_resource,private__identity_block_res2b_branch2b_biasadd_readvariableop_resource,private__identity_block_bn2b_branch2b_readvariableop_resource,private__identity_block_bn2b_branch2b_readvariableop_1_resource,private__identity_block_bn2b_branch2b_fusedbatchnormv3_readvariableop_resource,private__identity_block_bn2b_branch2b_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_res2b_branch2c_conv2d_readvariableop_resource,private__identity_block_res2b_branch2c_biasadd_readvariableop_resource,private__identity_block_bn2b_branch2c_readvariableop_resource,private__identity_block_bn2b_branch2c_readvariableop_1_resource,private__identity_block_bn2b_branch2c_fusedbatchnormv3_readvariableop_resource,private__identity_block_bn2b_branch2c_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_1_res2c_branch2a_conv2d_readvariableop_resource,private__identity_block_1_res2c_branch2a_biasadd_readvariableop_resource,private__identity_block_1_bn2c_branch2a_readvariableop_resource,private__identity_block_1_bn2c_branch2a_readvariableop_1_resource,private__identity_block_1_bn2c_branch2a_fusedbatchnormv3_readvariableop_resource,private__identity_block_1_bn2c_branch2a_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_1_res2c_branch2b_conv2d_readvariableop_resource,private__identity_block_1_res2c_branch2b_biasadd_readvariableop_resource,private__identity_block_1_bn2c_branch2b_readvariableop_resource,private__identity_block_1_bn2c_branch2b_readvariableop_1_resource,private__identity_block_1_bn2c_branch2b_fusedbatchnormv3_readvariableop_resource,private__identity_block_1_bn2c_branch2b_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_1_res2c_branch2c_conv2d_readvariableop_resource,private__identity_block_1_res2c_branch2c_biasadd_readvariableop_resource,private__identity_block_1_bn2c_branch2c_readvariableop_resource,private__identity_block_1_bn2c_branch2c_readvariableop_1_resource,private__identity_block_1_bn2c_branch2c_fusedbatchnormv3_readvariableop_resource,private__identity_block_1_bn2c_branch2c_fusedbatchnormv3_readvariableop_1_resource,private__conv_block_1_res3a_branch2a_conv2d_readvariableop_resource,private__conv_block_1_res3a_branch2a_biasadd_readvariableop_resource,private__conv_block_1_bn3a_branch2a_readvariableop_resource,private__conv_block_1_bn3a_branch2a_readvariableop_1_resource,private__conv_block_1_bn3a_branch2a_fusedbatchnormv3_readvariableop_resource,private__conv_block_1_bn3a_branch2a_fusedbatchnormv3_readvariableop_1_resource,private__conv_block_1_res3a_branch2b_conv2d_readvariableop_resource,private__conv_block_1_res3a_branch2b_biasadd_readvariableop_resource,private__conv_block_1_bn3a_branch2b_readvariableop_resource,private__conv_block_1_bn3a_branch2b_readvariableop_1_resource,private__conv_block_1_bn3a_branch2b_fusedbatchnormv3_readvariableop_resource,private__conv_block_1_bn3a_branch2b_fusedbatchnormv3_readvariableop_1_resource,private__conv_block_1_res3a_branch2c_conv2d_readvariableop_resource,private__conv_block_1_res3a_branch2c_biasadd_readvariableop_resource,private__conv_block_1_bn3a_branch2c_readvariableop_resource,private__conv_block_1_bn3a_branch2c_readvariableop_1_resource,private__conv_block_1_bn3a_branch2c_fusedbatchnormv3_readvariableop_resource,private__conv_block_1_bn3a_branch2c_fusedbatchnormv3_readvariableop_1_resource,private__conv_block_1_res3a_branch1_conv2d_readvariableop_resource,private__conv_block_1_res3a_branch1_biasadd_readvariableop_resource,private__conv_block_1_bn3a_branch1_readvariableop_resource,private__conv_block_1_bn3a_branch1_readvariableop_1_resource,private__conv_block_1_bn3a_branch1_fusedbatchnormv3_readvariableop_resource,private__conv_block_1_bn3a_branch1_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_2_res3b_branch2a_conv2d_readvariableop_resource,private__identity_block_2_res3b_branch2a_biasadd_readvariableop_resource,private__identity_block_2_bn3b_branch2a_readvariableop_resource,private__identity_block_2_bn3b_branch2a_readvariableop_1_resource,private__identity_block_2_bn3b_branch2a_fusedbatchnormv3_readvariableop_resource,private__identity_block_2_bn3b_branch2a_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_2_res3b_branch2b_conv2d_readvariableop_resource,private__identity_block_2_res3b_branch2b_biasadd_readvariableop_resource,private__identity_block_2_bn3b_branch2b_readvariableop_resource,private__identity_block_2_bn3b_branch2b_readvariableop_1_resource,private__identity_block_2_bn3b_branch2b_fusedbatchnormv3_readvariableop_resource,private__identity_block_2_bn3b_branch2b_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_2_res3b_branch2c_conv2d_readvariableop_resource,private__identity_block_2_res3b_branch2c_biasadd_readvariableop_resource,private__identity_block_2_bn3b_branch2c_readvariableop_resource,private__identity_block_2_bn3b_branch2c_readvariableop_1_resource,private__identity_block_2_bn3b_branch2c_fusedbatchnormv3_readvariableop_resource,private__identity_block_2_bn3b_branch2c_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_3_res3c_branch2a_conv2d_readvariableop_resource,private__identity_block_3_res3c_branch2a_biasadd_readvariableop_resource,private__identity_block_3_bn3c_branch2a_readvariableop_resource,private__identity_block_3_bn3c_branch2a_readvariableop_1_resource,private__identity_block_3_bn3c_branch2a_fusedbatchnormv3_readvariableop_resource,private__identity_block_3_bn3c_branch2a_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_3_res3c_branch2b_conv2d_readvariableop_resource,private__identity_block_3_res3c_branch2b_biasadd_readvariableop_resource,private__identity_block_3_bn3c_branch2b_readvariableop_resource,private__identity_block_3_bn3c_branch2b_readvariableop_1_resource,private__identity_block_3_bn3c_branch2b_fusedbatchnormv3_readvariableop_resource,private__identity_block_3_bn3c_branch2b_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_3_res3c_branch2c_conv2d_readvariableop_resource,private__identity_block_3_res3c_branch2c_biasadd_readvariableop_resource,private__identity_block_3_bn3c_branch2c_readvariableop_resource,private__identity_block_3_bn3c_branch2c_readvariableop_1_resource,private__identity_block_3_bn3c_branch2c_fusedbatchnormv3_readvariableop_resource,private__identity_block_3_bn3c_branch2c_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_4_res3d_branch2a_conv2d_readvariableop_resource,private__identity_block_4_res3d_branch2a_biasadd_readvariableop_resource,private__identity_block_4_bn3d_branch2a_readvariableop_resource,private__identity_block_4_bn3d_branch2a_readvariableop_1_resource,private__identity_block_4_bn3d_branch2a_fusedbatchnormv3_readvariableop_resource,private__identity_block_4_bn3d_branch2a_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_4_res3d_branch2b_conv2d_readvariableop_resource,private__identity_block_4_res3d_branch2b_biasadd_readvariableop_resource,private__identity_block_4_bn3d_branch2b_readvariableop_resource,private__identity_block_4_bn3d_branch2b_readvariableop_1_resource,private__identity_block_4_bn3d_branch2b_fusedbatchnormv3_readvariableop_resource,private__identity_block_4_bn3d_branch2b_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_4_res3d_branch2c_conv2d_readvariableop_resource,private__identity_block_4_res3d_branch2c_biasadd_readvariableop_resource,private__identity_block_4_bn3d_branch2c_readvariableop_resource,private__identity_block_4_bn3d_branch2c_readvariableop_1_resource,private__identity_block_4_bn3d_branch2c_fusedbatchnormv3_readvariableop_resource,private__identity_block_4_bn3d_branch2c_fusedbatchnormv3_readvariableop_1_resource,private__conv_block_2_res4a_branch2a_conv2d_readvariableop_resource,private__conv_block_2_res4a_branch2a_biasadd_readvariableop_resource,private__conv_block_2_bn4a_branch2a_readvariableop_resource,private__conv_block_2_bn4a_branch2a_readvariableop_1_resource,private__conv_block_2_bn4a_branch2a_fusedbatchnormv3_readvariableop_resource,private__conv_block_2_bn4a_branch2a_fusedbatchnormv3_readvariableop_1_resource,private__conv_block_2_res4a_branch2b_conv2d_readvariableop_resource,private__conv_block_2_res4a_branch2b_biasadd_readvariableop_resource,private__conv_block_2_bn4a_branch2b_readvariableop_resource,private__conv_block_2_bn4a_branch2b_readvariableop_1_resource,private__conv_block_2_bn4a_branch2b_fusedbatchnormv3_readvariableop_resource,private__conv_block_2_bn4a_branch2b_fusedbatchnormv3_readvariableop_1_resource,private__conv_block_2_res4a_branch2c_conv2d_readvariableop_resource,private__conv_block_2_res4a_branch2c_biasadd_readvariableop_resource,private__conv_block_2_bn4a_branch2c_readvariableop_resource,private__conv_block_2_bn4a_branch2c_readvariableop_1_resource,private__conv_block_2_bn4a_branch2c_fusedbatchnormv3_readvariableop_resource,private__conv_block_2_bn4a_branch2c_fusedbatchnormv3_readvariableop_1_resource,private__conv_block_2_res4a_branch1_conv2d_readvariableop_resource,private__conv_block_2_res4a_branch1_biasadd_readvariableop_resource,private__conv_block_2_bn4a_branch1_readvariableop_resource,private__conv_block_2_bn4a_branch1_readvariableop_1_resource,private__conv_block_2_bn4a_branch1_fusedbatchnormv3_readvariableop_resource,private__conv_block_2_bn4a_branch1_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_5_res4b_branch2a_conv2d_readvariableop_resource,private__identity_block_5_res4b_branch2a_biasadd_readvariableop_resource,private__identity_block_5_bn4b_branch2a_readvariableop_resource,private__identity_block_5_bn4b_branch2a_readvariableop_1_resource,private__identity_block_5_bn4b_branch2a_fusedbatchnormv3_readvariableop_resource,private__identity_block_5_bn4b_branch2a_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_5_res4b_branch2b_conv2d_readvariableop_resource,private__identity_block_5_res4b_branch2b_biasadd_readvariableop_resource,private__identity_block_5_bn4b_branch2b_readvariableop_resource,private__identity_block_5_bn4b_branch2b_readvariableop_1_resource,private__identity_block_5_bn4b_branch2b_fusedbatchnormv3_readvariableop_resource,private__identity_block_5_bn4b_branch2b_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_5_res4b_branch2c_conv2d_readvariableop_resource,private__identity_block_5_res4b_branch2c_biasadd_readvariableop_resource,private__identity_block_5_bn4b_branch2c_readvariableop_resource,private__identity_block_5_bn4b_branch2c_readvariableop_1_resource,private__identity_block_5_bn4b_branch2c_fusedbatchnormv3_readvariableop_resource,private__identity_block_5_bn4b_branch2c_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_6_res4c_branch2a_conv2d_readvariableop_resource,private__identity_block_6_res4c_branch2a_biasadd_readvariableop_resource,private__identity_block_6_bn4c_branch2a_readvariableop_resource,private__identity_block_6_bn4c_branch2a_readvariableop_1_resource,private__identity_block_6_bn4c_branch2a_fusedbatchnormv3_readvariableop_resource,private__identity_block_6_bn4c_branch2a_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_6_res4c_branch2b_conv2d_readvariableop_resource,private__identity_block_6_res4c_branch2b_biasadd_readvariableop_resource,private__identity_block_6_bn4c_branch2b_readvariableop_resource,private__identity_block_6_bn4c_branch2b_readvariableop_1_resource,private__identity_block_6_bn4c_branch2b_fusedbatchnormv3_readvariableop_resource,private__identity_block_6_bn4c_branch2b_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_6_res4c_branch2c_conv2d_readvariableop_resource,private__identity_block_6_res4c_branch2c_biasadd_readvariableop_resource,private__identity_block_6_bn4c_branch2c_readvariableop_resource,private__identity_block_6_bn4c_branch2c_readvariableop_1_resource,private__identity_block_6_bn4c_branch2c_fusedbatchnormv3_readvariableop_resource,private__identity_block_6_bn4c_branch2c_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_7_res4d_branch2a_conv2d_readvariableop_resource,private__identity_block_7_res4d_branch2a_biasadd_readvariableop_resource,private__identity_block_7_bn4d_branch2a_readvariableop_resource,private__identity_block_7_bn4d_branch2a_readvariableop_1_resource,private__identity_block_7_bn4d_branch2a_fusedbatchnormv3_readvariableop_resource,private__identity_block_7_bn4d_branch2a_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_7_res4d_branch2b_conv2d_readvariableop_resource,private__identity_block_7_res4d_branch2b_biasadd_readvariableop_resource,private__identity_block_7_bn4d_branch2b_readvariableop_resource,private__identity_block_7_bn4d_branch2b_readvariableop_1_resource,private__identity_block_7_bn4d_branch2b_fusedbatchnormv3_readvariableop_resource,private__identity_block_7_bn4d_branch2b_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_7_res4d_branch2c_conv2d_readvariableop_resource,private__identity_block_7_res4d_branch2c_biasadd_readvariableop_resource,private__identity_block_7_bn4d_branch2c_readvariableop_resource,private__identity_block_7_bn4d_branch2c_readvariableop_1_resource,private__identity_block_7_bn4d_branch2c_fusedbatchnormv3_readvariableop_resource,private__identity_block_7_bn4d_branch2c_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_8_res4e_branch2a_conv2d_readvariableop_resource,private__identity_block_8_res4e_branch2a_biasadd_readvariableop_resource,private__identity_block_8_bn4e_branch2a_readvariableop_resource,private__identity_block_8_bn4e_branch2a_readvariableop_1_resource,private__identity_block_8_bn4e_branch2a_fusedbatchnormv3_readvariableop_resource,private__identity_block_8_bn4e_branch2a_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_8_res4e_branch2b_conv2d_readvariableop_resource,private__identity_block_8_res4e_branch2b_biasadd_readvariableop_resource,private__identity_block_8_bn4e_branch2b_readvariableop_resource,private__identity_block_8_bn4e_branch2b_readvariableop_1_resource,private__identity_block_8_bn4e_branch2b_fusedbatchnormv3_readvariableop_resource,private__identity_block_8_bn4e_branch2b_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_8_res4e_branch2c_conv2d_readvariableop_resource,private__identity_block_8_res4e_branch2c_biasadd_readvariableop_resource,private__identity_block_8_bn4e_branch2c_readvariableop_resource,private__identity_block_8_bn4e_branch2c_readvariableop_1_resource,private__identity_block_8_bn4e_branch2c_fusedbatchnormv3_readvariableop_resource,private__identity_block_8_bn4e_branch2c_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_9_res4f_branch2a_conv2d_readvariableop_resource,private__identity_block_9_res4f_branch2a_biasadd_readvariableop_resource,private__identity_block_9_bn4f_branch2a_readvariableop_resource,private__identity_block_9_bn4f_branch2a_readvariableop_1_resource,private__identity_block_9_bn4f_branch2a_fusedbatchnormv3_readvariableop_resource,private__identity_block_9_bn4f_branch2a_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_9_res4f_branch2b_conv2d_readvariableop_resource,private__identity_block_9_res4f_branch2b_biasadd_readvariableop_resource,private__identity_block_9_bn4f_branch2b_readvariableop_resource,private__identity_block_9_bn4f_branch2b_readvariableop_1_resource,private__identity_block_9_bn4f_branch2b_fusedbatchnormv3_readvariableop_resource,private__identity_block_9_bn4f_branch2b_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_9_res4f_branch2c_conv2d_readvariableop_resource,private__identity_block_9_res4f_branch2c_biasadd_readvariableop_resource,private__identity_block_9_bn4f_branch2c_readvariableop_resource,private__identity_block_9_bn4f_branch2c_readvariableop_1_resource,private__identity_block_9_bn4f_branch2c_fusedbatchnormv3_readvariableop_resource,private__identity_block_9_bn4f_branch2c_fusedbatchnormv3_readvariableop_1_resource,private__conv_block_3_res5a_branch2a_conv2d_readvariableop_resource,private__conv_block_3_res5a_branch2a_biasadd_readvariableop_resource,private__conv_block_3_bn5a_branch2a_readvariableop_resource,private__conv_block_3_bn5a_branch2a_readvariableop_1_resource,private__conv_block_3_bn5a_branch2a_fusedbatchnormv3_readvariableop_resource,private__conv_block_3_bn5a_branch2a_fusedbatchnormv3_readvariableop_1_resource,private__conv_block_3_res5a_branch2b_conv2d_readvariableop_resource,private__conv_block_3_res5a_branch2b_biasadd_readvariableop_resource,private__conv_block_3_bn5a_branch2b_readvariableop_resource,private__conv_block_3_bn5a_branch2b_readvariableop_1_resource,private__conv_block_3_bn5a_branch2b_fusedbatchnormv3_readvariableop_resource,private__conv_block_3_bn5a_branch2b_fusedbatchnormv3_readvariableop_1_resource,private__conv_block_3_res5a_branch2c_conv2d_readvariableop_resource,private__conv_block_3_res5a_branch2c_biasadd_readvariableop_resource,private__conv_block_3_bn5a_branch2c_readvariableop_resource,private__conv_block_3_bn5a_branch2c_readvariableop_1_resource,private__conv_block_3_bn5a_branch2c_fusedbatchnormv3_readvariableop_resource,private__conv_block_3_bn5a_branch2c_fusedbatchnormv3_readvariableop_1_resource,private__conv_block_3_res5a_branch1_conv2d_readvariableop_resource,private__conv_block_3_res5a_branch1_biasadd_readvariableop_resource,private__conv_block_3_bn5a_branch1_readvariableop_resource,private__conv_block_3_bn5a_branch1_readvariableop_1_resource,private__conv_block_3_bn5a_branch1_fusedbatchnormv3_readvariableop_resource,private__conv_block_3_bn5a_branch1_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_10_res5b_branch2a_conv2d_readvariableop_resource,private__identity_block_10_res5b_branch2a_biasadd_readvariableop_resource,private__identity_block_10_bn5b_branch2a_readvariableop_resource,private__identity_block_10_bn5b_branch2a_readvariableop_1_resource,private__identity_block_10_bn5b_branch2a_fusedbatchnormv3_readvariableop_resource,private__identity_block_10_bn5b_branch2a_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_10_res5b_branch2b_conv2d_readvariableop_resource,private__identity_block_10_res5b_branch2b_biasadd_readvariableop_resource,private__identity_block_10_bn5b_branch2b_readvariableop_resource,private__identity_block_10_bn5b_branch2b_readvariableop_1_resource,private__identity_block_10_bn5b_branch2b_fusedbatchnormv3_readvariableop_resource,private__identity_block_10_bn5b_branch2b_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_10_res5b_branch2c_conv2d_readvariableop_resource,private__identity_block_10_res5b_branch2c_biasadd_readvariableop_resource,private__identity_block_10_bn5b_branch2c_readvariableop_resource,private__identity_block_10_bn5b_branch2c_readvariableop_1_resource,private__identity_block_10_bn5b_branch2c_fusedbatchnormv3_readvariableop_resource,private__identity_block_10_bn5b_branch2c_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_11_res5c_branch2a_conv2d_readvariableop_resource,private__identity_block_11_res5c_branch2a_biasadd_readvariableop_resource,private__identity_block_11_bn5c_branch2a_readvariableop_resource,private__identity_block_11_bn5c_branch2a_readvariableop_1_resource,private__identity_block_11_bn5c_branch2a_fusedbatchnormv3_readvariableop_resource,private__identity_block_11_bn5c_branch2a_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_11_res5c_branch2b_conv2d_readvariableop_resource,private__identity_block_11_res5c_branch2b_biasadd_readvariableop_resource,private__identity_block_11_bn5c_branch2b_readvariableop_resource,private__identity_block_11_bn5c_branch2b_readvariableop_1_resource,private__identity_block_11_bn5c_branch2b_fusedbatchnormv3_readvariableop_resource,private__identity_block_11_bn5c_branch2b_fusedbatchnormv3_readvariableop_1_resource,private__identity_block_11_res5c_branch2c_conv2d_readvariableop_resource,private__identity_block_11_res5c_branch2c_biasadd_readvariableop_resource,private__identity_block_11_bn5c_branch2c_readvariableop_resource,private__identity_block_11_bn5c_branch2c_readvariableop_1_resource,private__identity_block_11_bn5c_branch2c_fusedbatchnormv3_readvariableop_resource,private__identity_block_11_bn5c_branch2c_fusedbatchnormv3_readvariableop_1_resource,fc1000_matmul_readvariableop_resource,fc1000_biasadd_readvariableop_resource\", outputs = \"identity_RetVal\"}} {\r\n\r\n    %0 = \"tf.Const\"() {device = \"\", value = dense<[-1, 2048]> : tensor<2xi32>} : () -> tensor<2xi32>\r\n    %1 = \"tf.Conv2D\"(%arg0, %arg1) {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 2, 2, 1], use_cudnn_on_gpu = true} : (tensor<2x224x224x3xf32>, tensor<7x7x3x64xf32>) -> tensor<2x112x112x64xf32>\r\n    %2 = \"tf.BiasAdd\"(%1, %arg2) {data_format = \"NHWC\", device = \"\"} : (tensor<2x112x112x64xf32>, tensor<64xf32>) -> tensor<2x112x112x64xf32>\r\n    %y, %batch_mean, %batch_variance, %reserve_space_1, %reserve_space_2, %reserve_space_3 = \"tf.FusedBatchNormV3\"(%2, %arg3, %arg4, %arg5, %arg6) {data_format = \"NHWC\", device = \"\", epsilon = 1.000000e-03 : f32, exponential_avg_factor = 1.000000e+00 : f32, is_training = false} : (tensor<2x112x112x64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> (tensor<2x112x112x64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<*xf32>)\r\n    %3 = \"tf.Relu\"(%y) {device = \"\"} : (tensor<2x112x112x64xf32>) -> tensor<2x112x112x64xf32>\r\n    %4 = \"tf.MaxPool\"(%3) {data_format = \"NHWC\", device = \"\", explicit_paddings = [], ksize = [1, 3, 3, 1], padding = \"VALID\", strides = [1, 2, 2, 1]} : (tensor<2x112x112x64xf32>) -> tensor<2x55x55x64xf32>\r\n    %5 = \"tf.Conv2D\"(%4, %arg25) {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<2x55x55x64xf32>, tensor<1x1x64x256xf32>) -> tensor<2x55x55x256xf32>\r\n    %6 = \"tf.BiasAdd\"(%5, %arg26) {data_format = \"NHWC\", device = \"\"} : (tensor<2x55x55x256xf32>, tensor<256xf32>) -> tensor<2x55x55x256xf32>\r\n    %y_0, %batch_mean_1, %batch_variance_2, %reserve_space_1_3, %reserve_space_2_4, %reserve_space_3_5 = \"tf.FusedBatchNormV3\"(%6, %arg27, %arg28, %arg29, %arg30) {data_format = \"NHWC\", device = \"\", epsilon = 1.000000e-03 : f32, exponential_avg_factor = 1.000000e+00 : f32, is_training = false} : (tensor<2x55x55x256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) -> (tensor<2x55x55x256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<*xf32>)\r\n    %7 = \"tf.Conv2D\"(%4, %arg7) {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<2x55x55x64xf32>, tensor<1x1x64x64xf32>) -> tensor<2x55x55x64xf32>\r\n    %8 = \"tf.BiasAdd\"(%7, %arg8) {data_format = \"NHWC\", device = \"\"} : (tensor<2x55x55x64xf32>, tensor<64xf32>) -> tensor<2x55x55x64xf32>\r\n    %y_6, %batch_mean_7, %batch_variance_8, %reserve_space_1_9, %reserve_space_2_10, %reserve_space_3_11 = \"tf.FusedBatchNormV3\"(%8, %arg9, %arg10, %arg11, %arg12) {data_format = \"NHWC\", device = \"\", epsilon = 1.000000e-03 : f32, exponential_avg_factor = 1.000000e+00 : f32, is_training = false} : (tensor<2x55x55x64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) -> (tensor<2x55x55x64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<*xf32>)\r\n    %9 = \"tf.Relu\"(%y_6) {device = \"\"} : (tensor<2x55x55x64xf32>) -> tensor<2x55x55x64xf32>\r\n    %10 = \"tf.Conv2D\"(%9, %arg13) {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<2x55x55x64xf32>, tensor<3x3x64x64xf32>) -> tensor<2x55x55x64xf32>\r\n    return %10 : tensor<2x55x55x64xf32>\r\n  }\r\n}\r\n\r\n \r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48338\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48338\">No</a>\n"]}, {"number": 48313, "title": "Training Operations Dependencies ", "body": "**System information**\r\n- TensorFlow version: 2.4.1\r\n- Are you willing to contribute it (Yes/No): Yes when published \r\n\r\n**Description**\r\nI am trying to add a feature that enhances the optimization performance and it can be done in two ways:\r\n1. Creating custom optimizers: by inheriting  [optimizer_v2](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L290-L302) \r\n2. Extending existing optimizers\r\n\r\nI just implemented the first option and it works just find, now I need to add the feature to existing optimizers to measure the performance enhancement, e.g. SGD and Adam. I started with ([tensorflow/tensorflow/core/kernels/training_ops.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/training_ops.cc) &  [training_ops.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/training_ops.h)) but then I realized that there are many files that need to be changed. \r\nIn particular, I need to add functions like [ApplyGradientDescent](https://github.com/tensorflow/tensorflow/blob/68e6c8da5a56b4895114ec47fc6aeccb8027de88/tensorflow/core/kernels/training_ops.h#L32).\r\nCan I get a list of files (including build files and wrappers) that need to be changed to get such function working?\r\n\r\n\r\nThanks,\r\n", "comments": []}, {"number": 48312, "title": "Custom layer failed import message is not clear", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 / Linux etc...\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.6.7\r\n- CUDA/cuDNN version: 11\r\n- GPU model and memory:  RTX 3090\r\n\r\n**Describe the current behavior**\r\n\r\nCurrently when using custom layers, when the import is missing during the reloading the raised `ValueError` says:\r\n```\r\nValueError: Input tensors to a Similarity>SimilarityModel must come from `tf.keras.Input`. Received: None (missing previous layer metadata).\r\n``\r\n\r\nTrace:\r\n```\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\training\\tracking\\base.py in _method_wrapper(self, *args, **kwargs)\r\n    515     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    516     try:\r\n--> 517       result = method(self, *args, **kwargs)\r\n    518     finally:\r\n    519       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py in __init__(self, inputs, outputs, name, trainable, **kwargs)\r\n    118     generic_utils.validate_kwargs(kwargs, {})\r\n    119     super(Functional, self).__init__(name=name, trainable=trainable)\r\n--> 120     self._init_graph_network(inputs, outputs)\r\n    121 \r\n    122   @trackable.no_automatic_dependency_tracking\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\training\\tracking\\base.py in _method_wrapper(self, *args, **kwargs)\r\n    515     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    516     try:\r\n--> 517       result = method(self, *args, **kwargs)\r\n    518     finally:\r\n    519       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py in _init_graph_network(self, inputs, outputs)\r\n    155         base_layer_utils.create_keras_history(self._nested_outputs)\r\n    156 \r\n--> 157     self._validate_graph_inputs_and_outputs()\r\n    158 \r\n    159     # A Network does not create weights of its own, thus it is already\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py in _validate_graph_inputs_and_outputs(self)\r\n    689                          'must come from `tf.keras.Input`. '\r\n    690                          'Received: ' + str(x) +\r\n--> 691                          ' (missing previous layer metadata).')\r\n    692       # Check that x is an input tensor.\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe RaiseError should be more explicit about which layer was not properly imported so it's easier to debug. ", "comments": []}, {"number": 48302, "title": "MHLO to LMHLO function signature conversion: missing output argument and stale comments", "body": "With the tensorflow git HEAD at the current master tip a83625aff5add82a150b41922ff99ad0f27fbb35 (Apr 5, 2021), I see the following issues: most of them appear are be due to stale code and an incomplete update post migration to new utilities.\r\n\r\nINPUT\r\n```\r\nfunc @main(%arg0: tensor<1024x1024xf64>, %arg1: tensor<1024x1024xf64>) -> tensor<1024x1024xf64> {\r\n    %0 = \"mhlo.dot\"(%arg0, %arg1) : (tensor<1024x1024xf64>, tensor<1024x1024xf64>) -> tensor<1024x1024xf64>\r\n    \"mhlo.return\"(%0) : (tensor<1024x1024xf64>) -> ()\r\n  }\r\n```\r\n\r\nOUTPUT\r\n```\r\n$ tf-opt -hlo-legalize-to-lhlo  dot.mlir \r\nmodule  {\r\n  func @main(%arg0: memref<1024x1024xf64>, %arg1: memref<1024x1024xf64>) -> memref<1024x1024xf64> {\r\n    %0 = memref.alloc() : memref<1024x1024xf64>\r\n    \"lmhlo.dot\"(%arg0, %arg1, %0) {dot_dimension_numbers = {lhs_batching_dimensions = dense<> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}} : (memref<1024x1024xf64>, memref<1024x1024xf64>, memref<1024x1024xf64>) -> ()\r\n    \"lmhlo.copy\"(%0, %arg1) : (memref<1024x1024xf64>, memref<1024x1024xf64>) -> ()\r\n    \"lmhlo.terminator\"() : () -> ()\r\n  }\r\n}\r\n```\r\nThe tensor being returned has been copied into the wrong memref, and the signature hasn't been converted. Looks like there are assumptions on what it would convert correctly. If the `mhlo.return` is replaced with a standard return, the output is semantically correct:\r\n```\r\nfunc @main(%arg0: memref<1024x1024xf64>, %arg1: memref<1024x1024xf64>) -> memref<1024x1024xf64> {\r\n    %0 = memref.alloc() : memref<1024x1024xf64>\r\n    \"lmhlo.dot\"(%arg0, %arg1, %0) {dot_dimension_numbers = {lhs_batching_dimensions = dense<> : tensor<0xi64>, lhs_contracting_dimensions = dense<1> : tensor<1xi64>, rhs_batching_dimensions = dense<> : tensor<0xi64>, rhs_contracting_dimensions = dense<0> : tensor<1xi64>}} : (memref<1024x1024xf64>, memref<1024x1024xf64>, memref<1024x1024xf64>) -> ()\r\n    return %0 : memref<1024x1024xf64>\r\n  }\r\n```\r\nBut this is inconsistent with the large doct comment at `hlo_legalize_to_lhlo.cc` which says the return value would be converted to an output argument. The comment is probably stale and makes no mention of `-buffer-results-to-out-params` which I think is necessary to get such a conversion. \r\n\r\n```\r\n  // FuncOp signature conversion example:\r\n  //\r\n  // func @func_op(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\r\n  //   %0 = \"mhlo.maximum\"(%arg0, %arg1) : (tensor<4xf32>, tensor<4xf32>) ->\r\n  //   tensor<4xf32> %1 = \"mhlo.add\"(%arg0, %0)  : (tensor<4xf32>,\r\n  //   tensor<4xf32>) -> tensor<4xf32> return %1 : tensor<4xf32>\r\n  // }\r\n  //\r\n  // Transformed function with an extra argument for the result. The types have\r\n  // been converted from tensor to memref.\r\n  //\r\n  // func @func_op(%arg0: memref<4xf32>,\r\n  //               %arg1: memref<4xf32>,\r\n  //               %arg2: memref<4xf32>) {\r\n  //   %0 = alloc() : memref<4xf32>\r\n  \r\n  //   \"lmhlo.maximum\"(%arg0, %arg1, %0) :\r\n  //         (memref<4xf32>, memref<4xf32>, memref<4xf32>) -> ()\r\n  //   %1 = alloc() : memref<4xf32>\r\n  //   \"lmhlo.add\"(%arg0, %0, %1) :\r\n  //         (memref<4xf32>, memref<4xf32>, memref<4xf32>) -> ()\r\n  //   \"lmhlo.copy\"(%1, %arg2) : (memref<4xf32>, memref<4xf32>) -> ()\r\n  //   \"lmhlo.terminator\"() : () -> ()\r\n  // }\r\n```\r\nEven with a `-buffer-results-to-out-params`, a `linalg.copy` is generated instead of an `lmhlo.copy` and so this comment should be updated to reflect changes in MLIR transform utilities.\r\n\r\nCC: @silvasean @stellaraccident ", "comments": ["@sherhut who is more in touch with the mhlo->lmhlo legalization path."]}, {"number": 48296, "title": "Core ML delegate for TensorFlow Lite does not run examples/lite/examples/image_classification/ios on Neural Engine.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, according to https://www.tensorflow.org/lite/performance/coreml_delegate.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 11.2.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPad Air(Gen.4)\r\n- TensorFlow installed from (source or binary): I have no idea since I used pod 'TensorFlowLiteSwift', '~> 0.0.1-nightly'\r\n- TensorFlow version (use command below): I have no idea since I used pod 'TensorFlowLiteSwift', '~> 0.0.1-nightly'\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nUsing Core ML delegate does not have any performance boost compared with the original code on A14 processor.\r\n\r\n**Describe the expected behavior**\r\nUsing Core ML delegate should have performance boost compared with the original code by A14 Neural Engine.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nexamples/lite/examples/image_classification/ios/ in https://github.com/y-ich/examples/tree/coreml_delegate\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Could you take a look at this @lintian06 @yyoon "]}, {"number": 48285, "title": "Update/Refactor Community Page", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/community/contribute/community\r\n\r\n## Description of issue (what needs changing):\r\nThere some outdated and missing info in this page. See the thread at\r\nhttps://github.com/tensorflow/docs/pull/1868#issuecomment-812568979\r\n\r\n/cc @theadactyl ", "comments": ["@bhack  @theadactyl \r\nChecking if this is still an issue. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Palese keep this alive /cc @theadactyl \r\n\r\nSee the original thead at https://github.com/tensorflow/docs/pull/1868#issuecomment-812568979"]}, {"number": 48281, "title": "TensorflowLite (API C++)  No NNAPI acceleration", "body": "**System information**\r\n- OS Platform and Distribution : Linux Ubuntu 20.04\r\n- Mobile device : HUAWEI Honor 8X, Samsung S20, ...\r\n- Android ABI : ARM64\r\n- TensorFlow installed : from source\r\n- TensorFlow version : v2.4.1 and master branch\r\n- Python version: 3.8.8 (conda forge)\r\n- Bazel version : 4.0.0 (conda forge)\r\n- NDK : 21.4.7075529\r\n- NDK API : 29\r\n\r\nHello, I try to perform some pubic and private models with TensorflowLite-API-C++ on several phone under Android 10 at least.\r\n\r\nTo do that, I try to use GPU delegate, NNAPI, XNNPACK methods based on tensorflow/lite/examples/label_image code. \r\n\r\nBut unfortunately, I don't have any acceleration difference between NNAPI and pure CPU. Sometime with NNAPI is even worst.\r\n\r\nTo be sure that the bug not come from my code I use label_image (from tensorflow/lite/examples/label_image)  to test difference between nnapi and cpu acceralation and with several models like mobilenet, inceptionV4, .. (from https://www.tensorflow.org/lite/guide/hosted_models)\r\n\r\nSo, I follow the documentation of the label_image sample (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/label_image) : \r\n\r\n> git clone --depth 1 https://github.com/tensorflow/tensorflow.git tf_arm64\r\n> cd tf_arm64\r\n> ./configure\r\n> bazel build -c opt --config=android_arm64   //tensorflow/lite/examples/label_image:label_image\r\n> adb push bazel-bin/tensorflow/lite/examples/label_image/label_image /data/local/tmp\r\n> adb shell\r\n> ... \r\n> ./label_image -c 10 -a 1 ...\r\n> ...\r\n> ./label_image -c 10 ...\r\n\r\nBut with all the models I don't have any NNAPI acceleration like it's not used. I have the same behavior with Samsung S20 and others mobile phone, It's normal ?\r\n \r\nI want try NNAPI because with GPU delegate my model have unsupported graph node like  :\r\n\r\n>   E tflite  : Following operations are not supported by GPU delegate:\r\n>   E tflite  : GATHER: Operation is not supported.\r\n>   E tflite  : 143 operations will run on the GPU, and the remaining 9 operations will run on the CPU.\r\n>   E tflite  : TfLiteGpuDelegate Init: SPLIT: ReadNonConstantTensor: value is a constant tensor: 224\r\n>   I tflite  : Created 0 GPU delegate kernels.\r\n>   E tflite  : TfLiteGpuDelegate Prepare: delegate is not initialized\r\n>   E tflite  : Node number 152 (TfLiteGpuDelegateV2) failed to prepare.\r\n>   E tflite  : Restored original execution plan after delegate application failure.\r\n>   I libTFLAppAndroidConsole_arm64-v8a.so: Failed to apply  GPU  delegate.\r\n\r\nIf you have any advice. \r\n", "comments": ["@miaowang14 could you take a look at this report?", "First of all, the actual acceleration through NNAPI depends on the driver support on the device you are using. Generally, mid to high end devices with Android 10+ should have much better acceleration support. And certain devices only supports accelerating quantied models while others may support both.\r\n\r\nIf you are using floating point models, it might also worth trying with SetAllowFp16PrecisionForFp32(true) which is more friendly to mobile accelerators. \r\n\r\n", "Hello @miaowang14 thanks for you help. My model is in floating point and a can't reduce accuracy by setting precision to 16fp unfortunately ... \r\n\r\nMy first idea for my problem was it can be due that TensorflowLite wrong delegate with nnapi and that if I call directly my model with the nnapi with ndk (https://developer.android.com/ndk/guides/neuralnetworks) it will maybe work better ? Do you thing it's true ? Like, the possibility that TensorflowLite is not fully compatible with the nnapi version for android 10 and do not support all the graph node conversion. It's just a supposition, please tell me if i wrong. \r\n", "For my \"GPU delegate operations not supported\", (GATHER and SPLIT) do you think that create a custom delegate can solve this ? (https://www.tensorflow.org/lite/performance/implementing_delegate)\r\n\r\nOr I should more look in Xnnpack or Hexagon delegates ? I basically want run my inference as quickly as possible.", "> actual acceleration through NNAPI depends on the **driver support** on the device you are using\r\n\r\nIt's the same this GPU delegate or xnnpack ? Or it'll more depend of the hardware power (CPU / GPU) and not the driver support ?", "The GPU delegate and NNAPI GPU backend/driver are different implementations on top GPU. \r\nAnd IIRC, Hexagon Delegate only supports quantized models. But you might want to give XNNPACK delegate a shot.\r\n\r\nThe performance may not be good If the unsupported ops are all over the place in the model. But if they only occur once or towards the beginning or the end of the model, you may still get good performance with GPU delegate.\r\n\r\nAnd I suggest keep using NNAPI delegate to interfacing with NNAPI. but trying a Android 11 device, e.g. Pixel 4 to see how it goes. You can also enable NNAPI verbose logging by \"adb shell setprop debug.nn.vlog 1\" and logcat can then tell us more information on why it is running slow.", "Hello @miaowang14 thanks again for you help,\r\n\r\nLogcat results on my JSN-L21 (HONOR X8 - Octa Core Kirin 710 - ARM Mali-G51 MP4) :\r\n\r\nWith CPU : \r\n\r\n`./label_image -m mobilenet_v2_1.0_224.tflite -i grace_hopper.bmp -l labels.txt -c 10 `\r\n\r\n```\r\n04-06 11:25:06.598  6654  6654 I tflite  : Initialized TensorFlow Lite runtime.\r\n04-06 11:25:06.616  6654  6654 E libc    : Access denied finding property \"ro.hardware.chipname\"\r\n04-06 11:25:06.616  6654  6654 W label_image_arm: type=1400 audit(0.0:1421812): avc: denied { read } for pid=6654 name=\"u:object_r:vendor_default_prop:s0\" dev=\"tmpfs\" ino=4842 scontext=u:r:shell:s0 tcontext=u:object_r:vendor_default_prop:s0 tclass=file permissive=0\r\n```\r\n\r\n\r\nWith NNAPI and float 16 : \r\n\r\n`./label_image -m mobilenet_v2_1.0_224.tflite -i grace_hopper.bmp -l labels.txt -c 10 -a 1 -f 1`\r\n\r\n```\r\n04-06 10:51:15.245  5771  5771 I tflite  : Initialized TensorFlow Lite runtime.\r\n04-06 10:51:15.401  5771  5771 D skia    : HME hme_jpeg_dec version=JPEG_DEC_16.12.1, releasetime=09:31:09 Oct  8 2019\r\n04-06 10:51:15.402  5771  5771 D skia    : HME SkHmeDecFunction1 constructor ok\r\n04-06 10:51:15.403  5771  5771 I tflite  : Created TensorFlow Lite delegate for NNAPI.\r\n04-06 10:51:15.403  5771  5771 I Manager : DeviceManager::DeviceManager\r\n04-06 10:51:15.403  5771  5771 I Manager : findAvailableDevices\r\n04-06 10:51:15.416  5771  5771 W label_image: type=1400 audit(0.0:1421715): avc: denied { read } for pid=5771 name=\"u:object_r:vendor_default_prop:s0\" dev=\"tmpfs\" ino=4842 scontext=u:r:shell:s0 tcontext=u:object_r:vendor_default_prop:s0 tclass=file permissive=0\r\n04-06 10:51:15.418  5771  5771 E libc    : Access denied finding property \"ro.hardware.chipname\"\r\n```\r\nWith GPU : \r\n\r\n`./label_image -m mobilenet_v2_1.0_224.tflite -i grace_hopper.bmp -l labels.txt -c 10 -g 1`\r\n\r\n```\r\n04-06 11:25:57.594  6662  6662 I tflite  : Initialized TensorFlow Lite runtime.\r\n04-06 11:25:57.600  6662  6662 I tflite  : Created TensorFlow Lite delegate for GPU.\r\n04-06 11:25:57.600  6662  6662 E tflite  : Following operations are not supported by GPU delegate:\r\n04-06 11:25:57.600  6662  6662 E tflite  : SQUEEZE: Operation is not supported.\r\n04-06 11:25:57.600  6662  6662 E tflite  : 29 operations will run on the GPU, and the remaining 2 operations will run on the CPU.\r\n04-06 11:25:57.642  6662  6662 D libOpenCLv1: loaded /system/vendor/lib64/egl/libGLES_mali.so\r\n04-06 11:25:57.654  6662  6662 D libEGL  : loaded /vendor/lib64/egl/libGLES_mali.so\r\n04-06 11:25:58.027  1391  1855 D StubTelephonyRegistry: uid:10030 pid:1717 PhoneStateListener.LISTEN_CELL_LOCATION:16 events:2048\r\n04-06 11:25:58.037  2335  2790 E HwModemCapability: isCapabilitySupport java.lang.StringIndexOutOfBoundsException: length=7; index=7\r\n04-06 11:25:58.070  1391  1855 D StubTelephonyRegistry: uid:10030 pid:1717 PhoneStateListener.LISTEN_CELL_LOCATION:16 events:0\r\n04-06 11:25:46.870  2335  3058 D Mapcon  : VoWifiQoeService: [notifyWifiSignal]getCallbackSize, size=0\r\n04-06 11:26:02.007  1391  1519 I HwBatteryService: quick charge status : 0\r\n04-06 11:26:02.007  1391  1519 D HwBatteryService: onUEvent fcpStatus = 0\r\n04-06 11:26:02.007  1391  1519 D HwBatteryService: onUEvent scpStatus = 0 ,ovpStatus = 0\r\n04-06 11:26:02.007  1391  1519 I HwBatteryService: handleWirelessTxStatus\r\n04-06 11:26:02.007  1391  1519 I HwBatteryService: switch not open, return, mWirelessTxFlag : 0\r\n04-06 11:26:02.007  1391  1519 W HwBatteryService: need return. Invalid value: -1\r\n04-06 11:26:02.007  1391  1519 D HwBatteryService: setHwChargeTimeRemaining, time = -1 String time: -1\r\n04-06 11:26:02.010  1391  1519 I HwUsbDeviceManager: water_intrused state= 0\r\n04-06 11:26:02.860  6662  6662 I tflite  : Initialized OpenCL-based API.\r\n04-06 11:26:03.033  6662  6662 I tflite  : Created 1 GPU delegate kernels.\r\n```\r\nWith XNNPACK : \r\n\r\n`./label_image -m mobilenet_v2_1.0_224.tflite -i grace_hopper.bmp -l labels.txt -c 10 -x 1`\r\n\r\n```\r\n04-06 11:28:09.144  6683  6683 I tflite  : Initialized TensorFlow Lite runtime.\r\n04-06 11:28:09.148  6683  6683 W label_image_arm: type=1400 audit(0.0:1421845): avc: denied { read } for pid=6683 name=\"u:object_r:vendor_default_prop:s0\" dev=\"tmpfs\" ino=4842 scontext=u:r:shell:s0 tcontext=u:object_r:vendor_default_prop:s0 tclass=file permissive=0\r\n04-06 11:28:09.150  6683  6683 E libc    : Access denied finding property \"ro.hardware.chipname\"\r\n04-06 11:28:09.152  6683  6683 I tflite  : Created TensorFlow Lite XNNPACK delegate for CPU.\r\n```\r\n\r\nIt seems that NNAPI it's not even call finally . \r\n\r\n", "It seems that NNAPI Delegate was not able to find any available accelerator backend on HONOR X8, and automatically fallback to TFLite CPU implementation.\r\n\r\nTo confirm that, you can do \"adb shell lshal | grep neural\" to see if there are anything showing up.", "Hello @miaowang14,\r\n\r\nI think your right, seems that NNAPI Delegate is not able to find any available accelerator backend on my HONOR X8, the command : \r\n\r\n`adb shell lshal | grep neural`\r\n\r\nGive me nothing except warning (not catches by grep)  : \r\n\r\n```\r\nWarning: Skipping \"android.frameworks.cameraservice.service@2.0::ICameraService/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.frameworks.displayservice@1.0::IDisplayService/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.frameworks.schedulerservice@1.0::ISchedulingPolicyService/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.frameworks.sensorservice@1.0::ISensorManager/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.frameworks.stats@1.0::IStats/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.audio.effect@5.0::IEffectsFactory/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.audio@5.0::IDevicesFactory/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.biometrics.fingerprint@2.1::IBiometricsFingerprint/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.bluetooth@1.0::IBluetoothHci/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.camera.provider@2.4::ICameraProvider/legacy/0\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.gatekeeper@1.0::IGatekeeper/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.gnss@1.0::IGnss/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.gnss@1.1::IGnss/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.graphics.composer@2.1::IComposer/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.graphics.composer@2.2::IComposer/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.health@2.0::IHealth/backup\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.health@2.0::IHealth/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.keymaster@3.0::IKeymasterDevice/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.light@2.0::ILight/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.media.c2@1.0::IComponentStore/software\": no information for PID 832, are you root?\r\nWarning: Skipping \"android.hardware.memtrack@1.0::IMemtrack/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.nfc@1.0::INfc/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.nfc@1.1::INfc/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.nfc@1.2::INfc/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.power.stats@1.0::IPowerStats/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.power@1.0::IPower/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.radio.config@1.0::IRadioConfig/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.radio.config@1.1::IRadioConfig/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.radio@1.0::IRadio/slot1\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.radio@1.0::IRadio/slot2\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.radio@1.1::IRadio/slot1\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.radio@1.1::IRadio/slot2\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.radio@1.2::IRadio/slot1\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.radio@1.2::IRadio/slot2\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.radio@1.3::IRadio/slot1\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.radio@1.3::IRadio/slot2\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.radio@1.4::IRadio/slot1\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.radio@1.4::IRadio/slot2\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.secure_element@1.0::ISecureElement/SIM1\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.sensors@1.0::ISensors/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.thermal@1.0::IThermal/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.usb@1.0::IUsb/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.vibrator@1.0::IVibrator/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.wifi.supplicant@1.0::ISupplicant/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.wifi.supplicant@1.1::ISupplicant/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.wifi.supplicant@1.2::ISupplicant/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.wifi@1.0::IWifi/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.wifi@1.1::IWifi/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.wifi@1.2::IWifi/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hardware.wifi@1.3::IWifi/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/HwInSExtNode\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/SIM1\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/ashmem\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/backup\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/hiaiserver\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/hiaiserver_modelmanager\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/hiaiserver_v2\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/huawei.camera.cfgsvr\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/huawei.cameraresource.service\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/huaweiantitheft\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/huaweisigntool\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/hwfactoryinterface_hal\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/hwhiview_hal\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/hwstp\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/legacy/0\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/perfgenius\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/perfpolicy\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/rildi\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/rildi2\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/slot1\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/slot2\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/software\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.hidl.base@1.0::IBase/uniperf\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.system.net.netd@1.0::INetd/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.system.net.netd@1.1::INetd/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.system.suspend@1.0::ISystemSuspend/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"android.system.wifi.keystore@1.0::IKeystore/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.activity_recognition@1.0::IActivityRecognition/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.activity_recognition@1.1::IActivityRecognition/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.biometrics.fingerprint@2.1::IExtBiometricsFingerprint/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.biometrics.fingerprint@2.2::IExtBiometricsFingerprint/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.biometrics.hwfacerecognize@1.0::IBiometricsFaceRecognize/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.biometrics.hwfacerecognize@1.1::IBiometricsFaceRecognize/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.biometrics.hwfacerecognize@1.2::IBiometricsFaceRecognize/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.biometrics.hwsecurefacerecognize@1.0::ISecureBiometricsFaceRecognize/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.biometrics.hwsecurefacerecognize@1.1::ISecureBiometricsFaceRecognize/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.bluetooth@1.0::IHwBluetoothHciExt/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.camera.camResource@1.0::IHwCameraResourceService/huawei.cameraresource.service\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.camera.camResource@1.1::IHwCameraResourceService/huawei.cameraresource.service\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.camera.camResource@1.2::IHwCameraResourceService/huawei.cameraresource.service\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.camera.camResource@1.3::IHwCameraResourceService/huawei.cameraresource.service\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.camera.cfgsvr@1.0::IHwCamCfgSvr/huawei.camera.cfgsvr\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.camera.cfgsvr@1.1::IHwCamCfgSvr/huawei.camera.cfgsvr\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.camera.factory@1.0::ICameraFactory/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.dubai@1.0::IDubaiManager/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.dubai@1.1::IDubaiManager/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.fusd@1.0::IFusdLbs/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.fusd@1.1::IFusdLbs/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.fusd@1.2::IFusdLbs/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.fusd@1.3::IFusdLbs/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.fusd@1.4::IFusdLbs/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.gnss@1.0::IHWGnss/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.graphics.mediacomm@2.0::IMediaComm/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hinetmanager@1.0::IHinetmanagerDevice/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hisiradio@1.0::IHisiRadio/slot1\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hisiradio@1.0::IHisiRadio/slot2\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hisiradio@1.1::IHisiRadio/slot1\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hisiradio@1.1::IHisiRadio/slot2\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hisiradio@1.2::IHisiRadio/slot1\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hisiradio@1.2::IHisiRadio/slot2\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hisupl@1.0::ISuplclienttoserverInterface/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hivrar@2.0::IHiVRAR/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.huawei_security_vnode@1.0::IHwSecurityVNode/HwInSExtNode\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.huaweiantitheft@1.0::IHuaweiAntiTheft/huaweiantitheft\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.huaweisigntool@1.0::ISignTool/huaweisigntool\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hwdisplay.displayengine@1.0::IDisplayEngineWrapper/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hwdisplay.displayengine@1.1::IDisplayEngineWrapper/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hwdisplay.displayengine@1.2::IDisplayEngineWrapper/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hwdisplay@1.0::IDisplay/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hwfactoryinterface@1.0::IHwFactoryInterface/hwfactoryinterface_hal\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hwfactoryinterface@1.1::IHwFactoryInterface/hwfactoryinterface_hal\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hwhiview@1.0::IHwHiviewInterface/hwhiview_hal\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hwhiview@1.1::IHwHiviewInterface/hwhiview_hal\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hwstp@1.0::IHwStp/hwstp\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.hwvibrator@1.0::IHWVibrator/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.iawareperf@1.0::IUniPerf/uniperf\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.iawareperfpolicy@1.0::IPerfPolicy/perfpolicy\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.iked@1.0::IIkedDevice/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.libteec@3.0::ILibteecGlobal/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.light@2.0::ILight/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.modemchr@1.0::IModemchrDevice/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.nfc@1.0::IHWNfc/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.perfgenius@2.0::IPerfGenius/perfgenius\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.power@1.0::IHWPower/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.radio.chr@1.0::IRadioChr/slot1\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.radio.chr@1.0::IRadioChr/slot2\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.radio.deprecated@1.0::IOemHook/slot1\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.radio.deprecated@1.0::IOemHook/slot2\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.radio.ims@1.0::IRadioIms/rildi\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.radio.ims@1.0::IRadioIms/rildi2\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.radio.ims@1.1::IRadioIms/rildi\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.radio.ims@1.1::IRadioIms/rildi2\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.radio.ims@1.2::IRadioIms/rildi\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.radio.ims@1.2::IRadioIms/rildi2\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.radio@2.0::IRadio/slot1\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.radio@2.0::IRadio/slot2\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.sensors@1.0::ISensors/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.tp@1.0::ITouchscreen/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.wifi.supplicant@3.0::ISupplicant/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.wifi@1.0::IHwWifiExt/default\": cannot be fetched from service manager (null)\r\nWarning: Skipping \"vendor.huawei.hardware.wifi@1.1::IHwWifiExt/default\": cannot be fetched from service manager (null)\r\n```\r\n\r\nI test again with my Samsung S20 and I clearly saw the difference with and without NNAPI on quantified model. Furthermore I saw NNAPI activation too (with \"adb shell setprop debug.nn.vlog all\" \"adb logcat\" and \"adb shell lshal | grep neural\"). So it's specific to my HONOR 8X. \r\n\r\nDo you have a explanation ? It's related directly to NNAPI and my phone or it can be the Tensorflow Delegate method who don't support delegation with my phone ?\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "@artzet-s , thanks for trying on the phone. This is a device issue. Devices with mid to low end SoCs, especially the ones introduced before Android 10, often are not equipped with dedicated ML accelerator. Also, because of the diversity of hardware running Android, certain SoCs may be particularly strong at accelerating floating point models, while others may be good at quantized models (or both). Today, more and more devices are shipping with ML accelerator and the NNAPI drivers are getting better too, but there are still lots of room to improve.\r\n\r\nIn the case of no accelerator found on the device or no accelerator able to support the particular model, NNAPI delegate will by default fallback to TFLite CPU implementation.\r\n\r\n"]}, {"number": 48255, "title": "Trying To Resolve Eigen Library For TFLM Visual Studio Build", "body": "**System information**\r\n- Host OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from Source\r\n- Tensorflow version 2.4.1\r\n- Target platform Visual Studio\r\n\r\n\r\n\r\n**Describe the problem**\r\nI am attempting to build TFLM in Visual Studio, thus am unable to rely on the Bazel, CMake, or Make build tools (which seemed to have issue for TFLM example projects). I have a build that works without some kernels, like LSTM.\r\n\r\nI am needing to resolve my Eigen library dependency to be able to load my models properly in C++. This dependency first presents when I add the tensorflow/lite/kernels/add.cc file. This C++ includes the header file tensorflow/lite/kernels/internal/optimized/optimized_ops.h, which in turn includes two header file folders from the Eigen library:\r\n\r\n\r\n#include \"third_party/eigen3/Eigen/Core\"\r\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\r\n\r\nI manually attempted to download the Eigen library from here and add it into my project without any guidance from a build process, which seems to be the only way to troubleshoot with TFLM:\r\n\r\nhttps://gitlab.com/libeigen/eigen\r\n\r\nThere is an eigen/src/Core folder here (doesn't exactly match \"Eigen/Core\"):\r\n\r\nhttps://gitlab.com/libeigen/eigen/-/tree/master/Eigen/src/Core\r\n\r\nHowever, there is nothing matching the unsupported/Eigen/CXX11/Tensor pattern at all.\r\n\r\nCan someone please help me understand where/how to pull the Eigen library to satisfy the tensorflow/lite/kernels/add.cc dependency as a next step to resolving the rest of my dependency issues? \r\n\r\nI am fairly new to Bazel, CMake, and Make, but have been attempting to use them to build TFLM example projects from the tensorflow/lite/micro/examples folder, for example magic_wand, but there always seems to be issues with the build (for example, problems with checksums like this: https://github.com/tensorflow/tensorflow/issues/47467)\r\n\r\nThanks!", "comments": []}, {"number": 48232, "title": "Loading multiple Models over different Processes", "body": "I am creating a GUI for handling neural nets using Keras.\r\n\r\nI run, save and load models in different processes (multiprocessing.Process). I assumed the global state of tensorflow is being cleaned up after the process is being terminated. This works so far well when running and saving the models in different processes.\r\n\r\nSomehow when I load a net and the process terminates, somehow the global variables start blocking tensorflow again.\r\n\r\nWhat is a good approach working with multiple models with tensorflow 2. I have found approaches for Tensorflow 1, but a good approach for tensorflow 2 I have not found yet.\r\n\r\nIt is quite hard to provide a code snippet, since I only need a good solution working with different models and my project is quite complex.", "comments": ["Perhaps [tf.compat.v1.reset_default_graph](https://www.tensorflow.org/api_docs/python/tf/compat/v1/reset_default_graph) can work in your case.", "Thanks for your response. This does not work. @ymodak \r\n\r\nI called tf.compat.v1.reset_default_graph() before exiting the process where the net has been loaded. Other processes are still being blocked.", "What I am actually doing in the subprocess is the following:\r\n\r\n```    \r\n    with tf.keras.utils.custom_object_scope(custom_objects):\r\n\r\n        # path_net_config is the path to the yaml configuration of the net\r\n        with open(path_net_config, \"r\") as f:\r\n            loaded_config_yaml = yaml.full_load(f)\r\n\r\n            if name_base_class == \"Sequential\":\r\n                model = tf.keras.Sequential().from_config(loaded_config_yaml)\r\n            elif name_base_class == \"Model\":\r\n                model = tf.keras.Model().from_config(loaded_config_yaml)\r\n            else:\r\n                return None\r\n            model.load_weights(path_weights)\r\n\r\n    weights = model.get_weights()\r\n```"]}, {"number": 48223, "title": "fake_quant_with_min_max_vars  can only get five significant figures instead of six", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source): NO\r\n- GCC/Compiler version (if compiling from source): NO\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: Tesla V100-SXM2-32GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\nv2.4.0-49-22-g85c8b2a817f 2.4.1\r\n\r\n**Describe the current behavior**\r\n\"tf.quantization.fake_quabt_with_min_max_vars\" can only get five significant figures.\r\n\r\n**Describe the expected behavior**\r\n\"tf.quantization.fake_quabt_with_min_max_vars\" should get six significant figures when float32 has six significant figures.\r\nWith regard to pytorch \"torch.fake_quantize_per_tensor_affine\" always has six significant figures.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nimport tensorflow as tf\r\n\r\nmin_var = -10.5268\r\nmax_var = 15.1868\r\n\r\noutput_1 = tf.quantization.fake_quant_with_min_max_vars(0.27, min_var, max_var, num_bits=8, narrow_range=True)\r\nprint(output_1 ) #output_1 is 0.3037033\r\n\r\noutput_2 = tf.quantization.fake_quant_with_min_max_vars(-0.27, min_var, max_var, num_bits=8, narrow_range=True)\r\nprint(output_2 ) #output_2 is -0.30370426\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["\"fake_quant_with_min_max_vars \" is  implemented using XLA. For accuracy I wonder why it is different from pytorch which can ensure six significant figures.", "@xueyedamo521 \r\nWe see that you are using an old version of tensorflow, there is no support for tf 1.x hence please upgrade to 2.x and let us know.", "@Saduf2019 \r\nI have changed to TF2.x, but the error is same. Please check the information i re-edit above. \r\nThanks.", "output_1 = tf.quantization.fake_quant_with_min_max_vars(0.27, min_var, max_var, num_bits=8, narrow_range=True)\r\nprint(output_1 ) #output_1 is 0.3037033\r\n\r\nthe result is error. \r\nscale = (max_var-min_var)/(255-1) #should be 0.1012346, and float32 can guarantee six significant figures, \"6\" can not be ensured.\r\nthe fake quant result of \"0.27\" should be \"scale*3\". Observely, TF is not right with only five significant figures.", "@Saduf2019", "@ymodak \r\nI am able to replicate the issue reported please find the [gist here](https://colab.research.google.com/gist/Saduf2019/48b063c91a696b1d84da97d36d61eb3e/untitled583.ipynb).\r\nThanks!", "@ymodak \r\nLooking forward to any reply from you\uff0cthanks.", "I tried reproducing the issue in `2.6.0` and in nightly i.e `2.8.0-dev20211102`, the results are still similar to the one reported in the issue. Please find the [gist here](https://colab.sandbox.google.com/gist/sanatmpa1/3231a6791ba0dee202ba0e7645a580a8/untitled583.ipynb). Thanks!"]}, {"number": 48213, "title": "Exception when saving custom RNN model with a constant in the call function when using SavedModel format", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Redhat 7.8.2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.4.1 and 2.3.1 (tested)\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):  No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\n**Current Behavior**\r\nWhen saving (SavedModel format) an RNN model with a constant in the call function (like shown below), we get an exception.\r\n\r\n**Desired behaviour** \r\nWe should be able to save `model` defined below, in a SavedModel format, just like we can save it in an h5 format.\r\n\r\n**Code to reproduce the issue:**\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras as tfk\r\nimport tensorflow.keras.backend as K\r\nimport numpy as np\r\n\r\nclass MinimalRNNCell(tfk.layers.Layer):\r\n\r\n    def __init__(self, units, **kwargs):\r\n        self.units = units\r\n        self.state_size = units\r\n        super(MinimalRNNCell, self).__init__(**kwargs)\r\n\r\n    def build(self, input_shape):\r\n        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\r\n                                      initializer='uniform',\r\n                                      name='kernel')\r\n        self.recurrent_kernel = self.add_weight(\r\n            shape=(self.units, self.units),\r\n            initializer='uniform',\r\n            name='recurrent_kernel')\r\n        self.built = True\r\n\r\n    def call(self, inputs, states=None, constants=None, training=False, *args, **kwargs):\r\n        prev_output = states[0]\r\n        print(\"constants: \", constants)\r\n        h = K.dot(inputs, self.kernel) + constants[0]\r\n        output = h + K.dot(prev_output, self.recurrent_kernel)\r\n        return output, [output]\r\n\r\n    def get_config(self):\r\n        return dict(super().get_config(), **{'units': self.units})\r\n\r\ncell = MinimalRNNCell(32)\r\nx = tfk.Input((None, 5), name='x')\r\nz = tfk.Input((1), name='z')\r\nlayer = tfk.layers.RNN(cell, name='rnn')\r\ny = layer(x, constants=[z])\r\n\r\nmodel = tfk.Model(inputs=[x, z], outputs=[y])\r\nmodel.compile(optimizer='adam', loss='mse')\r\nmodel.save('tmp.h5') # This works ok\r\nmodel_loaded = tfk.models.load_model('tmp.h5', custom_objects={'MinimalRNNCell': MinimalRNNCell})\r\nprint(model_loaded.predict([np.array([[[0,0,0,0,0]]]), np.array([[0]])])) # This works ok\r\nmodel.save('tmp2') # This throws an exception\r\n```\r\n\r\n**Other info**\r\nThe stdout from the above is\r\n```\r\nconstants:  (<tf.Tensor 'Placeholder_1:0' shape=(None, 1) dtype=float32>,)\r\nconstants:  (<tf.Tensor 'Placeholder_1:0' shape=(None, 1) dtype=float32>,)\r\nconstants:  (<tf.Tensor 'Placeholder_1:0' shape=(None, 1) dtype=float32>,)\r\nconstants:  (<tf.Tensor 'Placeholder_1:0' shape=(None, 1) dtype=float32>,)\r\nconstants:  (<tf.Tensor 'model_1/Cast_1:0' shape=(None, 1) dtype=float32>,)\r\nconstants:  (<tf.Tensor 'model_1/Cast_1:0' shape=(None, 1) dtype=float32>,)\r\n[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\r\n  0. 0. 0. 0. 0. 0. 0. 0.]]\r\nconstants:  (<tf.Tensor 'z:0' shape=(None, 1) dtype=float32>,)\r\nconstants:  (<tf.Tensor 'z:0' shape=(None, 1) dtype=float32>,)\r\nconstants:  (<tf.Tensor 'constants:0' shape=(None, None, 5) dtype=float32>,)\r\n```\r\n\r\nI'll attach the full exception [here](https://github.com/tensorflow/tensorflow/files/6235635/tensorflow_rnn_exception.txt). Anyone should be able to reproduce. However, the main error is\r\n```\r\nValueError: Dimensions must be equal, but are 32 and 5 for '{{node add}} = AddV2[T=DT_FLOAT](MatMul, constants)' with input shapes: [?,32], [?,?,5].\r\n```\r\nFor some reason, the name of `z` changes to `constants` and the shape changes from the expected `(None, 1)` to `(None, None, 5)`. \r\n\r\nAny ideas appreciated. \r\n\r\nThanks in advance.\r\n\r\n", "comments": ["Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/70335ea8349dae6d8a98840b0cb3f78a/48213-2-3.ipynb) and [TF v2.4](https://colab.research.google.com/gist/amahendrakar/6ded3f5bd216f6b6bce44aa0a3367f82/48213.ipynb).\r\n\r\nWhereas with the latest [TF-nightly](https://colab.research.google.com/gist/amahendrakar/afa52880fc8f09635e60c6b5141eecbe/48213-tf-nightly.ipynb#scrollTo=K_zbK2SwaPyv), I am facing an error stating `TypeError: call() got multiple values for argument 'training'`. Please check the linked gist for reference. Thanks!", "Any news on this one? Anything else I can help with?", "On 2.4, changing to saving in the new model format (e.g. `model.save('tmp.model')`) instead of HDF5 results in the error occurring at save time.", "On TF 2.5 we get a different issue. \r\n```\r\nTypeError: call() got multiple values for argument 'training'\r\n```\r\nThis is probably where we should focus our efforts. ", "When I simply remove the `training=False` inside `call` we get the same issue as before (version 2.4 and lower) so no progress made.", "@k-w-w might you have any ideas? Or know who might? It seems like this is something in the serialization layer. Thank you!", "Updated example, with newer APIs and focusing on saving as the issue:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.linalg import matmul\r\nimport tensorflow.keras as tfk\r\nimport tensorflow.keras.backend as K\r\nimport numpy as np\r\n\r\nclass MinimalRNNCell(tfk.layers.Layer):\r\n\r\n    def __init__(self, units, **kwargs):\r\n        self.units = units\r\n        self.state_size = units\r\n        super(MinimalRNNCell, self).__init__(**kwargs)\r\n\r\n    def build(self, input_shape):\r\n        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\r\n                                      initializer='uniform',\r\n                                      name='kernel')\r\n        self.recurrent_kernel = self.add_weight(\r\n            shape=(self.units, self.units),\r\n            initializer='uniform',\r\n            name='recurrent_kernel')\r\n        self.built = True\r\n\r\n    def call(self, inputs, states=None, constants=None, *args, **kwargs):\r\n        prev_output = states[0]\r\n        print(\"constants: \", constants[0].name)\r\n        h = matmul(inputs, self.kernel) + constants[0]\r\n        output = h + matmul(prev_output, self.recurrent_kernel)\r\n        return output, [output]\r\n\r\n    def get_config(self):\r\n        return dict(super().get_config(), **{'units': self.units})\r\n\r\ncell = MinimalRNNCell(32)\r\nx = tfk.Input((None, 5), name='x')\r\nz = tfk.Input((1,), name='z')\r\nlayer = tfk.layers.RNN(cell, name='rnn')\r\ny = layer(x, constants=[z])\r\n\r\nmodel = tfk.Model(inputs=[x, z], outputs=[y])\r\nmodel.compile(optimizer='adam', loss='mse')\r\nmodel.predict([np.array([[[0,0,0,0,0]]]), np.array([[0]])])\r\nmodel.save('tmp.model')\r\n```", "Thanks for reporting this! Looks like there is a bug with the RNN shape inference when using custom cells. For now, please the following workaround: (The model should use a subclass of RNN)\r\n\r\n```python\r\nclass MinimalRNNCell(tfk.layers.Layer):\r\n  ... # same as above\r\n\r\n#  Create an RNN subclass to bypass the shape inference logic\r\nclass RNN(tfk.layers.RNN):\r\n  pass\r\n\r\ncell = MinimalRNNCell(32)\r\nx = tfk.Input((None, 5), name='x')\r\nz = tfk.Input((1,), name='z')\r\nlayer = RNN(cell, name='rnn')  #  This now uses RNN instead of tfk.layers.RNN\r\ny = layer(x, constants=[z])\r\n\r\nmodel = tfk.Model(inputs=[x, z], outputs=[y])\r\nmodel.compile(optimizer='adam', loss='mse')\r\nmodel.predict([np.array([[[0,0,0,0,0]]]), np.array([[0]])])\r\nmodel.save('tmp_model')\r\n\r\n# To load the model, the custom cell must be passed into `custom_objects`\r\nloaded = tfk.models.load_model('tmp_model',custom_objects={'MinimalRNNCell': MinimalRNNCell})\r\n```", "Hi @ingolfured ! Inline with @k-w-w comment.  I tried to replicate and resolve this  issue by adding\r\n\r\n>  .h5 extension\r\n\r\n for model file in last line. Attaching [Gist](https://colab.research.google.com/gist/mohantym/6c0653d7b7b41db348ad76ddcc8c0926/github_48213.ipynb#scrollTo=uoja--mNHoA6) in TF 2.6  for reference", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Still waiting for resolution, it's not stale.", "Hi @itamarst! Did you check with above [Gist ](https://colab.research.google.com/gist/mohantym/6c0653d7b7b41db348ad76ddcc8c0926/github_48213.ipynb#scrollTo=uoja--mNHoA6) ? Issues seems to be resolved by model extension in some cases.", "@mohantym you will notice that the original filed issue (at the top) was actually saving as `.h5`. So changing extension to `.h5` is not sufficient.\r\n\r\nSpecifically, with Tensorflow 2.6:\r\n\r\n```\r\n... lots and lots and lots of callstack ...\r\n  File \"/home/itamarst/Devel/sandbox/venv/lib64/python3.9/site-packages/tensorflow/python/framework/smart_cond.py\", line 58, in smart_cond\r\n    return false_fn()\r\n  File \"/home/itamarst/Devel/sandbox/venv/lib64/python3.9/site-packages/keras/saving/saved_model/utils.py\", line 166, in <lambda>\r\n    lambda: replace_training_and_call(False))\r\n  File \"/home/itamarst/Devel/sandbox/venv/lib64/python3.9/site-packages/keras/saving/saved_model/utils.py\", line 162, in replace_training_and_call\r\n    return wrapped_call(*args, **kwargs)\r\n  File \"/home/itamarst/Devel/sandbox/venv/lib64/python3.9/site-packages/keras/saving/saved_model/save_impl.py\", line 633, in call_and_return_conditional_losses\r\n    call_output = layer_call(*args, **kwargs)\r\nTypeError: call() got multiple values for argument 'training'\r\n```\r\n\r\nSo first, that doesn't always fix the bug.\r\n\r\nSecond, even if it is \"resolved in some cases\" that does not mean resolved in all cases, so this bug should stay open, otherwise other people will have random breakage and have to go through their own whole debugging, ticket filing, etc..", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48213\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48213\">No</a>\n", "opening with respect to @itamarst's comments.", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48213\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48213\">No</a>\n", "This is not yet fixed, and should be reopened.", "Hi @ingolfured !  It is still valid in [2.8](https://colab.sandbox.google.com/gist/mohantym/6361c4182be018b9ba02b9b8fc1bb799/github_48213.ipynb#scrollTo=IWCFNeDscLIz) version . Could you please post this on [Keras](https://github.com/keras-team/keras) repo and close this one here?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48213\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48213\">No</a>\n", "As far as I know this bug is still real.", "Yeah. It is replicating when saved without h5 extension."]}, {"number": 48212, "title": "Quantization Aware Training model has weird inference behavior", "body": "Hi,\r\nI have a pretrained detection model I trained in Tensorflow 2.3 with fp32 precision. I used this model's weights as initial pretrained weights for Quantization Aware Training (QAT). During training I could see that training converged and gave logical predictions (I visualized results during training).\r\nWhen trying to load with Tensorflow the Quantization Aware Training weights for inference the model behaves differently: When applying in inference the same preprocessing + normalization of mean/std, the predictions' probabilities are always around 0.25. Also, all the predictions' bounding boxes in inference are really small: around 15 pixels height/width, while during training I could see predictions having logical bounding boxes that are with varied dimensions and with much higher probabilities. This behavior appears even on images that were used for training so it's not related to overfitting issues.\r\n\r\nTechnical constraints for my implementation:\r\n1) I used Tensorflow's `tf.quantization.quantize_and_dequantize` in specific places in my model: After the activation function which appears in the following pattern: `Conv2D` --> BatchNormalization --> `Activation`. I attach here an example for such pattern: \r\n\r\n```\r\nfrom tensorflow.keras.layers import Conv2D, ZeroPadding2D, LeakyReLU, ReLU, BatchNormalization\r\n\r\ndef MyConv(x, filters, kernel_size, strides=1, batch_norm=True, is_quantized=False):\r\n    if strides == 1:\r\n        padding = \"same\"\r\n    else:\r\n        x = ZeroPadding2D(((1, 0), (1, 0)))(x)  # top left half-padding\r\n        padding = \"valid\"\r\n    x = Conv2D(filters=filters, kernel_size=kernel_size,\r\n               strides=strides, padding=padding,\r\n               use_bias=not batch_norm, kernel_regularizer=l2(0.0005))(x)\r\n\r\n    if batch_norm:\r\n        x = BatchNormalization()(x)\r\n        if is_quantized:\r\n            # We'll use The leaky version of Relu6 for stability of low-precision computations\r\n            # We define the Leaky Relu using Relu for conversion constrains to TensorRT later\r\n            x = ReLU(negative_slope=0.1, max_value=6)(x)\r\n        else:\r\n            x = LeakyReLU(alpha=0.1)(x)\r\n    if is_quantized:\r\n        # Using tf syntax of quantization - applying quantization only on the conv op, after the activation\r\n        x = tf.quantization.quantize_and_dequantize(x, input_min=-64, input_max=64, range_given=False)\r\n        # The input_min / input_max should be ignored due to range_given=False\r\n    return x\r\n```\r\nThis function is called when building the model both for training and inference. \r\nThe reason I place the `quantize_and_dequantize` op specifically after the activation function is because this model should be converter to TensorRT later- their [QAT guidelines](https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#quantization-training) note that this is the place to apply quantization op for QAT, due to the layer fusion TensorRT applies in the conversion and optimization process.\r\nI tried also applying QAT using the [Tensorflow Model Optimization Toolkit as describes here](https://www.tensorflow.org/model_optimization/guide/quantization/training_example), but this resulted with a model that includes unsupported nodes for TensorRT such as `ConvInteger`. A TF model that uses quantize_and_dequantize produces model that is possible to convert to TRT, but has the issues I described above due to the irregular behavior in TF during inference. Here is an example to the quant ops in Netron for the inference model:\r\n\r\n![qat structure example](https://user-images.githubusercontent.com/23454156/113120276-17bd0280-921a-11eb-9d79-9b0a80e8e7b9.PNG)\r\n\r\nThis is a Tensorflow-related issue since the issue appears during inference in Tensorflow with QAT weights.\r\n2) I create the model for inference using the following command:\r\n```\r\ntf.keras.backend.set_learning_phase(1)\r\nself.model = self._build_model()\r\n```\r\nwhere `build_model()` calls internally also the function I attached here which contains the quantization ops after every Activation function.\r\n\r\nIs there any additional command that needs to be applied in inference when using `tf.quantize.quantize_and_dequantize` ? I remember in TF 1.x there was `tf.contrib.create_eval_graph`, but I didn't see anything similar to it in TF 2.3.\r\nI prefer a solution that lets me still use `tf.quantize.quantize_and_dequantize` due to the TensorRT constraints I described (since I can't use Model Optimization Toolkit's `quantize_model`).\r\n\r\n**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow version: 2.3.0\r\n- Python version: 3.6\r\n", "comments": ["@xhae  could you triage this issue?", "@Xhark Can you take the issue?", "Another note, I would like to know if this is a possible cause for issues: When I inspect the onnx converted model from my QAT tensorflow model, I can see the following for all `QuantizeLinear` and `DequantizeLinear` nodes: `x_scale` values are always ~0.5039 (`x_zero_point` is 0, as it should be). I would have expected this quantization scale to be different for all layers since this is a learnable parameter for quantization op. Is it ok?", "I think I've found the issue- it's related to the way I loaded the weights with the relevant flags.\r\nPreviously I loaded the weights like this:\r\n```\r\nkeras_model.load_weights(transform_weight_file, by_name=True)\r\n```\r\nApparently the weights weren't loaded properly, and no warning or error was raised.\r\nI changed it to:\r\n```\r\nkeras_model.load_weights(transform_weight_file, by_name=False, skip_mismatch=False)\r\n```\r\nAnd now the inference values make sense for the QAT model both in probabilities and bounding boxes dimensions.\r\n\r\nI would still like to know if anyone can answer my question in the above commend, regarding the `x_scale` which seems to have the same value of ~0.5039 for all Quant/Dequant layers in my QAT models. ", "Hi Would you please give me more context about how did you apply QAT? (code, how much step you train, etc...)\r\n`x_scale` should changed during the training. (but depends on dataset)\r\n\r\nHi @daverim, do you have any context about TensorRT supports?", "It seems this is happening due to a tf2onnx issue- the `range_given=False` argument is currently ignored:\r\nhttps://github.com/onnx/tensorflow-onnx/issues/1438#issuecomment-814516723\r\nSo the settings in Tensorflow were OK.", "Seems that this is the issue -- looks like the scale is calculated from `input_min = -64, input_max = 64` (64/127 = .503937). Curious if the rest of this workflow is working correctly?", "The rest of the workflow is working correctly :) Only the scale was calculated in an wanted way"]}, {"number": 48210, "title": "[XLA PjRT] Ongoing progress and future plan of PjRT distributed runtime", "body": "Hi TF developers, \r\n\r\nThis issue intends to get more knowledge about the ongoing and future plan of XLA PjRT, especially on the distributed runtime of GPU.\r\n\r\nAlthough there are some related commits (e.g., 44e771aa50ad8be454b7f4469ad04a815d03cca1 and 8a72c4466a0f44de9bd1cfbf47a701727731036c), existing PjRT code seems quite preliminary in supporting distributed training on multiple hosts -- As far as I could see, it now only provides basic control operations such as setting up the connections and sharing GPU topology, and it is not trivial to run a real distributed demo with communication.  Another issue https://github.com/google/jax/issues/2731 seems also aligns with this point. \r\n\r\nSo I would like to confirm if TF team is also working on this and may probably release some new features soon. For now, our   interest is on using PjRT to run HLO with collective or P2P communication, possibly including all-reduce, all-gather, reduce-scatter, send/recv, etc.\r\n\r\nAny response will be appreciated. Thanks. ", "comments": []}, {"number": 48205, "title": "TF2 model output NaN when inferenced is performed using CPU", "body": "I have a TensorFlow model architecture and pre-trained weights. When I use it for prediction on GPU it works well, but on CPU it starts giving NaNs. The model weights are in hdf5 format. There is no batch normalization layer in the model. I am confused why it's not working on the CPU.  \r\nTensorflow version:2.30.  \r\nTested on Windows and Linux (same issue on both). A similar error is reported in #35307 . But in TensorFlow version 2, i am not sure how to use clear_device as mentioned in [the comment of this issue](https://github.com/tensorflow/tensorflow/issues/35307#issuecomment-568136811). The model I am using is available [here  ](https://github.com/xliucs/MTTS-CAN/blob/5db0928c7474bc2206d6132cd79b09aadc619897/code/model.py#L203)\r\n", "comments": ["@talhaanwarch,\r\nThe `model.py` file you've linked has only the class and function definitions. I did not get any output on running the code. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/045a3d9ba253e11ef575bfd7ebd77a78/48205.ipynb). \r\n\r\nIn order to reproduce the issue reported here, could you please provide a minimal code snippet and the dataset you are using. Thanks!", "Please check this\r\nhttps://colab.research.google.com/drive/16RbTjME6LcGjG7uSOSr5f8KQMXBH65WZ?usp=sharing\r\n\r\nRun it with CPU and with GPU", "@amahendrakar  is there any update regarding this", "@jvishnuvardhan,\r\nI was able to reproduce the issue with TF v2.3 and TF v2.4. Whereas Colab doesn't detect GPU with TF v2.5.0rc0 and TF-nightly.\r\n\r\nCode runs find on [GPU](https://colab.research.google.com/gist/amahendrakar/cac92aa388ec1b744ee804187ef29551/48205-gpu.ipynb), but gives NaN predictions with [CPU](https://colab.research.google.com/gist/amahendrakar/079fc1a475a8411a1ebe8aa79a883f4c/48205-cpu.ipynb). Please check the linked gist for reference. Thanks!", "Was able to reproduce the issue in TF 2.5 stable version. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/6f0d1aec8acb3c1938c7f0214ecf20bc/48205-cpu.ipynb).Thanks!"]}, {"number": 48204, "title": "TF2 equivalent of tf.contrib.graph_editor ", "body": "Hi Team,\r\n\r\nIn TF1 we have tf.contrib.graph_editor to conveniently edit a trained model. I couldn't find the equivalent in TF2. Editing a trained model is obviously a better design since in many cases separating models into different components (e.g., embedding generation) is a post-training request. It does not make sense to use tf.function to generate different signatures and train the model again.\r\n\r\nFor TF model devs that provide training libraries for users, this is especially painful since different users have different request for model separation. Separating models on users' custom needs in post training stage is a more scalable solution than the current tf.function solution.\r\n\r\nCan you add an equivalent graph editor in TF2?\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.4\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe feature should allow users to separate the model graph and change the input and output of each subgraphs.\r\n\r\n**Will this change the current api? How?**\r\nAdd a new api support for graph editting\r\n\r\n**Who will benefit with this feature?**\r\nAll TF users bothered with changing model signature with a trained model\r\n\r\n**Any Other info.**\r\n", "comments": ["Hi @rmlarsen , can you help answer the question?", "@Saduf2019 @ymodak Can you help look at this request? Thanks", "Hello, is there any update about this issue."]}, {"number": 48195, "title": "Recurrent layers fail on (at least some) Linuxes where use_bias=True, succeeds on Macos", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 (AWS Deep Learning AMI)\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3, 2.4.1, several 2.5 and 2.6-nightly samples\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 11\r\n- GPU model and memory: NVIDIA Corporation GV100GL [Tesla V100 SXM2 16GB]; BUT also occurs on CPU wheels\r\n\r\nCreating a subclass model with a~~n LSTM~~ recurrent layer when `use_bias=True` (i.e. the default) fails. The code can be found in the following gist:\r\n\r\nhttps://gist.github.com/zoxoi/9c331f9df1fa22112e061638ea200ec8\r\n\r\nNote that the model\r\n\r\n```python\r\nclass FailModel(Model):\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n\r\n        self.lstm = layers.LSTM(64, use_bias=True)\r\n\r\n    def call(self, input, training=False):\r\n        return self.lstm(input)\r\n```\r\n\r\nis the only important bit, as far as I know. The rest is a minor bit of supporting code just to trigger the issue such as compiling the model and creating a dummy dataset generator. The full traceback is in the included gist, to save space, but the ultimate error given is:\r\n\r\n```    \r\nValueError: Shape must be at least rank 3 but is rank 2 for '{{node BiasAdd}} = BiasAdd[T=DT_FLOAT, data_format=\"NCHW\"](add, bias)' with input shapes: [?,256], [256].\r\n```\r\n\r\nA few facts I've noticed:\r\n- It doesn't seem to fail when *calling* the model, this behavior is limited to things such as `model.fit()`. Explicitly calling `model(tf.zeros(1, 96, 100))` works fine from my testing.\r\n- The value given in the error `ValueError: Shape must be at least rank 3 but is rank 2 for '{{node BiasAdd}} = BiasAdd[T=DT_FLOAT, data_format=\"NCHW\"](add, bias)' with input shapes: [?,256], [256].` seems to always be 4x the number of cells (e.g. for 64 it's 250, 100 it's 400 etc). I don't know if this is surprising or not.\r\n- Since it may be related to NCHW format, I tried changing the `time_major` argument to true, and both leaving the data the same and (more correctly) manually permuting/transposing the input into time major order, no success.\r\n- This bug happens on multiple TF versions, 2.3 was the earliest I can test but persists up to the most recent 2.6 nightly.\r\n- This happens even if you force TF to not discover a GPU, or install tf-cpu instead.\r\n- However, this does NOT happen on my Macos Catalina 10.15.7 laptop, with an integrated GPU (i.e. tf-cpu is automatically installed), the layer works as intended with no errors.\r\n\r\nFor anybody else dealing with this: a workaround is to set **use_bias=False** when creating the LSTM. This has tradeoffs however, (beyond just the potential mathematical issues with using a bias allowing some correction), since it won't use the cuDNN kernel if use_bias is off which from my testing seems to have a fairly radical speed penalty (it's training more slowly than it is on my laptop CPU by about 40ms/step). \r\n\r\nIf you want to test on the **exact** same setup I'm using, I'm experiencing the issue on the AWS Deep Learning AMI Version 42.1.\r\n\r\nFurther important things I haven't tested:\r\n- ~~If this is limited to LSTM or affects other recurrent layers.~~ [Update: It affects GRU and SimpleRNN as well, updating title]\r\n- If this affects Windows in either configuration\r\n- ~~If, even though it fails on CPU, it's perhaps related to CUDA 11 specifically somehow~~ [Also failed on a GPU-less Linux AMI]\r\n- If this affects the functional API as well, I've only used subclass models", "comments": ["@zoxoi \r\nI ran the code shared on tf 2.4 and face a different error,Please find the [gist here](https://colab.research.google.com/gist/Saduf2019/b1933a7374348ca6034590589a4e5212/untitled575.ipynb).\r\nCould you please share a colab gist with the issue reported.", "I don't know what a collab gist is. I put the minimal code necessary to run the error in the gist I linked. It needs some supporting code, the code I pasted in the issue box was just me pointing out the relevant code the error is occurring in, I didn't post the whole thing to avoid taking up space.\r\n\r\nE: It's three files, two if you don't use the environment.yml to set up the conda environment. You could easily merge two of them into one by copy+pasting if desired, I only put them in two to make it easier to see where the error was.", "@zoxoi \r\nI ran the code but still face a differet error, Please find the [gist here](https://colab.research.google.com/gist/Saduf2019/46f92b2166217c75e27cc97a11ca15f8/untitled584.ipynb)\r\n\r\nAs per the error reported, can you refer to these links and let us know if it helps:[link](https://stackoverflow.com/questions/50162787/valueerror-shape-must-be-rank-2-but-is-rank-3-for-matmul)", "I fixed the code in your link by inlining the code from the micro-module:\r\nhttps://colab.research.google.com/drive/13YOXgRIjQ-QtKCXP8EcfUJGer8vCER03#scrollTo=R4gBFl1dMz1-&line=10&uniqifier=1\r\n\r\nIt runs successfully on the colab gist, but it does NOT run on what appears to be x86_64 Ubuntu 18.04, and possibly other environments. I'm not surprised it runs on whatever the Google Compute backend is, since TF is developed and maintained by Google that's expected, but the error triggers under different architectures.\r\n\r\nI searched numerous SO answers and such when researching this to make sure there isn't another problem. Including the one you linked. The issue here is that on some machines, as you can see, the code runs successfully, as it should since the shape is correct, but it doesn't run on certain architectures. I've tried with two different Ubuntu 18.04 AWS instances now (one with a GPU and one without) and both failed. I could also try running it locally via a Linux live installation if that's important, but it appears that to reproduce someone on the team will need to do it on an environment other than that gist.\r\n\r\nIf you want full reproduction steps:\r\n1. Start with a clean Ubuntu 18.04 installation on an x86_64 architecture, I'm unsure if AMD or Intel matters; presence of a GPU and CUDA does NOT seem to matter\r\n2. Install any core dependencies necessary not present on that installation (e.g. `build-essentials`, etc)\r\n3. Install either conda or miniconda and create and activate an environment via the `environment.yml` above, or else install Python 3 via your preferred method (for full reproduction: I was using 3.6 because that's what we're pinned to due to other dependencies) and manually install Tensorflow 2.3+\r\n4. Either copy that colab gist into one file, or download the two `.py` files in the gist in the OP and place them in a folder\r\n5. Run the file containing the model definition via `python3 ./main.py` (or whatever you named it)\r\n\r\nI can also attempt to see if it can be reproduced in a Docker container if that would be easier.", "For those looking for a workaround this worked for me when trying to load a serialized model.\r\n`tf.keras.backend.set_image_data_format(\"channels_last\")`", "@zoxoi We see that you are using older version of tensorflow. Many bugs have been fixed in latest version. We recommend that you upgrade to latest stable version of tensorflow 2.6.0 and let us know if the issue still persists in newer versions .Thanks!", "I'm not going to be able to test this for a bit, but it's on my to-do list. Just to stave off the inactivity autoclose."]}, {"number": 48178, "title": "Dataset random state not saved in checkpoint when reshuffling each iteration", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.9.2\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: NVIDIA Titan V\r\n\r\n**Describe the current behavior**\r\nI am creating a dataset with\r\n``` python\r\ndataset = tf.data.Dataset.from_tensor_slices((np.arange(5)))\r\ndataset.shuffle(5, reshuffle_each_iteration=True)\r\ndataset = dataset.batch(5)\r\n...\r\nfor epoch in range(1000):\r\n  iterator = iter(dataset)\r\n```\r\nI am saving this dataset as part of a checkpoint after every epoch. When I stop training, load the checkpoint, and create an iterator for a new epoch from the dataset, the batch is not the same as if I had kept training. Instead, it uses the global random seed, so the iterator is identical to the iterator from the very first epoch.\r\n\r\n**Describe the expected behavior**\r\nThe dataset random state should be held over. That is, when an iterator is created from this dataset, it should be identical to if I had never stopped training.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://gist.github.com/keyonvafa/2120c836a71dd783f7da0e73116681b6\r\n\r\nAt the first iteration, this prints: tf.Tensor([1 3 2 4 0], shape=(5,), dtype=int64). When we stop training and restore a checkpoint, it will go back and print the same thing. Removing the global random seed would fix this but the code would no longer be reproducible between runs.\r\n\r\nThank you!\r\n", "comments": ["@keyonvafa,\r\nWhen trying to reproduce your code, I've encountered with the error:\r\n\r\n```\r\nFATAL Flags parsing error: Unknown command line flag 'f'\r\nPass --helpshort or --helpfull to see help on flags.\r\nAn exception has occurred, use %tb to see the full traceback.\r\n\r\nSystemExit: 1\r\n```\r\n Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/ab554b889210447eeca8dbbc075d41d4/gh_48178.ipynb). Can you please help us reproduce the error? Thanks!", "Thank you for looking into it. It looks like `absl` doesn't work well with Colab, so I have removed it and can reproduce the issue:\r\n\r\nhttps://colab.research.google.com/gist/keyonvafa/cc3927fb7d9a0ea955e34219f605d4b8/gh_48178.ipynb", "Was able to replicate the issue with TF 2.6.0-dev20210603,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/08f26bc2483ca5cdd6e7c98fbf1bfb3c/untitled210.ipynb#scrollTo=HMKx35b6dVIJ)..Thanks !"]}]