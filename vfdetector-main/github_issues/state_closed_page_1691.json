[{"number": 2171, "title": "TF-Learn early stopping monitor doesn't disable dropout", "body": "I'm training a custom CNN model that uses dropout for regularization. I'm using tf-learn r0.8 (skflow)  ValidationMonitor as follows.\n\n``` Python\nfrom tensorflow.contrib.learn.python.learn import monitors\nm = monitors.ValidationMonitor(X_validate, Y_validate, 0, print_steps=100, \n    early_stopping_rounds=500)\nregressor.fit(X_train, Y_train, monitor=monitor)\n```\n\nI noticed that after early stopping is triggered, the validation error I calculate on the final model is significantly different from what the ValidationMonitor reports as the best validation error. To narrow the problem I set `early_stopping_rounds = 1` but even then the difference between the \"best\" validation error and the one calculated on the final model was too large. Large enough to make the early stopping feature unusable for my case.\n\nI tracked the issue down to the fact that ValidationMonitor calculates the validation error without disabling dropouts, which is why the results were so different from what you'd get from calling model.predict(). I implemented a fix here. \n\nI'd appreciate it if one of the maintainers reviews it and confirms that it's the right fix. If so, I'll create a pull request. \n\nhttps://github.com/waleedka/tensorflow/commit/ee095f83ffedf2e2cefff126b76191ab691d5097\n", "comments": ["@waleedka I've noticed the same behavior. \ud83d\udc4d  for tracking it down!\n", "Looks like 0.9RC0 (9425f822d8a5dc657022eed5c5142b4bf7b1087a) fixes this issue with the new Estimator API\n", "Closing since @ian-plosker found this is fixed in the 0.9 RC. The code has diverged significantly between the submitted diff and the current tflearn as well. Please reopen if 0.9 does not resolve this for you @waleedka. Thanks!\n"]}, {"number": 2170, "title": "Index into tensor", "body": "I'd like to have a `tf.index` function that works like this:\n\n``` python\nimport tensorflow as tf\n\nweights = tf.constant([[0, 0], [1, 1], [2, 2]])\nindices = tf.constant([0, 2, 1, 2])\n\nindexed = tf.index(weights, indices)\n#indexed = [[0, 0], [2, 2], [1, 1], [2, 2]]\n```\n\nThat is, `indexed[i] = weights[indices[i]]`. Of the existing functions, `tf.slice` comes close, but it doesn't seem to allow for multiple slices. A less efficient way of doing this is to use `tf.one_hot` and a matrix multiply, but this should be straightforward to do.\n", "comments": ["`tf.gather()` works for exactly the case you describe (it's not perfect, but it handles indexing on the 0th dimension):\n\n``` python\n>>> import tensorflow as tf\n>>> \n>>> weights = tf.constant([[0, 0], [1, 1], [2, 2]])\n>>> indices = tf.constant([0, 2, 1, 2])\n>>> indexed = tf.gather(weights, indices)\n>>> sess = tf.Session()\n>>> sess.run(indexed)\narray([[0, 0],\n       [2, 2],\n       [1, 1],\n       [2, 2]], dtype=int32)\n```\n", "I have a solution: please see https://github.com/sicongliu92/DL-code/blob/master/unpool(1).py"]}, {"number": 2168, "title": "Feature Request:  API for the Julia Language.", "body": "", "comments": ["There is this package that wraps the TensorFlow Python APIs using Julia's PyCall.\n\nhttps://github.com/benmoran/TensorFlow.jl\n\nAnother approach could be to wrap the C++ APIs (using Julia's Cxx.jl, which is very easy). I have not looked inside TensorFlow but talking to those who have, I understand that a substantial part of TensorFlow is in Python, and if one were to use the C++ APIs, a bunch of the other stuff in the python part of the codebase would need to be recreated. \n\nCc: @benmoran @ninjin \n", "wow thanx!!!\n", "There is now a reasonably sophisticated TensorFlow Julia package by @malmaud \r\n\r\nhttps://github.com/malmaud/TensorFlow.jl\r\n\r\nCan this be featured on the TensorFlow website?"]}, {"number": 2167, "title": "Dropout in skflow has a problem when save & restore the model because the graph collection is not loaded.", "body": "\u25b6 Operating System: \nubunutu 14.04 LTS\n\n\u25b6 Installed version of CUDA and cuDNN: \nCUDA 7.5.18 / cuDNN 7.0(v4)\nNVIDIA-SMI 352.39     Driver Version: 352.39\nGeForce GTX TITAN X\n\n\u25b6 I installed tensorflow with next command line. And that is a link of tensorflow Github, 'Linux GPU, python 2' whl file.\n\nsudo pip install --upgrade http://ci.tensorflow.org/view/Nightly/job/nigntly-matrix-linux-gpu/TF_BUILD_CONTAINER_TYPE=GPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-working/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.8.0-cp27-none-linux_x86_64.whl\n\n\u25b6 TensorFlow version: 0.8.0\n(outputs of python -c \"import tensorflow; print(tensorflow.**version**)\" )\n\n---\n### Steps to reproduce\n1. Save and Restore a tfmodel modifying [tensorflow/tensorflow/examples/skflow/mnist.py] \n   ![image](https://cloud.githubusercontent.com/assets/4004593/14917371/a93e2882-0e59-11e6-8ff0-8154c0cd318f.png)\n2. And see the accuracy is not deterministic but stochastic after restored.\n   ![image](https://cloud.githubusercontent.com/assets/4004593/14917401/d8e70342-0e59-11e6-86f6-aa778b66ede2.png)\n### What have you tried?\n\nWhen I try to predict MNIST data after save() and restore() the 'tfmodel' with skflow function, the dropout shows stochastic outcomes. And when I remove the dropout, there is no stochasticity. \n\n---\n\nI guess when the model has been restored, the collections of a graph about DROPOUT are may not properly loaded. So some codes on\n![image](https://cloud.githubusercontent.com/assets/4004593/14917208/dcbbfcc6-0e58-11e6-8b4f-8188a99fda8a.png)\n    \u25b6 feed_dict = {prob: 1.0 for prob in dropouts}\nthe function of \"_predict()\" at [tensorflow/contrib/learn/python/learn/estimators/base.py] may dont' work. \n\nIf you run following code after restore tfmodel, then you will see that there is no 'droputs' key.\n    \u25b6 print (classifier._graph.get_all_collection_keys())\n", "comments": ["I'm seeing the same thing. When I remove the dnn I'm not seeing this issue anymore.\n\n@imcomking have you found a workaround for this?\n", "@abronte I tried so many things but I couldn't find any solution. Sorry. This is very critical and fundamental problem. Without fixing the graph collection errors, this can't be solved clearly.\n", "A work around might when you're done training, save the dropouts object to a pickle \n`dropouts = self._graph.get_collection(DROPOUTS)`\n\nThen when predicting, load that pickle into dropouts?\n\nIt looks like a lot of code around this has changed in the master branch, @imcomking have you tried using tensorflow from source to see if this issue still exists?\n", "@abronte No I didn't try yet. I will test the master branch few days later.\n", "Hey guys, indeed we are changing a number of things - this problem should\nbe fixed though.\n\nThat said, currently master doesn't support save / restore for Estimators\nthe way it was done before. We are working on a bit revised functionality\nfor this.\n\nOn Wed, May 25, 2016 at 12:23 AM, Dong-Hyun Kwak notifications@github.com\nwrote:\n\n> @abronte https://github.com/abronte No I didn't try yet. I will test\n> the master branch few days later.\n> \n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2167#issuecomment-221493741\n\n## \n\nBest regards,\nIllia Polosukhin\n", "@ilblackdragon: If this is fixed at master, can you close it? \n", "Automatically closing due to lack of recent activity, please reopen when additional information becomes available.\n"]}, {"number": 2166, "title": "Restored variables have unknown shape?", "body": "When restoring a saved graph, the variables all have `<unknown>` shape. I have searched a lot, and haven't seen this problem anywhere else. It is also stated in the docs that the size of variables is indeed saved, so it is confusing to me why it is not restored?\n\nRelevant part of the code:\n\n```\nsaved = tf.train.import_meta_graph('save.ckpt.meta')\nsaved.restore(sess, 'save.ckpt')\nfor x in tf.trainable_variables():\n  print x.get_shape()\n```\n\nOutput:\n\n```\n<unknown>\n<unknown>\n<unknown>\n<unknown>\n... etc\n```\n", "comments": ["A fix for this has been checked in internally and will be available shortly.\n", "Just wondering if there is any news on this, as I seem to experience this\nissue with version 0.10\ncheers\n\n```\n        sess, meta_graph_def = session_bundle.LoadSessionBundleFromPath(\"/tmp/sess/00000001\") \n        with sess.as_default():\n            collection_def = meta_graph_def.collection_def\n            signatures_any = collection_def[constants.SIGNATURES_KEY].any_list.value\n            signatures = manifest_pb2.Signatures()\n            signatures_any[0].Unpack(signatures)\n            default_signature = signatures.default_signature\n            input_name = default_signature.classification_signature.input.tensor_name\n            output_name = default_signature.classification_signature.scores.tensor_name\n\n            print(\"INP\",sess.graph.get_tensor_by_name( input_name).get_shape())\n\n```\n\n```\ngives: INP <unknown>\n```\n"]}, {"number": 2165, "title": "Convolution for 1D", "body": "Dear community,\n\nDo you know of any implementation for 1 dimensional convolution in TensorFlow?\n\nFor the moment, I circumvented this by using conv2d with a filter of 5x1. This works good and I was able to surpass state-of-the-art performances on some datasets. However, this short-term 'hack' doesn't allow for longer strides. With a 5x1 filter, I cannot make strides of 2. Tensorflow will raise error that 'stride must be less than or equal to filter size'\n\nDo you know of any 1D implementation for tf.nn.conv2d?\nIf not, how can I recommend this to the developing team of TensorFlow? A 1D conv would make TensorFlow useful to the world of time-series classification, which is a plus for TensorFlow as well\n\nThank you in advance!\n\nRob\n", "comments": ["@ry recently added support for strides > filter, so if you use our nightly pip binaries or build from sources you should be able to do what you want!  It will be part of our next official release if you want to just wait.\n", "@RobRomijnders can i ask what you're using 1d conv for?\n", "Thanks @vrv \n\n@ry I'm working on CNN for time series. Therefore i use 1d conv\n\nLike these three projects\n- [Music genre classification with CNN](http://robromijnders.github.io/cnn_music/)\n- [Time-series classification with LSTM's in Tensorflow](http://robromijnders.github.io/LSTM_tsc/)\n- [Time-series classification with CNN's](http://robromijnders.github.io/CNN_tsc/)\n", "are there any docs explaining how to actually use the 1D conv/CNN?\n"]}, {"number": 2164, "title": "Inception needs retraining (Was: Op is deprecated in Image Recognition tutorial)", "body": "When I'm running the Inception examples from the Image Recognition tutorial (https://tensorflow.org/tutorials/image_recognition/), I'm getting a lot of warnings in the following form:\n\nW tensorflow/core/kernels/batch_norm_op.cc:36] Op is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().\n\nI recently upgraded Tensorflow to version 0.8.0, and I'm working on Mac OS X 10.10.2. While this isn't harming the performance, I'm wondering if there's a fix or an update that I'm missing.\n", "comments": ["@vincentvanhoucke: Do you know what the issue is since you deprecated the old batch norm? \n", "@girving someone just needs to retrain the model. @shlens FYI\n", "Oh, right, these are just warnings, and the GraphDef versioning mechanism is working fine in terms of not generating errors.  Yep, it'd be good to retrain the model since the old ones will vanish eventually.\n", "We might be able to get away without retraining by pointing the Image Recognition tutorial to the newer model provided here:\nhttps://github.com/tensorflow/models/blob/master/inception/README.md#getting-started-1\n\nThis would require a little bit of plumbing and identifying the appropriate input layer Tensor name.\n", "Ironically, I did the inverse to make classification work in the deepdream\ntutorial. Now I wish I had gone the other way around.\nOn Fri, Apr 29, 2016 at 18:46 Jon Shlens notifications@github.com wrote:\n\n> We might be able to get away without retraining by pointing the Image\n> Recognition tutorial to the newer model provided here:\n> \n> https://github.com/tensorflow/models/blob/master/inception/README.md#getting-started-1\n> \n> This would require a little bit of plumbing and identifying the\n> appropriate input layer Tensor name.\n> \n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2164#issuecomment-215923062\n", "Is there something I should do at this point, or just simply ignore these warnings?\n", "The warnings are safe to ignore. We have to update the saved graph and checkpoint to not use these deprecated ops. I will leave this issue open to remind us we have to do that.\n", "Got it -- thanks! \n", "@martinwicke: is anyone working on this?\n", "I don't think so. @xmbrst, can you take a look or reassign?\n", "Will take a look.\n\nOn Mon, Jul 25, 2016 at 7:25 PM, Martin Wicke notifications@github.com\nwrote:\n\n> I don't think so. @xmbrst https://github.com/xmbrst, can you take a\n> look or reassign?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2164#issuecomment-235116731,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AO-8AxVDBBCKHJNQYl_ZyasLTHGVAdMJks5qZUXggaJpZM4ISjT5\n> .\n", "@petewarden, @shlens suggested that you might be a good person to update the plumbing in classify_image.py for the newer model at https://github.com/tensorflow/models/blob/master/inception/\n", "@petewarden are you working on this or should we reassign?\n", "I'm not working on it, and I'm not likely to get to it soon, so reassigning makes sense.\n", "Assigning to aselle for triage then\n", "@shlens do you have any other suggestions on who might be a good person to fix this?\n", "This is now actually fixable if you run the tensorflow/python/tools:optimize_for_inference script, since it folds the batch normalization ops.\n", "@suryabhupa Do you want to try that?\n", "Automatically closing due to lack of recent activity. We will reopen when further info becomes available. Thanks!\n", "I'll try this soon and get back to you. \n", "@petewarden @aselle What's the status of this issue? \n", "In v0.12.0 this is a fatal error when using libtensorflow for go: Op BatchNormWithGlobalNormalization is not available in GraphDef version 17. It has been removed in version 9. Use tf.nn.batch_normalization().\r\n\r\nUsing optimize_for_inference allows me to load the model:\r\noptimize_for_inference --input=/tmp/output_graph.pb --output=/tmp/output_graph.pb_optimised --frozen_graph=True --input_names=Mul --output_names=final_result", "Reassigning to @shlens for triaging, since it's becoming more than a cosmetic issue.", "@petewarden What would it take to employ this version of Inception-v3 http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz for the image recognition tutorial?", "tried running: \r\n\r\ntensorflow/python/tools:optimize_for_inference script, \r\n\r\ngot: \r\nTraceback (most recent call last):\r\n  File \"optimize_for_inference.py\", line 66, in <module>\r\n    from tensorflow.python.framework import graph_io\r\n", "Can you sync and try again? I think it was a transient error.", "In case anyone else runs into this, I ended up optimizing Inception using the following command:\r\n\r\n```\r\npython tensorflow/python/tools/optimize_for_inference.py --input /tmp/inception/classify_image_graph_def.pb --output /tmp/inception/classify_image_graph_def_optimized.pb --input_names=DecodeJpeg/contents --output_names=pool_3/_reshape --frozen_graph=True --placeholder_type_enum 7\r\n```", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 2163, "title": "Floating point exception when computing gradients", "body": "Hi tensorflow developer team! \n\nI just wanted to first let you know that I really appreciate all the work you are putting into this amazing open source system. It has been a terrific system for me so far. However, I have come into a roadblock when using the tf.gather function in gradients.\n\nI am using\n### Environment info\n\nOperating System: CentOS\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\nCUDA version 7.5\ncuDNN version 7.0 (64 bit)\ntensorflow/0.8.0-gpu version\n### Steps to reproduce\n\nI obtain a floating point exception when I run the following code\n\n`A_ph = tf.placeholder(tf.float32, shape=(None, 2))`\n`ind_ph = tf.placeholder(tf.int32, shape=(None, 2))`\n\n`gather = tf.gather(A_ph, ind_ph)`\n`redsum = tf.reduce_sum(gather, 1)`\n`l2 = tf.reduce_sum(redsum)`\n\n`A = np.array([[0,0],[0,0],[0,0]])`\n`ind = np.zeros((0,2))`\n\n`grad_op = tf.gradients(l2, A_ph)`\n\n`out = sess.run(grad_op, feed_dict={A_ph:A, ind_ph:ind})`\n### What have you tried?\n\nIf you replace ind with a non-empty array, such as [[0,1]] it works fine. \n\nI suspect that the output of redsum = [], which makes l2 disconnected from A_ph for the given ind. Fed forward to computing l2, I get the correct value of 0.0 (since there is nothing to be summed). However, for backpropagation of gradients, I think the output of redsum=[] chokes the algorithm, and causes it to evaluate some illegal operation using an empty tensor, instead of just backpropagating 0.0. \n\nAre there any solutions to this problem currently? In my use of this computation, I won't know ahead of time whether or not the intermediate computations produce [](empty tensor), but I also want to be able to get the correct gradients when this happens. Thank you so much for your consideration! \n", "comments": ["Thanks for the bug report.\n\n@girving tf.gradiensts() is generating an unguarded div by 0.  Internal bug opened.\n", "Thanks for reporting!  We're going to fix TensorFlow to produce nice exceptions for integer zero divisions.\n", "Ah, there are two bugs here: one bug that we crash the process on integer division by zero, and another that we generate a division by zero.\n", "Yes, I think there should never be any division by zero in the above operations, at least from what I can tell from the backpropagation formulas. I think I should definitely be able to get a None gradient to indicate that that l2 and A_ph are disconnected, rather than an exception. Is the bug a complex one to fix?\n", "@altaetran: It's conceptually straightforward: one has to go through and sanitize all the occurrences of `//` in `math_grad.py`.  Unfortunately there are a bunch of them for a variety of different ops, and the fixes look different in the different cases.\n\nHowever, there's no way to get a `None` gradient, since the variables _are_ connected, just through an empty tensor bottleneck.  Since we don't necessarily know that the shapes are empty until runtime, we can't produce `None`. \n", "@altaetran: To make my initial comment make sense: we want to produce nice error messages even for malformed ops, so we want to fix the underlying divisions by zero regardless of whether we fix the gradient code. \n", "Oh I see, so those would be two separate tasks. Is there a reason for why an empty bottle neck tensor in the gradients produces division by zero though? Or is that something to be changed as well? \n", "As I said: the fixes are straightforward but there are a number of them.  Here is an example of a bad line: https://github.com/tensorflow/tensorflow/blob/5e22e3a3187e6729488f4a6da59b4cfbedc40946/tensorflow/python/ops/math_grad.py#L36\n", "Oh I see ok. So then this is likely something to occur in a future patch? \n", "I'm looking at it now, so not too long except for the upcoming weekend. :)\n\nI'll probably fix the gradients first since making integer division safe is slightly more awkward (to do in a reasonably performant manner).\n", "CL submitted, so it should be on Github within 24 hours.\n", "To clarify: I fixed the gradients not to divide by zero, which should solve your issue.  Integer divide by zero on its own will still crash the process.\n", "Great! Been running into a similar problem myself. As a question, is there a local version of the patch I could use? I'm running Tensorflow 0.8.0 on a cluster, and the sysadmins won't rebuild the cluster version before TF 0.9.0 (or the next stable version)\n", "@rbharath: Do you want source or binaries?  The source version should have the fix tomorrow.  Not sure when the nightly appears (tomorrow or the next day), but the links for those are here: https://github.com/tensorflow/tensorflow/blob/master/README.md\n", "@girving Sorry, should have been clearer. Is the fix python only? If so, would I be able to just use the corrected python module instead of the standard module without rebuilding the rest of the source? (The issue is that our cluster runs with an old CentOs with an old glibc, so the sysadmins have to do a ton of work to get tensorflow to build, so it's infeasible for us to use binaries or rebuild except at stable releases).\n", "@rbharath: Ah, yes: the fix I just made is Python only.  However, we don't have any setup for using different Python alongside C++, and in any case you'd have to carefully cherry-pick just my change on top of 0.8.0 to have a hope of succeeding.\n"]}, {"number": 2162, "title": "Fix variable names on confusion matrix metric", "body": "Considering that tensorflow metrics documentation states that every metric receives the parameters predictions and labels, this merge request fix the name of one of the confusion matrix variable, which is named \"targets\" and now is named \"labels\".\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n"]}, {"number": 2161, "title": "make data_feeder for sparse matrices", "body": "### Environment info\n\nOperating System: OS X El Capitan 10.11.4\n\nInstalled version of CUDA and cuDNN: N/A\n1. pip install: https://storage.googleapis.com/tensorflow/mac/tensorflow-0.8.0-py2-none-any.whl\n2. version: 0.8.0\n### Steps to reproduce\n\nRun this code with svm light data:\n\n``` python\nfrom sklearn import datasets, metrics\nfrom tensorflow.contrib import skflow\n\ntrain_data, train_labels = datasets.load_svmlight_file(TRAIN)\n\nclassifier = skflow.TensorFlowDNNClassifier(hidden_units=[10, 20, 10], n_classes=2)\nclassifier.fit(train_data, train_labels)\nscore = metrics.accuracy_score(train_labels, classifier.predict(train_data))\n```\n### What have you tried?\n\nI think the classifier doesn't handle the CSR matrix matrix produced by the loader\n### Logs or other output that would be helpful\n\n```\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-27-01a3edd34655> in <module>()\n      1 classifier = skflow.TensorFlowDNNClassifier(hidden_units=[10, 20, 10], n_classes=2)\n----> 2 classifier.fit(train_data, train_labels)\n      3 score = metrics.accuracy_score(train_labels, classifier.predict(train_data))\n\n/Users/nicolas/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/base.pyc in fit(self, X, y, monitor, logdir)\n    216         self._data_feeder = setup_train_data_feeder(X, y,\n    217                                                     self.n_classes,\n--> 218                                                     self.batch_size)\n    219 \n    220         if monitor is None:\n\n/Users/nicolas/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/io/data_feeder.pyc in setup_train_data_feeder(X, y, n_classes, batch_size)\n     97                              \"streaming learning to work.\")\n     98         data_feeder_cls = StreamingDataFeeder\n---> 99     return data_feeder_cls(X, y, n_classes, batch_size)\n    100 \n    101 \n\n/Users/nicolas/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/io/data_feeder.pyc in __init__(self, X, y, n_classes, batch_size, random_state)\n    189         x_dtype = np.int64 if X.dtype == np.int64 else np.float32\n    190         y_dtype = np.int64 if n_classes > 1 else np.float32\n--> 191         self.X = check_array(X, dtype=x_dtype)\n    192         self.y = (None if y is None else check_array(y, dtype=y_dtype))\n    193         self.n_classes = n_classes\n\n/Users/nicolas/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/io/data_feeder.pyc in check_array(array, dtype)\n    159         Original array or converted.\n    160     \"\"\"\n--> 161     array = np.array(array, dtype=dtype, order=None, copy=False)\n    162     return array\n    163 \n\nValueError: setting an array element with a sequence.\n```\n", "comments": ["Yes, currently it only support dense numpy matrix / pandas DataFrame and Series.\nIf you are interested in adding a data feeder for sparse matricies - that would be great contribution!\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 2160, "title": "build doc enhancement", "body": "build doc enhancements, described in https://github.com/tensorflow/tensorflow/issues/2083. @vrv , this is to replace #2113 , solved conflicts.\n", "comments": ["Can one of the admins verify this patch?\n", "Docs only change, merging.\n"]}, {"number": 2159, "title": "unchanging minibatches from skflow DataFeeder for unsupervised tasks", "body": "I'm pretty sure there's a bug [here](https://github.com/tensorflow/tensorflow/blob/ec0552f1bc0e7c6ec6bd84ea1c3c92ff046d6d3c/tensorflow/contrib/learn/python/learn/io/data_feeder.py#L272) in the DataFeeder class for skflow (tf.contrib.learn).  If there are no labels, then the mapping of input placeholders to inputs is returned without updating the minibatch offsets.  [The code for updating offsets and shuffling](https://github.com/tensorflow/tensorflow/blob/ec0552f1bc0e7c6ec6bd84ea1c3c92ff046d6d3c/tensorflow/contrib/learn/python/learn/io/data_feeder.py#L289) won't be reached.\n", "comments": ["Agreed that this looks like a bug. Thanks for the find! \n\nFeel free to submit a pull request (I can also submit a fix :)\n", "You're welcome.  I just happened to come across it when looking at how the data feeders work.  I don't think I'll have bandwidth for a PR in the near future, but I thought it'd be helpful to make note of it here.\n", "Thanks again! I'll submit the fix :)\n", "@cssndrx: Did this fix make it into the master branch?\n", "@mrry I think so. Looks like you are adding that commit internally. https://github.com/tensorflow/tensorflow/commit/2c3c49bb6a1a2f3a9ebcf0b9f48cf8988197f718\n", "Great, thanks!\n"]}, {"number": 2158, "title": "Division by num_steps when calculating language model perplexity", "body": "Hi all,\n\nI think I found something on the language modeling example that might be a bug: \n\nAs pointed in the tutorial, the perplexity can obtained from the averaged cross entropy. The code show [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py#L260) and [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py#L263) the division of the costs by `num_iters`.\n\nNevertheless, the seq2seq.sequence_loss_by_example is averaging losses across timesteps [(here)](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L871). As no value is passed to the parameter `average_across_timesteps` I'm assuming it is using the default value `True`.\n\nAfter that, the model is further averaging the losses by the batch size [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py#L133).\n\nIf I'm not wrong (and please correct me if I am), there is no need to divide the costs by `num_iters` as we already averaged over timesteps and the batch. There is no reference in the tutorial regarding why one should do that. If it is something that we should do it would be nice to include an instruction in the tutorial or a comment in the code. In addition, the Seq2seq tutorial is also showing the perplexity calculation and does not have any further division by the number of steps representing the length of the sequence, but by the number of steps between two verbosity points. \n\nAlso, imo, if the intention is averaging, the `step` parameter [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py#L249)  is the one that should be added to `iters`.\n", "comments": ["in `seq2seq.sequence_loss_by_example` the averaging happens across timesteps but only if the timesteps are specified as a list of elements.\n\nIn PTB case we concatenate all the steps before calling that function and `log_perps` ([batch_size \\* num_steps]) is divided by tf.ones([batch_size \\* num_steps]) which is esentially a no-op.\n\nInstead of calling sequence_loss_by_example we could directly call sparse_cross_entropy_with_logits because that's the only thing of interest here. In the past, that function was more complicated and we didn't want to copy its logic into ptb_word_lm.py. Feel free to send a PR.\n\nHope that makes sense!\n", "Hi @rafaljozefowicz, thanks for the reply.\n\nIndeed it makes sense, since I have not realized the weights are passed as one big vector and not as in the seq2seq examples.\n\nI'm also glad in contributing with a pull request although I have a question that might either my job easier or not:\n\nshould we just include a parameters pass `average_across_timesteps=True` to the function or change it to compute the `sparse_cross_entropy_with_logits` directly?\n\nI think the second option would make it more clear for the reader used to cross entropy and perplexities but would also create the need to change the tutorial.\n", "@giancds: Sorry for the delay; Rafal just left Google.  @ludimagister: Could you answer @giancds's question?  \n", "Hi @giancds,\n\nwhile your proposed change probably would make sense in a perfect world, possibly just this thread is as informative to understand what's going on? If you want you can send a PR but I am honestly not sure it is worth it given the info in this thread...\n", "Hey @ludimagister, thanks for the reply and sorry for the delay. \n\nSome things came up and I was away of all this discussions. I think you are right, does not worth the effort given what we have in this thread. Sorry for the delay but I think we can close the the issue OK for you.\n\nThanks again.\n"]}, {"number": 2157, "title": "bug fix in ApplyAdadelta update rule", "body": "Following the Zeiler ADADELTA paper (http://arxiv.org/pdf/1212.5701v1.pdf), numerator of update equation should be RMS(accum_update) \\* grad. The code previously did not perform the RMS operation.\n", "comments": ["Can one of the admins verify this patch?\n", "does this change cause any tests to fail?  if not, shouldn't it have? :)\n\ncc @Mistobaan \n", "Is definitely missing the RMS part. The test pass because is missing the RMS there too. \n", "I can also fix the test in this PR if you would like?\n", "Yes please!\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "@vrv can you point me to where the faulty test is? I looked in [adadelta_test.py](https://github.com/tensorflow/tensorflow/blob/16d395e5dea687ab3aece0a462e631de25c8d77d/tensorflow/python/training/adadelta_test.py), but the update rule there looks correct. Also, I rebased my pull request to be up to date, which may have messed up the CLA process. I'm not sure how to proceed there...\n", "Unfortunately because you rebased I've lost the history of the tests, and also you probably rebased in the incorrect way, since a bunch of other commits are showing up in this PR.  Try restarting the PR from scratch?\n", "Yeah, I agree. I will do that. Thank you!\n"]}, {"number": 2156, "title": "internal to github 2016/4/28", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "test this please\n", "Wahh @caisq what happened\n", "Oh @poxvoculi introduced a problem during the merge conflict, nevermind.\n", "@tensorflow-jenkins test this please\n", "Can one of the admins verify this patch?\n"]}, {"number": 2155, "title": "How to query the amount of memory needed for a model?", "body": "", "comments": ["No good way really, besides running and seeing if you run out of memory.\nYou may get more help on stackoverflow, we are trying to keep issues for bugs:\n\nrelated SO questions\nhttp://stackoverflow.com/questions/36514994/how-to-estimate-the-amount-of-memory-needed-for-rnn\nhttp://stackoverflow.com/questions/36331419/tensorflow-how-to-measure-how-much-gpu-memory-each-tensor-takes\n", "Thanks for your reply. I think this would be a useful feature. Maybe keep this open as a feature request?\n", "have you seen https://github.com/tensorflow/tensorflow/blob/fe454464681b036ff7fed3e42c6bb541fa52dd7c/tensorflow/python/tools/graph_metrics.py ?  It might not provide everything you want, and 'how much memory is needed' is a little hard to quantify, because you can trade off speed for memory.\n", "Oh, didn't know about graph_metrics....from first glance that seems to only calculate memory needed for parameters and not activations\n", "@vrv Thank you, this is what I was looking for.\n", "What superseded `graph_metrics.py`?", "@carlthome - https://github.com/tensorflow/tensorflow/issues/6983#issuecomment-274165889 says `tfprof` has superseded `graph_metrics.py`.", "Thanks @kaczmarj!"]}, {"number": 2154, "title": "Import error: TypeError: <method 'max' of 'numpy.ndarray' objects> is not a Python function", "body": "Having a import error on Ubuntu 14.04 LTS 64 bit with python 2.7, and trying to import tensorflow 0.8.0\n\nIn [1]: import tensorflow\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n\nTypeError                                 Traceback (most recent call last)\n<ipython-input-1-a649b509054f> in <module>()\n----> 1 import tensorflow\n\n/home/weiliu/.local/lib/python2.7/site-packages/tensorflow/**init**.py in <module>()\n     21 from **future** import print_function\n     22 \n---> 23 from tensorflow.python import *\n\n/home/weiliu/.local/lib/python2.7/site-packages/tensorflow/python/**init**.py in <module>()\n     60 from tensorflow.core.util.event_pb2 import *\n     61 # Import things out of contrib\n---> 62 import tensorflow.contrib as contrib\n     63 \n     64 # Framework\n\n/home/weiliu/.local/lib/python2.7/site-packages/tensorflow/contrib/**init**.py in <module>()\n     24 from tensorflow.contrib import framework\n     25 from tensorflow.contrib import layers\n---> 26 from tensorflow.contrib import learn\n     27 from tensorflow.contrib import linear_optimizer\n     28 from tensorflow.contrib import lookup\n\n/home/weiliu/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/**init**.py in <module>()\n     18 from **future** import print_function\n     19 \n---> 20 from tensorflow.contrib.learn.python.learn import *\n\n/home/weiliu/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/**init**.py in <module>()\n     18 from **future** import print_function\n     19 \n---> 20 from tensorflow.contrib.learn.python.learn import *\n\n/home/weiliu/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/**init**.py in <module>()\n     20 import numpy as np\n     21 \n---> 22 from tensorflow.contrib.learn.python.learn.io import *\n     23 from tensorflow.contrib.learn.python.learn.estimators import *\n     24 from tensorflow.contrib.learn.python.learn import ops\n\n/home/weiliu/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/io/**init**.py in <module>()\n     18 \n     19 from tensorflow.contrib.learn.python.learn.io.pandas_io import *\n---> 20 from tensorflow.contrib.learn.python.learn.io.dask_io import *\n\n/home/weiliu/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/io/dask_io.py in <module>()\n     21 \n     22 try:\n---> 23     import dask.dataframe as dd\n     24     allowed_classes = (dd.Series, dd.DataFrame)\n     25     HAS_DASK = True\n\n/home/weiliu/.local/lib/python2.7/site-packages/dask/dataframe/**init**.py in <module>()\n----> 1 from .core import (DataFrame, Series, Index, _Frame, map_partitions,\n      2                    repartition)\n      3 from .io import (read_csv, from_array, from_bcolz, from_array, from_bcolz,\n      4                  from_pandas, from_dask_array, from_castra, read_hdf,\n      5                  from_imperative)\n\n/home/weiliu/.local/lib/python2.7/site-packages/dask/dataframe/core.py in <module>()\n   1232 \n   1233 \n-> 1234 class Index(Series):\n   1235 \n   1236     _partition_type = pd.Index\n\n/home/weiliu/.local/lib/python2.7/site-packages/dask/dataframe/core.py in Index()\n   1264         return self.drop_duplicates().count()\n   1265 \n-> 1266     @derived_from(pd.Index)\n   1267     def max(self):\n   1268         # it doesn't support axis and skipna kwds\n\n/home/weiliu/.local/lib/python2.7/site-packages/dask/utils.pyc in wrapper(method)\n    524 \n    525             method_args = getargspec(method).args\n--> 526             original_args = getargspec(original_method).args\n    527 \n    528             not_supported = [m for m in original_args if m not in method_args]\n\n/home/weiliu/.local/lib/python2.7/site-packages/dask/compatibility.pyc in getargspec(func)\n    188             return _getargspec(func.__init__)\n    189         else:\n--> 190             return _getargspec(func)\n    191 \n    192 def skip(func):\n\n/home/weiliu/.local/lib/python2.7/site-packages/dask/compatibility.pyc in _getargspec(func)\n     54 \n     55     def _getargspec(func):\n---> 56         return inspect.getargspec(func)\n     57 \n     58     if sys.version_info[1] <= 7:\n\n/usr/lib/python2.7/inspect.pyc in getargspec(func)\n    814         func = func.im_func\n    815     if not isfunction(func):\n--> 816         raise TypeError('{!r} is not a Python function'.format(func))\n    817     args, varargs, varkw = getargs(func.func_code)\n    818     return ArgSpec(args, varargs, varkw, func.func_defaults)\n\nTypeError: <method 'max' of 'numpy.ndarray' objects> is not a Python function\n\n========== end of import error =========================\n\nIt looks like a upstream (dask package) error, because I can reproduce it by running: \n\nIn [2]: import dask.dataframe\n\n---\n\nTypeError                                 Traceback (most recent call last)\n<ipython-input-2-8010029fbbe3> in <module>()\n----> 1 import dask.dataframe\n\n/home/weiliu/.local/lib/python2.7/site-packages/dask/dataframe/**init**.py in <module>()\n----> 1 from .core import (DataFrame, Series, Index, _Frame, map_partitions,\n      2                    repartition)\n      3 from .io import (read_csv, from_array, from_bcolz, from_array, from_bcolz,\n      4                  from_pandas, from_dask_array, from_castra, read_hdf,\n      5                  from_imperative)\n\n/home/weiliu/.local/lib/python2.7/site-packages/dask/dataframe/core.py in <module>()\n   1232 \n   1233 \n-> 1234 class Index(Series):\n   1235 \n   1236     _partition_type = pd.Index\n\n/home/weiliu/.local/lib/python2.7/site-packages/dask/dataframe/core.py in Index()\n   1264         return self.drop_duplicates().count()\n   1265 \n-> 1266     @derived_from(pd.Index)\n   1267     def max(self):\n   1268         # it doesn't support axis and skipna kwds\n\n/home/weiliu/.local/lib/python2.7/site-packages/dask/utils.pyc in wrapper(method)\n    524 \n    525             method_args = getargspec(method).args\n--> 526             original_args = getargspec(original_method).args\n    527 \n    528             not_supported = [m for m in original_args if m not in method_args]\n\n/home/weiliu/.local/lib/python2.7/site-packages/dask/compatibility.pyc in getargspec(func)\n    188             return _getargspec(func.__init__)\n    189         else:\n--> 190             return _getargspec(func)\n    191 \n    192 def skip(func):\n\n/home/weiliu/.local/lib/python2.7/site-packages/dask/compatibility.pyc in _getargspec(func)\n     54 \n     55     def _getargspec(func):\n---> 56         return inspect.getargspec(func)\n     57 \n     58     if sys.version_info[1] <= 7:\n\n/usr/lib/python2.7/inspect.pyc in getargspec(func)\n    814         func = func.im_func\n    815     if not isfunction(func):\n--> 816         raise TypeError('{!r} is not a Python function'.format(func))\n    817     args, varargs, varkw = getargs(func.func_code)\n    818     return ArgSpec(args, varargs, varkw, func.func_defaults)\n\nTypeError: <method 'max' of 'numpy.ndarray' objects> is not a Python function\n======== end of import dask ==========\nThe dask verison is 0.8.2 from 'pip freeze'. \n\nI can provide more information if needed. \n", "comments": ["After upgrading pandas, I fixed the error. \n"]}, {"number": 2153, "title": "Optimisations do not seem to be active during building", "body": "When building Tensorflow with Bazel master following the instructions in #2109 , I get many warnings like:\n\n```\nIn file included from third_party/gpus/cuda/include/host_config.h:161:0,\n                 from third_party/gpus/cuda/include/cuda_runtime.h:76,\n                 from <command-line>:0:\n/usr/include/features.h:328:4: warning: #warning _FORTIFY_SOURCE requires compiling with optimization (-O) [-Wcpp]\n #  warning _FORTIFY_SOURCE requires compiling with optimization (-O)\n    ^\n```\n\nIn fact, running a simple network on TF built by me and linked against Cudnn 5  is10% slower than the wheel built by Google against the default Cudnn.\n", "comments": ["Did you build with -c opt ?\n", "@vrv Yes. The full command:\n\n```\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n```\n", "Hm, not entirely sure :(\n\n1) Did you build the pip package additionally with  --copt=-mavx ?  I think our pip wheels do that.\n2) Is there a previous commit where this didn't happen, or has this always been happening for you?\n\n@damienmg for help \n", "1) No, but I am running out on the GPU, so AVX wouldn't affect it, right?\nAnyway, the warning seems to indicate that the O2 flag is missing somewhere.\n\n2) Today I managed to build TF for the first time, so I can't tell, but I\ncan try an earlier version. Any suggestions? (It is not completely trivial,\nas I have to tweak with the include paths in the configuration).\n", "AVX can still help since not all operations are on GPU, and some code can still be accelerated by avx.  Worth a shot anyway :)\n\nI think if you have to tweak include paths and all of that jazz, something is really wrong -- most of our users don't have to do that.  Did the instructions to add --spawn-strategy and genrule-strategy=standalone not help?\n", "https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/crosstool/CROSSTOOL#L225 according to the crosstool it should have -O2\n\nWhat does the command line looks like with -s ?\n", "Closing for now due to lack of response to @damienmg's question.  @Dapid: Please comment if you have more information and I'm happy to reopen. \n", "I too am getting these warnings.", "Building the branch r1.0 from source gives me the same warning. I'm working on Gentoo with GCC 4.9.4. \r\n\r\nThe command is: \r\n\r\n`$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package `\r\n\r\nThanks", "I have the same issue\r\nI am using CentOS 7, python3.5, TensorFlow 1.1.0-rc1\r\n\r\nin the prompt of ./configure:\r\n\r\n```\r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3.5\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\nDo you wish to use jemalloc as the malloc implementation? [Y/n] Y\r\njemalloc enabled\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] n\r\n```\r\nexplicitly typing \" `-march=navie -O2`\" or \"`-O2`\" in the line `Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: `\"doesn't help. \r\n\r\nto compile I used:\r\n\r\n`bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nI tried as early as tensorflow version 1.0.0 and I always get the \"warning _FORTIFY_SOURCE requires compiling with optimization (-O)\". Tensorflow version 0.12.1 doesn't seem to have this problem for me. \r\n", "I also have this issue on CentOS 7, python2.7.5, and TensorFlow r1.1.  If I run with -s I can see the compiler wrapper is getting passed -O2 -march=native, so maybe this is spurious?", "The warning seems to come out when compile CUDA code. How to pass in NVCC flags?", "Any updates on this?", "According to @reverendbedford TensorFlow is actually getting optimized, so it seems like a CentOS toolchain but that I don't think we have control over (and CentOS is not an officially supported TensorFlow platform).", "@girving , I'm actually running Arch and I see the same problem, even when I build it myself.\r\nHere's my build process:\r\n```bash\r\n  export PYTHON_BIN_PATH=/usr/bin/python\r\n  export USE_DEFAULT_PYTHON_LIB_PATH=1\r\n  export CC_OPT_FLAGS=\"-march=native -O3\"\r\n  export TF_NEED_JEMALLOC=1\r\n  export TF_NEED_GCP=0\r\n  export TF_NEED_HDFS=0\r\n  export TF_ENABLE_XLA=1\r\n  export TF_NEED_VERBS=0\r\n  export TF_NEED_OPENCL=0\r\n  export TF_NEED_CUDA=1\r\n  export GCC_HOST_COMPILER_PATH=/usr/bin/gcc-5\r\n  export CUDA_TOOLKIT_PATH=/opt/cuda\r\n  export TF_CUDA_VERSION=$($CUDA_TOOLKIT_PATH/bin/nvcc --version | sed -n 's/^.*release \\(.*\\),.*/\\1/p')\r\n  export CUDNN_INSTALL_PATH=/opt/cuda\r\n  export TF_CUDNN_VERSION=$(sed -n 's/^#define CUDNN_MAJOR\\s*\\(.*\\).*/\\1/p' $CUDNN_INSTALL_PATH/include/cudnn.h)\r\n  export TF_CUDA_COMPUTE_CAPABILITIES=5.2\r\n  \r\n./configure\r\n  bazel build --config=opt --config=cuda //tensorflow:libtensorflow.so //tensorflow/tools/pip_package:build_pip_package\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package ${srcdir}/tmp\r\n```\r\nCould you tell me what flag am I missing?\r\nCheers,\r\n\r\nNick", "@XapaJIaMnu Can you add `-s` and see if it actually is getting optimized?", "@girving, nvm, my bad. I had wrongly installed the wrong packages. The warnings are gone if I use my locally compiled once with \"-march=native -O3\"\r\n\r\nThanks,\r\n\r\nNick"]}, {"number": 2152, "title": "fix scaler transform in boston skflow example", "body": "fit_transform was being used instead of transform for the test set in the boston example\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "My company (Civis Analytics) signed the CLA a while back.\n", "@ilblackdragon Should transform or fit_transform be preferred here?\n", "@willnorris if users sign the corp CLA, is there any way to get the label to be set to yes?\n", "The commit in this PR shows `mheilman@users.noreply.github.com` as the commit author.  You would need to use the email address that is associated with the corporate CLA in order to be recognized (presumably an `@civisanalytics.com` email?)\n\nsee also: https://cla.developers.google.com/about#add-contributors\n", "@poxvoculi transform should be used usually - as in serving you don't actually know what would be mean / std.dev of the distribution.\n", "@tensorflow-jenkins test this please.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Gah, sorry, I forgot that GH would use that generated email address since I just made a simple fix in the in-browser interface.  The email is correct now.\n", "@tensorflow-jenkins test this please\n"]}, {"number": 2151, "title": "corrected a one_hot example, fixed mark down formats", "body": "This PR corrects a `one_hot()` example, additionally fixes mark down formats.\n\ncf. https://www.tensorflow.org/versions/master/api_docs/python/array_ops.html#one_hot\n", "comments": ["Can one of the admins verify this patch?\n", "Our docs are automatically generated from source: can you modify the version here instead: https://github.com/tensorflow/tensorflow/blob/d8e8b872eae63188c75046d5bb068e03a81b3f85/tensorflow/python/ops/array_ops.py#L1760\n", "Now most issues are gone :-p\n", "@tensorflow-jenkins test this please\n\n(our markdown generator I guess isn't perfect in terms of spaces and tabs I guess?)\n"]}, {"number": 2150, "title": "rnn_cell.py improvements", "body": "## Motivation\n\nCurrent implementation of [rnn_cell.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py) does not support custom initialization on a gate and input-to-hidden/hidden-to-hidden level (like setting forgetgate bias to 0 while leaving updategate bias at 1, etc.). Further when debugging the gates in TensorBoard, the matrix is represented as one large matrix, which makes it difficult to see whats happening inside a specific matrix (e.g. hidden-to-hidden) of a specific gate (e.g. forgetgate).\n\nIn recurrent neural networks (RNNs) GRU and LSTM uses various gates with separate weights, for both hidden-to-hidden and input-to-hidden, to computing steps in a recurrent sequence.\nTo optimize computational speed these gates, and their separate weights, are often stacked and computed simultaneous at every step.\nIn TensorFlows rnn_cell.py the [GRUCell](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L123) and [BasicLSTMCell](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L153) are implemented using [linear](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L677) to handle weigths and computation hereof.\nHowever, the implementation of _linear_ does not initialize separate matrices for each gate, but initializes the gates, and their input-to-hidden/hidden-to-hidden matrices, as one big matrix for [weights](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L712) and [bias](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L719).\n## Proposal\n\nImplementing separate initialization of gates, and their input-to-hidden/hidden-to-hidden matrices, and concatenating these gates. This allows custom initializatio, TensorBoard information on hid_in/hid_hid/bias for every gate and still retains the advantage of weights in a large matrix.\n## Implementation\n\nWith minimal rewriting of the current structure in [rnn_cell.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py) I implemented a [lasagne-gate](https://lasagne.readthedocs.org/en/latest/modules/layers/recurrent.html#lasagne.layers.Gate) like structure, made a new _linear_ function and made some minor changes to _GRUCell_.\nAll of these changes should, with minor modifications, work for _BasicLSTM_ as well.\n\nDo notice that code below is for my own purpose, it is not rigorously tested yet.\n\nFirst, the gate to hold initialization for every weight matrix/bias in a gate (notice it also handles LSTM's by having W_cell)\n\n``` python\nclass Gate(object):\n  \"\"\"Gate to handle to handle initialization\"\"\"  \n\n  def __init__(self, W_in=init_ops.random_normal_initializer(stddev=0.1),\n               W_hid=init_ops.random_normal_initializer(stddev=0.1),\n               W_cell=init_ops.random_normal_initializer(stddev=0.1),\n               b=init_ops.constant_initializer(0.),\n               activation=None):\n    self.W_in = W_in\n    self.W_hid = W_hid\n    # Don't store a cell weight vector when cell is None\n    if W_cell is not None:\n        self.W_cell = W_cell\n    if b is not None:\n      self.b = b\n    # For the activation, if None is supplied, use identity\n    if activation is None:\n        self.activation = control_flow_ops.identity\n    else:\n        self.activation = activation\n```\n\nA modified GRU cell to handle weigths (took only minimal modification to **init** and **call**)\n\n``` python\nclass GRUCell(rnn_cell.RNNCell):\n  \"\"\"Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).\"\"\"\n\n  def __init__(self, num_units, input_size=None,\n               resetgate=Gate(W_cell=None, activation=sigmoid),\n               updategate=Gate(W_cell=None, activation=sigmoid),\n               candidategate=Gate(W_cell=None, activation=tanh)):\n    self._num_units = num_units\n    self._input_size = num_units if input_size is None else input_size\n    self._resetgate = resetgate\n    self._updategate = updategate\n    self._candidategate = candidategate\n\n  @property\n  def input_size(self):\n    return self._input_size\n\n  @property\n  def output_size(self):\n    return self._num_units\n\n  @property\n  def state_size(self):\n    return self._num_units\n\n  def __call__(self, inputs, state, scope=None):\n    \"\"\"Gated recurrent unit (GRU) with nunits cells.\"\"\"\n    with vs.variable_scope(scope or type(self).__name__):  # \"GRUCell\"\n      with vs.variable_scope(\"Gates\"):  # Reset gate and update gate.\n        # We start with bias of 1.0 to not reset and not update.\n        r, u = array_ops.split(1, 2, Modified_linear([inputs, state],\n          [(self._num_units, \"Reset\", self._resetgate),\n           (self._num_units, \"Update\", self._updategate)]))\n        r, u = self._resetgate.activation(r), self._updategate.activation(u)\n      with vs.variable_scope(\"Candidate\"):\n        c = Modified_linear([inputs, r * state],\n          (self._num_units, \"Candidate\", self._candidategate))\n        c = self._candidategate.activation(c)\n      new_h = u * state + (1 - u) * c\n    return new_h, new_h\n```\n\nI found _linear_ required the largest amount of rewriting, however I have tried to keep the original structure and functionality intact.\n\n``` python\ndef Modified_linear(args, output, scope=None):\n  \"\"\"Modified linear takes args and output.\n     Args is same as in linear, but output is a tuple consisting of:\n     output_size, name of gate, gate object (with all initializations)\n  \"\"\"\n  if args is None or (isinstance(args, (list, tuple)) and not args):\n    raise ValueError(\"`args` must be specified\")\n  if not isinstance(args, (list, tuple)):\n    args = [args]\n  if not isinstance(output, list):\n    output = [output]\n  shapes = [a.get_shape().as_list() for a in args]\n  for shape in shapes:\n    if len(shape) != 2:\n      raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shapes))\n    if not shape[1]:\n      raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shapes))\n\n  matrices = []\n  biases = []\n  with vs.variable_scope(scope or \"Linear\"):\n    for output_size, name, gate in output: # loops over every gate\n      with vs.variable_scope(name):\n        W_in = vs.get_variable(\"W_in\", [args[0].get_shape()[1], output_size],\n          initializer=gate.W_in)\n        W_hid = vs.get_variable(\"W_hid\", [args[1].get_shape()[1], output_size],\n          initializer=gate.W_hid)\n        if hasattr(gate, 'b'):\n          b = vs.get_variable(\"Bias\", [output_size],\n            initializer=gate.b)\n          biases.append(b)\n        if hasattr(gate, \"W_cell\"):\n          pass\n          # do some LSTM stuff ...\n        else:\n          matrix = array_ops.concat(0, [W_in, W_hid]) # concats all matrices\n        matrices.append(matrix)\n\n  total_matrix = array_ops.concat(1, matrices) # concats across gates\n  res = math_ops.matmul(array_ops.concat(1, args), total_matrix) # computes the results\n\n  if biases is not []:\n    total_bias = array_ops.concat(0, biases) # concats across gates biases\n    if total_matrix.get_shape()[1] != total_bias.get_shape()[0]:\n      raise ValueError('Must have same output dimensions for W and b')\n    res += total_bias\n  return res\n```\n## Questions\n- Would this be of interest for a PR to _rnn_cell.py_? (given further development of code and _BasicLSTMCell_ implementation)\n- General comments/thoughts would be much appreciated\n", "comments": ["Two comments:\n1. Gate seems like an overkill class.\n   If you want to allow setting the nonlinearity and initialization of a particular gate, why not just accept and store these parameters directly?\n2. We're deprecating linear() in favor of the layers functions, like fully_connected. In fact, linear can and will disappear as soon as can make this happen.\n", "@ebrevdo I agree with the gates, also they would only work for basic GRU/LSTM.\n\nI have updated the code based on your comments (though omitted fail-safes for clarifying functionality).\n\nWould this be compatible with your plans on `GRUCell` and `BasicLSTMCell`?\n\n``` python\nclass GRUCell(rnn_cell.RNNCell):\n  \"\"\"Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).\"\"\"\n\n  def __init__(self, num_units, input_size=None,\n               reset_W_in = init_ops.random_normal_initializer(stddev=0.1),\n               reset_W_hid = init_ops.random_normal_initializer(stddev=0.1),\n               reset_b = init_ops.constant_initializer(0.),\n               reset_activation = sigmoid, \n               update_W_in = init_ops.random_normal_initializer(stddev=0.1),\n               update_W_hid = init_ops.random_normal_initializer(stddev=0.1),\n               update_b = init_ops.constant_initializer(0.),\n               update_activation = sigmoid,\n               candidate_W_in = init_ops.random_normal_initializer(stddev=0.1),\n               candidate_W_hid = init_ops.random_normal_initializer(stddev=0.1),\n               candidate_b = init_ops.constant_initializer(0.),\n               candidate_activation = tanh):\n    self._num_units = num_units\n    self._input_size = num_units if input_size is None else input_size\n    self._reset_W_in = reset_W_in\n    self._reset_W_hid = reset_W_hid\n    self._reset_b = reset_b\n    self._reset_activation = reset_activation\n    self._update_W_in = update_W_in\n    self._update_W_hid = update_W_hid\n    self._update_b = update_b\n    self._update_activation = update_activation\n    self._candidate_W_in = candidate_W_in\n    self._candidate_W_hid = candidate_W_hid\n    self._candidate_b = candidate_b\n    self._candidate_activation = candidate_activation\n\n  @property\n  def input_size(self):\n    return self._input_size\n\n  @property\n  def output_size(self):\n    return self._num_units\n\n  @property\n  def state_size(self):\n    return self._num_units\n\n  def _compute(self, args, gate):\n    name, W_in_init, W_hid_init, b_init = gate\n    with vs.variable_scope(name):\n      W_in = vs.get_variable(\"W_in\",\n        [args[0].get_shape()[1], self._num_units],\n        initializer=W_in_init)\n      W_hid = vs.get_variable(\"W_hid\",\n        [args[1].get_shape()[1], self._num_units],\n        initializer=W_hid_init)\n      b = vs.get_variable(\"Bias\", [self._num_units],\n        initializer=b_init)\n      matrix = array_ops.concat(0, [W_in, W_hid])\n    return matrix, b\n\n  def __call__(self, inputs, state, scope=None):\n    \"\"\"Gated recurrent unit (GRU) with nunits cells.\"\"\"\n    args = [inputs, state]\n    with vs.variable_scope(scope or type(self).__name__):  # \"GRUCell\"\n      with vs.variable_scope(\"Gates\"):  # Reset gate and update gate.\n        matrices = []\n        biases = []\n        gates = [\n          (\"Reset\", self._reset_W_in, self._reset_W_hid, self._reset_b),\n          (\"Update\", self._update_W_in, self._update_W_hid, self._update_b)]\n        for gate in gates:\n          matrix, bias = self._compute(args, gate)\n          matrices.append(matrix)\n          biases.append(bias)\n        total_matrix = array_ops.concat(1, matrices)\n        total_bias = array_ops.concat(0, biases)\n        res_gates = math_ops.matmul(array_ops.concat(1, args), total_matrix)\n        res_gates += total_bias\n        r, u = array_ops.split(1, 2, res_gates)\n        r, u = self._reset_activation(r), self._update_activation(u)\n      with vs.variable_scope(\"Candidate\"):\n        candidate = (\"Candidate\", self._candidate_W_in, self._candidate_W_hid,\n            self._candidate_b)\n        matrix, bias = self._compute([inputs, r * state], candidate)\n        c = math_ops.matmul(array_ops.concat(1, args), matrix) + bias\n        c = self._candidate_activation(c)\n      new_h = u * state + (1 - u) * c\n    return new_h, new_h\n```\n", "Any news @ebrevdo?\n", "We currently use single fused weight and bias variables in order to speed up computation.  Having separate initializers would lead to a significant slowdown.  You are welcome to write your own RNNCell which creates separate weight variables, each with their own initializers.\n", "Closing this for now; as it sounds straightforward for you to create this custom LSTMCell in your own repository.\n", "So does anyone test the implementation above? It would be very useful if we could easily implement our own RNNCells. :)"]}, {"number": 2149, "title": "Fix typos in example 'adding an op'", "body": "The colons after `@tf.RegisterShape(\"ZeroOut\")` should not be there.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for the fix!\n"]}, {"number": 2148, "title": "R0.8", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}, {"number": 2147, "title": "Remove unused os import", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "@tensorflow-jenkins test this please\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n"]}, {"number": 2146, "title": "CTC GPU support", "body": "Very happy to see CTC in  #tensorflow! #32\nMy question is:\nIs someone working on CTC GPU implement internal? Or is it 'contribution welcomed'?\nBaidu has a CTC GPU implement:\n\nhttps://github.com/baidu-research/warp-ctc\n", "comments": ["As Eugene and Vijay commented, it is currently on the contrib directory. And once it matures enough, it will be moved to the core directory. \n\nAs always, contributions are welcome. For any significant code changes, it is optional to announce it first, so people would notice and avoid replicated work. \n", "Sorry to bother you again, can you point the ctc .cu.cc file, please? I can't find it.\n@zheng-xq \n", "+1, CTC GPU support would be awesome for handwritten text recognition, speech recognition etc. May key applications here!! \ud83c\udf89 \ud83d\ude00 \n", "I've merged tensorflow and baidu's warp-ctc in:\nhttps://github.com/Hibbert-pku/tensorflow/tree/only-ctc\nmaybe it's useful for you.(pls ignore the latest commits, I just want to add a travisCI for it but failed.)\n@cjh9 \n", "Is there any news?\n", "Adding @ebrevdo to comment on the status of CTC GPU implementation.\n", "Very much a \"contributions welcome\"; @Hibbert-pku has what looks like an implementation in their own github.  It includes some nvidia code which probably cannot be merged into the main tensorflow repository, and a lot of cleanup would have to happen before we could merge a pull request.\n", "Maybe we could open this issue back up then, so it's visible?\n", "I would also find a GPU implementation of CTC loss very helpful!\n", "+1 for a GPU implementation of CTC loss.\n", " +1 GPU CTC Loss\n", "+1 It would be much appreciated to see GPU implementation of connectionist temporal classification loss in Tensorflow.\n", "I managed to get the [**@MISingularity**](https://github.com/MISingularity/tensorflow) Warp CTC implementation working in post-0.10.0 TensorFlow. Not difficult, but as noted, it includes a bunch of proprietary code from NVIDIA. It also completely ignores the `preprocess_collapse_repeated` and `ctc_merge_repeated` flags, so if you need those, as I do, the only option is to pad the input with blanks, then remove the padding afterward. I decided to drop CTC and use a different sequence-to-sequence framework.\n", "Heads up that there is now a TensorFlow kernel that wraps warp-ctc in the master branch of the warp-ctc repository: https://github.com/baidu-research/warp-ctc\n\n(note that it too does not support proprocess_collapse_repeated=true or ctc_merge_repeated=false)\n", "@jaredcasper where is the kernel? I can't find it.\n", "@LinJM it is in the tensorflow_binding directory (and only on the master branch) https://github.com/baidu-research/warp-ctc/tree/master/tensorflow_binding\n", "Any progress?", "can warp-ctc be merged into tensorflow?", "As commented above, getting it to work wasn't hard, but I stopped working on it because of the dependencies on code with problematic licensing terms. If that's not an issue, it could be merged into the main TensorFlow sources pretty easily.\r\n\r\nI started working on adding support for the preprocess_merge_repeated and ctc_merge_repeated flags, but that's tricky, involving quite a few changes to the kernel--small, but all over the place.", "Just to update this this ticket in case others come here looking for Warp CTC integration.\r\n\r\nThe Warp CTC maintainers have released a tensorflow binding which works transparently with `proprocess_collapse_repeated=False` and `ctc_merge_repeated=True`. \r\n\r\nI've tested this with Tensorflow 0.12.0 and got good results, though the speed improvement was marginal in my case.\r\n\r\nhttps://github.com/baidu-research/warp-ctc/tree/master/tensorflow_binding", "@matth I just tried it and I can't see an improvement in speed. I see the CPU utilization is the same in both cases (`tf.nn.ctc_loss` and `warpctc_tensorflow.ctc`).", "@0x454447415244 yes I see similar, not a noticeable improvement in speed and similar CPU usage. Not 100% sure what is happening, I' pretty sure it usese the GPU as I can see it getting compiled with CUDA support.\r\n\r\nTBH I am not sure how much improvement I should expect to see, the warp ctc benchmarks do not compare against Tensorflow's implementation. \r\n\r\nPersonally I am doing CTC with audio features and a mini-batch size of 64 takes 12GB of GPU RAM, so perhaps the CTC  loss calculation is a negligible part of the execution time anyway. I'll stick with using warp though as it's certainly not made my training slower or less accurate.", "I tried it and I can't see an improvement in both CPU mode and gpu mode \r\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 2145, "title": "there is log about cannot enable peer access from device ordinal 0 to device ordinal 2", "body": "I tried to install tensorflow with gpu ,and then run demo code,hello word,it will report\n\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 0 to device ordinal 2\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 0 to device ordinal 3\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 1 to device ordinal 2\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 1 to device ordinal 3\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 2 to device ordinal 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 2 to device ordinal 1\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 3 to device ordinal 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 3 to device ordinal 1\n\nis it a issue?\nHow can it prevent it ?\n", "comments": ["there are 4 gpus in my machine\nThu Apr 28 17:05:37 2016  \n+------------------------------------------------------+  \n| NVIDIA-SMI 352.39     Driver Version: 352.39         |  \n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K40m          Off  | 0000:0A:00.0     Off |                    0 |\n| N/A   36C    P0    62W / 235W |     22MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K40m          Off  | 0000:0D:00.0     Off |                    0 |\n| N/A   32C    P0    61W / 235W |     22MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  Tesla K40m          Off  | 0000:2B:00.0     Off |                    0 |\n| N/A   36C    P0    63W / 235W |     22MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   3  Tesla K40m          Off  | 0000:30:00.0     Off |                    0 |\n| N/A   35C    P0    62W / 235W |     22MiB / 11519MiB |     61%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n", "The NIVIDIA driver function cuDeviceCanAccessPeer is returning false for all pairs of GPUs in your environment.   Perhaps your installation does not have correct access to the proper or full CUDA libraries?   @zheng-xq  Any clue how this should be diagnosed?\n", "Today, you are not losing much if peer-to-peer access is not enabled. But it might have performance implication for future TensorFlow releases. \n\nIf you want to debug this issue locally, you can run the Cuda SDK sample simpleP2P and see if there is a problem. \n", "Hi, I have the same issue. Is this a driver issue or a card issue?\n\nhere is the output from simpleP2P:\n\n```\nCUDA-capable device count: 4\n> GPU0 = \"GeForce GTX TITAN X\" IS  capable of Peer-to-Peer (P2P)\n> GPU1 = \"GeForce GTX TITAN X\" IS  capable of Peer-to-Peer (P2P)\n> GPU2 = \"GeForce GTX TITAN X\" IS  capable of Peer-to-Peer (P2P)\n> GPU3 = \"GeForce GTX TITAN X\" IS  capable of Peer-to-Peer (P2P)\n\nChecking GPU(s) for support of peer to peer memory access...\n> Peer access from GeForce GTX TITAN X (GPU0) -> GeForce GTX TITAN X (GPU1) : Yes\n> Peer access from GeForce GTX TITAN X (GPU0) -> GeForce GTX TITAN X (GPU2) : No\n> Peer access from GeForce GTX TITAN X (GPU0) -> GeForce GTX TITAN X (GPU3) : No\n> Peer access from GeForce GTX TITAN X (GPU1) -> GeForce GTX TITAN X (GPU0) : Yes\n> Peer access from GeForce GTX TITAN X (GPU1) -> GeForce GTX TITAN X (GPU2) : No\n> Peer access from GeForce GTX TITAN X (GPU1) -> GeForce GTX TITAN X (GPU3) : No\n> Peer access from GeForce GTX TITAN X (GPU2) -> GeForce GTX TITAN X (GPU0) : No\n> Peer access from GeForce GTX TITAN X (GPU2) -> GeForce GTX TITAN X (GPU1) : No\n> Peer access from GeForce GTX TITAN X (GPU2) -> GeForce GTX TITAN X (GPU3) : Yes\n> Peer access from GeForce GTX TITAN X (GPU3) -> GeForce GTX TITAN X (GPU0) : No\n> Peer access from GeForce GTX TITAN X (GPU3) -> GeForce GTX TITAN X (GPU1) : No\n> Peer access from GeForce GTX TITAN X (GPU3) -> GeForce GTX TITAN X (GPU2) : Yes\n```\n", "Maybe your GPU0 and GPU2 are you different Pcie Root Complexes? That would force communication to go through QPI link, so maybe that prevents peering? FB supposedly open-sourced it's Big Sur design which shows to configure all 8 GPUs to be on the same root complex, although I'm having trouble finding the actual specs\n", "So how it can make sure gpu0,gpu1 can communicate with gpu2,gpu3?\nshould need to be configure the hardware?\n", "btw,where can I download the simpleP2P code from cuda website,\nI can't find it in cuda website.\nCould you offer the related link?\n", "I have the same result after running simpleP2P\n[@sjs_88_31 release]# ./simpleP2P \n[./simpleP2P] - Starting...\nChecking for multiple GPUs...\nCUDA-capable device count: 4\n\n> GPU0 = \"     Tesla K40m\" IS  capable of Peer-to-Peer (P2P)\n> GPU1 = \"     Tesla K40m\" IS  capable of Peer-to-Peer (P2P)\n> GPU2 = \"     Tesla K40m\" IS  capable of Peer-to-Peer (P2P)\n> GPU3 = \"     Tesla K40m\" IS  capable of Peer-to-Peer (P2P)\n\nChecking GPU(s) for support of peer to peer memory access...\n\n> Peer access from Tesla K40m (GPU0) -> Tesla K40m (GPU1) : Yes\n> Peer access from Tesla K40m (GPU0) -> Tesla K40m (GPU2) : No\n> Peer access from Tesla K40m (GPU0) -> Tesla K40m (GPU3) : No\n> Peer access from Tesla K40m (GPU1) -> Tesla K40m (GPU0) : Yes\n> Peer access from Tesla K40m (GPU1) -> Tesla K40m (GPU2) : No\n> Peer access from Tesla K40m (GPU1) -> Tesla K40m (GPU3) : No\n> Peer access from Tesla K40m (GPU2) -> Tesla K40m (GPU0) : No\n> Peer access from Tesla K40m (GPU2) -> Tesla K40m (GPU1) : No\n> Peer access from Tesla K40m (GPU2) -> Tesla K40m (GPU3) : Yes\n> Peer access from Tesla K40m (GPU3) -> Tesla K40m (GPU0) : No\n> Peer access from Tesla K40m (GPU3) -> Tesla K40m (GPU1) : No\n> Peer access from Tesla K40m (GPU3) -> Tesla K40m (GPU2) : Yes\n> Enabling peer access between GPU0 and GPU1...\n> Checking GPU0 and GPU1 for UVA capabilities...\n> Tesla K40m (GPU0) supports UVA: Yes\n> Tesla K40m (GPU1) supports UVA: Yes\n> Both GPUs can support UVA, enabling...\n> Allocating buffers (64MB on GPU0, GPU1 and CPU Host)...\n> Creating event handles...\n> cudaMemcpyPeer / cudaMemcpy between GPU0 and GPU1: 9.52GB/s\n> Preparing host buffer and memcpy to GPU0...\n> Run kernel on GPU1, taking source data from GPU0 and writing to GPU1...\n> Run kernel on GPU0, taking source data from GPU1 and writing to GPU0...\n> Copy data back to host from GPU0 and verify results...\n> Disabling peer access...\n> Shutting down...\n> Test passed\n", "The results from `simpleP2P` seem like the problem is a CUDA configuration issue and not a TensorFlow-specific issue. I'm closing this for now.\n"]}, {"number": 2144, "title": "Tensorboard tensor representation when a Op (or a node) has multiple output tensors", "body": "I'm using the op 'tf.train.shuffle_batch,' has two output tensors, feature and label.\n\nWhen I visualize it on tensorboard, thickness of edge(tensor) looks weird.\nThe label tensor, to loss node, has thickness of the feature tensor.\n\nIt seems like current tensorboard visualizes thickness using first output tensor of the op for all out edges.\nIs it possible? multiple output tensors has their own shape?\n\n![screenshot from 2016-04-28 23 26 19](https://cloud.githubusercontent.com/assets/15023894/14878512/a15912fc-0d56-11e6-991a-f7445ff36823.png)\n", "comments": ["Hi,\n\nThanks for filing and apologies for the delay in response. This was fixed with f12b843d78b0119b3f8d0ffc99a9e417d9232e7c. You should be able to see correct edge shapes (not only the first output shape) in the 0.9 release, or if you don't want to wait, we just published a release candidate https://github.com/tensorflow/tensorflow/releases/tag/v0.9.0rc0\n"]}, {"number": 2143, "title": "Error like 'batchtospace_op_gpu.cu.pic.o' was not created", "body": "For bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:Ubuntu 16.04\n\nInstalled version of CUDA and cuDNN: \nCUDA toolkit 7.5\ncudnn 7.0(3.0)\n\nit seems NVIDIA have not provided the .run file for Ubuntu16.04, so I installed the 15.04 version.\n\ninstalled from sources, encounter the errors like below:\nERROR: /home/huangzh/Github/tensorflow/tensorflow/core/kernels/BUILD:1190:1: output 'tensorflow/core/kernels/_objs/batchtospace_op_gpu/tensorflow/core/kernnel/batchtospace_op_gpu.cu.pic.o' was not created.\nERROR: /home/huangzh/Github/tensorflow/tensorflow/core/kernels/BUILD:1190:1: not all outputs were created.\n", "comments": ["I have the exact same issue with same specs, except for cuDNN which is (5.0.5) on my computer.\n\nAlso, there are no errors besides the one object file was not created, so I have no idea what might be wrong.\n", "Try running bazel with `--verbose_failures`, otherwise we have no information to help.\n", "I managed to solve this by using these two fixes:\n\nFirst, I changed the flags as described here: https://github.com/tensorflow/tensorflow/issues/1066#issuecomment-200580370. This alone did not fixed the problem.\n\nSecond, I reverted the standard string.h to 2.22 as mentioned in the same issue \u2013 the two comments just below the first one.\n\nI don't know whether the first step was necessary.\n", "@MikulasZelinka I only conduct the second step u mentioned after altering gcc-5.3 to gcc-4.9, then it works. I'm on ubuntu 16.0.4 with cuda 7.5 cudnn v5. \ngcc-4.9  fixes the 'undefined' problem and reverting string.h can fix this one. \n", "Closing for now, since it seems like an NVIDIA bug.  I'm happy to reopen if there's something we can do about this on our end.\n", "I don't think it's wise to close this issue as it affects many. \n\nFor me, adding\n`cxx_flag: \"-D_MWAITXINTRIN_H_INCLUDED\"` in tensorflow/third_party/gpus/crosstool/CROSSTOOL\nfixed the problem.\n\nAdditionally, it is unproblematic to install cuda toolkit with gcc 5.3 on ubuntu 16.04 - just install with the override option.\n\nWhat I fear are performance issues, but I don't know if they exist.\n", "@Froskekongen The problem is that we're not sure if that cxx_flag will break other people.  Do you have a better sense for what it does, and whether other breakage is likely?\n", "For updated information, I had the exact same problem specified above. Following instructions from @Froskekongen, I was able to edit the CROSSTOOL file only and proceed with the instructions on https://www.tensorflow.org/versions/r0.9/get_started/os_setup.html#installing-from-sources\n\nThere was one exception to the instructions; I had to edit the file twice.  There are 2 instances of `cxx_flag: \"-std=c++11\"` in the current CROSSTOOL version. I added the line `cxx_flag: \"-D_MWAITXINTRIN_H_INCLUDED\"` below each of them as follows:\n\n---\n\n```\nnano tensorflow/third_party/gpus/crosstool/CROSSTOOL\n```\n- search for \"cxx_flag\"\n- found 2 of them\n- added `cxx_flag: \"-D_MWAITXINTRIN_H_INCLUDED\"` after both `cxx_flag: \"-std=c++11\"` entries\n\n```\ncat -n third_party/gpus/crosstool/CROSSTOOL |grep D_MWAITXINTRIN_H_INCLUDED\n    53    cxx_flag: \"-D_MWAITXINTRIN_H_INCLUDED\"\n   173    cxx_flag: \"-D_MWAITXINTRIN_H_INCLUDED\"\n```\n\nThe line numbers correspond to where I added the current entries.\n\n---\n\nMy layout:\n\nOS: Fedora: Kernel - 4.5.7-202.fc23.x86_64\nCompiler: gcc-5.3 (needed to override gcc-4.9 vs gcc-5.3 install check)\nCUDA: 7.5\ncuDNN: 5.1.3\n\n---\n\nAfter adding these lines, then everything ran smoothly from here. I followed the remaining instructions from\n\nhttps://www.tensorflow.org/versions/r0.9/get_started/os_setup.html#installing-from-sources\n", "I'm had the same error and added the `cxx_flag: \"-D_MWAITXINTRIN_H_INCLUDED` lines to fix it. It worked but I then got another error message: \n\n```\nERROR: /home/david/tensorflow/tensorflow/core/kernels/BUILD:1509:1: output \n'tensorflow/core/kernels/_objs/depth_space_ops_gpu/tensorflow/core/kernels/depthtospace_op_gpu.cu.o' was not created.\n```\n\nOS: Linux Mint 18\nCompiler: gcc-4.9\nCUDA: 7.5\nCuDNN: 4.0.7\n", "Same problem with dbikard\nOS: Ubuntu 16.04\nCompiler: gcc-4.9\nCUDA: 8.0\nCuDNN: 5.0\n\nFollowed this:\nhttp://stackoverflow.com/questions/38585357/errors-when-building-tensorflow-on-linux\n\nBut the problem remained.\n", "@lerynth , don't use gcc-4.9 with Ubuntu 16.04\nInstead, use gcc-5 (5.3 or 5.4)\nThe only fix in CUDA 8 you need to do is fixing this line:\n`/usr/local/cuda/include/host_config.h:81` :\n`#if __GNUC__ > 4 || (__GNUC__ == 4 && __GNUC_MINOR__ > 9)`\nto support your version.\nYou might also consider using an older bazel version (0.22, 0.22b) if it still doesn't work for you.\n", "@buriy, thanks for your suggestion. I tried your solution, \nincluding using older bazel version, but unfortunately \nit still doesn't work. Is this due to using GTX 1080?\n\nSo let me update my environment:\nOS: Ubuntu 16.04\nCompiler: gcc-5.4\nBazel: 0.2.2b\nCUDA: 8.0\nCuDNN: 5.0\nGTX 1080\n", "@lerynth I was able to solve this. See here:\nhttp://stackoverflow.com/q/38585357/697598\n\nOn Sun, 14 Aug 2016 at 06:40, lerynth notifications@github.com wrote:\n\n> @buriy https://github.com/buriy, thanks for your suggestion. I tried\n> you solution,\n> including using older bazel version, but unfortunately\n> it still doesn't work. Is this due to using GTX 1080?\n> \n> So let me update my environment:\n> \n> OS: Ubuntu 16.04\n> Compiler: gcc-4.9\n> CUDA: 8.0\n> CuDNN: 5.0\n> \n> GTX 1080\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2143#issuecomment-239655833,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAl3EoHPnZG-RDmIkXmTN0fX2wTI0xCHks5qfpwugaJpZM4IRpLm\n> .\n", "@buriy, @dbikard thanks for your help. Finally, I managed to make it works by following @wurrego at:\nhttps://github.com/tensorflow/tensorflow/issues/3786\n"]}, {"number": 2142, "title": "[WIP] TensorFlow Studio prototype", "body": "For now:\n- Added placeholder for TF Studio backend server.\n- To add index page.\n\nComing:\n- Config loading / writing (`.studio`, `~/.studio.rc`).\n- Example plugin with handlers and frontend.\n  - Datasets plugin\n  - Estimators plugin\n\nFYI @danmane @martinwicke @terrytangyuan \n", "comments": ["Closing this for now, will do make another PR in few weeks after tf.learn is solid.\n"]}, {"number": 2141, "title": "tf.learn: Loading pre-trained variables into Estimators", "body": "Add support for loading pre-trained variables into Estimators.\n\nSimple API can be:\n\n```\nest = learn.TensorFlowEstimator(..)\nest.restore_variables(path_to_checkpoints, {'embeddings': 'embed/matrix'})\n```\n\nwhere `restore_variables` takes path and map of new variable name to variable name in the checkpoint.\n\nRef https://github.com/tensorflow/skflow/issues/160\n", "comments": ["The TensorFlowEstimator class had save() and restore() functions, but then that the class was deprecated. Are there plans to add the same functions to the Estimator class? \n\nI noticed there is an export_estimator() function, but I can't find the corresponding import_estimator(). What would you suggest for someone how needs to save/restore a model now?\n", "Please use model_dir in constructor\n", "Thanks, I do use the model_dir param and it works. I meant to ask about the ability to save the execution graph as well, not just the weights. Such that I can send the model file to someone and they can load it and run it without having to send the python code with it. I believe that's what TensorFlowEstimator.save() does, right? Or am I misunderstanding it. \n", "Ok I see. Then I'll be waiting for @ilblackdragon to respond on the plans. \n", "Does metagraph solve this problem https://www.tensorflow.org/versions/r0.9/how_tos/meta_graph/index.html ?\n", "Hi, I think I'm looking for the same thing. I do want to store the graph as well as the weights, but I don't want to supply a file/directory name, because I'd like to store the information in a database (Mongo). So I need a serialized representation that I can write and read myself.\nhttps://www.tensorflow.org/versions/r0.9/how_tos/meta_graph/index.html seems to fit the bill but its usage is explained only in the context of \"pure\" TensorFlow (restoring a session). I'm not sure how I would have to use it to save and restore a complete DNNClassifier, for example.\n", "Can't you just mirror the filesystem (checkpoints etc) into mongo with key=filepath, value=blob?\n", "@waleedka You should use `.export()` method which saves (meta)graph + checkpoints. It enables to load using TensorFlow Servo model back and serve at scale. If you want to restore and use in Python - right now you need to have code around. We are working on making that to work reliably from exported graph, but it still a bit down the line.\n\n@snbuchholz You probably should use `.export()` on your Estimator to save into some temporary directory. Then write custom \"copier\" that would upload (meta)graph and checkpoints into database.\n", "Oh, and the original feature request for this has been implemented: `tf.contrib.framework.init_from_checkpoint`. Closing this, if there issues with exporting / importing models - please open a separate bug.\n", "Separate issue, please.\n\nOn Wed, Sep 6, 2017 at 7:35 AM, rhalbersma <notifications@github.com> wrote:\n\n> [Please let me know if this is off-topic and should belong in a new issue]\n>\n> Instead of saving/loading entire models from disk, I would also like the\n> Estimator API to treat the trained weights as first-class entities (e.g.\n> in a Weights class). E.g., the train() method could accept as input a\n> weight initializer (defaulted to the currently used initialization\n> routine), and expose the trained weights in a NumPy compatible layout.\n>\n> E.g. one use case that I frequently have is that I want to port an\n> existing regression model (e.g. fitted in other packages such as R). To\n> verify that the specified TF model_fn is correct, I'd like to swap in the\n> weights from R, feed the training set and run evaluate to confirm that\n> the loss function has the same value. Then I can add more layers/features\n> in TF, knowing that my starting point is correct.\n>\n> The use case for trained weights export in NumPy is that I don't always\n> want to deploy a model in TF, but e.g. implement it in C++ or another\n> environment.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/2141#issuecomment-327502167>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxXxR-LT81Rl9bZaLB_I0DA-TjOchks5sfq25gaJpZM4IRmKk>\n> .\n>\n\n\n\n-- \n - Alex\n"]}]