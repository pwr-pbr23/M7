[{"number": 9060, "title": "No toolchain found for cpu 'arm64-v8a' when building android demo app", "body": "Hi,\r\n\r\nI'm trying to build the android demo app with target cpu of 'arm64-v8a', from a mac. With command \"bazel build -c opt //tensorflow/examples/android:tensorflow_demo --cpu=arm64-v8a\", I got the following error:\r\n```\r\nERROR: No toolchain found for cpu 'arm64-v8a'. Valid cpus are: [\r\n  darwin,\r\n  armeabi-v7a,\r\n  x64_windows_msvc,\r\n  s390x,\r\n  ios_x86_64,\r\n].\r\n```\r\nI am able to build with the default target \"armeabi-v7a\". Any advice?\r\n\r\nI'm on a fairly recent SHA: 55a1f26547f18ddc5c5b5c8a07479591a4ba789d (the master as of Mar 29). My bazel version is 0.4.5\r\nI specified the NDK path in the WORKSPACE file as the following:\r\n```\r\nandroid_ndk_repository(\r\n    name = \"androidndk\",\r\n    path = \"/Users/_user_/Downloads/android-ndk-r12b\",\r\n    api_level = 14)\r\n```\r\nI searched the toolchains under folder `/Users/_user_/Downloads/android-ndk-r12b/toolchains` and found `android-ndk-r12b/toolchains/llvm/prebuilt/darwin-x86_64/bin/arm64-v8a`. Not sure why bazel does not use it.", "comments": ["Could you try with `bazel build --config=android_arm64 //tensorflow/examples/android:tensorflow_demo` instead?\r\n\r\n@andrewharp might have better suggestions.", "With the addition of the --cpu flag it seems to forget that it's building for Android due to a missing crosstool_top flag that is usually passed simultaneously. If you use `--fat_apk_cpu=arm64-v8a` instead  it should work.", "@asimshankar and @andrewharp ,\r\nThank you both for quick turnaround. It works like a charm. (I used the flag --fat_apk_cpu=arm64-v8a).", "Closing this out since the main issue was resolved.\r\n\r\n@andrewharp : Do you want to update https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/android/README.md ?\r\n"]}, {"number": 9059, "title": "Branch 152550050", "body": "", "comments": []}, {"number": 9058, "title": "AdadeltaOptimizer numeric issue", "body": "Hi,\r\n\r\nI tested it with following script, and expected the model update to be -sqrt(0+1)/sqrt(1+1)*1 = -0.7071. However, the output is -0.866. Input gradient is 1, and confirmed with GradientDescentOptimizer. I also verified in rho=1/epsilon=1 the output is -1 as expected, and rho=1/epsilon=0 the output is nan as expected. Reading the implementation of adadelta didn't tell me why the output is -0.866 for rho=0/epsilon=1. What am I missing?\r\n\r\n    def test_tf():\r\n        data  = [0.5]\r\n        label = [0]\r\n        dim   = 1\r\n        batch_size = 1\r\n        import tensorflow as tf\r\n        tf.reset_default_graph()\r\n\r\n        x = tf.placeholder(tf.float32, [batch_size, dim])\r\n        l = tf.placeholder(tf.int32, [batch_size, dim])\r\n        W = tf.Variable(tf.zeros((dim,)), name='W')\r\n        logits = x + W\r\n        print(logits)\r\n        loss = tf.losses.mean_squared_error(logits, l)\r\n        #optimizer = tf.train.GradientDescentOptimizer(1)\r\n        optimizer = tf.train.AdadeltaOptimizer(1, rho=0, epsilon=1)\r\n        train = optimizer.minimize(loss)\r\n\r\n        with tf.Session() as sess:\r\n            sess.run(tf.global_variables_initializer())\r\n            input_map = {x:[data], l:[label]}\r\n            print('loss', loss.eval(input_map))\r\n            sess.run(train, input_map)\r\n            print('\\n'.join([' {}\\n'.format(p.eval()) for p in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)]))\r\n        sess.close()\r\n\r\n    test_tf()\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "I do think this is a bug though. Tried to generate the ground truth according to https://github.com/tensorflow/tensorflow/blob/a0d784bdd31b27e013a7eac58a86ba62e86db299/tensorflow/python/training/adadelta_test.py#L98\r\n\r\n    def ground_truth():    \r\n        accum = 0\r\n        accum_update = 0\r\n        grad = 1\r\n        rho = 0\r\n        epsilon = 1\r\n        lr = 1\r\n        tot_update = 0\r\n        \r\n        accum = accum * rho + (grad**2) * (1 - rho)\r\n        update = (np.sqrt(accum_update + epsilon) *\r\n                 (1. / np.sqrt(accum + epsilon)) * grad)\r\n        accum_update = (accum_update * rho + (update**2) *\r\n                       (1.0 - rho))\r\n        tot_update += update * lr\r\n        print('expected:', tot_update)\r\n\r\nAnd the output is 0.7071 instead of 0.866"]}, {"number": 9057, "title": "R1.1", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->"]}, {"number": 9056, "title": "tensorboard doesnt display y-axis correctly.", "body": "![image](https://cloud.githubusercontent.com/assets/11971499/24820310/61102b8a-1bb6-11e7-8128-36ac5ab99bf4.png)\r\n\r\nThis plot makes it look like AUC is greater than 1.0 when it is not.\r\n\r\nc:\\Python35\\Scripts>pip show tensorflow-gpu\r\nName: tensorflow-gpu\r\nVersion: 1.1.0rc1\r\nSummary: TensorFlow helps the tensors flow\r\nHome-page: http://tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: opensource@google.com\r\nLicense: Apache 2.0\r\nLocation: c:\\python35\\lib\\site-packages\r\nRequires: numpy, wheel, protobuf, six, werkzeug\r\n", "comments": ["@isaacgerg : Can you provide any more detail that helps reproduce the problem? For example, a pointer to the logdir you use?\r\n\r\nCCing @dandelionmane just in case he has thoughts on this.", "@asimshankar I cannot send you any more detail unfortunately other than the values that make up the curve just from the screenshots as the data is proprietary.", "@isaacgerg : Sure, I just wanted to make sure that the AUC scalar being plotted is really less than 1.0 :)", "The hover text is for that point that's plotted wrong.\n\nSent from my Android.\n\nOn Apr 7, 2017 8:13 PM, \"Asim Shankar\" <notifications@github.com> wrote:\n\n> @isaacgerg <https://github.com/isaacgerg> : Sure, I just wanted to make\n> sure that the AUC scalar being plotted is really less than 1.0 :)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9056#issuecomment-292679792>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ALarqwcyq-2xrApkjK38sTOhNfler5k3ks5rttEagaJpZM4M3Z1f>\n> .\n>\n", "Ah, thanks for pointing that out. @dandelionmane?", "I wonder if its a render issue and the 1.00 should be moved up to the next y tick.", "This looks like a rounding/precision error. The labels for 0.999 and 0.998 appear twice, so the top most label should probably also be 1.00 and so your value is between 1.00 and 1.00 :), which is ok for 0.9999 when the \"real\" labels are 0.995 rounded to 1.00 and 1.0 rounded to 1.00.\r\n\r\nSo everything is displayed correctly but there are not enough decimal places for the labels to see it.", "> So everything is displayed correctly\r\n\r\nI understanding the rounding error but the graph is still wrong.  There is no way one could put a plot like that in a journal.", "I agree with you. The graph is not good and I wouldn't show such a graph to anyone, this only creates confusion and questions.\r\n\r\nWhat I wanted to say, there is no error in the graph (so there is nothing to fix there) but the axis labeling of the graph is confusing (so the selection of the y-axis range and decimal places should be changed).", "Makes sense -- thanks!", "Please provide a dataset (or code snippet) that reproduces this and we'll dig into it. @isaacgerg ", "@dandelionmane  I've moved on from this issue - sorry but its been over 3 weeks.  I no longer get the problem based on my new training scheme, the numbers have a much larger derivative.  Feel free to close if you'd like.", "If we get a repro script\u2014even with constant data\u2014I'd like to look into this. Until then, closing.\r\n\r\nPlease feel free to re-open on our new repository at https://github.com/tensorflow/tensorboard."]}, {"number": 9055, "title": "tf.matmul should be extended for rank 1 tensors", "body": "This:\r\n\r\n    import numpy as np\r\n    a = np.array([1, 2, 1])\r\n    w = np.array([[.5, .6], [.7, .8], [.7, .8]])\r\n    \r\n    print(np.dot(a, w))\r\n    # [ 2.6  3. ] # plain nice old matrix multiplication n x (n, m) -> m\r\n    \r\n    import tensorflow as tf\r\n    \r\n    a = tf.constant(a, dtype=tf.float64)\r\n    w = tf.constant(w)\r\n    \r\n    with tf.Session() as sess:\r\n        print(tf.matmul(a, w).eval())\r\n    \r\nresults in:\r\n\r\n    ValueError: Shape must be rank 2 but is rank 1 for 'MatMul' (op: 'MatMul') with input shapes: [3], [3,2].\r\n\r\nThat's surprising and keeps tripping people up (see: http://stackoverflow.com/q/34908033/281545,  http://stackoverflow.com/q/43284897/281545, for instance), which is natural as matrix multiplication should not have any constraints apart from dimension alignment. Workarounds are verbose and complicated:\r\n\r\n```\r\n  print(tf.matmul(tf.expand_dims(a,0), w).eval())\r\n  print((tf.reduce_sum(tf.multiply(tf.expand_dims(a,-1), w), axis=0)).eval())\r\n  print((tf.reduce_sum(tf.multiply(a, tf.transpose(w)), axis=1)).eval())\r\n```\r\n\r\n### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*: yes\r\n- *TensorFlow installed from (source or binary)?*: pip\r\n- *TensorFlow version*: `tf.__version__` gives '1.0.1' - windows and python 3.5.2\r\n- *Bazel version (if compiling from source)*: -\r\n- *CUDA/cuDNN version*: -\r\n- *GPU Model and Memory*: -\r\n- *Exact command to reproduce*: see above\r\n\r\n### Source Code / Logs\r\n\r\nFull traceback (why printing the same error twice ?)\r\n\r\n```\r\nC:\\_\\Python35\\python.exe C:/Users/MrD/.PyCharm2017.1/config/scratches/scratch_31.py\r\n[ 2.6  3. ]\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"CountExtremelyRandomStats\" device_type: \"CPU\"') for unknown op: CountExtremelyRandomStats\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"FinishedNodes\" device_type: \"CPU\"') for unknown op: FinishedNodes\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"GrowTree\" device_type: \"CPU\"') for unknown op: GrowTree\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ReinterpretStringToFloat\" device_type: \"CPU\"') for unknown op: ReinterpretStringToFloat\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"SampleInputs\" device_type: \"CPU\"') for unknown op: SampleInputs\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ScatterAddNdim\" device_type: \"CPU\"') for unknown op: ScatterAddNdim\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNInsert\" device_type: \"CPU\"') for unknown op: TopNInsert\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNRemove\" device_type: \"CPU\"') for unknown op: TopNRemove\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TreePredictions\" device_type: \"CPU\"') for unknown op: TreePredictions\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"UpdateFertileSlots\" device_type: \"CPU\"') for unknown op: UpdateFertileSlots\r\nTraceback (most recent call last):\r\n  File \"C:\\_\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\", line 671, in _call_cpp_shape_fn_impl\r\n    input_tensors_as_shapes, status)\r\n  File \"C:\\_\\Python35\\lib\\contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"C:\\_\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Shape must be rank 2 but is rank 1 for 'MatMul' (op: 'MatMul') with input shapes: [3], [3,2].\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/MrD/.PyCharm2017.1/config/scratches/scratch_31.py\", line 14, in <module>\r\n    print(tf.matmul(a, w).eval())\r\n  File \"C:\\_\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1765, in matmul\r\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\r\n  File \"C:\\_\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 1454, in _mat_mul\r\n    transpose_b=transpose_b, name=name)\r\n  File \"C:\\_\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"C:\\_\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2329, in create_op\r\n    set_shapes_for_outputs(ret)\r\n  File \"C:\\_\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1717, in set_shapes_for_outputs\r\n    shapes = shape_func(op)\r\n  File \"C:\\_\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1667, in call_with_requiring\r\n    return call_cpp_shape_fn(op, require_shape_fn=True)\r\n  File \"C:\\_\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\", line 610, in call_cpp_shape_fn\r\n    debug_python_shape_fn, require_shape_fn)\r\n  File \"C:\\_\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\", line 676, in _call_cpp_shape_fn_impl\r\n    raise ValueError(err.message)\r\nValueError: Shape must be rank 2 but is rank 1 for 'MatMul' (op: 'MatMul') with input shapes: [3], [3,2].\r\n\r\nProcess finished with exit code 1\r\n\r\n```", "comments": ["@rmlarsen, could you comment on this. I feel like you haven't allowed this since it would make an ambiguity between batch and non batch forms of matrix multiply.\r\n\r\nAlso, your example is not \"plain nice matrix multiplication\". It is left-side vector multiplication of a matrix with an implicit broadcast.  ", "Thanks for answering\r\n\r\n> Also, your example is not \"plain nice matrix multiplication\". It is left-side vector multiplication of a matrix with an implicit broadcast.\r\n\r\n it is matrix multiplication in the mathematical sense (1,n array with n,m array). Note that result `np.dot(a, w)` in my example has shape `(2,)` not `(2,1)`", "I'm a [long-time user](http://stackoverflow.com/search?q=user:419116+[wolfram-mathematica]) of Mathematica, which allows mixing ranks, and I'm slightly biased against this kind of matmul usage.\r\n\r\nIn Mathematica, you can take rank1 vec and do\r\n1. `vec ~Dot~ mat`. This treats `vec` as a \"row matrix\"\r\n2. `mat ~Dot~ vec` treats `vec` as a \"column matrix\"\r\n\r\nThis makes things more elegant in the short term. In the long term I've ended up with tricky numerics bugs caused by multiplying matrix on the wrong side, so I end up converting everything to rank-2 anyway to take advantage of dimension checks.", "I too would like to find a compromise to allow vector\\*matrix, matrix\\*vector to be expressed more concisely. \r\n\r\nHowever, we have not allowed this because it would be ambiguous in the presence of broadcasting (which we have not yet implemented) and batched matmul.  \r\n\r\nOne approach is that taken by numpy.matmul(**a**,\r\n **b**) (https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html), which special cases rank(**a**) == 1 by implicitly promoting **a** to a 1 x n matrix. Likewise it promotes **b** to a m x 1 matrix for rank(**b**) == 1. For other ranks, the usual numpy broadcasting rules apply. \r\n\r\nThe numpy approach is also not completely satisfactory, as I've seen (and written) several examples of code wanting to do batch_of_matrices * batch_of_vectors. An alternative to support this without extra code is to promote **a** of shape [k, l, ... n] => [k, l, ..., 1, n] when rank(**a**) == rank(**b**) - 1 (and similarly promote **b** of shape [s, t, ..., m] to [s,t,...,m,1] when rank(**b**) == rank(**a**) - 1.\r\nBut this alternative is not compatible with Numpy broadcasting.\r\n\r\nSo in the absence of a clear choice, we have left the functionality as is. I'd be curious to hear more opinions on this.\r\n", "I am trying to run this code:\r\n**import tensorflow as tf \r\n\r\nx1 = tf.constant(5)\r\nx2 = tf.constant(6)\r\nresult = **tf.matmul(x1,x2)**\r\nprint(result )\r\n\r\nsees = tf.Session()\r\nprint(sees.run(result))\r\nsees.close()**\r\n\r\n\r\nBut it's showing me this error:\r\n**Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/common_shapes.py\", line 686, in _call_cpp_shape_fn_impl\r\n    input_tensors_as_shapes, status)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Shape must be rank 2 but is rank 0 for 'MatMul' (op: 'MatMul') with input shapes: [], [].\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"tensor.py\", line 6, in <module>\r\n    result = tf.matmul(x1,x2)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\", line 2022, in matmul\r\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 2516, in _mat_mul\r\n    name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3162, in create_op\r\n    compute_device=compute_device)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3208, in _create_op_helper\r\n    set_shapes_for_outputs(op)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2427, in set_shapes_for_outputs\r\n    return _set_shapes_for_outputs(op)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2400, in _set_shapes_for_outputs\r\n    shapes = shape_func(op)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2330, in call_with_requiring\r\n    return call_cpp_shape_fn(op, require_shape_fn=True)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/common_shapes.py\", line 627, in call_cpp_shape_fn\r\n    require_shape_fn)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/common_shapes.py\", line 691, in _call_cpp_shape_fn_impl\r\n    raise ValueError(err.message)\r\nValueError: Shape must be rank 2 but is rank 0 for 'MatMul' (op: 'MatMul') with input shapes: [], [].**\r\nCan you help me in this???"]}, {"number": 9054, "title": "remove unused include", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 9053, "title": "Split on / Has Unexpected Behavior", "body": "Repost from: https://github.com/tensorflow/tensorflow/issues/8993#issuecomment-292632184 /CC @dandelionmane\r\n\r\nSo it looks like using `/`only works if the keys on the left does not already exist. \r\nFor example this works (both on 1.1rc1):\r\n![image](https://cloud.githubusercontent.com/assets/51059/24816749/f09f96e6-1ba7-11e7-9c1c-e36dd81bb0d0.png)\r\n\r\nbut this does not:\r\n![image](https://cloud.githubusercontent.com/assets/51059/24816764/01af4b98-1ba8-11e7-8dad-eca20d0ffb5d.png)\r\n", "comments": ["I believe that is intentional - in that TensorBoard encourages use of `/` and not `_` as the logical separator and thus your graphs will have to be created with `/`s in the names.", "Yea  but this doesn't really have to do with_. Rather seems like something odd happens when name collision. ", "Interesting. I can reproduce this. The following script demonstrates buggy behavior in TensorBoard:\r\n\r\n```py\r\nimport tensorflow as tf\r\n\r\ndef main():\r\n  k = tf.constant(0)\r\n  tf.summary.scalar('loss', k)\r\n  tf.summary.scalar('loss/regularization', k)\r\n  tf.summary.scalar('loss_extra_stuff', k)\r\n  summ = tf.summary.merge_all()\r\n\r\n  sess = tf.Session()\r\n  writer = tf.summary.FileWriter('/tmp/cat2')\r\n  writer.add_graph(sess.graph)\r\n  writer.add_summary(sess.run(summ))\r\n  writer.close()\r\n\r\nif __name__ == '__main__':\r\n  main()\r\n```\r\n\r\nThank you for reporting. I've migrated this to our new repository at https://github.com/tensorflow/tensorboard/issues/41."]}, {"number": 9052, "title": "Branch 152523909", "body": "", "comments": []}, {"number": 9051, "title": "Tensorboard change NPM script name prepare to prep", "body": "NPM 4.3 now calls the 'prepare' script as part of the install process.\r\nAs a result, running 'npm install' or 'npm run prepare' causes an\r\ninfinite loop. This removes the infinite loop. Relates to issue #8326 ", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "Could you follow the instructions to sign the CLA? We need this. Thanks!", "Will do asap\n\nOn Apr 10, 2017 1:21 PM, \"drpngx\" <notifications@github.com> wrote:\n\n> Could you follow the instructions to sign the CLA? We need this. Thanks!\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9051#issuecomment-293035086>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ACW9eN-P8Gq4DAoF50rGo6RQX5vIB4D5ks5runNGgaJpZM4M3Lk0>\n> .\n>\n", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@dandelionmane how does that look?", "@dsmilkov if you have cycles or can suggest someone who has cycles, otherwise we can just wait", "Jenkins, test this please.", "Thanks!", "No problem! Happy to help\n\nOn Apr 15, 2017 8:50 AM, \"drpngx\" <notifications@github.com> wrote:\n\n> Thanks!\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9051#issuecomment-294294498>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ACW9eN4aHxoIDf-PMkA0XAKtRBeu5liKks5rwMssgaJpZM4M3Lk0>\n> .\n>\n"]}, {"number": 9050, "title": "How to execute distributed training where each node has multiple workers", "body": "Hi, what is the command to run distributed training on multiple nodes where each node has multiple GPUs.\r\nThe example in https://github.com/tensorflow/models/tree/master/inception only shows the case where each node has 1 GPU/1 worker. In my cluster, each node has 4 GPUs which should require 4 workers.\r\n\r\nI tried the following command:\r\non **node 0**:\r\nbazel-bin/inception/imagenet_distributed_train \\\r\n--batch_size=32 \\\r\n--data_dir=$HOME/imagenet-data \\\r\n--job_name='worker' \\\r\n--task_id=0 \\\r\n--ps_hosts='ps0.example.com:2222' \\\r\n--worker_hosts='worker0.example.com:2222,worker0.example.com:2222,worker0.example.com:2222,worker0.example.com:2222,worker1.example.com:2222,worker1.example.com:2222,worker1.example.com:2222,worker1.example.com:2222' &\r\n......\r\n\r\nbazel-bin/inception/imagenet_distributed_train \\\r\n--batch_size=32 \\\r\n--data_dir=$HOME/imagenet-data \\\r\n--job_name='worker' \\\r\n--task_id=3 \\\r\n--ps_hosts='ps0.example.com:2222' \\\r\n--worker_hosts='worker0.example.com:2222,worker0.example.com:2222,worker0.example.com:2222,worker0.example.com:2222,worker1.example.com:2222,worker1.example.com:2222,worker1.example.com:2222,worker1.example.com:2222'\r\n\r\non **node 1**:\r\nbazel-bin/inception/imagenet_distributed_train \\\r\n--batch_size=32 \\\r\n--data_dir=$HOME/imagenet-data \\\r\n--job_name='worker' \\\r\n--task_id=4 \\\r\n--ps_hosts='ps0.example.com:2222' \\\r\n--worker_hosts='worker0.example.com:2222,worker0.example.com:2222,worker0.example.com:2222,worker0.example.com:2222,worker1.example.com:2222,worker1.example.com:2222,worker1.example.com:2222,worker1.example.com:2222' &\r\n......\r\n\r\nbazel-bin/inception/imagenet_distributed_train \\\r\n--batch_size=32 \\\r\n--data_dir=$HOME/imagenet-data \\\r\n--job_name='worker' \\\r\n--task_id=7 \\\r\n--ps_hosts='ps0.example.com:2222' \\\r\n--worker_hosts='worker0.example.com:2222,worker0.example.com:2222,worker0.example.com:2222,worker0.example.com:2222,worker1.example.com:2222,worker1.example.com:2222,worker1.example.com:2222,worker1.example.com:2222'\r\n\r\nNote that there is & at the end of each command so that they can be executed in parallel, but it has out of GPU memory error.\r\n\r\nI also tried to use only 1 worker in each node and each worker uses 4 GPU:\r\non **node 0**:\r\nbazel-bin/inception/imagenet_distributed_train \\\r\n--batch_size=32 \\\r\n--data_dir=$HOME/imagenet-data \\\r\n--job_name='worker' \\\r\n--gpus=4\r\n--task_id=0 \\\r\n--ps_hosts='ps0.example.com:2222' \\\r\n--worker_hosts='worker0.example.com:2222,worker1.example.com:2222'\r\n\r\non **node 1**:\r\nbazel-bin/inception/imagenet_distributed_train \\\r\n--batch_size=32 \\\r\n--data_dir=$HOME/imagenet-data \\\r\n--job_name='worker' \\\r\n--gpus=4\r\n--task_id=1 \\\r\n--ps_hosts='ps0.example.com:2222' \\\r\n--worker_hosts='worker0.example.com:2222,worker1.example.com:2222'\r\n\r\nBut in the end each node only uses 1 GPU. \r\n\r\nSo what is the exact command I should use? Thanks.\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9049, "title": "Android example using CMake on Windows not working", "body": "I installed CPU-only Tensorflow version 1.0 on Windows using the pip installer. I am trying to get the Android example to run using CMake as explained in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/android/cmake. I cloned the newest version of the tensorflow repository and created a new Android studio project and followed the instructions from the webpage above by modifying the gradle files.  I already found out that it should be \r\n`debugCompile project(path: ':TensorFlow-Android-Inference', configuration: 'debug')\r\nreleaseCompile project(path: ':TensorFlow-Android-Inference', configuration: 'release')`\r\ninstead of \r\n`debugCompile project(path: ':tensorflow_inference', configuration: 'debug')\r\nreleaseCompile project(path: ':tensorflow_inference', configuration: 'release')`\r\nNow, however, I get a build error stating \"Error:Project :app declares a dependency from configuration 'releaseCompile' to configuration 'release' which is not declared in the descriptor for project :TensorFlow-Android-Inference.\"\r\nHas anyone tried this or could anyone point me to a proper explanation of how to use cMake to build the project. \r\nAny help would be very much appreciated. Thanks. \r\n", "comments": ["@andrewharp : Any comments?", "@vincentbecker The specific error you're getting here sounds like a Gradle problem. Different versions expose or don't expose default build rules from Android Studio differently. I think for current versions of Gradle the same pattern as in tensorflow/examples/android/build.gradle needs to be followed, so we should probably fix that.\r\n\r\nHowever, even if this is corrected you won't be able to build Android TF on Windows via this approach. The cmake solution actually has to use make internally and so will not actually work on Windows.\r\n\r\nWe plan to eventually support a 100% cmake build solution. Until then see https://github.com/tensorflow/tensorflow/issues/6385#issuecomment-285208600 for a bash for Windows workaround and also instructions on using prebuilt binaries.", "Closing this as a duplicate of #6385", "We'll still need to fix whatever the issue is with contrib/android/cmake/ causing the specific error given here, so reopening to track.", "hey andrew, are you still actively working on this?", "Hey @andrewharp I'm running in this issue while trying to build using CMake on Ubuntu 16.04. I am making progress on this issue but I've gotten sort of stuck so I'd appreciate it if you would take another look. Thanks!", "Nagging Assignee @andrewharp: It has been 239 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please open a new issue if this persist with the latest tensorflow version by provinding all the information requested in [this template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n"]}, {"number": 9048, "title": "Add documentation about extra update ops to tf.layers.batch_normalization", "body": "`tf.layers.batch_normalization` uses extra ops for updating its moving_mean and moving_variance variables, but these are added to the graph in such a way that they will not be a dependency of the main training op. Instead, they must be run separately, accessed through the `tf.GraphKeys.UPDATE_OPS` collection. This was explained in the documentation for `tf.contrib.layers.batch_norm`, but this information seems not to have survived the transition to `tf.layers.batch_normalization`.\r\n\r\nLong term, if possible, it would be really nice if we didn't have to do anything extra to get these update operations to work. But in the meantime, here's a patch for the documentation which explains how to make sure the extra ops do get run.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please", "Huh, so it failed on `//tensorflow/python:layers_normalization_test`. Let me run tests locally to see why it failed.", "It's a known flaky test. Merging PR."]}, {"number": 9047, "title": "beta2_power is applied incorrectly in Adam optimizer", "body": "In `adam.py` and in the `ApplyAdam` op, the denominator is effectively:\r\n\r\n```\r\n(tf.sqrt(v_t) + epsilon_t) / tf.sqrt(1 - beta2_power)\r\n```\r\n\r\nHowever, this appears incorrect \u2013 per the paper, the correct EMA adjustment should give:\r\n\r\n```\r\ntf.sqrt(v_t / (1 - beta2_power)) + epsilon_t\r\n```\r\n\r\nOtherwise, when `epsilon_t` is large relative to `tf.sqrt(v_t)`, the effective epsilon used in the denominator is also scaled up by the correction factor, which doesn't match what's in the paper.\r\n\r\nDoes this seem right, or am I missing something here?", "comments": ["@georgedahl @allenlavoie @alextp : Any comments on this (or know whom to redirect to?)", "Just so I'm clear, you do agree that the TensorFlow implementation matches the expression just before Section 2.1, with the \"epsilon hat\" in it? It's just that \"epsilon hat\" is a scaled version of the epsilon in Algorithm 1 in the paper, right?\r\n\r\nIs there a use-case for having the un-scaled epsilon as an option?", "I think this is correct. If we look in the paper https://arxiv.org/pdf/1412.6980.pdf at the bottom of section 2 (Algorithm), it says \"the efficiency of algorithm 1 can, at the expense of clarity, be improved...\" and lists an update like what we are doing, no?\r\n\r\n![image](https://cloud.githubusercontent.com/assets/994930/24822992/edac7cae-1bc7-11e7-9a15-0f4ca0250a5e.png)\r\n", "Ah, the \"epsilon hat\" vs the regular epsilon is what I was missing (though \"epsilon hat\" is never defined that I can see, I assume it is a scaled version). I agree this is probably a bug.", "Oops, yeah \u2013 I think it should be something like:\r\n\r\n```python\r\nepsilon_hat = epsilon_t * tf.sqrt(1 - beta2_power)\r\n```\r\n\r\nOtherwise without this scaling the effective epsilon is scaled by `1. / tf.sqrt(1 - beta2_power)`.\r\n\r\nWith the default settings it probably barely matters (so you get an epsilon of ~3e-7 instead of 1e-8... big deal), but if you follow the recommendations in the comments and e.g. set epsilon to 1 or 0.1, then you effectively will barely update the weights for the first few iterations until `beta2_power` gets closer to 0 \u2013 like if you set `epsilon` to 1, then the effective \"epsilon\" you'd use for the first iteration would be ~31.\r\n\r\nI can't really think of a reason why you'd want to incorrectly scale the epsilon in this case.", "On the other hand, the recommendation in that comment is definitely referring to epsilon_hat being 1 or 0.1, not epsilon (i.e. the person who wrote it was using TensorFlow's implementation of Adam).\r\n\r\n@skywaLKer518: Any thoughts on epsilon vs. epsilon_hat? It looks like you authored that bit of advice on large values of epsilon for Adam.", "It is very possible that the optimizer gets modified after I tested and added the comments (I'm not exactly sure now -- the code is definitely at least re-organized and I do not recall the use of epsilon_hat clearly) so that the comment might not apply any more. But at the time we test it on Inception (summer 2015), we observe much better performance with large epsilon (e.g. 1.0) than the default values like 1e-8 (which sometimes even causes objective divergence if I remember correctly). ", "epsilon and epsilon_hat end up approximately the same after a few thousand iterations anyway \u2013 not using epsilon_hat just means that training will start out very slow at the beginning when using a large value of epsilon.", "Interesting. I'm inclined to keep behavior as-is, since it probably doesn't matter for the on-label use of avoiding short-term numerical instability due to near-zero gradients. In fact we'd need to be careful that a correction didn't introduce numerical issues.\r\n\r\nThe off-label advice for making long-term training more stable has developed for epsilon_hat, so we'd make people re-tune their hyperparameters for questionable benefit by changing the default (i.e. maybe very small updates for the first few thousand iterations is desirable).\r\n\r\nHowever, the documentation should certainly be updated to let people know that it's epsilon_hat rather than epsilon that they're setting.\r\n\r\n@taion: Does that sound reasonable? I'm happy to put together the documentation changes.", "Just checked a few other packages \u2013\r\n\r\n- Keras uses the same incorrect formulation as here: https://github.com/fchollet/keras/blob/7f58b6fbe702c1936e88a878002ee6e9c469bc77/keras/optimizers.py#L389-L400\r\n- PyTorch uses the same incorrect formulation as here: https://github.com/pytorch/pytorch/blob/f17cfe42936310a2e3fd573e1f4dec8c684d4003/torch/optim/adam.py#L68-L72\r\n- Lasagne uses the correct implementation, but in the unoptimized form: https://github.com/Lasagne/Lasagne/blob/45bb5689f0b2edb7114608e88305e8074d29bbe7/lasagne/updates.py#L620-L622\r\n\r\nI mean, I don't know. The correct version seems unlikely to cause issues for the on-label case, or people using Lasagne would have had problems.\r\n\r\nI don't really want to train an InceptionNet from scratch to see what this does for the off-label case; I will just note that when I tried using a larger epsilon a while ago, my model seemed to not train at all even when I used a higher learning rate, which would suggest that the current implementation makes things worse in the off-label case of using a higher epsilon.", "Thank you for taking a look at other frameworks! That's an interesting list.\r\n\r\nIn terms of numerical issues, I'm particularly worried about float16 users. AdamOptimizer already requires tuning the default epsilon_hat of 1e-8 to at least 1e-7 to avoid errors. If we naively divided it by anything more than 3, it would underflow. Specifying epsilon_hat directly at least makes reasoning about precision issues a bit easier.", "Oh! I didn't think of the float16 case.\r\n\r\nMaybe the first-best would be to rename the current `epsilon` to `epsilon_hat` (with a deprecation notice for the moment), then possibly add a proper `epsilon` later on?", "The current formulation is sufficiently useful for numerical stability that we'll want to keep it around. I'll do the documentation fix and mark this as closed once that propagates. Feel free to open a feature request for a second epsilon parameter (\"epsilon_nohat\"?), or work on a pull request, but I don't think it's going to be a priority without experimental evidence that it's useful.", "Sounds good, thanks.", "Fix is submitting, should be synced within a day or so. Thank you for the report!", "Thanks!"]}, {"number": 9046, "title": "Merge pull request #1 from tensorflow/master", "body": "update from tensorflow", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "The commit in this PR is empty. Looks like a misoperation or something similar. Closing.", "Could you follow the instructions to sign the CLA? Thanks.", "Actually, as @caisq pointed out, the changeset is empty. Feel free to open a new PR once ready."]}, {"number": 9045, "title": "[FeatureRequest ] Add sparse_column_with_cat_prob to tensorflow.contrib.layers.python.layers.feature_column_ops.py", "body": "Hi there,\r\n\r\nworking on deep learning for recommender systems I came across the Google Wide and Deep model (see [1] and [2]).\r\n\r\n## Problem\r\nIn my application context there are users and items as well as the interactions between those entities. Furthermore there are item and user features, that are both, continuous and categorical. Here, we have **_item_features = user_features_** (user_features derived from user interactions).\r\n\r\nA big problem is the **representation of categorical user features as a result from their interaction with different items with respectively different item features.** (Some intuition to be found below)\r\n\r\n- tf.contrib.layers.python.layers.**sparse_column_with_keys**\r\n- tf.contrib.layers.python.layers.**sparse_column_with_hash_bucket**\r\n\r\nallow to define or induce keys for categorical features that are then one-hot encoded behind the scenes - as far as I understood\r\nThis works for items that can just have one feature value, but users can have a multivalent preference that should be reflected by a categorcial probability distribution (cpd).\r\n\r\nTo capture this result we need TF to capture this cpd and compare it with the one-hot-encoded movie features. The latter is provided internally, but for realizing user profiles I couldn't find proper means meaning that within tensorflow.contrib.layers.python.layers there are no sparse columns providing this possibility which in fact is petty relevant.\r\n\r\n## Proposed Solution\r\nAdd following method feature column:\r\nsparse_column_with_cat_prob(column_name, value_prob_dict, counterpart)\r\n\r\n- **column_name**: see sparse_column_with_keys for example\r\n- **value_prob_dict**: dictionary containing feature values (value) and associated probabilities (prob)\r\n- **counterpart**:  eventually, name of the one-hot-encoded sparse column this one is refering to\r\n\r\nThis would allow for building user features for categorcial user interaction data, especially within recommendation contexts.\r\n\r\n## Intuition\r\nTo give some intuition see the following example from a movie recommendation context:\r\nuser 1 interacts with movies A, B, C, D, and E\r\nmovie_features: genre {Romance, Action}, \r\n\r\n```\r\n   movie_id movie_genre  movie_length\r\n0         0     Romance           120\r\n1         1      Action            95\r\n2         2      Action           130\r\n3         3     Romance           150\r\n4         4     Romance           110\r\n```\r\n```\r\n   user_id user_genre user_length\r\n0        0    unknown      unknown\r\n1        1    unknown      unknown\r\n```\r\n**Observed interactions:**\r\n```\r\n   user_id  movie_id\r\n0        0         0\r\n1        0         1\r\n2        1         1\r\n3        1         2\r\n4        1         3\r\n5        1         4\r\n```\r\nMerge, group by size and calculation the shares produces:\r\n```\r\nuser_id  movie_genre\r\n0        Action         0.333333\r\n         Romance        0.666667\r\n1        Action         0.500000\r\n         Romance        0.500000\r\n```\r\nSo, as we can observe user 0 rather prefers Romance movies whereas user 1 is indifferent between genres. As a learning outcome user 0 should be recommended more Romance than Action movies, analogously for user 1.\r\n\r\nThese should be handed over to sparse_column_with_cat_prob to solve this problem.\r\n\r\nResources\r\n[1] https://arxiv.org/abs/1606.07792\r\n[2] https://www.tensorflow.org/tutorials/wide_and_deep", "comments": ["@ispirmustafa : Any comments?", "let me clarify what I understood. You have some categorical features and associated weights. Instead of an one-hot/indicator, you need corresponding weight replaces 'ones'.\r\nCould you please check 'weighted_sparse_column'?", "Unfortunately `weighted_sparse_column `doesn't work, this would weight the features, for example genre vs. length.\r\nWhat I need is to specify all values a feature can have by a probability distribution, like one-hot encoding can also be interpreted as a probability distribution - just that a single value of the feature has 100%", "So, are there any further questions, comments or suggestions?\r\nI would just follow up and start building a feature column that does the required job and trigger a pull request for it when I'm finished. Does that sound reasonable?", "@ispirmustafa let me know if this one should be assigned to you.", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 9044, "title": "hang in google/protobuf/pyext/_message.so at exit", "body": "This is TensorFlow 1.0.1 installed via pip.\r\nIt runs via an embedded CPython (libpython).\r\n\r\nSometimes (maybe 30% of my runs) it hangs in `Py_Finalize()`, and I see this backtrace:\r\n\r\n```\r\n/work/asr2/zeyer/sprint-executables/20160902.235443.fad8965.linux-x86_64-standard/Flf/flf-tool.linux-intel-standard(_ZN17AssertionsPrivate15safe_stackTraceEi+0x21)[0xc5b891]\r\n/work/asr2/zeyer/sprint-executables/20160902.235443.fad8965.linux-x86_64-standard/Flf/flf-tool.linux-intel-standard[0xc5b8ef]\r\n/u/zeyer/tools/glibc217/libpthread.so.0(+0x113d0)[0x2b6d89bad3d0]\r\n/u/zeyer/tools/glibc217/libpthread.so.0(raise+0x29)[0x2b6d89bad2a9]\r\n/u/zeyer/py-envs/py2-ubuntu16/local/lib/python2.7/site-packages/faulthandler.so(+0x3198)[0x2b6dc2372198]\r\n/u/zeyer/tools/glibc217/libpthread.so.0(+0x113d0)[0x2b6d89bad3d0]\r\n/u/zeyer/py-envs/py2-ubuntu16/local/lib/python2.7/site-packages/google/protobuf/pyext/_message.so(+0xaa943)[0x2b6dc14f0943]\r\n/usr/lib/x86_64-linux-gnu/libpython2.7.so.1.0(+0x160f6b)[0x2b6d8b23af6b]\r\n/usr/lib/x86_64-linux-gnu/libpython2.7.so.1.0(+0xc8f0e)[0x2b6d8b1a2f0e]\r\n/usr/lib/x86_64-linux-gnu/libpython2.7.so.1.0(+0x15d747)[0x2b6d8b237747]\r\n/usr/lib/x86_64-linux-gnu/libpython2.7.so.1.0(PyDict_SetItem+0x7b)[0x2b6d8b23becb]\r\n/usr/lib/x86_64-linux-gnu/libpython2.7.so.1.0(_PyModule_Clear+0xb5)[0x2b6d8b278565]\r\n/usr/lib/x86_64-linux-gnu/libpython2.7.so.1.0(PyImport_Cleanup+0x437)[0x2b6d8b2280e7]\r\n/usr/lib/x86_64-linux-gnu/libpython2.7.so.1.0(Py_Finalize+0xfe)[0x2b6d8b1fed9e]\r\n/work/asr2/zeyer/sprint-executables/20160902.235443.fad8965.linux-x86_64-standard/Flf/flf-tool.linux-intel-standard(_ZN6Python11Initializer19AtExitUninitHandlerEv+0x2e)[0xff80de]\r\n/u/zeyer/tools/glibc217/libc.so.6(+0x39fe8)[0x2b6d8bc39fe8]\r\n/u/zeyer/tools/glibc217/libc.so.6(+0x3a035)[0x2b6d8bc3a035]\r\n/u/zeyer/tools/glibc217/libc.so.6(__libc_start_main+0xf7)[0x2b6d8bc20837]\r\n/work/asr2/zeyer/sprint-executables/20160902.235443.fad8965.linux-x86_64-standard/Flf/flf-tool.linux-intel-standard[0x7d6991]\r\n```\r\nor with GDB:\r\n```\r\n(gdb) bt full\r\n#0  0x00002b6dc14f0943 in std::tr1::_Hashtable<google::protobuf::DescriptorPool const*, std::pair<google::protobuf::DescriptorPool const* const, google::protobuf::python::PyDescriptorPool*>, std::allocator<std::pair<google::protobuf::DescriptorPool const* const, google::protobuf::python::PyDescriptorPool*> >, std::_Select1st<std::pair<google::protobuf::DescriptorPool const* const, google::protobuf::python::PyDescriptorPool*> >, std::equal_to<google::protobuf::DescriptorPool const*>, google::protobuf::hash<google::protobuf::DescriptorPool const*>, std::tr1::__detail::_Mod_range_hashing, std::tr1::__detail::_Default_ranged_hash, std::tr1::__detail::_Prime_rehash_policy, false, false, true>::erase (\r\n    __k=@0x7ffd1bbea740: 0x8269780, this=0x2b6dc1826e40 <google::protobuf::python::descriptor_pool_map>)\r\n    at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/tr1/hashtable.h:1041\r\n        __slot = <optimized out>\r\n        __saved_slot = <optimized out>\r\n        __code = 136746880\r\n        __n = 0\r\n        __result = 0\r\n#1  google::protobuf::python::cdescriptor_pool::Dealloc (self=0x2b6dc0d86880)\r\n    at google/protobuf/pyext/descriptor_pool.cc:152\r\nNo locals.\r\n#2  0x00002b6d8b23af6b in ?? () from /usr/lib/x86_64-linux-gnu/libpython2.7.so.1.0\r\nNo symbol table info available.\r\n#3  0x00002b6d8b1a2f0e in ?? () from /usr/lib/x86_64-linux-gnu/libpython2.7.so.1.0\r\nNo symbol table info available.\r\n#4  0x00002b6d8b237747 in ?? () from /usr/lib/x86_64-linux-gnu/libpython2.7.so.1.0\r\nNo symbol table info available.\r\n#5  0x00002b6d8b23becb in PyDict_SetItem () from /usr/lib/x86_64-linux-gnu/libpython2.7.so.1.0\r\nNo symbol table info available.\r\n#6  0x00002b6d8b278565 in _PyModule_Clear () from /usr/lib/x86_64-linux-gnu/libpython2.7.so.1.0\r\nNo symbol table info available.\r\n#7  0x00002b6d8b2280e7 in PyImport_Cleanup () from /usr/lib/x86_64-linux-gnu/libpython2.7.so.1.0\r\nNo symbol table info available.\r\n#8  0x00002b6d8b1fed9e in Py_Finalize () from /usr/lib/x86_64-linux-gnu/libpython2.7.so.1.0\r\nNo symbol table info available.\r\n#9  0x0000000000ff80de in Python::Initializer::AtExitUninitHandler() ()\r\nNo symbol table info available.\r\n#10 0x00002b6d8bc39fe8 in ?? () from /u/zeyer/tools/glibc217/libc.so.6\r\nNo symbol table info available.\r\n#11 0x00002b6d8bc3a035 in exit () from /u/zeyer/tools/glibc217/libc.so.6\r\nNo symbol table info available.\r\n#12 0x00002b6d8bc20837 in __libc_start_main () from /u/zeyer/tools/glibc217/libc.so.6\r\nNo symbol table info available.\r\n#13 0x00000000007d6991 in _start ()\r\nNo symbol table info available.\r\n```\r\n\r\nI.e. it happens in `_PyModule_Clear`, and then inside `google/protobuf/pyext/_message.so`, that's why I think this is TF related.\r\n\r\nIn the case when it does not hang, I see this output:\r\n\r\n```\r\nException AttributeError: AttributeError(\"'NoneType' object has no attribute 'raise_exception_on_not_ok_status'\",) in <bound method Session.__del__ of <tensorflow.python.client.session.Session object at 0x2afd625b12d0>> ignored\r\n```\r\n\r\n", "comments": ["There isn't enough information in this report to be able to help - for example what exact sequence of commands and/or code causes this problem.\r\n\r\nThat said, this seems to be a case where you're using TensorFlow in a Python interpreter embedded in another program. So it also isn't clear that this is a bug or if it is something esoteric about your use of the embedded interpreter.\r\n\r\nThis question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "I'm not sure if this is related to running the Python interpreter embedded in another program, as it's basically the same what CPython itself does. Anyway, I posted it [on StackOverflow here](http://stackoverflow.com/questions/43286726/hang-in-google-protobuf-pyext-message-so-at-exit)."]}, {"number": 9043, "title": "flatten inputs in tf.layers.dense", "body": "In the document of `tf.layers.dense` is writen that: \r\n\r\n> Note: if the inputs tensor has a rank greater than 2, then it is flattened prior to the initial matrix multiply by kernel.\r\n\r\nbut inputs is not flattened.\r\n\r\nI modified that to flatten inputs.", "comments": ["Can one of the admins verify this patch?", "@fchollet could you take a look?", "Not sure if that the original docstring is correct.  Nor that this PR fixes\nanything; more likely that it restricts the behavior of tf.layers.dense.\n\nthe current behavior is:\n\ninput is [a, b, c, ..., m], and num_units = 10\n\noutput is: [a, b, c, ..., 10]\n\n(here a, b, c, ..., need not be known at graph build time)\n\nthis PR seems to break that guarantee.\n\nOn Mon, Apr 10, 2017 at 11:18 AM, drpngx <notifications@github.com> wrote:\n\n> @fchollet <https://github.com/fchollet> could you take a look?\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9043#issuecomment-293034585>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimxxodGnvSAzXRr-udRwNhJ8nVsERks5runKNgaJpZM4M2438>\n> .\n>\n", "Eugene is right, the goal with the `Dense` layer is that we apply a densely-connected layer on the last axis of the input. This allows e.g. time-distributed densely-connected transformations, where the same transformation is applied at every timestep of an input. \r\n\r\nSo we don't want to flatten the output. And besides this overarching conceptual issue, there are also many technical problems with this PR.\r\n\r\nIf the current docstring is ambiguous, then we should update it to make the layer behavior clearer. But the behavior as currently implemented is the proper one.\r\n", "Hi, all, I searched everywhere but cannot find the difference between ft.layers.dense and tf.contrib.layers.fully_connected. \r\n\r\nCould you please point me out their difference?\r\n\r\nAnd if dense only applies to the last axis, how can we do a real full connections of elements from all dims like in Cafffe? In Caffe, input is [N, b, c, ..., m], and num_units = 10, then output is: [N,10]", "> And if dense only applies to the last axis, how can we do a real full connections of elements from all dims like in Cafffe?\r\n\r\nYou could either flatten your input then apply a `Dense` layer, or write a custom layer that does the same.\r\n\r\n> Could you please point me out their difference?\r\n\r\nThey are meant to be the same. They do have a slightly different API and a different implementation. In code going forward, you should use `tf.layers` rather than `tf.contrib.layers`, since `tf.layers` is guaranteed to be API-stable (as part of the 1.0 API) and to stay there for the long term.", "@fchollet Thanks. Can we add a parameter to Dense (full_connected) layer to make it caffe mode for all axis but the first one? You see it's a very often meeted feature as the last layer.", "@joyousrabbit it's really simple to just flatten your inputs... \r\n\r\nIn Keras:\r\n\r\n```python\r\nx = keras.layers.Flatten()(x)\r\nx = keras.layers.Dense(units)(x)\r\n```", "@fchollet Yes, I see in tensorflow, there is a same layer do the flatten. Thanks a lot.\r\ntf.contrib.layers.flatten"]}, {"number": 9042, "title": "Compiling a custom Op - Eigen including-loop", "body": "### System Info.\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*: Yes\r\n- *TensorFlow installed from (source or binary)?*: binary\r\n- *TensorFlow version*: latest\r\n- *Bazel version (if compiling from source)*:\r\n- *CUDA/cuDNN version*: 8.0/5.1\r\n- *GPU Model and Memory*: Quadro k1100m\r\n- *Exact command to reproduce*: trying to build\r\n\r\n### The problem\r\n\r\nI'm tyring to build the code in\r\nhttps://github.com/davidstutz/tensorflow-cpp-op-example\r\nfor Windows. The Windows version of Tf does not install with the source code so I cloned the repo and added the path to CMake. I then executed CMake as described in\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake\r\nYet the problem seems to be that the Eigen files don't do anything but re-including themselves. \r\n\r\nWere they not meant to be included? Am I using some wrong flags that result in those imports? Other ideas? suggestions?\r\n\r\nCheers,\r\n", "comments": ["@idofr : I'm not sure I understand what you mean by \"Yet the problem seems to be that the Eigen files don't do anything but re-including themselves.\" Could you please elaborate on what the problem is and the exact sequence of steps to reproduce it?\r\n\r\nSeparately, @gunan @av8ramit @mrry : Seems like the Windows pip package doesn't include the header files so the technique in https://www.tensorflow.org/extend/adding_an_op doesn't apply. Should we be including the header files now (or in release 1.2 onwards) since it seems that #8217 allows custom op libraries to be loaded in Windows?", "@asimshankar That would make sense. Is there a canonical list of what header files are included? Given a list of filenames or globs, it should be fairly easy to add them to the PIP directory.\r\n\r\n/cc @guschmue FYI.", "sure, totally for this. I think the list of header files should be the same as the one in the linux pip.\r\nThere is maybe one more thing needed: building the dll makes a few compiler flags/defines and I wonder if we should **generate** a build.bat or a CMakeLists.txt template that we include in the pip so people don't need to deal with it ... should not be very hard to create this. ", "@avr8amit lets block 1.1 on the addition of missing headers into windows\npip package.\n\nOn Apr 7, 2017 11:17 AM, \"Guenther Schmuelling\" <notifications@github.com>\nwrote:\n\nsure, totally for this. I think the list of header files should be the same\nas the one in the linux pip.\nThere is maybe one more thing needed: building the dll makes a few compiler\nflags/defines and I wonder if we should *generate* a build.bat or a\nCMakeLists.txt template that we include in the pip so people don't need to\ndeal with it ... should not be very hard to create this.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\n<https://github.com/tensorflow/tensorflow/issues/9042#issuecomment-292612639>,\nor mute the thread\n<https://github.com/notifications/unsubscribe-auth/AHlCOboRHXl3Mgj_vvf35LJs8yZqSDXGks5rtn3OgaJpZM4M2yhY>\n.\n", "@mrry, @av8ramit : The list of header files to include would be defined by the bazel target [`//tensorflow/tools/pip_package:include_headers`](https://github.com/tensorflow/tensorflow/blob/63b2f99/tensorflow/tools/pip_package/BUILD#L28). \r\n\r\nPerhaps we'll have to generate the list for CMakeLists.txt once and have some bazel test that checks that the two are always in sync. I'm sure @av8ramit will know what to do :)", "Hi, sorry for the late respnose.\r\n\r\nThank you all so much for the replies, and yes, including the headers in the pip package would eventually save a lot of time, since the compilation takes a while. It would also probably make life a bit easier for everyone, as it would make sure that the same flags and configs are used in build builds.\r\n\r\n@asimshankar \r\nI mean something similar to \r\nthird_party/eigen3/unsupported/Eigen/CXX11/Tensor\r\n( https://raw.githubusercontent.com/tensorflow/tensorflow/master/third_party/eigen3/unsupported/Eigen/CXX11/Tensor )\r\n\r\nIt does some minimal definitions and mostly just re-includes itself.\r\n\r\n@vit-stepanovs was kind enough to reply to my email and shared the following link with me\r\nhttps://gist.github.com/guschmue/2908e4411edc2faef6ddfe87a2ce4a1d\r\nThis seems to be exactly what I need (thanks @guschmue). Cmakes manages to create the VS project now and I'll compile either later on today or tomorrow at some point. I'll let you know how it went.\r\nIn case it's working though, it'd probably be a good idea to include this additional file into the main repo (assuming @guschmue won't mind)", "Ah, the equivalent of \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/adding_an_op/BUILD\r\nfor cmake ... sure. We need to figure if this should be a version that depends on the source tree or if this should be standalone version that only depends on the python package once the header files are include in the python wheel.\r\n\r\nFor your https://raw.githubusercontent.com/tensorflow/tensorflow/master/third_party/eigen3/unsupported/Eigen/CXX11/Tensor issue: there should be a ifdef to prevent recursion but I think the main issue is that it is trying to include another Tensor from external/eigen_archive/unsupported/Eigen/CXX11/Tensor which is extracted during build. Maybe that 2nd Tensor file is not there, for example the tree wasn't build. \r\n\r\n", "That Tensor issue was actually the reason I guessed to begin with that my flags were wrong. It basically seems like the file should not be included anyways. But yeah, I also added a few ifdef's there, just didn't submit a pull request for such a minor issue which probably doesn't affect the community anyways.", "Is there anything special to do after the compilation to include the new Op?\r\nI'm using the Ackermann_ops example, compiled the headers and then the Op and then tried to run the ackermann_test.py script.\r\nIt returns the following error though\r\n\r\n`\r\nERROR: testBasic (__main__.AckermannTest)\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/tf_compile/tensorflow/tensorflow/user_ops/ackermann_test.py\", line 30, in testBasic\r\n    ackermann = tf.load_op_library(library_filename)\r\n  File \"C:\\Users\\Me\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\load_library.py\", line 64, in load_op_library\r\n    None, None, error_msg, error_code)\r\ntensorflow.python.framework.errors_impl.NotFoundError: C:\\tf_compile\\tensorflow\\tensorflow\\user_ops\\ackermann_ops.so not found\r\n`\r\n\r\nNeedless to say that the file is in the directory.\r\nThe actual Tf version I'm using was installed directly from the pip package", "just tried this on my box and had the same issue. Silly mistake: \r\nmy example cmake copied to ackermann_ops.so where the test app was looking for ackermann_op.so. After fixing that it works on my box.\r\n", "That was the first thing I've noticed but it didn't make a difference.\r\nDo you link the entire path or just the file name?", "just tried this on a fresh master. After the the normal build is done, I build with user op with\r\nMSBuild /p:Configuration=RelWithDebInfo /p:BuildProjectReferences=false ackermann_ops.vcxproj\r\n\r\npython \\src\\tensorflow\\tensorflow\\user_ops\\ackermann_test.py\r\nand\r\ncd python \\src\\tensorflow\\tensorflow\\user_ops\\\r\npython ackermann_test.py\r\n\r\nboth work ok.\r\n\r\n", "@idofr : Are you able to build with the instructions from @guschmue ?", "Building was not the problem, including the .so file was the issue", "@idofr Did you use the resource loader? Did you try to load it by hand?", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.", "I have added the functionality to include headers in the cmake build."]}, {"number": 9041, "title": "import error in ipython but not in python", "body": "-*I have installed TensorFlow installed with virtualenv Linux(Ubuntu 16.10) with this codes*\r\n```shell\r\n sudo apt-get install python-pip python-dev python-virtualenv\r\n(tensorflow)$ pip install --upgrade tensorflow      # for Python 2.7\r\n```\r\n- *TensorFlow version 1.0.1\r\n- *Command to reproduce the error after installing the module through pip*\r\n```python\r\nimport tensorflow as tf\r\n```\r\n\r\n### I can import tensorflow module in python in my virtualenv but i can't import it in ipython\r\n\r\n### Logs from ipython\r\n```ipython\r\nPython 2.7.12+ (default, Sep 17 2016, 12:08:02) \r\nType \"copyright\", \"credits\" or \"license\" for more information.\r\n\r\nIPython 5.3.0 -- An enhanced Interactive Python.\r\n?         -> Introduction and overview of IPython's features.\r\n%quickref -> Quick reference.\r\nhelp      -> Python's own help system.\r\nobject?   -> Details about 'object', use 'object??' for extra details.\r\n\r\nIn [1]: import tensorflow as tf\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-41389fad42b5> in <module>()\r\n----> 1 import tensorflow as tf\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py in <module>()\r\n     22 \r\n     23 # pylint: disable=wildcard-import\r\n---> 24 from tensorflow.python import *\r\n     25 # pylint: enable=wildcard-import\r\n     26 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py in <module>()\r\n     70 for some common reasons and solutions.  Include the entire stack trace\r\n     71 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 72   raise ImportError(msg)\r\n     73 \r\n     74 # Protocol buffers\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 61, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\nImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n", "comments": ["This suggests some configuration issue with your virtualenv and ipython. The error you're getting suggests that it's trying to load CUDA, which means it is using the `tensorflow-gpu` pip package while your `pip install` command suggested it was installing the CPU-only version of TensorFlow.\r\n\r\nSince this appears to be an issue with your virtualenv/ipython setup and not with TensorFlow, I'm going to close this out. You might also want to take a look at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/install/install_linux.md#common-installation-problems\r\n\r\nHope that helps.\r\n", "Thanks @asimshankar this was helpfull and my problem is solved"]}, {"number": 9040, "title": "set save_model_secs=60 in mnist_replica.py", "body": "This would make restoring from a checkpoint more easily. The default value for save_model_secs is 600 which is relatively too long for this tutorial example model.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "Closing PR due to no author response. Feel free to re-open it if you'd like to work on it further."]}, {"number": 9039, "title": "Who can help me fix the problem of 'Net Crash' ?", "body": "I am training a convolutional network, but suddenly that happened. I retry for 3 times, same scene happened. The network looks as if it crashed ! Who can help me fix this problem?\r\nThe scene looks like the following:\r\nEpoch:100 training_accuracy:0.975 validation_accuracy:0.94242\r\nEpoch:101 training_accuracy:0.075 validation_accuracy:0.103736\r\nEpoch:102 training_accuracy:0.075 validation_accuracy:0.103736", "comments": ["I've seen some similar behavior during my training and my issue was similar to #1122. Can you try tuning your `weight_decay` parameters in the batch normalization layer and see if that helps?", "There isn't enough information in the report to be able to help. I'd suggest posting to [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) with more details as there is a larger community that monitors that and might be able to help."]}, {"number": 9038, "title": "Can't install tensorflow successfully", "body": "NOTE: Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow.\r\n\r\n### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*: No\r\n- *TensorFlow installed from (source or binary)?*: binary\r\n- *TensorFlow version*:1.0\r\n- *Bazel version (if compiling from source)*: \r\n- *CUDA/cuDNN version*:NA\r\n- *GPU Model and Memory*:HD5500 /  intrel core i5-5200\r\n- *Exact command to reproduce*:  NA\r\n\r\n### Describe the problem clearly\r\nI try to install tensorflow with Anaconda in my win10 following the instructions on [install_windows]https://www.tensorflow.org/install/install_windows).  When i move to step4:\r\nI type the code\r\n `pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.0.1-cp35-cp35m-win_amd64.whl`\r\n\r\nit fails and shows\r\n`tensorflow_gpu-1.0.1-cp35-cp35m-win_amd64.whl is not a supported wheel on this platform.`\r\n\r\nHow can I fix it?\r\n\r\n### Source Code / Logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full-traceback. Large logs and files should be attached. Try to reproducible test-case code the bare-minimum necessary to generate the problem\r\n", "comments": ["have u tried conda install, and granted administration mode", "@Mulf Hi, sorry you're facing issues. Could you please share your Python version? The log would also be helpful.\r\n", "I have faced the same problem when I tried to install TensorFlow with Anaconda 4.3 (the latest version) which uses Python 3.6. Unfortunately this Python version is not supported by TensorFlow on Windows. I think the easiest solution is to uninstall Anaconda 4.3 and install Anaconda 4.2. (from https://repo.continuum.io/archive/index.html) Then install TensorFlow again. It worked for me. Hope it helpls", "My version is 3.6\n\nAfter I type Python,  the command window gives me:\nPython 3.6.0 |Anaconda 4.3.1 (64-bit)| (default, Dec 23 2016, 11:57:41)\n[MSC v.1900 64 bit (AMD64)] on win32\n\n2017-04-07 12:05 GMT-04:00 Siby Jose Plathottam <notifications@github.com>:\n\n> I have faced the same problem when I tried to install TensorFlow with\n> Anaconda 4.3 (the latest version) which uses Python 3.6. Unfortunately this\n> Python version is not supported by TensorFlow on Windows. I think the\n> easiest solution is to uninstall Anaconda 4.3 and install Anaconda 4.2.\n> (from https://repo.continuum.io/archive/index.html) Then install\n> TensorFlow again. It worked for me. Hope it helpls\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9038#issuecomment-292578004>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AVvyT3QMGCGk1rqT3KSGFiwpHu-Pn4b1ks5rtl68gaJpZM4M2gXX>\n> .\n>\n", "@Mulf as you can observe, the wheel you are trying to download to install TensorFlow is compatible with Python 3.5 (`cp35-cp35m-win`) therefore the error telling you it is not compatible. \r\n\r\nThere are a couple of options:\r\n\r\n1. Create a conda environment for Python=3.5 and install TF on it\r\n2. Download a previous version of Anaconda with Python 3.5\r\n3. Build from source\r\n\r\nThe simplest is the first but you may choose what works best for you.\r\nIf you have any further doubts feel free to ask. If not this issue can be closed.\r\n", "Yup, TensorFlow on Windows does not support Python 3.6 yet (see #6999 for details).\r\nYou'll need a Python 3.5 environment.\r\n\r\nClosing this out as a duplicate of #6999.\r\n\r\nThanks!"]}, {"number": 9037, "title": "Modify seq2seq.py", "body": "Modify the tf.contrib.legacy_seq2seq.embedding_attention_seq2seq so that it allows user to provide a second cell for the encoding step, and if it's None then to fallback on the deepcopy.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "There was a download error, perhaps transient.\r\n\r\nJenkins, test this please.", "Git fetch error, trying again.\r\n\r\nJenkins, test this please.", "BTW, did you mean to send this to `1.1` or `master`?", "These changes were based on 1.1, so I wanted to send it to 1.1", "`1.1` is a branch for the TF team to cherry-pick into. If you think that it's a feature that should go into the main tensorflow, you should write into `master`.\r\n\r\n/cc: @yifeif ", "So if I want to do that should I close this PR and create a new PR into master?", "That would be great @oxwsds.", "Well, thank you!", "#9122 Please have a check."]}, {"number": 9036, "title": "Fixing 36899400.", "body": "Updating version suffix in version.h and updater script.", "comments": []}, {"number": 9035, "title": "can't copy tensorflow/python/pywrap_tensorflow_internal.py", "body": "```\r\nporter@fattire:~/Projects/tensorflow$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\nThu Apr 6 17:10:48 PDT 2017 : === Using tmpdir: /tmp/tmp.kCYPVwxjU5\r\n~/Projects/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles ~/Projects/tensorflow\r\n~/Projects/tensorflow\r\n/tmp/tmp.kCYPVwxjU5 ~/Projects/tensorflow\r\nThu Apr 6 17:10:49 PDT 2017 : === Building wheel\r\nerror: can't copy 'tensorflow/python/pywrap_tensorflow_internal.py': doesn't exist or not a regular file\r\n```\r\n\r\nThis happens after building tensorflow with bazel and running the binary build by bazel:\r\n\r\n`bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg`\r\n\r\npywrap_tensorflow_internal.py doesn't exist in the source directories:\r\n\r\nTensorflow commit f8dce81\r\nBazel 0.4.5\r\nCUDA release 8.0, V8.0.26, cuDNN 5105\r\nGPU: NVIDIA Titan 6GB", "comments": ["I met the same problem when installing r1.1.\r\n`bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package ` was completed sucessfully. \r\nThen I run `find . -name \"pywrap_tensorflow_internal.py\"` in the tf root dir, got nothing. \r\nThere's another relevant issue https://github.com/tensorflow/tensorflow/issues/8078 ", "A workaround:\r\ncopy `./tensorflow/python/pywrap_tensorflow.py`  to `./bazel-bin/tensorflow/python/pywrap_tensorflow_internal.py`. \r\nThen I got a different error `/tensorflow/python/gen_resource_variable_ops.py doesn't exist or not a regular file ` \r\n\r\nI think it's likely because some bazel compiling rules may be missed in `r1.1`.   ", "+1, experiencing the same issue\r\n\r\nTensorflow commit d48ef8f6b743c715e8209e7faf4d2c46ace57046\r\nBazel 0.4.5\r\nCUDA release 8.0, V8.0.26, cuDNN 5105\r\nGPU: NVIDIA GTX 1080\r\n\r\nThe file in question clearly exists: \r\n\r\n>[zamparol@server tensorflow]$ ls -lh tensorflow/python/pywrap_tensorflow.py\r\n-rw-r--r-- 1 zamparol cllab 2.5K Apr 25 15:47 tensorflow/python/pywrap_tensorflow.py\r\n\r\n", "FWIW, I resolved my issue (sol'n provided towards the end of #1334), I'm not sure what the problem is but I was able to build a wheel by doing the following from the root of the repo:\r\n\r\n```\r\ngit submodule update --init\r\nbazel clean --expunge\r\nbazel build -c opt --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package ~/utils/tensorflow_pkg\r\n```\r\n Maybe if one of the bazel build steps fails, you need to do `bazel clean --expunge`, and try again?\r\n\r\n", "Does that solve your issue @CharlesShang?", "Unfortunately, It failed again.\r\nThe same error mssg was prompted out. \r\nI'm on commit 1ec6ed5. ", "I had a similar issue and was able to fix it by removing the VERBS support from the configure. This is what (I guess) happened:  `bazel build` with the VERBS support actually failed, but I didn't notice it and it still created the file of 'bazel-bin/tensorflow/tools/pip_package/build_pip_package'. And this file gives me the error.\r\n", "@jiangxu87 Could you please give a more detailed howto? ", "@CharlesShang \r\n\r\nWas your error solved?\r\nI have the same error. please help\r\n", "No. \r\n@nishalpereira, I've installed the pre-built binary instead. ", "@CharlesShang \r\nThanks for your reply.\r\nCan you tell me how to install the pre-built binary.\r\n", "This problem is random. \r\nOn another machine (same OS, same cuda version, but different GPU), tf can be built successfully. So I just copy the built file to my machine, install it..", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing due to inactivity. If it's still an issue let us know and I'll reopen."]}, {"number": 9034, "title": "Computing 2nd-order tf.gradients of tensors throws Exception when used with batch_norm", "body": "### Describe the problem clearly\r\n\r\nIf the `updates_collections` of a `batch_norm` layer is set other than `tf.GraphKeys.UPDATE_OPS`, it is no longer possible to compute 2nd-order `tf.gradients` with respect to the weights of a `fully_connected` layer.\r\n\r\np.s. It is okay when `updates_collections` is set as `tf.GraphKeys.UPDATE_OPS`. I think `updates_collections` should not affect the ability to compute gradients?\r\n\r\n### Environments\r\n\r\n- Ubuntu 16.04 64bit\r\n- Python 3.6.0 |Anaconda 4.3.1 (64-bit)| (default, Dec 23 2016, 12:22:00) \r\n- [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\r\n- tensorflow-gpu 1.0.1 installed from pip\r\n- libcublas.so.8.0, libcudnn.so.5, libcufft.so.8.0,  libcuda.so.1, libcurand.so.8.0\r\n\r\n### Source Code\r\n```python\r\nimport tensorflow as tf\r\n\r\nwith tf.Session() as sess:\r\n    X = tf.placeholder(tf.float32, [None, 2])\r\n    is_training = tf.placeholder(tf.bool, [], name='is_training')\r\n\r\n    outputs = tf.contrib.layers.fully_connected(inputs=X, num_outputs=1)\r\n    outputs = tf.contrib.layers.batch_norm(\r\n        inputs=outputs,\r\n        is_training=is_training,\r\n        updates_collections='bad_collections')\r\n    # get gradients of X with respect to outputs values\r\n    grads = tf.gradients(outputs, [\r\n        X,\r\n    ])[0]\r\n    bad_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\r\n    # get gradients of weights with respect to gradients of X\r\n    bad_grads = tf.gradients(grads, bad_vars) # this line\r\n```\r\n\r\n## Logs\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 18, in <module>\r\n    bad_grads = tf.gradients(grads, bad_vars)\r\n  File \"$HOME/anaconda2/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 474, in gradients\r\n    out_grads[i] = control_flow_ops.ZerosLikeOutsideLoop(op, i)\r\n  File \"$HOME/anaconda2/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1303, in ZerosLikeOutsideLoop\r\n    pred = op_ctxt.pred\r\nAttributeError: 'NoneType' object has no attribute 'pred'\r\n```", "comments": ["Hi @shaform, this seems a more suitable question for StackOverflow", "Hi @Carmezim, thanks for the reply. I thought this was a bug since I\r\nbelieved that changing updates_collections shouldn't affect the ability to\r\ncompute gradients. Your comment seems to suggest that the exception is\r\ntotally expected and therefore shouldn't be fixed. (i.e. It is not a bug.)\r\nCould you share the rationale behind this, please? Thanks.\r\n\r\nOn Thu, Apr 13, 2017, 11:09 AM Adriano Carmezim <notifications@github.com>\r\nwrote:\r\n\r\n> Hi @shaform <https://github.com/shaform>, this seems a more suitable\r\n> question for StackOverflow\r\n>\r\n> \u2014\r\n> You are receiving this because you were mentioned.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/9034#issuecomment-293766070>,\r\n> or mute the thread\r\n> <https://github.com/notifications/unsubscribe-auth/AAWaRLYuEWp8Wgp5zFXdDzqYn0IXT_ayks5rvZHNgaJpZM4M2S1S>\r\n> .\r\n>\r\n", "I've updated the description and title to make it more clear.", "Looks like it's because when custom `updates_collections` is set, legacy BN implementation would be used https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/contrib/layers/python/layers/layers.py#L536. It's likely that this implementation would cause problems when one attempts 2-nd order `tf.gradients`.", "@fchollet any comment on this? (contrib.layers)", "Can you log the specific op for which you cannot obtain second-order gradients?", "@fchollet \r\nWhen the exception is thrown, the op on `out_grads[i] = control_flow_ops.ZerosLikeOutsideLoop(op, i)` is the following:\r\n`<tf.Operation 'gradients_6/BatchNorm_3/cond_1/Merge_grad/cond_grad' type=Switch>`\r\n", "> I think updates_collections should not affect the ability to compute gradients?\r\n\r\nWhen you are using a custom `updates_collections` you are routed to a *different* implementation of batch norm (the one in `tf.layers` does not support custom update collections), so this is not about `updates_collections` affecting gradient computation.\r\n\r\nAs far as I can tell you are calling an op (as part of the legacy batch norm layer implementation) that does not implement second-order gradients. Can anyone confirm?", "@yuanbyu is that something you've seen before?", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 9033, "title": "Problem anaconda tensorflow Windows : Traceback <most recent call last>:   File \"<stdin>\", line 1, in <module> ModuleNotFoundError: No module named 'tensorflow'", "body": "Hi Guys!  [ # if you don't won't to read everything go down directly to # SOLUTION ]\r\nhere is the way how i fixed the problem of installing tensorflow on Windows. I will start \ud83d\udc4dfrom the begining \ud83d\udc4d \r\n1. I downloaded the Anaconda 4.3.1 For Windows with Python 3.6 version. \r\n2. Create a conda environment named tensorflow by invoking the following command:\r\nC:> conda create -n tensorflow \r\n\r\n3. Activate the conda environment by issuing the following command:\r\nC:> activate tensorflow\r\n (tensorflow)C:>  # Your prompt should change \r\n\r\n3. Issue the appropriate command to install TensorFlow inside your conda environment. To install the CPU-only version of TensorFlow, enter the following command:\r\n\r\n(tensorflow)C:> pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.0.1-cp35-cp35m-win_amd64.whl \r\n\r\nMessage appear : can not install this wheel ......... [ i forgot the message, it's just couldn't find the point]\r\nTo install the GPU version of TensorFlow, enter the following command (on a single line):\r\n(tensorflow)C:> pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-1.0.1-cp35-cp35m-win_amd64.whl  \r\n\r\nsame message appear couldn't install \r\n\r\nso after that i wanted to write my first code \ud83d\udc4d \r\nc>python\r\npython version 3.6 .... (anaconda).............\r\n>>> import tensorflow as tf\r\n[it appears this Error : ]\r\n\r\nTraceback <most recent call last>:\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensorflow'\r\n\r\n-----------------------------------------------------------------------------\r\nSOLUTION\r\nFix The Problem :\r\nhttps://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.0.1-cp35-cp35m-win_amd64.whl\r\n\r\ntensorflow-1.0.1 -cp35-cp35m-win_amd64.whl\r\ntensorflow version 1.0.1 \r\n-cp 35 : python version needed\r\nwin_amd64 : windows x64 \r\n\r\nso the Anaconda 4.3.1 For Windows with Python 3.6 version so we need python 3.5 i downloaded it from other this web site cause anaconda has only the version 4.3.1 &  for python 2.7 : \r\nso this anaconda for python 3.5 : http://www.gurobi.com/downloads/get-anaconda \r\nthen i followed the other steps & everything worked fine :), i hope everything will work with you \r\n", "comments": ["Closing as a duplicate of #6999 ", "#aismshankar i read about that, to rename it cp36 won't help, it is still the same issue, :) that's why i posted this solution, i worked on it yesterday\r\nThank you shankar ", "Changing version from 3.6 to 3.5 worked for me.\r\nThe full process was : \r\n(inside virtual env)\r\nconda install python=3.5\r\npip install tensorflow ", "I'm using tensorflow for the first time, I've installed it successfully but when I try to run it gives me the following error kindly help me it's been 2 days I haven't found the solution.\r\n\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensorflow'", "> I'm using tensorflow for the first time, I've installed it successfully but when I try to run it gives me the following error kindly help me it's been 2 days I haven't found the solution.\r\n> \r\n> > > > import tensorflow as tf\r\n> > > > Traceback (most recent call last):\r\n> > > > File \"\", line 1, in \r\n> > > > ModuleNotFoundError: No module named 'tensorflow'\r\n\r\nYou have to install tensorflow first. To do it print in your Shell:\r\n\r\npip install tensorflow", "python 3.9 does not work with tensorflow-2.4. you have to try python 3.7"]}, {"number": 9032, "title": "Documentation for tf.contrib.learn's learn_runner", "body": "### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*: yes\r\n- *TensorFlow installed from (source or binary)?*: source\r\n- *TensorFlow version*: 1.0.1\r\n- *Bazel version (if compiling from source)*: 0.4.4\r\n- *CUDA/cuDNN version*: 8.0, 5.1\r\n- *GPU Model and Memory*: GTX 1080\r\n- *Exact command to reproduce*: n/a\r\n\r\n### Describe the problem clearly\r\nThis is a feature request to add documentation.\r\n\r\nAs recommended by the GCP Cloud ML Engine documentation (specifically the code used by [this tutorial](https://cloud.google.com/ml-engine/docs/how-tos/getting-started-training-prediction)), I am attempting to use `tf.contrib.learn`'s `learn_runner` interface for training. However, I cannot figure out how to configure the checkpoint saving behaviour. Normally, I would just post a Stackoverflow question, but I feel that this interface would benefit greatly from some official documentation, especially as it is being recommended by GCP.\r\n\r\nThe closest I could get to documentation on this specific issue is a [cryptic comment](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/run_config.py#L198).\r\n", "comments": ["Hmm...the `learn_runner` interface isn't really part of the documented API on www.tensorflow.org/api_docs. Perhaps we shouldn't be using it.\r\n\r\nCCing @jhseu @martinwicke  who might have comments on what to use instead and also whether and how the CloudML documentation should be updated.", "We will add documentation shortly as we move this utility from contrib to core. This should happen within the next month or so. \r\n\r\nNote that this is still in contrib and its interface may still change.", "Thanks for the report @skycoop . Closing this out for now, should be fixed when the API is finalized and moved into core (i.e., not part of `tf.contrib`) in a release or two."]}, {"number": 9031, "title": "Move MNIST pointers to the CVDF mirror", "body": "The [Common Visual Data Foundation](http://www.cvdfoundation.org/) now hosts a [mirror](https://github.com/cvdfoundation/mnist) of MNIST, which should alleviate the issue with occasional unavailability of the NYU website.\r\nMany thanks to Serge Belongie and Yann LeCun for making it happen.\r\nThis bug tracks moving all references to MNIST in the TensorFlow codebase to the CVDF mirror.\r\n", "comments": ["This was submitted and should propagate to github shortly."]}]