[{"number": 5454, "title": "Branch 138387835", "body": "", "comments": ["@benoitsteiner, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @petewarden and @tensorflower-gardener to be potential reviewers.\n"]}, {"number": 5453, "title": "tensorflow/core/util/event.proto: File not found while building with custom C++ project", "body": "I'm building tensorflow inside a C++ project and link with it using bazel build script. I'm using the master version of tensorflow and here a the BUILD file \r\n\r\n```\r\ncc_library(\r\n    name = \"tf-net\",\r\n    hdrs = [\"model.h\"],\r\n    srcs = [\r\n        \"model.cc\",\r\n    ],\r\n    linkopts = [\"-lm\"],\r\n    deps = [\r\n        \"@org_tensorflow//tensorflow/core:core_cpu\",\r\n        \"@org_tensorflow//tensorflow/core:lib\",\r\n        \"@org_tensorflow//tensorflow/core:tensorflow\"\r\n    ],\r\n\tvisibility=[\"//visibility:public\"]\r\n)\r\n```\r\n\r\nThis worked for previous version of tensorflow but now I get \r\n```\r\ntensorflow/core/util/event.proto: File not found.\r\ntensorflow/core/debug/debug_service.proto: Import \"tensorflow/core/util/event.proto\" was not found or had errors.\r\ntensorflow/core/debug/debug_service.proto:38:25: \"Event\" is not defined.\r\n```\r\n\r\nIt seems that it tries to compile the proto file debug_service.proto but can't find the needed dependencies.\r\n", "comments": ["Could you please add dependency to @org_tensorflow//tensorflow/core:protos_all?\n\nWe have probably moved thing around.\n\nSherry\n", "I have the same problem in a similar situation (though without the assurance that it used to work).\nAdding the dependency on protos_all does not resolve it. (It can't find protos_all and suggests protos_all_cc but that doesn't help either) \n\nAfter a little digging, I note that in the file\ntensorflow/core/debug/BUILD\nthere is a line (85) which currently reads\n\":debug_service_proto_cc\",  # TODO(cais): Confirm safe.\n\nIt seems that this line, and the whole debug_service_proto changeset, have been added, reverted, and unreverted over the last couple of months, and simply commenting out that line allows my build to complete. \n\nI do not fully appreciate what that code does and am not suggesting this as a fix, (although for jrabary it might suffice as a workaround, infering that they are like me not using Cuda) but it may be useful for working out what a fix would look like.\n", "@caisq Can you comment based on the \"Confirm safe\" TODO?\n", "In my project the problem has disappeared with the newest head (having restored the line I mentioned commenting out) \r\nI don't know whether @jrabary has the same experience.", "@girving The \"Confirm safe\" TODO is now obsolete and will be removed on the next successful push from internal. @jrabary, does the problem persist if you use use the latest master HEAD?", "Automatically closing due to lack of recent activity. We hope that you were able to resolve it on your own. However, since this is a support issue rather than a bug or feature request, you will probably get more information by posting it on StackOverflow."]}, {"number": 5452, "title": "Tensorflow for SegNet", "body": "Hi \r\n\r\nI just would like to know are there any Tensorflow implementation for the SegNet, (https://arxiv.org/pdf/1511.00561.pdf)\r\n\r\nThanks,\r\n\r\nNOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["Please reask this question on StackOverflow. GitHub issues are for problems, feature requests rather than how to use tensorflow or find available implementations of models. Thanks!\n"]}, {"number": 5451, "title": "fix failed copy for op without outputs but with control outputs", "body": "This PR fixed copying op without outputs but with control outputs. This kind of op (which is common, e.g. Assert) is not copied at all before this PR.\r\nbtw, I'd like to add tests for my changes but I cannot find how to run bazel tests. There seems to be no guidance for contributions to Tensorflow.", "comments": ["Can one of the admins verify this patch?\n", "@thjashin, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener and @drpngx to be potential reviewers.\n", "Yes, if you could add a test for that, that would be great.\n\nFor tests, just copy over an existing test, or use a test that's already there.\n\nTo test a target, use\n\n```\nbazel test -c opt //tensorflow/<test_target>\n```\n\nfor instance, `//tensorflow/contrib/graph_editor:subgraph_test`.\n", "@drpngx Thx. I tried and this command started compiling, but failed with gcc-6.2.0\n\n```\ntensorflow/core/kernels/quantize_op.cc:116:33: error: no matching function for call to 'std::function<float(float)>::function(<unresolved overloaded function type>)'\n                 .unaryExpr(std::function<float(float)>(round))\n                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n```\n\n#5419 seems to be related\n", "OK, let's wait until #5419 is resolved, or you can use an alternate compiler\n", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@drpngx tests added. It failed before the change.\n", "Jenkins, test this please.\n", "@drpngx fixed.\n", "Jenkins, test this please.\n", "@drpngx seems that the failed tests are not due to my PR?\n", "Seems like a transient errror.\n\nJenkins, test this please.\n", "@tensorflow-jenkins test this please\n", "Seems like we removed `list_diff` from the operators, maybe from windows only.\n\nJenkins, test this please.\n", "Still failed?\n", "Looks like some compression error.\n\nJenkins, test this please.\n\n```\n17:00:47 ======================================================================\n17:00:47 ERROR: testWriteGzipRead (__main__.TFRecordIteratorTest)\n17:00:47 ----------------------------------------------------------------------\n17:00:47 StopIteration\n17:00:47 \n17:00:47 During handling of the above exception, another exception occurred:\n17:00:47 \n17:00:47 Traceback (most recent call last):\n17:00:47   File \"C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/python/kernel_tests/reader_ops_test.py\", line 740, in testWriteGzipRead\n17:00:47     for r in tf.python_io.tf_record_iterator(zfn):\n17:00:47 SystemError: <built-in function delete_PyRecordReader> returned a result with an error set\n17:00:47 \n```\n", "Failure on mac:\r\n\r\n```\r\nFAIL: //tensorflow/python/kernel_tests:io_ops_test (see /Volumes/Transcend/bazel_cache/13e370a18c169b19baeafefb05212b85/execroot/tensorflow-pull-requests-mac/bazel-out/local-opt/testlogs/tensorflow/python/kernel_tests/io_ops_test/test.log).\r\nINFO: From Testing //tensorflow/python/kernel_tests:io_ops_test:\r\n==================== Test output for //tensorflow/python/kernel_tests:io_ops_test:\r\nF...\r\n======================================================================\r\nFAIL: testMatchingFiles (__main__.IoOpsTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/Volumes/Transcend/bazel_cache/13e370a18c169b19baeafefb05212b85/execroot/tensorflow-pull-requests-mac/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/io_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/io_ops_test.py\", line 82, in testMatchingFiles\r\n    self._subset(files, [1]))\r\nAssertionError: Items in the first set but not the second:\r\n'/var/folders/_6/yp58sshs32l53h1pq3h_v19r0008p3/T/io_ops_test/ABzDEF.GHv4cw_1oe'\r\n\r\n```", "Jenkins, test this please.", "It builds!", "@drpngx Cool. Thanks!"]}, {"number": 5450, "title": "Adding support for Linux s390x ", "body": "Adding support for Linux s390x. Could someone please review?", "comments": ["Can one of the admins verify this patch?\n", "@namrata-ibm, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @guschmue and @tensorflower-gardener to be potential reviewers.\n", "@vrv \nWe have re-created this PR after closing #4508 \n\nWe have incorporated @aselle's  suggestion about cpu_info.h. \nCould you please have a look?\n", "Jenkins, test this please.\r\nLeaving the review to @aselle ", "Windows failures are definitely caused by changes here. GPU failures are also suspicious, but I will rerun the tests to make sure.\r\n@namrata-ibm Could you take a look at windows failures here:\r\nhttps://ci.tensorflow.org/job/tensorflow-pr-win-cmake-py/286/\r\n\r\nFor GPU tests,\r\nJenkins, test this please.", "Windows failures are occurring because of following error:\r\n`c:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow\\core\\platform\\cpu_info.h(23): error C2065: '__ORDER_LITTLE_ENDIAN__': undeclared identifier (compiling source file C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow\\core\\platform\\file_system.cc)`\r\n\r\nThis change was added as per aselle's  [review comment](https://github.com/tensorflow/tensorflow/pull/4508/files#r81671579) in our earlier PR.\r\n\r\n@aselle , any inputs on how to make it work on Windows?", "__BYTE_ORDER__ is defined by gcc and windows compiles with msvc which does not define it.\r\nThere is no platform specific platform.h and looking over the source I see lots of places that include cpu_info.h directly. Maybe a decent compromise between clean and safe would be to create a new include platform/windows/msvc_compat.h and include this in cpu_info.h under #ifdef COMPILER_MSVC.\r\n@mrry what do you think?", "@guschmue I think the standard workaround for this would be to create `tensorflow/core/platform/windows/cpu_info.h`, `tensorflow/core/platform/posix/cpu_info.h`, etc. and have `tensorflow/core/platform/cpu_info.h` include the appropriate header based on what `PLATFORM_*` macro was defined. Is there a subtlety I'm missing here?", "sure that will work. @namrata-ibm , I can help with the change if you want me to.", "@guschmue , Thank you for your inputs. Would be great if you could help with above change suggested by mrry. ", "ok, should be getting to it today.", "@aselle, have added fix for another test here. \r\n\r\n@guschmue , thanks and let us know if anything needs to be done from our side.\r\n", "sorry, some else came up. should be getting to it tomorrow.", "@aselle, we have added a fix for decode_raw_op_test in the PR.\r\n\r\nAccording to the [readme](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard3/tf.decode_raw.md) the tf.decode_raw has an attribute little_endian whose default value is set to true in parsing_ops.cc (flag ignored for uint8). \r\nIf we set the value of the flag to False for big endian, the assert for endianness check doesn't fail on either systems (little endian or big endian) and the following data copying works on both systems. ", "Jenkins, test this please.", "@namrata-ibm - windows should compile now with your change but you need to merge platform\\cpu_info.h from master,", "@guschmue, Thank you for the PR for Windows support.\r\n@gunan , Could you please run a build in your CI now?", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->", "@Nayana-ibm  Could you Check the CLA?\r\nIf you read and signed the CLA, please add a comment that says \"I signed it!\"\r\n\r\nJenkins, test this please.", "I signed it! \r\n(covered as part of IBM Google Corporate CLA)", "Your commit email on git is shown as\r\nFrom: namrata-ibm <bhavenamrata@gmail.com>\r\n\r\nI'm guessing that's not the email you used to sign up to your internal google groups associated with the IBM account.  You might want to commit --amend with the email you've registered with your company.", "There are quite a few pylint failures, mostly inconsistencies with google style:\r\n```\r\nFAIL: Found 34 non-whitelited pylint errors:\r\ntensorflow/python/kernel_tests/decode_raw_op_test.py:32: [W0311(bad-indentation), ] Bad indentation. Found 9 spaces, expected 8\r\n\r\ntensorflow/python/kernel_tests/decode_raw_op_test.py:34: [W0311(bad-indentation), ] Bad indentation. Found 9 spaces, expected 8\r\n\r\ntensorflow/python/kernel_tests/decode_raw_op_test.py:53: [W0311(bad-indentation), ] Bad indentation. Found 9 spaces, expected 8\r\n\r\ntensorflow/python/kernel_tests/decode_raw_op_test.py:55: [W0311(bad-indentation), ] Bad indentation. Found 9 spaces, expected 8\r\n\r\ntensorflow/python/kernel_tests/decode_raw_op_test.py:60: [W0311(bad-indentation), ] Bad indentation. Found 9 spaces, expected 8\r\n\r\ntensorflow/python/kernel_tests/decode_raw_op_test.py:63: [W0311(bad-indentation), ] Bad indentation. Found 9 spaces, expected 8\r\n\r\ntensorflow/python/kernel_tests/decode_raw_op_test.py:75: [W0311(bad-indentation), ] Bad indentation. Found 9 spaces, expected 8\r\n\r\ntensorflow/python/kernel_tests/decode_raw_op_test.py:77: [W0311(bad-indentation), ] Bad indentation. Found 9 spaces, expected 8\r\n\r\ntensorflow/python/framework/tensor_util_test.py:48: [W0311(bad-indentation), ] Bad indentation. Found 5 spaces, expected 6\r\n\r\ntensorflow/python/framework/tensor_util_test.py:54: [W0311(bad-indentation), ] Bad indentation. Found 5 spaces, expected 6\r\n\r\ntensorflow/python/framework/tensor_util_test.py:66: [W0311(bad-indentation), ] Bad indentation. Found 5 spaces, expected 6\r\n\r\ntensorflow/python/framework/tensor_util_test.py:72: [W0311(bad-indentation), ] Bad indentation. Found 5 spaces, expected 6\r\n\r\ntensorflow/python/framework/tensor_util_test.py:84: [W0311(bad-indentation), ] Bad indentation. Found 5 spaces, expected 6\r\n\r\ntensorflow/python/framework/tensor_util_test.py:90: [W0311(bad-indentation), ] Bad indentation. Found 5 spaces, expected 6\r\n\r\ntensorflow/python/framework/tensor_util_test.py:103: [W0311(bad-indentation), ] Bad indentation. Found 5 spaces, expected 6\r\n\r\ntensorflow/python/framework/tensor_util_test.py:109: [W0311(bad-indentation), ] Bad indentation. Found 5 spaces, expected 6\r\n\r\ntensorflow/python/framework/tensor_util_test.py:121: [W0311(bad-indentation), ] Bad indentation. Found 5 spaces, expected 6\r\n\r\ntensorflow/python/framework/tensor_util_test.py:127: [W0311(bad-indentation), ] Bad indentation. Found 5 spaces, expected 6\r\n\r\ntensorflow/python/framework/tensor_util_test.py:139: [W0311(bad-indentation), ] Bad indentation. Found 5 spaces, expected 6\r\n\r\ntensorflow/python/framework/tensor_util_test.py:145: [W0311(bad-indentation), ] Bad indentation. Found 5 spaces, expected 6\r\n\r\ntensorflow/python/framework/tensor_util_test.py:168: [W0311(bad-indentation), ] Bad indentation. Found 5 spaces, expected 6\r\n\r\ntensorflow/python/framework/tensor_util_test.py:174: [W0311(bad-indentation), ] Bad indentation. Found 5 spaces, expected 6\r\n\r\ntensorflow/python/framework/tensor_util_test.py:254: [W0311(bad-indentation), ] Bad indentation. Found 5 spaces, expected 6\r\n\r\ntensorflow/python/framework/tensor_util_test.py:260: [W0311(bad-indentation), ] Bad indentation. Found 5 spaces, expected 6\r\n\r\ntensorflow/python/framework/tensor_util_test.py:320: [W0311(bad-indentation), ] Bad indentation. Found 5 spaces, expected 6\r\n\r\ntensorflow/python/framework/tensor_util_test.py:326: [W0311(bad-indentation), ] Bad indentation. Found 5 spaces, expected 6\r\n\r\ntensorflow/python/framework/tensor_util_test.py:338: [W0311(bad-indentation), ] Bad indentation. Found 5 spaces, expected 6\r\n\r\ntensorflow/python/framework/tensor_util_test.py:344: [W0311(bad-indentation), ] Bad indentation. Found 5 spaces, expected 6\r\n\r\ntensorflow/python/framework/tensor_util_test.py:359: [W0311(bad-indentation), ] Bad indentation. Found 5 spaces, expected 6\r\n\r\ntensorflow/python/framework/tensor_util_test.py:365: [W0311(bad-indentation), ] Bad indentation. Found 5 spaces, expected 6\r\n\r\ntensorflow/python/framework/tensor_util_test.py:396: [W0311(bad-indentation), ] Bad indentation. Found 5 spaces, expected 6\r\n\r\ntensorflow/python/framework/tensor_util_test.py:402: [W0311(bad-indentation), ] Bad indentation. Found 5 spaces, expected 6\r\n\r\ntensorflow/python/framework/tensor_util_test.py:413: [W0311(bad-indentation), ] Bad indentation. Found 5 spaces, expected 6\r\n\r\ntensorflow/python/framework/tensor_util_test.py:419: [W0311(bad-indentation), ] Bad indentation. Found 5 spaces, expected 6\r\n```\r\nCould you fix those, the I can rerun all the tests.\r\n(also the email issue Vijay mentioned)", "I have CLA signed for gmail id : nayana.thorat@gmail.com\r\nI could see corporate CLA entry here : https://cla.developers.google.com/clas\r\n", "I could see CLA for my id as well: bhavenamrata@gmail.com\r\nAlso our commit IDs and author IDs are the same. \r\nCould you please confirm if we are missing something here?", "One possible solution to the CLA issue is to squash all the commits in this PR to a single commit, and then retry creating the PR.\r\nWould that be OK?\r\n\r\nAlso, you will need further edits to fix the test failures reported by jenkins.", "@gunan, ok, we will work on recreating PR with the edits and by pulling latest changes to resolve conflicts shown above.", "@mrry, @guschmue, @vrv , While debugging sparse_split_op_test on big endian system, I could observe that tests involving column wise splitting are failing.\r\n This seems to happen because `context->input(0).scalar<int>()();`  returns 0 instead of 1  in OpKernel for sparse_split test ( from [file](https://github.com/tensorflow/tensorflow/blob/v0.10.0/tensorflow/core/kernels/sparse_split_op.cc ) - pasted code snippet below : ) \r\n```\r\nvoid Compute(OpKernelContext* context) override { \r\n     const int32 split_dim = context->input(0).scalar<int>()();  \r\n     const Tensor& input_indices = context->input(1); \r\n```\r\nlooks like the **scalar\\<int\\>()()** is not converting the data correctly. While stepping through the scalar function, came across below code snippets:\r\nFrom [file\r\n](https://github.com/tensorflow/tensorflow/blob/v0.10.0/tensorflow/core/framework/tensor.h#L621)\r\n\r\n```\r\n  template <typename T>\r\n  T* base() const {\r\n    return reinterpret_cast<T*>(data());\r\n  }\r\n};\r\n\r\ntemplate <typename T>\r\nT* Tensor::base() const {\r\n  return buf_ == nullptr ? nullptr : buf_->base<T>();\r\n}\r\n```\r\ndata() picks value[ from](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/tensor.cc#L92):\r\n\r\n` void* data() const override { return data_; }`\r\nHere printing *data_ gives 1! However this does not propagate to split_dim which remains 0.\r\n\r\n I am unable to catch the exact point where the manipulation is happening. Could you please have a look?", "@namrata-ibm: The decode_raw option for little endian is not whether the system is little endian, it is whether the data is little endian. So in particular, if you copy data that is little endian to a big endian machine you should be able to call decode_raw with false on those sequence of bytes and work.  What you said you did doesn't sound right in that regard. ", "@aselle , ok, so I tried running the test without setting decode_raw option for little endian, This caused the test to fail with error `\"UnimplementedError: Unimplemented support for little_endian=true\"`. \r\nNext, I removed below assert to verify if the further code for copying the data works. This resulted in test case to pass on Big Endian as it copies data  byte-by-byte.\r\n\r\n```\r\n     DCHECK_EQ(flat_in.size(), out.dimensions()[0]);\r\n-    OP_REQUIRES(\r\n-        context,\r\n-        little_endian_ == ::tensorflow::port::kLittleEndian || sizeof(T) == 1,\r\n-        errors::Unimplemented(\"Unimplemented support for little_endian=\",\r\n-                              little_endian_ ? \"true\" : \"false\"));\r\n     // Endianness matches, so just copy each string byte-for-byte.\r\n     T* out_data = out.data();\r\n     for (int64 i = 0; i < flat_in.size(); ++i) {\r\n\t   const T* in_data = reinterpret_cast<const T*>(flat_in(i).data());\r\n       memcpy(out_data, in_data, str_size);\r\n       out_data += added_dim;\r\n     }\r\n\r\n```\r\nThe only change in [test case ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/decode_raw_op_test.py)for Int16 for Big Endian was  \r\n```\r\n     if sys.byteorder == \"big\": \r\n        self.assertAllEqual([[ord(\"A\") * 256 + ord(\"a\"), \r\n                                ord(\"B\") * 256 + ord(\"C\")]], result) \r\n     else: \r\n        self.assertAllEqual([[ord(\"A\") + ord(\"a\") * 256, \r\n                                ord(\"B\") + ord(\"C\") * 256]], result) \r\n\r\n```\t\t\t\t\r\n", "@gunan , We have re-created PR for resolving CLA issue with single commit. \r\nAlso, we have fixed the test failures reported by jenkins.\r\n\r\nLink to new [PR ](https://github.com/tensorflow/tensorflow/pull/6681)\r\n\r\nClosing this PR.\r\n"]}, {"number": 5449, "title": "GridRNN cell uses tuples for output and states", "body": "(Redo of #4631 because I screwed up the PR)\r\n\r\nIn response to this (https://github.com/tensorflow/tensorflow/issues/2560) and recent changes in LSTMCell, this PR added `state_is_tuple=True` and `output_is_tuple=True` into the constructor of GridRNNCell:\r\n\r\n```\r\n> cell = tf.contrib.grid_rnn.Grid2LSTMCell(3, use_peepholes=True)\r\n> cell.state_size\r\n       (LSTMStateTuple(c=3, h=3), LSTMStateTuple(c=3, h=3))\r\n> cell.output_size\r\n       (3,)\r\n```\r\n\r\nThis means there are 2 LSTM cells in the grid, and the state of each cell has size of `(c=3, h=3)`. There is only one output dimension, whose size is 3.\r\n\r\nIn contrast:\r\n\r\n```\r\n> cell = tf.contrib.grid_rnn.Grid2BasicRNNCell(3)\r\n> cell.state_size\r\n     (3, 3)\r\n> cell.output_size\r\n     (3,)\r\n```\r\n\r\nThis means there are 2 BasicRNN cell in the grid, and the state of each cell has size of 3. There is only one output dimension, whose size is 3.\r\n\r\nPrevious behaviour is maintained by using `state_is_tuple=False, output_is_tuple=False` when creating the cell.\r\n\r\n```\r\n> cell = tf.contrib.grid_rnn.Grid2LSTMCell(3, use_peepholes=True, state_is_tuple=False, output_is_tuple=False)\r\n    WARNING:tensorflow:<tensorflow.contrib.grid_rnn.python.ops.grid_rnn_cell.Grid2LSTMCell object at 0x10de68f50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\r\n    WARNING:tensorflow:<tensorflow.contrib.grid_rnn.python.ops.grid_rnn_cell.Grid2LSTMCell** object at 0x10de68f50>: Using a concatenated output is slower and will soon be deprecated.  Use output_is_tuple=True.\r\n\r\n> cell.state_size\r\n    12\r\n> cell.output_size\r\n    3\r\n\r\n> cell = tf.contrib.grid_rnn.Grid2BasicRNNCell(3, state_is_tuple=False, output_is_tuple=False)\r\n    WARNING:tensorflow:<tensorflow.contrib.grid_rnn.python.ops.grid_rnn_cell.Grid2BasicRNNCell object at 0x10de4b950>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\r\n    WARNING:tensorflow:<tensorflow.contrib.grid_rnn.python.ops.grid_rnn_cell.Grid2BasicRNNCell object at 0x10de4b950>: Using a concatenated output is slower and will soon be deprecated.  Use output_is_tuple=True.\r\n\r\n> cell.state_size\r\n   6\r\n> cell.output_size\r\n   3\r\n```\r\n\r\nThis also fixes https://github.com/tensorflow/tensorflow/issues/4296\r\n\r\n# Backward compatibility\r\nThis change is not fully backward-compatible:\r\n- The old implementation concatenates the output and the state tensors, so when you have `g, s = cell(...)` then `g` and `s` are simply two tensors.\r\n- This implementation, by default, returns `g` and `s` as tuples of tensors. \r\n  - `g` is a tuple of length equals to the size of `output_dims` of the cell. Normally you only have one output dimension, so `g` will be a tuple of length 1.\r\n  - `s` is a tuple of length equals to the size of `recurrent_dims` of the cell, containing the states of all the recurrent cells in all recurrent dimensions.\r\nNow if you use LSTM cells for the recurrent dimensions, the state of each LSTM cell is a tuple of tensors (with `c` and `h` components). So for instance, the state of a Grid2LSTMCell will be a tuple of tuples: `((c=<tensor>, h=<tensor>), (c=<tensor>, h=<tensor>))`\r\nIf you use GRU or vanila RNN cells for the recurrent dimensions, the state of each cell is a tensor. So for instance, the state of a Grid2GRUCell will be a tuple of tensors: `(<tensor>, <tensor>)`\r\n\r\nOld code that depends on this cell can use the old behaviour by setting `state_is_tuple=False` and `output_is_tuple=False` when constructing the cell. \r\n\r\n```\r\ncell = tf.contrib.grid_rnn.Grid2LSTMCell(2, use_peepholes=True,\r\n                                             state_is_tuple=False,\r\n                                             output_is_tuple=False)\r\n```", "comments": ["Can one of the admins verify this patch?\n", "@phvu, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ilblackdragon, @ebrevdo and @drpngx to be potential reviewers.\n", "@ebrevdo: I specified the backward-compatibility in the summary. Let me know if there is anything unclear.\nI redid the tests a bit. `tf.convert_to_tensor` does not really do what I want. Instead I made all the asserts explicit in the code, so a function to get the shapes of all tensors in a tuple is not needed.\n", "@ebrevdo Can you take a look?", "@phvu sorry for the delay. Could you sync and push again?", "sure, I am on it", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 5448, "title": "Tensorflow hangs when initializing variables in a multi process setting", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nClosest I found was https://github.com/tensorflow/tensorflow/issues/4767\r\nBut I don't intend to change the datatypes  of any variables...\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04\r\n\r\nInstalled version of CUDA and cuDNN: CUDA 7.5 and CUDNN 4.0.7\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n-rw-r--r-- 1 root root 189170 Aug 25 02:29 /usr/local/cuda/lib/libcudadevrt.a\r\nlrwxrwxrwx 1 root root     16 Aug 25 02:29 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\r\nlrwxrwxrwx 1 root root     19 Aug 25 02:29 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\r\n-rwxr-xr-x 1 root root 311596 Aug 25 02:29 /usr/local/cuda/lib/libcudart.so.7.5.18\r\n-rw-r--r-- 1 root root 558020 Aug 25 02:29 /usr/local/cuda/lib/libcudart_static.a\r\n\r\nCUDNN libs are in /cuda/lib64/ (output of `ls -l /path/to/cuda/lib64/libcud*`):\r\n-rw-r--r-- 1 root root   322936 Aug 25 02:29 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Aug 25 02:29 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\r\nlrwxrwxrwx 1 root root       19 Aug 25 02:29 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\r\n-rwxr-xr-x 1 root root   383336 Aug 25 02:29 /usr/local/cuda/lib64/libcudart.so.7.5.18\r\n-rw-r--r-- 1 root root   720192 Aug 25 02:29 /usr/local/cuda/lib64/libcudart_static.a\r\n-rwxr-xr-x 1 root root 61453024 Aug 25 02:36 /usr/local/cuda/lib64/libcudnn.so\r\n-rwxr-xr-x 1 root root 61453024 Aug 25 02:36 /usr/local/cuda/lib64/libcudnn.so.4\r\n-rwxr-xr-x 1 root root 61453024 Aug 25 02:36 /usr/local/cuda/lib64/libcudnn.so.4.0.7\r\n-rw-r--r-- 1 root root 62025862 Aug 25 02:36 /usr/local/cuda/lib64/libcudnn_static.a\r\n\r\nBut **I have installed Tensorflow CPU only version in the virtualenv I am testing this code on**, hence it might not be using CUDA at all as far as this issue is concerned (I am debugging an issue that occurred on a EC2 machine which has no GPU, by reproducing it in my local system)\r\n \r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0rc2-cp27-none-linux_x86_64.whl\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.0.11.0rc2\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nI have saved a minimal working code which reproduces this issue at [gist](https://gist.github.com/ramnath-k/d299964fead58c0d1e0df9c2190a4f91)\r\n\r\nTo reproduce the issue:\r\n1. Call saver.save in the main thread and checkpoint at least one tf.Variable\r\n2. loading this checkpoint from main thread and evaluating the variable works fine\r\n3. if I now launch a subprocess and try to load the checkpoint in that, Tensorflow hangs at sess.run(tf.initialize_all_variables())\r\n\r\n### What other attempted solutions have you tried?\r\nI tried putting a container with names suffixed by the subprocess pid but it didn't help. I also tried a basic version of create_local_server and it didn't work too\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\nI don't see any errors in the console when the code hangs. ", "comments": ["It looks like the issue here is that Python 2.7 unconditionally uses `os.fork()` in to implement `multiprocessing.Process.start()`, but the TensorFlow runtime is not fork-safe (for one thing, because it creates various internal threads and initialized static members at startup). Since in your example the parent process invokes the TensorFlow runtime, the child will be in an unspecified state when it attempts to do the first `sess.run()` call. (When I ran your code, it was blocking on a condition variable, waiting for constant propagation to finish, for example.)\n\nAs far as I can tell, there is no way to make the Python 2.7 multiprocessing module do the \"right thing\" here. If you upgrade to Python 3.4, you can use [`multiprocessing.set_start_method('spawn')`](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.set_start_method) to avoid the issues over fork-safety. Alternatively, you could structure your program so that the `import tensorflow as tf` statement only runs in child processes.\n", "Thanks @mrry \nI tried out your suggestion to move the tensorflow code over to child processes. \nFirst up I moved the saving and restoring code to another module as in this [basic_saver_restore.py](https://gist.github.com/ramnath-k/2990b46b84278e5e24919beb44968a00)\nThen I called the saver and restore as before using multiprocessing as in [multi_process_saver_restore.py](https://gist.github.com/ramnath-k/fa25696f3f1def6ba32855f612353914)\nIn basic_saver_restore, I put a module level variable server equal to create_local_server (which I assume creates a child process to run the session right?). \nThis seems to fix the hang I was getting. \nWill this approach always work? Or is it that I am only fixing some issues but not all with this approach?\n\nMoving to Python 3 is something I will attempt later.\n\nMy actual hang occurs when a Hive launches a python user defined function which calls a restore tensorflow model function. Do you think this approach of having a module level create_local_server fix such an issue also? \n", "Hi @ramnath-k ,\r\n\r\nIt appears to have hung because it's waiting for the main thread (or process) to yield the global interpreter lock. Make your processes daemon processes and it should work.\r\n\r\n```python\r\nfor i in range(num_procs):\r\n      p = Process(target=subprocess, args=(i,))\r\n      p.daemon = True            <<<<< Add this line.\r\n      p.start() # process fails to get past initialize all variables\r\n      procs.append(p)\r\n```\r\n\r\nI just tried it and it worked for me. I also made some simplifications:\r\n\r\n```python\r\ndef add_model():\r\n  a = tf.Variable(2, name='a')\r\n  b = tf.Variable(5, name='b')\r\n  c = tf.mul(a, b, name='c')\r\n  return a, b, c\r\n\r\ndef save_session(unused_arg=None):\r\n  with tf.Session(graph=tf.Graph()) as sess:\r\n    a, b, c = add_model()\r\n    saver = tf.train.Saver(\r\n        tf.global_variables(),\r\n        max_to_keep=1)\r\n    init = [\r\n        tf.global_variables_initializer(),\r\n        tf.local_variables_initializer()]\r\n    sess.run(init)\r\n    e = tf.assign(a, 3, name='e')\r\n    f = tf.assign(b, 4, name='f')\r\n    sess.run([e, f])\r\n    val = sess.run(c)\r\n    print('val=', val)\r\n    checkpoint_dir = './debug'\r\n    if not os.path.exists(checkpoint_dir):\r\n      os.makedirs(checkpoint_dir)\r\n    checkpoint_prefix = os.path.join(checkpoint_dir, 'debug')\r\n    path = saver.save(\r\n        sess, checkpoint_prefix)\r\n    print('save session complete')\r\n\r\ndef restore_session(unused_arg=None):\r\n  graph = tf.Graph()\r\n  with tf.Session(graph=graph) as sess:\r\n    pid = os.getpid()\r\n    container_name = 'worker{}'.format(pid)\r\n    print('container:{}'.format(container_name))\r\n    with graph.container(container_name):\r\n      a, b, c = add_model()\r\n      saver = tf.train.Saver(tf.global_variables())\r\n      print('add model complete')\r\n      init = [\r\n          tf.global_variables_initializer(),\r\n          tf.local_variables_initializer()]\r\n      sess.run(init)\r\n      print('init model complete')\r\n      graph.finalize()\r\n      model_checkpoint ='debug/debug'\r\n      saver.restore(sess, model_checkpoint)\r\n      val = sess.run(c)\r\n      print('val=', val)\r\n\r\n\r\ndef subprocess(i):\r\n  print('inside subprocess {}'.format(i))\r\n  restore_session()\r\n  print('exiting subprocess {}'.format(i))\r\n\r\n\r\ndef main(unused_argv):\r\n  save_p = Process(target=save_session, args=(1,))\r\n  save_p.start()\r\n  save_p.join()\r\n  restore_p = Process(target=restore_session, args=(1,))\r\n  restore_p.start()\r\n  restore_p.join()\r\n  procs = []\r\n  num_procs = 3\r\n  for i in range(num_procs):\r\n      p = Process(target=subprocess, args=(i,))\r\n      p.daemon = True\r\n      p.start() # process fails to get past initialize all variables\r\n      procs.append(p)\r\n  for p in procs:\r\n      p.join()\r\n\r\nif __name__ == \"__main__\":\r\n  tf.app.run()\r\n```\r\n\r\nPlease` let me know if this works for you. Thanks.\r\n\r\nSherry\r\n", "Thanks for the update @sherrym\nYes moving everything to child processes makes it work. But to clarify, in the modified code you have provided I don't even need to set daemon=True as no Tensorflow session has been created in the main process.\nIt hangs only if I start a session in the main process and then fork a child process.\n\nScenario is something like, only after the save_session completes it must launch multiple workers which process the saved checkpoint. So all the restore_session calls must either be in same process as save_session is or must be its child processes.\n", "I use keras as wrapper. Moving all the imports into subprocess solves the issue. \r\n\r\n```\r\ndef f(x):\r\n    import keras\r\n    ....\r\n\r\nmultiprocessing.Pool().map(f, array) # works now\r\n```", "@mrry's Python3 solution worked for me. Thanks!\r\n\r\n```python\r\nimport multiprocessing\r\n\r\nimport tensorflow as tf\r\n\r\ndef f(x):\r\n    session = tf.Session()\r\n    a = tf.Variable(x, name='a')\r\n    b = tf.Variable(100, name='b')\r\n    c = tf.multiply(a, b, name='c')\r\n    session.run(tf.global_variables_initializer())\r\n\r\n    out = session.run(c)\r\n    print(\"OK: %s\" % out)\r\n\r\nif __name__ == '__main__':\r\n    multiprocessing.set_start_method('spawn')  # Comment me out to hang\r\n    f(0)\r\n    multiprocessing.Pool().map(f, range(10))\r\n```", "@mrry Your solution raises -\r\n```python3\r\n_pickle.PicklingError: Can't pickle <class 'module'>: attribute lookup module on builtins failed\r\n```\r\nI can provide more information if required.", "I am trying to solve https://github.com/tensorflow/tensorflow/issues/34456 , so I tried the main solution provided here (`multiprocessing.set_start_method('spawn')`), but I got the following error: `TypeError: can't pickle _thread.lock objects`.\r\n\r\nThis is because [tf is initializing a pool with a queue](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/utils/data_utils.py#L821-L823) which is [apparently not doable](https://stackoverflow.com/a/7865512/4332585)."]}, {"number": 5447, "title": "invalid conversion from 'cudnnDropoutStruct*' to 'int' [-fpermissive]     ", "body": "Hi all,\r\nI am trying to compile tensorflow-0.11 + bazel 0.3.2 on RHEL 6 with cuda 7.0 + cudnn 7.5.5.0 + gcc 4.9.\r\nThe compilation command is :\r\nEXTRA_BAZEL_ARGS=\"--jobs 10\" bazel build -c opt --config=cuda --jobs 10 //tensorflow/tools/pip_package:build_pip_package \r\n\r\nCompilation of rule '//tensorflow/stream_executor:stream_executor' fails with cuda specific message.\r\n\r\nI have latest version of compilers at non standard path , hence i had modified some variables in CROSSTOOL.tpl + third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl. I am attaching **compilation error logs**, **CROSSTOOL.tpl** and **third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl** for your reference.\r\n\r\n\r\nThough i am able to compile cpu-only version of tensorflow successfully. \r\nPlease let me know if any information is needed from my side.\r\nEagerly awaiting your replies.\r\n\r\n\r\n[crosstool_wrapper_driver_is_not_gcc.tpl.txt](https://github.com/tensorflow/tensorflow/files/575209/crosstool_wrapper_driver_is_not_gcc.tpl.txt)\r\n[CROSSTOOL.tpl.txt](https://github.com/tensorflow/tensorflow/files/575211/CROSSTOOL.tpl.txt)\r\n[tensorflow_build2.log.txt](https://github.com/tensorflow/tensorflow/files/575210/tensorflow_build2.log.txt)\r\n\r\n\r\n\r\n\r\n", "comments": ["Hi,\nI retried compilation of tensorflow 0.11 with cuda -7.0 & cudnn 7.0\ni got slightly different error:\ntensorflow/stream_executor/cuda/cuda_dnn.cc:154:39: error: '::cudnnBatchNormalizationBackward' has not been declared\n     typedef std::add_pointer<decltype(::__name)>::type FuncPoint\nDetailed error log is attached herewith.\nAs i am building tensorflow from source, i guess [cuda-7.0 should be compatible with tf-0.11.\n](https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html)\n\n[tensorflow_build3.log.txt](https://github.com/tensorflow/tensorflow/files/575585/tensorflow_build3.log.txt)\n", "@gunan, do we now require at least Cuda 7.5?\n", "I thought no, but it now certainly looks like it. Not sure who to check\nwith if the bump was intentional though.\n\nOn Nov 7, 2016 11:06 AM, \"Andrew Selle\" notifications@github.com wrote:\n\n> https://github.com/gunan\n> \n> @gunan https://github.com/gunan, do we now require at least Cuda 7.5?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5447#issuecomment-258930171,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AHlCOVmzjO8-GDhjCok2VxFEW56H-S40ks5q73a4gaJpZM4KrHhm\n> .\n", "Thank you all for the help,\ntried cuda-7 + cudnn 7.5 -> build failed (version incompatibility)\nI tried cuda-7 + cudnn 7.0.3.0  --> build failed\nI tried cuda-7 + cudnn 7.0.4.0  --> build was success!\nI checked the cudnn.h file for 7.0.3.0 , it didn't had the declaration of \"cudnnBatchNormalizationBackward\", whereas 7.0.4.0 's cudnn.h had the definition. This issue could be closed now.\n", "We should update our documentation to reflect this.\nSorry for the inconvenience.\n\nhowever, in your documentation, what do you mean by cudnn 7?\nI think there was a typo, the latest cudnn version out is cudnn 5.xx\n", "Hi,\r\nMy apologies for delayed reply. 7.0.4 was indeed a typo.\r\nin cudnn.h \r\n#define CUDNN_MAJOR      4\r\n#define CUDNN_MINOR      0\r\n#define CUDNN_PATCHLEVEL 7\r\nso CUDNN_MAJOR .  CUDNN_MINOR . CUDNN_PATCHLEVEL-> 4.0.7 (  7.0.4 )", "Problem might be the cudnn version.\r\nWe upgraded all our test machines to use cuda 8.0 and cudnn 5.0 and 5.1.\r\nAnything older we are not testing anymore."]}, {"number": 5446, "title": "Bus error in quantized network on Android", "body": "We have a tensorflow image processing network, which executes fine on osX, iOS and Android. Now that quantization has been pulled into core, we wanted to use it to optimize our install size on phones.\r\n\r\nWhen we run the quantization process there are as expected small differences in the output on the Mac, but nothing that seems unreasonable. Further, when the quantized network is run on iOS it gives a warning that zero is outside the quantization range and it has to revert to a slow version to handle convolution padding. However it runs fine and the numbers are plausible.\r\n\r\nOn Android, however, and running with precisely the same code that worked perfectly with the unquantized version, it completely breaks. On one phone it gives outputs on the order of 1000 where we expect values on the order of 1. On another, probably the more reasonable, phone it just dies following a bus error. This has been true on the latest commits sampled over the last several days.\r\n\r\nIf it's relevant, the network was converted from a caffe predecessor using this tool: https://github.com/ethereon/caffe-tensorflow\r\nRegrettably I'm unable to upload the source code or network in question.", "comments": ["@petewarden, do you have any thoughts on this or who can diagnose this?\n", "I just hit a similar \"Bus error\" problem with another model, so I'll take on debugging this.\n", "Hi there, I'm helping Pete with this bug. As a quick solution you can disable the new codepath, call:\n\ntensorflow::meta::SetEnabled(false);\n\nthis will revert to Eigen.\n", "Or even better try building with: TENSORFLOW_DISABLE_META defined\n", "@petewarden, @maciekcc, was there any resolution on this bug? Is it still reproducible and relevant?", "IIRC this bug has been fixed in: https://github.com/tensorflow/tensorflow/commit/38aec869681e0ac964e33bf44aef57016c5960c5"]}, {"number": 5445, "title": "tf.gather doesn't run on GPU for DT_INT32", "body": "As I know, `tf.gather` should be able to operate on a GPU device according to this issue: #2502\r\n\r\nI installed the latest version of `r0.11` branch (commit hash written below) and ran the following code:\r\n```python\r\n    import tensorflow as tf\r\n    x = tf.constant([1,2,3,4,5])\r\n    idx = tf.constant([0,1,2,3,4])\r\n    with tf.device('/gpu:0'):\r\n        y = tf.gather(x, idx)\r\n    sess = tf.Session()\r\n    print(sess.run(y))\r\n```\r\n\r\nand it returned the following error message:\r\n\r\n    tensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'Gather': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n         [[Node: Gather = Gather[Tindices=DT_INT32, Tparams=DT_INT32, validate_indices=true, _device=\"/device:GPU:0\"](Const, Const_1)]]\r\n\r\nIf I change `'/gpu:0'` into `'/cpu:0'`, it works well. Can you give me any hint on this?\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04\r\n\r\nInstalled version of CUDA and cuDNN: CUDA 8.0 and cuDNN 5.1.5\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n7197421df430a51383fba7c4e7ff9a3fd7795535\r\n2. The output of `bazel version`\r\nBuild label: 0.3.1\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Jul 29 09:09:52 2016 (1469783392)\r\nBuild timestamp: 1469783392\r\nBuild timestamp as int: 1469783392", "comments": ["try \n\n```\nsess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True))\n```\n\nNot sure why, but hard placement does not always seem to work for me too, but `soft_placement` always works.\n", "@MInner Thank you for your comment, but I don't want to use soft placement. I want it actually runs on GPU.\n", "A hint is that the error message is telling you what datatypes are being requested but are not registered for GPU.  In particular, the kernel it is looking for is Gather with datatypes DT_INT32, which are not currently registered on GPU (only float).  \n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/gather_functor_gpu.cu.cc registers gather for some types (https://github.com/tensorflow/tensorflow/blob/754048a0453a04a761e112ae5d99c149eb9910dd/tensorflow/core/framework/register_types.h#L163), but not DT_INT32.  You could try registering the kernel for DT_INT32 if you really wanted. \n", "@vrv Thank you very much! Let me close this issue because my problem is solved.\nNevertheless, I think it would be better to make the API document clear about currently supported types and devices...\n"]}, {"number": 5444, "title": "AttributeError:  module 'tensorflow.contrib.learn.python.learn.datasets.base' has no attribute 'load_csv'", "body": "I had install tensorflow in ubuntu with Anaconda install. I try to run neural network sample but get \r\nAttributeError:  module 'tensorflow.contrib.learn.python.learn.datasets.base' has no attribute 'load_csv' error.\r\nAny clue?\r\n\r\nThanks,", "comments": ["I apologize, but it is impossible to help you when you do not provide detailed information about what you are running and what you are running it on. Please provide the info requested on the new git hub issue template and create a new issue.  Include exactly what commands you are running to reproduce your problem. Thanks.\n", "I was trying to run a code \"import tensorflow as tf\r\nimport numpy as np\r\n\r\ntraining_set = tf.contrib.learn.datasets.base.load_csv(filename=\"train.csv\", target_dtype=np.int)\r\n\r\ntest_set =     tf.contrib.learn.datasets.base.load_csv(filename=\"test.csv\",  target_dtype=np.int)\r\n\r\nclassifier = tf.contrib.learn.DNNClassifier(hidden_units=[12, 12, 12], n_classes=2)\r\n\r\n# note: to set L1 or L2 regularization (for overfitting), learning rate, etc.\r\n#       then use DNNClassifier options described in:\r\n# https://www.tensorflow.org/versions/r0.9/api_docs/python/contrib.learn.html#DNNClassifier\r\n\r\nclassifier.fit(x=training_set.data, y=training_set.target, steps=500)\r\n\r\naccuracy = classifier.evaluate(x=test_set.data, y=test_set.target)[\"accuracy\"]\r\nprint('Accuracy: {0:f}'.format(accuracy))\r\n\r\nno_and_yes_samples = np.array(\r\n        [[4,1,1,3,2,1,3,1,1], [3,7,7,4,4,9,4,8,1]], dtype=np.int)\r\ny = classifier.predict(no_and_yes_samples)\r\nprint ('Predictions: {}'.format(str(y)))\" which is publicly available on github and I get an error of type\"AttributeError: module 'tensorflow.contrib.learn.python.learn.datasets.base' has no attribute 'load_csv'\"", "Dear fansongfs,\r\n\r\nHow is it going? I also met a similar problem. Would you please show your solutions?\r\n\r\nBest regards."]}, {"number": 5442, "title": "phrase suggestion in doc", "body": "I am not very sure, but \"as well\" is adverb, \"as well as\" is conjunction or preposition. The \"as well as\" is used for the two nouns: the \"total loss\" and the \"speed\".", "comments": ["@leonardgithub, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @vrv and @tensorflower-gardener to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "Thanks!\n"]}, {"number": 5441, "title": "Update 1_notmnist.ipynb", "body": "I'm learning Tensorflow on the Udacity,and while I flow the course to test code,I found that the url in Update 1_notmnist.ipynb is wrong,it can't be open.After a while,I found the url in the video is different from the one in the file,so I pause the vedio and remenber the url.This url is ok and it pass the test perfectly. Thx for ur attention.", "comments": ["Can one of the admins verify this patch?\n", "@fire717  Maybe this is because of you are in China.So you have a little problem connect to the google server. I'm in China too,I could use this url by this way :\n'url = 'https://commondatastorage.googleapis.com/books1000/''\nThis works well for me maybe I used VPN.\n", "I tried VPN,too.I can open FACEBOOK, but this page shown like this(U can see it more clear on the picture):\n\nThis XML file does not appear to have any style information associated with it. The document tree is shown below.\n      <Error><Code>AccessDenied</Code><Message>Access denied.</Message><Details>Anonymous users does not have storage.objects.list access to bucket books1000.</Details></Error>\n\n\u57282016\u5e7411\u670808 09\u65f631\u5206, \"gengqx\"notifications@github.com\u5199\u9053:\n\n@fire717  Maybe this is because of you are in China.So you have a little problem connect to the google server. I'm in China too,I could use this url by this way :\n'url = 'https://commondatastorage.googleapis.com/books1000/''\nThis works well for me maybe I used VPN.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n\n\u57282016\u5e7411\u670809 14\u65f615\u5206, \"googlebot\"notifications@github.com\u5199\u9053:\n\nThanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n\ud83d\udcddPlease visit https://cla.developers.google.com/ to sign.\n\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n", "@fire717  Have a try: Change \u2018http\u2019 into \u2018https\u2019.\n", "FYI, we won't be moving back to the yaroslavvb.com address, as it's someone's private server. Let me know if there is anything we can do to make the Google address work better for you, but I'm afraid it's out of my hands.\n"]}, {"number": 5440, "title": "Load user defined op with relative path", "body": "How to load a user defined op with relative path? It works fine with absolute path. But I want to do it using relative path.\r\n\r\n**I used this command to build the op:** \r\n`bazel build -s --copt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" -c opt //tensorflow/core/user_ops:zero_out.so`\r\n\r\n**The build file I used is:** \r\n\r\n```python\r\nload(\"//tensorflow:tensorflow.bzl\", \"tf_custom_op_library\")\r\ntf_custom_op_library(\r\n    name = \"zero_out.so\",\r\n    srcs = [\"zero_out.cc\"],\r\n)\r\n```\r\n\r\n**The output of the build command is :** \r\n\r\n```\r\nINFO: Found 1 target...\r\n>>>>> # //tensorflow/core/user_ops:zero_out.so [action 'Creating runfiles tree bazel-out/local-opt/bin/tensorflow/core/user_ops/zero_out.so.runfiles']\r\n(cd /private/var/tmp/_bazel_sahilsingla/c1460bc7debae191fe6190e883a41a47/execroot/tensorflow && \\\r\n  exec env - \\\r\n    PATH=/Library/Frameworks/Python.framework/Versions/2.7/bin:/Library/Frameworks/Python.framework/Versions/2.7/bin:/Users/sahilsingla/Downloads/earthengine-api/demos/trendy-lights/google-cloud/google-cloud-sdk/bin:/opt/local/bin:/opt/local/sbin:/opt/local/bin:/opt/local/sbin:/Users/sahilsingla/torch/install/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:/Users/sahilsingla/.local/bin:/opt/X11/bin:/Users/sahilsingla/torch/install/lib/luarocks/rocks:/usr/local/cuda/bin:/Users/sahilsingla/.local/bin:/usr/local/mysql/bin:/usr/local/sbin \\\r\n    TMPDIR=/var/folders/n6/kj5blsmn39qfzncks2_rt8pm0000gn/T/ \\\r\n  /private/var/tmp/_bazel_sahilsingla/c1460bc7debae191fe6190e883a41a47/execroot/tensorflow/_bin/build-runfiles bazel-out/local-opt/bin/tensorflow/core/user_ops/zero_out.so.runfiles_manifest bazel-out/local-opt/bin/tensorflow/core/user_ops/zero_out.so.runfiles)\r\n>>>>> # //tensorflow/core/user_ops:zero_out.so [action 'Compiling tensorflow/core/user_ops/zero_out.cc']\r\n(cd /private/var/tmp/_bazel_sahilsingla/c1460bc7debae191fe6190e883a41a47/execroot/tensorflow && \\\r\n  exec env - \\\r\n    PATH=/Library/Frameworks/Python.framework/Versions/2.7/bin:/Library/Frameworks/Python.framework/Versions/2.7/bin:/Users/sahilsingla/Downloads/earthengine-api/demos/trendy-lights/google-cloud/google-cloud-sdk/bin:/opt/local/bin:/opt/local/sbin:/opt/local/bin:/opt/local/sbin:/Users/sahilsingla/torch/install/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:/Users/sahilsingla/.local/bin:/opt/X11/bin:/Users/sahilsingla/torch/install/lib/luarocks/rocks:/usr/local/cuda/bin:/Users/sahilsingla/.local/bin:/usr/local/mysql/bin:/usr/local/sbin \\\r\n    TMPDIR=/var/folders/n6/kj5blsmn39qfzncks2_rt8pm0000gn/T/ \\\r\n  external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-D_GLIBCXX_USE_CXX11_ABI=0' '-std=c++0x' -MD -MF bazel-out/local-opt/bin/tensorflow/core/user_ops/_objs/zero_out.so/tensorflow/core/user_ops/zero_out.pic.d '-frandom-seed=bazel-out/local-opt/bin/tensorflow/core/user_ops/_objs/zero_out.so/tensorflow/core/user_ops/zero_out.pic.o' -fPIC -DEIGEN_MPL2_ONLY -iquote . -iquote bazel-out/local-opt/genfiles -iquote external/protobuf -iquote bazel-out/local-opt/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/local-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/local-opt/genfiles/external/eigen_archive -isystem external/protobuf/src -isystem bazel-out/local-opt/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/local-opt/genfiles/external/eigen_archive -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/core/user_ops/zero_out.cc -o bazel-out/local-opt/bin/tensorflow/core/user_ops/_objs/zero_out.so/tensorflow/core/user_ops/zero_out.pic.o)\r\n>>>>> # //tensorflow/core/user_ops:zero_out.so [action 'Linking tensorflow/core/user_ops/zero_out.so']\r\n(cd /private/var/tmp/_bazel_sahilsingla/c1460bc7debae191fe6190e883a41a47/execroot/tensorflow && \\\r\n  exec env - \\\r\n  external/local_config_cc/cc_wrapper.sh -shared -o bazel-out/local-opt/bin/tensorflow/core/user_ops/zero_out.so -Wl,-all_load bazel-out/local-opt/bin/tensorflow/core/user_ops/_objs/zero_out.so/tensorflow/core/user_ops/zero_out.pic.o bazel-out/local-opt/bin/external/protobuf/libprotobuf.pic.a bazel-out/local-opt/bin/external/protobuf/libprotobuf_lite.pic.a '' -lpthread -lstdc++ -lm -undefined dynamic_lookup -headerpad_max_install_names)\r\nTarget //tensorflow/core/user_ops:zero_out.so up-to-date:\r\n  bazel-bin/tensorflow/core/user_ops/zero_out.so\r\nINFO: Elapsed time: 3.717s, Critical Path: 3.36s\r\n```\r\n\r\n**Pleas let me know if there is a way to make it work.**\r\n", "comments": ["@jart, is this easily possible?\n", "I'm afraid I don't understand the question. Where are paths being used here?\n", "@jart @aselle \n\n```\nimport os.path\nimport tensorflow as tf\n_zero_out_module = tf.load_op_library(os.path.join(tf.resource_loader.get_data_files_path(),'zero_out.so'))\nzero_out = _zero_out_module.zero_out\n```\n\nThe above code doesn't work. It shows image not found error. \ntensorflow.python.framework.errors.NotFoundError: dlopen(zero_out.so, 6): image not found\n\n```\nimport os.path\nimport tensorflow as tf\n_zero_out_module = tf.load_op_library('/Users/sahilsingla/tensorflow/bazel-bin/tensorflow/core/user_ops/zero_out.so')\nzero_out = _zero_out_module.zero_out\n```\n\nBut the above code does work properly.\n", "Does it work if you say this?\n\n``` py\nimport os.path\nimport tensorflow as tf\n_zero_out_module = tf.load_op_library(\n    tf.resource_loader.get_path_to_datafile('tensorflow/core/user_ops/zero_out.so'))\nzero_out = _zero_out_module.zero_out\n```\n\nIf not, then try adding `data = [\"//tensorflow/core/user_ops:zero_out.so\"]` to whatever Bazel py_library or py_binary rule specifies the script above.\n\nAlso another nit I would recommend is that you say `name = \"zero_out\"` rather than `name = \"zero_out.so\"`. Because if you use the latter name, the shared object will probably be named \"zero_out.so.so\".\n", "```\npy_library(\n    name = \"zero_out_op\",\n    srcs = [\"user_ops/zero_out_op.py\"],\n    data = [\"//tensorflow/core/user_ops:zero_out.so\"],\n    srcs_version = \"PY2AND3\",\n)\n```\n\nThis is the build rule for that python file. I was already using `data =[\"//tensorflow/core/user_ops:zero_out.so\"]\n`\n\n```\n_zero_out_module = tf.load_op_library(tf.resource_loader.get_path_to_datafile('tensorflow/core/user_ops/zero_out.so'))\nzero_out = _zero_out_module.zero_out\n```\n\ndoesn't work either. \n\n`print(tf.resource_loader.get_path_to_datafile('tensorflow/core/user_ops/zero_out.so'))`\ngives `tensorflow/core/user_ops/zero_out.so`\n\n`name = \"zero_out\"` gives an error\n\n`ERROR: /Users/sahilsingla/tensorflow/tensorflow/core/user_ops/BUILD:8:1: in linkshared attribute of cc_binary rule //tensorflow/core/user_ops:zero_out: 'linkshared' used in non-shared library. Since this rule was created by the macro 'tf_custom_op_library', the error might have been caused by the macro implementation in /Users/sahilsingla/tensorflow/tensorflow/tensorflow.bzl:781:31.`\n\nPlease let me know if there is a solution. \n", "Apologies for the delayed response. Is your .so file being generated in the same directory as the script that loads it? If so, you should be able to say `data = [\"zero_out_op.py\"]` and then load the op library using `tf.resource_loader.get_path_to_datafile('zero_out.so')`. If you grep the codebase, particularly in contrib, you should see many examples of this."]}, {"number": 5439, "title": "Fail to export model with exporter with distributed session", "body": "Hi,\r\n\r\nI met an issue as follow when I was trying to export the model with distributed session,\r\n\r\nDo you have any idea about how to fix this issue. Thanks\r\n\r\nExporting trained model to./model/\r\nTraceback (most recent call last):\r\n  File \"dnn_train.py\", line 399, in <module>\r\n    tf.app.run()\r\n  File \"/gruntdata/DL_dataset/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 32, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"dnn_train.py\", line 396, in main\r\n    run_training(server.target, cluster_spec)\r\n  File \"dnn_train.py\", line 318, in run_training\r\n    default_graph_signature=signature)\r\n  File \"/gruntdata/DL_dataset/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/session_bundle/exporter.py\", line 198, in init\r\n    ops.add_to_collection(constants.GRAPH_KEY, graph_any_buf)\r\n  File \"/gruntdata/DL_dataset/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 4073, in add_to_collection\r\n    get_default_graph().add_to_collection(name, value)\r\n  File \"/gruntdata/DL_dataset/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2689, in add_to_collection\r\n    self._check_not_finalized()\r\n  File \"/gruntdata/DL_dataset/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2088, in _check_not_finalized\r\n    raise RuntimeError(\"Graph is finalized and cannot be modified.\")\r\nRuntimeError: Graph is finalized and cannot be modified.\r\n", "comments": ["It looks like you finalized your model at some point and then tried to modify it afterwards. Please provide a simple test case, or else you will likely need to look into it on your own.\n", "@aselle I used a grpc for a distruibed training, and it seemed that the graph will be finalized when i create the class Supervisor, see details in file \"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/supervisor.py\". \nAnd when i tried to dump the model with the following code, it said the above error.\n`  # Export inference model.\n\n  model_exporter = exporter.Exporter(saver)\n  signature = exporter.generic_signature({\"Reshape_2:0\":i_indices_reshape,\n                                          \"Reshape_3:0\":i_values,\n                                          \"Cast_1:0\":i_shape,\n                                          \"Reshape_1:0\":indices_shape,\n                                          \"softmax_linear/add:0\":logits})\n  model_exporter.init(init_op=init_op,\n                      graph_def=sess.graph.as_graph_def(),\n                      default_graph_signature=signature)\n  model_exporter.export(FLAGS.export_dir, tf.constant(FLAGS.export_version), sess)\n  print ('Successfully exported model to %s' % FLAGS.export_dir)\n`\nI found a work around method to write the model successfully . Firstly, I saved the checkpoint file during the training process (grpc distributed version). Then I restored the checkpoint file with another session (single node version) which do not use grpc. And I can write the model for this situation. \n\nHowever, I am still wondered that how to save the model in the distributed version code?\n", "@mrry, do you have any comments on what the proper way to do this is?\n", "I'm not sure what your code looks like, but I'd suggest that you construct the `Exporter` object before starting training (which I assume is using a `Supervisor` and therefore calling `Graph.finalize()`...) to avoid this error. You can construct it immediately after constructing a `tf.train.Saver`.\n", "@mrry Thanks for the suggestion. I tried your method, I could put the clause `model_exporter = exporter.Exporter(saver)` before `Supervisor` as your guiding, however, I can't put `model_exporter.init` clause before `Supervisor`, because it need `sess = sv.prepare_or_wait_for_session(target, config=sess_config)` as one of the input parameters. So I put this clause after `Supervisor`. Then it still said I couldn't modify the graph for the clause `model_exporter.init`. Furthermore, I don't understand why this clause modify the graph? \n\nBelow is the details of my model export initialization:\n\n```\n  signature = exporter.generic_signature({\"Reshape_2:0\":i_indices_reshape,\n                                          \"Reshape_3:0\":i_values,\n                                          \"Cast_1:0\":i_shape,\n                                          \"Reshape_1:0\":indices_shape,\n                                          \"softmax_linear/add:0\":logits})\n  model_exporter.init(sess.graph.as_graph_def(),\n                      default_graph_signature=signature)\n```\n", "Ah, you don't actually need `sess` in the call to `model_exporter.init()`. Instead you could do something like:\n\n``` python\nmodel_exporter.init(tf.get_default_graph().as_graph_def(), ...)\n```\n", "I met the same error(RuntimeError: Graph is finalized...) when I call model_exporter.export(export_path, tf.constant(FLAGS.export_version), sess) which taked sess as an argument  after the Supervisor.\n", "@firewu I suspect the `tf.constant()` is causing an error in this case. This tensor should be created before you define the supervisor.\n", "@mrry Thanks!That is it!I define the tensor before the supervisor, and it works. @jinhou Maybe the same solution will work. But I have to point that I wrote my code which is  reference to  [inception_export.py ](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/example/inception_export.py)at line 131:model_exporter.export(FLAGS.export_dir, tf.constant(global_step), sess).\n", "@jinhou Please reopen if this didn't solve the original issue. ", "@jinhou I was facing the same issue and solved it by manually unfinalizing -> exporting -> re-finalizing the graph like they do here: https://www.bountysource.com/issues/43253488-is-there-anything-example-about-how-to-apply-model-saved-by-distribution-tensorflow-in-tensorflow-serving\r\n\r\nNot sure if this is considered bad practice or what unintended consequences could be. However, it works and I didn't find another way to make it work with Supervisor.", "Thanks @shafy . Unfinalizing-then-finalized will work but I don't think it's the best way. If we use `tf.train.MonitoredTrainingSession`, it is not an `Session` and can't use this.\r\n\r\nI think the APIs for distributed training which need finalizing the graph and saved model which need modifying the graph are a little conflicting."]}, {"number": 5438, "title": "TensorBoard could not refresh automatically when use HDFS path as logdir?", "body": "Environment\r\nhttps://github.com/tensorflow/ecosystem/blob/master/docker/Dockerfile.hdfs\r\n\r\nI use the following command to start a tensorflow job. It works well. However, the tensor board could not refresh automatically unless restart the tensor board server.\r\n\r\n<pre><code>\r\npython mnist.py --data_dir=hdfs://hdpalt/user/danrtsey.wy/mnist-data --train_dir=hdfs://hdpalt/user/danrtsey.wy/.slider/checkpoints/test1\r\n</code></pre>\r\n<pre><code>\r\ntensorboard --logdir=hdfs://hdpalt/user/danrtsey.wy/.slider/checkpoints/test1\r\n</code></pre>\r\n\r\nBTW, i find the file size of event file on HDFS does not update. Although, the content has changed. Is this the reason?\r\n<pre><code>\r\n$hadoop fs -ls hdfs://hdpalt/user/danrtsey.wy/.slider/checkpoints/test1/events.out.tfevents.1478500140.8e103b0b7135\r\n-rw-r--r--   3 yarn danrtsey.wy         40 2016-11-07 14:29 hdfs://hdpalt/user/danrtsey.wy/.slider/checkpoints/test1/events.out.tfevents.1478500140.8e103b0b7135\r\n</code></pre>\r\n<pre><code>\r\n$hadoop fs -cat hdfs://hdpalt/user/danrtsey.wy/.slider/checkpoints/test1/events.out.tfevents.1478500140.8e103b0b7135 | wc -l\r\n9312\r\n</code></pre>", "comments": ["@jhseu, @danmane, any ideas won why this might be?\n", "@aselle, @jhseu, @danmane, I think the reason is that when a HDFS file writing, its length got by listStatus/getFileStatus from Namenode will not be updated until the block completed or the file created with SyncFlag.UPDATE_LENGTH. But the new data flushed from OutputStream will be available to read for new InputStream. A workaround is to reopen the inputstream repeatedly, like the implementation of HBase replication.\n", "Fixed internally and will show up during the next commit sync within a day or so.\n\nWe now reopen the inputstream upon reaching EOF as suggested by @renchunde.\n", "Also note that the file size still doesn't update when listing the directory, but the new contents are available for reading and show up on tensorboard.\n", "Thanks a lot for your attention to this issue. I will help to confirm after the next commit sync. \n"]}, {"number": 5437, "title": "AlreadyExistsError: Resource _tensor_arrays", "body": "I was trying to use a tf.while loop inside another tf.while loop . I cross checked many times , but I am getting this error . If somehow managed to get over this error , I am getting another error called nothing to read from index 0 of tensor array , as nothing has been written to it . If you guys want , I can post the full code . This code is based on the modification of code by @alrojo , from his wondeful tutorial . \r\n\r\n\r\n\r\n![issue1](https://cloud.githubusercontent.com/assets/10637096/20048372/1d95eeda-a4e2-11e6-8011-36fdfaf35f8c.png)\r\n![issue2](https://cloud.githubusercontent.com/assets/10637096/20048373/1d96cf9e-a4e2-11e6-9c78-e8e8836767f9.png)\r\n\r\n", "comments": ["Please try to reduce it to a meaningful and minimal test case that reproduces the Error. Screenshots are good to show context, but it is very important to post logs in text form so future users can benefit from this issue (it is also difficult to read and follow). Please provide information about exactly your version and environment (see the template for what to include).\n", "@aselle - I will provide a detailed explanation of what the code is and where I am getting the error . \nWhat I am trying to do is , I will get a 3 x 10 matrix in we_project and I am having 3 x 10 x 5 matrix in attention_input_mod . I need to do a matmul of each row of 3 x 10 matrix to corresponding matrix batch in 3 x 10 x 5 . That is , first row 0f 3 x 10 will take a  matmul with first batch of 3 x 10 x 5 ( [0, : ,:]) and so on . For that , I am making use of tf.while_loop inside the other loop which is mostly a duplication of @alrojo code . So , I use sub_initial and sub_input to hold the 2d and 3d matrix respectively and inside sub_decoder function I will try to iterate until I reach batch_len ( 3 by default ) and I read each row and matrix from sub_initial and sub_input respectively and then trying to store the result in temp_holder TensorArray . But getting the following error\n\nAlreadyExistsError: Resource _tensor_arrays/temp_holder/N10tensorflow11TensorArrayE \n\n```\ntarget_dims = 8\nattention_dims = 10\ninput_max_len =  5\nattn_len = 3\nmax_sequence_length = 3\nnum_units = 10\nbatch_len = 3\n\nweight_initializer = tf.truncated_normal_initializer(stddev=0.1)\n\nattention_input = tf.convert_to_tensor(np.random.rand(3,5,10).astype(np.float32)) ##### consider it as RNN encoder output\nattention_input_mod = tf.transpose(attention_input , [0,2,1])\ninitial_state = tf.convert_to_tensor(np.random.rand(3,10).astype(np.float32)) ######## Last states of 3 batches of RNN\ntarget_input = tf.convert_to_tensor(np.random.rand(3,3,8).astype(np.float32))   ###### target input\ninputs = tf.transpose(target_input, perm=[1, 0, 2])\n\nvar = tf.get_variable # for ease of use\n\nW_z_x = var('W_z_x', shape=[target_dims, num_units], initializer=weight_initializer)\nW_z_h = var('W_z_h', shape=[num_units, num_units], initializer=weight_initializer)\nb_z = var('b_z', shape=[num_units], initializer=weight_initializer)\nW_r_x = var('W_r_x', shape=[target_dims, num_units], initializer=weight_initializer)\nW_r_h = var('W_r_h', shape=[num_units, num_units], initializer=weight_initializer)\nb_r = var('b_r', shape=[num_units], initializer=weight_initializer)\nW_c_x = var('W_c_x', shape=[target_dims, num_units], initializer=weight_initializer)\nW_c_h = var('W_c_h', shape=[num_units, num_units], initializer = weight_initializer)\nb_c = var('b_c', shape=[num_units], initializer=weight_initializer)\nmiddle_matrix = var('middle', shape=[num_units, num_units], initializer = weight_initializer)\n\n\n###### Reading the first element in target of each batch and creating a new state\ninput_ta = tensor_array_ops.TensorArray(tf.float32, size=1, dynamic_size=True, name = 'input_ta')\ninput_ta = input_ta.unpack(inputs)\n# calculate the GRU\ntime = tf.constant(0)\nx_t = input_ta.read(time)\nz = tf.sigmoid(tf.matmul(x_t, W_z_x) + tf.matmul(initial_state, W_z_h) + b_z) # update gate\nr = tf.sigmoid(tf.matmul(x_t, W_r_x) + tf.matmul(initial_state, W_r_h) + b_r) # reset gate\nc = tf.tanh(tf.matmul(x_t, W_c_x) + tf.matmul(r*initial_state, W_c_h) + b_c) # proposed new state\nnew_state = (1-z)*c + z*initial_state # new state\ninitial_state = new_state\n\ndef decoder_cond(time, state, output_ta_t, attention_tracker):\n    return tf.less(time, max_sequence_length)\n\n\ndef decoder_body_builder(feedback=False):\n    def decoder_body(time, old_state, output_ta_t, attention_tracker):\n        if feedback:\n            def from_previous():\n                prev_1 = tf.matmul(old_state, W_out) + b_out\n                return tf.gather(embeddings, tf.argmax(prev_1, 1))\n            x_t = tf.cond(tf.greater(time, 0), from_previous, lambda: input_ta.read(0))\n        else:\n            x_t = input_ta.read(time)\n\n        # calculate the GRU\n\n\n\n        def sub_decoder_cond(sub_time,temp_holder_):\n                return tf.less(sub_time, 3) \n\n        def sub_decoder_body_builder():\n\n            def sub_decoder_body(sub_time ,temp_holder_t):\n                sub_x_t = tf.reshape(sub_initial_.read(sub_time) , [1,-1])\n                sub_i_t = sub_input.read(sub_time)\n                sub_res = tf.matmul(sub_x_t, sub_i_t)\n                temp_holder_t.write(sub_time, sub_res)\n\n                return(sub_time+1, temp_holder_t )\n            return sub_decoder_body\n\n\n        we_project = tf.tanh(tf.matmul( initial_state , middle_matrix ))\n\n        sub_initial_ = tensor_array_ops.TensorArray(tf.float32, size=1, dynamic_size=True ,tensor_array_name='sub_initial')\n        sub_input = tensor_array_ops.TensorArray(tf.float32, size=1, dynamic_size=True, tensor_array_name = 'sub_input')\n        temp_holder = tensor_array_ops.TensorArray(tf.float32, size=1, dynamic_size=True , tensor_array_name = 'temp_holder'  )\n\n        sub_initial_ = sub_initial_.unpack(we_project)\n        sub_input = sub_input.unpack(attention_input_mod)\n        sub_time = tf.constant(0)\n\n        sub_loop_vars = [sub_time, temp_holder]\n\n        _, temp_holder_out = tf.while_loop(sub_decoder_cond,\n                                       sub_decoder_body_builder(),\n                                       sub_loop_vars,\n                                       swap_memory=False)\n\n\n\n        alpha_time = temp_holder_out.pack()\n        # temp_holder.close()\n        alpha = alpha_time\n        alpha_softmax = alpha\n        # alpha_softmax = tf.nn.softmax(alpha)\n        z = tf.sigmoid(tf.matmul(x_t, W_z_x) + tf.matmul(old_state, W_z_h) + b_z) # update gate\n        r = tf.sigmoid(tf.matmul(x_t, W_r_x) + tf.matmul(old_state, W_r_h) + b_r) # reset gate\n        c = tf.tanh(tf.matmul(x_t, W_c_x) + tf.matmul(r*old_state, W_c_h) + b_c) # proposed new state\n        new_state = (1-z)*c + z*old_state # new state\n\n        # writing output\n        output_ta_t = output_ta_t.write(time+1, new_state)\n        attention_tracker = attention_tracker.write(time, alpha_softmax)\n        # context = tf.reduce_sum(tf.expand_dims(alpha_softmax, 2) * attention_input, [1])\n\n\n        return (time + 1, new_state, output_ta_t, attention_tracker)\n    return decoder_body\n\n\noutput_ta = tensor_array_ops.TensorArray(tf.float32, size=1, dynamic_size=True, infer_shape=False)\nattention_tracker = tensor_array_ops.TensorArray(tf.float32, size=1, dynamic_size=True, infer_shape=False)\ntime = tf.constant(0)\nloop_vars = [time, initial_state, output_ta, attention_tracker]\n\n_, state, output_ta, attention_tracker_holder = tf.while_loop(decoder_cond,\n                                               decoder_body_builder(),\n                                               loop_vars,\n                                               swap_memory=False)\nA = attention_tracker_holder.pack()\nO = output_ta.pack()\n\nsess = tf.InteractiveSession()\nsess.run(tf.initialize_all_variables())\n```\n", "Without testing the code:\n`AlreadyExistsError` might be because you redefine the TensorArray at every iteration of the `while_loop`.\n\nIt does not seem like the the TensorArrays you define are dependent on anything in the `while_loop`. Try defining them outside (before).\n", "@aselle @alrojo - But , 3 Tensor array has to be empty , at each start of firs loop ( tf.while loop for attention ) . Moreover , I tried initializing the 3 Tensor Arrays outside the loop , which results in UnboundLocalError: local variable 'sub_initial_' referenced before assignment , which makes sense from the Python Point of View . I have no idea , where the code is doing wrong from the logical aspect . \n", "@aselle @alrojo  @ebrevdo - Is this a bug or is this how , tf while loop is supposed to work ? \n", "What version of tf are you using? What happens if you don't pass the TensorArrays name arguments?\n", "@ebrevdo @aselle - Without passing tensor array name as arguments also , I am getting the same error . Tensor array name was more of a debugging mode for me , to understand which Tensor array is causing the error . AlreadyExistsError: Resource _tensor_arrays/while/TensorArray_6/N10tensorflow11TensorArrayE\n\nThe tensorflow version was 0.9 \n\nI upgraded it to 0.11 . Now the error has changed . \nInvalidArgumentError: TensorArray while/TensorArray_6_3: Could not read from TensorArray index 0 because it has not yet been written to.\nHow come this error is happening , if we are properly writing it in each iteration. Please have a look . \n", "Do you read multiple times from the same index?\n\nOn Nov 13, 2016 6:23 PM, \"s4sarath\" notifications@github.com wrote:\n\n> https://github.com/ebrevdo\n> \n> @ebrevdo https://github.com/ebrevdo @aselle https://github.com/aselle\n> - Without passing tensor array name as arguments also , I am getting the\n>   same error . Tensor array name was more of a debugging mode for me , to\n>   understand which Tensor array is causing the error . AlreadyExistsError:\n>   Resource _tensor_arrays/while/TensorArray_6/N10tensorflow11TensorArrayE\n> \n> The tensorflow version was 0.9\n> \n> I upgraded it to 0.11 . Now the error has changed .\n> InvalidArgumentError: TensorArray while/TensorArray_6_3: Could not read\n> from TensorArray index 0 because it has not yet been written to.\n> How come this error is happening , if we are properly writing it in each\n> iteration. Please have a look .\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5437#issuecomment-260234116,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim1R05gz2daPqO7ff4lqwyep1uN6Gks5q98YUgaJpZM4Kq4Y7\n> .\n", "@ebrevdo Yes , but at different iterations . So the idea is we are having a main while loop , which will run say 5 times . Inside that I calculate a (m =3 x 10) matrix ( 3 here is batch size , but may vary dynamically ) . My aim is , this 3 x 10 has to multiply with a (am= 3 x 10 x 5) matrix in the following way \nIn numpy way it is \n\n```\nstore = []\nfor i in range(batch_size=3):\n    store.append( np.dot( m[i, :] , am[i,:,:]))\nstore = np.array(store)\n```\n\nThis process has to happen every time the main while loop is running . So to achieve this , I make use of tensor array , which is storing the 3 x 10 and 2 x 10 x 5 matrix to separate Tensor Array , each time the main loop is running , because , this value changes with each loop of main while loop . In simple words , it is a nested for loop . Please have a look at the code in the top comments . \n", "@ebrevdo Further comments?\n", "@aselle @ebrevdo Why no response from anyone ?", "There is an argument you can pass to the TensorArray constructor so it won't erase the tensor after a read.  You need to enable that parameter.", "@aselle @ebrevdo I think you are mentioning about clear_after_read argument. I tried that also, still same error. Can anyone have a look at the code ?", "Probably won't make a difference but try creating your TensorArrays with size 0.", "Now sess.run(A) results in\n\narray([], shape=(3, 0, 1, 10), dtype=float32) .\n\nBut no value. Error remains the same for sess.run(O). I think, if\nsomeone familiar with Tensor\n\nArray, it is just a matter of few minutes. If the way, the code has\nbeen written is wrong, can anyone suggests\n\nme an alternative, but make sure that it is differentiable.\n\n\nOn Thu, Nov 24, 2016 at 11:09 PM, ebrevdo <notifications@github.com> wrote:\n\n> Probably won't make a difference but try creating your TensorArrays with\n> size 0.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5437#issuecomment-262823480>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AKJPKIWjMdhFBkKZDFuyzMC632zc-MYvks5rBcvTgaJpZM4Kq4Y7>\n> .\n>\n", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.", "Hi @s4sarath. I'm having the same issue, also using tf.while_loop. How did you solve it? Thanks!", "Hi , I did not resolve it yet . Find some complex alternate hacks .\n\nOn Nov 10, 2017 16:34, \"Eduard Ramon Maldonado\" <notifications@github.com>\nwrote:\n\n> Hi @s4sarath <https://github.com/s4sarath>. I'm having the same issue,\n> also using tf.while_loop. How did you solve it? Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5437#issuecomment-343444081>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AKJPKBw9Qfm6-hliXiaEjcqArrdX7QAxks5s1C3AgaJpZM4Kq4Y7>\n> .\n>\n", "What version of TF are you using?  I am not able to replicate the issue.  Can you repost a **minimal** example that causes it?  And have you tried in TF 1.4?"]}, {"number": 5436, "title": "How to know whether TF is using cudnn?", "body": ">>> import tensorflow as tf\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\r\n>>> \r\n\r\n", "comments": ["\"successfully opened cuda librray libcudnn.so locallly\" says that your TF is using cudnn\n", "thank you \uff01\n", "When I import tensorflow as tf , it returns nothing! Does this mean it isn't using CuDnn?"]}, {"number": 5435, "title": "User defined op without input + control dependencies causes op execution error", "body": "1. Define a user-defined op without input, with 1 output:\r\nREGISTER_OP(\"MyOp\")\r\n  .Output(\"output: T\")\r\n  .Attr(.....)\r\n\r\n2. in python code:\r\nop1 = my_op(attrs)\r\n\r\nwith tf.control_dependencies([op1]):\r\n  op2 = otherOp\r\n\r\nThis runs OK, both op1 and op2 execute once:\r\nsess.run(op2)\r\n\r\n\r\nThis is WRONG:\r\nsess.run([op1, op2])\r\nop1 will execute 2 times.\r\n\r\nIf adding an input for MyOp, op1 will execute only once as expected for \r\n sess.run([op1, op2])", "comments": ["@skydoorkai Please include complete code.  Your description so far is unlikely to be true, since a single call to run never runs ops multiple times.\n", "Closing due to lack of response."]}, {"number": 5434, "title": "0.11.0rc2 Problem on building target with GPU support", "body": "CentOS 7.2, \r\nPython 2.7.12, \r\ncuda 8.0   //cuda-repo-rhel7-8-0-local-8.0.44-1.x86_64.rpm\r\ncudnn 5.1  //cudnn-8.0-linux-x64-v5.1.tgz\r\n\r\n$ git rev-parse HEAD\r\n9d045888a3b565e856287381b581eed02750f976\r\n\r\n$ bazel build -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\nINFO: $TEST_TMPDIR defined: output root default is '/home/rnd/tmp'.\r\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.build/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\r\nINFO: Found 1 target...\r\nERROR: /home/rnd/tmp/_bazel_rnd/707043e71401a80a1e11714c15a7b311/external/pcre/BUILD:5:1: undeclared inclusion(s) in rule '@pcre//:pcre':\r\nthis rule is missing dependency declarations for the following files included by 'external/pcre/pcre_valid_utf8.c':\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/limits.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/syslimits.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/stddef.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/stdarg.h'\r\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/stdint.h'.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 3.073s, Critical Path: 0.68s\r\n$echo $TF_NEED_CUDA\r\n1\r\n$bazel version\r\nINFO: $TEST_TMPDIR defined: output root default is '/home/rnd/tmp'.\r\nBuild label: 0.4.0-2016-11-07 (@fa407e5)\r\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Sun Nov 6 17:25:58 2016 (1478453158)\r\nBuild timestamp: 1478453158\r\nBuild timestamp as int: 1478453158\r\n$\r\n\r\n\r\nif building without GPU will be OK. And get tensorflow-0.11.0rc2-cp27-cp27m-linux_x86_64.whl\r\nif add --config=cuda will get errors like above", "comments": ["as i see, you should clean all the bazel-\\* before you build tf.\n", "@tanguofu \nI deleted bazel's  directory(~/tmp/_bazel_rnd) and rebuild bazel before run \"bazel build\", You can see the build time of bazel \n\"Build time: Sun Nov 6 17:25:58 2016 (1478453158)\"\n", "$ git rev-parse HEAD\n72a3caa1cc223fe10ac5287083ef20a911f87400\n\n$ bazel build -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package --verbose_failures\nINFO: $TEST_TMPDIR defined: output root default is '/home/rnd/tmp'.\n......\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.build/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nINFO: Found 1 target...\nERROR: /home/rnd/tmp/_bazel_rnd/707043e71401a80a1e11714c15a7b311/external/zlib_archive/BUILD:5:1: undeclared inclusion(s) in rule '@zlib_archive//:zlib':\nthis rule is missing dependency declarations for the following files included by 'external/zlib_archive/gzclose.c':\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/stddef.h'\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/stdarg.h'\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/limits.h'\n  '/lib/gcc/x86_64-redhat-linux/4.8.5/include/syslimits.h'.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nINFO: Elapsed time: 6.166s, Critical Path: 1.28s\n$ \n\nthe new version still have errors, it seems that once you add \"--config=cuda\", you will get  the include error.\nI tried to build zlib at  $TEST_TMPDIR directory,  it succeeded,\nIn fact I have already installed zlib 1.2.8, why bazel  download another on?\n", "The correct path of stddef.h is\n'/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/stddef.h'\nand in error message is \n'/lib/gcc/x86_64-redhat-linux/4.8.5/include/stddef.h'\n\nits lack of /usr \n", "@AntonyM55 are you still having those problems with 0.12r?", "CentOS is not officially supported for neither bazel, nor TF.\r\nDid you ry using the cmake build under contrib?\r\nThat might be easier to run.", "Closing issue due to inactivity.\r\nIn the unsupported OSs, asking via Stackoverflow might get better help, as you will have a wider audience there, which will include more CentOS users."]}, {"number": 5433, "title": "Fix misleading SaveDef V2 format warning", "body": "Adding a line: **Notice: the V2 format output different set of files**\r\n\r\nThe original warning made me think I can switch simply by adding the `write_graph` argument, but it turns out the outputs of `saver.save(sess, \"model.ckpt\")` are also different with the new format.\r\n\r\nOriginally, it writes to the file `model.ckpt`, but now it writes to `model.ckpt.index` and `model.ckpt.data.*`, which breaks the logic.", "comments": ["Can one of the admins verify this patch?\n", "@raingo, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @tensorflower-gardener and @concretevitamin to be potential reviewers.\n", "Hey @raingo: thanks for your PR.  I don't think the deprecation warning is the best place to convey the technical details of the V2 format.  I'll think about what's a more appropriate place to document the different naming scheme.\n"]}, {"number": 5432, "title": "Error building from source", "body": "Hi,\r\n\r\nI'm trying to build from master branch and getting this error during the `./configure` step:\r\n```\r\nERROR: /home/ghedeon/tf/tensorflow/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_input//': Error downloading from https://github.com/polymerelements/iron-input/archive/1.0.10.tar.gz to /home/ghedeon/.cache/bazel/_bazel_ghedeon/dad96730576e51486fd99d150d1fdbd2/external/iron_input: Error downloading https://github.com/polymerelements/iron-input/archive/1.0.10.tar.gz to /home/ghedeon/.cache/bazel/_bazel_ghedeon/dad96730576e51486fd99d150d1fdbd2/external/iron_input/1.0.10.tar.gz: Timed out connecting to https://github.com/polymerelements/iron-input/archive/1.0.10.tar.gz : connect timed out and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/ghedeon/tf/tensorflow/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_input//': Error downloading from https://github.com/polymerelements/iron-input/archive/1.0.10.tar.gz to /home/ghedeon/.cache/bazel/_bazel_ghedeon/dad96730576e51486fd99d150d1fdbd2/external/iron_input: Error downloading https://github.com/polymerelements/iron-input/archive/1.0.10.tar.gz to /home/ghedeon/.cache/bazel/_bazel_ghedeon/dad96730576e51486fd99d150d1fdbd2/external/iron_input/1.0.10.tar.gz: Timed out connecting to https://github.com/polymerelements/iron-input/archive/1.0.10.tar.gz : connect timed out and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: Evaluation of query \"deps((//tensorflow/... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\r\n```\r\n\r\ntf master/9d04588\r\nBazel 0.4.0\r\nCUDA 7.5\r\ncuDNN 5.0\r\npython 3.5.2\r\n\r\n", "comments": ["see if you can wget https://github.com/polymerelements/iron-input/archive/1.0.10.tar.gz\nif so, try starting from a new sandbox or clean expunging bazel.\nyou did not specify what platform you are using as well. Thanks!\n", "Platform is Arch Linux (4.8.6-1, GeForce GTX 960M), Virtualenv installation, so, no sandboxes..\n\nWget works just fine and looks like it has nothing to do with that specific package. It's just extremely flaky and fails all the time on different packages either with `time out` either with `no route to host`. It's not the best connection (~100Mbps), but I believe it should be enough. Is there any way to increase timeouts for `.configure`?\n", "@jart do you have any ideas?\n", "Thank you for bringing this to our attention. We're aware of reliability issues with downloading dependencies, but right now we're predicting it's somewhere between 99% and 99.9% reliable. If it's less reliable than that, then chances are the issue might be with your internet connection or configuration. We recommend reaching out to your service provider, or reaching out to Stack Overflow to see if they can help you troubleshoot.\n", "Two more similar issues with no resolution (except downloading dependencies manually)\nDefinitely, it has something to do with bazel and not our connections.\nNo proxies are used and there are no issues with other build systems (maven, gradle).\n\nhttps://github.com/tensorflow/tensorflow/issues/5080\nhttps://github.com/tensorflow/tensorflow/issues/5029#issuecomment-254769127\n", "#5029 was due to the user configuring an HTTP proxy that didn't exist. #5080 is the same as yours, but the error is also due to GitHub.\n\nIf you're confident your Internet is OK, then this could just be issues with GitHub's service. According to their [status site](https://status.github.com/graphs/past_month) the service availability for GitHub was 98.7% the past thirty days, which is worse than I thought.\n\nIn the near future I'm going to be adding a feature to Bazel fixing https://github.com/bazelbuild/bazel/issues/1814 which will allow us to mirror all the files TensorFlow downloads, so a dependent server going down won't break builds.\n\nIf you still think this is an issue in Bazel, and can make a case for that with technical details, then I would recommend filing a bug against them.\n", "I just did a lot of work to improve Bazel's downloader. A change is going to be pushed soon adding super reliability.", "This has been fixed by https://github.com/bazelbuild/bazel/commit/ed7ced0018dc5c5ebd6fc8afc7158037ac1df00d. Try using Bazel at HEAD. A new Bazel will be cut in a few days. As soon as that happens, the TensorFlow repository will be updated to use multiple mirrors for each external file.", "Just as a heads-up: this or a similar issue still caused problems for me, when using ./configure before building TensorFlow (1.0) from source. On a Ubuntu 16.04 machine with reliable internet connection (no proxy or anything funny), CUDA 8.0, CuDNN 5.1, and apparently after @jart's bazel fix that includes multiple mirrors.\r\n\r\nIt seems to me to be some bazel timeout issue, where the timeout happens perhaps too quickly for some files. In my case configure couldn't download the swig-3.0.8.tar.gz file and ended with:\r\n\r\nBUILD:2275:1: every rule of type _py_wrap_cc implicitly depends upon the target '@swig//:swig', but this target could not be found because of: no such package '@swig//': Error downloading [http://bazel-mirror.storage.googleapis.com/ufpr.dl.sourceforge.net/project/swig/swig/swig-3.0.8/swig-3.0.8.tar.gz, http://ufpr.dl.sourceforge.net/project/swig/swig/swig-3.0.8/swig-3.0.8.tar.gz, http://pilotfiber.dl.sourceforge.net/project/swig/swig/swig-3.0.8/swig-3.0.8.tar.gz] to /home/bram/.cache/bazel/_bazel_bram/f835375b14587b3eef360885d19162a0/external/swig/swig-3.0.8.tar.gz: All mirrors are down: [].\r\nERROR: Evaluation of query \"deps((//tensorflow/... - //tensorflow/examples/android/...))\" failed: errors were encountered while computing transitive closure.\r\n\r\nI had no problems downloading the file with wget. It took a considerable number of seconds to make the connection though (although, like I said, my internet connection is solid -- but in Europe); which is why I suspect bazel timeouts a bit too quickly in some cases. It would be nice if this were easily configurable.\r\n\r\nIn the end I used the workaround suggested by sunil3590 on Aug 1, 2016 as described here: https://github.com/bazelbuild/bazel/issues/587 . Basically downloading the file yourself, using `python -m SimpleHTTPServer 8000` to host it locally, and adapt workspace.bzl to download that file from that local host instead of from the original, remote sites. That worked but isn't pretty.\r\n"]}, {"number": 5431, "title": "fix path error for libcudnn.x", "body": "fix Invalid path to cuDNN  toolkit. Neither of the following two files can be found when using cuda configure", "comments": ["Can one of the admins verify this patch?\n", "@atom2ueki, thanks for your PR! By analyzing the history of the files in this pull request, we identified @caisq, @vrv and @lukeiwanski to be potential reviewers.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "(Closing since we can't accept PRs without CLA signed, please sign if you would like this PR to go through, but first check that it is needed :)\n"]}, {"number": 5430, "title": "RNN AutoEncoder", "body": "Hello, I am writting implementation for RNN autoencoder. The decoder layer is trying to predict next word.\r\n\r\nMy code looks like this:\r\n```\r\nclass RnnAutoencoder():\r\n   def __init__(...):\r\n   ...\r\n   # Encoder\r\n   self.z_codes, self.enc_state = tf.nn.dynamic_rnn(self._enc_cell, inputs, dtype=tf.float32)\r\n\r\n   # Intermediate layer after encoder\r\n   last_output = self.z_codes[-1]\r\n   self.pred_m = tf.nn.relu(tf.matmul(last_output, self.weight_m) + self.bias_m)\r\n\r\n   # Decoder\r\n   dec_input = self.pred_m\r\n   dec_state = self.enc_state\r\n\r\n   dec_outputs = []\r\n   for step in range(len(inputs)):\r\n      if step>0: vs.reuse_variables()\r\n      dec_input_, dec_state = self._dec_cell(dec_input_, dec_state)\r\n      dec_input_ = tf.matmul(dec_input_, dec_weight_) + dec_bias_\r\n      dec_outputs.append(dec_input_)\r\n\r\n\r\ninputs = tf.placeholder(tf.float32, [1, None, 300])\r\n\r\n```\r\n\r\nBut I get error:\r\nTypeError: object of type 'Tensor' has no len()\r\non line:     for step in range(len(inputs)):\r\n\r\nI know that i cannot apply len on Tensor in tensorflow but how can I iterate(for variable time steps) and do it the way that **next input of decoder is output from previous step**(of the same decoder). Basic tf.nn.dynamic_rnn doesn't work with this situation.\r\n\r\nI read many implementations of lstm, rnn, auencoders on github but it is always with fixed size sequences or the implementation of decoder which doesn't get input which is output from previous step. I read seq2seq tutorial but it doesn't use the intermediate layer for encoding sequences as I need in my model and also i am not sure if it can handle dynamic size of sequences.\r\n\r\n", "comments": ["`Tensor` is not iterable, but you can get its shape by calling `.get_shape()`. [source](https://github.com/tensorflow/tensorflow/blob/f7ec99516ce0e0937e0b865e90aa02c748cd36c6/tensorflow/python/framework/ops.py#L345)\n", "@zafartahirov , yes i know that but again it is not helpful because shape is (1, ?, 300) and i need to know/iterate the length of sequence(which can be different for every sequence).\n", "Use `tf.shape`.\n"]}, {"number": 5429, "title": "Update README.md. Wrong graph name.", "body": "", "comments": ["@Ghedeon, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @martinwicke and @petewarden to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Thanks for this fix!\n"]}, {"number": 5428, "title": "How to make the ./configure find package in local place", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nMy server cannot connect to the Internet, when I `./configure`, it will download package from Internet, of course this will fail. So I download the package manually.\r\nE.G.\r\n`ERROR: package contains errors: tensorflow/core/debug.\r\nERROR: error loading package 'tensorflow/core/debug': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf//': Error downloading from http://github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz to /mnt/a/usr/75c879a7665c1f6bb168362b3b0eb86b/external/protobuf: Error downloading http://github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz to /mnt/a/usr/75c879a7665c1f6bb168362b3b0eb86b/external/protobuf/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: Failed to connect to http://github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz : github.com.`\r\n\r\nI download the `protobuf-008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz` and move it under `/mnt/a/usr/75c879a7665c1f6bb168362b3b0eb86b/external/protobuf/`, but it still fails. It seems the `./configure` will ignore the local package\r\n\r\nAnyway to solve this...? It's troubling to install tensorflow on the NFS without Internet.\r\n### Environment info\r\nOperating System:\r\nLinux gs07 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt25-2+deb8u3 (2016-07-02) x86_64 GNU/Linux\r\nInstalled version of CUDA and cuDNN: cuda 8.0 cudnn 5.1\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n```\r\n-rw-r--r-- 1 root staff   558720 Nov  4 10:59 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root staff       16 Nov  4 10:59 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root staff       19 Nov  4 10:59 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rwxr-xr-x 1 root staff   415432 Nov  4 10:59 /usr/local/cuda/lib64/libcudart.so.8.0.44\r\n-rw-r--r-- 1 root staff   775162 Nov  4 10:59 /usr/local/cuda/lib64/libcudart_static.a\r\n-rwxr-xr-x 1 root staff 79337624 Nov  4 11:10 /usr/local/cuda/lib64/libcudnn.so\r\n-rwxr-xr-x 1 root staff 79337624 Nov  4 11:10 /usr/local/cuda/lib64/libcudnn.so.5\r\n-rwxr-xr-x 1 root staff 79337624 Nov  4 11:10 /usr/local/cuda/lib64/libcudnn.so.5.1.5\r\n-rw-r--r-- 1 root staff 69756172 Nov  4 11:10 /usr/local/cuda/lib64/libcudnn_static.a\r\n```\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["Might be similar to #587 and #5029 \n", "Can you try installing the protobuf using `pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/protobuf-3.0.0-cp27-none-linux_x86_64.whl`\n\nYou can download the `.whl` file on another machine, and after transferring it to the one that is not connected to the internet, do `pip install --upgrade my_package_file.whl`\n", "I use ' pip install --upgrade protobuf-3.0.0-cp27-none-linux_x86_64.whl' but get:\n\n```\nUnpacking ./protobuf-3.0.0-cp27-none-linux_x86_64.whl\nDownloading/unpacking six>=1.9 (from protobuf==3.0.0)\n  Cannot fetch index base URL https://pypi.python.org/simple/\n  Could not find any downloads that satisfy the requirement six>=1.9 (from protobuf==3.0.0)\nCleaning up...\nNo distributions at all found for six>=1.9 (from protobuf==3.0.0)\nStoring debug log for failure in /home/chenggaofeng/.pip/pip.log\n```\n\n@zafartahirov  any idea?\n", "This is odd. Can you show the contents of the `pip.log`? I am using OS X Sierra, but `pip install` works fine, maybe it is only on Linux? I will try to run it on Ubuntu tomorrow, if no solution is found by then.\n\nHere is my run sequence:\n\n``` bash\n$> mkvirtualenv dev-test\n(dev-test) $> wget https://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.0.0-cp27-cp27m-macosx_10_11_x86_64.whl\nprotobuf-3.0.0-cp27-none- 100%[===================================>]   4.58M  2.33MB/s    in 2.0s\n(dev-test) $> pip install --upgrade protobuf-3.0.0-cp27-cp27m-macosx_10_11_x86_64.whl\nProcessing ./protobuf-3.0.0-cp27-cp27m-macosx_10_11_x86_64.whl\nRequirement already up-to-date: setuptools in /Users/zafar/.virtualenvs/dev-test/lib/python2.7/site-packages (from protobuf==3.0.0)\nCollecting six>=1.9 (from protobuf==3.0.0)\n  Downloading six-1.10.0-py2.py3-none-any.whl\nInstalling collected packages: six, protobuf\nSuccessfully installed protobuf-3.0.0 six-1.10.0\n```\n\nEDIT:\nOK, according to the [SO](http://stackoverflow.com/questions/24101300/pip-could-not-find-any-downloads-that-satisfy-the-requirement-sqlalchemy), you might have problems with your `.pypirc`. The possible solutions are in the formentioned SO post. Also, try [`--index`](http://pip.readthedocs.io/en/latest/reference/pip_wheel/#index-url).\n\nAccording to the Google search results, it might be happening because you are using `HTTP` and not `HTTPS`\n", "how about edit /tensorflow/workspace.bzl\nchange url to your local place\nfor example chang\nurl = \"http://www.bzip.org/1.0.6/bzip2-1.0.6.tar.gz\",\nto \nurl = \"file:////mnt/a/usr/bzip2-1.0.6.tar.gz\",\n", "@zafartahirov I download the source code of mock six protobuf etc... and make it....\n", "@zafartahirov  I do like this:\n`sudo pip install --upgrade six-1.9.0-py2.py3-none-any.whl`\n\n```\nUnpacking ./six-1.9.0-py2.py3-none-any.whl\nInstalling collected packages: six\nSuccessfully installed six\nCleaning up...\n```\n\nIt shows the six installation is successful.\nbut`pip show six`:\n\n```\nName: six\nVersion: 1.8.0\nLocation: /usr/lib/python2.7/dist-packages\nRequires:\n```\n\nany idea? :)\n", "@GaofengCheng What is your environment setup? Would it be possible to create a clean virtual environment and test it there? Also, can you provide the `pip list` and content of `pip.log`\n\nI think this is a related by different issue -- because even after install, your pip is using a different version, my suspicion is that your `python` and `pip` are located in different locations. If you try using `virtualenv`, it might fix that.\n", "@zafartahirov I find the way to install. First all the packages like six, mock update using `apt`, the python have default installed mock, six but the version is very low(at least in the debian jessie stable), OS will seem ignore the manually updating. Using the `apt`, installation of tensor(whl) is very easy.\n", "@zafartahirov Any way, Thx for your help!\n", "@AntonyM55 it seems change http url directly to local location doesn't work, any other idea?\n", "@AntonyM55  I just miss the `file:///`..... thxs! but I have encountered another problem. There is a url direct  ed to fit:\n\n```\n  native.new_git_repository(\n    name = \"linenoise\",\n    commit = \"c894b9e59f02203dbe4e2be657572cf88c4230c3\",\n    init_submodules = True,\n    remote = \"https://github.com/antirez/linenoise.git\",\n    build_file = str(Label(\"//:linenoise.BUILD\")),\n  )\n```\n\nI change`remote = \"https://github.com/antirez/linenoise.git\",` to remote = `\"file:////nobackup/datapool/project/proa/chenggaofeng/kaldi_tens/dependences/linenoise\",` \nbut when I `./configure`,  it shows:\n\n```\nERROR: /nobackup/datapool/project/proa/usr/kaldi_tens/tensorflow-master/tensorflow/tools/tfprof/BUILD:22:1: no such package '@linenoise//': Invalid Git repository URI: Invalid remote: origin and referenced by '//tensorflow/tools/tfprof:tfprof'.\nERROR: /nobackup/datapool/project/proa/usr/kaldi_tens/tensorflow-master/tensorflow/tools/tfprof/BUILD:22:1: no such package '@linenoise//': Invalid Git repository URI: Invalid remote: origin and referenced by '//tensorflow/tools/tfprof:tfprof'.\nERROR: Evaluation of query \"deps((//tensorflow/... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\n```\n", "Hello, GaofengCheng\r\n\r\nDid you solve this problem? If so, could you explain how you solve this?\r\n\r\nMy situation is exactly same as you. My company's firewall blocks my server from the Internet connection.  So no Internet connection is available for my server. This caused tens of problems, but I solved all the error except this part. I'll restate my problem as follows. When I run\r\n  $ ./configure\r\n, the following command fails.\r\n   $ bazel fetch //tensorflow/...\r\n\r\nThis bazel command is supposed to fetch all the packages in target tensorflow from the Internet. I also downloaded links manually with wget, but it seems something is wrong when I run the next command in the TensorFlow installation manual.\r\n\r\n$ bazel build -c opt -config=cuda //tensorflow/cc:tutorials_example_trainer\r\n\r\nI'd appreciate your feedback. \r\n"]}, {"number": 5427, "title": "tensorflow.contrib.layers.feature_column_ops with tf.placeholder tensors", "body": "Following up on the TF.Learn tutorials demonstrating linear models with categorical variables (https://www.tensorflow.org/versions/r0.9/tutorials/linear/overview.html), I have tried to re-use the functions & abstractions from `tensorflow.contrib.layers.feature_column_ops` to hash, encode & cross my input categorical columns while still handling the rest of the computing in low-level TensorFlow.\r\n\r\nHowever, when I pass a normal Tensor (or placeholder) as values of the `columns_to_tensors` parameters of `weighted_sum_from_feature_columns` (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/feature_column_ops.py#L460), it fails because of the `shape` parameter being not available.\r\n\r\nIs there any way or plan to enable passing any Tensors other than SparseTensors to this function?", "comments": ["Please ask this question on StackOverflow as many users of TensorFlow contribute answers to those questions.. If you discover a concrete feature or bug in behavior, please repost here. Thanks!\n"]}, {"number": 5426, "title": "bazel test failing for tensorflow new op on MAC OS X", "body": "import tensorflow as tf\r\n\r\nclass ZeroOutTest(tf.test.TestCase):\r\n\tzero_out_module = zero_out_module = tf.load_op_library('/Users/sahilsingla/tensorflow/bazel-bin/tensorflow/core/user_ops/zero_out.so')\r\n\tdef testZeroOut(self):\r\n\t\twith self.test_session():\r\n\t\t\tresult = zero_out_module.zero_out([5, 4, 3, 2, 1])\r\n\t\t\tvalue = result.eval()\r\n\t\tself.assertAllEqual(value, [5, 0, 0, 0, 0])\r\n\r\nif __name__ == \"__main__\":\r\n\ttf.test.main()\r\n\r\nWhen I run: bazel test tensorflow/python/kernel_tests:zero_out_op_test\r\n\r\nI get the error: tensorflow.python.framework.errors.NotFoundError: dlopen(zero_out.so, 6): image not found.\r\n\r\nThe funny thing is this does load when I do : python -c \"import tensorflow as tf; zero_out_module = tf.load_op_library('/Users/sahilsingla/tensorflow/bazel-bin/tensorflow/core/user_ops/zero_out.so')\"\r\nbut doesn't load with bazel test.\r\n\r\nCan anyone please suggest what I am doing wrong here? How can this error be removed?", "comments": ["I was able to solve it by changing the bazel build rules. \n\nbazel build -c opt //tensorflow/g3doc/how_tos/adding_an_op:zero_out_op_kernel_1.so\nbazel test //tensorflow/g3doc/how_tos/adding_an_op:zero_out_1_test\n\nThese commands worked properly (the ones given in the documentation didn't). So I got an idea from these to solve my issue.\n\nBut I am still not able to include the module without specifying an absolute path. How to solve that problem?\n"]}, {"number": 5425, "title": "XCode CpResource imagenet_comp_graph_label_strings.txt No such file or directory", "body": "When I compile the iOS simple project I get the following error:\r\n\r\n> CpResource data/imagenet_comp_graph_label_strings.txt /Users/CYL/Library/Developer/Xcode/DerivedData/tf_ios_makefile_example-agwbsoccjjkvrwgzihhrjmljlwss/Build/Products/Debug-iphonesimulator/tf_ios_makefile_example.app/imagenet_comp_graph_label_strings.txt\r\n>     cd /Users/CYL/tensorflowGIT/tensorflow/contrib/ios_examples/simple\r\n>     export PATH=\"/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/usr/bin:/Applications/Xcode.app/Contents/Developer/usr/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin\"\r\n>     builtin-copy -exclude .DS_Store -exclude CVS -exclude .svn -exclude .git -exclude .hg -resolve-src-symlinks /Users/CYL/tensorflowGIT/tensorflow/contrib/ios_examples/simple/data/imagenet_comp_graph_label_strings.txt /Users/CYL/Library/Developer/Xcode/DerivedData/tf_ios_makefile_example-agwbsoccjjkvrwgzihhrjmljlwss/Build/Products/Debug-iphonesimulator/tf_ios_makefile_example.app\r\n\r\n> error: /Users/CYL/tensorflowGIT/tensorflow/contrib/ios_examples/simple/data/imagenet_comp_graph_label_strings.txt: No such file or directory\r\n\r\n![screen shot 2016-11-06 at 12 13 27](https://cloud.githubusercontent.com/assets/10122382/20037125/98009b8e-a41a-11e6-88da-7758b79134c5.png)\r\n\r\nI did everything up to the compile in XCode part as described [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/ios_examples). I downgraded from XCode 8 to XCode 7.3.1 with no change. I have MacOS El Capitan.", "comments": ["Okay just found it out myself...\n\nIn the part in the tutorial\n\n> mkdir -p ~/graphs\n> curl -o ~/graphs/inception5h.zip \\\n>  https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip \\\n>  && unzip ~/graphs/inception5h.zip -d ~/graphs/inception5h\n> cp ~/graphs/inception5h/\\* tensorflow/contrib/ios_examples/benchmark/data/\n> cp ~/graphs/inception5h/\\* tensorflow/contrib/ios_examples/camera/data/\n> cp ~/graphs/inception5h/\\* tensorflow/contrib/ios_examples/simple/data/\n\nThe cp parts are meant to be executed seperately. So the first 4 lines are one command and the next 3 lines are seperate commands. I'm new to mac and don't have that much experience with Linux and I didn't got that at first.\n"]}, {"number": 5424, "title": "added full relative path to compile_ios_protobuf", "body": "The command before and after this command have the full path. That was confusing for me and is a mistake if you ask me.", "comments": ["@codingyourlife, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @martinwicke and @petewarden to be potential reviewers.\n", "Can one of the admins verify this patch?\n"]}]