[{"number": 49295, "title": "tensorflow 2.5.0 cuda and cudnn version", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\n\r\n[https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source)\r\n[https://github.com/tensorflow/docs/blob/master/site/en/install/source.md](https://github.com/tensorflow/docs/blob/master/site/en/install/source.md)\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\nThe document shows : \r\n<table>\r\n<tr><th>Version</th><th>Python version</th><th>Compiler</th><th>Build tools</th><th>cuDNN</th><th>CUDA</th></tr>\r\n<tr><td>tensorflow-2.5.0</td><td>3.6-3.9</td><td>GCC 7.3.1</td><td>Bazel 3.1.0</td><td>8.0</td><td>11.0</td></tr>\r\n</table>\r\n\r\nBut according to [https://github.com/tensorflow/tensorflow/releases/tag/v2.5.0](https://github.com/tensorflow/tensorflow/releases/tag/v2.5.0)\r\n\r\n\r\n```\r\nTensorFlow pip packages are now built with CUDA11.2 and cuDNN 8.1.0\r\n```\r\nThis table should be changed to : \r\n<table>\r\n<tr><th>Version</th><th>Python version</th><th>Compiler</th><th>Build tools</th><th>cuDNN</th><th>CUDA</th></tr>\r\n<tr><td>tensorflow-2.5.0</td><td>3.6-3.9</td><td>GCC 7.3.1</td><td>Bazel 3.1.0</td><td>8.1</td><td>11.2</td></tr>\r\n</table>\r\n", "comments": ["I very strongly second this ticket.  The outdated docs are [misleading and unhelpful](https://github.com/tensorflow/tensorflow/issues/49233).  TF 2.5.0 GPU works perfectly with CUDA 11.2.2 and cuDNN 8.1.1, at least on Windows 10 x64 in a conda environment running python 3.9 into which TF was installed using `pip install tensorflow`.\r\n\r\nI also think that the [GPU Support page](https://www.tensorflow.org/install/gpu#software_requirements) should be updated to reflect CUDA and cuDNN versions that TensorFlow 2.5.0 was built with and designed for.  Recommended TensorRT version should also be examined (look at 7.2.3: version 6.0 is outdated).", "The docs are now fixed with commit [40ce186](https://github.com/tensorflow/docs/commit/40ce1868dbb9a36c38227e4a5ca69545ed9cba67) Thanks!", "While the [Build from source](https://www.tensorflow.org/install/source) docs has been updated, the [GPU support](https://www.tensorflow.org/install/gpu#software_requirements) page has not. It's still referencing Cuda 11.0 which is not up-to-date with TF 2.5.0.", "There's a fix in flight for the gpu page too.\r\n\r\nhttps://github.com/tensorflow/docs/pull/1900"]}, {"number": 49294, "title": "TF Import Error ", "body": "Hello Everyone\r\n\r\nWhile importing Tensorflow, I am getting an error which is attached here. \r\n\r\nBut next time importing the TensorFlow, it works well.\r\n\r\ncan you please comment on it.\r\n\r\n![err2](https://user-images.githubusercontent.com/61820415/118755553-b3680800-b886-11eb-8147-da9f599c39a9.JPG)\r\n\r\n\r\n", "comments": ["Sorry we don't officially support TF in conda env here.\r\nIf you still need conda you can ask support at https://github.com/AnacondaRecipes/tensorflow_recipes", "@SanjuSoni ,\r\n\r\nWe see that you are using tf version 1.9.0, there is no support for 1.x, please update to 2.x and let us know if you are using same issue.\r\n\r\nAlso as bhack mentioned above, Installation issues within the Anaconda environment are tracked in the Anaconda repo. Could you please submit new issue in [Anaconda repository](https://github.com/ContinuumIO/anaconda-issues/issues).\r\n\r\nThanks!", "Thanks @tilakrayal  and @bhack  for your comment.\r\n\r\nI am going to upgrade Tensorflow Version.\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49294\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49294\">No</a>\n"]}, {"number": 49293, "title": "tflite::InterpreterBuilder::BuildLocalIndexToRegistrationMapping() SEGV_ACCERR", "body": "Hardware Model: iPhone8 - 12\r\nProcess: Demo [2110]\r\nVersion: 9.10.0(9.10.0)\r\nCode Type: ARM-64 (Native)\r\nParent Process:  [1]\r\nOS Version: iPhone OS 13.6 (17G68) (all version of IOS)\r\nReport Version: 104\r\n\r\nException Type: SIGSEGV\r\nException Codes: SEGV_ACCERR at 0x00000001238278f8\r\nCrashed Thread: 0\r\n\r\nThread 0 Crashed: \r\n0  Demo                     0x00000001021dec6c tflite::InterpreterBuilder::BuildLocalIndexToRegistrationMapping() +  108\r\n1  Demo                     0x00000001021e0114 tflite::InterpreterBuilder::operator()(std::__1::unique_ptr<tflite::Interpreter, std::__1::default_delete<tflite::Interpreter> >*, int) +  104\r\n2  Demo                     0x0000000101f1a918 mdw::LiteModel::Initialize(char const*, unsigned long, bool, mdw::Config const&) +  320", "comments": ["This crash occur all version iPhone&&IOS\uff0cabout 0.1% probability .I use mmap to load model on IOS, and byte alignment 64 compile. so please any suggest  THKS\uff01\uff01", "Were you able to reproduce this with a debug version?", "> Were you able to reproduce this with a debug version?\r\n\r\nIt's difficult o(\u2565\ufe4f\u2565)o, The probability is low, and our production is not allowed with debug version online.\r\nwe use same c++ code on android and ios. android is ok, the crash occur on ios when we use mmap load model\uff0cif we don't  use mmap is ok.\r\nfrom SEGV_ACCERR , I guess  when tflite load model the model file is changed.   but i'm not sure", "The information you provided here is unfortunately too high-level for us to be able to provide any concrete suggestions. I do agree that `SEGV_ACCERR` suggests some kind of read/write race condition, but not sure.\r\n\r\nCan you reduce this down to a smaller app where the crash issue can be reproduced?", "My question is solved, the guess before is correct, the model is changed by other thread when loadiing. Thank you very much", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49293\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49293\">No</a>\n"]}, {"number": 49292, "title": "Update metrics.py: Fix SparseCategoricalAccuracy.update_state() doc string", "body": "Update metrics.py: Fix SparseCategoricalAccuracy.update_state() doc string\r\n\r\nTry fixing issue #49252 \r\n\r\nFor SparseCategoricalAccuracy, `y_true` should be integer labels and `y_pred` should be probabilities.\r\n\r\n\r\n```\r\n    m = tf.keras.metrics.SparseCategoricalAccuracy()\r\n\r\n    m.update_state([[2], [1]], [[0.1, 0.6, 0.3], [0.05, 0.95, 0]])  # Correct usage,  `y_true` as integer labels and `y_pred` as probabilities.\r\n    print(m.result().numpy())\r\n    # >>> 0.5\r\n\r\n    m.update_state([[2], [1]], [[1], [1]])  # Wrong usage, both as integer labels.\r\n    print(m.result().numpy())\r\n    # >>> 0.25\r\n\r\n    m.update_state([[0, 1, 0], [0, 1, 0]], [[0.1, 0.6, 0.3], [0.05, 0.95, 0]])  # Wrong usage, both as probabilities.\r\n    print(m.result().numpy())\r\n    # >>> Error\r\n```", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49292) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "This PR is mirrored to https://github.com/keras-team/keras/commit/4244fba008d686f644b2ea3711e1897edf32432b. Closing it now."]}, {"number": 49291, "title": "Hola desde hace d\u00edas presento un problema con tensorflow lo he intentando todo he le\u00eddo cada gu\u00eda y parece siempre llego al mismo punto ", "body": "como se ve en la siguiente imagen mi problema es que al parecer \r\n![image](https://user-images.githubusercontent.com/67806145/118743784-aad8e880-b818-11eb-952a-fc4ac82d02f6.png)\r\n", "comments": ["Sorry we don't officially support  TF in conda env here.\r\nIf you still need conda you can ask support at https://github.com/AnacondaRecipes/tensorflow_recipes", "@diamar338 \r\nClosing this issue since it's related to conda env. Feel free to post another issue if still having problems installing TF using pip.\r\nIf you still require please post this in this [community](https://github.com/ContinuumIO/anaconda-issues/issues).\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49291\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49291\">No</a>\n"]}, {"number": 49290, "title": "CherryPick:2.4:PR #46974: Fix crash of tf.strings.substr when pos and len have different shapes", "body": null, "comments": []}, {"number": 49289, "title": "CherryPick:2.3:PR #46974: Fix crash of tf.strings.substr when pos and len have different shapes", "body": null, "comments": []}, {"number": 49288, "title": "CherryPick:2.2:PR #46974: Fix crash of tf.strings.substr when pos and len have different shapes", "body": null, "comments": []}, {"number": 49287, "title": "CherryPick:2.1:PR #46974: Fix crash of tf.strings.substr when pos and len have different shapes", "body": null, "comments": []}, {"number": 49286, "title": "[INTEL MKL] re-submit rsqrt op bfloat16 support", "body": "the reverted PR is here: \r\nhttps://github.com/tensorflow/tensorflow/pull/43167", "comments": ["the original PR was here:\r\nhttps://github.com/tensorflow/tensorflow/pull/43167", "Unfortunately, this was rolled back since it broke an MLIR test. Will investigate.", "> Unfortunately, this was rolled back since it broke an MLIR test. Will investigate.\r\n\r\n@reedwm Do you have estimate when your test can be fixed? We need this op bf16 support is enabled, or we have to cast back and forth from fp32 to bf16 due to this op in our models when we run bf16 model. Thanks!", "I tried fixing and rolling forward but it was rolled back again for a different reason. Will take a look very soon and get back to you.", "This is failing an internal test, which I am trying to fix. I'm guessing this will be submitted early next week.", "@reedwm Thanks a lot for help!", "This is rolled forward now, and hasn't been rolled back again for 4 days now so I think it's in permanently :)", "@reedwm Great! Thanks a lot!"]}, {"number": 49285, "title": "Allowlist certain data types to avoid a seg fault.", "body": "PiperOrigin-RevId: 356326671\nChange-Id: I23b65b52e93798cb5a6744632d31b0f88c6b6b31", "comments": []}, {"number": 49284, "title": "Allowlist certain data types to avoid a seg fault.", "body": "PiperOrigin-RevId: 356326671\nChange-Id: I23b65b52e93798cb5a6744632d31b0f88c6b6b31", "comments": []}, {"number": 49283, "title": "Allowlist certain data types to avoid a seg fault.", "body": "PiperOrigin-RevId: 356326671\nChange-Id: I23b65b52e93798cb5a6744632d31b0f88c6b6b31", "comments": []}, {"number": 49282, "title": "Allowlist certain data types to avoid a seg fault.", "body": "PiperOrigin-RevId: 356326671\nChange-Id: I23b65b52e93798cb5a6744632d31b0f88c6b6b31", "comments": []}, {"number": 49281, "title": "Fix an invalid address vulnerability in `tf.raw_ops.RaggedBincount`.", "body": "PiperOrigin-RevId: 368293153\nChange-Id: I4b4e493d3fd05e7dc55a55de3a041a80a4f275c3", "comments": []}, {"number": 49280, "title": "Fix an invalid address vulnerability in `tf.raw_ops.RaggedBincount`.", "body": "PiperOrigin-RevId: 368293153\nChange-Id: I4b4e493d3fd05e7dc55a55de3a041a80a4f275c3", "comments": []}, {"number": 49279, "title": "[INTEL MKL] Use static variables for checking env variable and platform", "body": "Refactor mkl_util.h by using static variable for checking\r\n\r\n(1) environment variable (TF_MKL_OPTIMIZE_PRIMITIVE_MEMUSE)\r\n\r\n(2) platform (HW has AVX512 or AVX2, or not)\r\n\r\nThe benefit is performance improvement (check once use multiple times). \r\n\r\nReference PR: https://github.com/tensorflow/tensorflow/pull/48951", "comments": ["@penpornk Thank you for the code review. I have committed my change based on your suggestion. "]}, {"number": 49277, "title": "Re-enable Fused BatchNorm + Add + Activation for the backprop", "body": "This PR re-enables the fused (BatchNorm + Add + Activation) for the backprop. This is essentially a combination of the two previous PRs + one line change to disable the remapping when XLA is on:\r\n1. Original PR: https://github.com/tensorflow/tensorflow/pull/48063 \r\n2. Fix an issue of dual BN inputs: https://github.com/tensorflow/tensorflow/pull/48893\r\n3. Disable the remapping when XLA is on.\r\n\r\ncc. @nluehr ", "comments": ["Out of curiosity, what is the motivation for this PR? Grappler is not very well-maintained now. Is it possible to use XLA via autoclustering or jit_compile and do the rewrite there instead?", "I've replied to your other email. The issue has nothing to do with XLA, raw\nTF code is making the failing call.\n\nOn Thu, May 20, 2021 at 12:42 PM Kaixi Hou ***@***.***> wrote:\n\n> ***@***.**** commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/core/grappler/optimizers/remapper.cc\n> <https://github.com/tensorflow/tensorflow/pull/49277#discussion_r636415799>\n> :\n>\n> > +    const bool valid_channel_dim = !props.empty() &&\n> +                                   props[0].shape().dim_size() == 4 &&\n> +                                   props[0].shape().dim(3).size() % 4 == 0;\n> +    if (!valid_channel_dim) return false;\n> +\n> +    // cuDNN must support CUDNN_BATCHNORM_SPATIAL_PERSISTENT mode.\n> +    if (!BatchnormSpatialPersistentEnabled()) return false;\n> +\n> +    // FusedBatchNormV2 and V3 have an extra type parameter.\n> +    if (node_def->op() != \"FusedBatchNorm\" &&\n> +        !HasDataType(node_def, DT_FLOAT, \"U\")) return false;\n> +\n> +    return true;\n> +  };\n> +\n> +  if (ctx.xla_auto_clustering_on) return false;\n>\n> I agree. The reason I put a line here is that I heard the XLA might not be\n> able to correctly process the new fused op. So, I tried to avoid remapping\n> this pattern when XLA is on. For the \"unsupported cudnn conv\" issue, if you\n> can share with more details on how to repro the failed model benchmark, I\n> can look into it.\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/49277#discussion_r636415799>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AACVGH77OFJOWQ37DSG7EBLTOVQZ7ANCNFSM45DPSMOQ>\n> .\n>\n", "Sorry clicked the wrong button.", "> We need to figure out whether the regression is still there before we can land it. I don't think that Volkswagen-like tests will get us there.\r\n\r\nHi @cheshire I am still working on the repro. This above commit is not to trigger the landing process. It is just to fix one bug we found internally when testing our models.", "Here is more background to show this is _not_ a \"Volkswagen-like tests\":\r\n\r\nWe hit this issue yesterday when training one of our JoC models: `InvalidArgumentError: FusedBatchNormGrad with activation is only supported when is_training=True` After some digging, I found that we accidentally use the fused batchnorm grad in the inference mode. However, the new remapping is supposed to be only working with training mode. Then, I realized that the remapper's FindXXX function should check the mode and if it is inference mode, we need to skip the remapping. After the fix, the issue is gone.\r\n\r\nSo, this has nothing to do with the repro you provided in the email and it has nothing to do to push the merge with this single commit. I am still working on that since it is not simple to get the benchmark compiled and run. I will let you know when we have progress there.\r\n\r\n@sanjoy @cheshire @nluehr @cliffwoolley", "> I don't think your check for XLA would work under jit_compile: could you test?\r\n\r\nSure. Do you mean the `tf.function(jit_compile=True)`? I've tried this code below, where I can see that if `tf.function` is applied, both the forward and backward passes will call the fused cudnn APIs. Instead, if the `@tf.function(jit_compile=True)` is used, there is no cudnn used for batch norm. I checked with our XLA team and learned that \"there are no graph optimization passes run with jit_compile=True at TF level, only XLA optimizations used post conversion/during compilation\". So, it sounds the jit_compile should be fine for this case.\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\n\r\nN, H, W, C = 2, 2, 3, 3\r\ntf.random.set_seed(1)\r\n\r\n\r\ndense = layers.Dense(4)\r\nconv2d = layers.Conv2D(4, 2, padding='same')\r\nbatch_norm = layers.BatchNormalization()\r\nrelu = layers.ReLU()\r\n\r\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(layers.Dense(4))\r\nmodel.add(layers.Conv2D(4, 2, padding='same'))\r\nmodel.add(layers.BatchNormalization())\r\nmodel.add(layers.ReLU())\r\n\r\n#@tf.function(jit_compile=True)\r\n@tf.function\r\ndef train_step(x):\r\n  with tf.GradientTape() as tape:\r\n    y = model(x, training=True)\r\n    loss = tf.reduce_mean(y)\r\n\r\n  grads = tape.gradient(loss, model.trainable_variables)\r\n  print(grads)\r\n\r\n  return loss, grads\r\n\r\nx = tf.random.uniform((N, H, W, C))\r\nloss, grads = train_step(x)\r\n\r\nprint(\"Loss:\", loss)\r\nprint(\"Grads:\", grads)\r\n```", "hi,\n\nUnfortunately this is a known Grappler/XLA wart, what you need to do is to\nwrap tf.function(jit_compile=True) in another (uncompiled) function, and\nthen you should see those grappler optimizations applied.\n\nOn Tue, Jun 15, 2021 at 2:39 PM Kaixi Hou ***@***.***> wrote:\n\n> I don't think your check for XLA would work under jit_compile: could you\n> test?\n>\n> Sure. Do you mean the tf.function(jit_compile=True)? I've tried this code\n> below, where I can see that if tf.function is applied, both the forward\n> and backward passes will call the fused cudnn APIs. Instead, if the\n> @tf.function(jit_compile=True) is used, there is no cudnn used for batch\n> norm. I checked with our XLA team and learned that \"there are no graph\n> optimization passes run with jit_compile=True at TF level, only XLA\n> optimizations used post conversion/during compilation\". So, it sounds the\n> jit_compile should be fine for this case.\n>\n> import tensorflow as tffrom tensorflow.keras import layers\n> N, H, W, C = 2, 2, 3, 3tf.random.set_seed(1)\n>\n> dense = layers.Dense(4)conv2d = layers.Conv2D(4, 2, padding='same')batch_norm = layers.BatchNormalization()relu = layers.ReLU()\n> tf.keras.mixed_precision.set_global_policy('mixed_float16')\n> model = tf.keras.Sequential()model.add(layers.Dense(4))model.add(layers.Conv2D(4, 2, padding='same'))model.add(layers.BatchNormalization())model.add(layers.ReLU())\n> ***@***.******@***.*** train_step(x):\n>   with tf.GradientTape() as tape:\n>     y = model(x, training=True)\n>     loss = tf.reduce_mean(y)\n>\n>   grads = tape.gradient(loss, model.trainable_variables)\n>   print(grads)\n>\n>   return loss, grads\n> x = tf.random.uniform((N, H, W, C))loss, grads = train_step(x)\n> print(\"Loss:\", loss)print(\"Grads:\", grads)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/49277#issuecomment-861923933>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AACVGHZWFA6VW3KA3LHTY73TS7XDFANCNFSM45DPSMOQ>\n> .\n>\n", "That is good to know. But could you be more specific on what I should put to the jit_compile function and what to the uncompiled function? I tried the code below but it seems not right:\r\n```python\r\n@tf.function(jit_compile=True)\r\ndef train_step(x):\r\n  with tf.GradientTape() as tape:\r\n    y = model(x, training=True)\r\n    loss = tf.reduce_mean(y)\r\n\r\n  grads = tape.gradient(loss, model.trainable_variables)\r\n  print(grads)\r\n\r\n  return loss, grads\r\n\r\ndef train_step_wrapper(x):\r\n  return train_step(x)\r\n```", "Sorry, I've meant uncompiled *tf.function*.\n\nIf you add ***@***.***` on top of `train_step_wrapper`, you should see\ngrappler passes being applied.\n\nSorry for this design wart.\n\nOn Tue, Jun 15, 2021 at 3:03 PM Kaixi Hou ***@***.***> wrote:\n\n> That is good to know. But could you be more specific on what I should put\n> to the jit_compile function and what to the uncompiled function? I tried\n> the code below but it seems not right:\n>\n> @tf.function(jit_compile=True)def train_step(x):\n>   with tf.GradientTape() as tape:\n>     y = model(x, training=True)\n>     loss = tf.reduce_mean(y)\n>\n>   grads = tape.gradient(loss, model.trainable_variables)\n>   print(grads)\n>\n>   return loss, grads\n> def train_step_wrapper(x):\n>   return train_step(x)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/49277#issuecomment-861931507>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AACVGH3QUR6JREJS62LKXTLTS7Z5TANCNFSM45DPSMOQ>\n> .\n>\n", "Thanks. I've tried different combination of decorators (`tf.function(jit_compile)` and `tf.function`) over `train_step` and `train_step_wrapper` and all works fine. Specifically:\r\n\r\n- jit_compile + jit_compile: no grappler (I think this is equivalent to a single function + jit_compile)\r\n- jit_compile + tf.function: grappler is seen (I think since BN nodes has already been remapped during XLA phase, grappler's remapper doesn't take effects)\r\n- tf.function + tf.function: grappler is seen (I think this is equivalent to a single function + tf.function)\r\n- tf.function + jit_compile: no grappler (I think this is also equivalent to a single function + jit_compile)\r\n\r\nFurthermore, I am curious what happens if only partial step (forward pass) is in a separate function, like:\r\n\r\n```python\r\n@tf.function\r\n#@tf.function(jit_compile=True)\r\ndef forward(x):\r\n  return model(x, training=True)\r\n\r\n#@tf.function\r\n@tf.function(jit_compile=True)\r\ndef train_step(x):\r\n  with tf.GradientTape() as tape:\r\n    y = forward(x)\r\n    loss = tf.reduce_mean(y)\r\n\r\n  grads = tape.gradient(loss, model.trainable_variables)\r\n  print(grads)\r\n\r\n  return loss, grads\r\n\r\nx = tf.random.uniform((N, H, W, C))\r\nloss, grads = train_step(x)\r\n```\r\nThe results are \r\n- jit_compile + jit_compile: no grappler\r\n- jit_compile + tf.function: grappler is seen (In this case, since forward BN node has already been remapped during XLA phase, grappler's remapper cannot see an eligible BN and doesn't take effects)\r\n- tf.function + tf.function: grappler is seen\r\n- tf.function + jit_compile: no grappler\r\n\r\nAny advise?", "yeah unfortunately right now we have this implementation quirk, and we need\nto make sure the code works with and without grappler (again, normally it's\nfine, since for compiled code grappler does not add to performance).\n\nThe reason is that we use the \"custom kernel\" mechanism to execute the\njit_compile'd function, so if there's no outer function, no graph is\ntransformed (since we launch the compiled kernel straight from the eager\nmode).\n\nWhen there's an outer uncompiled function, we rewrite the graph first, and\nonly then launch the compiled subgraph.\n\nIn case of your PR it needs to work when the grappler is on and the scope\nis compiled.\n\n\n\n\n\nOn Wed, Jun 16, 2021 at 8:32 AM Kaixi Hou ***@***.***> wrote:\n\n> Thanks. I've tried different combination of decorators (\n> tf.function(jit_compile) and tf.function) over train_step and\n> train_step_wrapper and all works fine. Specifically:\n>\n>    - jit_compile + jit_compile: no grappler (I think this is equivalent\n>    to a single function + jit_compile)\n>    - jit_compile + tf.function: grappler is seen (I think since BN nodes\n>    has already been remapped during XLA phase, grappler's remapper doesn't\n>    take effects)\n>    - tf.function + tf.function: grappler is seen (I think this is\n>    equivalent to a single function + tf.function)\n>    - tf.function + jit_compile: no grappler (I think this is also\n>    equivalent to a single function + jit_compile)\n>\n> Furthermore, I am curious what happens if only partial step (forward pass)\n> is in a separate function, like:\n>\n> @***@***.***(jit_compile=True)def forward(x):\n>   return model(x, training=True)\n> ***@***.***@tf.function(jit_compile=True)def train_step(x):\n>   with tf.GradientTape() as tape:\n>     y = forward(x)\n>     loss = tf.reduce_mean(y)\n>\n>   grads = tape.gradient(loss, model.trainable_variables)\n>   print(grads)\n>\n>   return loss, grads\n> x = tf.random.uniform((N, H, W, C))loss, grads = train_step(x)\n>\n> The results are\n>\n>    - jit_compile + jit_compile: no grappler\n>    - jit_compile + tf.function: grappler is seen (In this case, since\n>    forward BN node has already been remapped during XLA phase, grappler's\n>    remapper cannot see an eligible BN and doesn't take effects)\n>    - tf.function + tf.function: grappler is seen\n>    - tf.function + jit_compile: no grappler\n>\n> Any advise?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/49277#issuecomment-862615375>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AACVGH2R4D77VIBTZVFQKELTTDU3PANCNFSM45DPSMOQ>\n> .\n>\n", "I further modified the code as below which still works fine. Not sure if the `forward2` belongs to the \"outer uncompiled function\" you just mentioned. My intent is to make both jit and grapper remapper come into effects. So, the `forward` function is to use the \"custom kernel\" for BN and `forward2` will use fused cudnn BN after grappler. From the log, I can see that grapper remapper is on and successfully does the fusion.\r\n\r\n```python\r\n@tf.function(jit_compile=True)\r\ndef forward(x):\r\n  return model(x, training=True)\r\n\r\ndef forward2(x):\r\n  return model(x, training=True)\r\n\r\n@tf.function\r\ndef train_step(x):\r\n  with tf.GradientTape() as tape:\r\n    y = forward(x)\r\n    y = forward2(y)\r\n    loss = tf.reduce_mean(y)\r\n\r\n  grads = tape.gradient(loss, model.trainable_variables)\r\n  print(grads)\r\n\r\n  return loss, grads\r\n``` ", "OK it's great that it works, but I am a bit confused about how though -\nfrom what I understand, grappler produces the kernel XLA can not compile,\nso what does it do?\nDo you see the grappler rewrite kicking in from the logs?\n\nOn Wed, Jun 16, 2021 at 1:16 PM Kaixi Hou ***@***.***> wrote:\n\n> I further modified the code as below which still works fine. Not sure if\n> the forward2 belongs to the \"outer uncompiled function\" you just\n> mentioned. My intent is to make both jit and grapper remapper come into\n> effects. So, the forward function is to use the \"custom kernel\" for BN\n> and forward2 will use fused cudnn BN after grappler. From the log, I can\n> see that grapper remapper is on and successfully does the fusion.\n>\n> @tf.function(jit_compile=True)def forward(x):\n>   return model(x, training=True)\n> def forward2(x):\n>   return model(x, training=True)\n> @tf.functiondef train_step(x):\n>   with tf.GradientTape() as tape:\n>     y = forward(x)\n>     y = forward2(y)\n>     loss = tf.reduce_mean(y)\n>\n>   grads = tape.gradient(loss, model.trainable_variables)\n>   print(grads)\n>\n>   return loss, grads\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/49277#issuecomment-862792855>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AACVGH2G3ZYJFHFSAQILZ4DTTEWGJANCNFSM45DPSMOQ>\n> .\n>\n", "Yes, I can see the grappler rewrite logs for the above example:\r\n```\r\n2021-06-17 18:10:25.555918: I tensorflow/core/grappler/optimizers/remapper.cc:1699] Fuse FusedBatchNormGrad with ReluGrad:  fused_batch_norm_grad=gradient_tape/sequential/batch_normalization_1/FusedBatchNormGradV3 side_input=<none> activation=gradient_tape/sequential/re_lu_1/ReluGrad corresponding FusedBatchNorm=sequential/batch_normalization_1/FusedBatchNormV3\r\n...\r\n2021-06-17 18:10:25.556003: I tensorflow/core/grappler/optimizers/remapper.cc:1627] Fuse Relu with FusedBatchNorm: activation=sequential/re_lu_1/Relu side_input=<none> invalidated=<none> fused_batch_norm=sequential/batch_normalization_1/FusedBatchNormV3\r\n```\r\n\r\nI think I understand your concern. Let me reiterate what we wanted to do:\r\nLet's assume we have a fuseable network:\r\n\r\n- BN -> loss -> BN_Grad\r\n\r\nSo, applying the standalone jit or tf.function would generate the following respectively:\r\n\r\n- **jit:** customBN -> loss -> customBN_Grad (OK)\r\n\r\n- **grappler:** FusedBN -> loss -> FusedBN_Grad (OK)\r\n\r\nHowever, when mixing the jit and tf.function, it might cause problem:\r\n- **tf.function calls jit function**\r\n  - **jit:** customBN -> loss -> customBN_Grad\r\n  - **grappler:** customBN -> loss -> customBN_Grad (OK)\r\n\r\nThis scenario is probably fine because grappler doesn't know customBN and the customBN won't be remapped again.\r\n\r\n- **jit function calls tf.function**\r\n  - **grappler:** FusedBN -> loss -> FusedBN_Grad\r\n  - **jit:** customBN -> loss -> FusedBN_Grad (No, this is WRONG)\r\n\r\nThis would be problematic, since FusedBN_Grad has been added to the XLA's allowlist and thus becomes a standalone op without its corresponding FusedBN. However, in real use case, we usually call the tf.function from a jit function, where the grappler pass will be disabled. And we will see the following:\r\n\r\n- **jit function calls tf.function**\r\n  - **grappler (disabled):** BN -> loss -> BN_Grad\r\n  - **jit:** customBN -> loss -> customBN_Grad (OK)\r\n\r\nSo, I think the real question is how to repro the above scenario that can call a grappler-optimized tf.function in the jit function. Please let me know if my understanding is correct. And if yes, do you have any magic or ninja ways to do that?", "Hi Kaixi, sorry, I'm not sure I follow. Grappler should run first, only then followed by XLA compilation.\r\n\r\nFrom what I understand, this PR generated `FusedBN` and `FusedBN_Grad` which do not have an XLA lowering. Is that correct? Or am I missing something?\r\nHow would these ops behave when the compilation is requested?", "> Hi Kaixi, sorry, I'm not sure I follow. Grappler should run first, only then followed by XLA compilation.\r\n> \r\n> From what I understand, this PR generated `FusedBN` and `FusedBN_Grad` which do not have an XLA lowering. Is that correct? Or am I missing something?\r\n> How would these ops behave when the compilation is requested?\r\n\r\nYes, I agree. If that happens, I think it would expose some problem (as I mentioned in the above comment that we don't want a standalone `FusedBN_Grad` to be there).\r\n\r\nBut the question is how to trigger this. Here is some conflicting comments I've got:\r\n- You mentioned: \"Grappler should run first, only then followed by XLA compilation.\"\r\n- But I heard from our XLA team: \"There are no graph optimization passes run with jit_compile=True at TF level, only XLA optimizations used post conversion/during compilation\"\r\n\r\nAnd all of the above experiments (trying different combinations and wrapper functions) are actually exploring how to achieve \"Grappler should run first, only then followed by XLA compilation.\" The closest one in my opinion was the below one, because I thought the `forward` function would be applied with grappler and then be input to the jit scope. However, it turned out no grappler passes were applied. \r\n```python \r\n@tf.function\r\ndef forward(x):\r\n  return model(x, training=True)\r\n\r\n@tf.function(jit_compile=True)\r\ndef train_step(x):\r\n  with tf.GradientTape() as tape:\r\n    y = forward(x)\r\n    loss = tf.reduce_mean(y)\r\n\r\n  grads = tape.gradient(loss, model.trainable_variables)\r\n  print(grads)\r\n\r\n  return loss, grads\r\n```\r\n\r\nAt this moment, I have a feeling that \"Grappler should run first, only then followed by XLA compilation.\"  may not be easily achieved from python level and we might need to have a saved grappler optimized graph (that already contains both `FusedBN` and `FusedBN_Grad`) and then we apply XLA compilation (?).", "On Thu, Jun 17, 2021 at 11:54 AM Kaixi Hou ***@***.***> wrote:\n\n> Hi Kaixi, sorry, I'm not sure I follow. Grappler should run first, only\n> then followed by XLA compilation.\n>\n> From what I understand, this PR generated FusedBN and FusedBN_Grad which\n> do not have an XLA lowering. Is that correct? Or am I missing something?\n> How would these ops behave when the compilation is requested?\n>\n> Yes, I agree. If that happens, I think it would expose some problem (as I\n> mentioned in the above comment that we don't want a standalone\n> FusedBN_Grad to be there).\n>\n> But the question is how to trigger this. Here is some conflicting comments\n> I've got:\n>\n>    - You mentioned: \"Grappler should run first, only then followed by XLA\n>    compilation.\"\n>    - But I heard from our XLA team: \"There are no graph optimization\n>    passes run with jit_compile=True at TF level, only XLA optimizations used\n>    post conversion/during compilation\"\n>\n> > But I heard from our XLA team: \"There are no graph optimization passes\nrun with jit_compile=True at TF level, only XLA optimizations used post\nconversion/during compilation\"\n\nCould you point out where you've heard that? I think this only covers the\ncase where the compiled function is not nested.\n\n>\n>\n> And all of the above experiments (trying different combinations and\n> wrapper functions) are actually exploring how to achieve \"Grappler should\n> run first, only then followed by XLA compilation.\" The closest one in my\n> opinion was the below one, because I thought the forward function would\n> be applied with grappler and then be input to the jit scope. However, it\n> turned out no grappler passes were applied\n>\n@tf.functiondef forward(x):\n>   return model(x, training=True)\n> @tf.function(jit_compile=True)def train_step(x):\n>   with tf.GradientTape() as tape:\n>     y = forward(x)\n>     loss = tf.reduce_mean(y)\n>\n>   grads = tape.gradient(loss, model.trainable_variables)\n>   print(grads)\n>\n>   return loss, grads.\n>\n> In this case the uncompiled function is within the compiled one, so no\npasses would be applied.\n\nI think I was confused by your previous email --- I thought you've meant\nthat the grappler did run?\nAnyway, what happens when you do\n\n```\n@tf.function\ndef outer():\n   train_step() // Up to you to pass or hardcode the parameter\n\n@tf.function(jit_compile=True)\ndef train_step(..):\n   ...\n```\n\nIn my understanding, and from what I believe I saw from previous\nexperiments, it should trigger grappler. Please let me know if it doesn't.\n\n> At this moment, I have a feeling that \"Grappler should run first, only\n> then followed by XLA compilation.\" may not be easily achieved from python\n> level and we might need to have a saved grappler optimized graph (that\n> already contains both FusedBN and FusedBN_Grad) and then we apply XLA\n> compilation (?).\n>\nYes, loading SavedModel with a compiled section should also trigger\ngrappler rewrites first.\n\n\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/49277#issuecomment-863587894>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AACVGH72SUWEELZOTHMGD43TTJVIDANCNFSM45DPSMOQ>\n> .\n>\n", "Oh, sorry about the confusion. Let's first focus on your pattern (which should be equivalent to case 2 in https://github.com/tensorflow/tensorflow/pull/49277#issuecomment-862615375):\r\n```\r\n@tf.function\r\ndef outer():\r\n   train_step() // Up to you to pass or hardcode the parameter\r\n\r\n@tf.function(jit_compile=True)\r\ndef train_step(..):\r\n   ...\r\n```\r\nYes, still the grappler is detected in the log with TF_CPP_VMODULE=meta_optimizer=3. I also print out the nodes that the grappler can see:\r\n```\r\nZZZ node: _Retval\r\nZZZ node: _Retval\r\nZZZ node: _Retval\r\nZZZ node: _Retval\r\nZZZ node: _Retval\r\nZZZ node: _Retval\r\nZZZ node: _Retval\r\nZZZ node: StatefulPartitionedCall\r\nZZZ node: _Arg\r\nZZZ node: _Arg\r\nZZZ node: _Arg\r\nZZZ node: _Arg\r\nZZZ node: _Arg\r\nZZZ node: _Arg\r\nZZZ node: _Arg\r\nZZZ node: _Arg\r\nZZZ node: _Arg\r\n\r\n```\r\nOriginally, I put a comment:\r\n> I think since BN nodes has already been remapped during XLA phase, grappler's remapper doesn't take effects\r\n\r\nBut, I feel my original comment for this might be wrong? The reason why grappler's remapper doesn't take effects is not because XLA has already been applied, but because the wrapped function becomes a StatefulPartitionedCall?", "yeah but then grappler is supposed to go into the call anyway, but i\nsuspect it does not, since the call is not inlined. I did not realize that.\n\nOK then, it seems the only time your rewrite can interfere with XLA is via\nautoclustering, and you do check for that.\nLet's approve and see if anything breaks.\n\nOn Thu, Jun 17, 2021 at 1:04 PM Kaixi Hou ***@***.***> wrote:\n\n> Oh, sorry about the confusion. Let's first focus on your pattern (which\n> should be equivalent to case 2 in #49277 (comment)\n> <https://github.com/tensorflow/tensorflow/pull/49277#issuecomment-862615375>\n> ):\n>\n> @tf.function\n> def outer():\n>    train_step() // Up to you to pass or hardcode the parameter\n>\n> @tf.function(jit_compile=True)\n> def train_step(..):\n>    ...\n>\n> Yes, still the grappler is detected in the log with\n> TF_CPP_VMODULE=meta_optimizer=3. I also print out the nodes that the\n> grappler can see:\n>\n> ZZZ node: _Retval\n> ZZZ node: _Retval\n> ZZZ node: _Retval\n> ZZZ node: _Retval\n> ZZZ node: _Retval\n> ZZZ node: _Retval\n> ZZZ node: _Retval\n> ZZZ node: StatefulPartitionedCall\n> ZZZ node: _Arg\n> ZZZ node: _Arg\n> ZZZ node: _Arg\n> ZZZ node: _Arg\n> ZZZ node: _Arg\n> ZZZ node: _Arg\n> ZZZ node: _Arg\n> ZZZ node: _Arg\n> ZZZ node: _Arg\n>\n>\n> Originally, I put a comment:\n>\n> I think since BN nodes has already been remapped during XLA phase,\n> grappler's remapper doesn't take effects\n>\n> But, I feel my original comment for this might be wrong? The reason why\n> grappler's remapper doesn't take effects is not because XLA has already\n> been applied, but because the wrapped function becomes a\n> StatefulPartitionedCall?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/49277#issuecomment-863615489>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AACVGH4ZY7PGOOEDBRJM4RDTTJ5OLANCNFSM45DPSMOQ>\n> .\n>\n", "Rebased to resolve a conflict.", "This now has the following failure: Failure: F0709 00:43:15.026466    2393 shape_inference.cc:114] Check failed: output(i).IsSet() 5 for {{node fused_batch_norm_grad}} = _FusedBatchNormGradEx[T=DT_HALF, U=DT_FLOAT, activation_mode=\"Relu\", data_format=\"NHWC\", epsilon=0.1, is_training=true, num_side_inputs=1, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](output_grad_cast, input_cast, scale, fused_batch_norm:3, fused_batch_norm:4, fused_batch_norm:5, offset, fused_batch_norm)\r\n", "> This now has the following failure: Failure: F0709 00:43:15.026466 2393 shape_inference.cc:114] Check failed: output(i).IsSet() 5 for {{node fused_batch_norm_grad}} = _FusedBatchNormGradEx[T=DT_HALF, U=DT_FLOAT, activation_mode=\"Relu\", data_format=\"NHWC\", epsilon=0.1, is_training=true, num_side_inputs=1, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](output_grad_cast, input_cast, scale, fused_batch_norm:3, fused_batch_norm:4, fused_batch_norm:5, offset, fused_batch_norm)\r\n\r\nEmm... This should be straightforward to change. Originally, I shared the `FusedBatchNormGradShape` function for the new fused op, which doesn't set the output 5 - side_input_backprop. Let me work on a fix."]}, {"number": 49276, "title": "infinite dataset while it is actually finite from tf.data.experimental.choose_from_datasets", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): pip3 install\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 11.3 cudnn 8.2.0\r\n- GPU model and memory: NVIDIA TITAN V 12 GB\r\n\r\n**Describe the current behavior**\r\nFor datasets created by tf.data.experimental.choose_from_datasets\r\nwhen I do len(dataset), it always say it is infinite but when I iterate it, it is actually finite and completely functional\r\nThe problem with this is that when I want to work with keras model like using model.evaluate with this dataset, it gives error \r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\ndatasets = [tf.data.Dataset.from_tensors(\"foo\").repeat(),\r\n            tf.data.Dataset.from_tensors(\"bar\").repeat(),\r\n            tf.data.Dataset.from_tensors(\"baz\").repeat()]\r\n\r\nchoice_dataset = tf.data.Dataset.range(3).repeat(3)\r\n\r\nresult = tf.data.experimental.choose_from_datasets(datasets, choice_dataset)\r\n\r\nlen(result)\r\n#TypeError: dataset length is infinite.\r\n\r\nresult.cardinality()\r\n#<tf.Tensor: shape=(), dtype=int64, numpy=-1>\r\n\r\nfor i in result:\r\n  print(i)\r\n#finite and works fine\r\n\r\nmodel.evaluate(result,verbose=0)\r\n#ValueError: When providing an infinite dataset, you must specify the number of steps to run (if you did not intend to create an infinite dataset, make sure to not call `repeat()` on the dataset).\r\n#i.e. no longer works with keras model\r\n\r\n\r\n```\r\n", "comments": ["Can you check dataset `cardinality`?", "> Can you check dataset `cardinality`?\r\n\r\ncardinality() gives a tensor with value -1 which is the constant for INFINITE_CARDINALITY", "I think that internally It use interleave:\n\nhttps://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/data/experimental/ops/interleave_ops.py#L305\n\nYou could try to assert the cardinality\n\nhttps://github.com/tensorflow/tensorflow/issues/36531#issuecomment-587654123", "\r\n\r\n\r\n\r\n> I think that internally It use interleave:\r\n> \r\n> https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/data/experimental/ops/interleave_ops.py#L305\r\n> \r\n> You could try to assert the cardinality\r\n> \r\n> [#36531 (comment)](https://github.com/tensorflow/tensorflow/issues/36531#issuecomment-587654123)\r\n\r\nYes, after asserting the cardinality by \r\n`result = result.apply(tf.data.experimental.assert_cardinality(expected_cardinality))`,\r\nit works with keras model.\r\nThx a lot!", "@jsimsa Do you need that we need to add something to the documentation or can we close this?", "@laplacericky ,\r\n\r\nPlease feel free to move the issue to closed status, if the issue is resolved.\r\n\r\nThanks!", "The solution working fine so far. I am closing this.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49276\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49276\">No</a>\n"]}, {"number": 49275, "title": "[Pluggable Device] Use default settings when device \"architecture\" field is not set.", "body": "For pluggable devices (with device type GPU) in Grappler optimization pass there is currently code which expects the \"architecture\" field to be set. In pluggable interface currently there is no way to set the architecture field to provide these default values. This bypasses the check and sets some dummy values.  \r\n\r\n@penpornk , @reedwm ", "comments": []}, {"number": 49274, "title": "Cherry-pick: Add .zenodo.json for clean automated DOI numbers.", "body": "PiperOrigin-RevId: 374474720\nChange-Id: I739c9fc95c03648c50c6a0fc1931308507cdf12c", "comments": []}, {"number": 49273, "title": "Cherry-pick: Add .zenodo.json for clean automated DOI numbers.", "body": "PiperOrigin-RevId: 374474720\nChange-Id: I739c9fc95c03648c50c6a0fc1931308507cdf12c", "comments": []}, {"number": 49272, "title": "Cherry-pick: Add .zenodo.json for clean automated DOI numbers.", "body": "PiperOrigin-RevId: 374474720\nChange-Id: I739c9fc95c03648c50c6a0fc1931308507cdf12c", "comments": []}, {"number": 49271, "title": "Cherry-pick: Add .zenodo.json for clean automated DOI numbers.", "body": "PiperOrigin-RevId: 374474720\nChange-Id: I739c9fc95c03648c50c6a0fc1931308507cdf12c", "comments": []}, {"number": 49270, "title": "Cherry-pick: Add .zenodo.json for clean automated DOI numbers.", "body": "PiperOrigin-RevId: 374474720\nChange-Id: I739c9fc95c03648c50c6a0fc1931308507cdf12c", "comments": []}, {"number": 49269, "title": "TFLite GPU OpenGL delegate wrong results with NewConvolution1x1NodeShader", "body": "- TensorFlow version (use command below): master, happens with all releases I got (at least since 2.3)\r\n- GPU model and memory: Jetson TX2 and also Titan RTX\r\n\r\nI am getting wrong results using SENets 1x1 convolutions in TFLite using GPU OpenGL delegate. Results are good with CPU delegate and GPU->OpenCL delegate. I believe there is a bug in [NewConvolution1x1NodeShader ](https://github.com/tensorflow/tensorflow/blob/c91043826234f4c4366bde805a504b55a46f94a5/tensorflow/lite/delegates/gpu/gl/kernels/conv.cc#L166)function.\r\n\r\nI am sorry but I dont understand the syntax of the test code to create an official test case myself. However, my sample input is (BHWC) [1, ,1, 1, 72] and my sample output of keras layer Conv2D is [1,1,1,24] - with bias, thus the convolution weights are 24x1x1x72. I believe the exact values are not important and the bug will make itself apparent with any values. I did not test yet with smaller inputs, the code seems to break the convolution into 4-float chunks. If there is someone brave enough to create such a test case into [the test code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/gl/kernels/conv_test.cc\r\n) it would be much appreciated.\r\n\r\nThe main difference as of my problem with regards to the existing test case is that both the convolution kernel and the input are WxH=1x1, whereas in the present test case the input is WxH=2x1.\r\n\r\n\r\n\r\n\r\n", "comments": ["Exact code I call from Python, this is then converted to tflite module:\r\n\r\n`x = layers.Conv2D(squeeze_channels, kernel_size=1, padding='SAME')(x)`\r\n\r\nThese are the exact values in my example, sorry that it looks extremely cluttered but I think these are best to reproduce the problem.\r\n\r\n(1,1,1,72) input = [[[[0.23505978286266327, 0.41290754079818726, 1.054664969444275, 0.7081828713417053, 0.9560285210609436, 0.4052231013774872, 1.0399662256240845, 0.3407028019428253, 1.1552261114120483, 0.4847528040409088, 0.35466599464416504, 0.36041057109832764, 0.7577136754989624, 0.40622198581695557, 0.3705192506313324, 0.5921180248260498, 0.5737231969833374, 0.614184558391571, 0.18235209584236145, 0.8306506872177124, 1.4187341928482056, 0.21052108705043793, 0.4085419774055481, 0.5287638902664185, 0.620008647441864, 0.9492772221565247, 0.2836027145385742, 0.35014790296554565, 0.42510637640953064, 1.67617928981781, 0.3067736327648163, 0.2641625702381134, 0.3777209222316742, 0.5320253968238831, 1.0993772745132446, 0.25137996673583984, 0.7429025769233704, 1.185189962387085, 0.5014268159866333, 1.016284465789795, 0.5071154236793518, 0.5070053935050964, 0.27801698446273804, 1.1015533208847046, 0.6496829986572266, 0.4433874785900116, 1.3939553499221802, 0.2438175082206726, 0.1657715141773224, 0.8635092973709106, 0.25769680738449097, 0.9235060214996338, 0.32624465227127075, 0.36464911699295044, 0.7742152214050293, 1.5771764516830444, 0.8310389518737793, 0.49573636054992676, 0.201606884598732, 0.13865810632705688, 0.3257404565811157, 0.4631585478782654, 0.39202338457107544, 0.7922559380531311, 0.8170232772827148, 0.1748407632112503, 0.5090256333351135, 0.6402161717414856, 0.6080204248428345, 0.7674005031585693, 0.33140355348587036, 0.889797568321228]]]]\r\n\r\n(1,1,72,24) kernel = [[[[ 9.41182002e-02 -2.42228851e-01  3.86141129e-02  1.18638217e-01\r\n     2.22781762e-01 -4.91355136e-02  3.00005168e-01  9.09524783e-02\r\n     7.51396954e-01 -1.72158647e-02  2.19441831e-01  2.05750465e-01\r\n    -2.64687747e-01  1.56190962e-01 -1.93603277e-01 -4.90261652e-02\r\n    -1.65094003e-01 -4.05524254e-01 -2.59590149e-01  9.45144296e-01\r\n     7.40114152e-02 -2.69743919e-01  5.27375102e-01 -5.71174175e-02]\r\n   [-5.79197586e-01 -7.82149374e-01 -2.74624646e-01 -1.66579574e-01\r\n     1.05647171e+00 -1.39726520e-01 -3.54616523e-01 -2.84241229e-01\r\n    -5.30994311e-02 -5.98786771e-02 -8.15707147e-01 -1.64589822e-01\r\n    -3.52805644e-01 -1.86920390e-01  6.08748794e-02 -3.29323262e-02\r\n    -4.29721735e-02  3.71660143e-01 -2.96521336e-01 -6.82413671e-03\r\n     1.43141538e-01  1.14590071e-01 -4.76684868e-02  1.05788931e-01]\r\n   [ 4.50490862e-02  9.30025652e-02 -7.61692002e-02 -6.27290681e-02\r\n     2.35577509e-01 -2.64681041e-01  1.95891216e-01 -1.66098669e-01\r\n     5.02933025e-01  3.49690057e-02  9.59170982e-02 -5.38690090e-02\r\n     1.04134381e-01  1.85428839e-02 -1.61104918e-01 -1.33861825e-01\r\n    -2.29153752e-01  6.93520159e-02  2.75138468e-01  3.21638972e-01\r\n    -1.98097542e-01 -1.03755675e-01  3.09234019e-03 -3.40971574e-02]\r\n   [ 2.31209025e-01 -9.90471542e-02 -1.18164755e-01  1.86982900e-01\r\n     5.10393441e-01  4.86754030e-02  2.86690384e-01 -1.84826970e-01\r\n     1.01591909e+00  4.34686337e-03  3.64666164e-01 -1.44227564e-01\r\n     6.80747181e-02  1.70833886e-01 -1.59448326e-01 -3.37662660e-02\r\n    -1.95761412e-01  2.78237581e-01 -3.39410156e-01  3.06896237e-03\r\n    -3.10747866e-02  8.53334367e-02 -1.98368132e-01 -1.60017237e-01]\r\n   [ 2.13121884e-02 -1.30433798e-01 -2.16426268e-01 -1.30913332e-01\r\n     1.14955239e-01  6.08934700e-01  6.52812719e-02  5.90917543e-02\r\n    -6.09261394e-01 -1.65451676e-01  2.47864470e-01 -8.07959437e-02\r\n     2.76373755e-02 -1.67117178e-01 -2.66940594e-02 -2.06796274e-01\r\n     7.79739534e-03 -2.64443666e-01 -8.14452767e-02  9.19456005e-01\r\n    -3.61056983e-01 -5.36060408e-02  2.96876103e-01  1.64435729e-01]\r\n   [ 5.86268961e-01 -2.70662338e-01  6.95942119e-02  1.55232221e-01\r\n     2.68064797e-01  2.54273325e-01 -5.51787674e-01 -2.77087897e-01\r\n    -1.58319503e-01  4.35575619e-02 -4.51570600e-02  1.80216551e-01\r\n     6.02143183e-02  1.56580787e-02  1.82451904e-01 -2.82464564e-01\r\n    -9.51042995e-02 -5.91159426e-02  1.66043565e-01 -9.62806642e-01\r\n     1.88419983e-01 -1.83245867e-01 -4.70194906e-01 -2.21385911e-01]\r\n   [-2.73301333e-01 -1.06327735e-01  1.28453076e-01 -3.17842782e-01\r\n    -4.91989642e-01 -6.39478788e-02 -2.27666542e-01  1.94297686e-01\r\n    -2.87309512e-02 -2.55879939e-01 -3.68244529e-01 -3.82609963e-02\r\n     1.52449951e-01  8.54255259e-02  5.55958152e-02 -8.12627450e-02\r\n    -1.88413054e-01 -1.12565167e-01 -2.43395954e-01 -5.41217744e-01\r\n    -2.25995392e-01 -1.56094715e-01 -4.21821214e-02 -1.79740041e-01]\r\n   [-4.98147160e-01 -2.12939024e-01  1.37796819e-01 -3.09628755e-01\r\n     1.82199150e-01 -4.19555575e-01  3.76776874e-01 -3.38141099e-02\r\n     7.88102075e-02  1.10186189e-01  2.03071862e-01 -1.96193576e-01\r\n    -3.18707228e-01 -1.22959778e-01 -1.24213278e-01 -1.56341240e-01\r\n    -1.19159572e-01 -4.10358489e-01  1.94939300e-01  1.25378513e+00\r\n    -4.20984000e-01  2.14536786e-02 -1.52413681e-01 -1.76357239e-01]\r\n   [ 4.83245313e-01 -2.62070876e-02 -1.32717416e-01 -7.50870407e-02\r\n    -3.92005116e-01 -6.52644560e-02 -2.65146285e-01 -1.57689378e-02\r\n    -8.99897218e-01 -2.39372730e-01 -2.08380342e-01  1.06387377e-01\r\n     9.56720859e-03 -9.43794847e-02 -8.10878277e-02  1.24420583e-01\r\n    -1.82110384e-01  2.94396169e-02 -3.32597822e-01 -1.87102575e-02\r\n     2.39921123e-01  1.58518687e-01 -1.35613605e-01 -2.30651230e-01]\r\n   [ 2.28582367e-01  7.01757520e-02  5.06967567e-02  2.07235307e-01\r\n     1.22428143e+00 -3.92579526e-01  3.43197078e-01 -8.35891441e-02\r\n    -4.64289308e-01  1.17281221e-01  3.25470954e-01  5.86035252e-02\r\n     8.10135007e-02  2.40772009e-01  2.15246499e-01 -1.13406681e-01\r\n     2.51126975e-01  2.22511329e-02  2.57408381e-01  5.60598493e-01\r\n     6.32939413e-02 -3.24563496e-03 -3.45853895e-01 -1.59413666e-01]\r\n   [ 5.89271426e-01  3.32571030e-01 -1.68402240e-01 -1.67052969e-01\r\n     4.91591275e-01  1.89015448e-01  4.79868986e-02  1.06297307e-01\r\n     4.61342126e-01  1.20416954e-02  7.69025162e-02 -1.22318149e-01\r\n    -3.35618824e-01 -2.28818223e-01 -2.18350828e-01 -1.48026571e-01\r\n     2.22020313e-01 -4.94750857e-01 -6.66697443e-01  2.41590753e-01\r\n     1.59033954e-01  2.03429148e-01 -3.22618693e-01 -1.10184737e-01]\r\n   [ 9.90920048e-03 -1.62511796e-01  2.03754783e-01 -4.16755565e-02\r\n    -2.30208606e-01 -1.43434331e-01 -5.46941459e-01 -8.79654288e-02\r\n     1.43889129e-01 -1.12920217e-01 -1.73929945e-01 -1.54066741e-01\r\n     1.32656634e-01 -2.41071880e-01 -1.17533803e-01 -1.76636487e-01\r\n     3.15824300e-02  1.87707152e-02 -6.56134412e-02 -7.29957759e-01\r\n     2.16276139e-01  1.66269895e-02 -3.23992461e-01 -8.95032063e-02]\r\n   [-8.23434815e-03 -5.89343905e-01 -1.94943890e-01  6.09347373e-02\r\n    -5.91041803e-01  6.40208900e-01  2.25502148e-01 -1.21689782e-01\r\n     3.51687707e-02 -1.38233125e-01 -2.53748059e-01 -2.19174027e-02\r\n     2.71074250e-02 -2.46139869e-01 -6.53502941e-02  1.95986778e-03\r\n     7.66998231e-02 -1.43551528e-01 -2.03583598e-01 -7.29320884e-01\r\n     1.13942530e-02 -2.31575429e-01 -5.70232093e-01 -1.30821213e-01]\r\n   [ 7.02219009e-02 -4.16107297e-01 -2.13922113e-01  1.98582381e-01\r\n     3.63292933e-01 -2.95073450e-01 -1.15733206e-01 -2.13716328e-01\r\n    -2.25305378e-01 -1.18976235e-01 -4.69915092e-01 -1.60422504e-01\r\n    -3.35429679e-03 -1.16677873e-01 -5.13377786e-02 -1.18695766e-01\r\n     4.43360582e-02 -3.02010328e-02 -7.97517478e-01 -3.72919559e-01\r\n     2.87654519e-01  2.01248080e-02 -2.82545894e-01  1.98457733e-01]\r\n   [-1.56785905e-01  6.20406531e-02 -1.70161530e-01  1.65481523e-01\r\n    -1.34908808e-02  4.54235077e-01 -4.93464738e-01  1.83797747e-01\r\n     3.44066262e-01  1.54315326e-02  1.55289173e-01 -2.19233453e-01\r\n    -1.04160041e-01 -4.60273586e-02 -5.62753081e-02 -1.54962137e-01\r\n    -2.17443891e-03  4.11504149e-01  2.04640359e-01  3.54074657e-01\r\n    -1.70758843e-01  2.31804382e-02  7.23967671e-01  9.19426754e-02]\r\n   [-1.24636598e-01 -3.40774134e-02  2.06819981e-01 -2.28384092e-01\r\n    -1.77672971e-02  2.36655980e-01 -6.28224947e-03 -2.08253443e-01\r\n     7.64650777e-02 -1.56519607e-01 -8.85534286e-02 -1.47453487e-01\r\n     6.13624528e-02  5.01022264e-02 -1.62173271e-01  1.87491387e-01\r\n     3.38577956e-01 -8.84068459e-02  1.43990397e-01 -1.79516780e-03\r\n     2.84790285e-02 -8.61380436e-03  9.15648565e-02 -8.44068974e-02]\r\n   [-2.68162906e-01 -1.95424378e-01  1.85200676e-01  2.67884620e-02\r\n    -8.58698785e-01  1.93424955e-01 -5.73368132e-01 -1.44583747e-01\r\n     4.14414078e-01 -5.99151626e-02 -1.47554591e-01  2.03680098e-01\r\n    -2.42268190e-01 -6.11402988e-02 -7.16652870e-02 -2.11594671e-01\r\n    -2.39611968e-01 -1.98624298e-01 -6.85110152e-01 -4.78481650e-01\r\n    -2.78119557e-02 -1.61022656e-02  2.87516505e-01 -2.74664402e-01]\r\n   [-1.24340937e-01  2.15053558e-01  4.81263325e-02 -1.03749901e-01\r\n     1.21422470e-01  9.94511768e-02  9.28439423e-02 -2.90614754e-01\r\n     3.47598433e-01  6.37330115e-02  4.40756649e-01 -1.28717184e-01\r\n    -1.88231111e-01 -9.49550048e-02 -3.39258909e-02 -1.10510245e-01\r\n     1.62986051e-02 -2.55977213e-02  3.59118432e-01  6.86707079e-01\r\n     1.06586598e-01 -1.31085366e-01  1.02314927e-01 -2.07494989e-01]\r\n   [ 4.20844495e-01 -9.14960861e-01 -7.19818994e-02 -3.03323448e-01\r\n     2.21670195e-01 -1.18231595e-01  6.39442727e-02 -2.00246423e-02\r\n     1.32466272e-01 -1.90626636e-01  2.55423993e-01 -1.54487491e-02\r\n    -1.23185329e-02  9.71903503e-02 -2.44293988e-01 -2.72590995e-01\r\n    -1.06502675e-01 -1.43513888e-01 -3.31311852e-01  2.03012586e-01\r\n    -7.20454380e-03 -3.66038159e-02  2.86841899e-01 -2.63414979e-01]\r\n   [-2.11023405e-01 -1.72755361e-01  6.97160885e-03 -2.12141082e-01\r\n    -1.99509457e-01 -1.25685900e-01  1.99363813e-01  1.24444462e-01\r\n    -5.03132939e-01 -1.53874367e-01  1.01816028e-01  3.54219079e-02\r\n    -3.60734105e-01  2.18740806e-01  1.11688316e-01 -2.33437851e-01\r\n     2.52264440e-01 -1.58516183e-01  7.74216413e-01  3.86931926e-01\r\n     2.44078226e-02 -4.61697951e-03 -8.77719879e-01  1.40089765e-01]\r\n   [-9.86538082e-02  3.76034155e-02 -1.76474929e-01 -1.39345214e-01\r\n     2.35099584e-01 -1.76835403e-01  5.00752568e-01 -1.73747670e-02\r\n     1.36229873e-01  1.54160663e-01 -1.94636971e-01 -2.47343183e-01\r\n    -3.61804217e-01 -4.26384620e-02  7.69704580e-03  3.32816094e-02\r\n    -2.62931377e-01 -8.63662958e-02  5.22839606e-01  3.51958871e-01\r\n     1.66904882e-01 -6.81659067e-03  1.93031579e-01  1.53525919e-01]\r\n   [ 1.70935139e-01 -7.23416150e-01 -1.69720173e-01  1.39223963e-01\r\n     7.86992252e-01 -2.93605715e-01  6.97896600e-01 -2.79936761e-01\r\n     4.64574337e-01 -1.27177656e-01 -9.25216228e-02 -1.83332920e-01\r\n    -2.35885724e-01 -1.01599485e-01 -1.61072910e-01  4.51345462e-03\r\n     4.07905877e-02  3.39835919e-02 -8.05333480e-02 -2.69507885e-01\r\n     1.38923228e-01  1.31665185e-01  3.31477262e-02  8.89572222e-03]\r\n   [-1.83488786e-01  3.90176535e-01  2.03874141e-01  1.08360656e-01\r\n     4.97397222e-02 -3.57926756e-01  1.78431496e-01  1.56284556e-01\r\n     3.58269811e-01  1.50746420e-01 -8.61101598e-03 -1.35654211e-03\r\n    -2.48177126e-01 -2.23842770e-01  1.06754959e-01 -3.40465494e-02\r\n     2.10035801e-01 -3.54527146e-01 -1.29403189e-01 -5.91581404e-01\r\n     2.05666155e-01 -1.76701918e-02  2.09087789e-01  3.80588882e-02]\r\n   [-2.88269013e-01  2.15978906e-01 -1.89543813e-01  2.93789096e-02\r\n     2.35815689e-01  4.84793335e-01  1.36442617e-01 -2.49744952e-01\r\n     3.37527514e-01  1.12215929e-01 -3.12400639e-01  4.97065783e-02\r\n     1.03691816e-02 -2.48684764e-01 -1.12488151e-01  1.35778770e-01\r\n     4.56956662e-02 -3.65140945e-01  6.30506873e-03 -1.46518126e-01\r\n    -1.30506977e-01 -2.50986010e-01  5.54624677e-01 -1.86326280e-01]\r\n   [-1.11682482e-01  2.03061938e-01 -2.49443576e-01  1.61243483e-01\r\n     3.46183747e-01 -7.08472669e-01  3.95290941e-01  9.08381268e-02\r\n     3.54490280e-01 -2.01469585e-01 -1.60339624e-01 -1.70037270e-01\r\n    -1.81729436e-01 -1.99736699e-01 -1.45474374e-01 -2.46031716e-01\r\n     2.85434097e-01 -2.13900775e-01  3.56101573e-01 -3.89443249e-01\r\n     1.76827610e-01 -5.30580729e-02  2.89643645e-01 -1.62112355e-01]\r\n   [ 9.66947526e-02 -3.91300261e-01  2.56341323e-02 -2.01741830e-01\r\n     3.22204262e-01 -3.85301232e-01 -1.39441773e-01  5.12377955e-02\r\n    -4.99844015e-01 -1.60191789e-01  5.72875917e-01 -1.69334471e-01\r\n    -1.22055814e-01 -1.51843891e-01 -8.49599838e-02 -5.02021201e-02\r\n    -1.71019524e-01 -5.83996177e-01  4.57746297e-01  6.25020042e-02\r\n    -2.77010173e-01  1.67667419e-01 -2.27490470e-01  1.30204514e-01]\r\n   [ 3.93494040e-01  2.64059782e-01  1.45451967e-02 -1.31341815e-01\r\n     3.42922747e-01 -4.69216481e-02 -7.90196508e-02 -2.07510404e-02\r\n    -2.91768283e-01  1.04922958e-01 -5.40293217e-01 -5.79440594e-02\r\n    -3.17372352e-01  1.40438331e-02  9.43440795e-02 -5.31815812e-02\r\n    -9.86620113e-02  1.86889678e-01 -1.16828270e-01 -9.86448407e-01\r\n    -1.09761626e-01 -1.56405419e-01  3.65927905e-01 -5.42494468e-02]\r\n   [ 2.75729746e-01 -1.76989213e-01  3.52457166e-02 -4.68812436e-02\r\n    -1.43793249e+00  6.97553679e-02 -8.28680098e-01  1.22802161e-01\r\n     2.47663110e-01 -2.23018676e-01 -2.35883221e-01  1.50365293e-01\r\n    -3.11013550e-01  2.35556532e-02 -2.01242745e-01 -3.99296125e-03\r\n     1.22647874e-01 -6.28591254e-02  1.52934864e-01 -2.46862948e-01\r\n    -3.45862061e-02  1.53650686e-01 -9.32558104e-02 -9.62382481e-02]\r\n   [ 9.29119438e-02  3.25253278e-01 -7.51404911e-02 -3.26655924e-01\r\n    -2.37490341e-01  2.74245918e-01 -1.51125625e-01 -2.82966167e-01\r\n     4.09272909e-01 -9.23979878e-02  1.02327645e-01  1.89921856e-01\r\n    -2.35354826e-01  2.02333882e-01 -1.42151058e-01  7.20003843e-02\r\n     2.78187096e-01 -7.42307007e-02  7.87768513e-02 -7.04830825e-01\r\n     2.41041016e-02  1.10145332e-02  1.00583315e-01 -5.59307374e-02]\r\n   [ 7.54831880e-02 -2.38500029e-01  8.22171345e-02 -4.68933508e-02\r\n    -2.16739029e-01  6.02091216e-02  7.71030262e-02  1.21386230e-01\r\n     1.23362646e-01 -9.18897986e-02 -1.87538370e-01 -3.15263271e-02\r\n    -2.98776403e-02 -1.03660390e-01 -6.97432160e-02  2.01639652e-01\r\n    -5.04567362e-02 -1.41394898e-01 -3.53152007e-01  4.53969359e-01\r\n    -1.09952495e-01 -1.38719290e-01 -3.46897207e-02  1.85995698e-01]\r\n   [ 1.67165205e-01 -1.73960119e-01 -1.52858660e-01 -4.15292680e-02\r\n     4.80596393e-01 -1.93526782e-02 -2.54878432e-01  1.41427174e-01\r\n     1.50525635e-02  1.00550987e-01 -1.93676084e-01  2.86011696e-02\r\n    -1.65453572e-02  1.18840598e-01  5.48118949e-02  3.57775055e-02\r\n    -1.54230490e-01  3.16954479e-02  9.86084063e-03  5.18945605e-02\r\n     1.31842837e-01 -2.62260865e-02  5.13557255e-01 -1.39489700e-03]\r\n   [-5.51437074e-03 -3.91423076e-01  5.31227775e-02 -3.31172049e-01\r\n     5.58030903e-01  4.69393097e-04 -3.43855798e-01 -1.65851302e-02\r\n    -2.27579072e-01 -1.47565395e-01 -2.80600548e-01 -1.32615626e-01\r\n    -4.43964154e-01 -2.51705259e-01 -1.42816126e-01  1.08361810e-01\r\n    -1.47454336e-01  3.58271897e-01  2.95121036e-02 -3.41456383e-01\r\n    -5.59389070e-02 -1.99664518e-01  8.96948054e-02  1.70736760e-01]\r\n   [ 3.18006039e-01  2.61415005e-01  1.71929196e-01  1.31665513e-01\r\n     1.72960997e-01 -3.50865960e-01 -1.97560608e-01 -1.62521258e-01\r\n    -9.30902064e-02 -4.18046825e-02 -2.56710202e-01  5.97738624e-02\r\n    -9.64775905e-02 -2.11781651e-01 -1.05553448e-01  1.02356024e-01\r\n    -3.80890444e-02 -4.88288999e-01 -1.65706873e-01 -9.47377801e-01\r\n     1.41700923e-01 -2.65560951e-02 -6.26709998e-01 -2.67597884e-01]\r\n   [-2.12319896e-01 -1.10552244e-01  9.11119729e-02  1.99580908e-01\r\n     2.93464623e-02 -4.29140270e-01  3.48543972e-01 -2.64326245e-01\r\n     3.66607197e-02  1.81481510e-01  4.25693914e-02  1.30824447e-01\r\n    -2.30359226e-01  9.91383269e-02 -1.25715256e-01 -2.27475062e-01\r\n    -1.09461192e-02  3.48303206e-02  6.07879274e-02  2.30288684e-01\r\n    -1.82723045e-01 -7.76674300e-02 -2.64819473e-01 -1.13745451e-01]\r\n   [-4.52513844e-01  1.18053302e-01 -6.17623627e-02 -2.01434463e-01\r\n    -3.10386926e-01  8.63262564e-02 -1.06275771e-02 -1.55383572e-01\r\n     2.09185481e-01  1.25377253e-02 -3.82358700e-01 -9.12960172e-02\r\n    -2.59073637e-02 -3.49190868e-02  7.19876885e-02 -1.52243942e-01\r\n    -3.01030129e-01  1.33861545e-02 -1.15395665e-01  5.24280548e-01\r\n    -3.85661453e-01 -1.57692030e-01  3.49962085e-01 -2.77708679e-01]\r\n   [-1.36114247e-02  1.67941809e-01  8.69337395e-02 -1.79321066e-01\r\n     1.34530023e-01  5.65026700e-01  3.56831372e-01 -1.37272447e-01\r\n     2.47029811e-01  1.10268444e-01  9.97899771e-02 -2.06159592e-01\r\n    -2.54776776e-01 -2.02445656e-01 -1.28019094e-01  2.06336547e-02\r\n     3.10086440e-02  4.00651038e-01  1.35583023e-03 -1.35380805e-01\r\n    -5.92019521e-02 -2.60675997e-01  1.74138546e-01 -2.52573371e-01]\r\n   [ 2.72344887e-01  6.50283098e-01 -1.76020861e-01  1.83741078e-01\r\n    -6.21403083e-02  6.46334231e-01 -1.26029879e-01 -1.76016822e-01\r\n    -3.70060354e-02 -1.40323579e-01  6.39196277e-01 -4.29728627e-02\r\n     1.45721465e-01  2.15288177e-02 -2.06438839e-01  1.23857921e-02\r\n     2.04552054e-01  8.61152634e-02  3.28299344e-01 -3.54821146e-01\r\n    -2.28200510e-01 -2.22761095e-01 -8.43165517e-01 -2.12844655e-01]\r\n   [ 4.68725055e-01  4.03167367e-01  5.56031130e-02 -7.55433813e-02\r\n    -4.45527472e-02 -2.85481274e-01  2.32442096e-01  1.97719231e-01\r\n     4.55028892e-01  1.18651632e-02 -1.32779136e-01  8.53700042e-02\r\n     6.54529706e-02 -1.01505026e-01 -7.52111077e-02 -1.23175681e-01\r\n    -1.59290627e-01 -1.56649888e-01  2.42766157e-01  5.56971252e-01\r\n    -8.45896155e-02  1.91068165e-02 -5.81882298e-02  1.89058974e-01]\r\n   [ 2.01422110e-01  3.25706184e-01 -1.09332614e-01  5.60953356e-02\r\n    -4.10014428e-02 -3.32513094e-01  9.33216065e-02 -1.90552026e-01\r\n     1.13473915e-01 -1.34929731e-01 -1.58153191e-01 -1.79207146e-01\r\n    -1.92589268e-01 -1.56177893e-01 -2.02109993e-01 -3.05505339e-02\r\n    -2.12005377e-01 -1.88815296e-01  2.34024674e-01 -1.78199455e-01\r\n    -2.40418777e-01  1.00765489e-01 -3.64777632e-02 -2.62594044e-01]\r\n   [-3.07562560e-01  7.01132789e-02 -2.16677204e-01 -1.38960360e-02\r\n     3.63021314e-01  2.76468396e-01  5.44835210e-01  6.95413649e-02\r\n    -7.91927148e-03 -9.90415961e-02  5.00926934e-02 -1.51230872e-01\r\n     3.83217521e-02 -9.22589377e-02 -1.36378706e-01  9.74398702e-02\r\n    -2.20567569e-01  1.99034929e-01  9.89472270e-02 -5.58683909e-02\r\n    -8.17335621e-02 -2.38855109e-01  1.93574410e-02  3.65367718e-02]\r\n   [ 3.71363945e-02  3.61336432e-02 -2.26968259e-01 -1.05913185e-01\r\n    -4.62742329e-01  2.01111451e-01 -1.86856002e-01  1.08596429e-01\r\n     2.58525670e-01 -9.85936970e-02 -1.60911866e-03 -9.28270221e-02\r\n    -1.24600083e-01 -2.53747046e-01 -4.18274999e-02 -2.88185060e-01\r\n     3.18907619e-01  9.78000090e-02  4.11782742e-01 -7.38716543e-01\r\n    -3.36021692e-01 -2.18997553e-01  8.07186291e-02 -3.21224630e-02]\r\n   [-5.04220903e-01  7.62178972e-02 -2.85801262e-01  6.68255910e-02\r\n     2.11527243e-01 -1.09146446e-01  1.05758257e-01  5.97076453e-02\r\n     1.28851846e-01  8.69763270e-02  3.55157107e-01  7.37872720e-02\r\n     7.43560120e-02  5.06712161e-02 -1.91694856e-01  1.57451797e-02\r\n    -3.61667752e-01  1.54033810e-01 -5.48991151e-02  2.25710064e-01\r\n    -1.54451922e-01 -9.20513421e-02 -3.39467347e-01 -2.68415987e-01]\r\n   [-1.08993858e-01  7.49059692e-02 -1.22982174e-01 -3.03221345e-01\r\n     2.34627854e-02 -1.48117483e-01  1.27692759e-01 -1.10180013e-01\r\n    -3.84236991e-01 -2.56656289e-01 -8.01267803e-01 -1.61684215e-01\r\n     1.25051945e-01  3.71364094e-02 -1.83612645e-01 -3.11898202e-01\r\n     2.85987347e-01  3.05341750e-01  3.09594095e-01 -4.84434605e-01\r\n    -4.06207889e-02 -1.82673261e-01  6.67196289e-02  1.92315906e-01]\r\n   [-5.36954343e-01  1.56143010e-01 -1.61286891e-01  1.24687985e-01\r\n    -6.44959927e-01 -2.10650772e-01 -5.97711265e-01  6.49984404e-02\r\n    -4.23994750e-01 -1.37163460e-01  4.34028387e-01 -1.50513649e-03\r\n    -1.42184749e-01 -1.00816838e-01 -5.93754649e-02 -2.73094863e-01\r\n     1.46536350e-01 -2.01279167e-02  8.67566243e-02  5.06972134e-01\r\n    -1.45617172e-01 -2.78213918e-01 -3.28102589e-01  6.06579892e-02]\r\n   [ 7.48791471e-02 -9.33201492e-01 -1.70385331e-01  2.04628646e-01\r\n     2.10843161e-01  7.89146900e-01 -5.68455637e-01 -2.45302230e-01\r\n     6.25066459e-01  1.27181634e-01  1.43268064e-01 -2.17869401e-01\r\n    -2.06805080e-01 -2.64281277e-02  1.83485866e-01  1.23804301e-01\r\n    -3.98808211e-01  4.15947437e-01 -1.14143395e+00 -4.95855331e-01\r\n    -2.77016312e-01 -2.29380772e-01  2.93551147e-01  1.11878909e-01]\r\n   [-7.51453161e-01  2.64517754e-01  3.28838080e-03 -1.51982293e-01\r\n     1.22066152e+00  1.65905803e-02  1.79369614e-01 -8.75798166e-02\r\n    -6.81210637e-01 -6.69917315e-02 -6.47561371e-01  6.22391701e-03\r\n    -3.47734213e-01 -2.36543894e-01  3.63771319e-02 -2.98657686e-01\r\n    -4.49389443e-02 -2.21633896e-01 -7.11348429e-02 -8.03045809e-01\r\n    -1.06279179e-01  1.35926932e-01 -3.11294824e-01 -1.36068061e-01]\r\n   [-1.39401570e-01  5.89322783e-02  1.22598276e-01  1.05941258e-01\r\n    -1.66900706e+00  6.38928950e-01 -6.57359421e-01 -6.10837899e-02\r\n    -1.56363949e-01 -1.90860152e-01  1.98169604e-01 -1.52904809e-01\r\n     8.00549090e-02 -1.70396864e-01 -1.81669533e-01 -2.06675395e-01\r\n    -5.67941189e-01 -2.79398352e-01 -5.90313017e-01  3.69183481e-01\r\n     2.46179663e-02  1.08018167e-01 -1.14196949e-01  1.93625510e-01]\r\n   [ 7.22290725e-02  1.44681811e-01 -2.66310841e-01 -1.38675541e-01\r\n    -2.18698740e+00  3.05272967e-01  1.44427091e-01 -2.43940845e-01\r\n     3.32065403e-01 -2.13482641e-02  6.37236238e-01 -1.97387278e-01\r\n    -1.34703860e-01  1.82286248e-01  7.01705813e-02 -1.43354237e-01\r\n    -1.08225673e-01  8.53014514e-02 -5.74241519e-01  2.97203124e-01\r\n    -1.89578906e-01 -1.75286815e-01  5.34944773e-01  1.73540354e-01]\r\n   [-3.00087303e-01  5.24384499e-01  2.15469703e-01 -2.53562987e-01\r\n    -6.20400831e-02  1.14786603e-01 -1.07980054e-02  1.49355009e-01\r\n     5.93376830e-02 -9.10437480e-03  1.13872945e+00  2.39130855e-02\r\n    -3.27702224e-01  1.81153148e-01  4.49706912e-02 -2.83971220e-01\r\n    -4.07740086e-01 -1.86977059e-01 -5.49546722e-03  4.18540001e-01\r\n     4.81488509e-03 -5.19472212e-02  1.63600847e-01 -2.22344503e-01]\r\n   [ 3.14806610e-01 -3.34681004e-01 -4.46729138e-02 -1.53269663e-01\r\n     3.60627547e-02  3.79391372e-01  4.59520251e-01 -1.55808374e-01\r\n    -2.90518910e-01 -1.81424394e-01  3.17881286e-01  1.43398046e-01\r\n    -1.47905976e-01 -3.50437388e-02 -1.14637256e-01 -5.86908720e-02\r\n    -3.07662308e-01  3.64844114e-01 -1.29773557e-01  2.38606483e-01\r\n    -2.55548716e-01 -1.15708582e-01 -2.87835542e-02 -2.03991055e-01]\r\n   [ 5.38720042e-02  1.66522950e-01 -9.64479074e-02 -1.87562555e-01\r\n     5.63107669e-01 -2.15024397e-01  7.11579248e-02  6.44259155e-02\r\n    -3.84463102e-01 -2.47123986e-01 -2.21376657e-01 -1.15348756e-01\r\n     1.09560438e-01  7.22805634e-02  2.42789209e-01 -3.26201886e-01\r\n     6.90999553e-02  1.89232260e-01  1.44949704e-01 -7.22041488e-01\r\n     2.15903938e-01 -1.62825063e-01  4.02067959e-01 -2.39014030e-01]\r\n   [ 1.18715847e+00  2.06356138e-01  4.48648520e-02  4.02138606e-02\r\n    -2.00939104e-01  7.72106946e-02  4.25631702e-01 -1.44606158e-01\r\n    -4.36658114e-01  1.97504997e-01  5.08653164e-01  1.67971253e-02\r\n    -2.91745603e-01  1.73820436e-01 -3.70405912e-02 -1.81987524e-01\r\n    -4.75773990e-01 -4.73820269e-01 -1.12962890e+00  2.26261020e-01\r\n    -6.98727071e-02  1.96833163e-01  3.01010132e-01  1.36598870e-01]\r\n   [-3.88073951e-01 -3.86653543e-01 -2.98830755e-02 -2.33282238e-01\r\n     5.86354017e-01  7.50000894e-01 -7.01131001e-02 -4.64241137e-04\r\n    -8.23102951e-01 -2.03618139e-01 -5.46010360e-02  2.03288555e-01\r\n    -6.78901896e-02  1.63434707e-02 -1.14644945e-01  1.92017611e-02\r\n    -3.88082683e-01 -4.43539053e-01  1.37692839e-01  1.77894786e-01\r\n     1.47731779e-02  1.77346662e-01  8.00266862e-01 -1.13673262e-01]\r\n   [ 7.91132390e-01  7.91354895e-01  6.23619230e-03 -1.36076482e-02\r\n    -1.63185668e+00 -2.85204291e-01  2.77147979e-01 -2.38759462e-02\r\n    -4.37381655e-01  1.30343456e-02  4.86528277e-01 -1.20601594e-01\r\n    -2.77434085e-02  2.00723156e-01 -1.53065443e-01 -8.28920379e-02\r\n     3.06402445e-01 -7.81723559e-01 -3.27958912e-01 -1.36674389e-01\r\n    -5.48978806e-01 -2.63739347e-01 -2.47189194e-01 -7.90735148e-03]\r\n   [-1.36010796e-01  3.32425535e-01 -3.39709781e-02  2.87110489e-02\r\n     3.57104987e-01 -1.00417756e-01  1.43199891e-01 -8.40089247e-02\r\n     5.98878324e-01  1.21472061e-01 -9.38716233e-02  5.78740835e-02\r\n     8.91603604e-02 -1.53245389e-01 -1.93273902e-01  1.83549881e-01\r\n    -1.65584996e-01  6.06479831e-02  5.19302011e-01 -5.53185642e-01\r\n    -2.87744373e-01 -2.65736610e-01  4.26912814e-01 -1.38316646e-01]\r\n   [ 3.87901738e-02 -7.24521279e-01 -1.04444265e-01  2.50495553e-01\r\n    -2.72159338e-01 -3.83579224e-01 -5.66881657e-01  1.01134762e-01\r\n    -3.02023627e-02 -1.26763089e-02 -2.09103137e-01  6.35999441e-03\r\n     2.90612727e-01 -2.96525378e-02 -6.69720173e-02 -2.09032670e-02\r\n    -7.32104540e-01  2.01084897e-01 -6.42734990e-02 -1.05370259e+00\r\n     1.86833352e-01  1.67235970e-01  4.48824912e-01 -1.28122970e-01]\r\n   [-4.22129035e-01  5.87583482e-01 -2.93642879e-01 -2.96771079e-01\r\n     1.75570492e-02 -5.16806208e-02  4.44999151e-02 -1.46132603e-01\r\n     3.42360467e-01 -2.50727851e-02 -2.16627389e-01  2.01964319e-01\r\n     6.74237832e-02  3.37711014e-02  5.95833063e-02  2.47741323e-02\r\n     6.61589026e-01  1.88296735e-02  5.61272025e-01 -4.71276850e-01\r\n    -2.27696642e-01 -3.04006815e-01 -2.64157385e-01 -2.49348283e-01]\r\n   [ 9.86574367e-02 -2.04885706e-01  1.49416225e-02  4.90120091e-02\r\n     3.02542657e-01 -3.49317908e-01 -8.17140818e-01 -3.73355821e-02\r\n     5.43448627e-02 -1.29493400e-01  8.50895047e-01 -1.61359429e-01\r\n    -1.39267981e-01  1.39670938e-01 -1.12334430e-01 -1.69536248e-01\r\n    -1.06913134e-01 -1.08771533e-01  3.19464564e-01  4.67268854e-01\r\n     9.40235928e-02 -1.99837357e-01  1.80963874e-01 -1.49582326e-01]\r\n   [-3.72587442e-02  3.81644934e-01  9.06039625e-02 -2.79398561e-01\r\n     1.14558172e+00 -6.18105888e-01  1.55877784e-01  1.14541389e-01\r\n    -3.27941060e-01 -1.04121231e-01 -4.75616395e-01 -4.53363061e-02\r\n    -5.72913466e-03 -1.83598340e-01 -2.45678186e-01  1.08287491e-01\r\n    -3.95416506e-02  4.28691767e-02  1.69473350e-01 -7.19320118e-01\r\n     2.05601364e-01 -2.62321889e-01 -2.33183712e-01 -1.49690643e-01]\r\n   [ 1.58738375e-01 -1.32304681e-02 -7.02661555e-03 -1.58067495e-01\r\n     1.39236286e-01  6.46897614e-01  7.85125419e-02  2.99412534e-02\r\n    -8.08491826e-01 -3.93499173e-02  2.64229208e-01  5.72540760e-02\r\n     2.49122813e-01 -1.64353386e-01  2.00093269e-01 -2.91539077e-02\r\n    -3.02917242e-01  1.70350540e-02 -5.58048189e-01  4.88970369e-01\r\n     1.43670803e-02 -2.08190784e-01  1.69924766e-01 -2.25775406e-01]\r\n   [ 2.16908023e-01 -3.29523273e-02 -2.16944173e-01 -2.53005594e-01\r\n    -6.17734671e-01  9.11242813e-02  1.74270689e-01 -6.28481433e-02\r\n     1.75312102e-01 -1.71077222e-01 -1.79888695e-01 -7.36351609e-02\r\n    -3.26759189e-01 -1.15026228e-01 -1.74332261e-01 -2.78597474e-01\r\n     8.25764053e-03  2.68804967e-01  1.45545214e-01 -1.14035892e+00\r\n    -1.38385937e-01 -1.80828646e-01 -1.46110624e-01  7.83574954e-02]\r\n   [ 5.59706628e-01 -6.65172994e-01 -2.31146082e-01  1.26655579e-01\r\n     4.49302107e-01  1.58795208e-01  4.35408913e-02 -2.77436823e-01\r\n     1.29983887e-01 -1.61914229e-02  8.79193962e-01  2.05921471e-01\r\n     8.42810869e-02 -1.65152878e-01  2.19776034e-01  4.51354943e-02\r\n    -9.88282487e-02 -7.48950988e-02  2.34557539e-02  7.55306005e-01\r\n     2.03371897e-01  2.05199897e-01  3.35969478e-02 -2.54768223e-01]\r\n   [-7.49749914e-02 -1.49425775e-01 -2.79034942e-01  1.05953895e-01\r\n    -7.24779814e-02 -1.51979014e-01 -2.83942431e-01  1.17432319e-01\r\n    -2.39941105e-02 -5.05199004e-03 -3.24927509e-01 -1.92242026e-01\r\n    -2.32359871e-01  1.69598401e-01 -6.73909187e-02 -2.77285129e-01\r\n    -7.68555179e-02 -1.70542046e-01 -1.88991517e-01 -8.68433595e-01\r\n    -1.02858674e-02 -3.43116000e-03 -1.79375470e-01  1.18717417e-01]\r\n   [-2.04889283e-01  2.08591044e-01  9.64246616e-02 -1.86262533e-01\r\n     9.28857327e-02  2.78928667e-01  5.80411106e-02  9.86151630e-04\r\n    -5.19180000e-01 -8.78392756e-02 -2.75684237e-01 -3.86399031e-02\r\n    -1.55971497e-01 -9.68710613e-03 -2.85900235e-02  1.33511990e-01\r\n     7.53978118e-02  1.46718010e-01 -8.03706050e-02 -6.33813322e-01\r\n    -1.69238657e-01  1.71853319e-01  1.30034760e-01 -1.18172159e-02]\r\n   [-1.28542632e-01  4.25548434e-01 -2.29449928e-01  3.89312096e-02\r\n    -1.65885776e-01 -1.99971572e-02 -1.08196117e-01  8.95126611e-02\r\n     3.53904128e-01 -9.67229754e-02  4.15033728e-01 -2.24315524e-02\r\n     1.29609928e-01 -8.90183523e-02  8.38817358e-02  2.78006792e-02\r\n     1.51575685e-01 -3.87242615e-01  2.29464203e-01 -3.81488204e-01\r\n    -3.06030840e-01  6.84344545e-02  5.09080589e-01  2.61849575e-02]\r\n   [ 4.71375167e-01 -9.34574366e-01  1.08531432e-03 -7.87218735e-02\r\n    -4.89057116e-02 -8.69275704e-02 -1.71399415e-01 -1.00738801e-01\r\n    -4.30647641e-01  7.75474012e-02 -1.64827123e-01  3.39800715e-02\r\n     5.21249212e-02 -2.17245758e-01 -1.94392800e-01 -1.65979788e-01\r\n     2.43169427e-01  5.73095530e-02 -5.83508611e-01  4.43970487e-02\r\n     2.67295800e-02  1.18425176e-01  8.86634588e-02 -1.96755871e-01]\r\n   [ 1.82679504e-01  4.40659344e-01 -2.77637690e-01  5.46818227e-02\r\n    -5.98022640e-01  1.93664104e-01 -8.71252492e-02  9.70428735e-02\r\n    -2.42328830e-02  6.92596734e-02  4.04956378e-02 -2.31157959e-01\r\n    -2.22321317e-01  2.08656564e-01  2.46453464e-01 -1.30037129e-01\r\n     1.54963374e-01  8.59821290e-02 -2.20283955e-01 -8.33009183e-01\r\n    -1.78401425e-01  1.77939400e-01 -6.70896545e-02 -8.69612917e-02]\r\n   [-5.54511435e-02  2.67629474e-01  1.01174399e-01 -2.79899627e-01\r\n     6.19819045e-01 -5.66604128e-03  5.77240944e-01  6.94607720e-02\r\n     3.26803327e-01 -4.42883037e-02 -3.02192658e-01 -2.15992391e-01\r\n    -1.09978817e-01 -1.31681070e-01 -2.24570155e-01 -2.53850430e-01\r\n    -2.55495328e-02  3.06538284e-01  2.17517868e-01 -2.10747793e-01\r\n     1.11738577e-01 -3.55073869e-01  5.07014632e-01  4.22773287e-02]\r\n   [ 4.07418311e-01 -1.15276292e-01 -2.28524491e-01 -6.19836338e-02\r\n     5.20426854e-02 -3.66138637e-01  4.23618406e-01 -2.79300153e-01\r\n    -2.07439944e-01  3.62201668e-02 -4.25521880e-02 -1.04741216e-01\r\n    -2.34472036e-01  1.95396781e-01 -8.03100467e-02  9.91918743e-02\r\n    -1.79659724e-01 -4.80249375e-02  5.72227776e-01  6.15854204e-01\r\n    -1.49003193e-01 -1.54236540e-01  2.26080164e-01 -8.27159435e-02]\r\n   [ 5.26334941e-01  9.64579463e-01  9.69384983e-02  1.42855898e-01\r\n     4.36617017e-01  1.00900285e-01  4.30808574e-01 -9.22128744e-03\r\n     1.48015112e-01  1.28863990e-01  5.29787660e-01  4.47707772e-02\r\n    -6.42261505e-02 -2.39087775e-01  2.07085133e-01 -1.01645693e-01\r\n     7.45059624e-02 -2.70302773e-01 -3.36763680e-01 -3.89610827e-01\r\n    -1.77468956e-01  1.95882231e-01  9.40611139e-02 -2.82286793e-01]\r\n   [ 1.85213402e-01 -5.86649105e-02  2.05710500e-01  3.07661351e-02\r\n     9.05001834e-02  2.00060785e-01 -3.80850554e-01  4.20084558e-02\r\n    -3.83664072e-01  3.95992473e-02  8.42069983e-02  9.44125652e-02\r\n     3.21823843e-02  6.08641729e-02  2.27790177e-01  9.02339220e-02\r\n    -1.03957392e-02 -2.23970786e-02  2.90961802e-01 -4.73595679e-01\r\n    -7.47651467e-03  6.28157461e-04 -2.31577665e-01 -1.36070281e-01]\r\n   [-1.13131911e-01  8.25909078e-02 -2.03457922e-01  1.56570554e-01\r\n    -1.57545924e+00  2.55765706e-01 -5.63215911e-01  1.59264117e-01\r\n     1.41989393e-02 -3.21571976e-02  5.44473886e-01  1.63198650e-01\r\n     5.46823069e-02  2.25484610e-01 -1.78793609e-01  1.14633210e-01\r\n    -1.44573614e-01  7.45126465e-03 -3.11962128e-01 -1.20327257e-01\r\n    -2.81678945e-01 -5.19150086e-02 -5.98734140e-01  2.05801073e-02]]]]\r\n\r\n(24,) bias = [-0.02228645,  0.29005498, -0.03637461, -0.04572997,  0.24709156,\r\n       -0.31160167,  0.29033694, -0.03939106,  0.41529036, -0.01517697,\r\n       -0.05451803,  0.        , -0.08816326, -0.01140359,  0.        ,\r\n       -0.073258  ,  0.10698747, -0.08802168,  0.22477017,  0.12871496,\r\n       -0.02694024, -0.06330398,  0.03239549, -0.0437121 ]\r\n\r\n(1,1,1,24) output_cpu_correct = [[[[1.015128493309021, 0.35228434205055237, -2.612255334854126, -1.406202793121338, -3.079547166824341, 0.9717371463775635, -0.33331742882728577, -1.4159433841705322, 0.76167893409729, -2.0503242015838623, 1.4258131980895996, -1.583984613418579, -2.8278791904449463, -1.7418254613876343, -2.2799108028411865, -2.5715744495391846, -3.6979751586914062, -2.469231367111206, -0.8796918392181396, -2.2974114418029785, -3.2031168937683105, -2.070390224456787, 0.29765796661376953, -2.0511951446533203]]]]\r\n\r\n(1,1,1,24) output_gpu_opengl_wrong = [[[[4.683575630187988, 1.3978171348571777, 1.0626060962677002, -1.786192536354065, 0.01327398419380188, 0.5346361398696899, 3.513613700866699, -2.963850498199463, -0.6838104724884033, -2.3620340824127197, -0.2669147849082947, -3.568108320236206, -4.572218418121338, -4.519959926605225, -3.8082704544067383, 9.016161918640137, -7.485948085784912, -2.8423800468444824, -0.07386936247348785, -3.2079782485961914, -3.2031168937683105, -2.070390224456787, 0.2976588010787964, -2.0511956214904785]]]]\r\n\r\n\r\n\r\n\r\n", "It is worth noting that the last 4 output values are correct (up to precision of computation) also in OpenGL.", "@impjdi could you take a look?", "Could you please attach a minimal tflite file that can repro the issue?  it's difficult to port those floating point numbers into opengl textures :P \r\n\r\nAlso, are you running this in FP16 or FP32?", "I do not set the FP precision anywhere so I presume it runs in FP32. This is how I created the attached tfile module from tf2 Keras model, first converted to frozen graph concrete function:\r\n\r\n```\r\ntflite_converter = tf.lite.TFLiteConverter.from_concrete_functions([frozen_func])\\\r\ntflite_module = tflite_converter.convert()\r\nwith open(os.path.join(args.destination_dir, \"module.tflite\"), \"wb\") as f:\r\n    f.write(tflite_module)\r\n```\r\n\r\nThe example is not minimal, but it is what I can do now to reproduce on Jetson TX2. It is an image classification network, which ends with 1x1 Conv, Reshape and Flatten. So the layers[-3] is the problematic one. The last two layers do not manipulate values, only shape (I needed '1D' output). The network sample is cut from a much larger net. The input to this is \r\n\r\n`inputs = keras.Input(shape=(144, 72, 3), name=\"input_layer\")`\r\n\r\nwhere the input is zero mean unit variance data, hopefully you can reproduce with random data on TX2, if not I will provide also the input image. Running this exact tflite.module on Jetson TX2 returns correct results in CPU mode and wrong result in GPU mode.\r\n\r\n\r\n\r\n\r\n\r\n", "So it seems I was misguided, the Conv2D layer is optimized somewhere during conversion and actually fully_connected shader is running. It makes sense because the operation with data and convolution kernel with WxH=1x1 can be computed via fully connected layer. The problem remains, just that I blamed poor NewConvolution1x1NodeShader while it seems now the issue is with `FullyConnectedBuffers ` class.\r\n\r\nThis is the shader that is being used:\r\n\r\n```\r\nRegistry::GenerateCode() [ctx.op_type = fully_connected] generated_code.source_code:\r\n\r\n  const int threads = int(gl_WorkGroupSize.y);\r\n  const int workers = int(gl_WorkGroupSize.x);\r\n  ivec3 tid = ivec3(gl_LocalInvocationID);\r\n\r\n  if (gid.x < $dst_depth$) {\r\n    int offset = 4 * gid.x * $src_depth$ + 4 * tid.y;\r\n    int iterations = ($src_depth$ + threads-1) / threads;\r\n    for (int d = 0; d < iterations; d++, offset += 4 * threads) {\r\n      vec4 src = $input_data_0[0, 0, d * threads + tid.y]$;\r\n      value_0.x += dot(src, $weights[offset + 0]$);\r\n      value_0.y += dot(src, $weights[offset + 1]$);\r\n      value_0.z += dot(src, $weights[offset + 2]$);\r\n      value_0.w += dot(src, $weights[offset + 3]$);\r\n    }\r\n    sh_mem[workers * tid.y + tid.x] = value_0;\r\n  }\r\n  memoryBarrierShared();\r\n  barrier();\r\n\r\n  if (tid.y > 0 || gid.x >= $dst_depth$) {\r\n    return;\r\n  }\r\n\r\n  for (int t = 1; t < threads; t++) {\r\n    value_0 += sh_mem[workers * t + tid.x];\r\n  }\r\n  value_0 += $bias[gid.x]$;\r\n  $output_data_0[0, 0, gid.x] = value_0$;\r\n```\r\n\r\n", "Dammit, I made a mistake and was running on tflite 2.4.1 instead of 2.5.0. In 2.5.0 there is a [bugfix ](https://github.com/tensorflow/tensorflow/commit/6927b25a40869537e582cf77956affe9e9738198#diff-cb53ee4470925c90c8f5bf084c033e259b02778fd63dc9bfef474278625e9df0) of FullyConnected layer that might be relevant to this issue. I will rebuild my codes with 2.5.0 and close this report if all works well...", "Good find!", "Yep, it was this one missing bugfix. Thx for your help anyway!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49269\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49269\">No</a>\n"]}, {"number": 49268, "title": "M1 chip", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n# Copy and paste here\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n", "comments": []}, {"number": 49267, "title": "AttributeError: Layer genmodel is not connected, no input to return.", "body": "I want to train a custom GAN model, for which below is the code:\r\n```\r\nimport tensorflow as tf\r\nimport os\r\nimport numpy as np\r\nimport cv2\r\nimport matplotlib.pyplot as plt\r\nfrom tqdm import tqdm\r\nfrom imageio import imwrite\r\n\r\n\r\nclass Localization(tf.keras.layers.Layer):\r\n    def __init__(self):\r\n        super(Localization, self).__init__()\r\n        self.bpool1 = tf.keras.layers.MaxPool2D()\r\n        self.bpool2 = tf.keras.layers.MaxPool2D()\r\n        self.bpool3 = tf.keras.layers.MaxPool2D()\r\n        self.bpool4 = tf.keras.layers.MaxPool2D()\r\n\r\n        self.mpool1 = tf.keras.layers.MaxPool2D()\r\n        self.mpool2 = tf.keras.layers.MaxPool2D()\r\n        self.mpool3 = tf.keras.layers.MaxPool2D()\r\n        self.mpool4 = tf.keras.layers.MaxPool2D()\r\n\r\n        self.cpool1 = tf.keras.layers.MaxPool2D()\r\n        self.cpool2 = tf.keras.layers.MaxPool2D()\r\n        self.cpool3 = tf.keras.layers.MaxPool2D()\r\n        self.cpool4 = tf.keras.layers.MaxPool2D()\r\n\r\n        self.bconv1 = tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu')\r\n        self.bconv2 = tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu')\r\n        self.bconv3 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu')\r\n        self.bconv4 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu')\r\n        self.bconv5 = tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu')\r\n        self.bconv6 = tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu')\r\n        self.bconv7 = tf.keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu')\r\n        self.bconv8 = tf.keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu')\r\n\r\n        self.mconv1 = tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu')\r\n        self.mconv2 = tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu')\r\n        self.mconv3 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu')\r\n        self.mconv4 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu')\r\n        self.mconv5 = tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu')\r\n        self.mconv6 = tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu')\r\n        self.mconv7 = tf.keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu')\r\n        self.mconv8 = tf.keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu')\r\n\r\n        self.cconv1 = tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu')\r\n        self.cconv2 = tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu')\r\n        self.cconv3 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu')\r\n        self.cconv4 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu')\r\n        self.cconv5 = tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu')\r\n        self.cconv6 = tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu')\r\n        self.cconv7 = tf.keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu')\r\n        self.cconv8 = tf.keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu')\r\n\r\n        self.concatenate1 = tf.keras.layers.concatenate\r\n        self.concatenate2 = tf.keras.layers.concatenate\r\n        self.tconv1 = tf.keras.layers.Conv2D(512, (3, 3), padding='same', activation='relu')\r\n        self.tpool1 = tf.keras.layers.MaxPool2D()\r\n        self.tconv2 = tf.keras.layers.Conv2D(512, (3, 3), padding='same', activation='relu')\r\n        self.tpool2 = tf.keras.layers.MaxPool2D()\r\n        self.flatten = tf.keras.layers.Flatten()\r\n        self.fc0 = tf.keras.layers.Dense(100, activation='relu')\r\n        self.fc1 = tf.keras.layers.Dense(20, activation='relu')\r\n        self.fc2 = tf.keras.layers.Dense(6, activation=None, bias_initializer=tf.keras.initializers.constant(\r\n            [1.0, 0.0, 0.0, 0.0, 1.0, 0.0]), kernel_initializer='zeros',  activity_regularizer=tf.keras.regularizers.l2(1e-2))\r\n\r\n    def build(self, input_shape):\r\n        print(\"Building Localization Network with input shape:\", input_shape)\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return [None, 6]\r\n\r\n    def call(self, inputs):\r\n        mask, fg, bg, composite = inputs\r\n        xm = self.concatenate1([fg, mask])\r\n        xm = self.mconv1(xm)\r\n        xm = self.mconv2(xm)\r\n        xm = self.mpool1(xm)\r\n\r\n        xm = self.mconv3(xm)\r\n        xm = self.mconv4(xm)\r\n        xm = self.mpool2(xm)\r\n\r\n        xm = self.mconv5(xm)\r\n        xm = self.mconv6(xm)\r\n        xm = self.mpool3(xm)\r\n\r\n        xm = self.mconv7(xm)\r\n        xm = self.mconv8(xm)\r\n        xm = self.mpool4(xm)\r\n\r\n        xbg = self.bconv1(bg)\r\n        xbg = self.bconv2(xbg)\r\n        xbg = self.bpool1(xbg)\r\n\r\n        xbg = self.bconv3(xbg)\r\n        xbg = self.bconv4(xbg)\r\n        xbg = self.bpool2(xbg)\r\n\r\n        xbg = self.bconv5(xbg)\r\n        xbg = self.bconv6(xbg)\r\n        xbg = self.bpool3(xbg)\r\n\r\n        xbg = self.bconv7(xbg)\r\n        xbg = self.bconv8(xbg)\r\n        xbg = self.bpool4(xbg)\r\n\r\n\r\n        xc = self.cconv1(composite)\r\n        xc = self.cconv2(xc)\r\n        xc = self.cpool1(xc)\r\n\r\n        xc = self.cconv3(xc)\r\n        xc = self.cconv4(xc)\r\n        xc = self.cpool2(xc)\r\n\r\n        xc = self.cconv5(xc)\r\n        xc = self.cconv6(xc)\r\n        xc = self.cpool3(xc)\r\n\r\n        xc = self.cconv7(xc)\r\n        xc = self.cconv8(xc)\r\n        xc = self.cpool4(xc)\r\n\r\n        x = self.concatenate2((xbg, xm, xc))\r\n        x = self.tconv1(x)\r\n        x = self.tpool1(x)\r\n        x = self.tconv2(x)\r\n        x = self.tpool1(x)\r\n        x = self.flatten(x)\r\n        x = self.fc0(x)\r\n        x = self.fc1(x)\r\n        theta = self.fc2(x)\r\n        theta = tf.keras.layers.Reshape((2, 3))(theta)\r\n        return theta\r\n\r\n\r\nclass BilinearInterpolation(tf.keras.layers.Layer):\r\n    def __init__(self, height=320, width=320):\r\n        super(BilinearInterpolation, self).__init__()\r\n        self.height = height\r\n        self.width = width\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return [None, self.height, self.width, 1]\r\n\r\n    def get_config(self):\r\n        return {\r\n            'height': self.height,\r\n            'width': self.width,\r\n        }\r\n\r\n    def build(self, input_shape):\r\n        print(\"Building Bilinear Interpolation Layer with input shape:\", input_shape)\r\n\r\n    def advance_indexing(self, inputs, x, y):\r\n        '''\r\n        Numpy like advance indexing is not supported in tensorflow, hence, this function is a hack around the same method\r\n        '''\r\n        shape = tf.shape(inputs)\r\n        batch_size, _, _ = shape[0], shape[1], shape[2]\r\n\r\n        batch_idx = tf.range(0, batch_size)\r\n        batch_idx = tf.reshape(batch_idx, (batch_size, 1, 1))\r\n        b = tf.tile(batch_idx, (1, self.height, self.width))\r\n        indices = tf.stack([b, y, x], 3)\r\n        return tf.gather_nd(inputs, indices)\r\n\r\n    def call(self, inputs):\r\n        images, theta = inputs\r\n        homogenous_coordinates = self.grid_generator(batch=tf.shape(images)[0])\r\n        return self.interpolate(images, homogenous_coordinates, theta)\r\n\r\n    def grid_generator(self, batch):\r\n        x = tf.linspace(-1, 1, self.width)\r\n        y = tf.linspace(-1, 1, self.height)\r\n\r\n        xx, yy = tf.meshgrid(x, y)\r\n        xx = tf.reshape(xx, (-1,))\r\n        yy = tf.reshape(yy, (-1,))\r\n        homogenous_coordinates = tf.stack([xx, yy, tf.ones_like(xx)])\r\n        homogenous_coordinates = tf.expand_dims(homogenous_coordinates, axis=0)\r\n        homogenous_coordinates = tf.tile(homogenous_coordinates, [batch, 1, 1])\r\n        homogenous_coordinates = tf.cast(homogenous_coordinates, dtype=tf.float32)\r\n        return homogenous_coordinates\r\n\r\n    def interpolate(self, images, homogenous_coordinates, theta):\r\n        with tf.name_scope(\"Transformation\"):\r\n            transformed = tf.matmul(theta, homogenous_coordinates)\r\n            transformed = tf.transpose(transformed, perm=[0, 2, 1])\r\n            transformed = tf.reshape(transformed, [-1, self.height, self.width, 2])\r\n\r\n            x_transformed = transformed[:, :, :, 0]\r\n            y_transformed = transformed[:, :, :, 1]\r\n\r\n            x = ((x_transformed + 1.) * tf.cast(self.width, dtype=tf.float32)) * 0.5\r\n            y = ((y_transformed + 1.) * tf.cast(self.height, dtype=tf.float32)) * 0.5\r\n\r\n        with tf.name_scope(\"VariableCasting\"):\r\n            x0 = tf.cast(tf.math.floor(x), dtype=tf.int32)\r\n            x1 = x0 + 1\r\n            y0 = tf.cast(tf.math.floor(y), dtype=tf.int32)\r\n            y1 = y0 + 1\r\n\r\n            x0 = tf.clip_by_value(x0, 0, self.width - 1)\r\n            x1 = tf.clip_by_value(x1, 0, self.width - 1)\r\n            y0 = tf.clip_by_value(y0, 0, self.height - 1)\r\n            y1 = tf.clip_by_value(y1, 0, self.height - 1)\r\n            x = tf.clip_by_value(x, 0, tf.cast(self.width, dtype=tf.float32) - 1.0)\r\n            y = tf.clip_by_value(y, 0, tf.cast(self.height, dtype=tf.float32) - 1)\r\n\r\n        with tf.name_scope(\"AdvanceIndexing\"):\r\n            Ia = self.advance_indexing(images, x0, y0)\r\n            Ib = self.advance_indexing(images, x0, y1)\r\n            Ic = self.advance_indexing(images, x1, y0)\r\n            Id = self.advance_indexing(images, x1, y1)\r\n\r\n        with tf.name_scope(\"Interpolation\"):\r\n            x0 = tf.cast(x0, dtype=tf.float32)\r\n            x1 = tf.cast(x1, dtype=tf.float32)\r\n            y0 = tf.cast(y0, dtype=tf.float32)\r\n            y1 = tf.cast(y1, dtype=tf.float32)\r\n\r\n            wa = (x1 - x) * (y1 - y)\r\n            wb = (x1 - x) * (y - y0)\r\n            wc = (x - x0) * (y1 - y)\r\n            wd = (x - x0) * (y - y0)\r\n\r\n            wa = tf.expand_dims(wa, axis=3)\r\n            wb = tf.expand_dims(wb, axis=3)\r\n            wc = tf.expand_dims(wc, axis=3)\r\n            wd = tf.expand_dims(wd, axis=3)\r\n\r\n        return tf.math.add_n([wa * Ia + wb * Ib + wc * Ic + wd * Id])\r\n\r\n\r\nclass Composition(tf.keras.layers.Layer):\r\n    def __init__(self):\r\n        super(Composition, self).__init__()\r\n\r\n    def build(self, input_shape):\r\n        print(\"Building Composition Network with input shape:\", input_shape)\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return input_shape\r\n\r\n    def call(self, inputs):\r\n        mask, fg, bg = inputs\r\n        multiples = tf.constant([1, 1, 1, 3], tf.int32)\r\n        mask_mod = tf.tile(mask, multiples)\r\n        bg_mod = tf.keras.layers.Multiply()([bg, 1 - mask_mod])\r\n        fg_mod = tf.keras.layers.Multiply()([fg, mask_mod])\r\n        composite_image = tf.keras.layers.Add()([bg_mod, fg_mod])\r\n\r\n        return composite_image\r\n\r\n\r\nclass Genmodel(tf.keras.Model):\r\n    def __init__(self, bg_shape, iterations):\r\n        super(Genmodel, self).__init__()\r\n        self.bg_shape = bg_shape\r\n        self.iterations = iterations\r\n        self.localize = Localization()\r\n        self.bilinearintp1 = BilinearInterpolation(height=self.bg_shape[0], width=self.bg_shape[1])\r\n        self.bilinearintp2 = BilinearInterpolation(height=self.bg_shape[0], width=self.bg_shape[1])\r\n        self.compose1 = Composition()\r\n        self.compose2 = Composition()\r\n\r\n    def call(self, inputs):\r\n        mask, fg, bg = inputs\r\n        xmask = mask\r\n        xfg = fg\r\n        composite = self.compose1([xmask, xfg, bg])\r\n        for i in range(self.iterations):\r\n            theta = self.localize([xmask, xfg, bg, composite])\r\n            xmask = self.bilinearintp1([xmask, theta])\r\n            xfg = self.bilinearintp2([xfg, theta])\r\n            composite = self.compose2([xmask, xfg, bg])\r\n        return composite\r\n\r\n\r\ndef disc_model(input_shape):\r\n    inp = tf.keras.layers.Input(shape=input_shape)\r\n    x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), padding='same')(inp)\r\n    x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), padding='same')(x)\r\n    x = tf.keras.layers.LeakyReLU()(x)\r\n    x = tf.keras.layers.MaxPool2D()(x)\r\n\r\n    x = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), padding='same')(x)\r\n    x = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), padding='same')(x)\r\n    x = tf.keras.layers.LeakyReLU()(x)\r\n    x = tf.keras.layers.MaxPool2D()(x)\r\n\r\n    x = tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), padding='same')(x)\r\n    x = tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), padding='same')(x)\r\n    x = tf.keras.layers.LeakyReLU()(x)\r\n    x = tf.keras.layers.MaxPool2D()(x)\r\n\r\n    x = tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3), padding='same')(x)\r\n    x = tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3), padding='same')(x)\r\n    x = tf.keras.layers.LeakyReLU()(x)\r\n    x = tf.keras.layers.MaxPool2D()(x)\r\n\r\n    x = tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3), padding='same')(x)\r\n    x = tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3), padding='same')(x)\r\n    x = tf.keras.layers.LeakyReLU()(x)\r\n    x = tf.keras.layers.MaxPool2D()(x)\r\n\r\n    x = tf.keras.layers.Flatten()(x)\r\n    x = tf.keras.layers.Dense(25)(x)\r\n    x = tf.keras.layers.LeakyReLU()(x)\r\n    out = tf.keras.layers.Dense(1, activation='sigmoid')(x)\r\n    return tf.keras.models.Model(inputs=inp, outputs=out, name='discriminator')\r\n\r\n\r\ndef stgan2(disc, gen):\r\n    disc.trainable = False\r\n    inp = gen.input\r\n    x = gen(inp)\r\n    out = disc(x)\r\n    model = tf.keras.Model(inputs=inp, outputs=out)\r\n\r\n    model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5),\r\n                  metrics=['accuracy'])\r\n    return model\r\n\r\n\r\ngen = Genmodel(bg_shape=(420, 640, 3), iterations=5)\r\ndis = disc_model(input_shape=(420, 640, 3))\r\ndis.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5), metrics=['accuracy'])\r\ngan = stgan2(dis, gen)\r\n```\r\nUpon executing this code in google colab I am getting this error:\r\nAttributeError: Layer genmodel is not connected, no input to return.", "comments": ["@abhijit1247 ,\r\n\r\nPlease take a look at this issues with similar error log.[link1](https://github.com/tensorflow/tensorflow/issues/34454),[link2](https://stackoverflow.com/questions/62654908/layer-is-not-connected-no-input-to-return-error-while-trying-to-get-intermedi).It helps.\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 49266, "title": "Running TFLite inference on single thread using XNNPACK and Flex delegate", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 8\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: x86_64\r\n- TensorFlow installed from: source\r\n- TensorFlow version (use command below): 2.3.2\r\n- Bazel version (if compiling from source): 3.7.1\r\n- GCC/Compiler version (if compiling from source): 8.3.1\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A, 32GB RAM\r\n\r\nI am interested in running TFLite inference on single thread. My model uses a NonMaxSuppression layer for which I built TFLite with flex delegate dependency. I also included xnnpack delegate which runs inference on single thread by default ([link](https://fossies.org/linux/tensorflow/tensorflow/lite/delegates/xnnpack/README.md), [link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md))\r\n\r\nMy interpreter is defined as follows:\r\n```\r\ntflite::ops::builtin::BuiltinOpResolver builtins;\r\nstd::unique_ptr<tflite::Interpreter> interpreter;\r\ntflite::InterpreterBuilder(*flat_buffer_model, builtins)(&interpreter, 1);\r\ninterpreter->SetNumThreads(1);\r\nthread_interpreter.interpreter_.reset(interpreter.release());\r\n```\r\n\r\nwhere thread_interpreter is an object of type :\r\n```\r\nstruct ThreadInterpreter {\r\n\texplicit ThreadInterpreter() :\r\n\t\tinterpreter_(nullptr),\r\n\t\tinput_tensor(nullptr),\r\n\t\toutput_tensor(){}\r\n\r\n\tboost::shared_ptr<tflite::Interpreter> interpreter_;\r\n\tTfLiteTensor* input_tensor;\r\n\tstd::vector<TfLiteTensor*> output_tensor;\r\n};\r\n```\r\n\r\nThe program runs inference on few images and crashes. And here is the output of gdb:\r\n```\r\n(gdb) bt\r\n#0  0x00007ffff5e314c0 in __pthread_timedjoin_ex () from /lib64/libpthread.so.0\r\n#1  0x00007ffff3686661 in ?? ()\r\n   from /home/smaniyar/Projects/test_project/target/linux-x86-64/bin/validation_package/11/lib/libtensorflowlite.so\r\n#2  0x00007ffff3696137 in ?? ()\r\n   from /home/smaniyar/Projects/test_project/target/linux-x86-64/bin/validation_package/11/lib/libtensorflowlite.so\r\n#3  0x00007fffeb67dd69 in ?? ()\r\n   from /home/smaniyar/Projects/test_project/target/linux-x86-64/bin/validation_package/11/lib/libtensorflowlite.so\r\n#4  0x00007fffeb67e1b1 in ?? ()\r\n   from /home/smaniyar/Projects/test_project/target/linux-x86-64/bin/validation_package/11/lib/libtensorflowlite.so\r\n#5  0x00007fffeb640353 in tflite::flex::DelegateData::~DelegateData() ()\r\n   from /home/smaniyar/Projects/test_project/target/linux-x86-64/bin/validation_package/11/lib/libtensorflowlite.so\r\n#6  0x00007fffeb639134 in tflite::FlexDelegate::~FlexDelegate() ()\r\n   from /home/smaniyar/Projects/test_project/target/linux-x86-64/bin/validation_package/11/lib/libtensorflowlite.so\r\n#7  0x00007fffee762e3f in tflite::TfLiteDelegateFactory::DeleteSimpleDelegate(TfLiteDelegate*) ()\r\n   from /home/smaniyar/Projects/test_project/target/linux-x86-64/bin/validation_package/11/lib/libtensorflowlite.so\r\n#8  0x00007fffeb61799e in tflite::impl::Interpreter::~Interpreter() ()\r\n   from /home/smaniyar/Projects/test_project/target/linux-x86-64/bin/validation_package/11/lib/libtensorflowlite.so\r\n#9  0x00007ffff72bbdc7 in boost::checked_delete<tflite::impl::Interpreter> (x=0xe83960)\r\n    at /home/smaniyar/Projects/test_project/vendor/boost/boost/checked_delete.hpp:34\r\n#10 0x00007ffff72bc998 in boost::detail::sp_counted_impl_p<tflite::impl::Interpreter>::dispose (this=0x129b190)\r\n    at /home/smaniyar/Projects/test_project/vendor/boost/boost/smart_ptr/detail/sp_counted_impl.hpp:78\r\n#11 0x00007ffff70af8b6 in boost::detail::sp_counted_base::release (this=0x129b190)\r\n    at /home/smaniyar/Projects/test_project/vendor/boost/boost/smart_ptr/detail/sp_counted_base_gcc_x86.hpp:146\r\n#12 0x00007ffff70af949 in boost::detail::shared_count::~shared_count (this=0x100bd88, __in_chrg=<optimized out>)\r\n    at /home/smaniyar/Projects/test_project/vendor/boost/boost/smart_ptr/detail/shared_count.hpp:371\r\n#13 0x00007ffff72b962c in boost::shared_ptr<tflite::impl::Interpreter>::~shared_ptr (this=0x100bd80, __in_chrg=<optimized out>)\r\n    at /home/smaniyar/Projects/test_project/vendor/boost/boost/smart_ptr/shared_ptr.hpp:328\r\n#14 0x00007ffff72bca60 in TensorflowInferenceCore::ThreadInterpreter::~ThreadInterpreter (this=0x100bd80,\r\n    __in_chrg=<optimized out>)\r\n    at /home/smaniyar/Projects/test_project/src-main/deep_learning_inference/tensorflow_lite/TensorflowInferenceCore.h:17\r\n```\r\n\r\nDuring program execution I do see the flex delegate being deployed : \r\n```\r\nINFO: TfLiteFlexDelegate delegate: 1 nodes delegated out of 398 nodes with 1 partitions.\r\n```\r\nBut I do not see any such message for xnnpack delegate and I am not sure if my program is even using that for inference.\r\n\r\nIs there a way I can confine inference to run on single thread using flex and xnnpack delegates?\r\nAlso it would be very helpful if I could know how to verify that xnnpack is being used by my TFLite runtime.\r\n\r\nThanks.\r\n", "comments": ["@thaink Tagging you for your reference.", "@thaink could you take a look?", "**Correction**\r\nI found out from a separate [discussion](https://github.com/tensorflow/tensorflow/issues/42277), that I am **NOT** using xnnpack. I previously thought it is a requirement for single threaded execution. But right now, single threaded execution is achieved only by passing an argument to InterpreterBuilder. \r\nAlso the test program that I am running uses forking to create multiple child processes which seems to create this lock condition. ", "You are running multiple interpreters, each on a different thread, right.\r\nFor clarifying, did you got the error when running only one interpreter single-threaded?", "@thaink \r\nI am running one interpreter, single thread.\r\nThe gdb output is the error that I get with that.", "My test program uses forking. And I get this error with multiple forks running same interpreter. But I also get the same error even when only one child process is running inference.", "tflite::Interpreter is not design to be run concurrently, so you will need to create one for each process (and each flat_buffer_model object too).\r\nCan you test if it still happen if you change shared_ptr to unique_ptr?", "@thaink Thanks for your response.\r\nI am running into some other errors related to my test program when using unique_ptr. I will try to fix it and get back to you.\r\nIn the mean time, are there any plans for supporting tflite::Interpreter concurrently in upcoming releases?\r\nAlso I am using flex delegate. Any threading/concurrency issues that you are aware of for flex delegate?", "@thaink \r\nI moved my definition of Interpreter and flat-buffer model inside individual processes and now everything works perfectly! Thanks for your help.\r\n\r\nI did notice that when I use flex delegate, the total number of thread goes up to 62 but without flex, it is close to 4. Is there a way to suppress number of threads to 1 when using flex delegate?", "As I know, the interpreter->SetNumThreads should work for Flex delegate as well.", "I tried this too. The number of threads was still 62.", "@abattery Do you have more insight about the number of threads?", "The number of thread should be configured before the TensorFlow Lite Flex delegate initializes. Please consider using the interpreter builder to specify the number of thread.", "@abattery \r\nI did configure number of threads in the interpreter builder. This is my code snippet : \r\n```\r\ntflite::ops::builtin::BuiltinOpResolver builtins;\r\nstd::unique_ptr<tflite::Interpreter> interpreter;\r\ntflite::InterpreterBuilder(*flat_buffer_model, builtins)(&interpreter, 1);\r\ninterpreter->SetNumThreads(1);\r\nthread_interpreter.interpreter_.reset(interpreter.release());\r\n```\r\nIt stills generates multiple threads.", "We only allow one thread for single Flex op executions but we don't limit the inter op parallelism for now yet. Thanks for the report.", "How many Select TF operators do you have in your model? Even though multiple threads can be generated multiple times but threads for Flex delegate won't be increased by creating more TFLite interpreter instances since the overall TensorFlow threads will be limited. And also, if the model does not have many Select TF operators in your model, the model executions will be serialized anyway by the TFLite interpreter.", "I just have one Select TF operator (NonMaxSuppression).\r\nWhen I remove it from the model and add it as post processing step in C++, the program runs fine on 2 threads.\r\nBut I would like to use the NonMaxSuppression that is built-in in TensorFlow.\r\n", "Could you please upload a new issue for the above feature request, NonMaxSuppression builtin support?", "Closing since you have a workaround for it."]}, {"number": 49265, "title": " \"download_dependencies.sh\" doesn't work", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- Mac OS BigSur 11.2.3(arm64):\r\n- TensorFlow installed from (source or binary): https://github.com/tensorflow/tensorflow\r\n- TensorFlow version (use command below): the latest\r\n- Python version: 3.8\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nI try to use tensorflowlite with ios , and when i followed the instructions to the step \"download_dependencies.sh\" ,it shows that : \r\n usage: grep [-abcDEFGHhIiJLlmnOoqRSsUVvwxZ] [-A num] [-B num] [-C[num]]\r\n\t[-e pattern] [-f file] [--binary-files=value] [--color=when]\r\n\t[--context[=num]] [--directories=action] [--label] [--line-buffered]\r\n\t[--null] [pattern] [file ...]\r\n\r\nI think something wrong with \"grep -oP\"\r\n**Describe the expected behavior**\r\ndownload the resources \r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes): - Briefly describe your candidate solution\r\n(if contributing):\r\nmodify the grep -oP\r\n\r\n<img width=\"571\" alt=\"\u622a\u5c4f2021-05-18 \u4e0b\u53489 28 15\" src=\"https://user-images.githubusercontent.com/55579125/118660438-c8aa4b80-b820-11eb-9d9c-283ccd0cf1a2.png\">\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@haolingzhang1 \r\n\r\nseems like this is similar to issue [#35747](https://github.com/tensorflow/tensorflow/issues/35747). Please take a look at this and let us know if you are still facing the same issue. Thanks!", "It's not the same problem I think , I put some \"echo\" to test and I found that the problem starts from line 37 in \"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/make/download_dependencies.sh\"\r\nand I found some notes in this file in line 64 : TODO(petewarden): Some new code in Eigen triggers a clang bug with iOS arm64,#                   so work around it by patching the source.\r\n", "I am also hitting this same issue related to grep usage when running download_dependencies.sh on macOS Big Sur.\r\n\r\nAny recommendation for fix / workaround to resolve this issue? ", "How about using CMake?\r\nhttps://www.tensorflow.org/lite/guide/build_cmake", "`grep -oP` is not supported on macOS. \r\n\r\ndownload_dependencies.sh script starts working for me on macOS after updating all `grep -oP` usage with an equivalent perl command as shown below\r\n\r\nReplace\r\n`EIGEN_COMMIT=\"$(grep -oP 'EIGEN_COMMIT = \"\\K[0-9a-f]{40}' \"${EIGEN_WORKSPACE_BZL_PATH}\")\"`\r\n\r\nwith\r\n`EIGEN_COMMIT=$( perl -ne 'if (/EIGEN_COMMIT = \"([0-9a-f]{40})/) { print $1 . \"\\n\" }' ${EIGEN_WORKSPACE_BZL_PATH} )`\r\n\r\n", "@shobanasuresh can you share file here?", "@sylajarlind I've attached the modified download_dependencies.sh script\r\n\r\n[download_dependencies.zip](https://github.com/tensorflow/tensorflow/files/6997138/download_dependencies.zip)\r\n", "Hi @terryheo - Would you be able to checkin the changes in the script that I've attached in my comment above in the tensorflow repo? These new changes work on both macOs and linux machines.", "<img width=\"388\" alt=\"Screen Shot 2021-08-17 at 10 30 06\" src=\"https://user-images.githubusercontent.com/80002509/129691969-88ba8b4f-7556-497f-bcdb-acce94a0531d.png\">\r\nI cannot run your script either", "Hi, as I shared today,\r\nhttps://groups.google.com/a/tensorflow.org/g/tflite/c/NJq6abdIXkU/m/Rssm4t3pAgAJ\r\nthe Makefile build is deprecated. Plz try to use CMake.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49265\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49265\">No</a>\n"]}]