[{"number": 46912, "title": "`tf.sparse.eye` crashes(aborts) when num_rows contains large number", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n\r\n**Describe the current behavior**\r\n`tf.sparse.eye` crashes(aborts) when `num_rows` contains large number\r\n\r\n**Describe the expected behavior**\r\nexpect an exception message if the input unexpected instead of crash. \r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.sparse.eye(num_rows=9223372036854775807, num_columns=None)\r\n~~~\r\n\r\nOutput\r\n~~~python\r\n2021-02-04 04:36:57.184236: F tensorflow/core/framework/tensor_shape.cc:345] Check failed: size >= 0 (-9223372036854775808 vs. 0)\r\nAborted (core dumped)\r\n~~~", "comments": ["I have tried in colab with TF versions 2.1, 2.4, nightly versions (`2.5.0-dev20210203`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/d19e64c4c9dc491d53fa38368e84eea6/untitled657.ipynb). Thanks!", "Was able to reproduce the issue using TF 2.6 Nightly version and the colab crashes. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/ac52112902392e4498f05c15cefbe074/untitled92.ipynb).", "With PR #51359 merged, this is issue is fixed now. I will close this issue but feel free to reopen if the issue persist.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46912\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46912\">No</a>\n"]}, {"number": 46911, "title": "tf.keras.backend.tile crash(aborts) when n is large", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n\r\n\r\n\r\n**Describe the current behavior**\r\n`tf.keras.backend.tile` crash(aborts) when `n` is large\r\n\r\n**Describe the expected behavior**\r\nexpect an exception message if the input unexpected instead of crash. \r\n\r\n**Standalone code to reproduce the issue**\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.keras.backend.tile(x=np.ones((1,1,1)), n=[100000000,100000000, 100000000])\r\n~~~\r\nOutput\r\n~~~python\r\n2021-02-04 04:10:34.072054: F tensorflow/core/framework/tensor_shape.cc:353] Check failed: 0 <= new_num_elements (0 vs. -1)\r\nAborted (core dumped)\r\n~~~", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/c767ecbb5d79e0cfdda8bebb6fa4582e/46911.ipynb). Thanks!", "Colab crashes in TF 2.6 as well.Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/b6bdaeacb30114dcac93f2325c54dfd2/copy-of-untitled92.ipynb).Thanks!", "Added PR #51138 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46911\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46911\">No</a>\n"]}, {"number": 46909, "title": "tf.summary.create_file_writer aborts ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n**Describe the current behavior**\r\n`tf.summary.create_file_writer` crash (abort)\r\n\r\n**Describe the expected behavior**\r\nexpect an exception message if the input unexpected instead of crash. \r\n\r\n**Standalone code to reproduce the issue**\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.summary.create_file_writer(logdir='', flush_millis=np.ones((1,2)))\r\n~~~\r\n\r\nOutput:\r\n~~~python\r\n2021-02-04 03:59:32.339427: F tensorflow/core/framework/tensor.cc:669] Check failed: 1 == NumElements() (1 vs. 2)Must have a one element tensor\r\nAborted (core dumped)\r\n~~~\r\n", "comments": ["I have tried in colab with TF versions 2.1,2.4,nightly versions(`2.5.0-dev20210203`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/9329366f22f1d5559d31065583e6f21e/untitled656.ipynb). Thanks!", "Colab is still crashing in TF 2.6 when I executed the code. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/4d07531868b085424317d42652909927/untitled92.ipynb#scrollTo=RvUKM453ViFa).Thanks!", "Created a PR #51715 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46909\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46909\">No</a>\n"]}, {"number": 46908, "title": "tf.nn.softmax_cross_entropy_with_logits and tf.keras.backend.categorical_crossentropy crash(abort)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n\r\n**Describe the current behavior**\r\n`tf.nn.softmax_cross_entropy_with_logits` and `tf.keras.backend.categorical_crossentropy` crash(abort) when `axis` is large\r\n\r\n**Describe the expected behavior**\r\nexpect an exception message if the input unexpected instead of crash. \r\n\r\n**Standalone code to reproduce the issue**\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.keras.backend.categorical_crossentropy(target=np.ones((1, 1, 1)), output=np.ones((1, 1, 1)), axis=9223372036854775807, from_logits=True)\r\n~~~\r\nOutput\r\n~~~python\r\n2021-02-04 03:56:25.011064: F tensorflow/core/framework/tensor_shape.cc:345] Check failed: size >= 0 (-9223372036854775808 vs. 0)\r\nAborted (core dumped)\r\n~~~\r\n\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.nn.softmax_cross_entropy_with_logits(labels=1, logits=1, axis=9223372036854775807)\r\n~~~\r\nOutput\r\n~~~python\r\n2021-02-04 03:55:56.843085: F tensorflow/core/framework/tensor_shape.cc:345] Check failed: size >= 0 (-9223372036854775808 vs. 0)\r\nAborted (core dumped)\r\n~~~\r\n\r\n", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/d016460f94fdc22c24d5fd23bc626936/46908.ipynb). Thanks!", "Was able to reproduce the issue in TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/e4a58cfb82be4aded3c9617e25f06e6a/untitled35.ipynb)..Thanks !", "The issue has been fixed with the latest tf-nightly:\r\n\r\n```\r\n# python3\r\nPython 3.8.10 (default, Jun  2 2021, 10:49:15) \r\n[GCC 9.4.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2021-08-28 16:42:13.583357: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2021-08-28 16:42:13.583402: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n>>> import numpy as np\r\n>>> tf.keras.backend.categorical_crossentropy(target=np.ones((1, 1, 1)), output=np.ones((1, 1, 1)), axis=9223372036854775807, from_logits=True)\r\n2021-08-28 16:42:14.990345: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2021-08-28 16:42:14.990388: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\r\n2021-08-28 16:42:14.990430: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-87-192): /proc/driver/nvidia/version does not exist\r\n2021-08-28 16:42:14.990839: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-08-28 16:42:14.992700: W tensorflow/core/framework/op_kernel.cc:1694] OP_REQUIRES failed at sequence_ops.cc:83 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[9223372036854775807] and type int64 on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"/usr/local/lib/python3.8/dist-packages/keras/backend.py\", line 5006, in categorical_crossentropy\r\n    return tf.nn.softmax_cross_entropy_with_logits(\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[9223372036854775807] and type int64 on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:Range]\r\n>>> \r\n>>> \r\n>>> \r\n>>> tf.nn.softmax_cross_entropy_with_logits(labels=1, logits=1, axis=9223372036854775807)\r\n2021-08-28 16:42:25.317106: W tensorflow/core/framework/op_kernel.cc:1694] OP_REQUIRES failed at sequence_ops.cc:83 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[9223372036854775807] and type int64 on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\", line 7083, in raise_from_not_ok_status\r\n    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[9223372036854775807] and type int64 on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:Range]\r\n>>> \r\n```", "I will close this issue since it has been resolved. Please feel free to reopen if the issue persists.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46908\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46908\">No</a>\n"]}, {"number": 46906, "title": "Use the right input_dims[] in lite/micro/kernels/cast_test.cc", "body": "Issue #45608: Bug fix for the micro op CAST's test code. ", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 46905, "title": "libtensorflowlite_flex_jni.so not found when using tensorflow-lite.aar", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- Yes, I have overwritten the tensorflow lite Transpose method to accept tensors larger than 4D\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- macOS Catalina 10.15.7\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- Running an emulator in AndroidStudio 4.1.2, Pixel_3a_API_30_x86\r\n\r\n- TensorFlow installed from (source or binary):\r\n- Source\r\n\r\n- TensorFlow version (use command below):  2.5.0 master branch HEAD (2/3/2021, commit 54bcf2919f7824995835726630064c383eb47149)\r\n- with modified tensorflow/lite/kernels/transpose.cc, and registering function in tensorflow/lite/kernels/register.cc and builtin_op_kernels.h)\r\n- Python version:3.7.5\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): Apple clang version 11.0.3 (clang-1103.0.32.29)\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI used that tensorflow code (with my custom transpose operation) to convert a pb file of the Glow model into a tflite file using the Python API.  I tested the model out in Python, and was able to load/invoke it just fine.\r\n\r\nI built the tensorflow-lite.aar and tensorflow-lite-select-tf-ops.aar files with the current version of tensorflow (commands used to build below).  I try loading the model in Android using Java, and the Interpreter() constructor throws the following error:\r\nE/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: com.example.tflite_java, PID: 4451\r\n    java.lang.UnsatisfiedLinkError: dlopen failed: library \"libtensorflowlite_flex_jni.so\" not found\r\n        at java.lang.Runtime.loadLibrary0(Runtime.java:1087)\r\n        at java.lang.Runtime.loadLibrary0(Runtime.java:1008)\r\n        at java.lang.System.loadLibrary(System.java:1664)\r\n        at org.tensorflow.lite.flex.FlexDelegate.<clinit>(FlexDelegate.java:61)\r\n        at java.lang.Class.classForName(Native Method)\r\n        at java.lang.Class.forName(Class.java:454)\r\n        at java.lang.Class.forName(Class.java:379)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.maybeCreateFlexDelegate(NativeInterpreterWrapper.java:511)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegates(NativeInterpreterWrapper.java:476)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:88)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:66)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:287)\r\n        at com.example.tflite_java.MainActivity.loadModel(MainActivity.java:79)\r\n        at com.example.tflite_java.MainActivity.onCreate(MainActivity.java:115)\r\n        at android.app.Activity.performCreate(Activity.java:8000)\r\n        at android.app.Activity.performCreate(Activity.java:7984)\r\n        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1309)\r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3422)\r\n        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3601)\r\n        at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:85)\r\n        at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:135)\r\n        at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:95)\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2066)\r\n        at android.os.Handler.dispatchMessage(Handler.java:106)\r\n        at android.os.Looper.loop(Looper.java:223)\r\n        at android.app.ActivityThread.main(ActivityThread.java:7656)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:592)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:947)\r\nI/Process: Sending signal. PID: 4451 SIG: 9\r\n\r\n**Describe the expected behavior**\r\nI expected to be able to load and utilize the model, not this error: java.lang.UnsatisfiedLinkError: dlopen failed: library \"libtensorflowlite_flex_jni.so\" not found\r\n\r\n**Standalone code to reproduce the issue**\r\n===========MainActivity.java=============\r\npackage com.example.tflite_java;\r\n\r\nimport android.content.res.AssetFileDescriptor;\r\nimport android.content.res.AssetManager;\r\nimport android.os.Bundle;\r\nimport androidx.appcompat.app.AppCompatActivity;\r\nimport org.tensorflow.lite.Interpreter;\r\nimport java.io.FileInputStream;\r\nimport java.io.IOException;\r\nimport java.nio.ByteBuffer;\r\nimport java.nio.MappedByteBuffer;\r\nimport java.nio.channels.FileChannel;\r\n\r\npublic class MainActivity extends AppCompatActivity {\r\n    // Load tflite model from assets.\r\n    public MappedByteBuffer loadModelFile(String modelPath) throws IOException, IOException {\r\n        try (AssetFileDescriptor fileDescriptor = getAssets().openFd(modelPath);\r\n            FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor())) {\r\n            FileChannel fileChannel = inputStream.getChannel();\r\n            long startOffset = fileDescriptor.getStartOffset();\r\n            long declaredLength = fileDescriptor.getDeclaredLength();\r\n            return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);\r\n        }\r\n    }\r\n\r\n    // Load the tflite model\r\n    public Interpreter loadModel(String modelFile) {\r\n        Interpreter tflite;\r\n        try {\r\n            System.out.println(\"Loading model from file \" + modelFile);\r\n            ByteBuffer buffer = loadModelFile(modelFile);\r\n            Interpreter.Options opt = new Interpreter.Options();\r\n            opt.setNumThreads(1);\r\n            tflite = new Interpreter(buffer, opt);\r\n        } catch (IOException ex) {\r\n            tflite = null;\r\n            System.out.println(ex.getMessage());\r\n        }\r\n        return tflite;\r\n    }\r\n\r\n    @Override\r\n    protected void onCreate(Bundle savedInstanceState) {\r\n        super.onCreate(savedInstanceState);\r\n        Interpreter interpreter = loadModel(\"waterfall_glow_20210203_tfnightly.tflite\");\r\n    }\r\n}\r\n\r\n\r\n**Other info / logs** \r\nAndroid ndk version android-ndk-r18b-darwin-x86_64.zip\r\nAndroid sdk version 30.0.3\r\nBuild commands for tensorflow-lite and tensorflow-lite-select-tf-ops\r\nbazel build --cxxopt='--std=c++14' -c opt --config=android_arm --config=monolithic //tensorflow/lite/java:tensorflow-lite-select-tf-ops\r\nbazel build --cxxopt='--std=c++11' -c opt --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a //tensorflow/lite/java:tensorflow-lite", "comments": ["You need to depend on tensorflow-lite-select-tf-ops.aar for importing libtensorflowlite_flex_jni.so.", "Thank you for the quick response!\r\n\r\nI have the following in my app/build.gradle:\r\n    implementation project(':tensorflow-lite')\r\n    implementation project(':tensorflow-lite-support-0.1.0')\r\n    implementation project(':tensorflow-lite-select-tf-ops')\r\n\r\nAdditionally, I added all three modules to AndroidStudio via \"New\" -> \"New Module\" -> \"JAR/AAR\".", "Hi @dpmcgonigle, is your issue resolved? If so, please close this :)", "Hello @teijeong, my issue is not resolved.  @abattery suggested I need to \"depend on tensorflow-lite-select-tf-ops.aar for importing libtensorflowlite_flex_jni.so\".  I pointed out in my last post that I am including this as a dependency.  Thank you!\r\n\r\nSincerely,\r\nDan McGonigle", "@thaink could you take a look at this?", "@dpmcgonigle  I think the problem here is you only build tensorflow-lite-select-tf-ops.aar for arm:\r\n`bazel build --cxxopt='--std=c++14' -c opt --config=android_arm --config=monolithic //tensorflow/lite/java:tensorflow-lite-select-tf-ops`\r\nbut might be you are trying to run it on emulator, which is x86.", "Thank you very much for getting back to me!  That looks to be the problem.  Thank you very much for helping me out!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46905\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46905\">No</a>\n", "Hello everyone, I need help. When trying to run the sound classification model on android 4.4.2, there is a problem:\r\nE/dalvikvm: dlopen(\"/data/app-lib/......../libtensorflowlite_flex_jni.so\") failed: dlopen failed: cannot locate symbol \"fegetround\" referenced by \"libtensorflowlite_flex_jni.so\"...\r\n\r\non android 10, this problem does not arise, the application starts", "@sanyakuznezov could you upload a new issue? This issue has been closed and we would keep each issue focused.", "ok"]}, {"number": 46904, "title": "Add a MicroPrintf function that is independent of the ErrorReporter.", "body": "Additionally,\r\n* remove the global error reporter from micro_test.h\r\n* change all the kernel tests to make use of MicroPrintf\r\n* add a GetMicroErrorReporter() function that returns a pointer to a singleton MicroErrorReporter object.\r\n  - This enables the current change to not spread beyond the tests.\r\n  - Even if we move large parts of the TFLM code to make use MicroPrintf (in favor of error_reporter), there is still going to be shared TfLite/TFLM code that will need an error_reporter.\r\n    \r\nNext steps, if we want to continue down this path\r\n* remove the error_reporter from the TFLM functions and class implementations and instead use either MicroPrintf or GetMicroErrorReporter()\r\n* Add new APIs that do not have error_reporter to the TFLM classes and functions.\r\n* Ask users to switch to the new error_reporter-free APIs and deprecate the APIs that do make use of the error_reporter.\r\n* Remove the error_reporter APIs completely.\r\n    \r\nPrior to this change, we would have to use the ErrorReporter interface for all the logging. This was problematic on a few fronts:\r\n* The name ErrorReporter was often misleading since sometimes we just want to log, even when there isn't an error.\r\n* For even the simplest logging, we need to have access to an ErrorReporter object which means that pointers to an ErrorReporter are part of most classes in TFLM.\r\n    \r\nWith this change, we can simply call MicroPrintf(), and it will be a no-op if binary size is important.\r\n    \r\n    \r\nProgress towards http://b/158205789\r\n\r\nAs described in #46937, there is some unexpected behavior with the initialization with the targets that we are simulating via Renode. The current PR has a workaround and that issue will be addressed separately.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 46903, "title": "Use the right input_dims[] in lite/micro/kernels/exp_test.cc", "body": "Issue #45415. Bug fix for micro op EXP's test code.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 46902, "title": "Cannot Compile TF-Nightly with CUDA 11.2 and CUDNN 8.1", "body": "**System information**\r\n- Windows 10\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: tf-nightly\r\n- Python version: 3.8.6\r\n- Installed using virtualenv? pip? conda?: Built using instructions [here](https://www.tensorflow.org/install/source_windows)\r\n- Bazel version (if compiling from source): Bazel 3.7.2\r\n- GCC/Compiler version (if compiling from source): MSVC 14.28.29333\r\n- CUDA/cuDNN version: 11.2 / 8.1\r\n- GPU model and memory: Geforce RTX 3090 24GB\r\n\r\nWhen trying to build TF-nightly with CUDA 11.2 and CUDNN 8.1, this error occurs:\r\n`C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.28.29333\\include\\complex(354): error C2039: 'copysign': is not a member of ''global namespace''\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.28.29333\\include\\complex(354): error C3861: 'copysign': identifier not found\r\n`\r\n\r\nThis causes the build to fail.\r\n\r\nSteps to reproduce:\r\n1) Uninstall all global CUDA instances.\r\n2) Install CUDA 11.2\r\n3) Install CUDNN 8.1 by dragging the files into the install directory of CUDA\r\n4) Git clone tensorflow\r\n5) Checkout tf-nightly branch\r\n6) Set up Bazel, following [this](https://docs.bazel.build/versions/master/windows.html#build-c) guide (suggested by documentation)\r\n7) Follow the rest of the guide [here](https://www.tensorflow.org/install/source_windows).\r\n8) Execute bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n9) Receive the error.\r\n\r\nTraceback is attached.\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/5922256/log.txt)\r\n", "comments": ["@ayakashi-ml,\r\nCould you please try building TensorFlow with **CUDA 11.0** and **cuDNN 8** and let us know if you are facing the same error. \r\n\r\nAlso, please take a look at similar issues [#44682](https://github.com/tensorflow/tensorflow/issues/44682#issuecomment-732283634) and [#43588](https://github.com/tensorflow/tensorflow/issues/43588#issuecomment-699592351) and check if it helps. Thanks!", "For what it's worth I ran into this exact same error on Windows with CUDA 11.2 and \"cuDNN v8.1.0 (January 26th, 2021), for CUDA 11.0,11.1 and 11.2\" and found that switching from branch \"master\" to \"r2.4\" lead to a working build and setup (also using the numpy 1.20.x branch).  \r\n\r\nI suspect the problem here is maybe with MSVC - the VisualStudio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29333\\include\\complex header file appears to have a Ctraits double sized tensor copysign definition in it that is missing (more likely a typo) from the associated library. It looks to me like maybe r2.4 does not use and therefore does not expose this defect in this build of the MSVC runtime library.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "The issue is MSVC. https://www.gitmemory.com/issue/OpenImageIO/oiio/2799/752372181", "https://github.com/microsoft/vcpkg/issues/15369", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46902\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46902\">No</a>\n"]}, {"number": 46901, "title": "Fold StridedSliceOp when input is defined by ShapeOp.", "body": "Fixes #46879 and #46080. This PR adds a sub-shape folder from StridedSliceOp.\r\n\r\nFold StridedSliceOp when input is defined by ShapeOp. The pattern is common in TF python library like\r\n\r\n```python\r\nheight = tf.shape(x)[1]\r\nspatial_shape = tf.shape(x)[1:3]\r\n```\r\n\r\nWhen `x` has some dynamic dimensions (typically batch dim), `tf.shape` can not be folded so `height` and `spatial_shape` can not be inferred as a constant even if the corresponding dimensions are static. This PR folds this kind of patterns to improve sub-shape constant folding.\r\n\r\nNote that there is a workaround in python lib to use\r\n\r\n```python\r\nheight = x.shape[1] or tf.shape(x)[1]\r\nspatial_shape = [x.shape[i] or tf.shape(x)[i] for i in (1, 2)]\r\n```\r\n\r\nto make sure constant is propagated. However, most of TensorFlow codes do not use this.\r\n\r\nHi @abattery, would you mind taking a look at this? Thank you!", "comments": ["This change probably not needed. TF uses runtime to const fold some ops. In the issue that required this change the model has dynamic shape which will disable constant folding since the shape is dynamic. Check the issue i replied with more details\r\n\r\nThanks", "> This change probably not needed. TF uses runtime to const fold some ops.\r\n\r\nI don't quite get what you mean here ; can you expand?\r\n", "> > This change probably not needed. TF uses runtime to const fold some ops.\r\n> \r\n> I don't quite get what you mean here ; can you expand?\r\n\r\nSorry for the confusion, i should have been more specific.\r\n\r\nMy reply was regarding the specific issue the user was reporting, that's all. The user was assuming the shape was constant which wasn't true. I updated the issue with more details.\r\nTo be more specific, the issue is not blocked on this change.\r\n\r\nThanks \r\n\r\n", "We have some internal failure on a test-case that looks like:\r\n\r\n```\r\n// configuration: -pass-pipeline='func(canonicalize)' -verify-each\r\nmodule attributes {tf.versions = {bad_consumers = [], min_consumer = 0 : i32, producer = 667 : i32}}  {\r\n  func @main(%arg0: tensor<?x3x3x3x3xf32>, %arg1: tensor<?x3x3x3x3xf32>) -> tensor<i32> attributes {tf.entry_function = {control_outputs = \"\", inputs = \"args_0,args_0_1\", outputs = \"Identity\"}} {\r\n    %11 = \"tf.Const\"() {device = \"\", value = dense<-1> : tensor<1xi32>} : () -> tensor<1xi32>\r\n    %12 = \"tf.Const\"() {device = \"\", value = dense<0> : tensor<1xi32>} : () -> tensor<1xi32>\r\n    %13 = \"tf.Const\"() {device = \"\", value = dense<1> : tensor<1xi32>} : () -> tensor<1xi32>\r\n    %34 = \"tf.Shape\"(%arg0) {device = \"\"} : (tensor<?x3x3x3x3xf32>) -> tensor<5xi32>\r\n    %37 = \"tf.StridedSlice\"(%34, %11, %12, %13) {begin_mask = 0 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 0 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 1 : i64} : (tensor<5xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>\r\n    return %37 : tensor<i32>\r\n  }\r\n}\r\n```\r\n\r\nI haven't investigated too much further, but looking at the TensorFlow kernel implementation, I wonder if it has to do with this code:\r\n```\r\n    if (shrink_i) {\r\n        // If we are shrinking, the end index is now possibly incorrect. In\r\n        // particular foo[-1] produces sparse_begin = -1, sparse_end = 0.\r\n        // and canonical puts these to n-1 and 0, which implies a degenerate\r\n        // interval. Fortunately, it is now safe to re-create end as begin+1.\r\n        int64 x_fwd = begin_i < 0 ? dim_i + begin_i : begin_i;\r\n        begin_i = x_fwd;\r\n        end_i = begin_i + 1;\r\n        if (x_fwd < 0 || x_fwd >= dim_i) {\r\n          return errors::InvalidArgument(\r\n              \"slice index \", begin_i, \" of dimension \", i, \" out of bounds.\");\r\n        }\r\n      } else {\r\n        begin_i = canonical(begin_i, 0);\r\n        end_i = canonical(end_i, 1);\r\n      }\r\n```\r\n", "I've fixed the case that shrink_axis_mask is set and is with begin=-1. Please trigger the tests again. Thank you!\r\n", "Merged in: https://github.com/tensorflow/tensorflow/commit/cf18ba2332fa5cd561803e7425b67c8fd008c44f\r\n\r\nI tweaked the handling of the output type as we had some internal test that involved unranked output.", "This was reverted because of other test failures, here is one example that crashes:\r\n\r\n```\r\nmodule attributes {tf.versions = {bad_consumers = [], min_consumer = 0 : i32, producer = 0 : i32}}  {\r\n  func @main(%arg0: tensor<1x4x4x128xf32>) -> tensor<*xi32> {\r\n    %24 = \"tf.Const\"() {value = dense<0> : tensor<1xi32>} : () -> tensor<1xi32>\r\n    %25 = \"tf.Const\"() {value = dense<1> : tensor<1xi32>} : () -> tensor<1xi32>\r\n    %822 = \"tf.Shape\"(%arg0) {device = \"\"} : (tensor<1x4x4x128xf32>) -> tensor<4xi32>\r\n    %823 = \"tf.StridedSlice\"(%822, %24, %25, %25) {begin_mask = 0 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 0 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 1 : i64} : (tensor<4xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tens>\r\n    return %823 : tensor<*xi32>\r\n  }\r\n}\r\n```\r\n\r\n", "Hi @joker-eph, I updated the PR to use canonicalization pattern instead of folder in order to have better support for unranked and dynamic output; also, add testcases for them. Please take a look again. Thank you!", "> I updated the PR to use canonicalization pattern instead of folder in order to have better support for unranked and dynamic output; also, add testcases for them. Please take a look again. Thank you!\r\n\r\nIt is a bit unfortunate to loose the folding capabilities though. I'll see if I can adjust this internally into a folder instead.", "Sorry for back and forth change. I'm not aware that folder can have cast-compatible result. It should work with unranked/dynamic output with folder now."]}, {"number": 46900, "title": "tf.strings.substr crashes(aborts) ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n\r\n**Describe the current behavior**\r\n`tf.strings.substr` crashes(aborts)  when `len(pos)` > `len(input)`\r\n\r\n**Describe the expected behavior**\r\nexpect an exception message if the input unexpected instead of crash. \r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n~~~python\r\nimport tensorflow as tf\r\ntf.strings.substr(input='abc', len=1, pos=[1,-1])\r\n~~~\r\n\r\n~~~python\r\nimport tensorflow as tf\r\ntf.strings.substr(input='abc', len=1, pos=[1,2])\r\n~~~\r\n\r\n\r\nOutput:\r\n~~~python\r\n2021-02-03 22:46:41.234297: F ./tensorflow/core/framework/tensor.h:806] Check failed: new_num_elements == NumElements() (2 vs. 1)\r\nAborted (core dumped)\r\n~~~", "comments": ["@rmothukuru \r\nI ran the code shared on tf 2.4 and nightly, colab crashes, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/e258b6a130140d89c1fd3325317982bc/untitled520.ipynb).", "According to documentation https://www.tensorflow.org/api_docs/python/tf/strings/substr an error should be thrown out gracefully (instead of a crash). Added a PR #46974 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46900\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46900\">No</a>\n"]}, {"number": 46899, "title": "tf.ragged.range and  tf.range crash (abort) when `start` is large", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n**Describe the current behavior**\r\n`tf.ragged.range` and  `tf.range`  crash (abort) when `start` is large\r\n\r\n**Describe the expected behavior**\r\nexpect an exception message if the input unexpected instead of crash\r\n\r\n**Standalone code to reproduce the issue**\r\n~~~python\r\nimport tensorflow as tf\r\ntf.range(start=-1e+38, limit=1)\r\n~~~\r\nOutput:\r\n~~~python\r\n2021-02-03 22:29:09.074233: F tensorflow/core/framework/tensor_shape.cc:345] Check failed: size >= 0 (-9223372036854775808 vs. 0)\r\nAborted (core dumped)\r\n~~~\r\n\r\n\r\n\r\n\r\n~~~python\r\nimport tensorflow as tf\r\ntf.ragged.range(starts=1e+38)\r\n~~~\r\n\r\nOutput:\r\n~~~python\r\n2021-02-03 22:28:53.789455: F tensorflow/core/framework/tensor_shape.cc:345] Check failed: size >= 0 (-9223372036854775808 vs. 0)\r\nAborted (core dumped)\r\n~~~", "comments": ["I have tried in colab with TF versions 2.1, 2.4,nightly version(`2.5.0-dev20210203`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/9e8373f446483373acd93aee58f4c04f/untitled655.ipynb). Thanks!", "`tf.ragged.range` segfault with the following input\r\n~~~\r\ntf.ragged.range(starts=[-670192367, 1, -1], deltas=-1665872789)\r\n~~~\r\n\r\n", "Was able to reproduce the issue in TF 2.6.0-dev20210528,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/e1d6659d04b9f494a07d390a7402978c/untitled38.ipynb#scrollTo=8N26EJO9nYlS)..Thanks !", "Added a PR #51711 for the fixl.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46899\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46899\">No</a>\n"]}, {"number": 46897, "title": "tf.sparse.segment_sqrt_n/sum crashes(abort) when segment_ids is large uint", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n**Describe the current behavior**\r\n`tf.sparse.segment_sqrt_n` and `tf.sparse.segment_sum` crashes(abort) when `segment_ids` is large `uint`\r\n\r\n\r\n**Describe the expected behavior**\r\nexpect no crash if the input is unexpected instead of crash\r\n\r\n**Standalone code to reproduce the issue**\r\n~~~python\r\ntf.sparse.segment_sqrt_n(data=1.0, indices=[1], segment_ids=np.array([1205444461],dtype=np.uint32))\r\ntf.sparse.segment_sum(data=1, indices=[1], segment_ids=np.array([1205444461], dtype=np.uint32))\r\n~~~\r\n\r\nOutput:\r\n~~~python\r\n2021-02-03 21:19:17.206426: F tensorflow/core/framework/tensor_shape.cc:435] Check failed: d < dims() (0 vs. 0)\r\nAborted (core dumped)\r\n~~~\r\n", "comments": ["@ymodak \r\n\r\nI ran the code shared on tf 2.4 and nightly, colab crashes. please find the [gist here](\r\nhttps://colab.research.google.com/gist/Saduf2019/c2cc7958142f404915ff218a66b6a2ed/untitled520.ipynb)", "Similar issue in `tf.sparse.segment_mean` too\r\n~~~python\r\ntf.sparse.segment_mean(data=1.0, indices=[1], segment_ids=np.array([1205444461],dtype=np.uint32))\r\n~~~\r\n\r\nOutput:\r\n~~~\r\nAborted (core dumped)\r\n~~~", "```\r\n tf.sparse.segment_sum(data=1.0, indices=[0], segment_ids=np.array([3],dtype=np.uint32))\r\n```\r\nAlso crashes.\r\n\r\nBut\r\n```\r\ntf.sparse.segment_sum(data=[1.0], indices=[0], segment_ids=np.array([1205444461],dtype=np.uint32))\r\n```\r\nWorks.\r\n\r\nSo the problem is not caused large id, but it is likely a memory allocation error during the error handling in the eager mode when the shape of data is incorrect. \r\n\r\nThe function does not consume a scalar for the data argument.\r\n```\r\n@tf.function\r\ndef func():\r\n  return tf.sparse.segment_sum(data=1.0, indices=[0], segment_ids=np.array([1205444461],dtype=np.uint32))\r\nfunc()\r\n```\r\nreports the shape error correctly.\r\n\r\n\r\n", "Was able to reproduce the issue in TF v2.5,Please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/4e5b6372c7618b23e2a8686e3d9c25bb/untitled40.ipynb)..Thanks !", "Thanks! I will try to push a fix this week.", "This is fixed with tf-nightly and will be part of TF 2.6 release.\r\nPlease see [gist](https://colab.research.google.com/gist/ymodak/534703da44ac8c48a0a081332db2b014/untitled520.ipynb) for reference. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46897\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46897\">No</a>\n"]}, {"number": 46896, "title": "Custom \"Best\" Metric Not Tracking Best Accuracy", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n-- custom code in which the bug manifests, but test code is slightly edited stock example code from Tensorflow docs.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n-- MacOS and Linux\r\n- TensorFlow installed from (source or binary):\r\n-- via pip\r\n- TensorFlow version (use command below):\r\n-- 2.4.0\r\n- Python version:\r\n-- 3.8.2\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI've created a custom metric which tracks the max achieved result of a metric during a training run (so for example it should reflect the best model's accuracy seen so far at any given point in time). When I apply this to the simple MNIST code from the docs where the accuracy continuously moves up at the epoch level, this custom metric does not exactly match the raw accuracy (though it roughly tracks with it).\r\n\r\n**Describe the expected behavior**\r\n\r\nThe expected behaviour in this case would be for the custom metric to exactly match the accuracy since the accuracy improves monotonically at the epoch level. It seems like it's a bug that it does not. However, I admit that it could be a hole in my understanding of Tensorflow! Perhaps there's some kind of off-by-one error here in how I'm interpreting the metric's state maybe?\r\n\r\nAnother comment which may or may not be related: In working with multiple class instances of tensorflow metric objects in jupyter notebooks, I noticed that sometimes I had to be very liberal with my usage of reset_state even though I had separate class instances of the same metric. This makes me expect that at the root of both this and the bug I outline in this issue, that there's some kind of unintentionally shared state between multiple instances of the same metric class. If I can reproduce this someday again, I'll post in separate issue. I just wanted to include full context in case this provides clues that tie to other posted issues in tf's github repo.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nreproducible in this collab notebook: https://colab.research.google.com/drive/1hBcLce9VWynZgTAbzhWzEfdO3VZq4-0W?usp=sharing\r\n", "comments": ["I have tried in colab with TF version 2.4,nightly version(`2.5.0-dev20210203`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/938d0fd01de962220cd1f48710b26b72/untitled654.ipynb). Thanks!", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nThanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46896\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46896\">No</a>\n", "@ymodak it appears to be a bug to me. I could definitely be misunderstanding something which might make it not a bug, However, TF2 has had many corner case bugs so I wouldn't be surprised if it is a bug."]}, {"number": 46895, "title": "Finish porting micro op zeros_like and its testing code", "body": "PR4 for Issue #46049", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 46894, "title": "Automatically treat dataclasses as pytrees", "body": "This change enables dataclasses to be used in pytrees. The relevant tests are in jax: https://github.com/google/jax/pull/5618.\r\n\r\nFixes google/jax#2371", "comments": ["@gbaned @thomasjoerg hi, is there something I can do to move this along? thx", "Any update on this?", "@NathanHowell Can you please resolve conflicts? Thanks!", "@gbaned per google/jax#2371 it seems like this work is not desired. unless you have newer information I believe this PR can be closed.", "@NathanHowell Thank you for the confirmation. "]}, {"number": 46893, "title": "Add missing exception to Model.fit() docstring", "body": "The shuffle argument to the fit() method in the Model class gets ignored\r\nif the input x to the method is a tf.data.Dataset object. But this isn't\r\ndocumented in the docstring of the method.\r\n\r\nSigned-off-by: Suraj Upadhyay <usuraj35@gmail.com>\r\n\r\nCloses #46492", "comments": []}, {"number": 46892, "title": "[ROCm]  Update to use ROCm 4.0.1 (when building TF with --config=rocm) ", "body": "/cc @cheshire @chsigg ", "comments": []}, {"number": 46891, "title": "tf.transpose crashes(abort) if `a` is complex", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n**Describe the current behavior**\r\n`tf.transpose` crashes(abort) if `a` is complex and `conjugate`=True\r\n\r\n\r\n**Describe the expected behavior**\r\nexpect no crash\r\n\r\n**Standalone code to reproduce the issue**\r\n~~~python\r\nimport tensorflow as tf\r\ntf.transpose(conjugate=True, a=complex(1))\r\n~~~\r\n\r\nOutput:\r\n~~~python\r\n2021-02-03 17:58:05.565680: F ./tensorflow/core/kernels/transpose_functor.h:169] Check failed: in.dims() >= 2 (0 vs. 2)\r\nAborted (core dumped)\r\n~~~", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/3beaf907cf0caf938eefcab8d92def67/46891.ipynb). Thanks!", "Added PR #46973 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46891\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46891\">No</a>\n"]}, {"number": 46890, "title": "tf.image.resize/resize_with_crop_or_pad/pad_to_bounding_box/extract_glimpse crash(abort)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n\r\n**Describe the current behavior**\r\nThe following APIs crash(abortion) when the given size is large\r\n- tf.image.resiz\r\n- tf.image.resize_with_crop_or_pad\r\n- tf.image.pad_to_bounding_box\r\n- tf.image.extract_glimpse\r\n- `tf.keras.backend.resize_images`\r\n\r\n**Describe the expected behavior**\r\nexpect exception messages if the input is not expected instead of crash\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n### `tf.image.resize`\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.image.resize(images=np.ones((5,5,5)), size=[2065374891,1145309325])\r\n~~~\r\nOutput:\r\n~~~python\r\n2021-02-03 17:41:13.484992: F tensorflow/core/framework/tensor_shape.cc:353] Check failed: 0 <= new_num_elements (0 vs. -6619278462293758741)\r\nAborted (core dumped)\r\n~~~\r\n\r\n### `tf.image.resize_with_crop_or_pad`\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.image.resize_with_crop_or_pad(image=np.ones((1,1,1)), target_height=5191549470, target_width=5191549470)\r\n~~~\r\nOutput:\r\n~~~python\r\n2021-02-03 17:42:15.468265: F tensorflow/core/framework/tensor_shape.cc:353] Check failed: 0 <= new_num_elements (0 vs. -1)\r\nAborted (core dumped)\r\n~~~\r\n\r\n### `tf.image.pad_to_bounding_box`\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.image.pad_to_bounding_box(image=np.ones((1,1,1)), target_height=5191549470, target_width=5191549470, offset_height=1, offset_width=1)\r\n~~~\r\nOutput\r\n~~~python\r\n2021-02-03 17:42:52.556583: F tensorflow/core/framework/tensor_shape.cc:353] Check failed: 0 <= new_num_elements (0 vs. -1)\r\nAborted (core dumped)\r\n~~~\r\n\r\n### `tf.image.extract_glimpse`\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.image.extract_glimpse(input=np.ones((5,5,5,5)), size=[1574700351, 451745106], offsets=np.ones((5,2)))\r\n~~~\r\n\r\nOutput:\r\n~~~python\r\n2021-02-03 17:43:30.140277: F tensorflow/core/framework/tensor_shape.cc:338] Check failed: 0 <= n (0 vs. -662664649191246466)\r\nAborted (core dumped)\r\n~~~\r\n\r\n### `tf.keras.backend.resize_image`\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.keras.backend.resize_images(x=np.ones((1,5,3,15)), height_factor=5628955348197345288, width_factor=5628955348197345288, data_format='channels_last')\r\n~~~\r\n\r\nOutput:\r\n~~~python\r\n2021-02-03 17:54:01.192819: F tensorflow/core/framework/tensor_shape.cc:353] Check failed: 0 <= new_num_elements (0 vs. -5948468124908472256)\r\nAborted (core dumped)\r\n~~~", "comments": ["@rmothukuru \r\nI ran the code on tf 2.4 and nightly, colab crashes. please find the [gist here](\r\nhttps://colab.research.google.com/gist/Saduf2019/fd4dfbdc07480e95a5694b336944c4f8/untitled520.ipynb)", "BTW, I also find it in `tf.image.crop_and_resize` and `tf.image.resize_with_pad`\r\n\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.image.crop_and_resize(image=np.ones((1,1,1,1)), boxes=np.ones((11,4)), box_indices=np.ones((11)), crop_size=[2065374891,1145309325])\r\n~~~\r\n\r\nOutput:\r\n~~~python\r\n2021-02-05 17:02:57.884394: F tensorflow/core/framework/tensor_shape.cc:187] Non-OK-status: InitDims(dim_sizes) status: Internal: Encountered overflow when multiplying 22719123801 with 1145309325, result: -1\r\nAborted (core dumped)\r\n~~~\r\n\r\n\r\n\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.image.resize_with_pad(image=np.ones((5,5,5)), target_height=1635057735, target_width=1635057735)\r\n~~~\r\n\r\nOutput:\r\n~~~python\r\n2021-02-19 22:28:03.322414: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2021-02-19 22:28:03.332536: F tensorflow/core/framework/tensor_shape.cc:353] Check failed: 0 <= new_num_elements (0 vs. -5079675089792900491)\r\nAborted (core dumped)\r\n~~~", "Was able to reproduce the issue in TF 2.6.0-dev20210528 & colab crashes ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/2f0930249cfea6fd3be6c8f9ec4fca21/untitled41.ipynb#scrollTo=vkKKum0ltdoP)..Thanks !", "Update:\r\n1. `tf.image.resize` fixed aleady\r\n2. `tf.image.resize_with_crop_or_pad` to be fixed (PR #51717)\r\n3. `tf.image.pad_to_bounding_box` to be fixed (PR #51717)\r\n4. `tf.keras.backend.resize_image` fixed already\r\n5. `tf.image.crop_and_resize` to be fixed (PR #51732)\r\n5. `tf.image.resize_with_pad` fixed already", "@DNXie Could you please let us know if we can closed the issue with this [PR](https://github.com/tensorflow/tensorflow/pull/51732) ?Thank you!", "There are still a few PRs that need to land here.", "I think all of these landed", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46890\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46890\">No</a>\n"]}, {"number": 46889, "title": "tf.keras.backend.arange crash (abort) when start is large", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n- \r\n**Describe the current behavior**\r\n`tf.keras.backend.arange` crash (abort) when `start` is large\r\n**Describe the expected behavior**\r\nexpect no crash\r\n\r\n**Standalone code to reproduce the issue**\r\n~~~python\r\nimport tensorflow as tf\r\ntf.keras.backend.arange(start=1e+38)\r\n~~~\r\n\r\nOutput:\r\n~~~python\r\n2021-02-03 16:53:49.181545: F tensorflow/core/framework/tensor_shape.cc:187] Non-OK-status: InitDims(dim_sizes) status: Internal: Expected shape dimensions to be non-negative, got -9223372036854775808\r\nAborted (core dumped)\r\n~~~", "comments": ["I have tried in colab with TF version 2.4, nightly version(`2.5.0-dev20210203`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/4a94ece739a4907b5af0542ca440e6aa/untitled653.ipynb). Thanks!", "Was able to reproduce the issue in TF 2.6.0-dev20210528,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/6a229922be4724e77733241763e081fc/untitled42.ipynb)..Thanks !", "The issue will be fixed by PR #51711.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46889\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46889\">No</a>\n"]}, {"number": 46888, "title": "tf.math.segment_max/min/mean/sun/prod crashes(aborts) when segment_ids is large", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n\r\n**Describe the current behavior**\r\ntf.math.segment_max/min/mean/sun/prod crashes(aborts) when `segment_ids` is large\r\n\r\n**Describe the expected behavior**\r\nexpect an exception message if the input is unexpected instead of crash\r\n\r\n**Standalone code to reproduce the issue**\r\n~~~python\r\ntf.math.segment_max(data=np.ones((1,10,1)), segment_ids=[1676240524292489355])\r\ntf.math.segment_min(data=np.ones((1,10,1)), segment_ids=[1676240524292489355])\r\ntf.math.segment_mean(data=np.ones((1,10,1)), segment_ids=[1676240524292489355])\r\ntf.math.segment_sum(data=np.ones((1,10,1)), segment_ids=[1676240524292489355])\r\ntf.math.segment_prod(data=np.ones((1,10,1)), segment_ids=[1676240524292489355])\r\n~~~\r\n\r\nOutput:\r\n~~~python\r\n2021-02-03 16:44:25.849065: F tensorflow/core/framework/tensor_shape.cc:405] Check failed: 0 <= new_num_elements (0 vs. -1684338830784658056)\r\nAborted (core dumped)\r\n~~~\r\n\r\nRelated issue: #46696", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/26149c9c28de86ef49b179c9ce6425a0/46888.ipynb). Thanks!", "Was able to reproduce this issue in TF 2.6.0-dev20210528,please find the gist [here ](https://colab.research.google.com/gist/sushreebarsa/380145e341cb9341b9af8743ad359a46/untitled43.ipynb#scrollTo=fX4HwL-Fxk0D)..Thanks !", "Added a PR #51733 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46888\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46888\">No</a>\n"]}, {"number": 46887, "title": "`tf.math.floormod` crashes(floating point exception) when x is large", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n\r\n\r\n**Describe the current behavior**\r\n`tf.math.floormod` crashes(floating point exception) when `x` is large and `y` is negative\r\n\r\n**Describe the expected behavior**\r\nexpect no crash\r\n\r\n**Standalone code to reproduce the issue**\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.math.floormod(x=-9223372036854775808, y=[-1])\r\n~~~\r\n\r\nOutput:\r\n~~~python\r\nFloating point exception (core dumped)\r\n~~~", "comments": ["@ymodak \r\nI ran the code shared on tf 2.4 and nightly, colab crashes. please find the[ gist here](\r\nhttps://colab.research.google.com/gist/Saduf2019/8b6855e46cc1c3f3acc3e8ea5c609f67/untitled520.ipynb)", "BTW, API `tf.math.floordiv` has similar crash\r\n\r\nReproducing code:\r\n~~~python\r\nimport tensorflow as tf\r\ntf.math.floordiv(x=-9223372036854775808, y=[-1])\r\n~~~\r\n\r\nError:\r\n~~~python\r\nFloating point exception (core dumped)\r\n~~~", "This is an FPE in c++, causing TF to crash.  (same with truncatediv, truncatemod, floordiv).  Fix coming soon.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46887\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46887\">No</a>\n"]}, {"number": 46886, "title": "[ROCm] Workaround for a LLVM crash when doing codegen for MLIR generated Cast kernel", "body": "When you enable (uncomment) the `gen_kernel_library` rule for `cast` in `tensorflow/core/kernels/mlir_generated/BUILD`, the build would fail on the ROCm platform with the following error\r\n\r\n```\r\nLLVM ERROR: Cannot select: 0x56134e3c5b10: i1 = fp_to_sint 0x56134d1f53d8\r\n  0x56134d1f53d8: f16 = bitcast 0x56134e3c58a0\r\n    0x56134e3c58a0: i16,ch = load<(load 2 from %ir.lsr.iv)> 0x56134eaa3788, 0x56134e3c5698, undef:i64\r\n      0x56134e3c5698: i64,ch = CopyFromReg 0x56134eaa3788, Register:i64 %16\r\n        0x56134d41d620: i64 = Register %16\r\n      0x56134d1f5850: i64 = undef\r\nIn function: Cast_f16_i1_kernel\r\nTensorFlow crashed, please file a bug on https://github.com/tensorflow/tensorflow/issues with the trace below.\r\nStack dump:\r\n0.\tProgram arguments: bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel --unroll_factors=4 --tile_sizes=256 --arch=gfx803,gfx900,gfx906,gfx908 --input=bazel-out/k8-opt/bin/tensorflow/core/kernels/mlir_generated/cast_f16_i1.mlir --output=bazel-out/k8-opt/bin/tensorflow/core/kernels/mlir_generated/cast_f16_i1_kernel_generator_kernel.o --enable_ftz=False\r\n1.\t2.\tRunning pass 'CallGraph Pass Manager' on module 'acme'.\r\n3.\tRunning pass 'AMDGPU DAG->DAG Pattern Instruction Selection' on function '@Cast_f16_i1_kernel'\r\n...\r\n...\r\n```\r\n\r\nJack (@whchung) identified the root cause of this crash as lack of support for FPTOSI (fp16 to i1) instruction in the AMDGPU LLVM backend. The correct fix for this bug, is to add support for the same in the AMDGPU LLVM backend. Jack is in the process of upstreaming that fix to the LLVM repo.\r\n\r\nIn the meantime (i.e. until the TF LLVM commit pointer is updated to point to a commit that includes Jack's fix), we need to workaround this on the TF side, by adding a pass that converts the `fptosi f16 to i1` op to `fptosi f16 to i16` + `trunci i16 to i1`, which is what this commit does.\r\n\r\n-----------------------------\r\n\r\n\r\n/cc @whchung @akuegel @cheshire @chsigg ", "comments": []}, {"number": 46885, "title": "Tensorflow lite return zero in the third running time.", "body": "hello everyone, I build a tensorflow lite 1.15.0(1.15.5) c++ lib for android.\r\nIn the first and second time, the  interpreter output is normal. but after it, interpreter return all zero.\r\nI'm accept the camera video. \r\n![image](https://user-images.githubusercontent.com/29395068/106755884-9efe5780-6669-11eb-9274-25554774c53a.png)\r\n![image](https://user-images.githubusercontent.com/29395068/106755895-a160b180-6669-11eb-8386-947e606e723f.png)\r\n![image](https://user-images.githubusercontent.com/29395068/106755907-a45ba200-6669-11eb-8bd7-e3cf3667c5d6.png)\r\n\r\nthere is my code:\r\nint init()\r\n{\r\n    std::unique_ptr<tflite::FlatBufferModel> model;\r\n    model = tflite::FlatBufferModel::BuildFromBuffer(model_buffer,model_size);\r\n    tflite::ops::builtin::BuiltinOpResolver resolver;\r\n    tflite::InterpreterBuilder(*model, resolver)(&interpreter_);\r\n    if (interpreter_->AllocateTensors() != kTfLiteOk) {\r\n        return -1;\r\n    }\r\n    else\r\n    {\r\n        input_ = interpreter_->inputs()[0];\r\n        return 0;\r\n    }\r\n}\r\n\r\nint detect(android_camera_frame)\r\n{\r\n               /*\r\n                  process image\r\n             */\r\n             memcpy(interpreter_->typed_tensor<float>(input_),detect_mat.data,256 * 256 * 3 * sizeof(float));\r\n            TfLiteTensor *predict_tensor = interpreter_->tensor(interpreter_->outputs()[0]);\r\n            float *detect_out_data = predict_tensor->data.f;\r\n\r\n            for(int i = 0; i<20;i++)\r\n            {\r\n                printf(\"detect_out_data(%d):%f\\n\",i,detect_out_data[i]);\r\n            }\r\n}\r\n\r\n**init** is a initialization function,only run once. and the frame of android camera is process by **detect** always.\r\nIn the result only first and second return is true;after that, result is all zero or nan....\r\n\r\nthere is my compile option:\r\nbazel build //tensorflow/lite:libtensorflowlite.so --crosstool_top=//external:android/crosstool --cpu=arm64-v8a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=\"-std=c++11\"\r\n\r\ntensorflow vision is 1.15.0 and 1.15.5\r\n", "comments": ["TF 1.15 is too old TF version to get supported. We encourage you to use TensorFlow Lite 2.4 or the tf-nightly version for this.  Could you try your code with the recent TensorFlow version?", "@abattery \r\nthank you. I have compiled lite in tensorflow2.4 and tensorflow-master today.\r\nbut all that get another issue;\r\nwhen I link lite lib in my project, get\r\n\r\nCMakeFiles/xinyan_homework.dir/src/xinyan_homework_detect.cpp.o: In function `xinyan_homework::CXinYanHomeworkDetect::init(char const*, int)':\r\n/Users/admin/Desktop/work/AliveFaceDetc/xinyan_homework/native/src/xinyan_homework_detect.cpp:48: undefined reference to `tflite::InterpreterBuilder::operator()(std::unique_ptr<tflite::Interpreter, std::default_delete<tflite::Interpreter> >*)'\r\n\r\nthis is my code:\r\n    \r\n    std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromBuffer(model_buffer,model_size);\r\n    tflite::ops::builtin::BuiltinOpResolver resolver;\r\n    tflite::InterpreterBuilder builder(*model, resolver);\r\n    \r\n    builder(&interpreter_);   //this code is wrong\r\n    if (interpreter_->AllocateTensors() != kTfLiteOk) {\r\n        return XINYAN_HOMEWORK_ERROR_NULL;\r\n    }\r\n    else\r\n    {\r\n        input_ = interpreter_->inputs()[0];\r\n        return XINYAN_HOMEWORK_SUCCESS;\r\n    }\r\n\r\nwhen I delete error code----\"builder(&interpreter_)\". complie is success.\r\n\r\nmy NDK version is 18.1.5063045.NDK_API_LEVEL is 21.ANDROID_BUILD_TOOLS_VERSION=30.0.3  ANDROID_SDK_API_LEVEL=30\r\nand my build command is\uff1a\r\nbazel build --config=android --cpu=arm64-v8a //tensorflow/lite:libtensorflowlite.so --crosstool_top=//external:android/crosstool  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=\"-std=c++11\"", "Hi @abattery, can you comment?", "Adding @terryheo for building issues.", "You'd better build it with \"--config=android_arm64\"\r\n\r\nAlso if you're using CMake for your project, you can integrate it with TFLite directly.\r\nhttps://www.tensorflow.org/lite/guide/build_cmake", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46885\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46885\">No</a>\n"]}, {"number": 46884, "title": "Update nn_impl.py", "body": "Removed redundant name argument.\r\n\r\nFixes change suggested in Issue #40592", "comments": []}, {"number": 46883, "title": "give list of images instead of directory to \"flow_from_directory\" in keras image data generator.", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.2.0\r\n- Are you willing to contribute it (Yes/No): be an honor to.\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.** now, we just can pass directories of train, test and validation to the flow_from_directory for building the generator in keras. i want to give a list of image pathes with their label (like two lists) to build the generator.\r\n\r\n**Will this change the current api? How?** no.\r\n\r\n**Who will benefit with this feature?** me\r\n\r\n**Any Other info.** thanks\r\n", "comments": ["This provision already exists.There are two ways you can achieve this:\r\n1. tf.keras.preprocessing.image_dataset_from_directory() function . (docs [here](https://keras.io/api/preprocessing/image/)) here you can pass the list/tuple in  `labels` argument. This generates a `tf.data.Dataset`, which is explained below.\r\n\r\n 2. Tensorflow offers `tf.data` module (docs [here](https://www.tensorflow.org/api_docs/python/tf/data)) ,which consists of `tf.data.Dataset`. It can be used in any way that suits you. It can be optimized by usage of `@tf.function` decorator. Building such input pipelines is demonstrated in [tutorial](https://www.tensorflow.org/guide/data) ,[tutorial](https://www.tensorflow.org/guide/data_performance). Please check out these links.\r\n\r\nIf you are new to TensorFlow, I recommend going for option 1. I personally use option 2 because of its performance, and I recommend you use it, too. \r\nHope this helps.", "@imohamadhoseins,\r\nPlease take a look at @AdityaKane2001's comment and let us know if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46883\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46883\">No</a>\n"]}, {"number": 46882, "title": "No UInt8 support for logistic operator in TFLu", "body": "@tensorflow/micro\r\n\r\n**Describe the problem**\r\nI am working on running object detection demo with TFLu + ssd mobilenet v1 quantized model on i.mx8mp's Cortex-M7 core. The input tensor and output tensor's type for logistic operator is UInt8. But there is no UInt8 support in current code.\r\n\r\nI create the following patch to fix the problem. Please give me some advice.\r\nhttps://github.com/tensorflow/tensorflow/pull/46873\r\n\r\n", "comments": ["Hi man. I am working on the same project. Can you share your experience with me ? I dont know where should i start.", "@furknclk There are some examples for TFLu in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples. You can try them first.", "@furknclk I have solved the problem. This is my patch https://github.com/tensorflow/tensorflow/pull/46873/commits. It has not been merged yet. Anyway, you can try it.", "@alisonwh,\r\n\r\nI can see from this [PR](https://github.com/tensorflow/tensorflow/pull/46873/commits) that it is not merged and was awaiting your response, As you mentioned in previous comment that its solved, Can you confirm if we are good to close the issue? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46882\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46882\">No</a>\n"]}, {"number": 46881, "title": "TFLite C API: add cmake support", "body": "TFLite have cmake build support now, which is great. but we still need TFLite C API for prebuilt binary library workflow. This PR add cmake support for TFLite C API.", "comments": []}, {"number": 46880, "title": "micro: port operator FLOOR_DIV kernel from lite with test", "body": "Complete implementation of TFLM operator FLOOR_DIV and associated TFLM test code.\r\n\r\nThis represents PR step 5 of the work to port operator FLOOR_DIV as tracked in Issue #45657", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@ddavis-2015  Can you please resolve conflicts? Thanks!", "@ddavis-2015 This PR is in draft, any update on this? Please. Thanks!", "@ddavis-2015 This PR is in draft, any update on this? Please. Thanks!"]}]