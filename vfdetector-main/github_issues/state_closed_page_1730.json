[{"number": 999, "title": "ValueError when using tf.nn.moments with axes=[1,2,3]", "body": "mean, var = tf.nn.moments(images, [1, 2, 3])\n\n```\n/home/cesar/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/nn.pyc in moments(x, axes, name)\n    540     # The caller should have a fallback plan, however: this tensor may not be\n    541     # available if this function implementation changes.\n--> 542     x_centered = math_ops.sub(x, mean, name=\"x_centered\")\n    543     var = math_ops.mul(math_ops.reduce_sum(math_ops.square(x_centered), axes),\n    544                        divisor, name=\"variance\")\n\n/home/cesar/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.pyc in sub(x, y, name)\n   1398     A `Tensor`. Has the same type as `x`.\n   1399   \"\"\"\n-> 1400   return _op_def_lib.apply_op(\"Sub\", x=x, y=y, name=name)\n   1401 \n   1402 \n\n/home/cesar/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)\n    653         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n    654                          input_types=input_types, attrs=attr_protos,\n--> 655                          op_def=op_def)\n    656         outputs = op.outputs\n    657         return _Restructure(ops.convert_n_to_tensor(outputs), output_structure)\n\n/home/cesar/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes)\n   2046                     original_op=self._default_original_op, op_def=op_def)\n   2047     if compute_shapes:\n-> 2048       set_shapes_for_outputs(ret)\n   2049     self._add_op(ret)\n   2050     self._record_op_seen_by_control_dependencies(ret)\n\n/home/cesar/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in set_shapes_for_outputs(op)\n   1526       raise RuntimeError(\"No shape function registered for standard op: %s\"\n   1527                          % op.type)\n-> 1528   shapes = shape_func(op)\n   1529   if len(op.outputs) != len(shapes):\n   1530     raise RuntimeError(\n\n/home/cesar/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.pyc in _BroadcastShape(op)\n   1279     else:\n   1280       raise ValueError(\"Incompatible shapes for broadcasting: %s and %s\"\n-> 1281                        % (shape_x, shape_y))\n   1282   return [tensor_shape.TensorShape(return_dims)]\n   1283 \n\nValueError: Incompatible shapes for broadcasting: (128, 32, 32, 3) and (128,)\n```\n", "comments": ["One way to prevent that from happening would be to add keep_dims=True to reduce_sum in line 539 of tensorflow/python/ops/nn.py:\n\n``` python\n539: mean = math_ops.mul(math_ops.reduce_sum(x, axes, keep_dims=True), divisor, name=\"mean\")\n```\n\nThen we would also need to squeeze the dimensions when returning the mean:\n\n``` python\n546: return tf.squeeze(mean), var\n```\n\nAs a side note: it would be nice to add keep_dims to nn.moments arguments. We would then also need to add keep_dims=keep_dims to math_ops.reduce_sum in line 544:\n\n``` python\n544: var = math_ops.mul(math_ops.reduce_sum(math_ops.square(x_centered), axes, keep_dims=keep_dims),\n                        divisor, name=\"variance\")\n```\n\nand we would need to squeeze the returned mean depending on whether the keep_dims argument passed to nn.moments is true or false.\n\nThoughts?\n", "Confirmed. Thanks!\n", "Fix should propagate shortly.\n"]}, {"number": 998, "title": "Script for Jenkins parameterized matrix build", "body": "The script takes a few environment variables as input and invoke\nci_build.sh in the same directory in the proper way. It takes\ninformation about the build type (cpu | gpu | android), Python version,\nwhether \"-c opt\" is used and whether PIP install-test is to be\nperformed. It automatically examines the parameters to determine the\ncorrect build script to call and the argument flags to use with it. It\nalso examines whether Docker is available on the system to determine\nwhether the build will be done inside Docker or not. If GPU Docker build\nis detectd, it'll map the CUDA devices and libraries automatically.\nIn addition, it looks at some optional environment variables to set\nadditional command flags for Docker and the build command.\n\nSee experimental Jenkins matrix builds at: \nFor CPU (Linux and Mac): http://ci.tensorflow.org/view/Experimental/job/experimental-cais-matrix-linux-cpu-test/8/\nFor GPU (Linux): http://ci.tensorflow.org/view/Experimental/job/experimental-cais-matrix-linux-gpu-test/\nFor Android: http://ci.tensorflow.org/view/Experimental/job/experimental-cais-matrix-linux-android-test/\n", "comments": ["Can one of the admins verify this patch?\n", "I like this. @jendap?\n", "I addressed comments from @jendap. In addition, I realized that there was an issue with BUILD_TAG values from matrix builds. Those tags have special characters such as \",\" and \"=\", which make the Docker image names illegal. I modified ci_build.sh a little to solve that problem. \n", "@martinwicke Please hold on merging this one. There are some issues related to Docker runs for parameterized builds. I'm looking into it. I'll let you know when it's okay to merge. \n", "Ok\nOn Thu, Feb 4, 2016 at 20:29 caisq notifications@github.com wrote:\n\n> @martinwicke https://github.com/martinwicke Please hold on merging this\n> one. There are some issues related to Docker runs for parameterized builds.\n> I'm looking into it. I'll let you know when it's okay to merge.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/998#issuecomment-180188710\n> .\n", "@martinwicke @jendap Feel free to merge now. The issue on Jenkins matrix build is now resolved. The change set is now squashed and ready. \n", "I merged this, apparently five hours ago.\n"]}, {"number": 997, "title": "Python3 tests: getting easy ones out of the way", "body": "Adding PY2AND3 label for a Python test file. Getting rid some obvious Python 2-3 compatibility issues that cause test errors, including comparing lists with integers, string-byte array difference, and use of xrange. After this CL, there are still 11 tests that fail in Python3. Those are more involved issues and will be addressed separately. \n", "comments": ["Jenkins, test this please\n", "Using six.moves.xrange now. Commits squashed. \n", "Jenkins, test this please.\n", "merged\n"]}, {"number": 996, "title": "Make python_check relocatable", "body": "Uses $location in the genrule and a path that's relative to the\nscript to find expected files so that this works both as a local\nrule and if tensorflow is used as a remote repository.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks!  Minor comments.\n", "Jenkins, test this please.\n", "How do I run it on Jenkins?\n", "Can you rebase on master & squash commits?\n", "@ebrevdo: there's only one commit, and the branch has no conflicts with master.  We can probably just accept as is.\n", "Don't worry, our script can do that.\n", "merged.\n"]}, {"number": 995, "title": "Added a link to some tutorial examples.", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "I'm adding a section to the website in #1232, and pointing to it from the README. There's a number of them I still have to add, and I'd like to keep the README for resources contained inside the repo.\n", "I didn't realize the website source was in here too -- that's definitely a better place for it.  I'll close this.\n"]}, {"number": 994, "title": "Classification model for Imagenet classification tutorial", "body": "Hi!\n\nI'm an engineer looking to work with Tensorflow for computer vision applications. My team was looking into the Imagenet classification tutorial provided via the Tensorflow website and the github files at /models/image/imagenet and have become a little stumped. There's no information on how the classification model was built (like their was for the MNIST tutorials), and there is no information on the graph that has been used for in the classify_image.py file, aside from a convoluted binary .pb file.\n\nIs there any more information on how to define and train the Inception model (or any other classifier) with the Imagenet training set? Or how to create the graph? Or, has this information been made unavailable to the public?\n\nThanks\n\nOren \n\nps: sorry for writing this in an issue\u2014was having trouble trying to find a better place to put it.\n", "comments": ["We are in the process of publishing the trainable version of this, but we need some time to untangle it from our infrastructure.\n"]}, {"number": 993, "title": "How to run nvidia-docker with TensorFlow GPU docker", "body": "Thanks for looking at my issue, really appreciate it. \n### What I've Done\n\nI have setup an equivalent of a Nvidia DIGITS machine (running Ubuntu 14.04 server), and am attempting to run everything in docker containers.\n1. I have docker installed, and have run `nvidia-docker run nvidia/cuda nvidia-smi` described [here](https://github.com/NVIDIA/nvidia-docker), and I see my 4 TitanX graphic cards. \n2. I have also run the nvidia-docker-plugin described [here](https://github.com/NVIDIA/nvidia-docker/wiki/Using-nvidia-docker-plugin) as `sudo -u nvidia-docker nvidia-docker-plugin -s /var/lib/nvidia-docker` and I get the output:\n\n```\nnvidia-docker-plugin | 2016/02/04 12:54:02 Loading NVIDIA management library\nnvidia-docker-plugin | 2016/02/04 12:54:04 Loading NVIDIA unified memory\nnvidia-docker-plugin | 2016/02/04 12:54:04 Discovering GPU devices\nnvidia-docker-plugin | 2016/02/04 12:54:05 Provisioning volumes at /var/lib/nvidia-docker/volumes\nnvidia-docker-plugin | 2016/02/04 12:54:05 Serving plugin API at /var/lib/nvidia-docker\nnvidia-docker-plugin | 2016/02/04 12:54:05 Serving remote API at localhost:3476\n```\n\nwhich signifies to me that it's working.\n1. I ran the tests [here](https://github.com/NVIDIA/nvidia-docker/wiki/Testing-the-samples) and they all passed. \n### My Problem\n1. When I try to run the [TensorFlow GPU docker image](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/docker) using nvidia-docker\n\nI first run `sudo -u nvidia-docker nvidia-docker-plugin -s /var/lib/nvidia-docker` in a tmux session. \n\nThen I run `nvidia-docker run -it -p 8888:8888 b.gcr.io/tensorflow/tensorflow-devel-gpu` it downloads everything and runs the docker container. Next I run ipython and try to import tensorflow but I get the following errors:\n\n```\nIn [1]: import tensorflow as tf\nI tensorflow/stream_executor/dso_loader.cc:92] LD_LIBRARY_PATH: /usr/local/cuda/lib64\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:121] hostname: 16b84b6e71f9\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:146] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:257] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  352.79  Wed Jan 13 16:17:53 PST 2016\nGCC version:  gcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04)\n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel reported version is: 352.79\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1054] LD_LIBRARY_PATH: /usr/local/cuda/lib64\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1055] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so; dlerror: libcuda.so: cannot open shared object file: No such file or directory\n```\n\n**I think I just have a lack in understanding about how I should run the TensorFlow container, or maybe I have to build the container using nvidia-docker. \n\nAny ideas about how to do this, or general advice about what I'm doing wrong would be amazing. **\n\nThanks so much.\n\nBrad\n", "comments": ["Does this help? https://github.com/tensorflow/tensorflow/issues/808#issuecomment-178854998\n", "For future people stuck on this, the solution that worked for me is documented here:\n\nhttps://github.com/NVIDIA/nvidia-docker/issues/45\n\nThanks for the response @caisq!\n"]}, {"number": 992, "title": "Fix typos in strings and comments of core/kernels.", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thank you.\n"]}, {"number": 991, "title": "Made the fixed point code compile when AVX2 instructions ar enabled", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, test this please.\n", "your set of commits seems to have a protobuf change in it -- you might want to remove that since it looks like it's causing build failures?\n", "@vrv: I don't see a protobuf change in the diffs.In fact the files that I changed don't have any protobufs.\n", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Jenkins, test this please.\n", "@benoitsteiner can you resolve the conflict and rebase?  Looks good otherwise.\n", "When you resolve the conflict, please omit the \"external/eigen_archive/\" part of the paths, a change to bazel makes this the preferred way to referencing those files, so they can be used from targets outside tensorflow.\n", "I have resolved the merge conflicts and rebased. \n", "@martinwicke I'm hitting this now, how does an external build reference these files?\n", "@peterbraden, are you asking about the include paths? What are you trying to do?\n\nThese are changes made necessary in order to allow tensorflow to be included in another bazel project (for example, as a submodule). \n", "@martinwicke I'm trying to reference tensorflow from a non-bazel project (I'm trying to create nodejs bindings to tensorflow). Because of the references to the bazel-downloaded files, I can't successfully link. \n", "If you built tensorflow with bazel (just to get all the downloads), the\ndownloads will be symlinked in `tensorflow/bazel-tensorflow/external...`,\nand the built libraries will be symlinked in\n`tensorflow/bazel-out/external/...`.\n\nIf you need to fiddle with what bazel links in, you should probably ask\nover at bazel, they're more likely to know the internals.\n\nThere's an effort in PR728 to make a cmake build, which I'd love to accept\nto contrib. That may also solve your problem, at least partially.\n\nOn Wed, Feb 10, 2016 at 12:10 AM Peter Braden notifications@github.com\nwrote:\n\n> @martinwicke https://github.com/martinwicke I'm trying to reference\n> tensorflow from a non-bazel project (I'm trying to create nodejs bindings\n> to tensorflow). Because of the references to the bazel-downloaded files, I\n> can't successfully link.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/991#issuecomment-182247599\n> .\n"]}, {"number": 990, "title": "GPU device not found on Ubuntu 14.04", "body": "I have installed tensorflow python3 version using virtual environment on Ubuntu 14.04. There are three GPUs (Tesla K20c) available on the server that i work on.\n\nOn the tensorflow webpage it says: \"In general you do not have to specify CPUs or GPUs explicitly. TensorFlow uses your first GPU, if you have one, for as many operations as possible.\"\n\nWhen i checked the examples provided on Using GPUs about using multi-GPUs \n\nimport tensorflow as tf\n# Creates a graph.\n\nc = []\nfor d in ['/gpu:1', '/gpu:2']:\n  with tf.device(d):\n    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3])\n    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2])\n    c.append(tf.matmul(a, b))\nwith tf.device('/cpu:0'):\n  sum = tf.add_n(c)\n# Creates a session with log_device_placement set to True.\n\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n# Runs the op.\n\nprint(sess.run(sum))\n\nthe following error was displayed:\n\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library                                    libcublas.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library                                    libcudnn.so.6.5 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library                                    libcufft.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:93] Couldn't open CUDA library libcud                                   a.so. LD_LIBRARY_PATH: :/usr/local/cuda/lib64\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:121] hostname: dmlserver\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:146] libcuda reported vers                                   ion is: Not found: was unable to find libcuda.so DSO loaded into this program\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:257] driver version file c                                   ontents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  346.96  Sun Aug 23 2                                   2:29:21 PDT 2015\nGCC version:  gcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04)\n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel reported versi                                   on is: 346.96\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1060] LD_LIBRARY_PATH: :/                                   usr/local/cuda/lib64\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1061] failed to find libc                                   uda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so; dl                                   error: libcuda.so: cannot open shared object file: No such file or directory\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library                                    libcurand.so.7.0 locally\nI tensorflow/core/common_runtime/local_device.cc:40] Local device intra op paral                                   lelism threads: 16\nE tensorflow/stream_executor/cuda/cuda_driver.cc:481] failed call to cuInit: CUD                                   A_ERROR_NO_DEVICE\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:114] retrieving CUDA diagn                                   ostic information for host: dmlserver\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:121] hostname: dmlserver\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:146] libcuda reported vers                                   ion is: Not found: was unable to find libcuda.so DSO loaded into this program\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:257] driver version file c                                   ontents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  346.96  Sun Aug 23 2                                   2:29:21 PDT 2015\nGCC version:  gcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04)\n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel reported versi                                   on is: 346.96\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:127] DMA:\nI tensorflow/core/common_runtime/direct_session.cc:58] Direct session inter op p                                   arallelism threads: 16\nDevice mapping: no known devices.\nI tensorflow/core/common_runtime/direct_session.cc:134] Device mapping:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.4/site-packages/tensorflow/python/client/session.py\", l                                   ine 428, in _do_run\n    target_list)\ntensorflow.python.pywrap_tensorflow.StatusNotOK: Invalid argument: Cannot assign                                    a device to node 'Const_3': Could not satisfy explicit device specification '/g                                   pu:2'\n         [[Node: Const_3 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape:                                    [3,2] values: 1 2 3...>, _device=\"/gpu:2\"]()]]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"checking_2.py\", line 15, in <module>\n    print(sess.run(sum))\n  File \"/usr/lib/python3.4/site-packages/tensorflow/python/client/session.py\", l                                   ine 368, in run\n    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n  File \"/usr/lib/python3.4/site-packages/tensorflow/python/client/session.py\", l                                   ine 444, in _do_run\n    e.code)\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device                                    to node 'Const_3': Could not satisfy explicit device specification '/gpu:2'\n         [[Node: Const_3 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape:                                    [3,2] values: 1 2 3...>, _device=\"/gpu:2\"]()]]\nCaused by op 'Const_3', defined at:\n  File \"checking_2.py\", line 8, in <module>\n    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2])\n  File \"/usr/lib/python3.4/site-packages/tensorflow/python/ops/constant_op.py\",                                    line 165, in constant\n    attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\n  File \"/usr/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", li                                   ne 1834, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", li                                   ne 1043, in __init__\n    self._traceback = _extract_stack()\n\nDon't understand what the problem is? Need URGENT help!!! \n", "comments": ["See my comments at: https://github.com/tensorflow/tensorflow/issues/808#issuecomment-178854998\nThe comment is for GPU + Docker + TensorFlow. But the first three steps should apply to GPU + TensorFlow as well.\n", "@caisq  i followed the first three steps in the comments you referred me to and ended up with this:\n./deviceQuery Starting...\n\n CUDA Device Query (Runtime API) version (CUDART static linking)\n\ncudaGetDeviceCount returned 35\n-> CUDA driver version is insufficient for CUDA runtime version\nResult = FAIL\n\nwhat to do next?\n", "@kibtes Did you try \"sudo ./deviceQuery\"? \n", "@caisq  yes this is the result of it \nroot@dmlserver:/home/dmlserver/NVIDIA_CUDA-7.0_Samples/bin/x86_64/linux/release# sudo ./deviceQuery\n./deviceQuery Starting...\n\n CUDA Device Query (Runtime API) version (CUDART static linking)\n\ncudaGetDeviceCount returned 35\n-> CUDA driver version is insufficient for CUDA runtime version\nResult = FAIL\n", "@kibtes  As the error message says \"-> CUDA driver version is insufficient for CUDA runtime version\", you need to check the version of your NVIDIA driver.\n\nsudo apt --installed list | grep nvidia\n\nMost likely you'll need to uninstall the current one, and install a later version. I ran int a similar situation before. I uninstalled nvidia-340 and installed nvidia-352. \n", "Closing since it looks like it's a system environment issue.\n", "123", "\u600e\u4e48\u89e3\u51b3\uff0c\u6ca1\u627e\u5230\u65b9\u6cd5"]}, {"number": 989, "title": "Update 1_notmnist.ipynb", "body": "If download fails, force delete on file by default\n", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Can one of the admins verify this patch?\n", "I signed it!\n", "Hi @tensorflow-jenkins . The problem is that wrong downloaded file persist on memory disk.  \n", "tested and it works\n", "You have to sign the CLA (see the instructions above to make sure our bot knows about it) before we can look at this.  Closing for now but we'll reopen once the CLA is sorted out.\n"]}, {"number": 988, "title": "If download fails, the file downloaded should be deleted", "body": "", "comments": ["Inside this example\n\n[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/1_notmnist.ipynb](url)\n", "i made a pull request with file modification\n", "CLA didn't make it through somehow?\n"]}, {"number": 987, "title": "fix a typo", "body": "I fixed a typo.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "can you rebase / squash the commits? it seems to have become conflated with another commit\n", "Can one of the admins verify this patch?\n"]}, {"number": 986, "title": "Fix 952", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I think all these commits are in HEAD -- are you sure this is still needed?\n", "See #981 \n"]}, {"number": 985, "title": "Wrong file dimension when calling maybe_download function", "body": "Inside 1_notmnist.ipynb file maybe_download function is called twice and fails twice because of a wrong file dimension. For notMNIST_large.tar.gz file right dimension should be 74250249, and for notMNIST_small.tar.gz file  right dimension should be 6381568. PS: working on mac\n", "comments": ["I found that is a download problem. so i close the issue\n"]}, {"number": 984, "title": "Shuffling letters before assigning them to training and validation datasets", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Sounds like a good idea. Can you:\n- replace \"lets\" with \"let's\"\n- move the call to np.random.seed(133) to the top of the file, so that the seed is fixed for everything\n- add a:\n  valid_dataset, valid_labels = randomize(valid_dataset, valid_labels) where the other calls are\n\nI think that would take care of all randomization and reproducibility issues.\n", "Sure, I just do not get what do you mean by the third bullet point \"add a: valid_dataset, valid_labels = randomize(valid_dataset, valid_labels) where the other calls are\"\n", "We don't appear to be re-randomizing the validation set here:\ntrain_dataset, train_labels = randomize(train_dataset, train_labels)\ntest_dataset, test_labels = randomize(test_dataset, test_labels)\n", "I see. Should be OK now\n", "Merged. Thanks!\n"]}, {"number": 983, "title": "Silenced some bogus compilation warning generated by nvcc", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins: test this please\n", "Can one of the admins verify this patch?\n", "Merged\n"]}, {"number": 982, "title": "Print `tags` instead of `tag` since `tag` doesn't exist in funtion `Run`.", "body": "Otherwise, in order to print exception info for each tag,\nwe also are able to declare local variable `tag`, and\nset it with the value of `t` in for statement.\n", "comments": ["Can one of the admins verify this patch?\n", "LGTM, please rebase :+1: \n", "Can one of the admins verify this patch?\n", "Thank you, @vrv, @danmane and @martinwicke . I'll rebase it.\n", "Thank you too! :) \n", "Actually, I've taken it upon myself to re-write this script today so it will work a bit more cleanly, and serialize some new endpoints we're adding to the backend. Since this is such a small change, do you mind if I just add this directly into the new version of the script? Lower friction than going through the pull request. \n", "Sure, @danmane . After doing that, please just close this one. :-)\nThank you.\n", "Thanks @dongjoon-hyun. Closing.\n", "Hi, @martinwicke . \nI cannot find @danmane 's new code until now.\nIs it still internal? I thought you guys close my PR after merging yours.\nAnyway, I'm looking forward to seeing the rewritten code soon. :-)\n", "I'm sorry, I got all excited and went on a issue/pr closing spree. It should get here soon.\n", "Oh, never mind. I'm just a happy user. :)\n"]}, {"number": 981, "title": "Add FillFunctor definition for bool", "body": "", "comments": ["@caisq can you test this for GPU?\n", "I'll squash if it works.\n", "merged.\n"]}, {"number": 980, "title": "Fix directory_watcher_test on mac.  Fixes #964", "body": "Keeps track of seek position and seeks to current byte offset prior to reading\nfrom the file.  Apparently this is necessary on Mac, because once you've read EOF,\nyou need to seek to your current position to read new data that has since been\nappended to the file.\n", "comments": ["hm, failed on linux.  well this is going to be a fun one to fix.\n"]}, {"number": 979, "title": "Adding build info print-out for Jenkins Description Setter", "body": "This is addressing issue https://github.com/tensorflow/tensorflow/issues/736\n\nThe print-out JSON object line can be used by Jenkins Description Setter to extract and display information about the build, including source version, platform and build tools. See example at: \n\nhttp://ci.tensorflow.org/view/Experimental/job/experimental-cais-tensorflow-cpu-python27-copt_pip_install-test/29/\n", "comments": ["Can one of the admins verify this patch?\n", "Can you add the bazel --local_resources setting? I'm not sure how to get it out of bazel, but since we set it, we should know anyway. Actually I'm not sure whether we do set it. Only add it if we set it.\n", "Added container type and command, which covers @martinwicke's comment about local_resources.\n", "Jenkins, test this please. \n", "Can one of the admins verify this patch?\n", "Oops. Test this please.\n", "Can you squash?\n", "Squashed and merged.\n"]}, {"number": 978, "title": "Register ZerosLikeOp for all number types and bool", "body": "Fixes #952\n", "comments": ["This wasn't ready. It is breaking the GPU build. \n", "I'll send a rollback\n", "I know it looked ready. :/ sorry.\n", "Reverted\n"]}, {"number": 977, "title": "Word2Vec.py ReluGrad is not finite", "body": "Running the tutorial as downloaded from tensorflow.org results in an exception stating that the ReluGrad is not finite.  This happens with the learning rate set to .00001 and concurrent steps set to 1.  Once this exception is thrown, learning stops.  It happens at epoch 0, roughly step 500,000.\n", "comments": ["This seems to have fallen through the cracks.  @a-dai: Did you get a chance to glance at this?  @bugulnoz: Is it still an issue?\n", "Automatically closing because there was no response. Please reopen if it is still an issue.\n"]}, {"number": 976, "title": "parse_single_example() should give dense array for VarLenFeature", "body": "On master, the signature of parse_single_example has changed. When testing I noticed something unexpected; a tf.VarLenFeature results in a SparseTensor instead of a dense one.\n\n```\nfeatures = tf.parse_single_example(\n  serialized_example,\n  features = {\n    'data': tf.VarLenFeature(tf.float32),\n  }\n)\n```\n\n`features['data']` is now a SparseTensor. I don't see really why, shouldn't this be a (standard dense) Tensor instead?\n", "comments": ["If you then want to pass this data through, e.g., a FIFOQueue, so you can then perform minibatch processing, you will have different shaped tensors going in and as a result will not be able to dequeue_many.\n\nInstead, you want to serialize these 1D sparse tensors via sparse_ops.serialize_sparse, and feed the resulting string into the FIFOQueue.  Then after a dequeue_many, you'll use sparse_ops.deserialize_many_sparse to get back a 2D SparseTensor with the dimensions [batch index, variable depth].\n", "In other words, works as intended.\n", "I can't seem to find the ops you are mentioning in the documentation, but I'm sure what you are mentioning is a valid use case.\n\nMy use case is a bit different though. My code looks at the moment something like this:\n\n```\nFEA_DIM=13\nf = parse_single_example(\n  se,\n  features={\n    'input': tf.VarLenFeature(tf.float32),\n    'output': tf.VarLenFeature(tf.int64),\n  }\n)\n\nx = tf.sparse_tensor_to_dense(features['input'])\ny = tf.sparse_tensor_to_dense(features['output'])\n\nx = tf.reshape(x, [-1,FEA_DIM])\ny = tf.reshape(y, [-1,1])\n\nreturn tf.train.shuffle_batch([x,y], enqueue_many=True)\n```\n\nSo I use a single TfRecord to hold multiple x-y pairs.\u00b9 For me it looks wrong that data that is stored densely (as a protobuf float_list), would come to a sparse array first, when I am going to transform it right back to a dense array. Note that I can't use tf.FixedLenSequenceFeature as I have no clue how many examples there will be in one record (not even a maximum).\n\n\u00b9 In reality this is timeseries (speech) data and I do some striding before the features to the queue. This is why there are multiple pairs in one example, it saves space as I don't have to store the context on disk for every x-y separately. \n", "Aside from the bad naming, tf.FixedLenSequenceFeature is exactly what you want.  The \"FixedLen\" in your case is 13, but the first dimension of SequenceFeatures is considered unknown.  Store your data in SequenceExample proto (which is meant for data like speech), use parse_single_sequence_example, and pull out FixedLenSequenceFeature(float32, FEA_DIM),  and FixedLenSequenceFeature(int64, 1).  This will immediately return to you matrices of the shape [-1, FEA_DIM] and [-1, 1].\n\nAlternatively you can just use tf.reshape(x.values, [-1, FEA_DIM]) and tf.reshape(y.values, [-1, 1])  since you can access the values vectors directly from a SparseTensor via property values.  But this won't scale if you start having more complicated features (e.g. 2D per time step, variable-length per time step).\n\nThe former solution\n", "... the former solution will allow you to declare the more complex input formats and is meant for speech and other sequential data of unknown # of frames.\n", "Thanks! I have now changed my code to use the SequenceFeatures with parse_single_sequence_example and it works like a charm! \n", "Glad to find this discussion because I am learning to use tensorflow to deal with speech data, and never saw any documentation on parse_single_sequence_example and SequenceExample proto. Seems TF hides lots of good stuff undernearth :). Can you please provide an example code to use these on speech data? Thanks a lot.\n", "@jinghuangzhu  for anyone intersted in using sequence example, here is a good example(for variable length image caption/ids)\r\nhttps://github.com/tensorflow/models/blob/master/im2txt/im2txt/data/build_mscoco_data.py\r\nhttps://github.com/tensorflow/models/blob/master/im2txt/im2txt/ops/inputs.py\r\nhttps://github.com/tensorflow/models/blob/master/im2txt/im2txt/show_and_tell_model.py\r\nGreate demo code here\r\nhttp://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/", "@ebrevdo  and @psmit \r\n\r\nI am trying to parse SequenceExamples where my context feature is an image and the features are the bounding box locations for the objects in the image.\r\n\r\nI am not able to dequeue the data.\r\n**Here is how I create the Example:-**\r\n\r\ncontext = tf.train.Features(feature={'image_raw':self._bytes_feature(image)})\r\n\t\t\tfeature_lists = tf.train.FeatureLists(feature_list={'locations':self._float_feature_list(list(locs[:,0]))})\r\n\r\n\t\t\texample = tf.train.SequenceExample(context=context,feature_lists=feature_lists)\r\n\r\n**Here is how I am enqueing:-**\r\n\r\ndef load_from_tfRecord(self,filename_queue):\r\n\t\t\r\n\t\treader = tf.TFRecordReader()\r\n\t\t_, serialized_example = reader.read(filename_queue)\r\n\r\n\t\tcontext,sequence = tf.parse_single_sequence_example(\r\n\t\t\t\t\tserialized_example,\r\n\t\t\t\t\tcontext_features={'image_raw':tf.FixedLenFeature([], tf.string)},\r\n\t\t\t\t\tsequence_features={'locations':tf.FixedLenSequenceFeature([], dtype=tf.float32)}\r\n\t\t\t\t\t)\r\n\t\t\r\n\t\timage = tf.decode_raw(context['image_raw'], tf.uint8)\r\n\t\t#locations = tf.decode_raw(features['locations'], tf.int32)\r\n\r\n\t\timage_shape = tf.stack([1,self.ip_width,self.ip_height,3])\r\n\t\timage = tf.cast(tf.reshape(image,image_shape),tf.float32)\r\n\r\n\t\t# loc_shape = tf.stack([n_locs,4])\r\n\t\t# locations = tf.reshape(locations,loc_shape)\r\n\r\n\t\tlocations = sequence['locations']\r\n\t\t\r\n\r\n\r\n\t\tbatch_images,batch_locations = tf.train.shuffle_batch([image,locations]\r\n\t\t\t,enqueue_many=True,batch_size=self.batch_size,num_threads=1,capacity=1000,min_after_dequeue=500)\r\n\r\n\t\treturn batch_images,batch_locations\r\n\r\n**Here is how I am dequeuing:-**\r\n\r\nprint \"Loading Trained Model\"\r\n\t\tself.session.run(tf.local_variables_initializer())\r\n\r\n\t\t#self.session.run(tf.group(tf.global_variables_initializer(),tf.local_variables_initializer()))\r\n\r\n\t\tcoord = tf.train.Coordinator()\r\n\t\tthreads = tf.train.start_queue_runners(sess=self.session,coord=coord)\r\n\r\n\t\ttry:\r\n\t\t\twhile not coord.should_stop():\r\n\t\t\t\tbatch_imgs,batch_labels = self.session.run([self.images,self.labels])\r\n\t\t\t\tprint batch_imgs.shape,batch_labels.shape\r\n\t\texcept tf.errors.OutOfRangeError:\r\n\t\t\tprint \"Done training -- epoch limit reached\"\r\n\t\tfinally:\r\n\t\t\tcoord.request_stop()\r\n\r\n\t\tcoord.join(threads)\r\n\r\n\r\n\r\n", "Yes it does.\n\nOn Mon, Dec 25, 2017, 10:13 PM allen <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> dose\n> tf.parse_single_sequence_example works with new input pipeline api dataset ?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/976#issuecomment-353923781>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim4rVH2s5O9gdRfGkE3NXc8gFYC-tks5tEI6SgaJpZM4HSWQT>\n> .\n>\n"]}, {"number": 975, "title": "the time about gpu", "body": "I have installed the gpu version tf. I'm running a cnn with input size 256_256 and epoch size 32_32, but the speed is very slow,  gpu is 980ti\n\n```\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00GiB\nI tensorflow/core/common_runtime/direct_session.cc:45] Direct session inter op parallelism threads: 8\n2016-02-03 18:13:09.461207: step 0, loss = 250.84 (0.2 examples/sec; 144.089 sec/batch)\n2016-02-03 18:13:59.022429: step 10, loss = 248.88 (7.7 examples/sec; 4.167 sec/batch)\n2016-02-03 18:14:40.752574: step 20, loss = 246.82 (7.5 examples/sec; 4.274 sec/batch)\n2016-02-03 18:15:24.129283: step 30, loss = 244.89 (7.5 examples/sec; 4.277 sec/batch)\n```\n\nthe first time running the speed is fast, but after this the speed is slow. What's the reason?\nHow could I identify the program is running on a gpu or cpu?\n", "comments": ["This seems a more appropriate issue for tensorflow-discuss...\n\nThe most likely cause of this is that you are maybe calling session.run()  on an op and not fetching any tensor results?\n\nThis has an interesting behavior where all the computation is enqueued on the GPU device, but if you don't ask to copy any results back from the GPU the call to run() can return immediately. \n\nThe second time you call session.run(), enqueueing the GPU ops ends up waiting for the previous work to finish (for some reason).  The steady-state throughput is limited to 7.7/second, but the first measurement is misleading.\n\nYou can verify this hypothesis by either fetching back a small result tensor at the end of each step, or by adding a check_numerics op at the end of the step (which is always executed synchronously).\n", "See https://github.com/tensorflow/tensorflow/issues/838 for some debugging techniques. If GPU is found, it'll place everything it can on GPU. However some operations don't have GPU implementation and this greedy placement technique can be suboptimal due to data transfers (making GPU run 20 times slower than manual placement in the issue I linked)\n"]}, {"number": 974, "title": "Fix wrong variable name (pickle_name) in 1_notmnist.ipynb", "body": "When raise Exception at line 327, 'pickle_file' is not predefined and raise error.\n\n`\nNameError: global name 'pickle_file' is not defined\n`\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for pointing it out. I've actually fixed it upstream and the fix should propagate soon. (The tree wasn't in a state I could simply pull a PR at the time.)\n"]}, {"number": 973, "title": "Fix wrong variable name (pickle_name) in 1_notmnist.ipynb", "body": "When raise Exception at line 327, 'pickle_file' at line 328 is not predefined and raise error.\n\nNameError: global name 'pickle_file' is not defined\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n"]}, {"number": 972, "title": "Added missing PY2AND3 build annotation", "body": "Without this line pip package cannot be built in python3, similar to #901 \n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it~\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins: test this please\n", "Can one of the admins verify this patch?\n", "@tensorflow-jenkins: test this please\n", "Merged\n"]}, {"number": 971, "title": "Expose TensorFlow build details", "body": "Requesting addition of API method(s) to enable programmatic checking of TensorFlow version and other build capabilities (such as if TF was compiled with GPU support, for eg.). I would find this useful in deployment scripts that operate in foreign TensorFlow environments.\n", "comments": ["From the email thread: what we have so far is `tf.__version__` and the various GraphDef version numbers (`tf.GRAPH_DEF_VERSION` and such).  GPU support and exact commit would be useful.\n", "> > > import tensorflow as tf\n> > > tf.**version**\n> > > Traceback (most recent call last):\n> > >   File \"<stdin>\", line 1, in <module>\n> > > AttributeError: 'module' object has no attribute '**version**'\n\nAm I on an old version without the version feature or something?\n\nedit: indeed that seem to be the case. upgraded to 0.6.0 now and everything works well\n", "Editing title to reflect the focus.\n", "@martinwicke: Would you prefer contributions welcome or assigned to you? \n", "Do we plan to add more build details or this can be closed?", "I think whether GPU support is included is the only major thing not available that we might want to add.", "This is already possible, see e.g.\r\nhttps://stackoverflow.com/questions/38559755/how-to-get-current-available-gpus-in-tensorflow\r\nClosing this issue for now.\r\n"]}, {"number": 970, "title": "Work nvidia-docker rather than using custom docker start script", "body": "Right now the docs recommend using a [custom start command](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/docker#running-the-container) ([or here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#docker-installation)) rather than using the [`nvidia-docker` program published by nvidia](https://github.com/NVIDIA/nvidia-docker). I suggest having the image work with the more standardized approach to starting docker containers with nvidia support.\n\nSee: https://github.com/tensorflow/tensorflow/issues/808#issuecomment-175934093.\n", "comments": ["I agree that getting the container started with GPU support still isn't the smoothest experience, especially when it comes to matching library versions. That said, I feel like \"now go read the nvidia wiki and install this extra package\" has its own tradeoffs as a getting started experience.\n\nProbably the thing to do here is clear up the instructions:\n1. make sure to clearly route people to either `docker_run_gpu.sh` or the NVidia docker plugin, and\n2. add a few diagnostic steps (eg \"here's how you run `deviceQuery` ...\") for troubleshooting.\n\n@ebrevdo do you have time to take a look at this?\n", "I would not rush it. I have spend a couple of hours with nvidia-docker two weeks ago. I have tried to use it for ci.tensorflow.org. It was not fun. I would not recommend that at this time. It would, imho, make more harm than good.\n\nSimply try to install latest stable drivers. Check you have /dev/nvidia0, /dev/nvidiactl and /dev/nvidia-uvm outside docker and that you have /usr/lib/x86_64-linux-gnu/libcudart.so and /usr/lib/x86_64-linux-gnu/libcuda.so. If so the docker_run_gpu.sh script should work.\n\nBTW: I believe docker and cuda together will get better. But it is not going to be a great experience any time soon if ever.\n", "I would be curious to know more about what were the issues that you had with `nvidia-docker` that took a few hours. The program \"just worked for me\". The one caveat I would add is they rewrote the program ~ Jan 1 and also changed the base CUDA docker images (same images that TF extends for GPU). I think there are issues if you mix and match old images and new `nvidia-docker` or vice versa.\n\nOtherwise, using new versions of both, my experience has been great. I just run `nvidia-docker run` and it sets the volumes and devices.\n\nAlso the `docker_run_gpu.sh` does not seem to work for example when the CUDA SO's are not found in `/usr/lib/x86_64-linux-gnu/` which is the case for one of the boxes my company has (they are instead only in `/usr/lib64/`).\n", "I rebuilt my TF image against the updated nvidia base images. `nvidia-docker` still does not work out of the box. Running this within the container seems to work: `ln -s /usr/local/nvidia/lib64/libcuda.so.1 /usr/lib/x86_64-linux-gnu/libcuda.so`\n", "I mentioned it [here](https://github.com/tensorflow/tensorflow/issues/808/#issuecomment-180228698),  this is an issue with Tensorflow not loading the libcuda soname.\n", "/CC @zheng-xq @vrv \n", "I think @3XX0 is on the right track here -- in fact, at some point long ago @ebrevdo and I hit something like this and worked around it by creating that same symlink. Eugene, did we work around that at some point, or just leave the symlink intact and forget?\n\nIn any event, it seems like an easy thing to add in the container?\n", "+1\n(side note, @craigcitro , did you see flx42's comment https://github.com/NVIDIA/nvidia-docker/issues/45#issuecomment-185444991, might want to rebuild anyway with the refreshed images to get the security fix for glibc)\n", "nvidia-docker is a good idea but not working too well. I was trying it today again. It fails with no error message. Going without it there are at least errors to solve and get to a working state. I will report some bugs to nvidia-docker once I have time for it.\n\nThe 0.7 release had some issues so there will soon be a bugfix 0.7.1. It will go also with upgraded cuda and cudnn https://github.com/tensorflow/tensorflow/pull/1166.\n\n@cancan101 How does it work with the symlink? nvidia-docker is working for some people but it require to create that symlink? Inside of the container? If so we can just add it to the container tomorrow :-)\n\nBTW: Security? No big deal for something like this image. If anybody care about security they run apt-get update & upgrade any now and again anyway.\n", "Interesting. For whatever reason yet another try the same way and this time nvidia-docker worked! Nice. Sort of... The example nnvidia-smi run well, but tensorflow tests have failed!!!\n\nIt did not seem te require any link inside nor outside the container.\n", "@jendap:\n\n```\nnvidia-docker run -v /tmp/cifar10_data:/tmp/cifar10_data -it 4catalyzer/tensorflow:latest-devel-gpu-aws bash -c 'ln -s /usr/local/nvidia/lib64/libcuda.so.1 /usr/lib/x86_64-linux-gnu/libcuda.so && python /usr/local/lib/python2.7/dist-packages/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py'\n```\n", "That's not official container...\n\nWe will make sure the 0.7.1 will run on gpu. I'm happy to put the symlink inside the container. On the other hand the next development container (after https://github.com/tensorflow/tensorflow/pull/1166) seems to be fine without the link. But I have tried that after upgrading the base containers and stuff.\n", "Sure that is not official though I did build it off the Dockerfile from yesterday.\n", "Adding a symlink inside the container is a workaround, those symlinks should only be there for development purposes. Also, if the CUDA ABI breaks you will run into troubles.\n\nI'm pretty sure the only change required is adding `.1` [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/dso_loader.cc#L72).\nIt's already done correctly for the other CUDA libraries I guess you just missed this one.\n", "Yep, an oversight. I'll send a fix.\n", "Maybe. I can try it for open source build as well as internally.\n\nI'm not sure if the link is needed anymore but I'm going to put it into 0.7.1 release just to avoid problems.\n", "Perfect! Thanks @vrv!\n", ".1 was added in https://github.com/tensorflow/tensorflow/commit/d0a822fbcb04d95a643d8efe65699a8d1cdce98b\n", "I just tested and there is still a small issue if you use the development Docker image.\nThe devel image [overrides LD_LIBRARY_PATH](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel-gpu#L99) therefore you need to run `ldconfig` before running tensorflow.\n\nNote that this line is not needed because we already setup the library paths in our CUDA images: https://github.com/NVIDIA/nvidia-docker/blob/master/centos-7/cuda/7.5/runtime/Dockerfile#L29-L37\nSo removing it should do the trick!\n\nThe Tensorflow \"runtime\" image is not affected.\n", "That makes sense. Thank you @3XX0!\n\nNote: If you want to work on tensorflow you may take a look at [ci_build](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/ci_build).\n", "Sorry to bump this issue, but the new 7.1 runtime Docker image fails to find cuDNN probably because it hasn't been configured properly. See @flx42 [comment](https://github.com/tensorflow/tensorflow/issues/808#issuecomment-187931361) \n", "I'd like to second @3XX0 's bump, this issue still persists in the current `b.gcr.io/tensorflow/tensorflow:latest-gpu` image.\n\n``` terminal\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:99] Couldn't open CUDA library libcudnn.so. LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:\nI tensorflow/stream_executor/cuda/cuda_dnn.cc:1407] Unable to load cuDNN DSO\n...\nF tensorflow/stream_executor/cuda/cuda_dnn.cc:204] could not find cudnnCreate in cudnn DSO; dlerror: /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: cudnnCreate\n[I 16:47:11.347 NotebookApp] KernelRestarter: restarting kernel (1/5)\n```\n\nSurprising that this has gone untended for so long.\n", "@craigcitro, @jendap: If nvidia-docker is not ideal, should we close this issue?\n", "@girving actually, it's the recommended method now for GPU containers:\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/docker#running-the-container\n", "@flx42: Even better!  Does that mean this bug is fixed? \n", "Looks like, but that's not my call :)\n", "yeah, it is recommended only in that one file, because I was just updating that file a few days back. We should update all the places. I'm thinking about doing it tomorrow - so that it makes it into 0.9.\n", "What commit was this fixed in?\n\nIt looks like it was never merged to the `r0.9` branch: https://github.com/tensorflow/tensorflow/pull/2878#issuecomment-236296693\n"]}]