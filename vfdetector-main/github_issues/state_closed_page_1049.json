[{"number": 21825, "title": "TensorFlow lite android example simply does not sync or build.", "body": "", "comments": ["@gjsalot : Can you give more information, which example and how are you trying to build it?", "Please reopen with more details."]}, {"number": 21824, "title": "Improve correctness of auto_parallel", "body": "This PR improves the correctness of current auto_parallel in grappler.\r\n\r\nThe problem of current auto_parallel can be summarized as below.\r\n**First, each replica updates a variable independently.** This computation result could be different with a single device, which computes gradients for large batch size and updates a variable once if you use complex optimizers like momentum optimizer.  \r\n**Second, the result of gradient clipping is changed.** In the current version, auto_parallel aggregates the input of variable update operators(e.g., ApplyGradientDescent) so \"clipping->aggregation\" is the current order if clipping is used. However, \u201caggregation->clipping\u201d is the right order to make the same result as a single-device job.\r\n\r\nThis PR changes to aggregate gradients from auto differentiation of trainable variables.\r\nThe aggregations are defined explicitly instead of implicit aggregation through update operators. \r\n\r\n**[Current auto_parallel]**\r\n![current_auto_parallel](https://user-images.githubusercontent.com/2465713/40906054-a946024c-681a-11e8-85af-b3d2db7803e5.jpg)\r\n\r\n**[auto_parallel in this PR]**\r\n![auto_parallel_after_pr](https://user-images.githubusercontent.com/2465713/40886079-ac008a9c-676c-11e8-9397-bcf7f96513f0.jpg)\r\n\r\n**Changes in this PR**\r\n1. Assumptions\r\nCurrent: variables are updated by optimizer operations\r\nThis PR : all the gradients are made by tf.gradients, the gradients of trainable variables are the target of aggregation in synchronous training\r\n\r\n2. Correct gradient clipping(e.g. tf.clip_by_global_norm, tf.clip_by_value)\r\nRefer figures above.\r\n\r\n3. The number of updates\r\nCurrent : as many as the number of replicas(refer above)\r\nThis PR : one (it can also improve performance on multi-machine environment)", "comments": ["Nagging Reviewer @rmlarsen, @ezhulenev: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 164 days with no activity and the `awaiting review` label has been applied."]}, {"number": 21823, "title": "add op to tf_op_files.txt", "body": "1. add op to `tensorflow/contrib/makefile/tf_op_files.txt` to fix problem of op not registered\r\n2. download `farmhash` and `highwayhash` to fix `*.h` and `*.cc` not found problem", "comments": ["Sorry for the slow review response! We're beginning to deprecate the makefile build of TensorFlow in favor of TF Lite, but this looks like a reasonable change to include even so, thanks.", "@petewarden Has fixed conflict. Please approve it again."]}, {"number": 21822, "title": "Add support of CTC float64", "body": "Signed-off-by: Andrii Prymostka <andrey.primostka@gmail.com>", "comments": ["Does this help for long sequence lengths?", "For N=20000 (with code from #4193 ) plot looks like:\r\n![](https://user-images.githubusercontent.com/9934266/44534930-90be5a00-a701-11e8-9843-3c4f3a06edbb.png)\r\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "Sorry for the delay. You may need to rebase on head?", "Commit can be rebased without any merge.\r\nActually in my repo it is now on top of https://github.com/tensorflow/tensorflow/commit/ee51015a293662f502588d4f08b64a492cf7d800", "Hi,\r\nany news on this?\r\nThis patch would really solve some of my problems. ", "Nagging Reviewer @ebrevdo: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 164 days with no activity and the `awaiting review` label has been applied.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!", "I am interested in merging this request and answering to comments but not in rebasing changes because of lack of knowledge of current codebase.", "@aprimostka thank you , please open a new PR with this changes so that you can avoid rebasing."]}, {"number": 21821, "title": "[Intel MKL] Intel Optimizations for TensorFlow* 1.10.0 released.", "body": "Updating README.md with TensorFlow 1.10.0 links for Intel(R) MKL-DNN.\r\n\r\n@gunan and @drpngx Can you take a look at this?", "comments": []}, {"number": 21820, "title": "Dataset.padded_batch doc improvement request", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: n/a\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a\r\n- **TensorFlow installed from (source or binary)**: n/a\r\n- **TensorFlow version (use command below)**: 1.10\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: n/a\r\n\r\n### Describe the problem\r\nDocumentation problem. In the method documentation for 'tf.data.Dataset.padded_batch', the argument of `padded_shapes` is described to be \r\n\r\n> A nested structure of `tf.TensorShape` or `tf.int64` vector tensor-like objects...\r\n\r\nPython contains a myriad of different nesting structures. I would like the doc to at least refer to some complete definition of what this nested structure can be. Can it contain lists, tuples, dicts, sets? Maybe even more complex objects? Can it be an arbitrary non-cyclic graph? Please make clear any limitations on this structure.\r\n\r\n### Source code / logs\r\n[https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/data/ops/dataset_ops.py](https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/data/ops/dataset_ops.py)\r\n", "comments": ["Nagging Assignee @mrry: It has been 89 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Jiri: would you mind taking over this issue that I've been shamefully neglecting?", "tl;dr: I am open to concrete suggestions on how to improve the documentation.\r\n\r\nThe nesting structure of an element is derived from the structure of `tf.data` sources that support nesting (e.g. `from_tensors` and `from_tensor_slices`) and modified through `tf.data` transformations, particularly those whose output is a result of applying a user-provided function.\r\n\r\nThe supported nested constructs are `list`, `tuple`, and `dict`. Having said that, the `list` type seems to behave differently for `tf.data` sources and `tf.data` transformations. For `tf.data` sources, it results in extra tensor dimension, while for `tf.data` transformations it results in a nesting level (as if it was a `tuple`).\r\n\r\nTo illustrate this, consider the following example:\r\n```\r\ndataset_1 = tf.data.Dataset.from_tensors([1, 2])\r\ndataset_2 = tf.data.Dataset.range(1).map(lambda _: [1, 2])\r\n```\r\nThe former produces a single tensor of rank 1, while the latter produces a tuple of two tensors of rank 0.", "I'd like to take a stab at this if nobody else is.", "Sounds good. @aaudiber should be able to help with any questions you have as he is the current owner of tf.data API documentation.", "@tgruwell Thank you for taking an interest in improving the documentation! Currently, `padded_batch`, like all other `tf.data` transformations, supports nested structures of `tuples`, `namedtuple`, and `dict`. I'm happy to answer questions and review doc changes.", "> I'd like to take a stab at this if nobody else is.\r\n\r\nI'd almost forgotten about this, but great if you do! You are the hero open source projects need.", "@harahu,\r\n\r\nThe Python constructs that can be used to express the (nested) structure of elements include `tuple`, `dict`, `NamedTuple`, and `OrderedDict`. For more details you can refer [here](https://www.tensorflow.org/guide/data#dataset_structure).", "> @harahu,\n> \n> \n> \n> The Python constructs that can be used to express the (nested) structure of elements include `tuple`, `dict`, `NamedTuple`, and `OrderedDict`. For more details you can refer [here](https://www.tensorflow.org/guide/data#dataset_structure).\n\nThat's all I needed. Thanks!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 21819, "title": "[XLA] WhileLoopConstantSinking hangs compilation when used together with DCE", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.13.6 (17G65)\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.10.0\r\n- **Python version**: 3.6.0\r\n- **Bazel version (if compiling from source)**: 0.15.2-homebrew\r\n- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.1.0 (clang-902.0.39.2)\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nbazel-out/host/bin/tensorflow/compiler/aot/tfcompile --graph=tfgraph.pb --config=aot_config.pbtxt --entry_point=____xla_test_rnn --cpp_class=xla_test::TestNetwork --target_triple=x86_64-none-darwin --out_header=bazel-out/darwin-opt/genfiles/xla_test_rnn.h --out_metadata_object=bazel-out/darwin-opt/genfiles/xla_test_rnn_tfcompile_metadata.o --out_function_object=bazel-out/darwin-opt/genfiles/xla_test_rnn_tfcompile_function.o --target_features=+sse4.1 --target_features=+sse4.2 --target_features=+avx\r\n```\r\n\r\n[tfgraph.pb.zip](https://github.com/tensorflow/tensorflow/files/2311756/tfgraph.pb.zip)\r\n\r\naot_config.pbtxt:\r\n\r\n```\r\nfeed {\r\n  id { node_name: \"0\" }\r\n  shape {\r\n    dim { size: 1 }\r\n    dim { size: 128 }\r\n    dim { size: 194 }\r\n  }\r\n}\r\n\r\nfetch {\r\n  id { node_name: \"transpose_4713\" }\r\n}\r\n```\r\n\r\n### Describe the problem\r\n\r\nWhen tfcompile optimizes the attached graph with logging enabled, it seems to get stuck in an infinite loop in this fashion:\r\n\r\n```\r\n......\r\n\r\ntensorflow/compiler/xla/service/hlo_dce.cc:58] Removing dead root %constant.17644..sunk.5 = f32[256]{0} constant({...}), metadata={op_type=\"Add\" op_name=\"4GC\"} and it's unused operands\r\ntensorflow/compiler/xla/service/hlo_dce.cc:58] Removing dead root %constant.17657..sunk.5 = f32[384,256]{1,0} constant({...}), metadata={op_type=\"ConcatV2\" op_name=\"6Kn\"} and it's unused operands\r\ntensorflow/compiler/xla/service/hlo_dce.cc:58] Removing dead root %constant.17668..sunk.5 = f32[256]{0} constant({...}), metadata={op_type=\"Add\" op_name=\"5XK\"} and it's unused operands\r\ntensorflow/compiler/xla/service/hlo_pass_pipeline.cc:112]   HLO pass reshape-mover\r\ntensorflow/compiler/xla/service/hlo_pass_pipeline.cc:112]   HLO pass constant_folding\r\ntensorflow/compiler/xla/service/hlo_pass_pipeline.cc:112]   HLO pass simplify-conditional\r\ntensorflow/compiler/xla/service/hlo_pass_pipeline.cc:63] Running HLO pass pipeline simplification\r\ntensorflow/compiler/xla/service/hlo_pass_pipeline.cc:112]   HLO pass batchnorm_expander\r\ntensorflow/compiler/xla/service/hlo_pass_pipeline.cc:112]   HLO pass algsimp\r\ntensorflow/compiler/xla/service/hlo_pass_pipeline.cc:112]   HLO pass dce\r\ntensorflow/compiler/xla/service/hlo_pass_pipeline.cc:112]   HLO pass while-loop-invariant-code-motion\r\ntensorflow/compiler/xla/service/hlo_pass_pipeline.cc:112]   HLO pass tuple-simplifier\r\n(vvv This is actually WhileLoopConstantSinking not WhileLoopInvariantCodeMotion, and is causing the infinite loop)\r\ntensorflow/compiler/xla/service/hlo_pass_pipeline.cc:112]   HLO pass while-loop-invariant-code-motion\r\ntensorflow/compiler/xla/service/hlo_pass_pipeline.cc:112]   HLO pass simplify-while-loops\r\ntensorflow/compiler/xla/service/hlo_pass_pipeline.cc:112]   HLO pass dce\r\ntensorflow/compiler/xla/service/hlo_dce.cc:58] Removing dead root %constant.9423..sunk.6 = f32[284,256]{1,0} constant({...}), metadata={op_type=\"ConcatV2\" op_name=\"2IL\"} and it's unused operands\r\ntensorflow/compiler/xla/service/hlo_dce.cc:58] Removing dead root %constant.9433..sunk.6 = f32[256]{0} constant({...}), metadata={op_type=\"Add\" op_name=\"6hz\"} and it's unused operands\r\ntensorflow/compiler/xla/service/hlo_dce.cc:58] Removing dead root %constant.9444..sunk.6 = f32[256,256]{1,0} constant({...}), metadata={op_type=\"ConcatV2\" op_name=\"3hb\"} and it's unused operands\r\ntensorflow/compiler/xla/service/hlo_dce.cc:58] Removing dead root %constant.9454..sunk.6 = f32[256]{0} constant({...}), metadata={op_type=\"Add\" op_name=\"19n\"} and it's unused operands\r\ntensorflow/compiler/xla/service/hlo_dce.cc:58] Removing dead root %constant.9465..sunk.6 = f32[384,256]{1,0} constant({...}), metadata={op_type=\"ConcatV2\" op_name=\"6Ye\"} and it's unused operands\r\ntensorflow/compiler/xla/service/hlo_dce.cc:58] Removing dead root %constant.9475..sunk.6 = f32[256]{0} constant({...}), metadata={op_type=\"Add\" op_name=\"3z9\"} and it's unused operands\r\n\r\n......\r\n```\r\n\r\nI deleted all the \"Invariant checker\" lines in the log to highlight the passes. The graph mainly consists of lots of GRUs converted from an ONNX trace. To minimize any suspicions of tfcompile taking a long time simply due to a large model I left it running over a week without finishing. This was not the behavior in 1.8.0.\r\n\r\nI was able to narrow it down to a newly added optimization pass called `WhileLoopConstantSinking`. That particular pass seems to make changes that always triggers dead code elimination, causing the entire pass to never reach fix point. In particular, if I modify `RunHloPasses` to run `AlgebraicSimplifier` once, followed by a `HloPassFix` pipeline wrapping only a `HloDCE` then a `WhileLoopConstantSinking`, it'll get stuck in the pipeline. Once `WhileLoopConstantSinking` is disabled by commenting this line https://github.com/tensorflow/tensorflow/blob/e924d67bff8c4fb58c8316d00b662f8d1e80eb95/tensorflow/compiler/xla/service/cpu/cpu_compiler.cc#L285, `tfcompile` finishes under ten minutes.\r\n", "comments": ["@tatatodd Did you manage to reproduce the problem? Anything I could help with?", "Nagging Assignee @tatatodd: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Can you please try 1.11 or nightly version to check if this is still a problem?", "Tested with 903a6399aab19b549fefd0ead836af644f3d00f8, and it's fixed. Thanks!"]}, {"number": 21817, "title": "tflite conv bug", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:N/A\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:1.10.0\r\n- **Python version**:3.6.0\r\n- **Bazel version (if compiling from source)**:0.16.1\r\n- **GCC/Compiler version (if compiling from source)**:7.3.0\r\n- **CUDA/cuDNN version**:N/A\r\n- **GPU model and memory**:N/A\r\n- **Exact command to reproduce**:N/A\r\n\r\nThere is a bug in the multithreaded implementation of conv2d.  In certain cases when batch_size > 1, only the first image are considered while others ignored. For the reason see file contrib/lite/kernels/internal/optimized/multithreaded_conv.h\r\n```C++\r\n } else if (filter_height == input_height && filter_width == input_width &&\r\n               pad_width == 0 && pad_height == 0) {\r\n      // If the input data and filter have the same height/width,\r\n      // the 2D convolution is reduced to matrix multiplication.\r\n      const int k =  // Length of reduction dimension.\r\n          filter_width * filter_height * input_depth;\r\n      Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> dim_pair;\r\n      dim_pair[0] = Eigen::IndexPair<Eigen::DenseIndex>(1, 0);\r\n      EigenMatrix output(output_data, 1, filter_count);\r\n      ConstEigenMatrix input(input_data, 1, k);\r\n      ConstEigenMatrix filter(filter_data, k, filter_count);\r\n      MatMulConvFunctor<Eigen::ThreadPoolDevice, T>()(device, output, input,\r\n                                                      filter, dim_pair);\r\n    } else {\r\n```\r\nIn the above code, the input_batches are ignored.\r\nI have also verified a quick fix:\r\n```C++\r\n    } else if (filter_height == input_height && filter_width == input_width &&\r\n               pad_width == 0 && pad_height == 0) {\r\n      // If the input data and filter have the same height/width,\r\n      // the 2D convolution is reduced to matrix multiplication.\r\n      const int k =  // Length of reduction dimension.\r\n          filter_width * filter_height * input_depth;\r\n\t  Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> dim_pair;\r\n      dim_pair[0] = Eigen::IndexPair<Eigen::DenseIndex>(1, 0);\r\n      EigenMatrix output(output_data, input_batches, filter_count);\r\n      ConstEigenMatrix input(input_data, input_batches, k);\r\n      ConstEigenMatrix filter(filter_data, k, filter_count);\r\n      MatMulConvFunctor<Eigen::ThreadPoolDevice, T>()(device, output, input,\r\n\t\t  filter, dim_pair);\r\n    } else {\r\n```\r\nJust change 1 to input_batches.\r\nMy code works well based on my experiments.\r\nIt is a simple and quick fix, please merge it into the master if possible.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@andrehentz Can you verify the proposed fix generates intended behavior?", "@andrehentz Can you help to verify the proposed fix? Thanks.", "Thanks @zgxnet, you're right that's a bug, and your fix looks good. I'll prepare a change now (with a test) unless you prefer to submit a PR.", "@jdduke Have you prepared the change? I cannot see it on the master.", "The CL is still in review internally, but should land in the next day or two. Thanks for your patience!"]}, {"number": 21816, "title": "typo in saver docs", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Hi @rrricharrrd,\r\n\r\nThanks for the contribution! Before we can merge this though, you will need to sign the CLA linked above.\r\n\r\nThanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Please sign the CLA and reopen."]}, {"number": 21815, "title": "Please provide cmake-based build files", "body": "Currently tensorflow fails to build due to this problem: https://github.com/bazelbuild/bazel/issues/5763\r\nBazel project doesn't appear to be willing to fix the problem.\r\n\r\nCould you please provide cmake-based build? cmake is known to work much better than bazel, and many projects are using it without problems.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "OS: FreeBSD 11.2\r\nInstalled from: GitHub\r\nbazel-0.15.0\r\nNo CUDA.\r\n", "Unfortunately, the TensorFlow maintainers don't have the resources to maintain multiple build systems or more operating systems at this time and we rely on community support for CMake.\r\n\r\nSee [this thread](https://groups.google.com/a/tensorflow.org/d/msg/build/9vKi3ceP2ZI/fQxWQFHsDgAJ) and FreeBSD related issues [like this](https://github.com/tensorflow/tensorflow/issues/20421#issuecomment-401449378).\r\n\r\nThanks for your understanding.\r\n\r\n(CC @ewilderj @jakline @gunan )"]}, {"number": 21814, "title": "How to use GridLSTMCell or Grid3LSTMCell on image data in tensorflow?", "body": "I am thinking about applying GridLSTM on image data. But I cannot figure out how to orgnize my inputs to fit the requirements of `GridLSTMCell` or `Grid3LSTMCell`. I found some example use cases for `GridLSTMCell`, such as [link1 ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/kernel_tests/rnn_cell_test.py)and [link2](https://github.com/phvu/grid-lstm-tensorflow). But they focus on none-image data and help little for me.\r\n\r\nBy applying GridLSTM on image data with shape `(batch_size,38,38,64)` , I wish to get output with shape `(batch_size,38,38,num*4)`. Here, `num` refers to the count of units GridLSTM outputs in each position of the feature map with shape `(38,38)` starting from each feature map corner.\r\n\r\nThe following is my test code, I am just not able to make it work.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.rnn import static_rnn\r\nfrom tensorflow.contrib.grid_rnn import Grid3LSTMCell,Grid2LSTMCell\r\nimport numpy as np\r\n\r\n#tf.enable_eager_execution()\r\nwith tf.Session() as sess:\r\n    shape = (32,38,38,64)\r\n    inputs = tf.constant(np.ones(shape,dtype=np.float32),dtype=np.float32)\r\n    cell = Grid3LSTMCell(8)\r\n    out = static_rnn(cell,inputs,dtype=np.float32)\r\n    ouput = sess.run(out)\r\n```\r\nAny help is appropriated.\r\n\r\n\r\nHave I written custom code: n/a\r\nOS Platform and Distribution: n/a\r\nTensorFlow installed from: https://pypi.org/project/tensorflow/\r\nTensorFlow version: 1.7.0\r\nBazel version: n/a\r\nCUDA/cuDNN version: n/a\r\nGPU model and memory: n/a\r\nExact command to reproduce: n/a\r\nMobile device: n/a", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 30 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 21813, "title": "Develop upstream pre r1.10 v2", "body": "Don't Merge! PR to kick off CI check.", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 21812, "title": "Adding release notes for 1.10.1.", "body": "", "comments": []}, {"number": 21811, "title": "Estimators created from Keras models expect unconsumed outputs to be provided with data in training/evaluation.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian Jessie\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.9\r\n- **Python version**: 2.7.9\r\n- **Exact command to reproduce**: See gist [here](https://gist.github.com/zmjjmz/3f621aaafc5683238ade6224e5dedcb6)\r\n- **Bazel version**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Mobile device**: N/A\r\n\r\n### Describe the problem\r\nSorry, this is a bit arcane/edge-casey. Basically I have a Keras model that I want to convert into an Estimator for use with the `train_and_evaluate` framework. One aspect of these models is that I have layers that are outputs but do not contribute to the loss that are part of the deployed model's contract -- e.g. an `oov_code` that indicates if the input (text) was out-of-vocabulary. \r\n\r\nThe way I've been doing this _before_ switching to a `keras.model_to_estimator` method was to have the Keras model's loss and metrics be set to just the parts contribute to the loss in the `ModeKeys.TRAIN` or `ModeKeys.EVAL` modes and then have the `export_outputs` argument take all of them in the `PREDICT` mode, e.g. in this [gist](https://gist.github.com/zmjjmz/b84248a8a1ff21afd1c172e0abbba2e8). I can't do this fine-grained thing in `model_to_estimator`, which is sort of the first problem.\r\n\r\nHowever, theoretically the `Model.compile` function allows for the `loss` and `metrics` arguments to be dictionaries of output names (derived from the layer names) to their respective loss / metric functions. When using `Model.fit`, one can (as in line 49 of the provided reproduction) provide a dictionary of `output:labels` which are then appropriately routed to their respective losses / metrics (as can be seen by running it - note the `classes_accuracy` and `classes_loss` printouts.\r\n\r\nUnfortunately, when using `model_to_estimator` followed by `train_and_evaluate` my eternal enemy `_create_ordered_io` expects to find a value in `y` for all outputs, irrespective of the `loss` and `metrics` not specifying any such requirements. \r\n\r\nConfusingly I am promised separately that `we will not be expecting any data to be passed to \"reporting\" during training.` as shown in the provided logs.\r\n\r\nBasically, it would be nice if `_create_ordered_io` had some way to 'know' that these outputs should be ignored for these steps, OR for a way to specify different outputs according to the `ModeKeys` in `model_to_estimator`. I can workaround this at the moment by providing a fake output in the `numpy_input_fn`'s `y` argument (e.g. what happens if you set `incl_reporting=True` in the `get_input_fn` function in the reproduction script), however this is really not ideal for a variety of reasons.\r\n\r\n### Source code / logs\r\nThe gist provided [here](https://gist.github.com/zmjjmz/3f621aaafc5683238ade6224e5dedcb6) has both reproduction code and comments with the outputs.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nMobile device", "Have you checked https://github.com/tensorflow/tensorflow/issues/16493? /cc @pavithrasv ", "Seems related, but I guess the issue is that while the Keras APIs don't expect that data the Estimator ones do.", "This is fixed with latest tf-nightly version '1.15.0-dev20190726'. Thanks!"]}, {"number": 21810, "title": "Delete", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Hi @jjwang01, did you mean to delete this file? If so, why?\r\n", "Hi,\n\nI\u2019m not too sure how github works, but I just wanted to open an ipython notebook for an udacity course. For some reason it just opened in GitHub. Do you where to go from there?\n\nThanks,\nJustin\n________________________________\nFrom: Akshay Modi <notifications@github.com>\nSent: Thursday, August 23, 2018 10:13:46 AM\nTo: tensorflow/tensorflow\nCc: Justin Jiayi Wang; Mention\nSubject: Re: [tensorflow/tensorflow] Delete (#21810)\n\n\nHi @jjwang01<https://github.com/jjwang01>, did you mean to delete this file? If so, why?\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/pull/21810#issuecomment-415496215>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AglleGeChVJVl-k5FB_LCZyxt3-fRrVzks5uTuLKgaJpZM4WIeAb>.\n", "Hi @jjwang01,\r\n\r\nThere are some instructions on how to access these notebooks within a docker container here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/udacity\r\n\r\nI'm going to close this pull request since it seems it was opened unintentionally.\r\n\r\nThanks!\r\nAkshay", "Hi Akshay,\n\n\nI actually have a question about this page you've linked. How would I be able to use Docker on a Windows machine for this course, and if I don't have GCP instances? These instructions seem to be geared towards Linux and Mac machines.\n\n\nThanks,\n\nJustin\n\n________________________________\nFrom: Akshay Modi <notifications@github.com>\nSent: Thursday, August 23, 2018 11:56:27 AM\nTo: tensorflow/tensorflow\nCc: Justin Jiayi Wang; Mention\nSubject: Re: [tensorflow/tensorflow] Delete (#21810)\n\n\nHi @jjwang01<https://github.com/jjwang01>,\n\nThere are some instructions on how to access these notebooks within a docker container here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/udacity\n\nI'm going to close this pull request since it seems it was opened unintentionally.\n\nThanks!\nAkshay\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/pull/21810#issuecomment-415529381>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AglleL7Q14ZFwK9cTHMzV5ojq2hSu8buks5uTvrbgaJpZM4WIeAb>.\n", "Hi @jjwang01,\r\n\r\nIt should be possible to install docker on windows: https://docs.docker.com/docker-for-windows/install/\r\nOnce installed, you should be able to follow the commands on that page. If the commands don't work, please open an issue here: https://github.com/tensorflow/tensorflow/issues/new\r\n\r\nYou might also be able to get help getting up and running on the course's forums. \r\n\r\nApologies I couldn't be more help!\r\n\r\nThanks,\r\nAkshay"]}, {"number": 21809, "title": "100% CPU load after import tensorflow as tf when using USB camera stream on Windows 10", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: b'v1.10.0-rc1-19-g656e7a2b34' 1.10.0\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**: Python 3.5.5 :: Anaconda, Inc.\r\n-**Mobile device: No Mobile device\r\n- **CUDA/cuDNN version**: no GPU\r\n- **GPU model and memory**: no GPU\r\n- **Exact command to reproduce: python testCamera.py\r\n\r\n### Describe the problem\r\nI am using the pYuEye uEye API to get live images for my iDS industry USB 3.0 camera, see the simple attached code. When I run the code without importing tensorflow everything is fine. I get an image every 100 ms. But when I just add the line 'import tensorflow as tf' my CPU (i7 4770) has a 100% CPU load. I do not use any function from tensorflow.\r\n\r\nDoes anyone have an idea what could be the problem?\r\n\r\n\r\nCheers,\r\nWunder0\r\n### Source code / logs\r\n```\r\nfrom Kameraobjekt import Kamera\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef startCam():\r\n    EnableAutoparameter = True      \r\n    EnableHardwareGain = False \r\n    AOI_x = 112        \r\n    AOI_y = 84        \r\n    AOI_width = 800     \r\n    AOI_height = 600    \r\n    Buffersize = 3    \r\n    \r\n    Cam.init()  \r\n    Cam.SetParams(EnableAutoparameter, EnableHardwareGain)\r\n    Cam.SetAOI(AOI_x, AOI_y, AOI_width, AOI_height)\r\n    Cam.SetFPS(10.)  \r\n    Cam.AllocMem(Buffersize)\r\n    Cam.StartCaptureImage()\r\n    Cam.setExposure()\r\n\r\nCam = Kamera()\r\nstartCam()\r\nwhile True:\r\n    Puffer = Cam.GetImage() \r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce\nMobile device", "/CC @petewarden, any ideas?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Just an observation, first you talk about every 100ms an image, after import 100% CPU Utilization. These are two different measures, did you got each 100ms an image after import tf?\r\nPlease note that ids recommends to disable idle states. If you do this, then usually the cpu utilization in Windows task manager is 100% even if your application does not do much. \r\nMaybe this helps.", "For posterity in case someone else encounters this issue (mine was importing torch instead of tensorflow, but I believe the issue is the same):\r\n\r\nI was able to solve it by adjusting the OMP policy. Setting an environmental variable (OMP_NUM_THREADS = 2) greatly reduced CPU usage without slowing down my throughput. I went from using 100% of all 16 cores to using 100% of only two cores. Setting OMP_NUM_THREADS = 1 further reduced CPU load to using only 80% of a single core and didn't appear to affect throughput either. Still doing some testing though."]}, {"number": 21808, "title": "Avijit/fix license files", "body": "We discovered that the license file names are wrong when nGraph build is selected. This PR fixes these file names so that nGraph is built with TensorFlow when requested. This PR also fixes nGraph unit tests due to recent changes in the MKL bazel files.", "comments": []}, {"number": 21807, "title": "Profiling Tensorflow C++ with nvidia profiler/tf.Profiler", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a\r\n- **TensorFlow installed from (source or binary)**: bazel\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**:2.7\r\n- **Bazel version (if compiling from source)**: 0.14.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0 20160609\r\n- **CUDA/cuDNN version**: 9/7.1\r\n- **GPU model and memory**: 1080ti/11gb dual\r\n- **Exact command to reproduce**:\r\nnvprof bazel-bin/tensorflow/examples/label_image/label_image from [Link](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/label_image)\r\nworks, but when i run nvvp it throws me an error, that application profiled cannot be found. I am looking at an TF alternative [tf.Profiler](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler) , but it doesn't have any documentation for C++ api\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Nagging Assignee @shivaniag: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Switched to Timeline."]}, {"number": 21806, "title": "Add whl file to .gitignore.", "body": "", "comments": ["cc: @facaiy "]}, {"number": 21805, "title": "Unable to convert frozen graph model to required fromat", "body": "I am working on the AIY vision kit and trying to implement object detection in a video stream. I have tried following the method posted on the AIY website(https://aiyprojects.withgoogle.com/) but seem to be facing a problem when I try to convert the model using the bonnet model compiler. This is a brief summary of the steps I followed:\r\n\r\n    1.create annotations for each image in the training set in xml format(256x256).\r\n    2.convert all the images to 256x256.\r\n    3.create the tf records.\r\n    4.downloaded the base model of ssd_mobilnet_v1_coco.config and made the required changes.(set \r\n       num classes = 2, and path to train and val tf records).\r\n    5.trained the model\r\n    6.exported the frozen graph to binaryproto format. These 5 steps are can be seen by following this \r\n       link,the same images were used but resized.(https://medium.freecodecamp.org/trackin ... \r\n       c86419225e) all the functions such as create tf record and other related functions can be found in \r\n       this git repo https://github.com/bourdakos1/Custom-Object-Detection\r\nAfter I export the model, I get the following error while using the bonnet model compiler:\r\n\r\n2018-08-14 10:39:19.285588: I external/org_tensorflow/tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 556 operators, 893 arrays (0 quantized)\r\n\r\n2018-08-14 10:39:19.302398: I external/org_tensorflow/tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 556 operators, 893 arrays (0 quantized)\r\n\r\n2018-08-14 10:39:19.324886: F external/org_tensorflow/tensorflow/contrib/lite/toco/tooling_util.cc:628] Check failed: dim >= 1 (0 vs. 1)\r\n\r\nI have also attached th3 frozen graph below\r\n[frozen_inference_graph.zip](https://github.com/tensorflow/tensorflow/files/2310933/frozen_inference_graph.zip)\r\n", "comments": ["@crimsontrigger : Can you also include the exact conversion command that you used, it will help in debugging.", "@shashishekhar this is the conversion that gave me the error\r\n./bonnet_model_compiler.par \\\r\n  --frozen_graph_path=frozen_inference_graph.pb \\\r\n  --output_graph_path=customized_detector.binaryproto \\\r\n  --input_tensor_name=\"Preprocessor/sub\" \\\r\n  --output_tensor_names=\"concat,concat_1\" \\\r\n  --input_tensor_size=256 \\\r\n  --debug\r\nyou can find all the steps I used in this link\r\nhttps://medium.freecodecamp.org/tracking-the-millenium-falcon-with-tensorflow-c8c86419225e", "@crimsontrigger The frozen graph has batch number > 1, which is not supported. How did you generate the frozen graph? The inference graph should have a batch equal to 1.", "python object_detection/export_inference_graph.py \\\r\n        --input_type image_tensor \\\r\n        --pipeline_config_path faster_rcnn_resnet101.config \\\r\n        --trained_checkpoint_prefix model.ckpt-STEP_NUMBER \\\r\n        --output_directory output_inference_graph\r\nthis is the command I used. The python file mentioned above to convert can be found in this link\r\nhttps://github.com/bourdakos1/Custom-Object-Detection/blob/master/object_detection/export_inference_graph.py\r\n", "This is not a tflite issue, it's an AIY vision kit issue.", "@crimsontrigger \r\nFor further discussion, feel free to create another issue at AIY's github\r\nhttps://github.com/google/aiyprojects-raspbian\r\n\r\nIf I understand your training correctly, you are training a faster_rccn_resnet model? Such model structure is not supported because of limited hardware resources on VisionKit. \r\n\r\nPlease see the section\r\nhttps://aiyprojects.withgoogle.com/vision/#makers-guide--tensorflow-model-compiler\r\nHow to train and deploy a customized object detection model trained with TF\u2019s object detection API? "]}, {"number": 21803, "title": "Add tensor forest classification inference in core", "body": "According to https://github.com/tensorflow/community/blob/master/rfcs/20180626-tensor-forest.md\r\n\r\nThis is a first step. And the whole process is tracked by https://github.com/tensorflow/tensorflow/issues/21830\r\n\r\nWe are adding an canned TensorForestClassifier and TensorForestRegressor into core.\r\n\r\nThe Pr is only a first step implementing TensorForestClassifier with inference only functionality. \r\n\r\nFor inferencing purpose, we added one resource `TensorForestTreeResource`.\r\n\r\nand several kernels for operating the resource `TensorForestCreateTreeVariableOp `,  `TensorForestTreeSerializeOp`, `TensorForestTreeDeserializeOp`,  `TensorForestTreeSizeOp`,  `TensorForestTreeIsInitializedOp` . \r\n\r\nand one kernel for the inference `TensorForestTreePredictOp`.\r\n\r\n~~From the kernel side, we also added two helper class `LeafModelOperator` and  `BinaryDecisionNodeEvaluator`~~\r\n\r\nFor the protobuf class, we added  ~~`DecisionTree`~~ , ~~`Vector/SparseVector`~~, ~~`InequalityTest`~~, ~~`TreePath`~~ and ~~`Leaf`~~  ~~`BinaryNode`~~, ~~`Tree`~~ `DenseSplit`  reused    `Vector/SparseVector` `Leaf`, `Node` and `Tree` from boosted_trees\r\n\r\ncc @nataliaponomareva  \r\n", "comments": ["We can just keep is_regression around and do ifs based on that.\n\nOn Thu, Aug 23, 2018 at 3:03 PM Peng Yu <notifications@github.com> wrote:\n\n> *@yupbank* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/core/kernels/tensor_forest/leaf_model.h\n> <https://github.com/tensorflow/tensorflow/pull/21803#discussion_r212419907>\n> :\n>\n> > +#include \"tensorflow/core/framework/tensor.h\"\n> +#include \"tensorflow/core/framework/tensor_types.h\"\n> +#include \"tensorflow/core/kernels/tensor_forest/tensor_forest.pb.h\"\n> +\n> +namespace tensorflow {\n> +\n> +using tensorforest::Leaf;\n> +using tensorforest::LeafStat;\n> +\n> +typedef TTypes<const float, 2>::ConstTensor DenseTensorType;\n> +\n> +namespace {\n> +enum LeafModelType { CLASSIFICATION = 0, REGRESSION = 1 };\n> +}\n> +\n> +class LeafModelOperator {\n>\n> there is a difference for leafs when ExportModel and UpdateModel between\n> classification\n> https://github.com/tensorflow/tensorflow/pull/21579/files#diff-c000cbb3c7359ba2e87d29b7b3e4c399R63\n> and regression\n> https://github.com/tensorflow/tensorflow/pull/21579/files#diff-c000cbb3c7359ba2e87d29b7b3e4c399R70\n> .\n>\n> I think the idea is for every protobuf object, we have a operator class\n> equivalent for it.\n>\n> But still, what would you suggest if we remove this abstraction?\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/21803#discussion_r212419907>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEHQFYHKd4FGFVEuUBzlAvGq1fR_g1i9ks5uTvyHgaJpZM4WH0cE>\n> .\n>\n", "For API review, does this change the public TF API? I didn't see any changes to the golden API files, do they need to be regenerated (perhaps that is why the tests are failing)?", "Not sure why the windows Bazel GPU  is failing... \r\notherwise, i have addressed the failing tests.", "This is fine for TF API review, but you will need to regenerate the golden API files since we now have separate v1 and v2 directories.", "Thank you! Let me do that ", "Nagging Assignee @akshaym: It has been 21 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "i think the failing test case is irrelevant from this pr..  @nataliaponomareva  mind have a second look ?"]}, {"number": 21802, "title": "./ configure fails due to nccl2 on ubuntu 16.04", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution** :Linux Ubuntu 16.04)\r\n-CPU- Intel(R) Xeon(R) CPU           E5520  @ 2.27GHz\t\t: 1596,00MHz\r\n```\r\n$ uname -a\r\nLinux Dell-T5500 4.15.0-32-generic #35~16.04.1-Ubuntu SMP Fri Aug 10 21:54:34 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n- **TensorFlow installed from (source or binary)**:\r\ninstalled from source using git\r\n- **TensorFlow version (use command below)**: Release 1.10.0\r\n- **Python version**:\r\n```\r\n$ python\r\nPython 3.6.5 | packaged by conda-forge | (default, Apr  6 2018, 13:39:56) \r\n[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n```\r\n- **Bazel version (if compiling from source)**: bazel 0.15.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\n`gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10) `\r\n\r\n- **CUDA/cuDNN version**:\r\n```\r\n~$ apt search cudnn\r\nEn train de trier... Fait\r\nRecherche en texte int\u00e9gral... Fait\r\nlibcudnn7/now 7.1.4.18-1+cuda9.2 amd64  [install\u00e9, local]\r\n  cuDNN runtime libraries\r\nlibcudnn7-dev/now 7.1.4.18-1+cuda9.2 amd64  [install\u00e9, local]\r\n  cuDNN development libraries and headers\r\n```\r\n- **GPU model and memory**: _GTX960 4Gb_\r\n```\r\n$ nvidia-smi\r\nWed Aug 22 15:43:58 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 396.51                 Driver Version: 396.51                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 960     Off  | 00000000:03:00.0  On |                  N/A |\r\n| 39%   35C    P8    12W / 130W |    272MiB /  4042MiB |      1%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n```\r\n- **Mobile device:** NA\r\n- **Exact command to reproduce**:\r\n./configure was executed from a virtual environment:\r\n```\r\n$ ./configure\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.15.0 installed.\r\nPlease specify the location of python. [Default is /home/jeanpat/anaconda3/envs/DeepFish/bin/python]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /home/jeanpat/anaconda3/envs/DeepFish/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/jeanpat/anaconda3/envs/DeepFish/lib/python3.6/site-packages]\r\n\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: Y\r\njemalloc as malloc support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n\r\nNo Google Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: n\r\nNo Hadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Amazon AWS Platform support? [Y/n]: n\r\nNo Amazon AWS Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: n\r\nNo Apache Kafka Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: y\r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: y\r\nGDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: n\r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: Y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: 9.2\r\n\r\nPlease specify the location where CUDA 9.2 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 7.14\r\n\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: N\r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nPlease specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: \r\n\r\nPlease specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\n\r\nInvalid path to NCCL 2 toolkit, /usr/local/cuda-9.2/lib/libnccl.so.2 or /usr/local/cuda-9.2/include/nccl.h not found. Please use the O/S agnostic package of NCCL 2\r\nPlease specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]:\r\n```\r\n\r\n\r\n### Describe the problem\r\n**./configure** script failed to detect nccl2 which was install from a nvidia deb package:\r\n\r\n\r\n### Source code / logs\r\n```\r\napt show libnccl2\r\nPackage: libnccl2\r\nVersion: 2.2.13-1+cuda9.2\r\nPriority: optional\r\nSection: libs\r\nSource: nccl\r\nMaintainer: cudatools <cudatools@nvidia.com>\r\nInstalled-Size: 74,5 MB\r\nDepends: libc6 (>= 2.3.4), libgcc1 (>= 1:3.0), libstdc++6 (>= 4.1.1)\r\nDownload-Size: 27,6 MB\r\nAPT-Manual-Installed: yes\r\nAPT-Sources: file:/var/nccl-repo-2.2.13-ga-cuda9.2  Packages\r\nDescription: NVIDIA Collectives Communication Library (NCCL) Runtime\r\n NCCL (pronounced \"Nickel\") is a stand-alone library of standard collective\r\n communication routines for GPUs, such as all-gather, reduce, broadcast, etc.,\r\n that have been optimized to achieve high bandwidth over PCIe. NCCL supports up\r\n to eight GPUs and can be used in either single- or multi-process (e.g., MPI)\r\n applications.\r\n```\r\n\r\n```\r\n~$ locate libnccl\r\n/home/jeanpat/.cache/bazel/_bazel_jeanpat/25351aa6d762f8d39f0cff26612e1488/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/contrib/nccl/libnccl_ops_op_lib.lo\r\n/home/jeanpat/.cache/bazel/_bazel_jeanpat/25351aa6d762f8d39f0cff26612e1488/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/contrib/nccl/libnccl_ops_op_lib.lo-2.params\r\n/home/jeanpat/.cache/bazel/_bazel_jeanpat/25351aa6d762f8d39f0cff26612e1488/execroot/org_tensorflow/bazel-out/k8-opt/bin/external/nccl_archive/libnccl.pic.a\r\n/home/jeanpat/.cache/bazel/_bazel_jeanpat/25351aa6d762f8d39f0cff26612e1488/execroot/org_tensorflow/bazel-out/k8-opt/bin/external/nccl_archive/libnccl.pic.a-2.params\r\n/home/jeanpat/.cache/bazel/_bazel_jeanpat/25351aa6d762f8d39f0cff26612e1488/external/nccl_archive/debian/libnccl-dev.install\r\n/home/jeanpat/.cache/bazel/_bazel_jeanpat/25351aa6d762f8d39f0cff26612e1488/external/nccl_archive/debian/libnccl-dev.manpages\r\n/home/jeanpat/.cache/bazel/_bazel_jeanpat/25351aa6d762f8d39f0cff26612e1488/external/nccl_archive/debian/libnccl1.install.in\r\n/home/jeanpat/Developpement/Archives-cuda/CUDA 9/libnccl-dev_2.1.4-1+cuda9.0_amd64.deb\r\n/home/jeanpat/Developpement/Archives-cuda/CUDA 9/libnccl2_2.1.4-1+cuda9.0_amd64.deb\r\n/home/jeanpat/anaconda3/envs/DeepFish/lib/libnccl.so\r\n/home/jeanpat/anaconda3/envs/DeepFish/lib/libnccl.so.1\r\n/home/jeanpat/anaconda3/envs/DeepFish/lib/libnccl.so.1.3.5\r\n/home/jeanpat/anaconda3/envs/DeepFish/lib/libnccl_static.a\r\n/home/jeanpat/anaconda3/pkgs/nccl-1.3.4-cuda8.0_1/lib/libnccl.so\r\n/home/jeanpat/anaconda3/pkgs/nccl-1.3.4-cuda8.0_1/lib/libnccl.so.1\r\n/home/jeanpat/anaconda3/pkgs/nccl-1.3.4-cuda8.0_1/lib/libnccl.so.1.3.4\r\n/home/jeanpat/anaconda3/pkgs/nccl-1.3.4-cuda8.0_1/lib/libnccl_static.a\r\n/home/jeanpat/anaconda3/pkgs/nccl-1.3.5-cuda9.0_0/lib/libnccl.so\r\n/home/jeanpat/anaconda3/pkgs/nccl-1.3.5-cuda9.0_0/lib/libnccl.so.1\r\n/home/jeanpat/anaconda3/pkgs/nccl-1.3.5-cuda9.0_0/lib/libnccl.so.1.3.5\r\n/home/jeanpat/anaconda3/pkgs/nccl-1.3.5-cuda9.0_0/lib/libnccl_static.a\r\n/usr/lib/x86_64-linux-gnu/libnccl.so\r\n/usr/lib/x86_64-linux-gnu/libnccl.so.2\r\n/usr/lib/x86_64-linux-gnu/libnccl.so.2.2.13\r\n/usr/lib/x86_64-linux-gnu/libnccl_static.a\r\n/usr/share/doc/libnccl-dev\r\n/usr/share/doc/libnccl2\r\n/usr/share/doc/libnccl-dev/NCCL-SLA.txt.gz\r\n/usr/share/doc/libnccl-dev/changelog.Debian.gz\r\n/usr/share/doc/libnccl-dev/copyright\r\n/usr/share/doc/libnccl2/NCCL-SLA.txt.gz\r\n/usr/share/doc/libnccl2/changelog.Debian.gz\r\n/usr/share/doc/libnccl2/copyright\r\n/var/lib/dpkg/info/libnccl-dev.list\r\n/var/lib/dpkg/info/libnccl-dev.md5sums\r\n/var/lib/dpkg/info/libnccl2.list\r\n/var/lib/dpkg/info/libnccl2.md5sums\r\n/var/lib/dpkg/info/libnccl2.shlibs\r\n/var/lib/dpkg/info/libnccl2.triggers\r\n/var/nccl-repo-2.2.13-ga-cuda9.2/libnccl-dev_2.2.13-1+cuda9.2_amd64.deb\r\n/var/nccl-repo-2.2.13-ga-cuda9.2/libnccl2_2.2.13-1+cuda9.2_amd64.deb\r\n```\r\n\r\n```\r\n$ locate nccl.h\r\n/home/jeanpat/.cache/bazel/_bazel_jeanpat/25351aa6d762f8d39f0cff26612e1488/execroot/org_tensorflow/bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/nccl/third_party/nccl/nccl.h\r\n/home/jeanpat/.cache/bazel/_bazel_jeanpat/25351aa6d762f8d39f0cff26612e1488/external/nccl_archive/src/nccl.h\r\n/home/jeanpat/anaconda3/envs/DeepFish/include/nccl.h\r\n/home/jeanpat/anaconda3/envs/DeepFish/lib/python3.6/site-packages/torch/lib/include/torch/csrc/cuda/nccl.h\r\n/home/jeanpat/anaconda3/envs/DeepFish/lib/python3.6/site-packages/torch/lib/include/torch/csrc/cuda/python_nccl.h\r\n/home/jeanpat/anaconda3/envs/DeepFish/pkgs/pytorch-0.4.0-py36_cuda9.1.85_cudnn7.1.2_1/lib/python3.6/site-packages/torch/lib/include/torch/csrc/cuda/nccl.h\r\n/home/jeanpat/anaconda3/envs/DeepFish/pkgs/pytorch-0.4.0-py36_cuda9.1.85_cudnn7.1.2_1/lib/python3.6/site-packages/torch/lib/include/torch/csrc/cuda/python_nccl.h\r\n/home/jeanpat/anaconda3/envs/PyTorch/lib/python3.6/site-packages/torch/lib/include/torch/csrc/cuda/nccl.h\r\n/home/jeanpat/anaconda3/envs/PyTorch/lib/python3.6/site-packages/torch/lib/include/torch/csrc/cuda/python_nccl.h\r\n/home/jeanpat/anaconda3/pkgs/nccl-1.3.4-cuda8.0_1/include/nccl.h\r\n/home/jeanpat/anaconda3/pkgs/nccl-1.3.5-cuda9.0_0/include/nccl.h\r\n/home/jeanpat/anaconda3/pkgs/pytorch-0.4.0-py36_cuda8.0.61_cudnn7.1.2_1/lib/python3.6/site-packages/torch/lib/include/torch/csrc/cuda/nccl.h\r\n/home/jeanpat/anaconda3/pkgs/pytorch-0.4.0-py36_cuda8.0.61_cudnn7.1.2_1/lib/python3.6/site-packages/torch/lib/include/torch/csrc/cuda/python_nccl.h\r\n/home/jeanpat/anaconda3/pkgs/pytorch-0.4.0-py36_cuda9.0.176_cudnn7.1.2_1/lib/python3.6/site-packages/torch/lib/include/torch/csrc/cuda/nccl.h\r\n/home/jeanpat/anaconda3/pkgs/pytorch-0.4.0-py36_cuda9.0.176_cudnn7.1.2_1/lib/python3.6/site-packages/torch/lib/include/torch/csrc/cuda/python_nccl.h\r\n/home/jeanpat/anaconda3/pkgs/pytorch-0.4.0-py36_cuda9.1.85_cudnn7.1.2_1/lib/python3.6/site-packages/torch/lib/include/torch/csrc/cuda/nccl.h\r\n/home/jeanpat/anaconda3/pkgs/pytorch-0.4.0-py36_cuda9.1.85_cudnn7.1.2_1/lib/python3.6/site-packages/torch/lib/include/torch/csrc/cuda/python_nccl.h\r\n/home/jeanpat/anaconda3/pkgs/pytorch-0.4.0-py36hdf912b8_0/lib/python3.6/site-packages/torch/lib/include/torch/csrc/cuda/nccl.h\r\n/home/jeanpat/anaconda3/pkgs/pytorch-0.4.0-py36hdf912b8_0/lib/python3.6/site-packages/torch/lib/include/torch/csrc/cuda/python_nccl.h\r\n/home/jeanpat/anaconda3/pkgs/pytorch-cpu-0.4.0-py36_cpu_1/lib/python3.6/site-packages/torch/lib/include/torch/csrc/cuda/nccl.h\r\n/home/jeanpat/anaconda3/pkgs/pytorch-cpu-0.4.0-py36_cpu_1/lib/python3.6/site-packages/torch/lib/include/torch/csrc/cuda/python_nccl.h\r\n/usr/include/nccl.h\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nMobile device", "Finally, using nccl 1.3.5 allowed to build tensorflow 1.10", "Hi @jeanpat, take a look on this instructions\r\n\r\nThe version mentioned is a bit old but the process is the same for the new version of nccl2.\r\nHowever people was debating about TFServing there, the process is the valid for Tensorflow with nccl2.\r\n\r\nhttps://github.com/tensorflow/serving/issues/327#issuecomment-385712926", "In order to have NCCL automatically detected by `./configure` you should install the `O/S Agnostic` version of NCCL instead of using the debian package. Doing so allow you to place it in your *cuda* folder with all other libraries (*i.e. cudnn, ...*)"]}, {"number": 21801, "title": "tensorflow-gpu failed init from systemd service", "body": "I have wrote an application with Java / C / Python who works well since a few weeks and executed in Bash by:\r\n`source ../frozen-graph-env/bin/activate && /usr/bin/java -jar test.jar --start`\r\n\r\npip freeze of my environment:\r\n\r\n> absl-py==0.2.2\r\n> astor==0.6.2\r\n> bleach==1.5.0\r\n> gast==0.2.0\r\n> grpcio==1.12.1\r\n> html5lib==0.9999999\r\n> Markdown==2.6.11\r\n> numpy==1.14.5\r\n> pkg-resources==0.0.0\r\n> protobuf==3.6.0\r\n> six==1.11.0\r\n> tensorboard==1.6.0\r\n> tensorflow-gpu==1.6.0\r\n> termcolor==1.1.0\r\n> Werkzeug==0.14.1\r\n\r\nThe goal now is to execute the application with systemd:\r\n`ExecStart=/bin/bash -c \"source ../frozen-graph-env/bin/activate && /usr/bin/java -jar test.jar --start\"`\r\n\r\nThe Java application start then exit because tensorflow-gpu failed to be imported. I have no log in systemd / syslog to show it (not the best way to open an issue, sorry) but if I replace _tensorflow-gpu 1.6.0_ by _tensorflow 1.6.0_, i.e. without GPU support, the application works well.\r\n\r\nI'm importing tensorflow in my python script with:\r\n`import tensorflow as tf`\r\n\r\nIs it the same issue of [tensorflow gpu failed init from systemd service - Stack Overflow](https://www.google.fr/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&uact=8&ved=2ahUKEwjprca82IDdAhUhC8AKHZGVAiYQFjACegQIBxAB&url=https%3A%2F%2Fstackoverflow.com%2Fquestions%2F47429354%2Ftensorflow-gpu-failed-init-from-systemd-service&usg=AOvVaw0JTtqGmcGLXHUG9f_76xBz)?\r\n\r\n**Is there a specific way to use tensorflow-gpu with systemd? Is it an issue? How can I help to investigate?**\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code: yes**\r\n- **OS Platform and Distribution: Linux Ubuntu 16.04**\r\n- **Bazel version: NA**\r\n- **CUDA/cuDNN version: NA**\r\n- **GPU model and memory: NA**\r\n- **Exact command to reproduce: NA**\r\n- **Mobile device: NA**\r\n- **TensorFlow installed from pip**\r\n- **TensorFlow version: v1.6.0-0-gd2e24b6039 1.6.0**\r\n- **TensorFlow GPU version: v1.6.0-0-gd2e24b6039 1.6.0**", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "Ok thanks, done on [StackOverflow](https://stackoverflow.com/q/52039439/9947187)."]}, {"number": 21800, "title": "Custom tensorflow op with output shape determined by the input tensor", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES\r\n- Linux Ubuntu 16.04\r\n- PC\r\n- Binary\r\n- 1.10\r\n- 3.5\r\n- 9.0/7.0\r\n- V100 \r\n- **Exact command to reproduce**:\r\n```\r\nREGISTER_OP(\"GroupPoint\")\r\n    .Input(\"points: float32\")\r\n    .Input(\"idx: int32\")\r\n    .Input(\"length: int32\")\r\n    .Output(\"out: float32\")\r\n    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\r\n        ::tensorflow::shape_inference::ShapeHandle dims1; **// shape N x C**\r\n        c->WithRank(c->input(0), 2, &dims1);\r\n        ::tensorflow::shape_inference::ShapeHandle dims2; **// shape L**\r\n        c->WithRank(c->input(1), 1, &dims2);\r\n        ::tensorflow::shape_inference::ShapeHandle dims3; **// scalar, shape 0**\r\n        c->WithRank(c->input(2), 0, &dims3);\r\n        ::tensorflow::shape_inference::ShapeHandle output = c->MakeShape({c->Dim(dims2, 2), L});\r\n        **// L should have the value of input(2)**\r\n        c->set_output(0, output); **// shape input(2) x C**\r\n        return Status::OK();\r\n    });\r\n```\r\n\r\n### Describe the problem\r\nBasically I want to write a customized op to output a tensor with shape which is determined by the value of input(2), i.e the third input. \r\nMore specifically, I want to get 'length' number of 'idx' (i.e. idx[0:length] ) then to gather those indices from 'points'. ( i.e. tf.gather(points, idx[0:length]))\r\nThe stock tensor-flow apis such as tf.slice and tf.gather would not allow me to do this because my op (also an customised op) to get input 'idx' has to be run on GPU. Error Message as:\r\n\r\n**Tensor(\"strided_slice:0\", shape=(128, 16), dtype=float32, device=/device:GPU:0)\r\nTensor(\"strided_slice_1:0\", shape=(32,), dtype=int32, device=/device:GPU:0) Tensor(\"strided_slice_2:0\", shape=(), dtype=i\r\nnt32, device=/device:GPU:0)\r\n2018-08-22 13:28:55.108318: E tensorflow/core/common_runtime/executor.cc:696] Executor failed to create kernel. Not found\r\n: No registered 'QueryBallPoint' OpKernel for CPU devices compatible with node QueryBallPoint = QueryBallPoint[nsample=32\r\n, radius=1, _device=\"/device:GPU:0\"](Const_1, Const_2)\r\n        .  Registered:  device='GPU'\r\n[[Node: QueryBallPoint = QueryBallPoint[nsample=32, radius=1, _device=\"/device:GPU:0\"](Const_1, Const_2)]]\r\nTensor(\"GatherV2:0\", shape=(?,), dtype=int32, device=/device:GPU:0)**\r\n\r\nI have tried to digging from the source code of class ::tensorflow::shape_inference::InferenceContext*, but failed, I have no idea how to do this, any kind of suggestions would be greatly helpful, thanks", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nMobile device", "The shape functions run during graph construction, not execution - as a means to catch shape errors as early as possible. The value of input tensors is not available (because it hasn't been computed), and thus the shape functions cannot always infer a fully-specified shape.\r\n\r\nYou'll see this for example in [ops like `Unique`](https://github.com/tensorflow/tensorflow/blob/abd64d680b9afa37e8c0a7ce5504c1f7c1a81fbc/tensorflow/core/ops/array_ops.cc#L1362), where the operation would have to be computed in order to get the fully-specified shape.\r\n\r\nIf I understand your operation correctly, you'd have to do something like:\r\n\r\n```c++\r\noutput = c->MakeShape({c->Dim(dims2, 2), InferenceContext::kUnknownDim});\r\n```\r\n\r\nHope that helps.", "Thank you so much for the help @asimshankar , I've change my code based on your suggestion. (btw I've made a mistake in my question, it should be **c->MakeShape({L, c->Dim(dims1, 2)})** instead of original line **c->MakeShape({c->Dim(dims2, 2), L})** )\r\n\r\nNow my code looks like this:\r\n::tensorflow::shape_inference::ShapeHandle output = c->MakeShape({c->kUnknownDim, c->Dim(dims1, 2)});\r\nc->set_output(0, output);\r\n\r\nBut another error occured:\r\n**Tensor(\"strided_slice_1:0\", shape=(32,), dtype=int32, device=/device:GPU:0) Tensor(\"strided_slice_2:0\", shape=(), dtype=int32, device=/device:GPU:0)\r\n2018-08-23 06:58:18.876048: F /usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/shape_inference.h:803] Check failed: dim.IsSet() Internal error: Got nullptr for Dimension.\r\nAborted (core dumped)**\r\n\r\nI've also tired:\r\nc->set_output(0, c->Matrix(c->kUnknownDim, c->Dim(dims1, 2)));\r\nsame error\r\nMy guess is maybe because this is an GPU op? ", "No, it shouldn't have anything to do with being a GPU op, shape inference should work regardless. You probably actually want something like:\r\n\r\n```c++\r\n        ::tensorflow::shape_inference::ShapeHandle dims1; \r\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &dims1));\r\n        ::tensorflow::shape_inference::ShapeHandle dims2; \r\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &dims2));\r\n        ::tensorflow::shape_inference::ShapeHandle dims3; \r\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &dims3));\r\n        ::tensorflow::shape_inference::ShapeHandle output = c->MakeShape({c->UnknownDim(), c->Dim(dims2, 2)});\r\n        c->set_output(0, output); \r\n        return Status::OK();\r\n```", "UPDATE:\r\nc->set_output(0, c->Matrix(c->UnknownDim(), c->UnknownDim()));\r\nthis actually worked but with out put tensor dimension as:\r\nTensor(\"GroupPoint:0\", shape=(?, ?), dtype=float32, device=/device:GPU:0)\r\nwhich is not optimal", "@asimshankar solved, its because I should use c->Dim(dims2, 1) instead of c->Dim(dims2, 2), cause input(0) is rank 2..... T_T\r\nLOOOOOOOOL\r\nthank you for your help \r\n\r\n```\r\n        ::tensorflow::shape_inference::ShapeHandle dims1; \r\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &dims1));\r\n        ::tensorflow::shape_inference::ShapeHandle dims2; \r\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &dims2));\r\n        ::tensorflow::shape_inference::ShapeHandle dims3; \r\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &dims3));\r\n        ::tensorflow::shape_inference::ShapeHandle output = c->MakeShape({c->UnknownDim(), c->Dim(dims2, 1)});\r\n        c->set_output(0, output); \r\n        return Status::OK();\r\n```", "Hi, I'm trying to do the same thing. I'm wondering what you would do for allocating the memory for the output tensor? For example: OP_REQUIRES_OK(context, context->allocate_output(0, SHAPE, &output_tensor)); What did you use for SHAPE? Thanks!"]}, {"number": 21799, "title": "Keras.fit fails when using tf.data.Datasets with sparse labels", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: r1.10\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 5.2\r\n- **GPU model and memory**: NVIDIA TITAN X\r\n- **Exact command to reproduce**: See below\r\n\r\n### Describe the problem\r\nKeras model.fit function now accepts tf.data.Datasets as an argument, however sparse output labels cause an error. \r\n\r\n### Source code / logs\r\n\r\n1. Build a custom Keras model:\r\nmodel = tf.keras.applications.MobileNet(weights=None, include_top=True, input_shape=(32,32,3), classes=100)\r\n\r\n2. Compile with 'sparse_categorical_crossentropy':\r\nmodel.compile(optimizer='sgd', loss='sparse_categorical_crossentropy',     metrics=['accuracy'])\r\n\r\n3. This model will train successfully if passed a numpy array pair (x, sparse_y), but fails when this is wrapped in tf.data.Dataset interface. See traceback below:\r\n\r\n  File \"C:\\PycharmProjects\\mi_exp\\.venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1278, in fit\r\n    validation_split=validation_split)\r\n  File \"C:\\PycharmProjects\\mi_exp\\.venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 917, in _standardize_user_data\r\n    exception_prefix='target')\r\n  File \"C:\\PycharmProjects\\mi_exp\\.venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\", line 182, in standardize_input_data\r\n    'with shape ' + str(data_shape))\r\nValueError: Error when checking target: expected reshape_2 to have 2 dimensions, but got array with shape (None,)\r\n", "comments": ["Working on a fix, will update this thread when it becomes available", "The fix should now be available in the nightly build, please ```pip install tf-nightly``` (or ```pip install tf-nightly-gpu``` for GPU support) and let me know if that works for you", "Nice work! I tested with success on tf-nightly, but it failed on tf-nightly-gpu with the old error. Are you sure your fix got pushed to the tf-nightly-gpu branch?", "Hi @dlfelps are you still seeing this problem with the latest tf-nightly-gpu?", "Hi,\r\nI encountered the same problem (using TF version 1.10). After upgrading to nightly, the error message changes, but the issue is still there:\r\n`File \"/home/maks/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 332, in standardize_input_data\r\n    ' but got array with shape ' + str(data_shape))\r\nValueError: Error when checking input: expected Flat_Input to have shape (250882,) but got array with shape (1,)`\r\nAny solutions?\r\nCheers,\r\nMaks", "Hi,\r\nI may have jumped the gun saying this was fixed. When I initially tested the nightly it was no longer giving me errors, however there is still an issue. Comprehensive example here: https://github.com/dlfelps/tf_21799\r\nDaniel", "@maksym33 make sure you're batching your data, if you still see an error can you reply with a simple code snippet that reproduces?", "@tanzhenyu Can you take a look at the code @dlfelps posted? https://github.com/dlfelps/tf_21799\r\n\r\nIt looks like the Estimator created in `model_to_estimator` is not training when the Keras loss is sparse categorical crossentropy", "@omalleyt12 \r\nHey, \r\nThanks for the answer - I checked and you were right - I had to batch the tf dataset separately from the batch argument in model.fit\r\nCheers,\r\nMaks ", "Nagging Assignees @robieta, @omalleyt12: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This appears to be fixed now. I am not sure who did it or when it occurred, but I just checked the nightly build and the issue was no longer there. Thanks!"]}, {"number": 21798, "title": "Clean all those safe_div, _safe_div methods", "body": "tf.div_no_nan was introduced by #21621, which is designed to replace all those safe_div methods here.\r\n\r\nRef: #21784\r\n\r\n~~@martinwicke please add a API design label, because we add a new argument: negative_to_zero.~~", "comments": ["@martinwicke  negative_to_zero was removed.", "@akanimax @martinwicke could you comment or reassign? Thanks.", "Hey, what am I doing wrong? It seems that both reviewers are selected by github automatically.", "@alextp Hi, I think all tests pass, and the failures are unrelated.", "Hi, any update here? thanks"]}, {"number": 21797, "title": "why tf has no attribute 'ThreadPoolOptionProto'", "body": "tf version:  common cpu tensorflow v1.9 (I'v tried 1.10 too)\r\ni wanna config the num_threads in run_config, using:\r\n`tf.ThreadPoolOptionProto(num_threads=xx)`\r\n but an ERROR throw:\r\nmodel 'tensorflow' has no attrbute 'ThreadPoolOptionProto'.\r\n\r\nWhile, in the proto file: `tensorflow/core/protobuf/config.proto`,  The message of 'ThreadPoolOptionProto' has been defined.\r\nso, why i cannot use it, and how to use?\r\nthank you very much.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "**System information:**\r\n- Have I written custom code:\r\nI'v written custom estimator based on api, and the code is running well.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nCentos 7.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo\r\n- TensorFlow installed from:\r\nbinary cpu tf 1.10 (and tried binary 1.9 too)\r\n- TensorFlow version:\r\n1.10 and 1.9\r\n- Python version:\r\n3.6\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n- Exact command to reproduce:\r\nN/A\r\n- Mobile device:\r\nNo", "@Damon-wyg would you list the relevant lines from your source, and supply the stack trace, please?", "```\r\nimport tensorflow as tf  \r\n\r\n...   \r\nrun_config = tf.estimator.RunConfig(train_distribute=dist, **CONF.runconfig).replace(\r\n        session_config=tf.ConfigProto(device_count={'GPU': 1, 'CPU': 1},\r\n                                      session_inter_op_thread_pool=**tf.ThreadPoolOptionProto(num_threads=xx)**\r\n                                      inter_op_parallelism_threads=8,\r\n                                      intra_op_parallelism_threads=8,\r\n                                      allow_soft_placement=True))\r\n...\r\n```\r\nwhile, an ERROR throw:\r\nmodel 'tensorflow' has no attrbute 'ThreadPoolOptionProto'.", "If you would like to get ThreadPoolOptionProto, here is the code:\r\n```python\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.core.protobuf import config_pb2 \r\nconfig = tf.ConfigProto(\r\n            allow_soft_placement=True,\r\n            intra_op_parallelism_threads=1,\r\n            inter_op_parallelism_threads=1,\r\n            isolate_session_state=True,\r\n            session_inter_op_thread_pool=[config_pb2.ThreadPoolOptionProto(num_threads=1)])\r\n```\r\n        \r\n\r\n"]}, {"number": 21796, "title": "int() argument must be a string, a bytes-like object or a number at InceptionResNetV2", "body": "### System information\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): window10 and jupyter\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone X\r\n- TensorFlow installed from (source or binary): source \r\n- **TensorFlow version (use command below)**: 1.9\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.1 / 7.1\r\n- **GPU model and memory**: \r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\ni used tensorflow1.9 function `tf.keras.applications.InceptionResNetV2` and follow the tensorflow document from [here](https://tensorflow.google.cn/versions/r1.9/api_docs/python/tf/keras/applications/InceptionResNetV2?authuser=2&hl=vi) ,but can't used on the tensorflow  [example code,](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/image_captioning_with_attention.ipynb)\r\nit will be print error`int() argument must be a string, a bytes-like object or a number, not'TensorShape'`but others Library (vgg16,19 & inception_v3..and so on) can work!\r\n \r\n### Source code / \r\ninput  image-----\r\n`def load_image(image_path):`\r\n    `img = tf.read_file(image_path)`\r\n   ` img = tf.image.decode_jpeg(img, channels=3)`\r\n   ` img = tf.image.resize_images(img, (299, 299))`\r\n    `img = tf.keras.applications.inception_resnet_v2.preprocess_input(img)`\r\n   ` return img, image_path`\r\n\r\ncreate mode -----\r\n`image_model =tf.keras.applications.inception_resnet_v2.InceptionResNetV2(include_top=False, \r\n                                                                                                                        weights='imagenet')`\r\n\r\n\r\nthe error -------\r\n`TypeError                                 Traceback (most recent call last)\r\n<ipython-input-8-fe73201476b6> in <module>()\r\n      1 startTime=time.time()\r\n----> 2 image_model =tf.keras.applications.inception_resnet_v2.InceptionResNetV2(include_top=False, weights='imagenet')\r\n      3 \r\n      4 #image_model = inception_v4.create_model(weights='imagenet', include_top=True)\r\n      5 new_input = image_model.input\r\n\r\nD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\applications\\inception_resnet_v2.py in InceptionResNetV2(include_top, weights, input_tensor, input_shape, pooling, classes)\r\n    304   for block_idx in range(1, 11):\r\n    305     x = inception_resnet_block(\r\n--> 306         x, scale=0.17, block_type='block35', block_idx=block_idx)\r\n    307 \r\n    308   # Mixed 6a (Reduction-A block): 17 x 17 x 1088\r\n\r\nD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\applications\\inception_resnet_v2.py in inception_resnet_block(x, scale, block_type, block_idx, activation)\r\n    187       output_shape=K.int_shape(x)[1:],\r\n    188       arguments={'scale': scale},\r\n--> 189       name=block_name)([x, up])\r\n    190   if activation is not None:\r\n    191     x = Activation(activation, name=block_name + '_ac')(x)\r\n\r\nD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    712           input_shapes = nest.map_structure(lambda x: x.get_shape(), inputs)\r\n    713 \r\n--> 714         output_shapes = self.compute_output_shape(input_shapes)\r\n    715         output_shapes = nest.flatten(output_shapes)\r\n    716         outputs = [\r\n\r\nD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py in compute_output_shape(self, input_shape)\r\n    675 \r\n    676   def compute_output_shape(self, input_shape):\r\n--> 677     input_shape = tuple(tensor_shape.TensorShape(input_shape).as_list())\r\n    678 \r\n    679     if self._output_shape is None:\r\n\r\nD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py in __init__(self, dims)\r\n    539       else:\r\n    540         # Got a list of dimensions\r\n--> 541         self._dims = [as_dimension(d) for d in dims_iter]\r\n    542     self._ndims = None\r\n    543 \r\n\r\nD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py in <listcomp>(.0)\r\n    539       else:\r\n    540         # Got a list of dimensions\r\n--> 541         self._dims = [as_dimension(d) for d in dims_iter]\r\n    542     self._ndims = None\r\n    543 \r\n\r\nD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py in as_dimension(value)\r\n    480     return value\r\n    481   else:\r\n--> 482     return Dimension(value)\r\n    483 \r\n    484 \r\n\r\nD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py in __init__(self, value)\r\n     35       raise TypeError(\"Cannot convert %s to Dimension\" % value)\r\n     36     else:\r\n---> 37       self._value = int(value)\r\n     38       if (not isinstance(value, compat.bytes_or_text_types) and\r\n     39           self._value != value):\r\n\r\nTypeError: int() argument must be a string, a bytes-like object or a number, not 'TensorShape'`", "comments": ["How is the image connected to the code? What's the stack trace?", "\r\nthe error -------\r\n`TypeError                                 Traceback (most recent call last)\r\n<ipython-input-8-fe73201476b6> in <module>()\r\n      1 startTime=time.time()\r\n----> 2 image_model =tf.keras.applications.inception_resnet_v2.InceptionResNetV2(include_top=False, weights='imagenet')\r\n      3 \r\n      4 #image_model = inception_v4.create_model(weights='imagenet', include_top=True)\r\n      5 new_input = image_model.input\r\n\r\nD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\applications\\inception_resnet_v2.py in InceptionResNetV2(include_top, weights, input_tensor, input_shape, pooling, classes)\r\n    304   for block_idx in range(1, 11):\r\n    305     x = inception_resnet_block(\r\n--> 306         x, scale=0.17, block_type='block35', block_idx=block_idx)\r\n    307 \r\n    308   # Mixed 6a (Reduction-A block): 17 x 17 x 1088\r\n\r\nD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\applications\\inception_resnet_v2.py in inception_resnet_block(x, scale, block_type, block_idx, activation)\r\n    187       output_shape=K.int_shape(x)[1:],\r\n    188       arguments={'scale': scale},\r\n--> 189       name=block_name)([x, up])\r\n    190   if activation is not None:\r\n    191     x = Activation(activation, name=block_name + '_ac')(x)\r\n\r\nD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    712           input_shapes = nest.map_structure(lambda x: x.get_shape(), inputs)\r\n    713 \r\n--> 714         output_shapes = self.compute_output_shape(input_shapes)\r\n    715         output_shapes = nest.flatten(output_shapes)\r\n    716         outputs = [\r\n\r\nD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py in compute_output_shape(self, input_shape)\r\n    675 \r\n    676   def compute_output_shape(self, input_shape):\r\n--> 677     input_shape = tuple(tensor_shape.TensorShape(input_shape).as_list())\r\n    678 \r\n    679     if self._output_shape is None:\r\n\r\nD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py in __init__(self, dims)\r\n    539       else:\r\n    540         # Got a list of dimensions\r\n--> 541         self._dims = [as_dimension(d) for d in dims_iter]\r\n    542     self._ndims = None\r\n    543 \r\n\r\nD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py in <listcomp>(.0)\r\n    539       else:\r\n    540         # Got a list of dimensions\r\n--> 541         self._dims = [as_dimension(d) for d in dims_iter]\r\n    542     self._ndims = None\r\n    543 \r\n\r\nD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py in as_dimension(value)\r\n    480     return value\r\n    481   else:\r\n--> 482     return Dimension(value)\r\n    483 \r\n    484 \r\n\r\nD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py in __init__(self, value)\r\n     35       raise TypeError(\"Cannot convert %s to Dimension\" % value)\r\n     36     else:\r\n---> 37       self._value = int(value)\r\n     38       if (not isinstance(value, compat.bytes_or_text_types) and\r\n     39           self._value != value):\r\n\r\nTypeError: int() argument must be a string, a bytes-like object or a number, not 'TensorShape'`", "Nagging Assignee @drpngx: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Did you follow the documentation exactly?", "yes, I follow the document. and only inception_resnet_v2 will fail.\r\nthe other model (ex vgg16 ,inceptionV3) can work at same documentation ", "OK, then it looks like it might be a documentation issue.", "@Q82822 I tried with 1.10.1 and the code works fine. Could you retry with 1.10.1 and see if the issue still exists?", "yes. it still has.  can you share your code?\r\nmy error still is  ` int() argument must be a string, a bytes-like object or a number, not 'TensorShape'`", "I am using 1.10.1:\r\n```\r\n# python\r\nPython 2.7.12 (default, Dec  4 2017, 14:50:18) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> image_model =tf.keras.applications.inception_resnet_v2.InceptionResNetV2(include_top=False, weights='imagenet')\r\n2018-09-24 15:25:05.103588: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n>>> tf.VERSION\r\n'1.10.1'\r\n>>> \r\n```", "It looks to me like InceptionResNetV2 was broken in 1.9 and 1.10.\r\n\r\nIt appears to be fixed in 1.11.\r\n1.11rc1 is currently the default in colab, try it and reopen this if it doesn't work.", "it is fix my problem \r\nthanks!!"]}, {"number": 21795, "title": "R0.7", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 21794, "title": "Mapping data directories using -v flag gives empty directories in the container", "body": "Running the docker using docker run ... -v /dir:/dir \r\ngives the correct dir name in the container but without any data in it\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 30 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of response. If this is still an issue please reopen and provide all the information that the issue template  (https://github.com/tensorflow/tensorflow/issues/new) asks for.\r\nThank you."]}]