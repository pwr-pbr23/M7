[{"number": 18799, "title": "1.8r Cherrypick request-cherrypicks_30740: Fix for dropped metrics in evaluate function for Keras models.", "body": "This CL should be cherrypicked since it fixes the following:\r\n- Fixes metric name to metric value dictionary creation with the right 1:1 mapping.\r\n- The evaluate function dropped metrics since metric names were not set in compile or during evaluate. \r\n\r\n", "comments": ["@anj-s I needed to resolve a conflict in training_eager.py after merging #18747.  PTAL, thanks!"]}, {"number": 18798, "title": "Changing output/reaction at the moment of detection", "body": "  **Have I written custom code (as opposed to using a stock example script provided in TensorFlow):** No, I've followed a tutorial and haven't touched the base setup of it. https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10\r\n    **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** Windows 10\r\n    **TensorFlow installed from (source or binary):** Source\r\n    **TensorFlow version (use command below):** 1.5.0\r\n    **Python version:** 3.6\r\n    **Bazel version (if compiling from source):** Couldn't get the capture script to return this\r\n    **GCC/Compiler version (if compiling from source):** b'unknown'\r\n    **CUDA/cuDNN version:** 9.0 / 7.05\r\n    **GPU model and memory:** Nvidia GeForce GTX 960M / 2Gb?\r\n    **Exact command to reproduce:** No idea\r\n\r\n------------------------\r\n\r\n### Describe the problem\r\n\r\nHello TensorFlow and everyone else,\r\n\r\nI was wondering where in the code we can change the output or rather, the reaction the framework has on a detected item. Quick notice: I have no prior Python experience so it's a bit difficult to go around and deduce code.\r\n\r\nFor simplicity, let's say, at the moment Tensor detects an object, i'd like to create a file. I've been looking around but can not find a structure or UML-diagram saying which code leads to which, so I have no solid idea where to begin. I'd be very greatful if any directions could be given.\r\n\r\nThanks in advance\r\n\r\n### Source code / logs\r\n\r\nN/A for this question", "comments": ["Hi @Jart, anybody else,\r\n\r\nI noticed this post has been pushed back to page 2 without reaction (understandable because you're certainly very busy). Would it however please be possible to get some directions towards the correct part of the documentation, or, if possible a more direct answer?\r\n\r\nThanks in advance!", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 18797, "title": "Swtich to use axis instead of squeeze_dims in tf.squeeze", "body": "This PR is to change `squeeze_dims` to `axis` in `tf.squeeze` since the former one is deprecated now.\r\n\r\nThis fix switches from squeeze_dims to axis to remove below warnings according to [array_ops.py#L2579](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py#L2579).\r\n> @deprecation.deprecated_args(None, \"Use the `axis` argument instead\",\r\n>                              \"squeeze_dims\")", "comments": ["Hi @imsheridan ,\r\nWe are working on identifying redundant development and duplicate pull requests. We have found there is a pull request: https://github.com/tensorflow/tensorflow/pull/19227 which might be duplicate to this one and already merged into main stream. So maybe this pull request should be modified or closed.\r\nWe would really appreciate if you could help us to validate and give us some feedback. Thank you very much for your time!\r\n", "@imsheridan could you please rebase your branch and submit.", "Created new patch in this  [26219](https://github.com/tensorflow/tensorflow/pull/26219)"]}, {"number": 18796, "title": "Ensure Java Session closes the JNI on finalize", "body": "* Currently the JNI Session object does not close\r\nthe native library when the JVM deallocates\r\n* This ensures that when the JVM deallocates this\r\nobject from the system it also closes the native\r\ncode correctly\r\n\r\nObviously the user should call close explicitly or use try blocks, but this catches the cases where they don't and can prevent massive memory leaks on services.", "comments": ["If people think this is a good change I can make it on all the native objects", "Thanks for the contribution.\r\n\r\nWe have debated this before, and while I admit that the decision isn't set in stone, the feeling was that the finalizer should be avoided in this case. There is some discussion in the [Effective Java book](http://www.informit.com/articles/article.aspx?p=1216151&seqNum=7). In particular, since the native peers of these classes can hold on to a significant chunk of resources (e.g., large amounts of memory) - encouraging cleanup on the finalizer may seem convenient but actually makes it harder to reason about and debug the memory footprint of a program (for example, if the memory footprint goes up and down as the GC runs, making it hard to associate with the code that is missing the `close()` calls).\r\n\r\nSo I'd suggest that we do not merge this PR, but I say so with the humility that I could be wrong :)", "If the consensus is not to cleanup on finalize, perhaps a better situation might be an assertion that the object has been closed on finalize? I do think something has to be done to ensure that native components and their Java wrappers don't go out of sync and cause hard to debug memory leaks.", "@8W9aG : Won't adding the logging or assertion still have the performance overheads of finalizers mentioned in the book? For a `Graph` or `Session` that may still be okay (not sure) since they are generally not created and destroyed frequently, but doing this on a `Tensor` may start becoming onerous. And if we do it selectively on some objects but not the others, I wonder if that may make it more confusing.\r\n\r\nCurrently the consistent story across all classes in the `org.tensorflow` package is that if they implement the `AutoCloseable` interface, one should invoke `close()`. ", "@asimshankar I take your point, I don't know the performance implications of doing this on Tensors for example. I will fall back to thinking that this technique should be applied for Graph or Session though, having said that if consistency is king then I can close this PR.", "@8W9aG : Thanks for your understanding. Let's close this for now, with the understanding that it may make sense to revisit this in the future (e.g., if there is a lot of compelling feedback around this). Thanks!"]}, {"number": 18795, "title": "The net using while_loop with batch_normalization can't train", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:\r\nv1.7.0-18-g92e6c3e 1.7.0\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**:0.11.1\r\n- **GCC/Compiler version (if compiling from source)**:5.4\r\n- **CUDA/cuDNN version**:9.0\r\n- **GPU model and memory**:nvidia 1080 titian\r\n\r\n### Describe the problem\r\nI need add a batch_normalization layer in while_loop body, but it breaks down when i training the net. Everything is OK if i remove x = tf.layers.batch_normalization(x, training=flag)\r\n\r\n### Source code / logs\r\nThis a simple example\r\n```\r\nimport tensorflow as tf\r\nfrom data_pre import get_data\r\n\r\ndata, labels = get_data(\r\n    ['../UCR_TS_Archive_2015/ItalyPowerDemand/ItalyPowerDemand_TRAIN'], 24, 2,True, 0, 2)  #pylint: disable=line-too-long\r\n\r\nflag = True\r\n\r\ndef cond(i, x):\r\n    return i < 1\r\n\r\ndef body(i, x):\r\n    x = tf.layers.conv1d(x, 1, 7, padding='same')\r\n    x = tf.layers.batch_normalization(x, training=flag)\r\n    x = tf.nn.relu(x)\r\n    return i + 1, x\r\n\r\n_, y = tf.while_loop(cond, body, [0, data], back_prop=False)\r\n\r\ny = tf.layers.flatten(y)\r\nlogits = tf.layers.dense(y, 2)\r\n\r\nloss = tf.losses.mean_squared_error(labels, logits)\r\noptimizer = tf.train.AdamOptimizer()\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):\r\n    train_op = optimizer.minimize(loss, tf.train.get_global_step())\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    coord = tf.train.Coordinator()\r\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n    for _ in range(10):\r\n        sess.run(train_op)\r\n    coord.request_stop()\r\n    coord.join(threads)\r\n```\r\nThis is the error info\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1327, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1312, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1420, in _call_tf_sessionrun\r\n    status, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\", line 516, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: The node 'gradients/mean_squared_error/div_grad/Neg' has inputs from different frames. The input 'while/batch_normalization/AssignMovingAvg_1' is in frame 'while/while_context'. The input 'one_hot' is in frame ''.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"./test.py\", line 40, in <module>\r\n    sess.run(train_op)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 905, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1140, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: The node 'gradients/mean_squared_error/div_grad/Neg' has inputs from different frames. The input 'while/batch_normalization/AssignMovingAvg_1' is in frame 'while/while_context'. The input 'one_hot' is in frame ''.\r\n```\r\n\r\n", "comments": ["Could you try the workaround in https://github.com/tensorflow/tensorflow/issues/14699#issuecomment-356815645 and see if it works? There is a feature request for this in #14809 but received no responses.", "@ppwwyyxx I have tried it like this:\r\n```\r\ndef body(i, x):\r\n     with tf.control_dependencies(None):\r\n         x = tf.layers.conv1d(x, 1, 7, padding='same')\r\n         x = tf.layers.batch_normalization(x, training=flag)\r\n         x = tf.nn.relu(x)\r\n     return i + 1, x\r\n```\r\nit seems can't be used  in a loop\r\n`ValueError: Cannot use 'while/Identity_1' as input to 'while/conv1d/conv1d/ExpandDims' because 'while/Identity_1' is in a while loop. See info log for more details.`\r\nI alse tried contrib API and it has the same result.", "The following code can run without issues:\r\n```python\r\nimport tensorflow as tf\r\n\r\ndata = tf.random_normal(shape=[64, 10, 1])\r\nlabels = tf.random_normal(shape=[64, 2])\r\n\r\nflag = True\r\n\r\ndef cond(i, x):\r\n    return i < 1\r\n\r\ndef body(i, x):\r\n    x = tf.layers.conv1d(x, 1, 7, padding='same')\r\n    x = tf.contrib.layers.batch_norm(x, is_training=flag, updates_collections=None)\r\n    x = tf.nn.relu(x)\r\n    return i + 1, x\r\n\r\n_, y = tf.while_loop(cond, body, [0, data], back_prop=False)\r\n\r\ny = tf.layers.flatten(y)\r\nlogits = tf.layers.dense(y, 2)\r\n\r\nloss = tf.losses.mean_squared_error(labels, logits)\r\noptimizer = tf.train.AdamOptimizer()\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nassert len(update_ops) == 0\r\ntrain_op = optimizer.minimize(loss, tf.train.get_global_step())\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run(train_op)\r\n```\r\nSo it's unclear what do you mean by \"tried contrib API and it has the same result\".", "@ppwwyyxx Oh, i'm sorry for not noticing the difference between the contrib API and formal API. You are right it works fine. Thanks for your help\uff01(\u3003'\u25bd'\u3003)"]}, {"number": 18794, "title": "Keyword Extraction from a text followed by a key value using tensorflow", "body": "Hi \r\n\r\nI have a pdf file that contains information. I would like to extract few key terms/phrase along with a value for example (current balance : CHF (swiss francs) 1,000)\r\n\r\nI can convert pdf file to text using pdfminer . But how i can extract the above keyword using tensorflow text classification or other methods. I don't want to use rake, TF-IDF.\r\n\r\nCan anyone suggest how i can start with it? I haven't came across a single example with tensorflow. There is a question on stack overflow. but this doesn't help me much as I am a beginner.\r\n\r\nhttps://datascience.stackexchange.com/questions/10077/keyword-phrase-extraction-from-text-using-deep-learning-libraries", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 16 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 16 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 18793, "title": "Emphasis *any* `Estimator`", "body": "", "comments": []}, {"number": 18792, "title": "change to reference variable", "body": "use reference variable to make sure keypoints is updated.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Nagging Assignee @protoget: It has been 65 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 18791, "title": "tf-nightly-gpu wheels are no more deployed for python 3.6 since 2018/03/30", "body": "The tensorflow GPU wheels (https://pypi.org/project/tf-nightly-gpu) for python 3.6 are no more deployed since 2018/30/30, as shown by:\r\n\r\n```\r\npip3.6 install tf-nightly-gpu==\r\n```\r\n\r\nHave I written custom code: N/A\r\nOS Platform and Distribution: N/A\r\nTensorFlow installed from: N/A\r\nTensorFlow version: N/A\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Same goes for windows builds. They are not available for 1.9 and some 1.8 builds.", "Is this a jenkins issue or why are the windows build not there anymore?", "I just ran into the same issue. Very confusing.", "This is important for me. Is there any news on this Tensorflow Team?\r\nSame goes for cpu builds btw. ", "There is a problem also with the py3 related docker images:\r\nhttps://hub.docker.com/r/tensorflow/tensorflow/tags/", "/cc @av8ramit", "Thanks for filing. I've fixed the python 3.6 build for tf-nightly-gpu.", "I believe the nightly docker py3 images are fixed as well.", "Can we close this?", "Windows build fix is in flight. Closing this issue for now."]}, {"number": 18790, "title": "Tensorflow object detection API", "body": "Traceback (most recent call last):\r\n  File \"export_inference_graph.py\", line 147, in <module>\r\n    tf.app.run()\r\n  File \"C:\\Users\\Ali Salar\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"export_inference_graph.py\", line 143, in main\r\n    FLAGS.output_directory, input_shape)\r\n  File \"C:\\tensorflow2\\models\\research\\object_detection\\exporter.py\", line 453, in export_inference_graph\r\n    graph_hook_fn=None)\r\n  File \"C:\\tensorflow2\\models\\research\\object_detection\\exporter.py\", line 421, in _export_inference_graph\r\n    placeholder_tensor, outputs)\r\n  File \"C:\\tensorflow2\\models\\research\\object_detection\\exporter.py\", line 280, in write_saved_model\r\n    builder = tf.saved_model.builder.SavedModelBuilder(saved_model_path)\r\n  File \"C:\\Users\\Ali Salar\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\saved_model\\builder_impl.py\", line 90, in __init__\r\n    \"directory: %s\" % export_dir)\r\nAssertionError: Export directory already exists. Please specify a different export directory: inference_graph\\saved_model", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "As the error message indicates, the output directory you specified already exists. You should either 1) change the output directory; or 2) try to backup and delete the already-exist \"saved-model\" folder, then run it again", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "@zedyao thanks for responding on my request. Sir i am new in this field soo kindly guide me how i will change the output directory. \r\nWaiting for your response ", "This might be helpful :\r\nhttps://www.youtube.com/watch?v=Rgpfk6eYxJA"]}, {"number": 18789, "title": "can 1*1 kernel conv2d optimized using neon? the source code treat 1*1 kernel conv2d as mat-mul, but did not use neon optimization", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Nagging Assignee @drpngx: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "CC @ekelsen ", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Not every possible shape optimization is currently supported. There is a tradeoff of binary size vs spot optimizations for particular sizes. What use case are you targeting? @bjacob, do you have any further comment?\r\n\r\n\r\n \r\n", "The 1x1 kernel case is by far the most important case of Conv. For example, in MobileNets, all Conv nodes have 1x1 kernels.\r\n\r\nThe 1x1 kernel case of Conv is the case that is, literally, a matrix multiplication, i.e. is not at all 'convolutional'  (so the whole terminology here is misleading).  Other cases can be reduced to matrix-multiplication with some work, but that 1x1 case does not even require any such reduction work, it is literaly a mat-mul.\r\n\r\nTensorFlow Lite uses specialist libraries to provide the matrix multiplication implementation:\r\n - in floating-point inference, the Eigen library is used:\r\nhttps://github.com/tensorflow/tensorflow/blob/85aae3795775bf648d2e8baa56331f952d12e3e0/tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h#L1996\r\n  - in quantized inference, the gemmlowp library is used:\r\nhttps://github.com/tensorflow/tensorflow/blob/85aae3795775bf648d2e8baa56331f952d12e3e0/tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h#L2122\r\n\r\nBoth of these libraries have NEON-optimized paths.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 18788, "title": "import tensorflow.contrb.eager throws undefined symbol", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Centos\r\n- **TensorFlow installed from (source or binary)**:  source\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**: 3.5.3\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**: 4.8.5\r\n- **CUDA/cuDNN version**: cuda - 9.0, cudnn - 7.0\r\n- **GPU model and memory**: GRID K520, memory -4036MiB\r\n- **Exact command to reproduce**:\r\n\r\nI am trying to import tensorflow.contrib.eager but I get the following error -\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/lib/python/lib/python3.5/site-packages/tensorflow/contrib/__init__.py\", line 60, in <module>\r\n    from tensorflow.contrib import nccl\r\n  File \"/usr/lib/python/lib/python3.5/site-packages/tensorflow/contrib/nccl/__init__.py\", line 30, in <module>\r\n    from tensorflow.contrib.nccl.python.ops.nccl_ops import all_max\r\n  File \"/usr/lib/python/lib/python3.5/site-packages/tensorflow/contrib/nccl/python/ops/nccl_ops.py\", line 30, in <module>\r\n    resource_loader.get_path_to_datafile('_nccl_ops.so'))\r\n  File \"/usr/lib/python/lib/python3.5/site-packages/tensorflow/contrib/util/loader.py\", line 56, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"/usr/lib/python/lib/python3.5/site-packages/tensorflow/python/framework/load_library.py\", line 58, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename, status)\r\n  File \"/usr/lib/python/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 516, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: /usr/lib//python/lib/python3.5/site-packages/tensorflow/contrib/nccl/python/ops/_nccl_ops.so: undefined symbol: _ZN9perftools8gputools4cuda29ScopedActivateExecutorContextC1EPNS0_14StreamExecutorE\r\n```", "comments": []}, {"number": 18787, "title": "Default arguments of tensorflow.contrib.signal.inverse_stft do not invert tensorflow.contrib.signal.stft", "body": "This issue is a suggestion to modify `tensorflow.contrib.signal.inverse_stft` such that the default arguments invert the `stft`\r\n\r\n## Background\r\nCurrently the correct call to `inverse_stft` is:\r\n```python\r\n... = tf.contrib.signal.inverse_stft(\r\n    stfts=...,\r\n    frame_length=frame_length,\r\n    frame_step=frame_step,\r\n    # forward_window_fn\r\n    window_fn=tf.contrib.signal.inverse_stft_window_fn(\r\n        frame_step=frame_step,\r\n        forward_window_fn=functools.partial(tf.contrib.signal.hann_window, periodic=True),\r\n    )\r\n)\r\n```\r\nbut the default value for `window_fn` is `functools.partial(tf.contrib.signal.hann_window, periodic=True)` so the call to `inverse_stft_window_fn` is missing, because the `frame_step` is unknown for the default argument.\r\n\r\nNote: Because the hann window has some special properties, it can happen, that the inverse stft only introduces an amplitude error.\r\n\r\n## Suggestion\r\nIntroduce `forward_window_fn` in `tensorflow.contrib.signal.inverse_stft` and change the defaults to `window_fn=None, forward_window_fn=functools.partial(tf.contrib.signal.hann_window, periodic=True)`.\r\nThe code inside would then be:\r\n```python\r\nif window_fn is None:\r\n    window_fn = tf.contrib.signal.inverse_stft_window_fn(frame_step, forward_window_fn)\r\n```\r\n\r\n## Example code (Demonstration)\r\n\r\n```python\r\nimport functools\r\nimport tensorflow as tf\r\nimport tensorflow.contrib\r\nimport matplotlib.pyplot as plt\r\n\r\nframe_length = 32\r\nframe_step = 16\r\n\r\nx = tf.placeholder(tf.float32)\r\nX = tf.contrib.signal.stft(\r\n    x,\r\n    frame_length=frame_length,\r\n    frame_step=frame_step,\r\n)\r\nx_hat = tf.contrib.signal.inverse_stft(\r\n    X,\r\n    frame_length=frame_length,\r\n    frame_step=frame_step,\r\n)\r\nx_hat_2 = tf.contrib.signal.inverse_stft(\r\n    X,\r\n    frame_length=frame_length,\r\n    frame_step=frame_step,\r\n    # forward_window_fn\r\n    window_fn=tf.contrib.signal.inverse_stft_window_fn(\r\n        frame_step,\r\n        functools.partial(tf.contrib.signal.hann_window, periodic=True),\r\n        \r\n    )\r\n)\r\n\r\ndef normalize(x):\r\n    return x / (-np.min(x))\r\n\r\nwith tf.Session() as sess:\r\n    t = np.linspace(0,10,128)\r\n    x_np = np.sin(t)\r\n    x_hat_np, x_hat_2_np = sess.run([x_hat, x_hat_2], {x: x_np})\r\n  \r\n    plt.plot(t, x_np, label='orig')\r\n    plt.plot(t, x_hat_np, label='default')\r\n    plt.plot(t, x_hat_2_np, label='fixed')\r\n#     plt.plot(t, normalize(x_np), label='orig', linewidth=8)\r\n#     plt.plot(t, normalize(x_hat_np), label='default', linewidth=6)\r\n#     plt.plot(t, normalize(x_hat_2_np), label='fixed', linewidth=4)\r\n    plt.legend()\r\n\r\n```\r\n\r\n`frame_step == 8`:\r\n![frame_step8](https://user-images.githubusercontent.com/13744128/39106793-5e78d87a-46be-11e8-9e88-d0c101f4b8de.png)\r\n\r\n`frame_step == 16`:\r\n![frame_step16](https://user-images.githubusercontent.com/13744128/39106796-61da9b52-46be-11e8-8c1f-de64c10e4f2d.png)\r\n\r\nWhile the example with `frame_step == 8` introduces \"only\" a scaling error, the `frame_step == 16` example introduces distortions.\r\n\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: Name: tf-nightly\r\nVersion: 1.8.0.dev20180331\r\nSummary: TensorFlow helps the tensors flow\r\nHome-page: https://www.tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: opensource@google.com\r\nLicense: Apache 2.0\r\nLocation: /opt/anaconda/lib/python3.6/site-packages\r\nRequires: grpcio, astor, gast, termcolor, six, absl-py, numpy, wheel, protobuf, tb-nightly\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n", "comments": ["@boeddeker Is this issue fixed or still blocking you?", "I am using a wrapper around the tensorflow stft. So in my code it is working.\r\nNevertheless, it is annoying, when tensorlfow provides a stft and istft and you have to provide optional arguments to get the behavior correct.\r\n\r\nAs I mentioned in my first post, I would suggest introducing a forward_window_fn to the istft to be able to give it a proper default.", "I like your suggestion @boeddeker! It would be nice if the defaults were better. I'm planning to make a bunch of tf.signal improvements soon so hopefully I'll get to this.", "@boeddeker It seems you are using older versions(1.x versions) of Tensorflow which is not actively supported. Since contrib has been depreciated in Tensorflow 2.x ,Please do upgrade to a latest Tensorflow version.Attaching [migration](https://www.tensorflow.org/guide/migrate) guide for reference. Thanks!"]}, {"number": 18786, "title": "i have error in my code \"cannot import name 'label_map_util' , is there way to solve this issue ? ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Please provide all the information asked above. Also request you to provide a reproducible code snippet to generate the error from our end. \r\nYou can check the version of utils you used and please go through [this](https://stackoverflow.com/questions/51569669/python-from-utils-import-label-map-util-importerror-cannot-import-name-lab/51569973) link which address a similar issue.", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 18785, "title": "pywrap_tensorflow_internal cannot load/not found - Windows tensorflow CPU", "body": "I have finished attempting installations of tensorflow CPU for Windows using Python3.5 and 3.6. I've tried installing a wheel placed in the Python directory. \r\nI tried installing from the web vis a vis Pip. \r\nI've tried adding paths to PYTHONPATH and PATH (Windows).\r\n I've done everything but install in a virtual environment using Anaconda. \r\nThe traceback is always the same. \r\nThe pywrap_tensorflow_internal files are in place but do not seem to be visible to the import routines. \r\n(*.pyd, *.py, *.pyl). I placed the pyd DLL in the System32 directory with no change.\r\nI tried using Python 3.6 installed in the root and installing a c36 amd64 wheel and a c35 amd64 wheel from Python 3.5 in C:/User/nnnnn/AppData/Local---etc.. \r\nI'm using Win 10 Pro x64 on a laptop with Intel Core 2 Duo processor. No Nvidia GPU card. \r\nEverything installs and seems to run, including Swig, but still the install fails when I run \">>>import tensorflow as tf\" from a python command line session.\r\nI haven't tried the Bazel or Anaconda approaches since I'm using Tensorflow with Tensorlayer and SRGAN for image recognition. I don't know if the virtual environment will be compatible yet.\r\nThe output from the attempts follows:\r\n\r\n\">>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\jimjulian\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\jimjulian\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 903, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\jimjulian\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\jimjulian\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\jimjulian\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\jimjulian\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\jimjulian\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *  # pylint: disable=redefined-builtin\r\n  File \"C:\\Users\\jimjulian\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\jimjulian\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\jimjulian\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\jimjulian\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 903, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\jimjulian\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\jimjulian\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\jimjulian\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\jimjulian\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>>\"\r\n\r\nAny suggestions will be appreciated.\r\n", "comments": ["Which version of TensorFlow are you trying to install? Try installing 1.5 or 1.4.\r\n\r\n1.6 & later does not seem to work on Windows CPU well.", "Thanks Vijay. I got 1.5.1 going. The install needed an internet connection to find a compatible tensorboard, but after that, ,everything went smoothly.", "I'm curious if this issue with Windows CPU version is being looked on in any other open issues ? The TFLite  does not seem to supported in version <1.7 which means that I'm limited to not using TFLite for any mobile models."]}, {"number": 18784, "title": "Why is reading a CSV file with a TextLineDataset and decode_csv so slow?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**: 3.6.0\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.0.176/7.0.7\r\n- **GPU model and memory**: Titan X (12 GB)\r\n- **Exact command to reproduce**: See below\r\n\r\nI have a dataset saved on an SSD as a moderately large (~7 million line) .csv file. It's small enough to load into memory, but large enough that it takes a couple minutes to read the entire file into a NumPy array, and I don't always need the entire thing. This seemed like a perfect use case for tf.data.TextLineDataset.\r\n\r\nI attempted to use it similarly to as demonstrated in the documentation with the tf.decode_csv and tf.data.Dataset.map functions, as shown in the code below, but I'm finding it to be unreasonably slow to fetch batches from the TextLineDataset. Of course I'd expect it to be slower than if the data has already been read into memory. But if I compare the total time to read the entire thing into a NumPy array in memory first and process it as a TensorSliceDataset vs. creating a TextLineDataset and reading batches that way, the former is many times faster. \r\n\r\nAm I missing something, or is this an issue with TextLineDataset and/or tf.decode_csv?\r\n\r\n\r\n```\r\ndef make_tld(csv_filename, header_lines, delim, batch_size):\r\n    dataset = tf.data.TextLineDataset(filenames=csv_filename).skip(header_lines)\r\n\r\n    def parse_csv(line):\r\n        cols_types = [[]] * num_cols_  # all required\r\n        columns = tf.decode_csv(line, record_defaults=cols_types, field_delim=delim)\r\n        return tf.stack(columns)\r\n\r\n    dataset = dataset.map(parse_csv).batch(batch_size)\r\n    return dataset\r\n\r\n\r\ndef make_tsd(csv_filename, header_lines, delim, batch_size):\r\n    with open(csv_filename, \"r\") as f:\r\n        lines = f.readlines()\r\n\r\n    data_shape = (len(lines) - header_lines, len(lines[header_lines].strip().split(delim)))\r\n    data = np.empty(shape=data_shape, dtype=np.float32)\r\n\r\n    for idx, line in enumerate(lines[header_lines:]):\r\n        columns = [float(el) for el in line.strip().split(delim)]\r\n        data[idx, :] = np.array(columns)\r\n\r\n    dataset = tf.data.Dataset.from_tensor_slices(data).batch(batch_size)\r\n    return dataset\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    batch_size_ = 100\r\n\r\n    tld_start = datetime.datetime.now()\r\n    tld = make_tld(csv_filename_, header_lines_, delim_, batch_size_)\r\n    tld_next = tld.make_one_shot_iterator().get_next()\r\n    with tf.Session() as tld_sess:\r\n        tld_sess.run(tf.global_variables_initializer())\r\n        try:\r\n            while True:\r\n                tld_out = tld_sess.run(tld_next)\r\n        except tf.errors.OutOfRangeError:\r\n            print(\"Done\")\r\n    tld_end = datetime.datetime.now()\r\n    print(\"TextLineDataset: \" + str(tld_end - tld_start))\r\n\r\n    tsd_start = datetime.datetime.now()\r\n    tsd = make_tsd(csv_filename_, header_lines_, delim_, batch_size_)\r\n    tsd_next = tsd.make_one_shot_iterator().get_next()\r\n    with tf.Session() as tsd_sess:\r\n        tsd_sess.run(tf.global_variables_initializer())\r\n        try:\r\n            while True:\r\n                tsd_out = tsd_sess.run(tsd_next)\r\n        except tf.errors.OutOfRangeError:\r\n            print(\"Done\")\r\n    tsd_end = datetime.datetime.now()\r\n    print(\"TensorSliceDataset: \" + str(tsd_end - tsd_start))\r\n```\r\n\r\nOutput:\r\n```\r\nDone\r\nTextLineDataset: 0:11:24.675474\r\nDone\r\nTensorSliceDataset: 0:02:12.061404\r\n```", "comments": ["/CC @mrry can you take a look?", "Reassigning this one to @rachellim, who has been making improvements to CSV performance lately (and some that landed recently should help with this, if you'd like to try the nightly build).", "Thanks Derek! \r\n\r\n@recolgan, I'd be interested in knowing how much the recent changes (in the nightly build) help. Specifically, we merged a change that speeds up text -> float parsing significantly. I'm assuming you're reading float data? (assumption based on your defaults being `[[]]`, which tf interprets as float32 if i'm not mistaken) What's `num_cols_` in your case? ", "Yup, it's float data (in E-notation with 10 digits, e.g. 1.273661858e+02, but casting to float32 is fine). num_cols_ is 11. \r\n\r\nWith tf-nightly-gpu from pypi (which says it's version 1.8.0-dev20180330), the results are pretty much the same:\r\n```\r\nDone\r\nTextLineDataset: 0:11:00.005571\r\nDone\r\nTensorSliceDataset: 0:02:11.366419\r\n```", "Can you try `pip install --upgrade tf-nightly-gpu` to pull a later version of tensorflow? Looking at the version number, it seems out of date (20180330 - the changes were only made on 4/20). You should have a version number like `tf-nightly-gpu 1.9.0.dev20180426`", "Weird, I just did the install today, but in a Conda environment, so maybe that had something to do with it. Anyway, with 1.9.0-dev20180426 installed via native pip3, I get the following:\r\n\r\n```\r\nDone\r\nTextLineDataset: 0:06:13.424233\r\nDone\r\nTensorSliceDataset: 0:02:17.663664\r\n```\r\n\r\nSo, much better, but still not as fast as it ought to be...", "Thanks for the update, and for reporting this -- agreed that it's still slower than we would like. This example will be helpful as we work on improving performance for text datasets; I'll update this space as we make progress.", "Oh, one more thing, @recolgan -- can you try applying `batch` before `map`? So the code should look like:\r\n\r\n```\r\ndef make_tld(csv_filename, header_lines, delim, batch_size):\r\n    dataset = tf.data.TextLineDataset(filenames=csv_filename).skip(header_lines)\r\n\r\n    def parse_csv(line):\r\n        cols_types = [[]] * num_cols_  # all required\r\n        columns = tf.decode_csv(line, record_defaults=cols_types, field_delim=delim)\r\n        return tf.stack(columns)\r\n\r\n    dataset = dataset.batch(batch_size).map(parse_csv).\r\n    return dataset\r\n```\r\n\r\nI expect this to speed things up significantly, because the `map` function has significant overhead compared to the actual work of calling `decode_csv` on a small line. In your original example, the map function is applied to each line of the input file. When the data batched first, the map function is applied to a whole batch at a time, i.e. there are 100x fewer `map` operations being run. Let me know how this affects things. \r\n\r\nYou can read more about this optimization in the [tensorflow input pipeline performance guide ](https://www.tensorflow.org/versions/master/performance/datasets_performance) (Look for \"Map and Batch\")\r\n", "Ah, cool. Now (with 1.9.0-dev20180429) we're down to 2:59 for TextLineDataset/map, compared to 1:50 for TensorSliceDataset, and 8:12 with TextLineDataset/map under 1.7.0.\r\n\r\nThanks for the help @rachellim!", "Should also add that on a 12-core workstation adding num_parallel_calls=12 to the map call brings it down to 46 seconds, and .prefetch(1) after that brings it down to 31.", "Nagging Assignee @rachellim: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rachellim: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rachellim: It has been 46 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 18783, "title": "Switch from tf.contrib.metrics to tf.metrics", "body": "Much of the functions in `tf.contrib.metrics` has been deprecated in favor of `tf.metrics`. This fix\r\nswitches the usage of `tf.contrib.metrics` to `tf.metrics`, so that unnecessary warnings could be suppressed.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["This will likely get a LGTM, can you resolve the conflicts so we can then merge?", "@ekelsen @jart The PR has been rebased and pushed. Please take a look.", "Nagging Assignee @protoget: It has been 22 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 18782, "title": "Error in installation Tensorflow , windows ", "body": "Hi, I have installed Cuda and CudaDLl files and also defined paths to the respective folders. I am still getting an error on launching Tesorflow module saying DLL failed. I checked the folders. I think I have all the files in the respective folders. I think I am missing something below is the error screen on importing tensor flow.\r\n\r\n![image](https://user-images.githubusercontent.com/26611229/39100139-273ff188-4653-11e8-9778-435bc47e031b.png)\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hi, I am having a similar problem.\r\nOS - Windows 10 64 bit, \r\nPython- 3.6.5 (All liberaries in path)\r\nTensorflow version 1.7 GPU, Pip installation\r\nBazel not installed\r\nCUDA - 9.0 (All liberaries in path)\r\ncuDNN - 7.0 (All liberaries in path)\r\nGPU - Nvidia Titan X 12 GB\r\nimport tensorflow gives error \r\n\r\nThe error is as follows:\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *  # pylint: disable=redefined-builtin\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "Hi All, \r\nThank you for your time, I forced reinstalled tensorflow via conda and its working fine now. I am  not sure what the issue was. I had correct version of CUDA dll's and path. Below is the code and stack exchange lin.\r\n\r\npip install tensorflow --upgrade --force-reinstall\r\nhttps://stackoverflow.com/questions/35953210/error-running-basic-tensorflow-example\r\n\r\n"]}, {"number": 18781, "title": "tf.variable_scope(auxiliary_name_scope=False) alters name_scope", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary (pip)\r\n- **TensorFlow version (use command below)**: 1.8.0.dev20180329\r\n- **Python version**: 3.6.5\r\n\r\n### Describe the problem\r\nExpected behavior for the following code is to print two lines containing:\r\n```\r\n(<tf.Variable 'vs_outer/var:0' shape=(1,) dtype=float32_ref>, <tf.Tensor 'ns_outer/ns_inner_1/const:0' shape=() dtype=float32>)\r\n(<tf.Variable 'vs_outer/var:0' shape=(1,) dtype=float32_ref>, <tf.Tensor 'ns_outer/ns_inner_2/const:0' shape=() dtype=float32>)\r\n```\r\nWhat is actually printed:\r\n```\r\n(<tf.Variable 'vs_outer/var:0' shape=(1,) dtype=float32_ref>, <tf.Tensor 'ns_outer/ns_inner_1/const:0' shape=() dtype=float32>)\r\n(<tf.Variable 'vs_outer/var:0' shape=(1,) dtype=float32_ref>, <tf.Tensor 'ns_outer/ns_inner_1/const_1:0' shape=() dtype=float32>)\r\n```\r\nSomehow the first use of `with VSO:` memorizes the `name_scope` it is in and restores it the second time it is used even though `auxiliary_name_scope=False` is set and the documentation states: \r\n```\r\nauxiliary_name_scope: If True, we create an auxiliary name scope with the scope. If False, we don't touch name scope.\r\n```\r\n\r\n### Source code\r\n```python\r\nimport tensorflow as tf\r\n\r\nwith tf.Graph().as_default():\r\n  NSO = tf.name_scope('ns_outer').__enter__()\r\n\r\n  VSO = tf.variable_scope(\r\n        'vs_outer', auxiliary_name_scope=False, reuse=tf.AUTO_REUSE)\r\n\r\n  with tf.name_scope(NSO):\r\n    with tf.name_scope('ns_inner_1'):\r\n      with VSO:\r\n        print((tf.get_variable('var', [1]), tf.constant(1.0, name='const')))\r\n\r\n  with tf.name_scope(NSO):\r\n    with tf.name_scope('ns_inner_2'):\r\n      with VSO:\r\n        print((tf.get_variable('var', [1]), tf.constant(1.0, name='const')))\r\n```\r\n\r\nIt works when swapping the `name_scope` `with-block` with the `variable_scope` `with-block`.\r\n```python\r\nimport tensorflow as tf\r\n\r\nwith tf.Graph().as_default():\r\n  NSO = tf.name_scope('ns_outer').__enter__()\r\n\r\n  VSO = tf.variable_scope(\r\n        'vs_outer', auxiliary_name_scope=False, reuse=tf.AUTO_REUSE)\r\n\r\n  with tf.name_scope(NSO):\r\n    with VSO:\r\n      with tf.name_scope('ns_inner_1'):\r\n        print((tf.get_variable('var', [1]), tf.constant(1.0, name='const')))\r\n\r\n  with tf.name_scope(NSO):\r\n    with VSO:\r\n      with tf.name_scope('ns_inner_2'):\r\n        print((tf.get_variable('var', [1]), tf.constant(1.0, name='const')))\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Thank you for reporting the bug. I'm the author of `auxiliary_name_scope`. \r\nCould you delete `reuse=tf.AUTO_REUSE` and take a test again?  If I remember correctly, we didn't test the case of mixing `reuse` with `auxiliary_name_scope`. \r\nI'd like to make an investigation later. ", "Still the same behavior when running:\r\n```python\r\nwith tf.Graph().as_default():\r\n  NSO = tf.name_scope('ns_outer').__enter__()\r\n\r\n  VSO = tf.variable_scope('vs_outer', auxiliary_name_scope=False)\r\n\r\n  with tf.name_scope(NSO):\r\n    with tf.name_scope('ns_inner_1'):\r\n      with VSO:\r\n        print((tf.constant(1.0, name='const')))\r\n\r\n  with tf.name_scope(NSO):\r\n    with tf.name_scope('ns_inner_2'):\r\n      with VSO:\r\n        print((tf.constant(1.0, name='const')))\r\n\r\n```", "@sleighsoft Thank you for feedback. Actually `auxiliary_name_scope` is introduced in #14390 to resolve the name scope conflict of `tf.layers` when reentering variable scope, I modify your codes to make it work:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nwith tf.Graph().as_default():\r\n  NSO = tf.name_scope('ns_outer').__enter__()\r\n\r\n  with tf.variable_scope('vs_outer',\r\n                         auxiliary_name_scope=False,\r\n                         reuse=tf.AUTO_REUSE) as VSO:\r\n    pass\r\n\r\n  with tf.name_scope(NSO):\r\n    with tf.name_scope('ns_inner_1'):\r\n      with tf.variable_scope(VSO, auxiliary_name_scope=False):\r\n        print((tf.get_variable('var', [1]), tf.constant(1.0, name='const')))\r\n\r\n  with tf.name_scope(NSO):\r\n    with tf.name_scope('ns_inner_2'):\r\n      with tf.variable_scope(VSO, auxiliary_name_scope=False):\r\n        print((tf.get_variable('var', [1]), tf.constant(1.0, name='const')))\r\n\r\n# (<tf.Variable 'vs_outer/var:0' shape=(1,) dtype=float32_ref>, <tf.Tensor 'ns_outer/ns_inner_1/const:0' shape=() dtype=float32>)\r\n# (<tf.Variable 'vs_outer/var:0' shape=(1,) dtype=float32_ref>, <tf.Tensor 'ns_outer/ns_inner_2/const:0' shape=() dtype=float32>)\r\n\r\n```\r\n\r\nPerhaps we'd better to document that it is only designed for reentering case, or give an example about how to use it correctly? cc @lukaszkaiser ", "Better docs would be welcome, I think.", "Sure, I'll submit a fix for document later.", "Why do I have to use `with tf.variable_scope(VSO, auxiliary_name_scope=False):` again?\r\nMy point of specifying the `variable_scope` only once is to not specify all the settings again.", "Because `variable_scope` is a context manager, I think it is designed to be used like this. You could check all its re-entering test cases:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/variable_scope_test.py\r\n\r\nIn a word, if you want to re-enter a variable scope, \r\n```python\r\n# correct \r\nwith tf.variable_scope(old_scope):\r\n    # do something\r\n\r\n# might wrong\r\nwith old_scope:\r\n   # do something\r\n```\r\n\r\nPlease correct me if I'm wrong, @lukaszkaiser ", "@sleighsoft  As explained before, the argument is designed for re-entering case at the first: we can reenter variable_scope and its original name scope clearly:\r\n```python\r\nwith tf.variable_scope(s, auxiliary_name_scope=False) as ss:\r\n     with tf.name_scope(ss.original_name_scope) as n:\r\n          # do something\r\n```\r\n\r\nLet's clarify your request: specify the argument only once, like:\r\n```python\r\n  with tf.variable_scope('vs_outer',\r\n                         auxiliary_name_scope=False,\r\n                         reuse=tf.AUTO_REUSE) as VSO:\r\n    pass\r\n\r\n  with tf.name_scope(NSO):\r\n    with tf.name_scope('ns_inner_1'):\r\n      with tf.variable_scope(VSO):\r\n        print((tf.get_variable('var', [1]), tf.constant(1.0, name='const')))\r\n\r\n  with tf.name_scope(NSO):\r\n    with tf.name_scope('ns_inner_2'):\r\n      with tf.variable_scope(VSO):\r\n        print((tf.get_variable('var', [1]), tf.constant(1.0, name='const')))\r\n```\r\nright?\r\n\r\n@lukaszkaiser what do you think about the request?", "Yes, that is the behavior I expected. I have provided a working solution in my question when swapping the `variable_scope` and `name_scope` declarations. I just find it confusing that one behaves different from the other.", "We can only judge how complex this is when someone makes a PR and we can see it. In general, we would prefer to avoid adding much complexity as the scoping code has grown a lot anyway and the issue of specifying a few times instead of once doesn't seem to be that pressing, except if I'm missing something.", "I think for now it is fine to update the documentation of `auxiliary_name_scope` making the intentions clear, like @facaiy suggested. As a side note, I find all the scoping very troublesome. It is too complex and has so many side-effects. But that's just my 5cents.", "I prefer to update document as well, because scoping codes affects a lot of people and we'd better take care not to introduce unnecessary complexity if we already have a workaround. ", "I think the original issue is resolved, and #18948 is tracking the docs update. Closing. ", "Hi,\r\nThe document is still pretty much vague:\r\n\r\n> auxiliary_name_scope: If True, we create an auxiliary name scope with the scope. If False, we don't create it. Note that the argument is not inherited, and it only takes effect for once when creating. You should only use it for re-entering a premade variable scope.\r\n\r\nFirst of all, what is the meaning of an Auxiliary Name Scope? And what is its relation with reuse?\r\n\r\nWhat does it mean that it takes effect once? like in a for loop on multiple GPUs, should it be True for the whole loop, or False?", "If you can, I would suggest you move on to v2.0 and leave 1.x behind. It is sooo much easier to use.\r\n\r\n+ I agree, that wording is \"difficult\""]}, {"number": 18780, "title": "[TF mobile] change toolchain to clang", "body": "Change toolchain to clang, GCC Deprecated on Android ndk\r\nhttps://android.googlesource.com/platform/ndk/+/master/docs/Roadmap.md", "comments": ["Nagging Reviewer @andrehentz: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 182 days with no activity and the `awaiting review` label has been applied.", "Sorry I missed this! I'll approve the change but you might have to rebase.", "@andrehentz i update to master", "@case540  Do I need to do something else?"]}, {"number": 18779, "title": "Feature Request / Question: IP-Cam object detection (How-to)", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, I've followed a tutorial and haven't touched the base setup of it. https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.5.0\r\n- **Python version**:  3.6\r\n- **Bazel version (if compiling from source)**:  Couldn't get the capture script to return this\r\n- **GCC/Compiler version (if compiling from source)**: b'unknown'\r\n- **CUDA/cuDNN version**: 9.0 / 7.05\r\n- **GPU model and memory**: Nvidia GeForce GTX 960M / 2Gb?\r\n- **Exact command to reproduce**: No idea\r\n\r\n### Describe the problem\r\n\r\nHello Tensorflow, my question is pretty straightforward and maybe was already answered somewhere, but I'm not finding the answer : )).. Hence, I turn to you. If there is already a clear guide out there on how to do it, could you point me in the right direction?\r\n\r\nGiven this script that launches the object detection with a webcam, how do I modify it to launch the detector by giving an IP address as input and receiving that video feed (and bounding boxes) as output? (Source in the last section).\r\n\r\nI am aware we can always set an IP cam to be a webcam in the machine's webcam list, but I'd like to access the IP address directly. Can this be done? Is this already implemented? \r\n\r\nThanks in advance!\r\n\r\n\r\n### Source code / logs\r\n \r\n\r\nimport os\r\nimport cv2\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport sys\r\n\r\n\r\nsys.path.append(\"..\")\r\n\r\n\r\nfrom utils import label_map_util\r\nfrom utils import visualization_utils as vis_util\r\n\r\n\r\nMODEL_NAME = 'inference_graph'\r\n\r\nCWD_PATH = os.getcwd()\r\n\r\nPATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,'frozen_inference_graph.pb')\r\n\r\n\r\nPATH_TO_LABELS = os.path.join(CWD_PATH,'training','labelmap.pbtxt')\r\n\r\n\r\nNUM_CLASSES = 6\r\n\r\n\r\nlabel_map = label_map_util.load_labelmap(PATH_TO_LABELS)\r\ncategories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\r\ncategory_index = label_map_util.create_category_index(categories)\r\n\r\ndetection_graph = tf.Graph()\r\nwith detection_graph.as_default():\r\n    od_graph_def = tf.GraphDef()\r\n    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\r\n        serialized_graph = fid.read()\r\n        od_graph_def.ParseFromString(serialized_graph)\r\n        tf.import_graph_def(od_graph_def, name='')\r\n\r\n    sess = tf.Session(graph=detection_graph)\r\n\r\n\r\n\r\nimage_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\r\n\r\n\r\ndetection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\r\n\r\ndetection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\r\ndetection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\r\n\r\n\r\nnum_detections = detection_graph.get_tensor_by_name('num_detections:0')\r\n\r\n\r\nvideo = cv2.VideoCapture(0)\r\nret = video.set(3,1280)\r\nret = video.set(4,720)\r\n\r\nwhile(True):\r\n\r\n    ret, frame = video.read()\r\n    frame_expanded = np.expand_dims(frame, axis=0)\r\n\r\n   \r\n    (boxes, scores, classes, num) = sess.run(\r\n        [detection_boxes, detection_scores, detection_classes, num_detections],\r\n        feed_dict={image_tensor: frame_expanded})\r\n\r\n\r\n    vis_util.visualize_boxes_and_labels_on_image_array(\r\n        frame,\r\n        np.squeeze(boxes),\r\n        np.squeeze(classes).astype(np.int32),\r\n        np.squeeze(scores),\r\n        category_index,\r\n        use_normalized_coordinates=True,\r\n        line_thickness=8,\r\n        min_score_thresh=0.85)\r\n\r\n   \r\n    cv2.imshow('Object detector', frame)\r\n\r\n    if cv2.waitKey(1) == ord('q'):\r\n        break\r\n\r\nvideo.release()\r\ncv2.destroyAllWindows()\r\n\r\n", "comments": ["I just got it to work, as soon as I entered the IP address in video = cv2.VideoCapture(), so you can safely ignore this : )", "Good to hear you got it working!", "@SJRogue \r\nSorry I also wanted this method to work with Tensorflow object detection. I wanted to get a video of an IP cam, I did it in the following way but it came out wrong :(\r\ncap = cv2.VideoCapture ()\r\ncap.open (\"rtsp: // utpl: utpl1234@190.63.178.244: 554 / Streaming / Channels / 2\")\r\nCould you help me please?"]}, {"number": 18778, "title": "Change from squeeze_dims to axis when calling tf.squeeze", "body": "The `squeeze_dims` in `tf.squeeze` has been deprecated in favor of `axis` while many places still use `squeeze_dims`. That generates lots of warnings.\r\n\r\nThis fix switches from `squeeze_dims` to `axis` to remove all those warnings.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 18777, "title": "GraphDef not ok for the TensorRT API but ok for the TensorFlow iteslf ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**: Python 3.5\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0-6ubuntu1~16.04.9\r\n- **CUDA/cuDNN version**: 9.0 / 7.0.5\r\n- **GPU model and memory**: GTX 1070, 8Gb\r\n\r\n### Describe the problem\r\nI'm trying to convert `Keras` model (`MobileNet` or `Xception`) to `TensorRT` engine using `TensorFlow` API.\r\nFirst, I'm getting graph from `K.session()` and appropriate `GraphDef`.\r\nSecond, I'm freeze it with `convert_variables_to_constants`.\r\nThen, I'm able to make predictions using `TensorFlow`. But when I submit it to the the `create_inference_graph`, it complaints to bad dimensions:\r\n```\r\n2018-04-22 20:13:17.560097: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:412] subgraph conversion error for subgraph_index:47 due to: \"Unimplemented: Not supported constant type, at conv_pw_4_bn/Const_5\" SKIPPING......( 7 nodes)\r\n2018-04-22 20:13:17.563078: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:412] subgraph conversion error for subgraph_index:48 due to: \"Unimplemented: Not supported constant type, at conv_pw_1_bn/Const_5\" SKIPPING......( 7 nodes)\r\n2018-04-22 20:13:17.565057: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:412] subgraph conversion error for subgraph_index:49 due to: \"Unimplemented: Not supported constant type, at conv_dw_1_bn/Const_5\" SKIPPING......( 7 nodes)\r\n2018-04-22 20:13:17.567257: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:412] subgraph conversion error for subgraph_index:50 due to: \"Unimplemented: Not supported constant type, at conv_dw_12_bn/Const_5\" SKIPPING......( 7 nodes)\r\n2018-04-22 20:13:17.569343: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:412] subgraph conversion error for subgraph_index:51 due to: \"Unimplemented: Not supported constant type, at conv_dw_7_bn/Const_5\" SKIPPING......( 7 nodes)\r\n2018-04-22 20:13:17.571571: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:412] subgraph conversion error for subgraph_index:52 due to: \"Unimplemented: Not supported constant type, at conv_pw_10_bn/Const_5\" SKIPPING......( 7 nodes)\r\n2018-04-22 20:13:17.573851: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:412] subgraph conversion error for subgraph_index:53 due to: \"Unimplemented: Require 4 dimensional input. Got 1 conv_dw_6_bn/cond/batchnorm/add/Switch\" SKIPPING......( 6 nodes)\r\n```\r\nas many times as there are such `*/Switch` nodes. Digging deeper, I found such strange design in the graph's `GraphDef`:\r\n```\r\ntensor_shape {\r\n  dim {\r\n  }\r\n}\r\n```\r\nwithin some (**not all**) nodes of type `Const`, e.g.\r\n```\r\nnode {\r\n  name: \"conv_pw_8_bn/Const_5\"\r\n  op: \"Const\"\r\n  attr {\r\n    key: \"dtype\"\r\n    value {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n  attr {\r\n    key: \"value\"\r\n    value {\r\n      tensor {\r\n        dtype: DT_FLOAT\r\n        tensor_shape {\r\n          dim {\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n``` \r\nAll such nodes are mentioned in the traceback.\r\n\r\nSo, it's ok for `TensorFlow` itself but wrong for the `TensorRT`. I gues there is an error either in `GraphDef` generation or while parsing for `TensorRT`.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "**Exact command to reproduce**:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import tensorrt as trt\r\nfrom keras import backend as K\r\nfrom keras.applications import mobilenet\r\nfrom tensorflow.python.framework.graph_util import convert_variables_to_constants\r\n\r\n########################## Converting Keras model to GraphDef ##########################\r\n\r\n# def freeze_session(session, output_names, clear_devices=False):    \r\n#     graph = session.graph\r\n#     with graph.as_default():        \r\n#         input_graph_def = graph.as_graph_def()\r\n#         return convert_variables_to_constants(session, input_graph_def, output_names)\r\n\r\n# model = mobilenet.MobileNet(include_top=True, input_shape=(128, 128, 3))\r\n# model_output = 'reshape_2/Reshape'\r\n\r\n# frozen_graph = freeze_session(K.get_session(), [model_output])\r\n# tf.train.write_graph(frozen_graph, \".\", \"mobilenet.pb\", as_text=False)\r\n\r\n########################################################################################\r\n\r\nOUTPUT_LAYERS = ['reshape_2/Reshape']\r\nBATCH_SIZE = 4\r\nGRAPH_PATH = 'mobilenet.pb'\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.5\r\n\r\nwith tf.gfile.GFile(GRAPH_PATH, 'rb') as f:\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n\r\ngraphdef_fp32 = trt.create_inference_graph(\r\n    input_graph_def=graph_def, outputs=OUTPUT_LAYERS, max_batch_size=BATCH_SIZE, \r\n    max_workspace_size_bytes=int(4e+9), precision_mode=\"FP32\"\r\n)\r\n```", "And when I say that it works with `TensorFlow` itself, I mean\r\n```\r\nINPUT_LAYERS = ['input_1']\r\nOUTPUT_LAYERS = ['reshape_2/Reshape']\r\n\r\ndef get_graph_and_session(graphdef, name=''):\r\n    graph = tf.get_default_graph()\r\n    tf.import_graph_def(graphdef, name=name)\r\n    sess = tf.Session(graph=graph, config=config)\r\n    return graph, sess\r\n\r\ndef get_io(graph, name=''):\r\n    if name != '':\r\n        inp = graph.get_tensor_by_name('{}/{}:0'.format(name, INPUT_LAYERS[0]))\r\n        out = graph.get_tensor_by_name('{}/{}:0'.format(name, OUTPUT_LAYERS[0]))\r\n    else:\r\n        inp = graph.get_tensor_by_name('{}:0'.format(INPUT_LAYERS[0]))\r\n        out = graph.get_tensor_by_name('{}:0'.format(OUTPUT_LAYERS[0]))\r\n    return inp, out\r\n\r\ndef run(sess, inp, out, data):\r\n    return sess.run(out, feed_dict={inp: data})\r\n\r\ngraph_orig, sess_orig = get_graph_and_session(graph_def)\r\ninp_orig, out_orig = get_io(graph_orig)\r\nres_orig = run(sess_orig, inp_orig, out_orig, np.zeros((1, INPUT_H, INPUT_W, 3)))\r\n```", "@arassadin AFAIK convert_variables_to_constants is not sufficient to freeze the graph. See https://www.tensorflow.org/extend/tool_developers/#freezing\r\nAlso TensorRT 4 will have a better op support and mobilenet support will improve when we switch to TRT4.", "Hi @arassadin,\r\n\r\nIf you succeed in freezing the graph, can you close the issue?\r\n\r\nThanks", "Closing the issue due to lack of response"]}, {"number": 18776, "title": "Low Accuracy with static image on TFLite demo model", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Binary(PIP install)\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: Na\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: NA\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI've implemented the transfer learning example for mobilenet and deployed the Android app. The classification was happening perfectly with a good accuracy for the constant stream of images. Later I modified the app to pick up a file from storage or click an image, save to storage and then classify. When this is implemented, the accuracy has dropped acutely to the range 0f 0.01 - 0.3 for any of the Flower classes or the general mobilenet model with the 1500 classes. I've implemented the 224 x 224 version of the model. Below are the steps \r\n\r\n- Create a basic camera app\r\n- Take a picture and save it to storage\r\n- The uri of the image is saved and then a drawable is created from the URI.\r\n- This drawable is then converted to a bitmap.\r\n- The bitmap size is transformed to 224 x 224 to match the input of the Mobile Net model\r\n- This transformed bitmap is sent for classification to the pre-implemented class from the sample.\r\n\r\nLink to my question on stack overflow:\r\nhttps://stackoverflow.com/questions/49954439/low-accuracy-with-static-image-on-tflite-demo-model\r\n\r\n### Source code / logs\r\n```\r\nprivate static Bitmap getResizedBitmap(Bitmap bm, int newWidth, int newHeight, boolean isNecessaryToKeepOrig) {\r\n        int width = bm.getWidth();\r\n        int height = bm.getHeight();\r\n        float scaleWidth = ((float) newWidth) / width;\r\n        float scaleHeight = ((float) newHeight) / height;\r\n        // CREATE A MATRIX FOR THE MANIPULATION\r\n        Matrix matrix = new Matrix();\r\n        // RESIZE THE BIT MAP\r\n        matrix.postScale(scaleWidth, scaleHeight);\r\n\r\n        // \"RECREATE\" THE NEW BITMAP\r\n        Bitmap resizedBitmap = Bitmap.createBitmap(bm, 0, 0, width, height, matrix, false);\r\n        if(!isNecessaryToKeepOrig){\r\n            bm.recycle();\r\n        }\r\n        return resizedBitmap;\r\n    }\r\n\r\n```", "comments": ["StackOverflow is a better venue for a question like this, and it seems like you have cross-posted there. To prevent bifurcation of responses, I am closing this issue. If, after further discussion and research, you find that there is a bug or feature request for TensorFlow, please open a new issue.", "Hi @vijaysaimutyala . I face the same problem right now and I'm not sure what I'm supposed to do. In your case, do you mean you change the image size in the Android or the model training? Desperate for help...Thanks in advance."]}, {"number": 18775, "title": "Fix cmake build issues with GPU on Linux", "body": "This fix is an attempt to fix the build issues with GPU on Linux.\r\n\r\nPreviously cmake on Linux only works for CPU build.\r\n\r\nThis fix addresses multiple issues on cmake file so that GPU build could work on Linux as well.\r\n\r\nThe build is performed on Ubuntu 16.04 + CUDA 9.0 + NCCL v21, with the following command:\r\n```\r\n$ mkdir build && cd build\r\n$ cmake -Dtensorflow_ENABLE_GPU=ON \\\r\n  -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-9.0/ \\\r\n  -Dtensorflow_CUDNN_INCLUDE=/usr/local/cuda-9.0/include/ \\\r\n  -DCMAKE_BUILD_TYPE=Release ../tensorflow/contrib/cmake\r\n$ make -j 4 all\r\n```\r\n\r\nThis fix fixes #17232.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@mrry, good to go? Looks like the non-Windows build is fine.", "@martinwicke Fine by me. You might want to keep an eye on it in the merge, though (unless you've already tested it on the internal build), since that's where a lot of this path complication comes from.", "(Never mind, it's in `platform/default`, so shouldn't create a merge issue.)"]}, {"number": 18774, "title": "mobilenet_v1_eval.py has a big bug?", "body": "### System information\r\n- **Have I written custom code**:Yes\r\n- **OS Platform and Distribution**:Linux Ubuntu 16.04\r\n- **TensorFlow installed from**:binary(Anaconda)\r\n- **TensorFlow version**:1.7\r\n- **Python version**: Python 3.6.2\r\n- **Bazel version**:\r\nBuild label: 0.11.1\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue May 14 07:48:23 +50148 (1520362424903)\r\nBuild timestamp: 1520362424903\r\nBuild timestamp as int: 1520362424903\r\n- **CUDA/cuDNN version**:CUDA 9.0, cuDNN v7.0\r\n- **GPU model and memory**:GTX 1080Ti, 11GB\r\n- **Exact command to reproduce**:Just run mobilenet_v1_eval.py to evaluate the trained model, you will find the bug. If **is_training=True you can get the right result**.[The is_trainging are in mobilenet_v1.mobilenet_v1_arg_scope(is_training=True) and mobilenet_v1.mobilenet_v1(inputs, is_training=True) ]. **Or remove \"slim.batch_norm\" in \"with slim.arg_scope([slim.batch_norm, slim.dropout],\" can also get the right result.** \"with slim.arg_scope([slim.batch_norm, slim.dropout],\"  is in L362 of https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.py\r\n\r\n### Describe the problem\r\nTotally, I have two problems.\r\n**Firstly**,\r\nIn the mobilenet_v1_eval.py, yeah, it's eval not train. When is_training=False, you will not get a right result. When is_training=True, you can get a right answer. Obviously, it's wrong! I guess it's a big bug associate with batch norm, but I cannot figure it out. Someone knows that? \r\nAttentation, the is_training is not the one in  flower_input(is_training=False), they are in mobilenet_v1.mobilenet_v1_arg_scope(is_training=True) and mobilenet_v1.mobilenet_v1(inputs, is_training=True). And **flower_input() was a custom code referencing imagenet_input()** in https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1_eval.py\r\n```python\r\ndef build_model():\r\n  \"\"\"Build the mobilenet_v1 model for evaluation.\r\n\r\n  Returns:\r\n    g: graph with rewrites after insertion of quantization ops and batch norm\r\n    folding.\r\n    eval_ops: eval ops for inference.\r\n    variables_to_restore: List of variables to restore from checkpoint.\r\n  \"\"\"\r\n  g = tf.Graph()\r\n  with g.as_default():\r\n    inputs, labels = flower_input(is_training=False)\r\n    scope = mobilenet_v1.mobilenet_v1_arg_scope(\r\n        is_training=True, weight_decay=0.0)\r\n    with slim.arg_scope(scope):\r\n      logits, _ = mobilenet_v1.mobilenet_v1(\r\n          inputs,\r\n          is_training=True,\r\n          depth_multiplier=FLAGS.depth_multiplier,\r\n          num_classes=FLAGS.num_classes)\r\n\r\n    if FLAGS.quantize:\r\n      tf.contrib.quantize.create_eval_graph()\r\n\r\n    eval_ops = metrics(logits, labels)\r\n\r\n  return g, eval_ops\r\n``` \r\n**Secondly**,\r\nI use export_eval_pbtxt() in the following to get the \"mobilenet_v1_eval.pbtxt\".\r\n```python\r\ndef export_eval_pbtxt():\r\n  \"\"\"Export eval.pbtxt.\"\"\"\r\n  g = tf.Graph()\r\n  with g.as_default():\r\n    inputs = tf.placeholder(dtype=tf.float32,shape=[None,224,224,3])\r\n    scope = mobilenet_v1.mobilenet_v1_arg_scope(\r\n        is_training=False, weight_decay=0.0)\r\n    with slim.arg_scope(scope):\r\n      logits, _ = mobilenet_v1.mobilenet_v1(\r\n          inputs,\r\n          is_training=False,\r\n          depth_multiplier=FLAGS.depth_multiplier,\r\n          num_classes=FLAGS.num_classes)\r\n    eval_graph_file = '/home/lg/projects/mobilenet_v1_eval.pbtxt'\r\n    with tf.Session() as sess:\r\n          with open(eval_graph_file, 'w') as f:\r\n            f.write(str(g.as_graph_def()))\r\n\r\n```\r\nThen, frozen the graph:\r\n\r\n```python\r\nbazel-bin/tensorflow/python/tools/freeze_graph  \\\r\n  --input_graph=/home/lg/projects/mobilenet_v1_eval.pbtxt \\\r\n  --input_checkpoint=/home/lg/projects/checkpoint/model.ckpt-20000 \\\r\n  --input_binary=false \\\r\n  --output_graph=/home/lg/projects/frozen_mobilenet_v1_224.pb  \\\r\n  --output_node_names=MobilenetV1/Predictions/Reshape_1  \\\r\n  --checkpoint_version=2\r\n```\r\n\r\nThen I use the frozen .pb file to classify an image, it won't get a right result, no matter you use is_training is True or False above to get the \"mobilenet_v1_eval.pbtxt\".\r\nAnd the classify file is:\r\n```python\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport argparse\r\nimport os.path\r\nimport re\r\nimport sys\r\nimport tarfile\r\n\r\nimport numpy as np\r\nfrom six.moves import urllib\r\nimport tensorflow as tf\r\nFLAGS = None\r\n\r\n\r\ndef create_graph():\r\n  \"\"\"Creates a graph from saved GraphDef file and returns a saver.\"\"\"\r\n  # Creates graph from saved graph_def.pb.\r\n  with tf.gfile.FastGFile(os.path.join(FLAGS.model_dir, r'/home/lg/projects/frozen_mobilenet_v1_224.pb'), 'rb') as f:\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n    _ = tf.import_graph_def(graph_def,return_elements=['MobilenetV1/Predictions/Reshape_1:0'], name='lg')\r\n\r\ndef run_inference_on_image(image):\r\n  \"\"\"Runs inference on an image.\r\n  Args:\r\n    image: Image file name.\r\n  Returns:\r\n    Nothing\r\n  \"\"\"\r\n  if not tf.gfile.Exists(image):\r\n    tf.logging.fatal('File does not exist %s', image)\r\n\r\n  image_data = tf.gfile.FastGFile(image, 'rb').read()\r\n  img_data_jpg = tf.image.decode_jpeg(image_data) #\u56fe\u50cf\u89e3\u7801  \r\n  img_data_jpg = tf.image.convert_image_dtype(img_data_jpg, dtype=tf.float32) #\u6539\u53d8\u56fe\u50cf\u6570\u636e\u7684\u7c7b\u578b\r\n  img_data_jpg = tf.image.resize_image_with_crop_or_pad(img_data_jpg,224,224)\r\n\r\n  # Creates graph from saved GraphDef.\r\n  create_graph()\r\n\r\n  with tf.Session() as sess:\r\n    image_data = img_data_jpg.eval().reshape(-1,224,224,3)\r\n    softmax_tensor = sess.graph.get_tensor_by_name('lg/MobilenetV1/Predictions/Reshape_1:0')\r\n    predictions = sess.run(softmax_tensor, {'lg/Placeholder:0': image_data})\r\n    predictions = np.squeeze(predictions)\r\n    print('predictions: ',predictions)\r\n    # Read the labels from label.txt.\r\n    label_path = os.path.join(FLAGS.model_dir, '/home/lg/projects/labels.txt')\r\n    label = np.loadtxt(fname=label_path,dtype=str)\r\n\r\n    top_k = predictions.argsort()[-FLAGS.num_top_predictions:][::-1]\r\n    for node_id in top_k:\r\n      label_string = label[node_id]\r\n      score = predictions[node_id]\r\n      print('%s (score = %.5f)' % (label_string, score))\r\n\r\ndef main(_):\r\n  image = (FLAGS.image_file if FLAGS.image_file else os.path.join(FLAGS.model_dir, 'cropped_panda.jpg'))\r\n  run_inference_on_image(image)\r\n\r\nif __name__ == '__main__':\r\n  parser = argparse.ArgumentParser()\r\n  # graph_def.pb: Binary representation of the GraphDef protocol buffer.\r\n  # label.txt: the labels according to data tfrecord\r\n  parser.add_argument(\r\n      '--model_dir',\r\n      type=str,\r\n      default='/tmp/imagenet',\r\n      help='Path to graph_def.pb and label.txt'\r\n  )\r\n  parser.add_argument(\r\n      '--image_file',\r\n      type=str,\r\n      default=r'/home/lg/projects/data/flower_photos/daisy/5673728_71b8cb57eb.jpg',\r\n      help='Absolute path to image file.'\r\n  )\r\n  parser.add_argument(\r\n      '--num_top_predictions',\r\n      type=int,\r\n      default=2,\r\n      help='Display this many predictions.'\r\n  )\r\n  FLAGS, unparsed = parser.parse_known_args()\r\ntf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n```\r\n\r\nThe mobilenet v1 I used is in https://github.com/tensorflow/models/tree/master/research/slim/nets .\r\nI have tried inception v3, whether to eval or classify an image that can get right results. So I think  mobilenet_v1_eval.py must have a bug. \r\n\r\nActually, I have tried two experiments, one is the custom codes(tf_train.py, tf_eval.py, tf_input.py, tf_inference.py, tf_classify_image.py referencing https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10) in https://github.com/GarryLau/draft_notes/tree/master/TF.\r\nAnother one is referencing mobilenet_v1_train.py, mobilenet_v1_eval.py, mobilenet_v1.py, in https://github.com/tensorflow/models/tree/master/research/slim/nets .\r\nIn the begining, I think the custom codes have some problems, so I tried the second experiment that tensorflow official. But they are all wrong.\r\n\r\n### Source code / logs\r\nAll the codes I used are in:\r\nhttps://github.com/GarryLau/draft_notes/tree/master/TF\r\nhttps://github.com/GarryLau/draft_notes/tree/master/create_tfrecord\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nExact command to reproduce", "I used transfer learning to retrain mobilenet_v1, and got a good result. But doesn't it a bug?", "@tatatodd Any update on this?", "@GarryLau You can try [`mobilenet_v2.py`](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_v2.py) and post any errors here", "Hi @GarryLau , did you solve the problem. I'm having the same issue with you, setting `is_training=True` works well, but `is_training=False` produce weird results.", "@GarryLau ,\r\nCan you please try using [mobilenet_v2.py](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_v2.py) as suggested by @wt-huang and confirm if the issue still persist.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 18773, "title": "Fix numerical warning of np.float with explicitly specifying np.float32", "body": "This PR is to fix numerical warning of type np.float.\r\n> WARNING:tensorflow:float64 is not supported by many models, consider casting to float32\r\n\r\nThe [numpy](https://docs.scipy.org/doc/numpy-1.14.0/user/basics.types.html) data type `np.float` is shorthand for `np.float64`. So, `np.float32` is explicitly specified as solution.", "comments": ["I'm sorry it took so long to get a review, could you fix the conflict?", "I triggered the tests also, it may be worth looking into.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 18772, "title": "Android\uff1aNo OpKernel was registered to support Op 'Cumsum' with these attrs", "body": "i use http://ci.tensorflow.org/view/Nightly/job/nightly-android/lastStableBuild/  \r\nbut i get error on my cell phone\uff0cplease how can slove this question\r\n\r\n04-22 21:43:19.665 11453-11453/com.lemon.demo W/System.err: java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Cumsum' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n      <no registered kernels>\r\n    \t [[Node: Cumsum = Cumsum[T=DT_FLOAT, Tidx=DT_INT32, exclusive=false, reverse=false](crop_sub_imgs, Cumsum/axis)]]\r\n        at org.tensorflow.Session.run(Native Method)\r\n        at org.tensorflow.Session.access$100(Session.java:48)\r\n        at org.tensorflow.Session$Runner.runHelper(Session.java:298)\r\n        at org.tensorflow.Session$Runner.run(Session.java:248)\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:228)\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:197)\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "ok I use pip install tensorflow-gpu 1.7 in my computer\r\nandroid use tensorflow 1.8.0-rc0", "I use tf.cumsum", "@tensorflowbutler ", "Nagging Assignee @jart: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "Have you solved it? I met the same error. "]}, {"number": 18771, "title": "make safe_embedding_lookup_sparse method public and clean duplicate codes", "body": "In #17417, I found that same `safe_embedding_lookup_sparse` code exists in both embedding_ops.py and feature_column.py.\r\n\r\nTo resolve circular dependency, I create an `embedding_ops_py` target. The method looks not so good, any suggestion would be appreciated.", "comments": ["@ispirmustafa Thanks for your clarification. So let's move the method to `tensorflow/python/ops/embedding_ops.py` and make it public, right?", "@ispirmustafa Hi,  I have made the core version public, could you take a look again?", "@martinwicke Hi, could you make an API review? Thank you.", "This will fail the tests -- can you run the API golden update?", "Sure, API golden has been updated.", "@ispirmustafa Could you finish the review please?", "Hi, I merged the latest commits from master to fix code conflict. Thank @ispirmustafa 's approbation", "(good for API review)", "Does anyone can help restart all tests? Thanks.", "Nagging Reviewer @ispirmustafa: It has been 18 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @ispirmustafa: It has been 32 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @ispirmustafa: It has been 47 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "@martinwicke @ispirmustafa Thank you. All tests pass."]}, {"number": 18770, "title": "Configuring TFLite(Android) to use all CPU's?", "body": "### System information\r\n- **Have I written custom code (yes)**:\r\n- **OS Platform and Distribution (Android Things)**:\r\n- **TensorFlow installed from (source)**:\r\n- **TensorFlow version (1.8.0-rc0)**:\r\n- **Python version**:  2.7\r\n- **Bazel version (0.12.0)**:\r\n- **GCC/Compiler version (5.4.0)**:\r\n- **CUDA/cuDNN version**: 9.1/7.1\r\n- **GPU model and memory**: GTX1080 8G\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI use Raspberry Pi3 + Android Things + TFLite + MobileNet to make a image classifier sample, base this [Android Things Image Classifier](https://codelabs.developers.google.com/codelabs/androidthings-classifier/#4) tutorial.\r\n- I monitor the cpu by `adb shell top` and find it only use 1 CPU with quant model when inference, and 2 CPU with unquant model.\r\n  So how to configuring TFLite(Android) to use all CPUs?\r\n\r\n- Another question, maybe impact the question above, on the[ introduction of TFLite](https://www.tensorflow.org/mobile/tflite/), it say:\r\n  > On select Android devices, the Interpreter will use the Android Neural Networks API for hardware acceleration, or default to CPU execution if none are available.\r\n\r\n  You know, Raspberry has a GPU(Broadcom VideoCore IV), so how to use it? has it been used?\r\n\r\n### Source code / logs\r\n[Android Things Image Classifier](https://codelabs.developers.google.com/codelabs/androidthings-classifier/#4) tutorial.\r\n", "comments": ["Nagging Assignee @shivaniag: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 64 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 79 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@zhonghuabao1 : You can try using Interpreter#SetNumThreads: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/java/src/main/java/org/tensorflow/lite/Interpreter.java#L231 to change the number of threads.\r\n\r\nDo let me know if you still need help.\r\n", "thanks for you ans, I have tried SetNumThreads a month ago yet. I fond it work with quantis models but not used with nonquantis models, such as MobileNet. \r\nI have not time to test it more recently, do you meet the problem? is it a bug? ", "@zhonghuabao1 : you are correct, this was due to a bug, which we have recently fixed: see https://github.com/tensorflow/tensorflow/commit/2bfc7957dada57c1eb8491e04dac70d16b4369ac for the change, now it should work for non-quantized as well."]}]