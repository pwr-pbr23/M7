[{"number": 9244, "title": "fix warning about comparing signed/unsigned types in auto_parallel.cc", "body": "This fixes a harmless (but annoying) GCC warning about comparing signed/unsigned types in auto_parallel.cc. ", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 9243, "title": "tf.conv2d ValueError", "body": "I believe the following is a bug\r\n\r\nValueError: Shape must be rank 4 but is rank 1 for 'Conv2D' (op: 'Conv2D') with input shapes: [1,1,64,256], [4].\r\n\r\nFull Traceback:\r\nTraceback (most recent call last):\r\n  File \"chat.py\", line 17, in <module>\r\n    conv1 = tf.nn.conv2d(input=embed_a,filter=[1,8,256,128],strides=[1,1,8,256],padding=\"VALID\")\r\n  File \"/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 396, in conv2d\r\n    data_format=data_format, name=name)\r\n  File \"/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2329, in create_op\r\n    set_shapes_for_outputs(ret)\r\n  File \"/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1717, in set_shapes_for_outputs\r\n    shapes = shape_func(op)\r\n  File \"/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1667, in call_with_requiring\r\n    return call_cpp_shape_fn(op, require_shape_fn=True)\r\n  File \"/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.py\", line 610, in call_cpp_shape_fn\r\n    debug_python_shape_fn, require_shape_fn)\r\n  File \"/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.py\", line 676, in _call_cpp_shape_fn_impl\r\n    raise ValueError(err.message)\r\nValueError: Shape must be rank 4 but is rank 1 for 'Conv2D' (op: 'Conv2D') with input shapes: [1,1,64,256], [4].\r\n\r\nI believe that a tensor of shape [1,1,64,256] should be Rank 4 not Rank 1.", "comments": ["@xXDavidHuangXx  is your problem solved?Now I bumped into the same one.Pls tell me your solution if you solved, thanks.", "I have same issue\r\n\r\n> ValueError: Shape must be rank 4 but is rank 1 for 'Conv2D' (op: 'Conv2D') with input shapes: [?,28,28,1], [4].\r\n\r\nFor\r\n\r\n```\r\nx = tf.placeholder(tf.float32, shape=[None, 28, 28, 1], name=\"x\")  \r\nconv1 = tf.nn.conv2d(x, filter=[5, 5, 1, 20], strides=[1, 1, 1, 1], padding='VALID')\r\n```\r\n   \r\n", "@ilkarman I had the same issue. I found the error a bit confusing. The problem wasn't with the input but with the filter. I had to create a variable instead of just passing in a list.\r\n\r\nHere is what I did, based on the example [here](https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10.py ): \r\n```\r\nfilter = tf.get_variable('weights', [5, 5, 1, 64], initializer=tf.truncated_normal_initializer(stddev=5e-2, dtype=tf.float32), dtype=tf.float32)\r\nconv = tf.nn.conv2d(x, filter, [1,1,1,1], padding = \"SAME\")\r\n```", "@xXDavidHuangXx Has this error been resolved , I am getting same error with MNIST data set?", "@Spandyie try the following:\r\n```\r\nfilter_size = 4\r\ninput_channels = 3\r\noutput_filters = 64\r\nx = tf.placeholder(tf.float32, shape=[None, 512, 512, 3])\r\ny = tf.nn.conv2d(x, filter=tf.Variable(tf.truncated_normal([filter_size, filter_size, input_channels, output_filters], stddev=0.5)), strides=[1,1,1,1] , padding='SAME')\r\n```\r\n\r\nI guess this is the way to use `tf.nn.conv2d`.", "tf.nn.conv2d           : is MUCH complicated, say,the filter need to be specified first\r\ntf.layers.Conv2D       : recomended\r\ntf.layers.conv2d       : IS DEPRECATED\uff0cUse tf.keras.layers.Conv2D instead.\r\ntf.keras.layers.Conv2D : recomended"]}, {"number": 9242, "title": "added const", "body": "The const statement creates a variable whose value is fixed, that is, a read-only constant", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "@tensorflow-jenkins test this please", "@luizhenriquesoares , can you please sign the CLA before we merge this PR? Thanks.", " @caisq, I already signed the CLA, thanks", "CLAs look good, thanks!\n\n<!-- ok -->", "PR merged. Thanks, @luizhenriquesoares ! "]}, {"number": 9241, "title": "Module 'tensorflow' has no attribute 'constant' after local pip installation", "body": "Please go to Stack Overflow for help and support: http://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n>>>> Ubuntu 16 with Cuda 8 and Nvidia GEForce 780i, g++ 5.4.0, cudnn 6\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n>>>> no. simply following https://www.tensorflow.org/install/install_sources\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n>> Ubuntu 16 (xenial) \r\n\r\n- **TensorFlow installed from (source or binary)**:\r\n>>>> masterbranch\r\n- **TensorFlow version (use command below)**:\r\n>>> 1.1.0rc0\r\n- **Bazel version (if compiling from source)**:\r\n>>> Build label: 0.4.5\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Mar 16 12:19:38 2017 (1489666778)\r\nBuild timestamp: 1489666778\r\nBuild timestamp as int: 1489666778\r\n\r\n- **CUDA/cuDNN version**:\r\n>>>> 6\r\n\r\n- **GPU model and memory**:\r\n>>>> Nvidia GEForce 780i, 2G\r\n- **Exact command to reproduce**:\r\nkarun@karunlindev3:~/shadow/cuda-workspace$ python\r\nPython 3.6.0 |Anaconda 4.3.1 (64-bit)| (default, Dec 23 2016, 12:22:00) \r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> hello = tf.constant('Hello, TensorFlow!')\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute 'constant'\r\n\r\nI tried to get tf version\r\n\r\nkarun@karunlindev3:~/shadow/cuda-workspace$ python\r\nPython 3.6.0 |Anaconda 4.3.1 (64-bit)| (default, Dec 23 2016, 12:22:00) \r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> print(tf.GIT_VERSION, tf.VERSION)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute 'GIT_VERSION'\r\n\r\n\r\nYou can collect some of this information using our environment capture script: https://github.com/tensorflow/tensorflow/tree/master/tools\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\n\r\nwhen i install tensor flow with gpu support from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.1-cp34-cp34m-linux_x86_64.whl\r\nit works with some warnings that certain libraries could be faster.\r\nwhen i install local pip pacakge following https://www.tensorflow.org/install/install_sources\r\ni get this\r\nkarun@karunlindev3:~/shadow/cuda-workspace$ python\r\nPython 3.6.0 |Anaconda 4.3.1 (64-bit)| (default, Dec 23 2016, 12:22:00) \r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> hello = tf.constant('Hello, TensorFlow!')\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute 'constant'\r\n\r\n\r\n   * I have looked on stackoverflow, tensorflow github and i have made sure my situation is different. i am not using tensorflow as a filename, nor I am in the source directory where config is launched. \r\n   * i have checked directory permissions on site-packages and it looks good.\r\n   * i uninstalled the gpu package 1.0.1-cp34 before i installed local pip.\r\n\r\nWhat am I missing?\r\n\r\n### Source code / logs\r\n\r\nlocal source\r\nkarun@karunlindev3:~/shadow/tensorflow$ ls\r\nACKNOWLEDGMENTS  AUTHORS    bazel-genfiles  bazel-tensorflow  bower.BUILD  configure        ISSUE_TEMPLATE.md  models.BUILD  RELEASE.md  third_party  util\r\nADOPTERS.md      bazel-bin  bazel-out       bazel-testlogs    BUILD        CONTRIBUTING.md  LICENSE            README.md     tensorflow  tools        WORKSPACE\r\n\r\n\r\n\r\nHere is the output for installation of loca pip. I noticed that there are a __LOT__ more files from the gpu package on googlapis than local pip\r\n\r\n__Successfully uninstalled tensorflow-gpu-1.0.1__\r\n\r\nkarun@karunlindev3:~/shadow/cuda-workspace$ pip install --upgrade /tmp/tensorflow_pkg/tensorflow-1.1.0rc0-cp36-cp36m-linux_x86_64.whl\r\nProcessing /tmp/tensorflow_pkg/tensorflow-1.1.0rc0-cp36-cp36m-linux_x86_64.whl\r\nRequirement already up-to-date: bleach==1.5.0 in /home/karun/anaconda3/lib/python3.6/site-packages (from tensorflow==1.1.0rc0)\r\nRequirement already up-to-date: protobuf>=3.2.0 in /home/karun/anaconda3/lib/python3.6/site-packages (from tensorflow==1.1.0rc0)\r\nRequirement already up-to-date: wheel>=0.26 in /home/karun/anaconda3/lib/python3.6/site-packages (from tensorflow==1.1.0rc0)\r\nRequirement already up-to-date: numpy>=1.11.0 in /home/karun/anaconda3/lib/python3.6/site-packages (from tensorflow==1.1.0rc0)\r\nRequirement already up-to-date: werkzeug>=0.11.10 in /home/karun/anaconda3/lib/python3.6/site-packages (from tensorflow==1.1.0rc0)\r\nRequirement already up-to-date: six>=1.10.0 in /home/karun/anaconda3/lib/python3.6/site-packages (from tensorflow==1.1.0rc0)\r\nRequirement already up-to-date: html5lib==1.0b8 in /home/karun/anaconda3/lib/python3.6/site-packages (from tensorflow==1.1.0rc0)\r\nRequirement already up-to-date: markdown==2.2.0 in /home/karun/anaconda3/lib/python3.6/site-packages (from tensorflow==1.1.0rc0)\r\nRequirement already up-to-date: setuptools in /home/karun/anaconda3/lib/python3.6/site-packages (from protobuf>=3.2.0->tensorflow==1.1.0rc0)\r\nRequirement already up-to-date: appdirs>=1.4.0 in /home/karun/anaconda3/lib/python3.6/site-packages (from setuptools->protobuf>=3.2.0->tensorflow==1.1.0rc0)\r\nRequirement already up-to-date: packaging>=16.8 in /home/karun/anaconda3/lib/python3.6/site-packages (from setuptools->protobuf>=3.2.0->tensorflow==1.1.0rc0)\r\nRequirement already up-to-date: pyparsing in /home/karun/anaconda3/lib/python3.6/site-packages (from packaging>=16.8->setuptools->protobuf>=3.2.0->tensorflow==1.1.0rc0)\r\nInstalling collected packages: tensorflow\r\n  Found existing installation: tensorflow 1.1.0rc0\r\n    Uninstalling tensorflow-1.1.0rc0:\r\n      Successfully uninstalled tensorflow-1.1.0rc0\r\n\r\n__Successfully installed tensorflow-1.1.0rc0__\r\n", "comments": ["You said you're using Ubuntu 16 but the logs say Red Hat? This might be a better question for [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow). Please see our issue tracker [policy](https://github.com/tensorflow/tensorflow/issues/new).", "No red hat. That's what anaconda writes on a python prompt. I am following all directions and building without issues. Please don't close this issue.", "Is this possibly related to https://github.com/tensorflow/tensorflow/issues/7285?", "Thanks. I looked at that thread. The problem there was the name of script matched package name i.e. *tensorflow*. I am simply using a shell from anaconda python 3.6. The shell, as you can see is launched from a directory different than tensorflow source tree.\r\nThink there might be in issue with pip_package.\r\n ", "so i pulled latest and built again using --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" and now I cannot even import tensorflow. It fails with this error\r\nImportError: /home/karun/anaconda3/bin/../lib/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /home/karun/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n\r\nis gcc 5 compatible?", "AttributeError: module 'tensorflow' has no attribute 'constant'\r\nhow to solve this"]}, {"number": 9240, "title": "spectral ops make freeze_graph tool trigger \"No OpKernel was registered ...\" error", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.2 LTS\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.1.0-rc1-253-gd5a9356 1.1.0-rc1\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: 8.0/5.1\r\n- **GPU model and memory**: NVIDIA Geforce GTX 750 Ti\r\n- **Exact command to reproduce**: bazel-bin/tensorflow/python/tools/freeze_graph --input_graph=model.pb --input_checkpoint=model.ckpt --output_graph=frozen_model.pb --output_node_names=y_conv\r\n\r\n### Describe the problem\r\nThe [`freeze_graph`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py) tool malfunctions when processing a graph containing [spectral ops](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/api_guides/python/spectral_ops.md).\r\nWithout spectral ops, everything works just fine, all the way to inference.\r\n\r\nMight this have something to do with the fact that spectral ops don't have a CPU implementation yet?\r\nAnd if so, why would this lead to this error?\r\n\r\n### Source code / logs\r\n```\r\nTraceback (most recent call last):\r\n  File \"bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 1136, in _do_call\r\n    return fn(*args)\r\n  File \"bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 1114, in _run_fn\r\n    self._extend_graph()\r\n  File \"bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 1163, in _extend_graph\r\n    self._session, graph_def.SerializeToString(), status)\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'RFFT' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[Node: rfft = RFFT[](framesig, Const_1)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 218, in <module>\r\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 150, in main\r\n    FLAGS.variable_names_blacklist)\r\n  File \"bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 128, in freeze_graph\r\n    saver.restore(sess, input_checkpoint)\r\n  File \"bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py\", line 1545, in restore\r\n    {self.saver_def.filename_tensor_name: save_path})\r\n  File \"bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 786, in run\r\n    run_metadata_ptr)\r\n  File \"bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 994, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 1129, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 1149, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'RFFT' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[Node: rfft = RFFT[](framesig, Const_1)]]\r\n\r\nCaused by op 'rfft', defined at:\r\n  File \"bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 218, in <module>\r\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 150, in main\r\n    FLAGS.variable_names_blacklist)\r\n  File \"bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 103, in freeze_graph\r\n    _ = importer.import_graph_def(input_graph_def, name=\"\")\r\n  File \"bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/importer.py\", line 308, in import_graph_def\r\n    op_def=op_def)\r\n  File \"bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/ops.py\", line 2336, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/ops.py\", line 1228, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'RFFT' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[Node: rfft = RFFT[](framesig, Const_1)]]\r\n```", "comments": ["You are correct, this error is because of the lack of a CPU kernel for these ops. The error message is:\r\n\r\n```\r\n No OpKernel was registered to support Op 'RFFT' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n```\r\n\r\n@petewarden : Is `freeze_graph.py` meant to work for GPU devices?\r\n\r\nwhich means that there are \"no registered kernels\" for the \"CPU\" device.", "@asimshankar Why would `freeze_graph` need to run a `Session` anyway?\r\nShouldn't it just rewrite the graph to replace `Variable`s by respective `Constant`s?", "Keeping on hold until #9029 is merged (with RFFT support)", "#9029 got merged, closing"]}, {"number": 9239, "title": "Configure Failed: Timeout connecting to...", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.2\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.1\r\n- **Bazel version (if compiling from source)**:0.4.5\r\n- **CUDA/cuDNN version**: 8.0/5\r\n- **GPU model and memory**: 1080 ti\r\n- **Exact command to reproduce**: ./configure\r\n\r\n### Describe the problem\r\n\r\nPrevious night I successfully configured and installed TensorFlow. Tonight I'm going to try the exact process but when configuring always the following error is appeared:\r\n\r\n```\r\nINFO: Timeout connecting to https://raw.githubusercontent.com/mrdoob/three.js/r77/examples/js/controls/\\\r\nOrbitControls.js\r\n```\r\n\r\nAfter I changed my proxy server, it timeout connecting to another url. It is related to [#6613](https://github.com/tensorflow/tensorflow/issues/6613).\r\nIs it possible to download links locally?\r\n", "comments": ["Is it possible to resume the process after the latest timeout?", "What version of Bazel are you using? Could you attach the full log?", "@jart  The latest Bazel (0.4.5). Here is the complete log:\r\n\r\n```\r\namir@tyrion:~/projects/tensorflow$ ./configure \r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\nDo you wish to use jemalloc as the malloc implementation? [Y/n] \r\njemalloc enabled\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] \r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] \r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] \r\nNo XLA support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\nUsing python library path: /usr/local/lib/python2.7/dist-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] \r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] Y\r\nCUDA support will be enabled for TensorFlow\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: \r\nPlease specify the location where CUDA  toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: \r\nPlease specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 6.1\r\n.........................\r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\n...........\r\nINFO: Timeout connecting to https://raw.githubusercontent.com/waylonflinn/weblas/v0.9.0/dist/weblas.js\r\namir@tyrion:~/projects/tensorflow$ \r\n\r\n```", "I'm very interested in solving this problem for you.\r\n\r\nThe log you shared above doesn't include Bazel progress messages. Could you run:\r\n\r\n```sh\r\n./configure\r\nbazel clean --expunge\r\nbazel fetch //tensorflow/... |& tee log.txt\r\n```\r\n\r\nAnd then attach log.txt to this issue?\r\n\r\nAlso would you mind running the following commands and sharing their output?\r\n\r\n```sh\r\ncurl -I http://bazel-mirror.storage.googleapis.com/raw.githubusercontent.com/waylonflinn/weblas/v0.9.0/dist/weblas.js\r\ncurl -I https://raw.githubusercontent.com/waylonflinn/weblas/v0.9.0/dist/weblas.js\r\n```", "@jart I do appreciate your help.\r\nHere is the output of running the above commands (It seems that the output of `./configure` is changed today!):\r\n\r\n```\r\namir@tyrion:~/projects/tensorflow$ ./configure\r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3.5\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\nDo you wish to use jemalloc as the malloc implementation? [Y/n] \r\njemalloc enabled\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] \r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] \r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] \r\nNo XLA support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/local/lib/python3.5/dist-packages\r\n  /usr/lib/python3/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python3.5/dist-packages]\r\n\r\nUsing python library path: /usr/local/lib/python3.5/dist-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] \r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] Y\r\nCUDA support will be enabled for TensorFlow\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5\r\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 6.1\r\n..........\r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\n..........\r\nERROR: /home/amir/projects/tensorflow/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_slider//': Error downloading [https://github.com/polymerelements/paper-slider/archive/v1.0.10.tar.gz] to /home/amir/.cache/bazel/_bazel_amir/e5c2eccaced917235e0d83c07bb9a82a/external/paper_slider/v1.0.10.tar.gz: Unknown host: github.com and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: /home/amir/projects/tensorflow/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_slider//': Error downloading [https://github.com/polymerelements/paper-slider/archive/v1.0.10.tar.gz] to /home/amir/.cache/bazel/_bazel_amir/e5c2eccaced917235e0d83c07bb9a82a/external/paper_slider/v1.0.10.tar.gz: Unknown host: github.com and referenced by '//tensorflow/tensorboard/bower:bower'.\r\nERROR: Evaluation of query \"deps(((//tensorflow/... - //tensorflow/contrib/nccl/...) - //tensorflow/examples/android/...))\" failed: errors were encountered while computing transitive closure.\r\namir@tyrion:~/projects/tensorflow$ bazel clean --expunge\r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\namir@tyrion:~/projects/tensorflow$ bazel fetch //tensorflow/... |& tee log.txt\r\n..........\r\n____Downloading http://bazel-mirror.storage.googleapis.com/github.com/bazelbuild/rules_closure/archive/5ca1dab6df9ad02050f7ba4e816407f88690cf7d.tar.gz: 31,671 bytes\r\n.....continue in the log.txt......\r\n\r\n```\r\n\r\nHere is the generated log file:\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/924235/log.txt)\r\n", "And the output of curls:\r\n\r\n```\r\namir@tyrion:~/projects/tensorflow$ curl -I http://bazel-mirror.storage.googleapis.com/raw.githubusercontent.com/waylonflinn/weblas/v0.9.0/dist/weblas.js\r\nHTTP/1.1 200 OK\r\nX-GUploader-UploadID: AEnB2Uq4nuWC4Ae6Nh7jqtvzV9MRsWIxjfG5SJMNp2s99OzyiR1ZmsSVSkSlxsvVq0VVwR7ReoyxMgoyk5tNeZyUTctDVPZtxOLoE_ijeBgA0ZsXO9zRBTI\r\nDate: Sun, 16 Apr 2017 11:36:22 GMT\r\nCache-Control: public, max-age=31536000\r\nExpires: Mon, 16 Apr 2018 11:36:22 GMT\r\nLast-Modified: Sun, 19 Mar 2017 10:48:25 GMT\r\nETag: W/\"66aa885d945c585614bfda52ff372770\"\r\nx-goog-generation: 1489920505665000\r\nx-goog-metageneration: 2\r\nx-goog-stored-content-encoding: gzip\r\nx-goog-stored-content-length: 15201\r\nContent-Type: application/javascript\r\nContent-Language: en\r\nx-goog-hash: crc32c=7wwI3Q==\r\nx-goog-hash: md5=ZqqIXZRcWFYUv9pS/zcncA==\r\nx-goog-storage-class: STANDARD\r\nVary: Accept-Encoding\r\nWarning: 214 UploadServer gunzipped\r\nServer: UploadServer\r\nTransfer-Encoding: chunked\r\n\r\namir@tyrion:~/projects/tensorflow$ curl -I https://raw.githubusercontent.com/waylonflinn/weblas/v0.9.0/dist/weblas.js\r\ncurl: (7) Failed to connect to raw.githubusercontent.com port 443: Connection timed out\r\namir@tyrion:~/projects/tensorflow$ \r\n```", "@jart Is it a way to force the script to download only not available packages? (resume a broken run in any way)", "I'm curious why the logs don't indicate Bazel attempting to download the bazel-mirror URL for weblas. Did you change workspace.bzl at all?", "This issue is solved when I'm connecting to my proxy via openconnect in the terminal.\r\nUsing openconnect via network-manager gnome was made this issue."]}, {"number": 9238, "title": "fix description", "body": "fix test file package description", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins, test this please", "Same issue with XLA and coordinator, error not in this CL"]}, {"number": 9237, "title": "In ExponentialMovingAverage class in python/training/moving_averages.py, which op will first be executed,  opt_op or maintain_average_op?", "body": "Recently, I checked the code about moving averages. But I confused about which operation will first be executed, the opt_op which apply the gradient to the variable, or the maintain_average_op which maintain and update the shadow variable? I find that the following code:\r\n` with tf.control_dependencies([opt_op]):\r\n      training_op = tf.group(maintain_averages_op)`\r\ntraining_op depends on both opt_op and maintain_averages_op. But how about the relation of opt_op   and maintain_averages_op? Which one will be executed first? ", "comments": ["This might be a better question for [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow). Please see our issue tracker [policy](https://github.com/tensorflow/tensorflow/issues/new)."]}, {"number": 9236, "title": "add \"frames\" op to \"contrib/signal\"", "body": "Added op to frame a signal into overlapping frames.\r\nInspired by [`python_speech_features.sigproc.framesig`](https://github.com/jameslyons/python_speech_features/blob/master/python_speech_features/sigproc.py#L13)\r\n\r\nMay be used in front of [spectral functions](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/api_guides/python/spectral_ops.md).\r\n\r\n```\r\npcm = tf.placeholder(tf.float32, [None, 9152])\r\nframes = tf.contrib.signal.frames(pcm, 512, 180)\r\nmagspec = tf.abs(tf.spectral.rfft(frames, [512]))\r\nimage = tf.expand_dims(magspec, 3)\r\n```", "comments": ["Can one of the admins verify this patch?", "It should work now using `tf.gather`, but I am unsure about how batches are handled.", "Which of the two would be faster or more readable or whatsoever?\r\n\r\nThis is how it's done in `python_speech_features.sigproc.framesig`:\r\n`indices_steps = array_ops.transpose(array_ops.tile(array_ops.expand_dims(math_ops.range(0, num_frames * frame_step, frame_step), 0), [frame_length, 1]))`\r\n\r\nBut I figured out that, because `expand_dims` is already inserted explicitly:\r\n`indices_steps = array_ops.tile(array_ops.expand_dims(math_ops.range(0, num_frames * frame_step, frame_step), 1), [1, frame_length])`", "yeah try to avoid a transpose if you can. they're not free.\n\nOn Apr 17, 2017 2:31 PM, \"Androbin\" <notifications@github.com> wrote:\n\n> Which of the two would be faster or more readable or whatsoever?\n>\n> This is how it's done in python_speech_features.sigproc.framesig:\n> indices_steps = array_ops.transpose(array_ops.tile(array_ops.expand_dims(math_ops.range(0,\n> num_frames * frame_step, frame_step), 0), [frame_length, 1]))\n>\n> But I figured out that, because expand_dims is already inserted\n> explicitly:\n> indices_steps = array_ops.tile(array_ops.expand_dims(math_ops.range(0,\n> num_frames * frame_step, frame_step), 1), [1, frame_length])\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9236#issuecomment-294599205>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim5PhSyrOSWunqH4xuQst359kPRZEks5rw9oUgaJpZM4M-XOE>\n> .\n>\n", "Would it make a difference to use `tf.zeros` and `tf.concat` instead of `tf.pad` just like they did with numpy?", "pad is probably a bit faster\n\nOn Apr 17, 2017 4:44 PM, \"Androbin\" <notifications@github.com> wrote:\n\n> Would it make a difference to use tf.zeros and tf.concat instead of tf.pad\n> just like they did with numpy?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9236#issuecomment-294628257>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim7miiQcFmqPp9J2efmLGuGbwQkCqks5rw_lSgaJpZM4M-XOE>\n> .\n>\n", "I think that one issue might be that `tf.gather` seems to apply `indices` to the batch dimension.", "How could `tf.gather_nd` be utilized here? #206\r\nI don't want to `tf.transpose` the `signal` two times ...", "@ebrevdo WDYT about tf.contrib.signal instead? This isn't really a spectral operation per se.", "Other signal processing ops a la scipy.signal could live there too eventually.", "SGTM for tf.contrib.signal.\n\nOn Wed, Apr 19, 2017 at 8:56 AM, RJ Skerry-Ryan <notifications@github.com>\nwrote:\n\n> Other signal processing ops a la scipy.signal could live there too\n> eventually.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9236#issuecomment-295321044>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimwkMLbXSvA40hT2-7yFWJMUObya_ks5rxi6rgaJpZM4M-XOE>\n> .\n>\n", "Okay, I moved this to contrib/signal and adapted all references accordingly.", "Not sure I understand the problem.  Is the function failing?\n\nOn Thu, Apr 20, 2017 at 6:52 AM, Androbin <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo>\n> I have tried a few variations for num_frames:\n> num_frames = tf.cast(48.0, tf.int32)\n> at least results in a proper value.\n> Any modification to that seems to break it, including:\n> num_frames = 1 + tf.cast(48.0, tf.int32)\n> num_frames = tf.cast(tf.ceil(48.0), tf.int32)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9236#issuecomment-295745647>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim5aylooooqxYPyhAPQOQZoziPE1nks5rx2MvgaJpZM4M-XOE>\n> .\n>\n", "@ebrevdo \r\nThere was an issue with shape inference which caused `tf.global_variables_initializer()` to fail.\r\nSeems to have been resolved along the way. Sorry for the trouble.", "@yaroslavvb \r\nThe problem remainins that `tf.gather` lacks support for specifying an `axis` to apply the indices to.\r\nThe current workaround is to apply `tf.transpose` before and after.\r\nIt would be great if this could be removed.", "Meanwhile, I added a basic `TestCase` for this where I simply generate a range of indices and see if they get remapped as expected.", "Apart from the workaround concerning `tf.gather` using two `tf.transpose`,\r\nwhat is currently left to do for this PR?", "will look tomorrow\n\nOn Apr 20, 2017 1:20 PM, \"Androbin\" <notifications@github.com> wrote:\n\n> Apart from the workaround concerning tf.gather using two tf.transpose,\n> what is currently left to do for this PR to get merged?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9236#issuecomment-295891384>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimwB_xBmLrNitrcPZPGalWg7dDB21ks5rx74RgaJpZM4M-XOE>\n> .\n>\n", "Most of this should work now, but I can't figure out all of `BUILD > cuda_py_tests > additional_deps`", "API review not needed for contrib/ changes.", "What failures are you getting with your existing additional_deps?\n\nOn Mon, Apr 24, 2017 at 3:36 PM, josh11b <notifications@github.com> wrote:\n\n> API review not needed for contrib/ changes.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9236#issuecomment-296841463>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim_O61KKGYmQ3WrtLQkXQ_OF_xIpyks5rzSPjgaJpZM4M-XOE>\n> .\n>\n", "Sorry for the hiccup, I had to figure out how some parts of the framework were organized.\r\nI have now successfully compiled the `py_library` as well as the `cuda_py_tests` and `Ran 3 tests in 0.105s`. But I can only remember writing 2 tests instead of 3?", "Jenkins, test this please.", "Can you add a dependency for the pip package as well? It's in pip_package/BUILD.", "@martinwicke I added the dependency for the pip package and fixed the formatting.", "Jenkins, test this please.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Jenkins, test this please.", "When you do things within a name scope context, all ops inside that scope\nhave the name-scope string associated with their op names.  you can change\nyour code from:\n\nwith ops.name_scope(...) as name:\n   ....\n\nto just:\n\nwith ops.name_scope(...):\n  ....\n\nsince you don't need the \"name\" variable.\n\nOn Thu, Apr 27, 2017 at 12:51 PM, Androbin <notifications@github.com> wrote:\n\n> *@Androbin* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/contrib/signal/python/ops/shape_ops.py\n> <https://github.com/tensorflow/tensorflow/pull/9236#discussion_r113787031>\n> :\n>\n> > +    pad_signal = array_ops.pad(\n> +        signal, [[0, 0], [0, pad_length - signal_length]])\n> +\n> +    indices_frame = array_ops.expand_dims(math_ops.range(frame_length), 0)\n> +    indices_frames = array_ops.tile(indices_frame, [num_frames, 1])\n> +\n> +    indices_step = array_ops.expand_dims(\n> +        math_ops.range(num_frames) * frame_step, 1)\n> +    indices_steps = array_ops.tile(indices_step, [1, frame_length])\n> +\n> +    indices = indices_frames + indices_steps\n> +\n> +    # TODO(Androbin) remove `transpose` when `gather` gets `axis` support\n> +    pad_signal = array_ops.transpose(pad_signal)\n> +    frames = array_ops.gather(pad_signal, indices)\n> +    frames = array_ops.transpose(frames, perm=[2, 0, 1], name=name)\n>\n> I thought this should be attached to the final op? Should this be moved\n> somewhere else? May the name_scope alias be omitted?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9236#discussion_r113787031>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim51RXzBtTXT4Z29aKOzMLDvFtqp5ks5r0PHRgaJpZM4M-XOE>\n> .\n>\n", "I have made some fixes myself (ignore CLA bot, all commits are from people who have signed the CLA).\r\n\r\n@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "Seems like an unrelated failure, merging."]}, {"number": 9235, "title": "Update op_def_registry.py", "body": "The imported file does not exist, and it is resulting in an error for my code.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please.", "Please file this as an issue, or ask on stackoverflow. This is likely a build problem. The `_pb2` file is generated by the `protoc` compiler."]}, {"number": 9234, "title": "Segfaults/NaN's in SVD", "body": "I'm getting failures trying to run SVD on a particular matrix. The result is either all NaN's for u matrix, or it's segfaults like below.\r\n\r\nTo reproduce, run this script in Python3: [https://github.com/yaroslavvb/stuff/blob/master/svd_test.py](https://github.com/yaroslavvb/stuff/blob/master/svd_test.py)\r\n\r\nI can't see anything special about this matrix beside the fact that it's badly conditioned. IE, I can perform SVD on this matrix in Mathematica [fine](https://www.wolframcloud.com/objects/f16d71a7-cc47-4a3d-b686-da440670eed3)\r\n @rmlarsen \r\n\r\n```\r\n #0  0x00007fffe320e121 in Eigen::BDCSVD<Eigen::Matrix<float, -1, -1, 1, -1, -1> >::perturbCol0(Eigen::Ref<Eigen::Array<float, -1, 1, 0, -1, 1>, 0, Eigen::InnerStride<1> > const&, Eigen::Ref<Eigen::Array<float, -1, 1, 0, -1, 1>, 0, Eigen::InnerStride<1> > const&, Eigen::Ref<Eigen::Array<long, 1, -1, 1, 1, -1>, 0, Eigen::InnerStride<1> > const&, Eigen::Matrix<float, -1, 1, 0, -1, 1> const&, Eigen::Ref<Eigen::Array<float, -1, 1, 0, -1, 1>, 0, Eigen::InnerStride<1> > const&, Eigen::Ref<Eigen::Array<float, -1, 1, 0, -1, 1>, 0, Eigen::InnerStride<1> > const&, Eigen::Ref<Eigen::Array<float, -1, 1, 0, -1, 1>, 0, Eigen::InnerStride<1> >) ()\r\n#    from /home/yaroslav/.conda/envs/whitening/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n# #1  0x00007fffe320fa81 in Eigen::BDCSVD<Eigen::Matrix<float, -1, -1, 1, -1, -1> >::computeSVDofM(long, long, Eigen::Matrix<float, -1, -1, 0, -1, -1>&, Eigen::Matrix<float, -1, 1, 0, -1, 1>&, Eigen::Matrix<float, -1, -1, 0, -1, -1>&) ()\r\n#    from /home/yaroslav/.conda/envs/whitening/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n# #2  0x00007fffe321e21c in Eigen::BDCSVD<Eigen::Matrix<float, -1, -1, 1, -1, -1> >::divide(long, long, long, long, long) ()\r\n#    from /home/yaroslav/.conda/envs/whitening/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n# #3  0x00007fffe321dbb8 in Eigen::BDCSVD<Eigen::Matrix<float, -1, -1, 1, -1, -1> >::divide(long, long, long, long, long) ()\r\n#    from /home/yaroslav/.conda/envs/whitening/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n# #4  0x00007fffe32220bd in Eigen::BDCSVD<Eigen::Matrix<float, -1, -1, 1, -1, -1> >::compute(Eigen::Matrix<float, -1, -1, 1, -1, -1> const&, unsigned int) ()\r\n#    from /home/yaroslav/.conda/envs/whitening/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n# #5  0x00007fffe32227a1 in tensorflow::SvdOp<float>::ComputeMatrix(tensorflow::OpKernelContext*, tensorflow::gtl::InlinedVector<Eigen::Map<Eigen::Matrix<float, -1, -1, 1, -1, -1> const, 0, Eigen::Stride<0, 0> >, 4> const&, tensorflow::gtl::InlinedVector<Eigen::Map<Eigen::Matrix<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, 4>*) ()                                                                 from /home/yaroslav/.conda/envs/whitening/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so                                       #6  0x00007fffe3228c75 in tensorflow::LinearAlgebraOp<float>::ComputeTensorSlice(tensorflow::OpKernelContext*, long long, tensorflow::gtl::InlinedVector<tensorflow::Tensor const*, 4> const&, tensorflow::gtl::InlinedVector<tensorflow::TensorShape, 4> const&, tensorflow::gtl::InlinedVector<tensorflow::Tensor*, 4> const&, tensorflow::gtl::InlinedVector<tensorflow::TensorShape, 4> const&) ()\r\n#    from /home/yaroslav/.conda/envs/whitening/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n\r\n\r\n\r\n```", "comments": ["@rmlarsen Do you want to take a look or triage further?", "I'm also experiencing this issue, though only segfaults. `np.linalg.svd` works on the same matrices, but calling with `tf.py_func` is of course much slower than the \"native\" tf function.", "@strubell I've had good luck with SVD through numpy. Because SVD is O(n^3) operation, the O(n^2) overhead of copying things between scipy/TF becomes negligible for 1000x1000 mats or larger. Scipy/mkl version of SVD is considerably faster\u00a0than TF version due to multi-threading, and the few crashes I've seen with scipy can be worked around by setting MKL_NUM_THREADS to lower number (ie, 15)\r\n\r\nYou can do something like [SvdWrapper](https://github.com/yaroslavvb/stuff/blob/dcc5e8d63ee0aaa7052d061e4fd155aabd2dac7f/whitening_util.py#L816) to easily switch between TensorFlow and scipy versions of SVD , example usage is in [kfac_example](https://gist.github.com/yaroslavvb/3894d69491eb8db444fc6698d89cb48e)", "@rmlarsen maybe long term solution to speed/correctness is to make gesdd available in TensorFlow? Either new implementation, or MKL version -- [gesdd](https://software.intel.com/en-us/mkl-developer-reference-fortran-gesdd?language=en) . There's also [gesvd](https://software.intel.com/en-us/mkl-developer-reference-c-gesvd) which may be more robust, but also takes 3x longer", "I just reran the test in latest TensorFlow and it works if you use GPU.\r\nOn CPU I still get NaN's (there should be no NaNs)\r\n\r\n```\r\nexport CUDA_VISIBLE_DEVICES=\r\npython svd_test.py\r\n\r\nu any NaNs: True\r\nu all NaNs: True\r\nmatrix0 any NaNs: False\r\n```", "@yaroslavvb Cool! Latest as in head or release (1.3)?", "head, it looks like GPU op was added recently", "@strubell @yaroslavvb FYI if you use py_func on a CPU tensor there's no overhead to copy it back and forth between tf and numpy in most cases (we only copy now if the tensor is not in column-order), and since numpy doesn't hold the gil during SVD you shouldn't see too much python overhead.", "@alextp we no longer need to do numpy tricks to force alignment?", "You only need to force alignment when going from numpy to tensorflow, not\nthe other way around. I assume when you take the SVD you're not sending the\nfull U, S, and V matrices back but something smaller.\n\nOn Tue, Oct 10, 2017 at 8:00 AM, Yaroslav Bulatov <notifications@github.com>\nwrote:\n\n> @alextp <https://github.com/alextp> we no longer need to do numpy tricks\n> to force alignment?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9234#issuecomment-335499616>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxV72ppi3AVEF_pp7HIRv9c4iQs3mks5sq4Z4gaJpZM4M-R0j>\n> .\n>\n\n\n\n-- \n - Alex\n", "BTW, running this example under debug mode triggers a debug assert inside Eigen\r\n\r\n`python: external/eigen_archive/Eigen/src/Core/DenseCoeffsBase.h:180: Eigen::DenseCoeffsBase<Derived, 0>::CoeffReturnType Eigen::DenseCoeffsBase<Derived, 0>::operator()(Eigen::Index) const [with Derived = Eigen::Ref<Eigen::Array<long int, 1, -1> >; Eigen::DenseCoeffsBase<Derived, 0>::CoeffReturnType = const long int&; Eigen::Index = long int]: Assertion `index >= 0 && index < size()' failed.\r\n`\r\n\r\nindex is -1\r\n\r\nfull strack trace\r\n```\r\n#0  0x00007ffff6a19428 in __GI_raise (sig=sig@entry=6)\r\n    at ../sysdeps/unix/sysv/linux/raise.c:54\r\n#1  0x00007ffff6a1b02a in __GI_abort () at abort.c:89\r\n#2  0x00007ffff6a11bd7 in __assert_fail_base (fmt=<optimized out>, \r\n    assertion=assertion@entry=0x7fffe90b0dd7 \"index >= 0 && index < size()\", \r\n    file=file@entry=0x7fffe90b0a60 \"external/eigen_archive/Eigen/src/Core/DenseCoeffsBase.h\", line=line@entry=180, \r\n    function=function@entry=0x7fffe90f83a0 <Eigen::DenseCoeffsBase<Eigen::Ref<Eigen::Array<long, 1, -1, 1, 1, -1>, 0, Eigen::InnerStride<1> >, 0>::operator()(long) const::__PRETTY_FUNCTION__> \"Eigen::DenseCoeffsBase<Derived, 0>::CoeffReturnType Eigen::DenseCoeffsBase<Derived, 0>::operator()(Eigen::Index) const [with Derived = Eigen::Ref<Eigen::Array<long int, 1, -1> >; Eigen::DenseCoeffsBas\"...) at assert.c:92\r\n#3  0x00007ffff6a11c82 in __GI___assert_fail (\r\n    assertion=0x7fffe90b0dd7 \"index >= 0 && index < size()\", \r\n    file=0x7fffe90b0a60 \"external/eigen_archive/Eigen/src/Core/DenseCoeffsBase.h\", line=180, \r\n    function=0x7fffe90f83a0 <Eigen::DenseCoeffsBase<Eigen::Ref<Eigen::Array<long, 1, -1, 1, 1, -1>, 0, Eigen::InnerStride<1> >, 0>::operator()(long) const::__PRETTY_FUNCTION__> \"Eigen::DenseCoeffsBase<Derived, 0>::CoeffReturnType Eigen::DenseCoeffsBase<Derived, 0>::operator()(Eigen::Index) const [with Derived = Eigen::Ref<Eigen::Array<long int, 1, -1> >; Eigen::DenseCoeffsBas\"...)\r\n    at assert.c:101\r\n#4  0x00007fffe36caf0a in Eigen::DenseCoeffsBase<Eigen::Ref<Eigen::Array<long, 1, -1, 1, 1, -1>, 0, Eigen::InnerStride<1> >, 0>::operator() (\r\n    this=0x7fff9cff6850, index=-1)\r\n    at external/eigen_archive/Eigen/src/Core/DenseCoeffsBase.h:180\r\n#5  0x00007fffe386ece0 in Eigen::BDCSVD<Eigen::Matrix<float, -1, -1, 1, -1, -1> >::perturbCol0 (this=0x7fff9cff7540, col0=..., diag=..., perm=..., \r\n    singVals=..., shifts=..., mus=..., zhat=...)\r\n    at external/eigen_archive/Eigen/src/SVD/BDCSVD.h:908\r\n#6  0x00007fffe386b161 in Eigen::BDCSVD<Eigen::Matrix<float, -1, -1, 1, -1, -1> >::computeSVDofM (this=0x7fff9cff7540, firstCol=393, n=391, U=..., \r\n    singVals=..., V=...) at external/eigen_archive/Eigen/src/SVD/BDCSVD.h:637\r\n#7  0x00007fffe386806f in Eigen::BDCSVD<Eigen::Matrix<float, -1, -1, 1, -1, -1> >::divide (this=0x7fff9cff7540, firstCol=393, lastCol=783, firstRowW=393, \r\n    firstColW=393, shift=0)\r\n    at external/eigen_archive/Eigen/src/SVD/BDCSVD.h:533\r\n---Type <return> to continue, or q <return> to quit---\r\n#8  0x00007fffe3867253 in Eigen::BDCSVD<Eigen::Matrix<float, -1, -1, 1, -1, -1> >::divide (this=0x7fff9cff7540, firstCol=0, lastCol=783, firstRowW=0, \r\n    firstColW=0, shift=0) at external/eigen_archive/Eigen/src/SVD/BDCSVD.h:428\r\n#9  0x00007fffe38661f3 in Eigen::BDCSVD<Eigen::Matrix<float, -1, -1, 1, -1, -1> >::compute (this=0x7fff9cff7540, matrix=..., computationOptions=40)\r\n    at external/eigen_archive/Eigen/src/SVD/BDCSVD.h:277\r\n#10 0x00007fffe3865a67 in Eigen::BDCSVD<Eigen::Matrix<float, -1, -1, 1, -1, -1> >::BDCSVD (this=0x7fff9cff7540, matrix=..., computationOptions=40)\r\n    at external/eigen_archive/Eigen/src/SVD/BDCSVD.h:136\r\n#11 0x00007fffe386587a in tensorflow::SvdOp<float>::ComputeMatrix (\r\n    this=0x255f7d0, context=0x7fff9cff8900, inputs=..., \r\n    outputs=0x7fff9cff7770) at ./tensorflow/core/kernels/svd_op_impl.h:88\r\n#12 0x00007fffe38afc66 in tensorflow::LinearAlgebraOp<float>::ComputeTensorSlice (this=0x255f7d0, context=0x7fff9cff8900, matrix_index=0, inputs=..., \r\n    input_matrix_shapes=..., outputs=..., output_matrix_shapes=...)\r\n    at tensorflow/core/kernels/linalg_ops_common.cc:247\r\n#13 0x00007fffe38b4b5a in tensorflow::LinearAlgebraOp<float>::Compute(tensorflow::OpKernelContext*)::{lambda(long long, long long)#1}::operator()(long long, long long) const (__closure=0x7fff78000c20, begin=0, end=1)\r\n    at tensorflow/core/kernels/linalg_ops_common.cc:104\r\n#14 0x00007fffe38b73aa in std::_Function_handler<void (long long, long long), tensorflow::LinearAlgebraOp<float>::Compute(tensorflow::OpKernelContext*)::{lambda(long long, long long)#1}>::_M_invoke(std::_Any_data const&, long long&&, std::_Any_data const&) (__functor=..., \r\n    __args#0=<unknown type in /home/yaroslav/anaconda3/envs/tf13dbg/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, CU 0x1373b2da, DIE 0x137a9da2>, \r\n    __args#1=<unknown type in /home/yaroslav/anaconda3/envs/tf13dbg/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, CU 0x1373b2da, DIE 0x137a9da7>) at /usr/include/c++/5/functional:1871\r\n#15 0x00007fffe5606ec7 in std::function<void (long long, long long)>::operator()(long long, long long) const (this=0x7fff9cff7e00, __args#0=0, __args#1=1)\r\n    at /usr/include/c++/5/functional:2267\r\n#16 0x00007fffe814d296 in tensorflow::thread::ThreadPool::Impl::ParallelFor(long long, long long, std::function<void (long long, long long)>)::{lambda(long, long)#1}::operator()(long, long) const (__closure=0x7fff9cff7ba0, first=0, \r\n    last=1) at tensorflow/core/lib/core/threadpool.cc:100\r\n#17 0x00007fffe814f744 in std::_Function_handler<void (long, long), tensorflow:---Type <return> to continue, or q <return> to quit---\r\n:thread::ThreadPool::Impl::ParallelFor(long long, long long, std::function<void (long long, long long)>)::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long&&, std::_Any_data const&) (__functor=..., \r\n    __args#0=<unknown type in /home/yaroslav/anaconda3/envs/tf13dbg/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, CU 0x45a6c0ff, DIE 0x45a8a9d0>, \r\n    __args#1=<unknown type in /home/yaroslav/anaconda3/envs/tf13dbg/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, CU 0x45a6c0ff, DIE 0x45a8a9d5>) at /usr/include/c++/5/functional:1871\r\n#18 0x00007fffe2537f55 in std::function<void (long, long)>::operator()(long, long) const (this=0x7fff9cff7ba0, __args#0=0, __args#1=1)\r\n    at /usr/include/c++/5/functional:2267\r\n#19 0x00007fffe2537849 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const (this=0x7fff9cff7c10, n=1, cost=..., block_align=..., f=...)\r\n    at external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceThreadPool.h:177\r\n#20 0x00007fffe814ce49 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<void (long, long)>) const (\r\n    this=0x7fff9cff7c10, n=1, cost=..., f=...)\r\n    at external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceThreadPool.h:257\r\n#21 0x00007fffe814d4c8 in tensorflow::thread::ThreadPool::Impl::ParallelFor(long long, long long, std::function<void (long long, long long)>) (\r\n    this=0x106d810, total=1, cost_per_unit=5782683648, fn=...)\r\n    at tensorflow/core/lib/core/threadpool.cc:100\r\n#22 0x00007fffe814bec6 in tensorflow::thread::ThreadPool::ParallelFor(long long, long long, std::function<void (long long, long long)>) (this=0x1044540, \r\n    total=1, cost_per_unit=5782683648, fn=...)\r\n    at tensorflow/core/lib/core/threadpool.cc:128\r\n#23 0x00007fffe60ef1fc in tensorflow::Shard(int, tensorflow::thread::ThreadPool*, long long, long long, std::function<void (long long, long long)>) (\r\n    max_parallelism=12, workers=0x1044540, total=1, cost_per_unit=5782683648, \r\n    work=...) at tensorflow/core/util/work_sharder.cc:35\r\n#24 0x00007fffe38af084 in tensorflow::LinearAlgebraOp<float>::Compute (\r\n    this=0x255f7d0, context=0x7fff9cff8900)\r\n    at tensorflow/core/kernels/linalg_ops_common.cc:109\r\n#25 0x00007fffe5786d8b in tensorflow::ThreadPoolDevice::Compute (\r\n---Type <return> to continue, or q <return> to quit---\r\n    this=0xcdd990, op_kernel=0x255f7d0, context=0x7fff9cff8900)\r\n    at tensorflow/core/common_runtime/threadpool_device.cc:59\r\n#26 0x00007fffe572a36c in tensorflow::(anonymous namespace)::ExecutorState::Process (this=0x255dd60, tagged_node=..., scheduled_usec=0)\r\n    at tensorflow/core/common_runtime/executor.cc:1652\r\n#27 0x00007fffe572c358 in tensorflow::(anonymous namespace)::ExecutorState::<lambda()>::operator()(void) const (__closure=0x255eb60)\r\n    at tensorflow/core/common_runtime/executor.cc:2055\r\n#28 0x00007fffe5733284 in std::_Function_handler<void(), tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(const TaggedNodeSeq&, tensorflow::(anonymous namespace)::ExecutorState::TaggedNodeReadyQueue*)::<lambda()> >::_M_invoke(const std::_Any_data &) (__functor=...)\r\n    at /usr/include/c++/5/functional:1871\r\n#29 0x00007fffe1caabbe in std::function<void ()>::operator()() const (\r\n    this=0x2559f00) at /usr/include/c++/5/functional:2267\r\n#30 0x00007fffe814d1c6 in tensorflow::thread::EigenEnvironment::ExecuteTask (\r\n    this=0x2522338, t=...) at tensorflow/core/lib/core/threadpool.cc:81\r\n#31 0x00007fffe814f409 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop (this=0x2522330, thread_id=6)\r\n    at external/eigen_archive/unsupported/Eigen/CXX11/src/ThreadPool/NonBlockingThreadPool.h:232\r\n#32 0x00007fffe814dc36 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::NonBlockingThreadPoolTempl(int, bool, tensorflow::thread::EigenEnvironment)::{lambda()#1}::operator()() const ()\r\n    at external/eigen_archive/unsupported/Eigen/CXX11/src/ThreadPool/NonBlockingThreadPool.h:65\r\n#33 0x00007fffe8150758 in std::_Function_handler<void (), Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::NonBlockingThreadPoolTempl(int, bool, tensorflow::thread::EigenEnvironment)::{lambda()#1}>::_M_invoke(std::_Any_data const&) (__functor=...) at /usr/include/c++/5/functional:1871\r\n#34 0x00007fffe1caabbe in std::function<void ()>::operator()() const (\r\n    this=0x2553a80) at /usr/include/c++/5/functional:2267\r\n#35 0x00007fffe814cf2f in tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}::operator()() const (__closure=0x2553a80)\r\n    at tensorflow/core/lib/core/threadpool.cc:56\r\n#36 0x00007fffe814e6ac in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) (__functor=...) at /usr/include/c++/5/functional:1871\r\n\r\n\r\n```", "@rmlarsen any update?", "Just tried it out on with latest MKL-compiled TensorFlow on Amazon instance, and crash was not there -- https://github.com/yaroslavvb/stuff/blob/master/linalg-benchmark/launch_tensorflow_svd_crash.py\r\n\r\nHowever I suspect that takes a different code path (MKL)", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 9233, "title": "Hello All, i am using Floyd, yet I keep getting the error -  ImportError: No module named 'tensorflow'", "body": "Please go to Stack Overflow for help and support: http://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script: https://github.com/tensorflow/tensorflow/tree/master/tools\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\n\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 9232, "title": "One set of GPUs on same machine and same model work well, another gets OOM error", "body": "I am using multiple GPUs (num_gpus = 4) for training one model with multiple towers. The model is training well on one set of GPUs: `CUDA_VISIBLE_DEVICES = 0,1,2,3` while it gets OOM problem during the first graph evaluation with `CUDA_VISIBLE_DEVICES = 0,1,4,5`\r\n\r\n\r\nFollowing options are used for creating a session\r\n```python\r\nsession_config=tf.ConfigProto(\r\n      allow_soft_placement=True,\r\n      log_device_placement=False)\r\nsession_config.gpu_options.per_process_gpu_memory_fraction = 0.94\r\nsession_config.gpu_options.allow_growth=False\r\n```\r\n\r\nBatch size, is already super small, = 3\r\n\r\n\r\n\r\n   \r\n### System information\r\nTensorflow 1.0\r\nCuda 8.0\r\nUbuntu 14.04.5 LTS\r\nAll GPUs : GeForce GTX 1080 \r\n\r\n### Source code / logs\r\n```\r\nname: GeForce GTX 1080\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7335\r\npciBusID 0000:07:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 7.81GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0xcc4593a0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: \r\nname: GeForce GTX 1080\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7335\r\npciBusID 0000:08:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 7.81GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0xd2404670\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 2 with properties: \r\nname: GeForce GTX 1080\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7335\r\npciBusID 0000:18:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 7.81GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0xd25591b0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 3 with properties: \r\nname: GeForce GTX 1080\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7335\r\npciBusID 0000:1c:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 7.81GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 2 3 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y Y Y Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   Y Y Y Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 2:   Y Y Y Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 3:   Y Y Y Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:07:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 1080, pci bus id: 0000:08:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:2) -> (device: 2, name: GeForce GTX 1080, pci bus id: 0000:18:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:3) -> (device: 3, name: GeForce GTX 1080, pci bus id: 0000:1c:00.0)\r\n\r\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 47441 get requests, put_count=8461 evicted_count=1000 eviction_rate=0.118189 and unsatisfied allocation rate=0.844839\r\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.33GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 3.08GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 3.08GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 3.98GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 3.98GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.54GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.54GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 3.17GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.68GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 3.86GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2698 get requests, put_count=8709 evicted_c \r\n```", "comments": ["This might be a better question for [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow). Please see our issue tracker [policy](https://github.com/tensorflow/tensorflow/issues/new)."]}, {"number": 9231, "title": "(DO NOT MERGE) Branch 153227038", "body": "", "comments": ["Thanks! Do you need this now? I am still doing the merge from github.", "@drpngx no rush. I can wait for your pull to go through first.", "OK, thanks! Feel free to merge if you need to and I can resolve later, but otherwise it would be easier to wait.", "OK. I'll just leave the PR open for testing purposes. I'll open a new one once your pull is done.", "@tensorflow-jenkins test this please"]}, {"number": 9230, "title": "random ops across iterations of while loops execute in nondeterministic order", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes.  See below.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: \r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: `('0.12.1-2106-gf7d07f5-dirty', '0.12.head')`\r\n- **Bazel version (if compiling from source)**: (not a build issue)\r\n- **CUDA/cuDNN version**: CuDNN5, CUDA8 (though this runs on CPU)\r\n- **GPU model and memory**: 1x Titan X Pascal, 1x Titan X (though this runs on CPU)\r\n- **Exact command to reproduce**: `../virtualenv/bin/python nnrandom.py --broken; ../virtualenv/bin/python nnrandom.py --broken`\r\n\r\n### Describe the problem, source code, and logs\r\n\r\nWhen using augmentation primitives, sometimes, they create run-to-run non-determinism by ignoring the system random seed.  This makes reproducing a run impossible, even when using `tf.set_random_seed`.  One case in which they seem to do this is inside a `tf.device()` block.  Consider the following snippet:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ntf.app.flags.DEFINE_integer(\"seed\", 1, \"RNG seed\")\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\ntf.app.flags.DEFINE_float(\"augment_hue\", 0.5, \"hue augment factor\")\r\ntf.app.flags.DEFINE_boolean(\"no_augment\", False, \"whether to disable augmentation entirely\")\r\ndef mk_input():\r\n  np.random.seed(0)\r\n  a1s = np.random.uniform(size = [32, 128, 128, 3]).astype(np.float32)\r\n\r\n  if not FLAGS.no_augment:\r\n    def aug(img):\r\n      img = tf.image.random_hue(img, FLAGS.augment_hue, seed = 50001)\r\n\r\n      return img\r\n    a1s = tf.map_fn(aug, a1s)\r\n\r\n  return a1s\r\n\r\ntf.app.flags.DEFINE_boolean('broken', False, '')\r\ndef main(argv=None):\r\n  if len(argv) != 1:\r\n    print \"argv was\",argv\r\n    raise RuntimeError('unknown argument')\r\n  print('nn: building graph')\r\n\r\n  tf.set_random_seed(FLAGS.seed)\r\n\r\n  with tf.Session(config=tf.ConfigProto(allow_soft_placement = True)) as sess:\r\n    if FLAGS.broken:\r\n      with tf.device('/cpu:0'):\r\n        input = mk_input()\r\n    else:\r\n      input = mk_input()\r\n\r\n    a = tf.reduce_mean(input)\r\n\r\n    tf.global_variables_initializer().run()\r\n    tf.train.start_queue_runners(sess = sess)\r\n    print('nn: TF session initialized')\r\n\r\n    for step in xrange(10):\r\n      res = sess.run(a)\r\n      print(res)\r\n\r\nif __name__ == '__main__':\r\n  tf.app.run()\r\n```\r\n\r\nIf you run this without `--broken`, then you'll get the same result every time.  On my machine, the result without `--broken` starts 0.500045.  But if you run this with `--broken`, then you'll get a different result each time.  For instance:\r\n\r\n```\r\njwise@jwise-dt:/home/scratch.jwise_dl$ ../virtualenv/bin/python nnrandom.py  --broken 2>&1 | tail -n5\r\n0.500318\r\n0.500236\r\n0.500246\r\n0.500185\r\n0.500303\r\njwise@jwise-dt:/home/scratch.jwise_dl$ ../virtualenv/bin/python nnrandom.py  --broken 2>&1 | tail -n5\r\n0.500279\r\n0.500218\r\n0.500164\r\n0.500178\r\n0.500106\r\n```\r\n\r\nI have not dug any deeper yet as to where, exactly, the nondeterminism is coming from, but I figured that this is pretty close to a minimal repro case based on TF primitives.\r\n\r\nThanks,\r\njoshua", "comments": ["We had another set_random_seed issue filed recently in https://github.com/tensorflow/tensorflow/issues/9171. Thoughts @girving?", "@jart -- I took a look at #9171, and I think it is unrelated.  (#9171, arguably, is at least 'sensible' behavior, if a little unexpected.)  #9003 might be a little more plausible, though in this case, I'm actually /not/ using `seed=0` for either of them, so I think that's also unrelated (e9786df fixes one specific gotcha for `(0,0)` seeds, which this is not).", "I'm wondering if it's related to `tf.map_fn`.  Technically, there might not be a requirement for `tf.map_fn` to compute in any particular order; if this behavior is triggered it would explain your issue.\r\n\r\n@ebrevdo Does that sound right to you with your additional control flow knowledge?", "I just tested, and indeed, it does not happen without the `tf.map_fn`.\r\n\r\nBut if there is no requirement for `tf.map_fn` to run in a deterministic order, there probably should be one!  Otherwise, there is no way to deterministically augment a batch of images, right?", "one of the main benefits of map_fn is that it can perform the transforms in parallel... it's faster.  however it does affect how seeded ops work.", "i wonder however why this behaves differently on cpu and gpu.  perhaps on gpu the random seed is somehow serialized?  what if instead of map_fn you just have an unstack/apply/stack sequence (different seed for each image)?  same behavior?", "Unfortunately the stateful seeding behavior is buried pretty deep in the random ops.  Making `map_fn` work in parallel while treating that behavior correctly would require some significant redesigns.\r\n\r\nConceivably one could use the new stateless random ops here (https://github.com/tensorflow/tensorflow/commit/cc45456e4ad0eff16127d1727d0cf48afb71ca0e), but that would still require higher level layers to track the seeds in a reasonable way.", "Indeed, the random hue layer would have to allow stateless seeds;\n\nthe map_fn could then be changed to:\n\ndef sampler(img, seed):\n  this_seed = tf.stack((global_seed, seed))\n  return ...sample...(img, ..., seed=this_seed)\n\nsampled_images = map_fn(sampler, images, 5000 + tf.range(0,\nshape(images)[0]))\n\n\nOn Tue, Apr 18, 2017 at 8:53 AM, Geoffrey Irving <notifications@github.com>\nwrote:\n\n> Unfortunately the stateful seeding behavior is buried pretty deep in the\n> random ops. Making map_fn work in parallel while treating that behavior\n> correctly would require some significant redesigns.\n>\n> Conceivably one could use the new stateless random ops here (cc45456\n> <https://github.com/tensorflow/tensorflow/commit/cc45456e4ad0eff16127d1727d0cf48afb71ca0e>),\n> but that would still require higher level layers to track the seeds in a\n> reasonable way.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9230#issuecomment-294889184>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim6ZIn1CjXaATUjNF0bb353Gl4um2ks5rxNxngaJpZM4M-Mpw>\n> .\n>\n", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.", "@itsmeolivia I just tried this with the latest version of TensorFlow, installed from https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.2.0-cp27-none-linux_x86_64.whl .  The issue still reproduces, and so this should be reopened.  Unfortunately, I don't have access to do so.\r\n\r\nAs an aside, aging a bug out seems like bad policy.  I spent a long time (multiple days) tracking this bug down, and minimizing it to the point that it would be suitable to file in a bug tracker.  This was a very large effort investment from me; I could have simply worked around it on my own, but I reported it with the goal of helping the TensorFlow community.  Aging bugs out by default without an explicit fix sends the message to those who would report bugs that their work is not valued; it certainly made me feel very frustrated.  I hope the TensorFlow team will reconsider the decision to act in this fashion.", "@jwise We're happy to keep issues open if there is some hope of fixing them.  However, this particular issue is very deeply tied to TensorFlow's current operational semantics.  There are plans for future redesigns that might fix it, but it's not likely to happen anytime soon.\r\n\r\n@josh11b Maybe we should keep a list of bugs that could be fixed by your resource-oriented redesign of stateful ops?  I'll assign to you in case that sounds reasonable; feel free to rearrange if not.", "@girving Thanks.  NOTABUG is an understandable disposition (although it would make `random_hue` more difficult to use), though auto-closing is quite a lot more frustrating :smiley:  Looking forward to the longer-term fix for this!", "@jwise To clarify: I do think this should be fixed, so I'm not calling it NOTABUG.  It's just hard. :)", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Closing due to inactivity, but should be fixed, is hard, and might not be something we have the time to solve.", "I do have an initial change that adds *some* determinism for the state\ndropout, but it's not easy to add such behavior for the input or output\ndropouts.  It may be worth using the stateless random ops for dropout\n(optionally), but for that I would probably request we create a\nStatelessDropoutWrapper in contrib.  PRs are welcome in this case!\n\nOn Fri, Dec 22, 2017 at 1:27 PM, Justine Tunney <notifications@github.com>\nwrote:\n\n> Closed #9230 <https://github.com/tensorflow/tensorflow/issues/9230>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9230#event-1400141111>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim5t0EEma5vUiLgE56hpi66uFFZIEks5tDB7WgaJpZM4M-Mpw>\n> .\n>\n", "Again, sad to hear of it being closed from aging out.  (I note that this specific training instability, in my case, was not dropout, but augmentation, though.)", "It is possible to avoid the instability by running the\ndynamic_rnn/while_loop with parallel_iterations=1 ... at the risk of\nreducing performance.\n\nOn Sat, Dec 23, 2017 at 5:42 PM, Joshua Wise <notifications@github.com>\nwrote:\n\n> Again, sad to hear of it being closed from aging out. (I note that this\n> specific training instability, in my case, was not dropout, but\n> augmentation, though.)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9230#issuecomment-353760062>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim0vaF-8h7gdxmRFRIcEjDHFgRkpxks5tDavrgaJpZM4M-Mpw>\n> .\n>\n", "tf.image.random_hue is a stateful op and as a general rule tf.map_fn shouldn't be used with a stateful op. There should be a stateless version of tf.image.random_hue (implemented by e.g. stateless_normal from [cc45456](https://github.com/tensorflow/tensorflow/commit/cc45456e4ad0eff16127d1727d0cf48afb71ca0e))."]}, {"number": 9229, "title": "An ugly hack to compile on Tegra X1 /w Jetpack 2.3.1 release.", "body": "* Applied patch from https://github.com/jetsonhacks/installTensorFlowTX1\r\n* Revert Eigen to revision used in Tensorflow r0.11 to avoid cuda compile error\r\n* Remove expm1 op that was added with new additions to Eigen", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "@benoitsteiner do you have time to take a look at this PR?", "We certainly don't want to accept ugly hacks. I'll close this PR. I think we would welcome a principled solution to making this work on Tegra X1. @petewarden FYI.", "Sorry we couldn't take this one! I am very keen to see better TX1 support, would you be able to at least write up a quick doc about the process you went through, and maybe link to a fork containing this patch?"]}, {"number": 9228, "title": "Adding MKL op for reshape and several fixes to other ops", "body": "", "comments": ["Can one of the admins verify this patch?", "@zhangyaobit We have made the changes from the review. Please proceed with the review/testing.", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 9227, "title": "Correct tf.matmul() keyword reference in docs", "body": "In `tf.sparse_dense_matmul()` docstring:\r\n\r\nWas `tf.matmul(sp_a=True)`\r\n\r\nNow `tf.matmul(a_is_sparse)`", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!\n\nOn Fri., 14 Apr. 2017, 3:54 pm googlebot, <notifications@github.com> wrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project. Before we can look at your\n> pull request, you'll need to sign a Contributor License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed, please reply here (e.g. I signed it!) and we'll\n> verify. Thanks.\n> ------------------------------\n>\n>    - If you've already signed a CLA, it's possible we don't have your\n>    GitHub username or you're using a different email address. Check your\n>    existing CLA data <https://cla.developers.google.com/clas> and verify\n>    that your email is set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - If you signed the CLA as a corporation, please let us know the\n>    company's name.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9227#issuecomment-294251613>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AB64wyQzIxpdgEmLhwrXHFBtl-3WwVW6ks5rv_kpgaJpZM4M-KQ1>\n> .\n>\n", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins , test this please"]}, {"number": 9226, "title": "Keras + tensorflow + P100 : cudaErrorNotSupported = 71 error", "body": "Allow me please to cross post, upon sugestion, an error faced in using keras with tensorflow with machine equipped with P100 https://github.com/fchollet/keras/issues/6054\r\nApologies if this has been reported already at some other place, I have been googling for it quite a bit, with my colleague without success.\r\nWhile running the simple mnist example (https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py) with keras+tensorflow using P100 GPGPU we encounter an issue at the intersection of keras/tensorflow/cuda\r\n\r\nUsing TensorFlow backend.\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\r\nname: Tesla P100-PCIE-16GB\r\nmajor: 6 minor: 0 memoryClockRate (GHz) 1.3285\r\npciBusID 0000:02:00.0\r\nTotal memory: 15.89GiB\r\nFree memory: 15.51GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0: Y\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:02:00.0)\r\nF tensorflow/core/common_runtime/gpu/gpu_device.cc:121] Check failed: err == cudaSuccess (71 vs. 0)\r\nsrun: error: nid02011: task 0: Aborted\r\nsrun: Terminating job step 1262138.0\r\n\r\nwe are using keras 2.0.2, tensorflow 1.0.0. cuda 8.0.53.\r\nWe seem to be having this issue both in python2.7.12 and python3.5.2 (keras 1.2 and 2.0 ...)\r\nBare tensorflow runtest are going fine, which lead us to think that this is really at the intersection of keras/tensorflow/cuda.\r\nThe same test runs fine on various machine with the same version of the software but with TitanX GPGPU.\r\n\r\n seem to be tracing this back to\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/core/common_runtime/gpu/gpu_device.cc#L121\r\n\r\nhttp://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038\r\n\r\n\"\"\"\r\ncudaErrorNotSupported = 71\r\nThis error indicates the attempted operation is not supported on the current system or device.\r\n\"\"\"\r\n\r\nI am clueless on where to look next to solve this issue. I would greatly appreciate any feedback and guidance on this matter.\r\n", "comments": ["Please don't crosspost. This might be a better question for [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow). Please see our issue tracker [policy](https://github.com/tensorflow/tensorflow/issues/new).", "My apologies.\r\nThanks for the suggestion http://stackoverflow.com/questions/43471544/keras-tensorflow-p100-cudaerrornotsupported-71-error\r\n#9080 seems related.\r\n"]}, {"number": 9225, "title": "Restoring RNN model from checkpoint fails when hidden layer width is not equal to input/output width", "body": "Hi, I'm not sure whether this is actually a bug, or I'm doing something wrong, so please treat accordingly.\r\n\r\nI have an RNN model defined as following:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nnn_layers = 3\r\n\r\ndef RNN(x, weights, out_biases, input_size, n_steps, nn_hidden, keep_prob, forget_bias=0.0):\r\n    # Input data shape: (batch_size, n_steps, input_size)\r\n    # Output data shape: (batch_size, output_size)\r\n    # Permute, reshape and split to get n_steps tensors of shape (batch_size, input_size)\r\n    x = tf.transpose(x, [1, 0, 2])\r\n    x = tf.reshape(x, [-1, input_size])\r\n    x = tf.split(axis=0, num_or_size_splits=n_steps, value=x)\r\n    \r\n    # define RNN structure\r\n    \r\n    #layer_cell = tf.contrib.rnn.BasicLSTMCell(nn_hidden, forget_bias=forget_bias)\r\n    layer_cell = tf.contrib.rnn.GRUCell(nn_hidden)\r\n    \r\n    cell = tf.contrib.rnn.DropoutWrapper(layer_cell, output_keep_prob=keep_prob)\r\n    cell = tf.contrib.rnn.MultiRNNCell([cell] * nn_layers)\r\n    output, state = tf.contrib.rnn.static_rnn(cell, x, dtype=tf.float32)\r\n    \r\n    pred = tf.matmul(output[-1], weights[\"out\"]) + out_biases[\"out\"]\r\n    \r\n    \r\n    return pred\r\n\r\ndef vs3_RNN(x, input_size, n_steps, nn_hidden, output_size, keep_prob, forget_bias=0.0):\r\n    \r\n    # Define weights -- output layer\r\n    weights = {\r\n        'out': tf.Variable(tf.random_normal([nn_hidden, output_size]), name=\"smax_w\")\r\n    }\r\n    out_biases = {\r\n        'out': tf.Variable(tf.random_normal([output_size]), name=\"smax_b\")\r\n    }\r\n    \r\n    pred = RNN(x, weights, out_biases, input_size, n_steps, nn_hidden, keep_prob, forget_bias=forget_bias)\r\n    \r\n    return pred\r\n\r\n```\r\n\r\nI use the vs3_RNN method for both training and testing. The size of my input and output vectors (input_size, output_size) is 500. When I set the width of my hidden layer (nn_hidden) to 500, everything works great. However, when I set it to something else, I can train the model and save the checkpoint fine -- but when I try to restore the model from checkpoint, I get an error message.\r\n\r\nHere's a stack trace with nn_hidden equal to 600:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"thdvector/vs3_service.py\", line 43, in <module>\r\n    saver.restore(sess, tf.train.latest_checkpoint(SAVEDIR))\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1439, in restore\r\n    {self.saver_def.filename_tensor_name: save_path})\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 767, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 965, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [500] rhs shape= [600]\r\n\t [[Node: save/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@rnn/multi_rnn_cell/cell_0/gru_cell/candidate/biases\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](rnn/multi_rnn_cell/cell_0/gru_cell/candidate/biases, save/RestoreV2)]]\r\n\r\nCaused by op u'save/Assign', defined at:\r\n  File \"thdvector/vs3_service.py\", line 41, in <module>\r\n    saver = tf.train.Saver()\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1051, in __init__\r\n    self.build()\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1081, in build\r\n    restore_sequentially=self._restore_sequentially)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 675, in build\r\n    restore_sequentially, reshape)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 414, in _AddRestoreOps\r\n    assign_ops.append(saveable.restore(tensors, shapes))\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 155, in restore\r\n    self.op.get_shape().is_fully_defined())\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 47, in assign\r\n    use_locking=use_locking, name=name)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [500] rhs shape= [600]\r\n\t [[Node: save/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@rnn/multi_rnn_cell/cell_0/gru_cell/candidate/biases\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](rnn/multi_rnn_cell/cell_0/gru_cell/candidate/biases, save/RestoreV2)]]\r\n\r\n\r\n```\r\n\r\nI have tried this with either GRUCell or BasicLSTMCell, and I get the error message regardless.\r\n\r\nHere's the code that run that tries to restore the checkpoint and fails:\r\n\r\n```\r\nx = tf.placeholder(\"float\", [1, n_steps, input_size], name=\"x_in\")\r\n    \r\n    pred = vs_model.vs3_RNN(x, input_size, n_steps, nn_hidden, output_size, keep_prob=testing_keep_prob, forget_bias=forget_bias)\r\n    \r\n    init = tf.global_variables_initializer()\r\n    \r\n    with tf.Session() as sess:\r\n        sess.run(init)\r\n        print(\"Variables initialized\")\r\n        \r\n        saver = tf.train.Saver()\r\n        #saver = tf.train.import_meta_graph(META)\r\n        saver.restore(sess, tf.train.latest_checkpoint(SAVEDIR))\r\n        print(\"Model restored from \" + str(SAVEDIR))\r\n        ...\r\n```\r\n\r\nThanks.", "comments": ["This might be a better question for [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow). Please see our issue tracker [policy](https://github.com/tensorflow/tensorflow/issues/new)."]}, {"number": 9224, "title": "Build Error: libcudnn.so.5: file not recognized: File truncated collect2: error: ld returned 1 exit status", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.2\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: master\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: 8.0/5.1\r\n- **GPU model and memory**: 1080 Ti\r\n- **Exact command to reproduce**: bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\nYou can collect some of this information using our environment capture script: https://github.com/tensorflow/tensorflow/tree/master/tools\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nGet the latest source code and running \"bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\" to build GPU version. Cuda and CuDNN completely installed and all samples are passed.\r\n\r\n### Source code / logs\r\nERROR: /home/amir/projects/tensorflow/tensorflow/python/BUILD:2534:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed: link_dynamic_library.sh failed: error executing command external/bazel_tools/tools/cpp/link_dynamic_library.sh no ignored ignored ignored external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -shared -o ... (remaining 27 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nbazel-out/local_linux-py3-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Slib/libcudnn.so.5: file not recognized: File truncated\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 2259.385s, Critical Path: 2229.15s\r\n", "comments": ["From the error message: `libcudnn.so.5: file not recognized: File truncated` it seems that the `libcuddn.so.5` file is corrupt on your system. You might want ot make sure that it is pointing to the right place (run `file bazel-out/local_linux-py3-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Slib/libcudnn.so.5`) and that the file is indeed pristine.", "It seems this issue is related to [#7663](https://github.com/tensorflow/tensorflow/issues/7663). Re-Installing CuDNN solved the problem."]}, {"number": 9223, "title": "small change to format code", "body": "small change to format code", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 9222, "title": "Bazel build  error loading package 'tensorflow/core': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf//': Traceback (most recent call last):", "body": "I'm trying to build a simple script,  however while compiling i get the following error: \r\n\r\nERROR: C:/users/john/downloads/tensorflow-master/tensorflow/loader/BUILD:1:1: error loading package 'tensorflow/core': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf//': Traceback (most recent call last):\r\n        File \"C:/users/john/downloads/tensorflow-master/tensorflow/workspace.bzl\", line 88\r\n                _apply_patch(repo_ctx, repo_ctx.attr.patch_file)\r\n        File \"C:/users/john/downloads/tensorflow-master/tensorflow/workspace.bzl\", line 79, in _apply_patch\r\n                _execute_and_check_ret_code(repo_ctx, [\"patch\", \"-p1\", \"-d\", r...), <2 more arguments>])\r\n        File \"C:/users/john/downloads/tensorflow-master/tensorflow/workspace.bzl\", line 71, in _execute_and_check_ret_code\r\n                fail(\"Non-zero return code({1}) when ..., <2 more arguments>))\r\nNon-zero return code(256) when executing 'patch -p1 -d C:/tools/msys64/tmp/_bazel_john/mzdlugz6/external/protobuf -i C:/users/john/downloads/tensorflow-master/third_party/protobuf/add_noinlines.patch':\r\nStdout:\r\nStderr: java.io.IOException: CreateProcess(): The system cannot find the file specified.\r\n and referenced by '//tensorflow/loader:test850.dll'.\r\n\r\nprotobuf is installed:\r\n\r\n$ protoc --version\r\nlibprotoc 3.2.0\r\n\r\n$ bazel version\r\nBuild label: 0.4.5\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Mar 16 13:02:06 2017 (1489669326)\r\nBuild timestamp: 1489669326\r\nBuild timestamp as int: 1489669326\r\n\r\nI have build bazel with chocolatery as described on the bazel site.\r\n\r\nTried solution from issue #5029 (although the error is different). If i run the exact same script with bazel on a mac everything is fine and the build completes (no this is unfortunate not a solution). So i know the problem is with windows bazel and maybe the paths? \r\n\r\nAlso tried to compile with CMAKE however with no succes \r\n\r\n\r\n\r\n", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case.   We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!\r\n\r\n(In general, please fill out ALL the information asked for in the new issue template)", "Sorry my bad: \r\n\r\n------------------------\r\n\r\n### System information\r\nOS: windows 10 64 bit\r\nTensorflow installed with for windows python 3 (without GPU) from binary (pip3 install....)\r\nTensorflow version: \r\n>>> import tensorflow as tf; print(tf.__version__)\r\n1.0.1\r\n\r\n$ protoc --version\r\nlibprotoc 3.2.0\r\n\r\n$ bazel version\r\nBuild label: 0.4.5\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Mar 16 13:02:06 2017 (1489669326)\r\nBuild timestamp: 1489669326\r\nBuild timestamp as int: 1489669326\r\n\r\n### Describe the problem\r\nWhile building a script with tensorflow as dependancy, bazel fails on protobuf when bazel is analyzing. \r\n\r\nLast INFO before i get for the error: \r\nINFO: Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 40,960 bytes\r\n \r\nThe problem occurs when i try to build a simple script with tensorflow included, or when i just try to build anything with tensorflow as dependancy. \r\n\r\nProblem also only occurs when including deps[//tensorflow/core:tensorflow]. Problem does not occur when building the same files on a mac (also with tensorflow v1.0.1 and bazel 0.4.5).\r\n\r\n### Source code / logs\r\nusing msys2 to run bazel: \r\n\r\n$ bazel build --cpu=x64_windows_msvc :test.dll\r\nWARNING: C:/tools/msys64/tmp/_bazel_john/hawbuxgu/external/bazel_tools/tools/cpp/cc_configure.bzl:57:3:\r\nAuto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest Visual C++ installed.\r\n.\r\nWARNING: C:/tools/msys64/tmp/_bazel_john/hawbuxgu/external/bazel_tools/tools/cpp/cc_configure.bzl:57:3:\r\nAuto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variables,eg. VS140COMNTOOLS\r\n.\r\nWARNING: C:/tools/msys64/tmp/_bazel_john/hawbuxgu/external/bazel_tools/tools/cpp/cc_configure.bzl:57:3:\r\nAuto-Configuration Warning: Visual C++ build tools found at C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\\r\n.\r\nERROR: C:/_shared/_tensorflow_build_test/tf/tensorflow/tensorflow/_test_tensorflow_files/BUILD:1:1: error loading package 'tensorflow/core': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf//': Traceback (most recent call last):\r\n        File \"C:/_shared/_tensorflow_build_test/tf/tensorflow/tensorflow/workspace.bzl\", line 98\r\n                _apply_patch(repo_ctx, repo_ctx.attr.patch_file)\r\n        File \"C:/_shared/_tensorflow_build_test/tf/tensorflow/tensorflow/workspace.bzl\", line 87, in _apply_patch\r\n                _execute_and_check_ret_code(repo_ctx, [\"patch\", \"-p1\", \"-d\", r...), <2 more arguments>])\r\n        File \"C:/_shared/_tensorflow_build_test/tf/tensorflow/tensorflow/workspace.bzl\", line 79, in _execute_and_check_ret_code\r\n                fail(\"Non-zero return code({1}) when ..., <2 more arguments>))\r\nNon-zero return code(256) when executing 'patch -p1 -d C:/tools/msys64/tmp/_bazel_john/hawbuxgu/external/protobuf -i C:/_shared/_tensorflow_build_test/tf/tensorflow/third_party/protobuf/add_noinlines.patch':\r\nStdout:\r\nStderr: java.io.IOException: CreateProcess(): The system cannot find the file specified.\r\n and referenced by '//tensorflow/_test_tensorflow_files:test.dll'.\r\nERROR: Analysis of target '//tensorflow/_test_tensorflow_files:test.dll' failed; build aborted.\r\nINFO: Elapsed time: 12.000s\r\n ", "Ah, I see, the `patch` command is not available and needs to be installed.\r\nSee https://github.com/tensorflow/tensorflow/issues/8919#issue-218857845 for inspiration and let us know if that works.\r\n\r\nThanks.\r\n", "That seemed to be te problem indeed. Thank you very much!", "I have nearly the same problem here. (i think):\r\n\r\nSystem information\r\nOS: windows 7 64 bit\r\nTensorflow installed with for windows python 3 (with GPU) from binary (pip3 install....)\r\n\r\nTensorflow version:\r\n\r\nimport tensorflow as tf; tf.__version__\r\n'1.0.1'\r\n\r\nI do not have any protoc ...\r\n\r\n$ bazel version\r\nWARNING: ignoring http_proxy in environment.\r\nBuild label: 0.5.2- (@non-git)\r\nBuild target: bazel-out/msys_x64-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Jul 6 10:39:46 2017 (1499337586)\r\nBuild timestamp: 1499337586\r\nBuild timestamp as int: 1499337586\r\n\r\n\r\n\r\nDescribe the problem\r\n\r\nThe problem occurs when i try to build a simple script with tensorflow included https://www.tensorflow.org/api_guides/cc/guide\r\n\r\nin order to understand the problem better i changed names\r\n\r\nSource code / logs\r\nthe directory .../Sourcen/external/tensorflow-master/tensorflow/cc/example1/ contains 3 files\r\nWORKSPACE\r\nBUILD\r\nexample-code.cc\r\n\r\n..... WORKSPACE \r\n........ empty\r\n\r\n\r\n...... BUILD\r\ncc_binary(\r\n    name = \"example-out\",\r\n    srcs = [\"example-code.cc\"],\r\n    deps = [\r\n        \"//tensorflow/cc:cc_ops\",\r\n        \"//tensorflow/cc:client_session\",\r\n        \"//tensorflow/core:tensorflow\",\r\n    ],\r\n)\r\n...................................\r\n\r\n\r\n............ example-code.cc\r\n// tensorflow/cc/example/example.cc\r\n#include \"tensorflow/cc/client/client_session.h\"\r\n#include \"tensorflow/cc/ops/standard_ops.h\"\r\n#include \"tensorflow/core/framework/tensor.h\"\r\n\r\nint main() {\r\n  using namespace tensorflow;\r\n  using namespace tensorflow::ops;\r\n  Scope root = Scope::NewRootScope();\r\n  // Matrix A = [3 2; -1 0]\r\n  auto A = Const(root, { {3.f, 2.f}, {-1.f, 0.f}});\r\n  // Vector b = [3 5]\r\n  auto b = Const(root, { {3.f, 5.f}});\r\n  // v = Ab^T\r\n  auto v = MatMul(root.WithOpName(\"v\"), A, b, MatMul::TransposeB(true));\r\n  std::vector<Tensor> outputs;\r\n  ClientSession session(root);\r\n  // Run and fetch v\r\n  TF_CHECK_OK(session.Run({v}, &outputs));\r\n  // Expect outputs[0] == [19; -3]\r\n  LOG(INFO) << outputs[0].matrix<float>();\r\n  return 0;\r\n}\r\n..............\r\n\r\n\r\nusing msys2 to run bazel in .../Sourcen/external/tensorflow-master/\r\n\r\n\r\n$ bazel run -c opt //tensorflow/cc/example1:example-out\r\nWARNING: ignoring http_proxy in environment.\r\nWARNING: C:/tools/msys64/tmp/_bazel_xxxxxx/6esgz-yl/external/bazel_tools/tools/cpp/cc_configure.bzl:67:3:\r\nAuto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest Visual C++ installed.\r\n.\r\nWARNING: C:/tools/msys64/tmp/_bazel_xxxxxxx/6esgz-yl/external/bazel_tools/tools/cpp/cc_configure.bzl:67:3:\r\nAuto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variables,eg. VS140COMNTOOLS\r\n.\r\nWARNING: C:/tools/msys64/tmp/_bazel_xxxxxxxx/6esgz-yl/external/bazel_tools/tools/cpp/cc_configure.bzl:67:3:\r\nAuto-Configuration Warning: Visual C++ build tools found at C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\\r\n.\r\nERROR: xxxxxx...../tensorflow-master/tensorflow/cc/example1/BUILD:1:1: error loading package 'tensorflow/core': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf//': Traceback (most recent call last):\r\n        File \"xxxxxxxx.../tensorflow-master/tensorflow/workspace.bzl\", line 116\r\n                _apply_patch(repo_ctx, repo_ctx.attr.patch_file)\r\n        File \"xxxxx..../tensorflow-master/tensorflow/workspace.bzl\", line 107, in _apply_patch\r\n                _execute_and_check_ret_code(repo_ctx, cmd)\r\n        File \"xxxxx..../tensorflow-master/tensorflow/workspace.bzl\", line 91, in _execute_and_check_ret_code\r\n                fail(\"Non-zero return code({1}) when ..., <2 more arguments>))\r\nNon-zero return code(1) when executing 'c:/tools/msys64/usr/bin/bash.exe -c patch -p1 -d C:/tools/msys64/tmp/_bazel_xxxxx/6esgz-yl/external/protobuf -i xxxxx..../tensorflow-master/third_party/protobuf/add_noinlines.patch':\r\nStdout: can't find file to patch at input line 4\r\nPerhaps you used the wrong -p or --strip option?\r\nThe text leading up to this was:\r\n........................................\r\n|diff -u -r a/src/google/protobuf/compiler/cpp/cpp_file.cc b/src/google/protobuf/compiler/cpp/cpp_file.cc\r\n|--- a/src/google/protobuf/compiler/cpp/cpp_file.cc     2017-02-10 23:55:34.000000000 +0100\r\n|+++ b/src/google/protobuf/compiler/cpp/cpp_file.cc     2017-03-21 13:41:46.931979154 +0100\r\n..................................\r\nFile to patch:\r\nSkip this patch? [y]\r\nSkipping patch.\r\n3 out of 3 hunks ignored\r\n\r\nStderr:  and referenced by '//tensorflow/cc/example1:example-out'.\r\nERROR: Analysis of target '//tensorflow/cc/example1:example-out' failed; build aborted.\r\nINFO: Elapsed time: 9,402s\r\nERROR: Build failed. Not running target.\r\n\r\n"]}, {"number": 9221, "title": "In non-devel Docker images, positively confirm --allow-root (#9021)", "body": "to avoid an error that started to happen recently:\r\n[C 10:09:34.858 NotebookApp] Running as root is not recommended. Use\r\n--allow-root to bypass.\r\n\r\nE.g., see\r\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-docker-cpu/TF_DOCKER_BUILD_IS_DEVEL=NO,TF_DOCKER_BUILD_PYTHON_VERSION=PYTHON2,label=gcs-access/361/console", "comments": []}, {"number": 9220, "title": "How to recompile eigen3 library", "body": "I changed some code in the //tensorflow/third_party/eigen3, and recompile the C++ sample (label_image). I looked at the code of the `BUILD` file in label_image and it depends on the //tensorflow/core, and I think the latter depends on the eigen3. However when I run the recompiled label_image, the code I've changed in eigen3 doesn't work. I am not very familiar with bazel so I hope someone can help me on this. How can I recompile the eigen3 to be reflected in the example?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9219, "title": "R0.11", "body": "need to build tensorflow model", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "This looks like a mistake to me."]}, {"number": 9218, "title": "Docker build: fix bazel installer and LICENSE download issue", "body": "", "comments": []}, {"number": 9217, "title": "Docker build: fix bazel installer and LICENSE download issue", "body": "", "comments": ["Thanks, @drpngx. The Mac test failure is a known, unrelated flake."]}, {"number": 9216, "title": "AssertionError: Items are not equal: ACTUAL: 2147483647 DESIRED: -2147483648", "body": "\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  \r\nUbuntu 16.04 (ppc64le)\r\n- **TensorFlow installed from (source or binary)**: \r\n Installed from source (v1.0.1)\r\n- **TensorFlow version (use command below)**:    \r\n('v1.0.1-0-ge895d5c-dirty', '1.0.1')\r\n- **Bazel version (if compiling from source)**: \r\nbazel release 0.4.4-2017-04-10 (@80a07b5)\r\n- **CUDA/cuDNN version**: \r\nIn disable mode\r\n- **Exact command to reproduce**:\r\nbazel test //tensorflow/python/kernel_tests:cast_op_test\r\n\r\n\r\n\r\n### Describe the problem\r\nBuilt TF successfully , however I am getting `Items are not equal` error while running the `cast_op_test` \r\n\r\nTo cross verify the test results , I ran this test on X86 vm and that passed successfully. This test is failing only on ppc64le platform . Here I would like to know your suggestions and comments. \r\n\r\n### Source code / logs\r\n````\r\n$ bazel test //tensorflow/python/kernel_tests:cast_op_test\r\n\r\n\r\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\r\n-----------------------------------------------------------------------------\r\nI tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 16 visible devices\r\nI tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:\r\nI tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>\r\n/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/cast_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/cast_op_test.py:62: ComplexWarning: Casting complex values to real discards the imaginary part\r\n  np_ans = x.astype(dtype)\r\n....F.W tensorflow/core/framework/op_kernel.cc:983] Unimplemented: Cast int64 to string is not supported\r\nE tensorflow/core/common_runtime/executor.cc:594] Executor failed to create kernel. Unimplemented: Cast int64 to string is not supported\r\n         [[Node: Cast = Cast[DstT=DT_STRING, SrcT=DT_INT64, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Cast/x)]]\r\n........\r\n======================================================================\r\nFAIL: testInfNan (__main__.CastOpTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/cast_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/cast_op_test.py\", line 150, in testInfNan\r\n    self._compare(np.inf, np.int32, i4.min, False)\r\n  File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/cast_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/cast_op_test.py\", line 124, in _compare\r\n    x, dst_dtype, use_gpu=use_gpu), dst_dtype(expected))\r\n  File \"/usr/lib64/python2.7/site-packages/numpy/testing/utils.py\", line 425, in assert_equal\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nItems are not equal:\r\n ACTUAL: 2147483647\r\n DESIRED: -2147483648\r\n\r\n----------------------------------------------------------------------\r\nRan 14 tests in 2.485s\r\n\r\nFAILED (failures=1)\r\n`\r\n\r\n```", "comments": ["Thanks for pointing this out.\r\n\r\nSupport for big-endian architecture was added by @namrata-ibm in commit 1e44b15ff\r\nSupport for big-endian architectures in TensorFlow is community driven and we don't test that as part of every release at this time.\r\n\r\n@namrata-ibm : Could you take a look? Seems like this is the line that is failing:\r\nhttps://github.com/tensorflow/tensorflow/blob/87cdfafd44ff5e332fd820608783432fea83a4c9/tensorflow/python/kernel_tests/cast_op_test.py#L147\r\n\r\n", "Oh, I also notice that you mentioned you're running on `ppc64le`.\r\nIsn't that little endian? If so, could you share the output of the following python script:\r\n\r\n```\r\nimport sys\r\nprint sys.byteorder\r\n```\r\n\r\nBecause the test failure suggests that `sys.byteorder` is `big` even though your architecture is little-endian?", "The failure seems to be at line number 150 from above log:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/cast_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/cast_op_test.py\", line 150, in testInfNan\r\n\r\n```\r\n\r\nin function:\r\n\r\n`\r\n    self._compare(np.inf, np.int32, i4.min, False)\r\n`\r\n\r\n@asimshankar ,\r\n\r\nppc64le being little endian, doesn't enter the condition for `big endian` and follows the original code just like x86.\r\n\r\nI can also see error : \r\n` E tensorflow/core/common_runtime/executor.cc:594] Executor failed to create kernel.  Unimplemented: Cast int64 to string is not supported` in above log.\r\nThis would need some debugging on ppc64le.\r\n\r\n\r\n", "Oh, apologies, dunno why I misread the line.\r\n\r\n@sandipmgiri : Does seem that the cast kernel (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cast_op.cc#L116) isn't quite doing what we expected on ppc64le.\r\n\r\nAre you able to trace that? Contributions to fix it will be welcome.", "Actual failure at line number 150 , in function `self._compare(np.inf, np.int32, i4.min, False)`\r\n\r\nFor little-endian, I tried changing the value in above function :\r\nNow this test is passing successfully on both the platform (x86 and ppc64le) after replacing the `np.inf` with           \r\n  `-np.inf`. Getting same result as expected  i.e. `-2147483648`\r\n\r\n@asimshankar , Is there need to take care of ?\r\n", "I've also run into this issue on a `ppc64le` system.\r\n\r\nCasting a non-finite float point values (inf, -inf, nan) to an integer is an undefined behavior in C (see section 6.3.1.4 of the C99 standards).  These tests seems to be expecting a specific result from this undefined behavior which is not portable.  The expected result, the largest negative number supported by integer type being cast to, happens to be the integer indefinite value which the x87 FPU gives for floating-point invalid-operations.\r\n\r\nI'm not certain how these tests can be made portable given that the behavior being exercised is  architecture specific.", "Closing as duplicate of https://github.com/tensorflow/tensorflow/issues/9360, see discussion in https://github.com/tensorflow/tensorflow/issues/9360.", "This issue resolved in PR #10522 (merged commit d0d2308)"]}, {"number": 9215, "title": "encoder_inputs and decoder_inputs of a sequence to sequence model.", "body": "Hello,\r\nI've read the tutorial about seq2seq on the website and still can't figure out how my inputs have to be;\r\nhere's my problem:\r\neach of my inputs is a matrix with fixed number of columns but a variable number of rows( each matrix has its number of rows). And the same thing for my outputs.\r\nhow to build the encoder and decoder inputs?\r\nThanks in advance.\r\n", "comments": ["This might be a better question for [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow). Please see our issue tracker [policy](https://github.com/tensorflow/tensorflow/issues/new)."]}]