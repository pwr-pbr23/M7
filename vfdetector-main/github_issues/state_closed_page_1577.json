[{"number": 5604, "title": "Fix: Change the lock file location for GPU tests.", "body": "/var/lock does not exist on mac, so execution fails on mac.", "comments": ["Looks like macos does not have flock at all. \nSo I will need to skip this completely on macosx\n"]}, {"number": 5603, "title": "Branch 139105151", "body": "Internal push.", "comments": ["Windows cmake failure is a known issue and we are working on it at the moment.\nThis PR looks good, and can be merged.\n", "@tensorflow-jenkins test this please\n"]}, {"number": 5602, "title": "Branch 139090881", "body": "Push to public.", "comments": ["@rmlarsen, thanks for your PR! By analyzing the history of the files in this pull request, we identified @caisq, @tensorflower-gardener and @keveman to be potential reviewers.\n", "Oops, forgot to sync before pushing.\n"]}, {"number": 5601, "title": "wide_n_deep_tutorial.py fails with model_type=deep: Shapes (32561, 1) and (32561,) are incompatible", "body": "The [wide_n_deep tutorial](https://www.tensorflow.org/versions/r0.11/tutorials/wide_and_deep/index.html) fails when run with --model_type=deep\r\nIt immediately fails with the error:\r\nShapes (32561, 1) and (32561,) are incompatible\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nThis may be due to #4715 \r\nhttps://github.com/tensorflow/tensorflow/issues/3776 discusses the same flag, but appears to be an unrelated problem\r\n\r\n### Environment info\r\nOperating System: macOS Sierra 10.12.1\r\n\r\nInstalled version of CUDA and cuDNN: none\r\n\r\n1. A link to the pip package you installed:\r\nhttps://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.11.0-py2-none-any.whl\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n0.11.0\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n1. Download the tutorial code [wide_n_deep_tutorial.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/wide_n_deep_tutorial.py)\r\n\r\n2: Run with --model_type=deep\r\npython wide_n_deep_tutorial.py --model_type=deep\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nThe example runs with --model_type=wide or --model_type=wide_n_deep. Just the deep model fails.\r\n\r\n### Logs or other output that would be helpful\r\n[out.txt](https://github.com/tensorflow/tensorflow/files/590128/out.txt)\r\n\r\nThe log shows a lot of warnings about API changes, suggesting the tutorial code should be updated. E.g.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.", "comments": ["@ispirmustafa Can you take a look?\n", "Same issue as #4715 \nFixed in master branch but not in 0.11.0\n"]}, {"number": 5600, "title": "Problems Installing on Windows 10", "body": "I am having trouble building tensorflow on Windows. Here is a detailed list of all the steps I took to get it to build. \r\n\r\nAny help is much appreciated. \r\n\r\nStep-by-step Windows build\r\n \r\n1. Install the pre-requisites detailed above, and set up your environment.\r\no   CMake version 3.1 or later, \r\n\uf0a7  When installing  cmake-3.7.0-rc2-win64-x64.msi , I added CMake to the System Path for all users, in file location C:\\Program Files\\CMake\\\r\no   Git\r\n\uf0a7  I installed Git-2.10.1-64-bit.exe, selected the option Use Git from the Windows Command Prompt, Checkout Windows-style commit Unix-style line endings, Use Windows default console window\r\n\u2022         Enable both file system caching and git credential Manager\r\no   SWIG\r\n\uf0a7  I downloaded swigwin-3.0.10 from here http://www.swig.org/download.html , unzipped the folder and placed the contents in \u201cC:\\tools\u201d\r\n2. Run vcvarsall.bat\r\n       \r\n\u2022         The instructions say to run this command\u2026\r\no   D:\\temp> \"C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\bin\\amd64\\vcvarsall.bat\"\r\n \r\n\u2022         However, my \"vcvarsall.bat\" file was actually in a different location -> C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\vcvarsall.bat\r\no   So I ran this command...\r\no   D:\\temp> \"C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\vcvarsall.bat\"\r\n \r\n3. Clone TensorFlow\r\n \r\nD:\\temp> git clone https://github.com/tensorflow/tensorflow.git\r\nD:\\temp> cd tensorflow\\tensorflow\\contrib\\cmake\r\nD:\\temp\\tensorflow\\tensorflow\\contrib\\cmake> mkdir build\r\nD:\\temp\\tensorflow\\tensorflow\\contrib\\cmake> cd build\r\nD:\\temp\\tensorflow\\tensorflow\\contrib\\cmake\\build>\r\n \r\nI ran these commands and everything went smoothly.\r\n \r\n \r\n4. Invoke CMake to create Visual Studio solution and project files\r\n \r\nD:\\...\\build> cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release ^\r\nMore? -DSWIG_EXECUTABLE=C:/tools/swigwin-3.0.10/swig.exe ^\r\nMore? -DPYTHON_EXECUTABLE=C:/Users/%USERNAME%/AppData/Local/Continuum/Anaconda3/python.exe ^\r\nMore? -DPYTHON_LIBRARIES=C:/Users/%USERNAME%/AppData/Local/Continuum/Anaconda3/libs/python35.lib\r\n \r\nThe problem is my python.exe and python35.lib are in totally different file locations.\r\n \r\nC:\\Windows\\System32\\tensorflow\\tensorflow\\contrib\\cmake\\build> cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release ^\r\nMore? -DSWIG_EXECUTABLE=C:/tools/swigwin-3.0.10/swig.exe ^\r\nMore? -DPYTHON_EXECUTABLE=C:/Program Files/Anaconda3/python.exe ^\r\nMore? -DPYTHON_LIBRARIES= C:/Program Files/Anaconda3/libs/python35.lib\r\n \r\n \r\n \r\nHere is the error code\u2026\r\n \r\n-- The C compiler identification is unknown\r\n-- The CXX compiler identification is unknown\r\nCMake Error at CMakeLists.txt:5 (project):\r\n  No CMAKE_C_COMPILER could be found.\r\n \r\n \r\n \r\nCMake Error at CMakeLists.txt:5 (project):\r\n  No CMAKE_CXX_COMPILER could be found.\r\n \r\n \r\n \r\n-- Configuring incomplete, errors occurred!\r\nSee also \"C:/Windows/System32/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/CMakeOutput.log\".\r\nSee also \"C:/Windows/System32/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/CMakeError.log\".\r\n \r\nAfter adding \u201cget.exe\u201d to my path variable and running this command\r\n \r\nC:\\Windows\\System32\\tensorflow\\tensorflow\\contrib\\cmake\\build>cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release ^\r\nMore? -DSWIG_EXECUTABLE=C:/tools/swigwin-3.0.10/swig.exe ^\r\nMore? -DPYTHON_EXECUTABLE=C:/Program Files/Anaconda3/python.exe ^\r\nMore? -DPYTHON_LIBRARIES= C:/Program Files/Anaconda3/libs/python35.lib\r\n \r\n \r\nI get this error\u2026\r\n \r\nCMake Error: The source directory \"C:/Windows/System32/tensorflow/tensorflow/contrib/cmake/build/Files/Anaconda3/libs/python35.lib\" does not exist.\r\nSpecify --help for usage, or press the help button on the CMake GUI.\r\n \r\nC:\\Windows\\System32\\tensorflow\\tensorflow\\contrib\\cmake\\build>\r\n\r\n\r\n", "comments": ["Judging by the error message, it looks like the space in `C:/Program Files/Anaconda3/libs/python35.lib` is causing the problem, because it's looking for the Python library in `C:/Windows/System32/tensorflow/tensorflow/contrib/cmake/build/Files/Anaconda3/libs/python35.lib`.\n\n(As an aside, I wouldn't generally recommend building software under `C:\\Windows`, but that's up to you....)\n\nIt might be sufficient to put quotes around the definitions of `PYTHON_EXECUTABLE` and `PYTHON_LIBRARIES`. Can you try that and let us know how you get on?\n", "Thanks for the tip! I moved my project to C:\\Users\\miles\\Documents\\TensorflowWork and tried to rebuild tensorflow. \n\nC:\\Users\\miles\\Documents\\TensorflowWork\\tensorflow\\tensorflow\\contrib\\cmake\\build>cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release ^\nMore? -DSWIG_EXECUTABLE=C:/tools/swigwin-3.0.10/swig.exe ^\nMore? -DPYTHON_EXECUTABLE=C:/Users/%USERNAME%/AppData/Local/Continuum/Anaconda3/python.exe ^\nMore? -DPYTHON_LIBRARIES=C:/Users/%USERNAME%/AppData/Local/Continuum/Anaconda3/libs/python35.lib\n-- Building for: Visual Studio 14 2015\n-- The C compiler identification is MSVC 19.0.24213.1\n-- The CXX compiler identification is MSVC 19.0.24213.1\n-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe\n-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe\n-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Looking for pthread.h\n-- Looking for pthread.h - not found\n-- Found Threads: TRUE\n-- Found PythonInterp: C:/Users/miles/AppData/Local/Continuum/Anaconda3/python.exe (found version \"1.4\")\nCMake Error at tf_python.cmake:25 (message):\n  Cannot get Python include directory.  Is distutils installed?\nCall Stack (most recent call first):\n  CMakeLists.txt:210 (include)\n\n-- Configuring incomplete, errors occurred!\nSee also \"C:/Users/miles/Documents/TensorflowWork/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/CMakeOutput.log\".\nSee also \"C:/Users/miles/Documents/TensorflowWork/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/CMakeError.log\".\n\nThe build seems to have failed trying to open a filed called 'pthread.h'. Here is the CMakeError.log...\n\nDetermining if the include file pthread.h exists failed with the following output:\nChange Dir: C:/Users/miles/Documents/TensorflowWork/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/CMakeTmp\n\nRun Build Command:\"C:/Program Files (x86)/MSBuild/14.0/bin/MSBuild.exe\" \"cmTC_7438f.vcxproj\" \"/p:Configuration=Debug\" \"/p:VisualStudioVersion=14.0\"\nMicrosoft (R) Build Engine version 14.0.25420.1\n\nCopyright (C) Microsoft Corporation. All rights reserved.\n\nBuild started 11/16/2016 2:12:11 PM.\n\nProject \"C:\\Users\\miles\\Documents\\TensorflowWork\\tensorflow\\tensorflow\\contrib\\cmake\\build\\CMakeFiles\\CMakeTmp\\cmTC_7438f.vcxproj\" on node 1 (default targets).\n\nPrepareForBuild:\n\n  Creating directory \"cmTC_7438f.dir\\Debug\\\".\n\n  Creating directory \"C:\\Users\\miles\\Documents\\TensorflowWork\\tensorflow\\tensorflow\\contrib\\cmake\\build\\CMakeFiles\\CMakeTmp\\Debug\\\".\n\n  Creating directory \"cmTC_7438f.dir\\Debug\\cmTC_7438f.tlog\\\".\n\nInitializeBuildStatus:\n\n  Creating \"cmTC_7438f.dir\\Debug\\cmTC_7438f.tlog\\unsuccessfulbuild\" because \"AlwaysCreate\" was specified.\n\nClCompile:\n\n  C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\bin\\x86_amd64\\CL.exe /c /Zi /W3 /WX- /Od /Ob0 /D WIN32 /D _WINDOWS /D _DEBUG /D \"CMAKE_INTDIR=\\\"Debug\\\"\" /D _MBCS /Gm- /RTC1 /MDd /GS /fp:precise /Zc:wchar_t /Zc:forScope /Zc:inline /Fo\"cmTC_7438f.dir\\Debug\\\" /Fd\"cmTC_7438f.dir\\Debug\\vc140.pdb\" /Gd /TC /errorReport:queue C:\\Users\\miles\\Documents\\TensorflowWork\\tensorflow\\tensorflow\\contrib\\cmake\\build\\CMakeFiles\\CMakeTmp\\CheckIncludeFile.c\n\n  Microsoft (R) C/C++ Optimizing Compiler Version 19.00.24213.1 for x64\n\n  Copyright (C) Microsoft Corporation.  All rights reserved.\n\n  cl /c /Zi /W3 /WX- /Od /Ob0 /D WIN32 /D _WINDOWS /D _DEBUG /D \"CMAKE_INTDIR=\\\"Debug\\\"\" /D _MBCS /Gm- /RTC1 /MDd /GS /fp:precise /Zc:wchar_t /Zc:forScope /Zc:inline /Fo\"cmTC_7438f.dir\\Debug\\\" /Fd\"cmTC_7438f.dir\\Debug\\vc140.pdb\" /Gd /TC /errorReport:queue C:\\Users\\miles\\Documents\\TensorflowWork\\tensorflow\\tensorflow\\contrib\\cmake\\build\\CMakeFiles\\CMakeTmp\\CheckIncludeFile.c\n\n  CheckIncludeFile.c\n\nC:\\Users\\miles\\Documents\\TensorflowWork\\tensorflow\\tensorflow\\contrib\\cmake\\build\\CMakeFiles\\CMakeTmp\\CheckIncludeFile.c(1): fatal error C1083: Cannot open include file: 'pthread.h': No such file or directory [C:\\Users\\miles\\Documents\\TensorflowWork\\tensorflow\\tensorflow\\contrib\\cmake\\build\\CMakeFiles\\CMakeTmp\\cmTC_7438f.vcxproj]\n\nDone Building Project \"C:\\Users\\miles\\Documents\\TensorflowWork\\tensorflow\\tensorflow\\contrib\\cmake\\build\\CMakeFiles\\CMakeTmp\\cmTC_7438f.vcxproj\" (default targets) -- FAILED.\n\nBuild FAILED.\n\n\"C:\\Users\\miles\\Documents\\TensorflowWork\\tensorflow\\tensorflow\\contrib\\cmake\\build\\CMakeFiles\\CMakeTmp\\cmTC_7438f.vcxproj\" (default target) (1) ->\n\n(ClCompile target) -> \n\n  C:\\Users\\miles\\Documents\\TensorflowWork\\tensorflow\\tensorflow\\contrib\\cmake\\build\\CMakeFiles\\CMakeTmp\\CheckIncludeFile.c(1): fatal error C1083: Cannot open include file: 'pthread.h': No such file or directory [C:\\Users\\miles\\Documents\\TensorflowWork\\tensorflow\\tensorflow\\contrib\\cmake\\build\\CMakeFiles\\CMakeTmp\\cmTC_7438f.vcxproj]\n\n```\n0 Warning(s)\n\n1 Error(s)\n```\n\nTime Elapsed 00:00:00.48\n", "I think that failure is a red herring... we don't depend on the availability of pthreads to build on Windows, but the current CMake rules will check for it (and that check will fail without breaking the rest of the build).\n\nInstead I think this is the main error:\n\n```\nCMake Error at tf_python.cmake:25 (message):\nCannot get Python include directory. Is distutils installed?\nCall Stack (most recent call first):\nCMakeLists.txt:210 (include)\n```\n\nCan you ensure that `distutils` is installed in the Python environment you're using?\n", "distutils might also be a red herring, I've built this on windows 10 and got exactly the same errors (although I did download pthreads.h...).\n\nSet additional paths for the include files when building\n-DPYTHON_INCLUDE_DIR=<PATH_TO_INCLUDE_IN_ANACONDA>\n\nIf I remember correctly, you may have to add more that just this (numpy as well I think,  -DNUMPY_INCLUDE_DIR), I can't remember exactly what I ended up with.\n", "Yes, you can avoid the requirement on installing distutils if you specify `-DPYTHON_INCLUDE_DIR` (we [use distutils](https://github.com/tensorflow/tensorflow/blob/99284f4c9eff4bf387dea926c11e06bc513606e7/tensorflow/contrib/cmake/tf_python.cmake#L21) to discover that directory automatically, but won't use it if you specify a dir explicitly).\n", "Automatically closing due to lack of recent activity. When further information is posted we will reopen this. Thanks.", "You should change the installation location of Anaconda3, because the command line of the space will bring a lot of problems, More? -DPYTHON_EXECUTABLE = C: / Program Files / Anaconda3 / python.exe ^\r\nMore? -DPYTHON_LIBRARIES = C: / Program Files / Anaconda3 / libs / python35.lib There are spaces in both commands that may not be recognized, resulting in an error", "What about putting double quotes (\") around the paths? For example:\r\n-DPYTHON_EXECUTABLE = \"C: / Program Files / Anaconda3 / python.exe\" ^", "@mrry  distutils is now obsolete and not in the Anaconda channels.\r\n\r\n```\r\nc:\\ProgramData\\Anaconda3>conda install distutils\r\nFetching package metadata ...........\r\nPackageNotFoundError: Package missing in current win-64 channels:\r\n  - distutils\r\n```"]}, {"number": 5599, "title": "Error in Recurrent Neural Networks tutorial", "body": "I was suprised to see that the two implementations of `char-rnn` in tensorflow I found on the web fall in the same Python trap:\r\n - https://github.com/carpedm20/lstm-char-cnn-tensorflow/blob/master/models/LSTMTDNN.py#L151\r\n - https://github.com/sherjilozair/char-rnn-tensorflow/blob/master/model.py#L25\r\n\r\nThey use `MultiRNNCell` by copying the pointer to a unique cell instead of creating individual cell instances.\r\n\r\nThis error can be illustrated with the following example:\r\n\r\n```python\r\nIn [1]: a = [[]]*10\r\n\r\nIn [2]: a[0].append(3)\r\n\r\nIn [3]: a\r\nOut[3]: [[3], [3], [3], [3], [3], [3], [3], [3], [3], [3]]\r\n\r\nIn [4]: a = [[] for i in range(10)]\r\n\r\nIn [5]: a[0].append(3)\r\n\r\nIn [6]: a\r\nOut[6]: [[3], [], [], [], [], [], [], [], [], []]\r\n```\r\n\r\nIn the first case, we create 10 pointers to the same empty list, which is rarely what we want, while in the second case, we create 10 individual empty lists.\r\n\r\nIf two independent persons are doing the same mistake, it might be that the documentation is wrong.\r\n\r\nIn https://www.tensorflow.org/versions/r0.11/tutorials/recurrent/index.html it is written:\r\n```\r\nTo give the model more expressive power, we can add multiple layers of LSTMs to process the data. The output of the first layer will become the input of the second and so on.\r\n```\r\n\r\nHowever, the code is stacking pointers to the same LSTM cell. It is creating \"shared weights\" across those cells.\r\n\r\nYou can notice that if you go back to the original Lua code cited in the example:\r\nhttps://github.com/wojzaremba/lstm/blob/master/main.lua#L136 and https://github.com/wojzaremba/lstm/blob/master/base.lua#L33 , it talks about `cloning` and `avoiding pointers`.\r\n\r\nMore interestingly, in the README of https://github.com/wojzaremba/lstm , it speaks of achieving `115 perplexity for a small model in 1h`, which is not what the current code does (I obtained `116.983`), but is attained with the proposed fix (I got `115.063`), see https://gist.github.com/kevin-keraudren/f9607e281d9d75fee4000101f7e22a70#file-ptb_word_lm-txt .", "comments": ["@kevin-keraudren, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ebrevdo, @benoitsteiner and @vrv to be potential reviewers.\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "Can one of the admins verify this patch?\n", "I signed a CLA for this email address too.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Everything is OK since `RNNCell` doesn't have inside it any data. All variables in the `MultiRNNCell` created in [different variable scopes](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L804).\n", "Thanks a lot @Jihadik for the pointer to the variable scopes, I had a look at the saved tensors in the checkpoint with `tensorflow/python/tools/inspect_checkpoint.py` and I can see what you say.\nClosing the issue.\n", "I notice that a very similar code change to @kevin-keraudren's suggestion was actually later committed by @nealwu in https://github.com/tensorflow/models/pull/934\r\n\r\nBut the documentation change to force new instance creation in MultiRNNCell in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/tutorials/recurrent.md hasn't been made. Should it be?", "Yes, documentation contributions welcome.\n\nOn Apr 12, 2017 1:50 AM, \"Richard Davies\" <notifications@github.com> wrote:\n\n> I notice that a very similar code change to @kevin-keraudren\n> <https://github.com/kevin-keraudren>'s suggestion was actually later\n> committed by @nealwu <https://github.com/nealwu> in tensorflow/models#934\n> <https://github.com/tensorflow/models/pull/934>\n>\n> But the documentation change to force new instance creation in\n> MultiRNNCell in https://github.com/tensorflow/tensorflow/blob/master/\n> tensorflow/docs_src/tutorials/recurrent.md hasn't been made. Should it be?\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/5599#issuecomment-293514292>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim-VBIA0Mvgd9vO-3IrE89kgPaywcks5rvJBPgaJpZM4Kxfkq>\n> .\n>\n", "Thanks for bringing this to our attention @richardjdavies! I've updated the documentation. You should see it within a day or two."]}, {"number": 5598, "title": "tf.contrib.learn estimators that use feature columns via the input_fun not compatible with x inputs when predicting and not exportable to serving", "body": "I have run into into performance and portability issues with tf.conrtib.learn estimators that use high level feature columns and have done extensive code traversal to figure out a possible solution to make predictions faster.\r\n\r\nAt the core of the problem is that high level feature column tensors created in the input_fn are transformed and added as input tensors to the actual neural network and/or logistic model. After that the hidden layers and logit (output) layer is added to the graph and the logits for the linear model (if used) are also created. This happens every time you run predict on the estimator, at the cost an overhead of 3-7 seconds.\r\n\r\nOne solution I tried is based on posts that I read about the x parameter of the predict method (vs input_fn). If you create an iterator that feeds examples (that never sends the StopIteration exception) then you could keep the model \"resident\" avoiding the graph being built every time you call predict.\r\n\r\nHowever, the x parameter as iterable does not work,  because it seems that you can only provide it low level inputs (such as numpy arrays or pandas dataframe of type float or int only). It seems that this solution precludes the nice feature columns such as sparse tensors (for categoricals) and embeddings (to get inputs to the neural net), etc.\r\n\r\nAnother problem that arises with the above issues with the estimator design is that although it has an export method to create a model that tensorflow serving loads nicely, it will not work, because the graph needs to be built at predict time to extract model inputs from the higher level feature column tensors and of course tensor flow serving does not work like that. It loads a graph and runs inputs to create outputs - similar to the predict method using the x parameter. It does not create graphs on the fly every time a predict call is made.\r\n\r\nWhat is needed as a new feature is that the x and y parameters of the predict method should accept raw inputs (like strings for categorical features/embeddings) that is then passed into the feed_dict parameter of the session run call.\r\n\r\nSimilarly, for tensorflow serving, it should be possible to export the estimator and write the model client so that the raw feature column values (strings) can be sent to the model instantiated in tensorflow serving. \r\n\r\nThis feature is quite urgent in my opinion as models based on the cool abstractions available in the contrib.learn estimator API are not very useful beyond training and evaluation without this requested feature.\r\n", "comments": ["@martinwicke I don't quite understand this one.  Does it make sense to you?\n", "I'm not entirely sure what feature you are asking for, but it is clear that there are shortcomings around the sklearn interface, `input_fn`, and how they mix, especially where serving is concerned. \n\n@davidsoergel is working on an update to export that I believe should fix most of your problems there.\n\nWe are discussing changes to the predict interface at the moment, including possibly reusing the graph for multiple queries, and providing a utility to run exported models without starting up a full tensorflow/serving instance.\n\nI will close this bug, not because your complaints are invalid, but because I see no actionable feature request (beyond, \"Please fix this, it's no good\", which we're working on).\n", "Thanks Martin\n\nLooking forward to the fix.\n", "How to I follow the updates that @davidsoergel is working on that you said will fix my mentioned problems?\r\n", "@fventer Apparently, this landed in master as of 7942860247b2f945e42092d43b29d9641ad2a217.\r\nThe exported *SavedModel* (similar to SessionBundle) can then be served using tensorflow/serving with the `--use_saved_model=true` option. \r\nI can confirm it's working great!\r\n\r\nThanks @davidsoergel! \ud83c\udf89", "Thanks!\n\nI will be testing it asap.\n\nRegards\n\nFritz Venter\nfritz.venter@gmail.com\ncell: +15122937896\n\nOn Wed, Dec 7, 2016 at 12:17 PM, nubbel <notifications@github.com> wrote:\n\n> @fventer <https://github.com/fventer> Apparently, this landed in master\n> as of 7942860\n> <https://github.com/tensorflow/tensorflow/commit/7942860247b2f945e42092d43b29d9641ad2a217>\n> .\n> The exported *SavedModel* (similar to SessionBundle) can then be served\n> using tensorflow/serving with the --use_saved_model=true option.\n> I can confirm it's working great!\n>\n> Thanks @davidsoergel <https://github.com/davidsoergel>! \ud83c\udf89\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5598#issuecomment-265526869>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAwRcs3IETwwBcutopJgSFVa19nDbjgcks5rFvhLgaJpZM4KxaTj>\n> .\n>\n", "@nubbel  I see you have the SavedModel working, did you get the export working with Sparse based tensors ? specifically from the Deep and Wide examples, namely sparse_column_with_keys or sparse_column_with_hash_bucket, I can get an export working if I use real_valued_column and bucketized_column (using real_valued_columns), has to do with the resulting 'SparseTensor' object has no attribute 'name'\r\n\r\nPeter", "@sendit2me Hi Peter, I ran in the exact same problem.\r\nI managed to work around it by converting the `VarLenFeature`s to `FixedLenFeature`s:\r\n```python\r\ndef fix_feature_spec(feature_spec):\r\n    for key, feature in feature_spec.items():\r\n        if isinstance(feature, tf.VarLenFeature):\r\n            feature_spec[key] = tf.FixedLenFeature(shape=[1], dtype=feature.dtype, default_value=None)\r\n```\r\n\r\nI also share my jupyter notebook with you where I build a model (based on the wide and deep tutorial) and export it to a `SavedModel`: https://gist.github.com/nubbel/a26b6cbb21c34bc36abbe76b850f5541\r\n(sorry for the verbose debug logs and deprecation warnings, don't have the time for cleaning this up right now)\r\n\r\nHope that helps!\r\n\r\n", "Thanks, that works a charm, for exporting... and loading to Tensorflow Serving...\r\nbut am now trying to figure out how to get the client format correct, did you compile a tensorflow client, I was trying to use \"tensorflow-serving-python\" which has the protos but this does not seem to work... I have tried a number of formats. any ideas what is wrong, or should I go straight to a tf client... ", "I've build a [Ruby client library](https://github.com/nubbel/tensorflow_serving_client-ruby), because I need to perform predictions from a Rails app.\r\n\r\nMy client code looks like that:\r\n```ruby\r\nreq = Tensorflow::Serving::PredictRequest.new\r\nreq.model_spec = Tensorflow::Serving::ModelSpec.new name: 'targeting_deep'\r\n\r\nexample = Tensorflow::Example.new features: Tensorflow::Features.new\r\nexample.features.feature['age'] = Tensorflow::Feature.new float_list: Tensorflow::FloatList.new(value: [customer.age || -1])\r\nexample.features.feature['gender'] = Tensorflow::Feature.new bytes_list: Tensorflow::BytesList.new(value: [customer.gender || 'n/a'])\r\nexample.features.feature['state'] = Tensorflow::Feature.new bytes_list: Tensorflow::BytesList.new(value: [customer.gender || 'n/a'])\r\nexample.features.feature['purchase_recency'] = Tensorflow::Feature.new float_list: Tensorflow::FloatList.new(value: [customer.purchase_recency || -1])\r\nexample.features.feature['purchase_frequency'] = Tensorflow::Feature.new float_list: Tensorflow::FloatList.new(value: [customer.purchase_frequency || -1])\r\nexample.features.feature['monetary_value'] = Tensorflow::Feature.new float_list: Tensorflow::FloatList.new(value: [customer.monetary_value || -1])\r\nexample.features.feature['interaction_recency'] = Tensorflow::Feature.new float_list: Tensorflow::FloatList.new(value: [customer.interaction_recency || -1])\r\nexample.features.feature['interaction_frequency'] = Tensorflow::Feature.new float_list: Tensorflow::FloatList.new(value: [customer.interaction_frequency || -1])\r\n\r\nserialized_example = Tensorflow::Example.encode example\r\n\r\nreq.inputs['inputs'] = Tensorflow::TensorProto.new(\r\n  string_val: [serialized_example],\r\n  tensor_shape: Tensorflow::TensorShapeProto.new(dim: [Tensorflow::TensorShapeProto::Dim.new(size: 1)]),\r\n  dtype: Tensorflow::DataType::DT_STRING\r\n)\r\n\r\nstub = Tensorflow::Serving::PredictionService::Stub.new('model-server:8500', :this_channel_is_insecure)\r\nres = stub.predict req\r\nres.outputs\r\n```\r\n\r\nIt should be straightforward to translate that to python.", "@nubbel Thank you for your tutorial and  you ruby client. \r\n\r\nCould you please offer a java version predict client since so many issues and stackoverflow questions on this. "]}, {"number": 5597, "title": "Seq2seq buckets option confusion", "body": "The model_with_buckets function will **raise error when length of encoder_inputsut, targets, or weights is smaller than the largest (last) bucket ??** why? Shouldn't the opposite be true?\r\n\r\n", "comments": ["Ok, so model_with_buckets will **truncate the sequence by the size of the bucket**. That make sense now.\n"]}, {"number": 5596, "title": "grpc async server wrapper bug", "body": "code in distributed_runtime/rpc/grpc_master_service.cc\r\n```\r\n  void HandleRPCsLoop() override {\r\n    ENQUEUE_REQUEST(CreateSession, true);\r\n    ENQUEUE_REQUEST(ExtendSession, false);\r\n    for (int i = 0; i < 100; ++i) {\r\n      ENQUEUE_REQUEST(RunStep, true);\r\n    }\r\n    ENQUEUE_REQUEST(CloseSession, false);\r\n    ENQUEUE_REQUEST(ListDevices, false);\r\n    ENQUEUE_REQUEST(Reset, false);\r\n\r\n    void* tag;\r\n    bool ok;\r\n    while (cq_->Next(&tag, &ok)) {\r\n   \r\n      UntypedCall<GrpcMasterService>::Tag* callback_tag =\r\n          static_cast<UntypedCall<GrpcMasterService>::Tag*>(tag);\r\n      if (callback_tag) {\r\n        callback_tag->OnCompleted(this, ok);\r\n        delete callback_tag;\r\n      } else {\r\n        // NOTE(mrry): A null `callback_tag` indicates that this is\r\n        // the shutdown alarm.\r\n        cq_->Shutdown();\r\n      }\r\n    }\r\n  }\r\n```\r\n\r\n **While increasing the test pressure, I found that variable 'ok' will be set false, and \r\nin callback_tag->OnCompleted, it will not call callback if ok is false, thus   ENQUEUE_REQUEST will not be called, new incoming request will have no chance to be handled,\r\nmemory increase more and more, if any bug here?**", "comments": ["@supertim Can you provide instructions to reproduce?\n", "@mrry Anything obvious here?\n", "Possibly... gRPC requires that you enqueue a pending request object on the completion queue for each method that you want to handle. The `ENQUEUE_REQUEST` macros in `HandleRPCsLoop()` configure the initial pending request objects; and subsequent objects are enqueued under (some calls to) `callback_tag->OnCompleted()`, but only when `ok` is true.\n\nHowever... it's not clear whether receiving `!ok` from the completion queue is a recoverable error or not. The [docs](http://www.grpc.io/grpc/cpp/classgrpc_1_1_completion_queue.html#a86d9810ced694e50f7987ac90b9f8c1a) define it as \"true if read a regular event, false otherwise\". So I'd be happy to look into this, but we'll need some way to reproduce it first, and might need to follow up with the gRPC team.\n", "Reproduce:\nJust a simple grpc Async Client, send large Request(10M) indefinately. \n", "I found \n  while I increase test press (20 req / sec),  8M/req, client send small packet to server(I dont know what's that ), then call Next(ok), ok is false,1), but the server cpu is very low\n", "Are you sending these requests through the public TensorFlow API? Which methods are you calling? This does sound like it might be a bug, but without a reproducible failure case we won't be able to spend time on it.\n", "Closing due to lack of activity."]}, {"number": 5595, "title": "Saving optimizer state (adagrad/momentum/etc.)", "body": "Hey everybody,\r\n\r\nLast week I asked this question on stackoverflow: https://stackoverflow.com/questions/40547198/saving-the-state-of-the-adagrad-algorithm-in-tensorflow . \r\nMy problem is that I want to save the state of the optimizer (in my case the adagrad accumulators) so I can stop my learning and continue whenever I want. \r\n\r\nUnless I'm mistaken the state of the optimizer can't be saved (you cant pass an optimizer to a tf.train.Saver, right?). A quick (hacky?) solution for me might be is calling Optimizer.get_slot_names() and save the op of each slot. \r\nThe next problem would be putting this op back in the slots, as I don't think there is a set_slot(name,op) at the moment. \r\n\r\nSo my questions are: \r\n- Am I right that this is currently impossible? \r\n- Do we want to have a set_slot(name,op) function in the Optimizer class? (I am willing to help out with this)\r\n- Do we want to be able to pass an optimizer to a Saver object?", "comments": ["Thank you for asking the question on stackoverflow, which is a better place for it.  The optimizer state will be saved by default, and is only not saved because you are specifically telling the saver what to save.\n"]}, {"number": 5594, "title": "Doc Errors related to  tf.space_to_batch", "body": "In the[ api-doc related to](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/array_ops.md#tfspace_to_batchinput-paddings-block_size-namenone-space_to_batch) `tf.space_to_batch`, the result of example case of \r\n\r\n> (3) For the following input of shape `[1, 4, 4, 1]` and block_size of 2:\r\n```prettyprint\r\nx = [[[[1],   [2],  [3],  [4]],\r\n      [[5],   [6],  [7],  [8]],\r\n      [[9],  [10], [11],  [12]],\r\n      [[13], [14], [15],  [16]]]]\r\n```\r\n\r\nshould be \r\n```prettyprint\r\nx = [[[[1], [3]], [[9], [11]]],\r\n     [[[2], [4]], [[10], [12]]],\r\n     [[[5], [7]], [[13], [15]]],\r\n     [[[6], [8]], [[14], [16]]]]\r\n```\r\nrather than \r\n```prettyprint\r\nx = [[[[1], [3]], [[5], [7]]],\r\n     [[[2], [4]], [[10], [12]]],\r\n     [[[5], [7]], [[13], [15]]],\r\n     [[[6], [8]], [[14], [16]]]]\r\n```\r\n\r\nDocs related to `tf.space_to_batch_nd`, `tf.batch_to_space_nd`, `tf.batch_to_space` have similar problems in their **(3)** cases, too. \r\n\r\n", "comments": ["@gpapan Does this look right to you?\n", "The new  doc link is here, and it does seem to have  the problem still and prettyprint shown in all examples.\r\nhttps://www.tensorflow.org/api_docs/python/tf/space_to_batch", "This is fixed in master:  \r\nhttps://www.tensorflow.org/versions/master/api_docs/python/tf/space_to_batch \r\n\r\nand in v1.2:\r\nhttps://www.tensorflow.org/versions/r1.2/api_docs/python/tf/space_to_batch\r\n\r\nThe quoted link will get fixed automatically when we switch v1.2.  We generally don't cherrypick back cosmetic change like this, so r1.1 will have this for the foreseeable future.  I think we can close this."]}, {"number": 5593, "title": "windows build: avgpooling_op.obj : error LNK2019", "body": "**using the latest version of the tf_core_kernels, after msbuild(both tf_tutorials_example_trainer and  tf_python_build_pip_package ),build on release ,get errors:**\r\n\r\n> \r\n> avgpooling_op.obj : error LNK2019: \u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 \"public: void __cdecl tensorflow::functor::SpatialAvgPooling<struct E\r\n> igen::GpuDevice,struct Eigen::half>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::\r\n> Tensor<struct Eigen::half,4,1,__int64>,16,struct Eigen::MakePointer>,class Eigen::TensorMap<class Eigen::Tensor<stru\r\n> ct Eigen::half const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,int,int,int,enum Eigen::PaddingType const &)\" (\r\n> ??R?$SpatialAvgPooling@UGpuDevice@Eigen@@Uhalf@2@@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tens\r\n> or@Uhalf@Eigen@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@V?$TensorMap@V?$Tensor@$$CBUhalf@Eigen@@$03$00_J@Eigen@@$0BA@\r\n> UMakePointer@2@@4@HHHHAEBW4PaddingType@4@@Z)\uff0c\u8be5\u7b26\u53f7\u5728\u51fd\u6570 \"public: virtual void __cdecl tensorflow::AvgPoolingOp<struct Ei\r\n> gen::GpuDevice,struct Eigen::half>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$AvgPoolingOp@UGpuDevic\r\n> e@Eigen@@Uhalf@2@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z) \u4e2d\u88ab\u5f15\u7528 [H:\\docker\\tensorflow\\tensorflow\\contrib\\cmake\\bui\r\n> ld\\pywrap_tensorflow.vcxproj]\r\n>   avgpooling_op.obj : error LNK2019: \u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 \"public: void __cdecl tensorflow::functor::SpatialAvgPooling<struct E\r\n> igen::GpuDevice,float>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<float,\r\n> 4,1,__int64>,16,struct Eigen::MakePointer>,class Eigen::TensorMap<class Eigen::Tensor<float const ,4,1,__int64>,16,s\r\n> truct Eigen::MakePointer>,int,int,int,int,enum Eigen::PaddingType const &)\" (??R?$SpatialAvgPooling@UGpuDevice@Eigen\r\n> @@M@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@M$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@V?$\r\n> TensorMap@V?$Tensor@$$CBM$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HHHHAEBW4PaddingType@4@@Z)\uff0c\u8be5\u7b26\u53f7\u5728\u51fd\u6570 \"public: virtual v\r\n> oid __cdecl tensorflow::AvgPoolingOp<struct Eigen::GpuDevice,float>::Compute(class tensorflow::OpKernelContext *)\" (\r\n> ?Compute@?$AvgPoolingOp@UGpuDevice@Eigen@@M@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z) \u4e2d\u88ab\u5f15\u7528 [H:\\docker\\tensorflow\\te\r\n> nsorflow\\contrib\\cmake\\build\\pywrap_tensorflow.vcxproj]\r\n>   avgpooling_op.obj : error LNK2019: \u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 \"bool __cdecl tensorflow::RunAvePoolBackwardNHWC<struct Eigen::half>(\r\n> struct Eigen::half const * const,int,int,int,int,int,int,int,int,int,int,int,int,struct Eigen::half * const,struct E\r\n> igen::GpuDevice const &)\" (??$RunAvePoolBackwardNHWC@Uhalf@Eigen@@@tensorflow@@YA_NQEBUhalf@Eigen@@HHHHHHHHHHHHQEAU1\r\n> 2@AEBUGpuDevice@2@@Z)\uff0c\u8be5\u7b26\u53f7\u5728\u51fd\u6570 \"public: virtual void __cdecl tensorflow::AvgPoolingGradOpCustomGPUKernel<struct Eigen:\r\n> :half>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$AvgPoolingGradOpCustomGPUKernel@Uhalf@Eigen@@@tens\r\n> orflow@@UEAAXPEAVOpKernelContext@2@@Z) \u4e2d\u88ab\u5f15\u7528 [H:\\docker\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow.v\r\n> cxproj]\r\n>   avgpooling_op.obj : error LNK2019: \u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 \"bool __cdecl tensorflow::RunAvePoolBackwardNHWC<float>(float const *\r\n>  const,int,int,int,int,int,int,int,int,int,int,int,int,float * const,struct Eigen::GpuDevice const &)\" (??$RunAvePoo\r\n> lBackwardNHWC@M@tensorflow@@YA_NQEBMHHHHHHHHHHHHQEAMAEBUGpuDevice@Eigen@@@Z)\uff0c\u8be5\u7b26\u53f7\u5728\u51fd\u6570 \"public: virtual void __cdecl te\r\n> nsorflow::AvgPoolingGradOpCustomGPUKernel<float>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$AvgPooli\r\n> ngGradOpCustomGPUKernel@M@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z) \u4e2d\u88ab\u5f15\u7528 [H:\\docker\\tensorflow\\tensorflow\\contrib\\c\r\n> make\\build\\pywrap_tensorflow.vcxproj]\r\n>   H:\\docker\\tensorflow\\tensorflow\\contrib\\cmake\\build\\Release\\pywrap_tensorflow.dll : fatal error LNK1120: 4 \u4e2a\u65e0\u6cd5\u89e3\u6790\u7684\u5916\r\n> \u90e8\u547d\u4ee4 [H:\\docker\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow.vcxproj]\r\n> \r\n\r\nIs the erros relateds to the latest commit of the tf_core_kernels.cmake and how may I fix it?", "comments": ["I'm trying to use the old version before the \u2019tf_core_kernels commit to build a new version ,good luck with me ..\n", "trying #5561 ,maybe this will help\n", "succeed using #5561 \n"]}, {"number": 5592, "title": "The performance of fp16  is quite bad.", "body": "https://github.com/tensorflow/tensorflow/issues/1300\r\n\r\nI have got a nvidia p100 GPU which is support fp16, and I run the TF case 'cifar10_train.py'. Without option '--use_fp16', the performance is also 1600 examples/sec, and with the option '--use_fp16', the performance down to 500 examples/sec. Any ideas about this issue?\r\n\r\nuserid@ubuntu-WK-4xP100:~/weike/tensorflow-r0.11/tensorflow/models/image/cifar10$ vi cifar10_train.py \r\nuserid@ubuntu-WK-4xP100:~/weike/tensorflow-r0.11/tensorflow/models/image/cifar10$ python cifar10_train.py \r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.8.0 locally\r\n>> Downloading cifar-10-binary.tar.gz 100.0%\r\n...\r\n2016-11-14 00:07:57.143739: step 0, loss = 4.67 (12.1 examples/sec; 10.549 sec/batch)\r\n2016-11-14 00:07:58.395209: step 10, loss = 4.57 (1693.6 examples/sec; 0.076 sec/batch)\r\n2016-11-14 00:07:59.177525: step 20, loss = 4.97 (1668.9 examples/sec; 0.077 sec/batch)\r\n2016-11-14 00:07:59.957789: step 30, loss = 4.43 (1588.3 examples/sec; 0.081 sec/batch)\r\n2016-11-14 00:08:00.738431: step 40, loss = 4.52 (1690.5 examples/sec; 0.076 sec/batch)\r\n2016-11-14 00:08:01.501940: step 50, loss = 4.33 (1680.4 examples/sec; 0.076 sec/batch)\r\n2016-11-14 00:08:02.241604: step 60, loss = 4.20 (1733.4 examples/sec; 0.074 sec/batch)\r\n2016-11-14 00:08:03.001845: step 70, loss = 4.27 (1706.0 examples/sec; 0.075 sec/batch)\r\n2016-11-14 00:08:03.765522: step 80, loss = 4.18 (1601.3 examples/sec; 0.080 sec/batch)\r\n2016-11-14 00:08:04.516780: step 90, loss = 4.25 (1646.3 examples/sec; 0.078 sec/batch)\r\n\r\n\r\nuserid@ubuntu-WK-4xP100:~/weike/tensorflow-r0.11/tensorflow/models/image/cifar10$ python cifar10_train.py --use_fp16\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.8.0 locally\r\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\r\n..\r\n2016-11-14 00:09:09.382854: step 0, loss = 4.67 (12.3 examples/sec; 10.411 sec/batch)\r\n2016-11-14 00:09:11.923842: step 10, loss = 4.58 (659.6 examples/sec; 0.194 sec/batch)\r\n2016-11-14 00:09:13.918448: step 20, loss = 6.62 (460.6 examples/sec; 0.278 sec/batch)\r\n2016-11-14 00:09:16.211809: step 30, loss = 4.37 (583.7 examples/sec; 0.219 sec/batch)\r\n2016-11-14 00:09:18.327690: step 40, loss = 4.30 (618.3 examples/sec; 0.207 sec/batch)\r\n2016-11-14 00:09:20.395409: step 50, loss = 4.37 (643.8 examples/sec; 0.199 sec/batch)\r\n2016-11-14 00:09:22.466230: step 60, loss = 4.32 (574.0 examples/sec; 0.223 sec/batch)\r\n2016-11-14 00:09:24.533225: step 70, loss = 4.17 (646.2 examples/sec; 0.198 sec/batch)\r\n2016-11-14 00:09:26.609277: step 80, loss = 2.59 (601.5 examples/sec; 0.213 sec/batch)\r\n2016-11-14 00:09:28.703648: step 90, loss = 4.15 (625.5 examples/sec; 0.205 sec/batch)\r\n", "comments": ["@benoitsteiner Would additional information be helpful?\n", "@AlvinChen13 Did you compile TensorFlow yourself ? If so, which architecture did you target ?\n", "@benoitsteiner Yes, I compile r0.11 with cuda 8, and my system is x86_64 with Tesla P100. And I just follow the guideline step by step to build TF. \n\n# To build with GPU support:\n\n$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n\n$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\n\n# The name of the .whl file will depend on your platform.\n\n$ sudo pip install /tmp/tensorflow_pkg/tensorflow-0.11.0rc0-py2-none-any.whl\n\nAny special options need be given while compiling TF to support fp16?\n", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.", "Any news on that ? I have the same issue with the P100, with TensorFlow 1.3.0 (tensorflow-gpu) installed from pip.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been a while since I revisited cifar10 training in fp16.  \r\nI recently tried cifar10 training with --use_fp16 with a P100 using TF 1.4 and the perf is really bad compared to fp32.  \r\nSome observations:\r\n\r\n1. On my P100 fp32 trains at ~16000 examples/sec, while using --use_fp16 it drops down to ~780 examples/sec\r\n2. I notice that for fp16 the GPU is highly underutilized ... just about 4% (as seen with nvidia-smi) and the CPU cores are pretty busy at ~80%. - so it seems like most ops are being done on the CPU.  \r\n3. For fp32 training the CPU is about 30% busy and the GPU varies between 80 and 100%\r\n\r\nAnyone else have better luck with this?\r\n\r\n", "#15643 & #15585 seem like ~duplicates of this.", "I'm the author of #15585 and would like to comment on this / state a short version of my findings. Please see my issue for more.\r\n\r\n@quaeler \r\nYou're right, this could be a similar issue.\r\n\r\n@AlvinChen13 could you try to profile your cifar example using nvprof? Like I stated in my issue, it seems that Tensorflow is using slower CUDA functions (*maxwell_fp16_segmemm_fp16_128x128_nn* for FP16 which is worse than the cuBLAS method *maxwell_hgemm_256x128_nn*)\r\n\r\n@rajivkapoor I did not (yet) measure the CPU, since the actual GPU performance for elementary operations is worse... Could you also try running nvprof and check if the \"raw\" GPU performance drops when using FP16? In my case, the GPU was at 100%, I will check this for cifar10 when I find the time.\r\n  ", "I rolled in the duplicate issues.\r\n\r\nPlease note [Intel docs](https://software.intel.com/en-us/articles/performance-benefits-of-half-precision-floats) say float16 makes tradeoff on CPU for space. Further note AVX is needed to make float16 go fast, which likely means TensorFlow needs to be built from source with `-c opt --copt=-march=native` on an Intel sandybridge? or AMD bulldozer? or better. I'm not sure how it impacts GPUs.\r\n\r\ncc: @danielcdh and @rmlarsen who might know more about float16 performance.", "See also https://github.com/tensorflow/benchmarks/issues/77", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thank you @bhack for sharing that link. That looks like a very authoritative answer. Please also note recent TensorFlow binary releases enable AVX by default. Things should be significantly better than they were back in 2016 when this issue was originally filed.", "Better late than never :).  Recent TF builds (1.8) with CUDA 9.1 has indeed fixed the issue.  Depending on the batch size, I see pretty good fp16 perf. on NVidia P100 and V100 for both inference and training.\r\n\\rajiv", "@rajivkapoor Does the batch size typically need to be higher to get good performance when training with fp16? How much bigger?", "@Freddy Snijder - Strictly speaking about the TF CNN benchmarks, generally,\nfor a single V100 a bs >=8, fp16 gives pretty decent gains over fp32 - at\nleast for vgg11, inception3 and resnet*.\nI have seen gains starting at bs >=4 for 2 or more GPUs per server.\n\n\n\nOn Fri, Jul 20, 2018 at 3:45 PM Freddy Snijder <notifications@github.com>\nwrote:\n\n> @rajivkapoor <https://github.com/rajivkapoor> Does the batch size\n> typically need to be higher to get good performance when training with\n> fp16? How much bigger?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5592#issuecomment-406744304>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AaPr7_x8uLmPatI5O4E9dmXqBe-Asfsxks5uIl1vgaJpZM4Kw_KE>\n> .\n>\n", "Excuse me for the interruption, but float16 is also slow on Core2 CPU.", "Hi,\r\nIf I trained a model on fp16 on a turing gpu and I want to do the inference on cpu is it possible. Would the inference time be faster and does Tensorflow support such senario because as I see PyTorch doesn't support fp16 inference on cpu. Is support on fp16 a limitation of intel cpus or the framework( like Tensorflow , PyTorch etc) . ", "meet the same problem on v100: fp16 is very slow than fp32", "Met the same problem, fp16 is much slower than fp32", "Could you please provide us with a timeline of when the issue is expected to be resolved? I've faced the same problem on Jetson Nano as well as Laptop running CUDA 10, GTX 1050Ti. ", "meet the same problem on v100 with Cuda 9.0: fp16 is very slow than fp32."]}, {"number": 5591, "title": "I only have CUDA 8.0 but Couldn't open CUDA library libcudnn.so. LD_LIBRARY_PATH: ", "body": ">>> import tensorflow\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:105] Couldn't open CUDA library libcudnn.so. LD_LIBRARY_PATH: \r\nI tensorflow/stream_executor/cuda/cuda_dnn.cc:3448] Unable to load cuDNN DSO\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\r\n", "comments": ["Does installing cuDNN solve the problem?\n", "i have install cdunn and cuda\n", "@zsxgb What platform are you on?  Have you checked `LD_LIBRARY_PATH` or the equivalent?  What is the right path so the cudnn.so file?\n", "I had same problem before, easy way to fix is to install cudnn into local using\n\n```\ntar xvzf cudnn-8.0-linux-x64-v5.1.tgz\nsudo cp -P cuda/include/cudnn.h /usr/local/cuda/include\nsudo cp -P cuda/lib64/libcudnn* /usr/local/cuda/lib64\nsudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*\n```\n", "\r\n![6239a9335826955468832cd8f76b6bc7](https://cloud.githubusercontent.com/assets/16316669/20485643/7b3a5282-b037-11e6-9480-86ecd5035376.jpg)\r\nI probably have the same problem.  I could import tensorflow perfectly but in spyder2.7 there are mistakes above:\r\nCouldn't open CUDA library libcudnn.so. LD_LIBRARY_PATH. Who can help me ?", "@zsxgb and @zhanghan328 Your LD_LIBRARY_PATH environment variables are not set.  \r\nPlease follow the instructions here: https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#optional-linux-enable-gpu-support\r\n\r\nPlease feel free to reopen this issue if the above instructions do not fix your problem.", "Have you solved the problems? I have set the LD_LIBRARY_PATH environment and also useless....oops", "Same problem. @prb12  I have set LD_LIBRARY_PATH.\r\nBTW, I can import&use TF successfully 3 weeks ago in the same PC.\r\nI haven't touch cuda related file. Strange...\r\nBTW2, I cant find libcudnn.so not just in usr/local/cuda, but in the whole PC...\r\nhow could this file disappeared??", "Extract the cuDNN archive to a directory of your choice, referred to below as <installpath>.\r\nThen follow the platform-specific instructions as follows.\r\n\r\nLINUX\r\n\r\n    cd <installpath>\r\n    export LD_LIBRARY_PATH=`pwd`:$LD_LIBRARY_PATH\r\n\r\n    Add <installpath> to your build and link process by adding -I<installpath> to your compile\r\n    line and -L<installpath> -lcudnn to your link line.\r\n\r\nIt works for me."]}, {"number": 5590, "title": "Remove argv argument from tf.app.run()", "body": "The second argument in tf.app.run is causing a TypeError and I think we can remove this.  \r\n\r\n    $ python classify_image.py \r\n    Traceback (most recent call last):\r\n      File \"classify_image.py\", line 227, in <module>\r\n        tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n    TypeError: run() got an unexpected keyword argument 'argv'\r\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "@leoybkim, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @tensorflower-gardener and @josh11b to be potential reviewers.\n", "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/app.py#L26 run() indeed has an argv parameter now -- you are probably trying to use classify_image.py at HEAD when you're running an earlier version of the binary.\n"]}, {"number": 5589, "title": "Summary overwrite earlier models", "body": "Summary writer overwrites the previous version of models. For example, when I save any model that has (step + 1) % 100 == 0, then when saving model-1299 model-299 is deleted.\r\n\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n-rw-r--r-- 1 root root 189170 Sep 23  2015 /usr/local/cuda-7.5/lib/libcudadevrt.a\r\nlrwxrwxrwx 1 root root     16 Sep 23  2015 /usr/local/cuda-7.5/lib/libcudart.so -> libcudart.so.7.5\r\nlrwxrwxrwx 1 root root     19 Sep 23  2015 /usr/local/cuda-7.5/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\r\n-rwxr-xr-x 1 root root 311596 Sep 23  2015 /usr/local/cuda-7.5/lib/libcudart.so.7.5.18\r\n-rw-r--r-- 1 root root 558020 Sep 23  2015 /usr/local/cuda-7.5/lib/libcudart_static.a\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n0.11.0rc0\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nif (step + 1) % 100 == 0:\r\n  saver.save(sess, 'model', global_step = step)\r\n", "comments": ["The `Saver` class has options that control deleting previous checkpoints.  Please see the documentation for details: https://www.tensorflow.org/versions/r0.11/api_docs/python/state_ops.html#Saver \n"]}, {"number": 5588, "title": "bidirectional_dynamic_rnn: make sequence length optional", "body": "Qualifier: very new to TF, so I may be missing something very obvious.  If so, apologies.\r\n\r\nDOCUMENTATION: \r\n\r\nIs a little conflicting:\r\n\r\n\"sequence_length: An int32/int64 vector, size [batch_size], containing the actual lengths for each of the sequences.\" <-- implies sequence_length is required\r\n\r\nHowever...\r\n\r\n\"The initial state for both directions is zero by default (but can be set optionally) and no intermediate states are ever returned -- the network is fully unrolled for the given (passed in) length(s) of the sequence(s) or completely unrolled if length(s) is not given.\" <-- implies sequence_length is not required.\r\n\r\nFUNCTIONALITY\"\r\n\r\nBase on the function itself (\"_dynamic_\"), I would assume that allowing sequence_length=None is intended (although, in either case, the documentation should be straightened out).\r\n\r\nAssuming we do want to allow seq_length=None, the offending portion within bidirectional_dynamic_rnn would appear to be:\r\n\r\ninputs_reverse = array_ops.reverse_sequence(...seq_lengths=sequence_length...)\r\n\r\nreverse_sequence appears to require seq_lengths != None\r\n\r\nOddly enough, bidirectional_rnn may(?) be robust to this problem, as it uses _reverse_seq, which handles lengths=None.  \r\n\r\nThat said, I haven't tested this extensively, as I'm of course not 100% sure I've interpreted the above correctly and as to what the intended functionality is.\r\n\r\nGiven guidance, I could take a stab at a pull request to address the above.  Or maybe this is a quick fix/known issue (or non-issue...).\r\n\r\n", "comments": ["@ebrevdo My understanding is that sequence lengths should be required, since it doesn't make sense to do dynamic rnn fanciness otherwise.  However, that language does seem to imply it's optional.  Thoughts?\n", "You don't need sequence_lengths to do dynamic RNN; the fact that your time dimension is one of the axes of the inputs, and can vary from step to step, is what makes it dynamic.  Providing sequence_lengths is necessary for correctness though: if you pass these in, your final state value will be the true final state at the end of the sequence.  Otherwise it'll just keep calculating to the end of your inputs (which may include padding inputs - and this will throw your state off).  That said, it does seem that if sequence_lengths is None, we should just use array_ops.reverse to do a blind reversal.  The way bidirectional_rnn does.  PRs welcome.\n", "A follow up question: if we use padding inputs, and provide sequence_lengths to bidirectional_dynamic_rnn, would the backwark rnn read from the padding end or the true end of these padding inputs?\r\ne.g.\r\n\r\n`inputs = [[1,2,3,4],\r\n[5,2,3,0],\r\n[9,8,0,0]]`\r\nwould the backward rnn read from 0 or 3 in the second row? same question in row 3.", "This looks like a subtle bug introduced in a refactoring.  Instead of\narray_ops.reverse_sequence, the bidirectional code should call _reverse_seq\nin that module.  I'll fix it.\n\nOn Fri, Mar 3, 2017 at 10:57 AM, Dmitry Persiyanov <notifications@github.com\n> wrote:\n\n> Is there someone who works on this issue? I could prepare pull request.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5588#issuecomment-284039765>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim9q5PVqfE40MZiBQN7VS0uNWCW6uks5riGKqgaJpZM4Kw0_D>\n> .\n>\n", "According to [link1](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/ops/rnn.py#L1416-L1423) and [link2](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/ops/rnn.py#L268-L307), backward rnn read the sequence from the true end of the padded input @Jonbean ", "@vanpersie32 Thank you so much for pointing this out. ", "@ebrevdo \r\n\r\n> Otherwise it'll just keep calculating to the end of your inputs (which may include padding inputs - and this will throw your state off). That said, it does seem that if sequence_lengths is None, we should just use array_ops.reverse to do a blind reversal.\r\n\r\nDoes this operation has performance degraded?", "No more than the current version.\n\nOn Thu, Dec 14, 2017, 2:22 AM Bayberry Z. <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo>\n>\n> Otherwise it'll just keep calculating to the end of your inputs (which may\n> include padding inputs - and this will throw your state off). That said, it\n> does seem that if sequence_lengths is None, we should just use\n> array_ops.reverse to do a blind reversal.\n>\n> Does this operation has performance degraded?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5588#issuecomment-351669384>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim0tn0wgGJ1nxopq3Z8_MJcpaHFJZks5tAPbKgaJpZM4Kw0_D>\n> .\n>\n", "So in this example: inputs = [[9,8,0,0]]\r\ndoes the backward start reading from 8 or from 0?"]}, {"number": 5587, "title": "iOS No OpKernel to support TruncatedNormal", "body": "I have a NN similar to this one: http://stackoverflow.com/a/38576462/828184 and my iOS project is based on the simple contrib example.\r\n\r\nI write out the graph.pb file like so (and replace it with the original iOS one):\r\n`tf.train.write_graph(sess.graph_def, 'NNModel/', 'graph.pb', as_text=False)`\r\n\r\nBut XCode crashes on execution with this error:\r\n\r\n> RunModelViewController.mm: Could not create TensorFlow Graph: Invalid argument: No OpKernel was registered to support Op 'TruncatedNormal' with these attrs.  Registered kernels:\r\n>   <no registered kernels>\r\n> \t [[Node: OutputLayer/truncated_normal/TruncatedNormal = TruncatedNormal[T=DT_INT32, dtype=DT_FLOAT, seed=0, seed2=0](OutputLayer/truncated_normal/shape)]]\r\n\r\nI guess it's because of the usage of `tf.truncated_normal(...)`. Is there an alternative to that call or am I doing something wrong?\r\n\r\nSo far I got a minimalistic multiplication graph working on iOS but no trained NN.", "comments": ["@petewarden What do folks do if we haven't listed their favorite op in the mobile whitelist?\n", "We need to document this properly, but if you're using bazel, find the right .cc file and add it to android_extended_ops_group1 or 2 in tensorflow/core/kernels/BUILD, and add it to tensorflow/contrib/makefile/tf_op_files.txt too for the makefile build.\n", "Oh, and to find the right .cc file, look for REGISTER_KERNEL(\"TruncatedNormal\", ...) in the source code.\n", "@petewarden Should I leave this bug open to track the documentation part?\n", "@girving Yes, that does make sense.\n", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.", "I'm hitting this issue today. I created a simple NN (straight-line) example in Python with 1 node (1 weight + 1 bias), trained it and exported the graph. Using an example on StackOverflow, I tried to import this graph into C++ on a Raspberry Pi like device (C.H.I.P. $9 board) on which I had painstakingly compiled the static tensorflow C++ library. I tested the tensorflow installation using the pi_examples/ which worked fine. I also ensured that clear_devices was set to True when exporting the graph from Python as that was the training machine.\r\n\r\nterminate called after throwing an instance of 'std::runtime_error'\r\n  what():  Error creating graph: Invalid argument: No OpKernel was registered to support Op 'TruncatedNormal' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[Node: truncated_normal/TruncatedNormal = TruncatedNormal[T=DT_INT32, _output_shapes=[[1,1]], dtype=DT_FLOAT, seed=0, seed2=0](truncated_normal/shape)]]\r\nAborted\r\n\r\nchip@chip:~/tensorflow$ grep -Fsnr REGISTER_KERNEL . | grep \"TruncatedNormal\"\r\n./tensorflow/core/kernels/parameterized_truncated_normal_op.cc:361:  REGISTER_KERNEL_BUILDER(Name(\"ParameterizedTruncatedNormal\") \\\r\n./tensorflow/core/kernels/parameterized_truncated_normal_op.cc:375:  REGISTER_KERNEL_BUILDER(Name(\"ParameterizedTruncatedNormal\") \\\r\n\r\nGrep-ing, I couldn't find an exact match for the TruncatedNormal kernel in the source code either. Please help!", "**Tangential-note**: I just realized that the TruncatedNormal is the distribution I was using to randomly initialize my weights (in this case, 1 weight). Clearly, this kernel isn't needed as part of the inference graph as the weights have already been trained and will remain constant.\r\n\r\nReading up on optimizing the exported graph for inference, it looks like bazel is needed for that, along with some python tools. Is there any way to ask export_meta_graph() to only export the inference graph to side-step this issue?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "We know cover this a bit better in the documentation:\r\nhttps://www.tensorflow.org/mobile/prepare_models\r\n\r\nI hope this covers the problems people are running into, so closing out this bug unless there's new information."]}, {"number": 5586, "title": "AttributeError: 'LSTMStateTuple' object has no attribute 'get_shape'", "body": "My code \r\n```\r\ncell = cell_fn(args.rnn_size)\r\n\r\n        self.cell = cell = rnn_cell.MultiRNNCell([cell] * args.num_layers, state_is_tuple=True)\r\n\r\n        self.input_data = tf.placeholder(tf.int32, [args.batch_size, args.seq_length])\r\n        self.targets = tf.placeholder(tf.int32, [args.batch_size, args.seq_length])\r\n        self.initial_state = cell.zero_state(args.batch_size, tf.float32)\r\n\r\n        with tf.variable_scope('rnnlm'):\r\n            softmax_w = tf.get_variable(\"softmax_w\", [args.rnn_size, args.vocab_size])\r\n            softmax_b = tf.get_variable(\"softmax_b\", [args.vocab_size])\r\n            with tf.device(\"/cpu:0\"):\r\n                embedding = tf.get_variable(\"embedding\", [args.vocab_size, args.rnn_size])\r\n                inputs = tf.split(1, args.seq_length, tf.nn.embedding_lookup(embedding, self.input_data))\r\n                inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\r\n\r\n        def loop(prev, _):\r\n            prev = tf.matmul(prev, softmax_w) + softmax_b\r\n            prev_symbol = tf.stop_gradient(tf.argmax(prev, 1))\r\n            return tf.nn.embedding_lookup(embedding, prev_symbol)\r\n\r\n        initial_input = self.initial_state[0]\r\n        beam_decoder = BeamDecoder(num_classes=3, stop_token=2, beam_size=7, max_len=5)\r\n        _, final_state = tf.nn.seq2seq.rnn_decoder(\r\n                                [beam_decoder.wrap_input(initial_input)] + [None] * 4,\r\n                                beam_decoder.wrap_state(self.initial_state),\r\n                                beam_decoder.wrap_cell(cell),\r\n                                #loop_function = lambda prev_symbol, i: tf.reshape(prev_symbol, [-1, 1])\r\n                                loop_function=loop if infer else None, scope='rnnlm'\r\n                            )\r\n```\r\nthere is a problem appeared when I try to run ,the traceback info is as follows\r\n```\r\n[sun ~/workspace/sunxiaobiu/word-rnn-tensorflow 20:23:05]$ python sample.py\r\nTraceback (most recent call last):\r\n  File \"sample.py\", line 42, in <module>\r\n    main()\r\n  File \"sample.py\", line 25, in main\r\n    sample(args)\r\n  File \"sample.py\", line 32, in sample\r\n    model = Model(saved_args, True)\r\n  File \"/Users/sun/workspace/sunxiaobiu/word-rnn-tensorflow/model.py\", line 53, in __init__\r\n    loop_function=loop if infer else None, scope='rnnlm'\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/ops/seq2seq.py\", line 146, in rnn_decoder\r\n    output, state = cell(inp, state)\r\n  File \"/Users/sun/workspace/sunxiaobiu/word-rnn-tensorflow/beam_decoder.py\", line 172, in __call__\r\n    cell_outputs, raw_cell_state = self.cell(cell_inputs, past_cell_state)\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/ops/rnn_cell.py\", line 813, in __call__\r\n    cur_inp, new_state = cell(cur_inp, cur_state)\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/ops/rnn_cell.py\", line 310, in __call__\r\n    concat = _linear([inputs, h], 4 * self._num_units, True)\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/ops/rnn_cell.py\", line 889, in _linear\r\n    shapes = [a.get_shape().as_list() for a in args]\r\nAttributeError: 'LSTMStateTuple' object has no attribute 'get_shape'\r\n```\r\nI have tried to put the state_is_tuple=True , but it still not work~\r\n@lukaszkaiser ", "comments": ["The `LSTMStateTuple` is a tuple containing `c` and `h` fields.  Those fields are tensors.  The state tuple itself is not a tensor, and cannot be used as one.  If you want further assistance, please ask on StackOverflow.\n"]}, {"number": 5585, "title": "Reshape Op not the same as tensorflow.reshape", "body": "I am getting the error `Invalid argument: NodeDef mentions attr 'Tshape' not in Op<name=Reshape; signature=tensor:T, shape:int32 -> output:T; attr=T:type>; NodeDef: Y_GroundTruth = Reshape[T=DT_FLOAT, Tshape=DT_INT32](LOut_Add, Y_GroundTruth/shape)` when attempting to load the file on Android using `tensorflow::Status s = session->Create(graph_def);`.\r\n\r\nPeople with similar problems have mentioned that upgrading to Tensorflow 0.9.0rc0 fixed their problems. However, I am already using the most current Tensorflow build in my jni-build.\r\n\r\nCould this be a problem with the variables declared when generating the protobuf file?\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttps://github.com/tensorflow/tensorflow/issues/3002\r\nhttps://github.com/tensorflow/tensorflow/issues/1528\r\n\r\n### Environment info\r\nOperating System:\r\nMac OS 10.12\r\nAndroid Cyanogenmod 12\r\nTensorflow 0.10.0 on Jupyter\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n**Snippet**\r\n```    \r\n    def reg_perceptron(t, weights, biases):\r\n        t = tf.nn.relu(tf.add(tf.matmul(t, weights['h1']), biases['b1']), name = \"layer_1\")\r\n        t = tf.nn.sigmoid(tf.add(tf.matmul(t, weights['h2']), biases['b2']), name = \"layer_2\")\r\n        t = tf.add(tf.matmul(t, weights['hOut'], name=\"LOut_MatMul\"), biases['bOut'], name=\"LOut_Add\")\r\n\r\n        return tf.reshape(t, [-1], name=\"Y_GroundTruth\")\r\n\r\n    g = tf.Graph()\r\n    with g.as_default():\r\n       ...\r\n       rg_weights = {\r\n        'h1': vs.get_variable(\"weights0\", [n_input, n_hidden_1], initializer=tf.contrib.layers.xavier_initializer()),\r\n        'h2': vs.get_variable(\"weights1\", [n_hidden_1, n_hidden_2], initializer=tf.contrib.layers.xavier_initializer()),\r\n        'hOut': vs.get_variable(\"weightsOut\", [n_hidden_2, 1], initializer=tf.contrib.layers.xavier_initializer())\r\n        }\r\n\r\n\r\n        rg_biases = {\r\n        'b1': vs.get_variable(\"bias0\", [n_hidden_1], initializer=init_ops.constant_initializer(bias_start)),\r\n        'b2': vs.get_variable(\"bias1\", [n_hidden_2], initializer=init_ops.constant_initializer(bias_start)),\r\n        'bOut': vs.get_variable(\"biasOut\", [1], initializer=init_ops.constant_initializer(bias_start))\r\n        }\r\n        \r\n        #output node\r\n        pred = reg_perceptron(_x, rg_weights, rg_biases)\r\n        ...\r\n    ...\r\n\r\n    g_2 = tf.Graph()\r\n    with g_2.as_default():\r\n        ...\r\n        rg_weights_2 = {\r\n        'h1': vs.get_variable(\"weights0\", [n_input, n_hidden_1], initializer=tf.contrib.layers.xavier_initializer()),\r\n        'h2': vs.get_variable(\"weights1\", [n_hidden_1, n_hidden_2], initializer=tf.contrib.layers.xavier_initializer()),\r\n        'hOut': vs.get_variable(\"weightsOut\", [n_hidden_2, 1], initializer=tf.contrib.layers.xavier_initializer())\r\n        }\r\n\r\n        rg_biases_2 = {\r\n        'b1': vs.get_variable(\"bias0\", [n_hidden_1], initializer=init_ops.constant_initializer(bias_start)),\r\n        'b2': vs.get_variable(\"bias1\", [n_hidden_2], initializer=init_ops.constant_initializer(bias_start)),\r\n        'bOut': vs.get_variable(\"biasOut\", [1], initializer=init_ops.constant_initializer(bias_start))\r\n        }\r\n    \r\n        pred_2 = reg_perceptron(_x_2, rg_weights_2, rg_biases_2)\r\n        ...\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\nI will double check the Tensorflow version tomorrow and make sure that it is the same as the version I am using in the jni-build.\r\n\r\nI also attempted to strip the graph using\r\n```\r\n#!/bin/bash\r\nbazel-bin/tensorflow/python/tools/strip_unused --input_graph=/Users/leslie/Downloads/trained_model6.pb --output_graph=/Users/leslie/Downloads/stripped_graph.pb --input_node_names=X_Input --output_node_names=Y_GroundTruth --input_binary=true\r\n```\r\n\r\nbut the output is simply\r\n```\r\n6\r\n\r\nY_GroundTruth\u0012\u000bPlaceholder*\u000b\r\n\u0005dtype\u0012\u00020\u0001*\u000b\r\n\u0005shape\u0012\u0002:\r\n```\r\n\r\nand attempting to freeze the graph with\r\n\r\n```\r\n#!/bin/bash\r\nbazel-bin/tensorflow/python/tools/freeze_graph \\\r\n--input_graph=/Users/leslie/Downloads/trained_model6.pb \\\r\n--input_checkpoint=/Users/leslie/Downloads/Y6_1476978999 \\\r\n--output_graph=/Users/leslie/Downloads/frozen_graph.pb --output_node_names=Y_GroundTruth\r\n```\r\n\r\ngives the error\r\n```\r\nFile \"/Users/leslie/tensorflow-master/bazel-bin/tensorflow/python/tools/strip_unused.runfiles/org_tensorflow/tensorflow/python/tools/strip_unused.py\", line 77, in <module>\r\n    tf.app.run()\r\n  File \"/Users/leslie/tensorflow-master/bazel-bin/tensorflow/python/tools/strip_unused.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"/Users/leslie/tensorflow-master/bazel-bin/tensorflow/python/tools/strip_unused.runfiles/org_tensorflow/tensorflow/python/tools/strip_unused.py\", line 74, in main\r\n    FLAGS.placeholder_type_enum)\r\n  File \"/Users/leslie/tensorflow-master/bazel-bin/tensorflow/python/tools/strip_unused.runfiles/org_tensorflow/tensorflow/python/tools/strip_unused_lib.py\", line 89, in strip_unused_from_files\r\n    placeholder_type_enum)\r\n  File \"/Users/leslie/tensorflow-master/bazel-bin/tensorflow/python/tools/strip_unused.runfiles/org_tensorflow/tensorflow/python/tools/strip_unused_lib.py\", line 62, in strip_unused\r\n    output_node_names)\r\n  File \"/Users/leslie/tensorflow-master/bazel-bin/tensorflow/python/tools/strip_unused.runfiles/org_tensorflow/tensorflow/python/framework/graph_util.py\", line 158, in extract_sub_graph\r\n    assert d in name_to_node_map, \"%s is not in graph\" % d\r\nAssertionError: Y_GroundTruth is not in graph\r\nleslie@MacBook-Pro tensorflow-master $ ./stripunused.sh\r\n1 ops in the final graph.\r\nleslie@MacBook-Pro tensorflow-master $ ./freezegraph.sh\r\nTraceback (most recent call last):\r\n  File \"/Users/leslie/tensorflow-master/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 135, in <module>\r\n    tf.app.run()\r\n  File \"/Users/leslie/tensorflow-master/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"/Users/leslie/tensorflow-master/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 132, in main\r\n    FLAGS.output_graph, FLAGS.clear_devices, FLAGS.initializer_nodes)\r\n  File \"/Users/leslie/tensorflow-master/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 98, in freeze_graph\r\n    text_format.Merge(f.read().decode(\"utf-8\"), input_graph_def)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/encodings/utf_8.py\", line 16, in decode\r\n    return codecs.utf_8_decode(input, errors, True)\r\nUnicodeDecodeError: 'utf8' codec can't decode byte 0x80 in position 278: invalid start byte\r\n```\r\n### Logs or other output that would be helpful\r\nTo stop the post from being too cluttered, I have uploaded my code to generate the .PB-file and my model to create the model on Pastebin.\r\n\r\nSurprisingly enough, the code used to test the model with the graph used to generate the .PB-file  (lines 154 and 155) does not return any errors (still in the process of checking the outputs).\r\n[PBFileGeneration](http://pastebin.com/6rv1rWV7)\r\n\r\n[Model](http://pastebin.com/8DJx1kmL)\r\n", "comments": ["It looks like you are running a binary for inference that is still too old.\n\nhttps://github.com/tensorflow/tensorflow/blob/5e5dc97dd3523509ce5f536d7be5122d016fc6b5/tensorflow/core/ops/array_ops.cc#L1328 is currently at HEAD in master, which suggests you are using an up to date binary for generating the protobuf model.\n\nHowever, the binary is complaining it doesn't know what \"Tshape\" is, which suggests it is running at an older commit where it doesn't have that definition in its binary -- if you make sure your model-interpreting binary is built at least or beyond what the protobuf-generating binary is, you should be okay.  Let us know if that helps.  \n\nFor anybody that is interested, https://www.tensorflow.org/versions/r0.11/resources/data_versions.html#tensorflow-data-versioning-graphdefs-and-checkpoints describes our compatibility guarantees.\n", "It's very strange but the JNI-build in my Android is using the most actual Tensorflow build in GitHub. I am not 100% if what I did was correct (I could not find any documentation on how to port Tensorflow to the JNI-build) but I am currently following the build from [TensorflowAndroidMNIST](https://github.com/miyosuda/TensorFlowAndroidMNIST). Here I just replaced the entire `include/tensorflow` with that from `tensorflow/tensorflow` and rebuilt the .so-file. As no errors were returned and the .so-file was generated, I assume it was done correctly. However, I still get the same error when attempting to load the file.\n\nI also tried editing `array_ops.cc` in the original Tensorflow build then rebuilding the .so-file but I get the same result.\n\n **EDIT:** I just realized that the Tensorflow files from [TensorflowAndroidMNIST](https://github.com/miyosuda/TensorFlowAndroidMNIST) were extracted from the Tensorflow Android  example app and not directly from Tensorflow itself. I will give an update tomorrow.\n", "Weirdly enough I am still getting the error. I am currently trying to find other areas where the \"Reshape\" op could be defined and see if I can change it there.\n", "@leslietso To clarify: you're still getting the error after building from master?  That seems unlikely, since a recent build would definitely understand `Tshape`.\n", "Yes. However, I do not believe it is the problem with master. I am currently trying to figure out what is going wrong when I am rebuilding Tensorflow in Android. It could be that somehow Android is still using the old build despite the existence of a newer build. I will update once I figure out what went wrong.\n\n**EDIT:** It seems like I got it working. It turned out for some reason `tensorflow/core/ops` was not included in the `tensorflow/BUILD` file and once I added that the error went away.\n\nThank you for the help!\n", "@leslietso could you explain more details \"It turned out for some reason tensorflow/core/ops was not included in the tensorflow/BUILD file and once I added that the error went away.\"\n\nOr if possible, could you send me a sample of your project?\nI am still working on [TensorflowAndroidMNIST](https://github.com/miyosuda/TensorFlowAndroidMNIST) but it does not work.\n"]}, {"number": 5584, "title": "Who can help me about my cuda-convnet2 reproduction in low accuracy", "body": "I followed the tutorials of cnn and  tried the[ exercise](https://www.tensorflow.org/versions/r0.8/tutorials/deep_cnn/index.html#convolutional-neural-networks) about reproducing  cifar10 model in[ cuda-convnet2.](https://github.com/akrizhevsky/cuda-convnet2)\r\nRecently, I thought I had worked it out, with implementing the local unshared layer by extract_image_patch function and batch_matmul function. [Here is my code of the cifar10.py](https://github.com/SunAriesCN/tensorflow_learning/blob/master/cifar10/exercise/ex2/cifar10_ex2_cuda_convnet.py) However, I found that the trained model is useless, because its accuracy is just 0.101, too low, for evaluation as below:\r\n`precision @ 1 = 0.101`\r\nMy inference() function just like this:\r\n```\r\ndef inference(images):\r\n  \"\"\"Build the CIFAR-10 model.\r\n  Args:\r\n    images: Images returned from distorted_inputs() or inputs().\r\n  Returns:\r\n    Logits.\r\n  \"\"\"\r\n  # We instantiate all variables using tf.get_variable() instead of\r\n  # tf.Variable() in order to share variables across multiple GPU training runs.\r\n  # If we only ran this model on a single GPU, we could simplify this function\r\n  # by replacing all instances of tf.get_variable() with tf.Variable().\r\n  #\r\n  # conv1\r\n  with tf.variable_scope('conv1') as scope:\r\n    kernel = _variable_with_weight_decay('weights', shape=[5, 5, 3, 64],\r\n                                         stddev=1e-4, wd=0.0)\r\n    conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\r\n    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\r\n    bias = tf.nn.bias_add(conv, biases)\r\n    conv1 = tf.nn.relu(bias, name=scope.name)\r\n    _activation_summary(conv1)\r\n\r\n  # pool1\r\n  pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\r\n                         padding='SAME', name='pool1')\r\n  # norm1\r\n  norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\r\n                    name='norm1')\r\n\r\n  # conv2\r\n  with tf.variable_scope('conv2') as scope:\r\n    kernel = _variable_with_weight_decay('weights', shape=[5, 5, 64, 64],\r\n                                         stddev=1e-4, wd=0.0)\r\n    conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\r\n    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.1))\r\n    print('conv2 is', conv)\r\n    print('biases2 is', biases)\r\n    bias = tf.nn.bias_add(conv, biases)\r\n    conv2 = tf.nn.relu(bias, name=scope.name)\r\n    _activation_summary(conv2)\r\n\r\n  # norm2\r\n  norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\r\n                    name='norm2')\r\n  # pool2\r\n  pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1],\r\n                         strides=[1, 2, 2, 1], padding='SAME', name='pool2')\r\n\r\n  # local3\r\n  with tf.variable_scope('local3') as scope:\r\n    print('pool2 is', pool2)\r\n    local3_extract_imgs = tf.extract_image_patches(pool2,[1,3,3,1],[1,1,1,1],[1,1,1,1],'SAME')\r\n    print('local3_extract_imgs is', local3_extract_imgs)\r\n    #local3_reshape = tf.reshape(local3_extract_imgs,[FLAGS.batch_size,6,6,3*3*64,1])\r\n    #print('local3_reshape is', local3_reshape)\r\n    kernel = _variable_with_weight_decay('weights', shape=[6,6,64,3*3*64], stddev=0.04, wd=0.004)\r\n    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.1))\r\n    local3_list = []\r\n    for img in tf.unpack(value=local3_extract_imgs,axis=0):\r\n      #print('raw img is', img)\r\n      img = tf.reshape(img, [6, 6, 3*3*64, 1])\r\n      local = tf.reshape(tf.batch_matmul(kernel, img),[6,6,64])\r\n      #print( 'local is', local)\r\n      local3_list.append(local)\r\n\r\n    local3 = tf.nn.relu(tf.nn.bias_add(tf.pack(local3_list), biases), name=scope.name)\r\n    print('local3 is', local3)\r\n    _activation_summary(local3)\r\n\r\n  # local4\r\n  with tf.variable_scope('local4') as scope:\r\n    local4_extract_imgs = tf.extract_image_patches(local3,[1,3,3,1],[1,1,1,1],[1,1,1,1],'SAME')\r\n    #local4_reshape = tf.reshape(local4_extract_imgs, [FLAGS.batch_size, 6, 6, 3*3*64, 1])\r\n    kernel = _variable_with_weight_decay( 'weights', shape=[6,6,32,3*3*64], stddev=0.04, wd=0.004) \r\n    biases = _variable_on_cpu('biases', [32], tf.constant_initializer(0.1))\r\n    local4_list = []\r\n    for img in tf.unpack(value=local4_extract_imgs, axis=0):\r\n      img = tf.reshape(img, [6, 6, 3*3*64, 1])\r\n      local = tf.reshape(tf.batch_matmul(kernel, img), shape=[6,6,32])\r\n      local4_list.append(local)\r\n\r\n    local4 = tf.nn.relu(tf.nn.bias_add(tf.pack(local4_list), biases), name=scope.name)\r\n    print('local4 is', local4)\r\n    _activation_summary(local4)\r\n\r\n  # softmax, i.e. softmax(WX + b)\r\n  with tf.variable_scope('softmax_linear') as scope:\r\n    reshape = tf.reshape(tensor=local4, shape=[FLAGS.batch_size,-1])\r\n\r\n    weights = _variable_with_weight_decay('weights', [6*6*32, NUM_CLASSES],\r\n                                          stddev=1/192.0, wd=0.0)\r\n    biases = _variable_on_cpu('biases', [NUM_CLASSES],\r\n                              tf.constant_initializer(0.0))\r\n    softmax_linear = tf.nn.softmax(tf.add(tf.matmul(reshape, weights), biases, name=scope.name))\r\n    _activation_summary(softmax_linear)\r\n\r\nreturn softmax_linear\r\n```\r\n\r\nI have tried my best, and I think my local layer implementation is  theorectically correct, so I have no idea where is wrong on my code. Who can give me some advices?", "comments": ["Please use StackOverflow for requests for advice.  Github issues are for bug reports and feature requests.\n"]}, {"number": 5583, "title": "error duing running transflow model", "body": "I am following[ this](https://medium.com/jim-fleming/loading-a-tensorflow-graph-with-the-c-api-4caaff88463f#.azr38581z) tutorial:  But as i try to run the loader.cc file after succesfully building it, i get this error: Not found: models/train.pb\r\nI have already copyed the train.pb file into /transflow/transflow/model folder and i also tried to copy this file inside /transflow/bazel-bin/model but it is locked. So how do i remove this error. even after i unlocked the bazel folder and than copied the train.pb file inside models, it shows the same error. ", "comments": ["If you have to unlock folders, you are presumably putting files in the wrong place.  How are you running loader?\n", "Ok i have solved this proble. sorry that i didn't closed this question. \n"]}, {"number": 5582, "title": "error duing running transflow model:  ", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": []}, {"number": 5581, "title": "Fix CMake config if used as a subproject", "body": "With this fix we don't assume that the tensorflow project is the build\r\nroot. Instead `CMAKE_CURRENT_SOURCE_DIR`/`CMAKE_CURRENT_BINARY_DIR` is used\r\ninstead of just `CMAKE_SOURCE_DIR`/`CMAKE_BINARY_DIR`, which now builds\r\nsuccessfully, if it is built inside another, larger project.", "comments": ["Can one of the admins verify this patch?\n", "@NewProggie, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ageron, @lilac and @mrry to be potential reviewers.\n", "@tensorflow-jenkins test this please.\n", "Seems that the one failing test has nothing to do with my PR?\n", "Yes, the test failure looks unrelated. I'll merge the PR now.\n", "Can one of the admins verify this patch?\n"]}, {"number": 5580, "title": "TensorFlow r0.11 C impl protobuf package incompatible with Python 3.5", "body": "When building from source (master branch as r0.11 does not build; see https://github.com/tensorflow/tensorflow/issues/5578) bazel builds a protobuf python implementation.\r\n\r\n```\r\n$ python -c \"from google.protobuf.internal import api_implementation; print(api_implementation._default_implementation_type)\"\r\npython\r\n```\r\n\r\nThe C implementation referenced in the documentation is incompatible with Python 3.5 in Ubuntu 16.04:\r\n\r\n```\r\n$ pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/protobuf-3.0.0-cp3-none-linux_x86_64.whl\r\nprotobuf-3.0.0-cp3-none-linux_x86_64.whl is not a supported wheel on this platform.\r\n```\r\n\r\nRegression?? https://github.com/tensorflow/tensorflow/issues/3856", "comments": ["@martinwicke Is this a regression?\n", "It's quite possible that renaming this file to `protobuf-3.0.0-cp3-none-any.whl` will fix this. @yifeif did you make the new files? Maybe we should put those through `audit-wheel`?\n", "`protobuf-3.0.0-cp3-none-any.whl` is supported according to `pep425` tags for `pip  8.1.2`, so renaming hack should work :)\nLooks like we might need to rebuild these packages if we want to pass them through `auditwheel`, as `cp3-none-linux_x86_64` is still not a supported tag. \n", "@yifeif with our regular protobuf builds, is this now resolved?", "Sorry this hasn't bump up the piles as it can be simply resolved by renaming the downloaded package tags to \"cp3-none-any\".\r\n", "Actually just took a look at the downloading link on [install page ](https://www.tensorflow.org/get_started/os_setup#protobuf_library_related_issues), the binary is now named \"protobuf-3.0.0-**cp35-cp35m**-linux_x86_64.whl\", which should be a supported wheel name. Closing this issue. @hholst80 please feel free to re-open if it is still not working for you. Thanks!"]}, {"number": 5579, "title": "Minor corrections to \"Adding a New Op\" docs", "body": "", "comments": ["@dtrebbien, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @vrv and @tensorflower-gardener to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please.\n"]}, {"number": 5578, "title": "Compile TensorFlow r0.11 from source", "body": "1. setup build environment\r\n```\r\n$ cat Dockerfile\r\nFROM ubuntu:16.04\r\n\r\n# Bazel.\r\nRUN apt-get update && apt-get install -y --no-install-recommends curl ca-certificates\r\nRUN echo \"deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8\" | tee /etc/apt/sources.list.d/bazel.list\r\nRUN curl https://bazel.build/bazel-release.pub.gpg | apt-key add -\r\nRUN apt-get update && apt-get install -y --no-install-recommends bazel\r\n\r\n# TensorFlow src & dev deps.\r\nRUN apt-get install -y --no-install-recommends git python3-numpy swig python3-dev python3-wheel python3-setuptools\r\nRUN ln -s /usr/bin/python3 /usr/local/bin/python\r\nRUN git clone --recurse-submodules https://github.com/tensorflow/tensorflow.git /opt/tensorflow\r\nWORKDIR /opt/tensorflow\r\n$ docker build -t bazel .\r\n```\r\n\r\n2. start and attach to build environment:\r\n```\r\n$ docker run -it --name bazel-build bazel\r\nroot@c7027f1dec32:/opt/tensorflow# git checkout r0.11\r\nSwitched to branch 'r0.11'\r\nYour branch is up-to-date with 'origin/r0.11'.\r\nroot@c7027f1dec32:/opt/tensorflow# ./configure \r\n/opt/tensorflow /opt/tensorflow\r\nPlease specify the location of python. [Default is /usr/local/bin/python]: \r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] \r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] \r\nNo Hadoop File System support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/local/lib/python3.5/dist-packages\r\n  /usr/lib/python3/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python3.5/dist-packages]\r\n\r\n/usr/local/lib/python3.5/dist-packages\r\nDo you wish to build TensorFlow with GPU support? [y/N] \r\nNo GPU support will be enabled for TensorFlow\r\nConfiguration finished\r\nExtracting Bazel installation...\r\n.............\r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\n.............\r\nWARNING: /root/.cache/bazel/_bazel_root/fbc06f9baef46cade6e35d9e4137e37c/external/protobuf/protobuf.bzl:90:19: Variables HOST_CFG and DATA_CFG are deprecated in favor of strings \"host\" and \"data\" correspondingly.\r\nWARNING: /root/.cache/bazel/_bazel_root/fbc06f9baef46cade6e35d9e4137e37c/external/protobuf/protobuf.bzl:96:28: Variables HOST_CFG and DATA_CFG are deprecated in favor of strings \"host\" and \"data\" correspondingly.\r\nINFO: All external dependencies fetched successfully.\r\nroot@c7027f1dec32:/opt/tensorflow# \r\n```\r\n\r\n3. Now try to build:\r\n```\r\nroot@c7027f1dec32:/opt/tensorflow# bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\r\nWARNING: /root/.cache/bazel/_bazel_root/fbc06f9baef46cade6e35d9e4137e37c/external/protobuf/protobuf.bzl:90:19: Variables HOST_CFG and DATA_CFG are deprecated in favor of strings \"host\" and \"data\" correspondingly.\r\nWARNING: /root/.cache/bazel/_bazel_root/fbc06f9baef46cade6e35d9e4137e37c/external/protobuf/protobuf.bzl:96:28: Variables HOST_CFG and DATA_CFG are deprecated in favor of strings \"host\" and \"data\" correspondingly.\r\nERROR: /opt/tensorflow/tensorflow/python/BUILD:1728:1: in cc_library rule //tensorflow/python:tf_session_helper: non-test target '//tensorflow/python:tf_session_helper' depends on testonly target '//tensorflow/python:construction_fails_op' and doesn't have testonly attribute set.\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\r\nINFO: Elapsed time: 1.643s\r\nroot@c7027f1dec32:/opt/tensorflow# \r\n```\r\n", "comments": ["master branch builds just fine, but the output file is named `rc2`?\n\n```\nroot@c7027f1dec32:/opt/tensorflow# ls /tmp/tensorflow_pkg/\ntensorflow-0.11.0rc2-cp35-cp35m-linux_x86_64.whl\nroot@c7027f1dec32:/opt/tensorflow# \n```\n", "@martinwicke Possible 0.11 build issue.\n", "This is caused by bazel 0.4.\nbazel 0.4 started to enforce the checks forbidding the inclusion of testonly dependencies in non-test libraries. Therefore the build fails.\nIf you install bazel 0.3.2 by downloading the deb package and running dpkg -i install, your build should succeed.\n", "I had this problem too. #5143 mentions a workaround.\n", "Just for completeness, the updated `Dockerfile` for building TensorFlow 0.11 from source. This works for me (only for CPU compiled TF)\r\n\r\n```\r\nFROM ubuntu:16.04\r\n\r\nRUN apt-get update && apt-get install -y --no-install-recommends \\\r\n    build-essential \\\r\n    openjdk-8-jdk \\\r\n    pkg-config \\\r\n    zip \\\r\n    unzip \\\r\n    zlib1g-dev \\\r\n    bash-completion \\\r\n    git \\\r\n    wget \\\r\n    python && \\\r\n    apt-get clean\r\n\r\nENV BAZEL_VERSION 0.3.2\r\nRUN wget \"https://github.com/bazelbuild/bazel/releases/download/${BAZEL_VERSION}/bazel_${BAZEL_VERSION}-linux-x86_64.deb\" && \\\r\n    dpkg -i \"bazel_${BAZEL_VERSION}-linux-x86_64.deb\" && \\\r\n    rm \"bazel_${BAZEL_VERSION}-linux-x86_64.deb\"\r\n\r\n# Install TensorFlow\r\n\r\nRUN apt-get install -y --no-install-recommends git python3-numpy swig python3-dev python3-wheel && apt-get clean\r\nRUN ln -s /usr/bin/python3 /usr/local/bin/python\r\nRUN git clone --quiet --recurse-submodules https://github.com/tensorflow/tensorflow.git /tensorflow\r\nRUN apt-get install -y --no-install-recommends python3-setuptools rsync && apt-get clean\r\nWORKDIR /tensorflow\r\n``` "]}, {"number": 5577, "title": "Possible Bug - tf.nn.moments differing outputs", "body": "Hello,\r\n\r\nI am observing differing outputs from tf.nn.moments when I run it multiple times on the same data, using GPU.  This makes it difficult for me to replicate my learning process!  See the logs at the bottom for what I mean.  I would expect the difference between moments across runs on the same data to be zero, but that is not the case in the logs I attached below (and most of the times I run it).  \r\n\r\nIf I run on CPU it is fine though.  Also, if I compute moments over smaller sets of data the problems seems to occur less often.\r\n\r\nNot sure what would be causing this, my guess would be some sort of numerical instability?  Help would be appreciated!\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nNone found, I discovered this issue when trying to get replicable results using batch_norm in contrib.\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04.5 LTS (running in a [singularity](http://singularity.lbl.gov) container on a CentOS 6.7 host). \r\n\r\nInstalled version of CUDA and cuDNN: \r\nI am using CUDA 8.0 with NVIDIA driver 367.48, and cuDNN v5.1 . \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n```\r\nlibOpenCL.so\r\nlibOpenCL.so.1\r\nlibOpenCL.so.1.0\r\nlibOpenCL.so.1.0.0\r\nlibcublas.so\r\nlibcublas.so.8.0\r\nlibcublas.so.8.0.45\r\nlibcublas_device.a\r\nlibcublas_static.a\r\nlibcudadevrt.a\r\nlibcudart.so\r\nlibcudart.so.8.0\r\nlibcudart.so.8.0.44\r\nlibcudart_static.a\r\nlibcudnn.so\r\nlibcudnn.so.5\r\nlibcudnn.so.5.1.5\r\nlibcudnn_static.a\r\nlibcufft.so\r\nlibcufft.so.8.0\r\nlibcufft.so.8.0.44\r\nlibcufft_static.a\r\nlibcufftw.so\r\nlibcufftw.so.8.0\r\nlibcufftw.so.8.0.44\r\nlibcufftw_static.a\r\nlibcuinj64.so\r\nlibcuinj64.so.8.0\r\nlibcuinj64.so.8.0.44\r\nlibculibos.a\r\nlibcurand.so\r\nlibcurand.so.8.0\r\nlibcurand.so.8.0.44\r\nlibcurand_static.a\r\nlibcusolver.so\r\nlibcusolver.so.8.0\r\nlibcusolver.so.8.0.44\r\nlibcusolver_static.a\r\nlibcusparse.so\r\nlibcusparse.so.8.0\r\nlibcusparse.so.8.0.44\r\nlibcusparse_static.a\r\nlibnppc.so\r\nlibnppc.so.8.0\r\nlibnppc.so.8.0.44\r\nlibnppc_static.a\r\nlibnppi.so\r\nlibnppi.so.8.0\r\nlibnppi.so.8.0.44\r\nlibnppi_static.a\r\nlibnppial.so\r\nlibnppial.so.8.0\r\nlibnppial.so.8.0.44\r\nlibnppicc.so\r\nlibnppicc.so.8.0\r\nlibnppicc.so.8.0.44\r\nlibnppicom.so\r\nlibnppicom.so.8.0\r\nlibnppicom.so.8.0.44\r\nlibnppidei.so\r\nlibnppidei.so.8.0\r\nlibnppidei.so.8.0.44\r\nlibnppif.so\r\nlibnppif.so.8.0\r\nlibnppif.so.8.0.44\r\nlibnppig.so\r\nlibnppig.so.8.0\r\nlibnppig.so.8.0.44\r\nlibnppim.so\r\nlibnppim.so.8.0\r\nlibnppim.so.8.0.44\r\nlibnppist.so\r\nlibnppist.so.8.0\r\nlibnppist.so.8.0.44\r\nlibnppisu.so\r\nlibnppisu.so.8.0\r\nlibnppisu.so.8.0.44\r\nlibnppitc.so\r\nlibnppitc.so.8.0\r\nlibnppitc.so.8.0.44\r\nlibnpps.so\r\nlibnpps.so.8.0\r\nlibnpps.so.8.0.44\r\nlibnpps_static.a\r\nlibnvToolsExt.so\r\nlibnvToolsExt.so.1\r\nlibnvToolsExt.so.1.0.0\r\nlibnvblas.so\r\nlibnvblas.so.8.0\r\nlibnvblas.so.8.0.44\r\nlibnvgraph.so\r\nlibnvgraph.so.8.0\r\nlibnvgraph.so.8.0.44\r\nlibnvgraph_static.a\r\nlibnvrtc-builtins.so\r\nlibnvrtc-builtins.so.8.0\r\nlibnvrtc-builtins.so.8.0.44\r\nlibnvrtc.so\r\nlibnvrtc.so.8.0\r\nlibnvrtc.so.8.0.44\r\nstubs\r\n```\r\n\r\nI installed tensorflow using the 0.11.0rc2-gpu tag on docker hub.\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nbatch_size = 2**10\r\nchannel_size = 100\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    with tf.device('/gpu:0'):\r\n        x = tf.placeholder(tf.float32, shape=[None, channel_size],\r\n                           name='input')\r\n        mom = tf.nn.moments(x, [0])\r\n\r\n    feed_dict = {}\r\n    feed_dict['input:0'] = np.random.rand(batch_size, channel_size)\r\n\r\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\r\n        tf.initialize_all_variables().run()\r\n        [mom_out1] = sess.run([mom], feed_dict=feed_dict)\r\n        [mom_out2] = sess.run([mom], feed_dict=feed_dict)\r\n    print \"DIFFERENCE:\", mom_out1[0][0] - mom_out2[0][0]\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nNo other solutions tried.\r\n\r\n\r\n### Logs or other output that would be helpful\r\n\r\nOutput of running my the code once:\r\n```\r\nWARNING: Not mounting current directory: user bind control is disabled by system administrator\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: \r\nname: GeForce GTX TITAN X\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\r\npciBusID 0000:09:00.0\r\nTotal memory: 11.92GiB\r\nFree memory: 11.81GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:09:00.0)\r\nDIFFERENCE: -5.96046e-08\r\n```", "comments": ["TensorFlow GPU reductions are nondeterministic, so this is (for better or worse) expected behavior.  This is because the GPU reductions use asynchronous atomic adds.\n"]}, {"number": 5576, "title": "win10 pthread.lib can not be found", "body": "errot log:\r\n-- Looking for pthread.h\r\n-- Looking for pthread.h - found\r\n-- Looking for pthread_create\r\n-- Looking for pthread_create - not found\r\n-- Looking for pthread_create in pthreads\r\n-- Looking for pthread_create in pthreads - found\r\n-- Found Threads: TRUE\r\nhow can i fix this problem, i have solved the .h issue ,but the lib issue can be solved in vs by prgma ,but for cmake , i have put prgma in every file related to pthread.h", "comments": ["this is the only error happened during cmake, is it important for the build or i can just igonre this issue . \n", "by ignore this issue ,i msbuild the project ,but stuck in protobuf git and jsoncpp git ,is it related to the pthread issue ,i have successfully installed protobuf and jsoncpp testing by vs.\n", "i think this can be easily solved by adding prgma ,but the problem is where i should add it.\n", "This should be a benign error: TensorFlow on Windows does not require pthreads to build or run\u2014instead it uses C++11 threads, which are supported by the Windows C runtime library.\n\nIt should be possible to eliminate this error by wrapping the line `find_package (Threads)` in CMakeLists.txt in an `if(UNIX)` block.\n", "@mrry Thanks a lot ,remove the fine_package() is useful for me to remove the error during cmake ,but I have to msbuild it to see whether it works or not .Thanks again\n"]}, {"number": 5575, "title": "Seq2Seq model decode error problem \"Not found: Tensor name\"", "body": "I trained with tutorial neural translation model with my toy dataset .\r\n(python translate.py --data_dir data/ --train_dir data/train/ --size=64 --num_layers=2 --steps_per_checkpoint=50)\r\n\r\nBut when i type decode command (python translate.py --decode --data_dir data/ --train_dir data/train/)\r\n\r\nI got error like this. I have attached error log.\r\n\r\nPlease help me TT\r\n\r\nReading model parameters from data/train/translate.ckpt-236700\r\nW tensorflow/core/framework/op_kernel.cc:968] Not found: Tensor name \"embedding_attention_seq2seq/RNN/MultiRNNCell/Cell2/GRUCell/Gates/Linear/Matrix\" not found in checkpoint files data/train/translate.ckpt-236700\r\n         [[Node: save/restore_slice_14 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/restore_slice_14/tensor_name, save/restore_slice_14/shape_and_slice)]]\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttps://github.com/tensorflow/tensorflow/issues/3776\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04.3 LTS\r\n\r\nInstalled version of CUDA and cuDNN: \r\n\r\n-rw-r--r-- 1 root root  189170  7\uc6d4 29 00:33 /usr/local/cuda-7.5/lib/libcudadevrt.a\r\nlrwxrwxrwx 1 root root      16  7\uc6d4 29 00:33 /usr/local/cuda-7.5/lib/libcudart.so -> libcudart.so.7.5\r\nlrwxrwxrwx 1 root root      19  7\uc6d4 29 00:33 /usr/local/cuda-7.5/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\r\n-rwxr-xr-x 1 root root  311596  7\uc6d4 29 00:33 /usr/local/cuda-7.5/lib/libcudart.so.7.5.18\r\n-rw-r--r-- 1 root root  558020  7\uc6d4 29 00:33 /usr/local/cuda-7.5/lib/libcudart_static.a\r\nlrwxrwxrwx 1 root root      17  7\uc6d4 29 00:33 /usr/local/cuda-7.5/lib/libcuinj32.so -> libcuinj32.so.7.5\r\nlrwxrwxrwx 1 root root      20  7\uc6d4 29 00:33 /usr/local/cuda-7.5/lib/libcuinj32.so.7.5 -> libcuinj32.so.7.5.18\r\n-rwxr-xr-x 1 root root 5396088  7\uc6d4 29 00:33 /usr/local/cuda-7.5/lib/libcuinj32.so.7.5.18\r\n\r\n\r\nIf installed from binary pip package, provide:\r\n\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\r\n0.11.0rc0\r\n\r\n\r\nIf installed from source, provide \r\n\r\n### What other attempted solutions have you tried?\r\n- install latest version of tensorflow==0.11.0rc0\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n[error_log.txt](https://github.com/tensorflow/tensorflow/files/587629/error_log.txt)\r\n\r\n", "comments": ["I find that even if i train 2layer network. model wants to find third layer parameters.\n(\"embedding_attention_seq2seq/RNN/MultiRNNCell/Cell2/~\")\n\nbut i have only \n\"embedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/~\"\n\"embedding_attention_seq2seq/RNN/MultiRNNCell/Cell1/~\"\n", "Sorry, my default option was wrong\n\nmy default number of layer was 3\n\nThank you :)..\n"]}]