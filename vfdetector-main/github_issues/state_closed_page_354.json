[{"number": 43452, "title": "failed precondition: error while reading resource variable block8_sepconv3_bn/gamma", "body": "Hello,\r\n\r\nI have found very few documentation concerning the error : failed precondition: error while reading resource variable block8_sepconv3_bn/gamma from Container: localhost. This could mean that the variable was uninitialized. Not found : Resource localhost/block8_speconv3_bn/gamma/class tensorflow::Var does not exist\r\n\r\nI guess the problem must be in the train function before the model.predict and has something to do with the sessions and threads but I would be happy to get any help I can get.\r\n\r\nI use : \r\nPython 3.7.8\r\nCuda 10.1\r\nCUDNN v7.6.5\r\nTensorflow 2.3.0\r\nkeras 2.4.3\r\n\r\n\r\nYou can find my code in attachment or here : https://github.com/8rax/Carla/blob/master/JDO_Tutorial_5.py\r\n[Carla_Env_DQN.zip](https://github.com/tensorflow/tensorflow/files/5259658/Carla_Env_DQN.zip)\r\n\r\n", "comments": ["Can you check https://github.com/tensorflow/tensorflow/issues/28287#issuecomment-495168033", "Hello, thank you very much for the suggestion. I indeed had a look at that thread as well and tried the solution. But I might have done it wrong. My understanding about sessions, graphs and threads might be a little bit lackluster as well, apologies for that.\r\n\r\nI created on line 70 my gpu_options and my sess at the begining of my script after the regular imports: \r\n\r\n\r\n```python\r\ngpu_options=tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=MEMORY_FRACTION) #TENSORFLOW DON'T USE ALL MY GPU PLZ\r\nsess=set_session(tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options)))\r\n```\r\n\r\nI understand I should also declare here the graph globally instead of declaring it in the __init__ function of my DQNAgent and should therefore also put : \r\n\r\n```python\r\ngraph = tf.compat.v1.get_default_graph()\r\n```\r\n\r\nthen, if my understanding of the problem is correct, I should then set the session every time I am calling the .predict method in the train function of my DQNAgent class. \r\n\r\nI suppose that I should then use the following syntax: \r\n\r\n```python\r\nglobal sess\r\nglobal graph\r\nwith self.graph.as_default():\r\n  set_session(sess)\r\n  current_qs_list = self.model.predict(current_states, PREDICTION_BATCH_SIZE)\r\n  new_current_states = np.array([transition[3] for transition in minibatch])/255 \r\n```\r\n\r\n```python\r\nglobal sess\r\nglobal graph\r\nwith self.graph.as_default():\r\n  set_session(sess)\r\n  future_qs_list = self.target_model.predict(new_current_states, PREDICTION_BATCH_SIZE)\r\n```\r\n\r\nNot sure I should add it here though as there is no graph involved : \r\n\r\n```python\r\ndef get_qs(self, state):\r\n        return self.model.predict(np.array(state).reshape(-1, * state.shape)/255)[0]\r\n```\r\nwould that make sense?\r\n", "I think that it is not a bug so you can close this. \r\nBut if you need a specific support on your use case I suggest you to use our channel at:\r\nhttps://stackoverflow.com/questions/tagged/tensorflow", "Hi,\r\nAlright thank you for the suggestion, posted it there : https://stackoverflow.com/q/64004994/14319427", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43452\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43452\">No</a>\n"]}, {"number": 43451, "title": "how to save weights without optimizer weights by using checkpoint file format in tensorflow keras model?", "body": "", "comments": ["@xiongma \r\nLet us know if this helps: [link](https://www.tensorflow.org/api_docs/python/tf/keras/models/save_model#expandable-1), you may use include_optimizer=False.\r\nAlso please refer to [link](https://stackoverflow.com/questions/49503748/save-and-load-model-optimizer-state) as well.\r\n", "thx your quick answer.\r\n1. in your first answer, it saves the model, I just want to save the model weights. I want to save the weights just like this.\r\n```\r\nmodel.save_weights()\r\n```\r\n2. in your second answer, I don't think this related to this topic, maybe is my misunderstanding. thx very much\r\ndo u have any idea about this?", "@xiongma \r\nModel.save.weights will save only the model weights and model.save saves the architecture,weights and configs, please feel free to move this to closed status if it answers the question.", "I mean, is there a way to use `model.save_weights` that it won't save optimizer weights?", "@xiongma \r\nModel.save.weights will save only the model weights, the optimizer details (state) gets saved as configuration as explained above with \"model.save\".\r\nPlease refer to [this link](https://www.tensorflow.org/guide/checkpoint#loading_mechanics) as well"]}, {"number": 43450, "title": "Feature request: Test in cl_build for the esp32", "body": "Please make a test for the esp32 in the folder \r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/tools/ci_build\r\n\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/5605614/93850427-ec51cd80-fc62-11ea-93cf-850023ba26ad.png)\r\n\r\n\r\nMany other builds have tests and the esp32 has a makefile here https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/tools/make/targets\r\n", "comments": ["Thanks for the request! It would be great to have this support for continuous integration builds, so we can make sure that changes don't break the ESP32 version of the library. Unfortunately we don't have anybody internal working on this yet, but I will also ping Espressif to see if this is something they can help with.", "\r\n\r\nI am able to compile for esp32 the library as long as I delete both person_detection examples. They are causing an error in the build with a camera include file that is missing. This is what is working for me so far:\r\n\r\nFrom the main tensorflow folder\r\n\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile clean\r\n\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp TAGS=\"cmsis-nn\" generate_arduino_zip\r\n\r\n```\r\n\r\nIt builds and generates the zip files for each example and the main library. Unfortunately it does not seem to include relevant esp files. So presently will not compile. Still looking into it.\r\n\r\n\r\nP.S. If I can find out how the esp32  reports errors (My board is the NodeMCU-32s). I might be able to just use the Arduino_TensorflowLite library. Not having much luck with debug_log.cc. Any suggestions @petewarden ?\r\n", "@hpssjellis @petewarden I am planning to bring CI support soon. \r\n\r\nMaybe in couple of weeks. ", "That would be great @vikramdattu .  I am really good at finding mistakes so if you want anything tested touch base. I sent you a DM on twitter.", "Example build test is added here: https://github.com/tensorflow/tensorflow/pull/44588", "Closing this issue since the feature has been added with the PR tagged above. \r\nFeel free to reopen if required. Thanks!"]}, {"number": 43449, "title": "tf.autodiff.ForwardAccumulator fails for Embedding layer", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.4.0-dev20200813\r\n- Python version: 3.8.3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1.168/7.6.5\r\n- GPU model and memory:  GeForce GTX 1050/4GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nCalculating the Jacobian-vector product of an embedding layer produces\r\n`AttributeError: 'IndexedSlices' object has no attribute '_as_tf_output'`\r\n\r\n**Describe the expected behavior**\r\nNo error, just like Dense, LSTM, convolutional layers. See a notebook link below that shows this error is ONLY related to Embedding layer.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nclass RNN_Model(tf.keras.Model):\r\n    def __init__(self):\r\n        super(RNN_Model, self).__init__()\r\n        self.embed=tf.keras.layers.Embedding(5,1)\r\n        self.d2 = tf.keras.layers.Dense(2)\r\n        self(tf.constant([4,3,2])) # initialize\r\n    \r\n    @tf.function\r\n    def call(self, x):\r\n        x = self.embed(x)\r\n        return self.d2(x)\r\n     \r\nmodel=RNN_Model()\r\n\r\nv=[tf.ones(w.shape) for w in model.trainable_variables]\r\nwith tf.autodiff.ForwardAccumulator(primals = model.trainable_variables, tangents = v) as acc:\r\n    loss = tf.reduce_sum(tf.constant([1,0])-model(tf.constant([[2,2,2], [1,1,1]]), training=True))\r\nacc.jvp(loss)\r\n```\r\n\r\nSee a complete example in [this notebook](https://drive.google.com/file/d/10Cb1nxcovmBSNE5zJOvRhyu-lHYhux15/view?usp=sharing)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Have you tried to build the model  (`model.build`) after model creation `model=RNN_Model()`?", "Yes I did. Still got the same error. It has nothing to do with building but with embedding layer. Please see my newly added example notebook.", "I don't think it is only the embedding layer. Can you try to remove  `@tf.function` from the `call` in the embedding case?\r\n", "If I remove @tf.function from the call, there is some other error. Please take a look at the notebook hyper-link. It is only the embedding layer. Keeping @tf.function works fine for other layers.", "If I remove `@tf.function` from the call is working:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Dense, Embedding,Bidirectional,LSTM\r\n\r\nclass RNN_Model(tf.keras.Model):\r\n    def __init__(self, dataset):\r\n        super(RNN_Model, self).__init__()\r\n        self.embed=Embedding(maxfeature,64)\r\n        self.blstm = Bidirectional(LSTM(64))\r\n        self.d1 = Dense(64, activation='relu')\r\n        self.d2 = Dense(2)\r\n                \r\n    # Doing this to initialize model.trainable_variables\r\n        for text, labels in dataset:\r\n            self(text)\r\n            break\r\n\r\n    def call(self, x):\r\n        x = self.embed(x)\r\n        x = self.blstm(x)\r\n        x = self.d1(x)\r\n        x = self.d2(x)\r\n        return tf.nn.log_softmax(x)\r\n        \r\n## Training Settings\r\nbatch_size = 64\r\nmaxfeature=100\r\nSEQ_LEN=32\r\n\r\n## Load Problems\r\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=maxfeature)\r\nx_train=tf.keras.preprocessing.sequence.pad_sequences(x_train,maxlen=SEQ_LEN)\r\ntrain_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(5000).batch(batch_size)\r\n\r\nmodel=RNN_Model(train_ds)\r\n\r\nloss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\r\n\r\n# Compute JVP\r\ncard = train_ds.cardinality()\r\nstep=0\r\nfor images, labels in train_ds:\r\n    step+=1\r\n    print(\"\\r%d/%d\" % (step , card), end=\"\")\r\n    v=[tf.ones(w.shape) for w in model.trainable_variables]\r\n    with tf.autodiff.ForwardAccumulator(primals = model.trainable_variables, tangents = v) as acc:\r\n        loss = loss_obj(labels, model(images, training=True))\r\n    acc.jvp(loss)\r\n```", "/cc @alextp if could be interested in this tracing case.", "I am afraid on my end, it shows the following error with your code:\r\n\r\n> ---------------------------------------------------------------------------\r\n> StagingError                              Traceback (most recent call last)\r\n> <ipython-input-1-2ec93c3161d4> in <module>\r\n>      44     v=[tf.ones(w.shape) for w in model.trainable_variables]\r\n>      45     with tf.autodiff.ForwardAccumulator(primals = model.trainable_variables, tangents = v) as acc:\r\n> ---> 46         loss = loss_obj(labels, model(images, training=True))\r\n>      47     acc.jvp(loss)\r\n> \r\n> ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in __call__(self, *args, **kwargs)\r\n>     988 \r\n>     989         with ops.enable_auto_cast_variables(self._compute_dtype_object):\r\n> --> 990           outputs = call_fn(inputs, *args, **kwargs)\r\n>     991 \r\n>     992         if self._activity_regularizer:\r\n> \r\n> <ipython-input-1-2ec93c3161d4> in call(self, x)\r\n>      17     def call(self, x):\r\n>      18         x = self.embed(x)\r\n> ---> 19         x = self.blstm(x)\r\n>      20         x = self.d1(x)\r\n>      21         x = self.d2(x)\r\n> \r\n> ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\layers\\wrappers.py in __call__(self, inputs, initial_state, constants, **kwargs)\r\n>     528 \r\n>     529     if initial_state is None and constants is None:\r\n> --> 530       return super(Bidirectional, self).__call__(inputs, **kwargs)\r\n>     531 \r\n>     532     # Applies the same workaround as in `RNN.__call__`\r\n> \r\n> ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in __call__(self, *args, **kwargs)\r\n>     988 \r\n>     989         with ops.enable_auto_cast_variables(self._compute_dtype_object):\r\n> --> 990           outputs = call_fn(inputs, *args, **kwargs)\r\n>     991 \r\n>     992         if self._activity_regularizer:\r\n> \r\n> ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\layers\\wrappers.py in call(self, inputs, training, mask, initial_state, constants)\r\n>     641         forward_state, backward_state = None, None\r\n>     642 \r\n> --> 643       y = self.forward_layer(forward_inputs,\r\n>     644                              initial_state=forward_state, **kwargs)\r\n>     645       y_rev = self.backward_layer(backward_inputs,\r\n> \r\n> ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py in __call__(self, inputs, initial_state, constants, **kwargs)\r\n>     660 \r\n>     661     if initial_state is None and constants is None:\r\n> --> 662       return super(RNN, self).__call__(inputs, **kwargs)\r\n>     663 \r\n>     664     # If any of `initial_state` or `constants` are specified and are Keras\r\n> \r\n> ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in __call__(self, *args, **kwargs)\r\n>     988 \r\n>     989         with ops.enable_auto_cast_variables(self._compute_dtype_object):\r\n> --> 990           outputs = call_fn(inputs, *args, **kwargs)\r\n>     991 \r\n>     992         if self._activity_regularizer:\r\n> \r\n> ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent_v2.py in call(self, inputs, mask, training, initial_state)\r\n>    1269           # GPU implementation when GPU is available.\r\n>    1270           if can_use_gpu:\r\n> -> 1271             last_output, outputs, new_h, new_c, runtime = gpu_lstm(\r\n>    1272                 **gpu_lstm_kwargs)\r\n>    1273           else:\r\n> \r\n> ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent_v2.py in gpu_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask, time_major, go_backwards, sequence_lengths)\r\n>    1514       # Reverse axis 0 since the input is already convert to time major.\r\n>    1515       inputs = array_ops.reverse(inputs, axis=[0])\r\n> -> 1516     outputs, h, c, _ = gen_cudnn_rnn_ops.cudnn_rnn(\r\n>    1517         inputs, input_h=init_h, input_c=init_c, params=params, is_training=True,\r\n>    1518         rnn_mode='lstm')\r\n> \r\n> ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\gen_cudnn_rnn_ops.py in cudnn_rnn(input, input_h, input_c, params, rnn_mode, input_mode, direction, dropout, seed, seed2, is_training, name)\r\n>      97       pass\r\n>      98     try:\r\n> ---> 99       return cudnn_rnn_eager_fallback(\r\n>     100           input, input_h, input_c, params, rnn_mode=rnn_mode,\r\n>     101           input_mode=input_mode, direction=direction, dropout=dropout,\r\n> \r\n> ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\gen_cudnn_rnn_ops.py in cudnn_rnn_eager_fallback(input, input_h, input_c, params, rnn_mode, input_mode, direction, dropout, seed, seed2, is_training, name, ctx)\r\n>     180                              attrs=_attrs, ctx=ctx, name=name)\r\n>     181   if _execute.must_record_gradient():\r\n> --> 182     _execute.record_gradient(\r\n>     183         \"CudnnRNN\", _inputs_flat, _attrs, _result)\r\n>     184   _result = _CudnnRNNOutput._make(_result)\r\n> \r\n> ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\backprop.py in _record_gradient(op_name, inputs, attrs, results)\r\n>     173 \r\n>     174 def _record_gradient(op_name, inputs, attrs, results):\r\n> --> 175   return pywrap_tfe.TFE_Py_RecordGradient(op_name, inputs, attrs, results,\r\n>     176                                           ops.get_name_scope())\r\n>     177 \r\n> \r\n> ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\forwardprop.py in _jvp_dispatch(op_name, attr_tuple, inputs, outputs, tangents, use_batch)\r\n>     211   # means we may trace a few more exact shapes before moving on to relaxation.\r\n>     212   if _TRACE_COUNT.get(op_name, 0) < _TRACE_COUNT_LIMIT:\r\n> --> 213     return _jvp_exact_shapes(op_name, attr_tuple, inputs, outputs, tangents,\r\n>     214                              use_batch)\r\n>     215   return _jvp_relaxed_shapes(op_name, attr_tuple, inputs, outputs, tangents,\r\n> \r\n> ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n>    2937     with self._lock:\r\n>    2938       graph_function, flat_args, flat_kwargs = \\\r\n> -> 2939           self._maybe_define_function(args, kwargs)\r\n>    2940     return graph_function._filtered_call(flat_args, flat_kwargs)  # pylint: disable=protected-access\r\n>    2941 \r\n> \r\n> ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py in _maybe_define_function(self, args, kwargs)\r\n>    3342 \r\n>    3343       self._function_cache.missed.add(call_context_key)\r\n> -> 3344       graph_function = self._create_graph_function(args, kwargs)\r\n>    3345       self._function_cache.primary[cache_key] = graph_function\r\n>    3346 \r\n> \r\n> ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n>    3187     arg_names = base_arg_names + missing_arg_names\r\n>    3188     graph_function = ConcreteFunction(\r\n> -> 3189         func_graph_module.func_graph_from_py_func(\r\n>    3190             self._name,\r\n>    3191             self._python_function,\r\n> \r\n> ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n>     985         _, original_func = tf_decorator.unwrap(python_func)\r\n>     986 \r\n> --> 987       func_outputs = python_func(*func_args, **func_kwargs)\r\n>     988 \r\n>     989       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n> \r\n> ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py in wrapper(*args, **kwargs)\r\n>     972           except Exception as e:  # pylint:disable=broad-except\r\n>     973             if hasattr(e, \"ag_error_metadata\"):\r\n> --> 974               raise e.ag_error_metadata.to_exception(e)\r\n>     975             else:\r\n>     976               raise\r\n> \r\n> StagingError: in user code:\r\n> \r\n>     C:\\Users\\Fight4Life\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\forwardprop.py:179 _jvp_helper_wrapper  *\r\n>         return _jvp_helper(op_name, attr_tuple, inputs, outputs, tangents)\r\n>     C:\\Users\\Fight4Life\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\forwardprop.py:138 _jvp_helper  **\r\n>         nontrivial_output_tangents = transpose_tape.gradient(\r\n>     C:\\Users\\Fight4Life\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\backprop.py:1080 gradient\r\n>         flat_grad = imperative_grad.imperative_grad(\r\n>     C:\\Users\\Fight4Life\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py:71 imperative_grad\r\n>         return pywrap_tfe.TFE_Py_TapeGradient(\r\n>     C:\\Users\\Fight4Life\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\backprop.py:151 _gradient_function\r\n>         grad_fn = ops._gradient_registry.lookup(op_name)  # pylint: disable=protected-access\r\n>     C:\\Users\\Fight4Life\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\registry.py:98 lookup\r\n>         raise LookupError(\r\n> \r\n>     LookupError: gradient registry has no entry for: CudnnRNNBackprop", "I've just run your notebook on colab and it is running fine on CPU but we have the same error on the GPU runtime.", "The GPU one I think that it is another issue like https://github.com/tensorflow/tensorflow/issues/37091#issuecomment-612205749 /cc @kaixih ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43449\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43449\">No</a>\n", "Although it works on CPU now, it is still desirable to work on GPU.", "@alextp @allenlavoie Are you still interested for the tracing part of the issue?\r\n\r\n```\r\n@tf.function\r\ndef call(self, x):\r\n````", "Looks like the GPU issue has another bug (which is closed, but Scott would have more background there). It's just a missing RegisterGradient call for the gradient op.\r\n\r\n@bhack what tracing issue? The fix has a test with/without tf.function.", "@allenlavoie Yes I've retested the code example now and it is working also on GPU (at least on Colab). \r\nIt was just a problem about timezones with tf-nightly builds to catch that commit."]}, {"number": 43447, "title": "top_k crashes for certain large structures", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nyes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nwindows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nv2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version:\r\n3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n10.1 update 2\r\n- GPU model and memory:\r\nGTX 960m\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nThe following operation crashes (possible because of gpu l2 usage (?)\r\n```python\r\nimport tensorflow as tf\r\nx = tf.random.stateless_normal(shape=(1_000_000, 2), dtype=tf.float32, seed=(4,2))\r\nprint(tf.math.top_k(x, k=1)[0][:2])\r\n```\r\n\r\nCrashes on my machine. Interestingly using a shape of (2, 1_000_000) works fine\r\n\r\n**Describe the expected behavior**\r\nit shouldn't crash, and if there is a memory limitation it should be specified in the crash. this problem user that indirectly use top_k and not throwing the correct exception makes it very hard to debug (see https://github.com/tensorflow/probability/issues/1086)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n\r\n```\r\n2020-09-22 06:51:17.737213: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-22 06:51:19.945793: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-09-22 06:51:20.385293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 960M computeCapability: 5.0\r\ncoreClock: 1.176GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 74.65GiB/s\r\n2020-09-22 06:51:20.385745: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-22 06:51:20.390534: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-09-22 06:51:20.395306: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-09-22 06:51:20.396930: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-09-22 06:51:20.402414: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-09-22 06:51:20.405454: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-09-22 06:51:20.415167: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-09-22 06:51:20.416576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-09-22 06:51:20.417067: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-09-22 06:51:20.427041: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1a8f253a180 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-09-22 06:51:20.427523: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-09-22 06:51:20.429145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 960M computeCapability: 5.0\r\ncoreClock: 1.176GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 74.65GiB/s\r\n2020-09-22 06:51:20.429756: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-22 06:51:20.429991: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-09-22 06:51:20.430219: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-09-22 06:51:20.430449: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-09-22 06:51:20.430672: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-09-22 06:51:20.430902: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-09-22 06:51:20.431136: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-09-22 06:51:20.432293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-09-22 06:51:21.959567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-22 06:51:21.959917: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \r\n2020-09-22 06:51:21.960127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \r\n2020-09-22 06:51:21.961302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3031 MB memory) -> physical GPU (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n2020-09-22 06:51:21.965674: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1a89849b300 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-09-22 06:51:21.966109: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 960M, Compute Capability 5.0\r\n2020-09-22 06:51:25.072632: E tensorflow/stream_executor/cuda/cuda_driver.cc:939] could not synchronize on CUDA context: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure :: 0x00007FFBB77F8E85\ttensorflow::CurrentStackTrace\r\n0x00007FFBB752A4BE\ttensorflow::DeviceProperties::l2_cache_size\r\n0x00007FFBB7530BEE\tstream_executor::StreamExecutor::EnablePeerAccessTo\r\n0x00007FFBA431BF38\ttensorflow::StepStats::internal_default_instance\r\n0x00007FFBA432D064\tgoogle::protobuf::RepeatedPtrField<tensorflow::InterconnectLink>::Add\r\n0x00007FFBA4067712\tstd::vector<tensorflow::DtypeAndPartialTensorShape,std::allocator<tensorflow::DtypeAndPartialTensorShape> >::operator=\r\n0x00007FFB9EE4A4D1\ttensorflow::MemoryLogRawDeallocation::deferred\r\n0x00007FFB9EE54A01\tTFE_TensorHandleResolve\r\n0x00007FFB9EBBD6A3\tTFE_Py_TensorShapeSlice\r\n0x00007FFB9EBBADDA\tstd::vector<tensorflow::monitoring::Point::Label,std::allocator<tensorflow::monitoring::Point::Label> >::reserve\r\n0x00007FFC7010BDEF\tPyMethodDef_RawFastCallKeywords\r\n0x00007FFC7010B6E7\tPyArg_UnpackStack\r\n0x00007FFC70118484\tPyEval_EvalFrameDefault\r\n0x00007FFC7010B841\tPyArg_UnpackStack\r\n0x00007FFC70118484\tPyEval_EvalFrameDefault\r\n0x00007FFC7010C0FC\tPyEval_EvalCodeWithName\r\n0x00007FFC7010B95C\tPyArg_UnpackStack\r\n0x00007FFC70118A6E\tPyEval_EvalFrameDefault\r\n0x00007FFC7010B294\tPyFunction_FastCallDict\r\n0x00007FFC700F22D4\tPyObject_FastCall_Prepend\r\n0x00007FFC700F2225\tPySequence_GetItem\r\n0x00007FFC700FE78D\tPyObject_Str\r\n0x00007FFC7013807E\tPyFile_WriteObject\r\n0x00007FFC70137F5D\tPySys_EndInit\r\n0x00007FFC7010BE06\tPyMethodDef_RawFastCallKeywords\r\n0x00007FFC7010B8A3\tPyArg_UnpackStack\r\n0x00007FFC70118A6E\tPyEval_EvalFrameDefault\r\n0x00007FFC7010C0FC\tPyEval_EvalCodeWithName\r\n0x00007FFC70120197\tPyEval_EvalCodeEx\r\n0x00007FFC701200F5\tPyEval_EvalCode\r\n0x00007FFC7012009F\tPyArena_Free\r\n0x00007FFC70293E5D\tPyRun_FileExFlags\r\n0x00007FFC702945F9\tPyRun_SimpleFileExFlags\r\n0x00007FFC70293D3B\tPyRun_AnyFileExFlags\r\n0x00007FFC701E1054\tPy_UnixMain\r\n0x00007FFC701E10FB\tPy_UnixMain\r\n0x00007FFC7017936E\tPyErr_NoMemory\r\n0x00007FFC701309DB\tPy_Main\r\n0x00007FFC701309B6\tPy_Main\r\n0x00007FF7E889126D\t(unknown)\r\n0x00007FFCBAAA6FD4\tBaseThreadInitThunk\r\n0x00007FFCBB0DCEC1\tRtlUserThreadStart\r\n\r\nTraceback (most recent call last):\r\n  File \"<project>/bug4.py\", line 6, in <module>\r\n    print(tf.math.top_k(x, k=1)[0][:2])\r\n  File \"<env>\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 904, in __str__\r\n    return \"tf.Tensor(%s, shape=%s, dtype=%s)\" % (numpy_text(self), self.shape,\r\n  File \"<env>\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 264, in numpy_text\r\n    text = repr(tensor._numpy()) if is_repr else str(tensor._numpy())\r\n  File \"<env>\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 929, in _numpy\r\n    six.raise_from(core._status_to_exception(e.code, e.message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: GPU sync failed\r\n\r\nProcess finished with exit code 1\r\n```", "comments": ["@amitport \r\n\r\nI have tried in colab with TF version 2.3 and nightly version(2.4.0-dev20200921) and i am not seeing any issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/0000dda34bef8368b310e3690b40dce2/untitled382.ipynb).Please,verify once and close the issue. Thanks!", "@ravikyram\r\n\r\n* this is hardware related. Seems to have something to do with gpu's l2 cache size from the log\r\n(very much an issue even if not for the specific GPUs at colab)\r\n\r\n* Also, it is very important to note that this is an issue with tensorflow 2.2, not tensorflow-nightly. tensorflow 2.3 and nightly have many issues on windows that prevent upgrading and tensorflow 2.2 is supposed to be maintained afaik. (so when testing this on my machine with tf-nightly it doesn't work and there are many other known open issues that also get in the way)\r\n\r\nThanks again", "@amitport Is this still an issue for you. I ran it with `TF2.4.1` and don't see any issue. Please let us know whether the issue persists with newer TF versions. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/12161adc54523fb32f472702f6e27b7e/untitled382.ipynb). Thanks!", "I have other problems with 2.4 and 2.5 on my GTX 960M machine, I moved on the use the cloud more for now since this. Closing this, since I can't determine if it is solved or not. Thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43447\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43447\">No</a>\n"]}, {"number": 43446, "title": "Update version numbers for TensorFlow 2.3.1", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 3 -> 3\nPatch: 0 -> 1\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.3.0\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\nBinary file \ntensorflow/python/keras/mixed_precision/experimental/testdata/lso_savedmodel_tf2\n.2/saved_model.pb matches\ntensorflow/python/tpu/profiler/capture_tpu_profile.py:194:2.3.0\ntensorflow/lite/g3doc/performance/gpu.md:100:2.3.0\ntensorflow/lite/g3doc/performance/gpu.md:102:2.3.0\ntensorflow/tools/pip_package/setup.py:66:2.3.0\ntensorflow/tools/pip_package/setup.py:67:2.3.0\ntensorflow/tools/pip_package/setup.py:91:2.3.0\ntensorflow/tools/ci_build/builds/libtensorflow_nightly_symlink.sh:22:2.3.0\ntensorflow/tools/ci_build/builds/libtensorflow_nightly_symlink.sh:23:2.3.0\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.3.0\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\nBinary file \ntensorflow/python/keras/mixed_precision/experimental/testdata/lso_savedmodel_tf2\n.2/saved_model.pb matches\ntensorflow/python/tpu/profiler/capture_tpu_profile.py:194:2.3.0\ntensorflow/lite/g3doc/performance/gpu.md:100:2.3.0\ntensorflow/lite/g3doc/performance/gpu.md:102:2.3.0\ntensorflow/tools/pip_package/setup.py:66:2.3.0\ntensorflow/tools/pip_package/setup.py:67:2.3.0\ntensorflow/tools/pip_package/setup.py:91:2.3.0\ntensorflow/tools/ci_build/builds/libtensorflow_nightly_symlink.sh:22:2.3.0\ntensorflow/tools/ci_build/builds/libtensorflow_nightly_symlink.sh:23:2.3.0\n```", "comments": []}, {"number": 43445, "title": "Update version numbers for TensorFlow 2.2.1", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 2 -> 2\nPatch: 0 -> 1\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.2.0\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/tools/pip_package/setup.py:64:2.2.0\ntensorflow/tools/pip_package/setup.py:65:2.2.0\ntensorflow/tools/pip_package/setup.py:95:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:78:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:127:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:161:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:188:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:191:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:222:2.2.0\ntensorflow/lite/micro/tools/make/third_party_downloads.inc:26:2.2.0\ntensorflow/lite/micro/tools/make/third_party_downloads.inc:28:2.2.0\ntensorflow/lite/micro/examples/person_detection/README.md:275:2.2.0\ntensorflow/lite/micro/examples/person_detection/README.md:276:2.2.0\ntensorflow/lite/micro/examples/person_detection/README.md:282:2.2.0\ntensorflow/lite/micro/examples/person_detection/README.md:295:2.2.0\ntensorflow/lite/micro/examples/person_detection/README.md:331:2.2.0\ntensorflow/lite/micro/examples/hello_world/README.md:167:2.2.0\ntensorflow/lite/micro/examples/hello_world/README.md:168:2.2.0\ntensorflow/lite/micro/examples/hello_world/README.md:174:2.2.0\ntensorflow/lite/micro/examples/hello_world/README.md:187:2.2.0\ntensorflow/lite/micro/examples/hello_world/README.md:223:2.2.0\ntensorflow/lite/micro/examples/magic_wand/train/README.md:106:2.2.0\ntensorflow/lite/micro/examples/magic_wand/README.md:177:2.2.0\ntensorflow/lite/micro/examples/magic_wand/README.md:178:2.2.0\ntensorflow/lite/micro/examples/magic_wand/README.md:184:2.2.0\ntensorflow/lite/micro/examples/magic_wand/README.md:197:2.2.0\ntensorflow/lite/micro/examples/magic_wand/README.md:235:2.2.0\ntensorflow/lite/micro/examples/micro_speech/README.md:180:2.2.0\ntensorflow/lite/micro/examples/micro_speech/README.md:181:2.2.0\ntensorflow/lite/micro/examples/micro_speech/README.md:187:2.2.0\ntensorflow/lite/micro/examples/micro_speech/README.md:200:2.2.0\ntensorflow/lite/micro/examples/micro_speech/README.md:236:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:90:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:107:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:139:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:172:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:199:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:202:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:233:2.2.0\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.2.0\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/tools/pip_package/setup.py:64:2.2.0\ntensorflow/tools/pip_package/setup.py:65:2.2.0\ntensorflow/tools/pip_package/setup.py:95:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:78:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:127:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:161:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:188:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:191:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:222:2.2.0\ntensorflow/lite/micro/tools/make/third_party_downloads.inc:26:2.2.0\ntensorflow/lite/micro/tools/make/third_party_downloads.inc:28:2.2.0\ntensorflow/lite/micro/examples/person_detection/README.md:275:2.2.0\ntensorflow/lite/micro/examples/person_detection/README.md:276:2.2.0\ntensorflow/lite/micro/examples/person_detection/README.md:282:2.2.0\ntensorflow/lite/micro/examples/person_detection/README.md:295:2.2.0\ntensorflow/lite/micro/examples/person_detection/README.md:331:2.2.0\ntensorflow/lite/micro/examples/hello_world/README.md:167:2.2.0\ntensorflow/lite/micro/examples/hello_world/README.md:168:2.2.0\ntensorflow/lite/micro/examples/hello_world/README.md:174:2.2.0\ntensorflow/lite/micro/examples/hello_world/README.md:187:2.2.0\ntensorflow/lite/micro/examples/hello_world/README.md:223:2.2.0\ntensorflow/lite/micro/examples/magic_wand/train/README.md:106:2.2.0\ntensorflow/lite/micro/examples/magic_wand/README.md:177:2.2.0\ntensorflow/lite/micro/examples/magic_wand/README.md:178:2.2.0\ntensorflow/lite/micro/examples/magic_wand/README.md:184:2.2.0\ntensorflow/lite/micro/examples/magic_wand/README.md:197:2.2.0\ntensorflow/lite/micro/examples/magic_wand/README.md:235:2.2.0\ntensorflow/lite/micro/examples/micro_speech/README.md:180:2.2.0\ntensorflow/lite/micro/examples/micro_speech/README.md:181:2.2.0\ntensorflow/lite/micro/examples/micro_speech/README.md:187:2.2.0\ntensorflow/lite/micro/examples/micro_speech/README.md:200:2.2.0\ntensorflow/lite/micro/examples/micro_speech/README.md:236:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:90:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:107:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:139:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:172:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:199:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:202:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:233:2.2.0\n```", "comments": []}, {"number": 43444, "title": "Update version numbers for TensorFlow 2.1.2", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 1 -> 1\nPatch: 1 -> 2\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.1.1\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/compiler/xla/service/hlo_graph_dumper.cc:1457:2.1.1\ntensorflow/compiler/xla/service/hlo_graph_dumper.cc:1460:2.1.1\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.1.1\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/compiler/xla/service/hlo_graph_dumper.cc:1457:2.1.1\ntensorflow/compiler/xla/service/hlo_graph_dumper.cc:1460:2.1.1\n```", "comments": []}, {"number": 43443, "title": "Update version numbers for TensorFlow 2.0.3", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 0 -> 0\nPatch: 2 -> 3\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.0.2\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/python/tpu/profiler/capture_tpu_profile.py:93:2.0.2\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.0.2\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/python/tpu/profiler/capture_tpu_profile.py:93:2.0.2\n```", "comments": []}, {"number": 43442, "title": "Update version numbers for TensorFlow 1.15.4", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 1 -> 1\nMinor: 15 -> 15\nPatch: 3 -> 4\n\nNo lingering old version strings \"1.15.3\" found in source directory \n\"tensorflow/\". Good.\nNo lingering old version strings \"1.15.3\" found in source directory \n\"tensorflow/\". Good.\n```", "comments": []}, {"number": 43441, "title": "Update release notes for TensorFlow 2.3.1", "body": "This PR is intentionally incomplete. One of the Release Owners for 2.3.1\nneeds to fill in the internal release notes for this version before the PR gets\nsubmitted. Click on the :pencil2: icon in the header for `RELEASE.md` under\n\"Files Changed\" above.", "comments": []}, {"number": 43440, "title": "Update release notes for TensorFlow 2.2.1", "body": "This PR is intentionally incomplete. One of the Release Owners for 2.2.1\nneeds to fill in the internal release notes for this version before the PR gets\nsubmitted. Click on the :pencil2: icon in the header for `RELEASE.md` under\n\"Files Changed\" above.", "comments": []}, {"number": 43439, "title": "Update release notes for TensorFlow 2.1.2", "body": "This PR is intentionally incomplete. One of the Release Owners for 2.1.2\nneeds to fill in the internal release notes for this version before the PR gets\nsubmitted. Click on the :pencil2: icon in the header for `RELEASE.md` under\n\"Files Changed\" above.", "comments": []}, {"number": 43438, "title": "Update release notes for TensorFlow 2.0.3", "body": "This PR is intentionally incomplete. One of the Release Owners for 2.0.3\nneeds to fill in the internal release notes for this version before the PR gets\nsubmitted. Click on the :pencil2: icon in the header for `RELEASE.md` under\n\"Files Changed\" above.", "comments": []}, {"number": 43437, "title": "Update release notes for TensorFlow 1.15.4", "body": "This PR is intentionally incomplete. One of the Release Owners for 1.15.4\nneeds to fill in the internal release notes for this version before the PR gets\nsubmitted. Click on the :pencil2: icon in the header for `RELEASE.md` under\n\"Files Changed\" above.", "comments": []}, {"number": 43436, "title": "TF 2.3 concrete function spec issue: list index out of range", "body": "In TF 2.3, if we define a model in `class`, it errors out with following error when run predict:\r\n\r\n```\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _structured_signature_check_arg_types(self, args, kwargs)\r\n   1778     arg_specs, kwarg_specs = self.structured_input_signature\r\n   1779     for i, (arg, spec) in enumerate(zip(args, arg_specs)):\r\n-> 1780       name = self._function_spec.arg_names[i]\r\n   1781       self._structured_signature_check_arg_type(arg, spec, name)\r\n   1782     for (name, arg) in kwargs.items():\r\n\r\nIndexError: list index out of range\r\n```\r\n\r\nHere's a small code sample to reproduce this issue:\r\n\r\nhttps://colab.research.google.com/drive/1TukOd1vtYYtO5UrCYXl3VebAhcYZNYcc#scrollTo=uerkKqrPnTBg&uniqifier=1\r\n\r\nNote: this issue is **only in TF 2.3** if you change the TF version (first line) to `!pip install tensorflow==2.2`. This code snippet runs without any issue. \r\n", "comments": ["Was able to reproduce the issue. Works with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/f4499bbb57bf5c7af7507f3d1b7b9ff1/43436-2-2.ipynb).\r\n\r\nHowever, running the code with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/18e4a2cf7f935116c92c5dc67bf884bd/43436-2-3.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/fed519acd4c2cffe7d448f470088d657/43436-tf-nightly.ipynb) throws an error stating `IndexError: list index out of range`. Please find the attached gist. Thanks!", "@1duo I changed arguments of the class as shown below and it works as expected. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/1927d4020185c200a65aea20fbb4561e/43436-tf-nightly.ipynb). Thanks!\r\n\r\n```\r\nclass MyModel(tf.Module):\r\n    @tf.function(input_signature=[tf.TensorSpec(shape=(), dtype=tf.float32), tf.TensorSpec(shape=(), dtype=tf.float32)])\r\n    def add(self, x,y):\r\n        return tf.math.add(x,y)\r\n```", "Thanks for the reply! I'm fine with the workaround but ideally * args should work, isn't it? As its breaking the compatibility. Feel free to close it you think its not an issue. ", "@1duo Agree with you. We have updated the code and I cannot reproduce with `tf-nightly`. Please take a look at the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/45bfd6d184703023f6b54013317954e3/43436-tf-nightly.ipynb).\r\n\r\nThanks for creating this issue. I am closing this issue as this was resolved. Please feel free to reopen if the issue persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43436\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43436\">No</a>\n"]}, {"number": 43435, "title": "[INTEL MKL] MKL DNN 0.x code cleanup - Concat op", "body": "DNN 0.x cleanup of MklConcat Op:\r\n\r\n(1) Remove all DNN 0.x related code\r\n\r\n(2) Replace all DNN 1.x macro usages", "comments": []}, {"number": 43434, "title": "for #43320 Changed gen_lut params from std::function to function pointer", "body": "See https://github.com/tensorflow/tensorflow/pull/43320\r\nChanged gen_lut params from std::function to function pointer\r\n", "comments": ["Looks good to me! Just need to wait for @advaitjain 's review. Thanks again for providing the fix!"]}, {"number": 43433, "title": "nan is clipped to the upper", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.3\r\n- Python version: 3.8\r\n- CUDA/cuDNN version: 10.1/7.6\r\n- GPU model and memory: TITAN RTX\r\n\r\n**Describe the current behavior**\r\ntf.clip_by_value(tf.constant(np.nan), -1, 1) == 1\r\n\r\n**Describe the expected behavior**\r\ntf.clip_by_value(tf.constant(np.nan), -1, 1) == NaN", "comments": ["It is similar to the `Relu` case https://github.com/tensorflow/tensorflow/issues/42885#issuecomment-687151509.\r\nThis happens on GPU if you try on CPU it returns `nan`", "> It is similar to the `Relu` case [#42885 (comment)](https://github.com/tensorflow/tensorflow/issues/42885#issuecomment-687151509).\r\n> This happens on GPU if you try on CPU it returns `nan`\r\n\r\nPlease fix this on GPU.", "@PwnerHarry \r\nCould you please check on the latest tf version and let us know if this is still an issue.", "With nightly is \r\n```\r\n<tf.Tensor: shape=(), dtype=float32, numpy=nan>\r\n```\r\nso we could close this.", "@Saduf2019 I tested it on Windows with tf 2.4.1, its NaN (good).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43433\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43433\">No</a>\n", "@PwnerHarry \r\nThank you for the Update, glad it works fine."]}, {"number": 43432, "title": "Minor spelling tweaks", "body": "This PR addresses minor spelling tweaks in RELEASE.md", "comments": ["Also, cna you solve conflicts, please/", "@mihaimaruseac Sure, just resolved the conflict."]}, {"number": 43431, "title": "import error -> tensorflow", "body": "<em></em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 home edition \r\n- TensorFlow installed from (source or binary): Installed from conda using (pip install tensorflow) \r\n- TensorFlow version: 2.3.0\r\n- Python version: 3.8.3\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA \r\n- CUDA/cuDNN version: NA \r\n- GPU model and memory: 940mx \r\n\r\n\r\n\r\n**Describe the problem**\r\nTensorflow installed successfully but unable to import it using \"import tensorflow as tf\" \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nInstallation Commands: pip install tensorflow\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["We don't support anaconda directly. Please follow the official guide: https://www.tensorflow.org/install/pip\r\n", "@Blue-Squid \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Thanks!\r\n\r\n", "@ravikyram \r\nI am using a Kaby Lake - I5 8250u processor, (listed on the compatible processors as well) \r\nGPU is Nvidea 940mx. \r\n\r\n\r\nThe requirements that you have mentioned are completely met. (i,e C++ Redist and so on) \r\nI have used tensorflow on this machine before, I have reinstalled Windows and I am trying to set up same environment as before. \r\n", "@Blue-Squid \r\n\r\nSorry, but we don't provide support for issues with the conda environment.Please try using pip from [here]( https://www.tensorflow.org/install/pip).", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43431\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43431\">No</a>\n"]}, {"number": 43430, "title": "ValueError: Failed to convert value into readable tensor.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source): 1.15\r\n\r\n# Describe the problem\r\nI am trying to convert the model to TFLite format using representative dataset. But I got the error : ValueError: Failed to convert value into readable tensor.\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\ndef rep_data_gen():\r\n    f = '/home/pychen/traffic_3class/creDa_test_public/ImageSets/test_less.txt'\r\n    f_name = np.loadtxt(f, dtype = np.str).reshape(-1)\r\n    # print('Here', type(f_name))\r\n    # print(f_name)\r\n    NORM_H = 300\r\n    NORM_W = 600\r\n    for i in range(5):\r\n        image = next(iter(f_name))\r\n        image = tf.io.read_file(img_dir + image + '.jpg')\r\n        image = tf.io.decode_jpeg(image, channels=3)\r\n        image = tf.image.resize(image, [NORM_H, NORM_W])\r\n        image = tf.cast(image / 255., tf.float32)\r\n        image = tf.expand_dims(image, 0)\r\n        yield [image]\r\n\r\nfrozen_graph='./tflite_graph.pb'\r\ninput_arrays=[\"normalized_input_image_tensor\"]\r\noutput_arrays=['TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3']\r\ninput_shapes={\"normalized_input_image_tensor\":[1,300,600,3]}\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(frozen_graph,input_arrays,output_arrays,input_shapes=input_shapes)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset=rep_data_gen\r\nconverter.allow_custom_ops=True\r\n\r\ntflite_quant_model = converter.convert()\r\n\r\nwith open('quantized_model.tflite', 'wb') as f:\r\n    f.write(tflite_quant_model)\r\n\r\n\r\n# Copy and paste the output here.\r\n\r\nTraceback (most recent call last):\r\n  File \"convert_tflite.py\", line 79, in <module>\r\n    tflite_quant_model = converter.convert()\r\n  File \"/home/pychen/anaconda3/envs/tf_v115/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py\", line 993, in convert\r\n    inference_output_type)\r\n  File \"/home/pychen/anaconda3/envs/tf_v115/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py\", line 239, in _calibrate_quantize_model\r\n    inference_output_type, allow_float)\r\n  File \"/home/pychen/anaconda3/envs/tf_v115/lib/python3.7/site-packages/tensorflow_core/lite/python/optimize/calibrator.py\", line 75, in calibrate_and_quantize\r\n    self._calibrator.FeedTensor(calibration_sample)\r\n  File \"/home/pychen/anaconda3/envs/tf_v115/lib/python3.7/site-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py\", line 112, in FeedTensor\r\n    return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_FeedTensor(self, input_value)\r\nValueError: Failed to convert value into readable tensor.\r\n\r\n", "comments": ["Do you really need to work with an old TF version like `1.15`?", "The TFLite converter from the recent TF 2.x versions also is able to convert the TF 1.x models. Please try conversion with the recent version.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Hi,\r\n\r\nI have the same problem using TF2.2.0\r\nThe problem appears when using image data generator. If I just use a single image and yield this, no problem.\r\n\r\nI created a stack post with more info here: https://stackoverflow.com/questions/64795017/when-using-generator-for-representative-dataset-in-quantization-it-failed-to-co", "same question.", "> \r\n> \r\n> same question.\r\n\r\nHi @fmbao, please check out the stackoverflow post I made. This could be your answer. Cheers \r\n https://stackoverflow.com/questions/64795017/when-using-generator-for-representative-dataset-in-quantization-it-failed-to-co\r\n", "> > same question.\r\n> \r\n> Hi @fmbao, please check out the stackoverflow post I made. This could be your answer. Cheers\r\n> https://stackoverflow.com/questions/64795017/when-using-generator-for-representative-dataset-in-quantization-it-failed-to-co\r\n\r\nI slove the question. I convert model to tflite by Tf2.Howeve,interpret on TF 1.15. So the error happen. I change the envs to TF2, the question sovled"]}, {"number": 43429, "title": "Warning when trying to Load CIFAR10 in Tensorflow 1.x", "body": "**System information**\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** Google Colab\r\n- **TensorFlow version (use command below):** v1.15.2-0-g5d80e1e8e6 1.15.2\r\n- **Python version:** 3.6.9\r\n\r\n**Describe the current behavior**\r\nWARNING:tensorflow:Entity <function _get_dataset_from_filename at 0x7f1866eac6a8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <function _get_dataset_from_filename at 0x7f1866eac6a8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\nWARNING: Entity <function _get_dataset_from_filename at 0x7f1866eac6a8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <bound method TopLevelFeature.decode_example of FeaturesDict({\r\n    'image': Image(shape=(32, 32, 3), dtype=tf.uint8),\r\n    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\r\n})> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method TopLevelFeature.decode_example of FeaturesDict({\r\n    'image': Image(shape=(32, 32, 3), dtype=tf.uint8),\r\n    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\r\n})> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\r\nWARNING: Entity <bound method TopLevelFeature.decode_example of FeaturesDict({\r\n    'image': Image(shape=(32, 32, 3), dtype=tf.uint8),\r\n    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\r\n})> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\r\n**Describe the expected behavior**\r\n", "comments": ["@ayushm-agrawal \r\nPlease provide simple stand alone code to replicate the issue faced.\r\nCan you please try in 2.x and let us know if it helps resolve the issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43429\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43429\">No</a>\n"]}, {"number": 43428, "title": "[MLIR] Add involution based folding to TF", "body": "The involution property holds in a few of the TF ops so instead of adding redundant folding operations to every single one of them, we can simply label the ops we want to support with the involution tag and MLIR automatically folds.\r\n", "comments": ["@andyly @smit-hinsu ", "@ahmedsabie Can you please resolve conflicts? Thanks!", "@ahmedsabie Any update on this PR? Please. Thanks!"]}, {"number": 43427, "title": "flatbuffer_conversions changes for resize bilinear", "body": "Flatbuffer conversion changes related to porting RESIZE_BILINEAR to TFLu. As suggested by @advaitjain in [b/168339972](https://issuetracker.google.com/u/1/issues/168339972), this was made as a separate PR from [this PR](https://github.com/tensorflow/tensorflow/pull/43426), containing the rest of the changes related to the porting.", "comments": ["There appear to be some merge conflicts. Can you merge master again? "]}, {"number": 43426, "title": "Support RESIZE_BILINEAR in TFLu", "body": "Adds support for RESIZE_BILINEAR in TFLu. This PR is related to this issue [b/168339972](https://issuetracker.google.com/u/1/issues/168339972). It depends on the flatbuffer conversion changes in [this PR](https://github.com/tensorflow/tensorflow/pull/43427), which were submitted as a separate PR as suggested by @advaitjain, and is not relevant for review until that PR has been merged.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@patriklaurell Can you please check @advaitjain's comments and keep us posted ? Thanks!", "@patriklaurell Can you please resolve conflicts? Thanks!", "> @patriklaurell Can you please resolve conflicts? Thanks!\r\n\r\n@patriklaurell no need for any changes. Its on my list of things to address this PR but let's keep this on ice for now.", "> > @patriklaurell Can you please resolve conflicts? Thanks!\r\n> \r\n> @patriklaurell no need for any changes. Its on my list of things to address this PR but let's keep this on ice for now.\r\n\r\n@advaitjain I missed this message. I just merged in latest master and updated a few things that needed updating after merge", "Resolved the conflict. Ready for review @advaitjain ", "@patriklaurell Can you please resolve conflicts? Thanks!", "Resolved the conflicts. Ready for review @advaitjain @petewarden ", "Sorry for the failing test. There were some updates to the lite kernel that I failed to merge properly. I reset the branch to 081d051 re-did the merge with master and made some additional updates that were needed. @advaitjain @petewarden ", "@petewarden I fixed the formatting of the includes. I did not find any example of how to use TF_LITE_REPORT_ERROR in the other micro kernels, they seem to all use TF_LITE_KERNEL_LOG, so I used TF_LITE_KERNEL_LOG as well.\r\n\r\nI checked in [CONTRIBUTING.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md#before-submitting-your-pr) and it says to run all code through `clang-format --style=google`, which is what I do locally, but that did not fix those formatting of the includes. Do you know if there are additional formatting rules used in the internal tests?", "Thanks! You're right about the macro name, the checker error message mentions TF_LITE_REPORT_ERROR but TF_LITE_KERNEL_LOG is the right one to use.\r\n\r\nIt looks like you may have encountered a difference between the clang format rules we use internally and those available through the external tool, unfortunately. I would guess the open source tool doesn't care about whether blank lines separate blocks of includes (e.g. <> includes versus \"\" includes), but the internal check does. We're trying to ensure that this doesn't happen, but for now it looks like you'll have to rely on us manually communicating differences like this when they come up. Thanks for your patience!", "@petewarden I updated the copyright notice in the new files to address the failing \"TFLite Micro\" test", "@petewarden @advaitjain sorry for the failing test. I am looking into it now. Will push the fix as soon as possible.", "Internal checks seemed to be running into merge conflicts. I have pushed a commit that updates this PR to tip of tree. My assumption is that we should now be able to get this merged."]}, {"number": 43424, "title": "Update to include filesystem:filesystem_interface in libtensorflow_framework.so", "body": "While trying to build a modular file system from outside of TF code tree\r\n(with pip install tf-nightly headers), the following error happens:\r\n```\r\nSymbol not found: _TF_SetStatus\\n  Referenced\r\n```\r\n\r\nThe reason for the error was caused by `filesystem:filesystem_interface`\r\ndependency in bazel is not part of the .so.\r\n\r\nThis PR add `filesystem:filesystem_interface` as part of the dependency.\r\n\r\nAlso `filesystem:filesystem_interface` replaces `filesystem:modular_filesystem`\r\nas dependency can be reduced to just need `filesystem:filesystem_interface`.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 43423, "title": "Cannot fit concatenated model to two datasets as input", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: MX 150\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior** Can't input two datasets when a model merges two models. The error I get is : \r\n\r\n`Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'tensorflow.python.data.ops.dataset_ops.TensorDataset'>\"}), <class 'NoneType'>`\r\n\r\n**Describe the expected behavior** I should be able to input the two data sets or I should be able to somehow merge them together into a single dataset\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nCode : \r\n```\r\n\r\nimport tensorflow as tf\r\n\r\n# Create first model\r\nmodel1 = tf.keras.Sequential()\r\nmodel1.add(tf.keras.layers.Dense(1))\r\nmodel1.compile()\r\nmodel1.build([None,3])\r\n\r\n# Create second model\r\nmodel2 = tf.keras.Sequential()\r\nmodel2.add(tf.keras.layers.Dense(1))\r\nmodel2.compile()\r\nmodel2.build([None,3])\r\n\r\n\r\n# Concatenate\r\nfusion_model = tf.keras.layers.Concatenate()([model1.output, model2.output])\r\nt = tf.keras.layers.Dense(1, activation='tanh')(fusion_model)\r\nmodel = tf.keras.models.Model(inputs=[model1.input, model2.input], outputs=t)\r\nmodel.compile()\r\n\r\n#Datasets\r\nds1 = tf.data.Dataset.from_tensors(([1,2,3],1))\r\nds2 = tf.data.Dataset.from_tensors(([1,2,3], 2))\r\n\r\nprint(ds1)\r\nprint(ds2)\r\n# Fit\r\nmodel.fit([ds1,ds2])\r\n```\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["I think you could try to use `tf.data.Dataset.zip`", "@bhack .zip would try to zip the labels as well. In previous versions you would input the input parameters as [i1,i2,i_n]", "I also did:\r\n\r\n```\r\n ps = zip(train_text_v, train_ds)\r\n\r\n     for i, i2 in ps:\r\n      txt_data = i[0]\r\n      vis_data = i2[0]\r\n      labels = i[1]\r\n\r\n      print(\"INPUT SHAPES!\")\r\n      print(txt_data['input_token'].shape)\r\n      print(txt_data['masked_token'].shape)\r\n      print(vis_data.shape)\r\n\r\n      yield ({\r\n          'input_token' : txt_data['input_token'], \r\n          'masked_token' : txt_data['masked_token'], \r\n          'visual' : vis_data\r\n      }, labels)\r\n```\r\n\r\nUsing this as a function works! BUT! It only goes for a single epoch. Then the data runs out.\r\n\r\nSo for that I construct a tf.data.Dataset from the data and use this function as a generator, but that still gives me a problem `as_list() is not defined on an unknown TensorShape`.\r\n\r\nand even after providing the 'output_shapes' argument it is unclear how to use that (and gives me another error)", "Can you post your code with the `from_generator`?", "@bhack here's the full version (including .fit())\r\n\r\n\r\n```\r\n    def create_early_fusion_ds():\r\n     ps = zip(train_text_v, train_ds)\r\n\r\n     for i, i2 in ps:\r\n      txt_data = i[0]\r\n      vis_data = i2[0]\r\n      labels = i[1]\r\n\r\n      print(\"INPUT SHAPES!\")\r\n      print(txt_data['input_token'].shape)\r\n      print(txt_data['masked_token'].shape)\r\n      print(vis_data.shape)\r\n\r\n      yield {\r\n          'input_token' : txt_data['input_token'], \r\n          'masked_token' : txt_data['masked_token'], \r\n          'visual' : vis_data\r\n      }, labels\r\n\r\n    ds_train = tf.data.Dataset.from_generator(generator = create_early_fusion_ds,\r\n                                                       output_types=(\r\n                                                               {\r\n                                                                  'input_token' : tf.int64,\r\n                                                                  'masked_token' : tf.int64, \r\n                                                                  'visual': tf.float64\r\n                                                               }, tf.int64),\r\n                                                        output_shapes = (\r\n                                                            {\r\n                                                                'input_token' : tf.TensorShape([None, 300]),\r\n                                                                'masked_token' : tf.TensorShape([None, 300]),\r\n                                                                'visual' : tf.TensorShape([None, img_width, img_width, 3])\r\n                                                            }, tf.TensorShape([None, len(classes)])\r\n                                                        )\r\n                                                      )\r\n                    \r\n#     model.save(\"sxport.h5\" , save_format='tf')\r\n    history = model.fit(ds_train,\r\n                        #validation_data = create_early_fusion_ds_val(),\r\n                        epochs = 10,\r\n                        batch_size = 128,\r\n                        callbacks = [es])\r\n```\r\n\r\n\r\n\r\nThis results in the error:  `(0) Invalid argument:  ValueError: `generator` yielded an element of shape (2,) where an element of shape (None, 19) was expected.`\r\n\r\nwhere 2 is the batch size & 19 is the number of classes i have\r\n\r\n\r\nI think it's quite odd this doesn't work while calling the method create_early_fusion_ds() as argument for the fit(X) works for 1 epoch. Additionally the from_generator method requires output_types and output_shapes which further complicate this. Anyway, not sure what I'm doing wrong here :)", "Please re-edit the last post with a very minimal but complete, copy, paste and runnnable example.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43422, "title": "ValueError: Can only save/restore ResourceVariables when executing eagerly", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: No\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:v2.3.0-rc2-23-gb36436b087 \r\n-   **Python version**:3.7.9\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:10.1/7.6.5\r\n-   **GPU model and memory**:2080 Ti \r\n-   **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nWhen I run, prog run till base.save\r\nWhile run tf.compat.v1.train.Saver(), it hint\r\n`\r\n\r\n    Traceback (most recent call last):    \r\n      File \"main.py\", line 131, in <module>\r\n           main()\r\n      File \"main.py\", line 113, in main\r\n           T.train()\r\n      File \"/home/yang/Documents/paper/refer/MGNN-SPred-master/Train.py\", line 40, in train\r\n           self.model.save()\r\n      File \"/home/yang/Documents/paper/refer/MGNN-SPred-master/models/base.py\", line 116, in save\r\n           self.saver.save(self.sess, name)\r\n      File \"/home/yang/anaconda3/envs/tsorflw/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 1180, in save\r\n           checkpoint_file, build_save=True, build_restore=False)\r\n      File \"/home/yang/anaconda3/envs/tsorflw/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 852, in _build_eager\r\n           checkpoint_path, build_save=build_save, build_restore=build_restore)\r\n      File \"/home/yang/anaconda3/envs/tsorflw/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 886, in _build\r\n            build_restore=build_restore)\r\n      File \"/home/yang/anaconda3/envs/tsorflw/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 490, in _build_internal\r\n            names_to_saveables)\r\n      File \"/home/yang/anaconda3/envs/tsorflw/lib/python3.7/site-packages/tensorflow/python/training/saving/saveable_object_util.py\", line 360, in validate_and_slice_inputs\r\n            for converted_saveable_object in saveable_objects_for_op(op, name):\r\n       File \"/home/yang/anaconda3/envs/tsorflw/lib/python3.7/site-packages/tensorflow/python/training/saving/saveable_object_util.py\", line 209, in saveable_objects_for_op\r\n           \"executing eagerly, got type: %s.\" % type(op))\r\n       ValueError: Can only save/restore ResourceVariables when executing eagerly, got type: <class'tensorflow.python.framework.ops.Tensor'>.\r\n`\r\n### Source code / logs\r\nI run this model \r\nhttps://github.com/Autumn945/MGNN-SPred\r\nand  change some tf1 code to tf2\r\n\r\n`    \r\n    \r\n    def make_model(self):\r\n        with tf.compat.v1.variable_scope('Graph', reuse=tf.compat.v1.AUTO_REUSE, regularizer=self.l2_loss('all')) as self.graph_scope:\r\n            n = args.nb_nodes\r\n            k = args.dim_k\r\n            self.embedding_matrix = tf.compat.v1.get_variable(name='emb_w', shape=[n, k])\r\n            with tf.compat.v1.variable_scope('graph_agg', reuse=tf.compat.v1.AUTO_REUSE) as self.graph_agg_scope:\r\n                pass\r\n\r\n        with tf.compat.v1.variable_scope('Network', reuse=tf.compat.v1.AUTO_REUSE, regularizer=self.l2_loss('all')):\r\n            score, label = self.forward(*self.inputs)\r\n            seq_loss = tf.compat.v1.losses.softmax_cross_entropy(label, score)\r\n            tf.summary.scalar('seq_loss', seq_loss)\r\n\r\n        self.loss = seq_loss\r\n        self.loss += tf.compat.v1.losses.get_regularization_loss()\r\n\r\n        opt = tf.compat.v1.train.AdamOptimizer(learning_rate=args.lr)\r\n        self.minimizer = opt.minimize(self.loss)\r\n        tf.summary.scalar('loss', self.loss)\r\n\r\n\r\n        graph_var_list = tf.compat.v1.trainable_variables(scope='^Graph/')\r\n        network_var_list = tf.compat.v1.trainable_variables(scope='^Network/')\r\n        for v in graph_var_list:\r\n            print('graph', v)\r\n        for v in network_var_list:\r\n            print('network', v)\r\n\r\n        self.saver = tf.compat.v1.train.Saver()\r\n        self.sess = self.get_session()\r\n        self.sess.run(tf.compat.v1.global_variables_initializer())\r\n    def save(self):\r\n        name = f'{self.save_dir}/model.ckpt'\r\n        self.saver.save(self.sess, name)\r\n`\r\n\r\nand \r\n\r\n` \r\n\r\n     for ep in range(args.epochs):\r\n            pbar = tqdm(total=args.nb_vali_step, desc='training', leave=False)\r\n            loss = []\r\n            t0 = time.time()\r\n            for _ in range(args.nb_vali_step):\r\n                data = next(data_generator)\r\n                _loss = self.model.fit(data)\r\n\r\n                loss.append(_loss)\r\n                pbar.update(1)\r\n            pbar.close()\r\n            train_time = time.time() - t0\r\n\r\n            vali_v, vali_str = self.metric('vali')\r\n            if vali_v > best_vali:\r\n                brk = 0\r\n                best_vali = vali_v\r\n                self.model.save()\r\n            else:\r\n                brk += 1\r\n            red = (brk == 0)`\r\n\r\n\r\nIs it wrong ? Where? I need help", "comments": ["@unlimition \r\nCode provided is incomplete for us to replicate the issue faced, please share a colab gist with error reported.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43422\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43422\">No</a>\n", "\u6211\u4e5f\u8fd9\u6837\uff0c\u4e0d\u77e5\u9053\u4e3a\u5565\r\n", "> \r\n> \r\n> \u6211\u4e5f\u8fd9\u6837\uff0c\u4e0d\u77e5\u9053\u4e3a\u5565\r\n\r\n\u4f60\u73b0\u5728\u89e3\u51b3\u4e86\u5417", "I meet this issue when running codes in [this notebook](https://github.com/magenta/magenta-demos/blob/master/jupyter-notebooks/RL_Tuner.ipynb). I am able to resolve this by adding this line in the front.\r\n```python\r\ntf.disable_v2_behavior()\r\n```"]}, {"number": 43421, "title": "Unable to build Tensorflow 2.3 on OSX with LLVM", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: OSX 10.15.6\r\n- TensorFlow installed from: source\r\n- TensorFlow version: 2.3\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): clang version 10.0.1 \r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No GPU\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI am unable to build from source and I keep getting errors around:\r\n> bazel-out/darwin-opt-exec-50AE0418/bin/tensorflow/core/protobuf/saved_object_graph.pb.h:131:3: error: expected identifier TRUE = 1,\r\n> In file included from bazel-out/darwin-opt-exec-50AE0418/bin/tensorflow/core/protobuf/meta_graph.pb.h:43:\r\nbazel-out/darwin-opt-exec-50AE0418/bin/tensorflow/core/protobuf/saved_object_graph.pb.h:132:3: error: expected identifier\r\n  FALSE = 2,\r\n  ^\r\n/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include/mach/boolean.h:85:17: note: expanded from macro 'FALSE'\r\n#define FALSE   0\r\n\r\nI tried with different compilers from brew but this error keeps appearing.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n% export BAZEL_USE_CPP_ONLY_TOOLCHAIN=1\r\n% export CC=/usr/local/opt/llvm/bin/clang\r\n% bazel build --action_env CC=/usr/local/opt/llvm/bin/clang  --config=noaws --config=nogcp --config=mkl -c opt --copt=-O2 --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --copt=-mfma --copt=-mavx2 //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\n\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=174\r\nINFO: Reading rc options for 'build' from /Users/stb/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /Users/stb/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /Users/stb/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/Users/stb/opt/anaconda3/envs/tfmkl_py37/bin/python3 --action_env PYTHON_LIB_PATH=/Users/stb/opt/anaconda3/envs/tfmkl_py37/lib/python3.7/site-packages --python_path=/Users/stb/opt/anaconda3/envs/tfmkl_py37/bin/python3 --config=xla --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file /Users/stb/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /Users/stb/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /Users/stb/tensorflow/.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:noaws in file /Users/stb/tensorflow/.bazelrc: --define=no_aws_support=true\r\nINFO: Found applicable config definition build:nogcp in file /Users/stb/tensorflow/.bazelrc: --define=no_gcp_support=true\r\nINFO: Found applicable config definition build:mkl in file /Users/stb/tensorflow/.bazelrc: --define=build_with_mkl=true --define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 -c opt\r\nINFO: Found applicable config definition build:macos in file /Users/stb/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule git_repository defined at:\r\n  /private/var/tmp/_bazel_stb/c17790719609824de969482a41b5bf78/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (390 packages loaded, 30618 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /Users/stb/tensorflow/tensorflow/compiler/mlir/tensorflow/BUILD:1620:1: C++ compilation of rule '//tensorflow/compiler/mlir/tensorflow:compile_mlir_util_no_tf_dialect_passes' failed (Exit 1)\r\nIn file included from tensorflow/compiler/mlir/tensorflow/utils/compile_mlir_util.cc:16:\r\nIn file included from ./tensorflow/compiler/mlir/tensorflow/utils/compile_mlir_util.h:29:\r\nIn file included from ./tensorflow/core/common_runtime/device.h:42:\r\nIn file included from ./tensorflow/core/framework/resource_mgr.h:32:\r\n./tensorflow/core/framework/type_index.h:61:17: warning: unused variable 'hash_bit' [-Wunused-variable]\r\n    static bool hash_bit[1];\r\n                ^\r\nIn file included from tensorflow/compiler/mlir/tensorflow/utils/compile_mlir_util.cc:45:\r\nIn file included from ./tensorflow/compiler/mlir/tensorflow/translate/import_model.h:25:\r\nIn file included from ./tensorflow/cc/saved_model/bundle_v2.h:28:\r\nIn file included from bazel-out/darwin-opt-exec-50AE0418/bin/tensorflow/core/protobuf/meta_graph.pb.h:43:\r\nbazel-out/darwin-opt-exec-50AE0418/bin/tensorflow/core/protobuf/saved_object_graph.pb.h:131:3: error: expected identifier\r\n  TRUE = 1,\r\n  ^\r\n/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include/mach/boolean.h:81:17: note: expanded from macro 'TRUE'\r\n#define TRUE    1\r\n                ^\r\nIn file included from tensorflow/compiler/mlir/tensorflow/utils/compile_mlir_util.cc:45:\r\nIn file included from ./tensorflow/compiler/mlir/tensorflow/translate/import_model.h:25:\r\nIn file included from ./tensorflow/cc/saved_model/bundle_v2.h:28:\r\nIn file included from bazel-out/darwin-opt-exec-50AE0418/bin/tensorflow/core/protobuf/meta_graph.pb.h:43:\r\nbazel-out/darwin-opt-exec-50AE0418/bin/tensorflow/core/protobuf/saved_object_graph.pb.h:132:3: error: expected identifier\r\n  FALSE = 2,\r\n  ^\r\n/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include/mach/boolean.h:85:17: note: expanded from macro 'FALSE'\r\n#define FALSE   0\r\n                ^\r\nIn file included from tensorflow/compiler/mlir/tensorflow/utils/compile_mlir_util.cc:45:\r\nIn file included from ./tensorflow/compiler/mlir/tensorflow/translate/import_model.h:25:\r\nIn file included from ./tensorflow/cc/saved_model/bundle_v2.h:28:\r\nIn file included from bazel-out/darwin-opt-exec-50AE0418/bin/tensorflow/core/protobuf/meta_graph.pb.h:43:\r\nbazel-out/darwin-opt-exec-50AE0418/bin/tensorflow/core/protobuf/saved_object_graph.pb.h:138:31: error: cannot initialize a variable of type 'const tensorflow::ExperimentalCompile' with an rvalue of type 'int'\r\nconstexpr ExperimentalCompile ExperimentalCompile_MAX = FALSE;\r\n                              ^                         ~~~~~\r\n1 warning and 3 errors generated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /Users/stb/tensorflow/tensorflow/python/tools/BUILD:82:1 C++ compilation of rule '//tensorflow/compiler/mlir/tensorflow:compile_mlir_util_no_tf_dialect_passes' failed (Exit 1)\r\nINFO: Elapsed time: 4667.715s, Critical Path: 318.82s\r\nINFO: 5927 processes: 5927 local.\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["@ianlokh \r\n\r\nJust to verify did you follow the instructions mentioned [here.](https://www.tensorflow.org/install/source#macos_1).\r\nPlease, see tested build configurations from [here](https://www.tensorflow.org/install/source#cpu_2).Thanks!", "> @ianlokh\r\n> \r\n> Just to verify did you follow the instructions mentioned [here.](https://www.tensorflow.org/install/source#macos_1).\r\n> Please, see tested build configurations from [here](https://www.tensorflow.org/install/source#cpu_2).Thanks!\r\n\r\nHi Ravikyram, thanks for the follow up. Yes I followed the instructions but as my Xcode version is 12, I downloaded llvm 10.0.1 from brew and tried to compile using the llvm 10.0.1 as per build configuration. However I keep getting into the same issue.\r\n\r\nAs a correction to the issue posted, I used bazel 3.1 and llvm 10.0.1. This is as per tested build configuration listed on the Tensorflow webpage", "I am running into the same problem with XCode 12.\r\n\r\n@ravikyram it seems like the problem has been introduced by 57f0a5e0b67720f0bc94e4decefc09fbff2f2416. Reverting the commit fixes the build issue for me.", "> I am running into the same problem with XCode 12.\r\n> \r\n> @ravikyram it seems like the problem has been introduced by [57f0a5e](https://github.com/tensorflow/tensorflow/commit/57f0a5e0b67720f0bc94e4decefc09fbff2f2416). Reverting the commit fixes the build issue for me.\r\n\r\nHi Lukas, thanks for your inputs. How do I go about getting the previous commit? I ran \"git clone https://github.com/tensorflow/tensorflow.git\" and assumed that it was the latest stable version.\r\n\r\n", "use ON/OFF instead of TRUE/FALSE should work.\r\n\r\n```diff\r\ndiff --git a/tensorflow/core/protobuf/saved_object_graph.proto b/tensorflow/core/protobuf/saved_object_graph.proto\r\nindex e4293bcddf..c915d0224c 100644\r\n--- a/tensorflow/core/protobuf/saved_object_graph.proto\r\n+++ b/tensorflow/core/protobuf/saved_object_graph.proto\r\n@@ -153,8 +153,8 @@ message SavedVariable {\r\n // See `tf.function` for details.\r\n enum ExperimentalCompile {\r\n   NONE = 0;\r\n-  TRUE = 1;\r\n-  FALSE = 2;\r\n+  ON = 1;\r\n+  OFF = 2;\r\n }\r\n \r\n // Represents `FunctionSpec` used in `Function`. This represents a\r\ndiff --git a/tensorflow/python/saved_model/function_serialization.py b/tensorflow/python/saved_model/function_serialization.py\r\nindex 777af9bd5b..9660710c9d 100644\r\n--- a/tensorflow/python/saved_model/function_serialization.py\r\n+++ b/tensorflow/python/saved_model/function_serialization.py\r\n@@ -50,8 +50,8 @@ def _serialize_function_spec(function_spec, coder):\r\n   # See `tf.function` and the ExperimentalCompile proto for details.\r\n   proto.experimental_compile = {\r\n       None: saved_object_graph_pb2.ExperimentalCompile.NONE,\r\n-      True: saved_object_graph_pb2.ExperimentalCompile.TRUE,\r\n-      False: saved_object_graph_pb2.ExperimentalCompile.FALSE,\r\n+      True: saved_object_graph_pb2.ExperimentalCompile.ON,\r\n+      False: saved_object_graph_pb2.ExperimentalCompile.OFF,\r\n   }.get(function_spec.experimental_compile)\r\n \r\n   return proto\r\n\r\n```", "Hi Lukas and Koan-sin, thank you for your help!\r\n\r\nI eventually managed to compile it as suggested by Lukas - to revert to the previous commit.\r\n\r\ngit checkout b95f88a5e759367ae80bad988fdd35cbd0f604b4\r\n\r\nThanks again!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43421\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43421\">No</a>\n"]}]